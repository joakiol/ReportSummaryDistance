Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882?891,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA Tree Kernel-based Unified Frameworkfor Chinese Zero Anaphora ResolutionFang Kong  Guodong Zhou*JiangSu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and Technology Soochow University{kongfang, gdzhou}@suda.edu.cn* Corresponding authorAbstractThis paper proposes a unified framework forzero anaphora resolution, which can be di-vided into three sub-tasks: zero anaphor detec-tion, anaphoricity determination andantecedent identification.
In particular, all thethree sub-tasks are addressed using tree ker-nel-based methods with appropriate syntacticparse tree structures.
Experimental results on aChinese zero anaphora corpus show that theproposed tree kernel-based methods signifi-cantly outperform the feature-based ones.
Thisindicates the critical role of the structural in-formation in zero anaphora resolution and thenecessity of tree kernel-based methods inmodeling such structural information.
To ourbest knowledge, this is the first systematicwork dealing with all the three sub-tasks inChinese zero anaphora resolution via a unifiedframework.
Moreover, we release a Chinesezero anaphora corpus of 100 documents,which adds a layer of annotation to the manu-ally-parsed sentences in the Chinese Treebank(CTB) 6.0.1 IntroductionAs one of the most important techniques in dis-course analysis, anaphora resolution has been afocus of research in Natural Language Processing(NLP) for decades and achieved much success inEnglish recently (e.g.
Soon et al 2001; Ng andCardie 2002; Yang et al 2003, 2008; Kong et al2009).However, there is little work on anaphora reso-lution in Chinese.
A major reason for this phe-nomenon is that Chinese, unlike English, is a pro-drop language, whereas in English, definite nounphrases (e.g.
the company) and overt pronouns (e.g.he) are frequently employed as referring expres-sions, which refer to preceding entities.
Kim (2000)compared the use of overt subjects in English andChinese.
He found that overt subjects occupy over96% in English, while this percentage drops toonly 64% in Chinese.
This indicates the prevalenceof zero anaphors in Chinese and the necessity ofzero anaphora resolution in Chinese anaphora reso-lution.
Since zero anaphors give little hints (e.g.number or gender) about their possible antecedents,zero anaphora resolution is much more challengingthan traditional anaphora resolution.Although Chinese zero anaphora has beenwidely studied in the linguistics research (Li andThompson 1979; Li 2004), only a small body ofprior work in computational linguistics deals withChinese zero anaphora resolution (Converse 2006;Zhao and Ng 2007).
Moreover, zero anaphor de-tection, as a critical component for real applica-tions of zero anaphora resolution, has been largelyignored.This paper proposes a unified framework forChinese zero anaphora resolution, which can bedivided into three sub-tasks: zero anaphor detec-tion, which detects zero anaphors from a text, ana-phoricity determination, which determines whethera zero anaphor is anaphoric or not, and antecedentidentification, which finds the antecedent for ananaphoric zero anaphor.
To our best knowledge,this is the first systematic work dealing with all thethree sub-tasks via a unified framework.
Moreover,we release a Chinese zero anaphora corpus of 100documents, which adds a layer of annotation to the882manually-parsed sentences in the Chinese Tree-bank (CTB) 6.0.
This is done by assigning ana-phoric/non-anaphoric zero anaphora labels to thenull constituents in a parse tree.
Finally, this paperillustrates the critical role of the structural informa-tion in zero anaphora resolution and the necessityof tree kernel-based methods in modeling suchstructural information.The rest of this paper is organized as follows.Section 2 briefly describes the related work onboth zero anaphora resolution and tree kernel-based anaphora resolution.
Section 3 introduces theoverwhelming problem of zero anaphora in Chi-nese and our developed Chinese zero anaphoracorpus, which is available for research purpose.Section 4 presents our tree kernel-based unifiedframework in zero anaphora resolution.
Section 5reports the experimental results.
Finally, we con-clude our work in Section 6.2 Related WorkThis section briefly overviews the related work onboth zero anaphora resolution and tree kernel-based anaphora resolution.2.1 Zero anaphora resolutionAlthough zero anaphors are prevalent in many lan-guages, such as Chinese, Japanese and Spanish,there only have a few works on zero anaphoraresolution.Zero anaphora resolution in ChineseConverse (2006) developed a Chinese zero anaph-ora corpus which only deals with zero anaphoracategory ?-NONE- *pro*?
for dropped sub-jects/objects and ignores other categories, such as?-NONE- *PRO*?
for non-overt subjects in non-finite clauses.
Besides, Converse (2006) proposeda rule-based method to resolve the anaphoric zeroanaphors only.
The method did not consider zeroanaphor detection and anaphoric identification, andperformed zero anaphora resolution using theHobbs algorithm (Hobbs, 1978), assuming theavailability of golden anaphoric zero anaphors andgolden parse trees.Instead, Zhao and Ng (2007) proposed feature-based methods to zero anaphora resolution on thesame corpus from Convese (2006).
However, theyonly considered zero anaphors with explicit nounphrase referents and discarded those with split an-tecedents or referring to events.
Moreover, theyfocused on the sub-tasks of anaphoricity determi-nation and antecedent identification.
For zero ana-phor detection, a simple heuristic rule wasemployed.
Although this rule can recover almostall the zero anaphors, it suffers from very low pre-cision by introducing too many false zero anaphorsand thus leads to low performance in anaphoricitydetermination, much due to the imbalance betweenpositive and negative training examples.Zero anaphora resolution in JapaneseSeki et al (2002) proposed a probabilistic modelfor the sub-tasks of anaphoric identification andantecedent identification with the help of a verbdictionary.
They did not perform zero anaphor de-tection, assuming the availability of golden zeroanaphors.
Besides, their model needed a large-scale corpus to estimate the probabilities to preventthem from the data sparseness problem.Isozaki and Hirao (2003) explored some rankingrules and a machine learning method on zeroanaphora resolution.
However, they assumed thatzero anaphors were already detected and each zeroanaphor?s grammatical case was already deter-mined by a zero anaphor detector.Iida et al (2006) explored a machine learningmethod for the sub-task of antecedent identifica-tion using rich syntactic pattern features, assumingthe availability of golden anaphoric zero anaphors.Sasano et al (2008) proposed a fully-lexicalizedprobabilistic model for zero anaphora resolution,which estimated case assignments for the overtcase components and the antecedents of zero ana-phors simultaneously.
However, this model neededcase frames to detect zero anaphors and a large-scale corpus to construct these case frames auto-matically.For Japanese zero anaphora, we do not see anyreports about zero anaphora categories.
Moreover,all the above related works we can find on Japa-nese zero anaphora resolution ignore zero anaphordetection, focusing on either anaphoricity determi-nation or antecedent identification.
Maybe, it iseasy to detect zero anaphors in Japanese.
However,it is out of the scope of our knowledge and thispaper.Zero anaphora resolution in SpanishAs the only work we can find, Ferrandez and Peral(2000) proposed a hand-engineered rule-basedmethod for both anaphoricity determination and883antecedent identification.
That is, they ignored zeroanaphor detection.
Besides, they only dealt withzero anaphors that were in the subject position.2.2 Tree kernel-based anaphora resolutionAlthough there is no research on tree kernel-basedzero anaphora resolution in the literature, tree ker-nel-based methods have been explored in tradi-tional anaphora resolution to certain extent andachieved comparable performance with the domi-nated feature-based ones.
One main advantage ofkernel-based methods is that they are very effec-tive at reducing the burden of feature engineeringfor structured objects.
Indeed, the kernel-basedmethods have been successfully applied to minestructural information in various NLP techniquesand applications, such as syntactic parsing (Collinsand Duffy 2001; Moschitti 2004), semantic rela-tion extraction (Zelenko et al 2003; Zhao andGrishman 2005; Zhou et al 2007; Qian et al 2008),and semantic role labeling (Moschitti 2004).Representative works in tree kernel-basedanaphora resolution include Yang et al (2006) andZhou et al(2008).
Yang et al (2006) employed aconvolution tree kernel on anaphora resolution ofpronouns.
In particular, a document-level syntacticparse tree for an entire text was constructed by at-taching the parse trees of all its sentences to a new-added upper node.
Examination of three parse treestructures using different construction schemes(Min-Expansion, Simple-Expansion and Full-Expansion) on the ACE 2003 corpus showedpromising results.
However, among the three con-structed parse tree structures, there exists no obvi-ous overwhelming one, which can well coverstructured syntactic information.
One problem withthis tree kernel-based method is that all the con-structed parse tree structures are context-free anddo not consider the information outside the sub-trees.
To overcome this problem, Zhou et al (2008)proposed a dynamic-expansion scheme to auto-matically construct a proper parse tree structure foranaphora resolution of pronouns by taking predi-cate- and antecedent competitor-related informa-tion into consideration.
Besides, they proposed acontext-sensitive convolution tree kernel to com-pute the similarity between the parse tree structures.Evaluation on the ACE 2003 corpus showed thatthe dynamic-expansion scheme can well covernecessary structural information in the parse treefor anaphora resolution of pronouns and the con-text-sensitive convolution tree kernel much outper-formed other tree kernels.3 Task DefinitionThis section introduces the phenomenon of zeroanaphora in Chinese and our developed Chinesezero anaphora corpus.3.1 Zero anaphora in ChineseA zero anaphor is a gap in a sentence, which refersto an entity that supplies the necessary informationfor interpreting the gap.
Figure 1 illustrates an ex-ample sentence from Chinese TreeBank (CTB) 6.0(File ID=001, Sentence ID=8).
In this example,there are four zero anaphors denoted as ?i (i=1,2, ?4).
Generally, zero anaphors can be under-stood from the context and do not need to be speci-fied.A zero anaphor can be classified into either ana-phoric or non-anaphoric, depending on whether ithas an antecedent in the discourse.
Typically, azero anaphor is non-anaphoric when it refers to anextra linguistic entity (e.g.
the first or second per-son in a conversion) or its referent is unspecified inthe context.
Among the four anaphors in Figure 1,zero anaphors ?
1 and ?
4 are non-anaphoricwhile zero anaphors ?2 and ?3 are anaphoric,referring to noun phrase ????
?/building ac-tion?
and noun phrase ?????
?/new districtmanaging committee?
respectively.Chinese zero anaphora resolution is very diffi-cult due to following reasons: 1) Zero anaphorsgive little hints (e.g.
number or gender) about theirpossible antecedents.
This makes antecedent iden-tification much more difficult than traditionalanaphora resolution.
2) A zero anaphor can be ei-ther anaphoric or non-anaphoric.
In our corpus de-scribed in Section 3.2, about 60% of zero anaphorsare non-anaphoric.
This indicates the importanceof anaphoricity determination.
3) Zero anaphorsare not explicitly marked in a text.
This indicatesthe necessity of zero anaphor detection, which hasbeen largely ignored in previous research and hasproved to be difficult in our later experiments.884Figure 1: An example sentence from CTB 6.0, which contains four zero anaphors(the example is : ?????????????????????????????????????????????????????????????
?/ In order to standardize the building action and prevent theinorder phenomenon, the standing committee of new zone annouced a series of files to standardize building marketbased on the related provisions of China and Shanghai in time, and the realities of the development of Pudong areconsidered.
)3.2 Zero anaphora corpus in ChineseDue to lack of an available zero anaphora corpusfor research purpose, we develop a Chinese zeroanaphora corpus of 100 documents from CTB 6.0,which adds a layer of annotation to the manually-parsed sentences.
Hoping the public availability ofthis corpus can push the research of zero anaphoraresolution in Chinese and other languages.Figure 2: An example sentence annotated in CTB 6.0ID Cate-gory DescriptionAZAs ZAs1 -NONE-  *T*Used in topicalization andobject preposing con-structions6 7422 -NONE-  *Used in raising and pas-sive constructions 1 23 -NONE-  *PRO*Used in control structures.The *PRO* cannot besubstituted by an overtconstituent.219 3994 -NONE-  *pro*for dropped subject orobject.
394 4495 -NONE-  *RNR*Used for right node rais-ing (Cataphora) 0 366 Others Other unknown empty categories 92 92Total (100 documents, 35089 words) 712 1720Table 1: Statistics on different categories of  zeroanaphora (AZA and ZA indicates anaphoric zero ana-phor and zero anaphor respectively)885Figure 2 illustrates an example sentence anno-tated in CTB 6.0, where the special tag ?-NONE-?represents a null constituent and thus the occur-rence of a zero anaphor.
In our developed corpus,we need to annotate anaphoric zero anaphors usingthose null constituents with the special tag of ?-NONE-?.Table 1 gives the statistics on all the six catego-ries of zero anaphora.
Since we do not considerzero cataphora in the current version, we simplyredeem them non-anaphoric.
It shows that among1720 zero anaphors, only 712 (about 40%) areanaphoric.
This suggests the importance of ana-phoricity determination in zero anaphora resolution.Table 3 further shows that, among 712 anaphoriczero anaphors, 598 (84%) are intra-sentential andno anaphoric zero anaphors have their antecedentsoccurring two sentences before.Sentence distance AZAs0 5981 114>=2 0Table 3 Distribution of anaphoric zero anaphors oversentence distancesFigure 3 shows an example in our corpus corre-sponding to Figure 2.
For a non-anaphoric zeroanaphor, we replace the null constituent with ?E-iNZA?, where i indicates the category of zeroanaphora, with ?1?
referring to ?-NONE *T*?etc.
For an anaphoric zero anaphor, we replace itwith ?E-x-y-z-i AZA?, where x indicates the sen-tence id of its antecedent, y indicates the positionof the first word of its antecedent in the sentence, zindicates the position of the last word of its antece-dent in the sentence, and i indicates the category idof the null constituent.Figure 3: an example sentence annotated in our corpus4 Tree Kernel-based FrameworkThis section presents the tree kernel-based unifiedframework for all the three sub-tasks in zeroanaphora resolution.
For each sub-task, differentparse tree structures are constructed.
In particular,the context-sensitive convolution tree kernel, asproposed in Zhou et al (2008), is employed tocompute the similarity between two parse trees viathe SVM toolkit SVMLight.In the tree kernel-based framework, we performthe three sub-tasks, zero anaphor detection, ana-phoricity determination and antecedent identifica-tion in a pipeline manner.
That is, given a zeroanaphor candidate Z, the zero anaphor detector isfirst called to determine whether Z is a zero ana-phor or not.
If yes, the anaphoricity determiner isthen invoked to determine whether Z is an ana-phoric zero anaphor.
If yes, the antecedent identi-fier is finally awaked to determine its antecedent.In the future work, we will explore better ways ofintegrating the three sub-tasks (e.g.
joint learning).4.1 Zero anaphor detectionAt the first glance, it seems that a zero anaphor canoccur between any two constituents in a parse tree.Fortunately, an exploration of our corpus showsthat a zero anaphor always occurs just before apredicate1 phrase node (e.g.
VP).
This phenome-non has also been employed in Zhao and Ng (2007)in generating zero anaphor candidates.
In particular,if the predicate phrase node occurs in a coordinatestructure or is modified by an adverbial node, weonly need to consider its parent.
As shown in Fig-ure 1, zero anaphors may occur immediately to theleft of?
?/guide, ?
?/avoid, ?
?/appear, ?
?/according to, ??
/combine, ??
/promulgate,which cover the four true zero anaphors.
Therefore,it is simple but reliable in applying above heuristicrules to generate zero anaphor candidates.Given a zero anaphor candidate, it is critical toconstruct a proper parse tree structure for tree ker-nel-based zero anaphor detection.
The intuitionbehind our parser tree structure for zero anaphordetection is to keep the competitive information1 The predicate in Chinese can be categorized into verb predi-cate, noun predicate and preposition predicate.
In our corpus,about 93% of the zero anaphors are driven by verb predicates.In this paper, we only explore zero anaphors driven by verbpredicates.886about the predicate phrase node and the zero ana-phor candidate as much as possible.
In particular,the parse tree structure is constructed by first keep-ing the path from the root node to the predicatephrase node and then attaching all the immediateverbal phrase nodes and nominal phrase nodes.Besides, for the sub-tree rooted by the predicatephrase node, we only keep those paths ended withverbal leaf nodes and the immediate verbal andnominal nodes attached to these paths.
Figure 4shows an example of the parse tree structure corre-sponding to Figure 1 with the zero anaphor candi-date ?2 in consideration.During training, if a zero anaphor candidate hasa counterpart in the same position in the goldenstandard corpus (either anaphoric or non-anaphoric), a positive instance is generated.
Oth-erwise, a negative instance is generated.
Duringtesting, each zero anaphor candidate is presented tothe learned zero anaphor detector to determinewhether it is a zero anaphor or not.
Besides, since azero anaphor candidate is generated when a predi-cate phrase node appears, there may be two ormore zero anaphor candidates in the same position.However, there is normally one zero anaphor in thesame position.
Therefore, we just select the onewith maximal confidence as the zero anaphor inthe position and ignore others, if multiple zeroanaphor candidates occur in the same position.Figure 4: An example parse tree structure for zero ana-phor detection with the predicate phrase node and thezero anaphor candidate ?2  in black4.2 Anaphoricity determinationTo determine whether a zero anaphor is anaphoricor not, we limit the parse tree structure between theprevious predicate phrase node and the followingpredicate phrase node.
Besides, we only keep thoseverbal phrase nodes and nominal phrase nodes.Figure 5 illustrates an example of the parse treestructure for anaphoricity determination, corre-sponding to Figure 1 with the zero anaphor ?2 inconsideration.VPIPVV??
NP-SBJ VPNNNP-OBJ??
NP?
?VVpreventappearphenomenonFigure 5: An example parse tree structure for anaphoric-ity determination with the zero anaphor ?2 in consid-eration4.3 Antecedent identificationTo identify an antecedent for an anaphoric zeroanaphor, we adopt the Dynamic Expansion Tree,as proposed in Zhou et al (2008), which takespredicate- and antecedent competitor-related in-formation into consideration.
Figure 6 illustrates anexample parse tree structure for antecedent identi-fication, corresponding to Figure 1 with the ana-phoric zero anaphor ?
2 and the antecedentcandidate ????
?/building action?
in consid-eration.Figure 6: An example parse tree structure for antecedentidentification with the anaphoric zero anaphor ?2 andthe antecedent candidate ????
?/building action?
inconsiderationIn this paper, we adopt a similar procedure asSoon et al (2001) in antecedent identification.
Be-887sides, since all the anaphoric zero anaphors havetheir antecedents at most one sentence away, weonly consider antecedent candidates which are atmost one sentence away.
In particular, a document-level parse tree for an entire document is con-structed by attaching the parse trees of all its sen-tences to a new-added upper node, as done in Yanget al (2006), to deal with inter-sentential ones.5 Experimentation and DiscussionWe have systematically evaluated our tree kernel-based unified framework on our developed Chi-nese zero anaphora corpus, as described in Section3.2.
Besides, in order to focus on zero anaphorresolution itself and compare with related work, allthe experiments are done on golden parse treesprovided by CTB 6.0.
Finally, all the performancesare achieved using 5-fold cross validation.5.1 Experimental resultsZero anaphor detectionTable 4 gives the performance of zero anaphor de-tection, which achieves 70.05%, 83.24% and 76.08in precision, recall and F-measure, respectively.Here, the lower precision is much due to the simpleheuristic rules used to generate zero anaphors can-didates.
In fact, the ratio of positive and negativeinstances reaches about 1:12.
However, this ratio ismuch better than that (1:30) using the heuristic ruleas described in Zhao and Ng (2007).
It is alsoworth to point out that lower precision higher re-call is much beneficial than higher precision lowerrecall as higher recall means less filtering of truezero anaphors and we can still rely on anaphoricitydetermination to filter out those false zero ana-phors introduced by lower precision in zero ana-phor detection.P% R% F70.05 83.24 76.08Table 4: Performance of zero anaphor detectionAnaphoricity determinationTable 5 gives the performance of anaphoricity de-termination.
It shows that anaphoricity determina-tion on golden zero anaphors achieves very goodperformance of 89.83%, 84.21% and 86.93 in pre-cision, recall and F-measure, respectively, althoughuseful information, such as gender and number, isnot available in anaphoricity determination.
Thisindicates the critical role of the structural informa-tion in anaphoricity determination of zero anaphors.It also shows that anaphoricity determination onautomatic zero anaphor detection achieves 77.96%,53.97% and 63.78 in precision, recall and F-measure, respectively.
In comparison with ana-phoricity determination on golden zero anaphors,anaphoricity determination on automatic zero ana-phor detection lowers the performance by about 23in F-measure.
This indicates the importance andthe necessity for further research in zero anaphordetection.P% R% Fgolden zero anaphors 89.83 84.21 86.93zero anaphor detection 77.96 53.97 63.78Table 5: Performance of anaphoricity determinationAntecedent identificationTable 6 gives the performance of antecedent iden-tification given golden zero anaphors.
It shows thatantecedent identification on golden anaphoric zeroanaphors achieves 88.93%, 68.36% and 77.29 inprecision, recall and F-measure, respectively.
Italso shows that antecedent identification on auto-matic anaphoricity determination achieves 80.38%,47.28% and 59.24 in precision, recall and F-measure, respectively, with a decrease of about 8%in precision, about 21% in recall and about 18% inF-measure, in comparison with antecedent identifi-cation on golden anaphoric zero anaphors.
Thisindicates the critical role of anaphoricity determi-nation in antecedent identification.P% R% Fgolden anaphoric zero ana-phors88.90 68.36 77.29anaphoricity determination 80.38 47.28 59.54Table 6: Performance of antecedent identification givengolden zero anaphorsOverall: zero anaphora resolutionTable 7 gives the performance of overall zeroanaphora resolution with automatic zero anaphordetection, anaphoricity determination and antece-dent identification.
It shows that our tree kernel-based framework achieves 77.66%, 31.74% and45.06 in precision, recall and F-measure.
In com-parison with Table 6, it shows that the errorscaused by automatic zero anaphor detection de-crease the performance of overall zero anaphoraresolution by about 14 in F-measure, in compari-son with golden zero anaphors.888P% R% F77.66 31.74 45.06Table 7: Performance of zero anaphora resolutionFigure 7 shows the learning curve of zeroanaphora resolution with the increase of the num-ber of the documents in experimentation, with thehorizontal axis the number of the documents usedand the vertical axis the F-measure.
It shows thatthe F-measure is about 42.5 when 20 documentsare used in experimentation.
This figure increasesvery fast to about 45 when 50 documents are usedwhile further increase of documents only slightlyimproves the performance.auto ZA and AZA41424344454620 30 40 50 60 70 80 90 100  Figure 7: Learning curve of zero anaphora resolutionover the number of the documents in experimentationTable 8 shows the detailed performance of zeroanaphora resolution over different sentence dis-tance between a zero anaphor and its antecedent.
Itis expected that both the precision and the recall ofintra-sentential resolution are much higher thanthose of inter-sentential resolution, largely due tothe much more dependency of intra-sentential an-tecedent identification on the parse tree structures.Sentence distance P% R% F0 85.12 33.28 47.851 46.55 23.64 31.362 - - -Table 8: Performance of zero anaphora resolution oversentence distancesTable 9 shows the detailed performance of zeroanaphora resolution over the two major zeroanaphora categories, ?-NONE- *PRO*?
and ?-NONE- *pro*?.
It shows that our tree kernel-basedframework achieves comparable performance onthem, both with high precision and low recall.
Thisis in agreement with the overall performance.ID Category P% R% F3 -NONE-  *PRO* 79.37 34.23 47.834 -NONE-  *pro* 77.03 30.82 44.03Table 9: Performance of zero anaphora resolution overmajor zero anaphora categories5.2 Comparison with previous workAs a representative in Chinese zero anaphora reso-lution, Zhao and Ng (2007) focused on anaphoric-ity determination and antecedent identificationusing feature-based methods.
In this subsection, wewill compare our tree kernel-based framework withtheirs in details.CorpusZhao and Ng (2007) used a private corpus fromConverse (2006).
Although their corpus contains205 documents from CBT 3.0, it only deals withthe zero anaphors under the zero anaphora cate-gory of ?-NONE- *pro*?
for dropped sub-jects/objects.
Furthermore, Zhao and Ng (2007)only considered zero anaphors with explicit nounphrase referents and discarded zero anaphors withsplit antecedents (i.e.
split into two separate nounphrases) or referring to entities.
As a result, theircorpus is only about half of our corpus in the num-ber of zero anaphors and anaphoric zero anaphors.Besides, our corpus deals with all the types of zeroanaphors and all the categories of zero anaphoraexcept zero cataphora.MethodZhao and Ng (2007) applied feature-based methodson anaphoricity determination and antecedent iden-tification with most of features structural in nature.For zero anaphor detection, they used a very sim-ple heuristic rule to generate zero anaphor candi-dates.
Although this rule can recover almost all thezero anaphors, it suffers from very low precisionby introducing too many false zero anaphors andthus may lead to low performance in anaphoricitydetermination, much due to the imbalance betweenpositive and negative training examples with theratio up to about 1:30.In comparison, we propose a tree kernel-basedunified framework for all the three sub-tasks inzero anaphora resolution.
In particular, differentparse tree structures are constructed for differentsub-tasks.
Besides, a context sensitive convolutiontree kernel is employed to directly compute thesimilarity between the parse trees.For fair comparison with Zhao and Ng (2007),we duplicate their system and evaluate it on ourdeveloped Chinese zero anaphora corpus, using thesame J48 decision tree learning algorithm in Wekaand the same feature sets for anaphoricity determi-nation and antecedent identification.889Table 10 gives the performance of the feature-based method, as described in Zhao and Ng (2007),in anaphoricity determination on our developedcorpus.
In comparison with the tree kernel-basedmethod in this paper, the feature-based methodperforms about 16 lower in F-measure, largely dueto the difference in precision (63.61% vs 89.83%),when golden zero anaphors are given.
It alsoshows that, when our tree kernel-based zero ana-phor detector is employed 2 , the feature-basedmethod gets much lower precision with a gap ofabout 31%, although it achieves slightly higherrecall.P% R% Fgolden zero anaphors 63.61 79.71 70.76zero anaphor detection 46.17 57.69 51.29Table 10: Performance of the feature-based method(Zhao and Ng 2007) in anaphoricity determination onour developed corpusP% R% Fgolden anaphoric zero ana-phors77.45 51.97 62.20golden zero anaphpors andfeature-based anaphoricitydetermination75.17 29.69 42.57overall: tree kernel-basedzero anaphor detection andfeature-based anaphoricitydetermination70.67 23.64 35.43Table 11: Performance of the feature-based method(Zhao and Ng 2007) in antecedent identification on ourdeveloped corpusTable 11 gives the performance of the feature-based method, as described in Zhao and Ng (2007),in antecedent identification on our developed cor-pus.
In comparison with our tree kernel-basedmethod, it shows that 1) when using golden ana-phoric zero anaphors, the feature-based methodperforms about 11%, 17% and 15 lower in preci-sion, recall and F-measure, respectively; 2) whengolden zero anaphors are given and feature-basedanaphoricity determination is applied, the feature-based method performs about 5%, 18% and 17lower in precision, recall and F-measure, respec-tively; and 3) when tree kernel-based zero anaphordetection and feature-based anaphoricity determi-nation are applied, the feature-based method per-2 We do not apply the simple heuristic rule, as adopted in Zhaoand Ng (2007), in zero anaphor detection, due to its muchlower performance, for fair comparison on the other two sub-tsaks..forms about 7%, 8% and 10 lower in precision,recall and F-measure, respectively.In summary, above comparison indicates thecritical role of the structural information in zeroanaphora resolution, given the fact that most offeatures in the feature-based methods in Zhao andNg (2007) are also structural, and the necessity oftree kernel methods in modeling such structuralinformation, even if more feature engineering inthe feature-based methods may improve the per-formance to a certain extent.6 Conclusion and Further WorkThis paper proposes a tree kernel-based unifiedframework for zero anaphora resolution, which canbe divided into three sub-tasks: zero anaphor de-tection, anaphoricity determination and antecedentidentification.The major contributions of this paper include: 1)We release a wide-coverage Chinese zero anaphoracorpus of 100 documents, which adds a layer ofannotation to the manually-parsed sentences in theChinese Treebank (CTB) 6.0.
2) To our bestknowledge, this is the first systematic work dealingwith all the three sub-tasks in Chinese zero anaph-ora resolution via a unified framework.
3) Em-ployment of tree kernel-based methods indicatesthe critical role of the structural information in zeroanaphora resolution and the necessity of tree kernelmethods in modeling such structural information.In the future work, we will systematically evalu-ate our framework on automatically-generatedparse trees, construct more effective parse treestructures for different sub-tasks of zero anaphoraresolution, and explore joint learning among thethree sub-tasks.Besides, we only consider zero anaphors drivenby a verb predicate phrase node in this paper.
Inthe future work, we will consider other situations.Actually, among the remaining 7% zero anaphors,about 5% are driven by a preposition phrase (PP)node, and 2% are driven by a noun phrase (NP)node.
However, our preliminary experiments showthat simple inclusion of those PP-driven and NP-driven zero anaphors will largely increase the im-balance between positive and negative instances,which significantly decrease the performance.Finally, we will devote more on further develop-ing our corpus, with the ultimate mission of anno-tating all the documents in CBT 6.0.890AcknowledgmentsThis research was supported by Projects 60873150,90920004 and 61003153 under the National Natu-ral Science Foundation of China.ReferencesS.
Converse.
2006.
Pronominal Anaphora Resolution inChinese.
Ph.D. Thesis, Department of Computer andInformation Science.
University of Pennsylvania.M.
Collins and N. Duffy.
2001.
Convolution kernels fornatural language.
NIPS?2001:625-632.A.
Ferrandez and J. Peral.
2000.
A computational ap-proach to zero-pronouns in Spanish.
ACL'2000:166-172.R.
Iida, K. Inui, and Y. Matsumoto.
2006.
Exploitingsyntactic patterns as clues in zero-anaphora resolu-tion.
COLING-ACL'2006:625-632H.
Isozaki and T. Hirao.
2003.
Japanese zero pronounresolution based on ranking rules and machinelearning.
EMNLP'2003:184-191F.
Kong, G.D. Zhou and Q.M.
Zhu.
2009 Employing theCentering Theory in Pronoun Resolution from theSemantic Perspective.
EMNLP?2009: 987-996C.
N. Li and S. A. Thompson.
1979.
Third-person pro-nouns and zero-anaphora in Chinese discourse.
Syn-tax and Semantics, 12:311-335.W.
Li.
2004.
Topic chains in Chinese discourse.
Dis-course Processes, 37(1):25-45.A.
Moschitti.
2004.
A Study on Convolution Kernels forShallow Semantic Parsing, ACL?2004.L.H.
Qian, G.D. Zhou, F. Kong, Q.M.
Zhu and P.D.Qian.
2008.
Exploiting constituent dependencies fortree kernel-based semantic relation extraction.COLING?2008:697-704K.
Seki, A. Fujii, and T. Ishikawa.
2002.
A probabilisticmethod for analyzing Japanese anaphora intergrat-ing zero pronoun detection and resolution.COLING'2002:911-917R.
Sasano.
D. Kawahara and S. Kurohashi.
2008.
Afully-lexicalized probabilistic model for Japanesezero anaphora resolution.
COLING'2008:769-776W.M.
Soon, H.T.
Ng and D. Lim.
2001.
A machinelearning approach to coreference resolution of nounphrase.
Computational Linguistics, 2001, 27(4):521-544.V.
Ng and C. Cardie 2002.
Improving machine learningapproaches to coreference resolution.
ACL?2002:104-111X.F.
Yang, G.D. Zhou, J. Su and C.L.
Chew.
2003.Coreference Resolution Using Competition LearningApproach.
ACL?2003:177-184X.F.
Yang, J. Su and C.L.
Tan 2008.
A Twin-CandidateModel for Learning-Based Anaphora Resolution.Computational Linguistics 34(3):327-356N.
Xue, F. Xia, F.D.
Chiou and M. Palmer.
2005.
ThePenn Chinese TreeBank: Phrase structure annotationof a large corpus.
Natural Language Engineering,11(2):207-238.X.F.
Yang, J. Su and C.L.
Tan.
2006.
Kernel-basedpronoun resolution with structured syntactic knowl-edge.
COLING-ACL'2006:41-48.D.
Zelenko, A. Chinatsu and R. Anthony.
2003.
Kernelmethods for relation extraction.
Journal of MachineLearning Research, 3(2003):1083-1106S.
Zhao and H.T.
Ng.
2007.
Identification and Resolu-tion of Chinese Zero Pronouns: A Machine LearningApproach.
EMNLP-CoNLL'2007:541-550.S.
Zhao and R. Grishman.
2005.
Extracting relationswith integrated information using kernel methods.ACL?2005:419-426G.D.
Zhou, F. Kong and Q.M.
Zhu.
2008.
Context-sensitive convolution tree kernel for pronoun resolu-tion.
IJCNLP'2008:25-31G.D.
Zhou, M. Zhang, D.H. Ji and Q.M.
Zhu.
2007.Tree kernel-based relation extraction with context-sensitive structured parse tree information.
EMNLP-CoNLL?2007:728-736891
