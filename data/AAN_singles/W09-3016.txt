Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 99?107,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPHigh-Performance High-Volume Layered Corpora AnnotationTiago Lu?
?s and David Martins de MatosL2F - INESC-IDR.
Alves Redol 9, Lisboa, Portugal{tiago.luis,david.matos}@l2f.inesc-id.ptAbstractNLP systems that deal with large collec-tions of text require significant computa-tional resources, both in terms of spaceand processing time.
Moreover, these sys-tems typically add new layers of linguis-tic information with references to anotherlayer.
The spreading of these layered an-notations across different files makes themmore difficult to process and access thedata.
As the amount of input increases, sodoes the difficulty to process it.
One ap-proach is to use distributed parallel com-puting for solving these larger problemsand save time.We propose a framework that simplifiesthe integration of independently existingNLP tools to build language-independentNLP systems capable of creating layeredannotations.
Moreover, it allows the devel-opment of scalable NLP systems, that exe-cutes NLP tools in parallel, while offeringan easy-to-use programming environmentand a transparent handling of distributedcomputing problems.
With this frameworkthe execution time was decreased to 40times less than the original one on a clusterwith 80 cores.1 IntroductionLinguistic information can be automatically cre-ated by NLP systems.
These systems are com-posed by several NLP tools that are typically ex-ecuted in a pipeline, where each tool performs aprocessing step.
Therefore, each tool uses the re-sults produced by the previous processing stepsand produces new linguistic information that canbe later used by other tools.
The addition of newlayers of linguistic information (layered annota-tions) by NLP tools makes the processing and ac-cess to data difficult due to the spreading of thelayered annotations across different files.
More-over, whenever these tools are integrated, severalproblems related with information flow betweenthem may arise.
A given tool may need an annota-tion previously produced by another tool but someof the information in annotation can be lost in con-versions between the different tool data formats,because the expressiveness of each format may bedifferent and not completely convertible into otherformats.Besides tool integration problems, there is alsoanother problem related with the data-intensivenature of NLP and the computation power neededto produce the linguistic information.
The wealthof annotations has increased the amount of data toprocess.
Therefore, the processing of this linguis-tic information is a computation-heavy processand some algorithms continue to take a long time(hours or days) to produce their results.
This kindof processing can benefit from distributed parallelcomputing but it may create other problems, suchas fault tolerance to machine failures.
Becausesome NLP algorithms can take long time to pro-duce their results, it is important to automaticallyrecover from these failures, in order not to lose theresults of computations already performed.
Taskscheduling is also a problem due to data-intensivenature of NLP.
Data-driven scheduling (based ondata location) improves performance because it re-duces bandwidth usage.Our framework aims to simplify the integrationof independently developed NLP tools, while pro-viding an easy-to-use programming environment,and transparent handling of distributed computingproblems, such as fault tolerance and task schedul-ing, when executing the NLP tools in parallel.Moreover, NLP systems built on top of the frame-work are language-independent and produce lay-ered annotations.
We also measured the gains thatcan be achieved with the parallel execution of NLP99tools and the merging of the layered annotations.Section 2 discusses related work, Section 3present the framework?s architecture and a de-tailed description of its components, Section 4shows the integrated tools, Section 5 explains howthe information produced by tools is merged, andSection 6 presents the achieved results.
Finally,Section 7 presents concluding remarks.2 Related WorkGATE (Cunningham et al, 2002) is one of themost used framework for building NLP systems.However, it does not provide a controller for paral-lel execution, it only supports the execution of ap-plications on different machines over data sharedon the server (Bontcheva et al, 2004).
However,this solution cannot be applied in a large-scale dis-tributed environment because the shared reposi-tory becomes a bottleneck in computation due tothe accesses from all the machines making com-putations.UIMA (Ferrucci and Lally, 2004) is also usedto build NLP systems, and this framework sup-ports replication of pipeline components to im-prove throughput on multi-processor or multi-machine platforms.
However, we did not findany published results regarding the parallel execu-tion.
The UIMA framework has been successfullyleveraged (Egner et al, 2007) with Condor1, amanager of loosely coupled compute resources, al-lowing the parallel execution of multiple instancesof the NLP system built with UIMA.
The Condorscheduler allows to solve problems where there isno communication between tasks and complicatesthe development of parallel applications when thisinteraction is needed, like in our case, where itis necessary to merge multiple layers of annota-tions.
Also, the Condor does not move computa-tions closer to their input data, like the MapReduceapproach.The MapReduce paradigm has already beensuccessfully adopted by the Ontea semantic anno-tator (Laclav?
?k et al, 2008).
We think that GATEand UIMA frameworks could also benefit withthe MapReduce.
For example, the NLTK (Loperand Bird, 2002) adopted this paradigm, and al-ready have implementations of some algorithmslike term frequency-inverse document frequency(tf-idf) or expectation-maximization (EM).There are already tools for merging of layered1http://www.cs.wisc.edu/condor/ Figure 1: Framework architecture.annotations, like the ANC tool (Ide and Suder-man, 2006).
However, we did not find any ap-proach to this task in a scalable manner.Concerning the parallel programmingapproaches, Message Passing Interface(MPI) (Gropp, 2001) continues to be widelyused in parallel programming and therefore thereare currently many libraries built based on thisprogramming model.
However, this approachprovides very low level routines that are difficultto use and make for obscure algorithm imple-mentation, making code reuse and maintenancedifficult and time consuming.
MPI programmingcan be difficult because it is necessary to dividethe problem among processes with separateaddress spaces and coordinate these processeswith communication routines.MapReduce (Dean and Ghemawat, 2008)forces the programmer to consider the data par-allelism of the computation.
Also, this frame-work automatically schedules and distributes datato tasks.
The simple API provided by the systemallows programmers to write simple serial pro-grams that are run in a distributed way, while hid-ing several parallel programming details.
There-fore, this framework is accessible to a wide rangeof developers and allows them to write their ap-plications at a higher level of abstraction than theMPI approach.3 Framework ArchitectureOur framework aims to simplify the integration ofindependently developed NLP tools, while execut-ing the NLP tools in a parallel manner.
Our archi-tecture is composed by: Stage, Tool, and Unit (seeFigure 1).
Stages represent phases in the annota-tion process of the NLP system.
They can be in-terconnected in order to form an NLP system.
The100Tool interacts with the existing NLP tool and canreceive as input an annotation previously createdor a text file (structured or unstructured).
The textfiles received are divided into sets of independentUnits.
Units are used to represent the input filefragmentation (each fragment is represented by aUnit).
These independent Units are then processedin parallel.
Previously created annotations are al-ready divided, since they correspond to an annota-tion that refers the corresponding input fragment.Tools are wrapped in Stage components.
Stageshave two queues: an input and an output queue ofUnits.
Stages are responsible for consuming inputqueue Units, pass them to the Tool and, after theirprocessing, put the result on output queue.
Thesequeues allow multithreaded consumption and pro-duction of the Units.The framework was implemented usingthe MapReduce (Dean and Ghemawat, 2008)paradigm due to its scalability when dealing withlarge data volumes.
The Hadoop2 framework (de-scribed in the next section) was used as the basefor implementation.
The next sections describethe representation format used for annotations, theinput accepted, and the framework components inmore detail.3.1 HadoopHadoop is a MapReduce implementation writtenin Java.
One of the main advantages of using theMapReduce paradigm is task scheduling.
Whendealing with large datasets in a distributed man-ner, bandwidth to data becomes a problem.
TheMapReduce paradigm and the Hadoop DistributedFile System (HDFS) allows to reduce bandwidthconsumption because tasks are scheduled close totheir inputs whenever possible.Another advantage is fault tolerance and tasksynchronization handling.
These problems, inher-ent to distributed systems, are transparently solvedby the Hadoop framework, facilitating program-ming of distributed applications.The MapReduce framework operates exclu-sively on key/value pairs, i.e., the frameworkviews the input to the job as a set of key/value pairsand produces a set of key/value pairs as the outputof the job.
These key and value elements can beany user defined data type.The main tasks of MapReduce are the map andthe reduce task.
The map task produces a set of2http://hadoop.apache.org/core/ 	 		Figure 2: Graph-based model annotation example.intermediate key/value pairs from input key/valuepairs.
Each map is an individual task that runs on amachine.
The reduce phase creates a smaller set ofkey/value pairs from a set of intermediate valuesthat have the same key.
Since different mapperscan output the same key, the framework groupsreducer input key/value pairs with the same key.This grouping capability is used to merge annota-tions produced by different tools and that are re-lated with each other, as shown is the Section 5.3.2 Representation FormatIn order to represent linguistic information gener-ated by the tools, we chose the Linguistic Anno-tation Framework (LAF) (Ide and Romary, 2006)format, that uses a graph model to store annota-tions.An annotation can be viewed as a set of linguis-tic information items that are associated with somedata (a part of a text or speech signal, for example),called primary data.
Primary data objects are rep-resented by locations in the input.
These locationscan be the offset of a character comprising a sen-tence or word, in the case of a text input, or a pointat which a given temporal event begins or ends, inthe case of a speech signal input.
As such, primarydata objects have a simple structure.
However, it ispossible to build more complex data objects, com-posed by sets of contiguous or noncontiguous lo-cations.
Primary data objects are used to build seg-mentations over data.
A segmentation represents alist of ordered segments, where each segment rep-resents a linguistic element.
A segment is repre-sented by an edge between virtual nodes locatedbetween each character in the primary data (seeFigure 2).
It is possible to define multiple segmen-tations over the same primary data, and multipleannotations may refer to the same segmentation.An annotation is defined as a label and a featurestructure.
A feature structure is itself a graph in101<laf><edgeSet><edgeid=?e1" from=?0" to=?5"/><edgeid=?e2" from=?6" to="13"/></edgeSet><nodeSet><nodeid=?n1?edgesTo=?e1"><fs type=?segment?><fname=?SEGMENT?
value=?Great"/><fname=?POS" value=?NNP"/></fs></node><nodeid=?n2?edgesTo=?e2"><fs type=?segment?><fname=?SEGMENT?
value=?Britain"/><fname=?POS" value=?NNP"/></fs></node><nodeid=?n3?edgesTo=?e1e2"><fs type=?segment?><fname=?SEGMENT?
value=?GreatBritain"/><fname=?POS" value=?NNP"/></fs></node></nodeSet></laf>Figure 3: Morphosyntactic LAF annotation exam-ple.which nodes are labeled with feature/value pairs orother feature structures.
Hence, a morphosyntac-tic annotation is represented by a graph in whichnodes are labeled with feature/value pairs.
Thesepairs contain the morphosyntactic information.Figure 3 shows how the two possible segmenta-tions in the POS tagger annotation in Figure 2 canbe represented: the segment ?Great Britain?
has atotal of 13 characters; the edges use the characteroffsets to delimit the segment; the nodes built ontop of these edges contain the morphosyntactic in-formation, such as the POS, and the text pointed toby the segment.
As shown in the third node (withidentifier ?n3?
), it is possible to have a node refer-ring to multiple edges.
A node can also refer toother nodes to add other kinds of linguistic infor-mation, such as dependencies between segmentsor syntactic annotations.3.3 InputCurrently, the Tools integrated in our frameworkcan process three kinds of input files: structuredand unstructured text, and previously created an-notations.
The structured text format currentlysupported is TEI (Text Encoding Initiative)3.
Bothstructured and unstructured text are fragmentedinto a set of Units.
The division is currentlyparagraph-based, in the case of the unstructured3http://www.tei-c.org/< T E I .
2 l a n g = ?
e n " >< t e i H e a d e r >.
.
.< / t e i H e a d e r >< t e x t l a n g = ?
e n " >.
.
.< p i d = ?
p 1 6 " > G r e a t < / p >.
.
.< p i d = ?
p 2 9 " > B r i t a i n < / p >.
.
.< / t e x t >< / T E I .
2 >< l a f a d d r e s s i n g = ?
X P o i n t e r " >< e d g e S e t >< e d g e i d = ?
e 1 ?f r o m = ?
x p o i n t e r ( i d ( ?
p 1 6 ? )
/ t e x t ( ) / p o i n t [ p o s i t i o n ( ) = 0 ] ) ?t o = ?
x p o i n t e r ( i d ( ?
p 1 6 ? )
/ t e x t ( ) / p o i n t [ p o s i t i o n ( ) = 5 ] ) " / >< e d g e i d = ?
e 2 ?f r o m = ?
x p o i n t e r ( i d ( ?
p 2 9 ? )
/ t e x t ( ) / p o i n t [ p o s i t i o n ( ) = 0 ] ) ?t o = ?
x p o i n t e r ( i d ( ?
p 2 9 ? )
/ t e x t ( ) / p o i n t [ p o s i t i o n ( ) = 7 ] ) " / >< / e d g e S e t >< n o d e S e t >.
.
.< / n o d e S e t >< / l a f >T E I f i l eL A F a n n o t a t i o nFigure 4: TEI file LAF annotation example.text, and on XML textual elements, in the case ofthe TEI input.
However, it is possible to createUnits with other user-defined granularity.In order to make references to locations in theTEI input, we adopted the XPointer4 format (seeFigure 4).
Assuming that each text element in theTEI file has a unique identifier, the XPointer ofthe start and end tag will refer this identifier andthe word character offset.3.4 UnitWhen processing large files, with several giga-bytes, it is not efficient to process them in serialmode due to memory constraints.
Therefore, wedivide them into sets of Units that are processedindependently.Each Unit is associated with a portion of theinput file and contains the linguistic informationgenerated by a tool in a stage.
The Unit has aunique identifier, a set of dependencies (containsinformation about other Units that the Unit de-pends on), the identifier of the Stage that producedthe unit and the annotation (linguistic informa-tion produced by Tool).
Besides these elements, italso has a common identifier that is shared acrossthe layered annotations that are related with eachother.4http://www.w3.org/TR/xptr/1023.5 StageStages represent a phase in the annotation process.Each Stage has two queues: an input and an outputqueue of Units.
This component is responsible forconsuming input units, pass them to the Tool and,after their processing, putting them on the outputqueue.
The Units in the output queue can later beused by another Stage (by connecting the outputqueue to the input queue of the next stage) or writ-ten to a file.An NLP system can be composed of severalStages that are responsible for a specific annota-tion task.
The framework allows the compositionof various Tools to form a complete NLP system:each Tool receives the information produced bythe Tools in the previous Stages and produces aUnit with the annotation created with references tothe previous ones.
This information is maintainedin memory, along the created tool pipeline, and isonly written to disk at the end of the NLP system.3.6 ToolTools are responsible for specific linguistic tasks.Currently, these Tools include (without limitation)Tokenizers and Classifiers.
Tokenizers receive theinput text and produce segmentations (list of seg-ments) that refer to the input, i.e., divide the in-put sentences into words.
Classifiers produce setsof classifications for a given segmentation.
Theseclassifications can be, for example, the grammarclass of each word.
These tools accept two kindsof inputs: an input text or a previously created an-notation with a segmentation.In order to add new tools, it is necessary to ex-tend the previous classes and add the necessarycode in order to add the information produced bythe existing NLP tool in the LAF format.Because the framework is written in Java, andthe tools could have been developed in a differentlanguage, such as C++ or Perl, it was necessary tofind a way to interact with other programming lan-guages.
Hence, an existing tool can be integratedin various ways.
If a tool provides an API, we cur-rently provide an Remote Procedure Call (RPC)mechanism with the Thrift5 software library.
Ifthe API can be used in a C/C++ program, it is alsopossible to use the existing tool API with Java Na-tive Interface (JNI) (Liang, 1999).
The frameworkalso supports tools that can only be executed fromthe command line.5http://incubator.apache.org/thrift/4 ApplicationsThe tools that have been integrated can be dividedinto two classes: those capable of producing firstlevel annotations and those capable of producingsecond level annotations.
The first level tools pro-duce morphosyntactic annotation from an inputtext.
Second level tools receive morphosyntac-tic information as input and produce morphosyn-tactic annotations.
To show the language inde-pendence of the framework, we integrated toolsfrom four different languages: Arabic, Japanese,English, and Portuguese.
One of the tools capa-ble of analyzing Arabic texts is AraMorph (Buck-walter, 2002), a Java-based Arabic morpholog-ical analyzer.
For the Japanese language wechose Chasen (Matsumoto et al, 1999), a morpho-logical analyzer capable of processing Japanesetexts.
The Stanford POS Tagger (Toutanova, 2000;Toutanova et al, 2003) is only being used to pro-cess English texts but it can be easily adapted (bychanging its input dictionary) to process other lan-guages, like Chinese or German.
For processingPortuguese, we chose the Palavroso morphologi-cal analyzer (Medeiros, 1995).
The morphologicalanalyzers previously described produce first levelannotations, i.e., they receive text as input and pro-duce annotations.Besides these tools, we also integrated typesof tools for testing second level annotations:RuDriCo (Paulo, 2001) and JMARv (Ribeiro etal., 2003).
RuDriCo is a post-morphological an-alyzer that rewrites the results of a morphologi-cal analyzer.
RuDriCo uses declarative transfor-mation rules based on pattern matching.
JMARvis a tool that performs morphosyntactic disam-biguation (selects a classification from the possi-ble classifications in each segment from the in-put sequence).
The two previous tools wereused for processing Portuguese morphosyntacticinformation, but can be easily adapted to pro-cess other languages.
For example, JMARv couldbe used to disambiguate AraMorph classificationsand RuDriCo could translate Chasen?s JapanesePOS information into other formats.5 Merging Layered AnnotationsThe stand-off annotation provided by the LAF for-mat allows to add new layers of linguistic informa-tion by creating a tree whose nodes are referencesto another layer.
This approach offers many ad-vantages, like the possibility to distribute the an-103 								Figure 5: Illustration of the dependencies betweenannotation layers.notations without the source text, and the possi-bility to annotate discontinuous segments of text.However, these layers, although separate, dependon each other, and their information can be diffi-cult to access, because these layers can be spreadacross different files.
There is a na?
?ve approachto do this merge, that consists in loading all anno-tations from all files to memory and then resolvetheir dependencies.
However, these dependenciescan be dispersed across several large files (see Fig-ure 5.
Thus, the machine memory constraints be-come a problem for this solution.Therefore, we propose a novel solution to solvethe merging problem in a efficient manner us-ing the MapReduce programming paradigm.
Thegrouping capability offered by the Hadoop frame-work ?
a Java implementation of the MapReduceparadigm ?
allows to efficiently merge the annota-tions produced by the different tools, i.e., the lay-ered annotations.
This operation is performed asfollows:Map - this phase produces key/value pairs with akey equal to the identifier that is shared byannotations that depend on one another (seeFigure 6).
Thus, all related annotations aregrouped by the framework after this phase.Reduce - before the creation of the new anno-< u n i t i d = ?
f b 8 3 a 5 6 5 " c o m m o n L i d = ?
d b e f e b 9 7 " >< d e p e n d e n c i e s / >< a n n o t a t i o n >.
.
.< / a n n o t a t i o n >< / u n i t > S t a g e 1< u n i t i d = ?
3 9 7 3 0 2 9 1 " c o m m o n L i d = ?
d b e f e b 9 7 " >< d e p e n d e n c i e s >< u n i t i d = ?
f b 8 3 a 5 6 5 " / >< / d e p e n d e n c i e s >< a n n o t a t i o n >.
.
.< / a n n o t a t i o n >< / u n i t > S t a g e 2Figure 6: Codification of the layered annotationsdependencies.tation, merge the previously created annota-tions.
This merging process creates a singleannotation that contains all the annotationsthat were combined.
This unified annotationis then passed to the Tool.
The Tool pro-cesses the annotation and produces anotherone sharing a common identifier.
The newannotation is written at the end of this phase.The serialization of the intermediate key and valueelements from a pair in a binary format allows usto reduce bandwidth usage due to the more com-pact representation of the key and value comparedto the LAF (XML-based format representation ofthe input file).6 ResultsThe tests were performed on a cluster with 20 ma-chines.
Each machine had an Intel Quad-CoreQ6600 2.4 GHz processor, 8 GB of DDR2 RAM at667 MHz and was connected to a gigabit ethernet.To measure the amount of achieved parallelism,we used the speedup formula shown in Equation 1:Ts is the execution time of the sequential algo-rithm; Tp is the execution time of the parallel al-gorithm.
Speedup refers to how much a parallelsolution is faster than the corresponding sequen-tial solution.S = TsTp (1)The Hadoop framework was installed on all ma-chines and each one was configured to run 4 mapand 4 reduce tasks simultaneously.
The Hadoopuses HDFS as storage.
This file system was con-figured to split each file into 64 MB chunks.
Theseblocks are replicated across the machines in the104Data [MB] Stanford POS TaggerSerial Time [s]1 3082 6065 153110 305520 602150 15253Table 1: Serial processing time of the StanfordPOS Taggercluster in order to tolerate machine failures.
Weused a HDFS replication factor of three.To test the system, we used the speedup formulaand selected the Stanford POS Tagger.
Table 1shows the serial execution time of the StanfordPOS Tagger.
This time corresponds to the stan-dalone execution of the tool (without being inte-grated in the framework) on a single computer, forvarious sizes of input data (from 1 MB to 50 MB).Input and output were read/written from/to the lo-cal disk.In addition to the previous tool, we also testedJMARv in order to assess the impact of annotationmerging at execution time.
Unlike the other tools,this tool receives annotations as input.We must also consider the setup time forHadoop.
When executing the tools on top ofHadoop, it is necessary to store the input data onHDFS.
However, these files are, in many cases,rarely updated.
Therefore, they are perfect forthe write-once read-many nature of HDFS and thecopy times of the input data files were not consid-ered (the HDFS write speed was around 22 MB/s).Section 6.1 shows the speedups achieved withthe Stanford POS Tagger, and Section 6.2 the an-notation merging results, with the JMARv tool.6.1 Stanford POS Tagger ResultsFigure 7 shows the speedup values when consider-ing various values for the number of mappers andreducers, without any compression of the final out-put, for an input of 50 MB.
The large standalonetimes show that this tool is computationally heavy.With this tool it was possible to achieve a speedupvalue of approximately 40.The horizontal progression of the speedup is ex-plained by the heavy computation performed bythe tool.
Since processing from the Stanford POSTagger is performed in mappers, the increase inthe number of mappers improves speedup values.The execution time of this tool is around 4000510152025303540Speedup50 MB Stanford POS Tagger (English)50  100  150  200  250Mappers20406080100120ReducersFigure 7: Stanford POS Tagger speedup resultsInput [MB] Compressed UncompressedOutput [MB] Time [s] Output [MB] Time [s]1 3 56 24 572 6 66 48 675 15 91 119 9410 31 140 238 14020 62 235 476 22650 155 534 1192 529Table 2: Stanford POS tagger output compressionevaluation with a fixed number of 64 mappers and64 reducers.seconds, on the yellow (light gray) portion of Fig-ure 7 and 1700 seconds on the dark blue (black)portions.
On the intermediate values the executiontime is approximately 1000 seconds.The top right corner of the graph shows a smallspeedup decrease.
This can be explained by thelarge number of queued map and reduce tasks.6.1.1 Compression EvaluationTable 2 shows how output compression influencesexecution times values.
As shown in Table 2, thistool produces, approximately, an output 24 timeslarger than the input, without compression, and3 times larger with compression.
However, out-put compression does not improve execution timesdue to heavy computation performed by the tool.Hence, processing time dominates output writingtime.6.2 Annotation Merging ResultsUnlike the previous tool, JMARv does not processtext as input.
This tool receives an annotation asinput that, in this case, was previously created byPalavroso.In order to test the parallel annotation mergingon top of Hadoop, we measured three kinds oftimes: Palavroso execution time with output cre-105Palavroso Palavroso + JMARv JMARvTime [s] 171 s 323 s 179 sTable 3: Annotation merging time evaluation witha fixed number of 64 mappers and 64 reducers, foran input of 100 MB of text.ation time, execution time of JMARv with the pre-viously written Palavroso output and the time ofPalavroso and JMARv executed in a pipeline (in-termediate data is maintained in memory and theoutput produced is only written to disk after theexecution of the two tools).
The results are pre-sented in Table 3.
The first column shows the exe-cution time of the Palavroso tool.
The second col-umn shows the time of Palavroso and JMARv exe-cution in a pipeline.
Finally, the last column showsthe execution time of JMARv with the previouslycreated Palavroso output.In order to execute JMARv after Palavroso, itwas necessary to handle about 8 GB of outputproduced by the previous tool (already stored ondisk).
However, these results show that runningJMARv with this amount of data is practically thesame as running both tools in pipeline with theoriginal input (100 MB) and only write their out-put at the end.7 ConclusionsThis framework allowed us to build scalable NLPsystems that achieve significant speedups: inthe case of computation heavy tools, speedupsreached values of approximately 40.
In this case,an increase on the number of map tasks improvesspeedups, because processing time dominates theoutput writing time.In addition, the framework supports a widerange of linguistic annotations, thanks to the adop-tion of LAF.
The integration of tools does not con-sider any aspect related with the parallel execu-tion on top of the Hadoop.
Thus, the programmerfocuses only on representing the linguistic infor-mation produced by the tool for a given input textor previously created annotations.
In addition, theprogramming ease offered by the Hadoop frame-work allows to focus only on the problem we aresolving, i.e., linguistic annotation.
All the prob-lems inherent to distributed computing are trans-parently solved by the platform.
The MapRe-duce sort/grouping capabilities has been used toefficiently merge layered annotations produced bytools integrated in the framework.
Regarding fu-ture work, on the linguistic part, we plan to inte-grate tools that produce syntactic annotations (theLAF format already supports these annotations).This linguistic information can be merged with thecurrent tree by simply adding more nodes abovethe nodes that contain the morphosyntactic anno-tations.
Also, this work did not focus on informa-tion normalization.
The Data Category Registry(DCR) (Wright, 2004) could be explored in the fu-ture, in order to improve interoperability betweenlinguistic resources.Finally, the creation of NLP systems can be sim-plified by an XML parametrization.
This way it ispossible to compose a tool pipeline by simply edit-ing an XML file.
An graphical environment for vi-sualization and editing of LAF annotations is alsouseful.Our code is available at http://code.google.com/p/anota/.AcknowledgmentsThis work was supported by the partnership be-tween Carnegie Mellon University and Portu-gal?s National Science and Technology Founda-tion (FCT ?
Fundac?a?o para a Cie?ncia e a Tecnolo-gia).ReferencesKalina Bontcheva, Valentin Tablan, Diana Maynard,and Hamish Cunningham.
2004.
Evolving gate tomeet new challenges in language engineering.
Nat.Lang.
Eng., 10(3-4):349?373.Tim Buckwalter.
2002.
Buckwalter Arabic Morpho-logical Analyzer Version 1.0.
Linguistic Data Con-sortium, catalog number LDC2002L49, ISBN 1-58563-257-0.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
Gate: A framework and graph-ical development environment for robust nlp toolsand applications.
In Proceedings of the 40th AnnualMeeting of the ACL.Jeffrey Dean and Sanjay Ghemawat.
2008.
MapRe-duce: simplified data processing on large clusters.Commun.
ACM, 51(1):107?113, January.Michael Thomas Egner, Markus Lorch, and Edd Bid-dle.
2007.
Uima grid: Distributed large-scaletext analysis.
In CCGRID ?07: Proceedings of theSeventh IEEE International Symposium on ClusterComputing and the Grid, pages 317?326, Washing-ton, DC, USA.
IEEE Computer Society.106David Ferrucci and Adam Lally.
2004.
Uima: anarchitectural approach to unstructured informationprocessing in the corporate research environment.Nat.
Lang.
Eng., 10(3-4):327?348.William Gropp.
2001.
Learning from the Success ofMPI.
In Burkhard Monien, Viktor K. Prasanna, andSriram Vajapeyam, editors, HiPC, volume 2228 ofLecture Notes in Computer Science, pages 81?94.Springer.Nancy Ide and Laurent Romary.
2006.
Representinglinguistic corpora and their annotations.
In Proceed-ings of the Fifth Language Resources and Evalua-tion Conference (LREC.Nancy Ide and Keith Suderman.
2006.
Merging lay-ered annotations.
In Proceedings of Merging andLayering Linguistic Information, Genoa, Italy.Michal Laclav?
?k, Martin ?Seleng, and Ladislav Hluchy?.2008.
Towards large scale semantic annotation builton mapreduce architecture.
In ICCS ?08: Proceed-ings of the 8th international conference on Compu-tational Science, Part III, pages 331?338, Berlin,Heidelberg.
Springer-Verlag.Sheng Liang.
1999.
Java Native Interface: Program-mer?s Guide and Reference.
Addison-Wesley Long-man Publishing Co., Inc., Boston, MA, USA.Edward Loper and Steven Bird.
2002.
Nltk: the natu-ral language toolkit.
In Proceedings of the ACL-02Workshop on Effective tools and methodologies forteaching natural language processing and compu-tational linguistics, pages 63?70, Morristown, NJ,USA.
Association for Computational Linguistics.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Y.
Hirano, Hiroshi Matsuda, and Masayuki Asa-hara, 1999.
Japanese morphological analysis sys-tem ChaSen version 2.0 manual 2nd edition.
NaraInstitute of Science and Technology, technical reportnaist-istr99009 edition.Jos Carlos Medeiros.
1995.
Processamento morfol-gico e correco ortogrfica do portugu?s.
Master?s the-sis, Insituto Superior Tcnico ?
Universidade Tcnicade Lisboa, Portugal.Joana Lcio Paulo.
2001.
PAsMo - Ps Analisador Mor-folgico.
Master?s thesis, Insituto Superior Tcnico ?Universidade Tcnica de Lisboa, Portugal.Ricardo Ribeiro, Nuno J. Mamede, and Isabel Tran-coso.
2003.
Using Morphossyntactic Informationin TTS Systems: comparing strategies for EuropeanPortuguese.
In Computational Processing of thePortuguese Language: 6th International Workshop,PROPOR 2003, Faro, Portugal, June 26-27, 2003.Proceedings, volume 2721 of Lecture Notes in Com-puter Science.
Springer.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT-NAACL 2003, pages 252?259.Kristina Toutanova.
2000.
Enriching the knowledgesources used in a maximum entropy part-of-speechtagger.
In Proceedings of EMNLP/VLC 2000, pages63?70.S.
E. Wright.
2004.
A global data category registry forinteroperable language resources.
In Proceedingsof the Fourth Language Resources and EvaluationConference ?
LREC 2004, pages 123?126.
ELRAEuropean Language Resources Association.107
