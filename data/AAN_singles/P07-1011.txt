Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81?88,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsDetecting Erroneous Sentences using Automatically Mined SequentialPatternsGuihua Sun ?
Xiaohua Liu Gao Cong Ming ZhouChongqing University Microsoft Research Asiasunguihua5018@163.com {xiaoliu, gaocong, mingzhou}@microsoft.comZhongyang Xiong John Lee ?
Chin-Yew LinChongqing University MIT Microsoft Research Asiazyxiong@cqu.edu.cn jsylee@mit.edu cyl@microsoft.comAbstractThis paper studies the problem of identify-ing erroneous/correct sentences.
The prob-lem has important applications, e.g., pro-viding feedback for writers of English asa Second Language, controlling the qualityof parallel bilingual sentences mined fromthe Web, and evaluating machine translationresults.
In this paper, we propose a newapproach to detecting erroneous sentencesby integrating pattern discovery with super-vised learning models.
Experimental resultsshow that our techniques are promising.1 IntroductionDetecting erroneous/correct sentences has the fol-lowing applications.
First, it can provide feedbackfor writers of English as a Second Language (ESL)as to whether a sentence contains errors.
Second, itcan be applied to control the quality of parallel bilin-gual sentences mined from the Web, which are criti-cal sources for a wide range of applications, such asstatistical machine translation (Brown et al, 1993)and cross-lingual information retrieval (Nie et al,1999).
Third, it can be used to evaluate machinetranslation results.
As demonstrated in (Corston-Oliver et al, 2001; Gamon et al, 2005), the betterhuman reference translations can be distinguishedfrom machine translations by a classification model,the worse the machine translation system is.
?Work done while the author was a visiting student at MSRA?Work done while the author was a visiting student at MSRAThe previous work on identifying erroneous sen-tences mainly aims to find errors from the writing ofESL learners.
The common mistakes (Yukio et al,2001; Gui and Yang, 2003) made by ESL learnersinclude spelling, lexical collocation, sentence struc-ture, tense, agreement, verb formation, wrong Part-Of-Speech (POS), article usage, etc.
The previouswork focuses on grammar errors, including tense,agreement, verb formation, article usage, etc.
How-ever, little work has been done to detect sentencestructure and lexical collocation errors.Some methods of detecting erroneous sentencesare based on manual rules.
These methods (Hei-dorn, 2000; Michaud et al, 2000; Bender et al,2004) have been shown to be effective in detect-ing certain kinds of grammatical errors in the writ-ing of English learners.
However, it could be ex-pensive to write rules manually.
Linguistic expertsare needed to write rules of high quality; Also, itis difficult to produce and maintain a large num-ber of non-conflicting rules to cover a wide range ofgrammatical errors.
Moreover, ESL writers of differ-ent first-language backgrounds and skill levels maymake different errors, and thus different sets of rulesmay be required.
Worse still, it is hard to write rulesfor some grammatical errors, for example, detectingerrors concerning the articles and singular plural us-age (Nagata et al, 2006).Instead of asking experts to write hand-craftedrules, statistical approaches (Chodorow and Lea-cock, 2000; Izumi et al, 2003; Brockett et al, 2006;Nagata et al, 2006) build statistical models to iden-tify sentences containing errors.
However, existing81statistical approaches focus on some pre-defined er-rors and the reported results are not attractive.
More-over, these approaches, e.g., (Izumi et al, 2003;Brockett et al, 2006) usually need errors to be spec-ified and tagged in the training sentences, which re-quires expert help to be recruited and is time con-suming and labor intensive.Considering the limitations of the previous work,in this paper we propose a novel approach that isbased on pattern discovery and supervised learn-ing to successfully identify erroneous/correct sen-tences.
The basic idea of our approach is to builda machine learning model to automatically classifyeach sentence into one of the two classes, ?erro-neous?
and ?correct.?
To build the learning model,we automatically extract labeled sequential patterns(LSPs) from both erroneous sentences and correctsentences, and use them as input features for classi-fication models.
Our main contributions are:?
We mine labeled sequential patterns(LSPs)from the preprocessed training data to buildleaning models.
Note that LSPs are also verydifferent from N-gram language models thatonly consider continuous sequences.?
We also enrich the LSP features with other auto-matically computed linguistic features, includ-ing lexical collocation, language model, syn-tactic score, and function word density.
In con-trast with previous work focusing on (a spe-cific type of) grammatical errors, our model canhandle a wide range of errors, including gram-mar, sentence structure, and lexical choice.?
We empirically evaluate our methods on twodatasets consisting of sentences written byJapanese and Chinese, respectively.
Experi-mental results show that labeled sequential pat-terns are highly useful for the classificationresults, and greatly outperform other features.Our method outperforms Microsoft Word03and ALEK (Chodorow and Leacock, 2000)from Educational Testing Service (ETS) insome cases.
We also apply our learning modelto machine translation (MT) data as a comple-mentary measure to evaluate MT results.The rest of this paper is organized as follows.The next section discusses related work.
Section 3presents the proposed technique.
We evaluate ourproposed technique in Section 4.
Section 5 con-cludes this paper and discusses future work.2 Related WorkResearch on detecting erroneous sentences can beclassified into two categories.
The first categorymakes use of hand-crafted rules, e.g., templaterules (Heidorn, 2000) and mal-rules in context-freegrammars (Michaud et al, 2000; Bender et al,2004).
As discussed in Section 1, manual rule basedmethods have some shortcomings.The second category uses statistical techniquesto detect erroneous sentences.
An unsupervisedmethod (Chodorow and Leacock, 2000) is em-ployed to detect grammatical errors by inferringnegative evidence from TOEFL administrated byETS.
The method (Izumi et al, 2003) aims to de-tect omission-type and replacement-type errors andtransformation-based leaning is employed in (Shiand Zhou, 2005) to learn rules to detect errors forspeech recognition outputs.
They also require spec-ifying error tags that can tell the specific errorsand their corrections in the training corpus.
Thephrasal Statistical Machine Translation (SMT) tech-nique is employed to identify and correct writing er-rors (Brockett et al, 2006).
This method must col-lect a large number of parallel corpora (pairs of er-roneous sentences and their corrections) and perfor-mance depends on SMT techniques that are not yetmature.
The work in (Nagata et al, 2006) focuseson a type of error, namely mass vs. count nouns.In contrast to existing statistical methods, our tech-nique needs neither errors tagged nor parallel cor-pora, and is not limited to a specific type of gram-matical error.There are also studies on automatic essay scoringat document-level.
For example, E-rater (Bursteinet al, 1998), developed by the ETS, and IntelligentEssay Assessor (Foltz et al, 1999).
The evaluationcriteria for documents are different from those forsentences.
A document is evaluated mainly by its or-ganization, topic, diversity of vocabulary, and gram-mar while a sentence is done by grammar, sentencestructure, and lexical choice.Another related work is Machine Translation (MT)evaluation.
Classification models are employedin (Corston-Oliver et al, 2001; Gamon et al, 2005)82to evaluate the well-formedness of machine transla-tion outputs.
The writers of ESL and MT normallymake different mistakes: in general, ESL writers canwrite overall grammatically correct sentences withsome local mistakes while MT outputs normally pro-duce locally well-formed phrases with overall gram-matically wrong sentences.
Hence, the manual fea-tures designed for MT evaluation are not applicableto detect erroneous sentences from ESL learners.LSPs differ from the traditional sequential pat-terns, e.g., (Agrawal and Srikant, 1995; Pei et al,2001) in that LSPs are attached with class labels andwe prefer those with discriminating ability to buildclassification model.
In our other work (Sun et al,2007), labeled sequential patterns, together with la-beled tree patterns, are used to build pattern-basedclassifier to detect erroneous sentences.
The clas-sification method in (Sun et al, 2007) is differentfrom those used in this paper.
Moreover, instead oflabeled sequential patterns, in (Sun et al, 2007) themost significant k labeled sequential patterns withconstraints for each training sentence are mined tobuild classifiers.
Another related work is (Jindal andLiu, 2006), where sequential patterns with labels areused to identify comparative sentences.3 Proposed TechniqueThis section first gives our problem statement andthen presents our proposed technique to build learn-ing models.3.1 Problem StatementIn this paper we study the problem of identifyingerroneous/correct sentences.
A set of training datacontaining correct and erroneous sentences is given.Unlike some previous work, our technique requiresneither that the erroneous sentences are tagged withdetailed errors, nor that the training data consist ofparallel pairs of sentences (an error sentence and itscorrection).
The erroneous sentence contains a widerange of errors on grammar, sentence structure, andlexical choice.
We do not consider spelling errors inthis paper.We address the problem by building classifica-tion models.
The main challenge is to automaticallyextract representative features for both correct anderroneous sentences to build effective classificationmodels.
We illustrate the challenge with an exam-ple.
Consider an erroneous sentence, ?If Maggie willgo to supermarket, she will buy a bag for you.?
It isdifficult for previous methods using statistical tech-niques to capture such an error.
For example, N-gram language model is considered to be effectivein writing evaluation (Burstein et al, 1998; Corston-Oliver et al, 2001).
However, it becomes very ex-pensive if N > 3 and N-grams only consider contin-uous sequence of words, which is unable to detectthe above error ?if...will...will?.We propose labeled sequential patterns to effec-tively characterize the features of correct and er-roneous sentences (Section 3.2), and design somecomplementary features ( Section 3.3).3.2 Mining Labeled Sequential Patterns ( LSP )Labeled Sequential Patterns (LSP).
A labeled se-quential pattern, p, is in the form of LHS?
c, whereLHS is a sequence and c is a class label.
Let I be aset of items and L be a set of class labels.
Let D be asequence database in which each tuple is composedof a list of items in I and a class label in L. We saythat a sequence s1 =< a1, ..., am > is contained ina sequence s2 =< b1, ..., bn > if there exist integersi1, ...im such that 1 ?
i1 < i2 < ... < im ?
n andaj = bij for all j ?
1, ...,m. Similarly, we say thata LSP p1 is contained by p2 if the sequence p1.LHSis contained by p2.LHS and p1.c = p2.c.
Note thatit is not required that s1 appears continuously in s2.We will further refine the definition of ?contain?
byimposing some constraints (to be explained soon).A LSP p is attached with two measures, support andconfidence.
The support of p, denoted by sup(p),is the percentage of tuples in database D that con-tain the LSP p. The probability of the LSP p beingtrue is referred to as ?the confidence of p ?, denotedby conf(p), and is computed as sup(p)sup(p.LHS) .
Thesupport is to measure the generality of the pattern pand minimum confidence is a statement of predictiveability of p.Example 1: Consider a sequence database contain-ing three tuples t1 = (< a, d, e, f >,E), t2 = (<a, f, e, f >,E) and t3 = (< d, a, f >,C).
Oneexample LSP p1 = < a, e, f >?
E, which is con-tained in tuples t1 and t2.
Its support is 66.7% andits confidence is 100%.
As another example, LSP p283= < a, f >?
E with support 66.7% and confidence66.7%.
p1 is a better indication of class E than p2.2Generating Sequence Database.
We generate thedatabase by applying Part-Of-Speech (POS) taggerto tag each training sentence while keeping func-tion words1 and time words2.
After the process-ing, each sentence together with its label becomesa database tuple.
The function words and POS tagsplay important roles in both grammars and sentencestructures.
In addition, the time words are keyclues in detecting errors of tense usage.
The com-bination of them allows us to capture representativefeatures for correct/erroneous sentences by miningLSPs.
Some example LSPs include ?<a, NNS> ?Error?
(singular determiner preceding plural noun),and ?<yesterday, is>?Error?.
Note that the con-fidences of these LSPs are not necessary 100%.First, we use MXPOST-Maximum Entropy Part ofSpeech Tagger Toolkit3 for POS tags.
The MXPOSTtagger can provide fine-grained tag information.
Forexample, noun can be tagged with ?NN?
(singularnoun) and ?NNS?
(plural noun); verb can be taggedwith ?VB?, ?VBG?, ?VBN?, ?VBP?, ?VBD?
and?VBZ?.
Second, the function words and time wordsthat we use form a key word list.
If a word in atraining sentence is not contained in the key wordlist, then the word will be replaced by its POS.
Theprocessed sentence consists of POS and the words ofkey word list.
For example, after the processing, thesentence ?In the past, John was kind to his sister?
isconverted into ?In the past, NNP was JJ to his NN?,where the words ?in?, ?the?, ?was?, ?to?
and ?his?are function words, the word ?past?
is time word,and ?NNP?, ?JJ?, and ?NN?
are POS tags.Mining LSPs.
The length of the discovered LSPsis flexible and they can be composed of contiguousor distant words/tags.
Existing frequent sequentialpattern mining algorithms (e.g.
(Pei et al, 2001))use minimum support threshold to mine frequent se-quential patterns whose support is larger than thethreshold.
These algorithms are not sufficient for ourproblem of mining LSPs.
In order to ensure that allour discovered LSPs are discriminating and are capa-1http://www.marlodge.supanet.com/museum/funcword.html2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html3http://www.cogsci.ed.ac.uk/?jamesc/taggers/MXPOST.htmlble of predicting correct or erroneous sentences, weimpose another constraint minimum confidence.
Re-call that the higher the confidence of a pattern is, thebetter it can distinguish between correct sentencesand erroneous sentences.
In our experiments, weempirically set minimum support at 0.1% and mini-mum confidence at 75%.Mining LSPs is nontrivial since its search spaceis exponential, althought there have been a host ofalgorithms for mining frequent sequential patterns.We adapt the frequent sequence mining algorithmin (Pei et al, 2001) for mining LSPs with constraints.Converting LSPs to Features.
Each discovered LSPforms a binary feature as the input for classificationmodel.
If a sentence includes a LSP, the correspond-ing feature is set at 1.The LSPs can characterize the correct/erroneoussentence structure and grammar.
We give some ex-amples of the discovered LSPs.
(1) LSPs for erro-neous sentences.
For example, ?<this, NNS>?
(e.g.contained in ?this books is stolen.?
), ?<past,is>?(e.g.
contained in ?in the past, John is kind tohis sister.?
), ?<one, of, NN>?(e.g.
contained in ?it isone of important working language?, ?<although,but>?(e.g.
contained in ?although he likes it, buthe can?t buy it.?
), and ?<only, if, I, am>?(e.g.
con-tained in ?only if my teacher has given permission,I am allowed to enter this room?).
(2) LSPs for cor-rect sentences.
For instance, ?<would, VB>?
(e.g.contained in ?he would buy it.?
), and ?<VBD,yeserday>?(e.g.
contained in ?I bought this bookyesterday.?
).3.3 Other Linguistic FeaturesWe use some linguistic features that can be com-puted automatically as complementary features.Lexical Collocation (LC) Lexical collocation er-ror (Yukio et al, 2001; Gui and Yang, 2003) is com-mon in the writing of ESL learners, such as ?strongtea?
but not ?powerful tea.?
Our LSP features can-not capture all LCs since we replace some wordswith POS tags in mining LSPs.
We collect five typesof collocations: verb-object, adjective-noun, verb-adverb, subject-verb, and preposition-object from ageneral English corpus4.
Correct LCs are collected4The general English corpus consists of about 4.4 millionnative sentences.84by extracting collocations of high frequency fromthe general English corpus.
Erroneous LC candi-dates are generated by replacing the word in correctcollocations with its confusion words, obtained fromWordNet, including synonyms and words with sim-ilar spelling or pronunciation.
Experts are consultedto see if a candidate is a true erroneous collocation.We compute three statistical features for each sen-tence below.
(1) The first feature is computed bym?i=1p(coi)/n, where m is the number of CLs, n isthe number of collocations in each sentence, andprobability p(coi) of each CL coi is calculated us-ing the method (Lu?
and Zhou, 2004).
(2) The sec-ond feature is computed by the ratio of the numberof unknown collocations (neither correct LCs nor er-roneous LCs) to the number of collocations in eachsentence.
(3) The last feature is computed by the ra-tio of the number of erroneous LCs to the number ofcollocations in each sentence.Perplexity from Language Model (PLM) Perplex-ity measures are extracted from a trigram languagemodel trained on a general English corpus usingthe SRILM-SRI Language Modeling Toolkit (Stolcke,2002).
We calculate two values for each sentence:lexicalized trigram perplexity and part of speech(POS) trigram perplexity.
The erroneous sentenceswould have higher perplexity.Syntactic Score (SC) Some erroneous sentences of-ten contain words and concepts that are locally cor-rect but cannot form coherent sentences (Liu andGildea, 2005).
To measure the coherence of sen-tences, we use a statistical parser Toolkit (Collins,1997) to assign each sentence a parser?s score thatis the related log probability of parsing.
We assumethat erroneous sentences with undesirable sentencestructures are more likely to receive lower scores.Function Word Density (FWD) We consider thedensity of function words (Corston-Oliver et al,2001), i.e.
the ratio of function words to contentwords.
This is inspired by the work (Corston-Oliveret al, 2001) showing that function word density canbe effective in distinguishing between human refer-ences and machine outputs.
In this paper, we calcu-late the densities of seven kinds of function words 55including determiners/quantifiers, all pronouns, differentpronoun types: Wh, 1st, 2nd, and 3rd person pronouns, prepo-Dataset Type Source NumberJC(+) the Japan Times newspaperand Model English Essay 16,857(-)HEL (Hiroshima EnglishLearners?
Corpus) and JLE(Japanese Learners of En-glish Corpus)17,301CC (+) the 21st Century newspaper 3,200(-)CLEC (Chinese Learner Er-ror Corpus) 3,199Table 1: Corpora ((+): correct; (-): erroneous)respectively as 7 features.4 Experimental EvaluationWe evaluated the performance of our techniqueswith support vector machine (SVM) and NaiveBayesian (NB) classification models.
We also com-pared the effectiveness of various features.
In ad-dition, we compared our technique with two othermethods of checking errors, Microsoft Word03 andALEK method (Chodorow and Leacock, 2000).
Fi-nally, we also applied our technique to evaluate theMachine Translation outputs.4.1 Experimental SetupClassification Models.
We used two classificationmodels, SVM6 and NB classification model.Data.
We collected two datasets from different do-mains, Japanese Corpus (JC) and Chinese Corpus(CC).
Table 1 gives the details of our corpora.
Inthe learner?s corpora, all of the sentences are erro-neous.
Note that our data does not consist of parallelpairs of sentences (one error sentence and its correc-tion).
The erroneous sentences includes grammar,sentence structure and lexical choice errors, but notspelling errors.For each sentence, we generated five kinds of fea-tures as presented in Section 3.
For a non-binaryfeature X , its value x is normalized by z-score,norm(x) = x?mean(X)?var(X) , where mean(x) is the em-pirical mean of X and var(X) is the variance of X .Thus each sentence is represented by a vector.Metrics We calculated the precision, recall,and F-score for correct and erroneous sentences,respectively, and also report the overall accuracy.sitions and adverbs, auxiliary verbs, and conjunctions.6http://svmlight.joachims.org/85All the experimental results are obtained thorough10-fold cross-validation.4.2 Experimental ResultsThe Effectiveness of Various Features.
The exper-iment is to evaluate the contribution of each featureto the classification.
The results of SVM are given inTable 2.
We can see that the performance of labeledsequential patterns (LSP) feature consistently out-performs those of all the other individual features.
Italso performs better even if we use all the other fea-tures together.
This is because other features onlyprovide some relatively abstract and simple linguis-tic information, whereas the discovered LSP s char-acterize significant linguistic features as discussedbefore.
We also found that the results of NB are alittle worse than those of SVM.
However, all the fea-tures perform consistently on the two classificationmodels and we can observe the same trend.
Due tospace limitation, we do not give results of NB.In addition, the discovered LSPs themselves areintuitive and meaningful since they are intuitive fea-tures that can distinguish correct sentences from er-roneous sentences.
We discovered 6309 LSPs inJC data and 3742 LSPs in CC data.
Some exam-ple LSPs discovered from erroneous sentences are<a, NNS> (support:0.39%, confidence:85.71%),<to, VBD> (support:0.11%, confidence:84.21%),and <the, more, the, JJ> (support:0.19%, confi-dence:0.93%) 7; Similarly, we also give some exam-ple LSPs mined from correct sentences: <NN, VBZ>(support:2.29%, confidence:75.23%), and <have,VBN, since> (support:0.11%, confidence:85.71%)8.
However, other features are abstract and it is hardto derive some intuitive knowledge from the opaquestatistical values of these features.As shown in Table 2, our technique achievesthe highest accuracy, e.g.
81.75% on the Japanesedataset, when we use all the features.
However, wealso notice that the improvement is not very signif-icant compared with using LSP feature individually(e.g.
79.63% on the Japanese dataset).
The similarresults are observed when we combined the featuresPLM, SC, FWD, and LC.
This could be explained7a + plural noun; to + past tense format; the more + the +base form of adjective8singular or mass noun + the 3rd person singular presentformat; have + past participle format + sinceby two reasons: (1) A sentence may contain sev-eral kinds of errors.
A sentence detected to be er-roneous by one feature may also be detected by an-other feature; and (2) Various features give conflict-ing results.
The two aspects suggest the directionsof our future efforts to improve the performance ofour models.Comparing with Other Methods.
It is difficultto find benchmark methods to compare with ourtechnique because, as discussed in Section 2, exist-ing methods often require error tagged corpora orparallel corpora, or focus on a specific type of er-rors.
In this paper, we compare our technique withthe grammar checker of Microsoft Word03 and theALEK (Chodorow and Leacock, 2000) method usedby ETS.
ALEK is used to detect inappropriate usageof specific vocabulary words.
Note that we do notconsider spelling errors.
Due to space limitation, weonly report the precision, recall, F-scorefor erroneous sentences, and the overall accuracy.As can be seen from Table 3, our method out-performs the other two methods in terms of over-all accuracy, F-score, and recall, while the threemethods achieve comparable precision.
We realizethat the grammar checker of Word is a general tooland the performance of ALEK (Chodorow and Lea-cock, 2000) can be improved if larger training data isused.
We found that Word and ALEK usually cannotfind sentence structure and lexical collocation errors,e.g., ?The more you listen to English, the easy it be-comes.?
contains the discovered LSP <the, more, the,JJ>?
Error.Cross-domain Results.
To study the performanceof our method on cross-domain data from writersof the same first-language background, we collectedtwo datasets from Japanese writers, one is composedof 694 parallel sentences (+:347, -:347), and theother 1,671 non-parallel sentences (+:795, -:876).The two datasets are used as test data while we useJC dataset for training.
Note that the test sentencescome from different domains from the JC data.
Theresults are given in the first two rows of Table 4.
Thisexperiment shows that our leaning model trained forone domain can be effectively applied to indepen-dent data in the other domains from the writes of thesame first-language background, no matter whetherthe test data is parallel or not.
We also noticed that86Dataset Feature A (-)F (-)R (-)P (+)F (+)R (+)PJCLSP 79.63 80.65 85.56 76.29 78.49 73.79 83.85LC 69.55 71.72 77.87 66.47 67.02 61.36 73.82PLM 61.60 55.46 50.81 64.91 62 70.28 58.43SC 53.66 57.29 68.40 56.12 34.18 39.04 32.22FWD 68.01 72.82 86.37 62.95 61.14 49.94 78.82LC + PLM + SC + FWD 71.64 73.52 79.38 68.46 69.48 64.03 75.94LSP + LC + PLM + SC + FWD 81.75 81.60 81.46 81.74 81.90 82.04 81.76CCLSP 78.19 76.40 70.64 83.20 79.71 85.72 74.50LC 63.82 62.36 60.12 64.77 65.17 67.49 63.01PLM 55.46 64.41 80.72 53.61 40.41 30.22 61.30SC 50.52 62.58 87.31 50.64 13.75 14.33 13.22FWD 61.36 60.80 60.70 60.90 61.90 61.99 61.80LC + PLM + SC + FWD 67.69 67.62 67.51 67.77 67.74 67.87 67.64LSP + LC + PLM + SC + FWD 79.81 78.33 72.76 84.84 81.10 86.92 76.02Table 2: The Experimental Results (A: overall accuracy; (-): erroneous sentences; (+): correct sentences; F:F-score; R: recall; P: precision)Dataset Model A (-)F (-)R (-)PJCOurs 81.39 81.25 81.24 81.28Word 58.87 33.67 21.03 84.73ALEK 54.69 20.33 11.67 78.95CCOurs 79.14 77.81 73.17 83.09Word 58.47 32.02 19.81 84.22ALEK 55.21 22.83 13.42 76.36Table 3: The Comparison ResultsLSPs play dominating role in achieving the results.Due to space limitation, no details are reported.To further see the performance of our methodon data written by writers with different first-language backgrounds, we conducted two experi-ments.
(1) We merge the JC dataset and CC dataset.The 10-fold cross-validation results on the mergeddataset are given in the third row of Table 4.
Theresults demonstrate that our models work well whenthe training data and test data contain sentences fromdifferent first-language backgrounds.
(2) We use theJC dataset (resp.
CC dataset) for training while theCC dataset (resp.
JC dataset) is used as test data.
Asshown in the fourth (resp.
fifth) row of Table 4, theresults are worse than their corresponding results ofWord given in Table 3.
The reason is that the mis-takes made by Japanese and Chinese are different,thus the learning model trained on one data does notwork well on the other data.
Note that our method isnot designed to work in this scenario.Application to Machine Translation Evaluation.Our learning models could be used to evaluate theMT results as an complementary measure.
This isbased on the assumption that if the MT results canbe accurately distinguished from human referencesDataset A (-)F (-)R (-)PJC(Train)+nonparallel(Test) 72.49 68.55 57.51 84.84JC(Train)+parallel(Test) 71.33 69.53 65.42 74.18JC + CC 79.98 79.72 79.24 80.23JC(Train)+ CC(Test) 55.62 41.71 31.32 62.40CC(Train)+ JC(Test) 57.57 23.64 16.94 39.11Table 4: The Cross-domain Results of our Methodby our technique, the MT results are not natural andmay contain errors as well.The experiment was conducted using 10-foldcross validation on two LDC data, low-ranked andhigh-ranked data9.
The results using SVM as classi-fication model are given in Table 5.
As expected, theclassification accuracy on low-ranked data is higherthan that on high-ranked data since low-ranked MTresults are more different from human referencesthan high-ranked MT results.
We also found thatLSPs are the most effective features.
In addition, ourdiscovered LSPs could indicate the common errorsmade by the MT systems and provide some sugges-tions for improving machine translation results.As a summary, the mined LSPs are indeed effec-tive for the classification models and our proposedtechnique is effective.5 Conclusions and Future WorkThis paper proposed a new approach to identifyingerroneous/correct sentences.
Empirical evaluatingusing diverse data demonstrated the effectiveness of9One LDC data contains 14,604 low ranked (score 1-3) ma-chine translations and the corresponding human references; theother LDC data contains 808 high ranked (score 3-5) machinetranslations and the corresponding human references87Data Feature A (-)F (-)R (-)P (+)F (+)R (+)PLow-ranked data (1-3 score) LSP 84.20 83.95 82.19 85.82 84.44 86.25 82.73LSP+LC+PLM+SC+FWD 86.60 86.84 88.96 84.83 86.35 84.27 88.56High-ranked data (3-5 score) LSP 71.74 73.01 79.56 67.59 70.23 64.47 77.40LSP+LC+PLM+SC+FWD 72.87 73.68 68.95 69.20 71.92 67.22 77.60Table 5: The Results on Machine Translation Dataour techniques.
Moreover, we proposed to mineLSPs as the input of classification models from a setof data containing correct and erroneous sentences.The LSPs were shown to be much more effective thanthe other linguistic features although the other fea-tures were also beneficial.We will investigate the following problems in thefuture: (1) to make use of the discovered LSPs to pro-vide detailed feedback for ESL learners, e.g.
the er-rors in a sentence and suggested corrections; (2) tointegrate the features effectively to achieve better re-sults; (3) to further investigate the application of ourtechniques for MT evaluation.ReferencesRakesh Agrawal and Ramakrishnan Srikant.
1995.
Mining se-quential patterns.
In ICDE.Emily M. Bender, Dan Flickinger, Stephan Oepen, AnnemarieWalsh, and Timothy Baldwin.
2004.
Arboretum: Using aprecision grammar for grammmar checking in call.
In Proc.InSTIL/ICALL Symposium on Computer Assisted Learning.Chris Brockett, William Dolan, and Michael Gamon.
2006.Correcting esl errors using phrasal smt techniques.
In ACL.Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLinguistics, 19:263?311.Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, MartinChodorow, Lisa Braden-Harder, and Mary Dee Harris.
1998.Automated scoring using a hybrid feature identification tech-nique.
In Proc.
ACL.Martin Chodorow and Claudia Leacock.
2000.
An unsuper-vised method for detecting grammatical errors.
In NAACL.Michael Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proc.
ACL.Simon Corston-Oliver, Michael Gamon, and Chris Brockett.2001.
A machine learning approach to the automatic eval-uation of machine translation.
In Proc.
ACL.P.W.
Foltz, D. Laham, and T.K.
Landauer.
1999.
Automatedessay scoring: Application to educational technology.
In Ed-Media ?99.Michael Gamon, Anthony Aue, and Martine Smets.
2005.Sentence-level mt evaluation without reference translations:Beyond language modeling.
In Proc.
EAMT.Shicun Gui and Huizhong Yang.
2003.
Zhongguo XuexizheYingyu Yuliaohu.
(Chinese Learner English Corpus).
Shang-hai: Shanghai Waiyu Jiaoyu Chubanshe.
(In Chinese).George E. Heidorn.
2000.
Intelligent Writing Assistance.Handbook of Natural Language Processing.
Robert Dale,Hermann Moisi and Harold Somers (ed.).
Marcel Dekker.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Sup-nithi, and Hitoshi Isahara.
2003.
Automatic error detectionin the japanese learners?
english spoken data.
In Proc.
ACL.Nitin Jindal and Bing Liu.
2006.
Identifying comparative sen-tences in text documents.
In SIGIR.Ding Liu and Daniel Gildea.
2005.
Syntactic features forevaluation of machine translation.
In Proc.
ACL Workshopon Intrinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization.Yajuan Lu?
and Ming Zhou.
2004.
Collocation translation ac-quisition using monolingual corpora.
In Proc.
ACL.Lisa N. Michaud, Kathleen F. McCoy, and Christopher A. Pen-nington.
2000.
An intelligent tutoring system for deaf learn-ers of written english.
In Proc.
4th International ACM Con-ference on Assistive Technologies.Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and Naoki Isu.2006.
A feedback-augmented method for detecting errors inthe writing of learners of english.
In Proc.
ACL.Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Du-rand.
1999.
Cross-language information retrieval based onparallel texts and automatic mining of parallel texts from theweb.
In SIGIR, pages 74?81.Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, and Helen Pinto.2001.
Prefixspan: Mining sequential patterns efficiently byprefix-projected pattern growth.
In Proc.
ICDE.Yongmei Shi and Lina Zhou.
2005.
Error detection using lin-guistic features.
In HLT/EMNLP.Andreas Stolcke.
2002.
Srilm-an extensible language modelingtoolkit.
In Proc.
ICSLP.Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and MingZhou.
2007.
Mining sequential patterns and tree patterns todetect erroneous sentences.
In AAAI.Tono Yukio, T. Kaneko, H. Isahara, T. Saiga, and E. Izumi.2001.
The standard speaking test corpus: A 1 million-wordspoken corpus of japanese learners of english and its impli-cations for l2 lexicography.
In ASIALEX: Asian Bilingualismand the Dictionary.88
