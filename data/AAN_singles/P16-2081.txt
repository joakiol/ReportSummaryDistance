Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 499?505,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSemantics-Driven Recognition of Collocations Using Word EmbeddingsSara Rodr?
?guez-Fern?andez1, Luis Espinosa-Anke1, Roberto Carlini1, and Leo Wanner1,21NLP Group, Department of Information and Communication Technologies, Pompeu Fabra UniversityC/ Roc Boronat, 138, 08018 Barcelona (Spain)2Catalan Institute for Research and Advanced Studies (ICREA)sara.rodriguez.fernandez|luis.espinosa|roberto.carlini|leo.wanner@upf.eduAbstractL2 learners often produce ?ungrammat-ical?
word combinations such as, e.g.,*give a suggestion or *make a walk.
Thisis because of the ?collocationality?
of oneof their items (the base) that limits the ac-ceptance of collocates to express a spe-cific meaning (?perform?
above).
We pro-pose an algorithm that delivers, for a givenbase and the intended meaning of a collo-cate, the actual collocate lexeme(s) (make/ take above).
The algorithm exploits thelinear mapping between bases and collo-cates from examples and generates a collo-cation transformation matrix which is thenapplied to novel unseen cases.
The evalua-tion shows a promising line of research incollocation discovery.1 IntroductionCollocations of the kind make [a] suggestion, at-tend [a] lecture, heavy rain, deep thought, strongtea, etc., are restricted lexical co-occurrences oftwo syntactically bound lexical elements (Kilgar-riff, 2006).
The central role of collocations for sec-ond language (henceforth, L2) learning has beendiscussed in a series of theoretical and empiri-cal studies (Hausmann, 1984; Bahns and Eldaw,1993; Granger, 1998; Lewis and Conzett, 2000;Nesselhauf, 2005; Alonso Ramos et al, 2010) andis widely reflected in (especially English) learnerdictionaries.
In computational lexicography, sev-eral statistical measures have been used to retrievecollocations from corpora, among them, mutualinformation (Church and Hanks, 1989; Lin, 1999),entropy (Kilgarriff, 2006), pointwise mutual infor-mation (Bouma, 2010), and weighted pointwisemutual information (Carlini et al, 2014).1How-ever, the needs of language learners go beyondmere lists of collocations: the cited studies revealthat language learners often build ?miscolloca-tions?
(as, e.g., *give a suggestion or *have the cu-riosity) to express the intended meaning.
In otherwords, they fail to observe, in Kilgarriff?s terms,the ?collocationality?
restrictions of L2, which im-ply that in language production, one of the ele-ments of a collocation (the base) is freely cho-sen, while the choice of the other (the collocate)depends on the base (Hausmann, 1989; Cowie,1994).
For instance, to express the meaning of?do?
or ?perform?, the base suggestion promptsfor the choice of make as collocate: make [a]suggestion, while advice prompts for give: give[an] advice; to express the meaning of ?participatein?, lecture prompts for attend: attend [a] lecture,while operation prompts for assist: assist [an] op-eration; to express the meaning of ?intense?
inconnection with rain, the right collocate is heavy,while ?intense wind?
is strong wind.
And so on.The idiosyncrasy of collocations makes them alsolanguage-specific.
Thus, in English, you take [a]walk, in Spanish you ?give?
it (dar [un] paseo),and in German and French you ?make?
it ([einen]Spaziergang machen, faire [une] promenade); inEnglish, rain is heavy, while in Spanish and Ger-man it is ?strong?
(fuerte lluvia/starker Regen).In order to effectively support L2 learners, tech-niques are thus needed that are able not only toretrieve collocations, but also provide for a givenbase (or headword) and a given semantic gloss ofa collocate meaning, the actual collocate lexeme.In what follows, we present such a technique,which is grounded in Mikolov et al (2013c)?sword embeddings, and which leverages the factthat semantically related words in two different1See (Pecina, 2008) for a detailed survey of such mea-sures.499vector representations are related by linear trans-formation (Mikolov et al, 2013b).
This prop-erty has been exploited for word-based translationMikolov et al (2013b), learning semantic hierar-chies (hyponym-hypernym relations) in Chinese(Fu et al, 2014), and modeling linguistic sim-ilarities between standard (Wikipedia) and non-standard language (Twitter) (Tan et al, 2015).
Inour task, we learn a transition matrix over a smallnumber of collocation examples, where collocatesshare the same semantic gloss, to apply then thismatrix to discover new collocates for any previ-ously unseen collocation base.
We discuss the out-come of the experiments with ten different col-locate glosses (including ?do?
/ ?perform?, ?in-crease?, ?decrease?, etc.
), and show that for mostglosses, an approach that combines a stage ofthe application of a gloss-specific transition ma-trix with a pruning stage that is based on statisti-cal evidence outperforms approaches that exploitonly one of these stages as well as a baselinethat is based on collocation retrieval exploiting theembeddings property for drawing analogies, suchas, e.g., x ?
applause ?
heavy ?
rain (imply-ing x=thunderous) (Rodr?
?guez-Fern?andez et al,2016).2 Theoretical modelThe semantic glosses of collocates across collo-cations can be generalized into a generic seman-tic typology modeled, e.g., by Mel?
?cuk (1996)?sLexical Functions.
For instance, absolute, deep,strong, heavy in absolute certainty, deep thought,strong wind, and heavy storm can all be glossedas ?intense?
; make, take, give, carry out in make[a] proposal, take [a] step, give [a] hint, carry out[an] operation can be glossed as ?do?/?perform?;etc.
Our goal is to capture the relation that holdsbetween the training bases and the collocates withthe same gloss, such that given a new base anda gloss, we can retrieve its corresponding collo-cate(s) with this gloss.
Thus, given absolute cer-tainty, deep thought, and strong wind as trainingexamples, storm as input base and ?intense?
asgloss, we aim at retrieving the collocate heavy.
Asalready mentioned above, our approach is basedon Mikolov et al (2013b)?s linear transformationmodel, which associates word vector representa-tions between two analogous spaces.
In Mikolovet al?s original work, one space captures wordsin language L1and the other space words in lan-guage L2, such that the found relations are be-tween translation equivalents.
In our case, we de-fine a base space B and a collocate space C in or-der to relate bases with their collocates that havethe same meaning, in the same language.
To ob-tain the word vector representations in B and C,we use Mikolov et al (2013c)?s word2vec.2The linear transformation model is constructedas follows.
Let T be a set of collocations whosecollocates share the semantic gloss ?
, and let btiand ctibe the collocate respectively base of thecollocation ti?
T. The base matrix B?=[bt1, bt2.
.
.
btn] and the collocate matrix C?=[ct1, ct2.
.
.
ctn] are given by their correspondingvector representations.
Together, they constitute aset of training examples ?
?, composed by vectorpairs {bti, cti}ni=1.?
?is used to learn a linear transformation ma-trix ???
RB?C.
Following the notation in (Tan etal., 2015), this transformation can be depicted as:B??
?= C?We follow Mikolov et al?s original approachand compute ?
?as follows:min??|??|?i=1???bti?
cti?2Hence, for any given novel base bj?, we obtain anovel list of ranked collocates by applying ?
?bj?and filtering the resulting candidates by part ofspeech and NPMI , an association measure thatis based on the pointwise mutual information, buttakes into account the asymmetry of the lexical de-pendencies between a base and its collocate (Car-lini et al, 2014):NPMI =PMI(collocate, base)?log(p(collocate))3 Experiments3.1 Setup of the ExperimentsWe carried out experiments with 10 of the mostfrequent semantic collocate glosses (listed in thefirst column of Table 1).
As is common in pre-vious work on semantic collocation classification(Moreno et al, 2013; Wanner et al, 2016), ourtraining set consists of a list of manually anno-tated correct collocations.
For this purpose, we2https://code.google.com/archive/p/word2vec/500Semantic gloss Example # instances?intense?
absolute certainty 586?weak?
remote chance 70?perform?
give chase 393?begin to perform?
take up a chase 79?stop performing?
abandon a chase 12?increase?
improve concentration 73?decrease?
limit [a] choice 73?create?, ?cause?
pose [a] challenge 195?put an end?
break [the] calm 79?show?
exhibit [a] characteristic 49Table 1: Semantic glosses and size of training setrandomly selected nouns from the Macmillan Dic-tionary and manually classified their correspond-ing collocates with respect to the glosses.3Notethat there may be more than one collocate for eachbase.
Since collocations with different collocatemeanings are not evenly distributed in language(e.g., speakers use more often collocations con-veying the idea of ?intense?
and ?perform?
than?stop performing?
), the number of instances pergloss in our training data also varies significantly(see Table 1).Due to the asymmetric nature of collocations,not all corpora may be equally suitable for thederivation of word embedding representations forboth bases and collocates.
Thus, we may hypoth-esize that for modeling (nominal) bases, whichkeep in collocations their literal meaning, a stan-dard register corpus with a small percentage offigurative meanings will be more adequate, whilefor modeling collocates, a corpus which is poten-tially rich in collocations is likely to be more ap-propriate.
In order to verify this hypothesis, wecarried out two different experiments.
In the firstexperiment, we used for both bases and collocatesvectors pre-trained on the Google News corpus(GoogleVecs), which is available at word2vec?swebsite.
In the second experiment, the bases weremodeled by training their word vectors over a2014 dump of the English Wikipedia, while formodeling collocates, again, GoogleVecs has beenused.
In other words, we assumed that Wikipediais a standard register corpus and thus better formodelingB, while GoogleVecs is more suitable formodeling C. The figures in Section 3.2 below willgive us a hint whether this assumption is correct.3At this stage of our work, we considered only colloca-tions that involve single word tokens for both the base andthe collocate.
In other words, we did not take into account,e.g., phrasal verb collocates such as stand up, give up or calmdown.
We also left aside the problem of subcategorization incollocations; cf., e.g., into in take [into] consideration.For the calculation of NPMI during post-processing, the British National Corpus (BNC)was used.43.2 EvaluationThe outcome of each experiment was assessed byverifying the correctness of each retrieved candi-date from the top-10 candidates obtained for eachtest base.
A total of 10 bases was evaluated foreach gloss.
The ground truth test set was createdin a similar fashion as the training set: nouns fromthe Macmillan Dictionary were randomly chosen,and their collocates manually classified in termsof the different glosses, until a set of ten unseenbase?collocate pairs was obtained for each gloss.For the outcome of each experiment, we com-puted both precision (p) as the ratio of retrievedcollocates that match the targeted glosses to theoverall number of obtained collocates for eachbase, and Mean Reciprocal Rank (MRR), whichrewards the position of the first correct result in aranked list of outcomes:MRR =1|Q||Q|?i=11rankiwhereQ is a sample of experiment runs and rankirefers to the rank position of the first relevant out-come for the ith run.
MRR is commonly usedin Information Retrieval and Question Answering,but has also shown to be well suited for collocationdiscovery; see, e.g., (Wu et al, 2010).We evaluated four different configurations ofour technique against two baselines.
Thefirst baseline (S1) is based on the regulari-ties in word embeddings, with the vec(king) ?vec(man) + vec(woman) = vec(queen) exam-ple as paramount case.
In this context, we man-ually selected one representative example for eachsemantic gloss to discover collocates for novelbases following the same schema; cf., e.g., forthe gloss ?perform?
vec(take) ?
vec(walk) +vec(suggestion) = vec(make) (where make isthe collocate to be discovered); see (Rodr?
?guez-Fern?andez et al, 2016) for details.
The secondbaseline (S2) is an extension of S1 in that its output4As one of the reviewers pointed out, BNC might not beoptimal as a collocation reference corpus.
On the one hand,it does not capture collocations that might be idiosyncratic toAmerican English, and, on the other hand, it might be out-dated (and thus not contain more recent collocations).
It issubject of future work to verify whether another representa-tive corpus of English serves better.501Precision (p) Mean Reciprocal Rank (MRR)Semantic gloss S1 S2 S3 S4 S5 S6 S1 S2 S3 S4 S5 S6?intense?
0.08 0.43 0.04 0.50 0.24 0.72 0.18 0.35 0.35 0.15 0.66 0.82?weak?
0.09 0.11 0.23 0.45 0.27 0.39 0.31 0.15 0.69 0.64 0.61 0.47?perform?
0.05 0.17 0.01 0.06 0.13 0.40 0.22 0.32 0.01 0.35 0.70 0.79?begin to perform?
0.03 0.08 0.24 0.30 0.22 0.38 0.17 0.05 0.61 0.64 0.70 0.71?stop performing?
0.00 0.00 0.11 0.15 0.12 0.20 0.01 0.00 0.90 0.66 0.71 0.65?increase?
0.16 0.53 0.31 0.43 0.35 0.53 0.47 0.72 0.78 0.86 0.86 0.90?decrease?
0.07 0.05 0.28 0.25 0.27 0.28 0.18 0.04 0.57 0.38 0.37 0.30?create?, ?cause?
0.10 0.16 0.01 0.15 0.14 0.53 0.41 0.23 0.11 0.11 0.48 0.58?put an end?
0.05 0.09 0.15 0.20 0.08 0.25 0.28 0.10 0.38 0.36 0.33 0.38?show?
0.10 0.55 0.24 0.49 0.49 0.70 0.44 0.54 0.87 0.82 0.73 0.81Table 2: Precision and MRRSemantic gloss Base Retrieved candidates?intense?
caution extreme?weak?
change slight, little, modest, minor, noticeable, minimal, sharp, definite, small, big?perform?
calculation produce, carry?begin to perform?
cold catch, get, run, keep?stop performing?
career abandon, destroy, ruin, terminate, threaten, interrupt?increase?
capability enhance, increase, strengthen, maintain, extend, develop, upgrade, build, provide?decrease?
congestion reduce, relieve, cut, ease, combat?create?, ?cause?
challenge pose?put an end?
ceasefire break?show?
complexity demonstrate, reveal, illustrate, indicate, reflect, highlight, recognize, explainTable 3: Examples of retrieved collocationsis filtered with respect to the valid POS-patterns oftargeted collocations and NPMI .5The four configurations of our technique thatwe tested were: S3, which is based on the tran-sition matrix for which GoogleVecs is used as ref-erence vector space representation for both basesand collocates; S4, which applies POS-pattern andNPMI filters to the output of S3; S5, which isequivalent to S3, but relies on a vector space rep-resentation derived from Wikipedia for learningbases projections and on a vector space represen-tation from GoogleVecs for collocate projections;and, finally, S6, where the S5 output is, again, fil-tered by POS collocation patterns and NPMI .4 DiscussionThe results of the experiments are displayed inTable 2.
In general, the configurations S3 ?
S6largely outperform the baselines, with the excep-tion of the gloss ?increase?, for which S2 equalsS6 as far as p is concerned.
However, in this casetoo MRR is considerably higher for S6, whichachieves the highest MMR scores for 6 and thehighest precision scores for 7 out of 10 glosses5At the first glan ce, a state-of-the-art approach on cor-rection of collocation errors by suggesting alternative co-occurrences, such, as, e.g., (Dahlmeier and Ng, 2011; Parket al, 2008; Futagi et al, 2008), might appear as a suitablebaseline.
We discarded this option given that none of themuses explicit fine-grained semantics.
(see the S6 columns in Table 2).
In other words,the full pipeline promotes good collocate candi-dates to the first positions of the ranked result listsand is also best in terms of accuracy.Comparing S1, S3, S5 to S2, S4, and S6 , wemay conclude that the inclusion of a filtering mod-ule (and, in particular, of anNPMI filtering mod-ule) contributes substantially to the overall preci-sion in nearly all cases (?decrease?
being the onlyexception).
The comparison of the precision ob-tained for configurations S3 and S5 also revealsthat for 7 glosses the strategy to model C and Bon different corpora paid off.
This is different asfar as MRR is concerned.
Further investigation isneeded for the examination of this discrepancy.We can observe that certain glosses seem to ex-hibit less linguistic variation, requiring a less pop-ulated transformation function from bases to col-locates.
Consider the case of ?show?, which gen-erates with only 49 training pairs the second besttransition matrix, with p=0.70.
It is also informa-tive to contrast the performance on pairs of glosseswith opposite meanings, such as e.g., ?begin toperform?
vs. ?stop performing?
; ?increase?
vs.
?de-crease?
; ?intense?
vs.
?weak?
; and finally ?create,cause?
vs. ?put an end?.
Better performance isachieved consistently on the positive counterparts(e.g.
?begin to perform?
over ?stop performing?
).A closer look at the output reveals that in these502Semantic gloss S6?intense?
0.82?weak?
0.45?perform?
0.40?begin to perform?
0.42?stop performing?
0.22?increase?
0.55?decrease?
0.37?create?, ?cause?
0.59?put an end?
0.43?show?
0.85Table 4: Precision of the coarse-grained evaluationof the S6 configurationcases positive glosses are persistently classified asnegative.
Further research is needed to first under-stand why this is the case and then to come up withan improvement of the technique in particular onthe negative glosses.The fact that for some of the glosses precision israther low may be taken as a hint that the proposedtechnique is not suitable for the task of semantics-oriented recognition of collocations.
However, itshould be also stressed that our evaluation wasvery strict: a retrieved collocate candidate wasconsidered as correct only if it formed a colloca-tion with the base, and if it belonged to the tar-get semantic gloss.
In particular the first condi-tion might be too rigorous, given that, in somecases, there is a margin of doubt whether a com-bination is a free co-occurrence or a collocation;cf., e.g., huge challenge or reflect [a] concern,which were rejected as collocations in our eval-uation.
Since for L2 learners such co-occurrencesmay be also useful, we carried out a second eval-uation in which all the suggested collocate candi-dates that belonged to a target semantic gloss wereconsidered as correct, even if they did not form acollocation.6Cf.
Table 4 for the outcome of thisevaluation for the S6 configuration.
Only for ?per-form?
the precision remained the same as before.This is because collocates assigned to this glossare support verbs (and thus void of own lexical se-mantic content).5 ConclusionsAs already pointed out in Section 1, a substantialamount of work has been carried out to automati-cally retrieve collocations from corpora (Choueka,1988; Church and Hanks, 1989; Smadja, 1993;6Obviously, collocate candidates were considered as in-correct if they formed incorrect collocations with the base.Examples of such incorrect collocations are stop [the] calmand develop [a] calculation.Lin, 1999; Kilgarriff, 2006; Evert, 2007; Pecina,2008; Bouma, 2010; Futagi et al, 2008; Gao,2013).
Most of this work is based on statisticalmeasures that indicate how likely the elements ofa possible collocation are to co-occur, while ignor-ing the semantics of the collocations.
Semanticclassification of collocations has been addressed,for instance, in (Wanner et al, 2006; Gelbukh andKolesnikova., 2012; Moreno et al, 2013; Wanneret al, 2016).
However, to the best of our knowl-edge, our work is the first to automatically retrieveand typify collocations simultaneously.
We haveillustrated our approach with 10 semantic colloca-tion glosses.
We believe that this approach is alsovalid for the coverage of the remaining glosses(Mel?
?cuk (1996) lists in his typology 64 glossesin total).Distributed vector representations (or word em-beddings) (Mikolov et al, 2013c; Mikolov etal., 2013a), which we use, have proven use-ful in a plethora of NLP tasks, including se-mantic similarity and relatedness (Huang et al,2012; Faruqui et al, 2015; Camacho-Colladoset al, 2015; Iacobacci et al, 2015), dependencyparsing (Duong et al, 2015), and Named EntityRecognition (Tang et al, 2014).
We show thatthey also work for semantic retrieval of colloca-tions.
Only a small amount of collocations andbig unannotated corpora have been necessary toperform the experiments.
This makes our ap-proach highly scalable and portable.
Given thelack of semantically tagged collocation resourcesfor most languages, our work has the potential tobecome influential in the context of second lan-guage learning.
The datasets on which we per-formed the experiments as well as the details con-cerning the code and its use can be found athttp://www.taln.upf.edu/content/resources/765.6 AcknowledgementsThe present work has been partially funded bythe Spanish Ministry of Economy and Com-petitiveness (MINECO), through a predoctoralgrant (BES-2012-057036) in the framework ofthe project HARenES (FFI2011-30219-C02-02),and by the European Commission under the grantnumber H2020?645012?RIA.
We also acknowl-edge support from the Maria de Maeztu Excel-lence Program (MDM-2015-0502).
Many thanksto the three anonymous reviewers for insightfulcomments and suggestions.503ReferencesM.
Alonso Ramos, L. Wanner, O. Vincze,G.
Casamayor, N. V?azquez, E. Mosqueira, andS.
Prieto.
2010.
Towards a Motivated AnnotationSchema of Collocation Errors in Learner Corpora.In Proceedings of the 7th International Conferenceon Language Resources and Evaluation (LREC),pages 3209?3214, La Valetta, Malta.J.
Bahns and M. Eldaw.
1993.
Should we Teach EFLStudents Collocations?
System, 21(1):101?114.G.
Bouma.
2010.
Collocation Extraction beyond theIndependence Assumption.
In Proceedings of theACL 2010, Short paper track, Uppsala.J.
Camacho-Collados, M.T.
Pilehvar, and R. Nav-igli.
2015.
NASARI: a Novel Approach to aSemantically-Aware Representation of Items.
InProceedings of NAACL, pages 567?577.R.
Carlini, J. Codina-Filba, and L. Wanner.
2014.
Im-proving Collocation Correction by Ranking Sugges-tions Using Linguistic Knowledge.
In Proceedingsof the 3rd Workshop on NLP for Computer-AssistedLanguage Learning, Uppsala, Sweden.Y.
Choueka.
1988.
Looking for Needles in a Haystackor Locating Interesting Collocational Expressions inLarge Textual Databases.
In Proceedings of theRIAO, pages 34?38.K.
Church and P. Hanks.
1989.
Word AssociationNorms, Mutual Information, and Lexicography.
InProceedings of the 27th Annual Meeting of the ACL,pages 76?83.A.
Cowie.
1994.
Phraseology.
In R.E.
Asher andJ.M.Y.
Simpson, editors, The Encyclopedia of Lan-guage and Linguistics, Vol.
6, pages 3168?3171.Pergamon, Oxford.D.
Dahlmeier and H.T.
Ng.
2011.
Correcting SemanticCollocation Errors with L1-Induced Paraphrases.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 107?117.
Association for Computational Linguistics.L.
Duong, T. Cohn, S. Bird, and P. Cook.
2015.
ANeural Network Model for Low-Resource Univer-sal Dependency Parsing.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2015, Lisbon, Portugal,September 17-21, 2015, pages 339?348.S.
Evert.
2007.
Corpora and Collocations.
InA.
L?udeling and M. Kyt?o, editors, Corpus Lin-guistics.
An International Handbook.
Mouton deGruyter, Berlin.M.
Faruqui, J.
Dodge, Jauhar.
S.K., C. Dyer, E.H.Hovy, and N.A.
Smith.
2015.
Retrofitting WordVectors to Semantic Lexicons.
In NAACL HLT2015, The 2015 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Denver,Colorado, USA, May 31 - June 5, 2015, pages 1606?1615.R.
Fu, J. Guo, B. Qin, W. Che, H. Wang, and T. Liu.2014.
Learning Semantic Hierarchies via Word Em-beddings.
In Proceedings of the 52th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers, volume 1.Y.
Futagi, P. Deane, M. Chodorow, and J. Tetreault.2008.
A Computational Approach to Detecting Col-location Errors in the Writing of Non-Native Speak-ers of English.
Computer Assisted Language Learn-ing, 21(1):353?367.Z.M.
Gao.
2013.
Automatic Identification of En-glish Collocation Errors based on Dependency Re-lations.
Sponsors: National Science Council, Exec-utive Yuan, ROC Institute of Linguistics, AcademiaSinica NCCU Office of Research and Development,page 550.A.
Gelbukh and O. Kolesnikova.
2012.
Semantic Anal-ysis of Verbal Collocations with Lexical Functions.Springer, Heidelberg.S.
Granger.
1998.
Prefabricated Patterns in Ad-vanced EFL Writing: Collocations and Formulae.In A. Cowie, editor, Phraseology: Theory, Analy-sis and Applications, pages 145?160.
Oxford Uni-versity Press, Oxford.F.J.
Hausmann.
1984.
Wortschatzlernen ist Kolloka-tionslernen.
Zum Lehren und Lernen franz?osischerWortwendungen.
Praxis des neusprachlichen Un-terrichts, 31(1):395?406.F.J.
Hausmann.
1989.
Le dictionnaire de colloca-tions.
In F.J. Hausmann, O. Reichmann, H.E.
Wie-gand, and L. Zgusta, editors, W?orterb?ucher, Dictio-naries, Dictionnaires: An international Handbookof Lexicography, pages 1010?1019.
De Gruyter,Berlin/New-York.E.H.
Huang, R. Socher, C.D.
Manning, and A.Y.
Ng.2012.
Improving Word Representations via GlobalContext and Multiple Word Prototypes.
In Pro-ceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Association for Compu-tational Linguistics.I.
Iacobacci, M.T.
Pilehvar, and R. Navigli.
2015.SENSEMBED: Enhancing Word Embeddings forSemantic Similarity and Relatedness.
In Proceed-ings of ACL, Beijing, China, July.
Association forComputational Linguistics.A.
Kilgarriff.
2006.
Collocationality (and How toMeasure it).
In Proceedings of the Euralex Confer-ence, pages 997?1004, Turin, Italy.
Springer-Verlag.M.
Lewis and J. Conzett.
2000.
Teaching Colloca-tion.
Further Developments in the Lexical Approach.LTP, London.504D.
Lin.
1999.
Automatic Identification of Non-Compositional Phrases.
In Proceedings of the37th annual meeting of the Association for Compu-tational Linguistics on Computational Linguistics,pages 317?324.
Association for Computational Lin-guistics.I.A.
Mel??cuk.
1996.
Lexical functions: A Tool forthe Description of Lexical Relations in the Lexicon.In L. Wanner, editor, Lexical Functions in Lexicog-raphy and Natural Language Processing, pages 37?102.
Benjamins Academic Publishers, Amsterdam.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013a.Efficient Estimation of Word Representations inVector Space.
arXiv preprint arXiv:1301.3781.T.
Mikolov, Q.V.
Le, and I. Sutskever.
2013b.
Ex-ploiting Similarities among Languages for MachineTranslation.
arXiv preprint arXiv:1309.4168.T.
Mikolov, W. Yih, and G. Zweig.
2013c.
LinguisticRegularities in Continuous Space Word Representa-tions.
In HLT-NAACL, pages 746?751.P.
Moreno, G. Ferraro, and L. Wanner.
2013.
Canwe Determine the Semantics of Collocations with-out using Semantics?.
In I. Kosem, J. Kallas, P. Gan-tar, S. Krek, M. Langemets, and M. Tuulik, editors,Proceedings of the eLex 2013 conference, Tallinn& Ljubljana.
Trojina, Institute for Applied SloveneStudies & Eesti Keele Instituut.N.
Nesselhauf.
2005.
Collocations in a Learner Cor-pus.
Benjamins Academic Publishers, Amsterdam.T.
Park, E. Lank, P. Poupart, and M. Terry.
2008.
Isthe Sky Pure Today?
awkchecker: an Assistive Toolfor Detecting and Correcting Collocation Errors.
InProceedings of the 21st annual ACM symposium onUser interface software and technology, pages 121?130.
ACM.P.
Pecina.
2008.
A Machine Learning Approach toMultiword Expression Extraction.
In Proceedingsof the LREC 2008 Workshop Towards a Shared Taskfor Multiword Expressions (MWE 2008), pages 54?57, Marrakech.S.
Rodr?
?guez-Fern?andez, R. Carlini, L. Espinosa-Anke,and L. Wanner.
2016.
Example-based Acquisitionof Fine-grained Collocation Resources.
In Proceed-ings of LREC, Portoro?z, Slovenia.F.
Smadja.
1993.
Retrieving Collocations from Text:X-Tract.
Computational Linguistics, 19(1):143?177.L.
Tan, H. Zhang, C. Clarke, and M. Smucker.
2015.Lexical Comparison Between Wikipedia and TwitterCorpora by Using Word Embeddings.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 657?661,Beijing, China, July.
Association for ComputationalLinguistics.B.
Tang, H. Cao, X. Wang, Q. Chen, and H. Xu.2014.
Evaluating Word Representation Featuresin Biomedical Named Entity Recognition Tasks.BioMed research international, 2014.L.
Wanner, B. Bohnet, and M. Giereth.
2006.
MakingSense of Collocations.
Computer Speech and Lan-guage, 20(4):609?624.L.
Wanner, G. Ferraro, and P. Moreno.
2016.Towards Distributional Semantics-based Classifi-cation of Collocations for Collocation Dictio-naries.
International Journal of Lexicography,doi:10.1093/ijl/ecw002.J.C.
Wu, Y.C.
Chang, T. Mitamura, and J.S.
Chang.2010.
Automatic Collocation Suggestion in Aca-demic Writing.
In Proceedings of the ACL Confer-ence, Short paper track, Uppsala.505
