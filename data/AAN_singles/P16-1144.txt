Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1525?1534,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsThe LAMBADA dataset:Word prediction requiring a broad discourse context?Denis Paperno, Germ?an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham?,Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, Raquel Fern?andez?CIMeC - Center for Mind/Brain Sciences, University of Trento{firstname.lastname}@unitn.it,?quanpn90@gmail.com?Institute for Logic, Language & Computation, University of Amsterdamraquel.fernandez@uva.nlAbstractWe introduce LAMBADA, a dataset toevaluate the capabilities of computationalmodels for text understanding by meansof a word prediction task.
LAMBADAis a collection of narrative passages shar-ing the characteristic that human subjectsare able to guess their last word if theyare exposed to the whole passage, but notif they only see the last sentence preced-ing the target word.
To succeed on LAM-BADA, computational models cannot sim-ply rely on local context, but must be ableto keep track of information in the broaderdiscourse.
We show that LAMBADA ex-emplifies a wide range of linguistic phe-nomena, and that none of several state-of-the-art language models reaches accuracyabove 1% on this novel benchmark.
Wethus propose LAMBADA as a challengingtest set, meant to encourage the develop-ment of new models capable of genuineunderstanding of broad context in naturallanguage text.1 IntroductionThe recent spurt of powerful end-to-end-trainedneural networks for Natural Language Processing(Hermann et al, 2015; Rockt?aschel et al, 2016;Weston et al, 2015, a.o.)
has sparked interest intasks to measure the progress they are bringingabout in genuine language understanding.
Spe-cial care must be taken in evaluating such systems,since their effectiveness at picking statistical gen-eralizations from large corpora can lead to the il-lusion that they are reaching a deeper degree ofunderstanding than they really are.
For example,?Denis and Germ?an share first authorship.
Marco,Gemma, and Raquel share senior authorship.the end-to-end system of Vinyals and Le (2015),trained on large conversational datasets, producesdialogues such as the following:(1) Human: what is your job?Machine: i?m a lawyerHuman: what do you do?Machine: i?m a doctorSeparately, the system responses are appropriatefor the respective questions.
However, when takentogether, they are incoherent.
The system be-haviour is somewhat parrot-like.
It can locallyproduce perfectly sensible language fragments,but it fails to take the meaning of the broader dis-course context into account.
Much research ef-fort has consequently focused on designing sys-tems able to keep information from the broadercontext into memory, and possibly even performsimple forms of reasoning about it (Hermann etal., 2015; Hochreiter and Schmidhuber, 1997; Jiet al, 2015; Mikolov et al, 2015; Sordoni et al,2015; Sukhbaatar et al, 2015; Wang and Cho,2015, a.o.
).In this paper, we introduce the LAMBADAdataset (LAnguage Modeling Broadened toAccount for Discourse Aspects).
LAMBADA pro-poses a word prediction task where the target itemis difficult to guess (for English speakers) whenonly the sentence in which it appears is available,but becomes easy when a broader context is pre-sented.
Consider Example (1) in Figure 1.
Thesentence Do you honestly think that I would wantyou to have a ?
has a multitude of possible con-tinuations, but the broad context clearly indicatesthat the missing word is miscarriage.LAMBADA casts language understanding inthe classic word prediction framework of languagemodeling.
We can thus use it to test several ex-isting language modeling architectures, including1525systems with capacity to hold longer-term contex-tual memories.
In our preliminary experiments,none of these models came even remotely close tohuman performance, confirming that LAMBADAis a challenging benchmark for research on auto-mated models of natural language understanding.2 Related datasetsThe CNN/Daily Mail (CNNDM) benchmark re-cently introduced by Hermann et al (2015) isclosely related to LAMBADA.
CNNDM includesa large set of online articles that are published to-gether with short summaries of their main points.The task is to guess a named entity that has beenremoved from one such summary.
Although thedata are not normed by subjects, it is unlikelythat the missing named entity can be guessed fromthe short summary alone, and thus, like in LAM-BADA, models need to look at the broader con-text (the article).
Differences between the twodatasets include text genres (news vs. novels; seeSection 3.1) and the fact that missing items in CN-NDM are limited to named entities.
Most im-portantly, the two datasets require models to per-form different kinds of inferences over broaderpassages.
For CNNDM, models must be able tosummarize the articles, in order to make sense ofthe sentence containing the missing word, whereasin LAMBADA the last sentence is not a summaryof the broader passage, but a continuation of thesame story.
Thus, in order to succeed, modelsmust instead understand what is a plausible devel-opment of a narrative fragment or a dialogue.Another related benchmark, CBT, has been in-troduced by Hill et al (2016).
Like LAMBADA,CBT is a collection of book excerpts, with oneword randomly removed from the last sentencein a sequence of 21 sentences.
While there areother design differences, the crucial distinction be-tween CBT and LAMBADA is that the CBT pas-sages were not filtered to be human-guessable inthe broader context only.
Indeed, according to thepost-hoc analysis of a sample of CBT passages re-ported by Hill and colleagues, in a large proportionof cases in which annotators could guess the miss-ing word from the broader context, they could alsoguess it from the last sentence alone.
At the sametime, in about one fifth of the cases, the annotatorscould not guess the word even when the broadercontext was given.
Thus, only a small portion ofthe CBT passages are really probing the model?sability to understand the broader context, which isinstead the focus of LAMBADA.The idea of a book excerpt completion taskwas originally introduced in the MSRCC dataset(Zweig and Burges, 2011).
However, the latterlimited context to single sentences, not attemptingto measure broader passage understanding.Of course, text understanding can be testedthrough other tasks, including entailment detec-tion (Bowman et al, 2015), answering questionsabout a text (Richardson et al, 2013; Westonet al, 2015) and measuring inter-clause coher-ence (Yin and Sch?utze, 2015).
While differenttasks can provide complementary insights into themodels?
abilities, we find word prediction par-ticularly attractive because of its naturalness (it?seasy to norm the data with non-expert humans)and simplicity.
Models just need to be trainedto predict the most likely word given the previ-ous context, following the classic language mod-eling paradigm, which is a much simpler setupthan the one required, say, to determine whethertwo sentences entail each other.
Moreover, mod-els can have access to virtually unlimited amountsof training data, as all that is required to train alanguage model is raw text.
On a more generalmethodological level, word prediction has the po-tential to probe almost any aspect of text under-standing, including but not limited to traditionalnarrower tasks such as entailment, co-referenceresolution or word sense disambiguation.3 The LAMBADA dataset3.1 Data collection1LAMBADA consists of passages composed of acontext (on average 4.6 sentences) and a targetsentence.
The context size is the minimum num-ber of complete sentences before the target sen-tence such that they cumulatively contain at least50 tokens (this size was chosen in a pilot study).The task is to guess the last word of the target sen-tence (the target word).
The constraint that thetarget word be the last word of the sentence, whilenot necessary for our research goal, makes the taskmore natural for human subjects.The LAMBADA data come from the Book Cor-pus (Zhu et al, 2015).
The fact that it con-tains unpublished novels minimizes the potential1Further technical details are provided in the Supplemen-tary Material (SM): http://clic.cimec.unitn.it/lambada/1526(1) Context: ?Yes, I thought I was going to lose the baby.?
?I was scared too,?
he stated, sincerity flooding his eyes.
?Youwere ??
?Yes, of course.
Why do you even ask??
?This baby wasn?t exactly planned for.
?Target sentence: ?Do you honestly think that I would want you to have a ?
?Target word: miscarriage(2) Context: ?Why??
?I would have thought you?d find him rather dry,?
she said.
?I don?t know about that,?
said Gabriel.
?He was a great craftsman,?
said Heather.
?That he was,?
said Flannery.Target sentence: ?And Polish, to boot,?
said .Target word: Gabriel(3) Context: Preston had been the last person to wear those chains, and I knew what I?d see and feel if they were slippedonto my skin-the Reaper?s unending hatred of me.
I?d felt enough of that emotion already in the amphitheater.
Ididn?t want to feel anymore.
?Don?t put those on me,?
I whispered.
?Please.
?Target sentence: Sergei looked at me, surprised by my low, raspy please, but he put down the .Target word: chains(4) Context: They tuned, discussed for a moment, then struck up a lively jig.
Everyone joined in, turning the courtyard intoan even more chaotic scene, people now dancing in circles, swinging and spinning in circles, everyone makingup their own dance steps.
I felt my feet tapping, my body wanting to move.Target sentence: Aside from writing, I ?ve always loved .Target word: dancing(5) Context: He shook his head, took a step back and held his hands up as he tried to smile without losing a cigarette.
?Yesyou can,?
Julia said in a reassuring voice.
?I ?ve already focused on my friend.
You just have to click the shutter,on top, here.
?Target sentence: He nodded sheepishly, through his cigarette away and took the .Target word: camera(6) Context: In my palm is a clear stone, and inside it is a small ivory statuette.
A guardian angel.
?Figured if you?re goingto be out at night getting hit by cars, you might as well have some backup.?
I look at him, feeling stunned.
Likethis is some sort of sign.Target sentence: But as I stare at Harlin, his mouth curved in a confident grin, I don?t care about .Target word: signs(7) Context: Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, andI was glad to relax against the tree?s rough, brittle bark and begin my breakfast of buttery, toasted bread and freshfruit.
Even the water was tasty, it was so clean and cold.Target sentence: It almost made up for the lack of .Target word: coffee(8) Context: My wife refused to allow me to come to Hong Kong when the plague was at its height and ??
?Your wife,Johanne?
You are married at last ??
Johanne grinned.
?Well, when a man gets to my age, he starts to need a fewhome comforts.Target sentence: After my dear mother passed away ten years ago now, I became .Target word: lonely(9) Context: ?Again, he left that up to you.
However, he was adamant in his desire that it remain a private ceremony.
Heasked me to make sure, for instance, that no information be given to the newspaper regarding his death, not evenan obituary.Target sentence: I got the sense that he didn?t want anyone, aside from the three of us, to know that he?d even .Target word: died(10) Context: The battery on Logan?s radio must have been on the way out.
So he told himself.
There was no otherexplanation beyond Cygan and the staff at the White House having been overrun.
Lizzie opened her eyes witha flutter.
They had been on the icy road for an hour without incident.Target sentence: Jack was happy to do all of the .Target word: drivingFigure 1: Examples of LAMBADA passages.
Underlined words highlight when the target word (or itslemma) occurs in the context.1527usefulness of general world knowledge and ex-ternal resources for the task, in contrast to otherkinds of texts like news data, Wikipedia text, orfamous novels.
The corpus, after duplicate re-moval and filtering out of potentially offensive ma-terial with a stop word list, contains 5,325 nov-els and 465 million words.
We randomly dividedthe novels into equally-sized training and devel-opment+testing partitions.
We built the LAM-BADA dataset from the latter, with the idea thatmodels tackling LAMBADA should be trained onraw text from the training partition, composed of2662 novels and encompassing more than 200Mwords.
Because novels are pre-assigned to one ofthe two partitions only, LAMBADA passages areself-contained and cannot be solved by exploitingthe knowledge in the remainder of the novels, forexample background information about the char-acters involved or the properties of the fictionalworld in a given novel.
The same novel-based di-vision method is used to further split LAMBADAdata between development and testing.To reduce time and cost of dataset collection,we filtered out passages that are relatively easyfor standard language models, since such casesare likely to be guessable based on local contextalone.
We used a combination of four languagemodels, chosen by availability and/or ease of train-ing: a pre-trained recurrent neural network (RNN)(Mikolov et al, 2011) and three models trainedon the Book Corpus (a standard 4-gram model, aRNN and a feed-forward model; see SM for de-tails, and note that these are different from themodels we evaluated on LAMBADA as describedin Section 4 below).
Any passage whose targetword had probability ?0.00175 according to anyof the language models was excluded.A random sample of the remaining passageswere then evaluated by human subjects throughthe CrowdFlower crowdsourcing service2in threesteps.
For a given passage,1.
one human subject guessed the target wordbased on the whole passage (comprising thecontext and the target sentence); if the guesswas right,2.
a second subject guessed the target wordbased on the whole passage; if that guess wasalso right,3.
more subjects tried to guess the target wordbased on the target sentence only, until the2http://www.crowdflower.comword was guessed or the number of unsuc-cessful guesses reached 10; if no subject wasable to guess the target word, the passage wasadded to the LAMBADA dataset.The subjects in step 3 were allowed 3 guessesper sentence, to maximize the chances of catch-ing cases where the target words were guessablefrom the sentence alone.
Step 2 was added basedon a pilot study that revealed that, while step 3was enough to ensure that the data could not beguessed with the local context only, step 1 alonedid not ensure that the data were easy given thediscourse context (its output includes a mix ofcases ranging from obvious to relatively difficult,guessed by an especially able or lucky step-1 sub-ject).
We made sure that it was not possible forthe same subject to judge the same item in bothpassage and sentence conditions (details in SM).In the crowdsourcing pipeline, 84?86% itemswere discarded at step 1, an additional 6?7% atstep 2 and another 3?5% at step 3.
Only about onein 25 input examples passed all the selection steps.Subjects were paid $0.22 per page in steps 1and 2 (with 10 passages per page) and $0.15 perpage in step 3 (with 20 sentences per page).
Over-all, each item in the resulting dataset costed $1.24on average.
Alternative designs, such as havingstep 3 before step 2 or before step 1, were foundto be more expensive.
Cost considerations alsoprecluded us from using more subjects at stage 1,which could in principle improve the quality of fil-tering at this step.Note that the criteria for passage inclusion werevery strict: We required two consecutive subjectsto exactly match the missing word, and we madesure that no subject (out of ten) was able to provideit based on local context only, even when given 3guesses.
An alternative to this perfect-match ap-proach would have been to include passages wherebroad-context subjects provided other plausible orsynonymous continuations.
However, it is verychallenging, both practically and methodologi-cally, to determine which answers other than theoriginal fit the passage well, especially when thegoal is to distinguish between items that are solv-able in broad-discourse context and those wherethe local context is enough.
Theoretically, substi-tutability in context could be tested with manualannotation by multiple additional raters, but thiswould not be financially or practically feasible fora dataset of this scale (human annotators received1528over 200,000 passages at stage 1).
For this reasonwe went for the strict hit-or-miss approach, keep-ing only items that can be unambiguously deter-mined by human subjects.3.2 Dataset statisticsThe LAMBADA dataset consists of 10,022 pas-sages, divided into 4,869 development and 5,153test passages (extracted from 1,331 and 1,332 dis-joint novels, respectively).
The average passageconsists of 4.6 sentences in the context plus 1 tar-get sentence, for a total length of 75.4 tokens (dev)/ 75 tokens (test).
Examples of passages in thedataset are given in Figure 1.The training data for language models to betested on LAMBADA include the full text of 2,662novels (disjoint from those in dev+test), compris-ing 203 million words.
Note that the trainingdata consists of text from the same domain as thedev+test passages, in large amounts but not fil-tered in the same way.
This is partially motivatedby economic considerations (recall that each datapoint costs $1.24 on average), but, more impor-tantly, it is justified by the intended use of LAM-BADA as a tool to evaluate general-purpose mod-els in terms of how they fare on broad-context un-derstanding (just like our subjects could predictthe missing words using their more general textunderstanding abilities), not as a resource to de-velop ad-hoc models only meant to predict the fi-nal word in the sort of passages encountered inLAMBADA.
The development data can be usedto fine-tune models to the specifics of the LAM-BADA passages.3.3 Dataset analysisOur analysis of the LAMBADA data suggests that,in order for the target word to be predictable in abroad context only, it must be strongly cued in thebroader discourse.
Indeed, it is typical for LAM-BADA items that the target word (or its lemma)occurs in the context.
Figure 2(a) compares theLAMBADA items to a random 5000-item sam-ple from the input data, that is, the passages thatwere presented to human subjects in the filteringphase (we sampled from all passages passing theautomated filters described in Section 3.1 above,including those that made it to LAMBADA).
Thefigure shows that when subjects guessed the word(only) in the broad context, often the word it-self occurred in the context: More than 80% ofLAMBADA passages include the target word inthe context, while in the input data that was thecase for less than 15% of the passages.
To guessthe right word, however, subjects must still puttheir linguistic and general cognitive skills to gooduse, as shown by the examples featuring the targetword in the context reported in Figure 1.Figure 2(b) shows that most target words inLAMBADA are proper nouns (48%), followed bycommon nouns (37%) and, at a distance, verbs(7.7%).
In fact, proper nouns are hugely over-represented in LAMBADA, while the other cat-egories are under-represented, compared to thePOS distribution in the input.
A variety of factorsconverges in making proper nouns easy for sub-jects in the LAMBADA task.
In particular, whenthe context clearly demands a referential expres-sion, the constraint that the blank be filled by asingle word excludes other possibilities such asnoun phrases with articles, and there are reasons tosuspect that co-reference is easier than other dis-course phenomena in our task (see below).
How-ever, although co-reference seems to play a bigrole, only 0.3% of target words are pronouns.Common nouns are still pretty frequent inLAMBADA, constituting over one third of thedata.
Qualitative analysis reveals a mixture ofphenomena.
Co-reference is again quite common(see Example (3) in Figure 1), sometimes as ?par-tial?
co-reference facilitated by bridging mecha-nisms (shutter?camera; Example (5)) or throughthe presence of a near synonym (?lose the baby?
?miscarriage; Example (1)).
However, we also of-ten find other phenomena, such as the inference ofprototypical participants in an event.
For instance,if the passage describes someone having breakfasttogether with typical food and beverages (see Ex-ample (7)), subjects can guess the target word cof-fee without it having been explicitly mentioned.In contrast, verbs, adjectives, and adverbs arerare in LAMBADA.
Many of those items can beguessed with local sentence context only, as shownin Figure 2(b), which also reports the POS dis-tribution of the set of items that were guessed bysubjects based on the target-sentence context only(step 3 in Section 3.1).
Note a higher proportionof verbs, adjectives and adverbs in the latter set inFigure 2(b).
While end-of-sentence context skewsinput distribution in favour of nouns, subject filter-ing does show a clear differential effect for nounsvs.
other POSs.
Manual inspection reveals thatbroad context is not necessary to guess items like1529LAMBADA inputTarget wordin contextnot in context0.00.20.40.60.81.0PN CN V J R OLAMBADAinputsentence0.00.10.20.30.40.5PN CN V J R O0.00.10.20.30.40.5(a) (b) (c)Figure 2: (a) Target word in or not in context; (b) Target word POS distribution in LAMBADA vs.data presented to human subjects (input) and items guessed with sentence context only (PN=propernoun, CN=common noun, V=verb, J=adjective, R=adverb, O=other); (c) Target word POS distributionof LAMBADA passages where the lemma of the target word is not in the context (categories as in (b)).frequent verbs (ask, answer, call), adjectives, andclosed-class adverbs (now, too, well), as well astime-related adverbs (quickly, recently).
In thesecases, the sentence context suffices, so few of themend up in LAMBADA (although of course thereare exceptions, such as Example (8), where the tar-get word is an adjective).
This contrasts with othertypes of open-class adverbs (e.g., innocently, con-fidently), which are generally hard to guess withboth local and broad context.
The low propor-tion of these kinds of adverbs and of verbs amongguessed items in general suggests that trackingevent-related phenomena (such as script-like se-quences of events) is harder for subjects than co-referential phenomena, at least as framed in theLAMBADA task.
Further research is needed toprobe this hypothesis.Furthermore, we observe that, while explicitmention in the preceding discourse context is criti-cal for proper nouns, the other categories can oftenbe guessed without having been explicitly intro-duced.
This is shown in Figure 2(c), which de-picts the POS distribution of LAMBADA itemsfor which the lemma of the target word is notin the context (corresponding to about 16% ofLAMBADA in total).3Qualitative analysis ofitems with verbs and adjectives as targets sug-gests that the target word, although not present inthe passage, is still strongly implied by the con-3The apparent 1% of out-of-context proper nouns shownin Figure 2(c) is due to lemmatization mistakes (fictionalcharacters for which the lemmatizer did not recognize a linkbetween singular and plural forms, e.g., Wynn ?
Wynns).
Amanual check confirmed that all proper noun target words inLAMBADA are indeed also present in the context.text.
In about one third of the cases examined,the missing word is ?almost there?.
For instance,the passage contains a word with the same rootbut a different part of speech (e.g., death?diedin Example (6)), or a synonymous expression (asmentioned above for ?miscarriage?
; we find thesame phenomenon for verbs, e.g., ?deprived youof water?
?dehydrated).In other cases, correct prediction requires morecomplex discourse inference, including guessingprototypical participants of a scene (as in the cof-fee example above), actions or events strongly sug-gested by the discourse (see Examples (1) and(10), where the mention of an icy road helpsin predicting the target driving), or qualitativeproperties of participants or situations (see Exam-ple (8)).
Of course, the same kind of discoursereasoning takes place when the target word is al-ready present in the context (cf.
Examples (3) and(4)).
The presence of the word in context does notmake the reasoning unnecessary (the task remainschallenging), but facilitates the inference.As a final observation, intriguingly, the LAM-BADA items contain (quoted) direct speech sig-nificantly more often than the input items overall(71% of LAMBADA items vs. 61% of items inthe input sample), see, e.g., Examples (1) and (2).Further analysis is needed to investigate in whatway more dialogic discourse might facilitate theprediction of the final target word.In sum, LAMBADA contains a myriad of phe-nomena that, besides making it challenging fromthe text understanding perspective, are of greatinterest to the broad Computational Linguistics1530community.
To return to Example (1), solving itrequires a combination of linguistic skills rang-ing from (morpho)phonology (the plausible targetword abortion is ruled out by the indefinite deter-miner a) through morphosyntax (the slot should befilled by a common singular noun) to pragmatics(understanding what the male participant is infer-ring from the female participant?s words), in addi-tion to general reasoning skills.
It is not surprising,thus, that LAMBADA is so challenging for currentmodels, as we show next.4 Modeling experimentsComputational methods We tested several ex-isting language models and baselines on LAM-BADA.
We implemented a simple RNN (El-man, 1990), a Long Short-Term Memory network(LSTM) (Hochreiter and Schmidhuber, 1997),a traditional statistical N-Gram language model(Stolcke, 2002) with and without cache, and aMemory Network (Sukhbaatar et al, 2015).
Weremark that at least LSTM, Memory Network and,to a certain extent, the cache N-Gram model have,among their supposed benefits, the ability to takebroader contexts into account.
Note moreover thatvariants of RNNs and LSTMs are at the state ofthe art when tested on standard language model-ing benchmarks (Mikolov, 2014).
Our MemoryNetwork implementation is similar to the one withwhich Hill et al (2016) reached the best resultson the CBT data set (see Section 2 above).
Whilewe could not re-implement the models that per-formed best on CNNDM (see again Section 2),our LSTM is architecturally similar to the DeepLSTM Reader of Hermann et al (2015), whichachieved respectable performance on that data set.Most importantly, we will show below that mostof our models reach impressive performance whentested on a more standard language modeling dataset sourced from the same corpus used to buildLAMBADA.
This control set was constructed byrandomly sampling 5K passages of the same shapeand size as the ones used to build LAMBADAfrom the same test novels, but without filteringthem in any way.
Based on the control set re-sults, to be discussed below, we can reasonablyclaim that the models we are testing on LAM-BADA are very good at standard language model-ing, and their low performance on the latter cannotbe attributed to poor quality.In order to test for strong biases in the data,we constructed Sup-CBOW, a baseline modelweakly tailored to the task at hand, consisting of asimple neural network that takes as input a bag-of-word representation of the passage and attemptsto predict the final word.
The input representa-tion comes from adding pre-trained CBOW vec-tors (Mikolov et al, 2013) of the words in the pas-sage.4We also considered an unsupervised vari-ant (Unsup-CBOW) where the target word is pre-dicted by cosine similarity between the passagevector and the target word vector.
Finally, weevaluated several variations of a random guess-ing baseline differing in terms of the word pool tosample from.
The guessed word could be pickedfrom: the full vocabulary, the words that appearin the current passage and a random uppercasedword from the passage.
The latter baseline aims atexploiting the potential bias that proper names ac-count for a consistent portion of the LAMBADAdata (see Figure 2 above).Note that LAMBADA was designed to chal-lenge language models with harder-than-averageexamples where broad context understanding iscrucial.
However, the average case should not bedisregarded either, since we want language mod-els to be able to handle both cases.
For this rea-son, we trained the models entirely on unsuper-vised data and expect future work to follow sim-ilar principles.
Concretely, we trained the mod-els, as is standard practice, on predicting each up-coming word given the previous context, using theLAMBADA training data (see Section 3.2 above)as input corpus.
The only exception to this proce-dure was Sup-CBOW where we extracted from thetraining novels similar-shaped passages to those inLAMBADA and trained the model on them (about9M passages).
Again, the goal of this model wasonly to test for potential biases in the data and notto provide a full account for the phenomena we aretesting.
We restricted the vocabulary of the mod-els to the 60K most frequent words in the trainingset (covering 95% of the target words in the de-velopment set).
The model hyperparameters weretuned on their accuracy in the development set.The same trained models were tested on the LAM-BADA and the control sets.
See SM for the tuningdetails.Results Results of models and baselines are re-ported in Table 1.
Note that the measure of interest4http://clic.cimec.unitn.it/composes/semantic-vectors.html1531Data Method Acc.
Ppl.
RankLAMBADAbaselinesRandom vocabulary word 0 60000 30026Random word from passage 1.6 - -Random capitalized word from passage 7.3 - -Unsup-CBOW 0 57040 16352Sup-CBOW 0 47587 4660modelsN-Gram 0.1 3125 993N-Gram w/cache 0.1 768 87RNN 0 14725 7831LSTM 0 5357 324Memory Network 0 16318 846ControlbaselinesRandom vocabulary word 0 60000 30453Random word from passage 0 - -Random capitalized word from passage 0 - -Unsup-CBOW 0 55190 12950Sup-CBOW 3.5 2344 259modelsN-Gram 19.1 285 17N-Gram w/cache 19.1 270 18RNN 15.4 277 24LSTM 21.9 149 12Memory Network 8.5 566 46Table 1: Results of computational methods.
Accuracy is expressed in percentage.for LAMBADA is the average success of a modelat predicting the target word, i.e., accuracy (unlikein standard language modeling, we know that themissing LAMBADA words can be precisely pre-dicted by humans, so good models should be ableto accomplish the same feat, rather than just as-signing a high probability to them).
However, aswe observe a bottoming effect with accuracy, wealso report perplexity and median rank of correctword, to better compare the models.As anticipated above, and in line with what weexpected, all our models have very good perfor-mance when called to perform a standard languagemodeling task on the control set.
Indeed, 3 ofthe models (the N-Gram models and LSTM) canguess the right word in about 1/5 of the cases.The situation drastically changes if we look atthe LAMBADA results, where all models are per-forming very badly.
Indeed, no model is evenable to compete with the simple heuristics of pick-ing a random word from the passage, and, espe-cially, a random capitalized word (easily a propernoun).
At the same time, the low performance ofthe latter heuristic in absolute terms (7% accuracy)shows that, despite the bias in favour of names inthe passage, simply relying on this will not sufficeto obtain good performance on LAMBADA, andmodels should rather pursue deeper forms of anal-ysis of the broader context (the Sup-CBOW base-line, attempting to directly exploit the passage ina shallow way, performs very poorly).
This con-firms again that the difficulty of LAMBADA reliesmainly on accounting for the information availablein a broader context and not on the task of predict-ing the exact word missing.In comparative terms (and focusing on perplex-ity and rank, given the uniformly low accuracyresults) we observe a stronger performance ofthe traditional N-Gram models over the neural-network-based ones, possibly pointing to the dif-ficulty of tuning the latter properly.
In particu-lar, the best relative performance on LAMBADAis achieved by N-Gram w/cache, which takes pas-sage statistics into account.
While even this modelis effectively unable to guess the right word, itachieves a respectable perplexity of 768.1532We recognize, of course, that the evaluation weperformed is very preliminary, and it must only betaken as a proof-of-concept study of the difficultyof LAMBADA.
Better results might be obtainedsimply by performing more extensive tuning, byadding more sophisticated mechanisms such as at-tention (Bahdanau et al, 2014), and so forth.
Still,we would be surprised if minor modifications ofthe models we tested led to human-level perfor-mance on the task.We also note that, because of the way we haveconstructed LAMBADA, standard language mod-els are bound to fail on it by design: one of ourfirst filters (see Section 3.1) was to choose pas-sages where a number of simple language modelswere failing to predict the upcoming word.
How-ever, future research should find ways around thisinherent difficulty.
After all, humans were stillable to solve this task, so a model that claims tohave good language understanding ability shouldbe able to succeed on it as well.5 ConclusionThis paper introduced the new LAMBADAdataset, aimed at testing language models on theirability to take a broad discourse context into ac-count when predicting a word.
A number oflinguistic phenomena make the target words inLAMBADA easy to guess by human subjectswhen they can look at the whole passages theycome from, but nearly impossible if only the lastsentence is considered.
Our preliminary experi-ments suggest that even some cutting-edge neuralnetwork approaches that are in principle able totrack long-distance effects are far from passing theLAMBADA challenge.We hope the computational community will bestimulated to develop novel language models thatare genuinely capturing the non-local phenomenathat LAMBADA reflects.
To promote research inthis direction, we plan to announce a public com-petition based on the LAMBADA data.5Our own hunch is that, despite the initially dis-appointing results of the ?vanilla?
Memory Net-work we tested, the ability to store information ina longer-term memory will be a crucial compo-nent of successful models, coupled with the abilityto perform some kind of reasoning about what?s5The development set of LAMBADA, along with thetraining corpus, can be downloaded at http://clic.cimec.unitn.it/lambada/.
The test set will be madeavailable at the time of the competition.stored in memory, in order to retrieve the right in-formation from it.On a more general note, we believe that lever-aging human performance on word prediction is avery promising strategy to construct benchmarksfor computational models that are supposed tocapture various aspects of human text understand-ing.
The influence of broad context as explored byLAMBADA is only one example of this idea.AcknowledgmentsWe are grateful to Aurelie Herbelot, Tal Linzen,Nghia The Pham and, especially, Roberto Zam-parelli for ideas and feedback.
This project hasreceived funding from the European Union?s Hori-zon 2020 research and innovation programme un-der the Marie Sklodowska-Curie grant agreementNo 655577 (LOVe); ERC 2011 Starting Indepen-dent Research Grant n. 283554 (COMPOSES);NWO VIDI grant n. 276-89-008 (Asymmetry inConversation).
We gratefully acknowledge thesupport of NVIDIA Corporation with the donationof the GPUs used in our research.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
In ICLR.Samuel Bowman, Gabor Angeli, Christopher Potts, andChristopher Manning.
2015.
A large annotated cor-pus for learning natural language inference.
In Pro-ceedings of EMNLP, pages 632?642, Lisbon, Portu-gal.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Karl Moritz Hermann, Tom?a?s Ko?cisk?y, EdwardGrefenstette, Lasse Espeholt, Will Kay, MustafaSuleyman, and Phil Blunsom.
2015.
Teachingmachines to read and comprehend.
In Proceed-ings of NIPS, Montreal, Canada.
Publishedonline: https://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015.Felix Hill, Antoine Bordes, Sumit Chopra, and JasonWeston.
2016.
The Goldilocks principle: Read-ing children?s books with explicit memory repre-sentations.
In Proceedings of ICLR ConferenceTrack, San Juan, Puerto Rico.
Published on-line: http://www.iclr.cc/doku.php?id=iclr2016:main.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9(8):1735?178?.1533Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,and Jacob Eisenstein.
2015.
Document contextlanguage models.
http://arxiv.org/abs/1511.03962.Tomas Mikolov, Stefan Kombrink, Anoop Deoras,Lukar Burget, and Jan Honza Cernocky.
2011.Rnnlm - recurrent neural network language.
In Pro-ceedings of ASRU.
IEEE Automatic Speech Recog-nition and Understanding Workshop.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
http://arxiv.org/abs/1301.3781/.Tomas Mikolov, Armand Joulin, Sumit Chopra,Michael Mathieu, and Marc?Aurelio Ranzato.
2015.Learning longer memory in recurrent neural net-works.
In Proceedings of ICLR Workshop Track,San Diego, CA.
Published online: http://www.iclr.cc/doku.php?id=iclr2015:main.Tomas Mikolov.
2014.
Using neural net-works for modelling and representing nat-ural languages.
Slides presented at COL-ING, online at http://www.coling-2014.org/COLING\2014\Tutorial-fix\-\TomasMikolov.pdf.Matthew Richardson, Christopher Burges, and ErinRenshaw.
2013.
MCTest: A challenge dataset forthe open-domain machine comprehension of text.In Proceedings of EMNLP, pages 193?203, Seattle,WA.Tim Rockt?aschel, Edward Grefenstette, Karl MoritzHermann, Tom?a?s Ko?cisk?y, and Phil Blunsom.
2016.Reasoning about entailment with neural attention.In Proceedings of ICLR Conference Track, San Juan,Puerto Rico.
Published online: http://www.iclr.cc/doku.php?id=iclr2016:main.Alessandro Sordoni, Michel Galley, Michael Auli,Chris Brockett, Yangfeng Ji, Margaret Mitchell,Jian-Yun Nie, Jianfeng Gao, and Bill Dolan.
2015.A neural network approach to context-sensitive gen-eration of conversational responses.
In Proceedingsof NAACL, pages 196?205, Denver, CO.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In INTERSPEECH, volume 2002,page 2002.Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,and Rob Fergus.
2015.
End-to-end memorynetworks.
http://arxiv.org/abs/1503.08895.Oriol Vinyals and Quoc Le.
2015.
A neural conver-sational model.
In Proceedings of the ICML DeepLearning Workshop, Lille, France.
Published on-line: https://sites.google.com/site/deeplearning2015/accepted-papers.Tian Wang and Kyunghyun Cho.
2015.
Larger-context language modelling.
http://arxiv.org/abs/1511.03729.Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2015.
Towards AI-complete ques-tion answering: A set of prerequisite toy tasks.http://arxiv.org/abs/1502.05698.Wenpeng Yin and Hinrich Sch?utze.
2015.
Multi-GranCNN: An architecture for general matching oftext chunks on multiple levels of granularity.
In Pro-ceedings of ACL, pages 63?73, Beijing, China.Yukun Zhu, Ryan Kiros, Richard Zemel, RuslanSalakhutdinov, Raquel Urtasun, Antonio Torralba,and Sanja Fidler.
2015.
Aligning books and movies:Towards story-like visual explanations by watchingmovies and reading books.
In ICCV 2015, pages19?27.Geoffrey Zweig and Christopher Burges.
2011.The Microsoft Research sentence completion chal-lenge.
Technical Report MSR-TR-2011-129, Mi-crosoft Research.1534
