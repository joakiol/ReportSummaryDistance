Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887?896,Honolulu, October 2008. c?2008 Association for Computational LinguisticsAutomatic Prediction of Parser AccuracySujith Ravi and Kevin KnightUniversity of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292{sravi,knight}@isi.eduRadu SoricutLanguage Weaver, Inc.4640 Admiralty Way, Suite 1210Marina del Rey, California 90292rsoricut@languageweaver.comAbstractStatistical parsers have become increasinglyaccurate, to the point where they are useful inmany natural language applications.
However,estimating parsing accuracy on a wide varietyof domains and genres is still a challenge inthe absence of gold-standard parse trees.In this paper, we propose a technique that au-tomatically takes into account certain charac-teristics of the domains of interest, and ac-curately predicts parser performance on datafrom these new domains.
As a result, we havea cheap (no annotation involved) and effectiverecipe for measuring the performance of a sta-tistical parser on any given domain.1 IntroductionStatistical natural language parsers have recentlybecome more accurate and more widely available.As a result, they are being used in a variety ofapplications, such as question answering (Herm-jakob, 2001), speech recognition (Chelba and Je-linek, 1998), language modeling (Roark, 2001), lan-guage generation (Soricut, 2006) and, most notably,machine translation (Charniak et al, 2003; Galley etal., 2004; Collins et al, 2005; Marcu et al, 2006;Huang et al, 2006; Avramidis and Koehn, 2008).These applications are employed on a wide range ofdomains and genres, and therefore the question ofhow accurate a parser is on the domain and genre ofinterest becomes acute.
Ideally, one would want tohave available a recipe for precisely answering thisquestion: ?given a parser and a particular domain ofinterest, how accurate are the parse trees produced?
?The only recipe that is implicitly given in the largeliterature on parsing to date is to have human anno-tators build parse trees for a sample set from the do-main of interest, and consequently use them to com-pute a PARSEVAL (Black et al, 1991) score that isindicative of the intrinsic performance of the parser.Given the wide range of domains and genres forwhich NLP applications are of interest, combinedwith the high expertise required from human anno-tators to produce parse tree annotations, this recipeis, albeit precise, too expensive.
The other recipethat is currently used on a large scale is to measurethe performance of a parser on existing treebanks,such as WSJ (Marcus et al, 1993), and assume thatthe accuracy measure will carry over to the domainsof interest.
This recipe, albeit cheap, cannot provideany guarantee regarding the performance of a parseron a new domain, and, as experiments in this papershow, can give wrong indications regarding impor-tant decisions for the design of NLP systems thatuse a syntactic parser as an important component.This paper proposes another method for measur-ing the performance of a parser on a given domainthat is both cheap and effective.
It is a fully auto-mated procedure (no expensive annotation involved)that uses properties of both the domain of interestand the domain on which the parser was trained inorder to measure the performance of the parser onthe domain of interest.
It is, in essence, a solution tothe following prediction problem:Input: (1) a statistical parser and its training data,(2) some chunk of text from a new domain or genreOutput: an estimate of the accuracy of the parsetrees produced for that chunk of text887Accurate estimations for this prediction problemwill allow a system designer to make the right de-cisions for the given domain of interest.
Such deci-sions include, but are not restricted to, the choice ofthe parser, the choice of the training data, the choiceof how to implement various components such as thetreatment of unknown words, etc.
Altogether, a cor-rect estimation of the impact of such decisions on theresulting parse trees can guide a system designer in ahill-climbing scenario for which an extrinsic metric(such as the impact on the overall quality of the sys-tem) is usually too expensive to be employed oftenenough.
To provide an example, a machine transla-tion engine that requires parse trees as training datain order to learn syntax-based translation rules (Gal-ley et al, 2006) needs to employ a syntactic parseras soon as the training process starts, but it can takeup to hundreds and even thousands of CPU hours(for large training data sets) to train the engine be-fore translations can be produced and measured.
Al-though a real estimate of the impact of a parser de-sign decision in this scenario can only be gaugedfrom the quality of the translations produced, it isimpractical to create such estimates for each designdecision.
On the other hand, estimates using the so-lution proposed in this paper can be obtained fast,before submitting the parser output to a costly train-ing procedure.2 Related Work and ExperimentalFrameworkThere have been previous studies which explored theproblem of automatically predicting the task diffi-culty for various NLP applications.
(Albrecht andHwa, 2007) presented a regression based methodfor developing automatic evaluation metrics for ma-chine translation systems without directly relying onhuman reference translations.
(Hoshino and Nak-agawa, 2007) built a computer-adaptive system forgenerating questions to teach English grammar andvocabulary to students, by predicting the difficultylevel of a question using various features.
Therehave been a few studies of English parser accuracyin domains/genres other than WSJ (Gildea, 2001;Bacchiani et al, 2006; McClosky et al, 2006), butin order to make measurements for such studies, itis necessary to have gold-standard parses in the non-WSJ domain of interest.Gildea (2001) studied how well WSJ-trainedparsers do on the Brown corpus, for which a goldstandard exists.
He looked at sentences with 40words or less.
(Bacchiani et al, 2006) carried outa similar experiment on sentences of all lengths,and (McClosky et al, 2006) report additional re-sults.
The table below shows results from our ownmeasurements of Charniak parser1 (Charniak andJohnson, 2005) accuracy (F-measure on sentences ofall lengths), which are consistent with these studies.For the Brown corpus, the test set was formed fromevery tenth sentence in the corpus (Gildea, 2001).Training Set Test Set Sent.countCharniakaccuracyWSJ sec.
02-21 WSJ sec.
24 1308 90.48(39,832 sent.)
WSJ sec.
23 2343 91.13Brown-test 2186 86.34Here we investigate algorithms for predicting theaccuracy of a parser P on sentences, chunks of sen-tences, and whole corpora.
We also investigate andcontrast several scenarios for prediction: (1) the pre-dictor looks at the input text only, (2) the predictorlooks at the input text and the output parse trees ofP , and (3) the predictor looks at the input text, theoutput parse trees of P , and the outputs of other pro-grams, such as the output parse trees of a differentparser Pref used as a reference.
Under none of thesescenarios is the predictor allowed to look at gold-standard parses in the new domain/genre.The intuition behind what we are trying to achievehere can be compared to an analogous task?tryingto assess the performance of a median student froma math class on a given test, without having access tothe answer sheet.
Looking at the test only, we couldprobably tell whether the test looks hard or not, andtherefore whether the student will do well or not.Looking at the student?s answers will likely give usan even better idea of the performance.
Finally, theanswers of a second student with similar proficiencywill provide even better clues: if the students agreeon every answer, then they probably both did well,but if they disagree frequently, then they (and henceour student) probably did not do as well.Our first experiments are concerned with validat-ing the idea itself: can a predictor be trained such1Downloaded from ftp.cs.brown.edu/pub/nlparser/reranking-parserAug06.tar.gz in February, 2007.888that it predicts the same F-scores as the ones ob-tained using gold-trees?
We first validate this usingthe WSJ corpus itself, by dividing the WSJ treebankinto several sections:1.
Training (WSJ section 02-21).
The parser P istrained on this data.2.
Development (WSJ section 24).
We use thisdata for training our predictor.3.
Test (WSJ section 23).
We use this data formeasuring our predictions.
For each test sentence,we compute (1) the PARSEVAL F-measure scoreusing the test gold standard, and (2) our predictedF-measure.
We report the correlation coefficient (r)between the actual F-scores and our predicted F-scores.
We will also use a root-mean-square error(rms error) metric to compare actual and predictedF-scores.Section 3 describes the features used by our pre-dictor.
Given these features, as well as actualF-scores computed for the development data, weuse supervised learning to set the feature weights.To this end, we use SVM-Regression2 (Smola andSchoelkopf, 1998) with an RBF kernel, to learn thefeature weights and build our predictor system.3 Wevalidate the accuracy of the predictor trained in thisfashion on both WSJ (Section 4) and the Brown cor-pus (Section 5).3 Features Used for Predicting ParserAccuracy3.1 Text-based FeaturesOne hypothesis we explore is that (all other thingsbeing equal) longer sentences are harder to parsecorrectly than shorter sentences.
When exposedto the development set, SVM-Regression learnsweights to best predict F-scores using the values forthis feature corresponding to each sentence in thecorpus.Does the predicted F-score correlate with actualF-score on a sentence by sentence basis?
There wasa positive but weak correlation:2Weka software (http://www.cs.waikato.ac.nz/ml/weka/)3We compared a few regression algorithms like SVM-Regression (using different kernels and parameter settings) andMulti-Layer Perceptron (neural networks) ?
we trained the al-gorithms separately on dev data and picked the one that gavethe best cross-validation accuracy (F-measure).Feature set dev (r) test (r)Length 0.13 0.19Another hypothesis is that the parser performanceis influenced by the number of UNKNOWN wordsin the sentence to be parsed, i.e., the number ofwords in the test sentence that were never seen be-fore in the training set.
Training the predictor withthis feature produces a positive correlation, slightlyweaker compared to the Length feature.Feature set dev (r) test (r)UNK 0.11 0.11Unknown words are not the only ones that can in-fluence the performance of a parser.
Rare words,for which statistical models do not have reliable es-timates, are also likely to impact parsing accuracy.To test this hypothesis, we add a language modelperplexity?based (LM-PPL) feature.
We extract theyield of the training trees, on which we train a tri-gram language model.4 We compute the perplexityof each test sentence with respect to this languagemodel, and use it as feature in our predictor system.Note that this feature is meant as a refinement of theprevious UNK feature, in the sense that perplexitynumbers are meant to signal the occurrence of un-known words, as well as rare (from the training dataperspective) words.
However, the correlation we ob-serve for this feature is similar to the correlation ob-served for the UNK feature, which seems to suggestthat the smoothing techniques used by the parsersemployed in these experiments lead to correct treat-ment of the rare words.Feature set dev (r) test (r)LM-PPL 0.11 0.10We also look at the possibility of automaticallydetecting certain ?cue?
words that are appropriatefor our prediction problem.
That is, we want to seeif we can detect certain words that have a discrimi-nating power in deciding whether parsing a sentencethat contains them is difficult or easy.
To this end,we use a subset of the development data, which con-tains the 200 best-parsed and 200 worst-parsed sen-tences (based on F-measure scores).
For each wordin the development dataset, we compute the infor-mation gain (IG) (Yang and Pedersen, 1997) scorefor that word with respect to the best/worst parsed4We trained using the SRILM language modeling toolkit,with default settings.889dataset.
These words are then ranked by their IGscores, and the top 100 words are included as lex-ical features in our predictor system.
As expected,the correlation on the development set is quite high(given that these lexical cues are extracted from thisparticular set), but a positive correlation holds forthe test set as well.Feature set dev (r) test (r)lexCount100 0.43 0.183.2 Parser P?based FeaturesBesides exploiting the information present in the in-put text, we can also inspect the output tree of theparser P for which we are interested in predictingaccuracy.
We create a rootSYN feature based onthe syntactic category found at the root of the out-put tree (?is it S?
?, ?is it FRAG??).
We also createa puncSYN feature based on the number of wordslabeled as punctuation tags (based on the intuitionthat heavy use of punctuation can be indicative ofthe difficulty of the input sentences), and a label-SYN feature in which we bundled together informa-tion regarding the number of internal nodes in theparse tree output that have particular labels (?howmany nodes are labeled with PP??).
In our predictor,we use 72 such labelSYN features corresponding toall the syntactic labels found in the parse tree out-put for the development set.
The test set correlationgiven by the rootSYN and the labelSYN features arehigher than some of the text-based features, whereasthe puncSYN feature seems to have little discrimi-native power.Feature set dev (r) test (r)rootSYN 0.21 0.17puncSYN 0.09 0.01labelSYN 0.33 0.283.3 Reference Parser Pref ?based FeaturesIn addition to the text-based features and parser P?based features, we can bring in an additional parserPref whose output is used as a reference againstwhich the output of parser P is measured.
For thereference parser feature, our goal is to measure howsimilar/different are the results from the two parsers.We find that if the parses are similar, they are morelikely to be right.
In order to compute similarity, wecan compare the constituents in the two parse treesfrom P and Pref , and see how many constituentsmatch.
This is most easily accomplished by consid-ering Pref to be a ?gold standard?
(even though it isnot necessarily a correct parse) and computing theF-measure score of parser P against Pref .
We usethis F-measure score as a feature for prediction.For the experiments presented in this section weuse as Pref , the parser from (Bikel, 2002).
Intu-itively, the requirement for choosing parser Pref inconjunction with parser P seems to be that theyare different enough to produce non-identical treeswhen presented with the same input, and at thesame time to be accurate enough to produce reli-able parse trees.
The choice of P as (Charniak andJohnson, 2005) and Pref as (Bikel, 2002) fits thisbill, but many other choices can be made regardingPref , such as (Klein and Manning, 2003; Petrov andKlein, 2007; McClosky et al, 2006; Huang, 2008).We leave the task of creating features based on theconsensus of multiple parsers as future work.The correlation given by the reference parser?based feature Pref on the test set is the highestamong all the features we explored.Feature set dev (r) test (r)Pref 0.40 0.363.4 The Aggregated Power of FeaturesThe table below lists all the individual features wehave described in this section, sorted according tothe correlation value obtained on the test set.Feature set dev (r) test (r)Pref 0.40 0.36labelSYN 0.33 0.28lexCount500 0.56 0.23lexBool500 0.58 0.20lexCount1000 0.67 0.20lexBool1000 0.58 0.20Length 0.13 0.19lexCount100 0.43 0.18lexBool100 0.43 0.18rootSYN 0.21 0.17UNK 0.11 0.11LM-PPL 0.11 0.10puncSYN 0.09 0.01Note how the lexical features tend to over-fit thedevelopment data?the words were specifically cho-sen for their discriminating power on that particularset.
Hence, adding more lexical features to the pre-dictor system improves the correlation on develop-ment (due to over-fitting), but it does not produceconsistent improvement on the test set.
However,890Method (using 3 features:Length, UNK, Pref )# of randomrestartsdev (r)SVM Regression 0.421 0.1385 0.136Maximum Correlation 10 0.166Training (MCT) 25 0.178100 0.2321000 0.2710,000 0.401Table 1: Comparison of correlation (r) obtained using MCT versusSVM-Regression on development corpus.there is some indication that the counts of the lex-ical features are important, and count-based lexicalfeatures tend to have similar or better performancecompared to their boolean-based counterparts.Since these features measure different but over-lapping pieces of the information available, it is tobe expected that some of the feature combinationswould provide better correlation that the individualfeatures, but the gains are not strictly additive.
Bytaking the individual features that provide the bestdiscriminative power, we are able to get a correla-tion score of 0.42 on the test set.Feature set dev (r) test (r)Pref + labelSYN + Length + lexCount100 +rootSYN + UNK + LM-PPL0.55 0.423.5 Optimizing for Maximum CorrelationIf our goal is to obtain the highest correlationswith the F-score measure, is SVM regression thebest method?
Liu and Gildea (2007) recently in-troduced Maximum Correlation Training (MCT), asearch procedure that follows the gradient of the for-mula for correlation coefficient (r).
We implementedMCT, but obtained no better results.
Moreover, itrequired many random re-starts just to obtain resultscomparable to SVM regression (Table 1).4 Predicting Accuracy on MultipleSentencesThe results for the scenario presented in Section 3are encouraging, but other scenarios are also im-portant from a practical perspective.
For instance,we are interested in predicting the performance of aparticular parser not on a sentence-by-sentence ba-sis, but for a representative chunk of sentences fromthe new domain.
In order to predict the F-measureon multiple sentences, we modify our feature set togenerate information on a whole chunk of sentencesSentences inchunk (n)WSJ-test (r) WSJ-test(rms error)1 0.42 0.09820 0.61 0.02650 0.62 0.019100 0.69 0.015500 0.79 0.011Table 2: Performance of predictor on n-sentence chunks from WSJ-test(Correlation and rms error between actual/predicted accuracies).rather than a single sentence.
Predicting the corre-lation at chunk level is, not unexpectedly, an eas-ier problem than predicting correlation at sentencelevel, as the results in the first two columns of Ta-ble 2 show.For 100-sentence chunks, we also plot the pre-dicted accuracies versus actual accuracies for theWSJ-test set in Figure 1.
This scatterplot brings tolight an artifact of using correlation metric (r) forevaluating our predictor?s performance.
Althoughour objective is to improve correlation between ac-tual and predicted F-scores, the correlation metric (r)does not tell us directly how well the predictor isdoing.
In Figure 1, the system predicts that onan average, most sentence chunks can be parsedwith an accuracy of 0.9085 (which is the mean pre-dicted F-score on WSJ-test).
But the range of pre-dictions from our system [0.89,0.92] is smaller thanthe actual F-score range [0.86,0.95].
Hence, eventhough the correlation scores are high, this does notnecessarily mean that our predictions are on target.An additional metric, root-mean-square (rms) error,which measures the distance between actual and pre-dicted F-measures, can be used to gauge the qual-ity of our predictions.
For a particular chunk-size,lowering the rms error translates into aligning thepoints of a scatterplot as the one in Figure 1, closerto the x=y line, implying that the predictor is gettingbetter at exactly predicting the F-score values.
Thethird column in Table 2 shows the rms error for ourpredictor at different chunk sizes.
The results usingthis metric also show that the prediction problem be-comes easier as the chunk size increases.Assuming that we have the test set of WSJ sec-tion 23, but without the gold-standard trees, howcan we get an approximation for the overall accu-racy of a parser P on this test set?
One possibility,which we use here as a baseline, is to compute theF-score on a set for which we do have gold-standardtrees.
If we use our development set (WSJ section8910.850.860.870.880.890.90.910.920.930.940.950.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95ActualAccuracyPredicted Accuracyper-chunk-accuracyx=y lineFitted-lineFigure 1: Plot showing Actual vs.
Predicted accuracies forWSJ-test (100-sentence chunks).
Each plot point represents a100-sentence chunk.
(rms error = 0.015)0.850.860.870.880.890.90.910.920.930.940.950.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95ActualAccuracyPredicted Accuracyper-chunk-accuracyx=y lineFigure 2: Plot showing Actual vs.
Adjusted Predicted accu-racies (shifting with ?
= 0.757, skewing with ?
= 1.0) forWSJ-test (100-sentence chunks).
(rms error = 0.014)System F-measureCharniak F-measure on WSJ-dev (baseline) 90.48 (fd)Predictor (feature weights set with WSJ-dev) 90.85 (fp)Actual Charniak accuracy 91.13 (ft)Table 3: Comparing Charniak parser accuracy (from different systems)on entire WSJ-test corpus24) for this purpose, and (Charniak and Johnson,2005) as the parser P , the baseline is an F-score of90.48 (fd), which is the actual Charniak parser accu-racy on WSJ section 24.
Instead, if we run our pre-dictor on the test set (a single chunk containing allthe sentences in the test set), it predicts an F-scoreof 90.85 (fp).
These two predictions are listed asthe first two rows in Table 3.
Of course, having theactual gold-standard trees for WSJ section 23 helpsus decide which prediction is better: the actual ac-curacy of the Charniak parser on WSJ section 23 isan F-score of 91.13 (ft), which makes our predictionbetter than the baseline.4.1 Shifting Predictions to Match ActualAccuracyWe correctly predict (in Table 3) that theWSJ-test is easier to parse than the WSJ-dev (90.85 > 90.48).
However, our predictor is tooconservative?the WSJ-test is actually even easierto parse (91.13 > 90.85).
We can fix this by shift-ing the mean predicted F-score (which is equal tofp) further away from the dev F-measure (fd), andcloser to the actual F-measure (ft).
This is achievedby shifting all the individual predictions by a certainamount as shown below.Let p be an individual prediction from our system.The shifted prediction p?
is given by:p?
= p+ ?
(fp ?
fd) (1)We can tune ?
to make the new mean predic-tion (f ?p) to be equal to the actual F-measure (ft).f ?p = fp + ?
(fp ?
fd) (2)?
=ft ?
fpfp ?
fd(3)Using the F-score values from Table 3, we get an?
= 0.757 and an exact prediction of 91.13.
Ofcourse, this is because we tune on test, so we needto validate this idea on a new test set to see if it leadsto improved predictions (Section 5).4.2 Skewing to Widen Prediction RangeOur predictor is also too conservative about its dis-tribution (see Figure 1).
It knows (roughly) whichchunks are easier to parse and which are harder, butits range of predictions is lower than the range ofactual F-measure scores.We can skew individual predictions so that sen-tences predicted to be easy are re-predicted to beeven easier (and those that are hard to be evenharder).
For each prediction p?
(from Equation 1),we computep??
= p?
+ ?(p?
?
f ?p) (4)We simply set ?
to 1.0, doubling the distanceof each prediction p?
(in Equation 1) from the (ad-justed) mean prediction f ?p, to obtain the skewed pre-diction p?
?.Figure 2 shows how the points representing 100-sentence chunks in Figure 1 look after the predic-tions have been shifted (?
= 0.757) and skewed(?
= 1.0).
These two operations have the desiredeffect of changing the range of predictions from[0.89,0.92] to [0.87,0.94], much closer to the actual892Sentencesin chunk(n)WSJ-test(rms error)Brown-testPrediction(rms error)Brown-testAdjustedPrediction(rms error)1 0.098 0.129 0.13920 0.026 0.039 0.03650 0.019 0.032 0.029100 0.015 0.025 0.020500 0.011 0.038 0.024Table 4: Performance of predictor on n-sentence chunks from WSJ-testand Brown-test (rms error between actual/predicted accuracies).range of [0.86,0.95].
The points in the new plot (Fig-ure 2) also align closer to the ?x=y?
line than in theoriginal graph (Figure 1).
The rms error also dropsfrom 0.015 to 0.014 (7% relative reduction), show-ing that the predictions have improved.Since we use the WSJ-test corpus to tune the pa-rameter values for shifting and skewing, we need toapply our predictor on a different test set to see if weget similar improvements by using these techniques,which we do in the next section.5 Predicting Accuracy on the BrownCorpusThe Brown corpus represents a genuine challengefor our predictor, as it presents us with the oppor-tunity to test the performance of our predictor inan out-of-domain scenario.
Our predictor, trainedon WSJ data, is now employed to predict the per-formance of a WSJ-trained parser P on the Brown-test corpus.
As in the previous experiments, we use(Charniak and Johnson, 2005) trained on WSJ sec-tions 02-21 as parser P .
The feature weights for ourpredictor are again trained on section 24 of WSJ, andthe shifting and skewing parameters (?
= 0.757,?
= 1.0) are determined using section 23 of WSJ.The results on the Brown-test, both the origi-nal predictions and after they have been adjusted(shifted/skewed), are shown in Table 4, at differentlevel of chunking.
For chunks of size n > 1, theshifting and skewing techniques help in lowering therms error.
On 100-sentence chunks from the Browntest, shifting and skewing (?
= 0.757, ?
= 1.0)leads to a 20% relative reduction in the rms error.In a similar vein with the evaluation done in Sec-tion 4, we are interested in estimating the overall ac-curacy of a WSJ-trained parser P given an out-of-domain set such as the Brown test set (for which, atleast for now, we do not have access to gold-standardSystem F-measureBaseline1 (F-measure on WSJ sec.
23) 91.13Baseline2 (F-measure on WSJ sec.
24) 90.48Predictor (base) 88.48Adjusted Predictor (shifting using ?
= 0.757) 86.96Actual accuracy 86.34Table 5: Charniak parser accuracy on entire Brown-test corpustrees).
If we use (Charniak and Johnson, 2005) asparser P , a cheap and readily-available answer isto approximate the performance using the Charniakparser performance on WSJ section 23, which hasan F-score of 91.13.
Another cheap and readily-available answer is to take the Charniak parser per-formance on WSJ section 24 with an F-score of90.48.
Table 5 lists these baselines, along with theprediction made by our system when using a singlechunk containing all the sentences in the Brown testset (both base predictions and adjusted predictions,i.e.
shifting using ?
= 0.757).
Again, having gold-standard trees for the Brown test set helps us decidewhich prediction is better.
Our predictions are muchcloser to the actual Charniak parser performance onthe Brown-test set, with the adjusted prediction at86.96 compared to the actual F-score of 86.34.6 Ranking Parser PerformanceOne of the main goals for computing F-score figures(either by traditional PARSEVAL evaluation againstgold standards or by methods such as the one pro-posed in this paper) is to compare parsing accu-racy when confronted with a choice between vari-ous parser deployments.
Not only are there manyparsing techniques available (Collins, 2003; Char-niak and Johnson, 2005; Petrov and Klein, 2007;McClosky et al, 2006; Huang, 2008), but recentannotation efforts in providing training material forstatistical parsing (LDC, 2005; LDC, 2006a; LDC,2006b; LDC, 2006c; LDC, 2007) have compoundedthe difficulty of the choices (?Do I parse using parserX?
?, ?Do I train parser X using the treebank Y orZ??).
In this section, we show how our predictor canprovide guidance when dealing with some of thesechoices, namely the choice of the training materialto use with a statistical parser, prior to its applica-tion in an NLP task.For the experiments reported in this paper, weuse as parser P , our in-house implementation ofthe Collins parser (Collins, 2003), to which various893speed-related enhancements (Goodman, 1997) havebeen applied.
This choice has been made to betterreflect a scenario in which parser P would be usedin a data-intensive application such as syntax-drivenmachine translation, in which the parser must beable to run through hundreds of millions of trainingwords in a timely manner.
We use the more accurate,but slower Charniak parser (Charniak and Johnson,2005) as the reference parser Pref in our predictor(see Section 3.3).
In order to predict the Collins-style parser behavior on the ranking task, we use thesame predictor model (including feature weights andadjustment parameters) that was used for predictingCharniak parser behavior on the Brown corpus (Sec-tion 5).We compare three training scenarios that make forthree different parsers:(1) PWSJ - trained on sections 02-21 of WSJ.
(2) PNews - trained on the union of the EnglishChinese Translation Treebank (LDC, 2007) (newsstories from Xinhua News Agency translated fromChinese into English) and the English NewswireTranslation Treebank (LDC, 2005; LDC, 2006a;LDC, 2006b; LDC, 2006c) (An-Nahar new storiestranslated from Arabic into English).
(3) PWSJ?News - trained on the union of all theabove training material.When comparing the performance of these threeparsers on a development set from WSJ (section 0),we get the following F-scores.5Parser WSJ (sec.
0) Accuracy(F-scores)PWSJ 88.25PNews 83.00PWSJ?News 88.00Consider now that we are interested in compar-ing the parsing accuracy of these parsers on a do-main completely different from WSJ.
The rankingPWSJ>PWSJ?News>PNews, given by the evalua-tion above, provides some guidance, but is this guid-ance accurate?
The intuition here is that the in-formation that we already have about the new do-main of interest (which implicitly appears in texts5Because of tokenization differences between the differenttreebanks involved in these experiments, we have to adopt a to-kenization scheme different from the one used in the Penn Tree-bank, and therefore the F-scores, albeit in the same range, arenot directly comparable with the ones in the parsing literature.Parser Xinhua NewsPrediction(F-scores)Xinhua NewsAccuracy(F-scores)PWSJ 85.1 79.14PNews 87.0 84.84PWSJ?News 89.4 85.14Table 6: Performance of predictor on the Xinhua News domain, com-pared with actual F-scores.extracted from this domain), can be used to bet-ter guide this decision.
Our predictor is able tocapitalize on this information, and provide domain-informed guidance for choosing the most accurateparser to use with the new data, which in this caserelates to choosing the best training strategy for theparser P .
If we consider as our domain of interest,news stories from Xinhua News Agency, then usingour predictor on a chunk of 1866 sentences from thisdomain gives the F-scores shown in the second col-umn of Table 6.As with the previous experiments, we can com-pute the actual PARSEVAL F-scores (using gold-standard) for this particular 1866-sentence test set,as it happens to be part of the English Chinese Trans-lation Treebank (LDC, 2007).
These F-score fig-ures are shown in the third column of Table 6.
Asthese results show, for this particular domain the cor-rect ranking is PWSJ?News>PNews>PWSJ , whichis exactly the ranking predicted by our method, with-out the aid of gold-standard trees.We observe that even though the system predictsthe ranking correctly, the predictions in the XinhuaNews domain might not be as accurate in compar-ison to the predictions on Brown corpus (predictedF-score = 86.96, actual F-score = 86.34).
One pos-sible reason for this lower accuracy is that we usethe same prediction model without optimizing forthe particular parser on which we wish to make pre-dictions.
Still, the model was able to make distinc-tions between multiple parsers for the ranking taskcorrectly, and decide the best parser to use with thegiven data.
We believe this to be useful in typicalNLP applications which use parsing as a component,and where making the right choice between differ-ent parsers can affect the end-to-end accuracy of thesystem.7 ConclusionThe steady advances in statistical parsing over thelast years have taken this technology to the point894where it is accurate enough to be useful in a va-riety of natural language applications.
However,due to large variations in the characteristics of thedomains for which these applications are devel-oped, estimating parsing accuracy becomes moreinvolved than simply taking for granted accuracyestimates done on a certain well-studied domain,such as WSJ.
As the results in this paper show, itis possible to take into account these variations inthe domain characteristics (encoded in our predictoras text-based, syntax-based, and agreement-basedfeatures)?to make better predictions about the ac-curacy of certain statistical parsers (and under dif-ferent training scenarios), instead of relying on accu-racy estimates done on a standard domain.
We haveprovided a mechanism to incorporate these domainvariations for making predictions about parsing ac-curacy, without the costly requirement of creatinghuman annotations for each of the domains of inter-est.
The experiments shown in the paper were lim-ited to readily available statistical parsers (which arewidely deployed in a number of applications), andcertain domains/genres (because of ready access togold-standard data on which we could verify predic-tions).
However, the features we use in our predic-tor are independent of the particular type of parseror domain, and the same technique could be appliedfor making predictions on other parsers as well.There are many avenues for future work openedup by the work presented here.
The accuracy of thepredictor can be further improved by incorporatingmore complex syntax-based features and multiple-agreement features.
Moreover, rather than predict-ing an intrinsic metric such as the PARSEVAL F-score, the metric that the predictor learns to pre-dict can be chosen to better fit the final metric onwhich an end-to-end system is measured, in the styleof (Och, 2003).
The end-result is a finely-tuned toolfor predicting the impact of various parser design de-cisions on the overall quality of a system.8 AcknowledgementsWe wish to acknowledge our colleagues at ISI, whoprovided useful suggestions and constructive criti-cism on this work.
We are also grateful to all thereviewers for their detailed comments.
This workwas supported in part by NSF grant IIS-0428020.ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regression forsentence-level mt evaluation with pseudo references.In Proc.
of ACL.Eleftherios Avramidis and Philipp Koehn.
2008.
Enrich-ing morphologically poor languages for statistical ma-chine translation.
In Proc.
of ACL.Michiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochasticgrammars.
Computer Speech & Language, 20(1).Daniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
In Proc.of HLT.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitatively comparing the syntactic cover-age of english grammars.
In Proc.
of Speech and Nat-ural Language Workshop.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proc.
of ACL.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for statisticalmachine translation.
In Proc.
of MT Summit IX.
IAMT.Ciprian Chelba and Frederick Jelinek.
1998.
Exploitingsyntactic structure for language modeling.
In Proc.
ofACL.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4).Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.of HLT/NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inferences and training ofcontext-rich syntax translation models.
In Proc.
ofACL.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proc.
of EMNLP.Joshua Goodman.
1997.
Global thresholding andmultiple-pass parsing.
In Proc.
of EMNLP.Ulf Hermjakob.
2001.
Parsing and question classifica-tion for question answering.
In Proc.
of ACL Work-shop on Open-Domain Question Answering.Ayako Hoshino and Hiroshi Nakagawa.
2007.
A clozetest authoring system and its automation.
In Proc.
ofICWL.895Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
of AMTA.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proc.
of ACL.LDC.
2005.
English newswire translation tree-bank.
Linguistic Data Consortium, Catalog numberLDC2005E85.LDC.
2006a.
English newswire translation tree-bank.
Linguistic Data Consortium, Catalog numberLDC2006E36.LDC.
2006b.
GALE Y1 Q3 release - English translationtreebank.
Linguistic Data Consortium, Catalog num-ber LDC2006E82.LDC.
2006c.
GALE Y1 Q4 release - English translationtreebank.
Linguistic Data Consortium, Catalog num-ber LDC2006E95.LDC.
2007.
English chinese translation tree-bank.
Linguistic Data Consortium, Catalog numberLDC2007T02.Ding Liu and Daniel Gildea.
2007.
Source-language fea-tures and maximum correlation training for machinetranslation evaluation.
In Proc.
of NAACL-HLT.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
Spmt: Statistical machine trans-lation with syntactified target language phraases.
InProc.
of EMNLP.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2).David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In Proc.
of COLING-ACL.Franz Joseph Och.
2003.
Minimum error rate training inmachine translation.
In Proc.
of ACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proc.
of HLT/NAACL.Brian Roark.
2001.
Probabilistic top-down parsingand language modelling.
Computational Linguistics,27(2).A.J.
Smola and B. Schoelkopf.
1998.
A tutorial on sup-port vector regression.
NeuroCOLT2 Technical ReportNC2-TR-1998-030.Radu Soricut.
2006.
Natural Language Generation us-ing an Information-Slim Representation.
Ph.D. thesis,University of Southern California,.Y.
Yang and J. Pedersen.
1997.
A comparative studyon feature selection in text categorization.
In Proc.
ofICML.896
