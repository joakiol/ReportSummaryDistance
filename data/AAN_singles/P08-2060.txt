Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 237?240,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticssplitSVM: Fast, Space-Efficient, non-Heuristic, Polynomial KernelComputation for NLP ApplicationsYoav Goldberg and Michael ElhadadBen Gurion University of the NegevDepartment of Computer SciencePOB 653 Be?er Sheva, 84105, Israel{yoavg,elhadad}@cs.bgu.ac.ilAbstractWe present a fast, space efficient and non-heuristic method for calculating the decisionfunction of polynomial kernel classifiers forNLP applications.
We apply the method tothe MaltParser system, resulting in a Javaparser that parses over 50 sentences per sec-ond on modest hardware without loss of accu-racy (a 30 time speedup over existing meth-ods).
The method implementation is availableas the open-source splitSVM Java library.1 IntroductionOver the last decade, many natural language pro-cessing tasks are being cast as classification prob-lems.
These are then solved by of-the-shelfmachine-learning algorithms, resulting in state-of-the-art results.
Support Vector Machines (SVMs)have gained popularity as they constantly outper-form other learning algorithms for many NLP tasks.Unfortunately, once a model is trained, the de-cision function for kernel-based classifiers such asSVM is expensive to compute, and can grow lin-early with the size of the training data.
In contrast,the computational complexity for the decisions func-tions of most non-kernel based classifiers does notdepend on the size of the training data, making themorders of magnitude faster to compute.
For this rea-son, research effort was directed at speeding up theclassification process of polynomial-kernel SVMs(Isozaki and Kazawa, 2002; Kudo and Matsumoto,2003; Wu et al, 2007).
Existing accelerated SVMsolutions, however, either require large amounts ofmemory, or resort to heuristics ?
computing only anapproximation to the real decision function.This work aims at speeding up the decision func-tion computation for low-degree polynomial ker-nel classifiers while using only a modest amount ofmemory and still computing the exact function.
Thisis achieved by taking into account the Zipfian natureof natural language data, and structuring the compu-tation accordingly.
On a sample application (replac-ing the libsvm classifier used by MaltParser (Nivreet al, 2006) with our own), we observe a speedupfactor of 30 in parsing time.2 Background and Previous WorkIn classification based NLP algorithms, a word andits context is considered a learning sample, and en-coded as Feature Vectors.
Usually, context data in-cludes the word being classified (w0), its part-of-speech (PoS) tag (p0), word forms and PoS tags ofneighbouring words (w?2, .
.
.
, w+2, p?2, .
.
.
, p+2,etc.).
Computed features such as the length of aword or its suffix may also be added.
A feature vec-tor (F ) is encoded as an indexed list of all the fea-tures present in the training corpus.
A feature fi ofthe form w+1 = dog means that the word follow-ing the one being classified is ?dog?.
Every learningsample is represented by an n = |F | dimensionalbinary vector x. xi = 1 iff the feature fi is activein the given sample, 0 otherwise.
n is the numberof different features being considered.
This encod-ing leads to vectors with extremely high dimensions,mainly because of lexical features wi.SVM is a supervised binary classifier.
The re-sult of the learning process is the set SV of Sup-237port Vectors, associated weights ?i, and a constantb.
The Support Vectors are a subset of the trainingfeature vectors, and together with the weights and bthey define a hyperplane that optimally separates thetraining samples.
The basic SVM formulation is of alinear classifier, but by introducing a kernel functionK that non-linearly transforms the data fromRn intoa space of higher dimension, SVM can be used toperform non-linear classification.
SVM?s decisionfunction is:y(x) = sgn(?j?SV yj?jK(xj , x) + b)where x is an n dimensional feature vector tobe classified.
The kernel function we considerin this paper is a polynomial kernel of degree d:K(xi, xj) = (?xi ?
xj + c)d. When using binaryvalued features (with ?
= 1 and c = 1), this kernelfunction essentially implies that the classifier con-siders not only the explicitly specified features, butalso all available sets of size d of features.
Ford = 2, this means considering all feature pairs,while for d = 3 all feature triplets.
In practice, apolynomial kernel with d = 2 usually yields thebest results in NLP tasks, while higher degree ker-nels tend to overfit the data.2.1 Decision Function ComputationNote that the decision function involves a summa-tion over all support vectors xj in SV .
In natu-ral language applications, the size |SV | tends to bevery large (Isozaki and Kazawa, 2002), often above10,000.
In particular, the size of the support vectorsset can grow linearly with the number of training ex-amples, of which there are usually at least tens ofthousands.
As a consequence, the computation ofthe decision function is computationally expensive.Several approaches have been designed to speed upthe decision function computation.Classifier Splitting is a common, applicationspecific heuristic, which is used to speed up thetraining as well as the testing stages (Nivre et al,2006).
The training data is split into several datasetsaccording to an application specific heuristic.
A sep-arate classifier is then trained for each dataset.
Forexample, it might be known in advance that nounsusually behave differently than verbs.
In such acase, one can train one classifier on noun instances,and a different classifier on verb instances.
Whentesting, only one of the classifiers will be applied,depending on the PoS of the word.
This techniquereduces the number of support vectors in each clas-sifier (because each classifier was trained on only aportion of the data).
However, it relies on human in-tuition on the way the data should be split, and usu-ally results in a degradation in performance relativeto a single classifier trained on all the data points.PKI ?
Inverted Indexing (Kudo and Matsumoto,2003), stores for each feature the support vectors inwhich it appears.
When classifying a new sample,only the set of vectors relevant to features actuallyappearing in the sample are considered.
This ap-proach is non-heuristic and intuitively appealing, butin practice brings only modest improvements.Kernel Expansion (Isozaki and Kazawa, 2002)is used to transform the d-degree polynomial kernelbased classifier into a linear one, with a modifieddecision function y(x) = sgn(w ?
xd + b).
w is avery high dimensional weight vector, which is cal-culated beforehand from the set of support vectorsand their corresponding ?i values.
(the calculationdetails appear in (Isozaki and Kazawa, 2002; Kudoand Matsumoto, 2003)).
This speeds up the decisioncomputation time considerably, as only |x|d weightsneed to be considered, |x| being the number of ac-tive features in the sample to be classified, whichis usually a very small number.
However, even thesparse-representation version of w tends to be verylarge: (Isozaki and Kazawa, 2002) report that someof their second degree expanded NER models weremore than 80 times slower to load than the originalmodels (and 224 times faster to classify).1 This ap-proach obviously does not scale well, both to taskswith more features and to larger degree kernels.PKE ?
Heuristic Kernel Expansion, was intro-duced by (Kudo and Matsumoto, 2003).
This heuris-tic method addresses the deficiency of the KernelExpansion method by using a basket-mining algo-rithm in order to greatly reduce the number of non-zero elements in the calculated w. A parameter isused to control the number of non-zero elements inw.
The smaller the number, the smaller the memoryrequirement, but setting this number too low hurtsclassification performance, as only an approxima-1Using a combination of 33 classifiers, the overall loadingtime is about 31 times slower, and classification time is about21 times faster, than the non-expanded classifiers.238tion of the real decision function is calculated.
?Semi Polynomial Kernel?
was introduced by(Wu et al, 2007).
The intuition behind this opti-mization is to ?extend the linear kernel SVM towardpolynomial?.
It does not train a polynomial kernelclassifier, but a regular linear SVM.
A basket-miningbased feature selection algorithm is used to select?useful?
pairs and triplets of features prior to thetraining stage, and a linear classifier is then trainedusing these features.
Training (and testing) are fasterthen in the polynomial kernel case, but the result suf-fer quite a big loss in accuracy as well.2.3 Fast, Non-Heuristic ComputationWe now turn to present our fast, space efficient andnon-heuristic approach for computing the Polyno-mial Kernel decision function.3 Our approach is acombination of the PKI and the Kernel Expansionmethods.
While previous works considered kernelsof the form K(x, y) = (x ?
y + 1)d, we considerthe more general form of the polynomial kernel:K(x, y) = (?x ?
y + c)d.Our key observation is that in NLP classifica-tion tasks, few of the features (e.g., PoS is X,or prev word is the) are very frequent, whilemost others are extremely rare (e.g., next wordis polynomial).
The common features are ac-tive in many of the support-vectors, while the rarefeatures are active only in few support vectors.
Thisis true for most language related tasks: the Zipfiannature of language phenomena is reflected in the dis-tribution of features in the support vectors.It is because of common features that the PKI re-verse indexing method does not yield great improve-ments: if at least one of the features of the currentinstance is active in a support vector, this vector istaken into account in the sum calculation, and thecommon features are active in many support vectors.On the other hand, the long tail of rare featuresis the reason the Kernel Expansion methods requires2This loss of accuracy in comparison to the PKE approachis to be expected, as (Goldberg and Elhadad, 2007) showed thatthe effect of removing features prior to the learning stage ismuch more severe than removing them after the learning stage.3Our presentation is for the case where d = 2, as this is byfar the most useful kernel.
However, the method can be easilyadapted to higher degree kernels as well.
For completeness, ourtoolkit provides code for d = 3 as well as 2.so much space: every rare feature adds many possi-ble feature pairs.We propose a combined method.
We first splitcommon from rare features.
We then use KernelExpansion on the few common features, and PKIfor the remaining rare features.
This ensures smallmemory footprint for the expanded kernel vector,while at the same time keeping a low number of vec-tors from the reverse index.3.1 Formal DetailsThe polynomial kernel of degree 2 is: K(x, y) =(?x ?
y + c)2, where x and y are binary feature vec-tors.
x ?y is the dot product between the vectors, andin the case of binary feature vectors it correspondsto the count of shared features among the vectors.
Fis the set of all possible features.We define FR and FC to be the sets of rare andcommon features.
FR?FC = ?, FR?FC = F .
Themapping function ?R(x) zeros out all the elementsof x not belonging to FR, while ?C(x) zeroes outall the elements of x not in FC .
Thus, for every x:?R(x)+?C(x) = x, ?R(x)?
?C(x) = 0.
For brevity,denote ?C(x) = xC , ?R(x) = xR.We now rewrite the kernel function:K(x, y) = K(xR + xC , yR + yC) == (?
(xR + xC) ?
(yR + yC) + c)2= (?xR ?
yR + ?xC ?
yC + c)2= (?xR ?
yR)2+ 2?2(xR ?
yR)(xC ?
yC)+ 2c?
(xR ?
yR)+ (?
(xC ?
yC) + c)2The first 3 terms are non-zero only when atleast one rare feature exists.
We denote their sumKR(x, y).
The last term involves only common fea-tures.
We denote it KC(x, y).
Note that KC(x, y) isthe polynomial kernel of degree 2 over feature vec-tors of only common features.We can now write the SVM decision function as:?j?SVyj?jKR(xj , xR) +?j?SVyj?jKC(xj , xC) + bWe calculate the first sum via PKI, taking into ac-count only support-vectors which share at least onefeature with xR.
The second sum is calculated viakernel expansion while taking into account only the239common features.
Thus, only pairs of common fea-tures appear in the resulting weight vector using thesame expansion as in (Kudo and Matsumoto, 2003;Isozaki and Kazawa, 2002).
In our case, however,the expansion is memory efficient, because we con-sider only features in FC , which is small.Our approach is similar to the PKE approach(Kudo and Matsumoto, 2003), which used a basketmining approach to prune many features from theexpansion.
In contrast, we use a simpler approach tochoose which features to include in the expansion,and we also compensate for the feature we did notinclude by the PKI method.
Thus, our method gen-erates smaller expansions while computing the exactdecision function and not an approximation of it.We take every feature occurring in less than s sup-port vectors to be rare, and the other features to becommon.
By changing s we get a trade-of betweenspace and time complexity: smaller s indicate morecommon features (bigger memory requirement) butalso less rare features (less support vectors to in-clude in the summation), and vice-versa.
In con-trast to other methods, changing s is guaranteed notto change the classification accuracy, as it does notchange the computed decision function.4 Toolkit and EvaluationUsing this method, one can accelerate SVM-basedNLP application by just changing the classificationfunction, keeping the rest of the logic intact.
Weimplemented an open-source software toolkit, freelyavailable at http://www.cs.bgu.ac.il/?nlpproj/.
Ourtoolkit reads models created by popular SVM pack-ages (libsvm, SVMLight, TinySVM and Yamcha)and transforms them into our format.
The trans-formed models can then be used by our efficient Javaimplementation of the method described in this pa-per.
We supply wrappers for the interfaces of lib-svm and the Java bindings of SVMLight.
Changingexisting Java code to accommodate our fast SVMclassifier is done by loading a different model, andchanging a single function call.4.1 Evaluation: Speeding up MaltParserWe evaluate our method by using it as the classi-fication engine for the Java version of MaltParser,an SVM-based state of the art dependency parser(Nivre et al, 2006).
MaltParser uses the libsvmclassification engine.
We used the pre-trained En-glish models (based on sections 0-22 of the PennWSJ) supplied with MaltParser.
MaltParser alreadyuses an effective Classifiers Splitting heuristic whentraining these models, setting a high baseline for ourmethod.
The pre-trained parser consists of hundredsof different classifiers, some very small.
We reporthere on actual memory requirement and parsing timefor sections 23-24, considering the classifier combi-nation.
We took rare features to be those appear-ing in less than 0.5% of the support vectors, whichleaves us with less than 300 common features ineach of the ?big?
classifiers.
The results are summa-rized in Table 1.
As can be seen, our method parsesMethod Mem.
Parsing Time Sents/SecLibsvm 240MB 2166 (sec) 1.73ThisPaper 750MB 70 (sec) 53Table 1: Parsing Time for WSJ Sections 23-24 (3762sentences), on Pentium M, 1.73GHzabout 30 times faster, while using only 3 times asmuch memory.
MaltParser coupled with our fastclassifier parses above 3200 sentences per minute.5 ConclusionsWe presented a method for fast, accurate and mem-ory efficient calculation for polynomial kernels de-cisions functions in NLP application.
While themethod is applied to SVMs, it generalizes to otherpolynomial kernel based classifiers.
We demon-strated the method on the MaltParser dependencyparser with a 30-time speedup factor on overall pars-ing time, with low memory overhead.ReferencesY.
Goldberg and M. Elhadad.
2007.
SVM model tamper-ing and anchored learning: A case study in hebrew.
npchunking.
In Proc.
of ACL2007.H.
Isozaki and H. Kazawa.
2002.
Efficient support vectorclassifiers for named entity recognition.
In Proc.
ofCOLING2002.T.
Kudo and Y. Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In ACL-2003.J.
Nivre, J.
Hall, and J. Nillson.
2006.
Maltparser: Adata-driven parser-generator for dependency parsing.In Proc.
of LREC2006.Y.
Wu, J. Yang, and Y. Lee.
2007.
An approximate ap-proach for training polynomial kernel svms in lineartime.
In Proc.
of ACL2007 (short-paper).240
