Generation under UncertaintyOliver LemonHeriot-Watt UniversityEdinburgh, United Kingdomo.lemon@hw.ac.ukSrini JanarthanamEdinburgh UniversityEdinburgh, United Kingdoms.janarthanam@ed.ac.ukVerena RieserEdinburgh UniversityEdinburgh, United Kingdomvrieser@inf.ed.ac.ukAbstractWe invite the research community to con-sider challenges for NLG which arise fromuncertainty.
NLG systems should be ableto adapt to their audience and the genera-tion environment in general, but often theimportant features for adaptation are notknown precisely.
We explore generationchallenges which could employ simulatedenvironments to study NLGwhich is adap-tive under uncertainty, and suggest possi-ble metrics for such tasks.
It would be par-ticularly interesting to explore how differ-ent planning approaches to NLG performin challenges involving uncertainty in thegeneration environment.1 IntroductionWe would like to highlight the design of NLG sys-tems for environments where there may be incom-plete or faulty information, where actions may notalways have the same results, and where there maybe tradeoffs between the different possible out-comes of actions and plans.There are various sources of uncertainty in sys-tems which employ NLG techniques, for example:?
the current state of the user / audience (e.g.their knowledge, preferred vocabulary, goals,preferences....),?
the likely user reaction to the generated out-put,?
the behaviour of related components (e.g.
asurface realiser, or TTS module),?
noise in the environment (for spoken output),?
ambiguity of the generated output.The problem here is to generate output thattakes these types of uncertainty into account ap-propriately.
For example, you may need to choosea referring expression for a user, even though youare not sure whether they are an expert or novice inthe domain.
In addition, the next time you speakto that user, you need to adapt to new informa-tion you have gained about them (Janarthanam andLemon, 2010).
The issue of uncertainty for refer-ring expression generation has been discussed be-fore by (Reiter, 1991; Horacek, 2005).Another example is in planning an InformationPresentation for a user, when you cannot knowwith certainty how they will respond to it (Rieserand Lemon, 2009; Rieser et al, 2010).
In the worstcase, you may even be uncertain about the user?sgoals or information needs (as in ?POMDP?
ap-proaches to dialogue management (Young et al,2009; Henderson and Lemon, 2008a)), but youstill need to generate output for them in an appro-priate way.In particular, in interactive applications of NLG:?
each NLG action changes the environmentstate or context,?
the effect of each NLG action is uncertain.Several recent approaches describe NLG tasksas different kinds of planning, e.g.
(Koller and Pet-rick, 2008; Rieser et al, 2010; Janarthanam andLemon, 2010), or as contextual decision makingaccording to a cost function (van Deemter, 2009).It would be very interesting to explore how differ-ent approaches perform in NLG problems wheredifferent types of uncertainty are present in thegeneration environment.In the following we discuss possible genera-tion challenges arising from such considerations,which we hope will lead to work on an agreedshared challenge in this research community.
Insection 2 we briefly review recent work showingthat simulated environments can be used to evalu-ate generation under uncertainty, and in section 3we discuss some possible metrics for such tasks.Section 4 concludes by considering how a usefulgeneration challenge could be constructed usingsimilar methods.2 Generation in Uncertain SimulatedEnvironmentsFinding the best (or ?optimal?)
way to generateunder uncertainty requires exploring the possibleoutcomes of actions in stochastic environments.Therefore, related research on Dialogue Strategylearning has used data-driven simulated environ-ments as a cheap and efficient way to explore un-certainty (Lemon and Pietquin, 2007).
However,building good simulated environments is a chal-lenge in its own right, as we illustrate in the fol-lowing using the examples of Information Presen-tation and Referring Expression Generation.
Wealso point out the additional challenges these sim-ulations have to face when being used for NLG.2.1 User Simulations for InformationPresentationUser Simulations can provide a model of proba-ble, but uncertain, user reactions to NLG actions,and we propose that they are a useful potentialdirection for exploring and evaluate different ap-proaches to handling uncertainty in generation.User Simulations are commonly used to trainstrategies for Dialogue Management, see for ex-ample (Young et al, 2007).
A user simulation forInformation Presentation is very similar, in that itis a predictive model of the most likely next useract.
1 However, this NLG predicted user act doesnot actually change the overall dialogue state (e.g.by filling slots) but it only changes the generatorstate.
In other words, this NLG user simulationtells us what the user is most likely to do next, ifwe were to stop generating now.In addition to the challenges of building usersimulations for learning Dialogue policies, e.g.modelling, evaluation, and available data sets(Lemon and Pietquin, 2007), a crucial decision forNLG is the level of detail needed to train sensible1Similar to the internal user models applied in recentwork on POMDP (Partially Observable Markov DecisionProcess) dialogue managers (Young et al, 2007; Hendersonand Lemon, 2008b; Gasic et al, 2008) for estimation of useract probabilities.policies.
While high-level dialogue act descrip-tions may be sufficient for dialogue policies, NLGdecisions may require a much finer level of detail.The finer the required detail of user reactions, themore data is needed to build data-driven simula-tions.For content selection in Information Presen-tation tasks (choosing presentation strategy andnumber of attributes), for example, the level of de-scription can still be fairly abstract.
We were mostinterested in probability distributions over the fol-lowing possible user reactions:1. select: the user chooses one of the pre-sented items, e.g.
?Yes, I?ll take that one.
?.This reply type indicates that the informa-tion presentation was sufficient for the userto make a choice.2.
addInfo: The user provides more at-tributes, e.g.
?I want something cheap.?.
Thisreply type indicates that the user has morespecific requests, which s/he wants to specifyafter being presented with the current infor-mation.3.
requestMoreInfo: The user asks formore information, e.g.
?Can you recommendme one?
?, ?What is the price range of thelast item??.
This reply type indicates that thesystem failed to present the information theuser was looking for.4.
askRepeat: The user asks the system torepeat the samemessage again, e.g.
?Can yourepeat??.
This reply type indicates that theutterance was either too long or confusing forthe user to remember, or the TTS quality wasnot good enough, or both.5.
silence: The user does not say anything.In this case it is up to the system to take ini-tiative.6.
hangup: The user closes the interaction.We have built user simulations using n-grammodels of system (s) and user (u) acts, as firstintroduced by (Eckert et al, 1997).
In order toaccount for data sparsity, we apply different dis-counting (?smoothing?)
techniques including au-tomatic back-off, using the CMU Statistical Lan-guage Modelling toolkit (Clarkson and Rosenfeld,1997).
For example we have constructed a bi-gram model2 for the users?
reactions to the sys-tem?s IP structure decisions (P (au,t|IPs,t)), anda tri-gram (i.e.
IP structure + attribute choice)model for predicting user reactions to the system?scombined IP structure and attribute selection deci-sions: P (au,t|IPs,t, attributess,t).We have evaluated the performance of thesemodels by measuring dialogue similarity to theoriginal data, based on the Kullback-Leibler (KL)divergence, as also used by e.g.
(Cuaya?huitl et al,2005; Jung et al, 2009; Janarthanam and Lemon,2009).
We compared the raw probabilities as ob-served in the data with the probabilities generatedby our n-gram models using different discountingtechniques for each context.
All the models have asmall divergence from the original data (especiallythe bi-gram model), suggesting that they are rea-sonable simulations for training and testing NLGpolicies (Rieser et al, 2010).2.2 Other Simulated ComponentsIn some systems, NLG decisions may also dependon related components, such as the database, sub-sequent generation steps, or the Text-to-Speechmodule for spoken generation.
Building simula-tions for these components to capture their inher-ent uncertainty, again, is an interesting challenge.For example, one might want to adapt the gen-erated output according to the predicted TTS qual-ity.
Therefore, one needs a model of the expected/predicted TTS quality for a TTS engine (Boidin etal., 2009).Furthermore, NLG decisions might be inputsto a stochastic sentence realiser, such as SPaRKy(Stent et al, 2004).
However, one might not havea fully trained stochastic sentence realiser for thisdomain (yet).
In (Rieser et al, 2010) we thereforemodelled the variance as observed in the top rank-ing SPaRKy examples.2.3 Generating Referring Expressions underuncertaintyIn this section, we present an example user simu-lation (US) model, that simulates the dialogue be-haviour of users who react to referring expressionsdepending on their domain knowledge.
These ex-ternal simulation models are different from inter-nal user models used by dialogue systems.
In2Where au,t is the predicted next user action at time t,IPs,t was the system?s Information Presentation action at t,and attributess,t is the set of attributes selected by the sys-tem at t.particular, such models must be sensitive to asystem?s choices of referring expressions.
Thesimulation has a statistical distribution of in-builtknowledge profiles that determines the dialoguebehaviour of the user being simulated.
Uncer-tainty arises because if the user does not know areferring expression, then he is more likely to re-quest clarification.
If the user is able to interpretthe referring expressions and identify the refer-ences then he is more likely to follow the system?sinstruction.
This behaviour is simulated by the ac-tion selection models described below.The user simulation (US) receives the systemaction As,t and its referring expression choicesRECs,t at each turn.
The US responds with a useraction Au,t (u denoting user).
This can either be aclarification request (cr) or an instruction response(ir).
We used two kinds of action selection mod-els: a corpus-driven statistical model and a hand-coded rule-based model.2.4 Corpus-driven action selection modelThe user simulation (US) receives the systemaction As,t and its referring expression choicesRECs,t at each turn.
The US responds with a useraction Au,t (u denoting user).
This can either be aclarification request (cr) or an instruction response(ir).
The US produces a clarification request crbased on the class of the referent C(Ri), type ofthe referring expression Ti, and the current domainknowledge of the user for the referring expressionDKu,t(Ri, Ti).
Domain entities whose jargon ex-pressions raised clarification requests in the cor-pus were listed and those that had more than themean number of clarification requests were clas-sified as difficult and others as easy enti-ties (for example, ?power adaptor?
is easy - allusers understood this expression, ?broadband fil-ter?
is difficult).
Clarification requests areproduced using the following model.P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))where (Ri, Ti) ?
RECs,tOne should note that the actual literal expres-sion is not used in the transaction.
Only the entitythat it is referring to (Ri) and its type (Ti) are used.However, the above model simulates the processof interpreting and resolving the expression andidentifying the domain entity of interest in the in-struction.
The user identification of the entity issignified when there is no clarification request pro-duced (i.e.
Au,t = none).
When no clarificationrequest is produced, the environment actionEAu,tis generated using the following model.P (EAu,t|As,t) if Au,t!
= cr(Ri, Ti)Finally, the user action is an instruction re-sponse which is determined by the system ac-tion As,t.
Instruction responses can be ei-ther provide info, acknowledgement or otherbased on the system?s instruction.P (Au,t = ir|EAu,t, As,t)All the above models were trained on our cor-pus data using maximum likelihood estimationand smoothed using a variant of Witten-Bell dis-counting.
According to the data, clarification re-quests are much more likely when jargon expres-sions are used to refer to the referents that be-long to the difficult class and which the userdoesn?t know about.
When the system uses ex-pressions that the user knows, the user gener-ally responds to the instruction given by the sys-tem.
These user simulation models have beenevaluated and found to produce behaviour that isvery similar to the original corpus data, using theKullback-Leibler divergence metric (Janarthanamand Lemon, 2010).3 MetricsHere we discuss some possible evaluation met-rics that will allow different approaches to NLGunder uncertainty to be compared.
We envisagethat other metrics should be explored, in particularthose measuring adaptivity of various types.3.1 Adaptive Information PresentationGiven a suitable corpus, a data-driven evaluationfunction can be constructed, using a stepwise lin-ear regression, following the PARADISE frame-work (Walker et al, 2000).For example, in (Rieser et al, 2010) webuild a model which selects the features whichsignificantly influenced the users?
ratings forNLG strategies in a Wizard-of-Oz study.
Wealso assign a value to the user?s reactions(valueUserReaction), similar to optimising tasksuccess for DM (Young et al, 2007).
This re-flects the fact that good Information Presentationstrategies should help the user to select an item(valueUserReaction = +100) or provide moreconstraints addInfo (valueUserReaction =?0), but the user should not do anything else(valueUserReaction = ?100).
The regressionin equation 1 (R2 = .26) indicates that users?
rat-ings are influenced by higher level and lower levelfeatures: Users like to be focused on a small setof database hits (where #DBhits ranges over [1-100]), which will enable them to choose an item(valueUserReaction), while keeping the IP ut-terances short (where #sentence was in the range[2-18]):Reward = (?1.2)?#DBhits (1)+(.121)?
valueUserReaction?
(1.43)?#sentence3.2 Measuring Adaptivity of ReferringExpressionsWe have also designed a metric for the goal ofadapting referring expressions to each user?s do-main knowledge.
We present the Adaptation Ac-curacy score AA that calculates how accuratelythe agent chose the expressions for each referentr, with respect to the user?s knowledge.
Appro-priateness of an expression is based on the user?sknowledge of the expression.
So, when the userknows the jargon expression for r, the appropri-ate expression to use is jargon, and if s/he doesn?tknow the jargon, an descriptive expression is ap-propriate.
Although the user?s domain knowledgeis dynamically changing due to learning, we baseappropriateness on the initial state, because ourobjective is to adapt to the initial state of the userDKu,initial.
However, in reality, designers mightwant their system to account for user?s changingknowledge as well.
We calculate accuracy per ref-erent RAr as the ratio of number of appropriateexpressions to the total number of instances of thereferent in the dialogue.
We then calculate theoverall mean accuracy over all referents as shownbelow.RAr =#(appropriate expressions(r))#(instances(r))AdaptationAccuracyAA = 1#(r)?rRAr4 ConclusionWe have invited the research community to con-sider challenges for NLG which arise from uncer-tainty.
We argue that NLG systems, like dialoguemanagers, should be able to adapt to their audi-ence and the generation environment.
However,often the important features for adaptation are notprecisely known.
We then summarised 2 potentialdirections for such challenges ?
example genera-tion tasks which employ simulated uncertain en-vironments to study adaptive NLG, and discussedsome possible metrics for such tasks.
We hopethat this will lead to discussions on a shared chal-lenge allowing comparison of different approachesto NLG with respect to how well they handle un-certainty.AcknowledgmentsThe research leading to these results has receivedfunding from the European Community?s SeventhFramework Programme (FP7/2007-2013) undergrant agreement no.
216594 (CLASSiC projectwww.classic-project.org) and from theEPSRC, project no.
EP/G069840/1.ReferencesCedric Boidin, Verena Rieser, Lonneke van der Plas,Oliver Lemon, and Jonathan Chevelu.
2009.
Pre-dicting how it sounds: Re-ranking alternative in-puts to TTS using latent variables (forthcoming).
InProc.
of Interspeech/ICSLP, Special Session on Ma-chine Learning for Adaptivity in Spoken DialogueSystems.P.R.
Clarkson and R. Rosenfeld.
1997.
Statisti-cal Language Modeling Using the CMU-CambridgeToolkit.
In Proc.
of ESCA Eurospeech.Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, andHiroshi Shimodaira.
2005.
Human-computer dia-logue simulation using hidden markov models.
InProc.
of the IEEE workshop on Automatic SpeechRecognition and Understanding (ASRU).W.
Eckert, E. Levin, and R. Pieraccini.
1997.
Usermodeling for spoken dialogue system evaluation.
InProc.
of the IEEE workshop on Automatic SpeechRecognition and Understanding (ASRU).M.
Gasic, S. Keizer, F. Mairesse, J. Schatzmann,B.
Thomson, and S. Young.
2008.
Training andEvaluation of the HIS POMDP Dialogue System inNoise.
In Proc.
of SIGdial Workshop on Discourseand Dialogue.James Henderson and Oliver Lemon.
2008a.
MixtureModel POMDPs for Efficient Handling of Uncer-tainty in Dialogue Management.
In Proceedings ofACL.James Henderson and Oliver Lemon.
2008b.
MixtureModel POMDPs for Efficient Handling of Uncer-tainty in Dialogue Management.
In Proc.
of ACL.Helmut Horacek.
2005.
Generating referential de-scriptions under conditions of uncertainty.
In ENLG.Srinivasan Janarthanam and Oliver Lemon.
2009.
ATwo-tier User Simulation Model for ReinforcementLearning of Adaptive Referring Expression Genera-tion Policies.
In Proc.
of SIGdial.Srini Janarthanam and Oliver Lemon.
2010.
Learn-ing to adapt to unknown users: Referring expressiongeneration in spoken dialogue systems.
In Proceed-ings of ACL.
(to appear).Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-woo Jeong, and Gary Geunbae Lee.
2009.
Data-driven user simulation for automated evaluation ofspoken dialog systems.
Computer, Speech & Lan-guage, 23:479?509.Alexander Koller and Ronald Petrick.
2008.
Experi-ences with planning for natural language generation.In ICAPS.Oliver Lemon and Olivier Pietquin.
2007.
Machinelearning for spoken dialogue systems.
In Inter-speech.E.
Reiter.
1991.
Generating Descriptions that Exploit aUser?s Domain Knowledge.
In R. Dale, C. Mellish,and M. Zock, editors, Current Research in NaturalLanguage Generation, pages 257?285.
AcademicPress.Verena Rieser and Oliver Lemon.
2009.
Natural lan-guage generation as planning under uncertainty forspoken dialogue systems.
In EACL.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising information presentation for spoken dia-logue systems.
In Proceedings of ACL.
(to appear).Amanda Stent, Rashmi Prasad, and Marilyn Walker.2004.
Trainable sentence planning for complex in-formation presentation in spoken dialog systems.
InAssociation for Computational Linguistics.Kees van Deemter.
2009.
What game theory can dofor NLG: the case of vague language.
In 12th Eu-ropean Workshop on Natural Language Generation(ENLG).Marilyn A. Walker, Candace A. Kamm, and Diane J.Litman.
2000.
Towards Developing General Mod-els of Usability with PARADISE.
Natural Lan-guage Engineering, 6(3).SJ Young, J Schatzmann, K Weilhammer, and H Ye.2007.
The Hidden Information State Approach toDialog Management.
In ICASSP 2007.S.
Young, M.
Gas?ic?, S. Keizer, F. Mairesse, B. Thom-son, and K. Yu.
2009.
The Hidden InformationState model: a practical framework for POMDPbased spoken dialogue management.
ComputerSpeech and Language.
To appear.
