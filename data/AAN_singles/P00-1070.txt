Importance of Pronominal Anaphora resolution in QuestionAnswering systemsJose L. Vicedo and Antonio FerrandezDepartamento de Lenguajes y Sistemas InformaticosUniversidad de AlicanteApartado 99.
03080 Alicante, Spainfvicedo,antoniog@dlsi.ua.esAbstractThe main aim of this paper isto analyse the eects of applyingpronominal anaphora resolution toQuestion Answering (QA) systems.For this task a complete QA systemhas been implemented.
System eval-uation measures performance im-provements obtained when informa-tion that is referenced anaphoricallyin documents is not ignored.1 IntroductionOpen domain QA systems are dened astools capable of extracting the answer touser queries directly from unrestricted do-main documents.
Or at least, systems thatcan extract text snippets from texts, fromwhose content it is possible to infer the an-swer to a specic question.
In both cases,these systems try to reduce the amount oftime users spend to locate a concrete infor-mation.This work is intended to achieve two princi-pal objectives.
First, we analyse several docu-ment collections to determine the level of in-formation referenced pronominally in them.This study gives us an overview about theamount of information that is discarded whenthese references are not solved.
As second ob-jective, we try to measure improvements ofsolving this kind of references in QA systems.With this purpose in mind, a full QA systemhas been implemented.
Benets obtained bysolving pronominal references are measuredby comparing system performance with andwithout taking into account information ref-erenced pronominally.
Evaluation shows thatsolving these references improves QA perfor-mance.In the following section, the state-of-the-art of open domain QA systems will be sum-marised.
Afterwards, importance of pronom-inal references in documents is analysed.Next, our approach and system componentsare described.
Finally, evaluation results arepresented and discussed.2 BackgroundInterest in open domain QA systems is quiterecent.
We had little information about thiskind of systems until the First Question An-swering Track was held in last TREC confer-ence (TRE, 1999).
In this conference, nearlytwenty dierent systems were evaluated withvery dierent success rates.
We can clas-sify current approaches into two groups: text-snippet extraction systems and noun-phraseextraction systems.Text-snippet extraction approaches arebased on locating and extracting the most rel-evant sentences or paragraphs to the query bysupposing that this text will contain the cor-rect answer to the query.
This approach hasbeen the most commonly used by participantsin last TREC QA Track.
Examples of thesesystems are (Moldovan et al, 1999) (Singhalet al, 1999) (Prager et al, 1999) (Takaki,1999) (Hull, 1999) (Cormack et al, 1999).After reviewing these approaches, we cannotice that there is a general agreementabout the importance of several Natural Lan-guage Processing (NLP) techniques for QAtask.
Pos-tagging, parsing and Name En-tity recognition are used by most of the sys-tems.
However, few systems apply other NLPtechniques.
Particularly, only four systemsmodel some coreference relations between en-tities in the query and documents (Morton,1999)(Breck et al, 1999) (Oard et al, 1999)(Humphreys et al, 1999).
As example, Mor-ton approach models identity, denite noun-phrases and non-possessive third person pro-nouns.
Nevertheless, benets of applyingthese coreference techniques have not beenanalysed and measured separately.The second group includes noun-phrase ex-traction systems.
These approaches try tond the precise information requested byquestions whose answer is dened typically bya noun phrase.MURAX is one of these systems (Kupiec,1999).
It can use information from dierentsentences, paragraphs and even dierent doc-uments to determine the answer (the most rel-evant noun-phrase) to the question.
However,this system does not take into account theinformation referenced pronominally in docu-ments.
Simply, it is ignored.With our system, we want to determine thebenets of applying pronominal anaphora res-olution techniques to QA systems.
Therefore,we apply the developed computational sys-tem, Slot Unication Parser for Anaphora res-olution (SUPAR) over documents and queries(Ferrandez et al, 1999).
SUPAR's architec-ture consists of three independent modules:lexical analysis, syntactic analysis, and a reso-lution module for natural language processingproblems, such as pronominal anaphora.For evaluation, a standard based IR systemand a sentence-extraction QA system havebeen implemented.
Both are based on Saltonapproach (1989).
After IR system retrievesrelevant documents, our QA system processesthese documents with and without solvingpronominal references in order to compare -nal performance.As results will show, pronominal anaphoraresolution improves greatly QA systems per-formance.
So, we think that this NLP tech-nique should be considered as part of anyopen domain QA system.3 Importance of pronominalinformation in documentsTrying to measure the importance of informa-tion referenced pronominally in documents,we have analysed several text collections usedfor QA task in TREC-8 Conference as wellas others used frequently for IR system test-ing.
These collections were the following: LosAngeles Times (LAT), Federal Register (FR),Financial Times (FT), Federal Bureau Infor-mation Service (FBIS), TIME, CRANFIELD,CISI, CACM, MED and LISA.
This analy-sis consists on determining the amount andtype of pronouns used, as well as the numberof sentences containing pronouns in each ofthem.
As average measure of pronouns usedin a collection, we use the ratio between thequantity of pronouns and the number of sen-tences containing pronouns.
This measure ap-proximates the level of information that is ig-nored if these references are not solved.
Fig-ure 1 shows the results obtained in this anal-ysis.As we can see, the amount and type of pro-nouns used in analysed collections vary de-pending on the subject the documents talkabout.
LAT, FBIS, TIME and FT collectionsare composed from news published in dier-ent newspapers.
The ratio of pronominal ref-erence used in this kind of documents is veryhigh (from 35,96% to 55,20%).
These doc-uments contain a great number of pronomi-nal references in third person (he, she, they,his, her, their) whose antecedents are mainlypeople's names.
In this type of documents,pronominal anaphora resolution seems to bevery necessary for a correct modelling of rela-tions between entities.
CISI and MED collec-tions appear ranked next in decreasing ratiolevel order.
These collections are composedby general comments about document man-aging, classication and indexing and doc-uments extracted from medical journals re-spectively.
Although the ratio presented bythese collections (24,94% and 22,16%) is alsohigh, the most important group of pronominalreferences used in these collections is formedby "it" and "its" pronouns.
In this case,TEXT COLLECTION LAT FBIS TIME FT CISI MED CACM LISA FR CRANFIELDPronoun typeHE, SHE, THEY 38,59% 29,15% 31,20% 26,20% 15,38% 15,07% 8,59% 12,24% 13,31% 6,54%HIS, HER, THEIR 25,84% 21,54% 35,01% 20,52% 22,96% 21,46% 15,69% 31,03% 20,70% 10,35%IT, ITS 26,92% 39,60% 22,43% 46,68% 52,11% 57,41% 67,61% 47,86% 61,06% 79,76%HIM, THEM 7,04% 7,08% 7,82% 4,44% 6,38% 3,96% 4,87% 6,30% 3,45% 1,60%HIM, HER,IT(SELF), THEMSELVES 1,61% 2,63% 3,54% 2,17% 3,17% 2,10% 3,25% 2,57% 1,48% 1,75%Pronouns in SentencesContaining 0  pronouns 44,80% 48,09% 51,37% 64,04% 75,06% 77,84% 79,06% 83,79% 84,92% 90,95%Containing 1 pronoun 30,40% 31,37% 29,46% 23,07% 17,17% 15,02% 17,54% 13,01% 11,64% 8,10%Containing 2 pronouns 14,94% 12,99% 12,26% 8,54% 5,27% 4,75% 2,79% 2,56% 2,57% 0,85%Containing +2 pronouns 9,86% 7,55% 6,90% 4,34% 2,51% 2,39% 0,60% 0,64% 0,88% 0,09%Ratio of pronominal reference 55,20% 51,91% 48,63% 35,96% 24,94% 22,16% 20,94% 16,21% 15,08% 9,05%Figure 1: Pronominal references in text collectionsantecedents of these pronominal referencesare mainly concepts represented typically bynoun phrases.
It seems again important solv-ing these references for a correct modellingof relations between concepts expressed bynoun-phrases.
The lowest ratio results arepresented by CRANFIELD collection with a9,05%.
The reason of this level of pronominaluse is due to text contents.
This collection iscomposed by extracts of very high technicalsubjects.
Between the described percentageswe nd the CACM, LISA and FR collections.These collections are formed by abstracts anddocuments extracted from the Federal Regis-ter, from the CACM journal and from Libraryand Information Science Abstracts, respec-tively.
As general behaviour, we can noticethat as more technical document contents be-come, the pronouns "it" and "its" become themost appearing in documents and the ratioof pronominal references used decreases.
An-other observation can be extracted from thisanalysis.
Distribution of pronouns within sen-tences is similar in all collections.
Pronounsappear scattered through sentences contain-ing one or two pronouns.
Using more thantwo pronouns in the same sentence is quiteinfrequent.After analysing these results an importantquestion may arise.
Is it worth enough tosolve pronominal references in documents?
Itwould seem reasonable to think that resolu-tion of pronominal anaphora would only beaccomplished when the ratio of pronominaloccurrence exceeds a minimum level.
How-ever, we have to take into account that thecost of solving these references is proportionalto the number of pronouns analysed and con-sequently, proportional to the amount of in-formation a system will ignore if these refer-ences are not solved.As results above state, it seems reason-able to solve pronominal references in queriesand documents for QA tasks.
At least, whenthe ratio of pronouns used in documents rec-ommend it.
Anyway, evaluation and lateranalysis (section 5) contribute with empiri-cal data to conclude that applying pronom-inal anaphora resolution techniques improveQA systems performance.4 Our ApproachOur system is made up of three modules.
Therst one is a standard IR system that retrievesrelevant documents for queries.
The secondmodule will manage with anaphora resolutionin both, queries and retrieved documents.
Forthis purpose we use SUPAR computationalsystem (section 4.1).
And the third one isa sentence-extraction QA system that inter-acts with SUPAR module and ranks sentencesfrom retrieved documents to locate the an-swer where the correct answer appears (sec-tion 4.2).For the purpose of evaluation an IR sys-tem has been implemented.
This system isbased on the standard information retrievalapproach to document ranking described inSalton (1989).
For QA task, the same ap-proach has been used as baseline but usingsentences as text unit.
Each term in the queryand documents is assigned an inverse docu-ment frequency (idf ) score based on the samecorpus.
This measure is computed as:idf(t) = log(Ndf(t)) (1)where N is the total number of documentsin the collection and df(t) is the number ofdocuments which contains term t. Query ex-pansion consists of stemming terms using aversion of the Porter stemmer.
Document andsentence similarity to the query was computedusing the cosine similarity measure.
The LATcorpus has been selected as test collection dueto his high level of pronominal references.4.1 Solving pronominal anaphoraIn this section, the NLP Slot UnicationParser for Anaphora Resolution (SUPAR)is briey described (Ferrandez et al, 1999;Ferrandez et al, 1998).
SUPAR's architec-ture consists of three independent modulesthat interact with one other.
These modulesare lexical analysis, syntactic analysis, and aresolution module for Natural Language Pro-cessing problems.Lexical analysis module.
This moduletakes each sentence to parse as input, alongwith a tool that provides the system with allthe lexical information for each word of thesentence.
This tool may be either a dictio-nary or a part-of-speech tagger.
In addition,this module returns a list with all the neces-sary information for the remaining modulesas output.
SUPAR works sentence by sen-tence from the input text, but stores informa-tion from previous sentences, which it uses inother modules, (e.g.
the list of antecedents ofprevious sentences for anaphora resolution).Syntactic analysis module.
This mod-ule takes as input the output of lexical analy-sis module and the syntactic information rep-resented by means of grammatical formalismSlot Unication Grammar (SUG).
It returnswhat is called slot structure, which stores allnecessary information for following modules.One of the main advantages of this system isthat it allows carrying out either partial orfull parsing of the text.Module of resolution of NLP prob-lems.
In this module, NLP problems(e.g.
anaphora, extra-position, ellipsis or PP-attachment) are dealt with.
It takes the slotstructure (SS) that corresponds to the parsedsentence as input.
The output is an SS inwhich all the anaphors have been resolved.
Inthis paper, only pronominal anaphora resolu-tion has been applied.The kinds of knowledge that are going tobe used in pronominal anaphora resolution inthis paper are: pos-tagger, partial parsing,statistical knowledge, c-command and mor-phologic agreement as restrictions and severalheuristics such as syntactic parallelism, pref-erence for noun-phrases in same sentence asthe pronoun preference for proper nouns.We should remark that when we work withunrestricted texts (as it occurs in this paper)we do not use semantic knowledge (i.e.
atool such as WorNet).
Presently, SUPAR re-solves both Spanish and English pronominalanaphora with a success rate of 87% and 84%respectively.SUPAR pronominal anaphora resolutiondiers from those based on restrictions andpreferences, since the aim of our preferencesis not to sort candidates, but rather to dis-card candidates.
That is to say, preferencesare considered in a similar way to restrictions,except when no candidate satises a prefer-ence, in which case no candidate is discarded.For example in sentence: "Rob was asking usabout John.
I replied that Peter saw John yes-terday.
James also saw him."
After applyingthe restrictions, the following list of candi-dates is obtained for the pronoun him: [John,Peter, Rob], which are then sorted accordingto their proximity to the anaphora.
If pref-erence for candidates in same sentence as theanaphora is applied, then no candidate satis-es it, so the following preference is applied onthe same list of candidates.
Next, preferencefor candidates in the previous sentence is ap-plied and the list is reduced to the followingcandidates: [John, Peter ].
If syntactic par-allelism preference is then applied, only onecandidate remains, [John], which will be theantecedent chosen.Each kind of anaphora has its own set ofrestrictions and preferences, although they allfollow the same general algorithm: rst comethe restrictions, after which the preferencesare applied.
For pronominal anaphora, theset of restrictions and preferences that applyare described in Figure 2.Procedure SelectingAntecedent ( INPUT L: ListOfCandidates,OUTPUT Solution: Antecedent )Apply restrictions to L with a result of L1Morphologic agreementC-command constraintsSemantic consistencyCase of:NumberOfElements (L1) = 1Solution = TheFirstOne (L1)NumberOfElements (L1) = 0Exophora or cataphoraNumberOfElements (L1) > 1Apply preferences to L1 with a result of L21) Candidates in the same sentence as anaphor.2) Candidates in the previous sentence3) Preference for proper nouns.4) Candidates in the same position as the anaphorwith reference to the verb (before or after).5) Candidates with the same number of parsedconstituents as the anaphora6) Candidates that have appeared with the verb ofthe anaphor more than once7) Preference for indefinite NPs.Case of:NumberOfElements (L2) = 1Solution = TheFirstOne (L2)NumberOfElements (L2) > 1Extract from L2 in L3 those candidates that havebeen repeated most in the textIf NumberOfElements (L3) > 1Extract from L3 in L4 those candidates thathave appeared most with the verb of theanaphoraSolution = TheFirstOne (L4)ElseSolution = TheFirstOne (L3)EndIfEndCaseEndCaseEndProcedureFigure 2: Pronominal anaphora resolution al-gorithmThe following restrictions are rst appliedto the list of candidates: morphologic agree-ment, c-command constraints and semanticconsistency.
This list is sorted by proximity tothe anaphor.
Next, if after applying restric-tions there is still more than one candidate,the preferences are then applied, in the ordershown in this gure.
This sequence of prefer-ences (from 1 to 7 ) stops when, after havingapplied a preference, only one candidate re-mains.
If after applying preferences there isstill more than one candidate, then the mostrepeated candidates1in the text are extractedfrom the list after applying preferences.
Afterthis is done, if there is still more than one can-didate, then those candidates that have ap-peared most frequently with the verb of theanaphor are extracted from the previous list.Finally, if after having applied all the previ-ous preferences, there is still more than onecandidate left, the rst candidate of the re-sulting list, (the closest one to the anaphor),is selected.4.2 Anaphora resolution and QAOur QA approach provides a second level ofprocessing for relevant documents: Analysingmatching documents and Sentence ranking.Analysing Matching Documents.
Thisstep is applied over the best matching docu-ments retrieved from the IR system.
Thesedocuments are analysed by SUPAR moduleand pronominal references are solved.
As re-sult, each pronoun is associated with the nounphrase it refers to in the documents.
Then,documents are split into sentences as basictext unit for QA purposes.
This set of sen-tences is sent to the sentence ranking stage.Sentence Ranking.
Each term in thequery is assigned a weight.
This weight isthe sum of inverse document frequency mea-sure of terms based on its occurrence in theLAT collection described earlier.
Each docu-ment sentence is weighted the same way.
Theonly dierence with baseline is that pronounsare given the weight of the entity they referto.
As we only want to analyse the eectsof pronominal reference resolution, no morechanges are introduced in weighting scheme.For sentence ranking, cosine similarity is usedbetween query and document sentences.5 EvaluationFor this evaluation, several people unac-quainted with this work proposed 150 queries1Here, we mean that rstly we obtain the maxi-mum number of repetitions for an antecedent in theremaining list.
After that, we extract from that listthe antecedents that have this value of repetition.whose correct answer appeared at least onceinto the analysed collection.
These querieswere also selected based on their expressingthe user's information need clearly and theirbeing likely answered in a single sentence.First, relevant documents for each querywere retrieved using the IR system describedearlier.
Only the best 50 matching docu-ments were selected for QA evaluation.
Asthe document containing the correct answerwas included into the retrieved sets for only93 queries (a 62% of the proposed queries),the remaining 57 queries were excluded forthis evaluation.Once retrieval of relevant document setswas accomplished for each query, the sys-tem applied anaphora resolution algorithm tothese documents.
Finally, sentence matchingand ranking was accomplished as described insection 4.2 and the system presented a rankedlist containing the 10 most relevant sentencesto each query.For a better understanding of evaluation re-sults, queries were classied into three groupsdepending on the following characteristics: Group A.
There are no pronominal ref-erences in the target sentence (sentencecontaining the correct answer). Group B.
The information required asanswer is referenced via pronominalanaphora in the target sentence. Group C. Any term in the query is ref-erenced pronominally in the target sen-tence.Group A was made up by 37 questions.Groups B and C contained 25 and 31 queriesrespectively.
Figure 3 shows examples ofqueries classied into groups B and C.Evaluation results are presented in Figure4 as the number of target sentences appear-ing into the 10 most relevant sentences re-turned by the system for each query and also,the number of these sentences that are con-sidered a correct answer.
An answer is con-sidered correct if it can be obtained by sim-ply looking at the target sentence.
ResultsQuestion: ?Who is the village head man of Digha ?
?Answer: ?He is the sarpanch, or village head man ofDigha, a hamlet or mud-and-straw huts  10miles from ...?Group B ExampleAnaphora resolution: Ram BahaduQuestion: ?What did Democrats propose for low-incomefamilies?
?Answer: ?They also want to provide small subsidies forlow-income families in which both parents workat outside jobs.
?Group C ExampleAnaphora resolution: DemocratsFigure 3: Group B and C query examplesare classied based on question type intro-duced above.
The number of queries pertain-ing to each group appears in the second col-umn.
Third and fourth columns show base-line results (without solving anaphora).
Fifthand sixth columns show results obtained whenpronominal references have been solved.Results show several aspects we have totake into account.
Benets obtained from ap-plying pronominal anaphora resolution varydepending on question type.
Results forgroup A and B queries show us that relevanceto the query is the same as baseline system.So, it seems that pronominal anaphora res-olution does not achieve any improvement.This is true only for group A questions.
Al-though target sentences are ranked similarly,for group B questions, target sentences re-turned by baseline can not be considered ascorrect because we do not obtain the an-swer by simply looking at returned sentences.The correct answer is displayed only whenpronominal anaphora is solved and pronom-inal references are substituted by the nounphrase they refer to.
Only if pronominal ref-erences are solved, the user will not need toread more text to obtain the correct answer.For noun-phrase extraction QA systems theimprovement is greater.
If pronominal ref-erences are not solved, this information willBaseline              Anaphora solvedAnswer Type      Number Target included Correct answer Target included Correct answerA 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)B 25 (26,88%) 12 (48,00%) 0 (0,00%) 12 (48,00%) 12 (48,00%)C 31 (33,33%) 9 (29,03%) 9 (29,03%) 21 (67,74%) 21 (67,74%)A+B+C 93 (100,00%) 39 (41,94%) 27 (29,03%) 51 (54,84%) 51 (54,84%)Figure 4: Evaluation resultsnot be analysed and probably a wrong noun-phrase will be given as answer to the query.Results improve again if we analyse groupC queries performance.
These queries havethe following characteristic: some of thequery terms were referenced via pronominalanaphora in the relevant sentence.
Whenthis situation occurs, target sentences are re-trieved earlier in the nal ranked list than inthe baseline list.
This improvement is becausesimilarity increases between query and targetsentence when pronouns are weighted withthe same score as their referring terms.
Thepercentage of target sentences obtained in-creases 38,71 points (from 29,03% to 67,74%).Aggregate results presented in Figure 4measure improvement obtained consideringthe system as a whole.
General percentageof target sentences obtained increases 12,90points (from 41,94% to 54,84%) and the levelof correct answers returned by the system in-creases 25,81 points (from 29,03% to 54,84%).At this point we need to consider the follow-ing question: Will these results be the samefor any other question set?
We have analysedtest questions in order to determine if resultsobtained depend on question test set.
We ar-gue that a well-balanced query set would havea percentage of target sentences that containpronouns (PTSC) similar to the pronominalreference ratio of the text collection that isbeing queried.
Besides, we suppose that theprobability of nding an answer in a sentenceis the same for all sentences in the collec-tion.
Comparing LAT ratio of pronominalreference (55,20%) with the question test setPTSC we can measure how a question set canaect results.
Our question set PTSC valueis a 60,22%.
We obtain as target sentencescontaining pronouns only a 5,02% more thanexpected when test queries are randomly se-lected.
In order to obtain results according toa well-balanced question set, we discarded vequestions from both groups B and C. Figure 5shows that results for this well-balanced ques-tion set are similar to previous results.
Aggre-gate results show that general percentage oftarget sentences increases 10,84 points whensolving pronominal anaphora and the levelof correct answers retrieved increases 22,89points (instead of 12,90 and 25,81 obtainedin previous evaluation respectively).As results show, we can say that pronom-inal anaphora resolution improves QA sys-tems performance in several aspects.
First,precision increases when query terms are ref-erenced anaphorically in the target sentence.Second, pronominal anaphora resolution re-duces the amount of text a user has to readwhen the answer sentence is displayed andpronominal references are substituted withtheir coreferent noun phrases.
And third,for noun phrase extraction QA systems it isessential to solve pronominal references if agood performance is pursued.6 Conclusions and future researchThe analysis of information referencedpronominally in documents has revealed tobe important to tasks where high level ofrecall is required.
We have analysed andmeasured the eects of applying pronominalanaphora resolution in QA systems.
Asresults show, its application improves greatlyQA performance and seems to be essential insome cases.Three main areas of future work have ap-peared while investigation has been devel-oped.
First, IR system used for retrievingrelevant documents has to be adapted for QABaseline              Anaphora solvedAnswer Type      Number Target included Correct answer Target included Correct answerA 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)B 20 (21,51%) 10 (50,00%) 0 (0,00%) 10 (50,00%) 10 (50,00%)C 26 (27,96%) 9 (34,62%) 9 (34,62%) 18 (69,23%) 18 (69,23%)A+B+C 83 (89,25%) 37 (44,58%) 27 (32,53%) 46 (55,42%) 46 (55,42%)Figure 5: Well-balanced question set resultstasks.
The IR used, obtained the documentcontaining the target sentence only for 93 ofthe 150 proposed queries.
Therefore, its preci-sion needs to be improved.
Second, anaphoraresolution algorithm has to be extended todierent types of anaphora such as denitedescriptions, surface count, verbal phrase andone-anaphora.
And third, sentence rankingapproach has to be analysed to maximise thepercentage of target sentences included intothe 10 answer sentences presented by the sys-tem.ReferencesEric Breck, John Burger, Lisa Ferro, David House,Marc Light, and Inderjeet Mani.
1999.
A SysCalled Quanda.
In Eighth Text REtrieval Con-ference (TRE, 1999).Gordon V. Cormack, Charles L. A. Clarke,Christopher R. Palmer, and Derek I. E.Kisman.
1999.
Fast Automatic Passage Rank-ing (MultiText Experiments for TREC-8).
InEighth Text REtrieval Conference (TRE, 1999).Antonio Ferrandez, Manuel Palomar, and LidiaMoreno.
1998.
Anaphora resolution in unre-striced texts with partial parsing.
In 36th An-nual Meeting of the Association for Computa-tional Linguistics and 17th International Con-ference on Computational Lingustics COLING-ACL.Antonio Ferrandez, Manuel Palomar, and LidiaMoreno.
1999.
An empirical approach to Span-ish anaphora resolution.
To appear in MachineTranslation.David A.
Hull.
1999.
Xerox TREC-8 QuestionAnswering Track Report.
In Eighth Text RE-trieval Conference (TRE, 1999).Kevin Humphreys, Robert Gaizauskas, MarkHepple, and Mark Sanderson.
1999.
Universityof Sheeld TREC-8 Q&A System.
In EighthText REtrieval Conference (TRE, 1999).Julian Kupiec, 1999.
MURAX: Finding and Or-ganising Answers from Text Search, pages 311{331.
Kluwer Academic, New York.Dan Moldovan, Sanda Harabagiu, Marius Pasca,Rada Mihalcea, Richard Goodrum, RoxanaG^rju, and Vasile Rus.
1999.
LASSO: A Toolfor Surng the Answer Net.
In Eighth Text RE-trieval Conference (TRE, 1999).Thomas S. Morton.
1999.
Using Coreference inQuestion Answering.
In Eighth Text REtrievalConference (TRE, 1999).Douglas W. Oard, Jianqiang Wang, Dekang Lin,and Ian Soboro.
1999.
TREC-8 Experimentsat Maryland: CLIR, QA and Routing.
InEighth Text REtrieval Conference (TRE, 1999).John Prager, Dragomir Radev, Eric Brown, AnniCoden, and Valerie Samn.
1999.
The Use ofPredictive Annotation for Question Answering.In Eighth Text REtrieval Conference (TRE,1999).Gerard A. Salton.
1989.
Automatic Text Process-ing: The Transformation, Analysis, and Re-trieval of Information by Computer.
AddisonWesley, New York.Amit Singhal, Steve Abney, Michiel Bacchiani,Michael Collins, Donald Hindle, and FernandoPereira.
1999.
ATT at TREC-8.
In Eighth TextREtrieval Conference (TRE, 1999).Toru Takaki.
1999.
NTT DATA: Overview of sys-tem approach at TREC-8 ad-hoc and questionanswering.
In Eighth Text REtrieval Confer-ence (TRE, 1999).TREC-8.
1999.
Eighth Text REtrieval Confer-ence.
