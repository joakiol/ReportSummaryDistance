An Evaluation Exercise for Word AlignmentRada MihalceaDepartment of Computer ScienceUniversity of North TexasDenton, TX 76203rada@cs.unt.eduTed PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812tpederse@umn.eduAbstractThis paper presents the task definition, re-sources, participating systems, and compara-tive results for the shared task on word align-ment, which was organized as part of theHLT/NAACL 2003 Workshop on Building andUsing Parallel Texts.
The shared task in-cluded Romanian-English and English-Frenchsub-tasks, and drew the participation of seventeams from around the world.1 Defining a Word Alignment Shared TaskThe task of word alignment consists of finding correspon-dences between words and phrases in parallel texts.
As-suming a sentence aligned bilingual corpus in languagesL1 and L2, the task of a word alignment system is to indi-cate which word token in the corpus of language L1 cor-responds to which word token in the corpus of languageL2.As part of the HLT/NAACL 2003 workshop on ?Build-ing and Using Parallel Texts: Data Driven MachineTranslation and Beyond?, we organized a shared task onword alignment, where participating teams were providedwith training and test data, consisting of sentence alignedparallel texts, and were asked to provide automaticallyderived word alignments for all the words in the test set.Data for two language pairs were provided: (1) English-French, representing languages with rich resources (20million word parallel texts), and (2) Romanian-English,representing languages with scarce resources (1 millionword parallel texts).
Similar with the Machine Transla-tion evaluation exercise organized by NIST1, two sub-tasks were defined, with teams being encouraged to par-ticipate in both subtasks.1http://www.nist.gov/speech/tests/mt/1.
Limited resources, where systems are allowed to useonly the resources provided.2.
Unlimited resources, where systems are allowed touse any resources in addition to those provided.Such resources had to be explicitly mentioned in thesystem description.Test data were released one week prior to the deadlinefor result submissions.
Participating teams were askedto produce word alignments, following a common formatas specified below, and submit their output by a certaindeadline.
Results were returned to each team within threedays of submission.1.1 Word Alignment Output FormatThe word alignment result files had to include one linefor each word-to-word alignment.
Additionally, lines inthe result files had to follow the format specified in Fig.1.While the  and confidence fields overlap in theirmeaning, the intent of having both fields available is toenable participating teams to draw their own line on whatthey consider to be a Sure or Probable alignment.
Boththese fields were optional, with some standard values as-signed by default.1.1.1 A Running Word Alignment ExampleConsider the following two aligned sentences:[English] s snum=18 They had gone .
/s[French] s snum=18 Ils etaient alles .
/sA correct word alignment for this sentence is18 1 118 2 218 3 318 4 4stating that: all the word alignments pertain to sentence18, the English token 1 They aligns with the French to-ken 1 Ils, the English token 2 had, aligns with the Frenchtoken 2 etaient, and so on.
Note that punctuation is alsosentence no position L1 position L2 [ ] [confidence]where:?
sentence no represents the id of the sentence within thetest file.
Sentences in the test data already have an id as-signed.
(see the examples below)?
position L1 represents the position of the token that isaligned from the text in language L1; the first token in eachsentence is token 1.
(not 0)?
position L2 represents the position of the token that isaligned from the text in language L2; again, the first tokenis token 1.?
 can be either S or P, representing a Sure or Probablealignment.
All alignments that are tagged as S are also con-sidered to be part of the P alignments set (that is, all align-ments that are considered ?Sure?
alignments are also part ofthe ?Probable?
alignments set).
If the  field is missing, avalue of S will be assumed by default.?
confidence is a real number, in the range (0-1] (1 meaninghighly confident, 0 meaning not confident); this field is op-tional, and by default confidence number of 1 was assumed.Figure 1: Word Alignment file formataligned (English token 4 aligned with French token 4),and counts towards the final evaluation figures.Alternatively, systems could also provide an marker and/or a confidence score, as shown in the fol-lowing example:18 1 1 118 2 2 P 0.718 3 3 S18 4 4 S 1with missing  fields considered by default to be S,and missing confidence scores considered by default 1.1.2 Annotation Guide for Word AlignmentsThe annotation guide and illustrative word alignment ex-amples were mostly drawn from the Blinker AnnotationProject.
Please refer to (Melamed, 1999, pp.
169?182)for additional details.1.
All items separated by a white space are consideredto be a word (or token), and therefore have to bealigned.
(punctuation included)2.
Omissions in translation use the NULL token, i.e.token with id 0.
For instance, in the examples below:[English]: s snum=18 And he said , appoint methy wages , and I will give it .
/s[French]: s snum=18 fixe moi ton salaire , et jete le donnerai .
/sand he said from the English sentence has no cor-responding translation in French, and therefore allthese words are aligned with the token id 0....18 1 018 2 018 3 018 4 0...3.
Phrasal correspondences produce multiple word-to-word alignments.
For instance, in the examples be-low:English: s snum=18 cultiver la terre /sFrench: s snum=18 to be a husbandman /ssince the words do not correspond one to one, andyet the two phrases mean the same thing in the givencontext, the phrases should be linked as wholes, bylinking each word in one to each word in another.For the example above, this translates into 12 word-to-word alignments:18 1 1 18 1 218 1 3 18 1 418 2 1 18 2 218 2 3 18 2 418 3 1 18 3 218 3 3 18 3 42 ResourcesThe shared task included two different language pairs:the alignment of words in English-French parallel texts,and in Romanian-English parallel texts.
For each lan-guage pair, training data were provided to participants.Systems relying only on these resources were consideredpart of the Limited Resources subtask.
Systems makinguse of any additional resources (e.g.
bilingual dictionar-ies, additional parallel corpora, and others) were classi-fied under the Unlimited Resources category.2.1 Training DataTwo sets of training data were made available.1.
A set of Romanian-English parallel texts, consist-ing of about 1 million Romanian words, and aboutthe same number of English words.
These data con-sisted of: Parallel texts collected from the Web using asemi-supervised approach.
The URLs formatfor pages containing potential parallel transla-tions were manually identified (mainly fromthe archives of Romanian newspapers).
Next,texts were automatically downloaded and sen-tence aligned.
A manual verification of thealignment was also performed.
These data col-lection process resulted in a corpus of about850,000 Romanian words, and about 900,000English words. Orwell?s 1984, aligned within the MULTEXT-EAST project (Erjavec et al, 1997), with about130,000 Romanian words, and a similar num-ber of English words. The Romanian Constitution, for about 13,000Romanian words and 13,000 English words.2.
A set of English-French parallel texts, consisting ofabout 20 million English words, and about the samenumber of French words.
This is a subset of theCanadian Hansards, processed and sentence alignedby Ulrich Germann at ISI (Germann, 2001).All data were pre-tokenized.
For English and French,we used a version of the tokenizers provided within theEGYPT Toolkit2.
For Romanian, we used our own tok-enizer.
Identical tokenization procedures were used fortraining, trial, and test data.2.2 Trial DataTwo sets of trial data were made available at the sametime training data became available.
Trial sets consistedof sentence aligned texts, provided together with man-ually determined word alignments.
The main purposeof these data was to enable participants to better under-stand the format required for the word alignment resultfiles.
Trial sets consisted of 37 English-French, and 17Romanian-English aligned sentences.2.3 Test DataA total of 447 English-French aligned sentences (Ochand Ney, 2000), and 248 Romanian-English aligned sen-tences were released one week prior to the deadline.
Par-ticipants were required to run their word alignment sys-tems on these two sets, and submit word alignments.Teams were allowed to submit an unlimited number ofresults sets for each language pair.2.3.1 Gold Standard Word Aligned DataThe gold standard for the two language pair alignmentswere produced using slightly different alignment proce-dures, which allowed us to study different schemes forproducing gold standards for word aligned data.For English-French, annotators where instructed to as-sign a Sure or Probable tag to each word alignment theyproduced.
The intersection of the Sure alignments pro-duced by the two annotators led to the final Sure alignedset, while the reunion of the Probable alignments led tothe final Probable aligned set.
The Sure alignment set is2http://www.clsp.jhu.edu/ws99/projects/mt/toolkit/guaranteed to be a subset of the Probable alignment set.The annotators did not produce any NULL alignments.Instead, we assigned NULL alignments as a default back-up mechanism, which forced each word to belong to atleast one alignment.
The English-French aligned datawere produced by Franz Och and Hermann Ney (Och andNey, 2000).For Romanian-English, annotators were instructed toassign an alignment to all words, with specific instruc-tions as to when to assign a NULL alignment.
Annota-tors were not asked to assign a Sure or Probable label.Instead, we had an arbitration phase, where a third anno-tator judged the cases where the first two annotators dis-agreed.
Since an inter-annotator agreement was reachedfor all word alignments, the final resulting alignmentswere considered to be Sure alignments.3 Evaluation MeasuresEvaluations were performed with respect to four differ-ent measures.
Three of them ?
precision, recall, and F-measure ?
represent traditional measures in InformationRetrieval, and were also frequently used in previous wordalignment literature.
The fourth measure was originallyintroduced by (Och and Ney, 2000), and proposes the no-tion of quality of word alignment.Given an alignment , and a gold standard alignment, each such alignment set eventually consisting of twosets , , and , corresponding to Sure andProbable alignments, the following measures are defined(where  is the alignment type, and can be set to either Sor P).(1)(2)(3)    (4)Each word alignment submission was evaluated interms of the above measures.
Moreover, we conductedtwo sets of evaluations for each submission: NULL-Align, where each word was enforced to be-long to at least one alignment; if a word did not be-long to any alignment, a NULL Probable alignmentwas assigned by default.
This set of evaluations per-tains to full coverage word alignments. NO-NULL-Align, where all NULL alignments wereremoved from both submission file and gold stan-dard data.Team System name DescriptionLanguage Technologies Institute, CMU BiBr (Zhao and Vogel, 2003)MITRE Corporation Fourday (Henderson, 2003)RALI - Universite?
the Montre?al Ralign (Simard and Langlais, 2003)Romanian Academy Institute of Artificial Intelligence RACAI (Tufis?
et al, 2003)University of Alberta ProAlign (Lin and Cherry, 2003)University of Minnesota, Duluth UMD (Thomson McInnes and Pedersen, 2003)Xerox Research Centre Europe XRCE (Dejean et al, 2003)Table 1: Teams participating in the word alignment shared taskWe conducted therefore 14 evaluations for eachsubmission file: AER, Sure/Probable Precision,Sure/Probable Recall, and Sure/Probable F-measure,with a different figure determined for NULL-Align andNO-NULL-Align alignments.4 Participating SystemsSeven teams from around the world participated in theword alignment shared task.
Table 1 lists the names ofthe participating systems, the corresponding institutions,and references to papers in this volume that provide de-tailed descriptions of the systems and additional analysisof their results.All seven teams participated in the Romanian-Englishsubtask, and five teams participated in the English-Frenchsubtask.3 There were no restrictions placed on the num-ber of submissions each team could make.
This resultedin a total of 27 submissions from the seven teams, where14 sets of results were submitted for the English-Frenchsubtask, and 13 for the Romanian-English subtask.
Ofthe 27 total submissions, there were 17 in the Limited re-sources subtask, and 10 in the Unlimited resources sub-task.
Tables 2 and 3 show all of the submissions for eachteam in the two subtasks, and provide a brief descriptionof their approaches.While each participating system was unique, therewere a few unifying themes.Four teams had approaches that relied (to varying de-grees) on an IBM model of statistical machine translation(Brown et al, 1993).
UMD was a straightforward imple-mentation of IBM Model 2, BiBr employed a boostingprocedure in deriving an IBM Model 1 lexicon, Ralignused IBM Model 2 as a foundation for their recursivesplitting procedure, and XRCE used IBM Model 4 as abase for alignment with lemmatized text and bilinguallexicons.Two teams made use of syntactic structure in the textto be aligned.
ProAlign satisfies constraints derived froma dependency tree parse of the English sentence being3The two teams that did not participate in English-Frenchwere Fourday and RACAI.aligned.
BiBr also employs syntactic constraints thatmust be satisfied.
However, these come from parallel textthat has been shallowly parsed via a method known asbilingual bracketing.Three teams approached the shared task with baselineor prototype systems.
Fourday combines several intuitivebaselines via a nearest neighbor classifier, RACAI car-ries out a greedy alignment based on an automaticallyextracted dictionary of translations, and UMD?s imple-mentation of IBMModel 2 provides an experimental plat-form for their future work incorporating prior knowledgeabout cognates.
All three of these systems were devel-oped within a short period of time before and during theshared task.5 Results and DiscussionTables 4 and 5 list the results obtained by participatingsystems in the Romanian-English task.
Similarly, resultsobtained during the English-French task are listed in Ta-bles 6 and 7.For Romanian-English, limited resources, XRCE sys-tems (XRCE.Nolem-56k.RE.2 and XRCE.Trilex.RE.3)seem to lead to the best results.
These are systems thatare based on GIZA++, with or without additional re-sources (lemmatizers and lexicons).
For unlimited re-sources, ProAlign.RE.1 has the best performance.For English-French, Ralign.EF.1 has the best perfor-mance for limited resources, while ProAlign.EF.1 hasagain the largest number of top ranked figures for unlim-ited resources.To make a cross-language comparison, we paid partic-ular attention to the evaluation of the Sure alignments,since these were collected in a similar fashion (an agree-ment had to be achieved between two different anno-tators).
The results obtained for the English-FrenchSure alignments are significantly higher (80.54% best F-measure) than those for Romanian-English Sure align-ments (71.14% best F-measure).
Similarly, AER forEnglish-French (5.71% highest error reduction) is clearlybetter than the AER for Romanian-English (28.86% high-est error reduction).This difference in performance between the two datasets is not a surprise.
As expected, word alignment, likemany other NLP tasks (Banko and Brill, 2001), highlybenefits from large amounts of training data.
Increasedperformance is therefore expected when larger trainingdata sets are available.The only evaluation set where Romanian-English dataleads to better performance is the Probable alignmentsset.
We believe however that these figures are not di-rectly comparable, since the English-French Probablealignments were obtained as a reunion of the align-ments assigned by two different annotators, while forthe Romanian-English Probable set two annotators hadto reach an agreement (that is, an intersection of their in-dividual alignment assignments).Interestingly, in an overall evaluation, the limited re-sources systems seem to lead to better results than thosewith unlimited resources.
Out of 28 different evaluationfigures, 20 top ranked figures are provided by systemswith limited resources.
This suggests that perhaps usinga large number of additional resources does not seem toimprove a lot over the case when only parallel texts areemployed.Ranked results for all systems are plotted in Figures 2and 3.
In the graphs, systems are ordered based on theirAER scores.
System names are preceded by a marker toindicate the system type: L stands for Limited Resources,and U stands for Unlimited Resources.6 ConclusionA shared task on word alignment was organized as partof the HLT/NAACL 2003 Workshop on Building andUsing Parallel Texts.
In this paper, we presented thetask definition, and resources involved, and shortly de-scribed the participating systems.
The shared task in-cluded Romanian-English and English-French sub-tasks,and drew the participation of seven teams from around theworld.
Comparative evaluations of results led to interest-ing insights regarding the impact on performance of (1)various alignment algorithms, (2) large or small amountsof training data, and (3) type of resources available.
Dataand evaluation software used in this exercise are availableonline at http://www.cs.unt.edu/?rada/wpt.AcknowledgmentsThere are many people who contributed greatly to mak-ing this word alignment evaluation task possible.
We aregrateful to all the participants in the shared task, for theirhard work and involvement in this evaluation exercise.Without them, all these comparative analyses of wordalignment techniques would not be possible.We are very thankful to Franz Och from ISI and Her-mann Ney from RWTH Aachen for kindly making theirEnglish-French word aligned data available to the work-shop participants; the Hansards made available by Ul-rich Germann from ISI constituted invaluable data forthe English-French shared task.
We would also like tothank the student volunteers from the Department of En-glish, Babes-Bolyai University, Cluj-Napoca, Romaniawho helped creating the Romanian-English word aligneddata.We are also grateful to all the Program Committeemembers of the current workshop, for their commentsand suggestions, which helped us improve the definitionof this shared task.
In particular, we would like to thankDan Melamed for suggesting the two different subtasks(limited and unlimited resources), and Michael Carl andPhil Resnik for initiating interesting discussions regard-ing phrase-based evaluations.ReferencesM.
Banko and E. Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Pro-ceedings of the 39th Annual Meeting of the Associationfor Computational Lingusitics (ACL-2001), Toulouse,France, July.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguis-tics, 19(2):263?311.Herve Dejean, Eric Gaussier, Cyril Goutte, and KenjiYamada.
2003.
Reducing parameter space for wordalignment.
In HLT-NAACL 2003 Workshop: Buildingand Using Parallel Texts: Data Driven Machine Trans-lation and Beyond, pages 23?26, Edmonton, Alberta,Canada, May 31.
Association for Computational Lin-guistics.T.
Erjavec, N. Ide, and D. Tufis?.
1997.
Encoding andparallel alignment of linguistic corpora in six centraland Eastern European languages.
In Proceedings ofthe Joint ACH/ALL Conference, Queen?s University,Kingston, Ontario, June.U.
Germann.
2001.
Aligned hansards of the 36thparliament of canada.
http://www.isi.edu/natural-language/download/hansard/.John C. Henderson.
2003.
Word alignment baselines.
InHLT-NAACL 2003Workshop: Building and Using Par-allel Texts: Data Driven Machine Translation and Be-yond, pages 27?30, Edmonton, Alberta, Canada, May31.
Association for Computational Linguistics.Dekang Lin and Colin Cherry.
2003.
Proalign: Sharedtask system description.
In HLT-NAACL 2003 Work-shop: Building and Using Parallel Texts: Data DrivenMachine Translation and Beyond, pages 11?14, Ed-monton, Alberta, Canada, May 31.
Association forComputational Linguistics.D.I.
Melamed.
1999.
Empirical Methods for ExploitingParallel Texts.
MIT Press.F.
Och and H. Ney.
2000.
A comparison of alignmentmodels for statistical machine translation.
In Proceed-ings of the 18th International Conference on Computa-tional Linguistics (COLING-ACL 2000), Saarbrucken,Germany, August.Michel Simard and Philippe Langlais.
2003.
Statisti-cal translation alignment with compositionality con-straints.
In HLT-NAACL 2003 Workshop: Buildingand Using Parallel Texts: Data Driven Machine Trans-lation and Beyond, pages 19?22, Edmonton, Alberta,Canada, May 31.
Association for Computational Lin-guistics.Bridget Thomson McInnes and Ted Pedersen.
2003.
Theduluth word alignment system.
In HLT-NAACL 2003Workshop: Building and Using Parallel Texts: DataDriven Machine Translation and Beyond, pages 40?43, Edmonton, Alberta, Canada, May 31.
Associationfor Computational Linguistics.Dan Tufis?, Ana-Maria Barbu, and Radu Ion.
2003.
Treq-al: A word alignment system with limited languageresources.
In HLT-NAACL 2003 Workshop: Buildingand Using Parallel Texts: Data Driven Machine Trans-lation and Beyond, pages 36?39, Edmonton, Alberta,Canada, May 31.
Association for Computational Lin-guistics.D Tufis?.
2002.
A cheap and fast way to build usefultranslation lexicons.
In Proceedings of the 19th In-ternational Conference on Computational Linguistics,pages 1030?1036, Taipei, August.Bing Zhao and Stephan Vogel.
2003.
Word alignmentbased on bilingual bracketing.
In HLT-NAACL 2003Workshop: Building and Using Parallel Texts: DataDriven Machine Translation and Beyond, pages 15?18, Edmonton, Alberta, Canada, May 31.
Associationfor Computational Linguistics.System Resources DescriptionBiBr.EF.1 Limited baseline of bilingual bracketingBiBr.EF.2 Unlimited baseline of bilingual bracketing + English POS taggingBiBr.EF.3 Unlimited baseline of bilingual bracketing + English POS tagging and base NPBiBr.EF.4 Limited reverse direction of BiBr.EF.1BiBr.EF.5 Unlimited reverse direction of BiBr.EF.2BiBr.EF.6 Unlimited reverse direction of BiBr.EF.3BiBr.EF.7 Limited intersection of BiBr.EF.1 & BiBr.EF.3BiBr.EF.8 Unlimited intersection of BiBr.EF.3 & BiBr.EF.6ProAlign.EF.1 Unlimited cohesion between source and target language + English parser +distributional similarity for English wordsRalign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentationUMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4XRCE.Base.EF.1 Limited GIZA++ (IBM Model 4) with English and French lemmatizerXRCE.Nolem.EF.2 Limited GIZA++ only (IBM Model 4), trained with 1/4 of the corpusXRCE.Nolem.EF.3 Limited GIZA++ only (IBM Model 4), trained with 1/2 of the corpusTable 2: Short description for English-French systemsSystem Resources DescriptionBiBr.RE.1 Limited baseline of bilingual bracketingBiBr.RE.2 Unlimited baseline of bilingual bracketing + English POS taggingBiBr.RE.3 Unlimited baseline of bilingual bracketing + English POS tagging and base NPFourday.RE.1 Limited nearest neighbor combination of baseline measuresProAlign.RE.1 Unlimited cohesion between source and target language + English parser +distributional similarity for English wordsRACAI.RE.1 Unlimited translation equivalence dictionary (Tufis?, 2002) + POS taggingRalign.RE.1 Limited Giza (IBM Model 2) + recursive parallel segmentationUMD.RE.1 Limited IBM Model 2, trained with all the corpus, distortion 4, iterations 4UMD.RE.2 Limited IBM Model 2, trained with all the corpus, distortion 2, iterations 4XRCE.Base.RE.1 Limited GIZA++ (IBM Model 4), with English lemmatizerXRCE.Nolem.RE.2 Limited GIZA++ only (IBM Model 4)XRCE.Trilex.RE.3 Limited GIZA++ only (IBM Model 4), with English lemmatizer and trinity lexiconXRCE.Trilex.RE.4 Limited GIZA++ only (IBM Model 4), with English lemmatizer and trinity lexiconTable 3: Short description for Romanian-English systemsSystem AERLimited ResourcesBiBr.RE.1 70.65% 55.75% 62.32% 59.60% 57.65% 58.61% 41.39%Fourday.RE.1 0.00% 0.00% 0.00% 52.83% 42.86% 47.33% 52.67%Ralign.RE.1 92.00% 45.06% 60.49% 63.63% 65.92% 64.76% 35.24%UMD.RE.1 57.67% 49.70% 53.39% 57.67% 49.70% 53.39% 46.61%UMD.RE.2 58.29% 49.99% 53.82% 58.29% 49.99% 53.82% 46.18%XRCE.Base.RE.1 79.28% 61.14% 69.03% 79.28% 61.14% 69.03% 30.97%XRCE.Nolem-56K.RE.2 82.65% 62.44% 71.14% 82.65% 62.44% 71.14% 28.86%XRCE.Trilex.RE.3 80.97% 61.89% 70.16% 80.97% 61.89% 70.16% 29.84%XRCE.Trilex.RE.4 79.76% 61.31% 69.33% 79.76% 61.31% 69.33% 30.67%Unlimited ResourcesBiBr.RE.2 70.46% 55.51% 62.10% 58.40% 57.59% 57.99% 41.39%BiBr.RE.3 70.36% 55.47% 62.04% 58.17% 58.12% 58.14% 41.86%RACAI.RE.1 81.29% 60.26% 69.21% 81.29% 60.26% 69.21% 30.79%ProAlign.RE.1 88.22% 58.91% 70.64% 88.22% 58.91% 70.64% 29.36%Table 4: Results for Romanian-English, NO-NULL-AlignSystem AERLimited ResourcesBiBr.RE.1 70.65% 48.32% 57.39% 57.38% 52.62% 54.90% 45.10%Fourday.RE.1 0.00% 0.00% 0.00% 35.85% 45.88% 40.25% 59.75%Ralign.RE.1 92.00% 39.05% 54.83% 63.63% 57.13% 60.21% 39.79%UMD.RE.1 56.21% 43.17% 48.84% 45.51% 47.76% 46.60% 53.40%UMD.RE.2 56.58% 43.45% 49.15% 46.00% 47.88% 46.92% 53.08%XRCE.Base.RE.1 79.28% 52.98% 63.52% 61.59% 61.50% 61.54% 38.46%XRCE.Nolem-56K.RE.2 82.65% 54.12% 65.41% 61.59% 61.50% 61.54% 38.46%XRCE.Trilex.RE.3 80.97% 53.64% 64.53% 63.64% 61.58% 62.59% 37.41%XRCE.Trilex.RE.4 79.76% 53.14% 63.78% 62.22% 61.37% 61.79% 38.21%Unlimited ResourcesBiBr.RE.2 70.46% 48.11% 57.18% 56.01% 52.26% 54.07% 45.93%BiBr.RE.3 70.36% 48.08% 57.12% 56.05% 52.87% 54.42% 45.58%RACAI.RE.1 60.30% 62.38% 61.32% 59.87% 62.42% 61.12% 38.88%ProAlign.RE.1 88.22% 51.06% 64.68% 61.71% 62.05% 61.88% 38.12%Table 5: Results for Romanian-English, NULL-AlignSystem AERLimited ResourcesBiBr.EF.1 49.85% 79.45% 61.26% 67.23% 29.24% 40.76% 28.23%BiBr.EF.4 51.46% 82.42% 63.36% 66.65% 32.68% 43.86% 28.01%BiBr.EF.7 63.03% 74.59% 68.32% 66.11% 30.06% 41.33% 29.38%Ralign.EF.1 72.54% 80.61% 76.36% 77.56% 38.19% 51.18% 18.50%UMD.EF.1 37.98% 64.66% 47.85% 59.69% 23.53% 33.75% 38.47%XRCE.Base.EF.1 50.89% 84.67% 63.57% 83.22% 32.05% 46.28% 16.23%XRCE.Nolem.EF.2 55.54% 93.46% 69.68% 89.65% 34.92% 50.27% 8.93%XRCE.Nolem.EF.3 55.43% 93.81% 69.68% 90.09% 35.30% 50.72% 8.53%Unlimited ResourcesBiBr.EF.2 50.05% 79.89% 61.54% 66.92% 29.14% 40.60% 28.24%BiBr.EF.3 50.21% 80.26% 61.80% 63.79% 30.52% 41.29% 30.38%BiBr.EF.5 51.27% 82.17% 63.15% 67.22% 32.56% 43.87% 27.71%BiBr.EF.6 51.91% 83.26% 63.95% 62.21% 34.58% 44.45% 31.32%BiBr.EF.8 66.34% 74.86% 70.34% 61.62% 31.37% 41.57% 32.48%ProAlign.EF.1 71.94% 91.48% 80.54% 96.49% 28.41% 43.89% 5.71%Table 6: Results for English-French, NO-NULL-AlignSystem AERLimited ResourcesBiBr.EF.1 49.85% 79.45% 61.26% 60.32% 29.12% 39.28% 33.37%BiBr.EF.4 51.46% 82.42% 63.36% 61.64% 32.41% 42.48% 31.91%BiBr.EF.7 63.03% 74.59% 68.32% 51.35% 30.45% 38.23% 40.97%Ralign.EF.1 72.54% 80.61% 76.36% 77.56% 36.79% 49.91% 18.50%UMD.EF.1 37.19% 64.66% 47.22% 41.93% 24.08% 30.59% 51.71%XRCE.Base.EF.1 50.89% 84.67% 63.57% 64.96% 32.73% 43.53% 28.99%XRCE.Nolem.EF.2 55.54% 93.46% 69.68% 70.98% 35.61% 47.43% 22.10%XRCE.Nolem.EF.3 55.43% 93.81% 69.68% 72.01% 36.00% 48.00% 21.27%Unlimited ResourcesBiBr.EF.2 50.05% 79.89% 61.54% 59.89% 28.96% 39.04% 33.48%BiBr.EF.3 50.21% 80.26% 61.80% 57.85% 30.28% 39.75% 35.03%BiBr.EF.5 51.27% 82.17% 63.15% 62.05% 32.23% 42.43% 31.69%BiBr.EF.6 51.91% 83.26% 63.95% 58.41% 34.20% 43.14% 34.47%BiBr.EF.8 66.34% 74.86% 70.34% 48.50% 31.76% 38.38% 43.37%ProAlign.EF.1 71.94% 91.48% 80.54% 56.02% 30.05% 39.62% 33.71%Table 7: Results for English-French, NULL-AlignFigure 2: Ranked results for Romanian-English dataFigure 3: Ranked results for English-French data
