Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 168?175,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsTransforming Projective Bilexical Dependency Grammars intoefficiently-parsable CFGs with Unfold-FoldMark JohnsonMicrosoft Research Brown UniversityRedmond, WA Providence, RIt-majoh@microsoft.com Mark Johnson@Brown.eduAbstractThis paper shows how to use the Unfold-Fold transformation to transform ProjectiveBilexical Dependency Grammars (PBDGs)into ambiguity-preserving weakly equiva-lent Context-Free Grammars (CFGs).
TheseCFGs can be parsed in O(n3) time using aCKY algorithm with appropriate indexing,rather than the O(n5) time required by anaive encoding.
Informally, using the CKYalgorithm with such a CFG mimics the stepsof the Eisner-Satta O(n3) PBDG parsing al-gorithm.
This transformation makes all ofthe techniques developed for CFGs availableto PBDGs.
We demonstrate this by describ-ing a maximum posterior parse decoder forPBDGs.1 IntroductionProjective Bilexical Dependency Grammars (PB-DGs) have attracted attention recently for two rea-sons.
First, because they capture bilexical head-to-head dependencies they are capable of producingextremely high-quality parses: state-of-the-art dis-criminatively trained PBDG parsers rival the accu-racy of the very best statistical parsers available to-day (McDonald, 2006).
Second, Eisner-Satta O(n3)PBDG parsing algorithms are extremely fast (Eisner,1996; Eisner and Satta, 1999; Eisner, 2000).This paper investigates the relationship betweenContext-Free Grammar (CFG) parsing and the Eis-ner/Satta PBDG parsing algorithms, including theirextension to second-order PBDG parsing (McDon-ald, 2006; McDonald and Pereira, 2006).
Specifi-cally, we show how to use an off-line preprocessingstep, the Unfold-Fold transformation, to transform aPBDG into an equivalent CFG that can be parsed inO(n3) time using a version of the CKY algorithmwith suitable indexing (Younger, 1967), and extendthis transformation so that it captures second-orderPBDG dependencies as well.
The transformationsare ambiguity-preserving, i.e., there is a one-to-one mapping between dependency parses and CFGparses, so it is possible to map the CFG parses backto the PBDG parses they correspond to.The PBDG to CFG reductions make techniquesdeveloped for CFGs available to PBDGs as well.
Forexample, incremental CFG parsing algorithms canbe used with the CFGs produced by this transform,as can the Inside-Outside estimation algorithm (Lariand Young, 1990) and more exotic methods such asestimating adjoined hidden states (Matsuzaki et al,2005; Petrov et al, 2006).
As an example appli-cation, we describe a maximum posterior parse de-coder for PBDGs in Section 8.The Unfold-Fold transformation is a calculus fortransforming functional and logic programs intoequivalent but (hopefully) faster programs (Burstalland Darlington, 1977).
We use it here to trans-form CFGs encoding dependency grammars intoother CFGs that are more efficiently parsable.
SinceCFGs can be expressed as Horn-clause logic pro-grams (Pereira and Shieber, 1987) and the Unfold-Fold transformation is provably correct for such pro-grams (Sato, 1992; Pettorossi and Proeitti, 1992), itfollows that its application to CFGs is provably cor-rect as well.
The Unfold-Fold transformation is usedhere to derive the CFG schemata presented in sec-tions 5?7.
A system that uses these schemata (suchas the one described in section 8) can implement168these schemata directly, so the Unfold-Fold trans-formation plays a theoretical role in this work, justi-fying the resulting CFG schemata.The closest related work we are aware ofis McAllester (1999), which also describes a re-duction of PBDGs to efficiently-parsable CFGsand directly inspired this work.
However, theCFGs produced by McAllester?s transformation in-clude epsilon-productions so they require a special-ized CFG parsing algorithm, while the CFGs pro-duced by the transformations described here havebinary productions so they can be parsed withstandard CFG parsing algorithms.
Further, ourapproach extends to second-order PBDG parsing,while McAllester only discusses first-order PBDGs.The rest of this paper is structured as follows.Section 2 defines projective dependency graphs andgrammars and Section 3 reviews the ?naive?
encod-ing of PBDGs as CFGs with an O(n5) parse time,where n is the length of the string to be parsed.
Sec-tion 4 introduces the ?split-head?
CFG encoding ofPBDGs, which has an O(n4) parse time and servesas the input to the Unfold-Fold transform.
Section 5uses the Unfold-Fold transform to obtain a weakly-equivalent CFG encoding of PBDGs which can beparsed in O(n3) time, and presents timing resultsshowing that the transformation does speed parsing.Sections 6 and 7 apply Unfold-Fold in slightly morecomplex ways to obtain CFG encodings of PBDGsthat also make second-order dependencies availablein O(n3) time parsable CFGs.
Section 8 applies aPBDG to CFG transform to obtain a maximum pos-terior decoding parser for PBDGs.2 Projective bilexical dependency parsesand grammarsLet ?
be a finite set of terminals (e.g., words),and let 0 be the root terminal not in ?.
If w =(w1, .
.
.
, wn) ?
?
?, let w?
= (0, w1, .
.
.
, wn), i.e.,w?
is obtained by prefixing w with 0.
A dependencyparse G for w is a tree whose root is labeled 0 andwhose other n vertices are labeled with each of the nterminals in w. If G contains an arc from u to v thenwe say that v is a dependent of u, and if G containsa path from u to v then we say that v is a descendantof u.
If v is dependent of u that also precedes u inw?
then we say that v is a left dependent of u (rightdependent and left and right descendants are definedsimilarly).0 Sandy gave the dog a boneFigure 1: A projective dependency parse for the sen-tence ?Sam gave the dog a bone?.A dependency parse G is projective iff wheneverthere is a path from u to v then there is also a pathfrom u to every word between u and v in w?
as well.Figure 1 depicts a projective dependency parse forthe sentence ?Sam gave the dog a bone?.A projective dependency grammar defines a set ofprojective dependency parses.
A Projective Bilexi-cal Dependency Grammar (PBDG) consists of tworelations and , both defined over (??{0})??.
A PBDG generates a projective dependencyparse G iff u v for all right dependencies (u, v)in G and v u for all left dependencies (u, v) inG.
The language generated by a PBDG is the setof strings that have projective dependency parsesgenerated by the grammar.
The following depen-dency grammar generates the dependency parse inFigure 1.0 gave Sandy gavegave dog the doggave bone a boneThis paper does not consider stochastic depen-dency grammars directly, but see Section 8 for anapplication involving them.
However, it is straight-forward to associate weights with dependencies, andsince the dependencies are preserved by the transfor-mations, obtain a weighted CFG.
Standard methodsfor converting weighted CFGs to equivalent PCFGscan be used if required (Chi, 1999).
Alternatively,one can transform a corpus of dependency parsesinto a corpus of the corresponding CFG parses, andestimate CFG production probabilities directly fromthat corpus.3 A naive encoding of PBDGsThere is a well-known method for encoding a PBDGas a CFG in which each terminal u ?
?
is associatedwith a corresponding nonterminal Xu that expandsto u and all of u?s descendants.
The nonterminals ofthe naive encoding CFG consist of the start symbolS and symbols Xu for each terminal u ?
?, and169the productions of the CFG are the instances of thefollowing schemata:S ?
Xu where 0 uXu ?
uXu ?
Xv Xu where v uXu ?
Xu Xv where u vThe dependency annotations associated with eachproduction specify how to interpret a local tree gen-erated by that production, and permit us to map aCFG parse to the corresponding dependency parse.For example, the top-most local tree in Figure 2 wasgenerated by the production S ?
Xgave, and indi-cate that in this parse 0 gave.Given a terminal vocabulary of size m the CFGcontains O(m2) productions, so it is impractical toenumerate all possible productions for even modestvocabularies.
Instead productions relevant to a par-ticular sentence are generated on the fly.The naive encoding CFG in general requiresO(n5) parsing time with a conventional CKY pars-ing algorithm, since tracking the head annotations uand v multiplies the standard O(n3) CFG parse timerequirements by an additional factor proportional tothe O(n2) productions expanding Xu.An additional problem with the naive encodingis that the resulting CFG in general exhibits spuri-ous ambiguities, i.e., a single dependency parse maycorrespond to more than one CFG parse, as shownin Figure 2.
Informally, this is because the CFG per-mits left and the right dependencies to be arbitrarilyintermingled.4 Split-head encoding of PBDGsThere are several ways of removing the spurious am-biguities in the naive CFG encoding just described.This section presents a method we call the ?split-head encoding?, which removes the ambiguities andserves as starting point for the grammar transformsdescribed below.The split-head encoding represents each word uin the input string w by two unique terminals uland ur in the CFG parse.
A split-head CFG?s ter-minal vocabulary is ??
= {ul, ur : u ?
?
},where ?
is the set of terminals of the PBDG.
APBDG parse with yield w = (u1, .
.
.
, un) is trans-formed to a split-head CFG parse with yield w?
=(u1,l, u1,r, .
.
.
, un,l, un,r), so |w?| = 2|w|.Sthe dogXthe XdogXdogXgavegaveXgave XboneXaaXboneboneXgaveXSandySandyXgaveSthe dogXthe XdogXdogXboneXaaXboneboneXgaveXgavegaveXSandySandyXgaveXgaveFigure 2: Two parses using the naive CFG encod-ing that both correspond to the dependency parse ofFigure 1.The split-head CFG for a PBDG is given by thefollowing schemata:S ?
Xu where 0 uXu ?
Lu uR where u ?
?Lu ?
ulLu ?
Xv Lu where v uuR ?
uruR ?
uR Xv where u vThe dependency parse shown in Figure 1 corre-sponds to the split-head CFG parse shown in Fig-ure 3.
Each Xu expands to two new categories, Luand uR.
Lu consists of ul and all of u?s left descen-dants, while uR consists of ur and all of u?s rightdescendants.
The spurious ambiguity present in thenaive encoding does not arise in the split-head en-coding because the left and right dependents of ahead are assembled independently and cannot inter-mingle.As can be seen by examining the split-headschemata, the rightmost descendant of Lu is eitherLu or ul, which guarantees that the rightmost termi-nal dominated by Lu is always ul; similarly the left-most terminal dominated by uR is always ur.
Thus170dogRXSandyLSandySandylXdoggavergavelgaveRgaveRLaalaRarXa LbonebonelLbonebonerboneRXboneSandyRSandyrLgaveLgaveXgaveSgaveRLthetheltheRtherXthe LdogdoglLdogdogrFigure 3: The split-head parse corresponding to the dependency graph depicted in Figure 1.
Notice that ulis always the rightmost descendant of Lu and ur is always the leftmost descendant of uR, which means thatthese indices are redundant given the constituent spans.these subscript indices are redundant given the stringpositions of the constituents, which means we do notneed to track the index u in Lu and uR but can parsewith just the two categories L and R, and determinethe index from the constituent?s span when required.It is straight-forward to extend the split-head CFGto encode the additional state information requiredby the head automata of Eisner and Satta (1999);this corresponds to splitting the non-terminals Luand uR.
For simplicity we work with PBDGs in thispaper, but all of the Unfold-Fold transformations de-scribed below extend to split-head grammars withthe additional state structure required by head au-tomata.Implementation note: it is possible to directlyparse the ?undoubled?
input string w by modifyingboth the CKY algorithm and the CFGs describedin this paper.
Modify Lu and uR so they both ul-timately expand to the same terminal u, and special-case the implementation of production Xu ?
Lu uRand all productions derived from it to permit Lu anduR to overlap by the terminal u.The split-head formulation explains what initiallyseem unusual properties of existing PBDG algo-rithms.
For example, one of the standard ?sanitychecks?
for the Inside-Outside algorithm?that theoutside probability of each terminal is equal to thesentence?s inside probability?fails for these algo-rithms.
In fact, the outside probability of each ter-minal is double the sentence?s inside probability be-cause these algorithms implicitly collapse the twoterminals ul and ur into a single terminal u.5 A O(n3) split-head grammarThe split-head encoding described in the previoussection requires O(n4) parsing time because the in-dex v on Xv is not redundant.
We can obtain anequivalent grammar that only requires O(n3) pars-ing time by transforming the split-head grammar us-ing Unfold-Fold.
We describe the transformation onLu; the transformation of uR is symmetric.We begin with the definition of Lu in the split-head grammar above (?|?
separates the right-handsides of productions).Lu ?
ul | Xv Lu where v uOur first transformation step is to unfold Xv in Lu,i.e., replace Xv by its expansion, producing the fol-lowing definition for Lu (ignore the underlining fornow).Lu ?
ul | Lv vR Lu where v uThis removes the offending Xv in Lu, but the result-ing definition of Lu contains ternary productions andso still incurs O(n4) parse time.
To address this wedefine new nonterminals xMy for each x, y ?
?
:xMy ?
xR Lyand fold the underlined children in Lu into vMu:xMy ?
xR Ly where x, y ?
?Lu ?
ul | Lv vMu where v u171SdogrtherLdogtheRtheMdogthelLtheLdogdoglgavergaveRgaveMdoggaveldogRgaveRal araRbonelLboneaMboneLaLbonegaveMbone boneRbonergaveRLgaveLgaveSandyRSandyMgaveLSandySandyl SandyrFigure 4: The O(n3) split-head parse corresponding to the dependency graph of Figure 1.The O(n3) split-head grammar is obtained by un-folding the occurence of Xu in the S production anddropping the Xu schema as Xu no longer appears onthe right-hand side of any production.
The resultingO(n3) split-head grammar schemata are as follows:S ?
Lu uR where 0 uLu ?
ulLu ?
Lv vMu where v uuR ?
uruR ?
uMv vR where u vxMy ?
xR Ly where x, y ?
?As before, the dependency annotations on the pro-duction schemata permit us to map CFG parses tothe corresponding dependency parse.
This grammarrequires O(n3) parsing time to parse because the in-dices are redundant given the constituent?s string po-sitions for the reasons described in section 4.
Specif-ically, the rightmost terminal of Lu is always ul, theleftmost terminal of uR is always ur and the left-most and rightmost terminals of vMu are vl and urrespectively.The O(n3) split-head grammar is closely relatedto the O(n3) PBDG parsing algorithm given by Eis-ner and Satta (1999).
Specifically, the steps involvedin parsing with this grammar using the CKY algo-rithm are essentially the same as those performedby the Eisner/Satta algorithm.
The primary differ-ence is that the Eisner/Satta algorithm involves twoseparate categories that are collapsed into the singlecategory M here.To confirm their relative performance we imple-mented stochastic CKY parsers for the three CFGschemata described so far.
The production schematawere hard-coded for speed, and the implementationtrick described in section 4 was used to avoid dou-bling the terminal string.
We obtained dependencyweights from our existing discriminatively-trainedPBDG parser (not cited to preserve anonymity).
Wecompared the parsers?
running times on section 24of the Penn Treebank.
Because all three CFGs im-plement the same dependency grammar their Viterbiparses have the same dependency accuracy, namely0.8918.
We precompute the dependency weights,so the times include just the dynamic programmingcomputation on a 3.6GHz Pentium 4.CFG schemata sentences parsed / secondNaive O(n5) CFG 45.4O(n4) CFG 406.2O(n3) CFG 3580.06 An O(n3) adjacent-head grammarThis section shows how to further transform theO(n3) grammar described above into a form thatencodes second-order dependencies between ad-jacent dependent heads in much the way that aMarkov PCFG does (McDonald, 2006; McDonaldand Pereira, 2006).
We provide a derivation for theLu constituents; there is a parallel derivation for uR.We begin by unfolding Xv in the definition of Luin the split-head grammar, producing as before:Lu ?
ul | Lv vR LuNow introduce a new nonterminal vMLu, which is aspecialized version of M requiring that v is a left-dependent of u, and fold the underlined constituents172SthertheRtheMLdogthelLtheLdog dogRdogl dogrLboneLa aMLboneaRaralbonel bonergavergaveMRdog dogMbonegaveMRbone boneRgaveRgavelSandyrSandyRSandylSandyMLgaveLSandyLgaveFigure 5: The O(n3) adjacent-head parse corresponding to the dependency graph of Figure 1.
The boxedlocal tree indicates bone is the dependent of give following the dependent dog, i.e., give dog bone .into vMLu.vMLu ?
vR Lu where v uLu ?
ul | Lv vMLu where v uNow unfold Lu in the definition of vMLu, producing:vMLu ?
vR ul | vR Lv?
v?
MLu; v v?
uNote that in the first production expanding vMLu, vis the closest left dependent of u, and in the secondproduction v and v?
are adjacent left-dependents ofu.
vMLu has a ternary production, so we introducexMy as before to fold the underlined constituentsinto.xMy ?
xR Ly where x, y ?
?vMLu ?
vR ul | vMv?
v?MLu; v v?
uThe resulting grammar schema is as below, and asample parse is given in Figure 5.S ?
Lu uR where 0 uLu ?
ul u has no left dependentsLu ?
Lv vMLu v is u?s last left dep.vMLu ?
vR ul v is u?s closest left dep.vMLu ?
vMv?
v?MLu v v?
uuR ?
ur u has no right dependentsuR ?
uMRv vR v is u?s last right dep.uMRv ?
ur Lv v is u?s closest right dep.uMRv ?
uMRv?
v?Mv u v?
vxMy ?
xR Ly where x, y ?
?As before, the indices on the nonterminals are re-dundant, as the heads are always located at an edgeof each constituent, so they need not be computedor stored and the CFG can be parsed in O(n3) time.The steps involved in CKY parsing with this gram-mar correspond closely to those of the McDonald(2006) second-order PBDG parsing algorithm.7 An O(n3) dependent-head grammarThis section shows a different application of Unfold-Fold can capture head-to-head-to-head dependen-cies, i.e., ?vertical?
second-order dependencies,rather than the ?horizontal?
ones captured by thetransformation described in the previous section.Because we expect these vertical dependencies tobe less important linguistically than the horizontalones, we only sketch the transformation here.The derivation differs from the one in Section 6 inthat the dependent vR, rather than the head Lu, is un-folded in the initial definition of vMLu.
This results ina grammar that tracks vertical, rather than horizon-tal, second-order dependencies.
Since left-hand andright-hand derivations are assembled separately in asplit-head grammar, the grammar in fact only trackszig-zag type dependencies (e.g., where a grandpar-ent has a right dependent, which in turn has a leftdependent).The resulting grammar is given below, and a sam-ple parse using this grammar is shown in Figure 6.Because the subscripts are redundant they can beomitted and the resulting CFG can be parsed in173gaveMRbonegavergaveR LthethelgaveMthether doglLdogtheMLdogdogrdogRgaveR Laal argaveMa aMLboneLbonebonel bonerboneRgaveRLgavegavelSandyrSandyMLgaveSandylLSandygaveMRdogSLgaveFigure 6: The n3 dependent-head parse corresponding to the dependency graph of Figure 1.
The boxedlocal tree indicates that a is a left-dependent of bone, which is in turn a right-dependent of gave, i.e.,gave a bone .O(n3) time using the CKY algorithm.S ?
Lu uR where 0 uLu ?
ulLu ?
Lv vMLu where v uvMLu ?
vr Lu where v uvMLu ?
vMRw wMu where v w uuR ?
uruR ?
uMRv vR where u vuMRv ?
uR vl where u vuMRv ?
uMw wMLv where u w uxMy ?
xR Ly where x, y ?
?8 Maximum posterior decodingAs noted in the introduction, one consequence of thePBDG to CFG reductions presented in this paper isthat CFG parsing and estimation techniques are nowavailable for PBDGs as well.
As an example ap-plication, this section describes Maximum PosteriorDecoding (MPD) for PBDGs.Goodman (1996) observed that the Viterbi parseis in general not the optimal parse for evaluationmetrics such as f-score that are based on the numberof correct constituents in a parse.
He showed thatMPD improves f-score modestly relative to Viterbidecoding for PCFGs.Since dependency parse accuracy is just the pro-portion of dependencies in the parse that are correct,Goodman?s observation should hold for PBDG pars-ing as well.
MPD for PBDGs selects the parse thatmaximizes the sum of the marginal probabilities ofeach of the dependencies in the parse.
Such a de-coder might plausibly produce parses that score bet-ter on the dependency accuracy metric than Viterbiparses.MPD is straightforward given the PBDG to CFGreductions described in this paper.
Specifically, weuse the Inside-Outside algorithm to compute theposterior probability of the CFG constituents corre-sponding to each PBDG dependency, and then usethe Viterbi algorithm to find the parse tree that max-imizes the sum of these posterior probabilities.We implemented MPD for first-order PBDGsusing dependency weights from our existingdiscriminatively-trained PBDG parser (not cited topreserve anonymity).
These weights are estimatedby an online procedure as in McDonald (2006), andare not intended to define a probability distribution.In an attempt to heuristically correct for this, in thisexperiment we used exp(?wu,v) as the weight of thedependency between head u and dependent v, wherewu,v is the weight provided by the discriminatively-trained model and ?
is an adjustable scaling parame-ter tuned to optimize MPD accuracy on developmentdata.Unfortunately we found no significant differ-ence between the accuracy of the MPD and Viterbiparses.
Optimizing MPD on the development data(section 24 of the PTB) set the scale factor ?
=0.21 and produced MPD parses with an accuracyof 0.8921, which is approximately the same as theViterbi accuracy of 0.8918.
On the blind test data(section 23) the two accuracies are essentially iden-174tical (0.8997).There are several possible explanations for thefailure of MPD to produce more accurate parses thanViterbi decoding.
Perhaps MPD requires weightsthat define a probability distribution (e.g., a Max-Ent model).
It is also possible that discriminativetraining adjusts the weights in a way that ensuresthat the Viterbi parse is close to the maximum pos-terior parse.
This was the case in our experiment,and if this is true with discriminative training in gen-eral, then maximum posterior decoding will not havemuch to offer to discriminative parsing.9 ConclusionThis paper shows how to use the Unfold-Fold trans-form to translate PBDGs into CFGs that can beparsed in O(n3) time.
A key component of this isthe split-head construction, where each word u in theinput is split into two terminals ul and ur of the CFGparse.
We also showed how to systematically trans-form the split-head CFG into grammars which tracksecond-order dependencies.
We provided one gram-mar which captures horizontal second-order depen-dencies (McDonald, 2006), and another which cap-tures vertical second-order head-to-head-to-head de-pendencies.The grammars described here just scratch the sur-face of what is possible with Unfold-Fold.
Noticethat both of the second-order grammars have morenonterminals than the first-order grammar.
If one isprepared to increase the number of nonterminals stillfurther, it may be possible to track additional infor-mation about constituents (although if we insist onO(n3) parse time we will be unable to track the in-teraction of more than three heads at once).ReferencesR.M.
Burstall and John Darlington.
1977.
A transformationsystem for developing recursive programs.
Journal of theAssociation for Computing Machinery, 24(1):44?67.Zhiyi Chi.
1999.
Statistical properties of probabilistic context-free grammars.
Computational Linguistics, 25(1):131?160.Jason Eisner and Giorgio Satta.
1999.
Efficient parsing forbilexical context-free grammars and head automaton gram-mars.
In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics, pages 457?480,University of Maryland.Jason Eisner.
1996.
Three new probabilistic models for depen-dency parsing: An exploration.
In COLING96: Proceedingsof the 16th International Conference on Computational Lin-guistics, pages 340?345, Copenhagen.
Center for Sprogte-knologi.Jason Eisner.
2000.
Bilexical grammars and their cubic-timeparsing algorithms.
In Harry Bunt and Anton Nijholt, edi-tors, Advances in Probabilistic and Other Parsing Technolo-gies, pages 29?62.
Kluwer Academic Publishers.Joshua T. Goodman.
1996.
Parsing algorithms and metrics.
InProceedings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pages 177?183, Santa Cruz,Ca.K.
Lari and S.J.
Young.
1990.
The estimation of StochasticContext-Free Grammars using the Inside-Outside algorithm.Computer Speech and Language, 4(35-56).Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005.Probabilistic CFG with latent annotations.
In Proceedingsof the 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 75?82, Ann Arbor,Michigan, June.
Association for Computational Linguistics.David McAllester.
1999.
A reformulation of Eisner and Sata?scubic time parser for split head automata grammars.
Avail-able from http://ttic.uchicago.edu/?dmcallester/.Ryan McDonald and Fernando Pereira.
2006.
Online learn-ing of approximate dependency parsing algorithms.
In 11thConference of the European Chapter of the Association forComputational Linguistics, pages 81?88, Trento, Italy.Ryan McDonald.
2006.
Discriminative Training and SpanningTree Algorithms for Dependency Parsing.
Ph.D. thesis, Uni-versity of Pennyslvania, Philadelphia, PA.Fernando Pereira and Stuart M. Shieber.
1987.
Prolog and Nat-ural Language Analysis.
Center for the Study of Languageand Information, Stanford, CA.Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.2006.
Learning accurate, compact, and interpretable tree an-notation.
In Proceedings of the 21st International Confer-ence on Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics, pages433?440, Sydney, Australia, July.
Association for Computa-tional Linguistics.A.
Pettorossi and M. Proeitti.
1992.
Transformation of logicprograms.
In Handbook of Logic in Artificial Intelligence,volume 5, pages 697?787.
Oxford University Press.Taisuke Sato.
1992.
Equivalence-preserving first-order un-fold/fold transformation systems.
Theoretical Computer Sci-ence, 105(1):57?84.Daniel H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information and Control,10(2):189?208.175
