Proceedings of the Workshop on Information Extraction Beyond The Document, pages 48?55,Sydney, July 2006. c?2006 Association for Computational LinguisticsData Selection in Semi-supervised Learning for Name TaggingHeng Ji Ralph GrishmanDepartment of Computer ScienceNew York UniversityNew York, NY, 10003, USAhengji@cs.nyu.edu grishman@cs.nyu.eduAbstractWe present two semi-supervised learningtechniques to improve a state-of-the-artmulti-lingual name tagger.
For Englishand Chinese, the overall system obtains1.7% - 2.1% improvement in F-measure,representing a 13.5% - 17.4% relative re-duction in the spurious, missing, and in-correct tags.
We also conclude thatsimply relying upon large corpora is notin itself sufficient: we must pay attentionto unlabeled data selection too.
We de-scribe effective measures to automaticallyselect documents and sentences.1 IntroductionWhen applying machine learning approaches tonatural language processing tasks, it is time-consuming and expensive to hand-label the largeamounts of training data necessary for good per-formance.
Unlabeled data can be collected inmuch larger quantities.
Therefore, a natural ques-tion is whether we can use unlabeled data tobuild a more accurate learner, given the sameamount of labeled data.
This problem is oftenreferred to as semi-supervised learning.
It signifi-cantly reduces the effort needed to develop atraining set.
It has shown promise in improvingthe performance of many tasks such as name tag-ging (Miller et al, 2004), semantic class extrac-tion (Lin et al, 2003), chunking (Ando andZhang, 2005), coreference resolution (Bean andRiloff, 2004) and text classification (Blum andMitchell, 1998).However, it is not clear, when semi-supervisedlearning is applied to improve a learner, how thesystem should effectively select unlabeled data,and how the size and relevance of data impact theperformance.In this paper we apply two semi-supervisedlearning algorithms to improve a state-of-the-artname tagger.
We run the baseline name tagger ona large unlabeled corpus (bootstrapping) and thetest set (self-training), and automatically generatehigh-confidence machine-labeled sentences asadditional ?training data?.
We then iteratively re-train the model on the increased ?training data?.We first investigated whether we can improvethe system by simply using a lot of unlabeleddata.
By dramatically increasing the size of thecorpus with unlabeled data, we did get a signifi-cant improvement compared to the baseline sys-tem.
But we found that adding off-topicunlabeled data sometimes makes the performanceworse.
Then we tried to select relevant docu-ments from the unlabeled data in advance, andgot clear further improvements.
We also obtainedsignificant improvement by self-training (boot-strapping on the test data) without any additionalunlabeled data.Therefore, in contrast to the claim in (Bankoand Brill, 2001), we concluded that, for someapplications, effective use of large unlabeled cor-pora demands good data selection measures.
Wepropose and quantify some effective measures toselect documents and sentences in this paper.The rest of this paper is structured as follows.Section 2 briefly describes the efforts made byprevious researchers to use semi-supervisedlearning as well as the work of (Banko and Brill,2001).
Section 3 presents our baseline name tag-ger.
Section 4 describes the motivation for ourapproach while Section 5 presents the details oftwo semi-supervised learning methods.
Section 6presents and discusses the experimental resultson both English and Chinese.
Section 7 presentsour conclusions and directions for future work.2 Prior WorkThis work presented here extends a substantialbody of previous work (Blum and Mitchell, 1998;Riloff and Jones, 1999; Ando and Zhang, 2005)48that all focus on reducing annotation require-ments.
For the specific task of named entity an-notation, some researchers have emphasized thecreation of taggers from minimal seed sets(Strzalkowski and Wang, 1996; Collins andSinger, 1999; Lin et al, 2003) while another lineof inquiry (which we are pursuing) has sought toimprove on high-performance baseline taggers(Miller et al, 2004).Banko and Brill (2001) suggested that the de-velopment of very large training corpora may bemost effective for progress in empirical naturallanguage processing.
Their experiments show alogarithmic trend in performance as corpus sizeincreases without performance reaching an upperbound.
Recent work has replicated their work onthesaurus extraction (Curran and Moens, 2002)and is-a relation extraction (Ravichandran et al,2004), showing that collecting data over a verylarge corpus significantly improves system per-formance.
However, (Curran, 2002) and (Curranand Osborne, 2002) claimed that the choice ofstatistical model is more important than relyingupon large corpora.3 MotivationThe performance of name taggers has been lim-ited in part by the amount of labeled training dataavailable.
How can an unlabeled corpus help toaddress this problem?
Based on its original train-ing (on the labeled corpus), there will be sometags (in the unlabeled corpus) that the tagger willbe very sure about.
For example, there will becontexts that were always followed by a personname (e.g., "Capt.")
in the training corpus.
If wefind a new token T in this context in the unla-beled corpus, we can be quite certain it is a per-son name.
If the tagger can learn this fact aboutT, it can successfully tag T when it appears in thetest corpus without any indicative context.
In thesame way, if a previously-unseen context appearsconsistently in the unlabeled corpus beforeknown person names, the tagger should learn thatthis is a predictive context.We have adopted a simple learning approach:we take the unlabeled text about which the taggerhas greatest confidence in its decisions, tag it,add it to the training set, and retrain the tagger.This process is performed repeatedly to bootstrapourselves to higher performance.
This approachcan be used with any supervised-learning taggerthat can produce some reliable measure of confi-dence in its decisions.4 Baseline Multi-lingual Name TaggerOur baseline name tagger is based on an HMMthat generally follows the Nymble model (Bikelet al 1997).
Then it uses best-first search to gen-erate NBest hypotheses, and also computes themargin ?
the difference between the log prob-abilities of the top two hypotheses.
This is usedas a rough measure of confidence in our nametagging.1In processing Chinese, to take advantage ofname structures, we do name structure parsingusing an extended HMM which includes a largernumber of states (14).
This new HMM can han-dle name prefixes and suffixes, and transliteratedforeign names separately.
We also augmented theHMM model with a set of post-processing rulesto correct some omissions and systematic errors.The name tagger identifies three name types:Person (PER), Organization (ORG) and Geo-political (GPE) entities (locations which are alsopolitical units, such as countries, counties, andcities).5 Two Semi-Supervised Learning Meth-ods for Name TaggingWe have applied this bootstrapping approach totwo sources of data: first, to a large corpus ofunlabeled data and second, to the test set.
Todistinguish the two, we shall label the first "boot-strapping" and the second "self-training".We begin (Sections 5.1 and 5.2) by describingthe basic algorithms used for these two processes.We expected that these basic methods wouldprovide a substantial performance boost, but ourexperiments showed that, for best gain, the addi-tional training data should be related to the targetproblem, namely, our test set.
We present meas-ures to select documents (Section 5.3) and sen-tences (Section 5.4), and show (in Section 6) theeffectiveness of these measures.5.1 BootstrappingWe divided the large unlabeled corpus into seg-ments based on news sources and dates in orderto: 1) create segments of manageable size; 2)separately evaluate the contribution of each seg-ment (using a labeled development test set) andreject those which do not help; and 3) apply thelatest updated best model to each subsequent1 We have also used this metric in the context of rescoring ofname hypotheses (Ji and Grishman, 2005); Scheffer et al(2001) used a similar metric for active learning of name tags.49segment.
The procedure can be formalized asfollows.1.
Select a related set RelatedC from a large cor-pus of unlabeled data with respect to the test setTestT, using the document selection method de-scribed in section 5.3.2.
Split RelatedC into n subsets and mark themC1, C2?Cn.
Call the updated HMM name taggerNameM (initially the baseline tagger), and a de-velopment test set DevT.3.
For i=1 to n(1)  Run NameM on Ci;(2) For each tagged sentence S in Ci, if S istagged with high confidence, then keep S;otherwise remove S;(3) Relabel the current name tagger (NameM)as OldNameM, add Ci to the training data,and retrain the name tagger, producing anupdated model NameM;(4) Run NameM on DevT; if the performancegets worse, don?t use Ci and reset NameM= OldNameM;5.2 Self-trainingAn analogous approach can be used to tag thetest set.
The basic intuition is that the sentencesin which the learner has low confidence may getsupport from those sentences previously labeledwith high confidence.Initially, we build the baseline name taggerfrom the labeled examples, then gradually addthe most confidently tagged test sentences intothe training corpus, and reuse them for the nextiteration, until all sentences are labeled.
The pro-cedure can be formalized as follows.1.
Cluster the test set TestT into n clusters T1,T2, ?,Tn, by collecting document pairs with lowcross entropy (described in section 5.3.2) into thesame cluster.2.
For i=1 to n(1) NameM = baseline HMM name tagger;(2) While (there are new sentences tagged withconfidence higher than a threshold)a.
Run NameM on Ti;b.
Set an appropriate threshold for margin;c. For each tagged sentence S in Ti, if S istagged with high confidence, add S to thetraining data;d. Retrain the name tagger NameM withaugmented training data.At each iteration, we lower the threshold sothat about 5% of the sentences (with the largestmargin) are added to the training corpus.2  As anexample, this yielded the following graduallyimproving performance for one English clusterincluding 7 documents and 190 sentences.No.
ofiterationsNo.
ofsentencesaddedNo.
oftagschangedF-Measure0 0 0 91.41 37 28 91.92 69 22 92.13 107 21 92.44 128 11 92.65 146 9 92.76 163 8 92.87 178 6 92.88 190 0 92.8Table 1.
Incremental Improvement fromSelf-training (English)Self-training can be considered a cache modelvariant, operating across the entire test collection.But it uses confidence measures as weights foreach name candidate, and relies on names taggedwith high confidence to re-adjust the predictionof the remaining names, while in a cache model,all name candidates are equally weighted for vot-ing (independent of the learner?s confidence).5.3 Unlabeled Document SelectionTo further investigate the benefits of using verylarge corpora in bootstrapping, and also inspiredby the gain from the ?essence?
of self-training,which aims to gradually emphasize the predic-tions from related sentences within the test set,we reconsidered the assumptions of our approach.The bootstrapping method implicitly assumesthat the unlabeled data is reliable (not noisy) anduniformly useful, namely:2 To be precise, we repeatedly reduce the threshold by 0.1until an additional 5% or more of the sentences are included;however, if more than an additional 20% of the sentencesare captured because many sentences have the same margin,we add back 0.1 to the threshold.50?
The unlabeled data supports the acquisitionof new names and contexts, to provide newevidence to be incorporated in HMM and re-duce the sparse data problem;?
The unlabeled data won?t make the old esti-mates worse by adding too many nameswhose tags are incorrect, or at least are incor-rect in the context of the labeled training dataand the test data.If the unlabeled data is noisy or unrelated tothe test data, it can hurt rather than improve thelearner?s performance on the test set.
So it isnecessary to coarsely measure the relevance ofthe unlabeled data to our target test set.
We de-fine an IR (information retrieval) - style rele-vance measure between the test set TestT and anunlabeled document d as follows.5.3.1 ?Query set?
constructionWe model the information expected from theunlabeled data by a 'bag of words' technique.
Weconstruct a query term set from the test corpusTestT to check whether each unlabeled documentd is useful or not.?
We prefer not to use all the words in TestTas key words, since we are only concernedabout the distribution of name candidates.
(Adding off-topic documents may in fact in-troduce noise into the model).
For example,if one document in TestT talks about thepresidential election in France while d talksabout the presidential election in the US, theymay share many common words such as'election', ?voting?, 'poll', and ?camp?, but wewould expect more gain from other unlabeleddocuments talking about the French election,since they may share many name candidates.?
On the other hand it is insufficient to onlytake the name candidates in the top one hy-pothesis for each sentence (since we are par-ticularly concerned with tokens which mightbe names but are not so labeled in the tophypothesis).So our solution is to take all the name candi-dates in the top N best hypotheses for each sen-tence to construct a query set Q.5.3.2 Cross-entropy MeasureUsing Q, we compute the cross entropy H(TestT,d) between TestT and d by:???
?=QxdxprobTestTxprobdTestTH )|(log)|(),( 2where x is a name candidate in Q, andprob(x|TestT) is the probability (frequency) of xappearing in TestT while prob(x|d) is the prob-ability of x in d. If H(T, d) is smaller than athreshold then we consider d a useful unlabeleddocument3.5.4 Sentence SelectionWe don?t want to add all the tagged sentences ina relevant document to the training corpus be-cause incorrectly tagged or irrelevant sentencescan lead to degradation in model performance.The value of larger corpora is partly dependenton how much new information is extracted fromeach sentence of the unlabeled data compared tothe training corpus that we already have.The following confidence measures were ap-plied to assist the semi-supervised learning algo-rithm in selecting useful sentences for re-trainingthe model.5.4.1 Margin to find reliable sentencesFor each sentence, we compute the HMM hy-pothesis margin (the difference in log probabili-ties) between the first hypothesis and the secondhypothesis.
We select the sentences with marginslarger than a threshold4 to be added to the train-ing data.Unfortunately, the margin often comes downto whether a specific word has previously beenobserved in training; if the system has seen theword, it is certain, if not, it is uncertain.
There-fore the sentences with high margins are a mix ofinteresting and uninteresting samples.
We need toapply additional measures to remove the uninter-esting ones.
On the other hand, we may haveconfidence in a tagging due to evidence externalto the HMM, so we explored measures beyondthe HMM margin in order to recover additionalsentences.3 We also tried a single match method, using the query set tofind all the relevant documents that include any names be-longing to Q, and got approximately the same result ascross-entropy.
In addition to this relevance selection, weused one other simple filter: we removed a document if itincludes fewer than five names, because it is unlikely to benews.4 In bootstrapping, this margin threshold is selected by test-ing on the development set, to achieve more than 93% F-Measure.51Figure 1.
Bootstrapping for Name TaggingFigure 2.
Self-Training for Name TaggingData English ChineseBaselineTraining dataACE02,03,04 989,003 words Beijing Corpus +ACE03,04,051,460,648 wordsTotal 196,494 docs in Mar-Jun of 2003(69M words) from ACE05 unlabeled data41061 docs in Nov,Dec of 2000, and Janof 2001 (25M words) from ACE05 andTDT4 transcriptsSelectedDocs62584 docs (1,314,148 Sentences) 14,537 docs (222,359 sentences)UnlabeledDataSelectedSentences290,973 sentences (6,049,378 words) 55,385 sentences (1,128,505 words)Dev Set 20 ACE04 texts in Oct of 2000 90 ACE05 texts in Oct of 2000Test Set 20 ACE04 texts in Oct of 2000and 80 ACE05 texts in Mar-May of 2003(3093 names, 1205 PERs, 1021GPEs, 867ORGs)90 ACE05 texts in Oct of 2000(3093 names, 1013 PERs, 695 GPEs, 769ORGs)Table 2.
Data DescriptionC1 Ci ?
?Unlabeled DataCross-entropy based Document Selectioni=i+1Save Ti?
assystem outputT1 Ti ?
?Test SetCross-entropy based Document ClusteringTi??
Ti taggedwith NameMYesAdd Ti?
to training corpusRetrain NameMTi?
Empty?Ti??
sentencesselected from Ti?NoNameM ?
baseline taggeri?1i < n?Yesi?1NameM performsbetter on dev set?YesNoOldNameM ?
NameMCi?
?Ci tagged with NameMAdd Ci?
to training corpusRetrain NameMi=i+1i < n?Ci??
sentences selected from Ci?YesNameM ?
baseline taggerCn TnNameM ?
OldNameMSet margin threshold525.4.2 Name coreference to find more reliablesentencesNames introduced in an article are likely to bereferred to again, so a name coreferred to bymore other names is more likely to have beencorrectly tagged.
In this paper, we use simplecoreference resolution between names such assubstring matching and name abbreviation reso-lution.In the bootstrapping method we apply single-document coreference for each individual unla-beled text.
In self-training, in order to furtherbenefit from global contexts, we consider eachcluster of relevant texts as one single big docu-ment, and then apply cross-document coreference.Assume S is one sentence in the document, andthere are k names tagged in S: {N1, N2 .?..
Nk},which are coreferred to by {CorefNum1, Coref-Num2, ?CorefNumk} other names separately.Then we use the following average namecoreference count AveCoref as a confidencemeasure for tagging S:5?==kii kCorefNumAveCoref1/)(5.4.3 Name count and sentence length to re-move uninteresting sentencesIn bootstrapping on unlabeled data, the margincriterion often selects some sentences which aretoo short or don?t include any names.
Althoughthey are tagged with high confidence, they maymake the model worse if added into the trainingdata (for example, by artificially increasing theprobability of non-names).
In our experiments wedon?t use a sentence if it includes fewer than sixwords, or doesn?t include any names.5.5 Data FlowWe depict the above two semi-supervised learn-ing methods in Figure 1 and Figure 2.6 Evaluation Results and Discussions6.1 DataWe evaluated our system on two languages: En-glish and Chinese.
Table 2 shows the data used inour experiments.5 For the experiments reported here, sentences were selectedif AveCoref > 3.1 (or 3.1?number of documents for cross-document coreference) or the sentence margin exceeded themargin threshold.We present in section 6.2 ?
6.4 the overall per-formance of precision (P), recall (R) and F-measure (F) for both languages, and also somediagnostic experiment results.
For significancetesting (using the sign test), we split the test setinto 5 folders, 20 texts in each folder of English,and 18 texts in each folder of Chinese.6.2 Overall PerformanceTable 3 and Table 4 present the overall perform-ance6 by applying the two semi-supervised learn-ing methods, separately and in combination, toour baseline name tagger.Learner P R FBaseline 87.3 87.6 87.4Bootstrappingwith data selection88.2 88.6 88.4Self-training 88.1 88.4 88.2Bootstrapping with dataselection + Self-training89.089.289.1Table 3.
English Name TaggerLearner P R FBaseline 88.2 87.6 87.9Bootstrappingwith data selection89.8 89.5 89.6Self-training 89.5 88.3 88.9Bootstrapping with dataselection + Self-training90.289.790.0Table 4.
Chinese Name TaggerFor English, the overall system achieves a13.4% relative reduction on the spurious and in-correct tags, and 12.9% reduction in the missingrate.
For Chinese, it achieves a 16.9% relativereduction on the spurious and incorrect tags, and16.9% reduction in the missing rate.7 For each ofthe five folders, we found that both bootstrappingand self-training produced an improvement in Fscore for each folder, and the combination of twomethods is always better than each method alone.This allows us to reject the hypothesis that these6 Only names which exactly match the key in both extentand type are counted as correct; unlike MUC scoring, nopartial credit is given.7 The performance achieved should be considered in light ofhuman performance on this task.
The ACE keys used forthe evaluations were obtained by dual annotation and adju-dication.
A single annotator, evaluated against the key,scored F=93.6% to 94.1% for English and 92.5% to 92.7%for Chinese.
A second key, created independently by dualannotation and adjudication for a small amount of the Eng-lish data, scored F=96.5% against the original key.53improvements were random at a 95% confidencelevel.6.3 Analysis of Bootstrapping6.3.1 Impact of Data SizeFigure 3 and 4 below show the results as eachsegment of the unlabeled data is added to thetraining corpus.Figure 3.
Impact of Data Size (English)Figure 4.
Impact of Data Size (Chinese)We can see some flattening of the gain at theend, particularly for the larger English corpus,and that some segments do not help to boost theperformance (reflected as dips in the Dev Setcurve and gaps in the Test Set curve).6.3.2 Impact of Data SelectionIn order to investigate the contribution of docu-ment selection in bootstrapping, we performeddiagnostic experiments for Chinese, whose re-sults are shown in Table 5.
All the bootstrappingtests (rows 2 - 4) use margin for sentence selec-tion; row 4 augments this with the selectionmethods described in sections 5.4.2 and 5.4.3.Learner P R F(1) Baseline 88.2 87.6 87.9(2) (1) + Bootstrapping 88.9 88.7 88.8(3) (2) + DocumentSelection89.3 88.9 89.1(4) (3) + SentenceSelection89.8  89.5 89.6Table 5.
Impact of Data Selection (Chinese)Comparing row 2 with row 3, we find that notusing document selection, even though it multi-plies the size of the corpus, results in 0.3% lowerperformance (0.3-0.4% loss for each folder).
Thisleads us to conclude that simply relying uponlarge corpora is not in itself sufficient.
Effectiveuse of large corpora demands good confidencemeasures for document selection to remove off-topic material.
By adding sentence selection (re-sults in row 4) the system obtained 0.5% furtherimprovement in F-Measure (0.4-0.7% for eachfolder).
All improvements are statistically sig-nificant at the 95% confidence level.6.4 Analysis of Self-trainingWe have applied and evaluated different meas-ures to extract high-confidence sentences in self-training.
The contributions of these confidencemeasures to F-Measure are presented in Table 6.Confidence Measure English ChineseBaseline 87.4 87.9Margin 87.8 88.3Margin + single-docname coreference88.0 88.7Margin + cross-docname coreference88.2 88.9Table 6.
Impact of Confidence MeasuresIt shows that Chinese benefits more from add-ing name coreference, mainly because there aremore coreference links between name abbrevia-tions and full names.
And we also can see thatthe margin is an important measure for both lan-guages.
All differences are statistically signifi-cant at the 95% confidence level except for thegain using cross-document information for theChinese name tagging.7 Conclusions and Future WorkThis paper demonstrates the effectiveness of twostraightforward semi-supervised learning meth-ods for improving a state-of-art name tagger, and54investigates the importance of data selection forthis application.Banko and Brill (2001) suggested that the de-velopment of very large training corpora may becentral to progress in empirical natural languageprocessing.
When using large amounts of unla-beled data, as expected, we did get improvementby using unsupervised bootstrapping.
However,exploiting a very large corpus did not by itselfproduce the greatest performance gain.
Rather,we observed that good measures to select rele-vant unlabeled documents and useful labeled sen-tences are important.The work described here complements the ac-tive learning research described by (Scheffer etal., 2001).
They presented an effective activelearning approach that selects ?difficult?
(smallmargin) sentences to label by hand and then addto the training set.
Our approach selects ?easy?sentences ?
those with large margins ?
to addautomatically to the training set.
Combiningthese methods can magnify the gains possiblewith active learning.In the future we plan to try topic identificationtechniques to select relevant unlabeled docu-ments, and use the downstream information ex-traction components such as coreferenceresolution and relation detection to measure theconfidence of the tagging for sentences.
We arealso interested in applying clustering as a pre-processing step for bootstrapping.AcknowledgmentThis material is based upon work supported bythe Defense Advanced Research Projects Agencyunder Contract No.
HR0011-06-C-0023, and theNational Science Foundation under Grant IIS-00325657.
Any opinions, findings and conclu-sions expressed in this material are those of theauthors and do not necessarily reflect the viewsof the U. S. Government.ReferencesRie Ando and Tong Zhang.
2005.
A High-Performance Semi-Supervised Learning Methodsfor Text Chunking.
Proc.
ACL2005.
pp.
1-8.
AnnArbor, USAMichele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disam-biguation.
Proc.
ACL2001.
pp.
26-33.
Toulouse,FranceDavid Bean and Ellen Riloff.
2004.
UnsupervisedLearning of Contextual Role Knowledge forCoreference Resolution.
Proc.
HLT-NAACL2004.pp.
297-304.
Boston, USADaniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance Learning Name-finder.
Proc.
FifthConf.
on Applied Natural Language Processing.pp.194-201.
Washington D.C., USAAvrim Blum and Tom Mitchell.
1998.
CombiningLabeled and Unlabeled Data with Co-training.
Proc.of the Workshop on Computational Learning The-ory.
Morgan Kaufmann PublishersMichael Collins and Yoram Singer.
1999.
Unsuper-vised Models for Named Entity Classification.
Proc.of EMNLP/VLC-99.James R. Curran and Marc Moens.
2002.
Scaling con-text space.
Proc.
ACL 2002.
Philadelphia, USAJames R. Curran.
2002.
Ensemble Methods for Auto-matic Thesaurus Extraction.
Proc.
EMNLP 2002.Philadelphia, USAJames R. Curran and Miles Osborne.
2002.
A veryvery large corpus doesn?t always yield reliable es-timates.
Proc.
ACL 2002 Workshop on EffectiveTools and Methodologies for Teaching NaturalLanguage Processing and Computational Linguis-tics.
Philadelphia, USAHeng Ji and Ralph Grishman.
2005.
Improving NameTagging by Reference Resolution and Relation De-tection.
Proc.
ACL2005.
pp.
411-418.
Ann Arbor,USA.Winston Lin, Roman Yangarber and Ralph Grishman.2003.
Bootstrapping Learning of Semantic Classesfrom Positive and Negative Examples.
Proc.ICML-2003 Workshop on The Continuum from La-beled to Unlabeled Data.
Washington, D.C.Scott Miller, Jethran Guinness and Alex Zamanian.2004.
Name Tagging with Word Clusters and Dis-criminative Training.
Proc.
HLT-NAACL2004.
pp.337-342.
Boston, USADeepak Ravichandran, Patrick Pantel, and EduardHovy.
2004.
The Terascale Challenge.
Proc.
KDDWorkshop on Mining for and from the SemanticWeb (MSW-04).
pp.
1-11.
Seattle, WA, USAEllen Riloff and Rosie Jones.
1999.
Learning Diction-aries for Information Extraction by Multi-LevelBootstrapping.
Proc.
AAAI/IAAITobias Scheffer, Christian Decomain, and StefanWrobel.
2001.
Active Hidden Markov Models forInformation Extraction.
Proc.
Int?l Symposium onIntelligent Data Analysis (IDA-2001).Tomek Strzalkowski and Jin Wang.
1996.
A Self-Learning Universal Concept Spotter.
Proc.COLING.55
