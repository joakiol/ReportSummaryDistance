2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 152?161,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsSegmentation Similarity and AgreementChris FournierUniversity of OttawaOttawa, ON, Canadacfour037@eecs.uottawa.caDiana InkpenUniversity of OttawaOttawa, ON, Canadadiana@eecs.uottawa.caAbstractWe propose a new segmentation evaluationmetric, called segmentation similarity (S), thatquantifies the similarity between two segmen-tations as the proportion of boundaries thatare not transformed when comparing them us-ing edit distance, essentially using edit dis-tance as a penalty function and scaling penal-ties by segmentation size.
We propose severaladapted inter-annotator agreement coefficientswhich use S that are suitable for segmenta-tion.
We show that S is configurable enoughto suit a wide variety of segmentation evalua-tions, and is an improvement upon the state ofthe art.
We also propose using inter-annotatoragreement coefficients to evaluate automaticsegmenters in terms of human performance.1 IntroductionSegmentation is the task of splitting up an item, suchas a document, into a sequence of segments by plac-ing boundaries within.
The purpose of segmentingcan vary greatly, but one common objective is todenote shifts in the topic of a text, where multipleboundary types can also be present (e.g., major ver-sus minor topic shifts).
Human-competitive auto-matic segmentation methods can help a wide rangeof computational linguistic tasks which depend uponthe identification of segment boundaries in text.To evaluate automatic segmentation methods, amethod of comparing an automatic segmenter?s per-formance against the segmentations produced by hu-man judges (coders) is required.
Current meth-ods of performing this comparison designate onlyone coder?s segmentation as a reference to com-pare against.
A single ?true?
reference segmentationfrom a coder should not be trusted, given that inter-annotator agreement is often reported to be ratherpoor (Hearst, 1997, p. 54).
Additionally, to en-sure that an automatic segmenter does not over-fitto the preference and bias of one particular coder,an automatic segmenter should be compared directlyagainst multiple coders.The state of the art segmentation evaluation met-rics (Pk and WindowDiff) slide a window across adesignated reference and hypothesis segmentation,and count the number of windows where the numberof boundaries differ.
Window-based methods sufferfrom a variety of problems, including: i) unequalpenalization of error types; ii) an arbitrarily definedwindow size parameter (whose choice greatly af-fects outcomes); iii) lack of clear intuition; iv) in-applicability to multiply-coded corpora; and v) re-liance upon a ?true?
reference segmentation.In this paper, we propose a new method ofcomparing two segmentations, called segmentationsimilarity 1 (S), that: i) equally penalizes all er-ror types (unless explicitly configured otherwise);ii) appropriately responds to scenarios tested; iii) de-fines no arbitrary parameters; iv) is intuitive; andv) is adapted for use in a variety of popular inter-annotator agreement coefficients to handle multiply-coded corpora; and vi) does not rely upon a ?true?reference segmentation (it is symmetric).
Capitaliz-ing on the adapted inter-annotator agreement coeffi-cients, the relative difficulty that human segmentershave with various segmentation tasks can now bequantified.
We also propose that these coefficientscan be used to evaluate and compare automatic seg-mentation methods in terms of human agreement.This paper is organized as follows.
In Sec-tion 2, we review segmentation evaluation and inter-annotator agreement.
In Section 3, we present S and1A software implementation of segmentation similarity (S)is available at http://nlp.chrisfournier.ca/152inter-annotator agreement coefficient adaptations.
InSection 4, we evaluate S and WindowDiff in vari-ous scenarios and simulations, and upon a multiply-coded corpus.2 Related Work2.1 Segmentation EvaluationPrecision, recall, and their mean (F?-measure) havebeen previously applied to segmentation evaluation.Precision is the proportion of boundaries chosen thatagree with a reference segmentation, and recall isthe proportion of boundaries chosen that agree witha reference segmentation out of all boundaries in thereference and hypothesis (Pevzner and Hearst, 2002,p.
3).
For segmentation, these metrics are unsuitablebecause they penalize near-misses of boundaries asfull-misses, causing them to drastically overestimatethe error.
Near-misses are prevalent in segmentationand can account for a large proportion of the errorsproduced by a coder, and as inter-annotator agree-ment often shows, they do not reflect coder error,but the difficulty of the task.Pk (Beeferman and Berger, 1999, pp.
198?200)2is a window-based metric which attempts to solvethe harsh near-miss penalization of precision, recall,and F?-measure.
In Pk, a window of size k, wherek is defined as half of the mean reference segmentsize, is slid across the text to compute penalties.A penalty of 1 is assigned for each window whoseboundaries are detected to be in different segmentsof the reference and hypothesis segmentations, andthis count is normalized by the number of windows.Pevzner and Hearst (2002, pp.
5?10) highlighteda number of issues with Pk, specifically that: i) Falsenegatives (FNs) are penalized more than false pos-itives (FPs); ii) It does not penalize FPs that fallwithin k units of a reference boundary; iii) Its sen-sitivity to variations in segment size can cause it tolinearly decrease the penalty for FPs if the size ofany segments fall below k; and iv) Near-miss errorsare too harshly penalized.To attempt to mitigate the shortcomings of Pk,Pevzner and Hearst (2002, p. 10) proposed amodified metric which changed how penalties were2Pk is a modification of P?
(Beeferman et al, 1997, p. 43).Other modifications such as TDT Cseg (Doddington, 1998, pp.5?6) have been proposed, but Pk has seen greater usage.counted, named WindowDiff (WD).
A window ofsize k is still slid across the text, but now penal-ties are attributed to windows where the number ofboundaries in each segmentation differs (see Equa-tion 1, where b(Rij) and b(Hij) represents the num-ber of boundaries within the segments in a windowof size k from position i to j, and N the number ofsentences plus one), with the same normalization.WD(R,H) =1N ?
kN?k?i=1,j=i+k(|b(Rij)?b(Hij)| > 0) (1)WindowDiff is able to reduce, but not eliminate,sensitivity to segment size, gives more equal weightsto both FPs and FNs (FNs are, in effect, penalizedless3), and is able to catch mistakes in both smalland large segments.
It is not without issues though;Lamprier et al (2007) demonstrated that WindowD-iff penalizes errors less at the beginning and end ofa segmentation (this is corrected by padding the seg-mentation at each end by size k).
Additionally, vari-ations in the window size k lead to difficulties in in-terpreting and comparing WindowDiff?s values, andthe intuition of the method remains vague.Franz et al (2007) proposed measuring perfor-mance in terms of the number of words that are FNsand FPs, normalized by the number of word posi-tions present (see Equation 2).RFN =1N?wFN(w), RFP =1N?wFP (w) (2)RFN and RFP have the advantage that they takeinto account the severity of an error in terms of seg-ment size, allowing them to reflect the effects of er-roneously missing, or added, words in a segmentbetter than window based metrics.
Unfortunately,RFN and RFP suffer from the same flaw as preci-sion, recall, and F?-measure in that they do not ac-count for near misses.2.2 Inter-Annotator AgreementThe need to ascertain the agreement and reliabil-ity between coders for segmentation was recognized3Georgescul et al (2006, p. 48) note that both FPs and FNsare weighted by 1/N?k, and although there are ?equiprobablepossibilities to have a [FP] in an interval of k units?, ?the totalnumber of equiprobable possibilities to have a [FN] in an inter-val of k units is smaller than (N?k)?, making the interpretationof a full miss as a FN less probable than as a FP.153by Passonneau and Litman (1993), who adapted thepercentage agreement metric by Gale et al (1992,p.
254) for usage in segmentation.
This percentageagreement metric (Passonneau and Litman, 1993, p.150) is the ratio of the total observed agreement of acoder with the majority opinion for each boundaryover the total possible agreements.
This measurefailed to take into account chance agreement, or toless harshly penalize near-misses.Hearst (1997) collected segmentations from 7coders while developing the automatic segmenterTextTiling, and reported mean ?
(Siegel and Castel-lan, 1988) values for coders and automatic seg-menters (Hearst, 1997, p. 56).
Pairwise mean ?scores were calculated by comparing a coder?s seg-mentation against a reference segmentation formu-lated by the majority opinion strategy used in Pas-sonneau and Litman (1993, p. 150) (Hearst, 1997,pp.
53?54).
Although mean ?
scores attempt totake into account chance agreement, near misses arestill unaccounted for, and use of Siegel and Castel-lan?s (1988) ?
has declined in favour of other coeffi-cients (Artstein and Poesio, 2008, pp.
555?556).Artstein and Poesio (2008) briefly touch uponrecommendations for coefficients for segmentationevaluation, and though they do not propose a mea-sure, they do conjecture that a modification of aweighted form of ?
(Krippendorff, 1980; Krippen-dorff, 2004) using unification and WindowDiff maysuffice (Artstein and Poesio, 2008, pp.
580?582).3 Segmentation SimilarityFor discussing segmentation, a segment?s size (ormass) is measured in units, the error is quantifiedin potential boundaries (PBs), and we have adopteda modified form of the notation used by Artstein andPoesio (2008), where the set of:?
Items is {i|i ?
I} with cardinality i;?
Categories is {k|k ?
K} with cardinality k;?
Coders is {c|c ?
C} with cardinality c;?
Segmentations of an item i by a coder c is {s|s ?S}, where when sic is specified with only one sub-script, it denotes sc, for all relevant items (i); and?
Types of segmentation boundaries is {t|t ?
T} withcardinality t.3.1 Sources of DissimilarityLinear segmentation has three main types of errors:1. s1 contains a boundary that is off by n PBs in s2;2. s1 contains a boundary that s2 does not; or3.
s2 contains a boundary that s1 does not.These types of errors can be seen in Figure 1, andare conceptualized as a pairwise transposition of aboundary for error 1, and the insertion or deletion(depending upon your perspective) of a boundary forerrors 2 and 3.
Since we do not designate either seg-mentation as a reference or hypothesis, we refer toinsertions and deletions both as substitutions.s1s21 3 2Figure 1: Types of segmentations errorsIt is important to not penalize near misses as fullmisses in many segmentation tasks because codersoften agree upon the existence of a boundary, butdisagree upon its exact location.
In the previous sce-nario, assigning a full miss would mean that even aboundary loosely agreed-upon, as in Figure 1, error1, would be regarded as completely disagreed-upon.3.2 Edit DistanceIn S, concepts from Damereau-Levenshtein edit dis-tance (Damereau, 1964; Levenshtein, 1966) are ap-plied to model segmentation edit distance as two op-erations: substitutions and transpositions.4 Thesetwo operations represent full misses and near misses,respectively.
Using these two operations, a newglobally-optimal minimum edit distance is appliedto a pair of sequences of sets of boundaries to modelthe sources of dissimilarity identified earlier.5Near misses that are remedied by transposition arepenalized as b PBs of error (where b is the numberof boundaries transposed), as opposed to the 2b PBsof errors by which they would be penalized if theywere considered to be two separate substitution op-erations.
Transpositions can also be considered overn > 2 PBs (n-wise transpositions).
This is usefulif, for a specific task, near misses of up to n PBs arenot to be penalized as full misses (default n = 2).The error represented by the two operations canalso be scaled (i.e., weighted) from 1 PB each to a4Beeferman et al (1997, p. 42) briefly mention using an editdistance without transpositions, but discard it in favour of P?.5For multiple boundaries, an add/del operation is added, andtranspositions are considered only within boundary types.154fraction.
The distance over which an n-wise trans-position occurred can also be used in conjunctionwith the scalar operation weighting so that a transpo-sition is weighted using the function in Equation 3.te(n, b) = b?
(1/b)n?2 where n ?
2 and b > 0 (3)This transposition error function was chosen sothat, in an n-wise transposition where n = 2 PBsand the number of boundaries transposed b = 2, thepenalty would be 1 PB, and the maximum penalty aslimn??
te(n) would be b PBs, or in this case 2 PBs(demonstrated later in Figure 5b).3.3 MethodIn S, we conceptualize the entire segmentation, andindividual segments, as having mass (i.e., unit mag-nitude/length), and quantify similarity between twosegmentations as the proportion of boundaries thatare not transformed when comparing segmentationsusing edit distance, essentially using edit distance asa penalty function and scaling penalties by segmen-tation size.
S is a symmetric function that quantifiesthe similarity between two segmentations as a per-centage, and applies to any granularity or segmenta-tion unit (e.g., paragraphs, sentences, clauses, etc.
).Consider a somewhat contrived examplecontaining?for simplicity and brevity?only oneboundary type (t = 1).
First, a segmentation mustbe converted into a sequence of segment massvalues (see Figure 2).0 1 2 3 4 5 6?
1 3 2Figure 2: Annotation of segmentation massThen, a pair of segmentations are converted intoparallel sequences of boundary sets, where each setcontains the types of boundaries present at that po-tential boundary location (if there is no boundarypresent, then the set is empty), as in Figure 3.s1s21 2 2 3 3 1 2{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}1 2 1 2 6 2{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}Figure 3: Segmentations annotated with mass and theircorresponding boundary set sequencesThe edit distance is calculated by first identify-ing all potential substitution operations that couldoccur (in this case 5).
A search for all potential n-wise transpositions that can be made over n adja-cent sets between the sequences is then performed,searching from the beginning of the sequence to theend, keeping only those transpositions which do notoverlap and which result in transposing the mostboundaries between the sequences (to minimize theedit distance).
In this case, we have only one non-overlapping 2-wise transposition.
We then subtractthe number of boundaries involved in transpositionsbetween the sequences (2 boundaries) from the num-ber of substitutions, giving us an edit distance of 4PBs: 1 transposition PB and 3 substitution PBs.s1s21 2 2 3 3 1 2{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}1 2 1 2 6 2{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}TranspositionSub.
Sub.
Sub.Figure 4: Edit operations performed on boundary setsEdit distance, and especially the number of oper-ations of each type performed, is useful in identi-fying the number of full and near misses that haveoccurred?which indicates whether one?s choice oftransposition window size n is either too generousor too harsh.
Edit distance as a penalty does notincorporate information on the severity of an errorwith respect to the size of a segment, and is not aneasily comparable value without some form of nor-malization.
To account for these issues, we defineS so that boundary edit distance is used to subtractpenalties for each edit operation that occurs, fromthe number of potential boundaries in a segmenta-tion, normalizing this value by the total number ofpotential boundaries in a segmentation.S(si1, si2) =t ?mass(i)?
t?
d(si1, si1, T )t ?mass(i)?
t(4)S, as shown in Equation 4, scales the mass of theitem by the cardinality of the set of boundary types(t) because the edit distance function d(si1, si1, T )will return a value of [0, t ?
mass(i)] PBs, wheret ?
Z+?while subtracting the edit distance and t.66The number of potential boundaries in a segmentation si155The numerator is normalized by the total numberof potential boundaries per boundary type.
This re-sults in a function with a range of [0, 1].
It returns 0when one segmentation contains no boundaries, andthe other contains the maximum number of possibleboundaries.
It returns 1 when both segmentationsare identical.Using the default configuration of this equation,S = 9/13 = 0.6923, a very low similarity, whichWindowDiff also agrees upon (1?WD = 0.6154).The edit-distance function d(si1, si1, T ) can also beassigned values of the range [0, 1] as scalar weights(wsub, wtrp) to reduce the penalty attributed to par-ticular edit operations, and configured to use a trans-position error function (Equation 3, used by default).3.4 Evaluating Automatic SegmentersCoders often disagree in segmentation tasks (Hearst,1997, p. 56), making it improbable that a single,correct, reference segmentation could be identifiedfrom human codings.
This improbability is the re-sult of individual coders adopting slightly differ-ent segmentation strategies (i.e., different granular-ity).
In light of this, we propose that the best avail-able evaluation strategy for automatic segmentationmethods is to compare performance against multiplecoders directly, so that performance can be quanti-fied relative to human reliability and agreement.To evaluate whether an automatic segmenterperforms on par with human performance, inter-annotator agreement can be calculated with andwithout the inclusion of an automatic segmenter,where an observed drop in the coefficients wouldsignify that the automatic segmenter does not per-form as reliably as the group of human coders.7 Thiscan be performed independently for multiple auto-matic segmenters to compare them to each other?assuming that the coefficients model chance agree-ment appropriately?because agreement is calculated(and quantifies reliability) over all segmentations.3.5 Inter-Annotator AgreementSimilarity alone is not a sufficiently insightful mea-sure of reliability, or agreement, between coders.with t boundary types is t ?mass(i)?
t.7Similar to how human competitiveness is ascertained byMedelyan et al (2009, pp.
1324?1325) and Medelyan (2009,pp.
143?145) by comparing drops in inter-indexer consistency.Chance agreement occurs in segmentation whencoders operating at slightly different granularitiesagree due to their codings, and not their own in-nate segmentation heuristics.
Inter-annotator agree-ment coefficients have been developed that assume avariety of prior distributions to characterize chanceagreement, and to attempt to offer a way to iden-tify whether agreement is primarily due to chance,or not, and to quantify reliability.Artstein and Poesio (2008) note that most of acoder?s judgements are non-boundaries.
The classimbalance caused by segmentations often contain-ing few boundaries, paired with no handling of nearmisses, causes most inter-annotator agreement co-efficients to drastically underestimate agreement onsegmentations.
To allow for agreement coefficientsto account for near misses, we have adapted S for usewith Cohen?s ?, Scott?s pi, Fleiss?s multi-pi (pi?
), andFleiss?s multi-?
(??
), which are all coefficients thatrange from [Ae/1?Ae , 1], where 0 indicates chanceagreement, and 1 perfect agreement.
All four coeffi-cients have the general form:?, pi, ?
?, and pi?
=Aa ?
Ae1?
Ae(5)For each agreement coefficient, the set of cate-gories is defined as solely the presence of a bound-ary (K = {segt|t ?
T}), per boundary type (t).This category choice is similar to those chosen byHearst (1997, p. 53), who computed chance agree-ment in terms of the probability that coders wouldsay that a segment boundary exists (segt), and theprobability that they would not (unsegt).
We havechosen to model chance agreement only in terms ofthe presence of a boundary, and not the absence,because coders have only two choices when seg-menting: to place a boundary, or not.
Coders donot place non-boundaries.
If they do not make achoice, then the default choice is used: no boundary.This default option makes it impossible to determinewhether a segmenter is making a choice by not plac-ing a boundary, or whether they are not sure whethera boundary is to be placed.8 For this reason, weonly characterize chance agreement between codersin terms of one boundary presence category per type.8This could be modelled as another boundary type, whichwould be modelled in S by the set of boundary types T .1563.5.1 Scott?s piProposed by Scott (1955), pi assumes that chanceagreement between coders can be characterized asthe proportion of items that have been assigned tocategory k by both coders (Equation 7).
We cal-culate agreement (Apia ) as pairwise mean S (scaledby each item?s size) to enable agreement to quantifynear misses leniently, and chance agreement (Apie )can be calculated as in Artstein and Poesio (2008).Apia =?i?I mass(i) ?
S(si1, si2)?i?I mass(i)(6)Apie =?k?K(Ppie (k))2(7)We calculate chance agreement per category asthe proportion of boundaries (segt) assigned by allcoders over the total number of potential boundariesfor segmentations, as shown in Equation 8.Ppie (segt) =?c?C?i?I |boundaries(t, sic)|c ??i?I(mass(i)?
1) (8)This adapted coefficient appropriately estimateschance agreement in situations where there no in-dividual coder bias.3.5.2 Cohen?s ?Proposed by Cohen (1960), ?
characterizeschance agreement as individual distributions percoder, calculated as shown in Equations 9-10 usingour definition of agreement (Apia ) as shown earlier.A?a = Apia (9)A?e =?k?KP?e (k|c1) ?
P?e (k|c2) (10)We calculate category probabilities as in Scott?spi, but per coder, as shown in Equation 11.P?e (segt|c) =?i?I |boundaries(t, sic)|?i?I(mass(i)?
1) (11)This adapted coefficient appropriately estimateschance agreement for segmentation evaluationswhere coder bias is present.3.5.3 Fleiss?s Multi-piProposed by Fleiss (1971), multi-pi (pi?)
adaptsScott?s pi for multiple annotators.
We use Artsteinand Poesio?s (2008, p. 564) proposal for calculat-ing actual and expected agreement, and because allcoders rate all items, we express agreement as pair-wise mean S between all coders as shown in Equa-tions 12-13, adapting only Equation 12.Api?a =1(c2)c?1?m=1c?n=m+1?i?I mass(i) ?
S(sim, sin)?i?I(mass(i)?
1) (12)Api?e =?k?K(Ppie (k))2(13)3.5.4 Fleiss?s Multi-?Proposed by Davies and Fleiss (1982), multi-?(??)
adapts Cohen?s ?
for multiple annotators.
Weuse Artstein and Poesio?s (2008, extended version)proposal for calculating agreement just as in pi?, butwith separate distributions per coder as shown inEquations 14-15.A?
?a = Api?a (14)A?
?e =?k?K(1(c2)c?1?m=1c?n=m+1P?e (k|cm) ?
P?e (k|cn))(15)3.6 Annotator BiasTo identify the degree of bias in a group of coders?segmentations, we can use a measure of varianceproposed by Artstein and Poesio (2008, p. 572) thatis quantified in terms of the difference between ex-pected agreement when chance is assumed to varybetween coders, and when it is assumed to not.B = Api?e ?A?
?e (16)4 ExperimentsTo demonstrate the advantages of using S, as op-posed to WindowDiff (WD), we compare both met-rics using a variety of contrived scenarios, and thencompare our adapted agreement coefficients againstpairwise meanWD9 for the segmentations collectedby Kazantseva and Szpakowicz (2012).In this section, because WD is a penalty-basedmetric, it is reported as 1?WD so that it is easierto compare against S values.
When reported in thisway, 1?WD and S both range from [0, 1], where 1represents no errors and 0 represents maximal error.9Permuted, and with window size recalculated for each pair.1570 20 40 60 80 10000.20.40.60.81Number of full misses / false positives1?WDS(a) Increasing the number of full misses,or FPs, where k = 25 for WD0 2 4 6 8 100.70.80.91Distance between boundaries in each seg.
(units)1?WDS(n = 3)S(n = 5,scale)S(n = 5,wtrp = 0)(b) Increasing the distance between twoboundaries considered to be a near missuntil metrics consider them a full miss0 20 40 60 80 1000.20.40.60.81Segmentation mass (m)1?WDSk/m(c) Increasing the mass m of segmenta-tions configured as shown in Figure 10showing the effect of k on 1?WDFigure 5: Responses of 1?WD and S to various segmentation scenarios4.1 Segmentation CasesMaximal versus minimal segmentation Whenproposing a new metric, its reactions to extremamust be illustrated, for example when a maximalsegmentation is compared to a minimal segmenta-tion, as shown in Figure 6.
In this scenario, both1?WD and S appropriately identify that this caserepresents maximal error, or 0.
Though not shownhere, both metrics also report a similarity of 1.0when identical segmentations are compared.s1s2141 1 1 1 1 1 1 1 1 1 1 1 1 1Figure 6: Maximal versus minimal seg.
massesFull misses For the most serious source of error,full misses (i.e., FPs and FNs), both metrics appro-priately report a reduction in similarity for casessuch as Figure 7 that is very similar (1?WD =0.8462, S= 0.8461).
Where the two metrics differis when this type of error is increased.s1s21 2 2 2 4 2 11 2 8 2 1Figure 7: Full misses in seg.
massesS reacts to increasing full misses linearly, whereasWindowDiff can prematurely report a maximalnumber of errors.
Figure 5a demonstrates this ef-fect, where for each iteration we have taken seg-mentations of 100 units of mass with one matchingboundary at the first hypothesis boundary position,and uniformly increased the number of internal hy-pothesis segments, giving us 1 matching boundary,and [0, 98] FPs.
This premature report of maximalerror (at 7 FP) by WD is caused by the window size(k = 25) being greater than all of the internal hy-pothesis segment sizes, making all windows penal-ized for containing errors.Near misses When dealing with near misses, thevalues of both metrics drop (1?WD = 0.8182,S = 0.9231), but to greatly varying degrees.
Incomparison to full misses, WindowDiff penalizes anear miss, like that in Figure 8, far more than S.This difference is due to the distance between thetwo boundaries involved in a near miss; S shows,in this case, 1 PB of error until it is outside of then-wise transposition window (where n = 2 PBs),at which point it is considered an error of not onetransposition, but two substitutions (2 PBs).s1s26 87 7Figure 8: Near misses in seg.
massesIf we wanted to completely forgive near missesup to n PBs, we could set the weighting of trans-positions in S to wtrp = 0.
This is useful if a spe-cific segmentation task accepts that near misses arevery probable, and that there is little cost associatedwith a near miss in a window of n PBs.
We canalso set n to a high number, i.e., 5 PBs, and use thescaled transposition error (te) function (Equation 3)to slowly increase the error from b = 1 PB to b = 2PBs, as shown in Figure 5b, which shows how both158Scenario 1: FN, p = 0.5 Scenario 2: FP, p = 0.5 Scenario 3: FP and FN, p = 0.5(20,30) (15,35) (20,30) (15,35) (20,30) (15,35)WD 0.2340?
0.0113 0.2292?
0.0104 0.2265?
0.0114 0.2265?
0.0111 0.3635?
0.0126 0.3599?
0.0117S 0.9801?
0.0006 0.9801?
0.0006 0.9800?
0.0006 0.9800?
0.0006 0.9605?
0.0009 0.9603?
0.0009(10,40) (5,45) (10,40) (5,45) (10,40) (5,45)WD 0.2297?
0.0105 0.2206?
0.0079 0.2256?
0.0102 0.2184?
0.0069 0.3516?
0.0110 0.3254?
0.0087S 0.9799?
0.0007 0.9796?
0.0007 0.9800?
0.0006 0.9796?
0.0007 0.9606?
0.0010 0.9598?
0.0011Table 1: Stability of mean (with standard deviation) values of WD and S in three different scenarios, each definingthe: probability of a false positive (FP), false negative (FN), or both.
Each scenario varies the range of internalsegment sizes (e.g., (20, 30)).
Low standard deviation and similar within-scenario means demonstrates low sensitivityto variations in internal segment size.metrics react to increases in the distance between anear miss in a segment of 25 units.
These configura-tions are all preferable to the drop of 1?WD.4.2 Segmentation Mass Scale EffectsIt is important for a segmentation evaluation met-ric to take into account the severity of an error interms of segment size.
An error in a 100 unit seg-ment should be considered less severe than an er-ror in a 2 unit segment, because an extra boundaryplaced within a 100 unit segment (e.g., Figure 9 withm = 100) could probably indicate a weak boundary,whereas in a 4 unit segment the probability that anextra boundary exists right next to two agreed-uponboundaries should be small for most tasks, meaningthat it is probable that the extra boundary is an error,and not a weak boundary.s1s2m/4m/2m/4m/4m/4m/4m/4Figure 9: Two segmentations of mass m with a full missTo demonstrate that S is sensitive to segment size,Figure 5c shows how S and 1?WD respond whencomparing segmentations configured as shown inFigure 10 (containing one match and one full miss)with linearly increasing mass (4 ?
m ?
100).1?WD will eventually indicate 0.68, whereas S ap-propriately discounts the error as mass is increased,approaching 1 as limm??.
1?WD behaves in thisway because of how it calculates its window size pa-rameter, k, which is plotted as k/m to show how itsvalue influences 1?WD.s1s2m/4 m?
(m/4)m/4m/4m/2Figure 10: Two segmentations of mass m compared withincreasing m in Figure 5c (s1 as reference)4.3 Variation in Segment SizesWhen Pevzner and Hearst (2002) proposed WD,they demonstrated that it was not as sensitive asPk to variations in the size of segments inside asegmentation.
To show this, they simulated howWD performs upon a segmentation comprised of1000 segments with four different uniformly dis-tributed ranges of internal segment sizes (keepingthe mean at approximately 25 units) in compari-son to a hypothesis segmentation with errors (falsepositives, false negatives, and both) uniformly dis-tributed within segments (Pevzner and Hearst, 2002,pp.
11?12).
10 trials were performed for each seg-ment size range and error probability, with 100 hy-potheses generated per trial.
Recreating this simu-lation, we compare the stability of S in comparisonto WD, as shown in Table 1.
We can see that WDvalues show substantial within-scenario variation foreach segment size range, and larger standard devia-tions, than S.4.4 Inter-Annotator Agreement CoefficientsHere, we demonstrate the adapted inter-annotatoragreement coefficients upon topical paragraph-levelsegmentations produced by 27 coders of 20 chaptersfrom the novel The Moonstone by Wilkie Collinscollected by Kazantseva and Szpakowicz (2012).Figure 11 shows a heat map of each chapter wherethe percentage of coders who agreed upon each po-tential boundary is represented.
Comparing this heatmap to the inter-annotator agreement coefficients inTable 2 allows us to better understand why certainchapters have lower reliability.Chapter 1 has the lowest pi?S score in the table, andalso the highest bias (BS).
One of the reasons forthis low reliability can be attributed to the chapter?ssmall mass (m) and few coders (|c|), which makesit more sensitive to chance agreement.
Visually, the159123457891011121314151617181920211.00.0ChaptersPotential boundary positions (between paragraphs)Coderagreementperpotentialboundary(%)Figure 11: Heat maps for the segmentations of each chap-ter showing the percentage of coders who agree uponboundary positions (darker shows higher agreement)predominance of grey indicates that, although thereare probably two boundaries, their exact location isnot very well agreed upon.
In this case, 1?WDincorrectly indicates the opposite, that this chaptermay have relatively moderate reliability, because itis not corrected for chance agreement.1?WD indicates that the lowest reliability isfound in Chapter 19. pi?S indicates that this is oneof the higher agreement chapters, and looking at theheat map, we can see that it does not contain anystrongly agreed upon boundaries.
In this chapter,there is little opportunity to agree by chance due tothe low number of boundaries (|b|) placed, and be-cause the judgements are tightly clustered in a fairamount of mass, the S component of pi?S appropri-ately takes into account the near misses observedand gives it a high reliability score.Chapter 17 received the highest pi?S in the table,which is another example of how tight clustering ofboundary choices in a large mass leads pi?S to appro-priately indicate high reliability despite that there arenot as many individual highly-agreed-upon bound-aries, whereas 1?WD indicates that there is low re-liability.
1?WD and pi?S both agree, however, thatchapter 16 has high reliability.Despite WindowDiff?s sensitivity to near misses,it is evident that its pairwise mean cannot be usedto consistently judge inter-annotator agreement, orreliability.
S demonstrates better versatility whenaccounting for near misses, and when used as partof inter-annotator agreement coefficients, it prop-erly takes into account chance agreement.
Follow-ing Artstein and Poesio?s (2008, pp.
590?591) rec-Ch.
pi?S ?
?S BS 1?WD |c| |b| m1 0.7452 0.7463 0.0039 0.6641?
0.1307 4 13 132 0.8839 0.8840 0.0009 0.7619?
0.1743 6 20 153 0.8338 0.8340 0.0013 0.6732?
0.1559 4 23 384 0.8414 0.8417 0.0019 0.6019?
0.2245 4 25 465 0.8773 0.8774 0.0003 0.6965?
0.1106 6 34 427 0.8132 0.8133 0.0002 0.6945?
0.1822 6 20 158 0.8495 0.8496 0.0006 0.7505?
0.0911 6 48 399 0.8104 0.8105 0.0009 0.6502?
0.1319 6 35 3310 0.9077 0.9078 0.0002 0.7729?
0.0770 6 56 8311 0.8130 0.8135 0.0022 0.6189?
0.1294 4 73 11112 0.9178 0.9178 0.0001 0.6504?
0.1277 6 40 10213 0.9354 0.9354 0.0002 0.5660?
0.2187 6 21 5814 0.9367 0.9367 0.0001 0.7128?
0.1744 6 35 7015 0.9344 0.9344 0.0001 0.7291?
0.0856 6 40 9716 0.9356 0.9356 0.0000 0.8016?
0.0648 6 41 6917 0.9447 0.9447 0.0002 0.6717?
0.2044 5 23 7018 0.8921 0.8922 0.0005 0.5998?
0.1614 5 28 5919 0.9021 0.9022 0.0009 0.4796?
0.2666 5 15 3620 0.8590 0.8591 0.0003 0.6657?
0.1221 6 21 2121 0.9286 0.9286 0.0004 0.6255?
0.2003 5 17 60Table 2: S-based inter-annotator agreements and pairwisemean 1?WD and standard deviation with the number ofcoders, boundaries, and mass per chapterommendation, and given the low bias (mean codergroupBS = 0.0061?0.0035), we propose reportingreliability using pi?
for this corpus, where the meancoder group pi?S for the corpus is 0.8904 ?
0.0392(counting 1039 full and 212 near misses).5 Conclusion and Future WorkWe have proposed a segmentation evaluation met-ric which solves the key problems facing segmenta-tion analysis today, including an inability to: appro-priately quantify near misses when evaluating auto-matic segmenters and human performance; penalizeerrors equally (or, with configuration, in a mannerthat suits a specific segmentation task); compare anautomatic segmenter directly against human perfor-mance; require a ?true?
reference; and handle mul-tiple boundary types.
Using S, task-specific eval-uation of automatic and human segmenters can beperformed using multiple human judgements unhin-dered by the quirks of window-based metrics.In current and future work, we will show how Scan be used to analyze hierarchical segmentations,and illustrate how to apply S to linear segmentationscontaining multiple boundary types.AcknowledgmentsWe thank Anna Kazantseva for her invaluable feed-back and corpora, and Stan Szpakowicz, Martin Sca-iano, and James Cracknell for their feedback.160ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.
MIT Press, Cam-bridge, MA, USA.Doug Beeferman, Adam Berger, and John Lafferty.1997.
Text Segmentation Using Exponential Models.Proceedings of the Second Conference on EmpiricalMethods in Natural Language Processing, 2:35?46.Association for Computational Linguistics, Strouds-burg, PA, USA.Doug Beeferman and Adam Berger.
1999.
Statisti-cal models for text segmentation.
Machine learning,34(1?3):177?210.
Springer Netherlands, NL.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and Psychological Mea-surement, 20(1):37?46.
Sage, Beverly Hills, CA, USA.Frederick J. Damerau.
1964.
A technique for computerdetection and correction of spelling errors.
Commu-nications of the ACM, 7(3):171?176.
Association forComputing Machinery, Stroudsburg, PA, USA.Mark Davies and Joseph L. Fleiss.
1982.
Measur-ing agreement for multinomial data.
Biometrics,38(4):1047?1051.
Blackwell Publishing Inc, Oxford,UK.George R. Doddington.
1998.
The topic detection andtracking phase 2 (TDT2) evaluation plan.
DARPABroadcast News Transcription and UnderstandingWorkshop, pp.
223?229.
Morgan Kaufmann, Waltham,MA, USA.Joseph L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.
American Psychological Association,Washington, DC, USA.Martin, Franz, J. Scott McCarley, and Jian-Ming Xu.2007.
User-oriented text segmentation evaluationmeasure.
Proceedings of the 30th annual internationalACM SIGIR conference on Research and developmentin information retrieval, pp.
701?702.
Association forComputing Machinery, Stroudsburg, PA, USA.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992.
Estimating upper and lower boundson the performance of word-sense disambiguation pro-grams.
Proceedings of the 30th annual meeting ofthe Association for Computational Linguistics, pp.249?256.
Association for Computational Linguistics,Stroudsburg, PA, USA.Maria Georgescul, Alexander Clark, and Susan Arm-strong.
2006.
An analysis of quantitative aspects inthe evaluation of thematic segmentation algorithms.Proceedings of the 7th SIGdial Workshop on Dis-course and Dialogue, pp.
144?151.
Association forComputational Linguistics, Stroudsburg, PA, USA.Marti A. Hearst.
1997.
TextTiling: Segmenting Text intoMulti-paragraph Subtopic Passages.
ComputationalLinguistics, 23(1):33?64.
MIT Press, Cambridge, MA,USA.Anna Kazantseva and Stan Szpakowicz.
2012.
TopicalSegmentation: a Study of Human Performance.
Pro-ceedings of Human Language Technologies: The 2012Annual Conference of the North American Chapterof the Association for Computational Linguistics.
As-sociation for Computational Linguistics, Stroudsburg,PA, USA.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to Its Methodology, Chapter 12.
Sage, BeverlyHills, CA, USA.Klaus Krippendorff.
2004.
Content Analysis: An Intro-duction to Its Methodology, Chapter 11.
Sage, BeverlyHills, CA, USA.Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, andFrederic Saubion 2007.
On evaluation methodologiesfor text segmentation algorithms.
Proceedings of the19th IEEE International Conference on Tools with Ar-tificial Intelligence, 2:19?26.
IEEE Computer Society,Washington, DC, USA.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.
American Instituteof Physics, College Park, MD, USA.Olena Medelyan.
2009.
Human-competitive automatictopic indexing.
PhD Thesis.
University of Waikato,Waikato, NZ.Olena Medelyan, Eibe Frank, and Ian H. Witten.2009.
Human-competitive tagging using automatickeyphrase extraction.
Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pp.
1318?1327.
Association for Compu-tational Linguistics, Stroudsburg, PA, USA.Rebecca J. Passonneau and Diane J. Litman.
1993.Intention-based segmentation: human reliability andcorrelation with linguistic cues.
Proceedings of the31st annual meeting of the Association for Computa-tional Linguistics, pp.
148?155).
Association for Com-putational Linguistics, Stroudsburg, PA, USA.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28(1):19?36.
MITPress, Cambridge, MA, USA.William A. Scott.
1955.
Reliability of content analy-sis: The case of nominal scale coding.
Public OpinionQuarterly, 19(3):321?325.
American Association forPublic Opinion Research, Deerfield, IL, USA.Sidney Siegel and N. John Castellan, Jr. 1988.
Non-parametric Statistics for the Behavioral Sciences.
2ndEdition, Chapter 9.8.
McGraw-Hill, New York, USA.161
