Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1148?1158,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsIdentifying Cascading Errors using Constraints in Dependency ParsingDominick Ng and James R. Curran@-lab, School of Information TechnologiesUniversity of SydneyNSW 2006, Australia{dominick.ng,james.r.curran}@sydney.edu.auAbstractDependency parsers are usually evaluatedon attachment accuracy.
Whilst easily in-terpreted, the metric does not illustratethe cascading impact of errors, where theparser chooses an incorrect arc, and is sub-sequently forced to choose further incor-rect arcs elsewhere in the parse.We apply arc-level constraints to MST-parser and ZPar, enforcing the correctanalysis of specific error classes, whilstotherwise continuing with decoding.
Weinvestigate the direct and indirect impactof applying constraints to the parser.
Er-roneous NP and punctuation attachmentscause the most cascading errors, while in-correct PP and coordination attachmentsare frequent but less influential.
Punctu-ation is especially challenging, as it haslong been ignored in parsing, and servesa variety of disparate syntactic roles.1 IntroductionDependency parsers are evaluated using word-level attachment accuracy.
Whilst comparableacross systems, this does not provide insight intowhy the parser makes certain errors, or whethercertain misattachments are caused by other errors.For example, incorrectly identifying a modifierhead may only introduce a single attachment error,while misplacing the root of a sentence will createsubstantially more errors elsewhere.
In projectivedependency parsing, erroneous arcs can also forcethe parser to select other incorrect arcs.Kummerfeld et al (2012) propose a static post-parsing analysis to categorise groups of bracket er-rors in constituency parsing into higher level errorclasses such as clause attachment.
However, thiscannot account for cascading changes resultingfrom repairing errors, or limitations which mayprevent the parser from applying a repair.
It is un-clear whether the parser will apply the repair op-eration in its entirety, or if it will introduce otherchanges in response to the repairs.We develop an evaluation procedure to evalu-ate the influence of each error class in dependencyparsing without making assumptions about howthe parser will behave.
We define error classesbased on dependency labels, and use the depen-dencies in each class as arc constraints specifyingthe correct head and label for particular words ineach sentence.
We adapt parsers to apply theseconstraints, whilst otherwise proceeding with de-coding under their grammar and model.
By eval-uating performance with and without constraints,we can directly observe the cascading impact ofeach error class on each the parser.We implement our procedure for the graph-based MSTparser (McDonald and Pereira, 2006)and the transition-based ZPar (Zhang and Clark,2011) using basic Stanford dependencies over theOntoNotes 4.0 release of the WSJ Penn Treebankdata.
Our results show that erroneously attach-ing NPs, PPs, modifiers, and punctuation have thelargest overall impact on UAS.
Of those, NPs andpunctuation have the most substantial cascadingimpact, indicating that these errors have the mosteffect on the remainder of the parse.
Enforcingcorrect punctuation arcs has a particularly largeimpact on accuracy, even though most evaluationscripts ignore punctuation.
We find that punctu-ation arcs are commonly misplaced by large dis-tances in the final parse, crossing over and forcingother arcs to be incorrect in the process.We will make our source code available, and1148The LME stocks decline was about as expected , but the Comex gain was n?t .detamodnnnsubjpassauxpassauxpassadvmod punctcc detnn nsubjccompnegpunctrootThe LME stocks decline was about as expected , but the Comex gain was n?t .detnn nn nsubjadvmodadvmodccomppunctccdetnn nsubjconjnegpunctrootFigure 1: MSTparser output (top) and the gold parse (bottom) for a WSJ 22 sentence.
MSTparser pro-duces two independent errors: an NP bracketing error (red, dotted), and an incorrect root (blue, dashed).Parser UAS LAS usent lsentMSTparser 91.3 87.5 41.3 26.1ZPar 91.7 89.3 45.1 35.9Table 1: Baseline UAS and LAS scores on Stanforddependencies over WSJ 22.hope that our findings will drive efforts address-ing the remaining dependency parsing errors.2 MotivationTable 1 summarises the performance of MST-parser and ZPar on Stanford dependencies overOntoNotes 4 WSJ 22.
ZPar performs slightlybetter than MSTparser on UAS, and substantiallybetter on LAS.
However, these numbers do notshow what types of errors are being made by eachparser, what errors remain to be addressed, or hintat what underlying problems cause each error.Figure 1 depicts a WSJ 22 sentence as parsed byMSTparser, and the gold parse.
The UAS is 47.1%,with 8 of 17 arcs correct.
By contrast, ZPar (parsenot shown) scores 94.1%, with the sole attachmenterror being on LME (as with MSTparser).
Whilethere are nine incorrect arcs overall, MSTparserseems to have made only two underlying errors:?
LME attached to decline rather than stocks (NPinternal).
Correcting this repairs one error;?
expected being chosen as the root rather thanwas.
Correcting the root and moving all at-tachments to it from the old root repairs theremaining eight errors.Intuitively, it seems that the impact of the NPerror is limited.
By contrast, the root selectionerror has a substantial impact on the second halfof the sentence, causing a misplaced subject, mis-attached punctuation, and incorrect coordination.These cascaded errors appear to be caused by theincorrect root.What we do not know is whether these intu-itions actually hold.
Many dependency parsers,including MSTparser and ZPar, construct trees byrepeatedly combining fragments together until aspanning analysis is found, using a small windowof information to make each arc decision.
An er-ror in one part of the tree may have no influenceon a different part of the tree.
Alternatively, er-rors may exert long-range influence ?
particularlyif there are higher-order features or algorithmicconstraints such as projectivity over the tree.
Asparsing algorithms are complex, we wish to repairvarious error types in isolation without otherwisemaking assumptions regarding the subsequent ac-tions of the parser.3 Applying ConstraintsWe investigate how each parser behaves when cer-tain errors in the tree are corrected.
We force eachparser to select the correct head and label for spe-cific words, but otherwise allow it to construct itsbest parse.
Given a set of constraints, each ofwhich lists a word with the head and label to whichit must be attached, we investigate two measures:1. errors directly corrected by the constraints,called the constrained accuracy impact;2. the indirect impact of the constraints, includ-ing errors indirectly corrected, and correct1149arcs indirectly destroyed, together called thecascaded accuracy impact.The constrained accuracy impact tells us howoften the parser makes errors in the set of wordscovered by the constraints.
The cascaded accu-racy impact is less predictable, as it describes whateffect the errors made over the constrained set ofarcs have over the rest of the sentence.
It is theinfluence of the set of constraints over the otherattachments, which may be mediated through pro-jectivity requirements, or changes in the contextused for other parsing decisions.The core of our procedure is adapting eachparser to accept a set of constraints.
FollowingKummerfeld et al (2012), we define meaningfulerror classes grouped with the operations that re-pair them.
In dependency parsing, error classesare groups of Stanford dependency labels, ratherthan groups of node repair operations.
The Stan-ford labels provide a rich distinction in NP internalstructure, clauses, and modifiers, and map well tothe error categories of Kummerfeld et al (2012),allowing us to avoid excessive heuristics in themapping process.
Our technique can be applied toother dependency schemes such as LTH (Johans-son and Nugues, 2007) by defining new mappingsfrom labels to error types.The difficulty of the mapping task depends onthe intricacies of each formalism.
The majorchallenge with LTH dependencies is the enormousskew towards the nominal modifier NMOD label.This label occurs 11,335 times in WSJ 22, morethan twice as frequently as the next most fre-quent punctuation P. By contrast, the most com-mon Stanford label is punctuation, at 4,731 occur-rences.
The NMOD label is split into many smaller,but more informative nominal labels in the Stan-ford scheme, making it better suited for our goalof error analysis.The label grouping was performed with refer-ence to the Stanford dependencies manual v2.04(de Marneffe and Manning, 2008, updated 2012).For each error class, we generate a set of con-straints over WSJ 22 for all words with a gold-standard label in the set associated with the class.Our types are defined as follows:NP attachment: any label specifically attachingan NP, includes appos, dobj, iobj, nsubj,nsubjpass, pobj, and xsubj.NP internal: any label marking nominal struc-ture (not including adjectival modifiers), in-cludes abbrev, det, nn, number, poss,possessive, and predet.PP attachment: any label attaching a preposi-tional phrase, includes prep.
Also includespcomp if the POS of the word is TO or IN.Clause attachment: any label attaching aclause, includes advcl, ccomp, csubj,csubjpass, purpcl, rcmod, and xcomp.Also includes pcomp if the POS of the word isnot TO or IN.Modifier attachment: any label attachingan adverbial or adjectival modifier, includesadvmod, amod, infmod, npadvmod, num,partmod, quantmod, and tmod.Coordination attachment: conj, cc, andpreconj.Root attachment: the root label.Punctuation attachment: the punct label.Other attachment: all other Stanford labels,specifically acomp, attr, aux, auxpass,complm, cop, dep, expl, mark, mwe, neg,parataxis, prt, ref, and rel.For example, Root constraints specify sentenceroots, while PP constraints specify heads of prepo-sitional phrases.One deficiency of our implementation is that weapply constraints to all arcs of a particular errortype in each sentence, and do not isolate multipleinstances of the same error class in a sentence.
Wedo this since applying single constraints to a sen-tence at a time would require substantial modifica-tions to the standard evaluation regime.3.1 MSTparser implementationMSTparser is a graph-based, second-order parserthat uses Eisner (1996)?s algorithm for projectivedecoding (McDonald and Pereira, 2006).1Eis-ner?s algorithm constructs and caches subtreeswhich span progressively larger sections of thesentence.
These spans are marked either as com-plete, consisting of a head, a dependent, and allof the descendants of that head to one side, or in-complete, consisting of a head, a dependent, andan unfilled region where additional tokens may beattached.
Dependencies are formed between thehead and dependent in each complete span, whilelabel assignment occurs as a separate process.We enforce constraints by allowing completespans to be formed only from constrained tokens1As the variant of Stanford dependencies we use are pro-jective, we did not use non-projective decoding.1150to their correct heads with the correct labels.
Anycomplete span between an incorrect head and theconstrained token is forbidden.
The algorithm isforced to choose the constrained spans as it buildsthe parse; these constraints have no impact on theparser?s coverage as all possible head selectionsare considered.3.2 ZPar implementationZPar is an arc-eager transition-based parser(Zhang and Clark, 2011) that uses an incremen-tal process with a stack storing partial parse states(Nivre et al, 2004).
Each state represents tokensthat may accept further arcs.
The tokens of a sen-tence are initially stored in a buffer, and at eachpoint during parsing, the parser decides whetheror not to create an arc between the front token ofthe buffer and the top token on the stack.We apply constraints in a similar way to Nivreet al (2014).
Arc creation actions are factored onthe dependency label to be assigned to the arc.ZPar scores each possible action using a percep-tron model over features from the front of thebuffer and the top of the stack (as well as some ad-ditional context features which refer to previouslycreated states).
The highest scoring actions andtheir resulting states are kept in a beam; duringparsing, ZPar finds the optimal action for all itemsin the beam, and retains the highest scoring newstates at each step.We disallow any arc creation action that wouldcreate an arc that conflicts with any constraints.Due to the use of beam search, it is possible for allof the partial states containing the constrained arcsto be evicted from the beam if they score lowerunder the model than other states.
When this hap-pens, the parser will fail to find an analysis for thesentence, as no head will exist in the beam for theconstrained token.
We have deliberately chosen tonot address this issue as any solution (e.g.
increas-ing the beam size from its default of 64) wouldchange the decisions of the parser and model.We verified that our modifications were work-ing correctly for both parsers by passing in zeroconstraints (checking that the output matched thebaseline performance), and every possible con-straint (checking that the output scored 100%).4 Related WorkKummerfeld et al (2012) perform a comprehen-sive classification of constituency bracket errorsand their cascading impact, and their work isphilosophically similar to ours.
They associategroups of bracket errors in the parse with abstracterror classes, and identify the tree operations thatrepair these error types, such as the insertion, dele-tion, or substitution of nodes in the parse tree.The error classes in a particular parser?s outputare identified through a heuristic procedure that re-peatedly applies the operation repairing the largestnumber of bracket errors.
This approach differsfrom our methodology as it is a static post-processthat assumes the parser would respond perfectly toeach repair, when it is possible that the parser maynot perform the repair in full, or even be incapableof constructing the repaired tree.McDonald and Nivre (2011) perform an in-depth comparison of the graph-based MSTparserand transition-based MaltParser.
However, Malt-Parser uses support vector machines to determinis-tically predict the next transition, rather than stor-ing the most probable options in a beam like ZPar.Additionally, they do not focus on the cascad-ing impact of errors, and instead concentrate onhigher-level error classification (e.g.
by POS tag,labels and dependency lengths) in lieu of examin-ing how the parsers respond to forced corrections.Nivre et al (2014) describe several uses for arc-level constraints in transition-based parsing.
How-ever, these applications focus on improving pars-ing accuracy when constraints can be readily iden-tified, e.g.
imperatives at the beginning of a sen-tence are likely to be the root.
We focus our con-straints on evaluation, attempting to identify im-portant sources of error in dependency parsers.Our constraint-based approach shares similar-ities to oracle training and decoding methods,where an external source of truth is used to ver-ify parser decisions.
An oracle source of parseractions is a necessary component for trainingtransition-based parsers (Nivre, 2009).
Oracle de-coding, where a system is forced to produce cor-rect output if possible, can be used to assess its up-per performance bounds (Ng and Curran, 2012).Constraining the parser?s internal search spaceis akin to an optimal pruning operation.
Char-niak and Johnson (2005) use a coarse-to-fine, it-erative pruning approach for efficiently generat-ing high-quality n-best parses for a discriminativereranker.
Rush and Petrov (2012) use a similarcoarse-to-fine algorithm with vine grammars (Eis-ner and Smith, 2005) to accelerate graph-based de-1151The LME stocks decline was about as expected , but the Comex gain was n?t .detamodnn nsubj prep advmodpobjpunctccdetnn nsubjconjnegpunctrootFigure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced toits correct value.
The incorrect noun phrase error is not affected by the constraint (dashed, red), sixattachment errors are repaired (solid, blue), and two new errors are introduced (dotted, purple).Constraints 3 7 Remaining ErrorsNone 8 9 see Figure 1nn 9 8 All except decline?
LMEroot 14 3 decline?
LMEabout ?
expectedwas?
aboutpunct 14 3 decline?
LMEabout ?
expectedwas?
aboutccomp 16 1 decline?
LMETable 2: Correct and incorrect arcs, and the re-maining errors after applying various sets of con-straints to the sentence in Figure 1.pendency parsing, achieving parsing speeds closeto linear-time transition parsers despite encodingmore complex features.
Supertagging (Clark andCurran, 2007) and chart pruning (Zhang et al,2010) have been used to constrain the search spaceof a CCG parser, and to remove unlikely or for-bidden spans from repeated consideration.
In ourwork, we use pruning not for parsing speed, butevaluation, and so we prune items based on gold-standard constraints rather than heuristics.5 EvaluationWe use the training (sections 2-21) and develop-ment (section 22) data from the OntoNotes 4.0release of the Penn Treebank WSJ data (Mar-cus et al, 1993), as supplied by the SANCL2012 Shared Task on Parsing the Web (Petrovand McDonald, 2012).
OntoNotes annotates en-riched NP structure compared to the Penn Tree-bank (Weischedel et al, 2011), meaning that deter-mining NP attachments is less trivial.
We changedall marker tokens in the corpus (e.g.
-LRB- and-LCB-) to their equivalent unescaped punctuationmarks to ensure correct evaluation.
The corpus hasbeen converted to basic Stanford dependencies us-ing the Stanford Parser v2.0,2and part-of-speech2nlp.stanford.edu/software/lex-parser.shtmltagged using MXPOST (Ratnaparkhi, 1996).
Amodel trained on WSJ sections 2-21 was used totag the development set, and 10-fold jackknifetraining was used to tag the training data.We implement a custom evaluation script tofacilitate a straightforward comparative analysisbetween the unconstrained and constrained out-put.
The script is based on and produces identi-cal scores to eval06.pl, the official evaluationfor the CoNLL-X Shared Task on MultilingualDependency Parsing (Buchholz and Marsi, 2006).We ignore punctuation as defined by eval06.plin our evaluation; experiments with constraintsover punctuation tokens constrain those tokens inthe parse, but ignore them during evaluation.We run the modified parsers over WSJ 22 withand without each set of constraints.
We ex-amine the overall unlabeled and labeled attach-ment scores (UAS and LAS), as well as identify-ing the contribution to the overall UAS improve-ment from directly (constrained) and indirectlycorrected arcs.MSTparser uses coarse-grained tags and fine-grained POS tags in its features, both of whichwere provided by the CoNLL-X Shared Task.
Weapproximate the coarse-grained POS tags by takingthe first character of the MXPOST-assigned POStag, a technique also used by Bansal et al (2014)3.6 ResultsFigure 2 and Table 2 show the impact of applyingconstraints on tokens with various labels to MST-parser for the sentence in Figure 1.
Enforcing thegold nn arc between decline and LME repairs thatnoun phrase error, but does not affect any of theother errors.
Conversely, enforcing the gold rootarc does not affect the noun phrase error, but re-pairs nearly every other error in the parse.
Un-fortunately, the constrained root arc introduces3Mohit Bansal, p.c.1152Error class cover eff eff % disp UAS LAS ?UAS ?c ?uBaseline 100.0 - - - 91.3 87.5 - - -NP attachment 95.6 312 4.9 5.2 94.1 90.7 2.3 1.2 1.1NP internal 98.2 206 3.2 2.8 92.6 89.2 1.1 0.8 0.3Modifier attachment 96.8 321 7.9 3.8 93.4 90.3 1.7 1.2 0.5PP attachment 98.3 378 13.1 4.3 93.2 89.5 1.7 1.4 0.3Coordination attachment 97.7 238 16.0 5.8 92.9 89.5 1.3 0.9 0.4Clause attachment 96.7 228 17.9 6.9 93.0 89.6 1.4 0.9 0.5Root attachment 99.1 77 5.8 9.3 92.2 88.3 0.8 0.3 0.5Punctuation attachment 93.2 469 14.2 7.4 93.9 89.9 1.8 0.1 1.7Other attachment 94.3 210 7.0 6.1 93.5 90.8 1.4 0.8 0.6All attachments 98.5 2912 9.3 5.8 100.0 100.0 8.6 8.6 0.0Table 3: The coverage, effective constraints and percentage, error displacement, UAS, LAS, ?UAS overthe corrected arcs, and the constrained and cascaded ?
for MSTparser over WSJ 22 (covered by ZPar).Error class cover eff eff % disp UAS LAS ?UAS ?c ?uBaseline 100.0 - - - 91.7 89.2 - - -NP attachment 95.6 277 4.3 4.8 94.9 92.7 2.4 1.0 1.4NP internal 98.2 197 3.0 3.0 93.2 91.1 1.2 0.7 0.5Modifier attachment 96.8 303 7.5 3.9 94.0 92.3 1.8 1.1 0.7PP attachment 98.3 357 12.4 3.9 93.8 91.4 1.7 1.3 0.4Coordination attachment 97.7 240 16.2 5.8 93.5 91.1 1.3 0.9 0.4Clause attachment 96.7 166 13.0 5.6 93.4 91.2 1.2 0.6 0.6Root attachment 99.1 57 4.3 9.9 92.4 89.9 0.5 0.2 0.3Punctuation attachment 93.2 430 13.0 7.3 94.5 92.1 1.6 0.2 1.5Other attachment 94.3 187 6.3 5.5 94.2 92.7 1.3 0.7 0.6All attachments 98.5 2760 8.8 5.8 100.0 100.0 8.0 8.0 0.0Table 4: The coverage, effective constraints and percentage, error displacement, UAS, LAS, ?UAS overthe baseline, and the constrained and cascaded ?
for ZPar over WSJ 22.two new errors, with the parser incorrectly attach-ing the clausal complement headed by expectedand the modifier headed by about .
In fact, cor-recting the ccomp arc in isolation rather than theroot arc leads to MSTparser producing the fullcorrect analysis for the second half of the sentence(though again, it does not repair the separate nounphrase error).
This example highlights why wehave chosen to implement our evaluation as a setof constraints in the parser, rather than Kummer-feld et al (2012)?s post-processing approach, aswe cannot know that the parser will react as weexpect it to when repairing errors.Tables 3 and 4 summarise our results on MST-parser and ZPar, calculated over the sentencescovered by ZPar in WSJ 22.
Results over the fullWSJ 22 for MSTparser were consistent with thesefigures.
We focus on discussing UAS results in thispaper, since LAS results are consistent.The UAS of constrained arcs in each experimentis the expected 100%.
Effective constraints repairan error in the baseline, and the effective constraintpercentage is this figure expressed as a percentage,i.e.
the error rate.
Error displacement is the av-erage number of words that effective constraintsmoved an attachment point.
The overall ?UASimprovement is divided into ?c, the constrainedimpact, and ?u, the cascaded impact.It is important to note that a parser may makea substantial number of mistakes on a particularerror class (large effective constraint percentage),but correcting those mistakes may have very littlecascading impact (small ?c), limiting the overall?UAS improvement.
Conversely, there may be aclass with a small effective constraint percentage,but a large ?UAS due to a large cascading impactfrom the corrections, or simply because the classcontains more constraints.11536.1 Overall Parser ComparisonWhen applying all constraints, ZPar has a 8.8%effective constraint percentage compared to 9.3%for MSTparser.
This is directly related to the UASdifference between the parsers.
Aside from coor-dination, where the parsers made a nearly identi-cal number of errors, ZPar is more accurate acrossthe board.
It makes substantially fewer mistakeson clause attachments, punctuation dependencies,and NP attachments, whilst maintaining a smalladvantage across all of the other categories.The relative rank of the effective constraint per-centage per error category is similar across theparsers, with PP attachment, punctuation, modi-fiers, and coordination recording the largest num-ber of effective constraints, and thus the most er-rors.
This illustrates that the behaviour of bothparsers is very consistent, despite one consideringevery possible attachment point, and the other us-ing a linear transition-based beam search.
ZParis able to make fewer mistakes across each errorcategory, suggesting that the beam search pruningis maintaining more desired states than the graph-based parser is able to rank during its search.ZPar?s coverage is 98.5% when applying allconstraints.
However, as the number of con-straints is reduced, coverage also drops.
Thisseems counter-intuitive, but applying more con-straints eliminates more undesired states, leavingmore space in the beam for satisfying states.
Re-ducing the number of constraints permits morestates which do not yet violate a constraint, butonly yield undesired states later.Punctuation constraints have the largest impacton coverage, reducing it to 93.2%.
NP attach-ments, clauses, and modifier attachments also in-cur substantial coverage reductions.
This suggeststhat ZPar?s performance will degrade substantiallyover the sentences which it cannot cover, as theymust contain constructions which are dispreferredby the model and fall out of the beam.
Constraintswith the smallest effect on coverage include rootattachments, which only occur once per sentenceand are rarely incorrect, and NP internal and PP at-tachments.
For the latter two, the small displace-ments suggest that alternate attachment points of-ten lie within the same projective span.6.2 Noun phrasesApplying NP attachment constraints causes a 4.4%drop in coverage for ZPar, and the effective con-straint percentage is below 5% for both parsers.However, these constraints still result in the largest?UAS for both parsers, at 2.6% for MSTparserand 2.2% for ZPar.
This reflects the prevalenceof NP attachments in the corpus.
?UAS is split evenly between correcting con-strained (1.4%) and cascaded arcs (1.2%) forMSTparser, while it skews towards cascaded arcsfor ZPar (1.0% and 1.4%).
Most error classesskew in the other direction, while repairing one NPattachment error typically repairs another non-NPattachment error.For NP internal attachments, both parsers havea similar error rate, with 206 effective constraintsfor MSTparser and 197 for ZPar.
Although this isthe second largest class, applying these constraintsgives the second smallest ?u for both parsers.This implies that determining NP internal struc-ture is a strength, even with the more complexOntoNotes 4 NP structure.
?c is also small forboth parsers, reinforcing the limited displacementand cascading impact of NP internal errors.Despite fewer effective constraints (i.e.
lesserrors to fix), ZPar exhibits more cascading re-pair than MSTparser using both NP and NP in-ternal constraints.
This will be a common themethrough this evaluation: the transition-based ZParis better at propagating effective constraints intocascaded impact than the graph-based MSTparser,even though ZPar almost always begins with fewereffective constraints due to its better baseline per-formance.
One possibility to explain this is thatthe beam is actually pruning away other erroneousstates, while the graph-based MSTparser must stillconsider all of them.Table 5 summarises the error classes of cor-rected cascaded arcs for the two NP constrainttypes, which are closely related.
NP attachmentconstraints directly identify the head of the NP aswell as its correct attachment, providing strongcues for determining the internal structure.
NP in-ternal constraints implicitly identify the head of anNP.
We can see that for both types of constraints,many of the cascaded corrections come from theother NP error class.Table 5 also shows that, compared to MST-parser, ZPar repairs nearly twice as many NP inter-nal and coordination errors when using NP attach-ment constraints, and vice versa when using NPinternal constraints.
This suggests that ZPar hasmore difficulty identifying the correct heads for1154NP attachment NP internalError class MSTparser ZPar MSTparser ZParNP attachment - - 45 69NP internal 43 80 - -Modifier attachment 65 68 24 30PP attachment 26 36 2 10Coordination attachment 37 67 20 41Clause attachment 59 65 1 1Root attachment 24 21 2 1Punctuation attachment 79 80 26 41Other attachment 68 76 7 11Total 401 493 127 204Table 5: The number of unconstrained errors repaired per error class when enforcing NP attachment andNP internal constraints for MSTparser and ZPar over WSJ 22.nominal coordination, and often chooses a wordwhich should be a nominal modifier instead.6.3 Coordination, Modifiers and PPsThese categories are large error classes for bothparsers, with constraints leading to UAS improve-ments of 1.3 to 1.7%.PPs and coordination have high effective con-straint percentages relative to the other errorclasses for both parsers.
However, they are alsoamongst the most isolated errors, with only 0.3%and 0.4% ?u for MSTparser and ZPar respec-tively.
These errors also have minimal impact onZPar?s coverage.
Both classes seem to have rela-tively contained attachment options within a lim-ited projective span.
The small error displace-ments reinforce this idea.Modifiers are relatively isolated errors for MST-parser (0.5% ?u), but less so for ZPar (0.7% ?u).There are substantially more modifier constraintsthan PP or coordination, despite all yielding a sim-ilar UAS increase.
This suggests that modifiers areactually relatively well analysed by both parsers,but there are so many of them that they form alarge source of error.6.4 Clause attachmentMSTparser performs substantially worse thanZPar on clause attachments, with an effective con-straint percentage of 17.9% compared to 13.0%,and ?c of 0.9% compared with 0.6%.
MST-parser?s error rate is the worst of any error classon clause attachments, while it is second to coor-dination attachments for ZPar.
Attaching clausesis very challenging for dependency parsers, partic-ularly considering the small size of the class.ZPar again achieves a slightly larger cascadedimpact than MSTparser (0.6% to 0.5%), despitehaving far fewer effective constraints.
This im-plies that the additional clause errors being madeby MSTparser are largely self-contained, as theyhave not triggered a corresponding increase in?u.6.5 Root attachmentBoth parsers make few root attachment errors,though MSTparser is less accurate than ZPar.However, root constraints provide the largest UASimprovement per number of constraints for bothparsers.
Root errors are also the most displaced ofany error class, at 9.3 words for MSTparser and9.9 for ZPar.
When the root is incorrect, it is of-ten very far from its correct location, and causessubstantial cascading errors.6.6 PunctuationDespite ignoring punctuation dependencies inevaluation, applying punctuation constraints led tosubstantial UAS improvements.
On MSTparser,?u is 0.1% (due to some punctuation not beingexcluded from evaluation), but ?c is 1.7%.
OnZPar, the equivalent metrics are 0.2% and 1.5%.Enforcing correct punctuation has a disproportion-ate impact on the remainder of the parse.For both parsers, punctuation errors occur morefrequently than any other error type, with 469and 430 effective constraints respectively (thoughthe majority of these corrected errors are on non-evaluated arcs).
ZPar?s coverage is worst of allwhen enforcing punctuation constraints, suggest-ing that the remaining uncovered sentences will1155Error class MSTparser ZParNP attachment 75 51NP internal 25 27Modifier attachment 33 43PP attachment 45 55Coordination attachment 87 106Clause attachment 66 48Root attachment 59 27Other attachment 65 69Total 455 426Table 6: The number of unconstrained errors re-paired per error class when enforcing punctuationconstraints for MSTparser and ZPar.contain even more punctuation errors.Incorrect punctuation heads are displaced fromtheir correct locations by 7.4 words for MSTparserand 7.3 words for ZPar on average, second only toroot attachments.
Given that we are using projec-tive parsers and a projective grammar, the large av-erage displacement caused by errors indicates thatpunctuation affects and is in turn affected by therequirement for non-crossing arcs.Table 6 summarises the error classes of the re-paired cascaded arcs when punctuation constraintsare applied.
MSTparser has a more even distri-bution of repairs, while ZPar?s repairs are con-centrated in coordination attachment.
This showsthat MSTparser is relatively better at coordinationas a proportion of its overall performance com-pared to ZPar.
It also indicates that the majority ofpunctuation errors in both parsers (and especiallyZPar) stem from incorrectly identified coordina-tion markers such as commas.Punctuation is commonly ignored in depen-dency parser evaluation (Yamada and Matsumoto,2003; Buchholz and Marsi, 2006), and they areinconsistently treated across different grammars.Our results show that enforcing the correct punc-tuation attachments in a sentence has a substan-tial cascading impact, suggesting that punctua-tion errors are highly correlated with errors else-where in the analysis.
Given the broad simi-larities between Stanford dependencies and otherdependency schemes commonly used in parsing(S?gaard, 2013), we anticipate that the problemswith roots and punctuation will carry across dif-ferent treebanks and schemes.Punctuation is often placed at phrasal bound-aries and serves to split sentences into smaller sec-tions within a projective parser.
Graph-based andtransition-based parsers, both of which use a lim-ited local context to make parsing decisions, areequally prone to the cascading impact of erroneouspunctuation.
Removing the confounding presenceof punctuation from parsing and treating attach-ment as a global post-process may help to allevi-ate these issues.
Alternatively, more punctuation-specific features to account for its myriad roles insyntax could serve to improve performance.7 ConclusionWe have developed a procedure to classify the im-portance of errors in dependency parsers withoutany assumptions on how the parser will respondto attachment repairs.
Our approach constrains theparser to allow only correct arcs for certain tokens,whilst allowing it to otherwise form the parse thatit thinks is best.
Compared to Kummerfeld et al(2012), we can observe exactly how the parser re-sponds to the parse repairs, though at the cost ofrequiring modifications to the parser itself.Our results show that noun phrases remain chal-lenging for dependency parsers, both in choosingthe correct head, and in determining the internalstructure.
Punctuation, despite being commonlyignored in parsers and evaluation, causes substan-tial cascading errors when misattached.We are extending our work to other popular de-pendency parsers and non-projective parsing algo-rithms, and hope to develop features to improveand mitigate the cascading impact of punctuationattachment errors in parsing.
Given that con-stituency parsers perform strongly when convertedto dependencies (Cer et al, 2010; Petrov and Mc-Donald, 2012), it would also be interesting to in-vestigate how they perform on our metrics.We implement a robust procedure to identifythe cascading impact of dependency parser errors.Our results provide insights into which errors aremost damaging in parsing, and will drive furtherimprovements in parsing accuracy.ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representa-tions for dependency parsing.
In Proceedingsof the 52nd Annual Meeting of the Associationfor Computational Linguistics (ACL-14), pages809?815.
Baltimore, Maryland, USA.1156Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X Shared Task on Multilingual DependencyParsing.
In Proceedings of the Tenth Conferenceon Computational Natural Language Learning(CoNLL-06), pages 149?164.Daniel Cer, Marie-Catherine de Marneffe, DanJurafsky, and Chris Manning.
2010.
Parsingto Stanford Dependencies: Trade-offs betweenSpeed and Accuracy.
In Proceedings of the Sev-enth International Conference on Language Re-sources and Evaluation (LREC-10).Eugene Charniak and Mark Johnson.
2005.Coarse-to-Fine n-Best Parsing and MaxEnt Dis-criminative Reranking.
In Proceedings ofthe 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL-05), pages173?180.Stephen Clark and James R. Curran.
2007.Formalism-Independent Parser Evaluation withCCG and DepBank.
In Proceedings of the 45thAnnual Meeting of the Association for Compu-tational Linguistics (ACL-07), pages 248?255.Prague, Czech Republic.Marie-Catherine de Marneffe and Christopher D.Manning.
2008.
Stanford dependencies manual.Technical report, Stanford University.Jason Eisner.
1996.
Three New Probabilistic Mod-els for Dependency Parsing: An Exploration.
InProceedings of the 16th International Confer-ence on Computational Linguistics (COLING-96), pages 340?345.Jason Eisner and A. Noah Smith.
2005.
Parsingwith Soft and Hard Constraints on DependencyLength.
In Proceedings of the Ninth Interna-tional Workshop on Parsing Technology (IWPT-05), pages 30?41.Richard Johansson and Pierre Nugues.
2007.
Ex-tended Constituent-to-dependency Conversionfor English.
In Proceedings of the 16thNordic Conference of Computational Linguis-tics (NODALIDA-07), pages 105?112.
Tartu,Estonia.Jonathan K. Kummerfeld, David Hall, James R.Curran, and Dan Klein.
2012.
Parser Show-down at the Wall Street Corral: An EmpiricalInvestigation of Error Types in Parser Output.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Pro-cessing and Computational Natural LanguageLearning (EMNLP-CoNLL-12), pages 1048?1059.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga Large Annotated Corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Ryan McDonald and Joakim Nivre.
2011.
Analyz-ing and Integrating Dependency Parsers.
Com-putational Linguistics, 37(1):197?230.Ryan McDonald and Fernando Pereira.
2006.Online Learning of Approximate DependencyParsing Algorithms.
In Proceedings of the11th Conference of the European Chapter ofthe Association for Computational Linguistics(EACL-06), pages 81?88.Dominick Ng and James R. Curran.
2012.
Depen-dency Hashing for n-best CCG Parsing.
In Pro-ceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (ACL-12), pages 497?505.Joakim Nivre.
2009.
Non-Projective DependencyParsing in Expected Linear Time.
In Proceed-ings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural LanguageProcessing of the AFNLP (ACL-09), pages 351?359.Joakim Nivre, Yoav Goldberg, and Ryan McDon-ald.
2014.
Constrained arc-eager dependencyparsing.
Computational Linguistics, 40(2):249?257.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-Based Dependency Parsing.
In HLT-NAACL 2004 Workshop: Eighth Conferenceon Computational Natural Language Learning(CoNLL-04), pages 49?56.Slav Petrov and Ryan McDonald.
2012.
Overviewof the 2012 Shared Task on Parsing the Web.In Notes of the First Workshop on the SyntacticAnalysis of Non-Canonical Language (SANCL-12).Adwait Ratnaparkhi.
1996.
A Maximum EntropyModel for Part-of-Speech Tagging.
In Proceed-ings of the 1996 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-96), pages 133?142.Alexander Rush and Slav Petrov.
2012.
Vine Prun-ing for Efficient Multi-Pass Dependency Pars-1157ing.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Associationfor Computational Linguistics: Human Lan-guage Technologies (NAACL-12), pages 498?507.Anders S?gaard.
2013.
An Empirical Study ofDifferences between Conversion Schemes andAnnotation Guidelines.
In Proceedings of the2nd International Conference on DependencyLinguistics, pages 298?307.Ralph Weischedel, Eduard Hovy, Mitchell Mar-cus, Martha Palmer, Robert Belvin, SameerPradhan, Lance Ramshaw, and Nianwen Xue.2011.
OntoNotes: A Large Training Corpus forEnhanced Processing.
In Joseph Olive, CaitlinChristianson, and John McCary, editors, Hand-book of Natural Language Processing and Ma-chine Translation: DARPA Global AutonomousLanguage Exploitation, pages 54?63.
Springer.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Sta-tistical Dependency Analysis with Support Vec-tor Machines.
In Proceedings of the 8th In-ternational Workshop of Parsing Technologies(IWPT-03), pages 196?206.Yue Zhang, Byung-Gyu Ahn, Stephen Clark, CurtVan Wyk, James R. Curran, and Laura Rimell.2010.
Chart Pruning for Fast Lexicalised-Grammar Parsing.
In Proceedings of the23rd International Conference on Computa-tional Linguistics (COLING-10), pages 1471?1479.Yue Zhang and Stephen Clark.
2011.
SyntacticProcessing Using the Generalized Perceptronand Beam Search.
Computational Linguistics,37(1):105?151.1158
