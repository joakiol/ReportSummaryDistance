Hierarchical Reinforcement Learning for Adaptive Text GenerationNina DethlefsUniversity of Bremen, Germanydethlefs@uni-bremen.deHeriberto Cuaya?huitlUniversity of Bremen, Germanyheriberto@uni-bremen.deAbstractWe present a novel approach to natural lan-guage generation (NLG) that applies hierar-chical reinforcement learning to text genera-tion in the wayfinding domain.
Our approachaims to optimise the integration of NLG tasksthat are inherently different in nature, suchas decisions of content selection, text struc-ture, user modelling, referring expression gen-eration (REG), and surface realisation.
Italso aims to capture existing interdependen-cies between these areas.
We apply hierar-chical reinforcement learning to learn a gen-eration policy that captures these interdepen-dencies, and that can be transferred to otherNLG tasks.
Our experimental results?in asimulated environment?show that the learntwayfinding policy outperforms a baseline pol-icy that takes reasonable actions but withoutoptimization.1 IntroductionAutomatic text generation involves a number of sub-tasks.
(Reiter and Dale, 1997) list the following ascore tasks of a complete NLG system: content se-lection, discourse planning, sentence planning, sen-tence aggregation, lexicalisation, referring expres-sion generation and linguistic realisation.
However,decisions made for each of these core tasks are notindependent of each other.
The value of one gen-eration task can change the conditions of others,as evidenced by studies in corpus linguistics, andit can therefore be undesirable to treat them all asisolated modules.
In this paper, we focus on inter-related decision making in the areas of content se-lection, choice of text structure, referring expressionand surface form.
Concretely, we generate route in-structions that are tailored specifically towards dif-ferent user types as well as different environmentalfeatures.
In addition, we aim to balance the degreeof variation and alignment in texts and produce lex-ical and syntactic patterns of co-occurrence that re-semble those of human texts of the same domain.Evidence for the importance of this is provided by(Halliday and Hasan, 1976) who note the way thatlexical cohesive ties contribute to text coherence aswell as by the theory of interactive alignment.
Ac-cording to (Pickering and Garrod, 2004) we wouldexpect significant traces of lexical and syntactic self-alignment in texts.Approaches to NLG in the past have been ei-ther rule-based (Reiter and Dale, 1997) or statisti-cal (Langkilde and Knight, 1998).
However, the for-mer relies on a large number of hand-crafted rules,which makes it infeasible for controlling a largenumber of interrelated variables.
The latter typi-cally requires training on a large corpus of the do-main.
While these approaches may be better suitablefor larger domains, for limited domains such as ourown, we propose to overcome these drawbacks byapplying Reinforcement Learning (RL)?with a hi-erarchical approach.
Previous work that has used RLfor NLG includes (Janarthanam and Lemon, 2009)who employed it for alignment of referring expres-sions based on user models.
Also, (Lemon, 2008;Rieser and Lemon, 2009) used RL for optimisinginformation presentation styles for search results.While both approaches displayed significant effectsof adaptation, they focused on a single area of opti-misation.
For larger problems, however, such as theone we are aiming to solve, flat RL will not be appli-cable due to the large state space.
We therefore sug-gest to divide the problem into a number of subprob-lems and apply hierarchical reinforcement learning(HRL) (Barto and Mahadevan, 2003) to solve it.We describe our problem in more detail in Sec-tion 2, our proposed HRL architecture in Sections3 and 4 and present some results in Section 5.
Weshow that our learnt policies outperform a baselinethat does not adapt to contextual features.2 Generation tasksOur experiments are all drawn from an indoornavigation dialogue system which provides userswith route instructions in a university building andis described in (Cuaya?huitl et al, 2010).
We aimto optimise generation within the areas of contentselection, text structure, referring expression gener-ation and surface realisation.Content Selection Content selection decisionsare subject to different user models.
We distin-guish users who are familiar with the navigationenvironment and users who are not.
In this way,we can provide different routes for these userscorresponding to their particular information need.Specifically, we provide more detail for unfamiliarthan familiar users by adding any or several ofthe following: (a) landmarks at decision points,(b) landmarks lying on long route segments, (c)specifications of distance.Text Structure Depending on the type of userand the length of the route, we choose among threedifferent text generation strategies to ease the cogni-tive load of the user.
Examples of all strategies aredisplayed in Table 1.
All three types resulted froman analysis of a corpus of 24 human-written drivingroute instructions.
We consider the first type (se-quential) most appropriate for long or medium-longroutes and both types of user.
The second type (tem-poral) is appropriate for unfamiliar users and routesof short or medium length.
It divides the route intoan explicit sequence of consecutive actions.
Thethird type (schematic) is used in the remaining cases.Referring Expression Generation We dis-tinguish three types of referring expressions:common names, familiar names and descriptions.In this way, entities can be named according tothe users?
prior knowledge.
For example, oneand the same room can be called either ?thestudent union room?, ?room A3530?
or ?the roomright at the corner beside the entrance to the terrace?.Surface Realisation For surface realisation, weaim to generate texts that display a natural balanceof (self-)alignment and variation.
While it is a ruleof writing that texts should typically contain varia-tion of surface forms in order not to appear repetitiveand stylistically poor, there is evidence that humansalso get influenced by self-alignment processes dur-ing language production.
Specifically, (Garrod andAnderson, 1987; Pickering and Garrod, 2004) ar-gue that the same mental representations are usedduring language production and comprehension, sothat alignment occurs regardless of whether the lastutterance was made by another person or by thespeaker him- or herself (for experimental evidencesee (Branigan et al, 2000; Bock, 1986)).
We cantherefore hypothesise that coherent texts will, be-sides variation, also display a certain degree of self-alignment.
In order to determine a proper balanceof alignment and variation, we computed the degreeof lexical repetition from our corpus of 24 humanroute descriptions.
This analysis was based on (Hirstand St-Onge, 1998) who retrieve lexical chains fromtexts by identifying a number of relations betweenlexical items.
We focus here exclusively on Hirst& St-Onge?s ?extra-strong?
relations, since these canbe computed from shallow properties of texts and donot require a large corpus of the target domain.
Inorder to make a fair comparison between the humantexts and our own, we used a part-of-speech (POS)tagger (Toutanova and Manning, 2000)1 to extractthose grammatical categories that we aim to controlwithin our framework, i.e.
nouns, verbs, preposi-tions, adjectives and adverbs.
Based on these cat-egories, we compute the proportion of tokens thatare members in lexical chains, the ?alignment score?
(AS), according to the following equation:AS = Lexical tokens in chainsTotal number of tokens ?
100.
(1)We obtained an average alignment score of 43.3%for 24 human route instructions.
In contrast, the1http://nlp.stanford.edu/software/tagger.shtmlTable 1: Different text generation strategies for the same underlying route.Type 1: Sequential Type 2: Temporal Type 3: SchematicTurn around, and go straight First, turn around.
Second, - Turn around.to the glass door in front of go straight to the glass door - Go straight until the glass door in frontyou.
Turn right, then follow in front of you.
Third, turn of you.
(20 m)the corridor until the lift.
It right.
Fourth, follow the - Turn rightwill be on your left-hand corridor until the lift.
It will - Follow the corridor until the lift.
(20 m)side.
be on your left-hand side.
- It will be on your left-hand side.same number of instructions generated by GoogleMaps yielded 78.7%, i.e.
an almost double amountof repetition.
We will therefore train our agentto generate texts with an about medium alignmentscore.3 Hierarchical Reinforcement Learningfor NLGThe idea of text generation as an optimizationproblem is as follows: given a set of genera-tion states, a set of actions, and an objectivereward function, an optimal generation strategymaximizes the objective function by choosing theactions leading to the highest reward for everyreached state.
Such states describe the system?sknowledge about the generation task (e.g.
con-tent selection, text structure, REG, surface realiza-tion).
The action set describes the system?s ca-pabilities (e.g.
expand sequential aggregation, ex-pand schematic aggregation, expand lexical items,etc.).
The reward function assigns a numeric valuefor each taken action.
In this way, text generationcan be seen as a finite sequence of states, actionsand rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, wherethe goal is to find an optimal strategy automatically.To do that we use hierarchical reinforcement learn-ing in order to optimize a hierarchy of text genera-tion policies rather than a single policy.The hierarchy of RL agents consists of L lev-els and N models per level, denoted as M = M ij ,where j ?
{0, ..., N ?
1} and i ?
{0, ..., L ?
1}.Each agent of the hierarchy is defined as a Semi-Markov Decision Process (SMDP) consisting of a4-tuple < Sij, Aij , T ij , Rij >.
Sij is a set of states, Aijis a set of actions, and T ij is a transition function thatdetermines the next state s?
from the current states and the performed action a with a probability ofP (s?|s, a).
Rij(s?, ?
|s, a) is a reward function thatspecifies the reward that an agent receives for takingan action a in state s at time ?
.
Since SMDPs allowfor temporal abstraction, that is, actions may take avariable number of time steps to complete, the ran-dom variable ?
represents this number of time steps.Actions can be either primitive or composite.
Theformer yield single rewards, the latter (executed us-ing a stack mechanism) correspond to SMDPs andyield cumulative discounted rewards.
The goal ofeach SMDP is to find an optional policy ??
that max-imises the reward for each visited state, according to?
?ij(s) = arg maxa?A Q?ij(s, a).
(2)where Qij(s, a) specifies the expected cumulative re-ward for executing action a in state s and then fol-lowing ??.
For learning a generation policy, weuse hierarchical Q-Learning (HSMQ) (Dietterich,1999).
The dynamics of SMDPs are as follows:when an SMDP terminates its execution, it is poppedoff the stack of models to execute, and control istransferred to the next available SMDP in the stack,and so on until popping off the root SMDP.
AnSMDP terminates when it reaches one of its termi-nal states.
This algorithm is executed until the Q-values of the root agent stabilize.
The hierarchicaldecomposition allows to find context-independentpolicies with the advantages of policy reuse and fa-cilitation for state-action abstraction.
This hierarchi-cal approach has been applied successfully to dia-logue strategy learning (Cuayahuitl et al, 2010).4 Experimental Setting4.1 Hierarchy of SMDPsThe hierarchy consists of 15 agents.
It is depictedin Figure 1.
The root agent is responsible for deter-Figure 1: Hierarchy of agents for learning adaptive text generation strategies in the wayfinding domainmining a route instruction type for a navigation situ-ation.
We distinguish turning, passing, locating, go-ing and following instructions.
It also chooses a textgeneration strategy and the information structure ofthe clause (i.e., marked or unmarked theme (Hall-iday and Matthiessen, 2004)).
Leaf agents are re-sponsible for expanding constituents in which varia-tion or alignment can occur, e.g.
the choice of verbor prepositional phrase.4.2 State and action setsWe distinguish three kinds of state representations,displayed in Table 2.
The first (M010 and M10 ) en-codes information on the spatial environment anduser type so that texts can be tailored towards thesevariables.
These variables play a major part in oursimulated environment (Section 5.1).
The secondrepresentation (M11 - M15 and M23 ) controls sentencestructure and ensures that all required constituentsfor a message have been realised.
The third (all re-maining models) encodes variants of linguistic sur-face structure and represents the degree of alignmentof all variants.
We address the way that these align-ment values are computed in Section 4.4.
Actionscan be either primitive or composite.
Whereas theformer expand a logical form directly, the latter cor-respond to SMDPs at different levels of the hierar-chy.
All parent agents have both types of actions,only the leaf agents have exclusively primitive ac-tions.
The set of primitive actions is displayed in Ta-ble 2, all composite actions, corresponding to mod-els, are shown in Figure 1.
The average number ofstate-action pairs for a model is |S ?
A| = 77786.While in the present work, the action set was de-termined manually, future work can aim at learninghierarchies of SMDPs automatically from data.4.3 Prior KnowledgeAgents contain prior knowledge of two sorts.
First,the root agents and agents at the first level of the hi-erarchy contain prior probabilities of executing cer-tain actions.
For example, given an unfamiliar userand a long route, model M10 , text strategy, is initi-ated with a higher probability of choosing a sequen-tial text strategy than a schematic or temporal strat-egy.
Second, leaf agents of the hierarchy are initi-ated with values of a hand-crafted language model.These values indicate the probabilities of occurrenceof the different surface forms of the leaf agents listedin Table 2.
Both types of prior probabilities are usedby the reward functions described below.4.4 Reward functionsWe use two types of reward function, both of whichare directly motivated by the principles we stated inSection 2.
The first addresses interaction length (theshorter the better) and the choice of actions tailoredtowards the user model and spatial environment.R =??
?0 for reaching the goal state-10 for an already invoked subtaskp(a) otherwise(3)p(a) corresponds to the probability of the last ac-tion given the current state, described above as priorknowledge.
The second reward function addressesTable 2: State and action sets for learning adaptive text generation strategies in the wayfinding domainModel State Variables Action SetM00 text strategy (FV), info structure (FV), instruction (FV), expand text strategy (M10 ), turning (M23 ),slot in focus(0=action, 1=landmark), user type(0=unfamiliar, going (M21 ), passing (M25 ), following (M22 ),1=familiar) subtask termination(0=continue, 1=halt) locating instr.
(M24 ), expand unmarked themeM10 end(0=continue, 1=halt), text strategy (FV), route length expand schematic aggregation, expand sequen-(0=short, 1=medium, 2=long), user type(0=unfam., 1=fam.)
ce aggregation, expand temporal aggregationM11 going vp (FV), limit (FV), SV expand going vp (M20 ), expand limitM12 following vp (FV), SV, limit (FV) expand following vp (M21 ), expand limitM13 turning location (FV), turning vp (FV), expand turning vp (M32 ), expand turning loc.,SV, turning direction (FV) expand turning direction (M34 )M14 np locatum (FV), locating vp (FV), expand np locatum, expand locating vp (M25 ),static direction (FV), SV expand static dir.
(M26 )M15 np locatum (FV), passing vp (FV), SV, static direction (FV) expand pass.
vp (M26 ), expand static dir.
(M26 )M20 vp go straight ahead, vp go straight, vp move straight ahead, Actions correspond to expansions ofvp walk straight ahead, vp walk straight (all AS) lexemesM21 vp follow, vp go over, vp walk down, vp go down, Actions correspond to expansions ofvp go up, vp walk up, vp walk over (all AS) lexemesM22 vp walk, vp veer , vp hang, vp bear (all AS), vp go, vp head, Actions correspond to expansions ofvp turn (all AS) lexemesM23 identifiability(0=not id.,1=id.
), user type(0=un-, expand relatum id., expand relatum, not id.,fam.,1=fam,, relatum identifiability (FV), relatum name (FV) expand descriptive, expand common nameM24 pp nonphoric, pp nonphoric handedness, Actions correspond to expansions ofpp nonphoric poss, pp phoric pp nonphoric side (all AS) lexemesM25 vp be, vp be located at, vp get to, vp see (all AS) Actions correspond to expansions of lexemesM26 direction on, direction poss, direction to (all AS) Actions correspond to expansions of lexemesM27 vp move past, vp pass, vp pass by, vp walk past (all AS) Actions correspond to expansions of lexemes(FV = filling status): 0=unfilled, 1=filled.
(SV = shared variables): the variables np actor (FV), relatum (FV),sentence (FV) and information need (0=low, 1=high) are shared by several subagents; the same applies to theircorresponding expansion actions.
(AS = alignment score): 0=unaligned, 1=low AS, 2=medium AS, 3=high AS.the tradeoff between alignment and variation:R =??
?0 for reaching the goal statep(a) for medium alignment-0.1 otherwise(4)Whilst the former reward function is used by the rootand models M10 - M15 and M22 , the latter is used bymodels M20 - M21 and M23 - M27 .
It rewards the agentfor a medium alignment score, which correspondsto the score of typical human texts we computedin Section 2.
The alignment status of a constituentis computed by the Constituent Alignment Score(CAS) as follows, where MA stands for ?mediumalignment?.CAS(a) = Count of occurrences(a)Occurences of a without MA (5)From this score, we can determine the degree ofalignment of a constituent by assigning ?no align-ment?
for a constituent with a score of less than0.25, ?low alignment?
for a score between 0.25 and0.5, ?medium alignment?
for a score between 0.5 and0.75 and ?high alignment?
above.
On the whole thus,the agent?s task consists of finding a balance be-tween choosing the most probable action given thelanguage model and choosing an action that alignswith previous utterances.5 Experiments and Results5.1 Simulated EnvironmentThe simulated environment encodes information onthe current user type (un-/familiar with the environ-ment) and corresponding information need (low orhigh), the length of the current route (short, medium-long, long), the next action to perform (turn, gostraight, follow a path, pass a landmark or take noteof a salient landmark) and the current focus of at-tention (the action to be performed or some salientlandmark nearby).
Thus, there are five different statevariables with altogether 120 combinations, sam-pled from a uniform distribution.
This simple formof stochastic behaviour is used in our simulated en-vironment.
Future work can consider inducing alearning environment from data.5.2 Comparison of learnt and baseline policiesIn order to test our framework, we designed a sim-ulated environment that simulates different naviga-tional situations, routes of different lengths and dif-ferent user types.
We trained our HRL agent for10.000 episodes with the following learning param-eters: the step-size parameter ?
was initiated with 1and then reduced over time by ?
= 11+t , t being thetime step.
The discount rate parameter ?
was 0.99and the probability of random action ?
was 0.01 (see(Sutton and Barto, 1998) for details on these param-eters).
Figure 2 compares the learnt behaviour ofour agent with a baseline (averaged over 10 runs)that chooses actions at random in models M10 andM20 - M27 (i.e., the baseline does not adapt its textstrategy to user type or route length and neither per-forms adaptation of referring expressions or align-ment score).
The user study reported in (Cuaya?huitlet al, 2010) provided users with instruction usingthis baseline generation behaviour.
The fact thatusers had a user satisfaction score of 90% indicatesthat this is a sensible baseline, producing intelligi-ble instructions.
We can observe that after a certainnumber of episodes, the performance of the trainedagent begins to stabilise and it consistently outper-forms the baseline.6 Example of generationAs an example, Figure 3 shows in detail the genera-tion steps involved in producing the clause ?Follow102 103 104?65?60?55?50?45?40?35?30?25AverageRewardEpisodesLearnt BehaviourBaselineFigure 2: Comparison of learnt and baseline behaviour inthe generation of route descriptionsthe corridor until the copyroom?
for an unfamiliaruser and a route of medium length.
Generation startswith the root agent in state (0,0,0,0,0,0), which in-dicates that text strategy, info structure and instruc-tion are unfilled slots, the slot in focus of the sen-tence is an action, the status of subtask terminationis ?continue?
and the user type is unfamiliar.
Afterthe primitive action expand unmarked theme wasexecuted, the state is updated to (0,1,0,0,0,0), in-dicating the filled slot.
Next, the composite actiontext strategy is executed, corresponding to modelM10 .
The initial state (1,0,0) indicates a routeof medium length, an unfilled text strategy slotand an unfamiliar user.
After the primitive ac-tion expand sequential text was chosen, the ter-minal state is reached and control is returned tothe root agent.
Here, the next action is follow-ing instruction corresponding to model M12 .
Theinitial state (0,1,0,0,0,0) here indicates unfilled slotsfor following vp, np actor, sentence, path, limitand relatum, as well as a high information needof the current user.
The required constituentsare expanded in turn.
First, the primitive actionsexpand limit, expand np actor, expand s and ex-pand path cause their respective slots in the staterepresentation to be filled.
Next, the composite ac-tion expand relatum is executed with an initial state(0,1,0,0) representing an identifiable landmark, un-filled slots for a determiner and a referring expres-sion for the landmark and an unfamiliar user.
Twoprimitive actions, expand relatum identifiable andexpand relatum common name, cause the agent toreach its terminal state.
The generated referring ex-pression thus treats the referenced entity as eitherknown or easily recoverable.
Finally, model M21executes the composite action expand following vp,which is initialised with a number of variables cor-responding to the alignment status of different verbforms.
Since this is the first time this agent is called,none of them shows traces of alignment (i.e., all val-ues are 0).
Execution of the primitive action ex-pand following vp causes the respective slot to beupdated and the agent to terminate.
After this sub-task, model M12 has also reached its terminal stateand control is returned to the root agent.As a final step towards surface generation, all cho-sen actions are transformed into an SPL (Kasper,1989).
The type ?following instruction?
leads to theinitialisation of a semantically underspecified scaf-fold of an SPL, all other actions serve to supplementthis scaffold to preselect specific syntactic structuresor lexical items.
For example, the choice of ?ex-pand following vp?
leads to the lexical item ?fol-low?
being inserted.
Similarly, the choice of ?ex-pand path?
leads to the insertion of ?the corridor?into the SPL to indicate the path the user should fol-low.
?expand limit?, in combination with the choiceof referring expression, leads to the insertion of thePP ?until the copy room?.
For generation of morethan one instruction, aggregation has to take place.This is done by iterating over all instructions of atext and inserting them into a larger SPL that re-alises the aggregation.
Finally, the constructed SPLis passed to the KPML surface generator (Bateman,1997) for string realisation.7 DiscussionWe have argued in this paper that HRL is an es-pecially suited framework for generating texts thatare adaptive to different users, to environmental fea-tures and properties of surface realisation such asalignment and variation.
While the former tasks ap-pear intuitively likely to contribute to users?
com-prehension of texts, it is often not recognised thatthe latter task can have the same effect.
Differingsurface forms of identical concepts in texts withoutmotivation can lead to user confusion and deterio-rate task success.
This is supported by Clark?s ?prin-ciple of contrast?
(Clark, 1987), according to whichnew expressions are only introduced into an interac-tion when the speaker wishes to contrast them withother entities already present in the discourse.
Si-miliarly, a study by (Clark and Wilkes-Gibbs, 1986)showed that interlocutors tend to align their referringexpressions and thereby achieve more efficient andsuccessful dialogues.
We tackled the integration ofdifferent NLG tasks by applying HRL and presentedresults, which showed to be promising.
As an al-ternative to RL, other machine learning approachesmay be conceivable.
However, supervised learningrequires a large amount of training data, which maynot always be available, and may also produce un-predictable behaviour in cases where a user deviatesfrom the behaviour covered by the corpus (Levinet al, 2000).
Both arguments are directly trans-ferable to NLG.
If an agent is able to act only ongrounds of what it has observed in a training cor-pus, it will not be able to react flexibly to new staterepresentations.
Moreover, it has been argued thata corpus for NLG cannot be regarded as an equiv-alent gold standard to the ones of other domains ofNLP (Belz and Reiter, 2006; Scott and Moore, 2006;Viethen and Dale, 2006).
The fact that an expres-sion for a semantic concept does not appear in a cor-pus does not mean that it is an unsuited or impos-sible expression.
Another alternative to pure RL isto apply semi-learnt behaviour, which can be help-ful for tasks with very large state-action spaces.
Inthis way, the state-action space is reduced to onlysensible state-action pairs by providing the agentwith prior knowledge of the domain.
All remain-ing behaviour continues to be learnt.
(Cuaya?huitl,2009) suggests such an approach for learning dia-logue strategies, but again the principle is transfer-able to NLG.
While there is room for explorationof different RL methods, it is clear that neither tra-ditional rule-based accounts of generation, nor n-gram-based generators can achieve the same flexiblegeneration behaviour given a large, and partially un-known, number of state variables.
Since state spacesare typically very large, specifying rules for eachsingle condition is at best impractical.
Especially fortasks such as achieving a balanced alignment score,as we have shown in this paper, decisions depend onvery fine-grained textual cues such as patterns of co-occurrence which are hard to pin down accuratelyby hand.
On the other hand, statistical approaches????????????????????????????????????????????????????????????????
?M00 (0, 0, 0, 0, 0, 0){action = expand unmarkedtheme}M00 (0, 1, 0, 0, 0, 0){action = text strategy}?
?M10 (1, 0, 0){action = expand sequential text}M10 (1, 1, 0), (terminalstate)?
?M00 , (1, 1, 0, 0, 0, 0),{action = following instruction}????????????????????????????????????????????
?M21 (0, 1, 0, 0, 0, 0){action = expand limit}M21 (0, 1, 1, 0, 0, 0){action = expand np actor}M21 (0, 1, 1, 1, 0, 0){action = expand s}M21 (0, 1, 1, 0, 1, 0){action = expand path}M21 (0, 1, 1, 1, 1, 0){action = expand relatum}????????
?M23 (0, 0, 0, 0){action = expand relatumidentifiable}M23 (0, 1, 0, 0){action = expand relatumcommon name}M23 (0, 1, 1, 0), (terminalstate)????????
?M21 (0, 1, 1, 1, 1, 0){action = expand followingvp}???
?M21 (0, 0, 0, 0, 0, 0, 0, 0, 0){action = follow}M21 (1, 0, 0, 0, 0, 0, 0, 0, 0)(terminalstate)???
?M21 (1, 1, 1, 1, 1, 1)(terminalstate)????????????????????????????????????????????
?M00 (1, 1, 1, 0, 1, 0)(terminalstate)????????????????????????????????????????????????????????????????
?Figure 3: Example of generation for the clause ?Follow the corridor until the copy room?.
This example shows decisionmaking for a single instruction, adaptation and alignment occurs over longer sequences of text.to generation that are based on n-grams focus on thefrequency of constructions in a corpus without tak-ing contextual variables such as user type or environ-mental properties into account.
Further, they sharethe problem of supervised learning approaches dis-cussed above, namely, that it can act only on groundsof what it has observed in the past, and are not wellable to adapt to novel situations.
For a more de-tailed account of statistical and trainable approachesto NLG as well as their advantages and drawbacks,see (Lemon, 2008).8 ConclusionWe presented a novel approach to text generationthat applies hierarchical reinforcement learning tooptimise the following interrelated NLG tasks: con-tent selection, choice of text structure, referring ex-pressions and surface structure.
Generation deci-sions in these areas were learnt based on three differ-ent variables: the type of user, the properties of thespatial environment and the proportion of alignmentand variation in texts.
Based on a simulated envi-ronment, we compared the results of different poli-cies and demonstrated that the learnt policy outper-forms a baseline that chooses actions without takingcontextual variables into account.
Future work cantransfer our approach to different domains of appli-cation or to other NLG tasks.
In addition, our pre-liminary simulation results should be confirmed inan evaluation study with real users.AcknowledgementsThis work was partly supported by DFG SFB/TR8?Spatial Cognition?.ReferencesBarto, A. G. and Mahadevan, S. (2003).
Recent Ad-vances in Hierarchical Reinforcement Learning.
Dis-crete Event Dynamic Systems, 13:2003.Bateman, J.
A.
(1997).
Enabling technology for multi-lingual natural language generation: the KPML devel-opment environment.
Natural Language Engineering,3(1):15?55.Belz, A. and Reiter, E. (2006).
Comparing automatic andhuman evaluation of nlg systems.
In In Proc.
EACL06,pages 313?320.Bock, K. (1986).
Syntactic persistence in language pro-duction.
Cognitive Psychology, 18.Branigan, H. P., Pickering, M. J., and Cleland, A.
(2000).Syntactic coordination in dialogue.
Cognition, 75.Clark, E. (1987).
The principle of contrast: A constrainton language acquisition.
In MacWhinney, B., edi-tor, Mechanisms of Language Acquisition, pages 1?33.Lawrence Erlbaum Assoc., Hillsdale, NJ.Clark, H. H. and Wilkes-Gibbs, D. (1986).
Referring asa colloborative process.
Cognition, 22.Cuaya?huitl, H. (2009).
Hierarchical ReinforcementLearning for Spoken Dialogue Systems.
PhD thesis,School of Informatics, University of Edinburgh.Cuaya?huitl, H., Dethlefs, N., Richter, K.-F., Tenbrink, T.,and Bateman, J.
(2010).
A dialogue system for indoorwayfinding using text-based natural language.
In-ternational Journal of Computational Linguistics andApplications, ISSN 0976-0962.Cuayahuitl, H., Renals, S., Lemon, O., and Shimodaira,H.
(2010).
Evaluation of a hierarchical reinforcementlearning spoken dialogue system.
Computer Speechand Language, 24(2):395?429.Dietterich, T. G. (1999).
Hierarchical reinforcementlearning with the maxq value function decomposition.Journal of Artificial Intelligence Research, 13:227?303.Garrod, S. and Anderson, A.
(1987).
Saying What YouMean in Dialogue: A Study in conceptual and seman-tic co-ordination.
Cognition, 27.Halliday, M. A. K. and Hasan, R. (1976).
Cohesion inEnglish.
Longman, London.Halliday, M. A. K. and Matthiessen, C. M. I. M. (2004).An Introduction to Functional Grammar.
EdwardArnold, London, 3rd edition.Hirst, G. and St-Onge, D. (1998).
Lexical chains as rep-resentations of context for the detection and correctionof malapropisms.
In Fellbaum, C., editor, WordNet:An Electronic Database and Some of its Applications,pages 305?332.
MIT Press.Janarthanam, S. and Lemon, O.
(2009).
Learning lexi-cal alignment policies for generating referring expres-sions in spoken dialogue systems.
In ENLG ?09: Pro-ceedings of the 12th European Workshop on NaturalLanguage Generation, pages 74?81, Morristown, NJ,USA.Kasper, R. (1989).
SPL: A Sentence Plan Language fortext generation.
Technical report, USC/ISI.Langkilde, I. and Knight, K. (1998).
Generation that ex-ploits corpus-based statistical knowledge.
In ACL-36:Proceedings of the 36th Annual Meeting of the As-sociation for Computational Linguistics and 17th In-ternational Conference on Computational Linguistics,pages 704?710.Lemon, O.
(2008).
Adaptive Natural Language Gener-ation in Dialogue using Reinforcement Learning.
InSemDial.Levin, E., Pieraccini, R., and Eckert, W. (2000).
Astochastic model of computer-human interaction forlearning dialogue strategies.
IEEE Transactions onSpeech and Audio Processing, 8.Pickering, M. J. and Garrod, S. (2004).
Toward a mecha-nistc psychology of dialog.
Behavioral and Brain Sci-ences, 27.Reiter, E. and Dale, R. (1997).
Building applied naturallanguage generation systems.
Natural Language En-gineering, 3(1):57?87.Rieser, V. and Lemon, O.
(2009).
Natural language gen-eration as planning under uncertainty for spoken dia-logue systems.
In EACL ?09: Proceedings of the 12thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 683?691,Morristown, NJ, USA.Scott, D. and Moore, J.
(2006).
An NLG evaluation com-petition?
eight reasons to be cautious.
Technical re-port.Sutton, R. S. and Barto, A. G. (1998).
ReinforcementLearning: An Introduction.
MIT Press, Cambridge,MA, USA.Toutanova, K. and Manning, C. D. (2000).
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In Proceedings of the 2000 JointSIGDAT conference on Empirical methods in naturallanguage processing and very large corpora, pages63?70, Morristown, NJ, USA.
Association for Com-putational Linguistics.Viethen, J. and Dale, R. (2006).
Towards the evaluationof referring expression generation.
In In Proceedingsof the 4th Australiasian Language Technology Work-shop, pages 115?122.
