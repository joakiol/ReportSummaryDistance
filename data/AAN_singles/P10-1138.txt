Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1357?1366,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsWikipedia as Sense Inventory to Improve Diversity in Web Search ResultsCelina Santamar?
?a, Julio Gonzalo and Javier Artilesnlp.uned.esUNED, c/Juan del Rosal, 16, 28040 Madrid, Spaincelina.santamaria@gmail.com julio@lsi.uned.es javart@bec.uned.esAbstractIs it possible to use sense inventories toimprove Web search results diversity forone word queries?
To answer this ques-tion, we focus on two broad-coverage lex-ical resources of a different nature: Word-Net, as a de-facto standard used in WordSense Disambiguation experiments; andWikipedia, as a large coverage, updatedencyclopaedic resource which may have abetter coverage of relevant senses in Webpages.Our results indicate that (i) Wikipedia hasa much better coverage of search results,(ii) the distribution of senses in search re-sults can be estimated using the internalgraph structure of the Wikipedia and therelative number of visits received by eachsense in Wikipedia, and (iii) associatingWeb pages to Wikipedia senses with sim-ple and efficient algorithms, we can pro-duce modified rankings that cover 70%more Wikipedia senses than the originalsearch engine rankings.1 MotivationThe application of Word Sense Disambiguation(WSD) to Information Retrieval (IR) has been sub-ject of a significant research effort in the recentpast.
The essential idea is that, by indexing andmatching word senses (or even meanings) , the re-trieval process could better handle polysemy andsynonymy problems (Sanderson, 2000).
In prac-tice, however, there are two main difficulties: (i)for long queries, IR models implicitly performdisambiguation, and thus there is little room forimprovement.
This is the case with most stan-dard IR benchmarks, such as TREC (trec.nist.gov)or CLEF (www.clef-campaign.org) ad-hoc collec-tions; (ii) for very short queries, disambiguationmay not be possible or even desirable.
This isoften the case with one word and even two wordqueries in Web search engines.In Web search, there are at least three ways ofcoping with ambiguity:?
Promoting diversity in the search results(Clarke et al, 2008): given the query ?oa-sis?, the search engine may try to include rep-resentatives for different senses of the word(such as the Oasis band, the Organizationfor the Advancement of Structured Informa-tion Standards, the online fashion store, etc.
)among the top results.
Search engines aresupposed to handle diversity as one of themultiple factors that influence the ranking.?
Presenting the results as a set of (labelled)clusters rather than as a ranked list (Carpinetoet al, 2009).?
Complementing search results with searchsuggestions (e.g.
?oasis band?, ?oasis fash-ion store?)
that serve to refine the query in theintended way (Anick, 2003).All of them rely on the ability of the search en-gine to cluster search results, detecting topic simi-larities.
In all of them, disambiguation is implicit,a side effect of the process but not its explicit tar-get.
Clustering may detect that documents aboutthe Oasis band and the Oasis fashion store dealwith unrelated topics, but it may as well detecta group of documents discussing why one of theOasis band members is leaving the band, and an-other group of documents about Oasis band lyrics;both are different aspects of the broad topic Oa-sis band.
A perfect hierarchical clustering shoulddistinguish between the different Oasis senses at afirst level, and then discover different topics withineach of the senses.Is it possible to use sense inventories to improvesearch results for one word queries?
To answer1357this question, we will focus on two broad-coveragelexical resources of a different nature: WordNet(Miller et al, 1990), as a de-facto standard usedin Word Sense Disambiguation experiments andmany other Natural Language Processing researchfields; and Wikipedia (www.wikipedia.org), as alarge coverage and updated encyclopedic resourcewhich may have a better coverage of relevantsenses in Web pages.Our hypothesis is that, under appropriate con-ditions, any of the above mechanisms (clustering,search suggestions, diversity) might benefit froman explicit disambiguation (classification of pagesin the top search results) using a wide-coveragesense inventory.
Our research is focused on fourrelevant aspects of the problem:1.
Coverage: Are Wikipedia/Wordnet sensesrepresentative of search results?
Otherwise,trying to make a disambiguation in terms of afixed sense inventory would be meaningless.2.
If the answer to (1) is positive, the reversequestion is also interesting: can we estimatesearch results diversity using our sense inven-tories?3.
Sense frequencies: knowing sense frequen-cies in (search results) Web pages is crucialto have a usable sense inventory.
Is it possi-ble to estimate Web sense frequencies fromcurrently available information?4.
Classification: The association of Web pagesto word senses must be done with some unsu-pervised algorithm, because it is not possibleto hand-tag training material for every pos-sible query word.
Can this classification bedone accurately?
Can it be effective to pro-mote diversity in search results?In order to provide an initial answer to thesequestions, we have built a corpus consisting of 40nouns and 100 Google search results per noun,manually annotated with the most appropriateWordnet and Wikipedia senses.
Section 2 de-scribes how this corpus has been created, and inSection 3 we discuss WordNet and Wikipedia cov-erage of search results according to our testbed.As this initial results clearly discard Wordnet asa sense inventory for the task, the rest of the pa-per mainly focuses on Wikipedia.
In Section 4 weestimate search results diversity from our testbed,finding that the use of Wikipedia could substan-tially improve diversity in the top results.
In Sec-tion 5 we use the Wikipedia internal link structureand the number of visits per page to estimate rel-ative frequencies for Wikipedia senses, obtainingan estimation which is highly correlated with ac-tual data in our testbed.
Finally, in Section 6 wediscuss a few strategies to classify Web pages intoword senses, and apply the best classifier to en-hance diversity in search results.
The paper con-cludes with a discussion of related work (Section7) and an overall discussion of our results in Sec-tion 8.2 Test Set2.1 Set of WordsThe most crucial step in building our test set ischoosing the set of words to be considered.
Weare looking for words which are susceptible toform a one-word query for a Web search engine,and therefore we should focus on nouns whichare used to denote one or more named entities.At the same time we want to have some degreeof comparability with previous research on WordSense Disambiguation, which points to noun setsused in Senseval/SemEval evaluation campaigns1.Our budget for corpus annotation was enough fortwo persons-month, which limited us to handle40 nouns (usually enough to establish statisticallysignificant differences between WSD algorithms,although obviously limited to reach solid figuresabout the general behaviour of words in the Web).With these arguments in mind, we decided tochoose: (i) 15 nouns from the Senseval-3 lexi-cal sample dataset, which have been previouslyemployed by (Mihalcea, 2007) in a related ex-periment (see Section 7); (ii) 25 additional wordswhich satisfy two conditions: they are all am-biguous, and they are all names for music bandsin one of their senses (not necessarily the mostsalient).
The Senseval set is: {argument, arm,atmosphere, bank, degree, difference, disc, im-age, paper, party, performance, plan, shelter,sort, source}.
The bands set is {amazon, apple,camel, cell, columbia, cream, foreigner, fox, gen-esis, jaguar, oasis, pioneer, police, puma, rain-bow, shell, skin, sun, tesla, thunder, total, traffic,trapeze, triumph, yes}.For each noun, we looked up all its possiblesenses in WordNet 3.0 and in Wikipedia (using1http://senseval.org1358Table 1: Coverage of Search Results: Wikipedia vs. WordNetWikipedia WordNet# senses # documents # senses # documentsavailable/used assigned to some sense available/used assigned to some senseSenseval set 242/100 877 (59%) 92/52 696 (46%)Bands set 640/174 1358 (54%) 78/39 599 (24%)Total 882/274 2235 (56%) 170/91 1295 (32%)Wikipedia disambiguation pages).
Wikipedia hasan average of 22 senses per noun (25.2 in theBands set and 16.1 in the Senseval set), and Word-net a much smaller figure, 4.5 (3.12 for the Bandsset and 6.13 for the Senseval set).
For a conven-tional dictionary, a higher ambiguity might indi-cate an excess of granularity; for an encyclopaedicresource such as Wikipedia, however, it is justan indication of larger coverage.
Wikipedia en-tries for camel which are not in WordNet, for in-stance, include the Apache Camel routing and me-diation engine, the British rock band, the brandof cigarettes, the river in Cornwall, and the WorldWorld War I fighter biplane.2.2 Set of DocumentsWe retrieved the 150 first ranked documents foreach noun, by submitting the nouns as queries to aWeb search engine (Google).
Then, for each doc-ument, we stored both the snippet (small descrip-tion of the contents of retrieved document) and thewhole HTML document.
This collection of docu-ments contain an implicit new inventory of senses,based on Web search, as documents retrieved bya noun query are associated with some sense ofthe noun.
Given that every document in the topWeb search results is supposed to be highly rele-vant for the query word, we assume a ?one senseper document?
scenario, although we allow an-notators to assign more than one sense per doc-ument.
In general this assumption turned out to becorrect except in a few exceptional cases (such asWikipedia disambiguation pages): only nine docu-ments received more than one WordNet sense, and44 (1.1% of all annotated pages) received morethan one Wikipedia sense.2.3 Manual AnnotationWe implemented an annotation interface whichstored all documents and a short description forevery Wordnet and Wikipedia sense.
The annota-tors had to decide, for every document, whetherthere was one or more appropriate senses in eachof the dictionaries.
They were instructed to pro-vide annotations for 100 documents per name; ifan URL in the list was corrupt or not available,it had to be discarded.
We provided 150 docu-ments per name to ensure that the figure of 100 us-able documents per name could be reached with-out problems.Each judge provided annotations for the 4,000documents in the final data set.
In a second round,they met and discussed their independent annota-tions together, reaching a consensus judgement forevery document.3 Coverage of Web Search Results:Wikipedia vs WordnetTable 1 shows how Wikipedia and Wordnet coverthe senses in search results.
We report each nounsubset separately (Senseval and bands subsets) aswell as aggregated figures.The most relevant fact is that, unsurprisingly,Wikipedia senses cover much more search results(56%) than Wordnet (32%).
If we focus on thetop ten results, in the bands subset (which shouldbe more representative of plausible web queries)Wikipedia covers 68% of the top ten documents.This is an indication that it can indeed be usefulfor promoting diversity or help clustering searchresults: even if 32% of the top ten documents arenot covered by Wikipedia, it is still a representa-tive source of senses in the top search results.We have manually examined all documentsin the top ten results that are not covered byWikipedia: a majority of the missing senses con-sists of names of (generally not well-known) com-panies (45%) and products or services (26%); theother frequent type (12%) of non annotated doc-ument is disambiguation pages (from Wikipediaand also from other dictionaries).It is also interesting to examine the degree ofoverlap between Wikipedia and Wordnet senses.Being two different types of lexical resource,they might have some degree of complementar-ity.
Table 2 shows, however, that this is not thecase: most of the (annotated) documents either fitWikipedia senses (26%) or both Wikipedia andWordnet (29%), and just 3% fit Wordnet only.1359Table 2: Overlap between Wikipedia and Wordnet in Search Results# documents annotated withWikipedia & Wordnet Wikipedia only Wordnet only noneSenseval set 607 (40%) 270 (18%) 89 (6%) 534 (36%)Bands set 572 (23%) 786 (31%) 27 (1%) 1115 (45%)Total 1179 (29%) 1056 (26%) 116 (3%) 1649 (41%)Therefore, Wikipedia seems to extend the cover-age of Wordnet rather than providing complemen-tary sense information.
If we wanted to extend thecoverage of Wikipedia, the best strategy seems tobe to consider lists of companies, products and ser-vices, rather than complementing Wikipedia withadditional sense inventories.4 Diversity in Google Search ResultsOnce we know that Wikipedia senses are a rep-resentative subset of actual Web senses (coveringmore than half of the documents retrieved by thesearch engine), we can test how well search resultsrespect diversity in terms of this subset of senses.Table 3 displays the number of different sensesfound at different depths in the search results rank,and the average proportion of total senses that theyrepresent.
These results suggest that diversity isnot a major priority for ranking results: the topten results only cover, in average, 3 Wikipediasenses (while the average number of senses listedin Wikipedia is 22).
When considering the first100 documents, this number grows up to 6.85senses per noun.Another relevant figure is the frequency of themost frequent sense for each word: in average,63% of the pages in search results belong to themost frequent sense of the query word.
This isroughly comparable with most frequent sense fig-ures in standard annotated corpora such as Sem-cor (Miller et al, 1993) and the Senseval/Semevaldata sets, which suggests that diversity may notplay a major role in the current Google ranking al-gorithm.Of course this result must be taken with care,because variability between words is high and un-predictable, and we are using only 40 nouns forour experiment.
But what we have is a positiveindication that Wikipedia could be used to im-prove diversity or cluster search results: poten-tially the first top ten results could cover 6.15 dif-ferent senses in average (see Section 6.5), whichwould be a substantial growth.5 Sense Frequency Estimators forWikipediaWikipedia disambiguation pages contain no sys-tematic information about the relative importanceof senses for a given word.
Such information,however, is crucial in a lexicon, because sense dis-tributions tend to be skewed, and knowing themcan help disambiguation algorithms.We have attempted to use two estimators of ex-pected sense distribution:?
Internal relevance of a word sense, measuredas incoming links for the URL of a givensense in Wikipedia.?
External relevance of a word sense, measuredas the number of visits for the URL of a givensense (as reported in http://stats.grok.se).The number of internal incoming links is ex-pected to be relatively stable for Wikipedia arti-cles.
As for the number of visits, we performeda comparison of the number of visits received bythe bands noun subset in May, June and July 2009,finding a stable-enough scenario with one notori-ous exception: the number of visits to the nounTesla raised dramatically in July, because July 10was the anniversary of the birth of Nicola Tesla,and a special Google logo directed users to theWikipedia page for the scientist.We have measured correlation between the rela-tive frequencies derived from these two indicatorsand the actual relative frequencies in our testbed.Therefore, for each noun w and for each sense wi,we consider three values: (i) proportion of doc-uments retrieved for w which are manually as-signed to each sense wi; (ii) inlinks(wi): rela-tive amount of incoming links to each sense wi;and (iii) visits(wi): relative number of visits to theURL for each sense wi.We have measured the correlation betweenthese three values using a linear regression corre-lation coefficient, which gives a correlation valueof .54 for the number of visits and of .71 for thenumber of incoming links.
Both estimators seem1360Table 3: Diversity in Search Results according to Wikipediaaverage # senses in search results average coverage of Wikipedia sensesBands set Senseval set Total Bands set Senseval set TotalFirst 10 docs 2.88 3.2 3.00 .21 .21 .21First 25 4.44 4.8 4.58 .28 .33 .30First 50 5.56 5.47 5.53 .33 .36 .34First 75 6.56 6.33 6.48 .37 .43 .39First 100 6.96 6.67 6.85 .38 .45 .41to be positively correlated with real relative fre-quencies in our testbed, with a strong preferencefor the number of links.We have experimented with weighted combina-tions of both indicators, using weights of the form(k, 1?
k), k ?
{0, 0.1, 0.2 .
.
.
1}, reaching a max-imal correlation of .73 for the following weights:freq(wi) = 0.9?inlinks(wi)+0.1?visits(wi) (1)This weighted estimator provides a slight ad-vantage over the use of incoming links only (.73vs .71).
Overall, we have an estimator which hasa strong correlation with the distribution of sensesin our testbed.
In the next section we will test itsutility for disambiguation purposes.6 Association of Wikipedia Senses toWeb PagesWe want to test whether the information providedby Wikipedia can be used to classify search resultsaccurately.
Note that we do not want to considerapproaches that involve a manual creation of train-ing material, because they can?t be used in prac-tice.Given a Web page p returned by the searchengine for the query w, and the set of sensesw1 .
.
.
wn listed in Wikipedia, the task is to assignthe best candidate sense to p. We consider twodifferent techniques:?
A basic Information Retrieval approach,where the documents and the Wikipediapages are represented using a Vector SpaceModel (VSM) and compared with a standardcosine measure.
This is a basic approachwhich, if successful, can be used efficientlyto classify search results.?
An approach based on a state-of-the-art su-pervised WSD system, extracting training ex-amples automatically from Wikipedia con-tent.We also compute two baselines:?
A random assignment of senses (precision iscomputed as the inverse of the number ofsenses, for every test case).?
A most frequent sense heuristic which usesour estimation of sense frequencies and as-signs the same sense (the most frequent) toall documents.Both are naive baselines, but it must be notedthat the most frequent sense heuristic is usuallyhard to beat for unsupervised WSD algorithms inmost standard data sets.We now describe each of the two main ap-proaches in detail.6.1 VSM ApproachFor each word sense, we represent its Wikipediapage in a (unigram) vector space model, assigningstandard tf*idf weights to the words in the docu-ment.
idf weights are computed in two differentways:1.
Experiment VSM computes inverse docu-ment frequencies in the collection of re-trieved documents (for the word being con-sidered).2.
Experiment VSM-GT uses the statistics pro-vided by the Google Terabyte collection(Brants and Franz, 2006), i.e.
it replaces thecollection of documents with statistics from arepresentative snapshot of the Web.3.
Experiment VSM-mixed combines statisticsfrom the collection and from the GoogleTerabyte collection, following (Chen et al,2009).The document p is represented in the same vec-tor space as the Wikipedia senses, and it is com-pared with each of the candidate senses wi via thecosine similarity metric (we have experimented1361with other similarity metrics such as ?2, but dif-ferences are irrelevant).
The sense with the high-est similarity to p is assigned to the document.
Incase of ties (which are rare), we pick the first sensein the Wikipedia disambiguation page (which inpractice is like a random decision, because sensesin disambiguation pages do not seem to be orderedaccording to any clear criteria).We have also tested a variant of this approachwhich uses the estimation of sense frequenciespresented above: once the similarities are com-puted, we consider those cases where two or moresenses have a similar score (in particular, all senseswith a score greater or equal than 80% of the high-est score).
In that cases, instead of using the smallsimilarity differences to select a sense, we pick upthe one which has the largest frequency accordingto our estimator.
We have applied this strategy tothe best performing system, VSM-GT, resulting inexperiment VSM-GT+freq.6.2 WSD ApproachWe have used TiMBL (Daelemans et al, 2001),a state-of-the-art supervised WSD system whichuses Memory-Based Learning.
The key, in thiscase, is how to extract learning examples from theWikipedia automatically.
For each word sense, webasically have three sources of examples: (i) oc-currences of the word in the Wikipedia page forthe word sense; (ii) occurrences of the word inWikipedia pages pointing to the page for the wordsense; (iii) occurrences of the word in externalpages linked in the Wikipedia page for the wordsense.After an initial manual inspection, we decidedto discard external pages for being too noisy, andwe focused on the first two options.
We tried threealternatives:?
TiMBL-core uses only the examples foundin the page for the sense being trained.?
TiMBL-inlinks uses the examples found inWikipedia pages pointing to the sense beingtrained.?
TiMBL-all uses both sources of examples.In order to classify a page p with respect to thesenses for a word w, we first disambiguate all oc-currences of w in the page p. Then we choose thesense which appears most frequently in the pageaccording to TiMBL results.
In case of ties wepick up the first sense listed in the Wikipedia dis-ambiguation page.We have also experimented with a variant ofthe approach that uses our estimation of sense fre-quencies, similarly to what we did with the VSMapproach.
In this case, (i) when there is a tie be-tween two or more senses (which is much morelikely than in the VSM approach), we pick up thesense with the highest frequency according to ourestimator; and (ii) when no sense reaches 30% ofthe cases in the page to be disambiguated, we alsoresort to the most frequent sense heuristic (amongthe candidates for the page).
This experiment iscalled TiMBL-core+freq (we discarded ?inlinks?and ?all?
versions because they were clearly worsethan ?core?
).6.3 Classification ResultsTable 4 shows classification results.
The accuracyof systems is reported as precision, i.e.
the numberof pages correctly classified divided by the totalnumber of predictions.
This is approximately thesame as recall (correctly classified pages dividedby total number of pages) for our systems, becausethe algorithms provide an answer for every pagecontaining text (actual coverage is 94% becausesome pages only contain text as part of an imagefile such as photographs and logotypes).Table 4: Classification ResultsExperiment Precisionrandom .19most frequent sense (estimation) .46TiMBL-core .60TiMBL-inlinks .50TiMBL-all .58TiMBL-core+freq .67VSM .67VSM-GT .68VSM-mixed .67VSM-GT+freq .69All systems are significantly better than therandom and most frequent sense baselines (usingp < 0.05 for a standard t-test).
Overall, both ap-proaches (using TiMBL WSD machinery and us-ing VSM) lead to similar results (.67 vs. .69),which would make VSM preferable because it isa simpler and more efficient approach.
Taking a1362Figure 1: Precision/Coverage curves for VSM-GT+freq classification algorithmcloser look at the results with TiMBL, there are acouple of interesting facts:?
There is a substantial difference between us-ing only examples taken from the WikipediaWeb page for the sense being trained(TiMBL-core, .60) and using examples fromthe Wikipedia pages pointing to that page(TiMBL-inlinks, .50).
Examples taken fromrelated pages (even if the relationship is closeas in this case) seem to be too noisy for thetask.
This result is compatible with findingsin (Santamar?
?a et al, 2003) using the OpenDirectory Project to extract examples auto-matically.?
Our estimation of sense frequencies turnsout to be very helpful for cases where ourTiMBL-based algorithm cannot provide ananswer: precision rises from .60 (TiMBL-core) to .67 (TiMBL-core+freq).
The differ-ence is statistically significant (p < 0.05) ac-cording to the t-test.As for the experiments with VSM, the varia-tions tested do not provide substantial improve-ments to the baseline (which is .67).
Using idf fre-quencies obtained from the Google Terabyte cor-pus (instead of frequencies obtained from the setof retrieved documents) provides only a small im-provement (VSM-GT, .68), and adding the esti-mation of sense frequencies gives another smallimprovement (.69).
Comparing the baseline VSMwith the optimal setting (VSM-GT+freq), the dif-ference is small (.67 vs .69) but relatively robust(p = 0.066 according to the t-test).Remarkably, the use of frequency estimationsis very helpful for the WSD approach but not forthe SVM one, and they both end up with similarperformance figures; this might indicate that usingfrequency estimations is only helpful up to certainprecision ceiling.6.4 Precision/Coverage Trade-offAll the above experiments are done at maximalcoverage, i.e., all systems assign a sense for everydocument in the test collection (at least for everydocument with textual content).
But it is possibleto enhance search results diversity without anno-tating every document (in fact, not every documentcan be assigned to a Wikipedia sense, as we havediscussed in Section 3).
Thus, it is useful to inves-tigate which is the precision/coverage trade-off inour dataset.
We have experimented with the bestperforming system (VSM-GT+freq), introducinga similarity threshold: assignment of a documentto a sense is only done if the similarity of the doc-ument to the Wikipedia page for the sense exceedsthe similarity threshold.We have computed precision and coverage forevery threshold in the range [0.00?
0.90] (beyond0.90 coverage was null) and represented the resultsin Figure 1 (solid line).
The graph shows that we1363can classify around 20% of the documents with aprecision above .90, and around 60% of the docu-ments with a precision of .80.Note that we are reporting disambiguation re-sults using a conventional WSD test set, i.e., onein which every test case (every document) hasbeen manually assigned to some Wikipedia sense.But in our Web Search scenario, 44% of thedocuments were not assigned to any Wikipediasense: in practice, our classification algorithmwould have to cope with all this noise as well.Figure 1 (dotted line) shows how the preci-sion/coverage curve is affected when the algo-rithm attempts to disambiguate all documents re-trieved by Google, whether they can in fact be as-signed to a Wikipedia sense or not.
At a coverageof 20%, precision drops approximately from .90 to.70, and at a coverage of 60% it drops from .80 to.50.
We now address the question of whether thisperformance is good enough to improve search re-sults diversity in practice.6.5 Using Classification to Promote DiversityWe now want to estimate how the reported clas-sification accuracy may perform in practice to en-hance diversity in search results.
In order to pro-vide an initial answer to this question, we havere-ranked the documents for the 40 nouns in ourtestbed, using our best classifier (VSM-GT+freq)and making a list of the top-ten documents withthe primary criterion of maximising the numberof senses represented in the set, and the secondarycriterion of maximising the similarity scores of thedocuments to their assigned senses.
The algorithmproceeds as follows: we fill each position in therank (starting at rank 1), with the document whichhas the highest similarity to some of the senseswhich are not yet represented in the rank; once allsenses are represented, we start choosing a secondrepresentative for each sense, following the samecriterion.
The process goes on until the first tendocuments are selected.We have also produced a number of alternativerankings for comparison purposes:?
clustering (centroids): this method ap-plies Hierarchical Agglomerative Clustering?
which proved to be the most competitiveclustering algorithm in a similar task (Artileset al, 2009) ?
to the set of search results,forcing the algorithm to create ten clusters.The centroid of each cluster is then selectedTable 5: Enhancement of Search Results Diversityrank@10 # senses coverageOriginal rank 2.80 49%Wikipedia 4.75 77%clustering (centroids) 2.50 42%clustering (top ranked) 2.80 46%random 2.45 43%upper bound 6.15 97%as one of the top ten documents in the newrank.?
clustering (top ranked): Applies the sameclustering algorithm, but this time the topranked document (in the original Googlerank) of each cluster is selected.?
random: Randomly selects ten documentsfrom the set of retrieved results.?
upper bound: This is the maximal diversitythat can be obtained in our testbed.
Note thatcoverage is not 100%, because some wordshave more than ten meanings in Wikipediaand we are only considering the top ten doc-uments.All experiments have been applied on the fullset of documents in the testbed, including thosewhich could not be annotated with any Wikipediasense.
Coverage is computed as the ratio of sensesthat appear in the top ten results compared to thenumber of senses that appear in all search results.Results are presented in Table 5.
Note that di-versity in the top ten documents increases froman average of 2.80 Wikipedia senses representedin the original search engine rank, to 4.75 in themodified rank (being 6.15 the upper bound), withthe coverage of senses going from 49% to 77%.With a simple VSM algorithm, the coverage ofWikipedia senses in the top ten results is 70%larger than in the original ranking.Using Wikipedia to enhance diversity seems towork much better than clustering: both strategiesto select a representative from each cluster are un-able to improve the diversity of the original rank-ing.
Note, however, that our evaluation has a biastowards using Wikipedia, because only Wikipediasenses are considered to estimate diversity.Of course our results do not imply that theWikipedia modified rank is better than the original1364Google rank: there are many other factors that in-fluence the final ranking provided by a search en-gine.
What our results indicate is that, with simpleand efficient algorithms, Wikipedia can be used asa reference to improve search results diversity forone-word queries.7 Related WorkWeb search results clustering and diversity insearch results are topics that receive an increas-ing attention from the research community.
Diver-sity is used both to represent sub-themes in a broadtopic, or to consider alternative interpretations forambiguous queries (Agrawal et al, 2009), whichis our interest here.
Standard IR test collections donot usually consider ambiguous queries, and arethus inappropriate to test systems that promote di-versity (Sanderson, 2008); it is only recently thatappropriate test collections are being built, such as(Paramita et al, 2009) for image search and (Ar-tiles et al, 2009) for person name search.
We seeour testbed as complementary to these ones, andexpect that it can contribute to foster research onsearch results diversity.To our knowledge, Wikipedia has not explicitlybeen used before to promote diversity in searchresults; but in (Gollapudi and Sharma, 2009), itis used as a gold standard to evaluate diversifica-tion algorithms: given a query with a Wikipediadisambiguation page, an algorithm is evaluated aspromoting diversity when different documents inthe search results are semantically similar to dif-ferent Wikipedia pages (describing the alternativesenses of the query).
Although semantic similarityis measured automatically in this work, our resultsconfirm that this evaluation strategy is sound, be-cause Wikipedia senses are indeed representativeof search results.
(Clough et al, 2009) analyses query diversity ina Microsoft Live Search, using click entropy andquery reformulation as diversity indicators.
It wasfound that at least 9.5% - 16.2% of queries couldbenefit from diversification, although no correla-tion was found between the number of senses of aword in Wikipedia and the indicators used to dis-cover diverse queries.
This result does not discard,however, that queries where applying diversity isuseful cannot benefit from Wikipedia as a senseinventory.In the context of clustering, (Carmel et al,2009) successfully employ Wikipedia to enhanceautomatic cluster labeling, finding that Wikipedialabels agree with manual labels associated by hu-mans to a cluster, much more than with signif-icant terms that are extracted directly from thetext.
In a similar line, both (Gabrilovich andMarkovitch, 2007) and (Syed et al, 2008) provideevidence suggesting that categories of Wikipediaarticles can successfully describe common con-cepts in documents.In the field of Natural Language Processing,there has been successful attempts to connectWikipedia entries to Wordnet senses: (Ruiz-Casado et al, 2005) reports an algorithm thatprovides an accuracy of 84%.
(Mihalcea, 2007)uses internal Wikipedia hyperlinks to derive sense-tagged examples.
But instead of using Wikipediadirectly as sense inventory, Mihalcea then manu-ally maps Wikipedia senses into Wordnet senses(claiming that, at the time of writing the paper,Wikipedia did not consistently report ambiguityin disambiguation pages) and shows that a WSDsystem based on acquired sense-tagged examplesreaches an accuracy well beyond an (informed)most frequent sense heuristic.8 ConclusionsWe have investigated whether generic lexical re-sources can be used to promote diversity in Websearch results for one-word, ambiguous queries.We have compared WordNet and Wikipedia andarrived to a number of conclusions: (i) unsurpris-ingly, Wikipedia has a much better coverage ofsenses in search results, and is therefore more ap-propriate for the task; (ii) the distribution of sensesin search results can be estimated using the in-ternal graph structure of the Wikipedia and therelative number of visits received by each sensein Wikipedia, and (iii) associating Web pages toWikipedia senses with simple and efficient algo-rithms, we can produce modified rankings thatcover 70% more Wikipedia senses than the orig-inal search engine rankings.We expect that the testbed created for this re-search will complement the - currently short - setof benchmarking test sets to explore search re-sults diversity and query ambiguity.
Our testbedis publicly available for research purposes athttp://nlp.uned.es.Our results endorse further investigation on theuse of Wikipedia to organize search results.
Somelimitations of our research, however, must be1365noted: (i) the nature of our testbed (with everysearch result manually annotated in terms of twosense inventories) makes it too small to extractsolid conclusions on Web searches (ii) our workdoes not involve any study of diversity from thepoint of view of Web users (i.e.
when a Webquery addresses many different use needs in prac-tice); research in (Clough et al, 2009) suggeststhat word ambiguity in Wikipedia might not be re-lated with diversity of search needs; (iii) we havetested our classifiers with a simple re-ordering ofsearch results to test how much diversity can beimproved, but a search results ranking depends onmany other factors, some of them more crucialthan diversity; it remains to be tested how can weuse document/Wikipedia associations to improvesearch results clustering (for instance, providingseeds for the clustering process) and to providesearch suggestions.AcknowledgmentsThis work has been partially funded by the Span-ish Government (project INES/Text-Mess) and theXunta de Galicia.ReferencesR.
Agrawal, S. Gollapudi, A. Halverson, and S. Leong.2009.
Diversifying Search Results.
In Proc.
ofWSDM?09.
ACM.P.
Anick.
2003.
Using Terminological Feedback forWeb Search Refinement : a Log-based Study.
InProc.
ACM SIGIR 2003, pages 88?95.
ACM NewYork, NY, USA.J.
Artiles, J. Gonzalo, and S. Sekine.
2009.
WePS2 Evaluation Campaign: overview of the Web Peo-ple Search Clustering Task.
In 2nd Web PeopleSearch Evaluation Workshop (WePS 2009), 18thWWW Conference.
2009.T.
Brants and A. Franz.
2006.
Web 1T 5-gram, version1.
Philadelphia: Linguistic Data Consortium.D.
Carmel, H. Roitman, and N. Zwerdling.
2009.
En-hancing Cluster Labeling using Wikipedia.
In Pro-ceedings of the 32nd international ACM SIGIR con-ference on Research and development in informationretrieval, pages 139?146.
ACM.C.
Carpineto, S. Osinski, G. Romano, and DawidWeiss.
2009.
A Survey of Web Clustering Engines.ACM Computing Surveys, 41(3).Y.
Chen, S. Yat Mei Lee, and C. Huang.
2009.PolyUHK: A Robust Information Extraction Systemfor Web Personal Names.
In Proc.
WWW?09 (WePS-2 Workshop).
ACM.C.
Clarke, M. Kolla, G. Cormack, O. Vechtomova,A.
Ashkan, S. Bu?ttcher, and I. MacKinnon.
2008.Novelty and Diversity in Information Retrieval Eval-uation.
In Proc.
SIGIR?08, pages 659?666.
ACM.P.
Clough, M. Sanderson, M. Abouammoh, S. Navarro,and M. Paramita.
2009.
Multiple Approaches toAnalysing Query Diversity.
In Proc.
of SIGIR 2009.ACM.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2001.
TiMBL: Tilburg MemoryBased Learner, version 4.0, Reference Guide.
Tech-nical report, University of Antwerp.E.
Gabrilovich and S. Markovitch.
2007.
ComputingSemantic Relatedness using Wikipedia-based Ex-plicit Semantic Analysis.
In Proceedings of The20th International Joint Conference on Artificial In-telligence (IJCAI), Hyderabad, India.S.
Gollapudi and A. Sharma.
2009.
An Axiomatic Ap-proach for Result Diversification.
In Proc.
WWW2009, pages 381?390.
ACM New York, NY, USA.R.
Mihalcea.
2007.
Using Wikipedia for AutomaticWord Sense Disambiguation.
In Proceedings ofNAACL HLT, volume 2007.G.
Miller, C. R. Beckwith, D. Fellbaum, Gross, andK.
Miller.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicograph,3(4).G.A Miller, C. Leacock, R. Tengi, and Bunker R. T.1993.
A Semantic Concordance.
In Proceedings ofthe ARPA WorkShop on Human Language Technol-ogy.
San Francisco, Morgan Kaufman.M.
Paramita, M. Sanderson, and P. Clough.
2009.
Di-versity in Photo Retrieval: Overview of the Image-CLEFPhoto task 2009.
CLEF working notes, 2009.M.
Ruiz-Casado, E. Alfonseca, and P. Castells.
2005.Automatic Assignment of Wikipedia EncyclopaedicEntries to Wordnet Synsets.
Advances in Web Intel-ligence, 3528:380?386.M.
Sanderson.
2000.
Retrieving with Good Sense.
In-formation Retrieval, 2(1):49?69.M.
Sanderson.
2008.
Ambiguous Queries: Test Col-lections Need More Sense.
In Proceedings of the31st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 499?506.
ACM New York, NY, USA.C.
Santamar?
?a, J. Gonzalo, and F. Verdejo.
2003.Automatic Association of Web Directories to WordSenses.
Computational Linguistics, 29(3):485?502.Z.
S. Syed, T. Finin, and Joshi.
A.
2008.
Wikipediaas an Ontology for Describing Documents.
In Proc.ICWSM?08.1366
