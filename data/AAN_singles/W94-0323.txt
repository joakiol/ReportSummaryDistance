Real-time Natural Language Generation in NL-SoarRobert RubinoffComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA 15123rubinoff@cs.cmu.eduJill Fain Lehman?
Computer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA 15123jef@cs.cmu.eduAbstractNL-Soar is a computer system that performs language com-prehension and generation within the framework of the Soararchitecture \[New90\].
NL-Soar provides language capabilitiesfor systems working in a real-time nvironment.
Responding inreal time to changing situations requires aflexible way to shiftcontrol between language and task operations.
To provide thisflexibility, NL-Soar organizes generation as a sequence of in-cremental steps that can be interleaved with task actions as thesituation requires.
This capability has been demonstrated viathe integration of NL-Soar with two different independently-developed Soar-based systems.1 Real-time generation and NL-SoarNL-Soar is a language comprehension a d generation facilitydesigned to provide integrated real-time 1 language capabilitiesfor other systems built within the Soar architecture \[New90\].In particular, this requires integrating NL-Soar's generationsubsystem with other task(s) that Soar is performing.
2 Onepossible way to achieve this integration would be to use gen-eration as a kind of "back end" which other task(s) can callas a subroutine whenever they need to say something.
Thisapproach is widely used in applications such as database querysystems or expert systems, where the main system invokes thegenerator to express the answer to a query or to explain someaspect of its reasoning or conclusions.In applications that need to provide real-time behavior,though, this "subroutine" approach is problematic.
There isno way for the task to interrupt generation i  order to handlesome other (perhaps urgent) work.
In addition, if generation isan unbounded process, it may proceed to complete an utterancethat may have become unnecessary oreven harmful because ofchanges in the situation; the task has no way to modify what itwants to say once generation has been invoked.
While the taskcould of course simply stop NL-Soar in either of these cases,I NL-Soar is being used in applications that perform inboth simulated andactual real-time environments.2Similar issues arise in NL-Soar's language comprehension subsystem,which is not described here; see \[LLN91, Lew93\] for a discussion ofthis partof NL-Soar.there is no way to guarantee that generation will be interruptedin a state from which it can recover if reinvoked later.Furthermore, the problem isn't simply one of the speed ofgeneration; using faster processors to run Soar won't eliminatethe difficulties.
It might seem that we could simply assumeNL-Soar can run fast enough to finish constructing an utterancebefore the task has time to do anything else, this is not the case.First, generation is potentially unbounded; no matter how fasta computer is used, there will still be occasions when NL-Soartakes longer than the task can afford to wait.
More significantly,this assumes that NL-Soar can absorb all the speedup; this isnot reasonable.
If  we have faster processors, we want all thetasks to share the speedup equally; a faster NL-Soar will beinvoked by a task that can respond more quickly as well, andwill thus want to interrupt NL-Soar more quickly.The underlying difficulty with the subroutine approach isthat generation can take an unbounded amount of time; in areal-time situation, we need to guarantee that generation can'tprevent he task from responding promptly to changes in thesituation.
Generation must be incremental and interruptible.NL-Soar accomplishes this by dividing generation into smallsteps that Can be interleaved with task steps.
In cases wherethe small steps can't be directly carried out and require morecomplex computation, the sub-steps are designed so that inter-ruptions leave the system in a clean state (although some workmay be lost and need to be redone).
This allows NL-Soar tooperate without limiting the task's ability to respond to thingsthat happen during generation, and vice versa.2 A Brief Introduction to Soar 3Soar is an architecture for building cognitive models and AIsystems that has been applied to a wide range of problems\[RLN93\].
Soar carries out a task by applying a sequence ofoperators to the state of a problem-space until it reaches astatethat solves the goal Soar is working on.
When Soar is unableto directly carry out some step (e.g.
selecting or applying anoperator or selecting a problem space), it creates a subgoal toresolve the impasse that is preventing it from proceeding.
Inresponse to this subgoal, Soar selects an appropriate problem3For a more detailed description f Soar than is possible here, see \[LNR87\]or \[New90"l.1997th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994space, sets up the initial (sub-)state, and proceeds to apply op-erators until it determines a way to resolve the initial impasse.This process is recursive; processing ina sub-goal can itself hitan impasse leading to a further sub-goal, and so on.
The ele-ments of the Soar architecture are shown in Figure 1; numbersin the rest of this section refer to parts of the figure.
(1) ~COGN~ON MEMORY( Immedi~e lyavml~le~owl~ge)(2)(11)chunkingWORKING MEMORY(Problem spaces in context)(?)
!8)(3) ~ "~.
lmpasse  (11)~ .
~ J  ~\ [P r?~cee~ Decision Cycl(l: )~Figure 1: The Soar ArchitectureSoar has two different memories: a short-term or workingmemory (2) and a long-term production or recognition mem-ory (1).
Working memory consists of a set of states, onestate per active problem-space (3) and (9); each state has a setof attribute-value pairs.
The values can be simple constantsor can themselves have attached sets of attribute-value pairs;thus working memory is composed of trees (or networks) ofattributes and values rooted at the states.The production memory stores the knowledge Soar uses tocarry out its processing.
It contains productions whose left-hand-sides test for the presence and/or absence of structuresin working memory.
Production right-hand-sides indicate at-tributes and values to add to or remove from working mem-ory.
The right-hand-side can also contain proposals for newproblem-spaces, initial states, and operators for Soar to select.Note that, unlike many production-based systems, individualproduetionsin Soar do not correspond to its operators.
Instead,the knowledge about when to apply an operator and how to ap-ply it is spread out among a number of productions; thus theeffect of an operator can vary considerably depending on thestate to which it is applied.Soar's processing is organized around a sequence of "deci-sion cycles" (4).
In each decision cycle, all the productionswhose left-hand-sides match working memory are fired in par-allel; this process is called "elaboration" (5).
Since the changesthat result may trigger additional productions, the decision cy-cle will proceed through aseries of parallel production-finngs,until "quiescence" is reached when no more.
productions fire.The productions actually carry out two distinct tasks: they im-plement the operator that was selected in the previous cycle,and they also make proposals about which operator to applynext.
Thus the productions are simultaneously carrying outthe previous decision and gathering proposals for the next de-cision.
When quiescence is reached at the end of a decisioncycle, Soar attempts o decide what to do next (6).
If the cur-rent operator has been successfully applied, 4 and only one newoperator has been proposed, the new operator is selected andSoar proceeds with the next decision cycle.If the current operator could not be applied, no new operatorhas been proposed for the next cycle, or more than one hasbeen proposed with no way to decide between them, then Soarreaches an impasse (8) and creates a subgoal (9) to resolve theimpasse.
The new subgoal triggers productions that select andset up an appropriate sub-space for dealing with the impasse.Operators are then applied in the subspace until the conditionthat prevented Soar from proceeding in the superspace is re-solved (e.g.
in (s) Soar figures out which operator to applynext), Once the impasse is resolved, Soar removes the subgoaland continues processing in the higher problem space (3).Thus the basic structure of Soar is to select and apply asequence of operators inan attempt to reach some desired state.In the top space (3), the desired state(s) depend on the task Soaris working on.
In sub-spaces (9), the desired state is one inwhich the impasse blocking Soar from proceeding inthe super-space is eliminated.
In addition to simply resolving impasses,subgoals allow Soar to learn new productions via "chunking"(11).
Soar builds new productions (called "chunks") wheneverprocessing in a subgoal produces a result in the state of ahigher goal (i.e.
adds or removes an attribute-value pair).
Theleft-hand-side of the chunk contains any attributes or values inthe higher state that were used in the subgoal processing thatled to the result.
Thus chunks record the work done in thesubgoal, allowing Soar to produce the same result directly insubsequent cases, bypassing the impasse and subgoal that ledto them.
Chunking effectively moves the knowledge used forproblem-solving in the subgoal up into a higher space; Soarwill subsequently beable to use this knowledge automatically4This must be explicitly indicated by a production that recognizes theconditions characterizing successful complete application f the operator.2007th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994(or"recognitionally") without having to do deliberate problem-solving in a sub-space.3 NL-Soar: Models and OperatorsSince NL-Soar is responsible for carrying on a conversation, itneeds to maintain models of the ongoing discourse as well asthe semantics and syntax of individual utterances it is compre-hending and producing~ Four models are produced:Utterance Model (or u-model)This models the syntacticstructure of an utterance, using structures from Govern-ment and Binding Theory \[Cho81\]; each utterance is rep-resented by a tree ~whose links are labeled with syntacticrelations uch as head, specifier, and complement, andan explicit linear ordering on the leaves.
Utterances thatare not yet complete may be modeled by several GB treestructures and/or have some of the leaves of the tree filledby pointers to objects in the situation model.Situation Model (or s-model) This models the objects, prop-erties, and relations discussed in the discourse, i.e.
a rep-resentation of the semantics of what is being said.Discourse Model (or d,modei) This models individual dis-course turns.
5 For each turn, the model indicates the typeof discourse move ibeing made, the speaker and intendedhearer, the content, and the corresponding structures inthe situation-mode!
and utterance-model.Disc6urse Segment Model This models the overall discoursethe agent is currently engaged in.
The discourse segmentmodel keeps track Of the participants in the discourse anda history of the discourse moves they have made, 6 thesituation-model objects that have been introduced intothe discourse, and the goals the agent is trying to achievethrough the discourse.NL-Soar manages the generation process by successivelyapplying operators that make small modifications to the vari-ous models, gradually Working towards the decision of whatword(s) to actually produce.
The operators used by generationcurrently include:Discourse move operators These operators implement deci-sions about what sort of discourse move the agent wishesto make next.
They generate a new discourse move inthe discourse segment model and make any other ap-propriate modifications to that model.
For example, theaccept-discourse-segment operator can be used to indicatewillingness to participate in a conversation someone lse5Actually, only the current and previous turn are kept in working memoryat this level of detail.6The representation of the 'discourse moves at this level is more abstractthan in the discourse model, containing only the type of the move and thespeaker.has started; whereas the open-discourse-segment operatorinitiates anew conversation.d-generate This operator constructs a new discourse turnin the d-model to implement a move in the discourse-segment model.d-realize This operator realizes a d-model element as one ormore s-model and/or u-model elements, i.e.
as a set ofsemantic and/or syntactic structures.s-realize This operator realizes an s-model object as a (possi-bly partial) u-model structure.say This operator releases a word to the motor system to beprinted and/or spoken.The following section will demonstrate how these modelsand operators are used to generate an utterance.4 Managing a Conversation:NTD-Soar and NL-SoarThe first system to make use of NL-Soar's generation compo-nent is NTD-Soar.
NTD-Soar is a system designed to simulatethe activities of the NASA Test Director, the person responsi-ble for co-ordinating the activities involved in the launch of aspace shuttle \[NLJ94\].
The NTD's task involves (among otherthings) following along the planned launch steps in a manual,watching a set of television monitors that display pictures ofthe launch pad and related facilities, and communicating withother members of the launch team over a multi-band radio.Thus NTD-Soar must integrate its use of NL-Soar with theother activities it is engaged in.The examples here come from NTD-Soar's modeling of thefollowing conversation, taken from a transcript of an actuallaunch attempt (CVFS and FLT are two other members of thelaunch team):Speaker UtteranceCVFS NTD, CVFSNTD Go ahead, CVFSCVFS Ready for BFSup l inkNTD I copyNTD Houston Flight, NTDNTD Perform BFS pref l ightFLT In workupl ink loadingThe processing by which NL-Soar produces "Go ahead,CVFS" begins with the activity shown in the following trace.The trace shows (in a form simplified for readability) the se-quence of decisions made at the end of each decision cycle; ingeneral, this is an operator that Soar has decided to apply dur-ing the next cycle.
NL-Soar is running recognitionally here,i.e.
with all necessary knowledge built into chunks that firein the top space.
We will see in subsequent examples what2017th International Generation Workshop * Kennebunkport, Maine ?
June 21-24, 1994happens when NL-Soar reaches an impasse and must drop intoa subspace.. , .
(NTD-Soar  per fo rms var ious  tasks... and comprehends  "NTD, CVFS")57: accept -d -segment  opened by  "NTD, CVFS"58: d -generate  d i scourse  move 'answer'59: d - rea l i ze  'answer'At the beginning of decision cycle 57, NL-Soar's compre-hension code has built the discourse segment model shown inFigure 2 to represent the summons from the CVFS.7 Here thediscourse segment model represents he summons that openedthe discourse, with pointers into the situation model to indicatethe participants.Situation ModelN'ID CVFSyesDiscourse S gment / / /  ?
Model ~ .
/ / speaker.
/ partici~ /L f"- .
~k.)
._.
B summons opemng-move ~yl~Figure 2: The models after comprehending "NTD, CVFS"The existence of the discourse segment model prompts NL-Soar to consider discourse move operators for various ways torespond; since the NTD is required to explicitly acknowledgeany communications, the accept-discourse-segment operatoris chosen.
Application of this operator modifies the discoursesegment model as shown in the bottom portion of Figure 3.A new turn has been added to represent the NTD's response,and the discourse segment has been marked as accepted  toindicate that the NTD has decided to participate in it.Next, the d-generate operator selected in decision cycle 58builds a d-model representation f the NTD's utterance, fol-lowed by a d-realize operator that builds a u-model and s-modelrealization of it; NL-Soar bypasses the s-model to directlybuild a u-model here because the phrase "go ahead" directlyexpresses the discourse move "answer" (in the specialized sub-language of this domain).
The result of the operators selected7The utterance model and discourse model built for "NTD, CVFS"'are notshown here because the generation perators don't make use of them.in cycles 57-59 is the set of models for the utterance under con-struction shown in Figure 3.
The u-model structure representsthe partially-formed utterance"<NTD>, go ahead <CVFS >",where <NTD> and <CVFS>, are s-model objects that need tobe expanded into u-model structures by subsequent operators.Utterance Model/ t \ - -optional = s.mod~l / ~ ~modelm, si~a~o~li~~l~Tr~ -8o- "abeo Z / Discourse S?oment " i / /Model ~ ~~ker\V  -- -22S - - -~ _  ~ type "# answeraccepted ~ speakerFigure 3: The models after applying the d-realize operatorNL-Soar then proceeds to finish construction and output ofthe utterance:60: s - rea l i ze  <NTD> as ad jo in  of S61: say  "go"62 : say  "ahead"63: s - rea l i ze  <CVFS> as ad jo in  of S64 : say  "cvfs"... (NTD-Soar  p roceeds  w i th  o ther  tasks)The s-realize operator selected in decision cycle 60 "realizes"<NTD> as a null phrase, because it is marked as optional, s8This decision, as with many others in this example, is made via chunksbuilt up previously by processing in various subspaces.
If these chunks had not2027th International Generation Workshop ?
Kennebunkport, Maine * June 21-24, 1994The next two items in the utterance are words, so NL-Soarsimply uses say operators to output hem.
Finally, the s-realizeoperator selected in decision cycle 63 produces "CVFS", whichis immediately output by the subsequent say operator.
Theutterance is now complete.This example is a fairly simple one, but it demonstrates thebasic top-level structure that NL-Soar uses for generation.
Theexample doesn't show any interleaving ofgeneration with othertasks, because the current implementation f NTD-Soar givesgeneration operators priority over task operators.
Note, though,that the division of the generation processing into discrete op-erators that perform individual steps allows task operators tobe interleaved with the generation operators unproblematically(as will be shown below).
In addition, the processing here isentirely driven by NL-Soar's language code; there is no taskgoal driving the generation (although it was task knowledgethat made the decision in cycle 57 to accept he summons ratherthan, say, ignoring or rejecting it.)
This too is not generally thecase, as the next example will show.5 Interactions between task and NLThe following example shows a later portion of the conversa-tion in the previous ection; here the NTD has closed off hisconversation with the CVFS and is now proceeding to talk toHouston Flight:... (NTD says "I copy")153 : new- task  contact - f l t - fo r -b fs -up l ink154 : open-d iscourse -segmentIn the decision cycles leading up to 153, the NTD has beeninformed that the CVFS is ready for the BFS uplink.
In cycle153, NTD's task operations recognize that the NTD must nowtell FLT to start he BFS uplink.
The new-task operator carriesthis out by posting a communicative goal to order FIX to do so;it is the presence of this goal that triggers NL-Soar's generationcomponent.Since the NTD is not currently talking to FLT, NL-Soarapplies an open-discourse-segment operator to open up a con-versation.
Decision cycles 155 through 160 then proceed in amanner similar to the previous example, producing the utter-ance "Houston flight, NTD":155 : d -generate  d i scourse  move ' summons '156 : d - rea l i ze  'summons'157 : s - rea l i ze  <FLT>158: say  "houston f l ight"159 : s - rea l i ze  <NTD>160: say  "ntd"Before proceeding to generate its next utterance, NL-Soarmust wait until FLT explicitly acknowledges the NTD's sum-mons (this is a task-specific requirement).
So for the decisionyet been built up, the s-realizel operator here would reach an impasse, drivingNL-Soar into a sub-space in which it would perform deliberate r asoning abouthow to carry out the realization, asin the examples in Section 6.cycles after 160, NTD-Soar selects wait operators, which don'tdo anything.wai twa i t161:"162 :182: wa i tThe important point here is that it is NTD-Soar's task knowl-edge that is selecting the wait operators, not any part of NL-Soar.
In decision cycles 161-182, NL-Soar doesn't proposeany operators (because it's waiting for FLT's acknowledge-ment), so task operators are adopted even though NL-Soar isstill in the middle of responding to a communicative goal.
As ithappens, the NTD doesn't have any other current asks to carryout, so it simply selects wait operators.
If there were other tasksthe NTD needed to perform, though, they could be carried outduring these cycles without interfering with NL-Soar's work.
9Finally, in decision cycle 183, the NTD becomes impa-tient and proceeds to simply assume that FLT has heard thesummons and is ready to continue.
1?
This is modeled byadding an <impatient> property to NTD-Soar's representa-tion of the NTD if it has been waiting longer than a specifiedtime; this in turn triggers the assumption that the FIX has im-plicitly acknowledged the summons, indicated by adding an<accepted> flag to the discourse segment model (as was donewhen the NTD responded to the CVFS's summons in the ex-ample above).
This allows NL-Soar to resume working, andin decision cycles 183-189 it proceeds to generate "PerformBFS preflight uplink loading", thus achieving the original goalposted by the task back in decision cycle 154:183 : cont inue-d iscourse -segment184: d -generate  d i scourse  move 'direct ive'185 : d - rea l i ze  'd i rect ive'186: s - rea l i ze  <do-up l ink> as top- leve l187 : say  "perform"188: s - rea l i ze  <up l ink> as comp of V189: say  "BFS pre f l ight  up l ink  loading"This conversation demonstrates how NL-Soar allows lan-guage processing to be integrated and interleaved with othertasks.
First, the NTD acquires information from its conver-sation with the CVFS, namely that a particular step in thelaunch preparation is ready to be carried out.
The NTD thendecides that its next task should be to go ahead with the launchstep, so it should tell FLT to carry it out.
Doing this requirescommunicating in language, so NL-Soar starts applying op-erators to generate the necessary utterances.
When NL-Soardoesn't propose any language operators (because it's waitingfor a response), NTD-Soar can invoke other task operators tohandle whatever else it needs to do at the time.
Thus language9A previous version of NTD-Soar did, in fact, return to visually scanningthe manual page during this interval.1?This follows the real behavior inthe transcript, in which the NTD goeson after a pause ven though e's upposed towait for the acknowledgement;this is actually the most common deviation from NASA's official anguageprotocols that occurs in the transcripts.2037th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994and other tasks interact and can be interleaved as the situa-tion allows.
II This integration is still on a fairly large scale,though; in the next example we will see how language and taskoperations can be interleaved on an operator-by-operator basis.6 Interleaving and interruption:TacAir-Soar and NL-SoarTacAir-Soar is a system that simulates a fighter pilot \[RJJ?94\].TacAir-Soar flies a (simulated) plane, controls its radar andweapons, and also communicates with other planes.
12 TacAir-Soar is being integrated with NL-Soar to handle its communi-cation tasks.
Because TacAir-Soar is engaged in a task whererapid response can be critical, it requires a more fine-grainedinterleaving of language with its other operations than NTD-Soar.
In particular, the default assumption that language oper-ators proposed by NL-Soar should have the highest preference,made in NTD-Soar, is not tenable here.
Instead, TacAir-Soar's(current) default is to choose between language and other op-erators randomly; thus language and flying the plane can bothproceed as a default, while still allowing specific situations tooverride the default and force TacAir-Soar to concentrate oncritical maneuvers.The following example shows how TacAir-Soar can inter-leave language and task operators.
Here "Parrotl01" is thecall-sign of the plane TacAir-Soar is flying, and "Ostrich" isthe name of another plane supporting Parrot l01.
Unlike NTD-Soar, TacAir-Soar maintains a stack of subspaces for long pe-riods of time, because it uses operators that perform lengthyoperations (e.g.
the top-space operator for most of a missionis simply execute-mission).
As a result, NL-Soar's languageoperators must be able to fire in any of TacAir-Soar's paces,because the top-space already has an operator that can't be re-placed.
The traces for TacAir-Soar are therefore slightly morecomplex than the ones for NTD-Soar:... (TacAir-Soar f ly ing t o waypoint)... (TacAir-Soar detects unknown plane)19 ask- for-bogey- id20 look- f or-commit-cr i  teria21 open- dis c our s e - segmentIn the processing before decision cycle 19, TacAir-Soar hasdetected a "bogey" (i.e.
an unknown plane) on its radar.
In cycle19, the ask-for-bogey-id operator is selected to ask Ostrich toidentify the bogey; this operator posts a communicative goalto request he information.
Meanwhile, though, TacAir-Soarhas other things to do, so in decision cycle 20 it decides tocheck if the criteria for committing to responding to the bogeyl This also allows the task to cut off generation r alter what's being saidin the middle of an utterance bychanging the goal structures driving NL-Soar,although we have not yet implemented his.12These can be other copies of TacAir-Soar or planes controlled by othersimulation programs.
The planes are connected via ModSAF \[CStY'93\], a sep-arate program which provides ach plane with information about the simulatedworld they are all flying in.have been satisfied.
13 Then, in cycle 21, it decides to usean open-discourse-segment operator to start alking to Ostrich.The next few cycles are similar to the previous examples:21 open-di scourse- segment22 d-generate d iscourse move ' summons'23 d-real ize ' summons'24 say "ostrich"25 say "this"26 say "is"The operators in decision cycles 22-26 produce most of thesummons "Ostrich, this is <plane-call-sign>".
But then start-ing in cycle 27, more task operators fire even though NL-Soarhasn't completed the utterance: t4 (The indentation here indi-cates Soar when drops into a subspace because of an impasse.
)26 say "is"27 check-commit-cr i ter iaCommit cr i ter ia satisf ied?
Stopping Fly-To-Waypoint.28 intercept29 ==>?
?
.32 selec t- intercept-geometry33 : ==>36 choose- intercept-a l t i tudeOUTPUT: Set des i red- fpa to i0In decision cycle 27, TacAir-Soar detects that its commit cri-teria have indeed been satisfied, so it then decides in decisioncycle 28 to apply an intercept operator in order to intercept thebogey.
TacAir-Soar can't do this in the current problem-space,so it drops down into a subspace in which it decides (in cycle32) to apply a select-intercept-geometry operator.
This in turnimpasses into another sub-space in which TacAir-Soar appliesa choose-intercept-altitude operator which sets the desired alti-tude to 10,000 feet.
At this point, time is available for languageto continue, so language operators fire in the current ask space:37 s-real ize <Parrot l01> asadjoin of S= = >  3842434445real ize-by-namename- is -knowncategory-match: NP adjo in Sreturn-resul tIn decision cycle 37, while in the middle of trying to interceptthe bogey, TacAir-Soar decides to continue with its currentutterance, attempting to express <Parrotl01>.
In this case,though, the necessary knowledge to implement the s-realizeoperator has not yet been compiled into a chunk, so Soar dropsdown into a language sub-space.
The sub-space implementsthe s-realize operator by selecting a realization strategy (e.g.realize-by-name, r alize-by-pronoun, or realize-lexically).
Ift3Note that hese don't include having identified the bogey.14Some details of the processing that aren't relevant to the discussion areomitted here and in subsequent portions of the trace.2047th International Generation Workshop ?
Kennebunkport, Maine ?
June 2t-24, 1994the selected strategy produces a syntactic structure that satis-fies all relevant constraints (which can be syntactic, semantic,or pragmatic), it is returned to the top-space?
If  not, otherstrategies are attempted until one succeeds.Here the realize-by-name strategy, chosen in cycle 42, buildsthe noun-phrase "Parrotl01 '', which passes the relevant con-straints.
In cycle 45, Soa r returns this result, thus resolving theimpasse that arose in cycle 38.Before a say operator can be selected for "Parrotl01",though, TacAir-Soar invokes ome further task operators:.
o .4950?
.
?53search- last-posit ionOUTPUT: set desired-heading to -93say "parrotlOl"Task operators continue I to fire through decision cycle 52.
Fi-nally, in decision cycle 53, the say operator is selected and theutterance is completed.What this example demonstrates is how task and languageoperators can be interleaved in a very flexible way.
NL-Soarwas initially triggered by a communicative goal posted bythe ask-for-bogey-id operator selected in decision cycle 19;an additional task operator, however, was applied before NL-Soar began its work with the open-discourse-segment operatorin decision cycle 21.
After a series of language operatorsfired, a task operator was selected in decision cycle 27 eventhough the utterance was not yet complete.
Additional taskoperators were then selected until decision cycle 37, when thes-realize operator was finally selected.
When this operator hadcompleted (which required work in a subgoal to resolve animpasse), still more task operators were selected in decisioncycles 46---52; finally the say operator selected in decision cycle53 finished the utterance.In addition to this fine-grained interleaving, TacAir-Soar canin fact interrupt languag e operators that are not yet complete toselect ask operators when necessary, as in the following trace:s-real ize <detect-on-radar>= = >real ize- lexical lyPr0blem-space: lexical-choice7273.
o .77787981 pick-head82 ==>... \[generation continues working\]Ii0 lock-radar- for-missil  e... \[further task operations\]Here NL-Soar attempts o apply an s-realize operator to express<detect-on-radar>.
15 This leads to an impasse in decision15This will eventually ead to "I have a contact.
"cycle 73, with further processing oing down into several sub-spaces as NL-Soar attempts o carry out lexical choice.
Then, indecision cycle 110, TacAir-Soar makes adecision in a problem-space higher-up in the goal stack to apply another task operator.Specifically, it decides to lock its radar on the enemy plane inpreparation for shooting at it.
Selecting this operator causesSoar to lose all pending work at a lower level of the goal-stack,which in this case includes the s-realize selected in cycle 72.In essence, TacAir-Soar has interrupted NL-Soar's processingin order to proceed with shooting a missile.
This is just thekind of interruptability a real-time system needs to have; ifsomething needs to be done right away (e.g.
shooting amissile),it must be able to interrupt less critical activities in progress(e.g.
language generation)?
When critical task operations havebeen completed, NL-Soar can re-select he s-realize operatorand carry on with generation.
Note that not all of NL-Soar'swork has been lost; the language operators before cycle 72had already completed, so the models reflect their changes.Furthermore, any chunks built in the sub-spaces before theinterruption will fire when the s-realize operator is reselected,so some of the processing in cycles 73-110 won't need to berepeated.
At worst, NL-Soar will only have to repeat a single(top-level) operator; but the fine-grained interleaving NL-Soaris designed around ensures that this will be a relatively smallamount of work.7 Reactivity, Learning, and OperatorSizeThe discussion so far has glossed over an important question:how much work should each operator do?
Deciding that NL-Soar should generate incrementally doesn't necessarily implywhat the size of the incremental steps should be.
There aresome obvious general guidelines, of course.
Each operatormust be "large" enough to do something.
Conversely, operatorsmust not be so large as to overcommit Soar to a particularutterance; a single operator that generated a ten-minute speechwould be too large because it would leave Soar no way to makeeven a small modification to the speech to reflect its currentsituation?16 These guidelines leave a lot of freedom to chooseoperator sizes; it would be useful to have some further criteriato constrain the operators?Soar in fact provides further constraints on operator sizethrough its learning mechanism.
Whenever Soar is unable tocompletely apply an operator, 17 it creates a subgoal to figureout how to complete the operator.
The results of the processingin the subgoal are compiled into productions called "chunks"16Soar could still interrupt the utterance atsome point and discard theunspoken remainder.
But modifying the speech in any way would meanredoing much of the work that originally built it, because the intermediatesteps in its construction would have been compiled out.tTRecall from Section 2that completion fan operator must be explicitlyasserted by a production.2057th International Generation Workshop ?
Kennebunkport, Maine * June 21-24, 1994that are added to Soar's long-term emory.
The conditions inthe left-hand-side of the chunks generalize from the particularstructures currently in working memory.
Thus the chunks willapply in other similar situations, allowing Soar to transfer what?
it has learned from the impasse in the current situation.The possibility of transfer of knowledge depends, though,on the size of the operator.
A very large operator that integratesa lot of problem-solving in a subgoal is likely to lead to chunksthat are very specific, because they depend on and affect alot of working memory; they will therefore not transfer toother situations very often.
On the other hand, very smalloperators that do only a little work will not gain much fromchunking, because the chunks will compile only a small amountknowledge, preserving the need to apply a large number ofoperators at the top level.
There is a tradeoff between largeoperators that are very likely to have impasses (because fewprevious chunks will transfer to them) and small operatorsthat won't allow much learning.
The use of chunking thuspushes NL-Soar towards "medium-size" operators that havea moderate amount of transfer and whose chunks collapse amoderate amount of processing.There is a similar tradeoff between chunking and reactivity.The more work a single operator does, the faster NL-Soar willbe able to generate utterances (assuming the operator doesn'treach an impasse).
On the other hand, an operator that doesa lot of work will need a lot of processing in a subgoal (orsubgoals) to build its implementation, and the more time NL-Soar spends in the subgoal that builds the operator, the greaterthe chance that some other task will interrupt NL-Soar andreplace the top-level operator without resolving the impasseand building the corresponding chunks.
At the least, this willmean repeating work; in the worst case there might operatorsthat never get enough time to finish.
The tradeoff here isreally between before and after chunking has occurred: largeroperators will run faster once the necessary chunks are built,but are more likely to take too much time to get built in thefirst place.These tradeoffs provide a more refined answer to the ques-tion of operator size.
Operators hould be designed so thattheir pre-chunking implementation takes a moderate amountof time and will produce general but non-trivial results.
Thiswill provide the reactivity needed by a real-time system inboth pre-chunking and post-chunking situations; indeed, thespecific real-time requirements of a particular application candetermine the appropriate operator size.8 ConclusionThe primary challenge for NL-Soar's generation processing isto perform generation i  a way that allows flexible shifting ofcontrol between language and task operations.
As we haveseen, NL-Soar provides this flexibility by organizing enera-tion as a sequence of incremental steps that can be interleavedwith task actions as the situation requires.
The result is a lan-guage generation capability that can be integrated with taskoperations at a broad level (as we did in NTD-Soar) or a morefine-grained level (as we did in TacAir-Soar); in critical situ-ations, task operations can even interrupt language operators.This interleaving is driven by the requirements of the situationin which the system is operating; when there are no time-criticalnon-linguistic tasks pending, language can proceed uninter-rupted.
Thus NL-Soar provides the flexible control a systemoperating in a real-time nvironment eeds.AcknowledgementsWe wish to thank Greg Nelson and Rick Lewis for extensivehelp and feedback during the integration ofNL-Soar with NTD-Soar.References\[Cho81\]\[CSC+93\]\[Lew93\]\[LLN91\]\[LNR87\]\[New90\]\[NLJ94\]\[RJJ+94\]\[RLN931Noam Chomsky.
Lectures on Government and Binding.Foils Publications, Cinnaminson, NJ, 1981.R.
B. Calder, J. E. Smith, A. J. Courtemanche, J. M. EMar, and A.
Z. Ceranowicz.
ModSAF behavior sim-ulation and control In 3rd Conference on ComputerGenerated Forces and Behavioral Representation, 1993.Rick Lewis.
An Architecturally-based Theory of HumanSentence Comprehension.
PhD thesis, Carnegie MellonUniversity, 1993.
Also available as Technical ReportCMU-CS-93-226.Jill Fain Lehman, Rick Lewis, and Allen Newell.
Inte-grating knowledge sources in language comprehension.In Proceedings of the Thirteenth Annual Conferences ofthe Cognitive Science Society, 1991.John E. Laird, Alien Newell, and Paul S. Rosenbloom.Soar: An architecture for general intelligence.
ArtificialIntelligence, 33:1-64, 1987.Allen Newell.
Unified Theories of Cognition.
HarvardUniversity Press, Cambridge, MA, 1990.Greg Nelson, Jill E Lehman, and Bonnie E. John.
Expe-riences in interruptible language processing.
InProceed-ings of the 1994 AAAI Spring Symposium on Active NLP,1994.Paul Rosenbloom, Lewis Johnson, Randy Jones, FrankKoss, John Laird, Jill Lehman, Robert Rubinoff, KarlSchwamb, and Milind Tambe.
Intelligent automatedagents for tactical air simulation: A progress report.
In4th Conference on Computer Generated Forces and Be-havioralRepresentation, May 1994.Paul Rosenbloom, John Laird, and Allen Newell.
TheSoar Papers: Research on Integrated Intelligence.
MITPress, Cambridge, Massachusetts, 1993.206
