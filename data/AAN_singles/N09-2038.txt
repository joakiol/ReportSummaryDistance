Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsIncremental Adaptation of Speech-to-Speech TranslationNguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,Tanja Schultz, Ian Lane, Alex Waibel and Alan W. BlackInterACT, Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.eduAbstractIn building practical two-way speech-to-speechtranslation systems the end user will always wishto use the system in an environment different fromthe original training data.
As with all speech sys-tems, it is important to allow the system to adaptto the actual usage situations.
This paper investi-gates how a speech-to-speech translation system canadapt day-to-day from collected data on day one toimprove performance on day two.
The platform isthe CMU Iraqi-English portable two-way speech-to-speech system as developed under the DARPATransTac program.
We show how machine transla-tion, speech recognition and overall system perfor-mance can be improved on day 2 after adapting fromday 1 in both a supervised and unsupervised way.1 IntroductionAs speech-to-speech translation systems move from thelaboratory into field deployment, we quickly see that mis-match in training data with field use can degrade the per-formance of the system.
Retraining based on field us-age is a common technique used in all speech systemsto improve performance.
In the case of speech-to-speechtranslation we would particularly like to be able to adaptthe system based on its usage automatically without hav-ing to ship data back to the laboratory for retraining.
Thispaper investigates the scenario of a two-day event.
Wewish to improve the system for the second day based onthe data collected on the first day.Our system is designed for eyes-free use and henceprovides no graphical user interface.
This allows the userto concentrate on his surrounding environment during anoperation.
The system only provides audio control andfeedback.
Additionally the system operates on a push-to-talk method.
Previously the system (Hsiao et al, 2006;Bach et al, 2007) needed 2 buttons to operate, one for theEnglish speaker and the other one for the Iraqi speaker.W i i c o n t r o l l e rM i c & L i g h tL o u d  s p e a k e rFigure 1: The users interact with the systemTo make the system easier and faster to use, we proposeto use a single button which can be controlled by the En-glish speaker.
We mounted a microphone and a Wii re-mote controller together as shown in 1.Since the Wii controller has an accelerometer whichcan be used to detect the orientation of the controller, thisfeature can be applied to identify who is speaking.
Whenthe English speaker points towards himself, the systemwill switch to English-Iraqi translation.
However, whenthe Wii is pointed towards somebody else, the system willswitch to Iraqi-English translation.
In addition, we attacha light on the Wii controller providing visual feedback.This can inform an Iraqi speaker when to start speaking.The overall system is composed of five major compo-nents: two automatic speech recognition (ASR) systems,a bidirectional statistical machine translation (SMT) sys-tem and two text-to-speech (TTS) systems.2 Data ScenarioThe standard data that is available for the TransTacproject was collected by recording human interpretermediated dialogs between war fighters and Iraqi nativespeakers in various scenarios.
The dialog partners wereaware that the data was being collected for training ma-chine based translation devices, but would often talk di-rectly to the human interpreter rather than pretending itwas an automatic device.
This means that the dialog149partners soon ignored the recording equipment and useda mostly natural language, using informal pronunciationand longer sentences with more disfluencies than we findin machine mediated translation dialogs.Most users mismatch their language when they com-municate using an automatic speech-to-speech transla-tion system.
They often switch to a clearer pronuncia-tion and use shorter and simpler sentences with less dis-fluency.
This change could have a significant impact onspeech recognition and machine translation performanceif a system was originally trained on data from the inter-preter mediated dialogs.For this reason, additional data was collected duringthe TransTac meeting in June of 2008.
This data wascollected with dialog partners using the speech-to-speechtranslation systems from 4 developer participants in theTransTac program.
The dialog partners were given a de-scription of the specific scenario in form of a rough scriptand had to speak their sentences into the translation sys-tems.
The dialog partners were not asked to actually reactto the potentially incorrect translations but just followedthe script, ignoring the output of the translation system.This has the effect that the dialog partners are no longertalking to a human interpreter, but to a machine, press-ing push-to-talk buttons etc.
and will change their speechpatterns accordingly.The data was collected over two days, with around 2hours of actual speech per day.
This data was transcribedand translated, resulting in 864 and 824 utterance pairson day 1 and 2, respectively.3 ASR LM AdaptationThis section describes the Iraqi ASR system and how weperform LM adaptation on the day 1 data to improve ASRperformance on day 2.
The CMU Iraqi ASR system istrained with around 350 hours of audio data collected un-der the TransTac program.
The acoustic model is speakerindependent but incremental unsupervised MLLR adap-tation is performed to improve recognition.
The acous-tic model has 6000 codebooks and each codebook hasat most 64 Gaussian mixtures determined by merge-and-split training.
Semi-tied covariance and boosted MMIdiscriminative training is performed to improve the model(Povey et al, 2009).
The features for the acoustic modelis the standard 39-dimension MFCC and we concatenateadjacent 15 frames and perform LDA to reduce the di-mension to 42 for the final feature vectors.
The languagemodel of the ASR system is a trigram LM trained on theaudio transcripts with around three million words withKneser-Ney smoothing (Stolcke, 2002).To perform LM adaptation for the ASR system, we usethe ASR hypotheses from day 1 to build a LM.
This LMis then interpolated with the original trigram LM to pro-duce an adapted LM for day 2.
We also evaluate the effectof having transcribers provide accurate transcription ref-erences for day 1 data, and see how it may improve theperformance on day 2.
We compare unigram, bigram andtrigram LMs for adaptation.
Since the amount of day 1data is much smaller than the whole training set and wedo not assume transcription of day 1 is always available,the interpolation weight is chosen of be 0.9 for the orig-inal trigram LM and 0.1 for the new LM built from theday 1 data.
The WER of baseline ASR system on day 1is 32.0%.Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref31.3 30.9 31.2 31.1 30.6 30.5 30.4Table 1: Iraqi ASR?s WER on day 2 using different adaptationschemes for day 1 dataThe results in Table 1 show that the ASR benefits fromLM adaptation.
Adapting day 1 data can slightly improvethe performance of day 2.
The improvement is largerwhen day 1 transcript is available which is expected.
Theresult also shows that the unigram LM is the most robustmodel for adaptation as it works reasonably well whentranscripts are not available, whereas bigram and trigramLM are more sensitive to the ASR errors made on day 1.Day 1 Day 2No ASR adaptation 29.39 27.41Unsupervised ASR adaptation 31.55 27.66Supervised ASR adaptation 32.19 27.65Table 2: Impact of ASR adaptation to SMTTable 2 shows the impact of ASR adaptation on theperformance of the translation system in BLEU (Papineniet al, 2002).
In these experiments we only performedadaptation on ASR and still using the baseline SMT com-ponent.
There is no obvious difference between unsuper-vised and supervised ASR adaptation on performance ofSMT on day 2.
However, we can see that the differencein WER on day 2 of unsupervised and supervised ASRadaptation is relatively small.4 SMT AdaptationThe Iraqi-English SMT system is trained with around650K sentence pairs collected under the TransTac pro-gram.
We used PESA phrase extraction (Vogel, 2005)and a suffix array language model (Zhang and Vogel,2005).
To adapt SMT components one approach is to op-timize LM interpolation weights by minimizing perplex-ity of the 1-best translation output (Bulyko et al, 2007).Related work including (Eck et al, 2004) attempts to useinformation retrieval to select training sentences similarto those in the test set.
To adapt the SMT componentswe use a domain-specific LM on top of the background150language models.
This approach is similar to the workin (Chen et al, 2008).
sThe adaptation framework is 1)create a domain-specific LM via an n-best list of day 1machine translation hypothesis, or day 1 translation ref-erences; 2) re-tune the translation system on day 1 viaminimum error rate training (MERT) (Venugopal and Vo-gel, 2005).Use Day 1 Day 2Baseline 29.39 27.41500 Best 1gramLM 29.18 27.23MT Hypos 2gramLM 29.53 27.503gramLM 29.36 27.23Table 3: Performance in BLEU of unsupervised adaptation.The first question we would like to address is whetherour adaptation obtains improvements via an unsupervisedmanner.
We take day 1 baseline ASR hypothesis and usethe baseline SMT to get the MT hypothesis and a 500-best list.
We train a domain LM using the 500-best listand use the MT hypotheses as the reference in MERT.
Wetreat day 1 as a development set and day 2 as an unseentest set.
In Table 3 we compare the performance of foursystems: the baseline which does not have any adaptationsteps; and 3 adapted systems using unigram, bigram andtrigram LMs build from 500-best MT hypotheses.Use Day 1 Day 2Baseline (no tune) 29.39 27.41Baseline (tune) 29.49 27.30500 Best 1gramLM 30.27 28.29MT Hypos 2gramLM 30.39 28.303gramLM 28.36 24.64MT Ref 1gramLM MT Ref 30.53 28.35Table 4: Performance in BLEU of supervised adaptation.Experimental results from unsupervised adaptation didnot show consistent improvements but suggest we mayobtain gains via supervised adaptation.
In supervisedadaptation, we assume we have day 1 translation refer-ences.
The references are used in MERT.
In Table 4 weshow performances of two additional systems which arethe baseline system without adaptation but tuned towardday 1, and the adapted system which used day 1 trans-lation references to train a unigram LM (1gramLM MTRef).
The unigram and bigram LMs from 500-best andunigram LM from MT day 1 references perform rela-tively similar on day 2.
Using a trigram 500-best LMreturned a large degradation and this LM is sensitive tothe translation errors on day15 Joint AdaptationIn Sections 3 and 4 we saw that individual adaptationhelps ASR to reduce WER and SMT to increase BLEUASR SMT Day 1 Day 2No adaptation No adaptation 29.39 27.41Unsupervised ASR 1gramLM 500-Best 32.07 28.65adaptation with MT Hypo1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83Supervised ASR 1gramLM 500-Best 32.48 28.59adaptation with MT Hypo1gramLM transcription 1gramLM MT Ref 32.68 28.60Table 5: Performance in BLEU of joint adaptation.score.
The next step in validating the adaptation frame-work was to check if the joint adaptation of ASR andSMT on day 1 data will lead to improvements on day2.
Table 5 shows the combination of ASR and SMTadaptation methods.
Improvements are obtained by us-ing both ASR and SMT adaptation.
Joint adaptation con-sistently gained more than one BLEU point improvementon day 2.
Our best system is unsupervised ASR adapta-tion via 1gramLM of ASR day 1 transcription coupledwith supervised SMT adaptation via 1gramLM of day1 translation references.
An interesting result is that tohave a better result on day 2 our approach only requirestranslation references on day 1.
We selected 1gramLMof 500-best MT hypotheses to conduct the experimentssince there is no significant difference between 1gramLMand 2gramLM on day 2 as showed in Table 3.6 Selective AdaptationThe previous results indicate that we require humantranslation references on day 1 data to get improved per-formance on day 2.
However, our goal is to make a bettersystem on day 2 but try to minimize human efforts on day1.
Therefore, we raise two questions: 1) Can we still ob-tain improvements by not using all of day 1 data?
and 2)Can we obtain more improvements?To answer these questions we performed oracle exper-iments when we take the translation hypotheses on day1 of the baseline SMT and compare them with transla-tion references, then select sentences which have BLEUscores higher than a threshold.
The subset of day 1 sen-tences is used to perform supervised adaptation in a sim-ilar way showed in section 5.
These experiments alsosimulate the situation when we have a perfect confidencescore for machine translation hypothesis selection.
Table6 shows results when we use various portions of day 1 toperform adaptation.
By using day 1 sentences which havesmoothed sentence BLEU scores higher than 10 or 20 wehave very close performance with adaptation by using allday 1 data.
The results also show that by using 416 sen-tences which have sentence BLEU score higher than 40on day 1, our adapted translation components outperformthe baseline.
Performance starts degrading after 50.
Ex-perimental results lead to the answer for question 1) that151by using less day 1 data our adapted translation compo-nents still obtain improvements compare with the base-line, and 2) we did not see that using less data will leadus to a better performance compare with using all day 1data.No.
sents Day 1 Day 2Baseline 29.39 27.41?
0 864 30.27 28.29?
10 797 31.15 28.27?
20 747 30.81 28.24?
30 585 30.04 27.71?
40 416 29.72 27.65?
50 296 30.06 27.04Correct 98 29.18 27.19Table 6: Performance in BLEU of selective adaptationW i c o n t r o l e M & L gh r n udr cd o s p t c o ak   i a i it  r le M &  h r n u cd o s p t c o ak   i a i it  r le M &  h r n udr cd o s p t c o a ff fi i a i it  r lfl ffih r n u cd o s p t c o a ff fi i a i it  r !e " & # W  $k   i r a ff fi i a i it  r !e " & " #% & ' %% & ' (% & ' )% & ' *% *% * ' %% * ' (% * ' )% * ' *% +, -./0 1 2 3 4 2 4 5 6 7 8 6 7 9 5 : ; < 2 4 5 6 7Figure 2: Summarization of adaptation performances7 ConclusionsThis work clearly shows that improvement is possible us-ing collected data for adaptation.
The overall picture isshown in Figure 2.
However this result is only based onone such data set, it would be useful to do such adaptationover multiple days.
The best results however still requireproducing translation references, notably ASR transcrip-tions do not seem to help, but may still be required in theprocess of generating translation references.
We wish tofurther investigate automatic adaptation based on implicitconfidence scores, or even active participation of the usere.g.
by marking bad utterance which could be excludedfrom the adaptation.AcknowledgmentsThis work is in part supported by the US DARPA under the TransTac(Spoken Language Communication and Translation System for TacticalUse) program.
Any opinions, findings, and conclusions or recommen-dations expressed in this material are those of the authors and do notnecessarily reflect the views of DARPA.
We would also like to thankCepstral LLC and Mobile Technologies LLC, for support of some ofthe lower level software components.ReferencesNguyen Bach, Matthias Eck, Paisarn Charoenpornsawat, ThiloKhler, Sebastian Stker, ThuyLinh Nguyen, Roger Hsiao,Alex Waibel, Stephan Vogel, Tanja Schultz, and Alan Black.2007.
The CMU TransTac 2007 Eyes-free and Hands-freeTwo-way Speech-to-Speech Translation System.
In Proc.of the International Workshop on Spoken Language Trans-lation, Trento, Italy.Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, LongNguyen, and John Makhoul.
2007.
Language Model Adap-tation in Machine Translation from Speech.
In Proc.
of Int.Conf.
on Acoustics, Speech and Signal Processing, Honolulu,Hawaii, USA.Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008.Exploiting n-best hypotheses for smt self-enhancement.
InProceedings of ACL-08: HLT, Short Papers, pages 157?160,Columbus, Ohio, USA, June.Matthias Eck, Stephan Vogel, and Alex Waibel.
2004.
Lan-guage model adaptation for statistical machine translationbased on information retrieval.
In Proc.
LREC?04, Lisbon,Portugal.Roger Hsiao, Ashish Venugopal, Thilo Kohler, Ying Zhang,Paisarn Charoenpornsawat, Andreas Zollmann, Stephan Vo-gel, Alan W Black, Tanja Schultz, and Alex Waibel.
2006.Optimizing Components for Handheld Two-way SpeechTranslation for an English-Iraqi Arabic System.
In Proc.
ofInterspeech, Pittsburgh, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: A method for automatic evaluation ofmachine translation.
In Proceedings of ACL?02, pages 311?318, Philadelphia, PA, July.Daniel Povey, Dimitri Kanevsky, Brian Kingsbury, Bhu vanaRamabhadran, George Saon, and Karthik Visweswariah.2009.
Boosted MMI for model and feature-space discrim-inative training.
In Proc.
of Int.
Conf.
on Acoustics, Speechand Signal Processing, Las Vegas, USA.Andreas Stolcke.
2002.
SRILM ?
An extensible language mod-eling toolkit.
In Proc.
Intl.
Conf.
on Spoken Language Pro-cessing, volume 2, pages 901?904, Denver.Ashish Venugopal and Stephan Vogel.
2005.
Considerationsin maximum mutual information and minimum classificationerror training for statistical machine translation.
In Proceed-ings of EAMT-05, Budapest, Hungary.Stephan Vogel.
2005.
Pesa: Phrase pair extraction as sentencesplitting.
In Proc.
of MT SUMMIT X, Phuket, Thailand.Ying Zhang and Stephan Vogel.
2005.
An efficient phrase-to-phrase alignment model for arbitrarily long phrase and largecorpora.
In Proceedings of EAMT?05, Budapest, Hungary,May.
The European Association for Machine Translation.152
