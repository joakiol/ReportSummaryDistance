THE DEF IN IT IONAL POWER OF  WORDSBranimir K. BoguraevComputer LaboratoryUniversity of CambridgeCorn Exchange StreetCambridge CB2 3QG, EnglandI am deliberate in introducing ambiguity in the title.
Part of my thesis in this brief note isgoing to be that there is a wealth of information relevant o a range of natural anguage process-ing functions available and extractable from the definitions of words found in obvious places likedictionaries.
By dictionaries here I mean monolingual dictionaries of the style exemplified by,e.g., The Longman Dictionary of Contemporary English, The Collins English Dictionary, orWebster's Seventh New Collegiate Dictionary.
This is hardly surprising, given that what is to befound in a dictionary is essentially the result of a substantial amount of work on analysing andcollating data about real language and elicitating collocational and distributional properties ofwords and applying certain common principles of defining their meaning.
Furthermore, \[am go-ing to argue that carefully exploited interplay between otions of "words" and "primitives", canadd substantial leverage to the functionality and coverage of a natural language processing sys-tem.Several factors and related phenomena underlie the current interest in words, and conse-quently word resources.
"Over the last decade there has been the emergence of theories of gram-mar and grammatical frameworks (e.g., Lexical Functional Grammar, Generalized Phrase Struc-ture Grammar, PATR-II, Functional Unification Grammar, Lexicon Grammar, Word Grammar)placing heavy emphasis on the lexicon, where elaborate information about the grammatical andlogical idiosyncracies of words is stored and used to drive various parsing systems.
More relevantto this panel, however, is the progress in both the practical aspects of natural language process-ing (various techniques for e.g., performing text analysis or building and customising naturallanguage interfaces) and the theoretical issues of knowledge representation a d access.It would not be too provocative to state that the current understanding of how to go aboutbuilding practical systems is sufficient o make such a task tractable.
However, realistic naturallanguage processing programs fall in the general class of knowledge-based systems in AI, and theyall require significant amounts of structured knowledge about the real world, as well as about aparticular domain of discourse.
There are typically two problems here, one related to the scalingup of a prototype by expanding its knowledge base and the other related to the activity of tran-sporting or customising an existing system.
In both cases the real culprit is the knowledge ac-quisition bottleneck.
Given the availability online of suitable machine readable resources, namelydictionaries and encyclopaedias, there is strong hope that some model of the common world maybe localised and extracted from such sources.
Even if individual applications may require addi-tional elaboration of their knowledge bases and the introduction of specialised terms and con-cepts, these will still have to be related to the common world knowledge.A growing mass of work at present is focussed on making some use of the definitional com-ponent of a dictionary entry, where the dictionary itself is regarded as a knowledge base, albeitpresented in a loose and not very structured fashion.
Starting with the assumption that diction-11ary definitions both employ and imply a taxonomy of genus terms, the ultimate goal is to relatenatural anguage words to this underlying structure which relates together the defining conceptsin the dictionary.
There are many problems here, ranging from the arbitrariness of dictionarydefinitions to the distribution of a particular piece of data over a number of separate ntries tothe fundamental differences between dictionaries and encyclopaedias.Underneath even the most detailed dictionary definition there is still a substantial amount ofgeneral knowledge about the world.
Without it being formalised and clearly stated, no completeanalysis of the descriptive texts typically found in a dictionary can be fully achieved.
Still,current views on automatic natural language processing tend to agree that it is hard to pinpointa boundary between the semantic knowledge that the use of a particular word (sense) implies andthe expert background which prompts its use in a specific domain.
While it would be unreason-able to expect to find any of the latter in a dictionary source there is sufficient evidence to indi-cate that most of the former-whether p esented in terms of selectional restrictions, markers, for-mulae constructed from semantic primitives, frame-based structures, propositional systems withsortal information encoded in the form of meaning postulates and associated with predicate sym-bols, or by some other means-can be derived from a suitable dictionary.
(Note, however thatthe aim of the CYC project at MCC is precisely to derive a formal model of general worldknowledge down to some level of detail from an encyclopaedia source.
)The idea of using "what is behind words"-structured assignment to thesauri classes orlooser desQriptions via natural anguage xpressions--to perform functions uch as word-sense as-signment or lexical disambiguation is certainly not new.
Recent proposals for using dictionarydefinitions to determine relevant context consolidate and extend the pioneering efforts of theCambridge Language Research Unit of 30 years ago where the structure of the Roget's Thesauruswas applied as an MT interlingua nd supported word sense disambiguation.However, now we are in position to evaluate critically "what is behind words".
One of thereasons for the renewed interest in words is the availability, on-line (in the form of machine read-able dictionaries and encyclopaedias), of vast resources of information about words.
We alsohave the technology to both process these resources and extract from them what is relevant ocomputer programs concerned with various natural language activities.
Still, given the shallowworld knowledge typically contained in dictionaries, perhaps a complete semantic omponent fora natural anguage processing system cannot be derived fully automatically by suitable analysisof word definitions.
The meaning content of a dictionary holds a promise of a different nature.While there is no consensus on the kind of representation scheme best suited for capturingthe knowledge required for language interpretation and understanding, it is nonetheless possibleto identify the distinct classes of propositional systems and type-hierarchy systems.
The latterutilise, broadly speaking, general notions of frame-like concepts with slot-like role descriptions,organised in an inheritance hierarchy along generalisation / specialisation axes.
Most of the re-cent work on knowledge representation, whether representative of the strict type-hlerarchy ap-proach (exemplified by, e.g.
FRL, KRL, NETL, UNITS, KL-ONE) or of the hybrid style ofKRYPTON and KL-TWO, for example, can be cast into this general mould.For natural language processing, where general world knowledge is just as important asspecialised omain- and task-dependent knowledge, the utility of hierarchically structured net-12works of concepts need not be emphasised.
And while it is not entirely clear whether all of thestructured information required by the system functions can be derived in a systematic and con-sistent way from dictionary sources, such sources offer a particularly good and convenient start-ing point for initial compilation of taxonomically structured knowledge about the world.Taxonomies for existing dictionaries have been constructed, albeit in a semi-automatic way;more recently, fully automated procedures have been developed to accomplish the same task, andfurther elicitation of taxonomic structures i underway.
Typically, natural language processingtechniques and constraints from linguistic theory are applied to the word definitions in the dic-tionary; in some cases, the analysis process can be made sufficiently elaborate to accomplish notonly the identification (and disambiguation) of the superordinate (genus) term, but also furtherelaboration of the defining concept by extracting additional modifiers and predications.The resulting frame-like structures with filled-in slots, assigned efaults and specified con-straints (insofar as these have been given in the original word definition) allow a fairly accuratecharacterisation f a concept both in a particular domain of discourse and within the relevantfragment of the hierarchy.
The networks thus defined can be of enormous utility to a wide rangeof text and language processing applications.Still, a much more important consequence of the assumption for an underlying taxonomyand the techniques developed for extracting such a taxonomy is the interesting prospect of look-ing at taxonomic organisations across a variety of existing dictionaries.
Perhaps there is a com-mon structure underneath definitions from different sources?
Perhaps this common structurereflects a common world representation?
It is too much to expect o find the "right" dictionary,which employs a sufficiently detailed and analytically designed taxonomy, directly utilisable by anatural anguage processing system.
On the other hand, it would be unwise to ignore commonali-ties in the definitional spaces of a cross-section of dictionaries.From a slightly different perspective, the whole body of work which looks at word definitionsin dictionaries bears directly on the issue of deriving lexicons for language processing.
Clearly,the techniques referred to above can be applied directly to the task of compiling lexical semanticinformation from dictionary sources.
It turns out that these embody fairly detailed and complexlinguistic and general world distinctions.
Attempting to use individual word definitions to derive,semi-automatically, semantic formulae which incorporate selectional restrictions and shallowworld knowledge is not an isolated activity.
Still, as already noted, dictionaries are not completerepositories of linguistic and world knowledge, and it is yet not clear whether a direct mappingfrom a word definition to a lexical entry, derived by means indicated here, is going to be an en-tirely profitable xercise.In order to exploit fully the wealth of linguistic and general world knowledge mbodied in adictionary, we need to look at the old issue of semantic primitives from a different perspective.
Ithas already been suggested that fixed sets of primitive concepts cannot cope as the granularity ofdifferent domains and contexts varies.
A lot of questions have also been raised by the issue of ex-actly how to arrive at a particular set of primitives.
It would seem that the work currently beingdone on analysing the taxonomic organisations underlying a number of existing dictionaries offersa solution here.13At least one dictionary source has been produced by adhering to a principle of using a limit-ed core vocabulary of basic words, only in their central meanings, for all the definitions; this con-trolled vocabulary clearly has not been selected randomly.
Apart from allowing a fairly closetuning of a definitions analysis program (it turns out that there exists, in effect, a grammar forwriting word definitions), the application of such a program to the core vocabulary itself willyield the taxonomy of defining concepts used in the process of the dictionary writing.
Takingthis as a guideline for determining a set of semantic primitives, individual word definitions canthen be analysed and corresponding semantic formulae compiled within this primitive vocabu-lary.Perhaps the most pertinent point here is that neither this, nor any other dictionary has beendeveloped with the particular intention of being used by a natural anguage processing program.Given that we now seem to have a slightly better idea of what we would like our systems to do,that a growing collection of texts, from which significant lexical information can be derived, ex-ists in machine readable form, and that we know enough about parsing, a question which wouldbe appropriate to ask is this: what can "reverse engineering" of a dictionary do for a knowledge-intensive language processing system?REFERENCESAlshawi,H.
(1986) Processing Dictionary Definitions with Phrasal Pattern Hierarchies.
Comput-er Laboratory, University of Cambridge.Amsler,R.
(1980) The Structure of the Merriam-Webster Pocket Dictionary.
Doctoral disserta-tion, TR-164, Computer Science Department, University of Texas, Austin.Boguraev,B.
(1986) Machine-readable dictionaries and research in computational linguistics.Proceedings of a Workshop on Automating the Lexicon, Grosseto, Italy.Calzolari,N.
(1984) Detecting patterns in a iexical database.
Tenth International Congress onComputational Linguistics, Stanford, California, 2-6 July 1984.
Association for ComputationalLinguistics, Bell Communications Research, pp.
170-173.Chodorow,M.
; Byrd,R.
; and Heidorn,G.
(1985) Extracting semantic hierarchies from a large on-line dictionary.
Proceedings, 28rd Annual Meeting of the Association for Computational Linguis-tics, Chicago, Illinois, 8-12 July 1985.
Association for Computational Linguistics, Bell Communi-cations Research, pp.
299-304.Lenat,D.
; Prakash,M.
; and Shepherd,M.
(1986) CYC: Using common sense knowledge to over-come brittleness and knowledge acquisition bottlenecks.
AI Magazine, 6(4), 65-92.Lesk,M.
(1986) Automatic sense disambiguation: how to tell a pine cone from an ice cream cone.Bell Communications Research, Morristown, New Jersey.Masterman,M.
; Needham, R.M.
; Sparck Jones,K.
; and Mayoh,B.
(1957) "AGRICOLA INCUR-VO TERRAM DLMOVIT ARATRO".
First stage translation into English with the aid ofRoget's Thesaurus.
Report ML84) ML92, Cambridge Language Research Unit, Cambridge, Eng-land.14Wilks,Y.
(1977) Good and bad arguments about semantic primitives.
Communication a d Cog-nition~ 107 181-221.15
