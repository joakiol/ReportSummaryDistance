Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752?762,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning to Grade Short Answer Questions using Semantic SimilarityMeasures and Dependency Graph AlignmentsMichael MohlerDept.
of Computer ScienceUniversity of North TexasDenton, TXmgm0038@unt.eduRazvan BunescuSchool of EECSOhio UniversityAthens, Ohiobunescu@ohio.eduRada MihalceaDept.
of Computer ScienceUniversity of North TexasDenton, TXrada@cs.unt.eduAbstractIn this work we address the task of computer-assisted assessment of short student answers.We combine several graph alignment featureswith lexical semantic similarity measures us-ing machine learning techniques and showthat the student answers can be more accu-rately graded than if the semantic measureswere used in isolation.
We also present a firstattempt to align the dependency graphs of thestudent and the instructor answers in order tomake use of a structural component in the au-tomatic grading of student answers.1 IntroductionOne of the most important aspects of the learningprocess is the assessment of the knowledge acquiredby the learner.
In a typical classroom assessment(e.g., an exam, assignment or quiz), an instructor ora grader provides students with feedback on theiranswers to questions related to the subject matter.However, in certain scenarios, such as a number ofsites worldwide with limited teacher availability, on-line learning environments, and individual or groupstudy sessions done outside of class, an instructormay not be readily available.
In these instances, stu-dents still need some assessment of their knowledgeof the subject, and so, we must turn to computer-assisted assessment (CAA).While some forms of CAA do not require sophis-ticated text understanding (e.g., multiple choice ortrue/false questions can be easily graded by a systemif the correct solution is available), there are also stu-dent answers made up of free text that may requiretextual analysis.
Research to date has concentratedon two subtasks of CAA: grading essay responses,which includes checking the style, grammaticality,and coherence of the essay (Higgins et al, 2004),and the assessment of short student answers (Lea-cock and Chodorow, 2003; Pulman and Sukkarieh,2005; Mohler and Mihalcea, 2009), which is the fo-cus of this work.An automatic short answer grading system is onethat automatically assigns a grade to an answer pro-vided by a student, usually by comparing it to oneor more correct answers.
Note that this is differentfrom the related tasks of paraphrase detection andtextual entailment, since a common requirement instudent answer grading is to provide a grade on acertain scale rather than make a simple yes/no deci-sion.In this paper, we explore the possibility of im-proving upon existing bag-of-words (BOW) ap-proaches to short answer grading by utilizing ma-chine learning techniques.
Furthermore, in an at-tempt to mirror the ability of humans to understandstructural (e.g.
syntactic) differences between sen-tences, we employ a rudimentary dependency-graphalignment module, similar to those more commonlyused in the textual entailment community.Specifically, we seek answers to the followingquestions.
First, to what extent can machine learn-ing be leveraged to improve upon existing ap-proaches to short answer grading.
Second, does thedependency parse structure of a text provide cluesthat can be exploited to improve upon existing BOWmethodologies?7522 Related WorkSeveral state-of-the-art short answer grading sys-tems (Sukkarieh et al, 2004; Mitchell et al, 2002)require manually crafted patterns which, if matched,indicate that a question has been answered correctly.If an annotated corpus is available, these patternscan be supplemented by learning additional pat-terns semi-automatically.
The Oxford-UCLES sys-tem (Sukkarieh et al, 2004) bootstraps patterns bystarting with a set of keywords and synonyms andsearching through windows of a text for new pat-terns.
A later implementation of the Oxford-UCLESsystem (Pulman and Sukkarieh, 2005) comparesseveral machine learning techniques, including in-ductive logic programming, decision tree learning,and Bayesian learning, to the earlier pattern match-ing approach, with encouraging results.C-Rater (Leacock and Chodorow, 2003) matchesthe syntactical features of a student response (i.e.,subject, object, and verb) to that of a set of correctresponses.
This method specifically disregards theBOW approach to take into account the differencebetween ?dog bites man?
and ?man bites dog?
whilestill trying to detect changes in voice (i.e., ?the manwas bitten by the dog?
).Another short answer grading system, AutoTutor(Wiemer-Hastings et al, 1999), has been designedas an immersive tutoring environment with a graph-ical ?talking head?
and speech recognition to im-prove the overall experience for students.
AutoTutoreschews the pattern-based approach entirely in favorof a BOW LSA approach (Landauer and Dumais,1997).
Later work on AutoTutor(Wiemer-Hastingset al, 2005; Malatesta et al, 2002) seeks to expandupon their BOW approach which becomes less use-ful as causality (and thus word order) becomes moreimportant.A text similarity approach was taken in (Mohlerand Mihalcea, 2009), where a grade is assignedbased on a measure of relatedness between the stu-dent and the instructor answer.
Several measures arecompared, including knowledge-based and corpus-based measures, with the best results being obtainedwith a corpus-based measure using Wikipedia com-bined with a ?relevance feedback?
approach that it-eratively augments the instructor answer by inte-grating the student answers that receive the highestgrades.In the dependency-based classification compo-nent of the Intelligent Tutoring System (Nielsen etal., 2009), instructor answers are parsed, enhanced,and manually converted into a set of content-bearingdependency triples or facets.
For each facet of theinstructor answer each student?s answer is labelledto indicate whether it has addressed that facet andwhether or not the answer was contradictory.
Thesystem uses a decision tree trained on part-of-speechtags, dependency types, word count, and other fea-tures to attempt to learn how best to classify an an-swer/facet pair.Closely related to the task of short answer gradingis the task of textual entailment (Dagan et al, 2005),which targets the identification of a directional in-ferential relation between texts.
Given a pair of twotexts as input, typically referred to as text and hy-pothesis, a textual entailment system automaticallyfinds if the hypothesis is entailed by the text.In particular, the entailment-related works that aremost similar to our own are the graph matching tech-niques proposed by Haghighi et al (2005) and Ruset al (2007).
Both input texts are converted into agraph by using the dependency relations obtainedfrom a parser.
Next, a matching score is calculated,by combining separate vertex- and edge-matchingscores.
The vertex matching functions use word-level lexical and semantic features to determine thequality of the match while the the edge matchingfunctions take into account the types of relations andthe difference in lengths between the aligned paths.Following the same line of work in the textual en-tailment world are (Raina et al, 2005), (MacCartneyet al, 2006), (de Marneffe et al, 2007), and (Cham-bers et al, 2007), which experiment variously withusing diverse knowledge sources, using a perceptronto learn alignment decisions, and exploiting naturallogic.3 Answer Grading SystemWe use a set of syntax-aware graph alignment fea-tures in a three-stage pipelined approach to short an-swer grading, as outlined in Figure 1.In the first stage (Section 3.1), the system is pro-vided with the dependency graphs for each pair ofinstructor (Ai) and student (As) answers.
For each753Figure 1: Pipeline model for scoring short-answer pairs.node in the instructor?s dependency graph, we com-pute a similarity score for each node in the student?sdependency graph based upon a set of lexical, se-mantic, and syntactic features applied to both thepair of nodes and their corresponding subgraphs.The scoring function is trained on a small set of man-ually aligned graphs using the averaged perceptronalgorithm.In the second stage (Section 3.2), the node simi-larity scores calculated in the previous stage are usedto weight the edges in a bipartite graph representingthe nodes in Ai on one side and the nodes in As onthe other.
We then apply the Hungarian algorithmto find both an optimal matching and the score asso-ciated with such a matching.
In this stage, we alsointroduce question demoting in an attempt to reducethe advantage of parroting back words provided inthe question.In the final stage (Section 3.4), we produce anoverall grade based upon the alignment scores foundin the previous stage as well as the results of severalsemantic BOW similarity measures (Section 3.3).Using each of these as features, we use Support Vec-tor Machines (SVM) to produce a combined real-number grade.
Finally, we build an Isotonic Regres-sion (IR) model to transform our output scores ontothe original [0,5] scale for ease of comparison.3.1 Node to Node MatchingDependency graphs for both the student and in-structor answers are generated using the StanfordDependency Parser (de Marneffe et al, 2006) incollapse/propagate mode.
The graphs are furtherpost-processed to propagate dependencies across the?APPOS?
(apposition) relation, to explicitly encodenegation, part-of-speech, and sentence ID withineach node, and to add an overarching ROOT nodegoverning the main verb or predicate of each sen-tence of an answer.
The final representation is alist of (relation,governor,dependent) triples, wheregovernor and dependent are both tokens describedby the tuple (sentenceID:token:POS:wordPosition).For example: (nsubj, 1:provide:VBZ:4, 1:pro-gram:NN:3) indicates that the noun ?program?
is asubject in sentence 1 whose associated verb is ?pro-vide.
?If we consider the dependency graphs output bythe Stanford parser as directed (minimally cyclic)graphs,1 we can define for each node x a set of nodesNx that are reachable from x using a subset of therelations (i.e., edge types)2.
We variously define?reachable?
in four ways to create four subgraphsdefined for each node.
These are as follows:?
N0x : All edge types may be followed?
N1x : All edge types except for subject types,ADVCL, PURPCL, APPOS, PARATAXIS,ABBREV, TMOD, and CONJ?
N2x : All edge types except for those in N1x plusobject/complement types, PREP, and RCMOD?
N3x : No edge types may be followed (This setis the single starting node x)Subgraph similarity (as opposed to simple nodesimilarity) is a means to escape the rigidity involvedin aligning parse trees while making use of as muchof the sentence structure as possible.
Humans intu-itively make use of modifiers, predicates, and subor-dinate clauses in determining that two sentence en-tities are similar.
For instance, the entity-describingphrase ?men who put out fires?
matches well with?firemen,?
but the words ?men?
and ?firemen?
have1The standard output of the Stanford Parser produces rootedtrees.
However, the process of collapsing and propagating de-pendences violates the tree structure which results in a treewith a few cross-links between distinct branches.2For more information on the relations used in this experi-ment, consult the Stanford Typed Dependencies Manual athttp://nlp.stanford.edu/software/dependencies manual.pdf754less inherent similarity.
It remains to be determinedhow much of a node?s subgraph will positively en-rich its semantics.
In addition to the complete N0xsubgraph, we chose to include N1x and N2x as tight-ening the scope of the subtree by first removingmore abstract relations, then sightly more concreterelations.We define a total of 68 features to be used to trainour machine learning system to compute node-node(more specifically, subgraph-subgraph) matches.
Ofthese, 36 are based upon the semantic similarityof four subgraphs defined by N [0..3]x .
All eightWordNet-based similarity measures listed in Sec-tion 3.3 plus the LSA model are used to producethese features.
The remaining 32 features are lexico-syntactic features3 defined only for N3x and are de-scribed in more detail in Table 2.We use ?
(xi, xs) to denote the feature vector as-sociated with a pair of nodes ?xi, xs?, where xi isa node from the instructor answer Ai and xs is anode from the student answer As.
A matching scorecan then be computed for any pair ?xi, xs?
?
Ai ?As through a linear scoring function f(xi, xs) =wT?
(xi, xs).
In order to learn the parameter vec-tor w, we use the averaged version of the percep-tron algorithm (Freund and Schapire, 1999; Collins,2002).As training data, we randomly select a subset ofthe student answers in such a way that our set wasroughly balanced between good scores, mediocrescores, and poor scores.
We then manually annotateeach node pair ?xi, xs?
as matching, i.e.
A(xi, xs) =+1, or not matching, i.e.
A(xi, xs) = ?1.
Overall,32 student answers in response to 21 questions witha total of 7303 node pairs (656 matches, 6647 non-matches) are manually annotated.
The pseudocodefor the learning algorithm is shown in Table 1.
Af-ter training the perceptron, these 32 student answersare removed from the dataset, not used as trainingfurther along in the pipeline, and are not included inthe final results.
After training for 50 epochs,4 thematching score f(xi, xs) is calculated (and cached)for each node-node pair across all student answersfor all assignments.3Note that synonyms include negated antonyms (and viceversa).
Hypernymy and hyponymy are restricted to at mosttwo steps).4This value was chosen arbitrarily and was not tuned in anyway0.
set w ?
0, w?
0, n?
01. repeat for T epochs:2. foreach ?Ai;As?:3.
foreach ?xi, xs?
?
Ai ?As:4. if sgn(wT?
(xi, xs)) 6= sgn(A(xi, xs)):5. set w ?
w+A(xi, xs)?
(xi, xs)6. set w ?
w+w, n?
n+ 17. return w/n.Table 1: Perceptron Training for Node Matching.3.2 Graph to Graph AlignmentOnce a score has been computed for each node-nodepair across all student/instructor answer pairs, we at-tempt to find an optimal alignment for the answerpair.
We begin with a bipartite graph where eachnode in the student answer is represented by a nodeon the left side of the bipartite graph and each nodein the instructor answer is represented by a nodeon the right side.
The score associated with eachedge is the score computed for each node-node pairin the previous stage.
The bipartite graph is thenaugmented by adding dummy nodes to both sideswhich are allowed to match any node with a score ofzero.
An optimal alignment between the two graphsis then computed efficiently using the Hungarian al-gorithm.
Note that this results in an optimal match-ing, not a mapping, so that an individual node is as-sociated with at most one node in the other answer.At this stage we also compute several alignment-based scores by applying various transformations tothe input graphs, the node matching function, andthe alignment score itself.The first and simplest transformation involves thenormalization of the alignment score.
While thereare several possible ways to normalize a matchingsuch that longer answers do not unjustly receivehigher scores, we opted to simply divide the totalalignment score by the number of nodes in the in-structor answer.The second transformation scales the node match-ing score by multiplying it with the idf 5 of the in-structor answer node, i.e., replace f(xi, xs) withidf(xi) ?
f(xi, xs).The third transformation relies upon a certainreal-world intuition associated with grading student5Inverse document frequency, as computed from the British Na-tional Corpus (BNC)755Name Type # features DescriptionRootMatch binary 5 Is a ROOT node matched to: ROOT, N, V, JJ, or OtherLexical binary 3 Exact match, Stemmed match, close Levenshtein matchPOSMatch binary 2 Exact POS match, Coarse POS matchPOSPairs binary 8 Specific X-Y POS matches foundOntological binary 4 WordNet relationships: synonymy, antonymy, hypernymy, hyponymyRoleBased binary 3 Has as a child - subject, object, verbVerbsSubject binary 3 Both are verbs and neither, one, or both have a subject childVerbsObject binary 3 Both are verbs and neither, one, or both have an object childSemantic real 36 Nine semantic measures across four subgraphs eachBias constant 1 A value of 1 for all vectorsTotal 68Table 2: Subtree matching features used to train the perceptronanswers ?
repeating words in the question is easyand is not necessarily an indication of student under-standing.
With this in mind, we remove any wordsin the question from both the instructor answer andthe student answer.In all, the application of the three transforma-tions leads to eight different transform combina-tions, and therefore eight different alignment scores.For a given answer pair (Ai, As), we assemble theeight graph alignment scores into a feature vector?G(Ai, As).3.3 Lexical Semantic SimilarityHaghighi et al (2005), working on the entailmentdetection problem, point out that finding a goodalignment is not sufficient to determine that thealigned texts are in fact entailing.
For instance, twoidentical sentences in which an adjective from one isreplaced by its antonym will have very similar struc-tures (which indicates a good alignment).
However,the sentences will have opposite meanings.
Furtherinformation is necessary to arrive at an appropriatescore.In order to address this, we combine the graphalignment scores, which encode syntactic knowl-edge, with the scores obtained from semantic sim-ilarity measures.Following Mihalcea et al (2006) and Mohlerand Mihalcea (2009), we use eight knowledge-based measures of semantic similarity: shortest path[PATH], Leacock & Chodorow (1998) [LCH], Lesk(1986), Wu & Palmer(1994) [WUP], Resnik (1995)[RES], Lin (1998), Jiang & Conrath (1997) [JCN],Hirst & St. Onge (1998) [HSO], and two corpus-based measures: Latent Semantic Analysis [LSA](Landauer and Dumais, 1997) and Explicit Seman-tic Analysis [ESA] (Gabrilovich and Markovitch,2007).Briefly, for the knowledge-based measures, weuse the maximum semantic similarity ?
for eachopen-class word ?
that can be obtained by pairingit up with individual open-class words in the sec-ond input text.
We base our implementation onthe WordNet::Similarity package provided by Ped-ersen et al (2004).
For the corpus-based measures,we create a vector for each answer by summingthe vectors associated with each word in the an-swer ?
ignoring stopwords.
We produce a score inthe range [0..1] based upon the cosine similarity be-tween the student and instructor answer vectors.
TheLSA model used in these experiments was built bytraining Infomap6 on a subset of Wikipedia articlesthat contain one or more common computer scienceterms.
Since ESA uses Wikipedia article associa-tions as vector features, it was trained using a fullWikipedia dump.3.4 Answer Ranking and GradingWe combine the alignment scores ?G(Ai, As) withthe scores ?B(Ai, As) from the lexical seman-tic similarity measures into a single feature vector?
(Ai, As) = [?G(Ai, As)|?B(Ai, As)].
The fea-ture vector ?G(Ai, As) contains the eight alignmentscores found by applying the three transformationsin the graph alignment stage.
The feature vector?B(Ai, As) consists of eleven semantic features ?the eight knowledge-based features plus LSA, ESAand a vector consisting only of tf*idf weights ?
bothwith and without question demoting.
Thus, the en-tire feature vector ?
(Ai, As) contains a total of 30features.6http://Infomap-nlp.sourceforge.net/756An input pair (Ai, As) is then associated with agrade g(Ai, As) = uT?
(Ai, As) computed as a lin-ear combination of features.
The weight vector u istrained to optimize performance in two scenarios:Regression: An SVM model for regression (SVR)is trained using as target function the grades as-signed by the instructors.
We use the libSVM 7 im-plementation of SVR, with tuned parameters.Ranking: An SVM model for ranking (SVMRank)is trained using as ranking pairs all pairs of stu-dent answers (As, At) such that grade(Ai, As) >grade(Ai, At), where Ai is the corresponding in-structor answer.
We use the SVMLight 8 implemen-tation of SVMRank with tuned parameters.In both cases, the parameters are tuned using agrid-search.
At each grid point, the training data ispartitioned into 5 folds which are used to train a tem-porary SVM model with the given parameters.
Theregression passage selects the grid point with theminimal mean square error (MSE), and the SVM-Rank package tries to minimize the number of dis-cordant pairs.
The parameters found are then used toscore the test set ?
a set not used in the grid training.3.5 Isotonic RegressionSince the end result of any grading system is to givea student feedback on their answers, we need to en-sure that the system?s final score has some mean-ing.
With this in mind, we use isotonic regression(Zadrozny and Elkan, 2002) to convert the systemscores onto the same [0..5] scale used by the an-notators.
This has the added benefit of making thesystem output more directly related to the annotatedgrade, which makes it possible to report root meansquare error in addition to correlation.
We train theisotonic regression model on each type of systemoutput (i.e., alignment scores, SVM output, BOWscores).4 Data SetTo evaluate our method for short answer grading,we created a data set of questions from introductorycomputer science assignments with answers pro-vided by a class of undergraduate students.
The as-signments were administered as part of a Data Struc-7http://www.csie.ntu.edu.tw/?cjlin/libsvm/8http://svmlight.joachims.org/tures course at the University of North Texas.
Foreach assignment, the student answers were collectedvia an online learning environment.The students submitted answers to 80 questionsspread across ten assignments and two examina-tions.9 Table 3 shows two question-answer pairswith three sample student answers each.
Thirty-onestudents were enrolled in the class and submitted an-swers to these assignments.
The data set we workwith consists of a total of 2273 student answers.
Thisis less than the expected 31 ?
80 = 2480 as somestudents did not submit answers for a few assign-ments.
In addition, the student answers used to trainthe perceptron are removed from the pipeline afterthe perceptron training stage.The answers were independently graded by twohuman judges, using an integer scale from 0 (com-pletely incorrect) to 5 (perfect answer).
Both humanjudges were graduate students in the computer sci-ence department; one (grader1) was the teaching as-sistant assigned to the Data Structures class, whilethe other (grader2) is one of the authors of this pa-per.
We treat the average grade of the two annotatorsas the gold standard against which we compare oursystem output.Difference Examples % of examples0 1294 57.7%1 514 22.9%2 231 10.3%3 123 5.5%4 70 3.1%5 9 0.4%Table 4: Annotator AnalysisThe annotators were given no explicit instructionson how to assign grades other than the [0..5] scale.Both annotators gave the same grade 57.7% of thetime and gave a grade only 1 point apart 22.9% ofthe time.
The full breakdown can be seen in Table4.
In addition, an analysis of the grading patternsindicate that the two graders operated off of differ-ent grading policies where one grader (grader1) wasmore generous than the other.
In fact, when the twodiffered, grader1 gave the higher grade 76.6% of thetime.
The average grade given by grader1 is 4.43,9Note that this is an expanded version of the dataset used byMohler and Mihalcea (2009)757Sample questions, correct answers, and student answers GradesQuestion: What is the role of a prototype program in problem solving?Correct answer: To simulate the behavior of portions of the desired software product.Student answer 1: A prototype program is used in problem solving to collect data for the problem.
1, 2Student answer 2: It simulates the behavior of portions of the desired software product.
5, 5Student answer 3: To find problem and errors in a program before it is finalized.
2, 2Question: What are the main advantages associated with object-oriented programming?Correct answer: Abstraction and reusability.Student answer 1: They make it easier to reuse and adapt previously written code and they separate complexprograms into smaller, easier to understand classes.
5, 4Student answer 2: Object oriented programming allows programmers to use an object with classes that can bechanged and manipulated while not affecting the entire object at once.
1, 1Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smallermore manageable problems.
4, 4Table 3: A sample question with short answers provided by students and the grades assigned by the two human judgeswhile the average grade given by grader2 is 3.94.The dataset is biased towards correct answers.
Webelieve all of these issues correctly mirror real-worldissues associated with the task of grading.5 ResultsWe independently test two components of our over-all grading system: the node alignment detectionscores found by training the perceptron, and theoverall grades produced in the final stage.
For thealignment detection, we report the precision, recall,and F-measure associated with correctly detectingmatches.
For the grading stage, we report a singlePearson?s correlation coefficient tracking the anno-tator grades (average of the two annotators) and theoutput score of each system.
In addition, we re-port the Root Mean Square Error (RMSE) for thefull dataset as well as the median RMSE across eachindividual question.
This is to give an indication ofthe performance of the system for grading a singlequestion in isolation.105.1 Perceptron AlignmentFor the purpose of this experiment, the scores as-sociated with a given node-node matching are con-verted into a simple yes/no matching decision wherepositive scores are considered a match and negative10We initially intended to report an aggregate of question-levelPearson correlation results, but discovered that the datasetcontained one question for which each student received fullpoints ?
leaving the correlation undefined.
We believe thatthis casts some doubt on the applicability of Pearson?s (orSpearman?s) correlation coefficient for the short answer grad-ing task.
We have retained its use here alongside RMSE forease of comparison.scores a non-match.
The threshold weight learnedfrom the bias feature strongly influences the pointat which real scores change from non-matches tomatches, and given the threshold weight learned bythe algorithm, we find an F-measure of 0.72, withprecision(P) = 0.85 and recall(R) = 0.62.
However,as the perceptron is designed to minimize error rate,this may not reflect an optimal objective when seek-ing to detect matches.
By manually varying thethreshold, we find a maximum F-measure of 0.76,with P=0.79 and R=0.74.
Figure 2 shows the fullprecision-recall curve with the F-measure overlaid.00.20.40.60.810  0.2  0.4  0.6  0.8  1ScoreRecallPrecisionF-MeasureThresholdFigure 2: Precision, recall, and F-measure on node-levelmatch detection5.2 Question DemotingOne surprise while building this system was the con-sistency with which the novel technique of questiondemoting improved scores for the BOW similaritymeasures.
With this relatively minor change the av-erage correlation between the BOW methods?
sim-758ilarity scores and the student grades improved byup to 0.046 with an average improvement of 0.019across all eleven semantic features.
Table 5 showsthe results of applying question demoting to oursemantic features.
When comparing scores usingRMSE, the difference is less consistent, yielding anaverage improvement of 0.002.
However, for onemeasure (tf*idf), the improvement is 0.063 whichbrings its RMSE score close to the lowest of allBOW metrics.
The reasons for this are not entirelyclear.
As a baseline, we include here the results ofassigning the average grade (as determined on thetraining data) for each question.
The average gradewas chosen as it minimizes the RMSE on the train-ing data.?
w/ QD RMSE w/ QD Med.
RMSE w/ QDLesk 0.450 0.462 1.034 1.050 0.930 0.919JCN 0.443 0.461 1.022 1.026 0.954 0.923HSO 0.441 0.456 1.036 1.034 0.966 0.935PATH 0.436 0.457 1.029 1.030 0.940 0.918RES 0.409 0.431 1.045 1.035 0.996 0.941Lin 0.382 0.407 1.069 1.056 0.981 0.949LCH 0.367 0.387 1.068 1.069 0.986 0.958WUP 0.325 0.343 1.090 1.086 1.027 0.977ESA 0.395 0.401 1.031 1.086 0.990 0.955LSA 0.328 0.335 1.065 1.061 0.951 1.000tf*idf 0.281 0.327 1.085 1.022 0.991 0.918Avg.grade 1.097 1.097 0.973 0.973Table 5: BOW Features with Question Demoting (QD).Pearson?s correlation, root mean square error (RMSE),and median RMSE for all individual questions.5.3 Alignment Score GradingBefore applying any machine learning techniques,we first test the quality of the eight graph alignmentfeatures ?G(Ai, As) independently.
Results indicatethat the basic alignment score performs comparablyto most BOW approaches.
The introduction of idfweighting seems to degrade performance somewhat,while introducing question demoting causes the cor-relation with the grader to increase while increasingRMSE somewhat.
The four normalized componentsof ?G(Ai, As) are reported in Table 6.5.4 SVM Score GradingThe SVM components of the system are run on thefull dataset using a 12-fold cross validation.
Each ofthe 10 assignments and 2 examinations (for a totalof 12 folds) is scored independently with ten of theremaining eleven used to train the machine learn-Standard w/ IDF w/ QD w/ QD+IDFPearson?s ?
0.411 0.277 0.428 0.291RMSE 1.018 1.078 1.046 1.076Median RMSE 0.910 0.970 0.919 0.992Table 6: Alignment Feature/Grade Correlations usingPearson?s ?.
Results are also reported when inverse doc-ument frequency weighting (IDF) and question demoting(QD) are used.ing system.
For each fold, one additional fold isheld out for later use in the development of an iso-tonic regression model (see Figure 3).
The param-eters (for cost C and tube width ) were found us-ing a grid search.
At each point on the grid, the datafrom the ten training folds was partitioned into 5 setswhich were scored according to the current param-eters.
SVMRank and SVR sought to minimize thenumber of discordant pairs and the mean absoluteerror, respectively.Both SVM models are trained using a linear ker-nel.11 Results from both the SVR and the SVMRankimplementations are reported in Table 7 along witha selection of other measures.
Note that the RMSEscore is computed after performing isotonic regres-sion on the SVMRank results, but that it was unnec-essary to perform an isotonic regression on the SVRresults as the system was trained to produce a scoreon the correct scale.We report the results of running the systems onthree subsets of features ?
(Ai, As): BOW features?B(Ai, As) only, alignment features ?G(Ai, As)only, or the full feature vector (labeled ?Hybrid?
).Finally, three subsets of the alignment features areused: only unnormalized features, only normalizedfeatures, or the full alignment feature set.B CA ?
Ten FoldsB CA ?
Ten FoldsB CA ?
Ten FoldsIR ModelSVM ModelFeaturesFigure 3: Dependencies of the SVM/IR training stages.11We also ran the SVR system using quadratic and radial-basisfunction (RBF) kernels, but the results did not show signifi-cant improvement over the simpler linear kernel.759Unnormalized Normalized BothIAA Avg.
grade tf*idf Lesk BOW Align Hybrid Align Hybrid Align HybridSVMRankPearson?s ?
0.586 0.327 0.450 0.480 0.266 0.451 0.447 0.518 0.424 0.493RMSE 0.659 1.097 1.022 1.050 1.042 1.093 1.038 1.015 0.998 1.029 1.021Median RMSE 0.605 0.973 0.918 0.919 0.943 0.974 0.903 0.865 0.873 0.904 0.901SVRPearson?s ?
0.586 0.327 0.450 0.431 0.167 0.437 0.433 0.459 0.434 0.464RMSE 0.659 1.097 1.022 1.050 0.999 1.133 0.995 1.001 0.982 1.003 0.978Median RMSE 0.605 0.973 0.918 0.919 0.910 0.987 0.893 0.894 0.877 0.886 0.862Table 7: The results of the SVM models trained on the full suite of BOW measures, the alignment scores, and thehybrid model.
The terms ?normalized?, ?unnormalized?, and ?both?
indicate which subset of the 8 alignment featureswere used to train the SVM model.
For ease of comparison, we include in both sections the scores for the IAA, the?Average grade?
baseline, and two of the top performing BOW metrics ?
both with question demoting.6 Discussion and ConclusionsThere are three things that we can learn from theseexperiments.
First, we can see from the results thatseveral systems appear better when evaluating on acorrelation measure like Pearson?s ?, while othersappear better when analyzing error rate.
The SVM-Rank system seemed to outperform the SVR sys-tem when measuring correlation, however the SVRsystem clearly had a minimal RMSE.
This is likelydue to the different objective function in the corre-sponding optimization formulations: while the rank-ing model attempts to ensure a correct ordering be-tween the grades, the regression model seeks to min-imize an error objective that is closer to the RMSE.It is difficult to claim that either system is superior.Likewise, perhaps the most unexpected result ofthis work is the differing analyses of the simpletf*idf measure ?
originally included only as a base-line.
Evaluating with a correlative measure yieldspredictably poor results, but evaluating the error rateindicates that it is comparable to (or better than) themore intelligent BOW metrics.
One explanation forthis result is that the skewed nature of this ?natural?dataset favors systems that tend towards scores inthe 4 to 4.5 range.
In fact, 46% of the scores outputby the tf*idf measure (after IR) were within the 4 to4.5 range and only 6% were below 3.5.
Testing ona more balanced dataset, this tendency to fit to theaverage would be less advantageous.Second, the supervised learning techniques areclearly able to leverage multiple BOW measures toyield improvements over individual BOW metrics.The correlation for the BOW-only SVM model forSVMRank improved upon the best BOW featurefrom .462 to .480.
Likewise, using the BOW-onlySVM model for SVR reduces the RMSE by .022overall compared to the best BOW feature.Third, the rudimentary alignment features wehave introduced here are not sufficient to act as astandalone grading system.
However, even with avery primitive attempt at alignment detection, weshow that it is possible to improve upon grade learn-ing systems that only consider BOW features.
Thecorrelations associated with the hybrid systems (esp.those using normalized alignment data) frequentlyshow an improvement over the BOW-only SVM sys-tems.
This is true for both SVM systems when con-sidering either evaluation metric.Future work will concentrate on improving thequality of the answer alignments by training a modelto directly output graph-to-graph alignments.
Thislearning approach will allow the use of more com-plex alignment features, for example features thatare defined on pairs of aligned edges or on largersubtrees in the two input graphs.
Furthermore, givenan alignment, we can define several phrase-levelgrammatical features such as negation, modality,tense, person, number, or gender, which make bet-ter use of the alignment itself.AcknowledgmentsThis work was partially supported by a National Sci-ence Foundation CAREER award #0747340.
Anyopinions, findings, and conclusions or recommenda-tions expressed in this material are those of the au-thors and do not necessarily reflect the views of theNational Science Foundation.760ReferencesN.
Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-don, B. MacCartney, M.C.
de Marneffe, D. Ramage,E.
Yeh, and C.D.
Manning.
2007.
Learning align-ments and leveraging natural logic.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing, pages 165?170.
Association forComputational Linguistics.M.
Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-02), Philadelphia, PA,July.I.
Dagan, O. Glickman, and B. Magnini.
2005.
The PAS-CAL recognising textual entailment challenge.
In Pro-ceedings of the PASCAL Workshop.M.C.
de Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC 2006.M.C.
de Marneffe, T. Grenager, B. MacCartney, D. Cer,D.
Ramage, C. Kiddon, and C.D.
Manning.
2007.Aligning semantic graphs for textual inference andmachine reading.
In Proceedings of the AAAI SpringSymposium.
Citeseer.Y.
Freund and R. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37:277?296.E.
Gabrilovich and S. Markovitch.
2007.
ComputingSemantic Relatedness using Wikipedia-based ExplicitSemantic Analysis.
Proceedings of the 20th Inter-national Joint Conference on Artificial Intelligence,pages 6?12.A.D.
Haghighi, A.Y.
Ng, and C.D.
Manning.
2005.
Ro-bust textual inference via graph matching.
In Pro-ceedings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 387?394.
Association for Computa-tional Linguistics.D.
Higgins, J. Burstein, D. Marcu, and C. Gentile.
2004.Evaluating multiple aspects of coherence in studentessays.
In Proceedings of the annual meeting of theNorth American Chapter of the Association for Com-putational Linguistics, Boston, MA.G.
Hirst and D. St-Onge, 1998.
Lexical chains as repre-sentations of contexts for the detection and correctionof malaproprisms.
The MIT Press.J.
Jiang and D. Conrath.
1997.
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Proceed-ings of the International Conference on Research inComputational Linguistics, Taiwan.T.K.
Landauer and S.T.
Dumais.
1997.
A solution toplato?s problem: The latent semantic analysis theoryof acquisition, induction, and representation of knowl-edge.
Psychological Review, 104.C.
Leacock and M. Chodorow.
1998.
Combining lo-cal context and WordNet sense similarity for wordsense identification.
In WordNet, An Electronic Lex-ical Database.
The MIT Press.C.
Leacock and M. Chodorow.
2003.
C-rater: Auto-mated Scoring of Short-Answer Questions.
Comput-ers and the Humanities, 37(4):389?405.M.E.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of theSIGDOC Conference 1986, Toronto, June.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th InternationalConference on Machine Learning, Madison, WI.B.
MacCartney, T. Grenager, M.C.
de Marneffe, D. Cer,and C.D.
Manning.
2006.
Learning to recognize fea-tures of valid textual entailments.
In Proceedings ofthe main conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, page 48.
As-sociation for Computational Linguistics.K.I.
Malatesta, P. Wiemer-Hastings, and J. Robertson.2002.
Beyond the Short Answer Question with Re-search Methods Tutor.
In Proceedings of the Intelli-gent Tutoring Systems Conference.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and knowledge-based approaches to textsemantic similarity.
In Proceedings of the AmericanAssociation for Artificial Intelligence (AAAI 2006),Boston.T.
Mitchell, T. Russell, P. Broomhead, and N. Aldridge.2002.
Towards robust computerised marking of free-text responses.
Proceedings of the 6th InternationalComputer Assisted Assessment (CAA) Conference.M.
Mohler and R. Mihalcea.
2009.
Text-to-text seman-tic similarity for automatic short answer grading.
InProceedings of the European Association for Compu-tational Linguistics (EACL 2009), Athens, Greece.R.D.
Nielsen, W. Ward, and J.H.
Martin.
2009.
Recog-nizing entailment in intelligent tutoring systems.
Nat-ural Language Engineering, 15(04):479?501.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet:: Similarity-Measuring the Relatedness ofConcepts.
Proceedings of the National Conference onArtificial Intelligence, pages 1024?1025.S.G.
Pulman and J.Z.
Sukkarieh.
2005.
Automatic ShortAnswer Marking.
ACL WS Bldg Ed Apps using NLP.R.
Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels,K.
Toutanova, B. MacCartney, M.C.
de Marneffe, C.D.Manning, and A.Y.
Ng.
2005.
Robust textual infer-ence using diverse knowledge sources.
RecognizingTextual Entailment, page 57.761P.
Resnik.
1995.
Using information content to evalu-ate semantic similarity.
In Proceedings of the 14th In-ternational Joint Conference on Artificial Intelligence,Montreal, Canada.V.
Rus, A. Graesser, and K. Desai.
2007.
Lexico-syntactic subsumption for textual entailment.
RecentAdvances in Natural Language Processing IV: Se-lected Papers from RANLP 2005, page 187.J.Z.
Sukkarieh, S.G. Pulman, and N. Raikes.
2004.
Auto-Marking 2: An Update on the UCLES-Oxford Univer-sity research into using Computational Linguistics toScore Short, Free Text Responses.
International Asso-ciation of Educational Assessment, Philadephia.P.
Wiemer-Hastings, K. Wiemer-Hastings, andA.
Graesser.
1999.
Improving an intelligent tu-tor?s comprehension of students with Latent SemanticAnalysis.
Artificial Intelligence in Education, pages535?542.P.
Wiemer-Hastings, E. Arnott, and D. Allbritton.
2005.Initial results and mixed directions for research meth-ods tutor.
In AIED2005 - Supplementary Proceedingsof the 12th International Conference on Artificial In-telligence in Education, Amsterdam.Z.
Wu and M. Palmer.
1994.
Verb semantics and lexicalselection.
In Proceedings of the 32nd Annual Meetingof the Association for Computational Linguistics, LasCruces, New Mexico.B.
Zadrozny and C. Elkan.
2002.
Transforming classifierscores into accurate multiclass probability estimates.Edmonton, Alberta.762
