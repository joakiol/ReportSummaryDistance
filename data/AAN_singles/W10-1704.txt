Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54?59,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLIMSI?s statistical translation systems for WMT?10Alexandre Allauzen, Josep M. Crego, ?Ilknur Durgar El-Kahlout and Franc?ois YvonLIMSI/CNRS and Universite?
Paris-Sud 11, FranceBP 133, 91403 Orsay CedexFirstname.Lastname@limsi.frAbstractThis paper describes our Statistical Ma-chine Translation systems for the WMT10evaluation, where LIMSI participated fortwo language pairs (French-English andGerman-English, in both directions).
ForGerman-English, we concentrated on nor-malizing the German side through a properpreprocessing, aimed at reducing the lex-ical redundancy and at splitting complexcompounds.
For French-English, we stud-ied two extensions of our in-house N -codedecoder: firstly, the effect of integrating anew bilingual reordering model; second,the use of adaptation techniques for thetranslation model.
For both set of exper-iments, we report the improvements ob-tained on the development and test data.1 IntroductionLIMSI took part in the WMT 2010 evalua-tion campaign and developed systems for twolanguages pairs: French-English and German-English in both directions.
For German-English,we focused on preprocessing issues and performeda series of experiments aimed at normalizing theGerman side by removing some of the lexical re-dundancy and by splitting compounds.
For thispair, all the experiments were performed using theMoses decoder (Koehn et al, 2007).
For French-English, we studied two extensions of our n-grambased system: first, the effect of integrating anew bilingual reordering model; second, the useof adaptation techniques for the translation model.Decoding is performed using our in-house N -code(Marin?o et al, 2006) decoder.2 System architecture and resourcesIn this section, we describe the main characteris-tics of the phrase-based systems developed for thisevaluation and the resources that were used to trainour models.
As far as resources go, we used all thedata supplied by the 2010 evaluation organizers.Based on our previous experiments (De?chelotte etal., 2008) which have demonstrated that better nor-malization tools provide better BLEU scores (Pap-ineni et al, 2002), we took advantage of our in-house text processing tools for the tokenizationand detokenization steps.
Only for German datadid we used the TreeTagger (Schmid, 1994) tok-enizer.
Similar to last year?s experiments, all ofour systems are built in ?true-case?.3 German-English systemsAs German is morphologically more complex thanEnglish, the default policy which consists in treat-ing each word form independently from the oth-ers is plagued with data sparsity, which poses anumber of difficulties both at training and de-coding time.
When aligning parallel texts atthe word level, German compound words typi-cally tend to align with more than one Englishword; this, in turn, tends to increase the numberof possible translation counterparts for each En-glish type, and to make the corresponding align-ment scores less reliable.
In decoding, new com-pounds or unseen morphological variants of ex-isting words artificially increase the number out-of-vocabulary (OOV) forms, which severely hurtsthe overall translation quality.
Several researchershave proposed normalization (Niessen and Ney,2004; Corston-oliver and Gamon, 2004; Goldwa-ter and McClosky, 2005) and compound splitting(Koehn and Knight, 2003; Stymne, 2008; Stymne,2009) methods.
Our approach here is similar, yetuses different implementations; we also studiedthe joint effect of combining both techniques.3.1 Reducing the lexical redundancyIn German, determiners, pronouns, nouns and ad-jectives carry inflection marks (typically suffixes)54Input POS Lemma AnalysisIn APPR in APPR.Inder* ART d ART.Def.Dat.Sg.FemFolge NN Folge N.Reg.Dat.Sg.Fembefand VVFIN befinden VFIN.Full.3.Sg.Past.Inddie* ART d ART.Def.Nom.Sg.Femderart ADV derart ADVgesta?rkte* ADJA gesta?rkt ADJA.Pos.Nom.Sg.FemJustiz NN Justiz N.Reg.Nom.Sg.Femwiederholt ADJD wiederholt ADJD.Posgegen APPR gegen APPR.Accdie* ART d ART.Def.Acc.Sg.FemRegierung NN Regierung N.Reg.Acc.Sg.Femund KON und CONJ.Coord.-2insbesondere ADV insbesondere ADVgegen APPR gegen APPR.Accderen* PDAT d PRO.Dem.Subst.-3.Gen.Sg.FemGeheimdienste* NN Geheimdienst N.Reg.Acc.Pl.Masc.
$.
.
SYM.Pun.SentTable 1: TreeTagger and RFTagger outputs.
Starred word forms are modified during preprocessing.so as to satisfy agreement constraints.
Inflectionsvary according to gender, case, and number infor-mation.
For instance, the German definite deter-miner could be marked in sixteen different waysaccording to the possible combinations of genders(3), case (4) and number (2)1, which are fusedin six different tokens der, das, die, den, dem,des.
With the exception of the plural and gen-itive cases, all these words translate to the sameEnglish word: the.
In order to reduce the size ofthe German vocabulary and to improve the robust-ness of the alignment probabilities, we consideredvarious normalization strategies for the differentword classes.
In a nutshell, normalizing amountsto collapsing several German forms of a givenlemma into a unique representative, using manu-ally written normalization patterns.
A pattern typ-ically specifies which forms of a given morpho-logical paradigm should be considered equivalentwhen translating into English.
These normaliza-tion patterns use the lemma information computedby the TreeTagger and the fine-grained POS infor-mation computed by the RFTagger (Schmid andLaws, 2008), which uses a tagset containing ap-proximately 800 tags.
Table 1 displays the analy-sis of an example sentence.
2In most cases, normalization patterns replace aword form by its lemma; in order to partially pre-1For the plural forms, gender distinctions are neutralizedand the same 4 forms are used for all genders .2The English reference: Subsequently , the energized judi-ciary continued ruling against government decisions , embar-rassing the government ?
especially its intelligence agencies.serve some inflection marks, we introduced twogeneric suffixes, +s and +en which respectivelydenote plural and genitive wherever needed.
Typ-ical normalization rules take the following form:?
For articles, adjectives, and pronouns (Indef-inite , possessive, demonstrative, relative andreflexive), if a token has;?
Genitive case: replace with lemma+en(Ex.
des, der, des, der ?
d+en)?
Plural number: replace with lemma+s(Ex.
die, den ?
d+s)?
All other gender, case and number: re-place with lemma (Ex.
der, die, das, die?
d)?
For nouns;?
Plural number: replace with lemma+s(Ex.
Bilder, Bildern, Bilder ?
Bild+s))?
All other gender and case: replace withlemma (Ex Bild, Bilde, Bildes ?
Bild;Using these tags, a normalized version of previ-ous sentence is as follows: In d Folge befand d de-rart gesta?rkt Justiz wiederholt gegen d Regierungund insbesondere gegen d+en Geheimdienst+s.Several experiments were carried out to assess theeffect of different normalization schemes.
Remov-ing all gender and case information, except for thegenitive for articles, adjectives and pronouns, al-lowed to achieve the best BLEU scores.3.2 Compound SplittingCombining nouns, verbs and adjectives to forgenew words is a very common process in German.55It partly explains the difference between the num-ber of types and tokens between English and Ger-man in parallel texts.
In most cases, compoundsare formed by a mere concatenation of existingword forms, and can easily be split into simplerunits.
As words are freely conjoined, the vocab-ulary size increases vastly, yielding to sparse dataproblems that turn into unreliable parameter esti-mates.
We used the frequency-based segmenta-tion algorithm initially introduced in (Koehn andKnight, 2003) to handle compounding.
Our im-plementation extends this technique to handle themost common letter fillers at word junctions.
Inour experiments, we investigated different split-ting schemes in a manner similar to the work of(Stymne, 2008).4 French-English systems4.1 Baseline N -coder systemsFor this language pair, we used our in-houseN -code system, which implements the n-gram-based approach to SMT.
In a nutshell, the transla-tion model is implemented as a stochastic finite-state transducer trained using a n-gram modelof (source,target) pairs (Casacuberta and Vidal,2004).
Training this model requires to reordersource sentences so as to match the target wordorder.
This is performed by a stochastic finite-state reordering model, which uses part-of-speechinformation3 to generalize reordering patterns be-yond lexical regularities.In addition to the translation model, our sys-tem implements eight feature functions which areoptimally combined using a discriminative train-ing framework (Och, 2003): a target-languagemodel; two lexicon models, which give comple-mentary translation scores for each tuple; twolexicalized reordering models aiming at predict-ing the orientation of the next translation unit;a ?weak?
distance-based distortion model; andfinally a word-bonus model and a tuple-bonusmodel which compensate for the system prefer-ence for short translations.
One novelty this yearare the introduction of lexicalized reordering mod-els (Tillmann, 2004).
Such models require toestimate reordering probabilities for each phrasepairs, typically distinguishing three case, depend-ing whether the current phrase is translated mono-tone, swapped or discontiguous with respect to the3Part-of-speech information for English and French iscomputed using the above mentioned TreeTagger.previous (respectively next phrase pair).In our implementation, we modified the threeorientation types originally introduced and con-sider: a consecutive type, where the originalmonotone and swap orientations are lumped to-gether, a forward type, specifying a discontiguousforward orientation, and a backward type, specify-ing a discontiguous backward orientation.
Empir-ical results showed that in our case, the new orien-tations slightly outperform the original ones.
Thismay be explained by the fact that the model is ap-plied over tuples instead of phrases.Counts of these three types are updated foreach unit collected during the training process.Given these counts, we can learn probability dis-tributions of the form pr(orientation|(st)) whereorientation ?
{c, f, b} (consecutive, forwardand backward) and (st) is a translation unit.Counts are typically smoothed for the estimationof the probability distribution.The overall search process is performed by ourin-house n-code decoder.
It implements a beam-search strategy on top of a dynamic programmingalgorithm.
Reordering hypotheses are computedin a preprocessing step, making use of reorderingrules built from the word reorderings introducedin the tuple extraction process.
The resulting re-ordering hypotheses are passed to the decoder inthe form of word lattices (Crego and no, 2006).4.2 A bilingual POS-based reordering modelFor this year evaluation, we also experimentedwith an additional reordering model, which is esti-mated as a standard n-gram language model, overgeneralized translation units.
In the experimentsreported below, we generalized tuples using POStags, instead of raw word forms.
Figure 1 displaysthe same sequence of tuples when built from sur-face word forms (top), and from POS tags (bot-tom).Figure 1: Sequence of units built from surfaceword forms (top) and POS-tags (bottom).Generalizing units greatly reduces the numberof symbols in the model and enables to take larger56n-gram contexts into account: in the experimentsreported below, we used up to 6-grams.
This newmodel is thus helping to capture the mid-rangesyntactic reorderings that are observed in the train-ing corpus.
This model can also be seen as a trans-lation model of the sentence structure.
It modelsthe adequacy of translating sequences of sourcePOS tags into target POS tags.
Additional detailson these new reordering models can be found in(Crego and Yvon, 2010).4.3 Combining translation modelsOur main translation model being a conventionaln-gram model over bilingual units, it can directlytake advantage of all the techniques that exist forthese models.
To take the diversity of the availableparallel corpora into account, we independentlytrained several translation models on subpart ofthe training data.
These translation models werethen linearly interpolated, where the interpolationweights are chosen so as to minimize the perplex-ity on the development set.5 Language ModelsThe English and French language models (LMs)are the same as for the last year?s French-Englishtask (Allauzen et al, 2009) and are heavily tunedto the newspaper/newswire genre, using the firstpart of the WMT09 official development data(dev2009a).
We used all the authorized newscorpora, including the French and English Gi-gaword corpora, for translating both into French(1.4 billion tokens) and English (3.7 billion to-kens).
To estimate such LMs, a vocabulary wasdefined for both languages by including all to-kens in the WMT parallel data.
This initial vo-cabulary of 130K words was then extended withthe most frequent words observed in the trainingdata, yielding a vocabulary of one million wordsin both languages.
The training data was dividedinto several sets based on dates and genres (resp.7 and 9 sets for English and French).
On eachset, a standard 4-gram LM was estimated fromthe 1M word vocabulary with in-house tools usingKneser-Ney discounting interpolated with lowerorder models (Kneser and Ney, 1995; Chen andGoodman, 1998)4.
The resulting LMs were thenlinearly combined using interpolation coefficients4Given the amount of training data, the use of the modi-fied Kneser-Ney smoothing is prohibitive while previous ex-periments did not show significant improvements.chosen so as to minimize perplexity of the de-velopment set (dev2009a).
The final LMs werefinally pruned using perplexity as pruning crite-rion (Stolcke, 1998).For German, since we have less trainingdata, we only used the German monolingualtexts (Europarl-v5, News Commentary and NewsMonolingual) provided by the organizers to traina single n-gram language model, with modifiedKneser-Ney smoothing scheme (Chen and Good-man, 1998), using the SRILM toolkit (Stolcke,2002).6 TuningMoses-based systems were tuned using the imple-mentation of minimum error rate training (MERT)(Och, 2003) distributed with the Moses decoder,using the development corpus (news-test2008).The N -code systems were also tuned bythe same implementation of MERT, which wasslightly modified to match the requirements of ourdecoder.
The BLEU score is used as objectivefunction for MERT and to evaluate test perfor-mance.
The interpolation experiment for French-English was tuned on news-test2008a (first 1025lines).
Optimization was carried out over new-stest2008b (last 1026 lines).7 ExperimentsFor each system, we used all the available par-allel corpora distributed for this evaluation.
Weused Europarl and News commentary corpora forGerman-English task and Europarl, News com-mentary, United Nations and Gigaword corporafor the French-English tasks.
All corpora werealigned with GIZA++ for word-to-word align-ments with grow-diag-final-and and default set-tings.
For the German-English tasks, we appliednormalization and compound splitting as a pre-processing step.
For the French-English tasks, weused new POS-based reordering model and inter-polation.7.1 German-English TasksWe combined our two preprocessing schemes (seeSection 3) by applying compound splitting overnormalized data.
Our experiments showed that forGerman to English, using 4 characters as the mini-mum split length and 8 characters as the minimumcompound candidate, and allowing the insertion of-s -n -en -nen -e -es -er -ien) and the truncation of57-e -en -n yielded the best BLEU scores.
On thereverse direction, the best setting is different: 5characters as minimum split length, 10 charactersas minimum compound candidate, no truncation.These processes are performed before align-ment, training, tuning and decoding.
Before de-coding, we also replaced all OOV words with theirlemma.
We used the Moses (Koehn et al, 2007)decoder, with default settings, to obtain the trans-lations.
For translating from English to German,we used a two-level decoding.
The first decodingstep translates English to ?preprocessed German?,which is then turned into German by undoing theeffect of normalization.
In this second step, wethus aim at restoring inflection marks and at merg-ing compounds.
For this second ?translation?
step,we also use a Moses-based system.
To point outthe error rate of the second step, we also translatedthe preprocessed reference German text and com-puted the BLEU score as 97.05.
Our experimentsshowed that this two-level decoding strategy wasnot improving the direct baseline systems.
Table 2reports the BLEU scores5 on newstest2010 of ourofficial submissions.System De ?
En En ?
DeBaseline 20.0 15.3Norm+Split 21.3 15.0Table 2: Results for German-English7.2 French-English tasksAs explained above, in addition to the baselinesystem (base), two contrast systems were built.The first introduces an additional POS-based bilin-gual 6-gram reordering model (bilrm), the secondimplements the bilingual n-gram model after in-terpolating 4 models trained respectively on thenews, epps, UNdoc and gigaword subparts of theparallel corpus (interp).
Optimization was carriedout over newstest2008b (last 1026 lines) and testedover newstest2010 (2489 lines).
Table 3 reportstranslation accuracy for the three systems and forboth translation directions.As can be seen, the system using the newreordering model (base+bilrm) outperformed thebaseline system when translating into French,while no difference was measured when translat-ing into English.
The interpolation experiments5Scores are computed with the official script mteval-v11b.plSystem Fr ?
En En ?
Frbase 26.52 27.22base+bilrm 26.50 27.84base+bilrm+interp 26.84 27.62Table 3: Results for French-Englishdid not show any clear impact on performance.8 ConclusionsIn this paper, we presented our statistical MT sys-tems developed for the WMT?10 shared task, in-cluding several novelties, namely the preprocess-ing of German, and the integration of several newtechniques in our n-gram based decoder.AcknowledgmentsThis work was partly realized as part of the QuaeroProgram, funded by OSEO, the French agency forinnovation.ReferencesAlexandre Allauzen, Josep M. Crego, Aure?lien Max,and Franc?ois Yvon.
2009.
LIMSI?s statistical trans-lation systems for WMT?09.
In Proceedings ofWMT?09, Athens, Greece.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for lan-guage modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University.Simon Corston-oliver and Michael Gamon.
2004.Normalizing german and english inflectional mor-phology to improve statistical word alignment.
InProceedings of the Conference of the Association forMachine Translation in the Americas, pages 48?57.Springer Verlag.Josep M. Crego and Jose?
B. Mari no.
2006.
Improvingstatistical MT by coupling reordering and decoding.Machine Translation, 20(3):199?215.Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-nard, and Franc?ois Yvon.
2008.
LIMSI?s statisti-cal translation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.Sharon Goldwater and David McClosky.
2005.
Im-proving statistical MT through morphological analy-sis.
In Proceedings of Human Language Technology58Conference and Conference on Empirical Methodsin Natural Language Processing, pages 676?683,Vancouver, British Columbia, Canada, October.Reinhard Kneser and Herman Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing, ICASSP?95,pages 181?184, Detroit, MI.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In EACL ?03: Pro-ceedings of the tenth conference on European chap-ter of the Association for Computational Linguistics,pages 187?193.
Association for Computational Lin-guistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
Annual Meeting of the Association for Compu-tational Linguistics (ACL), demonstration session,Prague, Czech Republic.Jose?
B. Marin?o, Rafael E. Banchs R, Josep M. Crego,Adria` de Gispert, Patrick Lambert, Jose?
A.R.
Fonol-losa, and Marta R. Costa-Jussa`.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4):527?549.Sonja Niessen and Hermann Ney.
2004.
Statisti-cal machine translation with scarce resources usingmorpho-syntatic information.
Computational Lin-guistics, 30(2):181?204.Franz J. Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan.Helmut Schmid and Florian Laws.
2008.
Estima-tion of conditional probabilities with decision treesand an application to fine-grained POS tagging.
InProceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages777?784, Manchester, UK, August.
Coling 2008 Or-ganizing Committee.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In In Proceedings of theDARPA Broadcast News Transcription and Under-standing Workshop, pages 270?274.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Langage Processing(ICSLP), volume 2, pages 901?904, Denver, CO.Sara Stymne.
2008.
German compounds in factoredstatistical machine translation.
In GoTAL ?08: Pro-ceedings of the 6th international conference on Ad-vances in Natural Language Processing, pages 464?475, Berlin, Heidelberg.
Springer-Verlag.Sara Stymne.
2009.
A comparison of merging strate-gies for translation of german compounds.
In EACL?09: Proceedings of the 12th Conference of theEuropean Chapter of the Association for Compu-tational Linguistics: Student Research Workshop,pages 61?69, Morristown, NJ, USA.
Association forComputational Linguistics.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of the Human Language Technology con-ference / North American chapter of the Associationfor Computational Linguistics 2004, pages 101?104,Boston, MA, USA.59
