Proceedings of ACL-08: HLT, pages 701?709,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsImproving Search Results Quality by Customizing Summary LengthsMichael KaisserUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWm.kaisser@sms.ed.ac.ukMarti A. HearstUC Berkeley102 South HallBerkeley, CA 94705hearst@ischool.berkeley.eduJohn B. LowePowerset, Inc.475 Brannan St.San Francisco, CA 94107johnblowe@gmail.comAbstractWeb search engines today typically show re-sults as a list of titles and short snippets thatsummarize how the retrieved documents arerelated to the query.
However, recent researchsuggests that longer summaries can be prefer-able for certain types of queries.
This pa-per presents empirical evidence that judgescan predict appropriate search result summarylengths, and that perceptions of search resultquality can be affected by varying these resultlengths.
These findings have important impli-cations for search results presentation, espe-cially for natural language queries.1 IntroductionSearch results listings on the web have become stan-dardized as a list of information summarizing theretrieved documents.
This summary information isoften referred to as the document?s surrogate (Mar-chionini et al, 2008).In older search systems, such as those used innews and legal search, the document surrogate typ-ically consisted of the title and important metadata,such as date, author, source, and length of the article,as well as the document?s manually written abstract.In most cases, the full text content of the documentwas not available to the search engine and so no ex-tracts could be made.In web search, document surrogates typicallyshow the web page?s title, a URL, and informationextracted from the full text contents of the docu-ment.
This latter part is referred to by several dif-ferent names, including summary, abstract, extract,and snippet.
Today it is standard for web search en-gines to show these summaries as one or two linesof text, often with ellipses separating sentence frag-ments.
However, there is evidence that the ideal re-sult length is often longer than the standard snippetlength, and that furthermore, result length dependson the type of answer being sought.In this paper, we systematically examine the ques-tion of search result length preference, comparingdifferent result lengths for different query types.
Wefind evidence that desired answer length is sensitiveto query type, and that for some queries longer an-swers are judged to be of higher quality.In the following sections we summarize the re-lated work on result length variation and on querytopic classification.
We then describe two studies.
Inthe first, judges examined queries and made predic-tions about the expected answer types and the idealanswer lengths.
In the second study, judges ratedanswers of different lengths for these queries.
Thestudies find evidence supporting the idea that differ-ent query types are best answered with summariesof different lengths.2 Related Work2.1 Query-biased SummariesIn the early days of the web, the result summaryconsisted of the first few lines of text, due both toconcerns about intellectual property, and because of-ten that was the only part of the full text that thesearch engines retained from their crawls.
Eventu-ally, search engines started showing what are knownvariously as query-biased summaries, keyword-in-701context (KWIC) extractions, and user-directed sum-maries (Tombros and Sanderson, 1998).
In thesesummaries, sentence fragments, full sentences, orgroups of sentences that contain query terms are ex-tracted from the full text.
Early versions of this ideawere developed in the Snippet Search tool (Peder-sen et al, 1991) and the Superbook tool?s Table-of-Contents view (Egan et al, 1989).A query-biased summary shows sentences thatsummarize the ways the query terms are used withinthe document.
In addition to showing which subsetsof query terms occur in a retrieved document, thisdisplay also exposes the context in which the queryterms appear with respect to one another.Research suggests that query-biased summariesare superior to showing the first few sentences fromdocuments.
Tombros & Sanderson (1998), in astudy with 20 participants using TREC ad hoc data,found higher precision and recall and higher sub-jective preferences for query-biased summaries oversummaries showing the first few sentences.
Simi-lar results for timing and subjective measurementswere found by White et al (2003) in a study with24 participants.
White et al (2003) also describeexperiments with different sentence selection mech-anisms, including giving more weight to sentencesthat contained query words along with text format-ting.There are significant design questions surround-ing how best to formulate and display query-biasedsummaries.
As with standard document summariza-tion and extraction, there is an inherent trade-offbetween showing long, informative summaries andminimizing the screen space required by each searchhit.
There is also a tension between showing shortsnippets that contain all or most of the query termsand showing coherent stretches of text.
If the queryterms do not co-occur near one another, then the ex-tract has to become very long if full sentences andall query terms are to be shown.
Many web searchengine snippets compromise by showing fragmentsinstead of sentences.2.2 Studies Comparing Results LengthsRecently, a few studies have analyzed the results ofvarying search summary length.In the question-answering context (as opposed togeneral web search), Lin et al (2003) conducteda usability study with 32 computer science studentscomparing four types of answer context: exact an-swer, answer-in-sentence, answer-in-paragraph, andanswer-in-document.
To remove effects of incorrectanswers, they used a system that produced only cor-rect answers, drawn from an online encyclopedia.Participants viewed answers for 8 question scenar-ios.
Lin et al (2003) found no significant differ-ences in task completion times, but they did find dif-ferences in subjective responses.
Most participants(53%) preferred paragraph-sized chunks, noting thata sentence wasn?t much more information beyondthe exact answer, and a full document was often-times too long.
That said, 23% preferred full docu-ments, 20% preferred sentences, and one participantpreferred exact answer, thus suggesting that there isconsiderable individual variation.Paek et al (2004) experimented with showing dif-fering amounts of summary information in resultslistings, controlling the study design so that onlyone result in each list of 10 was relevant.
For halfthe test questions, the target information was visi-ble in the original snippet, and for the other half, theparticipant needed to use their mouse to view moreinformation from the relevant search result.
Theycompared three interface conditions:(i) a standard search results listing, in which amouse click on the title brings up the full textof the web page,(ii) ?instant?
view, for which a mouseclick ex-panded the document summary to show addi-tional sentences from the document, and thosesentences contained query terms and the an-swer to the search task, and(iii) a ?dynamic?
view that responded to a mousehover, and dynamically expanded the summarywith a few words at a time.Eleven out of 18 participants preferred instantview over the other two views, and on average allparticipants produced faster and more accurate re-sults with this view.
Seven participants preferred dy-namic view over the others, but many others foundthis view disruptive.
The dynamic view sufferedfrom the problem that, as the text expanded, themouse no longer covered the selected results, and702so an unintended, different search result sometimesstarted to expand.
Notably, none of the participantspreferred the standard results listing view.Cutrell & Guan (2007), compared search sum-maries of varying length: short (1 line of text),medium (2-3 lines) and long (6-7 lines) using searchengine-produced snippets (it is unclear if the sum-mary text was contiguous or included ellipses).They also compared 6 navigational queries (wherethe goal is to find a website?s homepage), with 6 in-formational queries (e.g., ?find when the Titanic setsail for its only voyage and what port it left from,?
?find out how long the Las Vegas monorail is?).
Ina study with 22 participants, they found that partic-ipants were 24 seconds faster on average with thelong view than with the short and medium view.
Thealso found that participants were 10 seconds sloweron average with the long view for the navigationaltasks.
They present eye tracking evidence whichsuggests that on the navigational task, the extra textdistracts the eye from the URL.
They did not re-port on subjective responses to the different answerlengths.Rose et al (2007) varied search results summariesalong several dimensions, finding that text choppi-ness and sentence truncation had negative effects,and genre cues had positive effects.
They did notfind effects for varying summary length, but theyonly compared relatively similar summary lengths(2 vs. 3 vs. 4 lines long).2.3 Categorizing Questions by ExpectedAnswer TypesIn the field of automated question-answering, mucheffort has been expended on automatically deter-mining the kind of answer that is expected for agiven question.
The candidate answer types areoften drawn from the types of questions that haveappeared in the TREC Question Answering track(Voorhees, 2003).
For example, the Webclopediaproject created a taxonomy of 180 types of ques-tion targets (Hovy et al, 2002), and the FALCONproject (Harabagiu et al, 2003) developed an an-swer taxonomy with 33 top level categories (suchas PERSON, TIME, REASON, PRODUCT, LOCA-TION, NUMERICAL VALUE, QUOTATION), andthese were further refined into an unspecified num-ber of additional categories.
Ramakrishnan et al(2004) show an automated method for determiningexpected answer types using syntactic informationand mapping query terms to WordNet.2.4 Categorizing Web QueriesA different line of research is the query log cate-gorization problem.
In query logs, the queries areoften much more terse and ill-defined than in theTREC QA track, and, accordingly, the taxonomiesused to classify what is called the query intent havebeen much more general.In an attempt to demonstrate how informationneeds for web search differ from the assumptionsof pre-web information retrieval systems, Broder(2002) created a taxonomy of web search goals, andthen estimated frequency of such goals by a com-bination of an online survey (3,200 responses, 10%response rate) and a manual analysis of 1,000 queryfrom the AltaVista query logs.
This taxonomy hasbeen heavily influential in discussions of query typeson the Web.Rose & Levinson (2004) followed up on Broder?swork, again using web query logs, but developinga taxonomy that differed somewhat from Broder?s.They manually classified a set of 1,500 AltaVistasearch engine log queries.
For two sets of 500queries, the labeler saw just the query and the re-trieved documents; for the third set the labeler alsosaw information about which item(s) the searcherclicked on.
They found that the classifications thatused the extra information about clickthrough didnot change the proportions of assignments to eachcategory.
Because they did not directly comparejudgments with and without click information onthe same queries, this is only weak evidence thatquery plus retrieved documents is sufficient to clas-sify query intent.Alternatively, queries from web query logs can beclassified according to the topic of the query, inde-pendent of the type of information need.
For ex-ample, a search involving the topic of weather canconsist of the simple information need of lookingat today?s forecast, or the rich and complex infor-mation need of studying meteorology.
Over manyyears, Spink & Jansen et al (2006; 2007) have man-ually analyzed samples of query logs to track a num-ber of different trends.
One of the most notable isthe change in topic mix.
As an alternative to man-703ual classification of query topics, Shen et al (2005)described an algorithm for automatically classifyingweb queries into a set of pre-defined topics.
More re-cently, Broder et al (2007) presented a highly accu-rate method (around .7 F-score) for classifying short,rare queries into a taxonomy of 6,000 categories.3 Study GoalsRelated work suggests that longer results are prefer-able, but not for all query types.
The goal of ourefforts was to determine preferred result length forsearch results, depending on type of query.
To dothis, we performed two studies:1.
We asked a set of judges to categorize a largeset of web queries according to their expectedpreferred response type and expected preferredresponse length.2.
We then developed high-quality answer pas-sages of different lengths for a subset of thesequeries by selecting appropriate passages fromthe online encyclopedia Wikipedia, and askedjudges to rate the quality of these answers.The results of this study should inform search in-terface designers about what the best presentationformat is.3.1 Using Mechanical TurkFor these studies, we make use of a web service of-fered by Amazon.com called Mechanical Turk, inwhich participants (called ?turkers?)
are paid smallsums of money in exchange for work on ?HumanIntelligence tasks?
(HITs).1 These HITs are gener-ated from an XML description of the task createdby the investigator (called a ?requester?).
The par-ticipants can come from any walk of life, and theiridentity is not known to the requesters.
We have inpast work found the results produced by these judgesto be of high quality, and have put into place vari-ous checks to detect fraudulent behavior.
Other re-searchers have investigated the efficacy of language1Website: http://www.mturk.com.
For experiment 1, ap-proximately 38,000 HITs were completed at a cost of about$1,500.
For experiment 2, approximately 7,300 HITs werecompleted for about $170.
Turkers were paid between $.01 and$.05 per HIT depending on task complexity; Amazon imposesadditional charges.1.
Person(s)2.
Organization(s)3.
Time(s) (date, year, time span etc.)4.
Number or Quantity5.
Geographic Location(s) (e.g., city, lake, address)6.
Place(s) (e.g.,?the White House?, ?at a supermar-ket?)7.
Obtain resource online (e.g., movies, lyrics, books,magazines, knitting patterns)8.
Website or URL9.
Purchase and product information10.
Gossip and celebrity information11.
Language-related (e.g., translations, definitions,crossword puzzle answers)12.
General information about a topic13.
Advice14.
Reason or Cause, Explanation15.
Yes/No, with or without explanation or evidence16.
Other17.
UnjudgableTable 1: Allowable responses to the question: ?What sortof result or results does the query ask for??
in the firstexperiment.1.
A word or short phrase2.
A sentence3.
One or more paragraphs (i.e.
at least several sen-tences)4.
An article or full document5.
A list6.
Other, or some combination of the aboveTable 2: Allowable responses to the question: ?How longis the best result for this query??
in the first experiment.annotation using this service and have found that theresults are of high quality (Su et al, 2007).3.2 Estimating Ideal Answer Length and TypeWe developed a set of 12,790 queries, drawn fromPowerset?s in house query database which con-tains representative subsets of queries from differentsearch engines?
query logs, as well as hand-editedquery sets used for regression testing.
There are adisproportionally large number of natural languagequeries in this set compared with query sets fromtypical keyword engines.
Such queries are oftencomplete questions and are sometimes grammaticalfragments (e.g., ?date of next US election?)
and soare likely to be amenable to interesting natural lan-guage processing algorithms, which is an area of in-704Figure 1: Results of the first experiment.
The y-axis shows the semantic type of the predicted answer, in the sameorder as listed in Table 1; the x-axis shows the preferred length as listed in Table 2.
Three bars with length greaterthan 1,500 are trimmed to the maximum size to improve readability (GeneralInfo/Paragraphs, GeneralInfo/Article,and Number/Phrase).terest of our research.
The average number of wordsper query (as determined by white space separation)was 5.8 (sd.
2.9) and the average number of char-acters (including punctuation and white space) was32.3 (14.9).
This is substantially longer than the cur-rent average for web search query, which was ap-proximately 2.8 in 2005 (Jansen et al, 2007); this isdue to the existence of natural language queries.Judges were asked to classify each query accord-ing to its expected response type into one of 17 cat-egories (see Table 1).
These categories include an-swer types used in question answering research aswell as (to better capture the diverse nature of webqueries) several more general response types suchas Advice and General Information.
Additionally,we asked judges to anticipate what the best resultlength would be for the query, as shown in Table 2.Each of the 12,790 queries received three assess-ments by MTurk judges.
For answer types, thenumber of times all three judges agreed was 4537(35.4%); two agreed 6030 times (47.1%), and noneagreed 2223 times (17.4%).
Not surprisingly, therewas significant overlap between the label General-Info and the other categories.
For answer lengthestimations, all three judges agreed in 2361 cases(18.5%), two agreed in 7210 cases (56.4%) and none3219 times (25.2%).Figure 1 summarizes expected length judgmentsby estimated answer category.
Distribution of thelength categories differs a great deal across the in-dividual expected response categories.
In general,the results are intuitive: judges preferred short re-sponses for ?precise queries?
(e.g., those asking fornumbers) and they preferred longer responses forqueries in broad categories like Advice or Gener-alInfo.
But some results are less intuitive: for ex-ample, judges preferred different response lengthsfor queries categorized as Person and Organization?
in fact for the latter the largest single selectionmade was List.
Reviewing the queries for thesetwo categories, we note that most queries about or-ganizations in our collection asked for companies705length type average std devWord or Phrase 38.1 25.8Sentence 148.1 71.4Paragraph 490.5 303.1Section 1574.2 1251.1Table 3: Average number of characters for each answerlength type for the stimuli used in the second experiment.(e.g.
?around the world travel agency?)
and forthese there usually is more than one correct answer,whereas the queries about persons (?CEO of mi-crosoft? )
typically only had one relevant answer.The results of this table show that there are sometrends but not definitive relationships between querytype (as classified in this study) and expected answerlength.
More detailed classifications might help re-solve some of the conflicts.3.3 Result Length StudyThe purpose of the second study was twofold: first,to see if doing a larger study confirms what is hintedat in the literature: that search result lengths longerthan the standard snippet may be desirable for atleast a subset of queries.
Second, we wanted tosee if judges?
predictions of desirable results lengthswould be confirmed by other judges?
responses tosearch results of different lengths.3.3.1 MethodIt has been found that obtaining judges?
agree-ment on intent of a query from a log can be difficult(Rose and Levinson, 2004; Kellar et al, 2007).
Inorder to make the task of judging query relevanceeasier, for the next phase of the study we focusedon only those queries for which all three assessorsin the first experiment agreed both on the categorylabel and on the estimated ideal length.
There were1099 such high-confidence queries, whose averagenumber of words was 6.3 (2.9) and average numberof characters was 34.5 (14.3).We randomly selected a subset of the high-agreement queries from the first experiment andmanually excluded queries for which it seemed ob-vious that no responses could be found in Wikipedia.These included queries about song lyrics, since in-tellectual property restrictions prevent these beingposted, and crossword puzzle questions such as ?afour letter word for water.
?The remaining set contained 170 queries.
MTurkannotators were asked to find one good text passage(in English) for each query from the Wikipedia on-line encyclopedia.
They were also asked to subdi-vide the text of this answer into each of the followinglengths: a word or phrase, a sentence, a paragraph,a section or an entire article.2 Thus, the shorter an-swer passages are subsumed by the longer ones.Table 3 shows the average lengths and standarddeviations of each result length type.
Table 4 con-tains sample answers for the shorter length formatsfor one query.
For 24 of the 170 queries the annota-tors could not find a suitable response in Wikipedia,e.g., ?How many miles between NY and Milwau-kee??
We collected two to five results for each of theremaining 146 queries and manually chose the bestof these answer passages.
Note that, by design, allresponses were factually correct; they only differedin their length.Ten MTurk judges saw each query/answer lengthpair, and for each of these, were told: ?Below yousee a search engine query and a possible response.We would like you to give us your opinion about theresponse.
We are especially interested in the lengthof the response.
Is it suitable for the query?
Is theretoo much or not enough information?
Please rate theresponse on a scale from 0 (very bad response) to 10(very good response).?
There were 124 judges in to-tal; of these, 16 did more than 146 HITs, meaningthey saw the same query more than one time (butwith different lengths).
Upon examination of the re-sults, we determined that two of these high-volumejudges were not trying to do the task properly, and sowe dropped their judgments from the final analysis.3.3.2 ResultsOur results show that judges prefer results of dif-ferent lengths, depending on the query.
The re-sults also suggest that judges?
estimates of a pre-ferred result length in the first experiment are ac-curate predictors when there is strong agreementamong them.
Figure 2 shows in four diagrams2Note the slight difference between the length categories inthe first and second experiment: The List and Other optionswere dropped for the second experiment because we wanted toconcentrate on textual length.
Additionally, to provide morethan one option between Sentence and Article, the categoryOne or more paragraphs was split up into two: (One) Para-graph and (One) Section.706query Who was the first person to scale K2?Paragraph An Italian expedition finally succeeded in ascending to the summit of K2 on July 31, 1954.The expedition was led by Ardito Desio, although the two climbers who actually reachedthe top were Lino Lacedelli and Achille Compagnoni.
The team included a Pakistani mem-ber, Colonel Muhammad Ata-ullah.
He had been a part of an earlier 1953 American expe-dition which failed to make the summit because of a storm which killed a key climber, ArtGilkey.
On the expedition also was the famous Italian climber Walter Bonatti.
He provedvital to the expeditions success in that he carried vital oxygen to 26,600ft for Lacedelliand Compagnoni.
His dramatic bivouac, at that altitude with the equipment, wrote anotherchapter in the saga of Himalayan climbing.Sentence The expedition was led by Ardito Desio, although the two climbers who actually reachedthe top were Lino Lacedelli and Achille Compagnoni.Phrase Lino Lacedelli and Achille CompagnoniTable 4: Sample answers of differing lengths used as input for the second study.
Note that the shorter answers arecontained in the longer ones.
For the full article case, judges were asked to follow a hyperlink to an article.Figure 2: Results of the second experiment, where each query/answer-length pair was assessed by 8?10 judges usinga scale of 0 (?very bad?)
to 10 (?very good?).
Marks indicate means and standard errors.
The top left graph showsresponses of different lengths for queries that were classified as best answered with a phrase in the first experiment.The upper right shows responses for queries predicted to be best answered with a sentence, lower left for best answeredwith one or more paragraphs and lower right for best answered with an article.707Slope Std.
Error p-valuePhrase -0.850 0.044 < 0.0001Sentence -0.550 0.050 < 0.0001Paragraph 0.328 0.049 < 0.0001Article 0.856 0.053 < 0.0001Table 5: Results of unweighted linear regression on thedata for the second experiment, which was separated intofour groups based on the predicted preferred length.how queries assigned by judges to one of the fourlength categories from the first experiment werejudged when presented with responses of the fiveanswer lengths from the second experiment.
Thegraphs show the means and standard error of thejudges?
scores across all queries for each predicted-length/presented-length combination.In order to test whether these results are signifi-cant we performed four separate linear regressions;one for each of the predicted preferred length cat-egories.
The snippet length, the independent vari-able, was coded as 1-5, shortest to longest.
Thescore for each query-snippet pair is the dependentvariable.
Table 5 shows that for each group there isevidence to reject the null hypothesis that the slopeis equal to 0 at the 99% confidence level.
Highscores are associated with shorter snippet lengthsfor queries with predicted preferred length phraseor sentence and also with longer snippet lengths forqueries with predicted preferred length paragraphsor article.
These associations are strongest for thequeries with the most extreme predicted preferredlengths (phrase and article).Our results also suggest the intuition that the bestanswer lengths do not form strictly distinct classes,but rather lie on a continuum.
If the ideal response isfrom a certain category (e.g., a sentence), returning aresult from an adjacent category (a phrase or a para-graph) is not strongly penalized by judges, whereasretuning a result from a category further up or downthe scale (an article) is.One potential drawback of this study format isthat we do not show judges a list of results forqueries, as is standard in search engines, and so theydo not experience the tradeoff effect of longer resultsrequiring more scrolling if the desired answer is notshown first.
However, the earlier results of Cutrell &Guan (2007) and Paek et al (2004) suggest that thepreference for longer results occurs even in contextsthat require looking through multiple results.
An-other potential drawback of the study is that judgesonly view one relevant result; the effects of showinga list of long non-relevant results may be more neg-ative than that of showing short non-relevant results;this study would not capture that effect.4 Conclusions and Future WorkOur studies suggest that different queries are bestserved with different response lengths (Experi-ment 1), and that for a subset of especially clearqueries, human judges can predict the preferred re-sult lengths (Experiment 2).
The results furthermoresupport the contention that standard results listingsare too short in many cases, at least assuming thatthe summary shows information that is relevant forthe query.
These findings have important implica-tions for the design of search results presentations,suggesting that as user queries become more expres-sive, search engine results should become more re-sponsive to the type of answer desired.
This maymean showing more context in the results listing, orperhaps using more dynamic tools such as expand-on-mouseover to help answer the query in place.The obvious next step is to determine how to au-tomatically classify queries according to their pre-dicted result length and type.
For classifying ac-cording to expected length, we have run some initialexperiments based on unigram word counts whichcorrectly classified 78% of 286 test queries (on 805training queries) into one of three length bins.
Weplan to pursue this further in future work.
For classi-fying according to type, as discussed above, mostautomated query classification for web logs havebeen based on the topic of the query rather than onthe intended result type, but the question answeringliterature has intensively investigated how to pre-dict appropriate answer types.
It is likely that thetechniques from these two fields can be productivelycombined to address this challenge.Acknowledgments.
This work was supported inpart by Powerset, Inc., and in part by Microsoft Re-search through the MSR European PhD ScholarshipProgramme.
We would like to thank Susan Gruberand Bonnie Webber for their helpful comments andsuggestions.708ReferencesA.
Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josi-fovski, and T. Zhang.
2007.
Robust classification ofrare queries using web knowledge.
Proceedings of SI-GIR 2007.A.
Broder.
2002.
A taxonomy of web search.
ACMSIGIR Forum, 36(2):3?10.E.
Cutrell and Z. Guan.
2007.
What Are You LookingFor?
An Eye-tracking Study of Information Usage inWeb Search.
Proceedings of ACM SIGCHI 2007.D.E.
Egan, J.R. Remde, L.M.
Gomez, T.K.
Landauer,J.
Eberhardt, and C.C.
Lochbaum.
1989.
Formativedesign evaluation of Superbook.
ACM Transactionson Information Systems (TOIS), 7(1):30?57.S.M.
Harabagiu, S.J.
Maiorano, and M.A.
Pasca.
2003.Open-domain textual question answering techniques.Natural Language Engineering, 9(03):231?267.E.
Hovy, U. Hermjakob, and D. Ravichandran.
2002.A question/answer typology with surface text patterns.Proceedings of the second international conference onHuman Language Technology Research, pages 247?251.B.J.
Jansen and Spink.
2006.
How are we searchingthe World Wide Web?
A comparison of nine searchengine transaction logs.
Information Processing andManagement, 42(1):248?263.B.J.
Jansen, A. Spink, and S. Koshman.
2007.
Websearcher interaction with the Dogpile.com metasearchengine.
Journal of the American Society for Informa-tion Science and Technology, 58(5):744?755.M.
Kellar, C. Watters, and M. Shepherd.
2007.
A Goal-based Classification of Web Information Tasks.
JA-SIST, 43(1).J.
Lin, D. Quan, V. Sinha, K. Bakshi, D. Huynh, B. Katz,and D.R.
Karger.
2003.
What Makes a Good Answer?The Role of Context in Question Answering.
Human-Computer Interaction (INTERACT 2003).G.
Marchionini, R.W.
White, and Marchionini.
2008.Find What You Need, Understand What You Find.Journal of Human-Computer Interaction (to appear).T.
Paek, S.T.
Dumais, and R. Logan.
2004.
WaveLens:A new view onto internet search results.
Proceedingson the ACM SIGCHI Conference on Human Factors inComputing Systems, pages 727?734.J.
Pedersen, D. Cutting, and J. Tukey.
1991.
Snippetsearch: A single phrase approach to text access.
Pro-ceedings of the 1991 Joint Statistical Meetings.G.
Ramakrishnan and D. Paranjpe.
2004.
Is question an-swering an acquired skill?
Proceedings of the 13thinternational conference on World Wide Web, pages111?120.D.E.
Rose and D. Levinson.
2004.
Understanding usergoals in web search.
Proceedings of the 13th interna-tional conference on World Wide Web, pages 13?19.D.E.
Rose, D. Orr, and R.G.P.
Kantamneni.
2007.
Sum-mary attributes and perceived search quality.
Pro-ceedings of the 16th international conference on WorldWide Web, pages 1201?1202.D.
Shen, R. Pan, J.T.
Sun, J.J. Pan, K. Wu, J. Yin,and Q. Yang.
2005.
Q2C@UST: our winning solu-tion to query classification in KDDCUP 2005.
ACMSIGKDD Explorations Newsletter, 7(2):100?110.Q.
Su, D. Pavlov, J. Chow, and W. Baker.
2007.
Internet-Scale Collection of Human-Reviewed Data.
Proceed-ings of WWW 2007.A.
Tombros and M. Sanderson.
1998.
Advantages ofquery biased summaries in information retrieval.
Pro-ceedings of the 21st annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 2?10.E.M.
Voorhees.
2003.
Overview of the TREC 2003Question Answering Track.
Proceedings of theTwelfth Text REtrieval Conference (TREC 2003).R.W.
White, J. Jose, and I. Ruthven.
2003.
A task-oriented study on the influencing effects of query-biased summarisation in web searching.
InformationProcessing and Management, 39(5):707?733.709
