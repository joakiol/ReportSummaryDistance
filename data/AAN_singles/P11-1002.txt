Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 12?21,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDeciphering Foreign LanguageSujith Ravi and Kevin KnightUniversity of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292{sravi,knight}@isi.eduAbstractIn this work, we tackle the task of ma-chine translation (MT) without parallel train-ing data.
We frame the MT problem as a de-cipherment task, treating the foreign text asa cipher for English and present novel meth-ods for training translation models from non-parallel text.1 IntroductionBilingual corpora are a staple of statistical machinetranslation (SMT) research.
From these corpora,we estimate translation model parameters: word-to-word translation tables, fertilities, distortion pa-rameters, phrase tables, syntactic transformations,etc.
Starting with the classic IBM work (Brown etal., 1993), training has been viewed as a maximiza-tion problem involving hidden word alignments (a)that are assumed to underlie observed sentence pairs(e, f ):argmax??e,fP?
(f |e) (1)= argmax??e,f?aP?
(f, a|e) (2)Brown et al (1993) give various formulas that boilP?
(f, a|e) down to the specific parameters to be es-timated.Of course, for many language pairs and domains,parallel data is not available.
In this paper, weaddress the problem of learning a full transla-tion model from non-parallel data, and we use thelearned model to translate new foreign strings.
Assuccessful work develops along this line, we expectmore domains and language pairs to be conqueredby SMT.How can we learn a translation model from non-parallel data?
Intuitively, we try to construct trans-lation model tables which, when applied to ob-served foreign text, consistently yield sensible En-glish.
This is essentially the same approach taken bycryptanalysts and epigraphers when they deal withsource texts.In our case, we observe a large number of foreignstrings f , and we apply maximum likelihood train-ing:argmax??fP?
(f) (3)Following Weaver (1955), we imagine that this cor-pus of foreign strings ?is really written in English,but has been coded in some strange symbols,?
thus:argmax?
?f?eP (e) ?
P?
(f |e) (4)The variable e ranges over all possible Englishstrings, and P (e) is a language model built fromlarge amounts of English text that is unrelated to theforeign strings.
Re-writing for hidden alignments,we get:argmax?
?f?eP (e) ??aP?
(f, a|e) (5)Note that this formula has the same freeP?
(f, a|e) parameters as expression (2).
We seekto manipulate these parameters in order to learn the12same full translation model.
We note that for eachf , not only is the alignment a still hidden, but nowthe English translation e is hidden as well.A language model P (e) is typically used in SMTdecoding (Koehn, 2009), but here P (e) actuallyplays a central role in training translation model pa-rameters.
To distinguish the two, we refer to (5) asdecipherment, rather than decoding.We can now draw on previous deciphermentwork for solving simpler substitution/transpositionciphers (Bauer, 2006; Knight et al, 2006).
We mustkeep in mind, however, that foreign language is amuch more demanding code, involving highly non-deterministic mappings and very large substitutiontables.The contributions of this paper are therefore:?
We give first results for training a full transla-tion model from non-parallel text, and we applythe model to translate previously-unseen text.This work is thus distinguished from prior workon extracting or augmenting partial lexiconsusing non-parallel corpora (Rapp, 1995; Fungand McKeown, 1997; Koehn and Knight, 2000;Haghighi et al, 2008).
It also contrasts withself-training (McClosky et al, 2006), which re-quires a parallel seed and often does not engagein iterative maximization.?
We develop novel methods to deal with large-scale vocabularies inherent in MT problems.2 Word Substitution DeciphermentBefore we tackle machine translation without par-allel data, we first solve a simpler problem?wordsubstitution decipherment.
Here, we do not have toworry about hidden alignments since there is onlyone alignment.
In a word substitution cipher, everyword in the natural language (plaintext) sequence issubstituted by a cipher token, according to a substi-tution key.
The key is deterministic?there exists a1-to-1 mapping between cipher units and the plain-text words they encode.For example, the following English plaintext se-quences:I SAW THE BOY .THE BOY RAN .may be enciphered as:xyzz fxyy crqq tmnz lxwzcrqq tmnz gdxx lxwzaccording to the key:THE ?
crqq, SAW ?
fxyy, RAN ?
gdxx,.
?
lxwz, BOY ?
tmnz, I ?
xyzzThe goal of word substitution decipherment is toguess the original plaintext from given cipher datawithout any knowledge of the substitution key.Word substitution decipherment is a good test-bedfor unsupervised statistical NLP techniques for tworeasons?
(1) we face large vocabularies and corporasizes typically seen in large-scale MT problems, soour methods need to scale well, (2) similar deci-pherment techniques can be applied for solving NLPproblems such as unsupervised part-of-speech tag-ging.Probabilistic decipherment: Our deciphermentmethod follows a noisy-channel approach.
We firstmodel the process by which the ciphertext sequencec = c1...cn is generated.
The generative story fordecipherment is described here:1.
Generate an English plaintext sequence e =e1...en, with probability P (e).2.
Substitute each plaintext word ei with a cipher-text token ci, with probability P?
(ci|ei) in orderto generate the ciphertext sequence c = c1...cn.We model P (e) using a statistical word n-gramEnglish language model (LM).
During decipher-ment, our goal is to estimate the channel model pa-rameters ?.
Re-writing Equations 3 and 4 for wordsubstitution decipherment, we get:argmax??cP?
(c) (6)= argmax?
?c?eP (e) ?n?i=1P?
(ci|ei) (7)Challenges: Unlike letter substitution ciphers(having only 26 plaintext letters), here we have todeal with large-scale vocabularies (10k-1M wordtypes) and corpora sizes (100k cipher tokens).
Thisposes some serious scalability challenges for wordsubstitution decipherment.13We propose novel methods that can deal withthese challenges effectively and solve word substi-tution ciphers:1.
EM solution: We would like to use the Expecta-tion Maximization (EM) algorithm (Dempsteret al, 1977) to estimate ?
from Equation 7, butEM training is not feasible in our case.
First,EM cannot scale to such large vocabulary sizes(running the forward-backward algorithm foreach iteration requires O(V 2) time).
Secondly,we need to instantiate the entire channel and re-sulting derivation lattice before we can run EM,and this is too big to be stored in memory.
So,we introduce a new training method (IterativeEM) that fixes these problems.2.
Bayesian decipherment: We also propose anovel decipherment approach using Bayesianinference.
Typically, Bayesian inference is veryslow when applied to such large-scale prob-lems.
Our method overcomes these challengesand does fast, efficient inference using (a) anovel strategy for selecting sampling choices,and (b) a parallelized sampling scheme.In the next two sections, we describe these meth-ods in detail.2.1 Iterative EMWe devise a method which overcomes memory andrunning time efficiency issues faced by EM.
Insteadof instantiating the entire channel model (with all itsparameters), we iteratively train the model in smallsteps.
The training procedure is described here:1.
Identify the top K frequent word types in boththe plaintext and ciphertext data.
Replace allother word tokens with Unknown.
Now, instan-tiate a small channel with just (K + 1)2 pa-rameters and use the EM algorithm to train thismodel to maximize likelihood of cipher data.2.
Extend the plaintext and ciphertext vocabular-ies from the previous step by adding the nextK most frequent word types (so the new vo-cabulary size becomes 2K + 1).
Regeneratethe plaintext and ciphertext data.3.
Instantiate a new (2K+1)?
(2K+1) channelmodel.
From the previous EM-trained channel,identify all the e ?
c mappings that were as-signed a probability P (c|e) > 0.5.
Fix thesemappings in the new channel, i.e.
set P (c|e) =1.0.
From the new channel, eliminate all otherparameters e ?
cj associated with the plain-text word type e (where cj 6= c).
This yields amuch smaller channel with size < (2K + 1)2.Retrain the new channel using EM algorithm.4.
Goto Step 2 and repeat the procedure, extend-ing the channel size iteratively in each stage.Finally, we decode the given ciphertext c by usingthe Viterbi algorithm to choose the plaintext decod-ing e that maximizes P (e) ?
P?trained(c|e)3, stretch-ing the channel probabilities (Knight et al, 2006).2.2 Bayesian DeciphermentBayesian inference methods have become popularin natural language processing (Goldwater and Grif-fiths, 2007; Finkel et al, 2005; Blunsom et al, 2009;Chiang et al, 2010; Snyder et al, 2010).
Thesemethods are attractive for their ability to manage un-certainty about model parameters and allow one toincorporate prior knowledge during inference.Here, we propose a novel decipherment approachusing Bayesian learning.
Our method holds sev-eral other advantages over the EM approach?
(1)inference using smart sampling strategies permitsefficient training, allowing us to scale to largedata/vocabulary sizes, (2) incremental scoring ofderivations during sampling allows efficient infer-ence even when we use higher-order n-gram LMs,(3) there are no memory bottlenecks since the fullchannel model and derivation lattice are never in-stantiated during training, and (4) prior specificationallows us to learn skewed distributions that are usefulhere?word substitution ciphers exhibit 1-to-1 cor-respondence between plaintext and cipher types.We use the same generative story as before fordecipherment, except that we use Chinese Restau-rant Process (CRP) formulations for the source andchannel probabilities.
We use an English word bi-gram LM as the base distribution (P0) for the sourcemodel and specify a uniform P0 distribution for the14channel.1 We perform inference using point-wiseGibbs sampling (Geman and Geman, 1984).
We de-fine a sampling operator that samples plaintext wordchoices for every cipher token, one at a time.
Usingthe exchangeability property, we efficiently scorethe probability of each derivation in an incrementalfashion.
In addition, we make further improvementsto the sampling procedure which makes it faster.Smart sample-choice selection: In the originalsampling step, for each cipher token we have to sam-ple from a list of all possible plaintext choices (10k-1M English words).
There are 100k cipher tokensin our data which means we have to perform ?
109sampling operations to make one entire pass throughthe data.
We have to then repeat this process for2000 iterations.
Instead, we now reduce our choicesin each sampling step.Say that our current plaintext hypothesis containsEnglish words X, Y and Z at positions i ?
1, i andi+1 respectively.
In order to sample at position i, wechoose the topK English words Y ranked by P (X YZ), which can be computed offline from a statisticalword bigram LM.
If this probability is 0 (i.e., X andZ never co-occurred), we randomly pick K wordsfrom the plaintext vocabulary.
We set K = 100 inour experiments.
This significantly reduces the sam-pling possibilities (10k-1M reduces to 100) at eachstep and allows us to scale to large plaintext vocab-ulary sizes without enumerating all possible choicesat every cipher position.2Parallelized Gibbs sampling: Secondly, we paral-lelize our sampling step using a Map-Reduce frame-work.
In the past, others have proposed parallelizedsampling schemes for topic modeling applications(Newman et al, 2009).
In our method, we split theentire corpus into separate chunks and we run thesampling procedure on each chunk in parallel.
At1For word substitution decipherment, we want to keep thelanguage model probabilities fixed during training, and hencewe set the prior on that model to be high (?
= 104).
We use asparse Dirichlet prior for the channel (?
= 0.01).
We use theoutput from Iterative EM decoding (using 101 x 101 channel)as initial sample and run the sampler for 2000 iterations.
Dur-ing sampling, we use a linear annealing schedule decreasing thetemperature from 1?
0.08.2Since we now sample from an approximate distribution, wehave to correct this with the Metropolis-Hastings algorithm.
Butin practice we observe that samples from our proposal distribu-tion are accepted with probability > 0.99, so we skip this step.the end of each sampling iteration, we combine thesamples corresponding to each chunk and collect thecounts of all events?this forms our cache for thenext sampling iteration.
In practice, we observe thatthe parallelized sampling run converges quickly andruns much faster than the conventional point-wisesampling?for example, 3.1 hours (using 10 nodes)versus 11 hours for one of the word substitution ex-periments.
We also notice a higher speedup whenscaling to larger vocabularies.3Decoding the ciphertext: After the sampling runhas finished, we choose the final sample and ex-tract a trained version of the channel model P?
(c|e)from this sample following the technique of Chi-ang et al (2010).
We then use the Viterbi algo-rithm to choose the English plaintext e that maxi-mizes P (e) ?
P?trained(c|e)3.2.3 Experiments and ResultsData: For the word substitution experiments, we usetwo corpora:?
Temporal expression corpus containing shortEnglish temporal expressions such as ?THENEXT MONTH?, ?THE LAST THREEYEARS?, etc.
The cipher data contains 5000expressions (9619 tokens, 153 word types).We also have access to a separate Englishcorpus (which is not parallel to the ciphertext)containing 125k temporal expressions (242kword tokens, 201 word types) for LM training.?
Transtac corpus containing full English sen-tences.
The data consists of 10k cipher sen-tences (102k tokens, 3397 word types); anda plaintext corpus of 402k English sentences(2.7M word tokens, 25761 word types) for LMtraining.
We use all the cipher data for deci-pherment training but evaluate on the first 1000cipher sentences.The cipher data was originally generated from En-glish text by substituting each English word with aunique cipher word.
We use the plaintext corpus to3Type sampling could be applied on top of our methods tofurther optimize performance.
But more complex problems likeMT do not follow the same principles (1-to-1 key mappings)as seen in word substitution ciphers, which makes it difficult toidentify type dependencies.15Method Decipherment Accuracy (%)Temporal expr.
Transtac9k 100k0.
EM with 2-gram LM 87.8 Intractable1.
Iterative EMwith 2-gram LM 87.8 70.5 71.82.
Bayesianwith 2-gram LM 88.6 60.1 80.0with 3-gram LM 82.5Figure 1: Comparison of word substitution deciphermentresults using (1) Iterative EM, and (2) Bayesian method.For the Transtac corpus, decipherment performance isalso shown for different training data sizes (9k versus100k cipher tokens).build an English word n-gram LM, which is used inthe decipherment process.Evaluation: We compute the accuracy of a particu-lar decipherment as the percentage of cipher tokensthat were correctly deciphered from the whole cor-pus.
We run the two methods (Iterative EM4 andBayesian) and then compare them in terms of wordsubstitution decipherment accuracies.Results: Figure 1 compares the word substitutionresults from Iterative EM and Bayesian decipher-ment.
Both methods achieve high accuracies, de-coding 70-90% of the two word substitution ciphers.Overall, Bayesian decipherment (with sparse priors)performs better than Iterative EM and achieves thebest results on this task.
We also observe that bothmethods benefit from better LMs and more (cipher)training data.
Figure 2 shows sample outputs fromBayesian decipherment.3 Machine Translation as a DeciphermentTaskWe now turn to the problem of MT without par-allel data.
From a decipherment perspective, ma-chine translation is a much more complex task thanword substitution decipherment and poses severaltechnical challenges: (1) scalability due to largecorpora sizes and huge translation tables, (2) non-determinism in translation mappings (a word canhave multiple translations), (3) re-ordering of words4For Iterative EM, we start with a channel of size 101x101(K=100) and in every pass we iteratively increase the vocabu-lary sizes by 50, repeating the training procedure until the chan-nel size becomes 351x351.C: 3894 9411 4357 8446 5433O: a diploma that?s good .D: a fence that?s good .C: 8593 7932 3627 9166 3671O: three families living here ?D: three brothers living here ?C: 6283 8827 7592 6959 5120 6137 9723 3671O: okay and what did they tell you ?D: okay and what did they tell you ?C: 9723 3601 5834 5838 3805 4887 7961 9723 3174 45189067 4488 9551 7538 7239 9166 3671O: you mean if we come to see you in the afternoon afterfive you?ll be here ?D: i mean if we come to see you in the afternoon after thirtyyou?ll be here ?...Figure 2: Comparison of the original (O) English plain-text with output from Bayesian word substitution deci-pherment (D) for a few samples cipher (C) sentencesfrom the Transtac corpus.or phrases, (4) a single word can translate into aphrase, and (5) insertion/deletion of words.Problem Formulation: We formulate the MT de-cipherment problem as?given a foreign text f (i.e.,foreign word sequences f1...fm) and a monolingualEnglish corpus, our goal is to decipher the foreigntext and produce an English translation.Probabilistic decipherment: Unlike parallel train-ing, here we have to estimate the translation modelP?
(f |e) parameters using only monolingual data.During decipherment training, our objective is to es-timate the model parameters ?
in order to maximizethe probability of the foreign corpus f .
From Equa-tion 4 we have:argmax?
?f?eP (e) ?
P?
(f |e)For P (e), we use a word n-gram LM trained onmonolingual English data.
We then estimate param-eters of the translation model P?
(f |e) during train-ing.
Next, we present two novel decipherment ap-proaches for MT training without parallel data.1.
EM Decipherment: We propose a new transla-tion model for MT decipherment which can beefficiently trained using the EM algorithm.2.
Bayesian Decipherment: We introduce a novelmethod for estimating IBM Model 3 parame-ters without parallel data, using Bayesian learn-ing.
Unlike EM, this method does not face any16memory issues and we use sampling to performefficient inference during training.3.1 EM DeciphermentFor the translation model P?
(f |e), we would liketo use a well-known statistical model such as IBMModel 3 and subsequently train it using the EMalgorithm.
But without parallel training data, EMtraining for IBM Model 3 becomes intractable dueto (1) scalability and efficiency issues because oflarge-sized fertility and distortion parameter tables,and (2) the resulting derivation lattices become toobig to be stored in memory.Instead, we propose a simpler generative story forMT without parallel data.
Our model accounts for(word) substitutions, insertions, deletions and localre-ordering during the translation process but doesnot incorporate fertilities or global re-ordering.
Wedescribe the generative process here:1.
Generate an English string e = e1...el, withprobability P (e).2.
Insert a NULL word at any position in the En-glish string, with uniform probability.3.
For each English word token ei (includingNULLs), choose a foreign word translation fi,with probability P?(fi|ei).
The foreign wordmay be NULL.4.
Swap any pair of adjacent foreign wordsfi?1, fi, with probability P?(swap).
We setthis value to 0.1.5.
Output the foreign string f = f1...fm, skippingover NULLs.We use the EM algorithm to estimate all the pa-rameters ?
in order to maximize likelihood of theforeign corpus.
Finally, we use the Viterbi algo-rithm to decode the foreign sentence f and pro-duce an English translation e that maximizes P (e) ?P?trained(f |e).Linguistic knowledge for decipherment: To helplimit translation model size and deal with data spar-sity problem, we use prior linguistic knowledge.
Weuse identity mappings for numeric values (for ex-ample, ?8?
maps to ?8?
), and we split nouns intomorpheme units prior to decipherment training (forexample, ?YEARS??
?YEAR?
?+S?
).Whole-segment Language Models: When usingword n-gram models of English for decipherment,we find that some of the foreign sentences aredecoded into sequences (such as ?THANK YOUTALKING ABOUT ??)
that are not good English.This stems from the fact that n-gram LMs have noglobal information about what constitutes a validEnglish segment.
To learn this information auto-matically, we build a P (e) model that only recog-nizes English whole-segments (entire sentences orexpressions) observed in the monolingual trainingdata.
We then use this model (in place of word n-gram LMs) for decipherment training and decoding.3.2 Bayesian MethodBrown et al (1993) provide an efficient algorithmfor training IBM Model 3 translation model whenparallel sentence pairs are available.
But we wishto perform IBM Model 3 training under non-parallelconditions, which is intractable using EM training.Instead, we take a Bayesian approach.Following Equation 5, we represent the transla-tion model as P?
(f, a|e) in terms of hidden align-ments a.
Recall the generative story for IBM Model3 translation which has the following formula:P?
(f, a|e) =l?i=0t?
(faj |ei) ?l?i=1n?
(?i|ei)?m?aj 6=0,j=1d?
(aj |i, l,m) ?l?i=0?i!?1?0!?(m?
?0?0)?p?01?
?
pm?2?00?
(8)The alignment a is represented as a vector; aj = iimplies that the foreign word fj is produced by theEnglish word ei during translation.Bayesian Formulation: Our goal is to learn theprobability tables t (translation parameters) n (fer-tility parameters), d (distortion parameters), and p(English NULL word probabilities) without paralleldata.
In order to apply Bayesian inference for de-cipherment, we model each of these tables using a17Chinese Restaurant Process (CRP) formulation.
Forexample, to model the translation probabilities, weuse the formula:t?
(fj |ei) =?
?
P0(fj |ei) + Chistory(ei, fj)?+ Chistory(ei)(9)where, P0 represents the base distribution (whichis set to uniform) and Chistory represents the countof events occurring in the history (cache).
Similarly,we use CRP formulations for the other probabilities(n, d and p).
We use sparse Dirichlet priors for allthese models (i.e., low values for ?)
and plug theseprobabilities into Equation 8 to get P?
(f, a|e).Sampling IBM Model 3: We use point-wise Gibbssampling to estimate the IBM Model 3 parameters.The sampler is seeded with an initial English sampletranslation and a corresponding alignment for everyforeign sentence.
We define several sampling oper-ators, which are applied in sequence one after theother to generate English samples for the entire for-eign corpus.
Some of the sampling operators are de-scribed below:?
TranslateWord(j): Sample a new English wordtranslation for foreign word fj , from all possi-bilities (including NULL).?
SwapSegment(i1, i2): Swap the alignmentlinks for English words ei1 and ei2 .?
JoinWords(i1, i2): Eliminate the English wordei1 and transfer its links to the word ei2 .During sampling, we apply each of these opera-tors to generate a new derivation e, a for the foreigntext f and compute its score as P (e) ?
P?
(f, a|e).These small-change operators are similar to theheuristic techniques used for greedy decoding byGerman et al (2001).
But unlike the greedy method,which can easily get stuck, our Bayesian approachguarantees that once the sampler converges we willbe sampling from the true posterior distribution.As with Bayesian decipherment for word sub-stitution, we compute the probability of each newderivation incrementally, which makes sampling ef-ficient.
We also apply blocked sampling on topof point-wise sampling?we treat all occurrencesof a particular foreign sentence as a single blockand sample a single derivation for the entire block.We also parallelize the sampling procedure (as de-scribed in Section 2.2).5Choosing the best translation: Once the samplingrun finishes, we select the final sample and extractthe corresponding English translations for every for-eign sentence.
This yields the final deciphermentoutput.3.3 MT Experiments and ResultsData: We work with the Spanish/English languagepair and use the following corpora in our MT exper-iments:?
Time corpus: We mined English newswiretext on the Web and collected 295k tempo-ral expressions such as ?LAST YEAR?, ?THEFOURTH QUARTER?, ?IN JAN 1968?, etc.We first process the data and normalize num-bers and names of months/weekdays?for ex-ample, ?1968?
is replaced with ?NNNN?,?JANUARY?
with ?
[MONTH]?, and so on.
Wethen translate the English temporal phrases intoSpanish using an automatic translation soft-ware (Google Translate) followed by manualannotation to correct mistakes made by thesoftware.
We create the following splits out ofthe resulting parallel corpus:TRAIN (English): 195k temporal expressions(7588 unique), 382k word tokens, 163 types.TEST (Spanish): 100k temporal expressions(2343 unique), 204k word tokens, 269 types.?
OPUS movie subtitle corpus: This is a largeopen source collection of parallel corpora avail-able for multiple language pairs (Tiedemann,2009).
We downloaded the parallel Span-ish/English subtitle corpus which consists ofaligned Spanish/English sentences from a col-lection of movie subtitles.
For our MT ex-periments, we select only Spanish/English sen-tences with frequency > 10 and create the fol-lowing train/test splits:5For Bayesian MT decipherment, we set a high prior valueon the language model (104) and use sparse priors for the IBM 3model parameters t, n, d, p (0.01, 0.01, 0.01, 0.01).
We use theoutput from EM decipherment as the initial sample and run thesampler for 2000 iterations, during which we apply annealingwith a linear schedule (2?
0.08).18Method Decipherment AccuracyTime expressions OPUS subtitles1a.
Parallel training (MOSES)with 2-gram LM 5.6 (85.6) 26.8 (63.6)with 5-gram LM 4.7 (88.0)1b.
Parallel training (IBM 3 without distortion)with 2-gram LM 10.1 (78.9) 29.9 (59.6)with whole-segment LM 9.0 (79.2)2a.
Decipherment (EM)with 2-gram LM 37.6 (44.6) 67.2 (15.3)with whole-segment LM 28.7 (48.7) 65.1 (19.3)2b.
Decipherment (Bayesian IBM 3)with 2-gram LM 34.0 (30.2) 66.6 (15.1)Figure 3: Comparison of Spanish/English MT performance on the Time and OPUS test corpora achieved by variousMT systems trained under (1) parallel?
(a) MOSES, (b) IBM 3 without distortion, and (2) decipherment settings?
(a) EM, (b) Bayesian.
The scores reported here are normalized edit distance values with BLEU scores shown inparentheses.TRAIN (English): 19770 sentences (1128unique), 62k word tokens, 411 word types.TEST (Spanish): 13181 sentences (1127unique), 39k word tokens, 562 word types.Both Spanish/English sides of TRAIN are used forparallel MT training, whereas decipherment usesonly monolingual English data for training LMs.MT Systems: We build and compare different MTsystems under two training scenarios:1.
Parallel training using: (a) MOSES, a phrasetranslation system (Koehn et al, 2007) widelyused in MT literature, and (b) a simpler versionof IBM Model 3 (without distortion param-eters) which can be trained tractably using thestrategy of Knight and Al-Onaizan (1998).2.
Decipherment without parallel data using:(a) EM method (from Section 3.1), and (b)Bayesian method (from Section 3.2).Evaluation: All the MT systems are run on theSpanish test data and the quality of the result-ing English translations are evaluated using twodifferent measures?
(1) Normalized edit distancescore (Navarro, 2001),6 and (2) BLEU (Papineni et6When computing edit distance, we account for substitu-tions, insertions, deletions as well as local-swap edit operationsrequired to convert a given English string into the (gold) refer-ence translation.al., 2002), a standard MT evaluation measure.Results: Figure 3 compares the results of vari-ous MT systems (using parallel versus deciphermenttraining) on the two test corpora in terms of edit dis-tance scores (a lower score indicates closer match tothe gold translation).
The figure also shows the cor-responding BLEU scores in parentheses for compar-ison (higher scores indicate better MT output).We observe that even without parallel trainingdata, our decipherment strategies achieve MT accu-racies comparable to parallel-trained systems.
Onthe Time corpus, the best decipherment (Method2a in the figure) achieves an edit distance score of28.7 (versus 4.7 for MOSES).
Better LMs yield bet-ter MT results for both parallel and deciphermenttraining?for example, using a segment-based En-glish LM instead of a 2-gram LM yields a 24% re-duction in edit distance and a 9% improvement inBLEU score for EM decipherment.We also investigate how the performance of dif-ferent MT systems vary with the size of the trainingdata.
Figure 4 plots the BLEU scores versus trainingsizes for different MT systems on the Time corpus.Clearly, using more training data yields better per-formance for all systems.
However, higher improve-ments are observed when using parallel data in com-parison to decipherment training which only usesmonolingual data.
We also notice that the scores donot improve much when going beyond 10,000 train-19Figure 4: Comparison of training data size versus MT ac-curacy in terms of BLEU score under different trainingconditions: (1) Parallel training?
(a) MOSES, (b) IBMModel 3 without distortion, and (2) Decipherment with-out parallel data using EM method (from Section 3.1).ing instances for this domain.It is interesting to quantify the value of parallelversus non-parallel data for any given MT task.
Inother words, ?how much non-parallel data is worthhow much parallel data in order to achieve the sameMT accuracy??
Figure 4 provides a reasonable an-swer to this question for the Spanish/English MTtask described here.
We see that deciphering with10k monolingual Spanish sentences yields the sameperformance as training with around 200-500 paral-lel English/Spanish sentence pairs.
This is the firstattempt at such a quantitative comparison for MTand our results are encouraging.
We envision thatfurther developments in unsupervised methods willhelp reduce this gap further.4 ConclusionOur work is the first attempt at doing MT with-out parallel data.
We discussed several novel deci-pherment approaches for achieving this goal.
Alongthe way, we developed efficient training methodsthat can deal with large-scale vocabularies and datasizes.
For future work, it will be interesting to see ifwe can exploit both parallel and non-parallel data toimprove on both.AcknowledgmentsThis material is based in part upon work supportedby the National Science Foundation (NSF) underGrant No.
IIS-0904684 and the Defense AdvancedResearch Projects Agency (DARPA) through theDepartment of Interior/National Business Center un-der Contract No.
NBCHD040058.
Any opinion,findings and conclusions or recommendations ex-pressed in this material are those of the author(s) anddo not necessarily reflect the views of the DefenseAdvanced Research Projects Agency (DARPA), orthe Department of the Interior/National BusinessCenter.ReferencesFriedrich L. Bauer.
2006.
Decrypted Secrets: Methodsand Maxims of Cryptology.
Springer-Verlag.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the Asian Federa-tion of Natural Language Processing (ACL-IJCNLP),pages 782?790.Peter Brown, Vincent Della Pietra, Stephen Della Pietra,and Robert Mercer.
1993.
The mathematics of statis-tical machine translation: Parameter estimation.
Com-putational linguistics, 19(2):263?311.David Chiang, Jonathan Graehl, Kevin Knight, AdamPauls, and Sujith Ravi.
2010.
Bayesian inference forfinite-state transducers.
In Proceedings of the Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics - Human LanguageTechnologies (NAACL/HLT), pages 447?455.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(1):1?38.Jenny Finkel, Trond Grenager, and Christopher Manning.2005.
Incorporating non-local information into infor-mation extraction systems by Gibbs sampling.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pages 363?370.Pascal Fung and Kathleen McKeown.
1997.
Finding ter-minology translations from non-parallel corpora.
InProceedings of the Fifth Annual Workshop on VeryLarge Corpora, pages 192?202.20Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6(6):721?741.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Proceed-ings of the 39th Annual Meeting on Association forComputational Linguistics, pages 228?235.Sharon Goldwater and Thomas Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 744?751.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexi-cons from monolingual corpora.
In Proceedings ofthe Annual Meeting of the Association for Compu-tational Linguistics - Human Language Technologies(ACL/HLT), pages 771?779.Kevin Knight and Yaser Al-Onaizan.
1998.
Transla-tion with finite-state devices.
In David Farwell, LaurieGerber, and Eduard Hovy, editors, Machine Transla-tion and the Information Soup, volume 1529 of LectureNotes in Computer Science, pages 421?437.
SpringerBerlin / Heidelberg.Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-mada.
2006.
Unsupervised analysis for deciphermentproblems.
In Proceedings of the Joint Conference ofthe International Committee on Computational Lin-guistics and the Association for Computational Lin-guistics, pages 499?506.Philipp Koehn and Kevin Knight.
2000.
Estimating wordtranslation probabilities from unrelated monolingualcorpora using the EM algorithm.
In Proceedings ofthe Seventeenth National Conference on Artificial In-telligence and Twelfth Conference on Innovative Ap-plications of Artificial Intelligence, pages 711?715.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions.Philip Koehn.
2009.
Statistical Machine Translation.Cambridge University Press.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the main conference on Human Language Tech-nology Conference of the North American Chapter ofthe Association of Computational Linguistics, pages152?159.Gonzalo Navarro.
2001.
A guided tour to approximatestring matching.
ACM Computing Surveys, 33:31?88,March.David Newman, Arthur Asuncion, Padhraic Smyth, andMax Welling.
2009.
Distributed algorithms fortopic models.
Journal of Machine Learning Research,10:1801?1828.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the Conference ofthe Association for Computational Linguistics, pages320?322.Benjamin Snyder, Regina Barzilay, and Kevin Knight.2010.
A statistical model for lost language decipher-ment.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1048?1057.Jo?rg Tiedemann.
2009.
News from OPUS - A collec-tion of multilingual parallel corpora with tools and in-terfaces.
In N. Nicolov, K. Bontcheva, G. Angelova,and R. Mitkov, editors, Recent Advances in NaturalLanguage Processing, volume V, pages 237?248.
JohnBenjamins, Amsterdam/Philadelphia.Warren Weaver.
1955.
Translation (1949).
Reproducedin W.N.
Locke, A.D. Booth (eds.).
In Machine Trans-lation of Languages, pages 15?23.
MIT Press.21
