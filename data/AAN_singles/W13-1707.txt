Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 58?62,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsApplying Unsupervised Learning To Support Vector Space Model BasedSpeaking AssessmentLei ChenEducational Testing Service600 Rosedale RdPrinceton, NJLChen@ets.orgAbstractVector Space Models (VSM) have beenwidely used in the language assessment fieldto provide measurements of students?
vocab-ulary choices and content relevancy.
How-ever, training reference vectors (RV) in a VSMrequires a time-consuming and costly humanscoring process.
To address this limitation, weapplied unsupervised learning methods to re-duce or even eliminate the human scoring steprequired for training RVs.
Our experimentsconducted on data from a non-native Englishspeaking test suggest that the unsupervisedtopic clustering is better at selecting responsesto train RVs than random selection.
In addi-tion, we conducted an experiment to totallyeliminate the need of human scoring.
Insteadof using human rated scores to train RVs, weused used the machine-predicted scores froman automated speaking assessment system fortraining RVs.
We obtained VSM-derived fea-tures that show promisingly high correlationsto human-holistic scores, indicating that thecostly human scoring process can be elimi-nated.Index Terms: Vector Space Model (VSM), speechassessment, unsupervised learning, document clus-tering1 IntroductionA Vector Space Model (VSM) is a simple, yet effec-tive, method to measure similarities between doc-uments or utterances, which has been utilized inthe educational testing field.
For example, VSMhas been applied to detect students?
off-topic es-says (Higgins et al 2006) and to automaticallyscore essays (Attali and Burstein, 2004).The following three steps are required to useVSM for automated assessment: (1) a collectionof responses are selected from each score categoryto construct reference vectors (RV); (2) for an in-put response under scoring, the same vectorizationmethod used for constructing RVs is applied to com-pute an input vector (IV); (3) similarities betweenthis IV and the RVs for all score categories are com-puted as features reflecting vocabulary usage andcontent relevancy, including a widely used feature,the cosine similarity between the IV and the RV forthe highest score category.Clearly, the quality of VSM-derived features de-pends on the proper training of RVs.
In languageassessment, we tend to use a large number of man-ually scored responses to build RVs for each testingquestion (called item in the assessment field).
How-ever, this raises an issue: the requirement of manualscoring of these responses by human raters.
Also,for large-scale assessments administrated globally,a high number of items are typically administeredto both ensure the assessment security and supportthe large volume of test-takers.
To address this chal-lenge of application of VSM, we will describe oursolutions based on applying unsupervised learningmethods in this paper.The rest of the paper is organized as follows: Sec-tion 2 reviews the related previous research; Sec-tion 3 describes the English assessment, the dataused in our experiments, and the Automatic SpeechRecognition (ASR) system used; Section 4 reports58the three experiments we conducted; and Section 5discusses our findings and plans for future research.2 Previous WorkAttali and Burstein (2004) used the VSM methodto measure non-native English writers?
vocabularychoices when scoring their essays by comparingthe words contained in an student?s response to thewords found in a sample of essays from each scorecategory.
One belief behind this methodology is thatgood essays will resemble each other in terms of theword choice.
In particular, two VSM-derived fea-tures were used, including the maximum cosine sim-ilarity and cosine similarity to the top score category.Higgins et al(2006) applied the VSM technology todetect students?
off-topic essays whereby the word-based IV from a student?s essay was compared to anRV built from a collection of on-topic essays.
Whenthe difference was larger than a pre-defined thresh-old, the essay was marked as off-topic.
Zechner andXi (2008) applied VSM as a content relevancy mea-surement to score non-native English speaking re-sponses.
Recently, Xie et al(2012) explored theVSM technology on automated speech scoring.
Us-ing a superior ASR to the one used in (Zechner andXi, 2008), they found that the VSM-derived featureshad moderately high correlations with human profi-ciency scores.Dimension reduction, a critical step in apply-ing VSM, removes the noises and minor details inword-based vectors and keeps a concise semanticstructure.
Latent Semantic Analysis (LSA) (Deer-wester et al 1990) and Latent Dirichlet Alloca-tion (LDA) (Blei et al 2003) are two widely useddimension-reduction methods.
Kakkonen et al(2005) systematically investigated the dimension re-duction methods used in the VSM methods for es-say grading.
Their experiments showed that LSAslightly out-performs LDA.Compared to supervised learning, unsupervisedlearning can skip the time-consuming and costlymanual labeling process and has been widely usedin many machine-learning tasks.
Both LSA andLDA have been utilized in unsupervised documentclustering (Hofmann, 2001) to automatically sep-arate a collection of documents into several setswithout any human intervention.
Co-training is atype of semi-supervised learning method (Blum andMitchell, 1998), consisting of two classifiers trainedfrom independent sets of features to predict the samelabels.
It uses automatically predicted labels fromone classifier to train the other classifier.3 DataThe data used in our experiments were collectedfrom the speaking section of Test Of English as aForeign Language (TOEFL R?
), an English speak-ing test used to evaluate students?
basic English-speaking skills for use in academic institutions thatuse English as their primary teaching language.
Ourdata contains the speech responses for a total of 24test items.
For each item, both the stimulus mate-rial and question were presented to test-takers fol-lowed by a short amount of preparation time.
Thetest-takers were then given up to 60 seconds to pro-vide their spoken responses.
These responses werescored by using carefully developed rating rubricsby a group of experienced human raters.
The scor-ing rubrics covered a comprehensive list of differ-ent aspects of speaking ability, such as pronuncia-tion, prosody, vocabulary, content organization, etc.A 4-point holistic scoring scale was used where thescore of 4 marks the most advanced English speak-ers in the TOEFL R?
test.
Table 1 summarizes the re-sponses across these 24 items, including mean, sd,and sample size (n) of the total number of responsesand the number of responses per each score level.Overall SC1 SC2 SC3 SC4mean 1969.63 81.88 701.96 963.46 222.33sd 12.92 30.02 62.36 67.24 37.79n 47271 1965 16847 23123 5336Table 1: Summary statistics of the number of total re-sponses and the number of responses per each score levelmeasured in mean, sd, and sample size n across 24 itemsThe transcriptions of these spoken responses wereobtained by running a state-of-the-art non-nativeASR system.
This ASR system uses a cross-wordtri-phone acoustic model (AM) and n-gram lan-guage models (LMs) that were trained on approx-imately 800 hours of spoken data and the corre-sponding transcriptions.
When being evaluated onan held-out data set transcribed by humans from thesame test, a 33.0% word error rate was obtained.594 ExperimentsThe three experiments described below shared thesame procedure: (1) for each item, available re-sponses were divided into two sets - a set for train-ing RVs and a set for evaluating the VSM-derivedfeatures; (2) RVs were trained by using different re-sponse selection methods investigated in this paper;(3) the trained RVs were used to compute the VSM-derived features; and (4) Pearson correlation coeffi-cients (rs) between the VSM-derived features andhuman-holistic scores were computed to measurethese features?
predictive abilities in speech scoring.This experimental procedure was conducted on all24 items and was repeated in 10 iterations by usingvaried training/evaluation-splitting plans and the av-erages of these results across the items and iterationsare reported.
Note that we removed some commonfunction words, such as a, the, etc., and some noisewords from ASR outputs, such as uh and um, whenapplying the VSM method and always used LSA di-mension reduction.
We used the Gensim (R?ehu?r?ekand Sojka, 2010) Python package to implement theVSM-related computations in this paper.
Also, inthis paper, we focused on one VSM-derived fea-ture cos4, the cosine distance between an IV to theRV representing the highest-score category (4) forTOEFL R?
test.4.1 Data size for training RVsIn previous studies, researchers typically used alarge number of responses to construct RVs.
For ex-ample, Zechner and Xi (2008) used 1, 000 responseswhile Xie et al(2012) increased the RV trainingdata to 2, 000 responses for each item.
We ask, isit possible to use fewer responses so that we wouldnot be forced to manually score so many responses?To answer this question, we have investigated the re-lationship between the size of the RV training dataand cos4?s predictive ability.For each item, we first randomly selected 1, 800responses as the RV training data and used the re-maining responses as the evaluation set.
We thengradually reduced the RV training set to 1, 000, 500,200, and even 50 responses and trained a series ofRVs.
On the evaluation set, using these trained RVs,we extracted cos4 VSM feature and calculated thercos4 for human-holistic scores.
Table 2 reports theaverage rcos4, which will de denoted as rcos4 there-after, for the different-sized RV training sets.
Table 2shows that rcos4 continuously increases with the in-crease of the dataset size for training RVs.
However,it is worth noting that using just 50 responses to trainRVs still provides a reasonably high rcos4 (0.383).Between the two sizeRV conditions: 200 vs. 1800,rcos4 did not show a statistically significant increasebased on a t-test (p = 0.314).sizeRV 50 200 500 1000 1800rcos4 0.383 0.428 0.435 0.439 0.440Table 2: rcos4, a measurement of VSM features?
scoringperformance, from different RV training data sizes4.2 Using document clustering for training RVsIn the experiment described in section 4.1, we foundthat using even a limited number of human-scoredresponses can provide useful VSM features with areasonably high r to human-holistic scores.
If wecan intelligently select such a small-sized dataset,we think that the VSM-derived features will showfurther improved predicting power.
Armed withthis idea, we proposed a solution to use unsuper-vised document clustering technology to find the re-sponses for training RVs.In particular, for each item, of the 1, 800 re-sponses used for training the RVs, we run an LDAdocument-clustering process to split all of responsesinto K clusters.
Then, for each cluster, we ran-domly selected M responses.
Therefore, we se-lected K ?M responses for human scoring and fortraining the RVs.
Note that K ?
M can be muchsmaller than the original dataset size (n = 1800).We believed that comprehensive coverage of all ofthe latent topics would produce a better VSM that,in turn, would provide more effective VSM-derivedfeatures for scoring.In our experiment, based upon a pilot study, wedecided to use K = 10 and M = 5 to controlthe total scoring demand to be 50 responses peritem.
Compared to the rcos4 value obtained fromrandomly selecting 50 responses for training RVs(0.383 in Table 2), the response selection based onthe document clustering improved the rcos4 to be0.411.
Furthermore, a t-test showed that such an in-crease in rcos4 is statistically significant (p < 0.05).604.3 Using machine predicted scores fortraining RVsMany of the previous automated speaking scoringsystems focused on the features measuring fluency,pronunciation, and prosody (Witt, 1999; Franco etal., 2010; Bernstein et al 2010; Chen et al 2009).The scores predicted by these systems show promis-ingly high correlations with human rated scores.
Inorder to eliminate the time-consuming and costlyhuman scoring step required by applications ofVSM, we considered using the scores automaticallyscored by algorithms (AS) instead of the scores ratedby humans (HS).In our experiment, we used a set of speech fea-tures following (Chen et al 2009) for automatedspeech scoring.
To estimate AS, a five-fold cross-validation was applied on the entire dataset.
Foreach fold, a linear regression model was trainedfrom 80% of responses by using their HS and wasused to predict regression results on the remaining20% of responses.
The continuous scores producedby the regression model were rounded to the fourdiscrete score levels (1 to 4) to serve as AS.
Betweenthe obtained AS and HS, a Pearson r 0.56 was ob-served.Using the predicted scores, we re-ran our VSMfeature experiment by using the 1, 800 responses totrain the RVs.
When the dataset sizes for training theRVs was at 1, 800, we found that the rcos4 was 0.410when using machine-predicted scores.
Although itwas lower than the rcos4 value obtained by usinghuman-rated scores (0.440), a feature with such cor-relational magnitude is still useful for building anautomatic scoring model.4.4 A summary of experimentsHS1800 HS50 HScluster50 AS1800rcos4 0.440 0.383 0.411 0.410Table 3: A summary of rcos4 using different RV trainingsizes, unsupervised-response clustering, and automated-predicted scoresTable 3 summarizes the three experiments de-scribed above.
HS1800 refers to using 1, 800 re-sponses with human scores (HS) to train RVs foreach item.
HS50 refers to using only 50 responseswith human rated scores.
HScluster50 refers to us-ing 50 responses that were selected to cover 10 la-tent topics detected by using an LDA unsupervisedtopic clustering method.
Compared to HS50, wefind that the unsupervised topic clustering methodhelped to improve rcos4.
AS1800 refers to using1, 800 responses with automatically predicted scores(AS) to train RVs for each item.
Compared toHS1800, AS1800 that avoids using a time-consumingand costly human scoring process, shows a reason-ably high rcos4.5 Conclusions and Future WorkVector Space Models (VSMs) have been widelyused in essay and speech assessment tasks to providevocabulary usage and content relevance measure-ments.
However, applying VSM on the assessmentswith many items requires a lot of work by humanraters.
To make the application of VSM in assess-ments more economical and efficient, we proposethe use of unsupervised learning methods to reduceand even eliminate the time-consuming and costlyhuman-scoring process.
First, we found that it waspossible to just use hundreds rather than thousandsof responses to train RVs when applying VSM.
Inour experiments with TOEFL R?
data, we found thatusing a minimum 200 responses to train RVs foreach item, was not statistically significantly differentfrom using 1, 800 responses.
Next, we used an LDAdocument-clustering method to identify latent top-ics from all of the items and used the topic informa-tion to select responses for training RVs.
Our exper-iments clearly suggest that such a method of selec-tion provides more effective VSM features than ran-dom selection.
Finally, we used the scores predictedby an automated speech scoring system that mostlyuses fluency and pronunciation features to replacehuman-rated scores in building the VSM.
Our exper-iments suggest that the features derived from such aVSM that can be constructed without the need of hu-man scoring show promisingly high correlations tohuman-holistic scores.This research can be extended in several new di-rections.
First, we will apply the proposed methodson other language assessment tasks, such as on long(written) essays, to fully test that the proposed meth-ods are universally helpful.
Second, we are consid-ering doing the third experiment in more iterations?
adding the VSM-derived features into the auto-61mated scoring model so that more accurate machine-predicted scores can be used for building further im-proved VSM.ReferencesY.
Attali and J. Burstein.
2004.
Automated essay scoringwith e-rater v.2.0.
In Presented at the Annual Meet-ing of the International Association for EducationalAssessment.J.
Bernstein, A.
Van Moere, and J. Cheng.
2010.
Val-idating automated speaking tests.
Language Testing,27(3):355.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
the Journal of ma-chine Learning research, 3:993?1022.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the eleventh annual conference on Computa-tional learning theory, page 92100.L.
Chen, K. Zechner, and X Xi.
2009.
Improved pro-nunciation features for construct-driven assessment ofnon-native spontaneous speech.
In NAACL-HLT.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American society for information science,41(6):391407.H.
Franco, H. Bratt, R. Rossier, V. Rao Gadde,E.
Shriberg, V. Abrash, and K. Precoda.
2010.
EduS-peak: a speech recognition and pronunciation scoringtoolkit for computer-aided language learning applica-tions.
Language Testing, 27(3):401.D.
Higgins, J. Burstein, and Y. Attali.
2006.
Identifyingoff-topic student essays without topic-specific trainingdata.
Natural Language Engineering, 12.Thomas Hofmann.
2001.
Unsupervised learning byprobabilistic latent semantic analysis.
Machine Learn-ing, 42(1):177?196.Tuomo Kakkonen, Niko Myller, Erkki Sutinen, and JariTimonen.
2005.
Comparison of dimension reductionmethods for automated essay grading.
Natural Lan-guage Engineering, 1:1?16.Radim R?ehu?r?ek and Petr Sojka.
2010.
Software Frame-work for Topic Modelling with Large Corpora.
In Pro-ceedings of the LREC 2010 Workshop on New Chal-lenges for NLP Frameworks, pages 45?50, Valletta,Malta, May.
ELRA.S.
M. Witt.
1999.
Use of Speech Recognition inComputer-assisted Language Learning.
Ph.D. thesis,University of Cambridge.S.
Xie, K. Evanini, and K. Zechner.
2012.
Exploringcontent features for automated speech scoring.
Pro-ceedings of the NAACL-HLT, Montreal, July.Klaus Zechner and Xiaoming Xi.
2008.
Towards auto-matic scoring of a test of spoken language with het-erogeneous task types.
In Proceedings of the ThirdWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, pages 98?106.
Association forComputational Linguistics.62
