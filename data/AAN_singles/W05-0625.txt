Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 181?184, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsGeneralized Inference with Multiple Semantic Role Labeling SystemsPeter Koomen Vasin Punyakanok Dan Roth Wen-tau YihDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{pkoomen2,punyakan,danr,yih}@uiuc.eduAbstractWe present an approach to semantic rolelabeling (SRL) that takes the output ofmultiple argument classifiers and com-bines them into a coherent predicate-argument output by solving an optimiza-tion problem.
The optimization stage,which is solved via integer linear pro-gramming, takes into account both the rec-ommendation of the classifiers and a setof problem specific constraints, and is thusused both to clean the classification resultsand to ensure structural integrity of the fi-nal role labeling.
We illustrate a signifi-cant improvement in overall SRL perfor-mance through this inference.1 SRL System ArchitectureOur SRL system consists of four stages: prun-ing, argument identification, argument classifica-tion, and inference.
In particular, the goal of pruningand argument identification is to identify argumentcandidates for a given verb predicate.
The systemonly classifies the argument candidates into theirtypes during the argument classification stage.
Lin-guistic and structural constraints are incorporatedin the inference stage to resolve inconsistent globalpredictions.
The inference stage can take as its inputthe output of the argument classification of a singlesystem or of multiple systems.
We explain the infer-ence for multiple systems in Sec.
2.1.1 PruningOnly the constituents in the parse tree are consideredas argument candidates.
In addition, our system ex-ploits the heuristic introduced by (Xue and Palmer,2004) to filter out very unlikely constituents.
Theheuristic is a recursive process starting from the verbwhose arguments are to be identified.
It first returnsthe siblings of the verb; then it moves to the parent ofthe verb, and collects the siblings again.
The processgoes on until it reaches the root.
In addition, if a con-stituent is a PP (propositional phrase), its childrenare also collected.
Candidates consisting of only asingle punctuation mark are not considered.This heuristic works well with the correct parsetrees.
However, one of the errors by automaticparsers is due to incorrect PP attachment leading tomissing arguments.
To attempt to fix this, we con-sider as arguments the combination of any consec-utive NP and PP, and the split of NP and PP insidethe NP that was chosen by the previous heuristics.1.2 Argument IdentificationThe argument identification stage utilizes binaryclassification to identify whether a candidate is anargument or not.
We train and apply the binary clas-sifiers on the constituents supplied by the pruningstage.
Most of the features used in our system arestandard features, which include?
Predicate and POS tag of predicate indicate the lemmaof the predicate and its POS tag.?
Voice indicates tbe voice of the predicate.?
Phrase type of the constituent.?
Head word and POS tag of the head word include headword and its POS tag of the constituent.
We use rulesintroduced by (Collins, 1999) to extract this feature.?
First and last words and POS tags of the constituent.?
Two POS tags before and after the constituent.?
Position feature describes if the constituent is before orafter the predicate relative to the position in the sentence.181?
Path records the traversal path in the parse tree from thepredicate to the constituent.?
Subcategorization feature describes the phrase structurearound the predicate?s parent.
It records the immediatestructure in the parse tree that expands to its parent.?
Verb class feature is the class of the active predicate de-scribed in PropBank Frames.?
Lengths of the target constituent, in the numbers of wordsand chunks separately.?
Chunk tells if the target argument is, embeds, overlaps,or is embedded in a chunk with its type.?
Chunk pattern length feature counts the number ofchunks from the predicate to the argument.?
Clause relative position is the position of the target wordrelative to the predicate in the pseudo-parse tree con-structed only from clause constituent.
There are fourconfigurations?target constituent and predicate share thesame parent, target constituent parent is an ancestor ofpredicate, predicate parent is an ancestor of target word,or otherwise.?
Clause coverage describes how much of the local clause(from the predicate) is covered by the argument.
It isround to the multiples of 1/4.1.3 Argument ClassificationThis stage assigns the final argument labels to the ar-gument candidates supplied from the previous stage.A multi-class classifier is trained to classify thetypes of the arguments supplied by the argumentidentification stage.
To reduce the excessive candi-dates mistakenly output by the previous stage, theclassifier can also classify the argument as NULL(?not an argument?)
to discard the argument.The features used here are the same as those usedin the argument identification stage with the follow-ing additional features.?
Syntactic frame describes the sequential pattern of thenoun phrases and the predicate in the sentence.
This isthe feature introduced by (Xue and Palmer, 2004).?
Propositional phrase head is the head of the first phraseafter the preposition inside PP.?
NEG and MOD feature indicate if the argument is abaseline for AM-NEG or AM-MOD.
The rules of theNEG and MOD features are used in a baseline SRL sys-tem developed by Erik Tjong Kim Sang (Carreras andMa`rquez, 2004).?
NE indicates if the target argument is, embeds, overlaps,or is embedded in a named-entity along with its type.1.4 InferenceThe purpose of this stage is to incorporate someprior linguistic and structural knowledge, such as?arguments do not overlap?
or ?each verb takes atmost one argument of each type.?
This knowledge isused to resolve any inconsistencies of argument clas-sification in order to generate final legitimate pre-dictions.
We use the inference process introducedby (Punyakanok et al, 2004).
The process is formu-lated as an integer linear programming (ILP) prob-lem that takes as inputs the confidences over eachtype of the arguments supplied by the argument clas-sifier.
The output is the optimal solution that maxi-mizes the linear sum of the confidence scores (e.g.,the conditional probabilities estimated by the argu-ment classifier), subject to the constraints that en-code the domain knowledge.Formally speaking, the argument classifier at-tempts to assign labels to a set of arguments, S1:M ,indexed from 1 to M .
Each argument Si can takeany label from a set of argument labels, P , and theindexed set of arguments can take a set of labels,c1:M ?
PM .
If we assume that the argument classi-fier returns an estimated conditional probability dis-tribution, Prob(Si = ci), then, given a sentence, theinference procedure seeks an global assignment thatmaximizes the following objective function,c?1:M = argmaxc1:M?PMM?i=1Prob(Si = ci),subject to linguistic and structural constraints.
Inother words, this objective function reflects the ex-pected number of correct argument predictions, sub-ject to the constraints.
The constraints are encodedas the followings.?
No overlapping or embedding arguments.?
No duplicate argument classes for A0-A5.?
Exactly one V argument per predicate considered.?
If there is C-V, then there has to be a V-A1-CV pattern.?
If there is an R-arg argument, then there has to be an argargument.?
If there is a C-arg argument, there must be an arg argu-ment; moreover, the C-arg argument must occur after arg.?
Given the predicate, some argument types are illegal (e.g.predicate ?stalk?
can take only A0 or A1).
The illegaltypes may consist of A0-A5 and their corresponding C-arg and R-arg arguments.
For each predicate, we lookfor the minimum value of i such that the class Ai is men-tioned in its frame file as well as its maximum value j.All argument types Ak such that k < i or k > j areconsidered illegal.1822 Inference with Multiple SRL SystemsThe inference process allows a natural way to com-bine the outputs from multiple argument classi-fiers.
Specifically, given k argument classifierswhich perform classification on k argument sets,{S1, .
.
.
, Sk}.
The inference process aims to opti-mize the objective function:c?1:N = argmaxc1:N?PNN?i=1Prob(Si = ci),where S1:N = ?ki=1 Si, andProb(Si = ci) = 1kk?j=1Probj(Si = ci),where Probj is the probability output by system j.Note that all systems may not output with thesame set of argument candidates due to the pruningand argument identification.
For the systems that donot output for any candidate, we assign the proba-bility with a prior to this phantom candidate.
In par-ticular, the probability of the NULL class is set to be0.6 based on empirical tests, and the probabilities ofthe other classes are set proportionally to their oc-currence frequencies in the training data.For example, Figure 1 shows the two candidatesets for a fragment of a sentence, ?..., traders say,unable to cool the selling panic in both stocks andfutures.?
In this example, system A has two argu-ment candidates, a1 = ?traders?
and a4 = ?the sell-ing panic in both stocks and futures?
; system B hasthree argument candidates, b1 = ?traders?, b2 = ?theselling panic?, and b3 = ?in both stocks and fu-tures?.
The phantom candidates are created for a2,a3, and b4 of which probability is set to the prior.Specifically for this implementation, we first traintwo SRL systems that use Collins?
parser and Char-niak?s parser respectively.
In fact, these two parsershave noticeably different output.
In evaluation, werun the system that was trained with Charniak?sparser 5 times with the top-5 parse trees output byCharniak?s parser1.
Together we have six differentoutputs per predicate.
Per each parse tree output, weran the first three stages, namely pruning, argument1The top parse tree were from the official output by CoNLL.The 2nd-5th parse trees were output by Charniak?s parser.cool1b1b4a4a22b 3ba3..., traders say, unable to the selling panic in both stocks and futures.aFigure 1: Two SRL systems?
output (a1, a4, b1, b2,and b3), and phantom candidates (a2, a3, and b4).identification, and argument classification.
Then ajoint inference stage is used to resolve the incon-sistency of the output of argument classification inthese systems.3 Learning and EvaluationThe learning algorithm used is a variation of theWinnow update rule incorporated in SNoW (Roth,1998; Roth and Yih, 2002), a multi-class classi-fier that is tailored for large scale learning tasks.SNoW learns a sparse network of linear functions,in which the targets (argument border predictionsor argument type predictions, in this case) are rep-resented as linear functions over a common featurespace.
It improves the basic Winnow multiplicativeupdate rule with a regularization term, which has theeffect of trying to separate the data with a large mar-gin separator (Grove and Roth, 2001; Hang et al,2002) and voted (averaged) weight vector (Freundand Schapire, 1999).Softmax function (Bishop, 1995) is used to con-vert raw activation to conditional probabilities.
Ifthere are n classes and the raw activation of class iis acti, the posterior estimation for class i isProb(i) = eacti?1?j?n eactj.In summary, training used both full and partialsyntactic information as described in Section 1.
Intraining, SNoW?s default parameters were used withthe exception of the separator thickness 1.5, the useof average weight vector, and 5 training cycles.
Theparameters are optimized on the development set.Training for each system took about 6 hours.
Theevaluation on both test sets which included running183Precision Recall F?=1Development 80.05% 74.83% 77.35Test WSJ 82.28% 76.78% 79.44Test Brown 73.38% 62.93% 67.75Test WSJ+Brown 81.18% 74.92% 77.92Test WSJ Precision Recall F?=1Overall 82.28% 76.78% 79.44A0 88.22% 87.88% 88.05A1 82.25% 77.69% 79.91A2 78.27% 60.36% 68.16A3 82.73% 52.60% 64.31A4 83.91% 71.57% 77.25A5 0.00% 0.00% 0.00AM-ADV 63.82% 56.13% 59.73AM-CAU 64.15% 46.58% 53.97AM-DIR 57.89% 38.82% 46.48AM-DIS 75.44% 80.62% 77.95AM-EXT 68.18% 46.88% 55.56AM-LOC 66.67% 55.10% 60.33AM-MNR 66.79% 53.20% 59.22AM-MOD 96.11% 98.73% 97.40AM-NEG 97.40% 97.83% 97.61AM-PNC 60.00% 36.52% 45.41AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 78.16% 76.72% 77.44R-A0 89.72% 85.71% 87.67R-A1 70.00% 76.28% 73.01R-A2 85.71% 37.50% 52.17R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 85.71% 57.14% 68.57R-AM-MNR 0.00% 0.00% 0.00R-AM-TMP 72.34% 65.38% 68.69V 98.92% 97.10% 98.00Table 1: Overall results (top) and detailed results onthe WSJ test (bottom).with all six different parse trees (assumed alreadygiven) and the joint inference took about 4.5 hours.Precision Recall F?=1Charniak-1 75.40% 74.13% 74.76Charniak-2 74.21% 73.06% 73.63Charniak-3 73.52% 72.31% 72.91Charniak-4 74.29% 72.92% 73.60Charniak-5 72.57% 71.40% 71.98Collins 73.89% 70.11% 71.95Joint inference 80.05% 74.83% 77.35Table 2: The results of individual systems and theresult with joint inference on the development set.Overall results on the development and test setsare shown in Table 1.
Table 2 shows the results ofindividual systems and the improvement gained bythe joint inference on the development set.4 ConclusionsWe present an implementation of SRL system whichcomposed of four stages?1) pruning, 2) argumentidentification, 3) argument classification, and 4) in-ference.
The inference provides a natural way totake the output of multiple argument classifiers andcombines them into a coherent predicate-argumentoutput.
Significant improvement in overall SRL per-formance through this inference is illustrated.AcknowledgmentsWe are grateful to Dash Optimization for the freeacademic use of Xpress-MP.
This research is sup-ported by ARDA?s AQUAINT Program, DOI?s Re-flex program, and an ONR MURI Award.ReferencesC.
Bishop, 1995.
Neural Networks for Pattern Recognition,chapter 6.4: Modelling conditional distributions, page 215.Oxford University Press.X.
Carreras and L. Ma`rquez.
2004.
Introduction to the conll-2004 shared tasks: Semantic role labeling.
In Proc.
ofCoNLL-2004.M.
Collins.
1999.
Head-driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, Computer Science Depart-ment, University of Pennsylvenia, Philadelphia.Y.
Freund and R. Schapire.
1999.
Large margin classifica-tion using the perceptron algorithm.
Machine Learning,37(3):277?296.A.
Grove and D. Roth.
2001.
Linear concepts and hidden vari-ables.
Machine Learning, 42(1/2):123?141.T.
Hang, F. Damerau, and D. Johnson.
2002.
Text chunkingbased on a generalization of winnow.
Journal of MachineLearning Research, 2:615?637.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.
Seman-tic role labeling via integer linear programming inference.
InProc.
of COLING-2004.D.
Roth and W. Yih.
2002.
Probabilistic reasoning for entity &relation recognition.
In Proc.
of COLING-2002, pages 835?841.D.
Roth.
1998.
Learning to resolve natural language ambigui-ties: A unified approach.
In Proc.
of AAAI, pages 806?813.N.
Xue and M. Palmer.
2004.
Calibrating features for semanticrole labeling.
In Proc.
of the EMNLP-2004, pages 88?94,Barcelona, Spain.184
