Proceedings of the 43rd Annual Meeting of the ACL, pages 371?378,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsUnsupervised Learning of Field Segmentation Modelsfor Information ExtractionTrond GrenagerComputer Science DepartmentStanford UniversityStanford, CA 94305grenager@cs.stanford.eduDan KleinComputer Science DivisionU.C.
BerkeleyBerkeley, CA 94709klein@cs.berkeley.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305manning@cs.stanford.eduAbstractThe applicability of many current information ex-traction techniques is severely limited by the needfor supervised training data.
We demonstrate thatfor certain field structured extraction tasks, suchas classified advertisements and bibliographic ci-tations, small amounts of prior knowledge can beused to learn effective models in a primarily unsu-pervised fashion.
Although hidden Markov models(HMMs) provide a suitable generative model forfield structured text, general unsupervised HMMlearning fails to learn useful structure in either ofour domains.
However, one can dramatically im-prove the quality of the learned structure by ex-ploiting simple prior knowledge of the desired so-lutions.
In both domains, we found that unsuper-vised methods can attain accuracies with 400 un-labeled examples comparable to those attained bysupervised methods on 50 labeled examples, andthat semi-supervised methods can make good useof small amounts of labeled data.1 IntroductionInformation extraction is potentially one of the mostuseful applications enabled by current natural lan-guage processing technology.
However, unlike gen-eral tools like parsers or taggers, which generalizereasonably beyond their training domains, extractionsystems must be entirely retrained for each appli-cation.
As an example, consider the task of turn-ing a set of diverse classified advertisements into aqueryable database; each type of ad would requiretailored training data for a supervised system.
Ap-proaches which required little or no training datawould therefore provide substantial resource savingsand extend the practicality of extraction systems.The term information extraction was introducedin the MUC evaluations for the task of finding shortpieces of relevant information within a broader textthat is mainly irrelevant, and returning it in a struc-tured form.
For such ?nugget extraction?
tasks, theuse of unsupervised learning methods is difficult andunlikely to be fully successful, in part because thenuggets of interest are determined only extrinsicallyby the needs of the user or task.
However, the terminformation extraction was in time generalized to arelated task that we distinguish as field segmenta-tion.
In this task, a document is regarded as a se-quence of pertinent fields, and the goal is to segmentthe document into fields, and to label the fields.
Forexample, bibliographic citations, such as the one inFigure 1(a), exhibit clear field structure, with fieldssuch as author, title, and date.
Classified advertise-ments, such as the one in Figure 1(b), also exhibitfield structure, if less rigidly: an ad consists of de-scriptions of attributes of an item or offer, and a setof ads for similar items share the same attributes.
Inthese cases, the fields present a salient, intrinsic formof linguistic structure, and it is reasonable to hopethat field segmentation models could be learned inan unsupervised fashion.In this paper, we investigate unsupervised learn-ing of field segmentation models in two domains:bibliographic citations and classified advertisementsfor apartment rentals.
General, unconstrained induc-tion of HMMs using the EM algorithm fails to detectuseful field structure in either domain.
However, wedemonstrate that small amounts of prior knowledgecan be used to greatly improve the learned model.
Inboth domains, we found that unsupervised methodscan attain accuracies with 400 unlabeled examplescomparable to those attained by supervised methodson 50 labeled examples, and that semi-supervisedmethods can make good use of small amounts of la-beled data.371(a) AUTHPearlAUTH,AUTHJ.DATE(DATE1988DATE)DATE.TTLProbabilisticTTLReasoningTTLinTTLIntelligentTTLSystemsTTL:TTLNetworksTTLofTTLPlausibleTTLInferenceTTL.PUBLMorganPUBLKaufmannPUBL.
(b) SIZESpaciousSIZE1SIZEBedroomSIZEaptSIZE.FEATnewlyFEATremodeledFEAT,FEATgatedFEAT,FEATnewFEATapplianceFEAT,FEATnewFEATcarpetFEAT,NBRHDnearNBRHDpublicNBRHDtransportionNBRHD,NBRHDcloseNBRHDtoNBRHD580NBRHDfreewayNBRHD,RENT$RENT500.00RENTDepositCONTACT(510)655-0106(c) RBNo,,PRPitVBDwasRBn?tNNPBlackNNPMonday..Figure 1: Examples of three domains for HMM learning: the bibliographic citation fields in (a) and classified advertisements forapartment rentals shown in (b) exhibit field structure.
Contrast these to part-of-speech tagging in (c) which does not.2 Hidden Markov ModelsHidden Markov models (HMMs) are commonlyused to represent a wide range of linguistic phe-nomena in text, including morphology, parts-of-speech (POS), named entity mentions, and eventopic changes in discourse.
An HMM consists ofa set of states S, a set of observations (in our casewords or tokens) W , a transition model specify-ing P(st|st?1), the probability of transitioning fromstate st?1 to state st, and an emission model specify-ing P(w|s) the probability of emitting word w whilein state s. For a good tutorial on general HMM tech-niques, see Rabiner (1989).For all of the unsupervised learning experimentswe fit an HMM with the same number of hiddenstates as gold labels to an unannotated training setusing EM.1 To compute hidden state expectationsefficiently, we use the Forward-Backward algorithmin the standard way.
Emission models are initializedto almost-uniform probability distributions, wherea small amount of noise is added to break initialsymmetry.
Transition model initialization varies byexperiment.
We run the EM algorithm to conver-gence.
Finally, we use the Viterbi algorithm withthe learned parameters to label the test data.All baselines and experiments use the same tok-enization, normalization, and smoothing techniques,which were not extensively investigated.
Tokeniza-tion was performed in the style of the Penn Tree-bank, and tokens were normalized in various ways:numbers, dates, phone numbers, URLs, and email1EM is a greedy hill-climbing algorithm designed for thispurpose, but it is not the only option; one could also use coordi-nate ascent methods or sampling methods.addresses were collapsed to dedicated tokens, andall remaining tokens were converted to lowercase.Unless otherwise noted, the emission models usesimple add-?
smoothing, where ?
was 0.001 for su-pervised techniques, and 0.2 for unsupervised tech-niques.3 Datasets and EvaluationThe bibliographic citations data is described inMcCallum et al (1999), and is distributed athttp://www.cs.umass.edu/~mccallum/.
It consists of500 hand-annotated citations, each taken from thereference section of a different computer science re-search paper.
The citations are annotated with 13fields, including author, title, date, journal, and soon.
The average citation has 35 tokens in 5.5 fields.We split this data, using its natural order, into a 300-document training set, a 100-document developmentset, and a 100-document test set.The classified advertisements data set isnovel, and consists of 8,767 classified ad-vertisements for apartment rentals in the SanFrancisco Bay Area downloaded in June 2004from the Craigslist website.
It is distributed athttp://www.stanford.edu/~grenager/.
302 of theads have been labeled with 12 fields, includingsize, rent, neighborhood, features, and so on.The average ad has 119 tokens in 8.7 fields.
Theannotated data is divided into a 102-documenttraining set, a 100-document development set,and a 100-document test set.
The remaining 8465documents form an unannotated training set.In both cases, all system development and param-eter tuning was performed on the development set,372sizerentfeaturesrestrictionsneighborhoodutilitiesavailablecontactphotosroomatesotheraddressauthortitleeditorjournalbooktitlevolumepagespublisherlocationtechinstitutiondateDTJJNNNNSNNPPRPCCMDVBDVBTOIN(a) (b) (c)Figure 2: Matrix representations of the target transition structure in two field structured domains: (a) classified advertisements(b) bibliographic citations.
Columns and rows are indexed by the same sequence of fields.
Also shown is (c) a submatrix of thetransition structure for a part-of-speech tagging task.
In all cases the column labels are the same as the row labels.and the test set was only used once, for running fi-nal experiments.
Supervised learning experimentstrain on documents selected randomly from the an-notated training set and test on the complete test set.Unsupervised learning experiments also test on thecomplete test set, but create a training set by firstadding documents from the test set (without anno-tation), then adding documents from the annotatedtraining set (without annotation), and finally addingdocuments from the unannotated training set.
Thusif an unsupervised training set is larger than the testset, it fully contains the test set.To evaluate our models, we first learn a set ofmodel parameters, and then use the parameterizedmodel to label the sequence of tokens in the test datawith the model?s hidden states.
We then comparethe similarity of the guessed sequence to the human-annotated sequence of gold labels, and compute ac-curacy on a per-token basis.2 In evaluation of su-pervised methods, the model states and gold labelsare the same.
For models learned in a fully unsuper-vised fashion, we map each model state in a greedyfashion to the gold label to which it most often cor-responds in the gold data.
There is a worry withthis kind of greedy mapping: it increasingly inflatesthe results as the number of hidden states grows.
Tokeep the accuracies meaningful, all of our modelshave exactly the same number of hidden states asgold labels, and so the comparison is valid.2This evaluation method is used by McCallum et al (1999)but otherwise is not very standard.
Compared to other evalu-ation methods for information extraction systems, it leads to alower penalty for boundary errors, and allows long fields alsocontribute more to accuracy than short ones.4 Unsupervised LearningConsider the general problem of learning an HMMfrom an unlabeled data set.
Even abstracting awayfrom concrete search methods and objective func-tions, the diversity and simultaneity of linguisticstructure is already worrying; in Figure 1 comparethe field structure in (a) and (b) to the parts-of-speech in (c).
If strong sequential correlations existat multiple scales, any fixed search procedure willdetect and model at most one of these levels of struc-ture, not necessarily the level desired at the moment.Worse, as experience with part-of-speech and gram-mar learning has shown, induction systems are quitecapable of producing some uninterpretable mix ofvarious levels and kinds of structure.Therefore, if one is to preferentially learn onekind of inherent structure over another, there mustbe some way of constraining the process.
We couldhope that field structure is the strongest effect inclassified ads, while parts-of-speech is the strongesteffect in newswire articles (or whatever we wouldtry to learn parts-of-speech from).
However, it ishard to imagine how one could bleach the localgrammatical correlations and long-distance topicalcorrelations from our classified ads; they are stillEnglish text with part-of-speech patterns.
One ap-proach is to vary the objective function so that thesearch prefers models which detect the structureswhich we have in mind.
This is the primary waysupervised methods work, with the loss function rel-ativized to training label patterns.
However, for un-supervised learning, the primary candidate for anobjective function is the data likelihood, and wedon?t have another suggestion here.
Another ap-proach is to inject some prior knowledge into the373search procedure by carefully choosing the startingpoint; indeed smart initialization has been criticalto success in many previous unsupervised learningexperiments.
The central idea of this paper is thatwe can instead restrict the entire search domain byconstraining the model class to reflect the desiredstructure in the data, thereby directing the search to-ward models of interest.
We do this in several ways,which are described in the following sections.4.1 BaselinesTo situate our results, we provide three differentbaselines (see Table 1).
First is the most-frequent-field accuracy, achieved by labeling all tokens withthe same single label which is then mapped to themost frequent field.
This gives an accuracy of 46.4%on the advertisements data and 27.9% on the cita-tions data.
The second baseline method is to pre-segment the unlabeled data using a crude heuristicbased on punctuation, and then to cluster the result-ing segments using a simple Na?
?ve Bayes mixturemodel with the Expectation-Maximization (EM) al-gorithm.
This approach achieves an accuracy of62.4% on the advertisements data, and 46.5% on thecitations data.As a final baseline, we trained a supervised first-order HMM from the annotated training data usingmaximum likelihood estimation.
With 100 trainingexamples, supervised models achieve an accuracy of74.4% on the advertisements data, and 72.5% on thecitations data.
With 300 examples, supervised meth-ods achieve accuracies of 80.4 on the citations data.The learning curves of the supervised training ex-periments for different amounts of training data areshown in Figure 4.
Note that other authors haveachieved much higher accuracy on the the citationdataset using HMMs trained with supervision: Mc-Callum et al (1999) report accuracies as high as92.9% by using more complex models and millionsof words of BibTeX training data.4.2 Unconstrained HMM LearningFrom the supervised baseline above we know thatthere is some first-order HMM over |S| states whichcaptures the field structure we?re interested in, andwe would like to find such a model without super-vision.
As a first attempt, we try fitting an uncon-strained HMM, where the transition function is ini-123456789101112(a) Classified Advertisements123456789101112(b) CitationsFigure 3: Matrix representations of typical transition modelslearned by initializing the transition model uniformly.tialized randomly, to the unannotated training data.Not surprisingly, the unconstrained approach leadsto predictions which poorly align with the desiredfield segmentation: with 400 unannotated trainingdocuments, the accuracy is just 48.8% for the ad-vertisements and 49.7% for the citations: better thanthe single state baseline but far from the supervisedbaseline.
To illustrate what is (and isn?t) beinglearned, compare typical transition models learnedby this method, shown in Figure 3, to the maximum-likelihood transition models for the target annota-tions, shown in Figure 2.
Clearly, they aren?t any-thing like the target models: the learned classifiedadvertisements matrix has some but not all of thedesired diagonal structure, and the learned citationsmatrix has almost no mass on the diagonal, and ap-pears to be modeling smaller scale structure.4.3 Diagonal Transition ModelsTo adjust our procedure to learn larger-scale pat-terns, we can constrain the parametric form of thetransition model to beP(st|st?1) =????
+ (1??
)|S| if st = st?1(1??
)|S| otherwisewhere |S| is the number of states, and ?
is a globalfree parameter specifying the self-loop probability:374(a) Classified advertisements(b) Bibliographic citationsFigure 4: Learning curves for supervised learning and unsuper-vised learning with a diagonal transition matrix on (a) classifiedadvertisements, and (b) bibliographic citations.
Results are av-eraged over 50 runs.the probability of a state transitioning to itself.
(Notethat the expected mean field length for transitionfunctions of this form is 11??
.)
This constraint pro-vides a notable performance improvement: with 400unannotated training documents the accuracy jumpsfrom 48.8% to 70.0% for advertisements and from49.7% to 66.3% for citations.
The complete learningcurves for models of this form are shown in Figure 4.We have tested training on more unannotated data;the slope of the learning curve is leveling out, butby training on 8000 unannotated ads, accuracy im-proves significantly to 72.4%.
On the citations task,an accuracy of approximately 66% can be achievedeither using supervised training on 50 annotated ci-tations, or unsupervised training using 400 unanno-tated citations.
3Although ?
can easily be reestimated with EM(even on a per-field basis), doing so does not yield3We also tested training on 5000 additional unannotated ci-tations collected from papers found on the Internet.
Unfortu-nately the addition of this data didn?t help accuracy.
This prob-ably results from the fact that the datasets were collected fromdifferent sources, at different times.Figure 5: Unsupervised accuracy as a function of the expectedmean field length 11??
for the classified advertisements dataset.Each model was trained with 500 documents and tested on thedevelopment set.
Results are averaged over 50 runs.better models.4 On the other hand, model accuracyis not very sensitive to the exact choice of ?, asshown in Figure 5 for the classified advertisementstask (the result for the citations task has a similarshape).
For the remaining experiments on the adver-tisements data, we use ?
= 0.9, and for those on thecitations data, we use ?
= 0.5.4.4 Hierarchical Mixture Emission ModelsConsider the highest-probability state emissionslearned by the diagonal model, shown in Figure 6(a).In addition to its characteristic content words, eachstate also emits punctuation and English functionwords devoid of content.
In fact, state 3 seems tohave specialized entirely in generating such tokens.This can become a problem when labeling decisionsare made on the basis of the function words ratherthan the content words.
It seems possible, then,that removing function words from the field-specificemission models could yield an improvement in la-beling accuracy.One way to incorporate this knowledge into themodel is to delete stopwords, which, while perhapsnot elegant, has proven quite effective in the past.A better founded way of making certain words un-available to the model is to emit those words fromall states with equal probability.
This can be accom-plished with the following simple hierarchical mix-ture emission modelPh(w|s) = ?Pc(w) + (1 ?
?
)P(w|s)where Pc is the common word distribution, and ?
is4While it may be surprising that disallowing reestimation ofthe transition function is helpful here, the same has been ob-served in acoustic modeling (Rabiner and Juang, 1993).375State 10 Most Common Words1 .
$ no !
month deposit , pets rent avail-able2 , .
room and with in large living kitchen-3 .
a the is and for this to , in4 [NUM1] [NUM0] , bedroom bath / - .car garage5 , .
and a in - quiet with unit building6 - .
[TIME] [PHONE] [DAY] call[NUM8] at(a)State 10 Most Common Words1 [NUM2] bedroom [NUM1] bath bed-rooms large sq car ft garage2 $ no month deposit pets lease rent avail-able year security3 kitchen room new , with living largefloors hardwood fireplace4 [PHONE] call please at or for [TIME] to[DAY] contact5 san street at ave st # [NUM:DDD] fran-cisco ca [NUM:DDDD]6 of the yard with unit private back abuilding floorComm.
*CR* .
, and - the in a / is with : of forto(b)Figure 6: Selected state emissions from a typical model learnedfrom unsupervised data using the constrained transition func-tion: (a) with a flat emission model, and (b) with a hierarchicalemission model.a new global free parameter.
In such a model, beforea state emits a token it flips a coin, and with probabil-ity ?
it allows the common word distribution to gen-erate the token, and with probability (1??)
it gener-ates the token from its state-specific emission model(see Vaithyanathan and Dom (2000) and Toutanovaet al (2001) for more on such models).
We tuned?
on the development set and found that a range ofvalues work equally well.
We used a value of 0.5 inthe following experiments.We ran two experiments on the advertisementsdata, both using the fixed transition model describedin Section 4.3 and the hierarchical emission model.First, we initialized the emission model of Pc to ageneral-purpose list of stopwords, and did not rees-timate it.
This improved the average accuracy from70.0% to 70.9%.
Second, we learned the emissionmodel of Pc using EM reestimation.
Although thismethod did not yield a significant improvement inaccuracy, it learns sensible common words: Fig-ure 6(b) shows a typical emission model learnedwith this technique.
Unfortunately, this techniquedoes not yield improvements on the citations data.4.5 Boundary ModelsAnother source of error concerns field boundaries.In many cases, fields are more or less correct, but theboundaries are off by a few tokens, even when punc-tuation or syntax make it clear to a human readerwhere the exact boundary should be.
One way to ad-dress this is to model the fact that in this data fieldsoften end with one of a small set of boundary tokens,such as punctuation and new lines, which are sharedacross states.To accomplish this, we enriched the Markov pro-cess so that each field s is now modeled by twostates, a non-final s?
?
S?
and a final s+ ?
S+.The transition model for final states is the same asbefore, but the transition model for non-final stateshas two new global free parameters: ?, the probabil-ity of staying within the field, and ?, the probabilityof transitioning to the final state given that we arestaying in the field.
The transition function for non-final states is thenP(s?|s?)
=???????????
(1 ?
?)(?
+ (1??
)|S?| ) if s?
= s??
(?+ (1??
)|S?| ) if s?
= s+(1??
)|S?| if s?
?
S?\s?0 otherwise.Note that it can bypass the final state, and transi-tion directly to other non-final states with probabil-ity (1 ?
?
), which models the fact that not all fieldoccurrences end with a boundary token.
The transi-tion function for non-final states is thenP(s?|s+) =????????
+ (1??
)|S?| if s?
= s?(1??
)|S?| if s?
?
S?\s?0 otherwise.Note that this has the form of the standard diagonalfunction.
The reason for the self-loop from the fi-nal state back to the non-final state is to allow forfield internal punctuation.
We tuned the free param-eters on the development set, and found that ?
= 0.5and ?
= 0.995 work well for the advertisements do-main, and ?
= 0.3 and ?
= 0.9 work well for thecitations domain.
In all cases it works well to set?
= 1 ?
?.
Emissions from non-final states are as376Ads CitationsBaseline 46.4 27.9Segment and cluster 62.4 46.5Supervised 74.4 72.5Unsup.
(learned trans) 48.8 49.7Unsup.
(diagonal trans) 70.0 66.3+ Hierarchical (learned) 70.1 39.1+ Hierarchical (given) 70.9 62.1+ Boundary (learned) 70.4 64.3+ Boundary (given) 71.9 68.2+ Hier.
+ Bnd.
(learned) 71.0 ?+ Hier.
+ Bnd.
(given) 72.7 ?Table 1: Summary of results.
For each experiment, we reportpercentage accuracy on the test set.
Supervised experimentsuse 100 training documents, and unsupervised experiments use400 training documents.
Because unsupervised techniques arestochastic, those results are averaged over 50 runs, and differ-ences greater than 1.0% are significant at p=0.05% or better ac-cording to the t-test.
The last 6 rows are not cumulative.before (hierarchical or not depending on the experi-ment), while all final states share a boundary emis-sion model.
Note that the boundary emissions arenot smoothed like the field emissions.We tested both supplying the boundary token dis-tributions and learning them with reestimation dur-ing EM.
In experiments on the advertisements datawe found that learning the boundary emission modelgives an insignificant raise from 70.0% to 70.4%,while specifying the list of allowed boundary tokensgives a significant increase to 71.9%.
When com-bined with the given hierarchical emission modelfrom the previous section, accuracy rises to 72.7%,our best unsupervised result on the advertisementsdata with 400 training examples.
In experiments onthe citations data we found that learning boundaryemission model hurts accuracy, but that given the setof boundary tokens it boosts accuracy significantly:increasing it from 66.3% to 68.2%.5 Semi-supervised LearningSo far, we have largely focused on incorporatingprior knowledge in rather general and implicit ways.As a final experiment we tested the effect of addinga small amount of supervision: augmenting the largeamount of unannotated data we use for unsuper-vised learning with a small amount of annotateddata.
There are many possible techniques for semi-supervised learning; we tested a particularly simpleone.
We treat the annotated labels as observed vari-ables, and when computing sufficient statistics in theM-step of EM we add the observed counts from theFigure 7: Learning curves for semi-supervised learning on thecitations task.
A separate curve is drawn for each number ofannotated documents.
All results are averaged over 50 runs.annotated documents to the expected counts com-puted in the E-step.
We estimate the transitionfunction using maximum likelihood from the an-notated documents only, and do not reestimate it.Semi-supervised results for the citations domain areshown in Figure 7.
Adding 5 annotated citationsyields no improvement in performance, but adding20 annotated citations to 300 unannotated citationsboosts performance greatly from 65.2% to 71.3%.We also tested the utility of this approach in the clas-sified advertisement domain, and found that it didnot improve accuracy.
We believe that this is be-cause the transition information provided by the su-pervised data is very useful for the citations data,which has regular transition structure, but is not asuseful for the advertisements data, which does not.6 Previous WorkA good amount of prior research can be cast assupervised learning of field segmentation models,using various model families and applied to var-ious domains.
McCallum et al (1999) were thefirst to compare a number of supervised methodsfor learning HMMs for parsing bibliographic cita-tions.
The authors explicitly claim that the domainwould be suitable for unsupervised learning, butthey do not present experimental results.
McCallumet al (2000) applied supervised learning of Maxi-mum Entropy Markov Models (MEMMs) to the do-main of parsing Frequently Asked Question (FAQ)lists into their component field structure.
More re-cently, Peng and McCallum (2004) applied super-vised learning of Conditional Random Field (CRF)sequence models to the problem of parsing the head-377ers of research papers.There has also been some previous work on un-supervised learning of field segmentation models inparticular domains.
Pasula et al (2002) performslimited unsupervised segmentation of bibliographiccitations as a small part of a larger probabilisticmodel of identity uncertainty.
However, their sys-tem does not explicitly learn a field segmentationmodel for the citations, and encodes a large amountof hand-supplied information about name forms, ab-breviation schemes, and so on.
More recently, Barzi-lay and Lee (2004) defined content models, whichcan be viewed as field segmentation models occur-ring at the level of discourse.
They perform un-supervised learning of these models from sets ofnews articles which describe similar events.
Thefields in that case are the topics discussed in thosearticles.
They consider a very different set of ap-plications from the present work, and show thatthe learned topic models improve performance ontwo discourse-related tasks: information orderingand extractive document summarization.
Most im-portantly, their learning method differs significantlyfrom ours; they use a complex and special purposealgorithm, which is difficult to adapt, while we seeour contribution to be a demonstration of the inter-play between model family and learned structure.Because the structure of the HMMs they learn issimilar to ours it seems that their system could ben-efit from the techniques of this paper.
Finally, Bleiand Moreno (2001) use an HMM augmented by anaspect model to automatically segment documents,similar in goal to the system of Hearst (1997), butusing techniques more similar to the present work.7 ConclusionsIn this work, we have examined the task of learn-ing field segmentation models using unsupervisedlearning.
In two different domains, classified ad-vertisements and bibliographic citations, we showedthat by constraining the model class we were ableto restrict the search space of EM to models of in-terest.
We used unsupervised learning methods with400 documents to yield field segmentation modelsof a similar quality to those learned using supervisedlearning with 50 documents.
We demonstrated thatfurther refinements of the model structure, includinghierarchical mixture emission models and boundarymodels, produce additional increases in accuracy.Finally, we also showed that semi-supervised meth-ods with a modest amount of labeled data can some-times be effectively used to get similar good results,depending on the nature of the problem.While there are enough resources for the citationtask that much better numbers than ours can be andhave been obtained (with more knowledge and re-source intensive methods), in domains like classi-fied ads for lost pets or used bicycles unsupervisedlearning may be the only practical option.
In thesecases, we find it heartening that the present systemsdo as well as they do, even without field-specificprior knowledge.8 AcknowledgementsWe would like to thank the reviewers for their con-sideration and insightful comments.ReferencesR.
Barzilay and L. Lee.
2004.
Catching the drift: Probabilisticcontent models, with applications to generation and summa-rization.
In Proceedings of HLT-NAACL 2004, pages 113?120.D.
Blei and P. Moreno.
2001.
Topic segmentation with an aspecthidden Markov model.
In Proceedings of the 24th SIGIR,pages 343?348.M.
A. Hearst.
1997.
TextTiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics,23(1):33?64.A.
McCallum, K. Nigam, J. Rennie, and K. Seymore.
1999.A machine learning approach to building domain-specificsearch engines.
In IJCAI-1999.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maximumentropy Markov models for information extraction and seg-mentation.
In Proceedings of the 17th ICML, pages 591?598.Morgan Kaufmann, San Francisco, CA.H.
Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser.
2002.Identity uncertainty and citation matching.
In Proceedings ofNIPS 2002.F.
Peng and A. McCallum.
2004.
Accurate information extrac-tion from research papers using Conditional Random Fields.In Proceedings of HLT-NAACL 2004.L.
R. Rabiner and B.-H. Juang.
1993.
Fundamentals of SpeechRecognition.
Prentice Hall.L.
R. Rabiner.
1989.
A tutorial on Hidden Markov Models andselected applications in speech recognition.
Proceedings ofthe IEEE, 77(2):257?286.K.
Toutanova, F. Chen, K. Popat, and T. Hofmann.
2001.
Textclassification in a hierarchical mixture model for small train-ing sets.
In CIKM ?01: Proceedings of the tenth interna-tional conference on Information and knowledge manage-ment, pages 105?113.
ACM Press.S.
Vaithyanathan and B. Dom.
2000.
Model-based hierarchicalclustering.
In UAI-2000.378
