Coupling CCG and Hybrid Logic Dependency SemanticsJason BaldridgeICCSDivision of Informatics2 Buccleuch PlaceUniversity of EdinburghEdinburgh, UK, EH8 9LWjmb@cogsci.ed.ac.ukGeert-Jan M. KruijffUniversita?t des SaarlandesComputational LinguisticsLehrstuhl UszkoreitBuilding 17, Postfach 15 11 5066041 Saarbru?cken, Germanygj@CoLi.Uni-SB.DEAbstractCategorial grammar has traditionally usedthe ?-calculus to represent meaning.
Wepresent an alternative, dependency-basedperspective on linguistic meaning and sit-uate it in the computational setting.
Thisperspective is formalized in terms of hy-brid logic and has a rich yet perspicuouspropositional ontology that enables a widevariety of semantic phenomena to be rep-resented in a single meaning formalism.Finally, we show how we can couple thisformalization to Combinatory CategorialGrammar to produce interpretations com-positionally.1 IntroductionThe ?-calculus has enjoyed many years as the stan-dard semantic encoding for categorial grammars andother grammatical frameworks, but recent work hashighlighted its inadequacies for both linguistic andcomputational concerns of representing natural lan-guage semantics (Copestake et al, 1999; Kruijff,2001).
The latter couples a resource-sensitive cate-gorial proof theory (Moortgat, 1997) to hybrid logic(Blackburn, 2000) to formalize a dependency-basedperspective on meaning, which we call here HybridLogic Dependency Semantics (HLDS).
In this pa-per, we situate HLDS in the computational contextby explicating its properties as a framework for com-putational semantics and linking it to CombinatoryCategorial Grammar (CCG).The structure of the paper is as follows.
In x2,we briefly introduce CCG and how it links syntaxand semantics, and then discuss semantic represen-tations that use indexes to identify subparts of logi-cal forms.
x3 introduces HLDS and evaluates it withrespect to the criteria of other computational seman-tics frameworks.
x4 shows how we can build HLDSterms using CCG with unification and x5 shows howintonation and information structure can be incorpo-rated into the approach.2 Indexed semantic representationsTraditionally, categorial grammar has capturedmeaning using a (simply typed) ?-calculus, build-ing semantic structure in parallel to the categorial in-ference (Morrill, 1994; Moortgat, 1997; Steedman,2000b).
For example, a (simplified) CCG lexical en-try for a verb such as wrote is given in (1).
(1) wrote ` (snn)=n : ?x:?y:write(y;x)Rules of combination are defined to operate on bothcategories and ?-terms simultaneously.
For exam-ple, the rules allow the following derivation for Edwrote books.
(2) Ed wrote booksn:Ed (snn)=n:?x:?y:write(y;x) n:books>snn : ?y:write(y;books)<s : write(Ed;books)Derivations like (2) give rise to the usual sortof predicate-argument structure whereby the orderin which the arguments appear (and are bound bythe ?
?s) is essentially constitutive of their meaning.Thus, the first argument could be taken to corre-spond to the writer, whereas the second argumentcorresponds to what is being written.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
319-326.Proceedings of the 40th Annual Meeting of the Association forOne deficiency of ?-calculus meaning representa-tions is that they usually have to be type-raised tothe worst case to fully model quantification, and thiscan reverberate and increase the complexity of syn-tactic categories since a verb like wrote will need tobe able to take arguments with the types of general-ized quantifiers.
The approach we advocate in thispaper does not suffer from this problem.For CCG, the use of the ?-terms is simply a con-venient device to bind arguments when presentingderivations (Steedman, 2000b).
In implementations,a more common strategy is to compute semantic rep-resentations via unification, a tactic explicitly em-ployed in Unification Categorial Grammar (UCG)(Zeevat, 1988).
Using a unification paradigm inwhich atomic categories are bundles of syntactic andsemantic information, we can use an entry such as(3) for wrote in place of (1).
In the unification set-ting, (3) permits a derivation analogous to (2).
(3) wrote ` (s : write(y;x)nn:y)=n:xFor creating predicate-argument structures of thiskind, strategies using either ?-terms or unificationto bind arguments are essentially notational vari-ants.
However, UCG goes beyond simple predicate-argument structures to instead use a semantic repre-sentation language called Indexed Language (InL).The idea of using indexes stems from Davidson(event variables), and are a commonly used mech-anism in unification-based frameworks and theoriesfor discourse representation.
InL attaches one to ev-ery formula representing its discourse referent.
Thisresults in a representation such as (4) for the sen-tence Ed came to the party.
(4) [e][party(x);past(e); to(e;x);come(e;Ed)]InL thus flattens logical forms to some extent, usingthe indexes to spread a given entity or event throughmultiple predications.
The use of indexes is crucialfor UCG?s account of modifiers, and as we will seelater, we exploit such referents to achieve similarends when coupling HLDS and CCG.Minimal Recursion Semantics (MRS) (Copestakeet al, 1999; Copestake et al, 2001) is a frame-work for computational semantics that is designedto simplify the work of algorithms which produceor use semantic representations.
MRS provides themeans to represent interpretations with a flat, un-derspecified semantics using terms of the predicatecalculus and generalized quantifiers.
Flattening isachieved by using an indexation scheme involvinglabels that tag particular groups of elementary pred-ications (EPs) and handles (here, h1;h2; :::) that ref-erence those EPs.
Underspecification is achievedby using unresolved handles as the arguments forscope-bearing elements and declaring constraints(with the =q operator) on how those handles can beresolved.
Different scopes can be reconstructed byequating unresolved handles with the labels of theother EPs obeying the =q constraints.
For example,(5) would be given as the representation for everydog chases some white cat.
(5) hh0;fh1:every(x;h2;h3);h4:dog(x);h11:cat(y);h8:some(y;h9;h10);h11:white(y);h7:chase(x;y)g;fh0=qh7; h2=qh4; h9=qh11giCopestake et al argue that these flat representa-tions facilitate a number of computational tasks, in-cluding machine translation and generation, withoutsacrificing linguistic expressivity.
Also, flatness per-mits semantic equivalences to be checked more eas-ily than in structures with deeper embedding, andunderspecification simplifies the work of the parsersince it does not have to compute every possiblereading for scope-bearing elements.3 Hybrid Logic Dependency SemanticsKruijff (2001) proposes an alternative way to rep-resenting linguistically realized meaning: namely,as terms of hybrid modal logic (Blackburn, 2000)explicitly encoding the dependency relations be-tween heads and dependents, spatio-temporal struc-ture, contextual reference, and information struc-ture.
We call this unified perspective combiningmany levels of meaning Hybrid Logic DependencySemantics (HLDS).
We begin by discussing how hy-brid logic extends modal logic, then look at the rep-resentation of linguistic meaning via hybrid logicterms.3.1 Hybrid LogicThough modal logic provides a powerful tool forencoding relational structures and their properties,it contains a surprising inherent asymmetry: states(?worlds?)
are at the heart of the model theory formodal logic, but there are no means to directlyreference specific states using the object language.This inability to state where exactly a propositionholds makes modal logic an inadequate representa-tion framework for practical applications like knowl-edge representation (Areces, 2000) or temporal rea-soning (Blackburn, 1994).
Because of this, compu-tational work in knowledge representation has usu-ally involved re-engineering first-order logic to suitthe task, e.g., the use of metapredicates such as Holdof Kowalski and Allen.
Unfortunately, such logicsare often undecidable.Hybrid logic extends standard modal logic whileretaining decidability and favorable complexity(Areces, 2000) (cf.
(Areces et al, 1999) for a com-plexity roadmap).
The strategy is to add nominals,a new sort of basic formula with which we can ex-plicitly name states in the object language.
Next topropositions, nominals are first-class citizens of theobject language: formulas can be formed using bothsorts, standard boolean operators, and the satisfac-tion operator ?@?.
A formula @i p states that theformula p holds at the state named by i.1 (Thereare more powerful quantifiers ranging over nomi-nals, such as #, but we do not consider them here.
)With nominals we obtain the possibility to explic-itly refer to the state at which a proposition holds.
AsBlackburn (1994) argues, this is essential for cap-turing our intuitions about temporal reference.
Astandard modal temporal logic with the modalitiesF and P (future and past, respectively) cannot cor-rectly represent an utterance such as Ed finished thebook because it is unable to refer to the specific timeat which the event occurred.
The addition of nomi-nals makes this possible, as shown in (6), where thenominal i represents the Reichenbachian event time.
(6) hPi(i^Ed-finish-book)Furthermore, many temporal properties can be de-fined in terms of pure formulas which use nominalsand contain no propositional variables.
For example,the following term defines the fact that the relationsfor F and P are mutually converse:1A few notes on our conventions: p;q;r are variables overany hybrid logic formula; i; j;k are variables over nominals; diand hi denote nominals (for dependent and head, respectively).
(7) @i[F]hPii ^ @i[P]hFiiIt is also possible to encode a variety of other rep-resentations in terms of hybrid logics.
For example,nominals correspond to tags in attribute-value matri-ces (AVMs), so the hybrid logic formula in (8) cor-responds to the AVM in (9).
(8) hSUBJi(i^hAGRisingular ^hPREDidog)^ hCOMPihSUBJii(9) 26664SUBJ 1"AGR singularPRED dog#COMPhSUBJ 1i37775A crucial aspect of hybrid logic is that nominalsare at the heart of a sorting strategy.
Different sortsof nominals can be introduced to build up a richsortal ontology without losing the perspicuity of apropositional setting.
Additionally, we can reasonabout sorts because nominals are part and parcel ofthe object language.
We can extend the language ofhybrid logic with fSort:Nominalg to facilitate the ex-plicit statement of what sort a nominal is in the lan-guage and carry this modification into one of the ex-isting tableaux methods for hybrid logic to reason ef-fectively with this information.
This makes it possi-ble to capture the rich ontologies of lexical databaseslike WordNet in a clear and concise fashion whichwould be onerous to represent in first-order logic.3.2 Encoding linguistic meaningHybrid logic enables us to logically capture two es-sential aspects of meaning in a clean and compactway, namely ontological richness and the possibilityto refer.
Logically, we can represent an expression?slinguistically realized meaning as a conjunction ofmodalized terms, anchored by the nominal that iden-tifies the head?s proposition:(10) @h(propositionVh?ii(di ^depi))Dependency relations are modeled as modal rela-tions h?ii, and with each dependent we associatea nominal di, representing its discourse referent.Technically, (10) states that each nominal di namesthe state where a dependent expressed as a proposi-tion depi should be evaluated and is a ?i successorof h, the nominal identifying the head.
As an exam-ple, the sentence Ed wrote a long book in Londonreceives the represention in (11).
(11) @h1(write^hACTi(d0^Ed)^hPATi(d5^book^hGRi(d7^long))^hLOCi(d9^London))The modal relations ACT, PAT, LOC, and GR standfor the dependency relations Actor, Patient, Loca-tive, and General Relationship, respectively.
SeeKruijff (2001) for the model-theoretic interpretationof expressions such as (11).Contextual reference can be modeled as a state-ment that from the current state (anaphor) thereshould be an accessible antecedent state at whichparticular conditions hold.
Thus, assuming an ac-cessibility relation XS, we can model the meaningof the pronoun he as in (12).
(12) @ihXSi( j ^male)During discourse interpretation, this statement isevaluated against the discourse model.
The pronounis resolvable only if a state where male holds is XS-accessible in the discourse model.
Different acces-sibility relations can be modeled, e.g.
to distinguisha local context (for resolving reflexive anaphors likehimself ) from a global context (Kruijff, 2001).Finally, the rich temporal ontology underlyingmodels of tense and aspect such as Moens andSteedman (1988) can be captured using the sortingstrategy.
Earlier work like Blackburn and Lascarides(1992) already explored such ideas.
HLDS employshybrid logic to integrate Moens and Steedman?s no-tion of the event nucleus directly into meaning rep-resentations.
The event nucleus is a tripartite struc-ture reflecting the underlying semantics of a type ofevent.
The event is related to a preparation (an ac-tivity bringing the event about) and a consequent (astate ensuing to the event), which we encode as themodal relations PREP and CONS, respectively.
Dif-ferent kinds of states and events are modeled as dif-ferent sorts of nominals, shown in (13) using the no-tation introduced above.
(13) @fActivity:e1ghPREPifAchievement:e2g^@fAchievement:e2ghCONSifState:e3gTo tie (13) in with a representation like (11), weequate the nominal of the head with one of the nom-inals in the event nucleus (E)a and state its temporalrelation (e.g.
hPi).
Given the event nucleus in (13),the representation in (11) becomes (14), where theevent is thus located at a specific time in the past.
(14) @h1(E(13) ^write^hACTi(d0^Ed)^hPATi(d5^book^hGRi(d7^long))^hLOCi(d9^London))^@h1fAchievement:e2g^hPifAchievement:e2gHybrid logic?s flexibility makes it amenable torepresenting a wide variety of semantic phenomenain a propositional setting, and it can furthermore beused to formulate a discourse theory (Kruijff andKruijff-Korbayova?, 2001).3.3 Comparison to MRSHere we consider the properties of HLDS withrespect to the four main criteria laid out byCopestake et al (1999) which a computational se-mantics framework must meet: expressive adequacy,grammatical compatibility, computational tractabil-ity, and underspecifiability.Expressive adequacy refers to a framework?s abil-ity to correctly express linguistic meaning.
HLDSwas designed not only with this in mind, but as itscentral tenet.
In addition to providing the meansto represent the usual predicate-valency relations,it explicitly marks the named dependency relationsbetween predicates and their arguments and modi-fiers.
These different dependency relations are notjust labels: they all have unique semantic importswhich project new relations in the context of differ-ent heads.
HLDS also tackles the representation oftense and aspect, contextual reference, and informa-tion structure, as well as their interaction with dis-course.The criterion of grammatical compatibility re-quires that a framework be linkable to other kinds ofgrammatical information.
Kruijff (2001) shows thatHLDS can be coupled to a rich grammatical frame-work, and in x4 we demonstrate that it can be tied toCCG, a much lower power formalism than that as-sumed by Kruijff.
It should furthermore be straight-forward to use our approach to hook HLDS up toother unification-based frameworks.The definition of computational tractability statesthat it must be possible to check semantic equiva-lence of different formulas straightforwardly.
LikeMRS, HLDS provides the means to view linguis-tic meaning in a flattened format and thereby easethe checking of equivalence.
For example, (15) de-scribes the same relational structure as (11).
(15) @h1(write^hACTid0 ^hPATid5 ^hLOCid9)^@d0Ed^@d5book^@d9London^@d7 long^@d5hGRid7This example clarifies how the use of nominals isrelated to the indexes of UCG?s InL and the labelsof MRS.
However, there is an important difference:nominals are full citizens of the object language withsemantic import and are not simply a device forspreading meaning across several elementary predi-cations.
They simultaneously represent tags on sub-parts of a logical form and discourse referents onwhich relations are predicated.
Because it is possi-ble to view an HLDS term as a flat conjunction ofthe heads and dependents inside it, the benefits de-scribed by Copestake et al with respect to MRS?sflatness thus hold for HLDS as well.Computational tractability also requires that itis straightforward to express relationships betweenrepresentations.
This can be done in the object lan-guage of HLDS as hybrid logic implicational state-ments which can be used with proof methods to dis-cover deeper relationships.
Kruijff?s model connect-ing linguistic meaning to a discourse context is oneexample of this.Underspecifiability means that semantic represen-tations should provide means to leave some semanticdistinctions unresolved whilst allowing partial termsto be flexibly and monotonically resolved.
(5) showshow MRS leaves quantifier scope underspecified,and such formulas can be transparently encoded inHLDS.
Consider (16), where the relations RESTRand BODY represent the restriction and body argu-ments of the generalized quantifiers, respectively.
(16) @h7(chase^hACTih4 ^hPATih11)^@h1(every^hRESTRii^hBODYi j)^@h8(some^hRESTRik^hBODYil)^@h4dog^@h11cat^@h11hGRi(h12^white)^@ihQEQih4 ^@khQEQih11MRS-style underspecification is thus replicated bydeclaring new nominals and modeling =q as a modalrelation between nominals.
When constructing thefully-scoped structures generated by an underspeci-fied one, the =q constraints must be obeyed accord-ing to the qeq condition of Copestake etal.
BecauseHLDS is couched directly in terms of hybrid logic,we can concisely declare the qeq condition as thefollowing implication:(17) @ihQEQi j !
@i j_ (@ihBODYik^@khQEQi j)Alternatively, it would in principle be possible toadopt a truly modal solution to the representationof quantifiers.
Following Alechina (1995), (general-ized) quantification can be modeled as modal opera-tors.
The complexity of generalized quantification isthen pushed into the model theory instead of forcingthe representation to carry the burden.4 CCG Coupled to HLDSIn Dependency Grammar Logic (DGL),Kruijff (2001) couples HLDS to a resource-sensitive categorial proof theory (CTL) (Moortgat,1997).
Though DGL demonstrates a procedure forbuilding HLDS terms from linguistic expressions,there are several problems we can overcome byswitching to CCG.
First, parsing with CCG gram-mars for substantial fragments is generally moreefficient than with CTL grammars with similarcoverage.
Also, a wide-coverage statistical parserwhich produces syntactic dependency structuresfor English is available for CCG (Clark et al,2002).
Second, syntactic features (modeled byunary modalities) in CTL have no intuitive semanticreflection, whereas CCG can relate syntactic andsemantic features perspicuously using unification.Finally, CCG has a detailed syntactic account of therealization of information structure in English.To link syntax and semantics in derivations, ev-ery logical form in DGL expresses a nominal iden-tifying its head in the format @i p. This handles de-pendents in a linguistically motivated way througha linking theory: given the form of a dependent, its(possible) role is established, after which its mean-ing states that it seeks a head that can take such arole.
However, to subsequently bind that dependentinto the verb?s argument slot requires logical axiomsabout the nature of various dependents.
This notonly requires extra reduction steps to arrive at thedesired logical form, but could also lead to problemsdepending on the underlying theory of roles.We present an alternative approach to binding de-pendents, which overcomes these problems withoutabandoning the linguistic motivation.
Because wework in a lexicalist setting, we can compile the ef-fects of the linguistic linking theory directly into cat-egory assignments.The first difference in our proposal is that argu-ments express only their own nominal, not the nom-inal of a head as well.
For example, proper nounsreceive categories such as (18).
(18) Ed ` n : @d1 EdThis entry highlights our relaxation of the strict con-nection between syntactic and semantic types tradi-tionally assumed in categorial grammars, a move inline with the MRS approach.In contrast with DGL, the semantic portion of asyntactic argument in our system does not declarethe role it is to take and does not identify the headit is to be part of.
Instead it identifies only its ownreferent.
Without using additional inference steps,this is transmuted via unification into a form similarto DGL?s in the result category.
(19) is an exampleof the kind of head category needed.
(19) sleeps ` s : @h2(sleep^hACTi(i^p))nn : @ipTo derive Ed sleeps, (18) and (19) combine via back-ward application to produce (20), the same term asthat built in DGL using one step instead of several.
(20) @h2(sleep^hACTi(d1^Ed))To produce HLDS terms that are fully compati-ble with the way that Kruijff and Kruijff-Korbayova?
(2001) model discourse, we need to mark the infor-mativity of dependents as contextually bound (CB)and contextually nonbound (NB).
In DGL, these ap-pear as modalities in logical forms that are used tocreate a topic-focus articulation that is merged withthe discourse context.
For example, the sentence hewrote a book would receive the following (simpli-fied) interpretation:(21) @h1([NB]write^ [NB]hPATi(d5^book)^ [CB]hACTi(d6 ^hXSi(d3^male)))DGL uses feature-resolving unary modalities(Moortgat, 1997) to instantiate the values of in-formativity.
In unification-based approaches suchas CCG, the transferal of feature information intosemantic representations is standard practice.
Wethus employ the feature inf and mark informativityin logical forms with values resolved syntactically.
(22) Ed ` ninf=CB : @d1 Ed(23) sleeps ` s : @h2([NB]sleep^ [q]hACTi(i^p))nninf=q:@ipCombining these entries using backward applicationgives the following result for Ed sleeps:(24) s : @h2([NB]sleep^ [CB]hACTi(d1^Ed))A major benefit of having nominals in our rep-resentations comes with adjuncts.
With HLDS, weconsider the prepositional verbal modifier in the sen-tence Ed sleeps in the bed as an optional Locativedependent of sleeps.
To implement this, we fol-low DGL in identifying the discourse referent of thehead with that of the adjunct.
However, unlike DGL,this is compiled into the category for the adjunct.
(25) in ` (s : @i(p^ [r]hLOCi(j^q))ns:@ip)=ninf=r:@jqTo derive the sentence Ed sleeps in the bed (seeFigure 1), we then need the following further entries:(26) the ` ninf=CB:p=ninf=NB:p(27) bed ` ninf=NB : @d3 bedThis approach thus allows adjuncts to insert theirsemantic import into the meaning of the head, mak-ing use of nominals in a manner similar to the use ofindexes in Unification Categorial Grammar.5 Intonation and Information StructureInformation Structure (IS) in English is in part deter-mined by intonation.
For example, given the ques-tion in (28), an appropriate response would be (29).2(28) I know what Ed READ.
But what did EdWRITE?
(29) (Ed WROTE) (A BOOK).L+H* LH% H* LL%Steedman (2000a) incorporates intonation intoCCG syntactic analyses to determine the contribu-tion of different constituents to IS.
Steedman callssegments such as Ed wrote of (29) the theme of thesentence, and a book the rheme.
The former indi-cates the part of the utterance that connects it withthe preceding discourse, whereas the latter providesinformation that moves the discourse forward.In the context of Discourse Representation The-ory, Kruijff-Korbayova?
(1998) represents IS bysplitting DRT structures into a topic/focus articula-tion of the form TOPIC ./ FOCUS .
We represent2Following Pierrehumbert?s notation, the intonational con-tour L+H* indicates a low-rising pitch accent, H* a sharply-rising pitch accent, and both LH% and LL% are boundary tones.Ed sleeps (= (24)) in the beds : @h2([NB]sleep^ [CB]hACTi(d1^Ed)) s : @i(p^ [r]hLOCi(j^q))ns:@ip)=ninf=r:@jq ninf=CB:s=ninf=NB:s ninf=NB:@d3 bed>ninf=CB : @d3 bed>s : @i(p^ [CB]hLOCi(d3^bed))ns:@ip<s : @h2([NB]sleep^ [CB]hACTi(d1^Ed)^ [CB]hLOCi(d3^bed))Figure 1: Derivation of Ed sleeps in the bed.this in HLDS as a term incorporating the ./ opera-tor.
Equating topic and focus with Steedman?s themeand rheme, we encode the interpretation of (29) as:(30) @h7([CB]write^ [CB]hACTi(d1^Ed)./ [NB]hPATi(d4^book))DGL builds such structures by using a rewriting sys-tem to produce terms with topic/focus articulationfrom the terms produced by the syntax.Steedman uses the pitch accents to produce lexi-cal entries with values for the INFORMATION fea-ture, which we call here sinf .
L+H* and H* setthe value of this feature as ?
(for theme) or ?
(for rheme), respectively.
He also employs cate-gories for the boundary tones that carry blockingvalues for sinf which stop incomplete intonationalphrases from combining with others, thereby avoid-ing derivations for utterances with nonsensical into-nation contours.Our approach is to incorporate the syntactic as-pects of Steedman?s analysis with DGL?s rewritingsystem for using informativity to partition senten-tial meaning.
In addition to using the syntactic fea-ture sinf , we allow intonation marking to instantiatethe values of the semantic informativity feature inf .Thus, we have the following sort of entry:(31) WROTE (L+H*) `ssinf=?:?nninf=w;sinf=?:@ip=ninf=x;sinf=?
:@jq?=@h2([CB]write^[w]hACTi(i^p)^[x]hPATi( j^q))We therefore straightforwardly reap the syntacticbenefits of Steedman?s intonation analysis, while ISitself is determined via DGL?s logical form rewrit-ing system operating on the modal indications ofinformativity produced during the derivation.
Thearticulation of IS can thus be performed uniformlyacross languages, which use a variety of strategiesincluding intonation, morphology, and word ordervariation to mark the informativity of different el-ements.
The resulting logical form plugs directlyinto DGL?s architecture for incorporating sentencemeaning with the discourse.6 Conclusions and Future WorkSince it is couched in hybrid logic, HLDS is ide-ally suited to be logically engineered to the task athand.
Hybrid logic can be made to do exactly whatwe want, answering to the linguistic intuitions wewant to formalize without yielding its core assets ?
arich propositional ontology, decidability, and favor-able computational complexity.Various aspects of meaning, like dependency re-lations, contextual reference, tense and aspect, andinformation structure can be perspicuously encodedwith HLDS, and the resulting representations canbe built compositionally using CCG.
CCG has closeaffinities with dependency grammar, and it providesa competitive and explanatorily adequate basis fora variety of phenomena ranging from coordinationand unbounded dependencies to information struc-ture.
Nonetheless, the approach we describe couldin principle be fit into other unification-based frame-works like Head-Driven Phrase Structure Grammar.Hybrid logic?s utility does not stop with senten-tial meaning.
It can also be used to model dis-course interpretation and is closely related to log-ics for knowledge representation.
This way we cancover the track from grammar to discourse with asingle meaning formalism.
We do not need to trans-late or make simplifying assumptions for differentprocessing modules to communicate, and we canfreely include and use information across differentlevels of meaning.We have implemented a (preliminary) Java pack-age for creating and manipulating hybrid logic termsand connected it to Grok, a CCG parsing system.3The use of HLDS has made it possible to improve3The software is available at http://opennlp.sf.netand http://grok.sf.net under an open source license.the representation of the lexicon.
Hybrid logic nom-inals provide a convenient and intuitive manner oflocalizing parts of a semantic structure, which hasmade it possible to greatly simplify the use of inher-itance in the lexicon.
Logical forms are created asan accumulation of different levels in the hierarchyincluding morphological information.
This is partic-ularly important since the system does not otherwisesupport typed feature structures with inheritance.Hybrid logics provide a perspicuous logical lan-guage for representing structures in temporal logic,description logic, AVMs, and indeed any relationalstructure.
Terms of HLDS can thus be marshalledinto terms of these other representations with thepotential of taking advantage of tools developed forthem or providing input to modules expecting them.In future work, we intend to combine techniquesfor building wide-coverage statistical parsers forCCG (Hockenmaier and Steedman, 2002; Clark etal., 2002) with corpora that have explicitly markedsemantic dependency relations (such as the PragueDependency Treebank and NEGRA) to produceHLDS terms as the parse output.AcknowledgementsWe would like to thank Patrick Blackburn, Johan Bos, NissimFrancez, Alex Lascarides, Mark Steedman, Bonnie Webber andthe ACL reviewers for helpful comments on earlier versions ofthis paper.
All errors are, of course, our own.
Jason Baldridge?swork is supported in part by Overseas Research Student AwardORS/98014014.
Geert-Jan Kruijff?s work is supported by theDFG Sonderforschungsbereich 378 Resource-Sensitive Cogni-tive Processes, Project NEGRA EM6.ReferencesNatasha Alechina.
1995.
Modal Quantifiers.
Ph.D. thesis, Uni-versity of Amsterdam, Amsterdam, The Netherlands.Carlos Areces, Patrick Blackburn, and Maarten Marx.
1999.
Aroad-map on complexity for hybrid logics.
In J. Flum andM.
Rodr?
?guez-Artalejo, editors, Computer Science Logic,number 1683 in Lecture Notes in Computer Science, pages307?321.
Springer-Verlag.Carlos Areces.
2000.
Logic Engineering.
The Case of Descrip-tion and Hybrid Logics.
Ph.D. thesis, University of Amster-dam, Amsterdam, The Netherlands.Patrick Blackburn and Alex Lascarides.
1992.
Sorts and oper-ators for temporal semantics.
In Proc.
of the Fourth Sympo-sium on Logic and Language, Budapest, Hungary.Patrick Blackburn.
1994.
Tense, temporal reference and tenselogic.
Journal of Semantics, 11:83?101.Patrick Blackburn.
2000.
Representation, reasoning, and rela-tional structures: a hybrid logic manifesto.
Logic Journal ofthe IGPL, 8(3):339?625.Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002.Building deep dependency structures using a wide-coverageCCG parser.
In Proc.
of the 40th Annual Meeting of the As-sociation of Computational Linguistics, Philadelphia, PA.Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard.1999.
Minimal recursion semantics: An introduction.
ms,www-csli.stanford.edu/?aac/newmrs.ps.Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001.An algebra for semantic construction in constraint-basedgrammars.
In Proc.
of the 39th Annual Meeting of theAssociation of Computational Linguistics, pages 132?139,Toulouse, France.Julia Hockenmaier and Mark Steedman.
2002.
Generativemodels for statistical parsing with combinatory categorialgrammar.
In Proc.
of the 40th Annual Meeting of the As-sociation of Computational Linguistics, Philadelphia, PA.Geert-Jan M. Kruijff and Ivana Kruijff-Korbayova?.
2001.
Ahybrid logic formalization of information structure sensitivediscourse interpretation.
In Proc.
of the Fourth Workshopon Text, Speech and Dialogue, volume 2166 of LNCS/LNAI,pages 31?38.
Springer-Verlag.Ivana Kruijff-Korbayova?.
1998.
The Dynamic Potential ofTopic and Focus: A Praguian Approach to Discourse Repre-sentation Theory.
Ph.D. thesis, Charles University, Prague,Czech Republic.Geert-Jan M. Kruijff.
2001.
A Categorial Modal Architec-ture of Informativity: Dependency Grammar Logic & Infor-mation Structure.
Ph.D. thesis, Charles University, Prague,Czech Republic.Marc Moens and Mark Steedman.
1988.
Temporal ontologyand temporal reference.
Computational Linguistics, 14:15?28.Michael Moortgat.
1997.
Categorial type logics.
In Johan vanBenthem and Alice ter Meulen, editors, Handbook of Logicand Language.
Elsevier Science B.V.Glyn V. Morrill.
1994.
Type Logical Grammar: CategorialLogic of Signs.
Kluwer Academic Publishers, Dordrecht,Boston, London.Mark Steedman.
2000a.
Information structure and the syntax-phonology interface.
Linguistic Inquiry, 34:649?689.Mark Steedman.
2000b.
The Syntactic Process.
The MITPress, Cambridge Mass.Henk Zeevat.
1988.
Combining categorial grammar and unifi-cation.
In Uwe Reyle and Christian Rohrer, editors, NaturalLanguage Parsing and Linguistic Theories, pages 202?229.Reidel, Dordrecht.
