Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 947?954,Honolulu, October 2008. c?2008 Association for Computational LinguisticsAutomatic Set Expansion for List Question AnsweringRichard C. Wang, Nico Schlaefer, William W. Cohen, and Eric NybergLanguage Technologies InstituteCarnegie Mellon University5000 Forbes AvenuePittsburgh PA 15213{rcwang,nico,wcohen,ehn}@cs.cmu.eduAbstractThis paper explores the use of set expan-sion (SE) to improve question answering (QA)when the expected answer is a list of entitiesbelonging to a certain class.
Given a smallset of seeds, SE algorithms mine textual re-sources to produce an extended list includingadditional members of the class representedby the seeds.
We explore the hypothesis thata noise-resistant SE algorithm can be used toextend candidate answers produced by a QAsystem and generate a new list of answers thatis better than the original list produced by theQA system.
We further introduce a hybrid ap-proach which combines the original answersfrom the QA system with the output from theSE algorithm.
Experimental results for severalstate-of-the-art QA systems show that the hy-brid system performs better than the QA sys-tems alone when tested on list question datafrom past TREC evaluations.1 IntroductionQuestion answering (QA) systems are designed toretrieve precise answers to questions posed in nat-ural language.
A list question expects a list as itsanswer, e.g.
Name the coffee-producing countries inSouth America.
The ability to answer list questionshas been tested as part of the yearly TREC QA eval-uation (Dang et al, 2006; Dang et al, 2007).
Thispaper focuses on the use of set expansion to improvelist question answering.
A set expansion (SE) algo-rithm receives as input a few members of a class orset, and mines various textual resources (e.g.
webpages) to produce an extended list including addi-tional members of the class or set that are not in theinput.
A well-known online SE system is GoogleSets1.
This system is publicly accessible, but since itis a proprietary system that might be changed at anytime, its results cannot be replicated reliably.
We ex-plore the hypothesis that a SE algorithm, when care-fully designed to handle noisy inputs, can be appliedto the output from a QA system to produce an overalllist of answers for a given question that is better thanthe answers produced by the QA system itself.
Wepropose to exploit large, redundant sources of struc-tured and/or semi-structured data and use linguisticanalysis to seed a shallow analysis of these sources.This is a hard problem since the linguistic evidenceused as seeds is noisy.
More precisely, we combinethe QA system Ephyra (Schlaefer et al, 2007) withthe SE system SEAL (Wang and Cohen, 2007) tocreate a hybrid approach that performs better thaneither system by itself when tested on data from theTREC 13-15 evaluations.
In addition, we apply ourSE algorithm to answers generated by the five QAsystems that performed the best on the list questionsin the TREC 15 evaluation and report improvementsin F1 scores for four of these systems.Section 2 of the paper gives an overview of theQA and SE systems used for our experiments.
Sec-tion 3 describes how the SE system was adapted todeal with noisy seeds produced by QA systems, andSection 4 presents the details of the experimental de-sign.
Experimental results are discussed in Section5, and the paper concludes in Section 6 with a dis-cussion of planned future work.1http://labs.google.com/sets9472 System Overview2.1 Ephyra Question Answering SystemEphyra (Schlaefer et al, 2006; Schlaefer et al,2007) is a QA system that has been evaluated inthe TREC QA track (Dang et al, 2006; Dang et al,2007).
The system combines three answer extrac-tion techniques for factoid and list questions: (1) ananswer type classification approach; (2) a syntacticpattern learning and matching approach; and (3) asemantic extractor that uses a semantic role label-ing system.
The answer type based extractor clas-sifies questions by their answer types and extractscandidates of the expected types.
The Ephyra pat-tern matching approach learns textual patterns thatrelate question key terms to possible answers andapplies these patterns to candidate sentences to ex-tract factoid answers.
The semantic approach gener-ates a semantic representation of the question that isbased on predicate-argument structures and extractsanswer candidates from similar structures in the cor-pus.
The source code of the answer extractors is in-cluded in OpenEphyra, an open source release of thesystem.2The answer candidates from these extractors arecombined and ranked by a statistical answer selec-tion framework (Ko et al, 2007), which estimatesthe probability of an answer based on a number ofanswer validation and similarity features.
Valida-tion features use resources such as gazetteers andWikipedia to verify an answer, whereas similarityfeatures measure the syntactic and semantic simi-larity to other candidates, e.g.
using string distancemeasures and WordNet relations.2.2 Set Expander for Any Language (SEAL)Set expansion (SE) refers to expanding a given par-tial set of objects into a more complete set.
SEAL3(Wang and Cohen, 2007) is a SE system which ac-cepts input elements (seeds) of some target set Stand automatically finds other probable elements ofSt in semi-structured documents such as web pages.SEAL also works on unstructured text, but its ex-traction mechanism benefits from structuring ele-ments such as HTML tags.
The algorithm is in-dependent of the human language from which the2http://www.ephyra.info/3http://rcwang.com/sealFigure 1: Examples of SEAL?s input and output.
Englishentities are reality TV shows, Chinese entities are popularTaiwanese food, and Japanese entities are famous cartooncharacters.Figure 2: An example graph constructed by SEAL.
Everyedge from node x to y actually has an inverse relationedge from node y to x that is not shown here (e.g.
m1 isextracted by w1).seeds are taken, and also independent of the markuplanguage used to annotate the documents.
Examplesof SEAL?s input and output are shown in Figure 1.In more detail, SEAL comprises three major com-ponents: the Fetcher, the Extractor, and the Ranker.The Fetcher focuses on retrieving web pages.
TheURLs of the web pages come from top results re-trieved from Google and Yahoo!
using the concate-nation of all seeds as the query.
The Extractor au-tomatically constructs page-specific extraction rules,or wrappers, for each page that contains the seeds.Every wrapper is defined by two character strings,which specify the left-context and right-context nec-essary for an entity to be extracted from a page.These strings are chosen to be maximally-long con-texts that bracket at least one occurrence of everyseed string on a page.
Most of the wrappers con-948tain HTML tags, which illustrates the importanceof structuring information in the source documents.All entity mentions bracketed by these contextualstrings derived from a particular page are extractedfrom the same page.
Finally, the Ranker builds agraph, and then ranks the extracted mentions glob-ally based on the weights computed by performing arandom graph walk.An example graph is shown in Figure 2, whereeach node di represents a document, wi a wrapper,and mi an extracted entity mention.
The graph mod-els the relationship between documents, wrappers,and mentions.
In order to measure the relative im-portance of each node within the graph, the Rankerperforms a graph walk until all node weights con-verge.
The idea is that nodes are weighted higherif they are connected to many other highly weightednodes.We apply this SE algorithm to answer candidatesfor list questions generated by Ephyra and otherTREC QA systems to find additional instances ofcorrect answers that were not in the original candi-date set.3 Proposed ApproachSEAL was originally designed to handle only rele-vant input seeds.
When provided with a mixture ofrelevant and irrelevant answers from a QA system,the performance would suffer.
In this section, wepropose three modifications to SEAL to improve itsability to handle noisy input seeds.3.1 Aggressive FetcherFor each expansion, SEAL?s fetcher concatenates allseeds and sends them as one query to the searchengines.
However, when the seeds are noisy, thedocuments fetched are constrained by the irrele-vant seeds, which decreases the chance of findinggood documents.
To overcome this problem, wedesigned an aggressive fetcher (AF) that increasesthe chance of composing queries containing onlyrelevant seeds.
It sends a two-seed query for ev-ery possible pair of seeds to the search engines.
Ifthere are n input seeds, then the total number ofqueries sent would be (n2).
For example, supposeSEAL is given a set of noisy seeds: Boston, Seattleand Carnegie-Mellon (assuming Carnegie-Mellon isirrelevant), then by using AF, one query will containonly relevant seeds (as shown in Table 1).
The docu-ments are then collected and sent to SEAL?s extrac-tor for learning wrappers.Queries Quality-AF #1: Boston Seattle Carnegie-Mellon Low+AF#1: Boston Seattle High#2: Boston Carnegie-Mellon Low#3: Seattle Carnegie-Mellon LowTable 1: Example queries and their quality giventhe seeds Boston, Seattle and Carnegie-Mellon, whereCarnegie-Mellon is assumed to be irrelevant.3.2 Lenient ExtractorSEAL?s extractor requires the longest commoncontexts to bracket at least one instance of everyseed per web page.
However, when seeds are noisy,such common contexts usually do not exist orare too short to be useful.
To solve this problem,we propose a lenient extractor (LE) which onlyrequires the contexts to bracket at least one in-stance of a minimum of two seeds, instead of everyseed.
This increases the chance of finding longestcommon contexts that bracket only relevant seeds.For instance, suppose SEAL is given the seedsfrom the previous example (Boston, Seattle andCarnegie-Mellon) and the passage below.
Then theextractor would learn the wrappers shown in Table 2.?While attending a hearing in Boston CityHall, Alan, a professor at Boston University,met Tina, his former student at Seattle Univer-sity, who is studying at Carnegie-Mellon UniversityArt School and will be working in Seattle City Hall.
?Learned Wrappers-LE #1: at [...] University+LE #1: at [...] University#2: in [...] City HallTable 2: Wrappers learned by SEAL?s extractor whengiven the passage in Section 3.2 and the seeds Boston,Seattle and Carnegie-Mellon.949As illustrated, with lenient extraction, SEAL isnow able to learn the second wrapper because itbrackets one instance of at least two seeds (Bostonand Seattle).
This can be very helpful if the listquestion is asking for city names rather than univer-sity names.
The extractor then uses these wrappersto extract additional answer candidates, by search-ing for other strings that fit into the placeholders ofthe wrappers.
Note that the example was simplifiedfor ease of presentation.
The wrappers are actuallycharacter-based (as opposed to word-based) and arelikely to contain HTML tags when generated fromreal web pages.3.3 Hinted ExpanderMost QA systems use keywords from the question toguide the retrieval of relevant documents and the ex-traction of answer candidates.
We believe these key-words are also important for SEAL to identify ad-ditional instances of correct answers.
For example,if the seeds are George Washington, John Adams,and Thomas Jefferson, then without using any con-text from the question, SEAL would output a mix-ture of founding fathers and presidents of the U.S.A.To solve this problem, we devised a hinted expan-sion (HE) technique that utilizes the context givenin the question to constrain SEAL?s search space onthe Web.
This is achieved by appending keywordsfrom the question to every query that is sent to thesearch engines.
The rationale is that the retrieveddocuments will also match the keywords, which mayincrease the chance of finding those documents thatcontain our desired set of answers.4 Experimental DesignWe conducted experiments in two phases.
In thefirst phase, we evaluated the SE approach by apply-ing SEAL to answers generated by Ephyra.
In thesecond phase, we evaluated the approach by apply-ing SEAL to the output from QA systems that per-formed the best on the list questions in the TREC 15evaluation.
In both phases, the answers found bySEAL were retrieved from the Web instead of theAQUAINT newswire corpus used in the TREC eval-uations.
However, we rejected answers if they couldonly be found in the Web and not in the AQUAINTcorpus to avoid an unfair advantage over the QAsystems: TREC participants were allowed to extractcandidates from the Web (or any other source), butthey had to identify a supporting document in theAQUAINT corpus for each answer and thus couldnot return answers that were not covered by the cor-pus.Preliminary experiments showed that we can ob-tain a good balance between the amount and qualityof the documents fetched by using only rare ques-tion terms as hint words.
In particular, we select thethree question words that occur least frequently in asample of the AQUAINT corpus as hints.
The can-didate answers were evaluated by using the answerkeys, composed of regular expression patterns, ob-tained from the TREC website.
We did not extendthe patterns with additional correct answers found inour experiments.
These answer keys were not offi-cially used in the TREC evaluation; thus the baselinescores we computed for Ephyra and other QA sys-tems in our experiments are slightly different fromthose officially reported.4.1 EphyraWe evaluated our SE approach on Ephyra using thelist questions from TREC 13, 14, and 15 (55, 93, and89 questions, respectively).
For each question, thetop four answer candidates from Ephyra were givenas input seeds to SEAL.
Initial experiments showedthat by adding additional seeds, the effectiveness ofour approach can be improved at the expense of alonger runtime.We report both mean average precision (MAP)and F1 scores.
For the F1 scores, we drop answercandidates with low confidence scores by applyinga relative cut-off threshold: an answer candidate isdropped if the ratio of its confidence score and thescore of the top answer is below a threshold.
Anoptimal threshold for a question is a threshold thatmaximizes the F1 score for that particular question.For each TREC dataset, we conducted three ex-periments: (1) evaluation of answer candidates us-ing MAP; (2) evaluation using average F1 with anoptimal threshold for each question; and (3) eval-uation using average F1 with thresholds trained by5-fold cross validation.
For each of those 5-fold val-idations, only one threshold was determined for allquestions in the training folds.950Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LETop 4 Ans.
+AF +AF+HETREC 13 25.95% 21.39% 23.76% 31.43% 34.22% 35.26%TREC 14 14.45% 8.71% 14.47% 17.04% 16.58% 18.82%TREC 15 13.42% 9.02% 13.17% 16.87% 17.12% 18.95%Table 3: Mean average precision of Ephyra, its top four answers, and various SEAL configurations, where LE isLenient Extractor, AF is Aggressive Fetcher, and HE is Hinted Expander.Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LETop 4 Ans.
+AF +AF+HETREC 13 35.74% 26.29% 30.53% 36.47% 40.08% 40.80%TREC 14 22.83% 14.05% 20.62% 22.81% 22.66% 24.88%TREC 15 22.42% 14.57% 19.88% 23.30% 24.04% 25.65%Table 4: Average F1 of Ephyra, its top four answers, and various SEAL configurations when using an optimal thresholdfor each question.4.2 Top QA SystemsWe evaluated two SE approaches, SEAL and GoogleSets, on the five QA systems that performed the beston the list questions in TREC 15.
For each question,the top four answer candidates4 from those systemswere given as input seeds to SEAL and Google Sets.Unlike the candidates found by Ephyra, these can-didates were provided without confidence scores;hence, we assumed they all have a score of 1.0.
Inour experiments with SEAL, we first determined asingle threshold that optimizes the average of the F1scores of the top five systems in both TREC 13 and14.
We then obtained evaluation results for the topsystems in TREC 15 by using this trained threshold.When performing hinted expansion, the keywords(or hint words) for each question were extracted byEphyra?s question analysis component.
In our exper-iments with Google Sets, we requested Small Sets ofitems and again measured the performance in termsof F1 scores.
We also tried requesting Large Sets butthe results were worse.5 Results and DiscussionIn Tables 3 and 4, we present evaluation results forall answers from Ephyra, only the top four answers,and various configurations of SEAL using the topfour answers as seeds.
Table 3 shows the MAP for4Obtained from http://trec.nist.gov/resultseach dataset (TREC 13, 14, and 15), and Table 4shows for each dataset the average F1 score whenusing optimal per-question thresholds.
The resultsindicate that SEAL achieves the best performancewhen configured with all three proposed extensions.In terms of MAP, the best-configured SEAL im-proves the quality of the input answers (relatively)by 65%, 116%, 110% for each dataset respectively,and improves Ephyra?s overall performance by 36%,30%, 41%.
In terms of optimal F1, SEAL improvesthe quality of the input answers by 55%, 77%, 76%and Ephyra?s overall performance by 14%, 9%, 14%respectively.
These results illustrate that a SE sys-tem is capable of improving a QA system?s perfor-mance on list questions, if we know how to selectgood thresholds.In practice, the thresholds are unknown and mustbe estimated from a training set.
Table 5 shows eval-uation results using 5-fold cross validation for eachdataset (TREC 13, 14, and 15) independently, andthe combination of all three datasets (All).
For eachvalidation, we determine the threshold that maxi-mizes the F1 score on the training folds, and wealso determine the F1 score on the test fold by ap-plying the trained threshold.
We repeat this valida-tion for each of the five test folds and present the av-erage threshold and F1 score for each configurationand dataset.
The F1 scores give an estimate of theperformance on unseen data and allow a fair com-951Ephyra SEAL+LE+AF+HE HybridAvg.
F1 Avg.
Threshold Avg.
F1 Avg.
Threshold Avg.
F1 Avg.
ThresholdTREC 13 25.55% 0.3808 30.71% 0.3257 29.04% 0.0796TREC 14 15.78% 0.2636 15.60% 0.1889 17.13% 0.0108TREC 15 15.19% 0.1192 15.64% 0.2581 16.47% 0.0123All 18.03% 0.2883 19.15% 0.2606 19.59% 0.0164Table 5: Average F1 of Ephyra, the best-configured SEAL, and the hybrid system, along with thresholds trained by5-fold cross validation.TREC 15 Baseline Top 4 Ans.
Google Sets SEAL+LE+AF+HE HybridQA Systems Avg.
F1 Avg.
F1 Avg.
F1 ?F1 Avg.
F1 ?F1 Avg.
F1 ?F1lccPA06 44.96% 32.67% 37.89% -15.72% 40.00% -11.04% 45.30% 0.76%cuhkqaepisto 18.27% 17.02% 15.96% -12.68% 19.75% 8.08% 19.13% 4.70%NUSCHUAQA1 18.40% 14.99% 16.70% -9.21% 18.74% 1.86% 18.06% -1.81%FDUQAT15A 19.71% 14.32% 18.79% -4.63% 19.78% 0.38% 20.61% 4.57%QACTIS06C 17.52% 15.22% 17.05% -2.72% 18.45% 5.26% 18.38% 4.85%Average 23.77% 18.84% 21.28% -10.49% 23.34% -1.81% 24.30% 2.20%Table 6: Average F1 of the QA systems, their top four answers, Google Sets, the best-configured SEAL, the hybridsystem, and their relative improvements over the QA systems.parison across systems.
Here, we also introduce ahybrid system (Hybrid) that intersects the answersfound by both systems by multiplying their proba-bilistic scores.Tables 3, 4, and 5 show that the effectiveness ofthe SE approach depends on the quality of the initialanswer candidates.
The improvements are most ap-parent for the TREC 13 dataset, where Ephyra hasa much higher performance compared to TREC 14and 15.
However, the best-configured SEAL did notimprove the F1 score on TREC 14, as reported inTable 5.
We suspect that this is due to the compar-atively low quality of Ephyra?s top four answers forthis dataset.
The experiments also illustrate that byintersecting the answer candidates found by Ephyraand SEAL, we can eliminate poor answer candi-dates and partially compensate for the low preci-sion of Ephyra on the harder TREC datasets.
How-ever, this comes at the expense of a lower recall,which slightly hurts the performance on the compar-atively easier TREC 13 questions.
We also evaluatedGoogle Sets on top four answers from Ephyra forTREC 13-15 and obtained F1 scores of 12%, 11%,and 9% respectively (compared to 29%, 17%, and16% for our hybrid approach with trained thresh-olds).Table 6 shows F1 scores for the SE approachapplied to the output from the five QA systemswith the highest performance on the list questionsin TREC 15.
Again, Hybrid intersects the answersfound by the QA system and SEAL by multiplyingtheir confidence scores.
Two thresholds were trainedseparately on the top five systems in both TREC 13and 14; one for SEAL (0.2376) and another for Hy-brid (0.2463).
As shown, the performance of GoogleSets is worse than SEAL and Hybrid, but better thanthe top four answers on average.
We believe our SEsystem outperforms Google Sets because we havemethods to handle noisy inputs (i.e.
AF, LE) and amethod for guiding the SE algorithm to search in theright space on the Web (i.e.
HE).The results show that both SEAL and Hybrid arecapable of improving four out of the five systems.We observed that one reason why SEAL did not im-prove ?lccPA06?
was the incompleteness of the an-swer keys.
Table 7 shows one of many exampleswhere SEAL was penalized for finding additionalcorrect answers.
As illustrated, Hybrid improvedall systems except ?NUSCHUAQA1?.
The reasonis that even though SEAL improved the baseline,their overlapping answer set is too small; thus hurt-ing the recall of Hybrid substantially.
Unfortunately,952Question 154.6: Name titles of movies, other than ?Superman?
movies, thatChristopher Reeve acted in.lccPA06 (F1: 75%) SEAL+LE+AF+HE (F1: 40%)+Rear Window +Rear Window+The Remains of the Day +The Remains of the Day+Snakes and Ladders -The Bostonians-Superman -Somewhere in Time-Village of the Damned-In the GloamingTable 7: Example of SEAL being penalized for finding correct answers (all are correct except the last one).
Answersfound in the answer keys are marked with ?+?.
All four answers from ?lccPA06?
were used as seeds.Question 170.6: What are the titles of songs written by John Prine?NUSCHUAQA1 (F1: 25%) SEAL+LE+AF+HE (F1: 44%)+I Just Want to Dance With You +I Just Want to Dance With You-Titled In Spite of Ourselves +Christmas in Prison+Christmas in Prison +Sam Stone-Grammy - Winning -Grandpa was a Carpenter-Sabu Visits the Twin Cities Alone+Angel from MontgomeryTable 8: Example demonstrating SEAL?s ability to handle noisy input seeds.
All four answers from ?NUSCHUAQA1?were used as seeds.
Again, SEAL is penalized for finding correct answers (all answers are correct).for the top TREC 15 systems we only had access tothe answers that were actually submitted by the par-ticipants, whereas for Ephyra we could utilize theentire list of generated answer candidates, includ-ing those that fell below the cutoff threshold for listquestions.
Nevertheless, the hybrid approach couldimprove the baseline by more than 2% on averagein terms of F1 score.
Table 8 shows that the best-configured SEAL is capable of expanding only therelevant seeds even when given a set of noisy seeds.Neither Google Sets nor the original SE algorithmwithout the proposed extensions could expand theseseeds with additional candidates.On average, SEAL required about 5 seconds forquerying the search engines, 10 seconds for crawl-ing the Web, 20 seconds for extracting answer can-didates from the web pages, and 5 seconds for rank-ing the candidates.
Note that the SE system has notbeen optimized extensively.
The runtime of the webpage retrieval step and much of the search is due tonetwork latency and can be reduced if the search isperformed locally.6 Conclusion and Future WorkWe have shown that our SE approach is capable ofimproving the performance of QA systems on listquestions by utilizing only their top four answer can-didates as seeds.
We have also illustrated a feasibleand effective method for integrating a SE approachinto any QA system.
We would like to emphasizethat for each of the experiments we conducted, allthat the SE system received as input were the topfour noisy answers from a QA system and three key-words from the TREC questions.
We have shownthat higher quality candidates support more effec-tive set expansion.
In the future, we will investigatehow to utilize more answer candidates from the QAsystem and determine the minimal quality of thosecandidates required for SE approach to make an im-provement.We have also shown that, in terms of F1 scoreswith trained thresholds, the hybrid method improvesthe Ephyra QA system on all datasets and also im-proves four out of the five systems that performed953the best on the list questions in TREC 15.
How-ever, the final list of answers only comprises candi-dates found by both the QA system and the SE al-gorithm.
In future experiments, we will investigateother methods of merging answer candidates, suchas taking the union of answers from both systems.We expect further improvements from adding can-didates that are found only by the QA system, butit is unclear how the confidence measures from thetwo systems can be combined effectively.We would also like to emphasize that the SE ap-proach is entirely language independent, and thuscan be readily applied to answer candidates in otherlanguages.
In future experiments, we will investi-gate its performance on question answering tasks inlanguages such as Chinese and Japanese.As pointed out previously, the performance of theSE approach highly depends on the accuracy of theseeds.
However, QA systems are usually not op-timized to provide few high-precision results, buttreat precision and recall as equally important.
Thisleaves room for further improvements, e.g.
by ap-plying stricter answer validation techniques to theseeds used for SE.We also plan to analyze the effectiveness of ourapproach across different question types and evalu-ate it on more complex questions such as the rigidlist questions in the new TAC QA evaluation, whichask for opinion holders and subjects.AcknowledgementsThis work was supported in part by the Google Re-search Awards program, IBM Open CollaborationAgreement #W0652159, and the Defense AdvancedResearch Projects Agency (DARPA) under ContractNo.
NBCHD030010.ReferencesH.T.
Dang, J. Lin, and D. Kelly.
2006.
Overview of theTREC 2006 question answering track.
Proceedings ofthe Fifteenth Text REtrieval Conference.H.T.
Dang, D. Kelly, and J. Lin.
2007.
Overview of theTREC 2007 question answering track.
Proceedings ofthe Sixteenth Text REtrieval Conference.J.
Ko, L. Si, and E. Nyberg.
2007.
A probabilistic frame-work for answer selection in question answering.
Pro-ceedings of NAACL-HLT.N.
Schlaefer, P. Gieselmann, and G. Sautter.
2006.
TheEphyra QA system at TREC 2006.
Proceedings of theFifteenth Text REtrieval Conference.N.
Schlaefer, G. Sautter, J. Ko, J. Betteridge, M. Pathak,and E. Nyberg.
2007.
Semantic extensions of theEphyra QA system in TREC 2007.
To appear in: Pro-ceedings of the Sixteenth Text REtrieval Conference.R.C.
Wang and W.W. Cohen.
2007.
Language-independent set expansion of named entities using theweb.
Proceedings of IEEE International Conferenceon Data Mining.954
