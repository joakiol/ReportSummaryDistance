Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1557?1567,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsIncorporating Discrete Translation Lexiconsinto Neural Machine TranslationPhilip Arthur?, Graham Neubig?
?, Satoshi Nakamura??
Graduate School of Information Science, Nara Institute of Science and Technology?
Language Technologies Institute, Carnegie Mellon Universityphilip.arthur.om0@is.naist.jp gneubig@cs.cmu.edu s-nakamura@is.naist.jpAbstractNeural machine translation (NMT) oftenmakes mistakes in translating low-frequencycontent words that are essential to understand-ing the meaning of the sentence.
We proposea method to alleviate this problem by aug-menting NMT systems with discrete transla-tion lexicons that efficiently encode transla-tions of these low-frequency words.
We de-scribe a method to calculate the lexicon proba-bility of the next word in the translation candi-date by using the attention vector of the NMTmodel to select which source word lexicalprobabilities the model should focus on.
Wetest two methods to combine this probabilitywith the standard NMT probability: (1) usingit as a bias, and (2) linear interpolation.
Exper-iments on two corpora show an improvementof 2.0-2.3 BLEU and 0.13-0.44 NIST score,and faster convergence time.11 IntroductionNeural machine translation (NMT, ?2; Kalchbren-ner and Blunsom (2013), Sutskever et al (2014))is a variant of statistical machine translation (SMT;Brown et al (1993)), using neural networks.
NMThas recently gained popularity due to its ability tomodel the translation process end-to-end using a sin-gle probabilistic model, and for its state-of-the-artperformance on several language pairs (Luong et al,2015a; Sennrich et al, 2016).One feature of NMT systems is that they treateach word in the vocabulary as a vector of1Tools to replicate our experiments can be found athttp://isw3.naist.jp/~philip-a/emnlp2016/index.htmlInput: I come from Tunisia.Reference: ??????????
?Chunisia no shusshindesu.
(I?m from Tunisia.
)System: ??????????
?Noruue- no shusshindesu.
(I?m from Norway.
)Figure 1: An example of a mistake made by NMTon low-frequency content words.continuous-valued numbers.
This is in contrast tomore traditional SMT methods such as phrase-basedmachine translation (PBMT; Koehn et al (2003)),which represent translations as discrete pairs ofword strings in the source and target languages.
Theuse of continuous representations is a major advan-tage, allowing NMT to share statistical power be-tween similar words (e.g.
?dog?
and ?cat?)
or con-texts (e.g.
?this is?
and ?that is?).
However, thisproperty also has a drawback in that NMT systemsoften mistranslate into words that seem natural in thecontext, but do not reflect the content of the sourcesentence.
For example, Figure 1 is a sentence fromour data where the NMT system mistakenly trans-lated ?Tunisia?
into the word for ?Norway.?
Thisvariety of error is particularly serious because thecontent words that are often mistranslated by NMTare also the words that play a key role in determiningthe whole meaning of the sentence.In contrast, PBMT and other traditional SMTmethods tend to rarely make this kind of mistake.This is because they base their translations on dis-crete phrase mappings, which ensure that sourcewords will be translated into a target word that has1557been observed as a translation at least once in thetraining data.
In addition, because the discrete map-pings are memorized explicitly, they can be learnedefficiently from as little as a single instance (barringerrors in word alignments).
Thus we hypothesizethat if we can incorporate a similar variety of infor-mation into NMT, this has the potential to alleviateproblems with the previously mentioned fatal errorson low-frequency words.In this paper, we propose a simple, yet effectivemethod to incorporate discrete, probabilistic lexi-cons as an additional information source in NMT(?3).
First we demonstrate how to transform lexi-cal translation probabilities (?3.1) into a predictiveprobability for the next word by utilizing attentionvectors from attentional NMT models (Bahdanau etal., 2015).
We then describe methods to incorporatethis probability into NMT, either through linear in-terpolation with the NMT probabilities (?3.2.2) or asthe bias to the NMT predictive distribution (?3.2.1).We construct these lexicon probabilities by usingtraditional word alignment methods on the trainingdata (?4.1), other external parallel data resourcessuch as a handmade dictionary (?4.2), or using a hy-brid between the two (?4.3).We perform experiments (?5) on two English-Japanese translation corpora to evaluate themethod?s utility in improving translation accuracyand reducing the time required for training.2 Neural Machine TranslationThe goal of machine translation is to translate a se-quence of source words F = f |F |1 into a sequence oftarget words E = e|E|1 .
These words belong to thesource vocabulary Vf , and the target vocabulary Verespectively.
NMT performs this translation by cal-culating the conditional probability pm(ei|F, ei?11 )of the ith target word ei based on the source F andthe preceding target words ei?11 .
This is done by en-coding the context ?F, ei?11 ?
a fixed-width vector ?i,and calculating the probability as follows:pm(ei|F, ei?11 ) = softmax(Ws?i + bs), (1)where Ws and bs are respectively weight matrix andbias vector parameters.The exact variety of the NMT model depends onhow we calculate ?i used as input.
While thereare many methods to perform this modeling, we optto use attentional models (Bahdanau et al, 2015),which focus on particular words in the source sen-tence when calculating the probability of ei.
Thesemodels represent the current state of the art in NMT,and are also convenient for use in our proposedmethod.
Specifically, we use the method of Luong etal.
(2015a), which we describe briefly here and referreaders to the original paper for details.First, an encoder converts the source sentence Finto a matrix R where each column represents a sin-gle word in the input sentence as a continuous vec-tor.
This representation is generated using a bidirec-tional encoder?
?r j = enc(embed(fj),?
?r j?1)?
?r j = enc(embed(fj),?
?r j+1)rj = [?
?r j ;?
?r j ].Here the embed(?)
function maps the words into arepresentation (Bengio et al, 2003), and enc(?)
isa stacking long short term memory (LSTM) neuralnetwork (Hochreiter and Schmidhuber, 1997; Gerset al, 2000; Sutskever et al, 2014).
Finally we con-catenate the two vectors ?
?r j and?
?r j into a bidirec-tional representation rj .
These vectors are furtherconcatenated into the matrix R where the jth col-umn corresponds to rj .Next, we generate the output one word at a timewhile referencing this encoded input sentence andtracking progress with a decoder LSTM.
The de-coder?s hidden state hi is a fixed-length continuousvector representing the previous target words ei?11 ,initialized as h0 = 0.
Based on this hi, we calculatea similarity vector ?i, with each element equal to?i,j = sim(hi, rj).
(2)sim(?)
can be an arbitrary similarity function, whichwe set to the dot product, following Luong et al(2015a).
We then normalize this into an attentionvector, which weights the amount of focus that weput on each word in the source sentenceai = softmax(?i).
(3)This attention vector is then used to weight the en-coded representation R to create a context vector cifor the current time stepc = Ra.1558Finally, we create ?i by concatenating the previoushidden state hi?1 with the context vector, and per-forming an affine transform?i = W?
[hi?1; ci] + b?,Once we have this representation of the currentstate, we can calculate pm(ei|F, ei?11 ) according toEquation (1).
The next word ei is chosen accordingto this probability, and we update the hidden state byinputting the chosen word into the decoder LSTMhi = enc(embed(ei),hi?1).
(4)If we define all the parameters in this model as?, we can then train the model by minimizing thenegative log-likelihood of the training data??
= argmin??
?F, E??i?
log(pm(ei|F, ei?11 ; ?
)).3 Integrating Lexicons into NMTIn ?2 we described how traditional NMT modelscalculate the probability of the next target wordpm(ei|ei?11 , F ).
Our goal in this paper is to improvethe accuracy of this probability estimate by incorpo-rating information from discrete probabilistic lexi-cons.
We assume that we have a lexicon that, givena source word f , assigns a probability pl(e|f) to tar-get word e. For a source word f , this probability willgenerally be non-zero for a small number of transla-tion candidates, and zero for the majority of wordsin VE .
In this section, we first describe how we in-corporate these probabilities into NMT, and explainhow we actually obtain the pl(e|f) probabilities in?4.3.1 Converting Lexicon Probabilities intoConditioned Predictive ProabilitiesFirst, we need to convert lexical probabilities pl(e|f)for the individual words in the source sentenceF to a form that can be used together withpm(ei|ei?11 , F ).
Given input sentence F , we canconstruct a matrix in which each column corre-sponds to a word in the input sentence, each rowcorresponds to a word in the VE , and the entry cor-responds to the appropriate lexical probability:LF =??
?pl(e = 1|f1) ?
?
?
pl(e = 1|f|F |)... .
.
.
...pl(e = |Ve||f1) ?
?
?
pl(e = |Ve||f|F |)???
.This matrix can be precomputed during the encodingstage because it only requires information about thesource sentence F .Next we convert this matrix into a predictive prob-ability over the next word: pl(ei|F, ei?11 ).
To do sowe use the alignment probability a from Equation(3) to weight each column of the LF matrix:pl(ei|F, ei?11 ) = LFai =??
?pl(e = 1|f1) ?
?
?
plex(e = 1|f|F |)... .
.
.
...pl(e = Ve|f1) ?
?
?
plex(e = Ve|f|F |)?????
?ai,1...ai,|F |???
.This calculation is similar to the way how attentionalmodels calculate the context vector ci, but over avector representing the probabilities of the target vo-cabulary, instead of the distributed representationsof the source words.
The process of involving aiis important because at every time step i, the lexi-cal probability pl(ei|ei?11 , F ) will be influenced bydifferent source words.3.2 Combining Predictive ProbabilitiesAfter calculating the lexicon predictive proba-bility pl(ei|ei?11 , F ), next we need to integratethis probability with the NMT model probabilitypm(ei|ei?11 , F ).
To do so, we examine two methods:(1) adding it as a bias, and (2) linear interpolation.3.2.1 Model BiasIn our first bias method, we use pl(?)
to biasthe probability distribution calculated by the vanillaNMT model.
Specifically, we add a small constant ?to pl(?
), take the logarithm, and add this adjusted logprobability to the input of the softmax as follows:pb(ei|F, ei?11 ) = softmax(Ws?i + bs+log(pl(ei|F, ei?11 ) + ?
)).We take the logarithm of pl(?)
so that the values willstill be in the probability domain after the softmax iscalculated, and add the hyper-parameter ?
to preventzero probabilities from becoming ??
after takingthe log.
When ?
is small, the model will be moreheavily biased towards using the lexicon, and when?
is larger the lexicon probabilities will be given lessweight.
We use ?
= 0.001 for this paper.15593.2.2 Linear InterpolationWe also attempt to incorporate the two probabil-ities through linear interpolation between the stan-dard NMT probability model probability pm(?)
andthe lexicon probability pl(?).
We will call this thelinear method, and define it as follows:po(ei|F, ei?11 ) =??
?pl(ei = 1|F, ei?11 ) pm(e = 1|F, ei?11 )... ...pl(ei = |Ve||F, ei?11 ) pm(e = |Ve||F, ei?11 )???[?1?
?
],where ?
is an interpolation coefficient that is the re-sult of the sigmoid function ?
= sig(x) = 11+e?x .x is a learnable parameter, and the sigmoid func-tion ensures that the final interpolation level falls be-tween 0 and 1.
We choose x = 0 (?
= 0.5) at thebeginning of training.This notation is partly inspired by Allamanis etal.
(2016) and Gu et al (2016) who use linear inter-polation to merge a standard attentional model witha ?copy?
operator that copies a source word as-isinto the target sentence.
The main difference is thatthey use this to copy words into the output while ourmethod uses it to influence the probabilities of alltarget words.4 Constructing Lexicon ProbabilitiesIn the previous section, we have defined some waysto use predictive probabilities pl(ei|F, ei?11 ) basedon word-to-word lexical probabilities pl(e|f).
Next,we define three ways to construct these lexical prob-abilities using automatically learned lexicons, hand-made lexicons, or a combination of both.4.1 Automatically Learned LexiconsIn traditional SMT systems, lexical translation prob-abilities are generally learned directly from paralleldata in an unsupervised fashion using a model suchas the IBM models (Brown et al, 1993; Och andNey, 2003).
These models can be used to estimatethe alignments and lexical translation probabilitiespl(e|f) between the tokens of the two languages us-ing the expectation maximization (EM) algorithm.First in the expectation step, the algorithm esti-mates the expected count c(e|f).
In the maximiza-tion step, lexical probabilities are calculated by di-viding the expected count by all possible counts:pl,a(e|f) =c(f, e)?e?
c(f, e?
),The IBM models vary in level of refinement, withModel 1 relying solely on these lexical probabil-ities, and latter IBM models (Models 2, 3, 4, 5)introducing more sophisticated models of fertilityand relative alignment.
Even though IBM modelsalso occasionally have problems when dealing withthe rare words (e.g.
?garbage collecting?
effects(Liang et al, 2006)), traditional SMT systems gen-erally achieve better translation accuracies of low-frequency words than NMT systems (Sutskever etal., 2014), indicating that these problems are lessprominent than they are in NMT.Note that in many cases, NMT limits the targetvocabulary (Jean et al, 2015) for training speed ormemory constraints, resulting in rare words not be-ing covered by the NMT vocabulary VE .
Accord-ingly, we allocate the remaining probability assignedby the lexicon to the unknown word symbol ?unk?
:pl,a(e = ?unk?|f) = 1?
?i?Vepl,a(e = i|f).
(5)4.2 Manual LexiconsIn addition, for many language pairs, broad-coverage handmade dictionaries exist, and it is desir-able that we be able to use the information includedin them as well.
Unlike automatically learned lexi-cons, however, handmade dictionaries generally donot contain translation probabilities.
To constructthe probability pl(e|f), we define the set of trans-lations Kf existing in the dictionary for particularsource word f , and assume a uniform distributionover these words:pl,m(e|f) ={ 1|Kf | if e ?
Kf0 otherwise .Following Equation (5), unknown source words willassign their probability mass to the ?unk?
tag.4.3 Hybrid LexiconsHandmade lexicons have broad coverage of wordsbut their probabilities might not be as accurate as the1560Data Corpus Sentence TokensEn JaTrain BTEC 464K 3.60M 4.97MKFTT 377K 7.77M 8.04MDev BTEC 510 3.8K 5.3KKFTT 1160 24.3K 26.8KTest BTEC 508 3.8K 5.5KKFTT 1169 26.0K 28.4KTable 1: Corpus details.learned ones, particularly if the automatic lexicon isconstructed on in-domain data.
Thus, we also test ahybrid method where we use the handmade lexi-cons to complement the automatically learned lexi-con.2 3 Specifically, inspired by phrase table fill-upused in PBMT systems (Bisazza et al, 2011), weuse the probability of the automatically learned lex-icons pl,a by default, and fall back to the handmadelexicons pl,m only for uncovered words:pl,h(e|f) ={pl,a(e|f) if f is coveredpl,m(e|f) otherwise(6)5 Experiment & ResultIn this section, we describe experiments we use toevaluate our proposed methods.5.1 SettingsDataset: We perform experiments on two widely-used tasks for the English-to-Japanese languagepair: KFTT (Neubig, 2011) and BTEC (Kikui etal., 2003).
KFTT is a collection of Wikipedia articleabout city of Kyoto and BTEC is a travel conversa-tion corpus.
BTEC is an easier translation task thanKFTT, because KFTT covers a broader domain, hasa larger vocabulary of rare words, and has relativelylong sentences.
The details of each corpus are de-picted in Table 1.We tokenize English according to the Penn Tree-bank standard (Marcus et al, 1993) and lowercase,2Alternatively, we could imagine a method where we com-bined the training data and dictionary before training the wordalignments to create the lexicon.
We attempted this, and resultswere comparable to or worse than the fill-up method, so we usethe fill-up method for the remainder of the paper.3While most words in the Vf will be covered by the learnedlexicon, many words (13% in experiments) are still left uncov-ered due to alignment failures or other factors.and tokenize Japanese using KyTea (Neubig et al,2011).
We limit training sentence length up to 50in both experiments and keep the test data at theoriginal length.
We replace words of frequency lessthan a threshold u in both languages with the ?unk?symbol and exclude them from our vocabulary.
Wechoose u = 1 for BTEC and u = 3 for KFTT, re-sulting in |Vf | = 17.8k, |Ve| = 21.8k for BTEC and|Vf | = 48.2k, |Ve| = 49.1k for KFTT.NMT Systems: We build the described models us-ing the Chainer4 toolkit.
The depth of the stackingLSTM is d = 4 and hidden node size h = 800.We concatenate the forward and backward encod-ings (resulting in a 1600 dimension vector) and thenperform a linear transformation to 800 dimensions.We train the system using the Adam (Kingma andBa, 2014) optimization method with the default set-tings: ?
= 1e?3, ?1 = 0.9, ?2 = 0.999, ?
=1e?8.
Additionally, we add dropout (Srivastava etal., 2014) with drop rate r = 0.2 at the last layer ofeach stacking LSTM unit to prevent overfitting.
Weuse a batch size of B = 64 and we run a total ofN = 14 iterations for all data sets.
All of the ex-periments are conducted on a single GeForce GTXTITAN X GPU with a 12 GB memory cache.At test time, we use beam search with beam sizeb = 5.
We follow Luong et al (2015b) in replac-ing every unknown token at position i with the tar-get token that maximizes the probability pl,a(ei|fj).We choose source word fj according to the high-est alignment score in Equation (3).
This unknownword replacement is applied to both baseline andproposed systems.
Finally, because NMT modelstend to give higher probabilities to shorter sentences(Cho et al, 2014), we discount the probability of?EOS?
token by 10% to correct for this bias.Traditional SMT Systems: We also prepare twotraditional SMT systems for comparison: a PBMTsystem (Koehn et al, 2003) using Moses5 (Koehn etal., 2007), and a hierarchical phrase-based MT sys-tem (Chiang, 2007) using Travatar6 (Neubig, 2013),Systems are built using the default settings, withmodels trained on the training data, and weightstuned on the development data.Lexicons: We use a total of 3 lexicons for the4http://chainer.org/index.html5http://www.statmt.org/moses/6http://www.phontron.com/travatar/1561System BTEC KFTTBLEU NIST RECALL BLEU NIST RECALLpbmt 48.18 6.05 27.03 22.62 5.79 13.88hiero 52.27 6.34 24.32 22.54 5.82 12.83attn 48.31 5.98 17.39 20.86 5.15 17.68auto-bias 49.74?
6.11?
50.00 23.20?
5.59?
19.32hyb-bias 50.34?
6.10?
41.67 22.80?
5.55?
16.67Table 2: Accuracies for the baseline attentional NMT (attn) and the proposed bias-based method usingthe automatic (auto-bias) or hybrid (hyb-bias) dictionaries.
Bold indicates a gain over the attnbaseline, ?
indicates a significant increase at p < 0.05, and ?
indicates p < 0.10.
Traditional phrase-based(pbmt) and hierarchical phrase based (hiero) systems are shown for reference.proposed method, and apply bias and linearmethod for all of them, totaling 6 experiments.
Thefirst lexicon (auto) is built on the training datausing the automatically learned lexicon method of?4.1 separately for both the BTEC and KFTT ex-periments.
Automatic alignment is performed usingGIZA++ (Och and Ney, 2003).
The second lexicon(man) is built using the popular English-Japanesedictionary Eijiro7 with the manual lexicon methodof ?4.2.
Eijiro contains 104K distinct word-to-wordtranslation entries.
The third lexicon (hyb) is builtby combining the first and second lexicon with thehybrid method of ?4.3.Evaluation: We use standard single referenceBLEU-4 (Papineni et al, 2002) to evaluate the trans-lation performance.
Additionally, we also use NIST(Doddington, 2002), which is a measure that puts aparticular focus on low-frequency word strings, andthus is sensitive to the low-frequency words we arefocusing on in this paper.
We measure the statisticalsignificant differences between systems using pairedbootstrap resampling (Koehn, 2004) with 10,000 it-erations and measure statistical significance at thep < 0.05 and p < 0.10 levels.Additionally, we also calculate the recall of rarewords from the references.
We define ?rare words?as words that appear less than eight times in the tar-get training corpus or references, and measure thepercentage of time they are recovered by each trans-lation system.5.2 Effect of Integrating LexiconsIn this section, we first a detailed examination ofthe utility of the proposed bias method when used7http://eijiro.jp0 1000 2000 3000 4000time (minutes)5101520BLEUattnauto-biashyb-biasFigure 2: Training curves for the baseline attn andthe proposed bias method.with the auto or hyb lexicons, which empiricallygave the best results, and perform a comparisonamong the other lexicon integration methods in thefollowing section.
Table 2 shows the results of thesemethods, along with the corresponding baselines.First, compared to the baseline attn, our biasmethod achieved consistently higher scores on bothtest sets.
In particular, the gains on the more diffi-cult KFTT set are large, up to 2.3 BLEU, 0.44 NIST,and 30% Recall, demonstrating the utility of the pro-posed method in the face of more diverse contentand fewer high-frequency words.Compared to the traditional pbmt systemshiero, particularly on KFTT we can see that theproposed method allows the NMT system to exceedthe traditional SMT methods in BLEU.
This is de-spite the fact that we are not performing ensembling,which has proven to be essential to exceed tradi-tional systems in several previous works (Sutskever1562Input Do you have an opinion regarding extramarital affairs?Reference ??????????????
?Furin ni kanshite iken ga arimasu ka.attn ?????????????????Sakka?
ni kansuru iken wa arimasu ka.
(Do you have an opinion about soccer?
)auto-bias ??????????????
?Furin ni kanshite iken ga arimasu ka.
(Do you have an opinion about affairs?
)Input Could you put these fragile things in a safe place?Reference ??????????????????????
?Kono kowaremono o anzen?na basho ni oite moraemasen ka.attn ???????????????
?Kicho?-hin o anzen ni dashitai nodesuga.
(I?d like to safely put out these valuables.
)auto-bias ??????????????????????
?Kono kowaremono o anzen?na basho ni oite moraemasen ka.
(Could you put these fragile things in a safe place?
)Table 3: Examples where the proposed auto-bias improved over the baseline system attn.
Underlinesindicate words were mistaken in the baseline output but correct in the proposed model?s output.et al, 2014; Luong et al, 2015a; Sennrich et al,2016).
Interestingly, despite gains in BLEU, theNMT methods still fall behind in NIST score onthe KFTT data set, demonstrating that traditionalSMT systems still tend to have a small advantage intranslating lower-frequency words, despite the gainsmade by the proposed method.In Table 3, we show some illustrative exampleswhere the proposed method (auto-bias) was ableto obtain a correct translation while the normal at-tentional model was not.
The first example is amistake in translating ?extramarital affairs?
into theJapanese equivalent of ?soccer,?
entirely changingthe main topic of the sentence.
This is typical of theerrors that we have observed NMT systems make(the mistake from Figure 1 is also from attn, andwas fixed by our proposed method).
The second ex-ample demonstrates how these mistakes can then af-fect the process of choosing the remaining words,propagating the error through the whole sentence.Next, we examine the effect of the proposedmethod on the training time for each neural MTmethod, drawing training curves for the KFTT datain Figure 2.
Here we can see that the proposed biastraining methods achieve reasonable BLEU scoresin the upper 10s even after the first iteration.
In con-trast, the baseline attn method has a BLEU scoreof around 5 after the first iteration, and takes signifi-cantly longer to approach values close to its maximalFigure 3: Attention matrices for baseline attn andproposed bias methods.
Lighter colors indicatestronger attention between the words, and boxes sur-rounding words indicate the correct alignments.accuracy.
This shows that by incorporating lexicalprobabilities, we can effectively bootstrap the learn-ing of the NMT system, allowing it to approach anappropriate answer in a more timely fashion.8It is also interesting to examine the alignment vec-tors produced by the baseline and proposed meth-8Note that these gains are despite the fact that one iterationof the proposed method takes a longer (167 minutes for attnvs.
275 minutes for auto-bias) due to the necessity to cal-culate and use the lexical probability matrix for each sentence.It also takes an additional 297 minutes to train the lexicon withGIZA++, but this can be greatly reduced with more efficienttraining methods (Dyer et al, 2013).1563(a) BTECLexicon BLEU NISTbias linear bias linear- 48.31 5.98auto 49.74?
47.97 6.11 5.90man 49.08 51.04?
6.03?
6.14?hyb 50.34?
49.27 6.10?
5.94(b) KFTTLexicon BLEU NISTbias linear bias linear- 20.86 5.15auto 23.20?
18.19 5.59?
4.61man 20.78 20.88 5.12 5.11hyb 22.80?
20.33 5.55?
5.03Table 4: A comparison of the bias and linearlexicon integration methods on the automatic, man-ual, and hybrid lexicons.
The first line without lexi-con is the traditional attentional NMT.ods, a visualization of which we show in Figure3.
For this sentence, the outputs of both meth-ods were both identical and correct, but we cansee that the proposed method (right) placed sharperattention on the actual source word correspond-ing to content words in the target sentence.
Thistrend of peakier attention distributions in the pro-posed method held throughout the corpus, withthe per-word entropy of the attention vectors being3.23 bits for auto-bias, compared with 3.81 bitsfor attn, indicating that the auto-bias methodplaces more certainty in its attention decisions.5.3 Comparison of Integration MethodsFinally, we perform a full comparison between thevarious methods for integrating lexicons into thetranslation process, with results shown in Table 4.In general the bias method improves accuracy forthe auto and hyb lexicon, but is less effective forthe man lexicon.
This is likely due to the fact thatthe manual lexicon, despite having broad coverage,did not sufficiently cover target-domain words (cov-erage of unique words in the source vocabulary was35.3% and 9.7% for BTEC and KFTT respectively).Interestingly, the trend is reversed for thelinear method, with it improving man systems,but causing decreases when using the auto andhyb lexicons.
This indicates that the linearmethod is more suited for cases where the lexi-con does not closely match the target domain, andplays a more complementary role.
Compared tothe log-linear modeling of bias, which strictly en-forces constraints imposed by the lexicon distribu-tion (Klakow, 1998), linear interpolation is intu-itively more appropriate for integrating this type ofcomplimentary information.On the other hand, the performance of linear in-terpolation was generally lower than that of the biasmethod.
One potential reason for this is the fact thatwe use a constant interpolation coefficient that wasset fixed in every context.
Gu et al (2016) have re-cently developed methods to use the context infor-mation from the decoder to calculate the different in-terpolation coefficients for every decoding step, andit is possible that introducing these methods wouldimprove our results.6 Additional ExperimentsTo test whether the proposed method is useful onlarger data sets, we also performed follow-up ex-periments on the larger Japanese-English ASPECdataset (Nakazawa et al, 2016) that consist of 2million training examples, 63 million tokens, and81,000 vocabulary size.
We gained an improvementin BLEU score from 20.82 using the attn baselineto 22.66 using the auto-bias proposed method.This experiment shows that our method scales tolarger datasets.7 Related WorkFrom the beginning of work on NMT, unknownwords that do not exist in the system vocabularyhave been focused on as a weakness of these sys-tems.
Early methods to handle these unknown wordsreplaced them with appropriate words in the targetvocabulary (Jean et al, 2015; Luong et al, 2015b)according to a lexicon similar to the one used in thiswork.
In contrast to our work, these only handleunknown words and do not incorporate informationfrom the lexicon in the learning procedure.There have also been other approaches that incor-porate models that learn when to copy words as-isinto the target language (Allamanis et al, 2016; Guet al, 2016; Gu?lc?ehre et al, 2016).
These models1564are similar to the linear approach of ?3.2.2, butare only applicable to words that can be copied as-is into the target language.
In fact, these models canbe thought of as a subclass of the proposed approachthat use a lexicon that assigns a all its probability totarget words that are the same as the source.
On theother hand, while we are simply using a static in-terpolation coefficient ?, these works generally havea more sophisticated method for choosing the inter-polation between the standard and ?copy?
models.Incorporating these into our linear method is apromising avenue for future work.In additionMi et al (2016) have also recently pro-posed a similar approach by limiting the number ofvocabulary being predicted by each batch or sen-tence.
This vocabulary is made by considering theoriginal HMM alignments gathered from the train-ing corpus.
Basically, this method is a specific ver-sion of our bias method that gives some of the vocab-ulary a bias of negative infinity and all other vocab-ulary a uniform distribution.
Our method improvesover this by considering actual translation probabil-ities, and also considering the attention vector whendeciding how to combine these probabilities.Finally, there have been a number of recent worksthat improve accuracy of low-frequency words us-ing character-based translation models (Ling et al,2015; Costa-Jussa` and Fonollosa, 2016; Chung etal., 2016).
However, Luong and Manning (2016)have found that even when using character-basedmodels, incorporating information about words al-lows for gains in translation accuracy, and it is likelythat our lexicon-based method could result in im-provements in these hybrid systems as well.8 Conclusion & Future WorkIn this paper, we have proposed a method to in-corporate discrete probabilistic lexicons into NMTsystems to solve the difficulties that NMT systemshave demonstrated with low-frequency words.
Asa result, we achieved substantial increases in BLEU(2.0-2.3) and NIST (0.13-0.44) scores, and observedqualitative improvements in the translations of con-tent words.For future work, we are interested in conductingthe experiments on larger-scale translation tasks.
Wealso plan to do subjective evaluation, as we expectthat improvements in content word translation arecritical to subjective impressions of translation re-sults.
Finally, we are also interested in improve-ments to the linear method where ?
is calculatedbased on the context, instead of using a fixed value.AcknowledgmentWe thank Makoto Morishita and Yusuke Oda fortheir help in this project.
We also thank the facultymembers of AHC lab for their supports and sugges-tions.This work was supported by grants from the Min-istry of Education, Culture, Sport, Science, andTechnology of Japan and in part by JSPS KAKENHIGrant Number 16H05873.ReferencesMiltiadis Allamanis, Hao Peng, and Charles Sutton.2016.
A convolutional attention network for extremesummarization of source code.
In Proceedings of the33th International Conference on Machine Learning(ICML).Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proceedings of the4th International Conference on Learning Representa-tions (ICLR).Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,pages 1137?1155.Arianna Bisazza, Nick Ruiz, and Marcello Federico.2011.
Fill-up versus interpolation methods for phrase-based SMT adaptation.
In Proceedings of the 2011International Workshop on Spoken Language Transla-tion (IWSLT), pages 136?143.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, pages 263?311.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, pages 201?228.Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the propertiesof neural machine translation: Encoder?decoder ap-proaches.
In Proceedings of the Workshop on Syntaxand Structure in Statistical Translation (SSST), pages103?111.1565Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio.2016.
A character-level decoder without explicit seg-mentation for neural machine translation.
In Proceed-ings of the 54th Annual Meeting of the Association forComputational Linguistics (ACL), pages 1693?1703.Marta R. Costa-Jussa` and Jose?
A. R. Fonollosa.
2016.Character-based neural machine translation.
In Pro-ceedings of the 54th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 357?361.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the Second Interna-tional Conference on Human Language TechnologyResearch, pages 138?145.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameterizationof IBM model 2.
In Proceedings of the 2013 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 644?648.Felix A. Gers, Ju?rgen A. Schmidhuber, and Fred A. Cum-mins.
2000.
Learning to forget: Continual predictionwith LSTM.
Neural Computation, pages 2451?2471.Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li.2016.
Incorporating copying mechanism in sequence-to-sequence learning.
In Proceedings of the 54th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 1631?1640.C?aglar Gu?lc?ehre, Sungjin Ahn, Ramesh Nallapati,Bowen Zhou, and Yoshua Bengio.
2016.
Pointing theunknown words.
In Proceedings of the 54th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 140?149.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural Computation, pages1735?1780.Se?bastien Jean, KyungHyun Cho, Roland Memisevic,and Yoshua Bengio.
2015.
On using very large tar-get vocabulary for neural machine translation.
In Pro-ceedings of the 53th Annual Meeting of the Associa-tion for Computational Linguistics (ACL) and the 7thInternationali Joint Conference on Natural LanguageProcessing of the Asian Federation of Natural Lan-guage Processing, ACL 2015, July 26-31, 2015, Bei-jing, China, Volume 1: Long Papers, pages 1?10.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1700?1709.Gen-ichiro Kikui, Eiichiro Sumita, Toshiyuki Takezawa,and Seiichi Yamamoto.
2003.
Creating corpora forspeech-to-speech translation.
In 8th European Confer-ence on Speech Communication and Technology, EU-ROSPEECH 2003 - INTERSPEECH 2003, Geneva,Switzerland, September 1-4, 2003, pages 381?384.Diederik P. Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
CoRR.Dietrich Klakow.
1998.
Log-linear interpolation of lan-guage models.
In Proceedings of the 5th InternationalConference on Speech and Language Processing (IC-SLP).Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof the 2003 Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics (HLT-NAACL), pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics (ACL), pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the 2006 Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL), pages 104?111.Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W.Black.
2015.
Character-based neural machine transla-tion.
CoRR.Minh-Thang Luong and Christopher D. Manning.
2016.Achieving open vocabulary neural machine translationwith hybrid word-character models.
In Proceedings ofthe 54th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 1054?1063.Minh-Thang Luong, Hieu Pham, and Christopher D.Manning.
2015a.
Effective approaches to attention-based neural machine translation.
In Proceedings ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1412?1421.Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, OriolVinyals, and Wojciech Zaremba.
2015b.
Addressingthe rare word problem in neural machine translation.In Proceedings of the 53th Annual Meeting of the As-sociation for Computational Linguistics (ACL) and the7th Internationali Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing, ACL 2015, July 26-31, 2015,Beijing, China, Volume 1: Long Papers, pages 11?19.1566Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn treebank.
ComputationalLinguistics, pages 313?330.Haitao Mi, Zhiguo Wang, and Abe Ittycheriah.
2016.Vocabulary manipulation for neural machine transla-tion.
In Proceedings of the 54th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 124?129.Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-moto, Masao Utiyama, Eiichiro Sumita, Sadao Kuro-hashi, and Hitoshi Isahara.
2016.
Aspec: Asian scien-tific paper excerpt corpus.
In Proceedings of the NinthInternational Conference on Language Resources andEvaluation (LREC 2016), pages 2204?2208.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptableJapanese morphological analysis.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 529?533.Graham Neubig.
2011.
The Kyoto free translation task.http://www.phontron.com/kftt.Graham Neubig.
2013.
Travatar: A forest-to-string ma-chine translation engine based on tree transducers.
InProceedings of the 51th Annual Meeting of the Associ-ation for Computational Linguistics (ACL), pages 91?96.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, pages 19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 311?318.Rico Sennrich, Barry Haddow, and Alexandra Birch.2016.
Improving neural machine translation modelswith monolingual data.
In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 86?96.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
Journal of Machine Learning Re-search, pages 1929?1958.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.
Se-quence to sequence learning with neural networks.
InProceedings of the 28th Annual Conference on NeuralInformation Processing Systems (NIPS), pages 3104?3112.1567
