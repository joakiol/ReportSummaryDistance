Generating Referring Expressions:Making Referents Easy to IdentifyIvandre?
Paraboni?EACH, University of Sa?o PauloKees van Deemter?
?Computing Science Department,University of AberdeenJudith Masthoff?Computing Science Department,University of AberdeenIt is often desirable that referring expressions be chosen in such a way that their referents are easyto identify.
This article focuses on referring expressions in hierarchically structured domains,exploring the hypothesis that referring expressions can be improved by including logicallyredundant information in them if this leads to a significant reduction in the amount of searchthat is needed to identify the referent.
Generation algorithms are presented that implement thisidea by including logically redundant information into the generated expression, in certain well-circumscribed situations.
To test our hypotheses, and to assess the performance of our algorithms,two controlled experiments with human subjects were conducted.
The first experiment confirmsthat human judges have a preference for logically redundant expressions in the cases where ourmodel predicts this to be the case.
The second experiment suggests that readers benefit from thekind of logical redundancy that our algorithms produce, as measured in terms of the effort neededto identify the referent of the expression.1.
IntroductionCommon sense suggests that speakers and writers who want to get their message acrossshould make their utterances easy to understand.
Broadly speaking, this view is con-firmed by empirical research (Deutsch 1976; Mangold 1986; Levelt 1989; Sonnenschein1982, 1984; Clark 1992; Cremers 1996; Arts 2004).
The present article will examine itsconsequences for the generation of referring expressions (GRE).
In doing this, we dis-tinguish between two aspects of the ?understanding?
of a referring expression, whichwe shall denote by the terms interpretation and resolution.
We take interpretation tobe the process whereby a hearer/reader determines the meaning or logical form of the?
Av.Arlindo Bettio, 1000 - 03828-000, Sa?o Paulo, Brazil.
E-mail: ivandre@usp.br.??
King?s College, Meston building, Aberdeen AB24 3UE, Scotland, UK.
E-mail: kvdeemte@csd.abdn.ac.uk.?
King?s College, Meston building, Aberdeen AB24 3UE, Scotland, UK.
E-mail: jmasthoff@csd.abdn.ac.uk.Submission received: 17 February 2004; revised submission received: 27 July 2006; accepted for publication:7 December 2006.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 2referring expression; we take resolution to be the identification of the referent of theexpression once its meaning has been determined.
It is resolution that will take centerstage in our investigation.Difficulty of resolution and interpretation do not always go hand in hand.
Considersentences (1a)?
(1c), uttered somewhere in Brighton but not on Lewes Road.
The de-scription in (1a) is longer (and might take more time to read and interpret) than (1b), butthe additional material in (1a) makes resolution easier once interpretation is successfullycompleted.
(1a) 968 Lewes Road, Moulsecoomb area(1b) 968 Lewes Road(1c) number 968The first two of these descriptions refer uniquely.
As for the third: Lewes Road is a longstreet.
Supposing that other streets in Brighton do not have numbers above 900, theneven (1c) is a unique description?but a pretty useless one, because it does not help youto find the house unless your knowledge of Brighton is exceptional.
We will explore howa natural-language-generation (NLG) program should make use of logically redundantproperties so as to simplify resolution (i.e., the identification of the referent).
When wewrite about identifying or ?finding?
the referent of a referring expression, we mean thisin the sense of determining which object is the intended referent.
This conceptual goalmay or may not require the hearer to make a physical effort, for example by turning thepages of a book, or more dramatically by walking and waiting for traffic lights.The fact that referring expressions tend to contain logically redundant informationhas been observed in many empirical studies.
Levelt (1989), for example, mentionsthe need for redundancy in situations of ?degraded communication?
(e.g., backgroundnoise); and even in normal situations, redundant nondiscriminating information canhelp the addressee identify the referent (Deutsch 1976; Mangold 1986; Sonnenschein1982, 1984; Arts 2004).
In Levelt?s words, psycholinguistic experiments show that[l]isteners apparently create a ?gestalt?
of the object for which they have to search.
It isharder to search for ?something red?
than for ?a big red bird?, even if the color would besufficiently discriminating.
Information about the kind of object to be looked for (e.g., abird) is especially helpful for constructing such a gestalt.
(Levelt 1989, page 131)Although early GRE algorithms have often followed the Gricean maxim, ?be brief?
(Grice 1975), by minimizing the number of properties in a generated description, Daleand Reiter (1995) proposed an algorithm that allows certain redundancies, for example,by guaranteeing that each generated description expresses the ontological ?type?
of thereferent, in the form of a noun, a move that addresses Levelt?s claim to some extent.1In corpus-based studies, it has been shown that logically redundant properties tendto be included when their inclusion fulfils one of a number of pragmatic functions,such as to indicate that a property is of particular importance to the speaker (i.e., itconstitutes one of her reasons for being interested in the referent) or to highlight the1 Dale and Reiter (1995, Section 5) also mention the use of ?navigational?
(or ?attention-directing?
)information in referring expressions, which they distinguish from ?discrimination information,?
andwhose function appears to be to move the attention of the reader/hearer towards an object.
The conceptis not defined precisely and it is not clear how navigational information should be used in GRE.230Paraboni, van Deemter, and Masthoff Making Referents Easy to Identifyspeaker?s awareness that the referent has the property in question (Jordan 2000, 2002).Implementations of such findings in NLG are not difficult to envisage.The present article takes this reader-oriented perspective on the redundancy ofreferring expressions a step further, by asking how a generator can use logically re-dundant information to reduce the search space within which a reader has to ?find?
areferent; this will be specifically useful when referents need to be found in situationswhere the extensions of some of the properties are not known to the reader/hearerin advance (cf., Edmonds [1994] for a related set of problems) and where some effortmay be needed to identify the referent.
By focusing on the information needs of thehearer/reader, our work, a further development of Paraboni and van Deemter (2002a)that also takes the results of Paraboni, Masthoff, and van Deemter (2006) into account,addresses an issue that lies close to the heart of NLG as a practical enterprise, whosepurpose is, after all, to make information accessible to people.
These issues originallycame to the fore while studying references to parts of documents (Paraboni 2000, 2003;Paraboni and van Deemter 2002a, 2002b) but their relevance extends to many othersituations.
Our findings will also shed light on the egocentricity debate among psy-cholinguists about the extent to which speakers take hearer?s knowledge into accountwhen they speak (Keysar, Lin, and Barr 2003).
Throughout the article, we shall focuson issues of Content Determination (as opposed to, for example Lexical Choice), andon the situations in which individuals are first mentioned (as opposed to ones inwhich linguistic context allows them to be shortened [e.g., Krahmer and Theune 2002;Siddharthan and Copestake 2004]).2.
Ease of Resolution in the Incremental AlgorithmGeneration of referring expressions (GRE) is a key task of NLG systems (e.g., Reiter andDale 2000, Section 5.4).
An important aspect of GRE is to find combinations of propertiesthat allow the generator to refer uniquely to an entity, called the target.
Crucially, GREalgorithms only use properties whose denotations are part of the common knowledgeof writer and reader.2 These algorithms are typically designed in such a way that gener-ation is performed quickly (e.g., their worst-case running time tends to be linear [Daleand Reiter 1995; van Deemter 2002]) but the processing effort of the reader is not takeninto account.
Some algorithms do make a point of generating descriptions that are asbrief as possible (Dale 1989), and this can be argued to make interpretation easier.
As wehave seen, however, in relation to Examples (1a?c), brevity can make resolution difficult.For concreteness, let us focus on one of the best-known algorithms in this area.
TheIncremental Algorithm (Dale and Reiter 1995) starts by arranging attributes in a list, af-ter which they are considered one by one, to see if any of their values contributes some-thing to the description, by removing ?distractors?
(i.e., objects other than the referent);if an attribute (e.g., COLOR) can contribute something, then a suitable value (e.g., RED)for this attribute is selected as part of the description.
This is repeated incrementally un-til the logical conjunction of all selected attribute?value combinations results in a uniqueidentification of the referent.
There is no backtracking, and this is what keeps the com-plexity of the algorithm linear; it is also what causes the algorithm to sometimes expressa property P even when properties that are added later make P logically redundant.2 A good example of a description failing this requirement occurs in Get off one stop before I do, in anexchange between two people who have just met, as a description of where the hearer should get off thebus (Appelt 1985, cited in Dale and Reiter 1995).231Computational Linguistics Volume 33, Number 2Suppose a referring expression identifies its referent uniquely.
Then at least twothings can stand in the way of finding its referent: the ?difficulty?
of the individualproperties used in the description (i.e., the fact that it may be difficult to ascertain whichobjects have the property in question [Horacek 2005]), or the size and structure of thesearch space.
To exemplify the first factor, suppose you are queuing up for a concert andwant to explain to a friend that a girl further ahead in the queue has his ticket.
Color isan attribute that speakers like to use, even if it leads to logical redundancy (Pechmann1989).
This might be done by describing the referent as the girl in a yellow dress, or asthe girl with green eyes, for example.
But arguably, the first property contributes moretowards your friend?s search, because the color of a person?s eyes may not leap outat him from afar.
In the Incremental Algorithm, the fact that DRESS COLOR is moreuseful than EYE COLOR could be tackled by letting it precede EYE COLOR in the listof attributes.
As a consequence, EYE COLOR would only be considered if the referentcannot be identified uniquely without using a combination of more preferred attributes,including DRESS COLOR.
Arguably, this is exactly as it should be, and it shows much ofwhat is good about the Incremental Algorithm.
It is not so obvious, however, how thealgorithm should deal with the second of the two possible obstacles to resolution: thesize and structure of the domain.3.
Problems for ResolutionIn this section we shall introduce a class of domains (Section 3.1) and a class of problemsfor resolution that can arise when objects in these domains are identified using a distin-guishing description (Section 3.2).
Section 4 will relate these problems to a simple modelof the resolution process and propose a remedy, which consists of generating logicallyredundant descriptions (in two different ways).
Sections 5 and 6 provide examples ofputting our ideas to the test: first, by investigating what kind of description is preferredby subjects who are given the choice (Section 5); then, more elaborately, by investigatingthe effect of redundant descriptions on readers (Section 6).3.1 Hierarchical DomainsExisting work on GRE tends to focus on fairly simple domains, dominated by one-placeproperties.
When relations (i.e., two-place properties) are taken into account at all (e.g.,Dale and Haddock 1991; Krahmer and Theune 2002), the motivating examples are keptso small that it is reasonable to assume that speaker and hearer know all the relevantfacts in advance.
Consequently, search is not much of an issue (i.e., resolution is easy):The hearer can identify the referent by simply intersecting the denotations of the proper-ties in the description, for example, intersecting the set of girls with the set of individualswho wear a yellow dress (both in the domain).
Although such simplifications permitthe study of many aspects of reference, other aspects come to the fore when larger, andsubtly structured, domains are considered.Interesting questions arise, for example, when a large domain is hierarchicallyordered.
For the purpose of this article, we consider a domain to be hierarchicallyordered if its inhabitants can be structured like a tree in which everything that belongsto a given node n belongs to at most one of n?s children, and everything that belongsto one of n?s children belongs to n. Examples include countries divided into provinces,which, in turn, may be divided into regions, and so on; years into months, then intoweeks, and then into days; documents into chapters, then sections, then subsections;232Paraboni, van Deemter, and Masthoff Making Referents Easy to Identifybuildings into floors, then rooms.
Clearly, hierarchies are among our favorite ways ofstructuring the world.3A crucial question, in all such cases, is what knowledge is shared between speakerand hearer at utterance time.
Later on (most explicitly in Section 6), we shall focus onmore realistic situations but, to get the idea, it will be useful to think about the extremecase where, before the start of resolution (i.e., before consulting the ?knowledge in theworld,?
as opposed to the hearer?s ?knowledge in the head?
[Norman 1988]), the hearerknows nothing about the domain.
When the utterance is made, the hearer?s blindfold isremoved, so to speak, and resolution can start.
No similar assumption about the speakeris made: We assume that the speaker knows everything about the domain, and that heknows that the hearer can achieve the same knowledge.
Many of our examples willbe drawn from a simple model of a University campus, structured into buildings androoms; the intended referent will often be a library located in one of the rooms.
Thelocation of the library is not known to the hearer, but it is known to the speaker.Each domain entity r will be associated with a TYPE (e.g., the type ?room?
), and withsome additional attributes such as its ROOM NUMBER or NAME, and we will assumethat it is always possible to distinguish r from its siblings in the tree structure byusing one or more of these properties.
(For example, ROOM NUMBER = 120 identifiesa room uniquely within a given building; BUILDINGNAME = Watts identifies a buildingwithin the university.)
This is a useful assumption, because without it, the existence ofa distinguishing description cannot be guaranteed.The kinds of referring expression that we are interested in (see Section 5 for motiva-tion) take the form of a listL = ?
(x1, P1), (x2, P2) .
.
.
(xn, Pn)?where x1 = r is the referent of the referring expression and, for every j > 1, xj is anancestor (not necessarily the parent) of xj?1 in the domain D. For every j, Pj is a set ofproperties that jointly identify xj within xj+1 or, if j = n, within the whole domain.
Thereference the library in room 120 of Cockcroft building, for example, is modeled asL = ?
(r, {type = library}), (x2, {type = room, roomnumber = 120}), (x3, {type =building, buildingname = Cockcroft})?3.2 Obstacles for ResolutionWe have argued that generating a uniquely referring expression is not always enough,because such an expression can leave the hearer with an unnecessarily large searchspace.
But the issue is an even starker one, especially?as we shall soon see?whenit is taken into account that references in hierarchically structured domains can makeuse of the position of the speaker and hearer in the domain.
(For simplicity, we assumethat these two locations coincide.
)Let us start with some informal observations, to be corroborated in Section 4.
Sup-pose a hierarchically ordered domain D contains only one entity whose TYPE is LIBRARY.3 If everything that belongs to a given node n belongs to exactly one of n?s children, then nodes can bethought of as being partitioned by its children.
Note that this is not always the case.
Not everythingon a given floor of a building, for example, has to be in a room (the corridors are not).233Computational Linguistics Volume 33, Number 2Figure 1A hierarchically structured domain; d is where the reference is uttered.Consider the following noun phrases, uttered in the position marked by d in Figure 1.
(The first three have the same intended referent.
)(2a) the library, in room 120 in the Cockcroft building(2b) the library, in room 120(2c) the library(2d) room 140Utterances like Examples (2a) and (2b) make use of the hierarchical structure of thedomain.4 We focus on the search for xn (i.e., the highest hierarchical level referred toin the description) because, under the assumptions that were just made (in particularthe fact that xj be identified uniquely in xj+1 by the properties Pj), this is the only placewhere problems can be expected (because no parent node is available).Even though each of Examples (2a)?
(2d) succeeds in characterizing their intendedreferent uniquely, some of these descriptions can be problematic for the hearer.
Onetype of problem occurs in Example (2d).
The expression is logically sufficient (i.e., thereis only one room labeled 140 in the entire university).
But, intuitively speaking, the ex-pression creates an expectation that the referent may be found nearby, within theWatts building, whereas, in fact, a match can only be found in another building.
In acase like this, we will speak of Lack of Orientation (LO).
Even more confusion mightoccur if another library was added to our example, for instance in Watts 110, whereasthe intended referent was the other library (i.e., in room 120 Cockcroft).
In this case,Example (2c) would misfire, of course.
The expression (2b), however, would succeed, bymutually using two parts of the description (the library and room 120) to identify another:There are two libraries, and two rooms numbered 120, but there is only one pair (a, b)such that a is a library and b is a room numbered 120, with a located in b.
Such cases of4 Recall that we focus on Content Determination, bypassing issues to do with lexical choice, linguisticrealization, and so on.
For example, we shall not worry whether it is better to say (i) the library, in room120, (ii) the library in room 120 (without a comma), or (iii) the library (room 120).
The difference is not trivial,because (ii), for example, might be viewed as having the unwanted implicature that there is more thanone library in the Watts building (Robert Dale, personal communication, August 2005.
)234Paraboni, van Deemter, and Masthoff Making Referents Easy to Identifymutual identification5 are unproblematic in small, transparent, domains where search isnot an issue, but in large hierarchical domains, they are awkward (see the Conclusion).For, like Example (2d), (2b) would force a reader to search through an unnecessarilylarge part of the domain; worse even, the search ?path?
that the reader is likely to followleads via an obstacle (namely, room 120 Watts) that matches a part of the description, al-though not being the intended referent of the relevant part of the description (i.e., room120 Cockcroft).
Confusion could easily result.
For even if the reader eventually finds thelibrary, she has no simple way of knowing whether it is the right one.
(Perhaps a libraryin Watts 120 has been overlooked.)
In cases like this, we speak of a Dead End (DE).Suppose the domain D is represented as a finite tree whose nodes have attributesassociated with them, one of which is the TYPE attribute.
As before, we shall assumethat the attributes and values suffice to identify every node within its parent node.Before defining LO and DE more precisely, we describe the related notions of SCOPE andSCOPEGROUP, and the notion of a search path.
We write x ?
D to say that x is a node inthe tree D; if A is an attribute applicable to x then A(x) denotes the value of A for x.Scope: Suppose x ?
D, and A1, .
.
.
, An are attributes associated with x.Then SCOPE(x, {A1, .
.
.An}) is the largest subtree S of D such that x ?
Swhile, for every y, z ?
S, the conjunction A1(y) = A1(z) & .
.
.& An(y) =An(z) implies y = z.SCOPE(x, {A1, .
.
.An}) is the largest subtree of D in which the values for the attributesA1, .
.
.
An jointly succeed in pinning down the referent.
In practice, we shall usuallyfocus on situations where n = 1, in which case we shall write SCOPE(x, A1), omitting thebrackets.
In our University domain, let x be room 140 of Cockcroft, then SCOPE(x, ROOMNUMBER) is the subtree rooted in Cockcroft, because within Cockcroft, all room numbersare unique, whereas at the level of the entire university (the next level up), this is notthe case (even though the room number 140 itself happens to be unique at that level).The notion of SCOPE gives rise to the notion of SCOPEGROUP in a straightforwardway.
Assuming, once again, that x ?
D, and letting U stand for a set of attributesassociated with x, we define:SCOPEGROUP(x, U) = {y ?
D | y ?
SCOPE(x, U) & TYPE(x) = TYPE(y)}Thus, SCOPEGROUP (x, {A1, .
.
.An}) is the set of those elements of SCOPE(x, {A1, .
.
.An}) that are of the same TYPE as x.
Again, we shall focus on cases where n = 1, andomit brackets.
Thus, in the example domain, SCOPEGROUP(x,ROOM NUMBER), where xis any room in Cockcroft, is the set of all the rooms in Cockcroft.
TYPE is kept constant inthe definition of SCOPEGROUP because it tends to be the only non-structural attributethat is used to identify domain objects (i.e., the only attribute that is not intended fordesignating a node of the domain tree).6 Non-structural attributes will be assumed tobe unproblematic, operating like a filter on the set of possible referents.
For example,a reader of the description the library in room 110 will only be looking for libraries(although they might be looking for them in the wrong building).5 A well-known example is the description the bowl on the table, in a domain that contains several tables andseveral bowls, but only one bowl on a table (Dale and Haddock 1991).6 For example, we have seldom found descriptions like ?the section containing tables,?
?the italicizedsection?
in the PILs corpus (ABPI 1997).235Computational Linguistics Volume 33, Number 2We are now in a position to define DE and LO more precisely, relative to a searchpath.
A search path is a series of steps in the search for a referent, representingvisits to nodes in the domain tree D. The path will be modeled by an ordered listof visited nodes: O = ?n1, n2, .
.
.
nm?.
The node n1 is visited first, then n2, and so on,until either the referent is found (success) or the reader gives up (failure).
As before,let L = ?
(x1, P1), (x2, P2) .
.
.
(xn, Pn)?
model the semantic structure of the description, inwhich xn is the entity of highest hierarchical level referred to in L. Furthermore, letA1,.
.
.
,Aj be the set of all attributes in Pn.
Then we predict problems for resolutionto occur if some y occurs prior to xn in O, for which TYPE(xn) = TYPE(y) and y 	?SCOPEGROUP(xn, A1, .
.
.Aj).
Calling such y an obstacle, there are two types of obstacle:the obstacles for which all the properties in Pn are true (these are perhaps the worstkind, because they can be mistaken for the intended referent), and the ones for whichthis is not the case.
If only obstacles of the latter kind arise then we will speak ofLack of Orientation (LO).
If there is at least one obstacle of the former, more seriouskind, we will speak of Dead End (DE).
For example, in the case of the DE Exam-ple (2b) (the library in room 120), the description itself can be modeled as the list L =?
(r, {type = library}), (x2, {type = room, roomnumber = 120})?, where Pn is the propertyROOM NUMBER = 120 and xn is the room where the library is.
Suppose that the searchpath for xn corresponds to the following sequence (because referents are always foundin leaf nodes, other nodes appear in brackets):O = ?Watts100, (Watts, )Watts110, (Watts, )Watts120, (Watts, )(University, )(Cockcroft, )Cockcroft100, (Cockcroft, )Cockcroft120?Part of this sequence is the obstacle y = Watts 120, which is of the same TYPE as xn (i.e.,both are rooms), and which does not belong to SCOPEGROUP(xn, ROOM NUMBER) (i.e.,it does not belong to the Cockcroft building).Because the property Pn (ROOM NUMBER = 120) is true of y, this constitutes a caseof DE.
If the room Watts 120 is removed from the domain, there no longer exists anobstacle of the most serious kind (because there is only one room whose room numberis 120), but rooms 100 and 110 in the Watts building are obstacles of the less serious kind,making this an example of LO.It seems likely that DEs and LO can disrupt search in sufficiently large or complexdomain structures.
In principle, DE and LO could result even in the most unlikelyregions of the domain.
Suppose the cup on the table is uttered in a room d, whichcontains the intended referent.
Now suppose (rather perversely perhaps) the hearerstarted searching in another room, say the kitchen, before looking at the nearest table(in d).
If the kitchen happens to contain a table as well, and this table does not supportany cups, DE would result.
Search, however, seems unlikely to proceed in this way.To make testable predictions, we will make some assumptions concerning the way inwhich referring expressions are resolved by hearers.
To explain what these assumptionsare, let us return to the examples in Section 3, repeated here for convenience.
(2a) the library in room 120 in the Cockcroft building(2b) the library in room 120(2c) the library(2d) room 140236Paraboni, van Deemter, and Masthoff Making Referents Easy to IdentifyWe assume that these sentences are uttered in the University, say at the location d, andthat d determines the starting point of the search for a referent.
Henceforth the startingpoint s will be assumed to be the parent node of d. The intuition behind this assumptionis simple: When searching, start looking nearby.It will often be useful to assume that resolution adheres to a principle that we willcall Ancestral Search.
In formulating this principle, we will use d?
as a name for thereferring expression (which, as we know, takes place at location d); we will use Ref(d?
)as short for the intended referent of d?.Ancestral Search: First, search for Ref(d?)
in the subtree dominated by the starting points.
If Ref(d?)
is not found there then search for Ref(d?)
in the subtree dominated by theparent of s, which is called s?.
If Ref(d?)
is not found there then move up to the parent s?
?of s?,.
.
.
, and so forth, until the root is reached.
[If, at this point, Ref(d?)
is still not found,search fails.
]Ancestral Search (AS) says that the hearer of a referring expression searches exhaus-tively through the current search space (e.g., the building in which the expression isuttered, or the current document section containing the expression) before inspecting alarger subtree.
AS does not say how the search within each subtree (i.e., the one dominatedby s or s?)
is carried out.
We do not claim that readers always adhere exactly to AS,especially not when they are confronted with unusual situations (as we shall see in oursecond experiment).
Rather, AS can be seen as an ?ideal model,?
much like a straight linecould be seen as an ideal model of how a pedestrian walks from one point to another.We shall see later that AS makes surprisingly accurate predictions in terms of whatreferences are found difficult by readers.4.
Generation AlgorithmsWhat kinds of expression would existing GRE algorithms produce in the situations ofinterest?
Because hierarchies involve relations, the first algorithm that comes to mindis the one proposed by Dale and Haddock (1991).
Essentially, this algorithm combinesone- and two-place predicates, until a combination is found that pins down the targetreferent.
A standard example involves a domain containing two tables and two bowls,although only one of the two tables has a bowl on it.
In this situation, the combination{bowl(x), on(x, y), table(y)} identifies x (and y as well), because only one value of x canverify the three predicates, and this justifies the description the bowl on the table.
Nowconsider Figure 2, with one additional library in room 110 of the Watts building.
Herethe combination {library(x), in(x, y), room(y), roomnumber(y) = 120} identifies x (and ytoo), because no other library is located in a room with room number 120 (and noother room numbered 120 contains a library).
Thus, the standard approach to relationaldescriptions allows precisely the kinds of situation that we have described as DE.Henceforth, we shall describe this as the Minimal Description (MD) approach to refer-ence because, in the situations of interest, it uses the minimum number of properties bywhich the referent can be distinguished.Another option would be to treat a relation like ?being in room 120?
as a one-placeproperty of the library, and to use the Incremental Algorithm (Dale and Reiter 1995) togenerate the descriptions in question.
This, however, would not produce results that areinterestingly different from MD.
Suppose, for example, that the TYPE attribute is mostpreferred (i.e., considered first by the algorithm), with values such as ?library?, ?room?,and so on.
Suppose, furthermore, that the attribute ROOM NUMBER is preferred over237Computational Linguistics Volume 33, Number 2Figure 2A university campus with two libraries in different buildings.the attribute BUILDING NAME and, crucially, that a property such as ROOM NUMBER =x is interpreted as true of all those objects in the university (regardless in which building)that are located in something whose room number is x.
Then the Incremental Algorithmstarts selecting TYPE = library, followed by ROOM NUMBER = 120, at which stagea distinguishing description is reached.
In other words, the same description wouldbe generated by this algorithm as by Dale and Haddock (1991) and, once, again, theinfamous LO and DE would occur.
Choosing a preference order in which buildingnames are preferred over room numbers would produce the library in Cockcroft.
Al-though this description seems defensible in this case, it is easy to see that this preferenceorder would produce excessively lengthy descriptions in other situations.
No singlepreference order produces acceptable results in all cases.We will now sketch two GRE algorithms, both of which are guaranteed to preventDE and LO if AS holds.
(These algorithms will be investigated empirically in Sections 5and 6.)
They operate by reducing the reader?s search space, including logically redun-dant information into the descriptions that they generate.
These algorithms, called FullInclusion (FI) and Scope-Limited (SL), are not the only ways in which resolution maybe aided, but we will see that they represent two natural options.
Both take as input ahierarchical domain D, a location d where the referring expression will materialize, andan intended referent r. The output is a list of properties L to be turned into an Englishdescription by a language realization program.The first algorithm, FI, represents a straightforward way of reducing the lengthof search paths, without particular attention to LO or DE.
It lines up properties thatidentify the referent uniquely within its parent node, then moves up to identify thisparent node within its parent node, and so on until reaching a subtree that includesthe starting point d.7 FI may be likened to existing treatments of salience.
In Krahmerand Theune?s (2002) approach to GRE, for example, distractors that have lower saliencethan the intended referent do not have to be removed.
We apply this idea to hierarchicaldomains using the assumption that from the point d where the utterance was made allnodes within d?s parent node are as salient as d itself, while more ?distant?
nodes aregradually less salient.
As in Krahmer and Theune, salience sometimes allows for shorter7 ?Includes?
is taken to be reflexive: a includes b iff a is an ancestor of b or a = b.238Paraboni, van Deemter, and Masthoff Making Referents Easy to Identifydescriptions, as when room 110 replaces room 110 in Watts when said in Watts building(but outside room 110).Full Inclusion(r):L := ??
{ Initialize L as the empty list }FI.Identify(r)The function FI.Identify is defined recursively: (For simplicity, L does not contain theindividual referents x1,.
.
.
,xj, but only their properties.
)FI.Identify(X):L := L + P, where P identifies X uniquely within Parent(X)X := Parent(X)IF X includes d THEN STOP ELSE FI.Identify(X)Applied to our earlier example of a reference to room 120, FI first builds up the list L =?
(type = room, roomnumber = 120)?, then expands it to L = ?
(type = room, roomnumber =120), (buildingname = Cockcroft)?.
Now that Parent(X) includes d , r has been identifieduniquely within D and we reach STOP.
L might be realized as room 120 in Cockcroft, forexample.FI gives maximal weight to ease of resolution.
But something has to give, and thatis brevity: By conveying logically redundant information, descriptions are lengthened,and this can have drawbacks, most evidently when there are limitations of space ortime.
The second algorithm, called SL, constitutes a compromise between brevity andease of resolution.
SL prevents DE and LO but opts for brevity when DE and LO donot occur.
Put differently, SL favors ease of resolution when there is a risk of DE orLO, but ease of interpretation when there is no such risk.
This is done by making useof the notion of SCOPE, which was used in the definition of DE and LO.
It may berecalled that a description (x, P) in which P conveys attributes A1, .
.
.
Aj leads to DE orLO when its hearer comes across a node of the same type as x that is not a memberof SCOPEGROUP(x, {A1, .
.
.Aj}).
It follows that when the hearer is searching withinSCOPE(x, {A1, .
.
.Aj}), the description (x, P), even if minimally distinguishing, cannotlead to DE or LO.
Consequently, (x, P) can be uttered in any position d within the subtreedenoted by SCOPE(x, {A1, .
.
.Aj}) with no risk of leading to DE or LO situations.
In otherwords, if SCOPE(x, {A1, .
.
.An}) contains d, and if A1, .
.
.An are the attributes conveyedin a description (x, P), then this description does not lead to DE or LO.
This allows SL touse logically redundant properties more sparingly:Scope-Limited(r):L := ??
{ Initialize L as the empty list }SL.Identify(r)SL.Identify(X):L := L + P, where P identifies X uniquely within Parent(X)239Computational Linguistics Volume 33, Number 2X := Root(Scope(X, {A1, .
.
.
, Aj})), where A1, .
.
.
, Aj are the attributesassociated with PIF X includes d THEN STOP ELSE SL.Identify(X)Whereas FI only terminates the generation of the description when a node that includesd is reached, SL concludes potentially much earlier, when an attribute (or a combinationof attributes) is used that is guaranteed to identify all objects of the relevant typeuniquely throughout a tree that includes d. By taking scope into account, SL avoidsthe inclusion of any hierarchical levels not strictly required for preventing DE and LO.Consider a description uttered in the position d = room 100 of Watts, with r =room 140 (in Cockcroft) as the intended referent.
Existing GRE approaches such as Daleand Reiter (1995) would tend to produce a minimally distinguishing description suchas room 140, causing LO.
SL, by contrast, would produce the description room 140 inCockcroft,8 which in this case is the same description produced by FI.
The differencebetween FI and SL becomes evident when we consider a case in which the minimallydistinguishing description does not lead to DE/LO?that is, when AS predicts thatthe reader will meet no DE or LO obstacles.
For example, let?s return to the situationdepicted in Figure 1, from Section 3.1, where there is only one library in the wholeuniversity.
A reference to r = library would be realized by FI as the library in room 120 inCockcroft.
By using SL, however, the same description would be realized simply as thelibrary, because the SCOPE of the attribute TYPE is the whole domain tree [more precisely,SCOPE(LIBRARY,ROOM NUMBER)= D] because there is only one entity of TYPE ?library?in the domain and hence no other properties are added.
Note that the addition of asecond library in the Watts building would reduce SCOPE(r,TYPE) to the subtree rootedin the ?building?
node (i.e., each library would be defined by the building to which itbelongs).
The behavior of the SL algorithm would change accordingly, producing thelibrary in Cockcroft.
Similarly, had we instead included the second library under anotherroom of Cockcroft, the SCOPE would have been reduced even further, causing SL todescribe r as the library in room 120 of Cockcroft, just like the FI algorithm.5.
First Experiment: Measuring Reader?s PreferencesIn this section we start putting the intuition that LO and DE are better avoided to thetest.
We report on a small experiment with human subjects, which involved a documentstructured in sections and subsections as an example of a hierarchically ordered domain.We chose this domain because, unlike most other domains, it allows us to show subjectsthe domain itself (i.e., a real document), rather than, for example, a pictorial represen-tation of it.
More specifically, we investigated the choice of so-called document-deicticreferences, such as the picture in part x of section y (Paraboni 2003), to check whetherthey avoid potential DE and LO situations by adding logically redundant properties(favoring ease of resolution) and, conversely, whether they choose shorter descriptionswhen there is no such risk (favoring ease of interpretation).8 The reason is that Root(Scope(r,ROOM NUMBER)) = Cockcroft, which does not include d. This causes thealgorithm to have to identify the Cockcroft building before the algorithm stops.240Paraboni, van Deemter, and Masthoff Making Referents Easy to Identify5.1 Experiment DesignSubjects.
15 academics with considerable practice in the authoring of papers on compu-tational linguistics.Procedure.
A within-subjects design was used.
All subjects were shown a printed doc-ument containing 18 incomplete statements.
Subjects were asked to put themselves inthe shoes of the author and to choose the description that they found more suitable foreach situation:Suppose you and a colleague are currently collaborating on this document.
Fortunatelyhe/she did almost all the work for you, and now all that you have to do is completecertain parts of the existing text [.
.
.
]Subjects completed the statements by choosing one of two alternatives provided: one?minimally distinguishing?
description and the other conveying logical redundancy(corresponding to the output of the FI or SL algorithms).
Both alternatives are un-ambiguous references to the same object.
Figure 3 shows a number of descriptionsof this kind (whose intended referents are elsewhere in the document) and objects(referred to by descriptions elsewhere).
Statement 11 gives a choice between a logicallyredundant description as generated by FI or SL (Part C of Section 2) and its minimallydistinguishing alternative Part C. Both alternatives are unambiguous because there isonly one part labeled as ?C?
in the document, but the shorter one may potentially leadto LO because the current document section does not contain a part labeled as ?C?.Similarly, Statement 12 gives a choice between a minimally distinguishing descriptionas generated by MD or SL, and a logically redundant alternative as generated byFI, but in this case none of the alternatives can lead to DE or LO because there isonly one ?table 2?
in the entire document.
The presentational order of alternativesFigure 3Fragment of the document used in the experiment.241Computational Linguistics Volume 33, Number 2Table 1Situations of reference for Experiment 1.Sit.
Type Reader Loc.
Referent Loc.
MD Redundant2 DE Part A Sec 1 Part B Sec 3 Pic 2 in Part B Pic 2 in Part B Sec 39 DE Part C Sec 2 Part B Sec 3 Pic 3 in Part B Pic 3 in Part B Sec 313 DE Part B Sec 3 Part A Sec 2 Pic 4 in Part A Pic 4 in Part A Sec 215 DE Part B Sec 3 Part A Sec 2 Pic 3 in Part A Pic 3 in Part A Sec 25 LO Part B Sec 1 Part A Sec 2 Pic 5 Pic 5 in Part A Sec 27 LO Part B Sec 1 Part C Sec 2 Part C Part C Sec 211 LO Part A Sec 3 Part C Sec 2 Part C Part C Sec 216 LO Part B Sec 3 Part A Sec 2 Pic 6 Pic 6 in Part A Sec 24 NONE Part A Sec 1 Part B Sec 3 Table 6 Table 6 in Part B Sec 310 NONE Part C Sec 2 Part A Sec 3 Table 5 Table 5 in Part A Sec 312 NONE Part A Sec 3 Part B Sec 2 Table 2 Table 2 in Part B Sec 218 NONE Part B Sec 3 Part A Sec 1 Table 1 Table 1 in Part A Sec 1(i.e., short versus redundant descriptions) was evenly distributed, to control for ordereffects.Research questions.
We were interested in seeing whether readers prefer longer (i.e.,logically redundant) descriptions when there is a risk of DE or LO and, conversely,whether they prefer minimally distinguishing descriptions when there is no such risk.Table 1 shows the type of situation (potential DE, LO, and non-problematic), the readerand referent location, and the descriptions used.
To break the monotony of the taskand to disguise the purpose of the experiment, another six situations were used thatwere not relevant to the experiment.
Half of the situations, in each of the types,involved backward references, the other half involved forward references.
Pictures wereenumerated per part so that we could compare short and long versions of potentiallyproblematic descriptions (e.g., Picture 5 in which the intended referent is not in thecurrent document part, which may or may not contain other pictures).
Within the LOsituations, two of the four statements involved references to pictures, whereas the othertwo involved references to sections.
This was done in order to test whether the type ofreferent had any influence on the choices made by the subjects.
All the questions relatedto potential DE situations involved references to pictures, because using DE referencesto sections would have led to highly artificial structures.Hypothesis 1.1: In a problematic DE situation, descriptions generated by FI or SL arepreferred over minimally distinguishing (MD) descriptions.We will use the DE situations in Table 1 to test this hypothesis, investigating how oftensubjects prefer FI/SL descriptions to MD ones.Hypothesis 1.2: In a problematic LO situation, descriptions generated by FI or SL arepreferred over minimally distinguishing (MD) descriptions.We will use the LO situations in Table 1 to test this hypothesis, investigating how oftensubjects prefer FI/SL descriptions to MD ones.
Note that in problematic situations, SLgenerates the same descriptions as FI.242Paraboni, van Deemter, and Masthoff Making Referents Easy to IdentifyWe also wanted to investigate whether subjects would prefer descriptions gen-erated by FI or SL in non-problematic situations (i.e., those not involving potentialDE or LO).
We did not use pictures as we did in the problematic cases because inthese cases both FI and SL would produce the same descriptions (e.g., Picture 5).9In order to compare these algorithms in non-problematic situations we used tablesenumerated throughout the document, in which case descriptions produced by SL areshort (e.g., Table 5) and descriptions produced by FI are longer (e.g., Table 5 in Part C ofSection 2).Hypothesis 1.3: In a non-problematic situation (i.e., a situation not involving DE orLO), SL or MD descriptions are preferred over those generated by FI.We will use the non-problematic situations in Table 1 to test this hypothesis, investigat-ing how often subjects prefer FI descriptions to MD/SL ones.
Note that in these non-problematic situations, SL generates the same descriptions as MD.Hypotheses 1.1 and 1.2 investigate whether ease of resolution (as in logically re-dundant descriptions generated by FI or SL) is favored over ease of interpretation (asin minimally distinguishing descriptions) when the description may lead to DE or LO.Hypothesis 1.3 investigates whether ease of interpretation (as in MD or SL descriptions)is favored over ease of resolution (as in descriptions generated by FI) when the formerdoes not lead to DE or LO situations.Materials.
DEs and LO can only occur in fairly complex domains.
Instead of trying tofind a large number of such documents, we made use of a specially designed schematicdocument.
The document was presented in a printed version (3 pages long), dividedinto sections (1?3) and subsections (?A?
and ?B?
); Section 2 contained also a subsectionlabelled ?C?.10References to pictures can be realized in many different ways.
For example, thereferent can be called Picture or Figure or just Fig.
; the reference can be constructed fromthe bottom up (Picture 3 in Section 4) or from the top downwards (Section 4, Picture 3);punctuation varies as well, as does the use of capitals.
In our experiments, we havemade one fairly arbitrary choice from among all these possibilities, motivated by thetypes of reference that we observed most frequently in an informal study of a collectionof patient information leaflets from the PILs corpus (ABPI 1997): We always used theword Picture, we constructed the references bottom up (going up one level at a time),and never used commas or semicolons.
Thus, for example, we asked subjects to comparePicture 3 in Part B of Section 3 with Picture 3 in Part B.11 Even though it is possible that adifferent realization choice would produce different experimental outcomes, this doesnot seem likely.Every description d and its referent r were always on different pages.
Had d and roccurred on the same page then physical proximity might have obscured navigational9 In the second experiment reported in Section 6 this was no longer an issue as we focus on ease ofresolution only, that is, it did not compare FI with SL.10 See Paraboni (2003), appendix 1, for the actual document.11 To get a feeling for the frequency of the expressions involved, one might enter ?picture OR figure OR fig1.
.
.
9 in part OR section 1. .
.
9?
into Google, using Advanced Search.
In July 2006, this produced as many as77, 000 hits, the great majority of which are of the intended kind.
(Because Advanced Search disregardspunctuation and capitalization, this includes a very small percentage of false positives, for example of theform ?Figure x.
In section y .
.
.
?.)
The materials of our second experiment (Section 6) were essentially thesame as the present ones, except for the use of capitals.243Computational Linguistics Volume 33, Number 2issues, causing a bias towards the shortest alternative.
Reference d and referent r werealways in document parts whose layout properties differed from each other (e.g., notboth in subsections labeled as ?C?
in different sections of the document).
Had d and roccurred in document parts with similar layout properties, there might have been a biastowards the most complete (i.e., the longest) description.5.2 ResultsHypotheses 1.1 and 1.2 were confirmed; hypothesis 1.3 was not.
In fact, DE wasavoided in 100% of all subjects?
decisions.
In situations involving LO, the FI versionwas chosen on average in 93% of cases (stdev = 15%), which is highly significant(Wilcoxon signed ranks test, Z = ?3.56, p < .0001).
In the cases not involving DE orLO, there was no significant preference for or against logical redundancy (Wilcoxonsigned ranks test Z = ?0.51, p = .61).
The trend is in the predicted direction (meanof 57% for MD descriptions), but the variation between subjects was very large(stdev = 41%).5.3 Discussion of First ExperimentThis first experiment supported the hypothesis that subjects prefer references thatinclude logically redundant information where there is a risk of DE/LO.
Arguably,it is precisely this kind of information that is needed for the construction of NLGalgorithms.
Where logically redundant information does not make the referent easierto identify, the results of the experiment are less clear, with the subjects being dividedbetween logically minimal and logically redundant descriptions.
In other words, whilesupporting the informal observations reported in Sections 2 and 3, the experimentdoes not point to a generic preference of one of the two GRE algorithms presented inSection 4.Evidently, there are many factors that this experiment did not address, such asthe ?distance?
between objects.
For example, if tables are enumerated throughout thedocument, is the brief, SL-type description Table 5 easy enough to resolve?
It depends:If there are tables on virtually every page then resolution is easy, because the tablenumbers support browsing not unlike page numbers; if tables are sparse, however,then searching through the entire document may take unacceptably long, and a moreredundant, FI-type description such as Table 5 in Section 4.3 is likely to be preferred.
Thenature of the domain is bound to matter as well.
For example, in a large spatial domainin which navigation requires physical effort, short, SL-style descriptions are probablyless acceptable than in a situation where the domain can be surveyed at a glance.
Toexemplify the first type of situation, let us return briefly to Examples (1a)?
(1c), assumingthat a city is divided into areas, and an area into streets:(1a) 968 Lewes Road, Moulsecoomb area (FI-style)(1b) 968 Lewes Road (SL-style)(1c) number 968 (MD-style)If these are uttered somewhere in Brighton but not on Lewes Road then AS predictsthat utterance (1c) leads to LO, because the hearer will start looking for a number 968 inthe street where the description is uttered.
Consequently, utterance (1c) is infelicitous244Paraboni, van Deemter, and Masthoff Making Referents Easy to Identifyanywhere except on Lewes Road.
But how about Examples (1a) and (1b)?
Both descrip-tions avoid LO and DE, because Brighton has only one Lewes Road.
Yet if the hearerdoes not know that Lewes Road is in Moulsecoomb, then the resolution of Example (1b)may involve more work than Example (1a).This experiment attempted to find out what types of references are favored byhuman judges when their opinion about these references is asked.
Although this has theadvantage that subjects were in a position to make trade-offs between the advantagesand disadvantages of the different expressions (perhaps balancing ease of interpreta-tion with ease of resolution), the method is limited in other respects.
One limitationarises from the fact that meta-linguistic judgments are sometimes thought to be anunreliable predictor of people?s linguistic behavior (e.g., van Deemter 2004).
Perhapsmore seriously, the experiment fails to tell us how difficult a given type of reference (forexample, one of the DE type) would actually be for a reader, and whether the difficultyis a matter of interpretation or resolution.
For these reasons, we decided to performanother experiment.6.
Second Experiment: Measuring Search EffortIn the previous experiment, we found that human authors often prefer logically redun-dant references, particularly when DE and LO can arise.
In a follow-up experiment,we investigate the effect of logical redundancy on the performance of readers.
Weare primarily interested in understanding the search process, so resolution rather thaninterpretation.
It will become clear that the new experiment necessitates a more carefuldesign and a more complex analysis than the previous one.6.1 Experiment DesignSubjects.
Forty-two students on a first-year Computing Science course participated inthe experiment as part of a scheduled practical.Procedure.
A within-subjects design was used.
All subjects were shown 20 on-linedocuments.
The order of the documents was randomized per subject, to control fororder effects.
The document structure was always visible, and so was the content ofthe current document part.
A screenshot of an example document providing this levelof information is shown in Figure 4.
Each document was initially opened in Part Bof either Section 2 or 3, where a task was given of the form ?Let?s talk about [topic].Please click on [referring expression];?
for instance: Let?s talk about elephants.
Please clickon picture 5 in part A.
Subjects could navigate through the document by clicking on thenames of the parts (e.g.
Part A as visible under Section 3).
As soon as the subject hadcorrectly clicked on the picture indicated, the next document was presented.
Subjectswere reminded throughout the document about the task to be accomplished, and thelocation at which the task was given.
All navigation actions were recorded.
At the startof the experiment, subjects were instructed to try to accomplish the task with a minimalnumber of navigation actions.Reader?s Knowledge.
We assume that readers do not have complete knowledge of thedomain.
So, they do not know which pictures are present in each part of each section.If readers had complete knowledge, then a minimal description would suffice: Forexample, if readers knew that there is only one picture 5 in the document, located in245Computational Linguistics Volume 33, Number 2Figure 4Fragment of the experiment interface.part B of section 3, then the description picture 5 would probably be completely clear.We do not, however, assume readers to be completely ignorant, either.
We assume thatthey have some knowledge of the domain, particularly of its hierarchical structure.
Thisbrings us to the question of how much knowledge we should assume our readers tohave.
In practice (unlike Section 3.1, where the hearer was pictured as blindfolded untilthe description is uttered) readers will always have some knowledge: If in Part B ofSection 2, then they would know (by convention) that there will also be a Section 1, anda Part A in Section 2, and so on.
It is also likely that being in Part B of Section 2 andseeing pictures 1, 2, 3, readers will infer that sections can have parts, that parts cancontain pictures, and that pictures are numbered (though not necessarily per part).Because of these kinds of consideration, it seems appropriate to give our readersknowledge about the entire document structure (the 5 sections and their parts) andthe content (i.e., the existing pictures) in the current document part (but crucially, noknowledge about pictures elsewhere in the document, which require navigation to bediscovered).
A navigation structure like the one in Figure 4 provides this knowledge tothe readers.Research Questions.
We want to test whether longer descriptions indeed help resolution,particularly in so-called problematic situations.
Table 2 shows the types of situation(potential DE, LO, and non-problematic),12 reader and referent location, and descrip-tions used.Hypothesis 2.1: In a problematic (DE/LO) situation, the number of navigation actionsrequired for a long (FI/SL) description is smaller than that required for a short (MD)description.12 In DE situations, there is another picture with the same number as the referent, but not in a part with thesame name as the part in which the referent is.
In LO situations, there is no other picture with the samenumber as the referent, and the reader location contains pictures.
In non-problematic situations, there isanother picture with the same number as the referent, but not in a part with the same name as the part inwhich the referent is.246Paraboni, van Deemter, and Masthoff Making Referents Easy to IdentifyTable 2Situations of reference for Experiment 2.Sit.
Type Reader Loc.
Referent Loc.
Short (MD) Long (FI/SL) Long (other)1 DE Part B Part A Pic 3 in Pic 3 in Part ASec 3 Sec 2 Part A Sec 22 DE Part B Part C Pic 4 in Pic 4 in Part CSec 2 Sec 3 Part C Sec 33 LO Part B Part A Pic 5 Pic 5 in Part A Pic 5 in Part ASec 3 Sec 3 Sec 34 LO Part B Part C Pic 4 Pic 4 in Part C Pic 4 in Part CSec 2 Sec 2 Sec 25 LO Part B Part A Pic 5 Pic 5 in Part A Pic 5 in Part ASec 3 Sec 4 Sec 46 LO Part B Part C Pic 4 Pic 4 in Part C Pic 4 in Part CSec 2 Sec 1 Sec 17 NONE Part B Part A Pic 3 in Pic 3 in Part ASec 2 Sec 2 Part A Sec 28 NONE Part B Part C Pic 4 in Pic 4 in Part CSec 3 Sec 3 Part C Sec 3This hypothesis is similar to hypotheses 1.1 and 1.2 of the previous experiment.
Wewill use the DE and LO situations in Table 2 to test this hypothesis, comparing for eachsituation the number of navigation actions of the short, that is, minimally distinguishing(MD) and long (FI/SL) expressions.In the previous experiment, we had an additional hypothesis about non-problematic situations, stating that MD descriptions would be preferred to long de-scriptions in non-problematic situations.
This is not a natural hypothesis in the newexperiment, because it might not happen very often that a shorter description will leadto fewer navigation actions (pace Cremers 1996).
(Note that in the previous experimentwe looked at the combination of interpretation and resolution, whereas we are nowfocusing on resolution only).
Instead, we will look at gain: the number of navigationactions required for a short description minus the number of navigation actions requiredfor a long description.For situation s, short description sd of s, and long description ld of s, Gain(s, sd, ld) = thenumber of navigation actions required in s for description sd minus the number ofnavigation actions required in s for description ld.Hypothesis 2.2: The gain achieved by a long description over an MD description willbe larger in a problematic situation than in a non-problematic situation, that is, forproblematic situation ps, non-problematic situation nps, MD description md of both psand nps, and long description ld of ps and nps: Gain(ps, md, ld) > Gain(nps, md, ld).We will use the DE and non-problematic situations in Table 2 to test this hypothesis,comparing the gain of situation 1 with that of situation 7, and the gain of situation 2with that of situation 8.Longer descriptions may always lead to fewer navigation actions, and it can beexpected that complete descriptions of the form picture x in part y of section z willoutperform shorter descriptions in any situation.
So, from a resolution point of view,an algorithm that would always give a complete description may produce better results247Computational Linguistics Volume 33, Number 2than the algorithms we proposed (e.g., situations 3 and 4 in Table 2).
The aim of ouralgorithms is to make the descriptions complete enough to prevent DE and LO inresolution, but not overly redundant as this may affect interpretation.
We would liketo show that the decisions taken by FI and SL are sensible, that is, that they producedescriptions that are neither too short nor too long.
Therefore:S1: We want to consider situations in which FI and SL have produced an incompletedescription, and investigate how much gain could have been made by using a completedescription in those cases.
We would like this gain to be negligible.
We will usesituations 3 and 4 for this, calculating the gain of the long, complete descriptions(namely, long (other) in Table 2) over the shorter, incomplete descriptions generated byour algorithms (long (FI/SL) in Table 2).S2: We want to consider situations in which FI and SL have produced a completedescription, and investigate how much gain has been made by using this compared to aless complete description that is still more complete than MD.
We would like this gainto be large.
We will use situations 5 and 6 for this, calculating the gain of the longcomplete descriptions generated by our algorithms (long (FI/SL) in Table 2) over theless complete descriptions (long (other)).Introducing separate hypotheses for cases S1 and S2 poses the problem of definingwhen a gain is ?negligible?
and when a gain is ?large.?
Instead, we will compare thegain achieved in S1 with the gain achieved in S2, expecting that the gain in S2 (which webelieve to be large) will be larger than the gain in S1 (which we believe to be negligible).Hypothesis 2.3: The gain of a complete description over a less complete one will belarger for situations in which FI and SL generated the complete one, than for situationsin which they generated the less complete one.
More formally, for situations S1 and S2,descriptions cd and ld, with cd a complete description of S1 and S2 that has beengenerated by FI and SL for S2, and with ld an incomplete but longer-than-MDdescription of S1 and S2 that has been generated by FI and SL for S1:Gain(S1, ld, cd) < Gain(S2, ld, cd).Materials.
Twenty on-line documents were produced,13 with the same document struc-ture (sections 1 to 5 with parts A to C) and containing 10 pictures.
Documents hada unique background color, title, and pictures appropriate for the title.
The numberof pictures in a section or part varied per document.
All of this was done to preventsubjects relying on memory.
For instance, if we had used the same document for alltasks, subjects might have remembered where a particular picture was located.
If we hadused documents that looked similar, subjects might have assumed that they were thesame.
If we had kept the distribution of images the same, subjects might have learnedthat a particular part always contained many pictures.Controlled experiments have advantages and disadvantages.
Instead of using arti-ficial, hand-crafted materials, we could have used real-world documents, like patientinformation leaflets, in order to make the tasks as realistic as possible.
However, itwould have been extremely difficult to find real-world documents that contain theright phenomena in a well-balanced way.
Firstly, real documents might not have theright descriptions in them, so we would probably have needed to change sentencesin the documents by hand.
Secondly, we need a set of documents that are sufficiently13 http://www.csd.abdn.ac.uk/?jmasthof/RefStudy/Intro.php.248Paraboni, van Deemter, and Masthoff Making Referents Easy to IdentifyTable 3Number of clicks used to complete the tasks.Short Long (FI/SL) Long (Other)Sit.
Type Mean STDEV Mean STDEV Mean STDEV1 DE 3.58 2.14 1.10 0.502 DE 3.85 3.28 1.30 1.313 LO 5.60 4.84 1.93 1.29 1.23 1.274 LO 2.50 1.97 1.60 1.28 1.38 2.075 LO 8.53 4.15 1.15 0.53 5.65 6.746 LO 7.38 5.49 1.25 1.03 4.08 2.357 NONE 1.58 0.98 1.63 2.618 NONE 1.48 0.96 1.05 0.32similar in structure that one can make a fair comparison between longer and shorterdescriptions.
Moreover, the structure should not allow subjects to learn where in thedocument pictures are most likely to be located.
Thirdly, semantic information or theirbackground knowledge of the domain should be irrelevant.
(For example, if we wereusing a real document on animals, and subjects read a section on lions, then they mightexpect a picture of a tiger to be in a nearby section, and a picture of an elephant to becloser than a picture of a pigeon.
)6.2 ResultsForty subjects completed the experiment.
Table 3 shows descriptive statistics for thenumber of clicks subjects made to complete each task.
To analyze the results with respectto Hypothesis 2.1, we used a General Linear Model (GLM) with repeated measures.
Weused two repeated factors: Situation (situations 1 to 6) and Description Length (shortand long(FI/SL) ).
We found a highly significant effect of Description Length on thenumber of clicks used to complete the task (F1,39 = 262.46, p < .001, ?2p = .87).
In allpotentially problematic situations, the number of clicks is smaller for the long than forthe short description.
This confirms Hypothesis 2.1.
We also found significant effectsof Situation (F5,35 = 13.11, p < .001, ?2p = .65), and of the interaction between Situationand Description Length (F5,35 = 18.02, p < .001, ?2p = .72).Table 4 shows descriptive statistics for the gain as used for Hypothesis 2.2.
We againused a GLM with repeated measures, using two repeated factors: Description Content(that of situations 1 and 7, and that of situations 2 and 8) and Situation Type (potentialDE and non-problematic).14 We found a highly significant effect of Situation Type onthe gain (F1,39 = 26.62, p < .001, ?2p = .41).
In the non-problematic situations the gain issmaller than in the potential DE situations.
This confirms Hypothesis 2.2.Table 5 shows descriptive statistics for the gain as used for Hypothesis 2.3.
We againused a GLM with repeated measures, using two repeated factors: Description Content(that of situations 3 and 5, and that of situations 4 and 6) and FI Decision (with 2 levels:14 There were no significant effects of Description Content and of the interaction between DescriptionContent and Situation Type.
From here on, we will focus on effects that were significant.249Computational Linguistics Volume 33, Number 2Table 4Gain as used for Hypothesis 2.2.Sit.
Type Mean STDEV1 DE 2.48 2.247 NONE ?0.05 2.772 DE 2.55 3.628 NONE 0.43 1.04complete and not complete).
We found a highly significant effect of FI Decision onthe gain (F1,39 = 24.10, p < .001, ?2p = .38).
The gain is smaller for situations where ouralgorithm decided to use an incomplete description than in situations where it chose acomplete one.
This confirms Hypothesis 2.3.6.3 Discussion of Second ExperimentWhat does the second experiment teach us, over and above what we learned from thefirst one?
First of all, the experiment suggests an explanation of why it was that, inproblematic situations, subjects (in the first experiment) preferred redundant descrip-tions: The new experiment suggests that the reason may lie in the fact that, in thepotentially problematic situations, the addition of structural information reduces theeffort involved in resolution.
This is, of course, exactly in line with the way in whichDE and LO were introduced in Section 3, and with the assumptions about ease ofresolution that were formulated in Paraboni and Van Deemter (2002a) and in the presentSection 2.Do our experiments, taken together, tell us how much redundancy is optimal in anygiven situation?
In answering this question, let us first realize that pragmatic factorsrelating to the utterance situation are likely to affect how much redundancy is needed.At one end of the spectrum, there may be highly fault-critical settings, where flawlessunderstanding is essential; at the other end, there may be discourse settings whereaccurate understanding is not important, and where the speaker/writer is under timepressure.
Surely, redundant information must be more common in the former than inthe latter.
No one algorithm can cater to all types of settings.On the other hand, our data do suggest quite strongly that, at least in the situation inwhich our subjects found themselves, a law of diminishing returns is in operation.
To seethis, let us first focus on the two non-problematic situations (Table 2): Averaging numbersTable 5Gain as used for Hypothesis 2.3.Sit.
FI Decision Mean STDEV3 NOT COMPLETE 0.70 1.405 COMPLETE 4.50 6.674 NOT COMPLETE 0.23 2.516 COMPLETE 2.83 2.16250Paraboni, van Deemter, and Masthoff Making Referents Easy to Identifyof clicks of all subjects over all relevant situations, short descriptions required a mere1.53 clicks; by adding redundant information (unlike SL/FI), this number gets reducedto an average of 1.34 clicks (long(other), in situations 7 and 8).
This very slight gain(0.19 clicks) is not statistically significant (F1,39 = .60, p = .44, ?2p = .02) and is bought atthe price of a description that is one and a half times longer, which makes it likely totake more time during interpretation.
As for the more interesting problematic situations,perhaps the best comparison is between situations 3 and 4 (where long(other) exists andis longer than long(FI/SL)).
Here, short descriptions lead to a pretty dismal averageof 4.05 clicks.
If we lengthen the descriptions as prescribed by FI/SL (long(FI/SL))then this figure is lowered drastically to what looks like a pretty acceptable 1.77 clicks,which constitutes a gain of 2.28.
By adding even more information (as in long(other)),the figure is lowered further, to 1.31 clicks.
Although this does represent a gain, it isnot statistically significant (F1,39 = 2.94, p = .095, ?2p = .07), and besides it is so small(at 0.46 clicks) that it seems likely to be more than offset by the disadvantages forinterpretation that are implied by the increased length of the description.
Needless to say,these effects can only become stronger if more complex documents are considered, andwith descriptions that are even longer.
Really excessive redundancy might have detri-mental effects on resolution as well as interpretation, because it confuses hearers.
(Ahearer might wonder, along Gricean lines, ?Why are they saying ?Picture 5 in Part A ofSection 3, printed in black and white?.
Surely if they have to give so much information,they cannot simply mean Picture 5??.
)Finally, we also explored the searching behavior of our subjects, focusing on the12 documents in which incomplete descriptions were given.
Ancestral Search predictsthat subjects will search the current section (where the question is asked) exhaustively,before moving on to another section.
Figure 5 shows subjects?
compliance with An-cestral Search in their first navigation action.
(Eight of the 12 documents contained adescription of the form Picture 5 in Part A, so for these it suffices to look at the firstnavigation action.)
Four subjects complied perfectly.
Half the subjects complied almostperfectly, deviating in at most 2 of the 12 cases.
However, five subjects deviated almostcompletely (10 or more times).
Closer inspection showed that these latter subjectsseemed to navigate randomly, not following any obvious pattern (e.g., top to bottom).It may well be that these subjects did not take the experiment seriously.
Nevertheless,we still have more deviation from Ancestral Search than expected.There are two possible explanations.
First, some subjects may have started usingAncestral Search, and then found that it was not effective when they encounteredsome documents in which the referent turned out to be in some far-away section, afterwhich they changed to a more random strategy.
(Recall that our experiment deliberatelyFigure 5Compliance with Ancestral Search during first navigation action.251Computational Linguistics Volume 33, Number 2included some unreasonably short descriptions.)
Our data seem to confirm this.
Forinstance, subject S11 started in compliance with Ancestral Search until encountering adocument asking, in Section 2, to find a picture in Part C. The subject clicked as manyas 6 times on Part C of Section 2, before finally finding the referent in Section 3.
He wenton to deviate four times from Ancestral Search.A second explanation for deviating from Ancestral Search is the kind of navigationthat we allowed.
Subjects could go directly from, say, Part C in Section 2, to part A inSection 3, without an extra navigation step to go into Section 3.
In fact, it may evenbe faster to navigate to another section than within the current one, depending onthe position of the mouse pointer.
(This contrasts with the university domain, whereone could not go directly from room 120 in Watts building to room 140 in Cockcroftbuilding without first having to walk between the buildings.)
It should be noted thatthis problem may be more pronounced after the first navigation action has been made.For instance, if one clicks on Part A in Section 2, then the mouse pointer is about asclose to Part C in Section 1 as to Part C in Section 2.
To explore this idea, we looked atthe four documents in which a description of the form picture 5 was given.
In 83 cases,subjects who complied with Ancestral Search for the first navigation action needed toperform a second action; in 77% of these cases, they also complied with Ancestral Searchin the second action.
Now in as many as 68% of the cases in which they did not comply,they clicked on the closest link in an adjacent section (e.g., Part A of the next sectionafter having first clicked on Part C).
This confirms our suspicion that the lack of effortrequired to deviate may have been a reason for deviation.
With hindsight, we shouldprobably have made the distance between the relevant sections larger.7.
ConclusionThis article has discussed generation strategies that facilitate resolution of a referringexpression by adding logically redundant properties.
We have shown that this canbe of crucial importance, especially in large domains, where minimally distinguishingdescriptions can sometimes be completely useless (witness, e.g., Example [1c]).
Twoalgorithms for generating logically redundant references along the lines described inthis article have been implemented.
The experiments reported in the previous sectionsindicate that these algorithms are fundamentally on the right track.We recently learned of an interesting series of experiments that investigate the roleof logically redundant properties in referring expressions (Arts 2004).
One of the out-comes of these experiments was that certain types of logically redundant informationalmost consistently led to accelerated resolution.
This was particularly true for informa-tion concerning the location of an object.
For example, a logically minimal descriptionlike the white button on the left took readers longer to resolve than a redundant one likethe white button at the top left (our emphasis).
It is interesting to note that these resultswere obtained in situations where neither LO nor DE could occur.This article has described an alternative to classical algorithms for GRE.
Suppose youare designing an NLG system and want to give it a GRE component; how do you knowwhether to use the new algorithm, instead of one of its predecessors?
Redundancy hasa role to play in different kinds of situations (see the Introduction of this article), butour algorithms focus on a class of cases that we believe to be particularly widespread,namely where the domain is hierarchical in the sense of Section 3.
Because hierarchiesinvolve relations, let us once again compare the predictions made by our algorithmswith those made by Dale and Haddock (1991).
Suppose their description the bowl on the252Paraboni, van Deemter, and Masthoff Making Referents Easy to Identifytable was said when there are two tables and two bowls, while (only) the table furthestaway from the hearer has a bowl on it.
FI and SL, by contrast, would generate somethingredundant like the bowl on the far-away table.
Which of the two descriptions is best?The answer is that it depends on the situation: When all the relevant facts areavailable to the reader without effort (e.g., all the domain objects are visible at a glance)then Dale and Haddock?s minimal descriptions are fine, but when search is required,the kind of ?studied?
redundancy embodied in FI and SL becomes necessary.
Considerthe example again.
If the tables and bowls are visible at a glance, then resolving theDE-inducing description the bowl on the table is unproblematic, because there is nothinghere to discover: The crucial part of the domain is directly available, and no search isneeded.
Consequently, it is superfluous to say anything about the location of the table.But suppose we are in a huge room, where it is not obvious for the hearer what is oneach table.
In this situation, the bowl on the table would be a rather unhelpful description,compared to the bowl on the far-away table (or the bowl on the table in the corner), aswould be consistent with our algorithms.
(The example can be made more dramaticby hiding the table with the bowl on it in another room.)
What this example highlightsis the distinction between the things that speaker and hearer know when a referringexpression is uttered, and the things they can discover.
It is in the latter case that searchbecomes an issue.
We have shown how this idea can be made precise and incorporatedinto a GRE algorithm, and we have demonstrated that this can improve the generateddescriptions from the perspective of the hearer.Recent work in psycholinguistics, focusing on spontaneous speech in dialogue,has shown that speakers and hearers often act as if they are completely oblivious ofthe epistemic limitations of their interlocutors, even when these limitations have beenmade perfectly obvious to them (e.g., Keysar, Lin, and Barr 2003).
These widely knownresults have caused some researchers to expect language users to behave with unbridleddescriptive ?egocentricity?
in all situations.
The first of our two experiments suggeststhat human writers (as opposed, perhaps, to speakers) can be highly altruistic in theirdescriptions of objects.
The second experiment demonstrates how descriptive altruismcan benefit readers.By exploring the benefits for the hearer (in terms of the effort required for identify-ing the referent), we have not only shown that it can be good to add logically redundantinformation to a referring expression; we have arguably also shed some light on thereason why redundant descriptions are sometimes preferred.
By counting the number ofclicks that subjects need in order to find the referent, and relating these to predictionsstemming from our Ancestral Search model, we believe that we have achieved a degreeof insight into the ?resolution?
processes in the head of the reader, not unlike theway in which insights in human language processing can be produced by eye-trackingexperiments.
It would be interesting to see whether the ideas discussed here can beconfirmed using such a more entrenched psycholinguistic paradigm.AcknowledgmentsThe authors are grateful for insightfulcomments from Emiel Krahmer, RichardPower, Sebastian Varges, the Aberdeen NLGgroup, and the anonymous reviewers.
Thesecond author acknowledges support fromthe UK?s EPSRC TUNA project, grantGR/S13330/01.ReferencesAppelt, Douglas E. 1985.
Planning Englishreferring expressions.
Artificial Intelligence,26:1?33.Arts, Anja.
2004.
Overspecification inInstructive Texts.
Ph.D. thesis, TilburgUniversity, The Netherlands.
WolfPublishers, Nijmegen.253Computational Linguistics Volume 33, Number 2Association of the British PharmaceuticalIndustry (ABPI).
1997.
1996?1997 ABPICompendium of Patient Information Leaflets.ABPI, London.Clark, Herbert.
1992.
Arenas of Language Use.CSLI Publications, Stanford, CA.Cremers, Anita.
1996.
Reference to Objects; anEmpirically Based Study of Task-orientedDialogues.
PhD.
thesis, University ofEindhoven.Dale, Robert.
1989.
Cooking up referringexpressions.
In Proceedings of the 27thAnnual Meeting of the Association forComputational Linguistics (ACL-1989),pages 68?75, Vancouver, Canada.Dale, Robert and Nicholas Haddock.
1991.Generating referring expressions involvingrelations.
In Proceedings of EACL-1991,pages 161?166, Berlin.Dale, Robert and Ehud Reiter.
1995.Computational interpretations of theGricean maxims in the generation ofreferring expressions.
Cognitive Science,18:233?263.Deutsch, W. 1976.
Sprachliche Redundanzund Objectidentifikation.
Ph.D.dissertation, University of Marburg.Edmonds, Philip G. 1994.
Collaboration onreference to objects that are not mutuallyknown.
In Proceedings of COLING-1994,pages 1118?1122, Kyoto.Grice, Herbert P. 1975.
Logic and conversation.In P. Cole and J. Morgan, editors, Syntaxand Semantics: Vol 3, Speech Acts.
AcademicPress, New York, pages 43?58.Horacek, Helmut.
2005.
Generatingreferential descriptions under conditionsof uncertainty.
In 10th European Workshopon Natural Language Generation (ENLG-2005),pages 58?67, Aberdeen, Scotland.Jordan, Pamela W. 2000.
Can nominalexpressions achieve multiple goals?An empirical study.
In ACL-2000,pages 142?149, Hong Kong.Jordan, Pamela W. 2002.
Contextual influenceson attribute selection for repeateddescriptions.
In K. van Deemter andR.
Kibble, editors, Information Sharing.
CSLIPublications, Stanford, CA, pages 295?328.Keysar, Boaz, Shuhong Lin, and Dale J. Barr.2003.
Limits on theory of mind use inadults.
Cognition 89:25?41.Krahmer, Emiel and Marie?t Theune.
2002.Efficient context-sensitive generation ofreferring expressions.
In K. van Deemterand R. Kibble, editors, InformationSharing.
CSLI Publications, Stanford, CA,pages 223?264.Levelt, Willem J. M. 1989.
Speaking: FromIntention to Articulation.
MIT Press,Cambridge, MA.Mangold, Roland.
1986.
Sensorische Faktorenbeim Verstehen ueberspezifizierterObjektbenennungen.
Peter Lang Verlag,Frankfurt.Norman, Donald.
1988.
The Design ofEveryday Things.
Doubleday, London.Paraboni, Ivandre?.
2000.
An algorithm forgenerating document-deictic references.
InProceedings of INLG-2000, ?Coherence inGenerated Multimedia,?
pages 27?31,Mitzpe Ramon, Israel.Paraboni, Ivandre?.
2003.
Generating Referencesin Hierarchical Domains: The Case ofDocument Deixis.
Ph.D thesis, University ofBrighton, U.K.Paraboni, Ivandre?, Judith Masthoff, and Keesvan Deemter.
2006.
Overspecified referencein hierarchical domains: Measuring thebenefits for readers.
In Proceedings ofINLG-2006, pages 55?62, Sydney.Paraboni, Ivandre?
and Kees van Deemter.2002a.
Generating easy references: the caseof document deixis.
In Proceedings ofINLG-2002, pages 113?119, New York.Paraboni, Ivandre?
and Kees van Deemter.2002b.
Towards the generation ofdocument-deictic references.
In K. vanDeemter and R. Kibble, editors, InformationSharing.
CSLI Publications, Stanford,pages 329?354.Pechmann, Thomas.
1989.
Incrementalspeech production and referentialoverspecification.
Linguistics, 27:98?110.Reiter, Ehud and Robert Dale.
2000.
BuildingNatural Language Generation Systems.Cambridge University Press, Cambridge.Siddharthan, Advaith and Ann Copestake.2004.
Generating referring expressions inopen domains.
In Proceedings of 42nd ACL,pages 408?415, Barcelona, Spain.Sonnenschein, Susan.
1982.
The effects ofredundant communications on listeners:When more is less.
Child Development,53:717?729.Sonnenschein, Susan.
1984.
The effect ofredundant communication on listeners:Why different types may have differenteffects.
Psycholinguistic Research, 13:147?166.van Deemter, Kees.
2002.
Generatingreferring expressions: Booleanextensions of the incremental algorithm.Computational Linguistics, 28(1):37?52.van Deemter, Kees.
2004.
Finetuning an NLGsystem through experiments with humansubjects: The case of vague descriptions.In Proceedings of INLG-2004, pages 31?40,Brockenhurst, UK.254
