Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 445?453,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsImproving the Use of Pseudo-Words for EvaluatingSelectional PreferencesNathanael Chambers and Dan JurafskyDepartment of Computer ScienceStanford University{natec,jurafsky}@stanford.eduAbstractThis paper improves the use of pseudo-words as an evaluation framework forselectional preferences.
While pseudo-words originally evaluated word sensedisambiguation, they are now commonlyused to evaluate selectional preferences.
Aselectional preference model ranks a set ofpossible arguments for a verb by their se-mantic fit to the verb.
Pseudo-words serveas a proxy evaluation for these decisions.The evaluation takes an argument of a verblike drive (e.g.
car), pairs it with an al-ternative word (e.g.
car/rock), and asks amodel to identify the original.
This pa-per studies two main aspects of pseudo-word creation that affect performance re-sults.
(1) Pseudo-word evaluations oftenevaluate only a subset of the words.
Weshow that selectional preferences shouldinstead be evaluated on the data in its en-tirety.
(2) Different approaches to select-ing partner words can produce overly op-timistic evaluations.
We offer suggestionsto address these factors and present a sim-ple baseline that outperforms the state-of-the-art by 13% absolute on a newspaperdomain.1 IntroductionFor many natural language processing (NLP)tasks, particularly those involving meaning, cre-ating labeled test data is difficult or expensive.One way to mitigate this problem is with pseudo-words, a method for automatically creating testcorpora without human labeling, originally pro-posed for word sense disambiguation (Gale et al,1992; Schutze, 1992).
While pseudo-words arenow less often used for word sense disambigation,they are a common way to evaluate selectionalpreferences, models that measure the strength ofassociation between a predicate and its argumentfiller, e.g., that the noun lunch is a likely objectof eat.
Selectional preferences are useful for NLPtasks such as parsing and semantic role labeling(Zapirain et al, 2009).
Since evaluating them inisolation is difficult without labeled data, pseudo-word evaluations can be an attractive evaluationframework.Pseudo-word evaluations are currently used toevaluate a variety of language modeling tasks(Erk, 2007; Bergsma et al, 2008).
However,evaluation design varies across research groups.This paper studies the evaluation itself, showinghow choices can lead to overly optimistic resultsif the evaluation is not designed carefully.
Weshow in this paper that current methods of apply-ing pseudo-words to selectional preferences varygreatly, and suggest improvements.A pseudo-word is the concatenation of twowords (e.g.
house/car).
One word is the orig-inal in a document, and the second is the con-founder.
Consider the following example of ap-plying pseudo-words to the selectional restrictionsof the verb focus:Original: This story focuses on the campaign.Test: This story/part focuses on the campaign/meeting.In the original sentence, focus has two arguments:a subject story and an object campaign.
In the testsentence, each argument of the verb is replaced bypseudo-words.
A model is evaluated by its successat determining which of the two arguments is theoriginal word.Two problems exist in the current use of445pseudo-words to evaluate selectional preferences.First, selectional preferences historically focus onsubsets of data such as unseen words or words incertain frequency ranges.
While work on unseendata is important, evaluating on the entire datasetprovides an accurate picture of a model?s overallperformance.
Most other NLP tasks today evalu-ate all test examples in a corpus.
We will showthat seen arguments actually dominate newspaperarticles, and thus propose creating test sets that in-clude all verb-argument examples to avoid artifi-cial evaluations.Second, pseudo-word evaluations vary in howthey choose confounders.
Previous work has at-tempted to maintain a similar corpus frequencyto the original, but it is not clear how best to dothis, nor how it affects the task?s difficulty.
Weargue in favor of using nearest-neighbor frequen-cies and show how using random confounders pro-duces overly optimistic results.Finally, we present a surprisingly simple base-line that outperforms the state-of-the-art and is farless memory and computationally intensive.
Itoutperforms current similarity-based approachesby over 13% when the test set includes all of thedata.
We conclude with a suggested backoff modelbased on this baseline.2 History of Pseudo-WordDisambiguationPseudo-words were introduced simultaneously bytwo papers studying statistical approaches to wordsense disambiguation (WSD).
Schu?tze (1992)simply called the words, ?artificial ambiguouswords?, but Gale et al (1992) proposed the suc-cinct name, pseudo-word.
Both papers cited thesparsity and difficulty of creating large labeleddatasets as the motivation behind pseudo-words.Gale et al selected unambiguous words from thecorpus and paired them with random words fromdifferent thesaurus categories.
Schu?tze paired hiswords with confounders that were ?comparable infrequency?
and ?distinct semantically?.
Gale etal.
?s pseudo-word term continues today, as doesSchu?tze?s frequency approach to selecting the con-founder.Pereira et al (1993) soon followed with a selec-tional preference proposal that focused on a lan-guage model?s effectiveness on unseen data.
Thework studied clustering approaches to assist insimilarity decisions, predicting which of two verbswas the correct predicate for a given noun object.One verb v was the original from the source doc-ument, and the other v?
was randomly generated.This was the first use of such verb-noun pairs, aswell as the first to test only on unseen pairs.Several papers followed with differing methodsof choosing a test pair (v, n) and its confounderv?.
Dagan et al (1999) tested all unseen (v, n)occurrences of the most frequent 1000 verbs inhis corpus.
They then sorted verbs by corpus fre-quency and chose the neighboring verb v?
of vas the confounder to ensure the closest frequencymatch possible.
Rooth et al (1999) tested 3000random (v, n) pairs, but required the verbs andnouns to appear between 30 and 3000 times intraining.
They also chose confounders randomlyso that the new pair was unseen.Keller and Lapata (2003) specifically addressedthe impact of unseen data by using the web to first?see?
the data.
They evaluated unseen pseudo-words by attempting to first observe them in alarger corpus (the Web).
One modeling differencewas to disambiguate the nouns as selectional pref-erences instead of the verbs.
Given a test pair(v, n) and its confounder (v, n?
), they used websearches such as ?v Det n?
to make the decision.Results beat or matched current results at the time.We present a similarly motivated, but new web-based approach later.Very recent work with pseudo-words (Erk,2007; Bergsma et al, 2008) further blurs the linesbetween what is included in training and test data,using frequency-based and semantic-based rea-sons for deciding what is included.
We discussthis further in section 5.As can be seen, there are two main factors whendevising a pseudo-word evaluation for selectionalpreferences: (1) choosing (v, n) pairs from the testset, and (2) choosing the confounding n?
(or v?
).The confounder has not been looked at in detailand as best we can tell, these factors have var-ied significantly.
Many times the choices are wellmotivated based on the paper?s goals, but in othercases the motivation is unclear.3 How Frequent is Unseen Data?Most NLP tasks evaluate their entire datasets, butas described above, most selectional preferenceevaluations have focused only on unseen data.This section investigates the extent of unseen ex-amples in a typical training/testing environment446of newspaper articles.
The results show that evenwith a small training size, seen examples dominatethe data.
We argue that, absent a system?s need forspecialized performance on unseen data, a repre-sentative test set should include the dataset in itsentirety.3.1 Unseen Data ExperimentWe use the New York Times (NYT) and Associ-ated Press (APW) sections of the Gigaword Cor-pus (Graff, 2002), as well as the British NationalCorpus (BNC) (Burnard, 1995) for our analysis.Parsing and SRL evaluations often focus on news-paper articles and Gigaword is large enough tofacilitate analysis over varying amounts of train-ing data.
We parsed the data with the Stan-ford Parser1 into dependency graphs.
Let (vd, n)be a verb v with grammatical dependency d ?
{subject, object, prep} filled by noun n. Pairs(vd, n) are chosen by extracting every such depen-dency in the graphs, setting the head predicate asv and the head word of the dependent d as n. Allprepositions are condensed into prep.We randomly selected documents from the year2001 in the NYT portion of the corpus as devel-opment and test sets.
Training data for APW andNYT include all years 1994-2006 (minus NYT de-velopment and test documents).
We also identifiedand removed duplicate documents2.
The BNC inits entirety is also used for training as a single datapoint.
We then record every seen (vd, n) pair dur-ing training that is seen two or more times3 andthen count the number of unseen pairs in the NYTdevelopment set (1455 tests).Figure 1 plots the percentage of unseen argu-ments against training size when trained on eitherNYT or APW (the APW portion is smaller in totalsize, and the smaller BNC is provided for com-parison).
The first point on each line (the high-est points) contains approximately the same num-ber of words as the BNC (100 million).
Initially,about one third of the arguments are unseen, butthat percentage quickly falls close to 10% as ad-ditional training is included.
This suggests that anevaluation focusing only on unseen data is not rep-resentative, potentially missing up to 90% of thedata.1http://nlp.stanford.edu/software/lex-parser.shtml2Any two documents whose first two paragraphs in thecorpus files are identical.3Our results are thus conservative, as including all singleoccurrences would achieve even smaller unseen percentages.0 2 4 6 8 10 12051015202530354045Number of Tokens in Training (hundred millions)Percent UnseenUnseen Arguments in NYT DevBNC AP NYT GoogleFigure 1: Percentage of NYT development setthat is unseen when trained on varying amounts ofdata.
The two lines represent training with NYT orAPW data.
The APW set is smaller in size fromthe NYT.
The dotted line uses Google n-grams astraining.
The x-axis represents tokens ?
108.0 2 4 6 8 10 120510152025303540Number of Tokens in Training (hundred millions)Percent UnseenUnseen Arguments by TypePreps Subjects ObjectsFigure 2: Percentage of subject/object/prepositionarguments in the NYT development set that is un-seen when trained on varying amounts of NYTdata.
The x-axis represents tokens ?
108.447The third line across the bottom of the figure isthe number of unseen pairs using Google n-gramdata as proxy argument counts.
Creating argu-ment counts from n-gram counts is described indetail below in section 5.2.
We include these Webcounts to illustrate how an openly available sourceof counts affects unseen arguments.
Finally, fig-ure 2 compares which dependency types are seenthe least in training.
Prepositions have the largestunseen percentage, but not surprisingly, also makeup less of the training examples overall.In order to analyze why pairs are unseen, we an-alyzed the distribution of rare words across unseenand seen examples.
To define rare nouns, we orderhead words by their individual corpus frequencies.A noun is rare if it occurs in the lowest 10% of thelist.
We similarly define rare verbs over their or-dered frequencies (we count verb lemmas, and donot include the syntactic relations).
Corpus countscovered 2 years of the AP section, and we usedthe development set of the NYT section to extractthe seen and unseen pairs.
Figure 3 shows the per-centage of rare nouns and verbs that occur in un-seen and seen pairs.
24.6% of the verbs in un-seen pairs are rare, compared to only 4.5% in seenpairs.
The distribution of rare nouns is less con-trastive: 13.3% vs 8.9%.
This suggests that manyunseen pairs are unseen mainly because they con-tain low-frequency verbs, rather than because ofcontaining low-frequency argument heads.Given the large amount of seen data, we be-lieve evaluations should include all data examplesto best represent the corpus.
We describe our fullevaluation results and include a comparison of dif-ferent training sizes below.4 How to Select a ConfounderGiven a test set S of pairs (vd, n) ?
S, we now ad-dress how best to select a confounder n?.
Work inWSD has shown that confounder choice can makethe pseudo-disambiguation task significantly eas-ier.
Gaustad (2001) showed that human-generatedpseudo-words are more difficult to classify thanrandom choices.
Nakov and Hearst (2003) furtherillustrated how random confounders are easier toidentify than those selected from semantically am-biguous, yet related concepts.
Our approach eval-uates selectional preferences, not WSD, but our re-sults complement these findings.We identified three methods of confounder se-lection based on varying levels of corpus fre-verbs nounsUnseen TestsSeen TestsDistribution of Rare Verbs and Nouns in TestsPercent Rare Words051015202530Figure 3: Comparison between seen and unseentests (verb,relation,noun).
24.6% of unseen testshave rare verbs, compared to just 4.5% in seentests.
The rare nouns are more evenly distributedacross the tests.quency: (1) choose a random noun, (2) choose arandom noun from a frequency bucket similar tothe original noun?s frequency, and (3) select thenearest neighbor, the noun with frequency clos-est to the original.
These methods evaluate therange of choices used in previous work.
Our ex-periments compare the three.5 Models5.1 A New BaselineThe analysis of unseen slots suggests a baselinethat is surprisingly obvious, yet to our knowledge,has not yet been evaluated.
Part of the reasonis that early work in pseudo-word disambiguationexplicitly tested only unseen pairs4.
Our evalua-tion will include seen data, and since our analysissuggests that up to 90% is seen, a strong baselineshould address this seen portion.4Recent work does include some seen data.
Bergsma etal.
(2008) test pairs that fall below a mutual informationthreshold (might include some seen pairs), and Erk (2007)selects a subset of roles in FrameNet (Baker et al, 1998) totest and uses all labeled instances within this subset (unclearwhat portion of subset of data is seen).
Neither evaluates allof the seen data, however.448We propose a conditional probability baseline:P (n|vd) ={C(vd,n)C(vd,?
)if C(vd, n) > 00 otherwisewhere C(vd, n) is the number of times the headword n was seen as an argument to the pred-icate v, and C(vd, ?)
is the number of timesvd was seen with any argument.
Given a test(vd, n) and its confounder (vd, n?
), choose n ifP (n|vd) > P (n?|vd), and n?
otherwise.
IfP (n|vd) = P (n?|vd), randomly choose one.Lapata et al (1999) showed that corpus fre-quency and conditional probability correlate withhuman decisions of adjective-noun plausibility,and Dagan et al (1999) appear to propose a verysimilar baseline for verb-noun selectional prefer-ences, but the paper evaluates unseen data, and sothe conditional probability model is not studied.We later analyze this baseline against a morecomplicated smoothing approach.5.2 A Web BaselineIf conditional probability is a reasonable baseline,better performance may just require more data.Keller and Lapata (2003) proposed using the webfor this task, querying for specific phrases like?Verb Det N?
to find syntactic objects.
Such a webcorpus would be attractive, but we?d like to findsubjects and prepositional objects as well as ob-jects, and also ideally we don?t want to limit our-selves to patterns.
Since parsing the web is unre-alistic, a reasonable compromise is to make roughcounts when pairs of words occur in close proxim-ity to each other.Using the Google n-gram corpus, we recordedall verb-noun co-occurrences, defined by appear-ing in any order in the same n-gram, up to andincluding 5-grams.
For instance, the test pair(throwsubject, ball) is considered seen if there ex-ists an n-gram such that throw and ball are bothincluded.
We count all such occurrences for allverb-noun pairs.
We also avoided over-countingco-occurrences in lower order n-grams that appearagain in 4 or 5-grams.
This crude method of count-ing has obvious drawbacks.
Subjects are not dis-tinguished from objects and nouns may not be ac-tual arguments of the verb.
However, it is a simplebaseline to implement with these freely availablecounts.Thus, we use conditional probability as de-fined in the previous section, but define the countC(vd, n) as the number of times v and n (ignoringd) appear in the same n-gram.5.3 Smoothing ModelWe implemented the current state-of-the-artsmoothing model of Erk (2007).
The model isbased on the idea that the arguments of a particularverb slot tend to be similar to each other.
Giventwo potential arguments for a verb, the correctone should correlate higher with the arguments ob-served with the verb during training.Formally, given a verb v and a grammatical de-pendency d, the score for a noun n is defined:Svd(n) =?w?Seen(vd)sim(n,w) ?
C(vd, w) (1)where sim(n,w) is a noun-noun similarity score,Seen(vd) is the set of seen head words filling theslot vd during training, and C(vd, n) is the num-ber of times the noun n was seen filling the slot vdThe similarity score sim(n,w) can thus be one ofmany vector-based similarity metrics5.
We eval-uate both Jaccard and Cosine similarity scores inthis paper, but the difference between the two issmall.6 ExperimentsOur training data is the NYT section of the Gi-gaword Corpus, parsed into dependency graphs.We extract all (vd, n) pairs from the graph, as de-scribed in section 3.
We randomly chose 9 docu-ments from the year 2001 for a development set,and 41 documents for testing.
The test set con-sisted of 6767 (vd, n) pairs.
All verbs and nounsare stemmed, and the development and test docu-ments were isolated from training.6.1 Varying Training SizeWe repeated the experiments with three differenttraining sizes to analyze the effect data size has onperformance:?
Train x1: Year 2001 of the NYT portion ofthe Gigaword Corpus.
After removing du-plicate documents, it contains approximately110 million tokens, comparable to the 100million tokens in the BNC corpus.5A similar type of smoothing was proposed in earlierwork by Dagan et al (1999).
A noun is represented by avector of verb slots and the number of times it is observedfilling each slot.449?
Train x2: Years 2001 and 2002 of the NYTportion of the Gigaword Corpus, containingapproximately 225 million tokens.?
Train x10: The entire NYT portion of Giga-word (approximately 1.2 billion tokens).
It isan order of magnitude larger than Train x1.6.2 Varying the ConfounderWe generated three different confounder setsbased on word corpus frequency from the 41 testdocuments.
Frequency was determined by count-ing all tokens with noun POS tags.
As motivatedin section 4, we use the following approaches:?
Random: choose a random confounder fromthe set of nouns that fall within some broadcorpus frequency range.
We set our range toeliminate (approximately) the top 100 mostfrequent nouns, but otherwise arbitrarily setthe lower range as previous work seems todo.
The final range was [30, 400000].?
Buckets: all nouns are bucketed based ontheir corpus frequencies6.
Given a test pair(vd, n), choose the bucket in which n belongsand randomly select a confounder n?
fromthat bucket.?
Neighbor: sort all seen nouns by frequencyand choose the confounder n?
that is the near-est neighbor of n with greater frequency.6.3 Model ImplementationNone of the models can make a decision if theyidentically score both potential arguments (mostoften true when both arguments were not seen withthe verb in training).
As a result, we extend allmodels to randomly guess (50% performance) onpairs they cannot answer.The conditional probability is reported as Base-line.
For the web baseline (reported as Google),we stemmed all words in the Google n-grams andcounted every verb v and noun n that appear inGigaword.
Given two nouns, the noun with thehigher co-occurrence count with the verb is cho-sen. As with the other models, if the two nounshave the same counts, it randomly guesses.The smoothing model is named Erk in the re-sults with both Jaccard and Cosine as the simi-larity metric.
Due to the large vector representa-tions of the nouns, it is computationally wise to6We used frequency buckets of 4, 10, 25, 200, 1000,>1000.
Adding more buckets moves the evaluation closerto Neighbor, less is closer to Random.trim their vectors, but also important to do so forbest performance.
A noun?s representative vectorconsists of verb slots and the number of times thenoun was seen in each slot.
We removed any verbslot not seen more than x times, where x variedbased on all three factors: the dataset, confounderchoice, and similarity metric.
We optimized xon the development data with a linear search, andused that cutoff on each test.
Finally, we trimmedany vectors over 2000 in size to reduce the com-putational complexity.
Removing this strict cutoffappears to have little effect on the results.Finally, we report backoff scores for Google andErk.
These consist of always choosing the Base-line if it returns an answer (not a guessed unseenanswer), and then backing off to the Google/Erkresult for Baseline unknowns.
These are labeledBackoff Google and Backoff Erk.7 ResultsResults are given for the two dimensions: con-founder choice and training size.
Statistical sig-nificance tests were calculated using the approx-imate randomization test (Yeh, 2000) with 1000iterations.Figure 4 shows the performance change over thedifferent confounder methods.
Train x2 was usedfor training.
Each model follows the same pro-gression: it performs extremely well on the ran-dom test set, worse on buckets, and the lowest onthe nearest neighbor.
The conditional probabilityBaseline falls from 91.5 to 79.5, a 12% absolutedrop from completely random to neighboring fre-quency.
The Erk smoothing model falls 27% from93.9 to 68.1.
The Google model generally per-forms the worst on all sets, but its 74.3% perfor-mance with random confounders is significantlybetter than a 50-50 random choice.
This is no-table since the Google model only requires n-gramcounts to implement.
The Backoff Erk model isthe best, using the Baseline for the majority ofdecisions and backing off to the Erk smoothingmodel when the Baseline cannot answer.Figure 5 (shown on the next page) varies thetraining size.
We show results for both Bucket Fre-quencies and Neighbor Frequencies.
The only dif-ference between columns is the amount of trainingdata.
As expected, the Baseline improves as thetraining size is increased.
The Erk model, some-what surprisingly, shows no continual gain withmore training data.
The Jaccard and Cosine simi-450Varying the Confounder FrequencyRandom Buckets NeighborBaseline 91.5 89.1 79.5Erk-Jaccard 93.9* 82.7* 68.1*Erk-Cosine 91.2 81.8* 65.3*Google 74.3* 70.4* 59.4*Backoff Erk 96.6* 91.8* 80.8*Backoff Goog 92.7?
89.7 79.8Figure 4: Trained on two years of NYT data (Trainx2).
Accuracy of the models on the same NYT testdocuments, but with three different ways of choos-ing the confounders.
* indicates statistical signifi-cance with the column?s Baseline at the p < 0.01level, ?
at p < 0.05.
Random is overly optimistic,reporting performance far above more conserva-tive (selective) confounder choices.Baseline DetailsTrain Train x2 Train x10Precision 96.1 95.5* 95.0?Accuracy 78.2 82.0* 88.1*Accuracy +50% 87.5 89.1* 91.7*Figure 6: Results from the buckets confounder testset.
Baseline precision, accuracy (the same as re-call), and accuracy when you randomly guess thetests that Baseline does not answer.
All numbersare statistically significant * with p-value < 0.01from the number to their left.larity scores perform similarly in their model.
TheBaseline achieves the highest accuracies (91.7%and 81.2%) with Train x10, outperforming the bestErk model by 5.2% and 13.1% absolute on buck-ets and nearest neighbor respectively.
The back-off models improve the baseline by just under 1%.The Google n-gram backoff model is almost asgood as backing off to the Erk smoothing model.Finally, figure 6 shows the Baseline?s precisionand overall accuracy.
Accuracy is the same asrecall when the model does not guess betweenpseudo words that have the same conditional prob-abilities.
Accuracy +50% (the full Baseline inall other figures) shows the gain from randomlychoosing one of the two words when uncertain.Precision is extremely high.8 DiscussionConfounder Choice: Performance is strongly in-fluenced by the method used when choosing con-founders.
This is consistent with findings forWSD that corpus frequency choices alter the task(Gaustad, 2001; Nakov and Hearst, 2003).
Ourresults show the gradation of performance as onemoves across the spectrum from completely ran-dom to closest in frequency.
The Erk modeldropped 27%, Google 15%, and our baseline 12%.The overly optimistic performance on random datasuggests using the nearest neighbor approach forexperiments.
Nearest neighbor avoids evaluatingon ?easy?
datasets, and our baseline (at 79.5%)still provides room for improvement.
But perhapsjust as important, the nearest neighbor approachfacilitates the most reproducibile results in exper-iments since there is little ambiguity in how theconfounder is selected.Realistic Confounders: Despite its over-optimism, the random approach to confounder se-lection may be the correct approach in some cir-cumstances.
For some tasks that need selectionalpreferences, random confounders may be more re-alistic.
It?s possible, for example, that the optionsin a PP-attachment task might be distributed morelike the random rather than nearest neighbor mod-els.
In any case, this is difficult to decide withouta specific application in mind.
Absent such spe-cific motiviation, a nearest neighbor approach isthe most conservative, and has the advantage ofcreating a reproducible experiment, whereas ran-dom choice can vary across design.Training Size: Training data improves the con-ditional probability baseline, but does not help thesmoothing model.
Figure 5 shows a lack of im-provement across training sizes for both jaccardand cosine implementations of the Erk model.
TheTrain x1 size is approximately the same size usedin Erk (2007), although on a different corpus.
Weoptimized argument cutoffs for each training size,but the model still appears to suffer from addi-tional noise that the conditional probability base-line does not.
This may suggest that observing atest argument with a verb in training is more re-liable than a smoothing model that compares alltraining arguments against that test example.High Precision Baseline: Our conditionalprobability baseline is very precise.
It outper-forms the smoothed similarity based Erk modeland gives high results across tests.
The only com-bination when Erk is better is when the trainingdata includes just one year (one twelfth of theNYT section) and the confounder is chosen com-451Varying the Training SizeBucket Frequency Neighbor FrequencyTrain x1 Train x2 Train x10 Train x1 Train x2 Train x10Baseline 87.5 89.1 91.7 78.4 79.5 81.2Erk-Jaccard 86.5* 82.7* 83.1* 66.8* 68.1* 65.5*Erk-Cosine 82.1* 81.8* 81.1* 66.1* 65.3* 65.7*Google - - 70.4* - - 59.4*Backoff Erk 92.6* 91.8* 92.6* 79.4* 80.8* 81.7*Backoff Google 88.6 89.7 91.9?
78.7 79.8 81.2Figure 5: Accuracy of varying NYT training sizes.
The left and right tables represent two confounderchoices: choose the confounder with frequency buckets, and choose by nearest frequency neighbor.Trainx1 starts with year 2001 of NYT data, Trainx2 doubles the size, and Trainx10 is 10 times larger.
*indicates statistical significance with the column?s Baseline at the p < 0.01 level, ?
at p < 0.05.pletely randomly.
These results appear consistentwith Erk (2007) because that work used the BNCcorpus (the same size as one year of our data) andErk chose confounders randomly within a broadfrequency range.
Our reported results include ev-ery (vd, n) in the data, not a subset of particu-lar semantic roles.
Our reported 93.9% for Erk-Jaccard is also significantly higher than their re-ported 81.4%, but this could be due to the randomchoices we made for confounders, or most likelycorpus differences between Gigaword and the sub-set of FrameNet they evaluated.Ultimately we have found that complex modelsfor selectional preferences may not be necessary,depending on the task.
The higher computationalneeds of smoothing approaches are best for back-ing off when unseen data is encountered.
Condi-tional probability is the best choice for seen exam-ples.
Further, analysis of the data shows that asmore training data is made available, the seen ex-amples make up a much larger portion of the testdata.
Conditional probability is thus a very strongstarting point if selectional preferences are an in-ternal piece to a larger application, such as seman-tic role labeling or parsing.Perhaps most important, these results illustratethe disparity in performance that can come aboutwhen designing a pseudo-word disambiguationevaluation.
It is crucially important to be clearduring evaluations about how the confounder wasgenerated.
We suggest the approach of sortingnouns by frequency and using a neighbor as theconfounder.
This will also help avoid evaluationsthat produce overly optimistic results.9 ConclusionCurrent performance on various natural languagetasks is being judged and published based onpseudo-word evaluations.
It is thus importantto have a clear understanding of the evaluation?scharacteristics.
We have shown that the evalu-ation is strongly affected by confounder choice,suggesting a nearest frequency neighbor approachto provide the most reproducible performance andavoid overly optimistic results.
We have shownthat evaluating entire documents instead of sub-sets of the data produces vastly different results.We presented a conditional probability baselinethat is both novel to the pseudo-word disambigua-tion task and strongly outperforms state-of-the-artmodels on entire documents.
We hope this pro-vides a new reference point to the pseudo-worddisambiguation task, and enables selectional pref-erence models whose performance on the tasksimilarly transfers to larger NLP applications.AcknowledgmentsThis work was supported by the National ScienceFoundation IIS-0811974, and the Air Force Re-search Laboratory (AFRL) under prime contractno.
FA8750-09-C-0181.
Any opinions, ndings,and conclusion or recommendations expressed inthis material are those of the authors and do notnecessarily reect the view of the AFRL.
Thanksto Sebastian Pado?, the Stanford NLP Group, andthe anonymous reviewers for very helpful sugges-tions.452ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In ChristianBoitet and Pete Whitelock, editors, ACL-98, pages86?90, San Francisco, California.
Morgan Kauf-mann Publishers.Shane Bergsma, Dekang Lin, and Randy Goebel.2008.
Discriminative learning of selectional prefer-ence from unlabeled text.
In Empirical Methods inNatural Language Processing, pages 59?68, Hon-olulu, Hawaii.Lou Burnard.
1995.
User Reference Guide for theBritish National Corpus.
Oxford University Press,Oxford.Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.1999.
Similarity-based models of word cooccur-rence probabilities.
Machine Learning, 34(1):43?69.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In 45th Annual Meet-ing of the Association for Computational Linguis-tics, Prague, Czech Republic.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
Work on statistical methods forword sense disambiguation.
In AAAI Fall Sympo-sium on Probabilistic Approaches to Natural Lan-guage, pages 54?60.Tanja Gaustad.
2001.
Statistical corpus-based wordsense disambiguation: Pseudowords vs. real am-biguous words.
In 39th Annual Meeting of the Asso-ciation for Computational Linguistics - Student Re-search Workshop.David Graff.
2002.
English Gigaword.
LinguisticData Consortium.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29(3):459?484.Maria Lapata, Scott McDonald, and Frank Keller.1999.
Determinants of adjective-noun plausibility.In European Chapter of the Association for Compu-tational Linguistics (EACL).Preslav I. Nakov and Marti A. Hearst.
2003.
Category-based pseudowords.
In Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 67?69, Edmonton, Canada.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
In31st Annual Meeting of the Association for Com-putational Linguistics, pages 183?190, Columbus,Ohio.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via em-based clustering.
In37th Annual Meeting of the Association for Compu-tational Linguistics, pages 104?111.Hinrich Schutze.
1992.
Context space.
In AAAI FallSymposium on Probabilistic Approaches to NaturalLanguage, pages 113?120.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Inter-national Conference on Computational Linguistics(COLING).Beat Zapirain, Eneko Agirre, and Llus Mrquez.
2009.Generalizing over lexical features: Selectional pref-erences for semantic role classification.
In JointConference of the 47th Annual Meeting of the As-sociation for Computational Linguistics and the4th International Joint Conference on Natural Lan-guage Processing, Singapore.453
