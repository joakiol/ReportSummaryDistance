Spontaneous Speech Effects In Large Vocabulary Speech Recognition ApplicationsJohn Butzberger, Hy Murveit, Elizabeth Shriberg, Patti PriceSRI InternationalSpeech Research and Technology ProgramMenlo Park, CA 94025ABSTRACTWe describe three analyses on the effects of spontaneousspeech on continuous speech recognition performance.
Wehave found that: (1) spontaneous speech effects ignifi-cantly degrade recognition performance, (2) fluent sponta-neous speech yields word accuracies equivalent to readspeech, and (3) using spontaneous speech training data cansignificantly improve performance for recognizing sponta-neous speech.
We conclude that word accuracy can beimproved by explicitly modeling spontaneous effects in therecognizer, and by using as much spontaneous speech train-ing data as possible.
Inclusion of read speech training data,even within the task domain, does not significantly improveperformance.1.
INTRODUCTIONRecognition of spontaneous speech is an important featureof database-query spoken-language systems (SLS).
How-ever, most speech recognition research as focussed onacoustic and language modeling developed for recognitionof read speech \[1\].
Read speech as been used extensivelyin the past for both training and testing speech recognitionsystems because it is significantly less expensive to collectthan spontaneous speech, and because the lexical and syn-tactic content of the data can be controlled.The multi-site data collection effort \[3\] has provided achal-lenging corpus for research and development in the AirlineTravel Information System (ATIS) domain.
We haveobserved a significant increase in word error rate comparedto the previous task domain, the read-speech navalResource Management (RM) task \[2,6\].
Word error ratesfor RM systems have typically been in the 5% range,whereas ATIS word error rates have exceeded 10% \[4\], forcomparable perplexities.The speaking style typically exhibited in the RM domainhad a very consistent rate and articulation, within andacross entences, and across speakers.
There were no dis-fluencies, such as word fragments, hesitations, or self-edits,since utterances containing these effects were removedfrom the corpus.
The utterances tended to be short anddirect (3.3 seconds long, on average).
No pause fillers (uh,um), false starts, repairs, or excessively long pausesoccurred.
The speakers were able to concentrate on speechproduction, rather than query formation or problem solv-ing.
Furthermore, the training and testing texts were gener-ated using a fixed vocabulary, and with the same, knownlanguage model, which quite adequately represented thesource and target languages.The speaking style typically exhibited in the ATIS domaindiffers from that in the RM domain all of the above aspects.The speaking rate is highly inconsistent, both within utter-ances, across utterances within a session, and across ses-sions and speakers.
The articulation is highly variable, withstressed forms of function words and reduced forms of con-tent words typically not observed in read speech.
The sen-tence lengths vary widely, and are typically longer than RMsentences (7.5 seconds long, on average).
Some words inATIS sentences may not exist in the recognizer's lexicon,and an appropriate language model must be developed.Most importantly, however, ATIS speech contains ponta-neous effects and disfluencies: filled pauses, stressed orlengthened function words, false-starts and self-edits, wordfragments, breaths, long pauses, and extraneous noisessuch as paper ustling and beeps.
Data collected using sys-tems containing automatic speech recognition and naturallanguage components contain frequent occurrences ofhyperarticulated words, elicited by the subjects in anattempt to overcome recognition or understanding errors\[5\].
Additionally, the data have been collected in normaloffice conditions (rather than in a soundproof booth), andrecording quality and conditions vary across ites \[3\].2.
ERROR ANALYSISWe begin by analyzing the errors that occurred in the Feb-ruary 1991 evaluation set of 148 Class-A sentences, forwhich our recognition word error rate exceeded 18%.These sentences are examined because they are believed tobe a particularly difficult sampling of ATIS speech.339Phonetic alignments were automatically generated corre-sponding to both the reference and recognized word strings,and we: listened to each utterance was listened to very care-fully.
The acoustic and language model scores were com-pared, and a subjective judgment was made as to the likelysource of the error (the acoustic model, the language model,the articulation quality of the segment, or other effects uchas breaths, out-of-vocabulary words, or extraneous noise).We found that 30% of the errors (Table 1) could be attrib-uted to poor articulation or poorly modeled articulation(usually reductions, emphatic stress, or speaking rate varia-tions), 20% were due to out-of-vocabulary words or poorbigram probabilities, 20% were due to urnnodeled pause-fillers (uh, um, breaths), and the remaining portion unex-plainable, but probably due to inadequate acoustic-phoneticmodeling.We see that 70% of the errors are due to effects observed inthe ATIS domain, but not in the RM domain.
If these rrorswere removed, we would approach an error rate typicallyseen in a comparable RM system (with a perplexity 60wordpair grammar).CorpusATIS onlyCause for ErrorPoor ArticulationPortion30%Vocabulary and Grammar 20%Pause Fillers 20%ATIS and RM Other 30%Table 1: Summary of error sources for the Class-AFeb91 ATIS evaluation set (148 sentences).3.
READ VS. SPONTANEOUS SPEECHTo deterrnme the impact of spontaneous versus read speak-ing styles on recognition performance given a fixed trainingcondition, arecognition experiment with two test sets wasconstructed.
The first set contained spontaneous speechutterances; the second set contained read versions of thosesame utterances, given later by the same subjects.The training data consisted ofRM, TIMIT, and pilot-corpusATIS utterances (with the read-spontaneous and spontane-ous test data held out).
This left rather little ATIS-specificdata for training, almost none of it spontaneous.
The recog-nition was run without a grammar (perplexity 1025) toremove any corrective ffects of the grammar, so that onlythe acoustic effect of the spontaneous speech could be eval-uated.
The spontaneous test sentences were categorized aseither fluent or disfluent based on the existence of specialmarkings in their corresponding SRO* files.We found that the primary difference in error rates betweenthe read and spontaneous test sets was due directly to disflu-encies in the spontaneous speech (Table 2).
Non-disfluentspontaneous speech ad the same error rate as read speech.The disfluencies include pause-fillers, word fragments,overly lengthened oroverly stressed function words, self-edits, mispronunciations, and overly long pauses.
This listof disfluency types is derived from the special markingsused in the SRO transcriptions.
The observation that non-disfiuent spontaneous speech error rate approaches readspeech error rate is consistent with the fact that the testspeech much more closely resembles the training data.
Thetraining data was fluently and consistently articulated, justas was the non-disfluent spontaneous speech.CharacteristicReadSpontaneousSpontaneous - DisfluentSpontaneous - FluentNum WordSents Error241 33%241 43%97 56%144 32%Table 2: Error rate versus peaking style.
Readspeech and fluent spontaneous speech ave equivalenterror rates.The breakdown of error rate versus disfluency type (Table3) shows that a significant portion of the errors were due tofilled pauses, long pauses, lengthenings, and stress.
Sen-tences with these disfluencies had twice the word error rateof fluent speech.
The filled pause rrors happened becausethere were no models for breath/uh/um events in this partic-ular recognizer's lexicon.
The stress and lengthening errorshappened (most likely) because of the lack of sufficientobservations of these events in the training data, andbecause of the lack of explicit models for these ffects.
Thelong pauses usually caused insertions within the pauseregions neighboring the phrase-initial nd phrase-finalwords.From these observations, we conclude that more trainingdata containing these effects would improve the matchbetween the acoustic models and the spontaneous testspeech, and therefore would improve the recognition per-formance.
Furthermore, these ffects hould be explicitlymodeled m the recognizer's lexicon, once sufficient trainingdata is obtained.
However, this process depends on the reli-ability of the SRO labeling across ites, which tends to besubjective and inconsistent.
*The SRO transcription contains adetaileddescription of all the acoustic events occurring in autterance.340Disfluency Num DisfluencyType Sents Causes ErrorSelf-Edit 7 71%Filled Pause 24 92%Long Pause 17 94%Lengthening 36 81%Stress 22 59%Mispronunciation 2 100%Fragment 5 100%Table 3: Number of sentences afflicted with eachdisfluency type, and the percentage ofoccurrenceswhere the disfluency causes an error.4.
TRAINING DATA VARIATIONSFurther evidence for the importance of modeling spontane-ous phenomena is found by manipulating the content of thetraining data sets that are used for acoustic-phonetic model-ing.
In this experiment, we compare spontaneous speechrecognition performance given different combinations ofread, spontaneous, ATIS, and non-ATIS training subsets.The training subsets (Table 4) consist of the standard RMand TIMIT training data, and read and spontaneous subdi-visions of all the ATIS and MADCOW data available as ofOctober 1, 1991.
The "Breaths" corpus refers to an inter-nally collected atabase of inhalations and exhalations,used to train a breath model, which is allowed to occuroptionally between words during recognition.
Much of theATIS-read data was also collected intemally at SRI.CorpusATIS-ReadSize7,932ATIS-Spontaneous 6,745TIMIT 4,200Resource Management 3,990Breaths 800Table 4: Training data subsets, which are combinedin various ways to determine the impact of read andspontaneous training data on recognition of spontaneousspeech.Recognition was conducted using a development test-set of447 spontaneous MADCOW utterances \[3\], with a perplex-ity 20 bigram grammar trained on all the available sponta-neous peech transcriptions (roughly 10,000 sentences).
Allof the experiments outlined below use discrete-distributionHMMs, and every training set combination i cludes the 800breath utterances.Using all the available ATIS and MADCOW data yielded asystem with a word error rate of 9.6% (Table 5).
Using onlyspontaneous ATIS speech reduced performance by only 6%,to 10.2% word error.
Training with a roughly equivalentquantity of read ATIS speech increased the error rate signif-icantly, by 58% to 15.2%.
This suggests that having gainingdata which is consistent in speaking mode with the test datacan significantly improve performance.
However, the effectof lexical and phonetic overage in the training sets mightbe a factor in causing this performance difference.
Thisissue is discussed inSection 5.Training Set Size ErrorATIS-Read 8,732 15.2%ATIS-Spontaneous 7,545 10.2%ATIS-All 15,477 9.6%Table 5: Training set variations for ATIS-only systems.This table indicates that having speaking-mode-consistentdata is a major contributor to performance improvement.We also look at the impact of using non-ATIS read speechfor additional training data (Table 6).
Using successivelymore training data gives the expected result, an improve-ment in performance.
However, when using all the availabledata (RM, TIMIT, ATIS and MADCOW), the performancematches that of the system gained exclusively on ATIS andMADCOW data.
Furthermore, the performance ofthe sys-tem trained using all the available read speech (16,922 sen-tences) performed much worse than the system gained onlyon spontaneous speech (7,545 sentences).Training SetTIMITSize5,000Error26.9%TIMIT + RM 8,990 20.5%TIMIT + RM + ATIS-Read 16,922 14.6%TIMIT + RM + ATIS-All 23,667 9.6%Table 6: Training set variations using non-ATIS data.The error rates is reduced when ATIS-read data is added,and is reduced further when ATIS-spontaneous data isadded.341We can conclude from these xperiments hat having speak-ing-mode-consistent training data is more important thansimply having a large quantity of training data.
However,we cannot be certain that the phonetic ontent of the ATIS-spontaneous training set better matches the development setthan the ATIS-read training set.
This issue is addressed inthe next section.We compared the errors of two different recognizers usedon the same test set of spontaneous speech.
Both recogniz-ers were trained on a comparable number of utterances, butone recognizer was trained on read speech only (TIM1T+R-M+ATIS-Read), and the other on spontaneous speech only(ATIS-Spontaneous).
We found that substitutions of onefunction word for another form a significant portion of theerrors in both test sets, and in roughly the same proportions.However, there were significantly fewer substitutions ofcontent words for other content words for the recognizertrained on spontaneous speech compared to the recognizertrained on read speech.Similarly, the recognizer trained on spontaneous speechmanifested significantly fewer errors in substitution of apause filler for a function word.
"Homophone" errors,which can lead to understanding errors, formed a significantportion of the errors in the recognizer trained on readspeech, although almost none of these appeared for the rec-ognizer trained on spontaneous speech.
We believe that thisis because many words that can be homophonous in readspeech ("for"-"four" and "to"-"two", for example) are nolonger homophones in spontaneous speech ("fer"-"four"and "tuh"-"two").5.
Phonetic Coverage AnalysisOne potential reason for the dramatic performance varia-tions could be that the phonetic ontent of the developmenttest-set is better covered by the ATIS-Spontaneous subsetthan the ATIS-Read subset.
In this section, we attempt todisprove that theory, giving further strength to the argumentthat speaking-mode consistency is the primary factor affect-ing performance.We reason that the more detailed (more context-dependent)acoustic-phonetic models there are available for testing, themore adequate the training data has been in representingthis dimension (the better the phonetic overage).
There-fore, for this analysis, we determine the average contextlevel (or detail) of HMM states that each frame of test datavisits during recogmtion.
This is computed by assigning aninteger-valued number to each model type (increasing ascontext level increases), then computing the percentage ofall frames of data visiting states corresponding toa particu-lar level of context.The series of context-dependent model types used in theDECIPHER system is listed in Table 7.
A model with a par-ticular context level will be generated by the DECIPHERtrainer if there is sufficient data to train that model.Model Type Context LevelMonophone 1Left-general biphone 2Right-general biphone 2Left biphone 3Right biphone 3General triphone 4Left-general triphone 5Right-general triphone 5Triphone 6Word-specific 7Table 7: Assignments of an integer-valued contextlevel to each context-dependent model type.
Modelswith increasing detail are assigned higher context levelvalues.The expectation is that the higher the average context levelencountered during recognition, the better the performance.This trend is indeed captured in Table 8, where the systemwith the least task-specific training data (TIMIT) had theleast average context level (and the lowest performance),and the system with the most training data (TIMIT+RM+ATIS-All) had the highest average context level (and thehighest performance).The important point to note is that the average context levelof the best-trained read speech system (TIMIT+RM+ATIS-Read) was roughly equal to that of the best spontaneous-only system (ATIS-Spontaneous), but the error rate was sig-nificantly higher (14.6% versus 10.2%, respectively).
Thissuggests that although models of equivalent detail are beingused for recognition, the performance difference is due tothe spontaneous speaking-mode of the training set, which isconsistent with the speaking-mode of the test set.342Training SetsTIMIT+RM+ATIS-AllErrorRate9.6%ContextLevel6.31ATIS-All 9.6% 6.26ATIS-Spontaneous 10.2% 6.03TIMIT+RM+ATIS-Read 14.6% 6.14ATIS-Read 15.2% 5.96TIMIT+RM 20.5% 5.06TIMIT 26.9% 4.56Table 8: Context level versus word error.
This tableindicates that despite similar model detail (contextlevel), the spontaneous-trained system significantly out-performs the best read-trained system.3.4.5.6.DARPA Speech and Natural Language Workshop, R.Stem(ed.
), Morgan Kaufmann, 1990.MADCOW, "Multi-Site Data Collection for a Spoken Lan-guage System," Prec.
DARPA Speech and Natural Lan-guage Workshop, M. Marcus (ed.
), Morgan Kaufmann,1992.Murveit, H., J. Butzherger, and M. Weintraub, "Perfor-mance of SRI's DECIPHER Speech Recognition Systemson DARPA's ATIS Task," Prec.
DARPA Speech and Natu-ral Language Workshop, M. Marcus (ed.
), Morgan Kauf-mann, 1992.Shriberg, E., E. Wade, and R Price, "Human-MachineProblem Solving Using Spoken Language Systems (SLS):Factors Affecting Performance and User Satisfaction,"Prec.
DARPA Speech and Natural Language Workshop, M.Marcus (ed.
), Morgan Kaufmann, 1992.Murveit, H., J. Butzherger, and M. Weintrauh, "SpeechRecognition i SRI's Resource Management and ATISSystems," Proc.
DARPA Speech and Natural LanguageWorkshop, P.Price (ed.
), Morgan Kaufmann, 1991.6.
CONCLUSIONThese studies have convinced us of the importance of usingas much spontaneous speech material as possible in trainingour system.
Furthermore, we have found that spontaneousspeech effects can significantly degrade recognition perfor-mance, although fluent spontaneous speech yields wordaccuracies equivalent to read speech.Word accuracy can be improved by using as much sponta-neous speech training data as possible, and by explicitlymodeling such effects in the recognizer's lexicon (such asoptional interword breath and pause-filler models).
Inclu-sion of read speech training data did not significantlyimprove performance, given that the phonetic overage ofthe training sets were roughly equivalent.AcknowledgmentsWe gratefully acknowledge support for this work fromDARPA through the Office of Naval Research contractN00014-90-C-0085.
The govemrnent has certain rights inthis material.
Any opinions, findings, and conclusions orrecommendations expressed in this material are those of theauthors and do not necessarily reflect he view of the gov-ernment funding agencies.REFERENCESI.2.Price, P., W. Fisher, J. Bernstein, and D. Pallet, "TheDARPA 1000-Word Resource Management Database forContinuous Speech Recognition," Prec.
ICASSP, 1988.Pallet, D., J. Fiscus, and J. Garofolo, "DARPA ResourceManagement Benchmark Test Results June 1990", Prec.343
