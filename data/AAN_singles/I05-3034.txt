Chinese Word Segmentation Based On Direct Maximum EntropyModelWu-Guang ShiPeking University, Beijing, 100871, Chinashiwuguang@pku.edu.cnAbstractChinese word segmentation is a fun-damental and important issue in Chi-nese information processing.
In orderto find  a unified approach for Chineseword segmentation, the author developa Chinese lexical analyzer PCWS usingdirect maximum entropy model.
Thepaper presents the general descriptionof PCWS, as well as the result andanalysis of its performance at the Sec-ond International Chinese Word Seg-mentation Bakeoff.1 IntroductionOch and Ney(2002) present a framework basedon direct maximum entropy model to constructthe machine translation system.
The model treatsknowledge sources as feature functions, and al-lows the system to be extended easily by addingnew feature functions.
We think the model canbe used to provide a unified approach for Chi-nese word segmentation.
PCWS is the systembased on this thinking.2 System DescriptionPCWS consists of four components: Word gen-eration, Disambiguation, Select the best wordsequence and Output the result.
They are de-scribed below.2.1 Word GenerationThe procedure of word generation involves twosteps: (1) generation of the common wordswhich are listed in the Dictionary.
(2) generationof the unknown words.
The unknown wordshandled by the system involve numeric expres-sion, time expression, personal name, locationname and organization name.
PCWS can recog-nize the abbreviation of person name, and fail tofind the abbreviation of location name and or-ganization name.PCWS constructs an integrated segmentationgraph.
The node in the graph is the minimalsegmentation unit that cannot be split in anystage that follows.
The unit consists of Chinesecharacter, punctuation, Arabic numeral stringand English character string.
Every word that begenerated is an edge in the graph.Making the integrated segmentation graph is toavoid the blind spots in segmentation, but itbrings the graph more complex, and make thesystem?s speed slow.Every word will belong to a class.Given a word Wi, its class is defined by Figure 1.ic?????
?????
?Wi    iff Wi is listed in the segmentation lexicon.PER iff Wi is a person nameLOC iff Wi is a location nameORG iff Wi is an organization nameNUM iff Wi is a numeral expressionTIME iff Wi is a time expressionFigure 1: Class Definition of word Wi2.2 DisambiguationIn constructing the graph, PCWS detect the am-biguities of the segmentation and classify theambiguities into two classes: the false ambiguityand the true ambiguity.
The former is simplysolved by querying a table.
The segmentationinformation around the true ambiguities will becollected and PCWS will give an estimate ofeach possible segmentation mode of the true193am guities.
These estimates will be used byf two proc-generated by each node incontextmodel tom entropy model andMbiselecting the best path.2.3 Select the Best Word SequenceThe procedure of this part consists oesses: generating the candidate word sequencesand finding the best word sequence.If S is a Chinese sentence which is a charactersequence, W is the all possible word sequencesgiven S, ?# = {W1?W2??
?WN} is a word se-quence, ?={C1?C2??
?CN} is a correspondingclass sequence of ?#.
We use Viterbi algorithmto generate the candidate paths.
In order to con-trol the search space, all the paths will be rankedby a class mode score.
The maximum number ofthe candidate pathsthe graph cannot be larger than a Numberthreshold we give.The class mode score we used can be written asgenerateScore( ) = P ( | )P ( )# #w w c cThe P(C) and P(?#?C) is similar to the one de-fined by Gao et al(2003).We use the direct maximum entropyfind the best word sequence.
If ??
is the bestpath we need.
W* is the candidate set.Giving the direct maximuneglecting its renormalization, we can obtain thefollowing decision rule:#m* 1arg m ax h ,#+ mw w mw w sO??
?
?
?
?S) is the feature function of the wordthes:, ) = genper( | )# i is w c?Hereiw c c   ?
?3) Candidate location name featurN3i==1h ( , ) = genloc( | )# i iw s w c??
?hi (W#?sequence.
The parameter ?i is the power offeature.In PCWS, we define five feature function1) Context feature#w s c1 contexth ( , ) = -log P ( )2) Candidate person name featureN2h (wi==1generategenper( | ) = i ii iw c ?-logP ( | )   iff PER0             elseeHeregenerate-logP ( | )   iff LOCgenloc( | ) =0             elsei i ii iw c cw c??
?4) Candidate organization name featureN4i==1h ( , ) = genorg( | )# i iw s w c?Heregenerate-logP ( | )   iff ORGgenorg( | ) =0                           elsei i ii iw c cw c??
?5) The length of the path5h ( , ) = Length( )#w s cWe realize the GIS algorithm which can handleany type of real-valued features to train the val-ues of 51O .2.4 Output the ResultThe component outputs the best word sequenceand adjusts the result form based on the standardof test corpora.
In PCWS, we only adjust theform of the unknown words the system recog-nizes.For example, in ?????????
?, ???????
and ?????
will be recognized inde-pendently as unknown word.
Base on the stan-dard of Msr corpora, we combine the continuouswords which belong to the class TIME.
?2.5 System OverviewThe overall architecture of our word segmenta-tion system is presented in figure 2.Generate Common WordsFigure 2: Overall architecture of PCWSDictionarywithFeatureLabelGenerate Unknown WordsDisambiguationGenerating Candidate Word SequencesFind The Best Word SequenceOutput the Result194TrackTOTAL TRUEWORD COUNTTOTAL TESTWORD COUNT P   R F OOV Roov RivMsr_Open  106873 106624 0.913 0.915 0.914 0.026 0.725 0.918Table 1: Test Result on Msr-open Track3 EvaluationBecause of the bakeoff?s rule and the limit oftime, we only attend the track of Msr_open.Table 1 is the result of PCWS in this bakeoff.Form the table, we can know the system is re-markable in OOV recognize.Due to only small named entity words are in-cluded in the PCWS?s dictionary, most ofnamed entity words are generated by the system.However, the system?s overall performance isnot in balance with its good Roov.
We noticeRiv of the result is low.
The main reason causesRiv low is the difference between the segmenta-tion standard of PKU training corpora and thesegmentation standard of MSR test corpora.
Thedifference is so distinctly even in the segmenta-tion standard of common words.
For instance:Example 1[Correct Result] ??????????????
?
??
????
????
??
??????
[PCWS Result] ?????????
?????
?
??
????
????
??
?????
?The result?s True Words Recall = 0.875, TestWords Precision = 0.778Example 2[Correct Result] ???
??
?
?
?
??
????????
[PCWS Result] ??
???????????????
?The result?s True Words Recall = 0.700, TestWords Precision = 0.700Before we get the result, we neglected the prob-lem.
We put our attention in named entity rec-ognize, which need the training corpora with thelabel information, so we use six month PKUcorpora to construct our system and not use wellthe MSR training corpora in the bakeoff.In order to know the influence of the problem,we test our system in PKU test corpora, Table 2is the result of PCWS in the test.Track  P  R  FPku_Open 0.973887 0.978405 0.976141Table 2: Test Result on Pku_openIt?s an excellent result and powerful proves oursuppose.We wish to make our system more adaptable todifferent standards in the near future.4 ConclusionWe have presented our Chinese word segmenta-tion system PCWS and its result for Msr_opentrack.
We are glad to see its good performanceof OOV recognize.
In the course of the bakeoff,we find some problems in PCWS.
We will try toselect more useful feature functions into the ex-isting segmentation model in future work.
Weare confident the system?s performance willhave a big progress next time.5 AcknowledgementThank Prof. Hou-Feng Wang for his attentionand aid.
The author would especially expressgratitude to his family, their encouragement andsupport make the author to hold the line.ReferencesFranz J. Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models forstatistical machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 295-302,Philadelphia, PA, July.Gao, Jianfeng, Mu Li and Chang-Ning Huang.
2003.Improved source-channel model for Chinese wordsegmentation.
In: ACL2003.195
