Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1438?1442,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsThe VerbCorner Project: Toward an Empirically-Based SemanticDecomposition of VerbsJoshua K. HartshorneDepartment of Brain and Cognitive SciencesMassachusetts Institute of Technology77 Massachusetts AvenueCambridge, MA 02139, USAjkhartshorne@gmail.comClaire Bonial, Martha PalmerDepartment of LinguisticsUniversity of Colorado at BoulderHellems 290, 295 UCBBoulder, CO 80309, USA{CBonial, MPalmer}@colorado.eduAbstractThis research describes efforts to use crowd-sourcing to improve the validity of the seman-tic predicates in VerbNet, a lexicon of about6300 English verbs.
The current semanticpredicates can be thought of semantic prim-itives, into which the concepts denoted by averb can be decomposed.
For example, theverb spray (of the Spray class), involves thepredicates MOTION, NOT, and LOCATION,where the event can be decomposed into anAGENT causing a THEME that was originallynot in a particular location to now be in thatlocation.
Although VerbNet?s predicates aretheoretically well-motivated, systematic em-pirical data is scarce.
This paper describes arecently-launched attempt to address this issuewith a series of human judgment tasks, posedto subjects in the form of games.1 IntroductionOne key application of Natural Language Processing(NLP) is meaning extraction.
Of particular impor-tance is propositional meaning: To understand ?Jes-sica sprayed paint on the wall,?
it is not enough toknow who Jessica is, what paint is, and where thewall is, but that, by the end of the event, some quan-tity of paint that was not previously on the wall nowis.
One must extract not only meanings for individ-ual words but also the relations between them.One option is to learn these relations in a largelybottom-up, data-driven fashion (Chklovski and Pan-tel, 2004; Poon and Domingos, 2009).
For instance,Poon and Domingos (2009) first extracts depen-dency trees, converts those into quasi-logical form,recursively induces lambda expressions from them,and uses clustering to derive progressively abstractknowledge.An alternative is to take a human-inspired ap-proach, mapping the linguistic input onto the kindsof representations that linguistic and psychologi-cal research suggests are the representations em-ployed by humans.
While the exact characteriza-tion of meaning (and by extension, thought) remainsan area of active research in the cognitive sciences(Margolis and Laurence, 1999), decades of researchin linguistics and psychology suggests that much ofthe meaning of a sentence ?
as well as its syntacticstructure ?
can be accounted for by invoking a smallnumber of highly abstract semantic features (usu-ally represented as predicates), such as causation,agency, basic topological relations, and directed mo-tion (Ambridge et al 2013; Croft, 2012; Jackend-off, 1990; Levin and Rappaport Hovav, 2005; Peset-sky, 1995; Pinker, 1989).
For instance, a given verbcan appear in some syntactic frames (Sally broke thevase.
Sally broke the vase with the hammer.
The vasebroke.)
and not others (*Sally broke the vase to thefloor.
*Sally broke John the vase.).
When verbs areclassified according to the syntactic frames they canappear in, most if not all the verbs in a class involvethe same set of abstract semantic features.1Interestingly, roughly these same features (causa-tion, etc.)
have been singled out by developmentalpsychologists as part of ?core knowledge?
?
a set ofearly-learned or perhaps innate concepts upon which1Whether all verbs in a class share the same abstract pred-icates or merely most is an area of active research (Levin andRappaport Hovav, 2005).1438the rest of cognition is built (Spelke and Kinzler,2007).
Thus these semantic features/predicates maybe not only crucial to describing linguistic mean-ing but may be central organizing principles for ahuman?s (reasonably successful) thinking about andconceptualization of the world.
As such, they pro-vide a potentially rewarding target for NLP.2 VerbNet2.1 Overview and StructurePerhaps the most comprehensive implementationof this approach appears in VerbNet (Kipper et al2008; based on Levin, 1993).
VerbNet classifiesverbs based on the syntactic frames they can appearin, providing a semantic description of each framefor each class.
An example entry is shown below:Syntactic Frame NP V NP PP.DESTINATIONExample Jessica sprayed the wall.Syntax AGENT V THEME {+LOC|+DEST CONF}DESTINATIONSemantics MOTION(DURING(E), THEME)NOT(PREP(START(E), THEME, DESTINATION))PREP(END(E), THEME, DESTINATION)CAUSE(AGENT, E)The ?Syntactic Frame?
provides a flat syntacticparse.
?Syntax?
provides semantic role labels foreach of the NPs and PPs, which are invoked in ?Se-mantics?.
VerbNet decomposes the semantics ofthis sentence into four separate predicates: 1) theTHEME (the paint) moves doing the event E; 2) atthe start of the event E, the THEME (the paint) isnot at the DESTINATION (on the wall), whereas 3)at the end of the event E, the THEME (the paint) isat the DESTINATION (on the wall), and; 4) the eventis caused by the AGENT (Sally).
Note that this cap-tures only the core aspects of semantics shared by allverbs in the class; differences between verbs in thesame class (e.g., spray vs. splash) are omitted.Importantly, the semantics of the sentence is de-pendent on both the matrix verb (paint) and the syn-tactic frame.
Famously, when inserted in the slightlydifferent frame NP V NP.DESTINATION PP.THEME?
?Sally sprayed the wall with paint?
?
?spray?
en-tails that destination (the wall) is now fully painted,an entailment that does not follow in the exampleabove (Pinker, 1989).2.2 Uses and LimitationsVerbNet has been used in a variety of NLP appli-cations, such as semantic role labeling (Swier andStevenson, 2004), inferencing (Zaenen et al 2008),verb classification (Joanis et al 2008), and informa-tion extraction (Maynard, Funk, and Peters, 2009).While such applications have been successful thusfar, an important constraint on how well VerbNet-based NLP applications can be expected to performis the accuracy of the semantics encoded in Verb-Net.
Here, several issues arise.
Leaving aside mis-categorized verbs and other inaccuracies, as notedabove VerbNet assumes that all verbs in the sameclass share the same core predicates, which may ormay not be empirically justified.
Given the numberof semantic predicates (146),2 verb entries (6580),and unique verb lemmas (6284) it is not feasible fora single research team to check, particularly since af-ter a certain number of verbs, intuitions become lessclear.
In any case, it may not be ideal to rely solelyon the intuitions of invested researchers, whose in-tuitions about subtle judgments may be clouded bytheoretical commitments (Gibson and Federenko,2013); the only way to ensure this is not the caseis through independent validation.
Unfortunately, ofthe 280 verb classes in VerbNet, this has been donefor only a few (cf Ambridge et al 2013).3 VerbCornerThe VerbCorner project was designed to addressthese issues by crowd-sourcing the semantic judg-ments online (gameswithwords.org/VerbCorner/).Several previous projects have successfully crowd-sourced linguistic annotations, such as Phrase De-tectives, where volunteers have contributed 2.5 mil-lion judgments on anaphoric relations (Poesio et al2012).
Below, we outline the VerbCorner projectand describe one specific annotation task in detail.3.1 Developing Semantic Annotation TasksCollecting accurate judgments on subtle questionsfrom naive participants with limited metalinguistic2Note that these vary in applicability from those specific toa small number of verbs (CHARACTERIZE, CONSPIRE) to thosefrequently invoked (BEGIN, EXIST).1439skills is difficult.
Rare is the non-linguist who canimmediately answer the question, ?Does the verb?throw,?
when used transitively, entail a change oflocation on the part of its THEME??
Thus, we beganby developing tasks that isolate semantic features ina way accessible to untrained annotators.We converted the metalinguistic judgments(?Does this verb entail this abstract predicate??)
intoreal-world problems, which previous research sug-gests should be easier (Cosmides and Tooby, 1992).Each judgment tasks involved a fanciful backstory.For instance, in ?Simon Says Freeze?, a task de-signed to elicit judgments about movement, theGalactic Overlord (Simon) decrees ?Galactic StayWhere You Are Day,?
during which nobody is al-lowed to move from their current location.
Partici-pants read descriptions of events and decide whetheranyone violated the rule.
In ?Explode on Contact?,designed to elicit judgments about physical contact,objects and people explode when they touch one an-other.
The participant reads descriptions of eventsand decides whether anything has exploded.3Each task was piloted until inter-coder reliabilitywas acceptably high and the modal response nearlyalways corresponded with researcher intuitions.
Assuch, these tasks cannot be used to establish whetherresearcher intuitions for the pilot stimuli are correct(this would be circular); however, there is no guar-antee that agreement with the researcher will gener-alize to new items (the pilot stimuli cover a trivialproportion of all verbs in VerbNet).3.2 Crowd-sourcing Semantic JudgmentsThe pilot experiments showed that it is possible toelicit reliable semantic judgments corresponding toVerbNet predicates from naive participants (see sec-tion 3.3).
At the project website, volunteers chooseone of the tasks from a list and begin tagging sen-tences.
The sentences are sampled smartly, avoid-ing sentences already tagged by that volunteer andbiased in favor of of the sentences with the fewest3Note that each task is designed to elicit judgments aboutentailments ?
things that must be true rather than are merelylikely to be true.
If John greeted Bill, they might have comeinto contact (e.g., by shaking hands), but perhaps they did not.Previous work suggests that it is entailments that matter, partic-ularly for explaining the syntactic behavior of verbs (Levin andRappaport Hovav, 2005)judgments so far.
Rather than assessing annotatorquality through gold standard trials with known an-swers (which wastes data ?
the answers to these tri-als are known), approximately 150 sentences werechosen to be ?over-sampled.?
As the volunteer tagssentences, approximately one out of every five arefrom this over-sampled set until that volunteer hastagged all of them.
This guarantees that any givenvolunteer will have tried some sentences targetedby many other volunteers, allowing inter-annotatoragreement to be used to assess annotator quality.Following the example of Zooniverse (zooni-verse.org), a popular ?Citizen Science?
platform,volunteers are encouraged but required to register(requiring registration prior to seeing the tasks wasfound to be a significant barrier to entry).
Regis-tration allows collecting linguistic and educationalbackground from the volunteer, and also makes itpossible to track the same volunteer across sessions.Multiple gamification elements were incorporatedinto VerbCorner in order to recruit and motivate vol-unteers.
Each task has a leaderboard, where thevolunteer can see his/her rank out of all volunteersin terms of number of contributions made.
In ad-dition, there is a general leaderboard, which sumsacross tasks.
Volunteers can earn badges, displayedon their homepage, for answering certain numbersof questions in each task.
Finally, at random inter-vals bonus points are awarded, with the explanationfor the bonus points tailored to the task?s backstory.VerbCorner was launched on May 21, 2013.
Aftersix weeks, 555 volunteers had provided at least oneannotation, for a total of 39,274 annotations, demon-strating the feasibility of collecting large numbers ofannotations through this method.3.3 Case Study: Equilibrium?Equilibrium?
was designed to elicit judgmentsabout application of force, frequently argued to bea core semantic feature in the sense discussed above(Pinker, 1989).
The backstory involves the ?Zen Di-mension,?
in which nobody is allowed to exert forceon anything else.
The participant reads descriptionsof events (Sally sprayed paint onto the wall) and de-cides whether they would be allowable in the ZenDimension ?
and, in particular, which participantsin the event are illegally applying force.In order to minimize unwanted effects of world1440knowledge, the verb?s arguments are replaced withnonsense words or randomly chosen proper names(Sally sprayed the dax onto the blicket).
In thecontext of the story, this is explained as necessaryanonymization: You are a government official de-termining whether certain activities are allowable,and ensuring anonymity is an important safeguardagainst favoritism and corruption.
An alternativewouod be to use multiple different content words,randomly chosen for each annotator.
However, thisgreatly increases the number of annotators neededand quickly becomes infeasible.3.3.1 Pilot ResultsThe task was piloted on 138 sentences, which com-prised all possible syntactic frames for three verbsfrom each of five verb classes in VerbNet.
Aftertwo rounds of piloting (between the first and second,wording in the backstory was adjusted for claritybased on pilot subject feedback and results), Kripp?salpha reached .76 for 8 annotators, which representsa reasonably high level of inter-annotator agreement.Importantly, the modal response matched the intu-itions of the researchers in 137 of 138 cases.43.3.2 Preliminary VerbCorner Results?Equillibrium?
was one of the first tasks posted onVerbCorner, with data currently being collected on12 of the 280 VerbNet classes, for a total of 5,171sentences.
As of writing, 414 users have submitted14,294 judgments.
Individual annotators annotatedanywhere from 1 to 195 sentences (mean=8, me-dian=4).
While most sentences have relatively fewjudgments, each of the 194 over-sampled sentenceshas between 15 and 20 judgments.5Comparing the modal response with the re-searchers?
intuitions resulted in a match for 184 of194 sentences.
In general, where the modal response4The remaining case was ?The crose smashed sondily.?
forwhich four pilot subjects thought involved the crose applyingforce ?
matching researcher intuition ?
and four thought didnot involve any application of force, perhaps interpreting thesentence was a passive.5These are the same 15 verbs used in the piloting.
The num-ber of sentences is larger in order to test a wider range of pos-sible arguments.
In particular, wherever appropriate, separatesentences were constructed using animate and inanimate argu-ments.
Compare Sally sprayed the dax onto Mary and Sallysprayed the dax onto the blicket.did not match researcher intuitions, the modal re-sponse was itself not popular, comprising an aver-age of 53% of responses, compared with an aver-age of 77% where the modal response matched re-searcher intuitions.
Thus, these appear to be cases ofdisagreement, either because the correct intuition re-quires more work to obtain or because of differencesacross idiolects (at the moment, there is no obviouspattern as to which sentences caused difficulty, butthe sample size is small).
Thus, follow-up investi-gation of sentences with little inter-coder agreementmay be warranted.4 Conclusion and Future WorkData-collection is ongoing.
VerbNet identifies ap-proximately 150 different semantic predicates.
An-notating every verb in each of its syntactic frames foreach semantic predicate would take many millionsof judgments.
However, most of the semantic predi-cates employed in VerbNet are very narrow in scopeand only apply to a few classes.
Thus, we have be-gun with broad predicates that are thought to applyto many verbs and are adding progressively narrowerpredicates as work progresses.
At the current rate,we should complete annotation for the half-dozenmost frequent semantic predicates in the space of ayear.Future work will explore using an individualannotator?s history across trials to weight thatuser?s contributions, something that VerbCorner wasspecifically designed to allow (see above).
How toassess annotator quality without gold standard datais an active area of research (Passonneau and Car-penter, 2013; Rzhetsky, Shatkay and Wilbur, 2009;Whitehill et al 2009).
For instance, Whitehill andcolleagues (2009) provide an algorithm for jointlyestimating both annotator quality and annotationdifficulty (including the latter is important becausesome annotators will have low agreement with oth-ers due to their poor luck in being assigned difficult-to-annotate sentences).
This algorithm is shown tooutperform using the modal response.Note that this necessarily biases against annota-tors with few responses.
In our case study above, ex-cluding annotators who contributed small numbersof annotations led to progressively worse match toresearcher intuition, suggesting that the loss in data1441caused by excluding these annotations may not beworth the increased confidence in annotation quality.Future research will be needed to assess this trade-off.The above work shows the feasibility of crowd-sourcing VerbNet semantic entailments, as has beenshown for a handful of other linguistic judgments(Artignan, Hascoet and Lafourcade, 2009; Poesio etal., 2012; Venhuizen et al 2013).
There are manydomains in which gold standard human judgmentsare scarce; crowd-sourcing has considerable poten-tial at addressing this need.ReferencesB.
Ambridge, J. M. Pine, C. F. Rowland, F. Chang, andA.
Bidgood.
2013.
The retreat from overgeneral-ization in child language acquisition: Word learning,morphology, and verb argument structure.
Wiley In-terdisciplinary Reviews: Cognitive Science.
4:47-62.G.
Artignan, M. Hascoet, and M. Lafourcade.
2009.Mutliscale visual analysis of lexical networks.
Pro-ceedings of the 13th International Conference on In-formation Visualisation.
Barcelona, Spain.T.
Chklovski and P. Pantel.
2004.
VerbOcean: Miningthe Web for fine-grained semantic relations.
Proceed-ings of Empirical Methods in Natural Language Pro-cessing (EMNLP).
Barcelona, Spain.L.
Cosmides and J. Tooby.
1992.
Cognitive adaptationsfor social exchange.
in The Adapted Mind.
(J. Barkow,L.
Cosmides, and J. Tooby, Eds.)
Oxford UniversityPress, Oxford, UK.W.
Croft.
2012.
Verbs: Aspect and Argument Structure.Oxford University Press, Oxford, UK.D.
R. Dowty.
1991.
Thematic proto-roles and argumentselection.
Language.
67:547-619.E.
Gibson and E. Fedorenko.
2013.
The need for quanti-tative methods in syntax and semantics research.
Lan-guage and Cognitive Processes.
28(1-2):88?124.R.
Jackendoff.
1990.
Semantic Structures.
The MITPress, Cambridge, MA.E.
Joanis, S. Stevenson, and D. James.
2008.
A generalfeature space for automatic verb classification.
Natu-ral Language Engineering.
14(3):337-367.K.
Kipper, A. Korhonen, N. Ryant and M. Palmer.
2008.A large-scale classification of English verbs.
Lan-guage Resources and Evaluation Journal, 42:21?40E.
Margolis and S. Laurence 1999.
Concepts: CoreReadings.
The MIT Press, Cambridge, MA.B.
Levin.
1993.
English Verb Classes and Alternations:A Preliminary Investigation.
University of ChicagoPress, Chicago.B.
Levin and M. Rappaport Hovav.
2005.
ArgumentRealization.
Cambridge University Press, Cambridge,UK.D.
Maynard, A. Funk, and W. Peters.
2009.
Usinglexico-syntactic ontology design patterns for ontologycreation and population.
Proceedings of Workshop onOntology Patterns (WOP 2009).
Washington, DCR.
J. Passonneau and B. Carpenter 2013.
The benefitsof a model of annotation.
7th Linguistic AnnotationWorkshop and Interoperability with Discourse.
Sofia,Bulgaria.D.
Pesetsky.
1995.
Zero Syntax: Experiencers and Cas-cades.
The MIT Press, Cambridge, MA.S.
Pinker.
1989.
Learnability and Cognition.
The MITPress, Cambridge, MA.M.
Poesio, J. Camberlain, U. Kruschwitz, L. Robaldo,and L. Ducceschi.
2012.
The Phrase Detective Multi-lingual Corpus, Release 0.1.
Proceedings of the Col-laborative Resource Development and Delivery Work-shop.
Istanbul, TurkeyH.
Poon and P. Domingos.
2009.
Unsupervised seman-tic parsing.
Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing.Singapore.A.
Rzhetsky, H. Shatkay, and W. J. Wilbur.
2009.
Howto get the most out of your curation effort.
PLoS Com-putational Biology, 5(5):1?13.E.
S. Spelke and K. D. Kinzler.
2007.
Core knowledge.Developmental Science, 10(1):89?96.R.
Swier and S. Stevenson.
2004.
Unsupervised seman-tic role labeling.
Proceedings of the Generative Lexi-con Conference, GenLex-09.
Pisa, Italy.N.
Venhuizen, V. Basile, K. Evang, and J. Bos.
2013.Gamification for word sense labeling.
Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS 2013).
Potsdam, GermanyJ.
Whitehill, P. Ruvolo, T. F. Wu, J. Bergsma.
and J.Movellan.
2009.
Whose vote should count more: Op-timal integration of labels from labelers of unknownexpertise.
Advances in Neural Information ProcessingSystems, 22.
Vancouver, CanadaA.
Zaenen, C. Condoravdi, and D G. Bobrow.
2008.
Theencoding of lexical implications in VN.
Proceedingsof LREC 2008.
Morocco1442
