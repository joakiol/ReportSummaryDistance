IIIIIIIIIIII|IIilIIIIIIIIIIIIIIII|I!|Implementing a Sense Tagger ina General Architecture for Text EngineeringHamish  Cunn ingham,  Mark  S tevenson  and  Yor ick  Wi lksDepar tment  of Computer  Science,Univers i ty of Sheffield,Regent Court ,  211 Portobel lo  Street,Sheffield S1 4DP, UK{hamish, marks, yorick}@dcs.shef.ac.ukhttp://www, des.
shef.
ac.
uk/research/groups/nlp/gate/AbstractWe describe two systems: GATE (General Ar-chitecture for Text Engineering), an architec-ture to aid in the production and deliveryof language ngineering systems which signifi-cantly reduces development time and ease ofreuse in such systems.
We also describe asense tagger which we implemented within theGATE architecture, and which achieves highaccuracy (92% of all words in text to a broadsemantic level).
We used the implementationof the sense tagger as a real-world task onwhich to evaluate the usefulness of the GATEarchitecture and identified strengths and weak-nesses in the architecture.1 In t roduct ionThis paper is about two things: a novel hybrid sensetagger for unrestricted text (Wilks and Stevenson,1997), and the experience of developing this sys-tem within GATE - a General Architecture for TextEngineering (Cunninham et al, 1997; Cunningham,Wilks, and Gaizauskas, 1996a).We hope you can forgive this mild schizophrenia- we feel that both topics are relevant o the sub-ject of new methods in NLP: the first because boththe problem of arriving at methods for sense tag-ging and of tuning those methods to specific domainsand lexical resources i an increasingly active topicin the field (Basili, Della Rocca, and Pazienza, 1997;Harley and Gleanon, 1997); the second because workon NLP components shares a heap of problems withother language processing work to do with reusabil-ity, data visualisation, and software-level robustnessand efficiency that, we feel, are best solved by theprovision of a inclusive and general architecture anddevelopment environment for the field.We begin with a review of the general conceptbehind GATE (section 2), then describe the practi-calities of the system that are relevant to the sensetagging system we have developed (section 3).
Nextwe discuss the sense tagging problem (section 4),and then our system (section 5).
Finally we look atthe experience of developing the tagger within thearchitecture (section 6), and draw out some lessonsfor the future (section 7).2 GATE - the conceptGATE is an architecture and development environ-ment for research and development workers in NLPand Language Engineering 1.
It is an architecturein the sense that it specifies a macro-level organisa-tionai pattern for the various components and dataresources that make up a language processing (ac-tually at present only text processing) system (Shawand Garlan, 1996).
It is also a development envi-ronment hat adds a rich set of graphical tools tothe architecture enabling the developer to easily in-tegrate new processing componemts, tomanage flowof control between components, tovisualise the dataproduced by and passed between components, andevaluate the contribution of components osome ex-ternally defined and measured language processingtask.As we've noted elsewhere (Cunningham ,Gaizauskas, and Wilks, 1995; Cunningham,Wilks, and Gaizauskas, 1996b), the motivatingfactors behind development of the architectureincluded the facilitation of reuse of components(which has previously been successful in NLP onlytThe application of NLP and CL theory to the cre-ation of practical applications oftware has recently be-come known as Language Engineering, or LE, or NLE,and has been defined in various ways in e.g.
(Mitkov,1996; Thompson, 1985; Boguraev, Garigiiano, and Tait,1995; Gazdar, 1996).
Our gloss on these various defin-itions is that Language Engineering is the discipline oract of engineering software systems that perform tasksinvolving processing human language.
Both the con-struction process and its outputs are measurable andpredictable.
The literature of the field relates to bothapplication of relevant scientific results and to a body ofpractise.Cunningham, Stevenson and Wilks 59 Implementing a Sense TaggerHamish Cunningham, Mark Stevenson a d Yorick WilEs (1998) Implementing a Sense Tagger in a General Architecture forText Engineering.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and ComputationalNatural Language Learning, ACL, pp 59-71.for data resources (Curmingham,, Freeman, andBlack, 1994; Cunningham, 1994)), comparative andtask-based evaluation, collaborative research, andsoftware-level robustness, efficiency and portability.The design we arrived at in support of these aims issketched in the rest of this section.NLP systems produce information about texts(which may sometimes be the results of automaticspeech recognition) and existing systems that aim toprovide software infrastructure for NLP can be clas-sifted as belonging to one of three types according tothe way in which they treat this information:addit ive, or markup-based:  information pro-duced is added to the text in the formof markup, e.g.
in SGML (Thompson andMcKelvie, 1996);referential ,  or annotat ion-based: informationis stored separately with references back to theoriginal text, e.g.
in the TIPSTER architecture(Grishman, 1996);abstract ion-based:  the original text is preservedin processing only as parts of an integrated datastructure that represents information aboutthe text in a uniform theoretically-motivatedmodel, e.g.
attribute-value structures in theALEP system (Simkins, 1994).A fourth category might be added to cater for thosesystems that provide communication and controlinfrastructure without addressing the text-specificneeds of NLP (e.g.
Verbmobil's ICE architecture(Amtrup, 1995)).As noted at a previous conference in this series(Cunningham, Wilks, and Gaizauskas, 1996b), webelieve that performance and other considerationsfavour the referential pproach, but also that SGMLis a key part of any general text processing strategy.The first design decision we made, then, was to baseGATE on a referential core using the TIPSTER ar-chitecture, and to cater for SGML via I/O formatconversion filters.
This led to the development ofone of three key pillars of the system: GDM, theGATE Document Manager.
GDM and the TIP-STER API that it implements forms a buffer be-tween processing modules in a GATE-based NLPsystem.
Modules no longer talk to each other, withthe coherence and coupling implications that directunrestricted communication can imply, but to GDMvia the TIPSTER API.One of the key benefits of adopting an explicit ar-chitecture for data management is that it becomespossible to easily add a of layer graphical interfaceaccess to architecural services and data visualisationtools, and such a layer is our second pillar: GGI, theGATE graphical interface.
GGI has functions forcreating, viewing and editing the collections of doc-uments which are managed by the GDM and thatform the corpora which LE modules and systems inGATE use as input data.
It also has facilities todisplay the results of module or system execution -new or changed annotations associated with the doc-ument.
These annotations can be viewed either inraw form, using a generic annotation viewer, or in anannotation-specific way, if special annotation view-ers are available.
For example, named entity annota-tions which identify and classify proper names (e.g.organization ames, person names, location names)are shown by colour-coded highlighting of relevantwords; phrase structure annotations are shown bygraphical presentation ofparse trees.
Note that theviewers are general for particular types of annota-tion, so, for example, the same procedure is used forany POS tag set, Named-Entity markup etc.
Thusdevelopers euse GATE data visualisation code withnegligible overhead.Lastly, the third pillar of the system is the one thatdoes all the real work of processing texts and discov-ering information about their content: CREOLE, aCollection of REusable Objects for Language Engi=neering.
In a sense CREOLE isn't part of GATE atall, but is the set of resources currently integratedwith the system, but we also use the term to refer tothe mechanismss available for integrating modulesinto GATE.
This process has been automated to alarge degree and can be driven from the interface.The developer is required to produce some C++ orTcl code that uses the GDM TIPSTER API to getinformation from the database and write back re-sults.
When the module pre-dates integration, thisis called a wrapper as it encapsulates the module ina standard form that GATE expects.
When mod-ules are developed specifically for GATE they canembed TIPSTER calls throughout their code anddispense with the wrapper intermediary.
The under-lying module can be an external executable writtenin any language (the current CREOLE set includesProlog, Lisp and Perl programs, for example).CREOLE wrappers encapsulate informationabout the preconditions for a module to run (datathat must be present in the GDM database)and post-conditions (data that will result).
Thisinformation is needed by GGI, and is providedby the developer in a configuration file, whichalso details what sort of viewer to use for themodule's results and any parameters that needpassing to the module.
These parameters can bechanged from the interface at run-time, e.g.
to tellCunningham, Stevenson and Wilks 60 Implementing a Sense TaggerIIII!1IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIa parser to use a different lexicon.
Aside from theinformation needed for GGI to provide access toa module, GATE compatibility equals T IPSTERcompatibility - i.e.
there will be very little overheadin making any TIPSTER module run in GATE.Given an integrated module, all other interfacefunctions happen automatically.
For example,the module will appear in a graph of all modulesavailable, with permissible links to other modulesautomatically displayed, having been derived fromthe module pre- and post-conditions.
At any pointthe developer can create a new graph from a subsetof available CREOLE modules to perform a task ofspecific interest.The integration mechanisms also reduce the doc-umentation load: users can reference the TIPSTERAPI to describe the interchange format of the datathey produce and the GATE documentation for in-tegration details.
Of course GATE doesn't solve allthe problems involved in plugging diverse LE mod-ules together.
There are three barriers to such inte-gration:?
managing storage and exchange of informationabout texts;?
incompatibility of representation f informationabout texts;?
incompatibility of type of information used andproduced by different modules.GATE provides a solution to the first two of these,allowing the integrator to concentrate on the coreissue of the meaningful content of the informationexchanged.3 GATE - p ract i ca l i t iesA main purpose of GGI is to allow execution of themodules within GATE and to provide a graphicalaccess point to the results they produce.
Section3.1 describes the meaning of the primitives in thegraph and how it is executed, section 3.2 describesthe method used to autogenerate he graph, sec-tion 3.3 discusses the method of creating manageablesubgraphs, and section 3.4 discusses results visuali-sation facilities.3.1 Graph Syntax and Semant icsAn example of a system graph is shown in figure12.
A system graph is an executable graph, and is~These and other screen dumps below look better incolour!
The description below will be a bit like the TVsnooker commentator who said "For those of you watch-ing in black and white, the pink is behind the blue".a simple data flow program.
Modules are shown asnodes in the graph, with the data flow indicated bythe arcs.
Each incoming arc to a module indicatesa dependency on results of previous processing.
Allmodules at the source of arcs connecting to a de-pendent module must be run before the dependentmodule is executed, except where the incoming arcsare connected by lines, in which case the module re-quires the execution of only one of the modules atthe other end of the arc (these arcs are then termedor-arcs).
Thus, in the example graph of figure 1,the buChart Parser module may only be run if theresults of the Gazetteer  Lookup module and eitherthe Tagged Morph module or the Morph module areavailable.
They in turn have earlier dependencies.The Tokenizer module has no dependencies andso begins execution.
There are two modules withno downstream children: MUG-6 Results and MUG-6NE Results,  so either of these must produce an endresult.
However, because results from modules inthe middle of the graph may be of interest to a NLPresearcher, any module can be chosen as the finalone that will be executed.
,e ~..At any point in time, the state of execution of thesystem, or, more accurately, the availability of datafrom various modules, is depicted through colour-coding of the module boxes.
Figure 1 shows a sys-tem window.
Light grey modules (green, in the realdisplay) can be executed.
Modules that require in-put from others not yet executed, and so cannot beexecuted yet, are shown with a white background(amber, in reality).
The modules that have alreadybeen executed are shown in dark grey (red), at whichpoint their results are available from a menu associ-ated with each box (see below).The system graph can either be run in batch modeor in an interactive manner.
To run in batch mode,the user selects a path though the graph and clickson the final module.
The current state of the graph,and the document (or collection of documents) cur-rently undergoing execution is shown.
The systemensures that the path chosen by the user is valid byonly allowing a module to be selected if all its inputshave already been selected.
Selected modules are ex-ecuted in a data driven manner, with modules beingexecuted as soon as their input data is available.The interactive mode is designed for module de-velopers.
The modules under development can beexecuted as with the batch mode then the moduleor modules to be retried (after the underlying codeor resources have been changed) can be reset by amouse click.
This clears the database of the post-condition annotations and allows the modules to bererun.Cunningham, Stevenson and Wilks 61 Implementing a Sense TaggerCollection: Jhome/IPete rr/gatelB m'kl~Figure 1: The GATE System GraphThe nature of-the database (where each moduleproduces a specific set of annotation types) meansthat it is possible to view partial results of execu-tion without recourse to buffering intermediate data(Woodruff and Stonebreaker, 1995).3.2 Autogenerat ionThe graph shown in figure 1 is in fact the customgraph.
This is the system graph that shows allthe modules in the particular GATE environment.The custom window is automatically generated fromthe configuration i formation that is associated witheach module, e.g., for the buChart module:seZ creole_config(buchart) {title {buChart Parser}pre_condi~ions {document_attributes {language_english}annotations {token sentence morph lookup}}post_conditions {document_attributes {language_english}annotations {name syntax semantics} }viewers {{name single_span}{syntax tree}{semantics raw}}}This data structure (actually a Tcl array (Ouster-hout, 1994)) describes the TIPSTER objects thata module requires to run, the objects it produces,and the types of viewers to use for visualising its re-sults.
Along with code that uses the TIPSTER APIto get information from the database and to storeresults back there, this configuration file is all thatan integrator need produce to connect a module toGATE.
Typically the biggest overhead here is con-verting pre-existing modules to preseve byte-offsetinformation.
See (Cunningham et al, 1996) for de-tails.The autogeneration algorithm creates data flowarcs from modules that have an annotation typein their postconditions to the other modules thathave the same annotation type in their precondition.For example, Gazetteer  Lookup has the annotationtype lookup in its postconditions, soan arc connectsit with buChart Parser,  which has that annotationtype in its preconditions.
Arcs are not created be-tween modules that operate on different languages,however in figure 1, all the modules operate on Eng-lish language documents.
When more than one mod-ule has the same annotation type in its postconditionthen it is assumed that either module may producethe required result, and so the two arcs are or-arcsand are connected by a line (both l~orph and TaggedHorph produce the same annotation and so have or-arcs into buChart Parser).The most computationally expensive part of auto-generation goes into discarding redundant arcs.
Re-dundant arcs are those that connect an upstreammodule to a downstream odule where it can bededuced that the preconditions of modules betweenthe two given modules cover the annotation typesthat the arc represents.
For example, the Tokenizerproduces annotation types required by buChartParser ,  but there is no need for a data flow arc be-tween these modules as modules between them alsorequire these annotation types.The autogeneration facility allows easy integrationof new modules into GGI.
Most NLP tasks can beexpressed in the simple data flow techniques of thissystem, but it is currently not possible to integrateNLP tasks that require iteration.Some modules have the same annotation type inboth pre- and postconditions.
These modify the re-sult of previous computation and pass the data flowCunningham, Stevenson and Wilks 62 Implementing a Sense TaggerIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIil|mNIIBmBBBg|BI|down stream.
This kind of module, termed a filter,cannot be automatically positioned in the diagram,instead the user selects the position of filters fromthe arcs on which they may appear (arcs from mod-ules that produce the annotation type the filter op-erates on).
During execution filters are treated asnormal modules.3.3 Customis ing  GraphsThe system graphs are displayed with a graph draw-ing tool which is is also used in tree based visuali-sation tools available for display of e.g.
syntacticparse results.
This tool allows commands to be as-sociated with nodes, hence it can be used for dataflow graphs.
It has a layout algorithm based onthe method used by daVinci (FrShlich and Werner,1995) to minimise arc crossing.GGI suffers from the scaling problem (Burnett etal., 1987), as the size of the custom graph quicklybecomes unmanageable.
This can be alleviated bycreating new system graphs from specified subgraphsof the custom graph.
A later release will allow col-lapsing of graph sections.It is possible to group these derived system graphstogether so that the user may chose from a selectionof tasks at the top level of GGI (not shown here forspace reasons).
Having chosen a task (e.g.
parsing),an intermediate l vel display appears, presenting theuser with a selection of icons, one for each of the oneor more specific systems capable of performing theselected task (e.g.
the buChart parser or the Plinkparser).
Once a particular system is selected, a finalwindow appears displaying the appropriate systemgraph.3.4 Visuallsing ResultsNLP data is wide ranging in scope but has specificcharacteristics that mean the problems with visual-ising large amounts of data (Burnett et al, 1987)are less significant.
This is because ither the infor-mation can be visualised as coloured markup on thetext (meaning that the text can be displayed usingtraditional textual techniques (Jonassen, 1982)), orthe information is grouped over small segments oftext, such as paragraphs or sentences.GGI has several viewers for the display of TIP-STER annotations.
The viewer for each postcondi-tion annotation is specified by the module config-uration file, an example of which i o given in sec-tion 3.2.
The viewers can be classified into thosewhich display the text and overlay the annota-tions as colours or shades ('single span', 'multiplespan', 'text-attribute'); and those that visualise amore complex relationship between annotations in= ?
- w~ ~ to the a~ly  ~e~s l~ i t?oa of vicul ~ c~ ky  la~ Cotla:;, a ~iv~e~y heldwa~ l~eVicL~ly ~c,~lde~.
a~l ?
:~.~ q~at .
i~g of f i~~ese p~eit lens ~ ' t  be filled, la -~d,  Larry ~.
Harl~.~eviauely e~ive  v i~ ~Id~ of ~:$; ?
~ i ~  |~?
the~ i ~  uuit, was n ~  tO the ~ ly  ~eated ~ of ~e~id~ of~:S: opezatiaa~.
~d alaug with the head ~ Interaatloaalo~a~i~,  ~ill re1~ gl~re~ly to Oal~ P. P~,  t~ ~LVOaf f~ of t~ l~a:mt ~m~m~,~efle~s the ~ugreu?
e~is  ~t Mazy ~ay an i~e~a~ti~a2.
'~a~i~.
~ ~i l l  be invo lv  '~1 ?n d~/elopla~ the~era~: i~t  ?aqam~la~ ~z~y,  ~ aald.5D~mtss IFigure 2: Multiple Span Vieweran acyclic graph format ('tree').
Where no vieweris specified, a default annotation dump is displayed.The configuration file for the buChart Parser mod-ule in section 3.2 specifies that the 'name' annotationtype is assigned the 'single span' viewer, 'syntax' the'tree' viewer, and 'semantics' the 'raw' or annotationdump viewer.
New viewers can be written where thedefault ones are not appropriate for new annotationtypes.The 'single span' and 'text-attribute' viewers arefairly simple, assigning different colours to each an-notation.
'multiple span' is more complex, as it isdesigned to view annotation chains.
An annotationchain is a list of annotations specified by annotationreferences.
The user chooses a highlighted part ofthe text, and all the other highlights that are partof the same chain are displayed.
Figure 2 shows thisviewer displaying the results of a coreference task.Coreference identifies elements of the text that areinterpreted as referring to the same real world en-tity.
For example, a person and a pronoun might becoreferential.
In figure 2 the user has chosen one ofthe highlights referring to 'Richard Bartlett'.The 'tree' viewer containing 'syntax' annotations(produced by the buChart Parser) is shown in fig-ure 3.
The parse trees currently integrated intoGATE span at most a sentence, so that the tree sizeis always manageable.The viewers are activated by first clicking withthe mouse on a module whose results are present(i.e.
it has been executed and it's box has turnedred) which reveals a menu of annotations; choosingan annotation brings up the appropriate viewer.There is a certain amount of connectivity betweenthese viewers, as it is possible to click on a node inthe parse tree and have the area of text highlightedin a text display window, or it is possible to highlightareas of text and display the raw annotations thatare contained within the highlighted span.Cunningham, Stevenson and Wilks 63 Implementing a Sense Tagger__~fEI)!aca:l lb ) (  v J - ._D!,.
: IiRichard C. Bartl~&t was n ~4 to  ~ho n~ly  C~ga~. '
'4  poolt~on of v?~ ~haizz~aa of Rklr~ Kay ~.
,  a priva~oly hold , ~ - ~D~ mdss \]Figure 3: Tree Viewer3.5 GATE UsersGATE version 1 was released in Novem-ber 1996 and is in use for a number ofprojects around the World - see for ex-ample hztp: llw~a, sics.
se/humle/proj ects/svensk /svensk ,  html, who evaluatedthe system relative to ALEP, andht~p://wvv, des.
shef.
ac.
uk/research/gr0ups/nlp/gate/users.html.
Figure 4 lists the sites thathave licenced the system so far.4 Word  sense  tagg ingWe have recently implemented a sense tagger withinthe GATE framework.Sense tagging is the process of assigning the ap-propriate sense from some semantic lexicon to eachword 3 in a text.
This is similar to the more widelyknown technology of part-of-speech tagging, but thetags which are assigned in sense tagging are semantictags from a dictionary rather than the grammaticaltags assigned by a part-of-speech tagger.Our sense tagger uses the machine readable ver-sion of Longman Dictionary of Contemporary Eng-lish (LDOCE) (Procter, 1978) to provide the se-mantic tag set.
LDOCE is a learners' dictionary -one designed not for native speakers of English butfor those learning English as a second language andhas been used extensively in machine readable dic-tionary research ((Ide and Veronis, 1994), (Cowie,3This is often loosened to each content word.Guthrie, and Guthrie, 1992), (Bruce and Wiebe,1994)).The clearest way to understand what a sense tag-ger does is to look at an example of the output wewould like it to produce.
Consider the sentence "Theinterest on my bank account accrued over the years",our tagger should assign a single sense from LDOCEto each of the content words in the sentence.
Thechoice of senses in the assignment should be the sameas that a human would choose.
An example of a de-sired assignment is shown in figure 5.As can be seen from the senses assigned, eachLDOCE sense has a homograph and sense number,these are used to identify different levels of seman-tic distinction between senses and act as identifyingmarkers.
Homograph distinctions ignify broad se-mantic differences between senses (such as the 'edgeof river' and 'financial institution' senses of bank)while sense distinctions ignify differences betweensenses which are more related (such as the 'building'and 'company' senses of the word).
These numbersare followed by the textual definition of the senseand, possibly, by an example sentence which is a par-ticular use of the sense and is printed in this type 4.The information provided by these tags is poten-tially valuable for downstream tasks in a languageprocessing system.
For example, the system couldbenefit from knowing that "bank" in this case means4LDOCE senses have additional information such assubject categories, ubcategorisation information and se-lectional restrictions which we do not show here.Cunningham, Stevenson and Wilks 64 Implementing a Sense Taggermmmmmmmmmmmmmmmmmsenses in texts.
A natural extension to this obser-vation is to create a disambiguation system whichmakes use of several of these independent knowledgesources and combines their results in an intelligentway.Our system is based on a set of partial taggers,each of which uses a different knowledge source, withtheir results being combined.
Our system is in thetradition of McRoy (McRoy, 1992), who also madeuse of several knowledge sources for word sense dis-ambiguation, although the information sources sheused were not independent, making it difficult toevaluate the contribution of each component.
Oursystem makes use of strictly independent knowledgesources and is implemented within GATE whoseplug-and-play architecture makes the evaluation ofindividual components more straightforward.At the moment he sense tagger consists of sixstages (shown in figure 6), the first two preprocessthe text which is to be disambiguated while the re-maining four carry out the disambiguation.P reprocess ing?
Named-entity identification?
Dictionary look-upDisarnbiguation?
Part-of-speech filtering?
Dictionary definition overlap?
Domain code overlap* Scoring mechanismFigure 6: Stages in the Sense Tagging process1.
The text is first processed by a named-entity identifier, which we developed aspart of Sheffield's entry for MUC-6 (Wakao,Gaizauskas, and Humphries, 1996; Gaizauskaset al, 1996).
This identifies certain forms ofproper names in the text and classifies them aseither place, person, organization or location.For details of the classification scheme see (Def,1995).
We make no use of these classificationsat present, however, they are of potential useto a module carrying out disambignation usingselectional restrictions.The tagger does not attempt o disambignateany words which are identified as part of anamed-entity.2.
The remaining text is stemmed, leaving onlymorphological roots, and split into sentences.Then words belonging to a list of stop wordsare removed.
The words which have not beenidentified as part of a named entity or removedbecause it is a stop word are considered by thesystem to be ambiguous words and those are thewords which are disambignated.For each of the ambiguous words, its set ofpossible senses is extracted from LDOCE andstored.
Each sense in LDOCE contains a shorttextual definition (such as those shown in figure5) which, when extracted from the dictionary,is processed to remove stop words and stem theremaining words..
The text is tagged using the Brill tagger (Brill,1992) and a translation is carried out using amanually defined mapping from the syntactictags assigned by Brill (Penn Tree Bank tags(Marcus, Santorini, and Marcinkiewicz, 1993))onto the simpler part-of-speech ategories asso-ciated with LDOCE senses 6.
We then removefrom consideration any of the senses whose part-of-speech isnot consistent with the one assignedby the tagger, if none of the senses are consis-tent with the part-of-speech we assume the tag-ger has made an error and leave the set of sensesfor that word unaltered.4.
Our next module is based on a proposal by Lesk(Lesk, 1986) that words in a sentence could bedisambiguated by choosing the the sense whichproduced the maximum overlap of the contentwords in the textual definitions of the word'ssenses.
In practise this led to massive compu-tations with as many as 10 l?
possible combina-tions of senses for a single sentence.Cowie et.
al.
(Cowie, Guthrie, and Guthrie,1992) used simulated annealing (Kirkpatrick,Gelatt, and Vecci, 1983), a numerical optimisa-tion algorithm, to make this process tractable.5In our system a stop word is defined to be any wordwhich is not a noun, verb, adjective or adverb.
Preposi-tions are included in the list of stop words and are notdisambiguated.6The BriU tagger uses the tag set from the Penn TreeBank which contains 48 tags (Marcus, Santorini, andMarcinkiewicz, 1993), LDOCE uses a set of 17, moregeneral, tags.Cunningham, Stevenson and Wilks 66 Implementing a Sense TaggerIIIIIIIIIIIIIIIIIIIIIIIiIIilmm|ImmRImmmThe simulated annealing algorithm proceeds bydisambiguating a sentence at a time.
A ran-dom configuration of senses is chosen such thatone Sense is assigned to each ambiguous wordin the sentence.
A score is given to this config-uration based on the number of content wordswhich are shared between the textual definitionin the senses.
Other, random, configurationsare then generated and the simulated annealingalgorithm is used to optimise over them.
Whenthis process is complete the algorithm returns aconfiguration which assigns the optimal config-uration of senses based on the overlap of wordsin the definition text.This process identifies a single condidateLDOCE sense for each ambiguous word.5.
The text is then run through a module whichoptimises the overlap of domain codes for thesenses of nouns in each paragraph of the text.The optimisation algorithm used is similar tosimulated annealing (see section 4), althoughit has been modified in two ways.
Firstly, wemaximise the overlap of the pragmatic codes as-sociated with the word senses rather than thecontent words in their definitions.
Secondly, weoptimise over entire paragraphs at a time ratherthan just sentences, this is done because thereis good evidence (Gale, Church, and Yarowsky,1992) that a wide context, of around 100 words,is optimal when disambiguating using domaincodes.
This process, like the previous module,identifies a single candidate sense for each am-biguous word.6.
The final stage is to combine the results ofthe preceding processes.
This is done using avery simple mechanism which we plan to re-place with an optimisation algorithm.
We as-sign a score to each of the senses of the am-biguous words.
These scores are initialised to0 and +1 is added to a sense's core for eachof the simulated annealing or pragmatic odemodules which select hat sense.
The sense withthe highest score is chosen as the tag for eachambiguous word.
If there is a tie (two senseswith the same score, which will happen if thetwo partial taggers disagree) it is broken bychoosing the first sense, as listed in the dictio-nary.
This is a sensible tie-breaker since thesenses are roughly ordered by frequency of oc-currence in text 7.
After this process is com-7We are using the 1st Edition of LDOCE in which thepublishers make no claim that the senses are ordered bypleted every ambiguous word has exactly onesense from LDOCE associated with it, this senseis the tag which our system has assigned to thatword.We have conducted some preliminary testingof our tagger: our tests were run on 14 hand-disambiguated (by one of the authors) sentencesfrom the Wall Street Journal, amounting to a 250word corpus.
We found that, of the tokens withmore than 1 homograph in LDOCE, 92% were as-signed the correct homograph and 75% the correctsense using our tagger.
These figures hould be com-pared to the 72% correct homograph assignment and47% correct sense assignment reported by Cowie et.al.
(Cowie, Guthrie, and Guthrie, 1992) using sim-mulated annealing alone on the same test set.6 Deve lop ing  the  tagger  w i th  GATEThe sense tagger was implemented as a set of 11CREOLE modules, 6 of which had been imple-mented as part of VIE and the remaining 5 were de-veloped specifically for the sense tagger.
These wereimplemented in a variety of programming languages:C\[++\], Perl and Prolog.
These five modules are var-ied in their implementation methods.
Two are writ-ten entirely in C++ and are linked with the GATEexecutable at runtime using GATE's dynamic load-ing facility (see (Cunningham et al, 1996)).
Threeare made up of a variety of Perl scripts, Prolog savedstates or C executables, which are run as externalprocesses via GATE's Tcl (Ousterhout, 1994.)
API.This is typical of systems we have seen built usingGATE, and illustrates its flexibility with respect oimplementation ptions.The GATE graphical representation f the sensetagger is shown in figure 7.A special viewer was implemented within GATEto display the results of the sense tagging process.After the final module in the tagger has been run itis possible to call a viewer which displays the textwhich has been processed with the ambiguous wordshighlighted (see figure 8).
Clicking on one of thesehighlighted words causes another window to appearwhich contains the sense which has been assigned tothat word by the tagger (see figure 9).
Using thisviewer we can quickly see that the tagger has as-signed the 'chosen for job' sense of "appointment"in "Kando, whose appointment takes effect from to-day ..." which is the correct sense in this context.frequency of occurrence intext (although they do in latereditions).
However, (Guo, 1989) has found evidence thatthere is a correspondence b tween the order in whichsense are listed and the frequency of occurrence.Cunningham, Stevenson and Wilks 67 Implementing a Sense TaggerilIIIIIIIIIIII!iI!IIIIIIthe rapid re-use of existing modules and reduced theneed to provide data-transfer routes between mod-ules.
Almost the entire preprocessing of the text wascarried out using modules which had already beenimplemented within GATE: the tokeniser, sentencesplitter, Brill part-of-speech tagger and the moduleswhich made up the Named Entity identifier.
Thismeant hat we could quickly implementat the mod-ules which carried out the disambiguation, and thosewere the modules in which we were most interested.The implementation was further speeded up by theuse of results viewers which allowed us to examinethe annotations in the TIPSTER DataBase after amodule had been run, allowing us to discover bugsfar more quickly than would have been possible in asystem which is not as explicitly modular as GATE.One aspect of sense tagging in which we are inter-ested is the effect of including and excluding differentmodules, and this could be easily carried out usingGGI.One particular limitation of the current GATE im-plementation became apparent during this work, viz.the necessity of cascading module reset in the pres-ence of non-monotonic database updates.
For exam-ple, the POS filter modules remove some of the sensedefinitions associated with words by the lexical pre-processing stages.
When reseting these modules it istherefore necessary to reset he preprocessor stage inorder that the database is returned to a consistentstate (this is done automatically by GATE, whichidentifies cases where modules alter previously ex-isting annotations by examination of the pre-/post-conditions of the module supplied by the developeras configuration i formation prior to loading).
Thisleads to redundant processing, and in the case ofslow modules (like our LDOCE lookup module) thiscan be an appreciable brake on the development cy-cle.
The planned solution is to change the impl-mentation of the reset function.
Currently this sim-ply deletes the database objects created by a mod-ule.
Given a database implementation that supportstransactions we can use timestamp and rollback fora more intelligent reset, and avoid the redundantprocessing caused by reset cascading.An additional, esser problem, is the complexity ofthe generation algorithms for the task graphs, andthe diffculty of managing these graphs as the num-ber of modules in the system grows.
The graphs cur-rently make two main contributions to the system:they give a graphical representation f control flow,and allow the user to manipulate xecution of mod-ules; they give a graphical entry point to results vi-sualisation.
These benefits will have to be balancedagainst heir disadvantages in future versions of thesystem.
Another problem may arise when the archi-tecture includes facilities for distributed processing(Zajac et al, 1997; Zajac, 1997), as it is not ob-vious how the linear model currently embodied inthe graphs could be extended to support non-linearcontrol strucures.8 Conc lus ionThe previous ection indicates that GATE version 1goes a long way to meeting it's design goals (noted insection 2).
The reuse of components we have experi-enced in the sense tagging project and a number ofother local and collaborative projects is in itself jus-tification of the development effort spent on the sys-tem, and, hopefully, these savings will be multipliedaccross other users of the system.
Future versionswill address the problems we uncovered above.D is t r ibut ionGATE and a MUC-6 (Grishman and Sundheim,1996) style Information Extraction (Gaizauskas etal., 1996; Humphreys et al, 1996) system thatcomes with it is free for academic research - seehttp://~v, des.
shef.
ac.
uk/research/groups/nip~gate~ for details.AcknowledgementsThis work has been supported by the UK's EPSRCand the European Commission Language Engineer.ing programme under grants GR/K25267 (GATE),LE1-2238 (AVENTINUS) and LE1-2110 (ECRAN).Re ferencesAmtrup, J.W.
1995.
ICE - INTARC Communica-tion Environment User Guide and Reference Man-ual Version 1.4.
Technical report, University ofHamburg.Basili, R., M. Della Rocca, and M.T.
Pazienza.1997.
Towards a bootstrapping framework forcorpus semantic tagging.
In Proceedings of theSIGLEX Workshop "Tagging Text with LexiealSemantics: What, why and how?
", Washington,D.C., April.
ANLP.Boguraev, B., R. Garigliano, and J. Tait.
1995.
Ed-itorial.
Natural Language Engineering., 1 Part 1.Brill, E. 1992.
A simple rule-based part of speechtagger.
In Proceedings ofthe DARPA Speech andNatural Language Workshop.
Harriman, NY.Bruce, R. and L. Guthrie.
1992.
Genus dis-ambiguation: A study in weighted preference.Cunningham, Stevenson a d Wilks 69 Implementing a Sense TaggerIIIIII!1IIilIIIIIIIIIImJonassen, D.H., editor.
1982.
The Technology ofText.
Educational Technology Publications.Kirkpatrick, S., C. Gelatt, and M. Vecci.
1983.Optimisation by simulated annealing.
Science,220(4598):671-680.Lesk, M. 1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceed-ings of ACM SIGDOC Conference, pages 24-26,Toronto, Ontario.Mahesh, K., S. Nirenburg, S. Beale, E. Viegas,V.
Raskin, and B. Onyshkeyvych.
1997.
Wordsense disambiguation: Why have statistics whenwe have these numbers.
In Proceedings of the7th International Conference on Theoretical andMethodological Issues in Machine Translation,pages 151-159, July.Marcus, M., B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of Eng-lish: The Penn Tree Bank.
Computational Lin-guistics, 19(2):313-330.McRoy, S. 1992.
Using multiple knowledge sourcesfor word sense disambiguation.
ComputationalLinguistics, 18(1):1-30.Mitkov, R. 1996.
Language Engineering: towards aclearer picture.
In Proceedings of ICML.Ng, H. T. and H. B. Lee.
1996.
Integrating multi-ple knowldge sources to disambiguate word sense:An exemplar-based approach.
In Proceedings ofACL96.Ousterhout, J.K. 1994.
Tcl and the Tk Toolkit.Addison-Wesley.Pedersen, T. and R. Bruce.
1997.
Distinguishingword senses in untagged text.
In Proceedings ofthe Second Conference on Empirical Methods inNatural Language Processing, Providence, RI, Au-gust.Procter, P. 1978.
Longman Dictionary of Contem-porary English.
Longraan Group, Essex, England.Schiitze, H. 1992.
Dimensions of meaning.
In Pro-ceedings of Supercomputing '92, pages 787-796,Minneapolis, MN.Shaw, M. and D. Garlan.
1996.
Software Architec-ture.
Prentice Hall.Simkins, N. K. 1994.
An Open Architecture forLanguage Engineering.
In First Language Engi-neering Convention, Paris.Thompson, H. 1985.
Natural anguage processing:a critical analysis of the structure of the field, withsome implications for parsing.
In K. Sparck Jonesand Y. Wilks, editors, Automatic Natural Lan-guage Parsing.
Ellis Horwood.Thompson, H.S.
and D. McKelvie.
1996.
A SoftwareArchitecture for Simple, Efficient SGML Applica-tions.
In Proceedings of SGML Europe '96, Mu-nich.Wakao, T., R. Gaizanskas, and K. Humphries.
1996.Evaluation of an algorithm for the recognition andcalssification of proper names.
In Proceedings ofthe 16th International Conference on Computa-tional Linguistics (COLINGg6), pages 418-423,Copenhagen, Denmark.Wilks, Y. and M. Stevenson.
1997.
Sensetagging: Semantic tagging with a lexi-con.
In Proceedings of the SIGLEX Work-shop "Tagging Text with Lexieal Semantics:What, why and how?".
ANLP.
Available ashttp: / /xxx,  lanl .
gov/ps/c~p-lg/9705016.Woodruff, A. and M. Stonebreaker.
1995.
Bufferingof Intermediate R sults in Dataflow Diagrams.
InProceedings VL'95 11th International IEEE Sym-posium on Visual Languages, Darmstadt.
IEEEComputer Society Press.Yarowsky, D. 1993.
One sense per collocation.
InProceedings ARPA Human Language TechnologyWorkshop, pages 266-271.Yarowsky, D. 1995.
Unsupervised word-sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of ACL95.Zajac, R. 1997.
An Open Distributed Architecturefor Reuse and Integration of Heterogenous NLPComponents.
In Proceedings of the 5th conferenceon Applied Natural Language Processing (ANLP-97).Zajac, R., V. Malaesh, H. Pfeiffer, and M. Casper.1997.
The CoreUi Document Processing architec-ture.
Technical report, Computing Research Lab,New Mexico State University.Cunningham, Stevenson a d Wilks 71 Implementing a Sense Taggermmmmmmmmmmmmmmmmm
