Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 17?24,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Parsing Word-Aligned Parallel Corpora in a Grammar Induction ContextJonas KuhnThe University of Texas at Austin, Department of Linguisticsjonask@mail.utexas.eduAbstractWe present an Earley-style dynamic pro-gramming algorithm for parsing sentencepairs from a parallel corpus simultane-ously, building up two phrase structuretrees and a correspondence mapping be-tween the nodes.
The intended use ofthe algorithm is in bootstrapping gram-mars for less studied languages by usingimplicit grammatical information in par-allel corpora.
Therefore, we presuppose agiven (statistical) word alignment under-lying in the synchronous parsing task; thisleads to a significant reduction of the pars-ing complexity.
The theoretical complex-ity results are corroborated by a quantita-tive evaluation in which we ran an imple-mentation of the algorithm on a suite oftest sentences from the Europarl parallelcorpus.1 IntroductionThe technical results presented in this paper1 aremotivated by the following considerations: It is con-ceivable to use sentence pairs from a parallel corpus(along with the tentative word correspondences froma statistical word alignment) as training data for agrammar induction approach.
The goal is to inducemonolingual grammars for the languages under con-sideration; but the implicit information about syn-tactic structure gathered from typical patterns in thealignment goes beyond what can be obtained fromunlabeled monolingual data.
Consider for instancethe sentence pair from the Europarl corpus (Koehn,2002) in fig.
1 (shown with a hand-labeled wordalignment): distributional patterns over this and sim-ilar sentences may show that in English, the subject1This work was in part supported by the German ResearchFoundation DFG in the context of the author?s Emmy Noetherresearch group at Saarland University.
(the word block ?the situation?)
is in a fixed struc-tural position, whereas in German, it can appear invarious positions; similarly, the finite verb in Ger-man (here: stellt) systematically appears in secondposition in main clauses.
In a way, the translationof sentences into other natural languages serves asan approximation of a (much more costly) manualstructural or semantic annotation ?
one might speakof automatic indirect supervision in learning.
Thetechnique will be most useful for low-resource lan-guages and languages for which there is no fundingfor treebanking activities.
The only requirement willbe that a parallel corpus exist for the language underconsideration and one or more other languages.2Induction of grammars from parallel corpora israrely viewed as a promising task in its own right;in work that has addressed the issue directly (Wu,1997; Melamed, 2003; Melamed, 2004), the syn-chronous grammar is mainly viewed as instrumentalin the process of improving the translation model ina noisy channel approach to statistical MT.3 In thepresent paper, we provide an important prerequisitefor parallel corpus-based grammar induction work:an efficient algorithm for synchronous parsing ofsentence pairs, given a word alignment.
This workrepresents a second pilot study (after (Kuhn, 2004))for the longer-term PTOLEMAIOS project at Saar-land University4 with the goal of learning linguis-tic grammars from parallel corpora (compare (Kuhn,2005)).
The grammars should be robust and assign a2In the present paper we use examples from English/Germanfor illustration, but the approach is of course independent of thelanguage pair under consideration.3Of course, there is related work (e.g., (Hwa et al, 2002; Lu?et al, 2002)) using aligned parallel corpora in order to ?project?bracketings or dependency structures from English to anotherlanguage and exploit them for training a parser for the otherlanguage.
But note the conceptual difference: the ?parse projec-tion?
approach departs from a given monolingual parser, with aparticular style of analysis, whereas our project will explore towhat extent it may help to design the grammar topology specifi-cally for the parallel corpus case.
This means that the emergingEnglish parser may be different from all existing ones.4http://www.coli.uni-saarland.de/?jonask/PTOLEMAIOS/17Heute stellt sich die Lage jedoch vo?llig anders darThe situation now however is radically differentFigure 1: Word-aligned German/English sentence pair from the Europarl corpuspredicate-argument-modifier (or dependency) struc-ture to sentences, such that they can be applied inthe context of multilingual information extraction orquestion answering.2 Synchronous grammarsFor the purpose of grammar induction from parallelcorpora, we assume a fairly straightforward exten-sion of context-free grammars to the synchronousgrammar case (compare the transduction grammarsof (Lewis II and Stearns, 1968)): Firstly, the termi-nal and non-terminal categories are pairs of sym-bols, one for each language; as a special case, oneof the two symbols can be NIL for material realizedin only one of the languages.
Secondly, the linearsequence of daughter categories that is specified inthe rules can differ for the two languages; therefore,an explicit numerical ranking is used for the linearprecedence in each language.
We use a compactrule notation with a numerical ranking for the lin-ear precedence in each language.
The general formof a grammar rule for the case of two parallel lan-guages is N0/M0 ?
N1:i1/M1:j1 .
.
.
Nk:ik/Mk:jk,where Nl,Ml are NIL or a terminal or nonterminalsymbol for language L1 and L2, respectively, andil, jl are natural numbers for the rank of the phrasein the sequence for L1 and L2 respectively (for NILcategories a special rank 0 is assumed).5 Since linearordering of daughters in both languages is explic-itly encoded by the rank indices, the specificationsequence in the rule is irrelevant from a declarativepoint of view.
To facilitate parsing we assume a nor-mal form in which the right-hand side is ordered bythe rank in L1, with the exception that the categoriesthat are NIL in L1 come last.
If there are several such5Note that in the probabilistic variants of these grammars,we will typically expect that any ordering of the right-hand sidesymbols is possible (but that the probability will of course vary?
in a maximum entropy or log-linear model, the probabilitywill be estimated based on a variety of learning features).
Thismeans that in parsing, the right-hand side categories will be ac-cepted as they come in, and the relevant probability parametersare looked up accordingly.NIL categories in the same rule, they are viewed asunordered with respect to each other.6Fig.
2 illustrates our simple synchronous gram-mar formalism with some rules of a sample grammarand their application on a German/English sentencepair.
Derivation with a synchronous grammar givesrise to a multitree, which combines classical phrasestructure trees for the languages involved and alsoencodes the phrase level correspondence across thelanguages.
Note that the two monolingual trees infig.
2 for German and English are just two ways ofunfolding the common underlying multitree.Note that the simple formalism goes along withthe continuity assumption that every complete con-stituent is continuous in both languages.
Various re-cent studies in the field of syntax-based StatisticalMT have shown that such an assumption is problem-atic when based on typical treebank-style analyses.As (Melamed, 2003) discusses for instance, in thecontext of binary branching structures even simpleexamples like the English/French pair a gift for youfrom France ?
un cadeau de France pour vouz [agift from France for you] lead to discontinuity of a?synchronous phrase?
in one of the two languages.
(Gildea, 2003) and (Galley et al, 2004) discuss dif-ferent ways of generalizing the tree-level crosslin-guistic correspondence relation, so it is not confinedto single tree nodes, thereby avoiding a continuityassumption.
We believe that in order to obtain fullcoverage on real parallel corpora, some mechanismalong these lines will be required.However, if the typical rich phrase structure anal-yses (with fairly detailed fine structure) are replacedby flat, multiply branching analyses, most of thehighly frequent problematic cases are resolved.7 In6This detail will be relevant for the parsing inference rule(5) below.7Compare the systematic study for English-French align-ments by (Fox, 2002), who compared (i) treebank-parser styleanalyses, (ii) a variant with flattened VPs, and (iii) dependencystructures.
The degree of cross-linguistic phrasal cohesion in-creases from (i) to (iii).
With flat clausal trees, we will comeclose to dependency structures with respect to cohesion.18Synchronous grammar rules:S/S ?
NP:1/NP:2 Vfin:2/Vfin:3 Adv:3/Adv:1NP:4/PP:5 Vinf:5/Vinf:4NP/NP ?
Pron:1/Pron:1NP/PP ?
Det:1/Det:2 N:2/N:4 NIL:0/P:1 NIL:0/Adj:3Pron/Pron ?
wir:1/we:1Vfin/Vfin ?
mu?ssen:1/must:1Adv/Adv ?
deshalb:1/so:1NIL/P ?
NIL:0/at:1Det/Det ?
die:1/the:1NIL/Adj ?
NIL:0/agricultural:1N/N ?
Agrarpolitik:1/policy:1Vinf/Vinf ?
pru?fen:1/look:1German tree:SNP Vfin Adv NP VinfPron Det NWir mu?ssen deshalb die Agrarpolitik pru?fenwe must therefore the agr.
policy examineEnglish tree:SAdv NP Vfin Vinf PPPron P Det Adj NSo we must look at the agricultural policyMultitree:S/SNP:1/NP:2 Vfin:2/Vfin:3 Adv:3/Adv:1 NP:4/PP:5 Vinf:5/Vinf:4Pron:1/Pron:1 NIL:0/P:1 Det:1/Det:2 NIL:0/Adj:3 N:2/N:4Wir/we mu?ssen/must deshalb/so NIL/at die/the NIL/agricultural Agrarpolitik/policy pru?fen/lookFigure 2: Sample rules and analysis for a synchronous grammarthe flat representation that we assume, a clause isrepresented in a single subtree of depth 1, with allverbal elements and the argument/adjunct phrases(NPs or PPs) as immediate daughters of the clausenode.
Similarly, argument/adjunct phrases are flatinternally.
Such a flat representation is justifiedboth from the point of view of linguistic learningand from the point of view of grammar application:(i) Language-specific principles of syntactic struc-ture (e.g., the strong configurationality of English),which are normally captured linguistically by thericher phrase structure, are available to be inducedin learning as systematic patterns in the relative or-dering of the elements of a clause.
(ii) The predicate-argument-modifier structure relevant for applicationof the grammars, e.g., in information extraction canbe directly read off the flat clausal representation.It is a hypothesis of our longer-term project thata word alignment-based consensus structure whichworks with flat representations and under the con-tinuity assumption is a very effective starting pointfor learning the basic language-specific constraintsrequired for a syntactic grammar.
Linguistic phe-nomena that fall outside what can be captured in thisconfined framework (in particular unbounded de-pendencies spanning more than one clause and dis-continuous argument phrases) will then be learnedin a later bootstrapping step that provides a richerset of operations.
We are aware of a number of openpractical questions, e.g.
: Will the fact that real paral-lel corpora often contain rather free translations un-dermine our idea of using the consensus structurefor learning basic syntactic constraints?
Statisticalalignments are imperfect ?
can the constraints im-posed by the word alignment be relaxed accordinglywithout sacrificing tractability and the effect of indi-rect supervision?83 Alignment-guided synchronous parsingOur dynamic programming algorithm can be de-scribed as a variant of standard Earley-style chartparsing (Earley, 1970) and generation (Shieber,1988; Kay, 1996).
The chart is a data structurewhich stores all sub-analyses that cover part of theinput string (in parsing) or meaning representation(in generation).
Memoizing such partial results hasthe standard advantage of dynamic programmingtechniques ?
it helps one to avoid unnecessary re-computation of partial results.
The chart structurefor context-free parsing is also exploited directly indynamic programming algorithms for probabilisticcontext-free grammars (PCFGs): (i) the inside (oroutside) algorithm for summing over the probabil-ities for every possible analysis of a given string,(ii) the Viterbi algorithm for determining the mostlikely analysis of a given string, and (iii) the in-8Ultimately, bootstrapping of not only the grammars, butalso of the word alignment should be applied.19side/outside algorithm for re-estimating the param-eters of the PCFG in an Expectation-Maximizationapproach (i.e., for iterative training of a PCFG onunlabeled data).
This aspect is important for the in-tended later application of our parsing algorithm ina grammar induction context.A convenient way of describing Earley-style pars-ing is by inference rules.
For instance, the centralcompletion step in Earley parsing can be describedby the rule9(1) ?X ?
?
?
Y ?, [i, j]?, ?Y ?
?
?, [j, k]?
?X ?
?
Y ?
?, [i, k]?Synchronous parsing.
The input in synchronousparsing is not a one-dimensional string, but a pair ofsentences, i.e., a two-dimensional array of possibleword pairs (or a multidimensional array if we arelooking at a multilingual corpus), as illustrated infig.
3.policy ?agriculturalthe ?atlook ?must ?we ?So ?0 1 2 3 4 5 6L 2:L1: Wir mu?ssen deshalb die Agrar- pru?fenpolitikFigure 3: Synchronous parsing: two-dimensional in-put (with word alignment marked)The natural way of generalizing context-free pars-ing to synchronous grammars is thus to control theinference rules by string indices in both dimensions.Graphically speaking, parsing amounts to identify-ing rectangular crosslinguistic constituents ?
by as-sembling smaller rectangles that will together coverthe full string spans in both dimensions (compare(Wu, 1997; Melamed, 2003)).
For instance in fig.
4,the NP/NP rectangle [i1, j1, j2, k2] can be combinedwith the Vinf/Vinf rectangle [j1, k1, i2, j2] (assum-ing there is an appropriate rule in the grammar).9A chart item is specified through a position (?)
in a pro-duction and a string span ([l1, l2]).
?X ?
?
?
Y ?, [i, j]?means that between string position i and j, the beginning ofan X phrase has been found, covering ?, but still missing Y ?.Chart items for which the dot is at the end of a production (like?Y ?
?
?, [j, k]?)
are called passive items, the others active.Vinf/VinfNP/NPi1 j1 k1k2j2i2herinterviewsie interviewenFigure 4: Completion in two-dimensional chart:parsing part of Can I interview her?/Kann ich sieinterviewen?More generally, we get the inference rules (2) and(3) (one for the case of parallel sequencing, one forcrossed order across languages).
(2) ?X1/X2 ?
?
?
Y1:r1/Y2:r2 ?, [i1, j1, i2, j2]?,?Y1/Y2 ?
?
?, [j1, k1, j2, k2]?
?X1/X2 ?
?
Y1:r1/Y2:r2 ?
?, [i1, k1, i2, k2]?
(3) ?X1/X2 ?
?
?
Y1:r1/Y2:r2 ?, [i1, j1, j2, k2]?,?Y1/Y2 ?
?
?, [j1, k1, i2, j2]?
?X1/X2 ?
?
Y1:r1/Y2:r2 ?
?, [i1, k1, i2, k2]?Since each inference rule contains six free vari-ables over string positions (i1, j1, k1, i2, j2, k2), weget a parsing complexity of order O(n6) for unlexi-calized grammars (where n is the number of wordsin the longer of the two strings from language L1 andL2) (Wu, 1997; Melamed, 2003).
For large-scalelearning experiments this may be problematic, es-pecially when one moves to lexicalized grammars,which involve an additional factor of n4.10As a further issue, we observe that the inferencerules are insufficient for multiply branching rules,in which partial constituents may be discontinuousin one dimension (only complete constituents needto be continuous in both dimensions).
For instance,by parsing the first two words of the German stringin fig.
1 (Heute stellt), we should get a partial chartitem for a sentence, but the English correspondentsfor the two words (now and is) are discontinuous, sowe couldn?t apply rule (2) or (3).Correspondence-guided parsing.
As an alterna-tive to the standard ?rectangular indexing?
approach10The assumption here (following (Melamed, 2003)) is thatlexicalization is not considered as just affecting the grammarconstant, but that in parsing, every terminal symbol has to beconsidered as the potential head of every phrase of which it isa part.
Melamed demonstrates: If the number of different cat-egory symbols is taken into consideration as l, we get O(l2n6)for unlexicalized grammars, and O(l6n10) for lexicalized gram-mars; however there are some possible optimizations.20to synchronous parsing we propose a conceptuallyvery simple asymmetric approach.
As we will showin sec.
4 and 5, this algorithm is both theoreticallyand practically efficient when applied to sentencepairs for which a word alignment has previouslybeen determined.
The approach is asymmetric inthat one of the languages is viewed as the ?masterlanguage?, i.e., indexing in parsing is mainly basedon this language (the ?primary index?
is the stringspan in L1 as in monolingual parsing).
The otherlanguage contributes a secondary index, which ismainly used to guide parsing in the master language?
i.e., certain options are eliminated.
The choice ofthe master language is in principle arbitrary, but forefficiency considerations it is better to pick the onethat has more words without a correspondent.A way of visualizing correspondence-guidedparsing is that standard Earley parsing is applied toL1, with primary indexing by string position; as thechart items are assembled, the synchronous gram-mar and the information from the word alignmentis used to check whether the string in L2 could begenerated (essentially using chart-based generationtechniques; cf.
(Shieber, 1988; Neumann, 1998)).The index for chart items consists of two compo-nents: the string span in L1 and a bit vector for thewords in L2 which are covered.
For instance, basedon fig.
3, the noun compound Agrarpolitik corre-sponding to agricultural policy in English will havethe index ?
[4, 5], [0, 0, 0, 0, 0, 0, 1, 1]?
(assuming forillustrative purposes that German is the master lan-guage in this case).The completion step in correspondence-guidedparsing can be formulated as the following single in-ference rule:11(4) ?X1/X2 ?
?
?
Y1:r1/Y2:r2 ?, ?
[i, j], v?
?,?Y1/Y2 ?
?
?, ?
[j, k],w??
?X1/X2 ?
?
Y1:r1/Y2:r2 ?
?, ?
[i, k], u?
?where(i) j 6= k;(ii) OR(v,w) = u;(iii) w is continuous (i.e., it contains maximallyone subsequence of 1?s).Condition (iii) excludes discontinuity in passivechart items, i.e., complete constituents; active items11We use the bold-faced variables v,w,u for bit vectors; thefunction OR performs bitwise disjunction on the vectors (e.g.,OR([0, 1, 1, 0, 0], [0, 0, 1, 0, 1]) = [0, 1, 1, 0, 1]).
(i.e., partial constituents) may well contain discon-tinuities.
The success condition for parsing a stringwith N words in L1 is that a chart item with index?
[0, N ],1?
has been found for the start category pairof the grammar.Words in L2 with no correspondent in L1 (let?scall them ?L1-NIL?s for short), for example thewords at and agricultural in fig.
3,12 can in princi-ple appear between any two words of L1.
Thereforethey are represented with a ?variable?
empty L1-string span like for instance in ?
[i, i], [0, 0, 1, 0, 0]?.At first blush, such L1-NILs seem to introduce anextreme amount of non-determinism into the algo-rithm.
Note however that due to the continuity as-sumption for complete constituents, the distributionof the L1-NILs is constrained by the other words inL2.
This is exploited by the following inference rule,which is the only way of integrating L1-NILs into thechart:(5) ?X1/X2 ?
?
?
NIL:0/Y2:r2 ?, ?
[i, j], v?
?,?NIL/Y2 ?
?
?, ?
[j, j], w??
?X1/X2 ?
?
NIL:0/Y2:r2 ?
?, ?
[i, j], u?
?where(i) w is adjacent to v (i.e., unioning vectors wand v does not lead to more 0-separated 1-sequences than v contains already);(ii) OR(v,w) = u.The rule has the effect of finalizing a cross-linguistic constituent (i.e., rectangle in the two-dimensional array) after all the parts that have corre-spondents in both languages have been found.
134 ComplexityWe assume that the two-dimensional chart is ini-tialized with the correspondences following from aword alignment.
Hence, for each terminal that isnon-empty in L1, both components of the index areknown.
When two items with known secondary in-dices are combined with rule (4), the new secondary12It is conceivable that a word alignment would list agricul-tural as an additional correspondent for Agrarpolitik; but weuse the given alignment for illustrative purposes.13For instance, the L1-NILs in fig.
3 ?
NIL/at andNIL/agricultural ?
have to be added to incomplete NP/PPconstituent in the L1-string span from 3 to 5, consist-ing of the Det/Det die/the and the N/N Agrarpolitik/policy.With two applications of rule (5), the two L1-NILs can beadded.
Note that the conditions are met, and that as a re-sult, we will have a continuous NP/PP constituent with index?
[3, 5], [0, 0, 0, 0, 1, 1, 1, 1]?, which can be used as a passiveitem Y1/Y2 in rule (4).21index can be determined by bitwise disjunction ofthe bit vectors.
This operation is linear in the lengthof the L2-string (which is of the same order as thelength of the L1-string) and has a very small con-stant factor.14 Since parsing with a simple, non-lexicalized context-free grammar has a time com-plexity of O(n3) (due to the three free variablesfor string positions in the completion rule), we getO(n4) for synchronous parsing of sentence pairswithout any L1-NILs.
Note that words from L1 with-out a correspondent in L2 (which we would have tocall L2-NILs) do not add to the complexity, so thelanguage with more correspondent-less words canbe selected as L1.For the average complexity of correspondence-guided parsing of sentence pairs without L1-NILs wenote an advantage over monolingual parsing: cer-tain hypotheses for complete constituents that wouldhave to be considered when parsing only L1, are ex-cluded because the secondary index reveals a dis-continuity.
An example from fig.
3 would be the se-quence mu?ssen deshalb, which is adjacent in L1, butdoesn?t go through as a continuous rectangle whenL2 is taken into consideration (hence it cannot beused as a passive item in rule (4)).The complexity of correspondence-guided pars-ing is certainly increased by the presence of L1-NILs, since with them the secondary index can nolonger be uniquely determined.
However, with theadjacency condition ((i) in rule (5)), the number ofpossible variants in the secondary index is a func-tion of the number of L1-NILs.
Let us say there arem L1-NILs, i.e., the bit vectors contain m elementsthat we have to flip from 0 to 1 to obtain the final bitvector.
In each application of rule (5) we pick a vec-tor v, with a variable for the leftmost and rightmostL1-NIL element (since this is not fully determinedby the primary index).
By the adjacency condition,14Note that the operation does not have to be repeated whenthe completion rule is applied on additional pairs of items withidentical indices.
This means that the extra time complexity fac-tor of n doesn?t go along with an additional factor of the gram-mar constant (which we are otherwise ignoring in the presentconsiderations).
In practical terms this means that changes inthe size of the grammar are much more noticable than movingfrom monolingual parsing to alignment-guided parsing.An additional advantage is that in an Expectation Maximiza-tion approach to grammar induction (with a fixed word align-ment), the bit vectors have to be computed only in the first iter-ation of parsing the training corpus, later iterations are cubic.either the leftmost or rightmost marks the boundaryfor adding the additional L1-NIL element NIL/Y2 ?hence we need only one new variable for the newlyshifted boundary among the L1-NILs.
So, in additionto the n4 expense of parsing non-nil words, we getan expense of m3 for parsing the L1-NILs, and weconclude that for unlexicalized synchronous pars-ing, guided by an initial word alignment the com-plexity class is O(n4m3) (where n is the total num-ber of words appearing in L1, and m is the numberof words appearing in L2, without a correspondentin L1).
Recall that the complexity for standard syn-chronous parsing is O(n6).Since typically the number of correspondent-lesswords is significantly lower than the total number ofwords (at least for one of the two languages), theseresults are encouraging for medium-to-large-scalegrammar learning experiments using a synchronousparsing algorithm.5 Empirical EvaluationIn order to validate the theoretical complexity resultsempirically, we implemented the algorithm and ranit on sentence pairs from the Europarl parallel cor-pus.
At the present stage, we are interested in quan-titative results on parsing time, rather than qualita-tive results of parsing accuracy (for which a moreextensive training of the rule parameters would berequired).Implementation.
We did a prototype implementa-tion of the correspondence-guided parsing algorithmin SWI Prolog.15 Chart items are asserted to theknowledge base and efficiently retrieved using in-dexing by a hash function.
Besides chart construc-tion, the Viterbi algorithm for selecting the mostprobable analysis has been implemented, but for thecurrent quantitative results only chart constructionwas relevant.Sample grammar extraction.
The initial prob-ablistic grammar for our experiments was ex-tracted from a small ?multitree bank?
of 140 Ger-man/English sentence pairs (short examples fromthe Europarl corpus).
The multitree bank was an-notated using the MMAX2 tool16 and a specially15http://www.swi-prolog.org ?
The advantage of using Pro-log is that it is very easy to experiment with various conditionson the inference rules in parsing.16http://mmax.eml-research.de22tailored annotation scheme for flat correspondencestructures as described in sec.
2.
A German and En-glish part-of-speech tagger was used to determineword categories; they were mapped to a reduced cat-egory set and projected to the syntactic constituents.To obtain parameters for a probabilistic grammar,we used maximum likelihood estimation from thesmall corpus, based on a rather simplistic genera-tive model,17 which for each local subtree decides(i) what categories will be the two heads, (ii) howmany daughters there will be, and for each non-head sister (iii) whether it will be a nonterminal ora terminal (and in that case, what category pair),and (iv) in which position relative to the head toplace it in both languages.
In order to obtain arealistically-sized grammar, we applied smoothingto all parameters; so effectively, every sequence ofterminals/nonterminals of arbitrary length was pos-sible in parsing.Parsing sentences without NIL words00.10.20.30.40.50.60.70.80.94 5 6 7 8 9 10number of words (in L1)parsingtime[sec] Monolingual parsing L1CGSPFigure 5: Comparison of synchronous parsing withand without exploiting constraints from L2Results.
To validate empirically that the pro-posed correspondence-guided synchronous parsingapproach (CGSP) can effectively exploit L2 as aguide, thereby reducing the search space of L1parses that have to be considered, we first ran acomparison on sentences without L1-NILs.
The re-sults (average parsing time for Viterbi parsing withthe sample grammar) are shown in fig.
5.18 Theparser we call ?monolingual?
cannot exploit any17For our learning experiments we intend to use a MaximumEntropy/log-linear model with more features.18The experiments were run on a 1.4GHz Pentium M proces-sor.alignment-induced restrictions from L2.19 Note thatCGSP takes clearly less time.Comparison wrt.
# NIL words00.20.40.60.811.21.45 6 7 8 9 10number of words (in L1)parsingtime[sec]3 L1-NILs,CGSP2 L1-NILs,CGSP1 L1-NIL,CGSPno L1-NILs,CGSPmonolingualparsing (L1)Figure 6: Synchronous parsing with a growing num-ber of L1-NILsFig.
6 shows our comparative results for parsingperformance on sentences that do contain L1-NILs.Here too, the theoretical results are corroborated thatwith a limited number of L1-NILs, the CGSP is stillefficient.The average chart size (in terms of the number ofentries) for sentences of length 8 (in L1) was 212for CGSP (and 80 for ?monolingual?
parsing).
Thefollowing comparison shows the effect of L1-NILs(note that the values for 4 and more L1-NILs arebased on only one or two cases):(6) Chart size for sentences of length 8 (in L1)Number ofL1-NILs0 1 2 3 4 5 6Avg.
num-ber of chartitems77 121 175 256 (330) (435) (849)We also simulated a synchronous parser whichdoes not take advantage of a given word alignment(by providing an alignment link between any pairof words, plus the option that any word could be aNULL word).
For sentences of length 5, this parsertook an average time of 22.3 seconds (largely inde-pendent of the presence/absence of L1-NILs).2019The ?monolingual?
parser used in this comparison parsestwo identical copies of the same string synchronously, with astrictly linear alignment.20While our simulation may be significantly slower than a di-rect implementation of the algorithm (especially when some ofthe optimizations discussed in (Melamed, 2003) are taken intoaccount), the fact that it is orders of magnitude slower does in-23Finally, we also ran an experiment in which thecontinuity condition (condition (iii) in rule (4)) wasdeactivated, i.e., complete constituents were allowedto be discontinuous in one of the languages.
The re-sults in (7) underscore the importance of this condi-tion ?
leaving it out leads to a tremendous increasein parsing time.
(7) Average parsing time in seconds with and with-out continuity conditionSentence length (with no L1-NILs)4 5 6Avg.
parsing time with CGSP(incl.
continuity condition)0.005 0.012 0.026Avg.
parsing time without thecontinuity condition0.035 0.178 1.0256 ConclusionWe proposed a conceptually simple, yet efficient al-gorithm for synchronous parsing in a context wherea word alignment can be assumed as given ?
for in-stance in a bootstrapping learning scenario.
One ofthe two languages in synchronous parsing acts as themaster language, providing the primary string spanindex, which is used as in classical Earley parsing.The second language contributes a bit vector as asecondary index, inspired by work on chart gener-ation.
Continuity assumptions make it possible toconstrain the search space significantly, to the pointthat synchronous parsing for sentence pairs with few?NULL words?
(which lack correspondents) may befaster than standard monolingual parsing.
We dis-cussed the complexity both theoretically and pro-vided a quantitative evaluation based on a prototypeimplementation.The study we presented is part of the longer-termPTOLEMAIOS project.
The next step is to applythe synchronous parsing algorithm with probabilis-tic synchronous grammars in grammar induction ex-periments on parallel corpora.ReferencesJay C. Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.dicate that our correspondence-guided approach is a promisingalternative for an application context in which a word alignmentis available.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP 2002),pages 304?311.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proceed-ings of the Human Language Technology Conference of theNorth American Chapter of the Association for Computa-tional Linguistics: HLT-NAACL 2004, pages 273?280.Daniel Gildea.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics (ACL?03),Sapporo, Japan, pages 80?87.Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2002.Breaking the resource bottleneck for multilingual parsing.In Proceedings of LREC.Martin Kay.
1996.
Chart generation.
In Proceedings of the34th Annual Meeting of the Association for ComputationalLinguistics, Santa Cruz, CA.Philipp Koehn.
2002.
Europarl: A multilingual corpus for eval-uation of machine translation.
Ms., University of SouthernCalifornia.Jonas Kuhn.
2004.
Experiments in parallel-text based grammarinduction.
In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics: ACL 2004,pages 470?477.Jonas Kuhn.
2005.
An architecture for parallel corpus-basedgrammar learning.
In Bernhard Fisseni, Hans-ChristianSchmitz, Bernhard Schro?der, and Petra Wagner, editors,Sprachtechnologie, mobile Kommunikation und linguisti-sche Ressourcen.
Beitra?ge zur GLDV-Tagung 2005 in Bonn,pages 132?144, Frankfurt am Main.
Peter Lang.Philip M. Lewis II and Richard E. Stearns.
1968.
Syntax-directed transduction.
Journal of the Association of Com-puting Machinery, 15(3):465?488.Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang.
2002.Learning chinese bracketing knowledge based on a bilinguallanguage model.
In COLING 2002 - Proceedings of the 19thInternational Conference on Computational Linguistics.I.
Dan Melamed.
2003.
Multitext grammars and synchronousparsers.
In Proceedings of NAACL/HLT.I.
Dan Melamed.
2004.
Statistical machine translation by pars-ing.
In Proceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics: ACL 2004, pages653?660.Gu?nter Neumann.
1998.
Interleaving natural language parsingand generation through uniform processing.
Artifical Intelli-gence, 99:121?163.Stuart Shieber.
1988.
A uniform architecture for parsing andgeneration.
In Proceedings of the 12th International Con-ference on Computational Linguistics (COLING), Budapest.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.24
