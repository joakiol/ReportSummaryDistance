SESSION 10: CORPORA AND EVALUATIONClifford J. WeinsteinMIT  L inco ln  Laboratory244 Wood StreetLex ington,  MA 02173INTRODUCTIONThis session on corpora and evaluation was composed oftwo distinct parts.
Before the break, four papers dealing with arange of important aspects of evaluation of written languagesystems and spoken language systems were presented.
Aprinted version of each of these papers is included in theconference proceedings.
After the break, a series of informalreports (not included as proceedings papers) were givensummarizing the work of the Corpora and PerformanceEvaluation Committee (CPEC) of the DARPA Spoken LanguageSystems (SLS) Program, with specific reports from severalworking groups which have been dealing with various aspectsof corpora collection and performance valuation in the SLSProgram.
A lively and extended iscussion followed theseworking group reports, including presentations of a number ofalternate viewpoints.SUMMARY OF  PAPERS PRESENTEDThird Message Understanding Conference(MUC-3): Phase I Status ReportwBethSundheimBeth Sundheim reported on the MUC-3 evaluation effort, forwhich a dry run had just been completed during the week prior tothe Speech and Natural Language Workshop.
The general senseof the presentation was that much progress had been maderelative to prior MUC evaluation efforts.
The number of sitesreported had increased to twelve.
The evaluation was broader inscope than previous ones in most respects, including textcharacteristics, task specifications, and performance measures.Specifically, the overgeneration a d fallout measures were newin this round.
A semi-automated scoring program had beendeveloped and distributed to all sites, including computation ofrecall, precision, overgeneration, and fallout measures designedspecifically for the MUC task.
A selected set of dry run resultswas presented, most of which are summarized in the printedpaper.
It was emphasized that the dry run results should notnecessarily be expected to be predictive of the results of officialtesting, which will take place in May 1991.
As an item ofinterest, Sundheim noted tests which had been performed atNYU using a "system" which ignored all but the dateline ofeach message, and which actually outperformed some "serious"systems in some of the evaluation dimensions.
In thediscussion, Paul Bamberg suggested that the scoring could bemodified so that a "trick" system could not perform well bysimply relying on a priori probabilities and guessing the mostlikely message.
The new scoring procedure would favor systemswhich produced the right information for unlikely inputmessages.A P rocedure  fo r  Quant i ta t ive ly  Compar ing  theSyntact i c  Coverage  o f  Eng l i sh  Grammars - -Ez ra  Black, et alEzra Black reported, on behalf of a group of fourteen co-authors/grammarians, on a new method of quantitativelycomparing parses provided by different grammarians, wheneach grammarian provides the single parse which he/she wouldideally want his/her grammar to produce.
The comparisonmethod is well-defined, relatively simple, and, moreimportantly, has been agreed upon by all the grammarians/co-authors.
The procedure judges each parse only by itsconstituent boundaries (and not by the names assigned to theseconstituents), compares the parse to a standard parse, andproduces two scores (the Recall Rate and the CrossingParentheses Rate) for the candidate parse.
Initially, the standardparse was taken as a majority parse derived from thegrammarians' parses.
Then it was determined that anindependent standard parse (the "hand parse" from theUniversity of Pennsylvania Treebank) worked just as well.
TheUPerm Treebank parse was then accepted as the standard parse.Based on the scores produced on a test sample consisting of 14sentences from the Brown corpus, the hand parses produced bythe grammarians were very consistent (average Recall 94% andaverage Crossing Rate <1%).
Black stated that a key next stepwill be to apply this evaluation procedure to machine parses,and remarked that initial tests had indicated that the grammarsdid not do nearly as well as the grammarians (the Recall Rate onthe best of four machine parsers tested so far was about 60%).One questioner from the audience asked how the 14sentences were selected ("pseudo-randomly"), and whether 14was enough for a meaningful test.
Black suggested that 14 was areasonable start, and that it was intended that 50 sentenceswould be used for test in the next round, which would involvemachine parsers.
All those with suitable machine parsers wereinvited to participate in this next test.
A second questionerasked whether the use of semantic knowledge by thegrammarians made the test unfair to syntactic grarnrnars.
Blackresponded that the main issue was to develop a comparativemeasure for parses, independent of what knowledge sourceswere used.Evaluating Text Categorization--David LewisDavid Lewis described and compared a variety of methods forevaluation of text categorization systems.
He explained therelationship between text categorization, and the more-frequently-evaluated task of text retrieval.
He defined thesystem effectiveness measures of recall, precision, and fallout,in terms of a contingency table for binary decisions, anddiscussed the difficult issue of clef'ruing a standard of correctnesson test data sets.
He contrasted "microaveraging" and297"macroaveraging" strategies in evaluation of retrieval systems,and commented that MUC uses microaveraging, where alldecisions made on a document are considered as a single groupwhen computing recall, precision, and fallout.
He describedindirect evaluation methods, wherein text categorization isevaluated based on the performance of a system (e.g., textretrieval or text extraction) which uses the results of the textcategorization.
The paper serves to place in perspective anumber of important techniques, as well as issues which mustbe addressed, in evaluation of text categorization systems.A P roposa l  fo r  Inc rementa l  D ia logueEva luat ion - -Made le ine  Bates  and  Damar isAyusoLyn Bates presented two proposed techniques for evaluationof SLS systems that deal with dialogue.
The examples in thispaper dealt directly with the Air Travel Information System(ATIS) task being used in the SLS program, hence this paperserved as a good bridge to the ATIS-oriented presentations afterthe break.
The first proposal suggested a methodology forcomparing systems based on their ability to handle diverseutterances in context.
For example, each system would betested with a specific "context-setting" query pair, followed bya set of (say, 10) alternative "next-utterances," each of whichwould directly follow the context-setting query pair.
Thesystem would be judged based on its answers for each of thealternative "next-utterances."
The second proposal suggested amodification of the performance metric to encourage partialunderstanding.
This proposal was aimed at permitting systemssome leeway in responding to partially-understood queries, aswell as providing credit for reasonable responses.
It was notedthat the judgment of whether an answer was reasonable wouldalmost eertainly have to be made by human arbiters.
Bothproposals in this paper were offered for further consideration bythe SLS evaluation committees.CORPORA AND PERFORMANCEEVALUATION COMMITTEE (CPEC)REPORTSThe collection and distribution of common speech corpora(including text transcriptions), and the definition andexecution of procedures and standards for the performanceevaluation of SLS systems processing these corpora, have beencentral activities in the SLS program.
These efforts have beencarried out under the aegis of a Corpora and PerformanceEvaluation Committee (CPEC), with the t~asks divided amongseveral Working Groups.
This part of the session was devotedto reports by the CPEC chairperson (David Pallett) and thechairpersons of each of the Working Groups.
Each reportincluded a brief summary of (1) the charter and goals of thegroup; (2) activities, issues addressed, decisions, andaccomplishments; and (3) open issues and work remaining tobe done.
The ensuing discussion included presentation of anumber of alternative viewpoints on some of the issuesaddressed.The order of prd~entations was as follows:CPEC Overview--Dave PallettATIS Corpora Working Group Report--Dave PallettPerformance Evaluation Working Group Report--Bill FisherATIS Relational Database Working Group Report--Bob MooreATIS Speech Recognition Working Group Report--Victor ZueInformal ATIS Speech Recognition Baseline Defmition--Doug PaulSpeech Corpora Working Group Report--Janet BakerCPEC Overv iew- -Dave  Pa l le t tCPEC Members.
Dave Pallett (NIST, Chair); Janet Baker(Dragon); Lyn Bates; (BBN); Bob Moore (SRI); Alex Rudnicky(CMU); Victor Zue (MIT-LCS).CPEC Issues and Role.
Pallett identified some of themany issues and details which CPEC has had to address toachieve the goals of a useful common corpus and meaningfulevaluation procedures.
For ATIS, these issues included: subjectscenarios; subject instructions; wizard instructions;display/feedback; data collection protocols (e.g., push-to-talk); transcription conventions; data classification; classdefinition (A, D1, etc.
); comparator evisions; canonicalanswer formats.
The specifies of these issues were delegated tothe Working Groups, with CPEC responsible for integration.Pallett also described the relationship between the CPECand working groups, the contracting agent (N/ST), and thedatabase contractors (TI and SRI).
Starting around July 91,NIST beearne DARPA's agent for data collection contracts, andhence became the official interface with the contractors.
NIST,in turn, had the challenge of integrating the decisions andadviee of the Working Groups into its direction of thecontractors' efforts.CPEC Challenges, February 91.
Pallett noted thefollowing challenges for CPEC and the Working Groups in themonths to come: (1) more ATIS data, collected at a faster ate;(2) better, more consistent, ATIS data; (3) improveddocumentation and communication; (4) refinements inevaluation procedures; and (5) development of new evaluationprocedures.AT IS  Corpora  Work ing  Group  Repor t - -DavePa l le t tThis report was integrated with the CPEC report, sincePallett chairs both groups.ATIS Corpora Working Group Members.
DavePallett (N/ST, Chair); Charles Hemphill (TI); Lew Norton(UNISYS); Patti Price (SRI); Alex Rudnicky (CMU); StephanieSeneff (MIT); Jay Wilpon (ATI'-BL).ATIS Corpora Work Group Pr imary Task.
Agreeon data collection paradigm for "contracted" ATIS Corpora(additional, ad hoe ATIS data was collected by some of the SLSgroups).ATIS Corpora Status.
Training data for the February91 evaluation was provided by both contractors (SRI and TI).The test data for the February 91 evaluation was provided by TL298Performance Evaluation Working GroupReport--Bill FisherPerformance Evaluation Working GroupMembers.
Bill Fisher (NIST, Chair); Janet Baker (Dragon);Debbie Dahl (UNISYS); Lyn Bates (BBN); Lynette Hirschman(MIT-LCS); Doug Appelt (SRI); Wayne Ward (CMU).Per fo rmance  Eva luat ion  I ssues  andAccomplishments.
Issues addressed by this WorkingGroup included: principles of interpretation, common answerspecification, query classification, and strategies for dialogevaluation.
A major accomplishment was the addition of a testinvolving some dialog dependency (the "DI" class of query), inaddition to the purely class A tests conducted in June 90.Areas for Further Work.
Updates are needed for thePrinciples of Interpretation Document and for theClassification Document.
The suite of tests needs to becarefully extended to include more dialog phenomena.ATIS Relational Database Working GroupReport--Bob MooreWorking Group Members.
Bob Moore (SRI, Chair);Rusty Bobrow (BBN); John Garofolo (NIST); Charles HemphillfH); Don McKay (UNISYS); Joe Polifroni (M1T/LCS).Goals, Approach, Issues.
Moore emphasized that thedatabase obtained from OAG was not in relational databaseform.
He commented that credit was due to TI, particularlyCharles Hemphill, for doing a good job, under heavy timepressure, in organizing the ten-city database.
He noted severalissues which the Working Group had addressed, includingdisplays, schema, etc.
He noted controversies about certainissues, such as the display presented to the user.
He suggestedthat it would be veery desirable to add connecting flights to thedatabase, as this would make ATIS a richer task; and noted thatsignificant work, including serious changes to the schema,would be needed to include connecting flights.
In thediscussion, the Stephanie Seneff suggested that it would beimportant to include not only connecting flights, but also theirfare structure.ATIS Speech Recognition Ad Hoc WorkingGroup Report--Victor ZueWorking Group Members.
Victor Zue (MIT-LCS,Chair); Janet Baker (Dragon); Kai-Fu Lee (Apple); Hy Murveit(SRI); Doug Paul (MIT-LL); Rich Schwartz (BBN); Rich Stern(CMU); Jay Wilpon (A'VI'-BL).Goal, Issues, and Outcome.
The goal of this WorkingGroup was to propose, in the ATIS domain: (1) a speechrecognition test protocol including definition of training andtest sets, a language model, and a vocabulary; (2) a scoringprocedure.
Issues addressed by the Group included:(1) Modularity of the speech recognition component andappropriateness of the evaluation--the k y issues here werehow to evaluate speech recognition in an environmentwhere speech recognition is only an intermediate result,and speech understanding is the goal.
(2) Need to converge quickly on a solution.After considerable discussion, the Working Group reachedagreement on the test set and on a modified scoring algorithm.Training set, language model, and vocabulary were left openfor the official February 91 evaluation.
Further considerationof ATIS speech recognition evaluation was deferred until afterthe February 91 tests.
However, an informal baselinespecification of training set, language model, and vocabularywere proposed by Doug Paul and Rich Schwartz.Informal ATIS CSR Baseline Test Definition--Doug PaulDoug Paul described the informal ATIS CSR baseline testspecification developed by himself and Rich Schwartz.
Thisspecification included:(1) a designated set of acoustic training and development testdata;(2) a vocabulary of 1065 words, consisting of 921 observedwords and additional words added to close classes (e.g.,months, days); and(3) a perplexity 17.8 bigrarn backoff language model,including the extended vocabulary and an "unknown word"class.All data and information for this baseline was madeavailable to all sites, with a note encouraging sites to test bothunder the baseline conditions and under other conditions oftheir choosing.
BBN and Lincoln conducted tests using thesebaseline conditions, and other sites made use of parts of thebaseline data, such as the vocabulary or the grammar.Speech  Corpora  Work ing  Group  Report- -Janet BakerSpeech Corpora Working Group Members.
JanetBaker (Dragon, Chair); Francis Kubala (BBN); Doug Paul(Lincoln); Bob Weide (CMU); Mitch Weintraub (SRI).Goal and Approach.
The goals of this Working Groupare to identify key research areas and provide resources notcurrently available, to stimulate and advance research andevaluation in continuous speech recognition (CSR) in theDARPA SLS Program.
The Group's approach was to define a setof desired attributes for CSR corpora, and identify thoseattributes most important o the research groups in the SLSprogram.Proposal for CSR Corpora.
The Working Group isproposing coUection of speech from two different applicationdomains: (I) the HANSARD domain of Canadian parliamentaryhearings; and (2) the CALS domain of logistical material inmaintenance repair manuals for planes, tanks, and othermilitary equipment.
Both domains are supported by large textcorpora on CD-ROM.
HANSARD has government/politicalflavor, general English, large vocabulary, high perplexity.CALS has military flavor, specialized, modest vocabulary andperplexity.
Read speech would be collected for beth domains,299to support a variety of training and test conditions for CSRresearch.Discussion PeriodFrancis Kubala (a member of the Speech Corpora WorkingGroup) presented a dissenting point-of-view with respect oCSR corpora collection.
He argued that to achieve the goal ofoperational human/machine interfaces, the most valuable datawould be spontaneous evaluation test data from severaldomains, and suggested that the SLS demonstration domainsshould be used.
He suggested several problems with Hansard(not a human/machine domain; read speech; single unfamiliardomain).John Makhoul described a range of data collection scenariosfor collecting goal-directed spontaneous peech.
Thesescenarios include: (1) using a Wizard (as at SRI and TI); (2)using a typist, with a machine to interpret queries (as at MIT);and (3) using a real-time speech recognition (new suggestion)system in conjunction with a Wizard or typist.
The thirdscenario was now becoming possible at sites (including BBN)which have real-time recognizers, and was proposed to haveadvantages both in realism and efficiency.A good deal of discussion followed, giving various views onboth the CSR corpus proposal, and on collection of speechcorpora in general.300
