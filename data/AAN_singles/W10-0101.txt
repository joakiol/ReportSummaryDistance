Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 1?9,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Variance as a Stopping Criterionfor Active Learning of Frame AssignmentMasood GhayoomiGerman Grammar GroupFreie Universita?t BerlinBerlin, 14195masood.ghayoomi@fu-berlin.deAbstractActive learning is a promising method to re-duce human?s effort for data annotation in dif-ferent NLP applications.
Since it is an itera-tive task, it should be stopped at some pointwhich is optimum or near-optimum.
In thispaper we propose a novel stopping criterionfor active learning of frame assignment basedon the variability of the classifier?s confidencescore on the unlabeled data.
The important ad-vantage of this criterion is that we rely only onthe unlabeled data to stop the data annotationprocess; as a result there are no requirementsfor the gold standard data and testing the clas-sifier?s performance in each iteration.
Ourexperiments show that the proposed methodachieves 93.67% of the classifier maximumperformance.1 IntroductionUsing supervised machine learning methods is verypopular in Natural Language Processing (NLP).However, these methods are not applicable for mostof the NLP tasks due to the lack of labeled data.
Al-though a huge amount of unlabeled data is freelyavailable, labeling them for supervised learningtechniques is very tedious, expensive, time consum-ing, and error prone.Active learning is a supervised machine learningmethod in which informative instances are chosenby the classifier for labeling.
Unlike the normal su-pervised set-up where data annotation and learningare completely independent, active learning is a se-quential process (Settles, 2009; Busser and Morante,2005).
This learning method is used in a variety ofNLP tasks such as information extraction (Thomp-son et al, 1999), semantic role labeling (Busser andMorante, 2005), machine translation (Haffari andSarkar, 2009), and name entity recognition (Lawsand Schu?tze, 2008).
In our study, we apply thismethod for the frame assignment task as a kind ofsemantic analysis.The process of active learning is as follows: thelearner takes a set of labeled instances, called seeddata, as an input for initial training of the classifier;and then a larger set of unlabeled instances will beselected by the classifier to be labeled with the hu-man interaction.
Even a small set of well selectedsamples for labeling can achieve the same level ofperformance of a large labeled data set; and the ora-cle?s effort will be reduced as a result.The motivation behind active learning is select-ing the most useful examples for the classifier andthereby minimizing the annotation effort while stillkeeping up the performance level (Thompson et al,1999).
There are two major learning scenarios inactive learning which are very popular among re-searchers and frequently used in various NLP tasks:stream-based sampling (Cohn et al, 1994) and pool-based sampling (Lewis and Gale, 1994).The samples that are selected should be hard andvery informative.
There are different query meth-ods for sample selection which are independent ofthe active learning scenarios (Settles, 2009).
Amongthem, uncertainty sampling (Lewis and Gale, 1994)is the most well-known and the simplest sam-ple selection method which only needs one classi-fier (Baldridge and Osborne, 2004).
In this querymethod, the samples that the classifier is least con-1Algorithm 1 Uncertainty Sampling in Active LearningInput: Seed data S, Pool of unlabeled samples UUse S to train the classifier Cwhile the stopping criterion is met doUse C to annotate USelect the top K samples from U predicted byC which have the lowest confidenceLabel K, augment S with theK samples, and re-move K from UUse S to retrain Cend whilefident on their labels are selected and handed out tothe oracle.
To this aim, a confidence score is re-quired which is in fact the prediction of the classi-fier with the highest probability for the label of thesample (Busser and Morante, 2005).The approach taken in active learning for our taskis based on the uncertainty of the classifier with ac-cess to the pool of data.
The learning process ispresented in Algorithm 1.
Since active learning isan iterative process (Busser and Morante, 2005), itshould be stopped at some point which is optimumor at least near-optimum.
A learning curve is usedas a means to illustrate the learning progress of thelearner, so that we can monitor the performance ofthe classifier.
In fact, the curve signals when thelearning process should stop as almost no increaseor even a drop in the performance of the classifieris observed.
At this point, additional training datawill not increase the performance any more.
In thispaper, we propose a new stopping criterion based onthe variability of the classifier?s confidence score onthe selected unlabeled data so that we avoid usingthe labeled gold standard.The structure of the paper is as follows.
In Sec-tion 2, we briefly describe frame semantics as it isthe domain of application for our model.
Section 3introduces our stopping criterion and describes theidea behind it.
In Section 4, we describe our dataset and present the experimental results.
In Section5, related work on stopping criteria is outlined; andfinally Section 6 summarizes the paper.2 Frame SemanticsSyntactic analysis such as part-of-speech (POS) tag-ging and parsing has been widely studied and hasachieved a great progress.
However, semantic anal-ysis did not have such a rapid progress.
This prob-lem has recently motivated researches to pay specialattention to natural language understanding since itis one of the essential parts in information extractionand question-answering.Frame semantic structure analysis which is basedon the case grammar of Fillmore (1968) is one ofthe understanding techniques to provide the knowl-edge about the actions, the participants of the ac-tion, and the relations between them.
In Fillmore?sview, a frame is considered as an abstract scene hav-ing some participants as the arguments of the pred-icate, and some sentences to describe the scene.
Infact the frames are the conceptual structures for thebackground knowledge of the abstract scenes repre-sented by the lexical units and provide context to theelements of the action.
FrameNet (Baker and Lowe,1998) is a data set developed at ICSI Berkley Uni-versity based on the frame semantics.In frame semantic structure analysis, the semanticroles of the elements participating in the action areidentified.
Determining and assigning the semanticroles automatically require two steps: frame assign-ment, and role assignment (Erk and Pado, 2006).The first step consists in identifying the frame whichis evoked by the predicate to determine the uniqueframe that is appropriate for the sample.
The nextstep is identifying the arguments of the predicate andassigning the semantic roles to the syntactic argu-ments of the given frame.
In our research, we studythe first step, and leave the second step for futurework.3 The Proposed Stopping CriterionThe main idea behind the stopping criteria is to stopthe classifier when it has reached its maximum per-formance and labeling of further examples from theunlabeled data set will not increase the classifier?sperformance any more.
Determining this point isvery difficult experimentally without access to thegold standard labels to evaluate the performance;however, we should find a criterion to stop activelearning in a near-optimum point.
To this aim, wepropose a novel stopping criterion which uses thevariance of the classifier?s confidence score for thepredicted labels to represent the degree of spread-ing out the confidence scores around their mean.
Wehypothesize that there is a correlation between the2performance saturation of the classifier and the vari-ability on the confidence of the selected instances.Generally, as we will see in Section 5, a stoppingcriterion could be based either on the performanceof the classifier on the test data, or on the confidencescore of the classifier on the unlabeled data.
In ourmethod, we used the second approach.
The biggestadvantage of this model is that no gold standard datais required to evaluate the performance of the systemin each iteration.3.1 Mean and VarianceMean and variance are two of the well-known sta-tistical metrics.
Mean is a statistical measurementfor determining the central tendency among a set ofscores.
In our study, we have computed the mean(M) of the classifier?s confidence score for the pre-dicted labels of 5 samples selected in each iteration.Variance is the amount of variability of the scoresaround their mean.
To compute the variability of theclassifier?s confidence score for the selected samplesin each iteration, the following equation is used inour task:V ariance =?Ki=1(Ci ?M)2K (1)where Ci is the confidence score of each selectedsample in each iteration, M is the mean of the confi-dence scores for the predicted labels, and K is thenumber of samples selected in the same iteration(K=5 in our study).3.2 The General IdeaAccording to the pool-based scenario, in each iter-ation K samples of the extra unlabeled data whichhave the lowest confidence score are selected, andafter labeling by the oracle they are added to thetraining data.
In the early iterations, the mean of theclassifier?s confidence score for the selected samplesis low.
Since the classifier is not trained enough inthese iterations, most of the scores are low and theydo not have a high degree of variability.
As a resultthe variance of the confidence score for these sam-ples is low.
We call this step the untrained stage ofthe classifier.As the classifier is training with more data, theconfidence score of the samples will gradually in-crease; as a result, there will be a high degree ofvariability in the confidence scores which spread outaround their mean.
In these iterations, the classifieris relatively in the borderline of the training stage,passing from untrained to trained; so that there willbe a high variability of confidence scores whichleads to have a high variance.
This is the trainingstage of the classifier.When the classifier is trained, the confidencescore of the classifier on the selected samples willincrease.
However, from a certain point that the clas-sifier is trained enough, all of the confidence scoresare located tightly around their mean with a low de-gree of variability; as a result, the variance of thesamples decreases.
This is the stage that the classi-fier is trained.The curve in Figure 1 represents the behavior ofthe variance in different iterations such that the xaxis is the number of iterations, and the y axis is thevariance of the confidence scores in each iteration.Figure 1: Normal distribution of variance for the classi-fier?s confidence scoreBased on our assumption, the best stopping point iswhen variance reaches its global peak and starts todecrease.
In this case, the classifier passes the train-ing stage and enters into the trained stage.3.3 The Variance ModelIt is difficult to determine the peak of the varianceon the fly, i.e.
without going through all iterations.One easy solution is to stop the learning process assoon as there is a decrease in the variance.
However,as it is very likely to stick in the local maxima of thevariance curve, this criterion does not work well.
Inother words, it is possible to have small peaks beforereaching the global peak, the highest variability ofthe classifier?s confidence score; so that we mightstop at some point we are not interested in and itshould be ignored.3To avoid this problem, we propose a model, calledvariance model (VM), to stop active learning whenvariance (V) decreases in n sequential iterations; i.e.Vi < Vi?1 < ... < Vi?n .There is a possibility that this condition is not satis-fied at all in different iterations.
In such cases, ac-tive learning will not stop and all data will be la-beled.
This condition is usually met when there areinstances in the data which are inherently ambigu-ous.
Having such data is generally unavoidable andit is often problematic for the learner.Although the above model can deal with the lo-cal maxima problem, there is a possibility that thedecreased variance in n sequential iterations is verysmall and it is still possible to stick in the local max-ima.
To avoid this problem and have a better stop-ping point, we extend the proposed model by settinga threshold m, called the Extended Variance Model(EVM), in which the minimum variance decrementin n sequential iterations must be m; i.e.Vi < Vi?1 ?
m < ... < Vi?n ?
m.4 Experimental Results4.1 Setup of ExperimentWhat we aim to do in our study is assigning frameswith active learning.
We have chosen the pool-basedscenario by using the uncertainty sampling method.In our task, since we have a small data set, 5 in-stances (K=5) with the lowest confidence score ofthe predited labels will be selected in each iterationfrom the pool of data and handed out to the oracle tobe labeled.We have used a toolkit for the supervised wordsense disambiguation task called Majo (Rehbein etal., 2009) which has a graphical user interface (GUI)for semantic annotation based on active learning.The toolkit supports German and English; and ituses the openNLP MAXENT package1 to build themodel.
In this toolkit, the confidence score of theclassifier is the posterior probability of the mostprobable label assigned to each sample.In addition, there are some built-in plugins in thetool for syntactic and semantic pre-processing toprovide the relevant features for the classifier.
Weutilized the following plugins that support English:1http://maxent.sourceforge.net/?
Stanford Word Range Plugin provides featuresbased on the local context of the surface stringfor the target.
The window size of the localcontext can be set manually in the GUI.
Basedon initial experiments for the target verbs, wefound out that a window ?3 performs the best.?
Stanford POS Tag Word Range Plugin providesthe POS tags of the words within a sentenceby using Stanford POS Tagger.
In this plugin,the window size could also be set manually toextract the POS local context of the target word.Based on initial experiments, a window of ?3achieved the best performance.?
Berkley Sentence Phrase Plugin utilizes theBerkley Parser and provides the syntactic anal-ysis of the sentence.
This plugin is used to ex-tract all word forms of the children nodes froma particular syntactic mother node (VP in ourstudy) and add them to the feature set.?
Berkley Sentence Phrase POS Tag Plugin usesthe Berkley POS tagger such that we define themother node of the target word in the parse tree(VP in our study) and it identifies and extractsall children of this mother node and uses theirPOS as features.4.2 CorpusThe annotated data that we used for our ex-periments is the current version of the Berke-ley FrameNet (Baker and Lowe, 1998) for En-glish which consists of 139,437 annotated exam-ples from the British National Corpus for 10,196predicates.
Among the predicates that FrameNet in-volvs, namely verbs, nouns, adjectives, and prepo-sitions, we only considered verbs; as a result thedata reduced to 61,792 annotated examples for 2,770unique verb-frames.In the next step, we removed all verbs that haveonly one frame as they are not ambiguous.
Hav-ing only ambiguous verbs, the number of predicatesreduced to 451 unique verbs.
Out of these targets,there are only 37 verbs which have more than 100annotated samples.
Among these verbs, we concen-trated on 14 verbs selected randomly; however, inthe selection we tried to have a balance distributionof frames that the targets have.
Therefore, we se-lected 4 targets (phone, rush, scream, throw) with4Table 1: Data distribution of the targetsVerb Frames Freq.
S E TBend 4 115 11 82 22Feel 5 134 13 95 26Follow 3 113 10 81 22Forget 3 101 9 72 20Hit 4 142 12 102 28Look 3 183 15 134 34Phone 2 166 14 121 31Rise 4 110 11 77 22Rush 2 168 14 123 31Scream 2 148 12 108 28Shake 4 104 10 73 21Smell 3 146 13 106 27Strike 3 105 10 75 20Throw 2 155 13 113 29two frames, 5 targets (follow, forget, look, smell,strike) with three frames, 4 targets (bend, hit, rise,shake) with four frames, and 1 target (feel) with fiveframes.4.3 Data DistributionThe total amount of data prepared for the 14 verbsare divided into three non-overlapping sets in a bal-anced form in terms of both the number of the targetpredicate frames, and the relevant instances of eachframe.
In other words, the distribution should besuch that different frames of the target verb is foundin each of the three data sets.
10% is considered asinitial seed data (S); 20% as test data (T), and the restof 70% as extra unlabeled data (E).
Table 1 presentsthe data distribution in which 5-fold cross-validationis performed to minimize the overfitting problem.As mentioned, our proposed stopping criterionhas two parameters, n and m, that should be tuned.For this purpose, we divided the 14 targets into theheld-out set and the test set.
To this aim, 7 tar-gets, namely feel, look, phone, rise, shake, smell,and throw are selected as the held-out set; and 7 tar-gets, namely bend, follow, forget, hit, rush, scream,and strike are used as the test set.4.4 ResultsFigures 2 and 3 illustrate the learning curves of theactive learning process and random sampling as thebaseline for the targets look and rise.
The curvesare the average of the 5 folds.
As can be seen, inthese targets our classifier has beaten the majorityFigure 2: Learning curve of the verb look for 5 foldsFigure 3: Learning curve of the verb rise for 5 foldsclass baseline; and also active learning with uncer-tainty sampling has a relatively better performancethan random sampling.Figures 4 and 5 present the average variancecurves of 5 folds for the two targets.
These curvesverify our assumption about the behavior of the vari-ance curve as described in Section 3.2.
As thegraphs show, following our assumption the variabil-ity around the mean is tight in the early stages oftraining; then as the classifier is trained with moredata, the variability around the mean spreads out;and finally, the variability will be tight again aroundthe mean.Applying our proposed stopping criterion, in eachiteration we compute the variance of the classifier?sconfidence score for the selected samples in eachfold.
To evaluate how well our stopping criterionis, we have compared our results with the maximumaverage performance of the classifier for the 5 foldsin which the whole data is labeled.Applying our model on the held-out set, we foundthat n=2 is the best value based on our data set, sothat we stop active learning when variance decreasesin two sequential iterations; i.e.Vi < Vi?1 and Vi?1 < Vi?2 .Our idea is shown in Figure 6 for fold 5 of the tar-get rise, such that the proposed stopping criterion issatisfied in iteration 11.
As shown, the decrement of5Figure 4: Variance curve of the verb look for 5 foldsFigure 5: Variance curve of the verb rise for 5 foldsvariance in iterations 3, 5, and 7 is the local maximaso that active learning does not stop in these itera-tions and they are ignored.The summary of the result for the uncertaintysampling method of the test set is shown in Table 2in which the F-score serves as the evaluation met-ric.
Comparing the applied variance model as thestopping criterion on the test set with the maximumperformance (M) of the uncertainty sampling as anupper bound in our experiment, we see that for twotargets (bend, rush) the maximum performance ofthe classifier is achieved at the stopping point; fortwo targets (follow, hit) there is a minor reduction inthe performance; while for the other targets (forget,scream, strike) there is a big loss in the performance.Averagely, the variance model achieved 92.66% ofthe maximum performance.To determine the advantage of our stopping cri-terion, we present the total numbers of annotatedinstances (A) for each target, their relevant num-bers of annotated instances for the maximum per-formance, and the variance model in Table 3.
Av-Figure 6: Variance curve of the verb riseTable 2: The comparison of the average performance ofthe classifier (F-score) on the stopping point with themaximum performance in uncertainty samplingVerb M VMBend 53.00 53.00Follow 71.81 70.00Forget 51.00 41.00Hit 65.71 63.56Rush 89.03 89.03Scream 72.14 62.85Strike 64.00 53.00Average 66.67 61.78Table 3: The comparison of the number of the annotateddata for all data, at the maximum performance, and at thestopping pointVerb A M VMBend 93 46 55Follow 91 75 54Forget 81 79 51Hit 114 67 71Rush 137 24 51Scream 120 62 64Strike 85 85 41Average 103 62.57 55.29eragely, if we have 103 samples for annotation, weneed to annotate almost 63 instances to reach themaximum performance of 66.67%; while by apply-ing our stopping criterion, the learning process stopsby annotating at least 55 instances with 61.78% per-formance.
I.e., annotating a smaller number of in-stances, our active learner achieves a near-optimumperformance.
It is worth to mention that since it isvery difficult to achieve the upper bound of the clas-sifier?s performance automatically, all data is labeledto find the maximum performance of the classifier.Looking carefully on the variance curves of the 5folds of the held-out set, we have seen that in someiterations the decreased variance in two sequentialiterations is very small and it may still stick in thelocal maxima as can be seen in iteration 8 of fold 3of the target look in Figure 7.To avoid sticking in such local maxima, we usedthe extended version of our original model and seta threshold (m) in the held-out set.
Experimentallywe found out that the decreasing variance in two se-quential iterations must be bigger than 0.5; i.e.Vi < Vi?1 - 0.5 and Vi?1 < Vi?2 - 0.5;6Figure 7: Variance curve of the verb lookso that in Figure 7 we stop in iteration 18.
We ap-plied the extended variance model on the test set andcompared the results to our original variance model.We found out for two targets (forget, scream) theextended model has achieved a very good perfor-mance; for four targets (follow, hit, rush, strike) itwas ineffective; and for one target (bend) it causedto have a small reduction in the performance.The summary of the classifier performance afterapplying the extended model for uncertainty sam-pling is shown in Table 4.
To ease the comparison,the performance of our original model is repeatedin this table.
As presented in the table, the averageperformance in the extended model has a 13.70%relative improvement compared to the average per-formance in the original variance model.Table 4: The comparison of the average performance ofthe classifier (F-score) on the variance model and the ex-tended variance modelVerb VM EMBend 53.00 52.00Follow 70.00 70.00Forget 41.00 46.00Hit 63.56 63.56Rush 89.03 89.03Scream 62.85 63.56Strike 53.00 53.00Average 61.78 62.455 Related Work on Stopping CriteriaThe simplest stopping criterion for active learningis when the training set has reached a desirable sizeor a predefined threshold.
In this criterion, the ac-tive learning process repeatedly provides informa-tive examples to the oracle for labeling, and updatesthe training set, until the desired size is obtainedor the predefined stopping criterion is met.
Practi-cally, it is not clear how much annotation is suffi-cient for inducing a classifier with maximum effec-tiveness (Lewis and Gale, 1994).Schohn and Cohn (2000) have used support vectormachines (SVM) for document classification usingthe selective sampling method and they have pro-posed a criterion to stop the learning process in theirtask.
Based on their idea, when there is no informa-tive instance in the pool which is closer to the sep-arating hyperplane than any of the support vectors,the margin exhausts and the learning process stops.Zhu and Hovey (2007) have used a confidence-based approach for the stopping criteria by utlizingthe maximum confidence and the minimum error ofthe classifier.
The maximum confidence is based onthe uncertainty measurement when the entropy ofthe selected unlabeled sample is less than a prede-fined threshold close to zero.
The minimum error isthe feedback from the oracle when active learningasks for the true label of the selected unlabeled sam-ple and the accuracy prediction of the classifier forthe selected unlabeled sample is larger than a prede-fined accuracy threshold.
These criteria are consid-ered as upper-bound and lower-bound of the stop-ping condition.Zhu et al (2008) proposed another stoppingcriterion based on a statistical learning approachcalled minimum expected error strategy.
In this ap-proach, the maximum effectiveness of the classifieris reached when the classifier?s expected errors onfuture unlabeled data is minimum.Vlachos (2008) has used the classifier confidencescore as a stopping criterion for the uncertainty sam-pling.
He has applied his model to two NLP tasks:text classification and named entity recognition.
Hehas built his models with the SVM and the maxi-mum entropy.
The idea is when the confidence ofthe classifier remains at the same level or drops fora number of consecutive iterations, the learning pro-cess should terminate.Laws and Schu?tze (2008) suggested three crite-ria -minimal absolute performance, maximum pos-sible performance, and convergence- to stop activelearning for name entity recognition using the SVMmodel with the uncertainty sampling method.
Inminimal absolute performance, a threshold is pre-defined by the user; and then the classifier esti-mates its own performance by using only the unla-beled reference test set.
Since there is no available7labeled test set, the evaluation performance is notpossible.
The maximum possible performance is aconfidence-based stopping criterion in which activelearning is stopped where the optimal performanceof the classifier is achieved.
Again, in this approachthere is no labeled test data to evaluate the perfor-mance.
The convergence criterion is met when moreexamples from the pool of unlabeled data do notcontribute more information to the classifier?s per-formance, so that the classifier has reached its maxi-mum performance.
Laws and Schu?tze computed theconvergence as the gradient of the classifier?s esti-mated performance or uncertainty.Tomanek and Hahn (2008) proposed a stoppingcriterion based on the performance of the classi-fier without requiring a labeled gold standard for acommittee-based active learning on the name en-tity recognition application.
In their criterion, theyapproximated the progression of the learning curvebased on the disagreement among the committeemembers.
They have used the validation set agree-ment curve as an adequate approximation for theprogression of the learning curve.
This curve wasbased on the data in each active learning iterationthat makes the agreement values comparable be-tween different active learning iterations.Bloodgood and Vijay-Shanker (2009) explainedthree areas of stopping active learning that shouldbe improved: applicability (restricting the usage incertain situation), lack of aggressive stopping (find-ing the stopping points which are too far, so moreexamples than necessary are annotated), instability(well working of a method on some data set but notthe other data set).
Further, they presented a stop-ping criterion based on stabilizing predictions thataddresses each of the above three areas and providesa user-adjustable stopping behavior.
In this method,the prediction of active learning was tested on exam-ples which do not have to be labeled and it is stoppedwhen the predictions are stabilized.
This criterionwas applied to text classification and named entityrecognition tasks using the SVM and the maximumentropy models.6 Summary and Future WorkIn this paper, after a brief overview of frame seman-tics and active learning scenarios and query meth-ods, we performed the frame assignment in the pool-based active learning with the uncertainty samplingmethod.
To this end, we chose 14 frequent targetsfrom FrameNet data set for our task.One of the properties of active learning is its itera-tivness which should be stopped when the classifierhas reached its maximum performance.
Reachingthis point is very difficult; therefore, we proposeda stopping criterion which stops active learning in anear-optimum point.
This stopping criterion is basedon the confidence score of the classifier on the extraunlabeled data such that it uses the variance of theclassifier?s confidence score for the predicted labelsof a certain number of samples selected in each it-eration.
The advantage of this criterion is that thereis no need to the labeled gold standard data and test-ing the performance of the classifier in each itera-tion.
Based on this idea, we proposed a model whichis satisfied by n sequential decrease on a variancecurve.
The original model is expanded by setting athreshold m on the amount of the decrement of vari-ance in n sequential iterations.
We believe that ourproposed criterion can be applied to any active learn-ing setting based on uncertainty sampling and it isnot limited to the frame assignment.To find out how effective our model is, we com-pared the achieved results of our variance modelwith the maximum performance of the classifier andwe found that 92.66% of the performance is kept inthe test data.
In the extended variance model, weachieved a higher performance of the classifier inwhich 93.67% of the performance is kept.For the future word, while in our current re-search the learner selects 5 instances in each itera-tion, this number could be different and investiga-tion is needed to find out how much our proposedcriterion depends on the K. The other possibility toexpand our proposed model is using the variance ofthe classifier?s confidence score for the predicted la-bels of the whole unlabeled data in each iteration andnot the selected samples.7 AcknowledgmentsThe author?s special gratitude goes to CarolineSporleder and Ines Rehbein at Saarland Universityfor their support and helpful comments in the re-search.
Masood Ghayoomi is funded by the Ger-man research council DFG under the contract num-ber MU 2822/3-1.8ReferencesC.
F. Baker and C. J. Fillmore J.
B. Lowe.
1998.
Theberkeley framenet project.
In Proceedings of ACL,pages 86?90, Montreal, QC.J.
Baldridge and M. Osborne.
2004.
Active learningand the total cost of annotation.
In Proceedings ofEMNLP, pages 9?16, Barcelona, Spain.M.
Bloodgood and K. Vijay-Sarkar.
2009.
A methodfor stopping active learning based on stabilizing pre-dictions and the need for user-adjustable stopping.In 13th Conf.
on Computational Natural LanguageLearning, pages 39?47, Boulder, Colorado.B.
Busser and R. Morante.
2005.
Designing an activelearning based system for corpus annotation.
In Re-vista de Procesamiento del Lenguaje Natural, num-ber 35, pages 375?381.D.
Cohn, A. L. Atlas, and R. E. Ladner.
1994.
Improvinggeneralization with active learning.
Machine Learn-ing, 15(2):201?221.K.
Erk and S. Pado.
2006.
Shalmaneser - a toolchain forshallow semantic parsing.
In Proceedings of LREC,Genoa, Italy.C.
J. Fillmore.
1968.
The case for case.
In Emmon W.Bach and Robert T. Harms, editors, Universals in Lin-guistic Theory, pages 1?88, New York.
Rinehart andWinston.G.
Haffari and A. Sarkar.
2009.
Active learning for mul-tilingual statistical machine translation.
In Proceed-ings of the 47th ACL-IJCNLP, Singapore.F.
Laws and H. Schu?tze.
2008.
Stopping criteria for ac-tive learning of named entity recognition.
In Proceed-ings of the 22nd CoLing, pages 465?472, Manchester.D.D.
Lewis and W. Gale.
1994.
A sequential algo-rithm for training text classifiers.
In Proceedings ofthe ACM SIGIR Conf.
on Research and Developmentin IR, pages 3?12.I.
Rehbein, J. Ruppenhofer, and J. Sunde.
2009.
Majo- a toolkit for supervised word sense disambiguationand active learning.
In Proceedings of the 8th Int.Workshop on Treebanks and Linguistic Theories, Mi-lan, Italy.G.
Schohn and D. Cohn.
2000.
Less is more: Activelearning with support vector machines.
In Proceed-ings of 17th Int.
Conf.
on Machine Learning, StanfordUniversity.B.
Settles.
2009.
Active learning literature survey.
Com-puter Sciences Technical Report 1648, University ofWisconsin?Madison.C.
A. Thompson, M.E.
Califf, and R.J. Mooney.
1999.Active learning for natural language parsing and in-formation extraction.
In Proceedings of the 16th Int.Conf.
on Machine Learning, pages 406?414.K.
Tomanek and U. Hahn.
2008.
Approximating learn-ing curves for active-learning-driven annotation.
In6th Int.
Language Resources and Evaluation Confer-ence, pages 1319?1324.A.
Vlachos.
2008.
A stopping criterion for active learn-ing.
Journal of Computer, Speech and Language,22(3):295?312.J.
Zhu and E. Hovy.
2007.
Active learning for word sensedisambiguation with methods for addressing the classimbalance problem.
In Proceedings of the EMNLP-CoNLL, pages 783?790, Prague.J.
Zhu, H. Wang, and E. Hovy.
2008.
Learning a stop-ping criterion for active learning for word sense dis-ambiguation and text classification.
In Proceedings ofthe 3rd IJNLP, pages 366?372, Heydarabad, India.9
