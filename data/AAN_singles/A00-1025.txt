Examining the Role of Statistical and Linguistic KnowledgeSources in a General-Knowledge Question-Answering SystemCla i re  Card ie  1 and V incent  Ng  1 and Dav id  P ie rce  1 and Chr i s  Buck ley  2Depar tment  of Computer  Science, Cornell  University, I thaca, NY 148531SaB IR  Research 2E-mail: cardie,yung,pierce@cs.cornel l .edu, chr isb@sabir .comAbstractWe describe and evaluate an implemented systemfor general-knowledge question answering.
The sys-tem combines techniques for standard ad-hoc infor-mation retrieval (IR), query-dependent text summa-rization, and shallow syntactic and semantic sen-tence analysis.
In a series of experiments we examinethe role of each statistical and linguistic knowledgesource in the question-answering system.
In con-trast to previous results, we find first that statisti-cal knowledge of word co-occurrences a computedby IR vector space methods can be used to quicklyand accurately locate the relevant documents foreach question.
The use of query-dependent textsummarization techniques, however, provides onlysmall increases in performance and severely limitsrecall levels when inaccurate.
Nevertheless, it is thetext summarization component that allows subse-quent linguistic filters to focus on relevant passages.We find that even very weak linguistic knowledgecan offer substantial improvements over purely IR-based techniques for question answering, especiallywhen smoothly integrated with statistical prefer-ences computed by the IR subsystems.1 In t roduct ionIn this paper, we describe and evaluate an imple-mented system for general-knowledge question an-swering.
Open-ended question-answering systemsthat allow users to pose a question of any type, inany language, without domain restrictions, remainbeyond the scope of today's text-processing systems.We investigate instead a restricted, but neverthelessuseful variation of the problem (TREC-8, 2000):Given a large text collection and a set ofquestions specified in English, find answersto the questions in the collection.In addition, the restricted task guarantees that:?
the answer exists in the collection,?
all supporting information for the answer lies ina single document, and?
the answer is short m less than 50 bytes inlength.Consider, for example, the question Which countryhas the largest part of the Amazon rain forest?, takenfrom the TREC8 Question Answering developmentcorpus.
The answer (in document LA032590-0089)is BrazilPrevious research as addressed similar question-answering (QA) scenarios using a variety of natu-ral language processing (NLP) and information re-trieval (IR) techniques.
Lehnert (1978) tackles thedifficult task of answering questions in the context ofstory understanding.
Unlike our restricted QA task,questions to Lehnert's ystem often require answersthat are not explicitly mentioned in the story.
Hergoal then is to answer questions by making infer-ences about actions and actors in the story usingworld knowledge in the form of scripts, plans, andgoals (Schank and Abelson, 1977).
More recently,Burke et al (1995; 1997) describe a system that an-swers natural anguage questions using a database ofquestion-answer pairs built from existing frequently-asked question (FAQ) files.
Their FAQFinder sys-tem uses IR techniques to match the given questionto questions in the database.
It then uses the Word-Net lexical semantic knowledge base (Miller et al,1990; Fellbaum, 1998) to improve the quality of thematch.Kupiec (1993) investigates a closed-class QA taskthat is similar in many respects to the TREC8QA task that we address here: the system answersgeneral-knowledge questions using an encyclopedia.In addition, Kupiec assumes that all answers arenoun phrases.
Although our task does not explic-itly include a "noun phrase" constraint, the answerlength restriction effectively imposes the same biastoward noun phrase answers.
Kupiec's MURAX sys-tem applies a combination of statistical (IR) andlinguistic (NLP) techniques.
A series of secondaryboolean search queries with proximity constraints icombined with shallow parsing methods to find rele-vant sections of the encyclopedia, to extract answerhypotheses, and to confirm phrase relations speci-fied in the question.
In an evaluation on 70 "Trivial180questiondocumentcollectionRetrieval i documents, ii text passages ii .-iiIR Subsystems iSummarizationParsingSemanticTypesLinguisticRelationshipsLinguistic Filtersanswerhypotheses!Figure 1: General Architecture of the Question-Answering SystemPursuit" who and what questions, Kupiec concludesthat robust natural language analysis can add to thequality of the information retrieval process.
In addi-tion, he claims that, for their closed-class QA task,vector space IR methods (Salton et al, 1975) appearinadequate.We present here a new approach to the re-stricted question-answering task described above.Like MURAX, our system draws from both statisti-cal and linguistic sources to find answers to general-knowledge questions.
The underlying architecture ofthe system, however, is very different: it combinesvector space IR techniques for document retrieval, avector space approach to query-dependent text sum-marization, shallow corpus-based syntactic analysis,and knowledge-based semantic analysis.
We eval-uate the system on the TREC8 QA developmentcorpus as well as the TREC8 QA test corpus.
Inparticular, all parameters for the final QA systemare determined using the development corpus.
Ourcurrent results are encouraging but not outstanding:the system is able to correctly answer 22 out of 38 ofthe development questions and 91 out of 200 of thetest questions given five guesses for each question.Furthermore, the first guess is correct for 16 out ofthe 22 development questions and 53 out of 91 of thetest questions.More importantly, we investigate the relative roleof each statistical and linguistic knowledge sourcein the proposed IR/NLP question-answering system.In contrast o previous results, we find that sta-tistical knowledge of word co-occurrences a com-puted by vector space models of IR can be used toquickly and accurately ocate relevant documents inthe restricted QA task.
When used in isolation, vec-tor space methods for query-dependent text summa-rization, however, provide relatively small increasesin performance.
In addition, we find that the textsummarization component can severely limit recalllevels.
Nevertheless, it is the summarization compo-nent that allows the linguistic filters to focus on rele-vant passages.
In particular, we find that very weaklinguistic knowledge can offer substantial improve-ments over purely IR-based techniques for questionanswering, especially when smoothly integrated withthe statistical preferences computed by the IR sub-systems.In the next section, we describe the general archi-tecture of the question-answering system.
Section 3describes the baseline system and its information re-trieval component.
Sections 4-7 describe and evalu-ate a series of variations to the baseline system thatincorporate, in turn, query-dependent text summa-rization, a syntactic filter, a semantic filter, and analgorithm that allows syntactic knowledge to influ-ence the initial ordering of summary extracts.
Sec-tion 8 compares our approach to some of those inthe recent TREC8 QA evaluation (TREC-8, 2000)and describes directions for future work.2 System Arch i tec tureThe basic architecture of the question-answering sys-tem is depicted in Figure 1.
It contains two maincomponents: the IR subsystems and the linguisticfilters.
As a preliminary, ofl\]ine step, the IR sub-system first indexes the text collection from whichanswers are to be extracted.
Given a question, thegoal of the IR component is then to return a rankedlist of those text chunks (e.g.
documents, entences,or paragraphs) from the indexed collection that aremost relevant o the query and from which answerhypotheses can he extracted.
Next, the QA systemoptionally applies one or more linguistic filters tothe text chunks to extract an ordered list of answerhypotheses.
The top hypotheses are concatenated toform five 50-byte guesses as allowed by the TREC8guidelines.
Note that many of these guesses maybe difficult to read and judged as incorrect by the181TREC8 assessors: we will also describe the resultsof generating single phrases as guesses wherever thisis possible.In the sections below, we present and evaluate aseries of instantiations of this general architecture,each of which makes different assumptions regardingthe type of information that will best support heQA task.
The next section begins by describing thebaseline QA system.3 The Vector Space Model forDocument RetrievalIt is clear that a successful QA system will needsome way to find the documents hat are most rele-vant to the user's question.
In a baseline system, weassume that standard IR techniques can be used forthis task.
In contrast to MURAX, however, we hy-pothesize that the vector space retrieval model willsuffice.
In the vector space model, both the ques-tion and the documents are represented asvectorswith one entry for every unique word that appearsin the collection.
Each entry is the term weight, areal number that indicates the presence or absenceof the word in the text.
The similarity between aquestion vector, Q = ql ,q2,.
.
.
,qn, and a documentvector, D = dl, d2,.
.
.
,  tin, is traditionally computedusing a cosine similarity measure:n8im(Q,D)  = Z d, .q,i..~ lUsing this measure, the IR system returns a rankedlist of those documents most similar to the question.The  Baseline QA System: The  Smart  Vec-tor  Space Model .
For the IR component of thebaseline QA system, we use Smart (Salton, 1971),a sophisticated text-processing system based on thevector space model and employed as the retrievalengine for a number of the top-performing systemsat recent Text REtrieval Conferences (e.g.
Buckleyet al, 1998a, 1998b).
Given a question, Smart re-turns a ranked list of the documents most relevantto the question.
For the baseline QA system and allsubsequent variations, we use Smart with standardterm-weighting strategies I and do not use automaticrelevance f edback (Buckley, 1995).
In addition, thebaseline system applies no linguistic filters.
To gen-erate answers for a particular question, the systemstarts at the beginning of the top-ranked ocumentreturned by Smart for the question and constructsfive 50-byte chunks consisting of document text withstopwords removed.lWe use Lnu term weighting for documents and Itu termweighting for the question (Singhal et al, 1996).Evaluation.
As noted above, we evaluate achvariation of our QA system on 38 TREC8 devel-opment questions and 200 TREC8 test questions.The indexed collection is TREC disks 4 and 5 (with-out Congressional Records).
Results for the baselineSmart IR QA system are shown in the first row ofTable 1.
The system gets 3 out of 38 developmentquestions and 29 out of 200 test questions correct.We judge the system correct if any of the five guessescontains each word of one of the answers.
The finalcolumn of results hows the mean answer ank acrossall questions correctly answered.Smart is actually performing much better than itsscores would suggest.
For 18 of the 38 developmentquestions, the answer appears in the top-ranked doc-ument; for 33 questions, the answer appears in oneof the top seven documents.
For only two questionsdoes Smart fail to retrieve a good document in thetop 25 documents.
For the test corpus, over halfof the 200 questions are answered in the top-rankeddocument (110); over 75% of the questions (155) areanswered in top five documents.
Only 19 questionswere not answered in the top 20 documents.4 Query-Dependent TextSummar izat ion  fo r  Quest ionAnsweringWe next hypothesize that query-dependent textsummarization algorithms will improve the perfor-mance of the QA system by focusing the systemon the most relevant portions of the retrieved oc-uments.
The goal for query-dependent summariza-tion algorithms is to provide a short summary ofa document with respect to a specific query.
Al-though a number of methods for query-dependenttext summarization are beginning to be developedand evaluated in a variety of realistic settings (Maniet al, 1999), we again propose the use of vector spacemethods from IR, which can be easily extended tothe summarization task (Salton et al, 1994):1.
Given a question and a document, divide thedocument into chunks (e.g.
sentences, para-graphs, 200-word passages).2.
Generate the vector epresentation forthe ques-tion and for each document chunk.3.
Use the cosine similarity measure to determinethe similarity of each chunk to the question.4.
Return as the query-dependent summary themost similar chunks up to a predetermined sum-mary length (e.g.
10% or 20% of the originaldocument).This approach to text summarization was shownto be quite successful in the recent SUMMAC eval-uation of text summarization systems (Mani et al,1999; Buckley et al, 1999).
Our general assumption182here is that Ii~ approaches can be used to quicklyand accurately find both relevant documents andrelevant document portions.
In related work, Chaliet al (1999) also propose text summarization tech-niques as a primary component for their QA system.They employ a combination of vector-space meth-ods and lexical chaining to derive their sentence-based summaries.
We hypothesize that  deeper anal-ysis of the summary extracts is better accomplishedby methods from NLP that can determine syntac-tic and semantic relationships between relevant con-stituents.
There is a risk in using query-dependentsummaries to focus the search for answer hypothe-ses, however: if the summarization algorithm is inac-curate, the desired answers will occur outside of thesummaries and will not be accessible to subsequentcomponents of the QA system.The Query -Dependent  Text  Summar izat ionQA System.
In the next version of the QA sys-tem, we augment he baseline system to performquery-dependent text summarization for the top kretrieved ocuments.
More specifically, the IR sub-system returns the summary extracts (sentences orparagraphs) for the top k documents after sort-ing them according to their cosine similarity scoresw.r.t, the question.
As before, no linguistic filters areapplied, and answers are generated by constructing50-byte chunks from the ordered extracts after re-moving stopwords.
In the experiments below, k = 7for the development questions and k = 6 for the testquestions.
2Eva luat ion .
Results for the Text SummarizationQA system using sentence-based summaries areshown in the second row of Table 1.
Here we seea relatively small improvement: the system nowanswers four development and 45 test questionscorrectly.
The mean answer rank, however, im-proves noticeably from 3.33 to 2.25 for the develop-ment corpus and from 3.07 to 2.67 for the test cor-pus.
Paragraph-based summaries yield similar butslightly smaller improvements; as a result, sentencesummaries are used exclusively in subsequent sec-tions.
Unfortunately, the system's reliance on query-dependent text summarization actually limits its po-tential: in only 23 of the 38 development questions(61%), for example, does the correct answer appearin the summary for one of the top k -- 7 documents.The QA system cannot hope to answer correctly anyof the remaining 15 questions.
For only 135 of the200 questions in the test corpus (67.5%) does thecorrect answer appear in the summary for one of2The value for k was chosen so that at least 80% of thequestions in the set had answers appearing in the retrieveddocuments ranked 1-k. We have not experimented exten-sively with many values of k and expect that better perfor-mance can be obtained by tuning k for each text collection.the top k -- 6 documents.
3 It is possible that au-tomatic relevance feedback or coreference r solutionwould improve performance.
We are investigatingthese options in current work.The decision of whether or not to incorporate textsummarization i the QA system depends, in part,on the ability of subsequent processing components(i.e.
the linguistic filters) to locate answer hypothe-ses.
If subsequent components are very good atdiscarding implausible answers, then summarizationmethods may limit system performance.
Therefore,we investigate next the use of two linguistic filters inconjunction with the query-dependent text summa-rization methods evaluated here.5 Incorporat ing  the  Noun PhraseF i l te rThe restricted QA task that we investigate requiresanswers to be short - -  no more than 50 bytes inlength.
This effectively eliminates how or why ques-tions from consideration.
Almost all of the remain-ing question types are likely to have noun phrases asanswers.
In the TREC8 development corpus, for ex-ample, 36 of 38 questions have noun phrase answers.As a result, we next investigate the use of avery simple linguistic filter that considers only nounphrases as answer hypotheses.
The filter operates onthe ordered list of summary extracts for a particularquestion and produces a list of answer hypotheses,one for each noun phrase (NP) in the extracts in theleft-to-right order in which they appeared.The  NP-based  QA System.
Our implementa-tion of the NP-based QA system uses the Empirenoun phrase finder, which is described in detail inCardie and Pierce (1998).
Empire identifies baseNPs - -  non-recursive noun phrases - -  using a verysimple algorithm that matches part-of-speech tag se-quences based on a learned noun phrase grammar.The approach is able to achieve 94% precision andrecall for base NPs derived from the Penn TreebankWall Street Journal (Marcus et al, 1993).
In theexperiments below, the NP filter follows the applica-tion of the document retrieval and text summariza-tion components.
Pronoun answer hypotheses arediscarded, and the NPs are assembled into 50-bytechunks.Eva luat ion.
Results for the NP-based QA sys-tem are shown in the third row of Table 1.
Thenoun phrase filter markedly improves system per-formance for the development corpus, nearly dou-3Paragraph-based summaries provide better coverage onthe test corpus than sentence-based summaries: for 151 ques-tions, the correct answer appears in the summary for one ofthe top k documents.
This suggests that paragraph sum-maries might be better suited for use with more sophisticatedlinguistic filters that are capable of discerning the answer inthe larger summary.1~"~ 183Development Corpus Test CorpusSmart Vector Space ModelQuery-Dependent Text SummarizationText Summarization + NPsText Summarization + NPs + Semantic TypeText Summarization with Syntactic Ordering +NPs + Semantic TypeCorrect (%) MAR3/38 0.079 3.334/38 0.105 2.257/38 0.184 2.2921/38 0.553 1.3822/38 0.579 1.32Correct(%) MAR29/200 0.145 3.0745/200 0.225 2.6750/200 0.250 2.6686/200 0.430 1.9091/200 0.455 1.82Table 1: Evaluation of the Role of Statistical and Limited Linguistic Knowledge for the TREC8 QuestionAnswering Task.
Results for 38 development and 200 test questions are shown.
The mean answer ank(MAR) is computed w.r.t, all questions correctly answered.bling the number of questions answered correctly.We found these results somewhat surprising sincethis linguistic filter is rather weak: we expected itto work well only in combination with the semanticfilter described below.
The noun phrase filter hasmuch less of an effect on the test corpus, improvingperformance on questions answered from 45 to 50.In a separate experiment, we applied the NP filterto the baseline system that includes no text summa?rization component.
Here the NP filter does notimprove performance - - the system gets only twoquestions correct.
This indicates that the NP filterdepends critically on the text summarization com-ponent.
As a result, we will continue to use query-dependent text summarization i  the experimentsbelow.The NP filter provides the first opportunity tolook at single-phrase answers.
The preceding QAsystems produced answers that were rather unnat-urally chunked into 50-byte strings.
When suchchunking is disabled, only one development and 20test questions are answered.
The difference in per-formance between the NP filter with chunking andthe NP filter alone clearly indicates that the NP fil-ter is extracting ood guesses, but that subsequentlinguistic processing is needed to promote the bestguesses to the top of the ranked guess list.6 Incorporat ing  Semant ic  TypeIn fo rmat ionThe NP filter does not explicitly consider the ques-tion in its search for noun phrase answers.
It is clear,however, that a QA system must pay greater atten-tion to the syntactic and semantic onstraints spec-ified in the question.
For example, a question likeWho was president of the US in 19957 indicatesthat the answer is likely to be a person.
In addition,there should be supporting evidence from the answerdocument that the person was president, and, morespecifically, held this office in the US and in 1995.We introduce here a second linguistic filter thatconsiders the primary semantic onstraint from thequestion.
The filter begins by determining the ques-tion type, i.e.
the semantic type requested in thequestion.
It then takes the ordered set of summaryextracts supplied by the IR subsytem, uses the syn-tactic filter from Section 5 to extract NPs, and gen-erates an answer hypothesis for every noun phrasethat is semantically compatible with the questiontype.
Our implementation of this semantic class fil-ter is described below.
The filter currently makes noattempt to confirm other linguistic relations men-tioned in the question.The  Semantic Type  Checking QA System.For most questions, the question word itself deter-mines the semantic type of the answer.
This is truefor who, where, and when questions, for example,which request a person, place, and time expressionas an answer.
For many which and what questions,however, determining the question type requires ad-ditional syntactic analysis.
For these, we currentlyextract the head noun in the question as the questiontype.
For example, in Which country has the largestpart o$ the Amazon rain :forest?
we identify countryas the question type.
Our heuristics for determiningquestion type were based on the development cor-pus and were designed to be general, but have notyet been directly evaluated on a separate questioncorpus.?
Given the question type and an answer hypoth-esis, the Semantic Type Checking QA System thenuses WordNet o check that an appropriate ancestor-descendent relationship holds.
Given Brazil as ananswer hypothesis for the above question, for exam-ple, Wordnet's type hierarchy confirms that Brazilis a subtype of country, allowing the system to con-clude that the semantic type of the answer hypoth-esis matches the question type.For words (mostly proper nouns) that do not ap-pear in WordNet, heuristics are used to determinesemantic type.
There are heuristics to recognize13 basic question types: Person, Location, Date,Month, Year, Time, Age, Weight, Area, Volume,Length, Amount, and Number.
For Person ques-tions, for example, the system relies primarily on arule that checks for capitalization and abbreviations' I IOA  184in order to identify phrases that correspond to peo-ple.
There are approximately 20 such rules that to-gether cover all 13 question types listed above.
Therules effectively operate as a very simple named en-tity identifier.Eva luat ion .
Results for the Semantic TypeChecking variation of the QA system are shown inthe fourth row of Table 1.
Here we see a dramaticincrease in performance: the system answers threetimes as many development questions (21) correctlyover the previous variation.
This is especially en-couraging iven that the IR and text summarizationcomponents limit the maximum number correct o23.
In addition, the mean answer rank improvesfrom 2.29 to 1.38.
A closer look at Table 1, however,indicates problems with the semantic type checkinglinguistic filter.
While performance on the develop-ment corpus increases by 37 percentage points (from18.4% correct to 55.3% correct), relative gains forthe test corpus are much smaller.
There is only animprovement of 18 percentage points, from 25.0%correct (50/200) to 43.0% correct (86/200).
Thisis a clear indication that the heuristics used in thesemantic type checking component, which were de-signed based on the development corpus, do not gen-eralize well to different question sets.
Replacing thecurrent heuristics with a Named Entity identifica-tion component or learning the heuristics using stan-dard inductive learning techniques should help w i ththe scalability of this linguistic filter.Nevertheless, it is somewhat surprising that veryweak syntactic information (the NP filter) and weaksemantic lass information (question type checking)can produce such improvements.
In particular, itappears that it is reasonable to rely implicitly onthe IR subsystems to enforce the other linguistic re-lationships pecified in the query (e.g.
that Clintonis president, hat this office was held in the US andin 1995).Finally, when 50-byte chunking is disabled forthe semantic type checking QA variation, there isa decrease in the number of questions correctly an-swered, to 19 and 57 for the development and testcorpus, respectively.7 Syntact i c  P re ferences  fo r  Order ingSummary  Ext rac tsSyntactic and semantic linguistic knowledge hasbeen used thus far as post-processing filters that lo-cate and confirm answer hypotheses from the statis-tically specified summary extracts.
We hypothesizedthat further improvements might be made by allow-ing this linguistic knowledge to influence the initialordering of text chunks for the linguistic filters.
In afinal system, we begin to investigate this claim.
Ourgeneral approach is to define a new scoring mea-sure that operates on the summary extracts and canbe used to reorder the extracts based on linguisticknowledge.The  QA System wi th  L inguist ic  Reorder ingo f  Summary  Ext racts .
As described above, ourfinal version of the QA system ranks summary ex-tracts according to both their vector space similarityto the question as well as linguistic evidence that theanswer lies within the extract.
In particular, eachsummary extract E for question q is ranked accord-ing to a new score, Sq:sq(E) = w(E) .
LRq(E)The intuition behind the new score is to prefer sum-mary extracts that exhibit the same linguistic rela-tionships as the question (as indicated by LRq) andto give more weight (as indicated by w) to linguisticrelationship matches in extracts from higher-rankeddocuments.
More specifically, LRq(E ) is the num-ber of linguistic relationships from the question thatappear in E. In the experiments below, LRq(E)is just the number of base NPs from the questionthat appear in the summary extract.
In futurework, we plan to include other pairwise linguisticrelationships (e.g.
subject-verb relationships, verb-object relationships, pp-attachment relationships).The weight w(E) is a number between 0 and 1 thatis based on the retrieval rank r of the document thatcontains E:w(E) = max(m, 1 - p. r)In our experiments, m = 0.5 and p = 0.1.
Bothvalues were selected manually based on the develop-ment corpus; an extensive search for the best suchvalues was not done.The summary extracts are sorted according to thenew scoring measure and the ranked list of sentencesis provided to the linguistic filters as before.Eva luat ion .
Results for this final variation of theQA system are shown in the bottom row of Table 1.Here we see a fairly minor increase in performanceover the use of linguistic filters alone: the systemanswers only one more question correctly than theprevious variation for the development corpus andanswers five additional questions for the test cor-pus.
The mean answer rank improves only negligi-bly.
Sixteen of the 22 correct answers (73%) appearas the top-ranked guess for the development corpus;only 53 out of 91 correct answers (58%) appear asthe top-ranked guess for the test corpus.
Unfortu-nately, when 50-byte chunking is disabled, systemperformance drops precipitously, by 5% (to 20 outof 38) for the development corpus and by 13% (to65 out of 200) for the test corpus.
As noted above,this indicates that the filters are finding the answers,but more sophisticated linguistic sorting is neededto promote the best answers to the top.
Through185its LRq term, the new scoring measure does pro-vide a mechanism for allowing other linguistic re-lationships to influence the initial ordering of sum-mary extracts.
The current results, however, indi-cate that with only very weak syntactic information(i.e.
base noun phrases), the new scoring measureis only marginally successful in reordering the sum-mary extracts based on syntactic information.As noted above, the final system (with the liberal50-byte answer chunker) correctly answers 22 out of38 questions for the development corpus.
Of the 16errors, the text retrieval component is responsible forfive (31.2%), the text summarization component forten (62.5%), and the linguistic filters for one (6.3%).In this analysis we consider the linguistic filters re-sponsible for an error if they were unable to pro-mote an available answer hypothesis to one of thetop five guesses.
A slightly different situation arisesfor the test corpus: of the 109 errors, the text re-trieval component is responsible for 39 (35.8%), thetext summarization component for 26 (23.9%), andthe linguistic filters for 44 (40.4%).
As discussed inSection 6, the heuristics that comprise the semantictype checking filter do not scale to the test corpusand are the primary reason for the larger percentageof errors attributed to the linguistic filters for thatcorpus.8 Re la ted  Work  and  Conc lus ionsWe have described and evaluated a series ofquestion-answering systems, each of which incorpo-rates a different combination of statistical and lin-guistic knowledge sources.
We find that even veryweak linguistic knowledge can offer substantial im-provements over purely IR-based techniques espe-cially when smoothly integrated with the text pas-sage preferences computed by the IR subsystems.Although our primary goal was to investigate theuse of statistical and linguistic knowledge sources, itis possible to compare our approach and our resultsto those for systems in the recent TREC8 QA evalu-ation.
Scores on the TREC8 test corpus for systemsparticipating in the QA evaluation ranged between3 and 146 correct.
Discarding the top three scoresand the worst three scores, the remaining eight sys-tems achieved between 52 and 91 correct.
Using theliberal answer chunker, our final QA system equalsthe best of these systems (91 correct); without it,our score of 65 correct places our QA system nearthe middle of this group of eight.Like the work described here, virtually all of thetop-ranked TREC8 systems use a combination ofIR and shallow NLP for their QA systems.
IBM'sAnSel system (Prager et al, 2000), for example,employs finite-state patterns as its primary shallowNLP component.
These are used to recognize afairly broad set of about 20 named entities.
TheIR component indexes only text passages associ-ated with these entities.
The AT&T QA system(Singhal et al, 2000), the Qanda system (Breck etal., 2000), and the SyncMatcher system (Oard etal., 2000) all employ vector-space methods from IR,named entity identifiers, and a fairly simple ques-tion type determiner.
In addition, SyncMatcheruses a broad-coverage d pendency parser to enforcephrase relationship constraints.
Instead of the vec-tor space model, the LASSO system (Moldovan etal., 2000) uses boolean search operators for para-graph retrieval.
Recognition of answer hypothesesin their system relies on identifying named entities.Finally, the Cymphony QA system (Srihari and Li,2000) relies heavily on named entity identification; italso employs tandard IR techniques and a shallowparser.In terms of statistical and linguistic knowledgesources employed, the primary difference betweenthese systems and ours is our lack of an adequatenamed entity tagger.
Incorporation of such a tag-ger will be a focus of future work.
In addition, webelieve that the retrieval and summarization compo-nents can be improved by incorporating automaticrelevance feedback (Buckley, 1995) and coreferenceresolution.
Morton (1999), for example, shows thatcoreference r solution improves passage retrieval fortheir question-answering system.
We also plan toreconsider paragraph-based summaries given theircoverage on the test corpus.
The most critical areafor improvement, however, is the linguistic filters.The semantic type filter will be greatly improved bythe addition of a named entity tagger, but we believethat additional gains can be attained by augmentingnamed entity identification with information fromWordNet.
Finally, we currently make no attempt toconfirm any phrase relations from the query.
With-out this, system performance will remain severelylimited.9 AcknowledgmentsThis work was supported in part by NSF Grants IRI-9624639 and GER-9454149.Re ferencesE.
Breck, J. Burger, L. Ferro, D. House, M. Light,and I. Mani.
2000.
A Sys Called Qanda.
InE.
Voorhees, editor, Proceedings of the EighthText REtrieval Conference TREC 8.
NIST Spe-cial Publication.
In press.C.
Buckley, M. Mitra, J. Walz, and C. Cardie.1998a.
SMART high precision: TREC 7.
InE.
Voorhees, editor, Proceedings of the SeventhText REtrieval Conference TREC 7, pages 285-298.
NIST Special Publication 500-242.C.
Buckley, M. Mitra, J. Walz, and C. Cardie.1998b.
Using clustering and superconcepts within186SMART : TREC 6.
In E. Voorhees, editor, Pro-ceedings of the Sixth Text REtrieval ConferenceTREC 6, pages 107-124.
NIST Special Publica-tion 500-240.C.
Buckley, C. Cardie, S. Mardis, M. Mitra,D.
Pierce, K. Wagstaff, and J. Walz.
1999.
TheSmart/Empire TIPSTER IR System.
In Proceed-ings, TIPSTER Text Program (Phase III).
Mor-gan Kauhnann.
To appear.Chris Buckley.
1995.
Massive Query Expansion/or Relevance Feedback.
Cornell University, Ph.D.Thesis, Ithaca, New York.R.
Burke, K. Hammond, and J. Kozlovsky.1995.
Knowledge-Based Information Retrievalfrom Semi-Structured Text.
In Working Notes ofthe AAAI Fall Symposium on AI Applications inKnowledge Navigation and Retrieval, pages 19-24.AAAI Press.R.
Burke, K. Hammond, V. Kulyukin, S. Lyti-hen, N. Tomuro, and S. Schoenberg.
1997. ques-tion answering from Frequently-Asked QuestionFiles.
Technical Report TR-97-05, University ofChicago.C.
Cardie and D. Pierce.
1998.
Error-Driven Prun-ing of Treebank Grammars for Base Noun PhraseIdentification.
In Proceedings of the 36th An-nual Meeting of the Association .for Computa-tional Linguistics and COLING-98, pages 218-224, University of Montreal, Montreal, Canada.Association for Computational Linguistics.Y.
Chali, S. Matwin, and S. Szpakowicz.
1999.Query-Biased Text Summarization as a Question-Answering Technique.
In Proceedings o.f the AAAIFall Symposium on Question Answering Systems,pages 52-56.
AAAI Press.
AAAI TR FS-99-02.C.
Fellbaum.
1998.
WordNet: An Electronical Lex-iced Database.
MIT Press, Cambridge, MA.J.
Kupiec.
1993.
MURAX: A Robust Linguistic ap-proach For Question Answering Using An On-Line Encyclopedia.
In Proceedings of A CM SI-GIR, pages 181-190.W.
Lehnert.
1978.
The Process o/ Question Answer-ing.
Lawrence Erlbaum Associates, Hillsdale, NJ.I.
Mani, T. Firmin, D. House, G. Klein, B. Sund-heim, and L. Hirschman.
1999.
The TIPSTERSUMMAC Text Summarization Evaluation.
InNinth Annual Meeting o.f the EACL, Universityof Bergen, Bergen, Norway.M.
Marcus, M. Marcinkiewicz, and B. Santorini.1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Lin-guistics, 19(2):313-330.G.
A. Miller, R. Beckwith, C. FeUbaum, D. Gross,and K. J. Miller.
1990.
WordNet: an on-line lex-ical database.
International Journal of Lexicogra-phy, 3(4):235-245.D.
Moldovan, S. Harabagiu, M. Pa~ca, R. Mihal-cea, R. Goodrum, R. Girju, and V. Rus.
2000.LASSO: A Tool for Surfing the Answer Net.
InE.
Voorhees, editor, Proceedings of the EighthText REtrieval Conference TREC 8.
NIST Spe-cial Publication.
In press.T.
S. Morton.
1999.
Using Coreference to Im-prove Passage Retrieval for Question Answering.In Proceedings of the AAAI Fall Symposium onQuestion Answering Systems, pages 72-74.
AAAIPress.
AAAI TR FS-99-02.D.
W. Oard, J. Wang, D. Lin, and I. Soboroff.
2000.TREC-8 Experiments at Maryland: CLIR, QAand Routing.
In E. Voorhees, editor, Proceedingso.f the Eighth Text REtrieval Conference TREC 8.NIST Special Publication.
In press.J.
Prager, D. Radev, E. Brown, A. Coden, andV.
Samn.
2000.
The Use of Predictive Anno-tation for Question Answering in TRECS.
InE.
Voorhees, editor, Proceedings o/ the EighthText REtrieval Conference TREC 8.
NIST Spe-cial Publication.
In press.G.
Salton, A. Wong, and C.S.
Yang.
1975.
A vectorspace model for information retrieval.
Communi-cations o/the ACM, 18(11):613-620.G.
Salton, J. Allan, C. Buckley, and M. Mitra.
1994.Automatic analysis, theme generation and sum-marization of machine-readable t xts.
Science,264:1421-1426, June.Gerard Salton, editor.
1971.
The SMART Re-trieval System--Experiments in Automatic Doc-ument Processing.
Prentice Hall Inc., EnglewoodCliffs, NJ.R.
C. Schank and R. P. Abelson.
1977.
Scripts,plans, goals, and understanding.
Lawrence Erl-bantu Associates, Hillsdale, NJ.Amit Singhal, Chris Buckley, and Mandar Mitra.1996.
Pivoted document length normalization.
InH.
Frei, D. Harman, P. Schauble, and R. Wilkin-son, editors, Proceedings o/the Nineteenth An-nual International ACM SIGIR Conference onResearch and Development in Information Re-trieval, pages 21-29.
Association for ComputingMachinery.A.
Singhal, S. Abney, M. Bacchiani, M. Collins,D.
Hindle, and F. Pereira.
2000.
AT&T at TREC-8.
In E. Voorhees, editor, Proceedings of theEighth Text REtrieval Conference TREC 8.
NISTSpecial Publication.
In press.R.
Srihari and W. Li.
2000.
Question Answer-ing Supported by Information Extraction.
InE.
Voorhees, editor, Proceedings of the EighthText REtrieval Conference TREC 8.
NIST Spe-cial Publication.
In press.TREC-8.
2000.
Proceedings of the Eighth Text RE-trieval Conference TREC 8.
NIST.
In press.1Q'7  187
