Proceedings of the Eighteenth Conference on Computational Language Learning, pages 151?159,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsFactored Markov Translation with Robust ModelingYang Feng?Trevor Cohn?Xinkai Du?Information Sciences Institue?Computing and Information SystemsComputer Science Department The University of MelbourneUniversity of Southern California VIC 3010 Australia{yangfeng145, xinkaid}@gmail.com t.cohn@unimelb.edu.auAbstractPhrase-based translation models usuallymemorize local translation literally andmake independent assumption betweenphrases which makes it neither generalizewell on unseen data nor model sentence-level effects between phrases.
In this pa-per we present a new method to modelcorrelations between phrases as a Markovmodel and meanwhile employ a robustsmoothing strategy to provide better gen-eralization.
This method defines a re-cursive estimation process and backs offin parallel paths to infer richer structures.Our evaluation shows an 1.1?3.2% BLEUimprovement over competitive baselinesfor Chinese-English and Arabic-Englishtranslation.1 IntroductionPhrase-based methods to machine translation(Koehn et al., 2003; Koehn et al., 2007) have dras-tically improved beyond word-based approaches,primarily by using phrase-pairs as translationunits, which can memorize local lexical con-text and reordering patterns.
However, this lit-eral memorization mechanism makes it general-ize poorly to unseen data.
Moreover, phrase-basedmodels make an independent assumption, statingthat the application of phrases in a derivation is in-dependent to each other which conflicts with theunderlying truth that the translation decisions ofphrases should be dependent on context.There are some work aiming to solve the twoproblems.
Feng and Cohn (2013) propose aword-based Markov model to integrate translationand reordering into one model and use the so-phisticated hierarchical Pitman-Yor process whichbacks off from larger to smaller context to pro-vide dynamic adaptive smoothing.
This modelshows good generalization to unseen data whileit uses words as the translation unit which can-not handle multiple-to-multiple links in real wordalignments.
Durrani et al.
(2011) and Durrani etal.
(2013) propose an operation sequence model(OSM) which models correlations between mini-mal translation units (MTUs) and evaluates proba-bilities with modified Kneser-Ney smoothing.
Onone hand the use of MTUs can help retain themultiple-to-multiple alignments, on the other handits definition of operations where source wordsand target words are bundled into one operationmakes it subjected to sparsity.
The common fea-ture of the above two methods is they both back offin one fixed path by dropping least recent eventsfirst which precludes some useful structures.
Forthe segment pairs <b?a t?a k?aol`v j`?nq`u, take it intoaccount> in Figure 1, the more common structureis <b?a ... k?aol`v j`?nq`u, take ... into account>.
Ifwe always drop the least recent events first, thenwe can only learn the pattern <... t?a k?aol`v j`?nq`u,... it into account>.On these grounds, we propose a method withnew definition of correlations and more robustprobability modeling.
This method defines aMarkov model over correlations between minimalphrases where each is decomposed into three fac-tors (source, target and jump).
In the meantimeit employs a fancier smoothing strategy for theMarkov model which backs off by dropping mul-tiple conditioning factors in parallel in order tolearn richer structures.
Both the uses of factorsand parallel backoff give rise to robust modelingagainst sparsity.
In addition, modeling bilingualinformation and reorderings into one model in-stead of adding them to the linear model as sep-arate features allows for using more sophisticatedestimation methods rather than get a loose weightfor each feature from tuning algorithms.We compare the performance of our model withthat of the phrase-based model and the hierarchi-cal phrase-based model on the Chinese-Englishand Arabic-English NIST test sets, and get an im-151Figure 1: Example Chinese-English sentence pairwith word alignments shown as filled grid squares.provement up to 3.2 BLEU points absolute.12 ModellingOur model is phrase-based and works like aphrase-based decoder by generating target trans-lation left to right using phrase-pairs while jump-ing around the source sentence.
For each deriva-tion, we can easily get its minimal phrase (MPs)sequence where MPs are ordered according to theorder of their target side.
Then this sequence ofevents is modeled as a Markov model and the logprobability under this Markov model is includedas an additional feature into the linear SMT model(Och, 2003).A MP denotes a phrase which cannot containother phrases.
For example, in the sentence pairin Figure 1, <b?a t?a , take it> is a phrase but nota minimal phrase, as it contains smaller phrasesof <b?a , take> and <t?a , it>.
MPs are a com-plex event representation for sequence modelling,and using these naively would be a poor choicebecause few bigrams and trigrams will be seenoften enough for reliable estimation.
In orderto reason more effectively from sparse data, weconsider more generalized representations by de-composing MPs into their component events: thesource phrase (source?f ), the target phrase (tar-get e?)
and the jump distance from the precedingMP (jump j), where the jump distance is countedin MPs, not in words.
For sparsity reasons, wedo not use the jump distance directly but insteadgroup it into 12 buckets:{insert,?
?5,?4,?3,?2,?1, 0, 1, 2, 3, 4,?
5},where the jump factor is denoted as insert whenthe source side is NULL.
For the sentence pair in1We will contribute the code to Moses.Figure 1, the MP sequence is shown in Figure 2.To evaluate the Markov model, we conditioneach MP on the previous k ?
1 MPs and modeleach of the three factors separately based on achain rule decomposition.
Given a source sentencef and a target translation e, the joint probability isdefined asp(?eI1, jI1,?fI1) =I?i=1p(e?i|?fii?k+1, jii?k+1, e?i?1i?k+1)?I?i=1p(?fi|?fi?1i?k+1, jii?k+1, e?i?1i?k+1)?I?i=1p(ji|?fi?1i?k+1, ji?1i?k+1, e?i?1i?k+1)(1)where?fi, e?iand jiare the factors of MPi,?fI1=(?f1,?f2, .
.
.
,?fI) is the sequence of source MPs,?eI1= (e?1, e?2, .
.
.
, e?I) is the sequence of tar-get MPs, and jI1= (j1, j2, .
.
.
, jI) is the vec-tor of jump distance between MPi?1and MPi, orinsert for MPs with null source sides.2To eval-uate each of the k-gram models, we use modifiedKeneser-Ney smoothing to back off from largercontext to smaller context recursively.In summary, adding the Markov model into thedecoder involves two passes: 1) training a modelover the MP sequences extracted from a wordaligned parallel corpus; and 2) calculating theprobability of the Markov model for each trans-lation hypothesis during decoding.
This Markovmodel is combined with a standard phrase-basedmodel3(Koehn et al., 2007) and used as an addi-tional feature in the linear model.In what follows, we will describe how to estati-mate the k-gram Markov model, focusing on back-off (?2.1) and smoothing (?2.2).2.1 Parallel BackoffBackoff is a technique used in language model ?when estimating a higher-order gram, instead ofusing the raw occurrence count, only a portion isused and the remainder is computed using a lower-order model in which one of the context factors2Note that factors at indices 0,?1, .
.
.
,?
(k ?
1) are setto a sentinel value to denote the start of sentence.3The phrase-based model considers larger phrase-pairsthan just MPs, while our Markov model consider only MPs.As each phrase-pair is composed of a sequence of MPs un-der fixed word alignment, by keeping the word alignment foreach phrase, a decoder derivation unambiguously specifiesthe MP sequence for scoring under our Markov model.152index sentence pair minimal phrase sequencew?om?en y?ingg?ai b?a t?a y?e k?aol`v j`?nq`u jump source target1 We T11 w?om?en We2 should T21 y?ingg?ai should3 also T33 y?e also4 take T4-2 b?a take5 it T51 t?a it6 into account T62 k?aol`v j`?nq`u into accountFigure 2: The minimal phrase sequence T1, ..., T6extracted from the sentence pair in Figure 1.step 3-gram e?3|?f3, j3, e?2,?f2, j2, e?1,?f1, j10 into account | k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2?
11 into account | k?aol`v j`?nq`u, 2, it, t?a, ?, take, b?a, -2?
t?a2 into account | k?aol`v j`?nq`u, 2, it, ?, ?, take, b?a, -2?
it3 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, b?a, -2?
-24 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, b?a, ??
b?a5 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, ?, ??
take6 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, ?, ?, ??
27 into account | k?aol`v j`?nq`u, ?, ?, ?, ?, ?, ?, ??
k?aol`v j`?nq`u8 into account | ?, ?, ?, ?, ?, ?, ?, ?Figure 3: One backoff path for the 3-gram inEquation 2.
The symbols besides each arrow meanthe current factor to drop; ???
is a placeholder forfactors which can take any value.is dropped.
Here the probabilities of the lower-order which is used to construct the higher-order iscalled the backoff probability of the higher-ordergram.
Different from standard language modelswhich drop the least recent words first, we em-ploy a different backoff strategy which considersall possible backoff paths.
Taking as an examplethe 3-gram T4T5T6in Figure 2, when estimatingthe probability of the target factorp(into account | k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2 ) ,(2)Figure 4: The backoff graph for the 3-gram modelof the target factor.
The symbol beside each arrowis the factor to drop.we consider two backoff paths: path1drops thefactors in the order -2, b?a, take, 1, t?a, it, 2,k?aol`v j`?nq`u; path2uses order 1, t?a, it, -2, b?a,take, 2, k?aol`v j`?nq`u.
Figure 3 shows the backoffprocess for path2.
In this example with two back-off paths, the backoff probability g is estimated asg(into acc.|c) =12p(into acc.|c?
)+12p(into acc.|c??)
,where c =< k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2 >,c?=< k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, ?
> andc?
?=< k?aol`v j`?nq`u, 2, it, t?a, ?, take, b?a, -2 >.Formally, we use the notion of backoff graph todefine the recursive backoff process of a k-gram153and denote as nodes the k-gram and the lower-order grams generated by the backoff.
Once onenode occurs in the training data fewer than ?
times,then estimates are calculated by backing off to thenodes in the next lower level where one factor isdropped (denoted using the placeholder ?
in Fig-ure 4).
One node can have one or several candidatebackoff nodes.
In the latter case, the backoff prob-ability is defined as the average of the probabilitiesof the backoff nodes in the next lower level.We define the backoff process for the 3-grammodel predicting the target factor, e?3, as illustratedin Figure 4.
The top level is the full 3-gram, fromwhich we derive two backoff paths by droppingfactors from contextual events, one at a time.
For-mally, the backoff strategy is to drop the previ-ous two MPs one by one while for each MP thedropping routine is first the jump factor, then thesource factor and final the target factor.
Each stepon the path corresponds to dropping an individ-ual contextual factor from the context.
The pathsconverge when only the third MP left, then thebackoff proceeds by dropping the jump action, j3,then finally the source phrase,?f3.
The paths B-D-F-H-J and C-E-G-I-K show all the possible or-derings (corresponding to c?
?and c?, respectively)for dropping the two previous MPs.
The exam-ple backoff in Figure 3 corresponds the path A-B-D-F-H-J-L-M-N in Figure 4, shown as heavierlines.
When generizing to the k-gram for targetp(e?k|?fk1, jk1, e?k?11), the backoff strategy is to firstdrop the previous k-1 MPs one by one (for eachMP, still drops in the order of jump, source andtarget), then the kth jump factor and finally the kthsource factor.
According to the strategy, the topnode has k-1 nodes to back off to and for the nodee?k|?fk2, jk2, e?k?12where only the factors of MP1aredropped, there are k-2 nodes to back off to.2.2 Probability EstimationWe adopt the technique used in factor languagemodels (Bilmes and Kirchhoff, 2003; Kirchhoff etal., 2007) to estimate the probability of a k-gramp(e?i|c) where c =?fii?k+1, jii?k+1, e??1i?k+1.
Ac-cording to the definition of backoff, only when thecount of the k-gram exceeds some given threshold,its maximum-likelihood estimate, pML(e?k|c) =N(e?k,c)N(c) is used, where N(?)
is the count of anevent and/or context.
Otherwise, only a portion ofpML(e?k|c) is used and the remainder is constructedfrom a lower-level (by dropping a factor).
In or-der to ensure valid probability estimates, i.e.
sumsto unity, probability mass needs to be ?stolen?from the higher level and given to the lower level.Hence, the whole definition isp(e?i|c) ={dN(e?i,c)pml(e?i|c) if N(e?i, c) > ?k?
(c)g(e?i, c) otherwise(3)where dN(e?i,c) is a discount parameter which re-serves probability from the maximum-likelihoodestimate for backoff smoothing at the next lower-level, and we estimate dN(e?i,c) using modifiedKneser-Ney smoothing (Kneser and Ney, 1995;Chen and Goodman, 1996); ?kis the threshold forthe count of the k-gram, ?
(c) is the backoff weightused to make sure the entire distribution still sumsto unity,?
(c) =1??e?:N(e?,c)>?kdN(e?,c)pML(e?|c)?e?:N(e?,c)?
?kg(e?, c),and g(e?i, c) is the backoff probability which weestimate by averaging over the nodes in the nextlower level,g(e?i, c) =1??c?p(e?i|c?)
,where ?
is the number of nodes to back off, c?isthe lower-level context after dropping one factorfrom c.The k-gram for the source and jump factors areestimated in the same way, using the same backoffsemantics.4Note (3) is applied independently toeach of the three models, so the use of backoff maydiffer in each case.3 DiscussionAs a part of the backoff process our methodcan introduce gaps in estimating rule probabili-ties; these backoff patterns often bear close re-semblance to SCFG productions in the hierarchi-cal phrase-based model (Chiang, 2007).
For ex-ample, in step 0 in Figure 3, as all the jump factorsare present, this encodes the full ordering of theMPs and gives rise to the aligned MP pairs shownin Figure 5 (a).
Note that an X1placeholder isincluded to ensure the jump distance from the pre-vious MP to the MP <b?a, take> is -2.
The ap-proximate SCFG production for the MP pairs is<b?a t?a X1k?aol`v j`?nq`u, X1take it into account>.4Although there are fewer final steps, L-M-N in Fig.
4,as we assume the MP is generated in the order jump, sourcephrase then target phrase in a chain rule decomposition.154Figure 5: Approximate SCFG patterns for step 0,3 of Figure 3.
X is a non-terminal which can onlybe rewritten by one MP.
?
and ?
?
?
denote gapsintroduced by the left-to-right decoding algorithmand ?
can only cover one MP while ?
?
?
cancover zero or more MPs.In step 1, as the jump factor 1 is dropped, we donot know the orientation between b?a and t?a.
How-ever several jump distances are known: from X1to b?a is distance -2 and t?a to k?aol`v j`?nq`u is 2.
Inthis case, the source side can beb?a t?a X1k?aol`v j`?nq`u,b?a ?
X1?
?
?
t?a ?
k?aol`v j`?nq`u,t?a b?a k?aol`v j`?nq`u X1,t?a ?
k?aol`v j`?nq`u ?
?
?
b?a ?
X1,where X and ?
can only hold one MP while ?
?
?can cover zero or more MPs.
In step 3 after drop-ping t?a and it, we introduce a gap X2as shown inFigure 5 (b).From above, we can see that our model has twokinds of gaps: 1) in the source due to the left-to-right target ordering (such as the ?
in step 3); and2) in the target, arising from backoff (such as theX2in step 3).
Accordingly our model supportsrules than cannot be represented by a 2-SCFG(e.g., step 3 in Figure 5 requires a 4-SCFG).
Incontrast, the hierarchical phrase-based model al-lows only 2-SCFG as each production can rewriteas a maximum of two nonterminals.
On the otherhand, our approach does not enforce a valid hier-archically nested derivation which is the case forChiang?s approach.4 Related WorkThe method introduced in this paper uses fac-tors defined in the same manner as in Feng andCohn (2013), but the two methods are quite differ-ent.
That method (Feng and Cohn, 2013) is word-based and under the frame of Bayesian modelwhile this method is MP-based and uses a sim-pler Kneser-Ney smoothing method.
Durrani etal.
(2013) also present a Markov model based onMPs (they call minimal translation units) and fur-ther define operation sequence over MPs whichare taken as the events in the Markov model.
Forthe probability estimation, they use Kneser-Neysmoothing with a single backoff path.
Differentfrom operation sequence, our method gives a neatdefinition of factors which uses jump distance di-rectly and avoids the bundle of source words andtarget words like in their method, and hence miti-gates sparsity.
Moreover, the use of parallel back-off infers richer structures and provides robustmodeling.There are several other work focusing on mod-eling bilingual information into a Markov model.Crego et al.
(2011) develop a bilingual languagemodel which incorporates words in the source andtarget languages to predict the next unit, and useit as a feature in a translation system.
This lineof work was extended by Le et al.
(2012) who de-velop a novel estimation algorithm based arounddiscriminative projection into continuous spaces.Neither work includes the jump distance, and nor155do they consider dynamic strategies for estimatingk-gram probabilities.Galley and Manning (2010) propose a methodto introduce discontinuous phrases into the phrase-based model.
It makes use of the decoding mecha-nism of the phrase-based model which jumps overthe source words and hence can hold discontin-uous phrases naturally.
However, their methoddoesn?t touch the correlations between phrases andprobability modeling which are the key points wefocus on.5 ExperimentsWe design experiments to first compare ourmethod with the phrase-based model (PB), the op-eration sequence model (OSM) and the hierarchi-cal phrase-based model (HPB), then we presentseveral experiments to test:1. how each of the factors in our model and par-allel backoff affect overall performance;2. how the language model order affects the rel-ative gains, in order to test if we are just learn-ing a high order LM, or something more use-ful;3. how the Markov model interplay with thedistortion and lexical reordering models ofMoses, and are they complemenatary;4. whether using MPs as translation units is bet-ter in our approach than the simpler tactic ofusing only word pairs.5.1 Data SetupWe consider two language pairs: Chinese-Englishand Arabic-English.
The Chinese-English paral-lel training data is made up of the non-UN por-tions and non-HK Hansards portions of the NISTtraining corpora, distributed by the LDC, having1,658k sentence pairs with 40m and 44m Chineseand English words.
We used the NIST 02 test setas the development set and evaluated performanceon the test sets from NIST 03 and 05.For the Arabic-English task, the training datacomprises several LDC corpora,5including 276ksentence pairs and 8.21m and 8.97m words in Ara-bic and English, respectively.
We evaluated on theNIST test sets from 2003 and 2005, and the NIST02 test set was used for parameter tuning.On both cases, we used the factor languagemodel module (Kirchhoff et al., 2007) of theSRILM toolkit (Stolcke, 2002) to train a Markov5LDC2004E72, LDC2004T17, LDC2004T18,LDC2006T02model with the order = 3 over the MP sequences.6The threshold count of backoff for all nodes was?
= 2.We aligned the training data sets by first usingGIZA++ toolkit (Och and Ney, 2003) to produceword alignments on both directions and then com-bining them with the diag-final-and heuristic.
Allexperiments used a 5-gram language model whichwas trained on the Xinhua portion of the GIGA-WORD corpus using the SRILM toolkit.
Transla-tion performance was evaluated using BLEU (Pa-pineni et al., 2002) with case-insensitive n ?
4-grams.
We used minimum error rate training (Och,2003) to tune the feature weights to maximize theBLEU score on the development set.We used Moses for PB and Moses-chart forHPB with the configuration as follows.
For both,max-phrase-length=7, ttable-limit7=20, stack-size=50 and max-pop-limit=500; For Moses,search-algorithm=1 and distortion-limit=6; ForMoses-chart, search-algorithm=3 and max-char-span8=20 for Moses-chart.
We used both the dis-tortion model and the lexical reordering model forMoses (denoted as Moses-l) except in ?5.5 we onlyused the distortion model (denoted as Moses-d).We implemented the OSM according to Durraniet al.
(2013) and used the same configuration withMoses-l. For our method we used the same config-uration as Moses-l but adding an additional featureof the Markov model over MPs.5.2 Performance ComparisonWe first give the results of performance compar-ison.
Here we add another system (denoted asMoses-l+trgLM): Moses-l together with the targetlanguage model trained on the training data set,using the same configuration with Moses-l. Thissystem is used to test whether our model gains im-provement just for using additional information onthe training set.
We use the open tool of Clark etal.
(2011) to control for optimizer stability and teststatistical significance.The results are shown in Tables 1 and 2.
Thetwo language pairs we used are quite different:Chinese has a much bigger word order differ-ence c.f.
English than does Arabic.
The resultsshow that our system can outperform the baseline6We only employed MPs with the length ?
3.
If a MP hadmore than 3 words on either side, we omitted the alignmentlinks to the first target word of this MP and extracted MPsaccording to the new alignment.7The maximum number of lexical rules for each sourcespan.8The maximum span on the source a rule can cover.156System NIST 02 (dev) NIST 03 NIST 05Moses-l 36.0 32.8 32.0Moses-chart 36.9 33.6 32.6Moses-l+trgLM 36.4 33.9 32.9OSM 36.6 34.0 33.1our model 37.9 36.0 35.1Table 1: BLEU % scores on the Chinese-Englishdata set.System NIST 02 (dev) NIST 03 NIST 05Moses-l 60.4 52.0 52.8Moses-chart 60.7 51.8 52.4Moses-l+trgLM 60.8 52.6 53.3OSM 61.1 52.9 53.4our model 62.2 53.6 53.9Table 2: BLEU % scores on the Arabic-Englishdata set.systems significantly (with p < 0.005) on bothlanguage pairs, nevertheless, the improvement onChinese-English is bigger.
The big improvementover Moses-l+trgLM proves that the better perfor-mance of our model does not solely comes fromthe use of the training data.
And the gain overOSM means our definition of factors gives a betterhandling to sparsity.
We also notice that HPB doesnot give a higher BLEU score on Arabic-Englishthan PB.
The main difference between HPB andPB is that HPB employs gapped rules, so this re-sult suggests that gaps are detrimental for Arabic-English translation.
In ?5.3, we experimentallyvalidate this claim with our Markov model.5.3 Impact of Factors and Parallel BackoffWe now seek to test the contribution of target,jump, source factors, as well as the parallel back-off technique in terms of BLEU score.
Weperformed experiments on both Chinese-Englishand Arabic-English to test whether the contri-bution was related to language pairs.
We de-signed the experiments as follows.
We firsttrained a 3-gram Markov model only over tar-get factors, p(?eI1|?fI1) =?Ii=1p(e?i|e?i?1i?2), de-noted +t.
Then we added the jump fac-tor (+t+j), such that we now consideringboth target and jump events, p(?eI1,?jI1|?fI1) =?Ii=1p(e?i|?jii?2, e?i?1i?2)p(?ji|?ji?1i?2, e?i?1i?2).
Next weadded the source factor (+t+j+s) such that now allthree factors are included from Equation 1.
Forthe above three Markov models we used simpleleast-recent backoff (akin to a standard languagemodel), and consequently these methods cannotrepresent gaps in the target.
Finally, we trained an-System Chinese-English Arabic-EnglishNIST 02 NIST 03 NIST 02 NIST 03Moses-l 36.0 32.8 60.4 52.0+t 36.3 33.8 60.9 52.4+t+j 37.1 34.7 62.1 53.4+t+j+s 37.6 34.8 62.5 53.9+t+j+s+p 37.9 36.0 62.2 53.6Table 3: The impact of factors and parallel back-off.
Key: t?target, j?jump, s?source, p?parallelbackoff.System 2gram 3gram 4gram 5gram 6gramMoses-l 27.2 32.4 33.0 32.8 33.2our method 31.6 34.0 35.8 36.0 36.2Table 4: The impact of the order of the standardlanguage models.other Markov model by introducing parallel back-off to the third one as described in ?2.1.
Eachof the four Markov model approaches are imple-mented as adding an additional feature, respec-tively, into the Moses-l baseline.The results are shown in Table 3.
Observe thatadding each factor results in near uniform per-formance improvements on both language pairs.The jump factor gives big improvements of about1% BLEU in both language pairs.
However whenusing parallel backoff, the performance improvesgreatly for Chinese-English but degrades slightlyon Arabic-English.
The reason may be parallelbackoff is used to encode common structures tocapture the different word ordering between Chi-nese and English while for Arabic-English thereare fewer consistent reordering patterns.
This isalso consistent with the results in Table 1 and 2where HPB gets a little bit lower BLEU scores.5.4 Impact of LM orderOur system resembles a language model in com-mon use in SMT systems, in that it uses a Markovmodel over target words, among other factors.This raises the question of whether its improve-ments are due to it functioning as a target languagemodel.
Our experiments use order k = 3 over MPsequences and each MP can have at most 3 words.Therefore the model could in principle memorize9-grams, although usually MPs are much smaller.To test whether our improvements are from usinga higher-order language model or other reasons,we evaluate our system and the baseline systemwith a range of LMs of different order.
If we canget consistent improvements over the baseline for157System NIST 02 (dev) NIST 03Moses-d 35.1 31.3Moses-l 36.0 32.8Moses-d+M 36.4 34.8Moses-l+M 37.9 36.0Table 5: Comparison between our Markov model(denoted as M) and the lexical reordering modelof Moses.both small and large n, this suggests it?s not thelong context that plays the key role but is otherinformation we have learned (e.g., jumps or richstructures).Table 4 shows the results of using standard lan-guage models with orders 2 ?
6 in Moses-l andour method.
We can see that language model or-der is very important.
When we increase the orderfrom 2 to 4, the BLEU scores for both systems in-creases drastically, but levels off for 4-gram andlarger.
Note that our system outperforms Moses-lby 4.4, 1.6, 2.8, 3.2 and 3.0 BLEU points, respec-tively.
The large gain for 2-grams is likely due tothe model behaving like a LM, however the factthat consistent gains are still realized for higherk suggests that the approach brings considerablecomplementary information, i.e., it is doing muchmore than simply language modelling.5.5 Comparison with Lexical ReorderingOur Markov model learns a joint model of jump,source and target factors and this is similar to thelexical reordering model of Moses (Koehn et al.,2007), which learns general orientations of pairsof adjacent phrases (classed as monotone, swap orother).
Our method is more complex, by learningexplicit jump distances, while also using broadercontext.
Here we compare the two methods, andtest whether our approach is complementary by re-alizing gains over the lexicalized reordering base-line.
We test this hypothesis by comparing theresults of Moses with its simple distortion model(Moses-d), then with both simple distortion andlexicalized reordering (Moses-l), and then with ourMarkov model (denoted as Moses-d+M or Moses-l+M, for both baselines respectively).The results are shown in Table 5.
Comparingthe results of Moses-l and Moses-d, we can see thatthe lexical reordering model outperforms the dis-tortion model by a margin of 1.5% BLEU.
Com-paring Moses-d+M with Moses-l, our Markovmodel provides further improvements of 2.0%System NIST 02 (dev) NIST 03Moses-l 36.0 32.8Moses-l+word 36.9 34.0Moses-l+MP 37.6 34.8Table 6: Comparison between the MP-basedMarkov model and the word-based Markov model.BLEU.
Our approach does much more than modelreordering, so it is unlikely that this improvementis solely due to being better a model of distor-tion.
This is underscored by the final result inTable 5, for combining lexicalized distortion withour model (Moses-l+M) which gives the highestBLEU score, yielding another 1.2% increase.5.6 Comparison with Word-based MarkovOur approach uses minimal phrases as its basicunit of translation, in order to preserve the many-to-many links found from the word alignments.However we now seek to assess the impact of thechoice of these basic units, considering instead asimpler word-based setting which retains only 1-to-1 links in a Markov model.
To do this, weprocessed target words left-to-right and for tar-get words with multiple links, we only retainedthe link which had the highest lexical translationprobability.
Then we trained a 3-gram word-basedMarkov model which backs off by dropping thefactors of the least recent word pairs in the order offirst jump then source then target.
This model wasincluded as a feature in the Moses-l baseline (de-noted as Moses-l+word), which we compared to asystem using a MP-based Markov model backingoff in the same way (denoted as Moses-l+MP).According to the results in Table 6, using MPsleads to better performance.
Surprisingly eventhe word based method outperforms the baseline.This points to inadequate phrase-pair features inthe baseline, which can be more robustly esti-mated using a Markov decomposition.
In additionto allowing for advanced smoothing, the Markovmodel can be considered to tile phrases over oneanother (each k-gram overlaps k?1 others) ratherthan enforcing a single segmentation as is done inthe PB and HPB approaches.
Fox (2002) statesthat phrases tend to move as a whole during re-ordering, i.e., breaking MPs into words opens thepossibility of making more reordering errors.
Wecould easily use larger phrase pairs as the basicunit, such as the phrases used during decoding.However, doing this involves a hard segmentation158and would exacerbate issues of data sparsity.6 ConclusionsIn this paper we try to give a solution to the prob-lems in phrase-based models, including weak gen-eralization to unseen data and negligence of cor-relations between phrases.
Our solution is to de-fine a Markov model over minimal phrases so asto model translation conditioned on context andmeanwhile use a fancy smoothing technique tolearn richer structures such that can be applied tounseen data.
Our method further decomposes eachminimal phrase into three factors and operates inthe unit of factors in the backoff process to providea more robust modeling.In our experiments, we prove that our defini-tion of factored Markov model provides comple-mentary information to lexicalized reordering andhigh order language models and the use of paral-lel backoff infers richer structures even those outof the reach of 2-SCFG and hence brings big per-formance improvements.
Overall our approachgives significant improvements over strong base-lines, giving consistent improvements of between1.1 and 3.2 BLEU points on large scale Chinese-English and Arabic-English evaluations.7 AcknowledgesThe first author is supported by DARPA BOLT,contract HR0011-12-C-0014.
The second au-thor is the recipient of an Australian Re-search Council Future Fellowship (project numberFT130101105).
Thank the anonymous reviews fortheir insightful comments.ReferencesJeff Bilmes and Katrin Kirchhoff.
2003.
Factored lan-guage models and generalized parallel backoff.
InProc.
of HLT-NAACL.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proc.
of ACL, pages 310?318.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33:201?228.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In Proc.
of ACL-HLT, pages 176?181.Josep Maria Crego, Franc?ois Yvon, and Jos?e B.Mari?no.
2011.
Ncode: an open source bilingualn-gram smt toolkit.
Prague Bull.
Math.
Linguistics,96:49?58.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proc.
of ACL-HLT, pages1045?1054, June.Nadir Durrani, Alexander Fraser, and Helmut Schmid.2013.
Model with minimal translation units, but de-code with phrases.
In Proc.
of NAACL, pages 1?11.Yang Feng and Trevor Cohn.
2013.
A markovmodel of machine translation using non-parametricbayesian inference.
In Proc.
of ACL, pages 333?342.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proc.
of EMNLP, pages 304?311, July.Michel Galley and Christopher D. Manning.
2010.Accurate non-hierarchical phrase-based translation.In Proc.
of NAACL, pages 966?974.Katrin Kirchhoff, Jeff Bilmes, and Kevin Duh.
2007.Factored language models tutorial.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In InProceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing, vol-ume 1, pages 181?184.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
of HLT-NAACL, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of ACL, Demonstration Ses-sion.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proc.
of NAACL, pages 39?48.Frans J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29:19?51.Frans J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proc.
of ICSLP.159
