Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1012?1017,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsWeakly Supervised Tweet Stance Classification by Relational BootstrappingJavid Ebrahimi and Dejing Dou and Daniel LowdDepartment of Computer and Information Science, University of OregonEugene, Oregon 97403, USA{javid,dou,lowd}@cs.uoregon.eduAbstractSupervised stance classification, in such do-mains as Congressional debates and onlineforums, has been a topic of interest in thepast decade.
Approaches have evolved fromtext classification to structured output predic-tion, including collective classification and se-quence labeling.
In this work, we investigatecollective classification of stances on Twitter,using hinge-loss Markov random fields (HL-MRFs).
Given the graph of all posts, users,and their relationships, we constrain the pre-dicted post labels and latent user labels to cor-respond with the network structure.
We focuson a weakly supervised setting, in which onlya small set of hashtags or phrases is labeled.Using our relational approach, we are able togo beyond the stance-indicative patterns andharvest more stance-indicative tweets, whichcan also be used to train any linear text classi-fier when the network structure is not availableor is costly.1 IntroductionStance classification is the task of determining fromtext whether the author of the text is in favor of,against, or neutral towards a target of interest.
Thisis an interesting task to study on social networksdue to the abundance of personalized and opinion-ated language.
Studying stance classification can bebeneficial in identifying electoral issues and under-standing how public stance is shaped (Mohammadet al, 2015).Twitter provides a wealth of information: pub-lic tweets by individuals, their profile information,whom they follow, and more.
Exploiting all thesepieces of information, in addition to the text, couldhelp build better NLP systems.
Examples of thisapproach include user preference modeling (Li etal., 2014), stance classification (Rajadesingan andLiu, 2014), and geolocation identification (Jurgens,2013; Rahimi et al, 2015).
For stance classification,knowing the author?s past posting behavior, or herfriends?
stances on issues, could improve the stanceclassifier.
These are inherently structured problems,and they demand structured solutions, such as Statis-tical Relational Learning (SRL) (Getoor, 2007).
Inthis paper, we use hinge-loss Markov random fields(HL-MRFs) (Bach et al, 2015), a recent develop-ment in the SRL community.SemEval 2016 Task 6 organizers (Mohammad etal., 2016) released a dataset with Donald Trump asthe target, without stance annotation.
The goal ofthe task was to evaluate stance classification sys-tems, which used minimal labeling on phrases.
Thisscenario is becoming more and more relevant due tothe vast amount of data and ever-changing nature ofthe language on social media.
This is critical in ap-plications in which a timely detection is highly de-sired, such as violence detection (Cano Basave et al,2013) and disaster situations.Our work is the first to use SRL for stance classi-fication on Twitter.
We formulate the weakly super-vised stance classification problem as a bi-type col-lective classification problem: We start from a smallset of stance-indicative patterns and label the tweetsas positive and negative, accordingly.
Then, our re-lational learner uses these noisy-labeled tweets, aswell as the network structure, to classify the stance1012of other tweets and authors.
Our goal will be toconstrain pairs of similar tweets, pairs of tweets andtheir authors, and pairs of neighboring users to havesimilar labels.
We do this through hinge-loss featurefunctions that encode our background knowledgeabout the domain: (1) A person is pro/against Trumpif she writes a tweet with such stance; (2) Friends ina social network often agree on their stance towardTrump; (3) similar tweets express similar stances.2 Related WorkStance classification is related to sentiment classifi-cation with a major difference that the target of inter-est may not be explicitly mentioned in the text andit may not be the target of opinion in the text (Mo-hammad et al, 2016).
Previous work has focused onCongressional debates (Thomas et al, 2006; Yesse-nalina et al, 2010), company-internal discussions(Agrawal et al, 2003), and debates in online fo-rums (Anand et al, 2011; Somasundaran and Wiebe,2010).
Stance classification has newly been posedas structured output prediction.
For example, cita-tion structure (Burfoot et al, 2011) or rebuttal links(Walker et al, 2012) are used as extra information tomodel agreements or disagreements in debate postsand to infer their labels.
Arguments and counter-arguments occur in sequences; Hasan and Ng (2014)used this observation and posed stance classificationin debate forums as a sequence labeling task, andused a global inference method to classify the posts.Sridhar et al (2015) use HL-MRFs to collec-tively classify stance in online debate forums.
Weaddress a weakly supervised problem, which makesour approach different as we do not rely on localtext classifiers.
Rajadesingan et al (2014) propose aretweet-based label propagation method which startsfrom a set of known opinionated users and labels thetweets posted by the people who were in the retweetnetwork.3 Stance Classification on Twitter3.1 Markov Random FieldsMarkov random fields (MRFs) are widely usedin machine learning and statistics.
DiscriminativeMarkov random fields such as conditional randomfields (Lafferty et al, 2001) are defined by a jointdistribution over random variables Y1, ..., Ym con-ditioned on X1, ..., Xn that is specified by a vec-tor of d real-valued potential functions ?l(y, x) forl = 1, ..., d, and a parameter (weight) vector ?
?
Rd:P(y|x;?)
= 1Z(?, x)exp(?
?, ?
(y, x)?
)where ?
?, ?
(y, x)?
denotes the dot product of theparameters and the potential functions, and Z(?, x)is the partition function.3.2 HL-MRFs for Tweet Stance ClassificationFinding the maximum a posteriori (MAP) state is adifficult discrete optimization problem and, in gen-eral, is NP-hard.
One particular class of MRFs thatallows for convex inference is hinge-loss Markovrandom fields (HL-MRFs) (Bach et al, 2015).
Inthis graphical model, each potential function is ahinge-loss function, and instead of discrete vari-ables, MAP inference is performed over relaxedcontinuous variables with domain [0, 1]n. Thesehinge-loss functions, multiplied by the correspond-ing model parameters (weights), act as penalizers forsoft linear constraints in the graphical model.Consider ti, uj as the random variables denotingthe ith tweet and the jth user.
The potential function,?
(ti, uj), relating a user and her tweet is as follows,max(0, tik ?
ujk) (1)where tik and ujk denote the respective assertionsthat ti has label k, and uj has label k .
The functioncaptures the distance between the label for a user andher tweet.
In other words, this function measures thepenalty for dissimilar labels for a user and her tweet.For users who are ?friends?
(i.e., who ?follow?each other on Twitter), we add this potential func-tion,max(0, uik ?
ujk) (2)and for the tweet-tweet relations,sijmax(0, tik ?
tjk) (3)where sij measures the similarity between twotweets.
This scalar helps penalize violations in pro-portion to the similarity between the tweets.
For thesimilarity measure, we simply used the cosine simi-larity between the n-gram (1-4-gram) representationof the tweets and set 0.7 as the cutoff threshold.1013Finally, two hard linear constraints are added, toensure that ti, and uj are each assigned a single la-bel, or in other words, are fractionally assigned la-bels with weights that sum to one.
?ktik = 1 ,?kuik = 1 (4)Weight learning is performed by an improved struc-tured voted perceptron (Lowd and Domingos, 2007),at every iteration of which we estimate the labels ofthe users by hard EM.
This formulation can work inweakly supervised settings, because the constraintssimply dictate similar/neighboring nodes to havesimilar labels.In the language of Probabilistic Soft Logic (PSL)(Bach et al, 2015), the constraints can be defined bythe following rules:PSL Rules:tweet-label(T , L) ?
tweet-user(T , U ) ?
user-label(U , L)user-label(U1, L) ?
friend(U1, U2) ?
user-label(U2, L)tweet-label(T1, L) ?
similar(T1, T2) ?
tweet-label(T2, L)PredicateConstraint.Functional , on : user-labelPredicateConstraint.Functional , on : tweet-labelOur post-similarity constraint implementation isdifferent from the original PSL implementation dueto the multiplicative similarity scalar1.This work is a first step toward relational stanceclassification on Twitter.
Incorporating other re-lational features, such as mention networks andretweet networks can potentially improve our re-sults.
Similarly, employing textual entailment tech-niques for tweet similarity will most probably im-prove our results.4 Experiments and Results4.1 DataSemEval-2016 Task 6.b (Mohammad et al, 2016)provided 78,000+ tweets associated with ?DonaldTrump?.
The protocol of the task only allowed min-imal manual labeling, i.e.
?tweets or sentences thatare manually labeled for stance?
were not allowed,but ?manually labeling a handful of hashtags?
waspermitted.
Additionally, using Twitter?s API, wecollected each user?s follower list and their profileinformation.
This often requires a few queries per1The original implementation would result in the function,max(0, tik + sij ?
tjk ?
1), which is less intuitive than ours.Algorithm Relational BootstrappingInput:Unlabeled pairs of tweets and authors (ti, ui).Friendship pairs (ui, uj) between users.Similarity triplets (ti, tj , sij) between tweets.Stance-indicative regexes R.// Create an initial dataset.Training set X = {}.Harvest positive and negative tweets based on R.Add the harvested tweets to X.// Augment the dataset by the relational classifier.Learning & inference over P (U,T|X) by our HL-MRF.Add some classified tweets to training set: X = X + T.Output: X.Favor.
make( ?
)america( ?
)great( ?
)again, #trumpfor-president, I{?m, am} voting trump, #illegal(.
*), patriot,#boycottmacyAgainst.
racist, bigot, idiot, hair, narcissis(.+)Table 1: Patterns to collect pro-Trump and anti-Trump tweets.user.
We only considered the tweets which containno URL, are not retweets, and have at most threehashtags and three mentions.This task?s goal was to test stance towards the tar-get in 707 tweets.
The authors in the test set are notidentified, which prevents us from pursuing a fullyrelational approach.
Thus, we adopt a two-phase ap-proach: First, we predict the stance of the trainingtweets using our HL-MRF.
Second, we use the la-beled instances as training for a linear text classifier.This dataset-augmenting procedure is summarizedin the Algorithm Relational Bootstrapping.4.2 Experimental SetupWe pick the pro-Trump and anti-Trump indicativeregular expressions and hashtags, which are shownin Table 1.
Tweets that have at least one positiveor one negative pattern, and do not have both posi-tive and negative patterns, are considered as our ini-tial positive and negative instances.
This gives usa dataset with noisy labels; for example, the tweet?his #MakeAmericaGreatAgain #Tag is a bummer.
?is against Donald Trump, incorrectly labeled favor-able.
A quantitative analysis of the impact of noise,and the goodness of initial patterns, can be pursuedin the future through a supervised approach.Tweets in the ?neither?
class range from newsabout the target of interest, to tweets totally irrele-1014Figure 1: An example of the output of our relational bootstrapper.
A small excerpt of the network, consisting of three users, fourtweets and two friendship links.
The tweet in regular type face is labeled as anti-Trump in the first phase, because of the word?racist?
in the tweet.
The other tweets, which are in boldface, are found through SRL harvesting, and are automatically labeled asanti-Trump tweets correctly.vant to him.
This makes it difficult to collect neutraltweets, and we will classify tweets to be in that classbased on a heuristic described in the next subsection.Given the limited number of seeds, we need tocollect more training instances to build a stance clas-sifier.
Because of the original noise in the labels andthe imposed fragmentary view of data, self-learningwould perform poorly.
Instead, we augment thedataset with tweets that our relational model clas-sifies as positive or negative with a minimum con-fidence (class value 0.52 for pro-Trump and 0.56for anti-Trump).
The hyper-parameters were foundthrough experimenting on a development set, whichwas the stance-annotated dataset of SemEval Task6.a.
The targets of that dataset include Hillary Clin-ton, Abortion, Climate Change, and Athesim.
Sincethere are more anti-Trump tweets than pro-Trump(Mohammad et al, 2016), for our grid search weprefer a higher confidence threshold for the anti-Trump class, making it harder for the class bias toadversely impact the quality of harvested tweets.
Wealso exclude the tweets that were sent by a user withno friends in the network.
An example which show-cases relational harvesting of tweets can be seen inFigure 1, wherein given the evidence, some of whichis shown, three new tweets are found.4.3 ClassificationWe convert the tweets to lowercase, and we removestopwords and punctuation marks.
For tweet clas-sification, we use a linear-kernel SVM, which hasproven to be effective for text classification and ro-bust in high-dimensional spaces.
We use the imple-No.
total tweets 21,000No.
initial pro tweets 1,100No.
initial anti tweets 1,490No.
relational-harvested pro tweets 960No.
relational-harvested anti tweets 780No.
edges in tweet similarity network 7,400No.
edges in friend network 131,000Table 2: Statistics of the datamentation of Pedregosa et al (2011), and we em-ploy the features below, which are normalized to unitlength after conjoinment.N-grams: tf-idf of binary representation of wordn-grams (1?4 gram) and character n-grams (2?6gram).
After normalization, we only pick the top5% most frequent grams.Lexicon: Binary indicators of positive-emotion andnegative-emotion words in LIWC2007 categories(Tausczik and Pennebaker, 2010).Sentiment: Sentiment distribution, based on a sen-timent analyzer for tweets, VADER (Hutto andGilbert, 2014).Table 3 demonstrates the results of stance classi-fication.
The metrics used are the macro-average ofthe F1-score for favor, against, and average of thesetwo.
The best competing system for the task useda deep convolutional neural network to train on proand against instances, which were collected throughlinguistic patterns.
At test time, they randomly as-signed the instances, about which the classifier wasless confident, to the ?neither?
class.
Another base-1015Method Ffavor Fagainst FavgSVM-ngrams-comb 18.42 38.45 28.43best-system 57.39 55.17 56.28SVM-IN 30.43 59.52 44.97SVM-NB 47.67 57.53 52.60SVM-RB 52.14 59.26 55.70SVM-RB-N 54.27 60.77 57.52Table 3: Evaluation on SemEval-2016 Task 6.b.line is an SVM, trained on another stance classifi-cation dataset (Task 6.a), using a combination of n-gram features (SVM-ngrams-comb).SVM-IN is trained on the initial dataset createdby linguistic patterns, SVM-RB is trained on therelational-augmented dataset, and SVM-NB is anaive bootstrapping method that simply adds moreinstances, from the users in the initial dataset, withthe same label as their tweets in the initial dataset,and for those who have both positive and negativetweets, does not add more of their tweets.At test time, we could predict an instance to beof the ?neither?
class if it contains none of ourstance-indicative patterns, nor any of the top 100word grams that have the highest tf-idf weightin the training set.
SVM-RB-N follows this heuris-tic for the ?neither?
class, while SVM-RB ignoresthis class altogether.4.4 Demographics of the UsersAs an application of stance classification, we ana-lyze the demographics of the users based on theirprofile information.
Due to the demographics ofTwitter users, one has to be cautious about drawinggeneralizing conclusions from the analysis of Twit-ter data.
We pick a balanced set of 1000 users withthe highest degree of membership to any of the twogroups.
In Figure 2, we plot states represented byat least 50 users in the dataset.
We can see that thefigure correlates with US presidential electoral poli-tics; supporters of Trump dominate Texas, and theyare in the clear minority in California.5 Conclusions and Future WorkIn this paper, we propose a weakly supervised stanceclassifier that leverages the power of relational learn-ing to incorporate extra features that are generallypresent on Twitter and other social media, i.e., au-Figure 2: Distribution of Twitter users in a number of states.thorship and friendship information.
HL-MRFs en-ables us to use a set of hard and soft linear con-straints to employ both the noisy-labeled instancesand background knowledge in the form of soft con-straints for stance classification on Twitter.While the relational learner tends to smooth outthe incorrectly labeled instances, this model still suf-fers from noise in the labels.
Labeling features andenforcing model expectation can be used to alleviatethe impact of noise; currently, the initial linguisticpatterns act as hard constraints for the label of thetweets, which can be relaxed by techniques such asgeneralized expectation (Druck et al, 2008).The SemEval dataset has only one target of in-terest, Donald Trump.
But the target of the opin-ion in the tweet may not necessarily be him, but re-lated targets, such as Hillary Clinton and Ted Cruz.Thus, automatic detection of targets and inferringthe stance towards all of the targets is the nextstep toward creating a practical weakly-supervisedstance classifier.6 AcknowledgmentsThis work was supported by NIH grantR01GM103309 and ARO grant W911NF-15-1-0265.
We would like to thank anonymousreviewers for their helpful comments, Saed Rezayifor helping with Twitter API, and Ellen Klowdenfor discussions.1016ReferencesRakesh Agrawal, Sridhar Rajagopalan, RamakrishnanSrikant, and Yirong Xu.
2003.
Mining newsgroupsusing networks arising from social behavior.
In Pro-ceedings of WWW, pages 529?535.Pranav Anand, Marilyn Walker, Rob Abbott, Jean E FoxTree, Robeson Bowmani, and Michael Minor.
2011.Cats rule and dogs drool!
: Classifying stance in onlinedebate.
In Proceedings of the Workshop on Computa-tional Approaches to Subjectivity and Sentiment Anal-ysis, pages 1?9.Stephen H. Bach, Matthias Broecheler, Bert Huang, andLise Getoor.
2015.
Hinge-loss Markov randomfields and probabilistic soft logic.
arXiv:1505.04406[cs.LG].Clinton Burfoot, Steven Bird, and Timothy Baldwin.2011.
Collective classification of congressional floor-debate transcripts.
In Proceedings of ACL, pages1506?1515.Amparo Elizabeth Cano Basave, Yulan He, Kang Liu,and Jun Zhao.
2013.
A weakly supervised Bayesianmodel for violence detection in social media.
In Pro-ceedings of IJCNLP, pages 109?117.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using general-ized expectation criteria.
In Proceedings of SIGIR,pages 595?602.Lise Getoor.
2007.
Introduction to statistical relationallearning.
MIT press.Kazi Saidul Hasan and Vincent Ng.
2014.
Why are youtaking this stance?
Identifying and classifying reasonsin ideological debates.
In Proceedings of EMNLP,pages 751?762.Clayton J Hutto and Eric Gilbert.
2014.
Vader: A par-simonious rule-based model for sentiment analysis ofsocial media text.
In Proceedings of ICWSM, pages216?225.David Jurgens.
2013.
That?s what friends are for: Infer-ring location in online social media platforms based onsocial relationships.
In Proceedings of ICWSM, pages273?282.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML, pages 282?289.Jiwei Li, Alan Ritter, and Dan Jurafsky.
2014.
Infer-ring user preferences by probabilistic logical reasoningover social networks.
arXiv:1411.2679 [cs.SI].Daniel Lowd and Pedro Domingos.
2007.
Efficientweight learning for Markov logic networks.
In Pro-ceedings of PKDD, pages 200?211.Saif M Mohammad, Xiaodan Zhu, Svetlana Kiritchenko,and Joel Martin.
2015.
Sentiment, emotion, purpose,and style in electoral tweets.
Information Processing& Management, 51(4):480?499.Saif M. Mohammad, Svetlana Kiritchenko, Parinaz Sob-hani, Xiaodan Zhu, and Colin Cherry.
2016.
Semeval-2016 task 6: Detecting stance in tweets.
In Proceed-ings of SemEval, pages 31?41.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Afshin Rahimi, Trevor Cohn, and Timothy Baldwin.2015.
Twitter user geolocation using a unified textand network prediction model.
In Proceedings of ACL,pages 630?636.Ashwin Rajadesingan and Huan Liu.
2014.
Identifyingusers with opposing opinions in Twitter debates.
InProceedings of SBP, pages 153?160.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the Workshop on Computational Ap-proaches to Analysis and Generation of Emotion inText, pages 116?124.Dhanya Sridhar, James Foulds, Bert Huang, Lise Getoor,and Marilyn Walker.
2015.
Joint models of disagree-ment and stance in online debate.
In Proceedings ofACL, pages 116?125.Yla R Tausczik and James W Pennebaker.
2010.
Thepsychological meaning of words: LIWC and comput-erized text analysis methods.
Journal of Language andSocial Psychology, 29(1):24?54.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition from con-gressional floor-debate transcripts.
In Proceedings ofEMNLP, pages 327?335.Marilyn A Walker, Pranav Anand, Robert Abbott, andRicky Grant.
2012.
Stance classification using di-alogic properties of persuasion.
In Proceedings ofNAACL-HLT, pages 592?596.Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010.Multi-level structured models for document-level sen-timent classification.
In Proceedings of EMNLP,pages 1046?1056.1017
