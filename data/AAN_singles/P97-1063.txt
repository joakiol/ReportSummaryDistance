A Word-to-Word Model of Translational EquivalenceI.
Dan  Me lamedDept .
of Computer  and Information ScienceUnivers i ty  of Pennsy lvan iaPh i lade lph ia ,  PA, 19104, U.S.A.raelamed~unagi, c is.
upenn, eduAbst ractMany multilingual NLP applications needto translate words between different lan-guages, but cannot afford the computa-tional expense of inducing or applying a fulltranslation model.
For these applications,we have designed a fast algorithm for esti-mating a partial translation model, whichaccounts for translational equivalence onlyat the word level .
The model's preci-sion/recall trade-off can be directly con-trolled via one threshold parameter.
Thisfeature makes the model more suitable forapplications that are not fully statistical.The model's hidden parameters can be eas-ily conditioned on information extrinsic tothe model, providing an easy way to inte-grate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc..Our model can link word tokens in paral-lel texts as well as other translation mod-els in the literature.
Unlike other transla-tion models, it can automatically producedictionary-sized translation lexicons, and itcan do so with over 99% accuracy.1 In t roduct ionOver the past decade, researchers at IBM have devel-oped a series of increasingly sophisticated statisticalmodels for machine translation (Brown et al, 1988;Brown et al, 1990; Brown et al, 1993a).
However,the IBM models, which attempt o capture a broadrange of translation phenomena, are computation-ally expensive to apply.
Table look-up using an ex-plicit translation lexicon is sufficient and preferablefor many multilingual NLP applications, including"crummy" MT on the World Wide Web (Church& I-Iovy, 1993), certain machine-assisted translationtools (e.g.
(Macklovitch, 1994; Melamed, 1996b)),concordancing for bilingual lexicography (Catizoneet al, 1993; Gale & Church, 1991), computer-assisted language learning, corpus linguistics (Melby.1981), and cross-lingual information retrieval (Oard&Dorr,  1996).In this paper, we present a fast method for in-ducing accurate translation lexicons.
The methodassumes that words are translated one-to-one.
Thisassumption reduces the explanatory power of ourmodel in comparison to the IBM models, but, asshown in Section 3.1, it helps us to avoid what wecall indirect associations, a major source of errors inother models.
Section 3.1 also shows how the one-to-one assumption enables us to use a new greedycompetitive linking algorithm for re-estimating themodel's parameters, instead of more expensive algo-rithms that consider a much larger set of word cor-respondence possibilities.
The model uses two hid-den parameters to estimate the confidence of its ownpredictions.
The confidence stimates enable directcontrol of the balance between the model's preci-sion and recall via a simple threshold.
The hiddenparameters can be conditioned on prior knowledgeabout the bitext to improve the model's accuracy.2 Co-occur renceWith the exception of (Fung, 1998b), previousmethods for automatically constructing statisticaltranslation models begin by looking at word co-occurrence frequencies in bitexts (Gale & Church,1991; Kumano & Hirakawa, 1994; Fung, 1998a;Melamed, 1995).
A bitext comprises a pair of textsin two languages, where each text is a translationof the other.
Word co-occurrence an be defined invarious ways.
The most common way is to divideeach half of the bitext into an equal number of seg-ments and to align the segments so that each pair ofsegments Si and Ti are translations of each other(Gale & Church, 1991; Melamed, 1996a).
Then,two word tokens (u, v) are said to co-occur  in the490aligned segment pair i if u E Si and v E Ti.
Theco-occurrence r lation can also be based on distancein a bitext space, which is a more general represen-tations of bitext correspondence (Dagan et al, 1993;Resnik & Melamed, 1997), or it can be restricted towords pairs that satisfy some matching predicate,which can be extrinsic to the model (Melamed, 1995;Melamed, 1997).3 The  Bas ic  Word- to -Word  Mode lOur translation model consists of the hidden param-eters A + and A-, and likelihood ratios L(u, v).
Thetwo hidden parameters are the probabilities of themodel generating true and false positives in the data.L(u,v)  represents the likelihood that u and v canbe mutual translations.
For each co-occurring pair ofword types u and v, these likelihoods are initially setproportional to their co-occurrence frequency n(u,v)and inversely proportional to their marginal frequen-cies n(u) and n(v) z, following (Dunning, 1993) 2.When the L(u, v) are re-estimated, the model's hid-den parameters come into play.After initialization, the model induction algorithmiterates:1.
Find a set of "links" among word tokens in thebitext, using the likelihood ratios and the com-petitive linking algorithm.2.
Use the links to re-estimate A+, A-, and thelikelihood ratios.3.
Repeat from Step 1 until the model convergesto the desired degree.The competitive linking algorithm and its one-to-oneassumption are detailed in Section 3.1.
Section 3.1explains how to re-estimate the model parameters.3.1 Compet i t i ve  L inking A lgor i thmThe competitive linking algorithm is designed toovercome the problem of indirect associations, illus-trated in Figure 1.
The sequences of u's and v'srepresent corresponding regions of a bitext.
If ukand vk co-occur much more often than expected bychance, then any reasonable model will deem themlikely to be mutual translations.
If uk and Vk areindeed mutual translations, then their tendency toZThe co-occurrence frequency of a word type pair issimply the number of times the pair co-occurs in thecorpus.
However, n(u) = ~-~v n(u.v), which is not thesame as the frequency of u, because ach token of u canco-occur with several differentv's.2We could just as easily use other symmetric "asso-ciation" measures, uch as ?2 (Gale & Church, 1991) orthe Dice coefficient (Smadja, 1992).?
?
?
Uk .
1 tJk ~ = Uk+l  ?
?
?t?
, ?
Vk .
1 Vk  Vk+l  ?
.
.Figure 1: Uk and vk often co-occur, as do uk anduk+z.
The direct association between uk and vk, andthe direct association between uk and Uk+l give riseto an indirect association between v~ and uk+l.co-occur is called a direct  associat ion.
Now, sup-pose that uk and Uk+z often co-occur within theirlanguage.
Then vk and uk+l will also co-occur moreoften than expected by chance.
The arrow connect-ing vk and u~+l in Figure 1 represents an indi rectassociat ion,  since the association between vk andUk+z arises only by virtue of the association betweeneach of them and uk.
Models of translational equiv-alence that are ignorant of indirect associations have"a tendency ... to be confused by collocates" (Daganet al, 1993).Fortunately, indirect associations are usually notdifficult to identify, because they tend to be weakerthan the direct associations on which they are based(Melamed, 1996c).
The majority of indirect associ-ations can be filtered out by a simple competitionheuristic: Whenever several word tokens ui in onehalf of the bitext co-occur with a particular word to-ken v in the other half of the bitext, the word that ismost likely to be v's translation is the one for whichthe likelihood L(u, v) of translational equivalence ishighest.
The competitive linking algorithm imple-ments this heuristic:1.
Discard all likelihood scores for word typesdeemed unlikely to be mutual translations, i.e.all L(u,v)  < 1.
This step significantly reducesthe computational burden of the algorithm.
Itis analogous to the step in other translationmodel induction algorithms that sets all prob-abilities below a certain threshold to negligiblevalues (Brown et al, 1990; Dagan et al, 1993;Chen, 1996).
To retain word type pairs thatare at least twice as likely to be mutual transla-tions than not, the threshold can be raised to 2.Conversely, the threshold can be lowered to buymore coverage at the cost of a larger model thatwill converge more slowly.2.
Sort all remaining likelihood estimates L(u, v)from highest to lowest.3.
Find u and v such that the likelihood ratioL(u,v)  is highest.
Token pairs of these types491n(u,v)Nk(u.v)KTk+k-B(k{n,p)= frequency of co-occurrence between word types u and v= ~"\].
(u.,,) n(u.v) = total number of co-occurrences in the bitext= frequency of links between word types u and v= ~"\].
(u,v) k(u.,,) = total number of links in the bitext= Pr( mutual translations I co-occurrence )= Pr( link I co-occurrence )= Pr( link \[ co-occurrence of mutual translations )= Pr( link I co-occurrence of not mutual translations )= Pr (k in ,p) ,  where k has a binomial distribution with parameters n and pN.B.
: k + and )~- need not sum to 1, because they are conditioned on different events.Figure 2: Variables used to estimate the model parameters.would be the winners in any competitions in-volving u or v.4.
Link all token pairs (u, v) in the bitext.5.
The one-to-one assumption means that linkedwords cannot be linked again.
Therefore, re-move all linked word tokens from their respec-tive texts.6.
If there is another co-occurring word token pair(u, v) such that L(u, v) exists, then repeat fromStep 3.The competitive linking algorithm is more greedythan algorithms that try to find a set of link typesthat are jointly most probable over some segment ofthe bitext.
In practice, our linking algorithm can beimplemented so that its worst-case running time isO(lm), where l and m are the lengths of the alignedsegments.The simplicity of the competitive linking algo-rithm depends on the one- to -one  assumpt ion :Each word translates to at most one other word.Certainly, there are cases where this assumption isfalse.
We prefer not to model those cases, in order toachieve higher accuracy with less effort on the caseswhere the assumption is true.3.2 Parameter  Es t imat ionThe purpose of the competitive linking algorithm isto help us re-estimate the model parameters.
Thevariables that we use in our estimation are summa-rized in Figure 2.
The linking algorithm produces aset of links between word tokens in the bitext.
Wedefine a l ink token  to be an ordered pair of wordtokens, one from each half of the bitext.
A l inktype  is an ordered pair of word types.
Let n(u.,,) bethe co-occurrence frequency of u and v and k(~,,,) bethe number of links between tokens of u and v 3.
An3Note that k(u,v) depends on the linking algorithm,but n(u.v) is a constant property of the bitext.important property of the competitive linking algo-rithm is that the ratio kiu.,,)/n(u,v ) tends to be veryhigh if u and v are mutual translations, and quitelow if they are not.
The bimodality of this ratiofor several values of n(u.,,i is illustrated in Figure 3.This figure was plotted after the model's first iter-ation over 300000 aligned sentence pairs from theI0(0),ooLI,.
{0} 0 ~(u V)/n(u v) o~ ,Figure 3: A fragment of the joint frequency(k(u.v)/n(u.v), n(u.v)).
Note that the frequencies areplotted on a log scale -- the bimodality is quite sharp.Canad ian  Hansard  bitext.
Note  that the frequenciesare plotted on a log scale -- the b imodal i ty  is quitesharp.The  linking algorithm creates all the links of agiven type independently of each other, so the num-ber k(u,v ) of links connecting word  types u and  vhas a binomial  distribution with parameters  n(u.,,land P(u.,,)- If u and v are mutual translations, thenP(u,,,) tends to a relatively high probability, which wewill call A +.
If u and v are not mutual translations,then P(u,v) tends to a very low probability, whichwe will call A-.
A + and A- correspond to the twopeaks in the frequency distribution of k(u.,,)/niu.v~in Figure 2.
The two parameters can also be inter-preted as the percentage of true and false positives.If the translation in the bitext is consistent and the492model is accurate, then A + should be near 1 and A-should be near 0.To find the most probable values of the hiddenmodel parameters A + and A-, we adopt he standardmethod of maximum likelihood estimation, and findthe values that maximize the probability of the linkfrequency distributions.
The one-to-one assumptionimplies independence between different link types,so thatPr(linkslm?del) = H Vr(k(u,v)\[n(u,v), A +, A-).R~V(1)The factors on the right-hand side of Equation 1 canbe written explicitly with the help of a mixture co-efficient.
Let r be the probability that an arbitraryco-occurring pair of word types are mutual transla-tions.
Let B(kln,p ) denote the probability that klinks are observed out of n co-occurrences, where khas a binomial distribution with parameters n and p.Then the probability that u and v are linked k(u,v)times out of n(u,v) co-occurrences is a mixture of twobinomials:Pr(k(u,v) ln(u,v), A +, A-) = (2)= rB(k(u,v)ln(u,v), A +)?
(1 - r )B(k (u ,v ) ln (u ,v ) ,A - )One more variable allows us to express r in termsof A + and A- : Let A be the probability that an arbi-trary co-occuring pair of word tokens will be linked,regardless of whether they are mutual translations.Since r is constant over all word types, it also repre-sents the probability that an arbitrary co-occurringpair of word tokens are mutual translations.
There-fore,A = rA + + (1 - r)A-.
(3)A can also be estimated empirically.
Let K be thetotal number of links in the bitext and let N be thetotal number of co-occuring word token pairs: K =~(u,v) k(u,v/, N = ~(~,v) n(u,v).
By definition,A = KIN.
(4)Equating the right-hand sides of Equations (3) and(4) and rearranging the terms, we get:KIN  - ,X-- (5 )A+ _ )~-Since r is now a function of A + and A-, only thelatter two variables represent degrees of freedom inthe model.The probability function expressed by Equations 1and 2 has many local maxima.
In practice, thesec-1.2-1.4E -1.6"~ -1.8 )0Figure 4: Pr(links\[model) has only one global max-imum in the region of interest.local maxima are like pebbles on a mountain, in-visible at low resolution.
We computed Equation 1over various combinations of A + and A- after themodel's first iteration over 300000 aligned sentencepairs from the Canadian Hansard bitext.
Figure 4shows that the region of interest in the parameterspace, where 1 > A + > A > A- > 0, has only oneclearly visible global maximum.
This global maxi-mum can be found by standard hill-climbing meth-ods, as long as the step size is large enough to avoidgetting stuck on the pebbles.Given estimates for A + and A-, we can computeB(ku,,,\[nu,v, A +) and B(ku,v\[nu,v, A-).
These areprobabilities that k(u,v) links were generated by analgorithm that generates correct links and by an al-gorithm that generates incorrect links, respectively,out ofn(u,v) co-occurrences.
The ratio of these prob-abilities is the likelihood ratio in favor of u and vbeing mutual translations, for all u and v:B(ku,vln<,,,,, ),+)L(u,v) = B(ku,vln~,v, A_ ) .
(614 C lass -Based  Word- to -WordMode lsIn the basic word-to-word model, the hidden param-eters A + and A- depend only on the distributions oflink frequencies generated by the competitive link-ing algorithm.
More accurate models can be inducedby taking into account various features of the linkedtokens.
For example, frequent words are translatedless consistently than rare words (Melamed, 1997).To account for this difference, we can estimate sep-arate values of X + and A- for different ranges ofn(u,v).
Similarly, the hidden parameters can be con-ditioned on the linked parts of speech.
Word ordercan be taken into account by conditioning the hid-den parameters on the relative positions of linkedword tokens in their respective sentences.
Just aseasily, we can model links that coincide with en-tries in a pre-existing translation lexicon separately493from those that do not.
This method of incorporat-ing dictionary information seems simpler than themethod proposed by Brown et ai.
for their models(Brown et al, 1993b).
When the hidden parametersare conditioned on different link classes, the estima-tion method does not change; it is just repeated foreach link class.5 Eva luat ionA word-to-word model of translational equivalencecan be evaluated either over types or over tokens.It is impossible to replicate the experiments used toevaluate other translation models in the literature,because neither the models nor the programs thatinduce them are generally available.
For each kindof evaluation, we have found one case where we cancome close.We induced a two-class word-to-word model oftranslational equivalence from 13 million words ofthe Canadian Hansards, aligned using the methodin (Gale & Church, 1991).
One class repre-sented content-word links and the other representedfunction-word links 4.
Link types with negativelog-likelihood were discarded after each iteration.Both classes' parameters converged after six it-erations.
The value of class-based models wasdemonstrated by the differences between the hid-den parameters for the two classes.
(A +,A-)  con-verged at (.78,00016) for content-class links and at(.43,.000094) for function-class links.5.1 L ink TypesThe most direct way to evaluate the link types ina word-level model of translational equivalence is totreat each link type as a candidate translation lexi-con entry, and to measure precision and recall.
Thisevaluation criterion carries much practical import,because many of the applications mentioned in Sec-tion 1 depend on accurate broad-coverage transla-tion lexicons.
Machine readable bilingual dictionar-ies, even when they are available, have only limitedcoverage and rarely include domain-specific terms(Resnik & Melamed, 1997).We define the recall of a word-to-word translationmodel as the fraction of the bitext vocabulary repre-sented in the model.
Translation model precision isa more thorny issue, because people disagree aboutthe degree to which context should play a role injudgements of translational equivalence.
We hand-evaluated the precision of the link types in our modelin the context of the bitext from which the model4Since function words can be identified by table look-up, no POS-tagger was involved.was induced, using a simple bilingual concordancer.A link type (u, v) was considered correct if u and vever co-occurred as direct translations of each other.Where the one-to-one assumption failed, but a linktype captured part of a correct translation, it wasjudged "incomplete."
Whether incomplete links arecorrect or incorrect depends on the application.1009896~) 94u 92 t_9O888684(99.2%) ~(9~ .6%) t'",,,""-,}(89.2%).
.
.
.
.
.
.
.  "
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
xincomplete =incorrect ......... -~(86.8%)3'6 4'6 9'0% recallFigure 5: Link type precision with 95~ confidenceintervals at varying levels of recall.We evaluated five random samples of 100 linktypes each at three levels of recall.
For our bitext,recall of 36%, 46% and 90% corresponded to trans-lation lexicons containing 32274, 43075 and 88633words, respectively.
Figure 5 shows the precision ofthe model with 95% confidence intervals.
The uppercurve represents precision when incomplete links areconsidered correct, and the lower when they are con-sidered incorrect.
On the former metric, our modelcan generate translation lexicons with precision andrecall both exceeding 90%, as well as dictionary-sized translation lexicons that are over 99% correct.Though some have tried, it is not clear how toextract such accurate lexicons from other publishedtranslation models.
Part of the difficulty stems fromthe implicit assumption in other models that eachword has only one sense.
Each word is assigned thesame unit of probability mass, which the model dis-tributes over all candidate translations.
The  correcttranslations of a word that has several correct rans-lations will be assigned a lower probability than thecorrect translation of a word that has only one cor-rect translation.
This imbalance foils thresholdingstrategies, clever as they might be (Gale & Church,1991; Wu ~z Xia, 1994; Chen, 1996).
The likelihoodsin the word-to-word model remain unnormalized, sothey do not compete.The word-to-word model maintains high preci-sion even given much less training data.
Resnik& Melamed (1997) report that the model produced494translation lexicons with 94% precision and 30% re-call, when trained on French/English software man-uals totaling about 400,000 words.
The modelwas also used to induce a translation lexicon froma 6200-word corpus of French/English weather e-ports.
Nasr (1997) reported that the translationlexicon that our model induced from this tiny bitextaccounted for 30% of the word types with precisionbetween 84% and 90%.
Recall drops when there istess training data, because the model refuses to makepredictions that it cannot make with confidence.
Formany applications, this is the desired behavior.5.2 Link Tokenstype of error errors made by errors madeIBM Model 2 by our modelwrong linkmissing linkpartial linkclass conflicttokenizationparaphrase32127339736105236TOTAL 93 96Table 1: Erroneous link tokens generated by twotranslation models.The most detailed evaluation of link tokens todate was performed by (Macklovitch & Hannan,1996), who trained Brown et al's Model 2 on 74million words of the Canadian Hansards.
These au-thors kindly provided us with the links generatedby that model in 51 aligned sentences from a held-out test set.
We generated links in the same 51sentences using our two-class word-to-word model,and manually evaluated the content-word links fromboth models.
The IBM models are directional; i.e.they posit the English words that gave rise to eachFrench word, but ignore the distribution of the En-glish words.
Therefore, we ignored English wordsthat were linked to nothing.The errors are classified in Table 1.
The "wronglink" and "missing link" error categories should beself-explanatory.
"Partial inks" are those where oneFrench word resulted from multiple English words,but the model only links the French word to one ofits English sources.
"Class conflict" errors resultedfrom our model's refusal to link content words withfunction words.
Usually, this is the desired behavior,but words like English auxiliary verbs are sometimesused as content words, giving rise to content wordsin French.
Such errors could be overcome by a modelthat classifies each word token, for example using apart-of-speech tagger, instead of assigning the sameclass to all tokens of a given type.
The bitext pre-processor for our word-to-word model split hyphen-ated words, but Macklovitch &Hannan's preproces-sor did not.
In some cases, hyphenated words wereeasier to link correctly; in other cases they were moredifficult.
Both models made some errors because ofthis tokenization problem, albeit in different places.The "paraphrase" category covers all link errors thatresulted from paraphrases in the translation.
Nei-ther IBM's Model 2 nor our model is capable of link-ing multi-word sequences to multi-word sequences,and this was the biggest source of error for bothmodels.The test sample contained only about 400 contentwords 5, and the links for both models were evaluatedpost-hoc by only one evaluator.
Nevertheless, it ap-pears that our word-to-word model with only twolink classes does not perform any worse than IBM'sModel 2, even though the word-to-word model wastrained on less than one fifth the amount of data thatwas used to train the IBM model.
Since it doesn'tstore indirect associations, our word-to-word modelcontained an average of 4.5 French words for everyEnglish word.
Such a compact model requires rel-atively little computational effort to induce and toapply.des screamingvents .
winds,A ,dechames ---" andet dangerousune ~ seamer .
conditionsspde'montee.-""Figure 6: An example of the different sorts of er-rors made by the word-to-word model and the IBMModel 2.
Solid lines are links made by both mod-els; dashes lines are links made by the IBM modelonly.
Only content-class links are shown.
Neithermodel makes the correct links (ddcha?nds,screaming)and (ddmontde, dangerous).5The exact number depends on the tokenizationmethod.495In addition to the quantitative differences betweenthe word-to-word model and the IBM model, thereis an important qualitative difference, illustrated inFigure 6.
As shown in Table 1, the most commonkind of error for the word-to-word model was a miss-ing link, whereas the most common error for IBM'sModel 2 was a wrong link.
Missing links are more in-formative: they indicate where the model has failed.The level at which the model trusts its own judge-ment can be varied directly by changing the likeli-hood cutoff in Step 1 of the competitive linking algo-rithm.
Each application of the word-to-word modelcan choose its own balance between link token pre-cision and recall.
An application that calls on theword-to-word model to link words in a bitext couldtreat unlinked words differently from linked words,and avoid basing subsequent decisions on uncertaininputs.
It is not clear how the precision/recall trade-off can be controlled in the IBM models.One advantage that Brown et al's Model i hasover our word-to-word model is that their objec-tive function has no local maxima.
By using theEM algorithm (Dempster et al, 1977), they canguarantee convergence towards the globally opti-mum parameter set.
In contrast, the dynamic na-ture of the competitive linking algorithm changesthe Pr(datalmodel ) in a non-monotonic fashion.
Wehave adopted the simple heuristic that the model"has converged" when this probability stops increas-ing.6 Conc lus ionMany multilingual NLP applications need to trans-late words between different languages, but cannotafford the computational expense of modeling thefull range of translation phenomena.
For these ap-plications, we have designed afast algorithm for esti-mating word-to-word models of translational equiv-alence.
The estimation method uses a pair of hid-den parameters to measure the model's uncertainty,and avoids making decisions that it's not likely tomake correctly.
The hidden parameters can be con-ditioned on information extrinsic to the model, pro-viding an easy way to integrate pre-existing knowl-edge.So far we have only implemented a two-classmodel, to exploit the differences in translation con-sistency between content words and function words.This relatively simple two-class model linked wordtokens in parallel texts as accurately as other trans-lation models in the literature, despite being trainedon only one fifth as much data.
Unlike other transla-tion models, the word-to-word model can automat-ically produce dictionary-sized translation lexicons,and it can do so with over 99% accuracy.Even better accuracy can be achieved with a morefine-grained link class structure.
Promising featuresfor classification include part of speech, frequencyof co-occurrence, relative word position, and trans-lational entropy (Melamed, 1997).
Another inter-esting extension is to broaden the definition of a"word" to include multi-word lexical units (Smadja,1992).
If such units can be identified a priori, theirtranslations can be estimated without modifying theword-to-word model.
In this manner, the model canaccount for a wider range of translation phenomena.AcknowledgementsThe French/English software manuals were providedby Gary Adams of Sun MicroSystems Laboratories.The weather bitext was prepared at the Universityof Montreal, under the direction Of Richard Kit-tredge.
Thanks to Alexis Nasr for hand-evaluatingthe weather translation lexicon.
Thanks also to MikeCollins, George Foster, Mitch Marcus, Lyle Ungar,and three anonymous reviewers for helpful com-ments.
This research was supported by at.
equip-ment grant from Sun MicroSystems and by ARPAContract #N66001-94C-6043.ReferencesP.
F. Brown, J. Cocke, S. Della Pietra, V. DellaPietra, F. Jelinek, R. Mercer, & P. Roossin, "AStatistical Approach to Language Translation,"Proceedings of the 12th International Conferenceon Computational Linguistics, Budapest, Hun-gary, 1988.P.
F. Brown, J. Cocke, S. Della Pietra, V. DellaPietra, F. Jelinek, R. Mercer, & P. Roossin,"A Statistical Approach to Machine Translation,"Computational Linguistics 16(2), 1990.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra& R. L. Mercer, "The Mathematics of Statisti-cal Machine Translation: Parameter Estimation,"Computational Linguistics 19(2), 1993.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra,M.
J. Goldsmith, J. Hajic, R. L. Mercer & S. Mo-hanty, "But Dictionaries are Data Too," Proceed-ings of the ARPA HLT Workshop, Princeton, N J,1993.R.
Catizone, G. Russell & S. Warwick "DerivingTranslation Data from Bilingual Texts," Proceed-ings of the First International Lexical AcquisitionWorkshop, Detroit, MI, 1993.496S.
Chen, Building Probabilistic Models for Natu-ral Language, Ph.D. Thesis, Harvard University,1996.K.
W. Church & E. H. Hovy, "Good Applications forCrummy Machine Translation," Machine Transla-tion 8, 1993.I.
Dagan, K. Church, & W. Gale, "Robust WordAlignment for Machine Aided Translation," Pro-ceedings of the Workshop on Very Large Corpora:Academic and Industrial Perspectives, Columbus,OH, 1993.A.
P. Dempster, N. M. Laird & D. B. Rubin, "Maxi-mum likelihood from incomplete data via the EMalgorithm," Journal of the Royal Statistical Soci-ety 34(B), 1977.T.
Dunning, "Accurate Methods for the Statisticsof Surprise and Coincidence," Computational Lin-guistics 19(1), 1993.P.
Fung, "Compiling Bilingual Lexicon Entries froma Non-Parallel English-Chinese Corpus," Proceed-ings of the Third Workshop on Very Large Cor-pora, Boston, MA, 1995a.P.
Fung, "A Pattern Matching Method for Find-ing Noun and Proper Noun Translations fromNoisy Parallel Corpora," Proceedings of the 33rdAnnual Meeting of the Association for Computa-tional Linguistics, Boston, MA, 1995b.W.
Gale & K. W. Church, "A Program for Align-ing Sentences in Bilingual Corpora" Proceedingsof the 29th Annual Meeting of the Association forComputational Linguistics, Berkeley, CA, 1991.W.
Gale & K. W. Church, "Identifying Word Corre-spondences in Parallel Texts," Proceedings of theDARPA SNL Workshop, 1991.A.
Kumano & H. Hirakawa, "Building an MT Dic-tionary from Parallel Texts Based on Linguisticand Statistical Information," Proceedings of the15th International Conference on ComputationalLinguistics, Kyoto, Japan, 1994.E.
Macklovitch :'Using Bi-textual Alignment forTranslation Validation: The TransCheck Sys-tem," Proceedings of the 1st Conference of the As-sociation for Machine Translation in the Ameri-cas, Columbia, MD, 1994.E.
Macklovitch & M.-L. Hannan, "Line 'Em Up: Ad-vances in Alignment Technology and their Impacton Translation Support Tools," 2nd Conferenceof the Association for Machine Translation in theAmericas, Montreal, Canada, 1996.I.
D. Melamed "Automatic Evaluation and UniformFilter Cascades for Inducing N-best TranslationLexicons," Proceedings of the Third Workshop onVery Large Corpora, Boston, MA, 1995.I.
D. Melamed, "A Geometric Approach to MappingBitext Correspondence," Proceedings of the FirstConference on Empirical Methods in Natural Lan-guage Processing, Philadelphia, PA, 1996a.I.
D. Melamed "Automatic Detection of Omissionsin Translations," Proceedings of the 16th Interna-tional Conference on Computational Linguistics,Copenhagen, Denmark, 1996b.I.
D Melamed, "Automatic Construction of CleanBroad-Coverage Translation Lexicons," 2nd Con-ference of the Association for Machine Transla-tion in the Americas, Montreal, Canada, 1996c.I.
D. Melamed, "Measuring Semantic Entropy," Pro-ceedings of the SIGLEX Workshop on TaggingText with Lexical Semantics, Washington, DC,1997.I.
D. Melamed, "A Portable Algorithm for MappingBitext Correspondence," Proceedings of the 35thConference of the Association for ComputationalLinguistics, Madrid, Spain, 1997.
(in this volume)A. Melby, "A Bilingual Concordance System and itsUse in Linguistic Studies," Proceedings of the En-glish LACUS Forum, Columbia, SC, 1981.A.
Nasr, personal communication, 1997.P.
Resnik & I. D. Melamed, "Semi-Automatic A qui-sition of Domain-Specific Translation Lexicons,"Proceedings of the 7th ACL Conference on Ap-plied Natural Language Processing, Washington,DC, 1997.D.
W. Oard & B. J. Dorr, "A Survey of MultilingualText Retrieval, UMIACS TR-96-19, University ofMaryland, College Park, MD, 1996.F.
Smadja, "How to Compile a Bilingual Collo-cational Lexicon Automatically," Proceedings ofthe AAAI Workshop on Statistically-Based NLPTechniques, 1992.D.
Wu & X. Xia, "Learning an English-ChineseLexicon from a Parallel Corpus," Proceedings ofthe First Conference of the Association for Ma-chine Translation in the Americas, Columbia,MD, 1994.497
