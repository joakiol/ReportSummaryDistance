Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 137?140,Prague, June 2007. c?2007 Association for Computational LinguisticsDFKI2: An Information Extraction Based Approach to PeopleDisambiguationAndrea HeylGerman Research Center forArtificial Intelligence - DFKI,Saarbru?cken, Germanyandrea.heyl@dfki.deGu?nter NeumannGerman Research Center forArtificial Intelligence - DFKI,Saarbru?cken, Germanyguenter.neumann@dfki.deAbstractWe propose an IE based approach to peopledisambiguation.
We assume the mentioningof NEs and the relational context of a per-son in the text to be important discriminat-ing features in order to distinguish differentpeople sharing a name.1 IntroductionIn this paper, we propose a system with a linguis-tic view on people disambiguation that exploits therelational and NE context of a person name as dis-criminating features.Texts about different people differ from eachother by the names of persons, places and organiza-tions connected to these people and by the relationsin which a person?s name is connected to other enti-ties.
Therefore we had the hypothesis that the NEs inthe documents for a person name should be a maindistinctive criterion for disambiguating people.Furthermore, the relational context of a personname should also be able to give good clues for dis-ambiguation.
Sentence patterns related to a name,i.e.
patterns that contain the name as subject orobject like ?be(Person X, lawyer)?
often conveyuniquely identifying information about a person.Our system was not built specifically for the webpeople search task WePS (Artiles et al, 2007), butis an early version of an IE system that has the moregeneral goal to discover relations between NEs.
Wesee the WePS task as a specific instance of the set oftasks our system should be able to handle.
There-fore, we only adapted it slightly to work with theWePS data, but did not make any further customiza-tion w.r.t.
the special requirements of people disam-biguation.
As our system was built to handle puretexts rather than structured web pages, we reliedcompletely on linguistic information and did not ex-ploit the html structure of the documents provided.2 Related WorkOur system was inspired by the preemptive and on-demand IE approaches by Sekine and Shinyama(Sekine, 2006; Shinyama, 2006) that cluster news-paper articles into classes of articles that talk aboutthe same type of event.
They proposed a system todiscover in advance all possible relations and to re-turn them in form of tables.We took the idea of distinctive personal attributesas a criterion for disambiguation from the work ofBollegala et al (2006).
They propose an unsu-pervised learning approach to extract phrases thatuniquely identify a person from the web and usethese discriminative features for clustering.3 System OverviewThe goal of the WePS task is to cluster the top 100web pages returned by a web search engine for acertain name as search query and classify them w.r.t.the underlying different people they refer to.The problem of clustering documents about peo-ple into different entities can be seen as two sub-problems: The determination of the correct num-ber of clusters and the clustering of the given doc-uments into this number of entities.
These problemscould either be solved consecutively by first estimat-ing the number of classes and then produce this pre-137html ?
textcoreferenceresolution ''PPPNE-tagging// semanticparsingvvmmmmfeature vectors // clusteringFigure 1: System Overviewset number of clusters or by determining the numberof classes dynamically during the clustering process.Figure 1 gives an overview of our system, thatclusters web documents into a pre-defined numberof classes, thereby being only concerned with thesecond problem and neglecting the estimation of dif-ferent namesakes for now.Every web page in the WePS training data is rep-resented by the set of its files.
As our system workson plain text only, we first needed to separate thetextual parts of all files.
Therefore, we extracted thetext from the html pages.
We merged the texts fromall different html pages belonging to a single web-site into one document so that we obtained for everyperson?s name 100 text files as the basis for furtherclustering.These text files were processed by a coreferenceresolution tool.
On the resulting texts, we ran bothan NE tagger and an NLP tool for semantic parsing.This tool represents sentences containing the respec-tive person name as predicate argument structures.We constructed two feature vectors for each filebased on the counts of the NEs and predicate ar-gument structures that contain the specific personname.
Those feature vectors were our basis for theclustering process.The clustering unit of the system consecutivelymerged clusters, that at first contained a single fileeach, until the pre-set number of classes was reachedand returned the clustering as an xml file.4 System Components4.1 Estimating the Number of ClassesIn principle, the number of different people that arerepresented in the data cannot be known in advance.However, for the clustering process, either the num-ber of classes has to be fixed before clustering, orsome kind of termination criterion has to be foundthat tells the algorithm when to stop clustering.A good estimation of the number of different en-tities is a necessary prerequisite for successful clus-tering.
Clustering into too many classes would meanassigning documents to classes that have actually noown entity they refer to.
Clustering into too fewclasses means merging two entities into one class.Our initial intuition was to distinguish people bynormally unique properties, like phone numbers oremail addresses.
So we assumed that the number ofdifferent email addresses and phone numbers occur-ring in all documents for one name would be a goodmeans to estimate the number of different personssharing this name, but we could not find any corre-lation between these features and the class number.Therefore, we decided to estimate the averagenumber of classes from the training data.
The aver-age number of different people for one name in thetraining data was about 18.
Based on the observa-tion that an underestimated number of classes leadsto better results than assuming too many classes, wedecided to guess 12 different persons for each name.4.2 PreprocessingFor the extraction of plain text information from theweb pages, we used the html2text 1 converter.
Incase that a web page consisted of more than one htmldocument, we put all the output from the converterinto one single file.
By omitting any wrapping ofthe html pages, we obviously lost useful structuralinformation but got the textual information for ourlinguistic analysis.Afterward, we applied several linguistic prepro-cessing tools.
We used coreference resolution to re-place pronouns referring to a person, and variationsof a name (like ?Mr.
Smith?
after a mention of ?JohnSmith?
earlier in the text) with the person?s name inthe form of its first mention in the text.For NE-tagging, we used the three NE types PER-SON, LOCATION and ORGANIZATION.
For bothNE tagging and coreference resolution, we used theLingPipe toolkit 2.
We counted the occurrences ofevery NE in every file and replaced all instancesby their specific NE type combined with a uniquely1http://www.mbayer.de/html2text/index.shtml2http://www.alias-i.com/lingpipe/138identifying number, e.g.
we replaced all occurrencesof ?Paris?
with ?LOCATION27?, in order to ensurethat the predicate argument parser could work cor-rectly and would not split up multi-word NEs intotwo or more arguments.We passed all sentences with NEs that con-tained the specified persons family name (e.g.?Mr.
Cooper?
for the name ?Alvin Cooper?)
toMontyLingua 3, that returns a semantic represen-tation of the sentence like (?live?
?PERSON2?
?in LOCATION3?).
These representations abstractfrom the actual surface form of a sentence as theyrepresent every sentence in its underlying semanticform (?predicate?
?semantic subject?
?semantic ob-ject1?...)
rather than just determining the syntacticsubject and objects of a sentence.
We called thesestructures ?patterns?
and kept only those that actu-ally contained the respective NE.4.3 ClusteringWe decided on building two vectors for every textfile, one for the NEs and one for sentence patternsconnected to a person?s name in order to give to theNEs a weight different from that for the patterns.After tagging the documents for NEs, we countedthe frequency of the different occurring NEs for onename.
We built a first feature vector for each docu-ment that contained as entries the counts of the oc-curring NEs in this document.
We set a threshold nto use only the n best NEs in the vectors, countedover all documents for one name.
We then built forevery document a second feature vector containingthe counts of the MontyLingua patterns for the doc-ument.For the actual clustering process, we used hierar-chical clustering.
We started with every file, rep-resented by a pair of normalized feature vectors,constituting a single cluster.
As distance measure-ment we used the weighted sum of the absolute dis-tances between the centers of two clusters with re-gard to both feature vectors, respectively, i.e.
wechose distance = w?distanceNEs+distancepatterns.In every step, we made a pairwise comparison of allclusters and merged those with the lowest distance.The clustering terminated when the algorithm camedown to the pre-set number of 12 clusters.
So far3http://web.media.mit.edu/ hugo/montylingua/we have not made any further use of the binary treestructure within each cluster.We assigned every file to exactly one cluster.
Wehad neither a ?discarded?
category nor did we handlethe possibility that a page refers to more than oneperson and would hence belong to different clusters.5 Experiments5.1 Training of ParametersWe evaluated the system on the provided WePStraining data to estimate the following parameters:number of classes, number of best NEs to be consid-ered and weight of the NE vector compared to thepattern vector.The relevant evaluation score is the F-measure(?
= 0.5) as the harmonic mean of purity and in-verse purity as described by Hotho et al (2003).As our attempt to use distinctive features for theestimation of class numbers failed, we examined theinfluence of a wrongly estimated number of classeson the clustering results.
Table 1 shows exemplarilyfor 2 person names how the F-measure varies if thecorrect number of classes is incorrectly assumed as ahigher or lower value.
We concluded that it is betterto estimate the class number too low than too high.name A. Macomb E. Foxcorrect number of classes 21 1610 classes assumed 0.76 0.8012 classes assumed 0.75 0.7514 classes assumed 0.72 0.7616 classes assumed 0.69 0.6018 classes assumed 0.60 0.5820 classes assumed 0.48 0.7222 classes assumed 0.56 0.5524 classes assumed 0.59 0.5826 classes assumed 0,52 0.56Table 1: F-measure for different numbers of as-sumed classesPrimarily meant as a means to reduce computa-tion time, we gave our system the possibility not touse all occurring NEs for clustering, but only a cer-tain number of entities with maximal frequencies.Test runs did not confirm our hypothesis that con-sidering a higher number of NEs leads to better re-sults (cf.
table 2).
For both training of the number ofNEs and the NE weight we assumed that we alreadyknew the correct class number.As the F-measure did not increase for more con-sidered NEs, we believe that the most important NEs139are already covered within the best 100 and thatadding more NEs rather adds coincidental informa-tion than any new important facts.
Usually, the best100 NEs already cover most of those which occurmore than once in a text.NEs average.
F-measure100 0.66200 0.68500 0.681000 0.67w average F-measure0.5 0.661.0 0.682.0 0.684.0 0.67Table 2: varying the number of considered entitiesand weight of the feature vectorsThe third parameter to estimate was the weightw given to the NE feature vector compared to thefeature vector for sentence patterns.
During training,this weight also appeared to have little influence onthe clustering results (cf.
2).
We have the hypothesisthat sentence pattern detection is not very successfulfor the often unstructured web page texts.5.2 Results for WePS Test DataIn the WePS evaluation, our system scored with apurity of 0.39, an inverse purity of 0.83 and a result-ing overall F-measure (?
= 0.5) of 0.5.One main reason for our test results to be worsethan our training results is the fact that the test datahad a much higher average number of classes (about46 classes).
Our F-measure was best for those nameswith the fewest number of referents.
We had an av-erage F-Measure (?
= 0.5) of 0.66 for those nameswith less than 30 instances compared to an overallaverage of 0.50.
These numbers show the impor-tance of a correct estimation of the assumed numberof referents for a name.Our purity was much lower than the inverse pu-rity, i.e.
there is too much noise in our clusteringcompared to the real partition, whereas the real clus-ters are well covered by our clustering.
This is dueto a too low estimation of the number of referents.6 Conclusions and Future WorkOne obvious improvement , that would accommo-date the general relation extraction idea of our sys-tem, is to include the use of structural informationfrom the html documents in addition to our purelylinguistic view on web pages.
Additionally, weshould weight our NEs using e.g.
a TF/IDF formula.A promising direction for further research in peo-ple search will certainly include a better control ofthe number of classes.
This could be done eitherby estimating this number in advance, or by settingthe number of classes dynamically during cluster-ing.
The latter could include comparing the size ofthe current clusters to the overall feature space of allclusters or an approach of counting occurrences ofuniquely identifying attributes within a cluster.This second approach could match the originalpurpose of our system, namely to build tables thatrepresent the most salient relations in a set of docu-ments in the way Sekine and Shinyama did.
If sucha table, that represents the slots of a relation in itscolumns and every article in a row, is built for alldocuments in a cluster, we would expect the table tocontain roughly the same information in every row.One could define a consistency measure for the re-sulting tables and stop clustering as soon as the ta-bles are no longer consistent enough, i.e.
when theycontain too much contradictory information.AcknowledgmentThe work presented here was supported by a re-search grant from the Investitionsbank Berlin to theDFKI project IDEX (Interactive Dynamic IE).ReferencesJavier Artiles, Julio Gonzalo and Satoshi Sekine.
2007.The SemEval-2007 WePS Evaluation: Establishing aBenchmark for the Web People Search Task.
Proceed-ings of Semeval 2007, ACL.Danushka Bollegala, Yutaka Matsuo and MitsuruIshizuka.
2006.
Extracting Key Phrases to Disam-biguate Personal Name Queries in Web Search.
Pro-ceedings of the Workshop on How Can ComputationalLinguistics Improve Information Retrieval, p. 17?24.Andreas Hotho, Steffen Staab and Gerd Stumme.
2003.Wordnet Improves Text Document Clustering.
Pro-ceedings of the Semantic Web Workshop at SIGIR-2003, 26th Annual International ACM SIGIR Confer-ence, Toronto, Canada.Satoshi Sekine.
2006.
On-Demand IE.
InternationalCommittee on Comp.
Ling.
and the ACL.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive Information Extraction using Unrestricted Rela-tion Discovery.
Human Language Technology con-ference - North American chapter of the ACL annualmeeting; New York City.140
