Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 25?30,Columbus, June 2008. c?2008 Association for Computational LinguisticsA Re-examination on Features in Regression Based Approach to Auto-matic MT EvaluationShuqi Sun, Yin Chen and Jufeng LiSchool of Computer Science and TechnologyHarbin Institute of Technology, Harbin, China{sqsun, chenyin, jfli}@mtlab.hit.edu.cnAbstractMachine learning methods have been exten-sively employed in developing MT evaluationmetrics and several studies show that it canhelp to achieve a better correlation with hu-man assessments.
Adopting the regressionSVM framework, this paper discusses the lin-guistic motivated feature formulation strategy.We argue that ?blind?
combination of avail-able features does not yield a general metricswith high correlation rate with human assess-ments.
Instead, certain simple intuitive fea-tures serve better in establishing theregression SVM evaluation model.
With sixfeatures selected, we show evidences to sup-port our view through a few experiments inthis paper.1 IntroductionThe automatic evaluation of machine translation(MT) system has become a hot research issue inMT circle.
Compared with the huge amount ofmanpower cost and time cost of human evaluation,the automatic evaluations have lower cost and re-usability.
Although the automatic evaluation met-rics have succeeded in the system level, there arestill on-going investigations to get reference trans-lation better (Russo-Lassner et al, 2005) or to dealwith sub-document level evaluation (Kulesza et al,2004; Leusch et al 2006).N-grams?
co-occurrence based metrics such asBLEU and NIST can reach a fairly good correla-tion with human judgments, but due to their con-sideration for the capability of generalizationacross multiple languages, they discard the inher-ent linguistic knowledge of the sentence evaluated.Actually, for a certain target language, one couldexploit this knowledge to help us developing amore ?human-like?
metric.
Gim?nez and M?rquez(2007) showed that compared with metrics limitedin lexical dimension, metrics integrating deep lin-guistic information will be more reliable.The introduction of machine learning methodsaimed at the improvement of MT evaluation met-rics?
precision is a recent trend.
Corston-Oliver etal.
(2001) treated the evaluation of MT outputs asclassification problem between human translationand machine translation.
Kulesza et al (2004) pro-posed a SVM classifier based on confidence score,which takes the distance between feature vectorand the decision surface as the measure of the MTsystem?s output.
Joshua S. Albrecht et al (2007)adopted regression SVM to improve the evaluationmetric.In the rest of this paper, we will first discusssome pitfalls of the n-gram based metrics such asBLEU and NIST, together with the intuition thatfactors from the linguist knowledge can be used toevaluate MT system?s outputs.
Then, we will pro-pose a MT evaluation metric based on SVM re-gression using information from various linguisticlevels (lexical level, phrase level, syntax level andsentence-level) as features.
Finally, from empiricalstudies, we will show that this metric, with lesssimple linguistic motivated features, will result in abetter correlation with human judgments than pre-vious regression-based methods.2 N-gram Based vs Linguistic MotivatedMetricsN-gram co-occurrence based metrics is the maintrend of MT evaluation.
The basic idea is to com-pute the similarity between MT system output and25several human reference translations through theco-occurrence of n-grams.
BLEU (Papineni et al,2002) is one of the most popular automatic evalua-tion metrics currently used.
Although with a goodcorrelation with human judgment, it still has somedefects:?
BLEU considers precision regardless of recall.To avoid a low recall, BLEU introduces a brevitypenalty factor, but this is only an approximation.?
Though BLEU makes use of high order n-grams to assess the fluency of a sentence, it doesnot exploit information from inherent structures ofa sentence.?
BLEU is a ?perfect matching only?
metric.This is a serious problem.
Although it can be alle-viated by adding more human reference transla-tions, there may be still a number of informativewords that will be labeled as ?unmatched?.?
BLEU lacks models determining each n-gram?s own contribution to the meaning of the sen-tence.
Correct translations of the headwords whichexpress should be attached more importance tothan that of accessory words e.g.?
While computing geometric average of preci-sions from unigram to n-gram, if a certain preci-sion is zero, the whole score will be zero.In the evaluation task of a MT system with cer-tain target language, the intuition is that we canfully exploit linguistic information, making theevaluation progress more ?human-like?
while leav-ing the capability of generalization across multiplelanguages (just the case that BLEU considers) outof account.Following this intuition, from the plentiful lin-guist information, we take the following factors into consideration:?
Content words are important to the semanticmeaning of a sentence.
A better translation willinclude more substantives translated from thesource sentence than worse ones.
In a similar way,a machine translation should be considered a betterone, if more content words in human referencetranslations are included in it.?
At the phrase level, the situation above re-mains the same, and what is more, real phrases areused to measure the quality of the machine transla-tions instead of merely using n-grams which are oflittle semantic information.?
In addition, the length of translation is usuallyin good proportion to the source language.
We be-lieve that a human reference translation sentencehas a moderate byte-length ratio to the source sen-tence.
So a machine translation will be depreciatedif it has a ratio considerably different from the ratiocalculated from reference sentences.?
Finally, a good translation must be a ?well-formed?
sentence, which usually brings a highprobability score in language models, e.g.
n-grammodel.In the next section, using regression SVM, wewill build a MT evaluation metric for Chinese-English translation with features selected fromabove aspects.3 A Regression SVM Approach Based onLinguistic Motivated FeaturesIntroducing machine learning methods to establishMT evaluation metric is a recent trend.
Providedthat we could get many factors of human judg-ments, machine learning will be a good method tocombine these factors together.
As proved in therecent literature, learning from regression is of abetter quality than from classifier (Albrecht andHwa, 2007; Russo-Lassner et al, 2005; Quirk,2004).
In this paper, we choose regression supportvector machine (SVM) as the learning model.3.1 Learning from human assessment dataThe machine translated sentences for model train-ing are provided with human assessment data scoretogether with several human references.
Each sen-tence is treated as a training example.
We extractfeature vectors from training examples, and humanassessment score will act as the output of the targetfunction.
The regression SVM will generate anapproximated function which maps multi-dimensional feature vectors to a continuous realvalue with a minimal error rate according to a lossfunction.
This value is the result of the evaluationprocess.Figure 1 shows our general framework for re-gression based learning, in which we train theSVM with a number of sentences x1, x2, ?
withhuman assessment scores y1, y2, ?
and use thetrained model to evaluate an test sentence x withfeature vector (f1, f2 ,?, fn).
To determine whichindicators of a sentence are chosen as features isresearch in progress, but we contend that ?the morefeatures, the better quality?
is not always true.Large feature sets require more computation cost,though maybe result in a metric with a better corre-26lation with human judgments, it can also beachieved by introducing a much smaller feature set.Moreover, features may conflict with each others,and bring down the performance of the metric.
Wewill show this in the next section, using less than10 features stated in section 3.2.
Some details ofthe implementation will also be described.Figure 1: SVM based model of automatic MT evalua-tion metric3.2 Feature selectionA great deal of information can be extracted fromthe MT systems?
output using linguistic knowledge.Some of them can be very informative while easyto obtain.As considered in section 2, we choose factorsfrom lexical level, phrase level, syntax level andsentence-level as features to train the SVM.?
Features based on translation quality of con-tent wordsThe motivation is that content words are carry-ing more important information of a sentencecompared with function words.
In this paper, con-tent words include nouns, verbs, adjectives, adver-bials, pronouns and cardinal numerals.
Thecorresponding features are the precision of contentwords defined in Eq.
1 and the recall defined in Eq.2 where ref means reference translation.
( )# _ _ _ _# _ _conprecision tcorrectly translated cons in tcons in t=         (1)( )# _ _ _ _ _ _# _ _ _conrecall tcons in ref correctly translated in tcons in the ref=     (2)?
Features based on cognate words matchingEnglish words have plenty of morphologicalchanges.
So if a machine translation sentenceshares with a human reference sentence some cog-nates, it contains at least some basic informationcorrect.
And if we look at it in another way, wordsthat do not match in the original text maybe matchafter morphological reduction.
Thus, differencesbetween poor translations will be revealed.
Simi-larly, we here define the content word precisionand recall after morphological reduction in Eq.
3and Eq.
4 where mr_cons means content wordsafter morphological reduction:_ ( )# _ _ _ _ _# _ _ _mr conprecision tcorrectly translated mr cons in tmr cons in t=   (3)_ ( )# _ _ _ _ _ _ _# _ _ _ _mr conrecall tmr cons in ref correctly translated in tmr cons in the ref=  (4)?
Features based on translation quality ofphrasesPhrases are baring the weight of semantic in-formation more than words.
In manual evaluation,or rather, in a human?s mind, phrases are paid spe-cial attention to.
Here we parse every sentence1 andextract several types of phrases, then, compute theprecision and recall of each type of phrase accord-ing to Eq.
5 and Eq.
62:tinphrstinphrstranslatedcorrectlytprecisionphr__#____#)( =      (5)reftheinphrtintranslatedcorrectlyrefinphrtrecallphr___#______#)( =    (6)In practice, we found that if we compute thesetwo indicators by matching phrases case-insensitive, we will receive a metric with higherperformance.
We speculate that by doing this thedifference between poor translations is revealedjust like morphological reduction.?
Features based on byte-length ratioGale and Church (1991) noted that he byte-length ratio of target sentence to source sentence isnormally distributed.
We employ this observationby computing the ratio of reference sentences to1 The parser we used is proposed by Michael Collins in Col-lins (1999).2 Only precision and recall of NP are used so far.
Other typesof phrase will be added in future study.MachineTranslation SentenceFeature extractionx = (f1, f2 ,?, fn)Regression SVMy = g(x)Assessmentx2=(f1, f2 ,?, fn), y = y2x1=(f1, f2 ,?, fn), y = y1Training Set?27source sentences, and then calculating the mean cand variance s of this ratio.
So if we take the ratio ras a random variable, (r-c)/s has a normal distribu-tion with mean 0 and variance 1.
Then we computethe same ratio of machine translation sentence tosource sentence, and take the output of p-normfunction as a feature:)__/__()(scsrcoflengthtoflenghtPtf norm?=      (7)?
Features based on parse scoreThe usual practice to model the ?well-formedness?
of a sentence is to employ the n-gramlanguage model or compute the syntactic structuresimilarity (Liu and Gildea 2005).
However, thelanguage model is widely adopted in MT, resultingless discrimination power.
And the present parseris still not satisfactory, leading much noise in parsestructure matching.To avoid these pitfalls in using LM and parser,here we notice that the score of a parse by theparser also reflects the quality of a sentence.
It maybe regarded as a syntactic based language modelscore as well as an approximate representation ofparse structure.
Here we introduce the featurebased on parser?s score as:parserbygiventofmarktscorepaser_____100)(_?=            (8)4 ExperimentsWe use SVM-Light (Joachims 1999) to train ourlearning models.
Our main dataset is NIST?s 2003Chinese MT evaluations.
There are 6?919=5514sentences generated by six systems together withhuman assessment data which contains a fluencyscore and adequacy score marked by two humanjudges.
Because there is bias in the distributions ofthe two judges?
assessment, we normalize thescores following Blatz et al (2003).
The normal-ized score is the average of the sum of the normal-ized fluency score and the normalized adequacyscore.To determine the quality of a metric, we useSpearman rank correlation coefficient which isdistribution-independent between the score givento the evaluative data and human assessment data.The Spearman coefficient is a real number rangingfrom -1 to +1, indicating perfect negative correla-tions or perfect positive correlations.
We take thecorrelation rates of the metrics reported in Albrechtand Hwa (2007) and a standard automatic metricBLEU as a baseline comparison.Among the features described in section 3.2, wefinally adopted 6 features:?
Content words precision and recall after mor-phological reduction defined in Eq.
3 and Eq.
4.?
Noun-phrases?
case insensitive precision andrecall.?
P-norm (Eq.
7) function?s output.?
Rescaled parser score defined in Eq.
8.
Ourfirst experiment will compare the correlation ratebetween metric using rescaled parser score and thatusing parser score directly.4.1 Different kernelsIntuitively, features and the resulting assessmentare not in a linear correlation.
We trained twoSVM, one with linear kernel and the other withGaussian kernel, using NIST 2003 Chinese dataset.Then we apply the two metrics on NIST 2002 Chi-nese Evaluation dataset which has 3?878=2634sentences (3 systems total).
The results are summa-rized in Table 1.
For comparison, the result fromBLEU is also included.Feature Linear Gaussian  BLEURescale 0.320 0.329Direct 0.317 0.2240.244Table 1: Spearman rank-correlation coefficients for re-gression based metrics using linear and Gaussian kernel,and using rescaled parser score or directly the parserscore.
Coefficient for BLEU is also involved.Table 1 shows that the metric with Gaussiankernel using rescaled parser score gains the highestcorrelation rate.
That is to say, Gaussian kernelfunction can capture characteristics of the relationbetter, and rescaling the parser score can help toincrease the correlation with human judgments.Moreover, as other features range from 0 to 1, wecan discover in the second row of Table 1 thatGaussian kernel is suffering more seriously fromthe parser score which is ranging distinctly.
In fol-lowing experiments, we will adopt Gaussian kernelto train the SVM and rescaled parser score as afeature.4.2 Comparisons within the year 2003We held out 1/6 of the assessment dataset for pa-rameter turning, and on the other 5/6 of dataset, weperform a five-fold cross validation to verify themetric?s performance.
In comparison we introduce28several metrics?
coefficients reported in Albrechtand Hwa (2007) including smoothed BLEU (Linand Och, 2004), METEOR (Banerjee and Lavie,2005), HWCM (Liu and Gildea 2005), and the me-tric proposed in Albrecht and Hwa (2007) usingthe full feature set.
The results are summarized inTable 2:Metric CoefficientOur Metric 0.515Albrecht, 2007 0.520Smoothed BLEU 0.272METEOR 0.318HWCM 0.288Table 2: Comparison among various metrics.
Learning-based metrics are developed from NIST 2003 ChineseEvaluation dataset and tested under five-fold cross vali-dation.Compared with reference based metrics such asBLEU, the regression based metrics yield a highercorrelation rate.
Generally speaking, for a givensource sentence, there is usually a lot of feasibletranslations, but reference translations are alwayslimited though this can be eased by adding refer-ences.
On the other hand, regression based metricsis independent of references and make the assess-ment by mapping features to the score, so it canmake a better judgment even dealing with a trans-lation that doesn?t match the reference well.We can also see that our metric which uses only6 features can reach a pretty high correlation ratewhich is close to the metric proposed in Albrechtand Hwa (2007) using 53 features.
That confirmsour speculation that a small feature set can alsoresult in a metric having a good correlation withhuman judgments.4.3 Crossing yearsThough the training set and test set in the experi-ment described above are not overlapping, in thelast, they come from the same dataset (NIST 2003).The content of this dataset are Xinhua news andAFC news from Jan. 2003 to Feb. 2003 which hasan inherent correlation.
To test the capability ofgeneralization of our metric, we trained a metric onthe whole NIST 2003 Chinese dataset (20% dataare held out for parameter tuning) and applied itonto NIST 2002 Chinese Evaluation dataset.
Weuse the same metrics introduced in section 4.2 forcomparison.
The results are summarized in Table 3:Metric CoefficientOur Metric 0.329Albrecht, 2007 0.309Smoothed BLEU 0.269METEOR 0.290HWCM 0.260Table 3: Cross year experiment result.
All the learningbased metrics are developed from NIST 2003.The content of NIST 2002 Chinese dataset isXinhua news and Zaobao?s online news from Mar.2002 to Apr.
2002.
The most remarkable character-istic of news is its timeliness.
News come from theyear 2002 are nearly totally unrelated to that fromthe year 2003.
It can be seen from Table 3 that wehave got the expected results.
Our metric can gen-eralize well across years and yields a better corre-lation with human judgments.4.4 DiscussionsAlbrecht and Hwa (2007) and this paper bothadopted a regression-based learning method.
Infact, the preliminary experiment is strictly set ac-cording to their paper.
The most distinguishingdifference is that the features in Albrecht and Hwa(2007) are collections of existing automatic evalua-tion metrics.
The total 53 features are computa-tionally heavy (for the features from METEOR,ROUGE, HWCM and STM).
In comparison, ourmetric made use of six features coming from lin-guistic knowledge which can be easily obtained.Moreover, the experiments show that our metriccan reach a correlation with human judgmentsnearly as good as the metric described in Albrechtand Hwa (2007), with a much lower computationcost.
And when we applied it to a different year?sdataset, its correlation rate is much better than thatof the metric from Albrecht and Hwa (2007),showing us a good capability of generalization.To account for this, we deem that the regressionmodel is not resistant to data overfiting.
If pro-vided too much cross-dependent features for a lim-ited training data, the model is prone to a lessgeneralized result.
But, it is difficult in practice tolocate those key features in human perception oftranslation quality because we are lack of explicitevidences on what human actually use in transla-tion evaluation.
In such cases, this paper uses only?simple feature in key linguistic aspects?, whichreduces the risk of overfitting and bring a moregeneralized regression results.29Compared with the literature, the ?byte-lengthratio between source and translation?
and the?parse score?
are original in automatic MT evalua-tion modeling.
The parse score is proved to be agood alternative to LM.
And it helps to avoid theerrors of parser in parse structure (the experimentto verify this claim is still on-going).It should be noted that feature selection is ac-complished by empirically exhaustive test on thecombination of the candidate features.
In futurework, we will test if this strategy will help to getbetter results for MT evaluation, e.g.
try-on theselection between the 53 features in Albrecht andHwa (2007).
And, we will also test to see if lin-guistic motivated feature augmentation wouldbring further benefit.5 ConclusionFor the metrics based on regressing, it is not al-ways true that more features and complex featureswill help in performance.
If we choose featureselaborately, simple features are also effective.
Inthis paper we proposed a regression based metricwith a considerably small feature set that yield per-formance of the same level to the metrics with alarge set of 53 features.
And the experiment of thecross-year validation proves that our metric bring amore generalized evaluation results by correlatingwith human judgments better.AcknowledgementsThis research is support by Natural Science Foun-dation of China (Grant No.
60773066) and Na-tional 863 Project (Grant No.
2006AA01Z150)ReferencesJoshua S. Albrecht and Rebecca Hwa.
2007.
A Re-examination of Machine Learning Approaches forSentence-Level MT Evaluation.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics , pages 880-887, Prague,Czech Republic, June.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of the Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarizationat the Association for Computational LinguisticsConference 2005: 65-73.
Ann Arbor, Michigan.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, Alberto San-chis, and Nicola Ueffing.
2003.
Confidence estima-tion for machine translation.
In Technical ReportNatural Language Engineering Workshop Final Re-port, pages 97-100, Johns Hopkins University.Simon Corston-Oliver, Michael Gamon, and ChrisBrockett.
2001.
A machine learning approach to theautomatic evaluation of machine translation.
In Pro-ceedings of the 39th Annual Meeting of the Associa-tion for Computational Linguistics, pages 140-147,Toulouse, France, July.W.
Gale and K. W. Church.
1991.
A Program for Align-ing Sentences in Bilingual Corpora.
In Proceedingsof the 29th Annual Meeting of the Association forComputational Linguistics, pages 177-184, Berkeley.Jes?s Gim?nez and Llu?s M?rquez.
2007.
LinguisticFeatures for Automatic Evaluation of HeterogenousMT Systems.
In Proceedings of the Second Work-shop on Statistical Machine Translation, pages 256-264, Prague, Czech Republic, June.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Sch?elkopf, Christo-pher Burges, and Alexander Smola, editors, Ad-vances in Kernel Methods - Support Vector Learning.MIT Press.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in Ma-chine Translation (TMI), pages 75-84, Baltimore,MD, October.Gregor Leusch, Nicola Ueffing, and Hermann Ney.2006.
CDER: Efficient MT evaluation using blockmovements.
In The Proceedings of the ThirteenthConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 241-248.Chin-Yew Lin & Franz Josef Och.
2004.
AutomaticEvaluation of Machine Translation Quality UsingLongest Common Subsequence and Skip-BigramStatistics.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,pages 606-613, Barcelona, Spain, July.Ding Liu and Daniel Gildea.
2005.
Syntactic featuresfor evaluation of machine translation.
In ACL 2005Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summari-zation, pages 25-32, June.Christopher B. Quirk.
2004.
Training a Sentence-LevelMachine Translation Confidence Measure, In Pro-ceedings of LREC 2004, pages 825-828.Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.2005.
A Paraphrase-Based Approach to MachineTranslation Evaluation.
In Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, Univer-sity of Maryland, College Park, August.30
