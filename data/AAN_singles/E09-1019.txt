Proceedings of the 12th Conference of the European Chapter of the ACL, pages 157?165,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsWeb augmentation of language models for continuous speech recognitionof SMS text messagesMathias Creutz1, Sami Virpioja1,2 and Anna Kovaleva11Nokia Research Center, Helsinki, Finland2Adaptive Informatics Research Centre, Helsinki University of Technology, Espoo, Finlandmathias.creutz@nokia.com, sami.virpioja@tkk.fi, annakov@gmx.deAbstractIn this paper, we present an efficient queryselection algorithm for the retrieval of webtext data to augment a statistical languagemodel (LM).
The number of retrieved rel-evant documents is optimized with respectto the number of queries submitted.The querying scheme is applied in the do-main of SMS text messages.
Continuousspeech recognition experiments are con-ducted on three languages: English, Span-ish, and French.
The web data is utilizedfor augmenting in-domain LMs in generaland for adapting the LMs to a user-specificvocabulary.
Word error rate reductionsof up to 6.6 % (in LM augmentation) and26.0 % (in LM adaptation) are obtained insetups, where the size of the web mixtureLM is limited to the size of the baselinein-domain LM.1 IntroductionAn automatic speech recognition (ASR) systemconsists of acoustic models of speech sounds andof a statistical language model (LM).
The LMlearns the probabilities of word sequences fromtext corpora available for training.
The perfor-mance of the model depends on the amount andstyle of the text.
The more text there is, the betterthe model is, in general.
It is also important thatthe model be trained on text that matches the styleof language used in the ASR application.
Wellmatching, in-domain, text may be both difficultand expensive to obtain in the large quantities thatare needed.A popular solution is to utilize the World WideWeb as a source of additional text for LM train-ing.
A small in-domain set is used as seed data,and more data of the same kind is retrieved fromthe web.
A decade ago, Berger and Miller (1998)proposed a just-in-time LM that updated the cur-rent LM by retrieving data from the web using re-cent recognition hypotheses as queries submittedto a search engine.
Perplexity reductions of up to10 % were reported.1 Many other works have fol-lowed.
Zhu and Rosenfeld (2001) retrieved pageand phrase counts from the web in order to updatethe probabilities of infrequent trigrams that occurin N-best lists.
Word error rate (WER) reductionsof about 3 % were obtained on TREC-7 data.In more recent work, the focus has turned tothe collection of text rather than n-gram statisticsbased on page counts.
More effort has been putinto the selection of query strings.
Bulyko et al(2003; 2007) first extend their baseline vocabularywith words from a small in-domain training cor-pus.
They then use n-grams with these new wordsin their web queries in order to retrieve text of acertain genre.
For instance, they succeed in ob-taining conversational style phrases, such as ?wewere friends but we don?t actually have a relation-ship.?
In a number of experiments, word errorrate reductions of 2-3 % are obtained on Englishdata, and 6 % on Mandarin.
The same method forweb data collection is applied by C?etin and Stolcke(2005) in meeting and lecture transcription tasks.The web sources reduce perplexity by 10 % and4.3 %, respectively, and word error rates by 3.5 %and 2.2 %, respectively.Sarikaya et al (2005) chunk the in-domain textinto ?n-gram islands?
consisting of only contentwords and excluding frequently occurring stopwords.
An island such as ?stock fund portfolio?
isthen extended by adding context, producing ?mystock fund portfolio?, for instance.
Multiple is-lands are combined using and and or operations toform web queries.
Significant word error reduc-tions between 10 and 20 % are obtained; however,the in-domain data set is very small, 1700 phrases,1All reported percentage differences are relative unlessexplicitly stated otherwise.157which makes (any) new data a much needed addi-tion.Similarly, Misu and Kawahara (2006) obtainvery good word error reductions (20 %) in spo-ken dialogue systems for software support andsightseeing guidance.
Nouns that have high tf/idfscores in the in-domain documents are used in theweb queries.
The existing in-domain data setspoorly match the speaking style of the task andtherefore existing dialogue corpora of different do-mains are included, which improves the perfor-mance considerably.Wan and Hain (2006) select query strings bycomparing the n-gram counts within an in-domaintopic model to the corresponding counts in an out-of-domain background model.
Topic-specific n-grams are used as queries, and perplexity reduc-tions of 5.4 % are obtained.It is customary to postprocess and filter thedownloaded web texts.
Sentence boundaries aredetected using some heuristics.
Text chunks with ahigh out-of-vocabulary (OOV) rate are discarded.Additionally, the chunks are often ranked accord-ing to their similarity with the in-domain data, andthe lowest ranked chunks are discarded.
As a sim-ilarity measure, the perplexity of the sentence ac-cording to the in-domain LM can be used; for in-stance, Bulyko et al (2007).
Another measurefor ranking is relative perplexity (Weilhammer etal., 2006), where the in-domain perplexity is di-vided by the perplexity given by an LM trainedon the web data.
Also the BLEU score familiarfrom the field of machine translation has been used(Sarikaya et al, 2005).Some criticism has been raised by Sethy et al(2007), who claim that sentence ranking has aninherent bias towards the center of the in-domaindistribution.
They propose a data selection algo-rithm that selects a sentence from the web set, ifadding the sentence to the already selected set re-duces the relative entropy with respect to the in-domain data distribution.
The algorithm appearsefficient in producing a rather small subset (1/11)of the web data, while degrading the WER onlymarginally.The current paper describes a new method forquery selection and its applications in LM aug-mentation and adaptation using web data.
Thelanguage models are part of a continuous speechrecognition system that enables users to usespeech as an input modality on mobile devices,such as mobile phones.
The particular domain ofinterest is personal communication: The user dic-tates a message that is automatically transcribedinto text and sent to a recipient as an SMS textmessage.
Memory consumption and computa-tional speed are crucial factors in mobile applica-tions.
While most studies ignore the sizes of theLMs when comparing models, we aim at improv-ing the LM without increasing its size when webdata is added.Another aspect that is typically overlooked isthat the collection of web data costs time and com-putational resources.
This applies to the querying,downloading and postprocessing of the data.
Thequery selection scheme proposed in this paper iseconomical in the sense that it strives to downloadas much relevant text from the web as possible us-ing as few queries as possible avoiding overlap be-tween the set of pages found by different queries.2 Query selection and web data retrievalOur query selection scheme involves multiplesteps.
The assumption is that a batch of querieswill be created.
These queries are submitted toa search engine and the matching documents aredownloaded.
This procedure is repeated for multi-ple query batches.In particular, our scheme attempts to maximizethe number of retrieved relevant documents, whentwo restrictions apply: (1) queries are not ?free?
:each query costs some time or money; for in-stance, the number of queries submitted within aparticular period of time is limited, and (2) thenumber of documents retrieved for a particularquery is limited to a particular number of ?tophits?.2.1 N-gram selection and prospectionqueryingSome text reflecting the target domain must beavailable.
A set of the most frequent n-grams oc-curring in the text is selected, from unigrams up tofive-grams.
Some of these n-grams are character-istic of the domain of interest (such as ?HogwartsSchool of Witchcraft and Wizardry?
), others arejust frequent in general (?but they did not say?
);we do not know yet which ones.All n-grams are submitted as queries to the websearch engine.
Exact matches of the n-grams arerequired; different inflections or matches of thewords individually are not accepted.158The search engine returns the total number ofhits h(qs) for each query qsas well as the URLsof a predefined maximum number of ?top hit?
webpages.
The top hit pages are downloaded and post-processed into plain text, from which duplicateparagraphs and paragraphs with a high OOV rateare removed.N-gram language models are then trained sep-arately on the in-domain text and the the filteredweb text.
If the amount of web text is very large,only a subset is used, which consists of the partsof the web data that are the most similar to thein-domain text.
As a similarity measure, relativeperplexity is used.
The LM trained on web data iscalled a background LM to distinguish it from thein-domain LM.2.2 Focused queryingNext, the querying is made more specific and tar-geted on the domain of interest.
New queries arecreated that consist of n-gram pairs, requiring thata document contain two n-grams (?but they did notsay?+?Hogwarts School of Witchcraft and Wiz-ardry?
).2If all possible n-gram pairs are formed fromthe n-grams selected in Section 2.1, the numberof pairs is very large, and we cannot afford usingthem all as queries.
Typical approaches for queryselection include the following: (i) select pairs thatinclude n-grams that are relatively more frequentin the in-domain text than in the background text,(ii) use some extra source of knowledge for select-ing the best pairs.2.2.1 Extra linguistic knowledgeWe first tested the second (ii) query selection ap-proach by incorporating some simple linguisticknowledge: In an experiment on English, querieswere obtained by combining a highly frequent n-gram with a slightly less frequent n-gram that hadto contain a first- or second-person pronoun (I,you, we, me, us, my, your, our).
Such n-gramswere thought to capture direct speech, which ischaracteristic for the desired genre of personalcommunication.
(Similar techniques are reportedin the literature cited in Section 1.
)Although successful for English, this scheme ismore difficult to apply to other languages, whereperson is conveyed as verbal suffixes rather thansingle words.
Linguistic knowledge is needed for2Higher order tuples could be used as well, but we haveonly tested n-gram pairs.every language, and it turns out that many of thequeries are ?wasted?, because they are too specificand return only few (if any) documents.2.2.2 Statistical approachThe other proposed query selection technique (i)allows for an automatic identification of the n-grams that are characteristic of the in-domaingenre.
If the relative frequency of an n-gram ishigher in the in-domain data than in the back-ground data, then the n-gram is potentially valu-able.
However, as in the linguistic approach, thereis no guarantee that queries are not wasted, sincethe identified n-gram may be very rare on the In-ternet.
Pairing it with some other n-gram (whichmay also be rare) often results in very few hits.To get out the most of the queries, we pro-pose a query selection algorithm that attempts tooptimize the relevance of the query to the targetdomain, but also takes into account the expectedamount of data retrieved by the query.
Thus, thepotential queries are ranked according to the ex-pected number of retrieved relevant documents.Only the highest ranked pairs, which are likely toproduce the highest number of relevant web pages,are used as queries.We denote queries that consist of two n-gramss and t by qs?t.
The expected number of retrievedrelevant documents for the query qs?tis r(qs?t):r(qs?t) = n(qs?t) ?
?
(qs?t|Q), (1)where n(qs?t) is the expected number of retrieveddocuments for the query, and ?
(qs?t|Q) is the ex-pected proportion of relevant documents within alldocuments retrieved by the query.
The expectedproportion of relevant documents is a value be-tween zero and one, and as explained below, it isdependent on all past queries, the query history Q.Expected number of retrieved documentsn(qs?t).
From the prospection querying phase(Section 2.1), we know the numbers of hits forthe single n-grams s and t, separately: h(qs) andh(qt).
We make the operational, but overly simpli-fying, assumption that the n-grams occur evenlydistributed over the web collection, independentlyof each other.
The expected size of the intersectionqs?tis then:h?
(qs?t) =h(qs) ?
h(qt)N, (2)where N is the size of the web collection that ourn-gram selection covers (total number of docu-159ments).
N is not known, but different estimatescan be used, for instance, N = max?qsh(qs),where it is assumed that the most frequent n-gramoccurs in every document in the collection (prob-ably an underestimate of the actual value).Ideally, the expected number of retrieved doc-uments equals the expected number of hits, butsince the search engine returns a limited maximumnumber of ?top hit?
pages, M , we get:n(qs?t) = min(h?(qs?t),M).
(3)Expected proportion of relevant documents?(qs?t|Q).
As in the case of n(qs?t), an inde-pendence assumption can be applied in the deriva-tion of the expected proportion of relevant docu-ments for the combined query qs?t: We simplyput together the chances of obtaining relevant doc-uments by the single n-gram queries qsand qtin-dividually.
The union equals:?
(qs?t|Q) =1 ?
(1 ?
?(qs|Q))?
(1 ?
?(qt|Q)).
(4)However, we do not know the values for?
(qs|Q) and ?(qt|Q).
As mentioned earlier, it isstraightforward to obtain a relevance ranking for aset of n-grams: For each n-gram s, the LM prob-ability is computed using both the in-domain andthe background LM.
The in-domain probability isdivided by the background probability and the n-grams are sorted, highest relative probability first.The first n-gram is much more prominent in thein-domain than the background data, and we wishto obtain more text with this crucial n-gram.
Theopposite is true for the last n-gram.We need to transform the ranking into ?(?)
val-ues between zero and one.
There is no absolute di-vision into relevant and irrelevant documents fromthe point of view of LM training.
We use a proba-bilistic query ranking scheme, such that we definethat of all documents containing an x% relevantn-gram, x% are relevant.
When the n-grams havebeen ranked into a presumed order of relevance,we decide that the most relevant n-gram is 100 %relevant and the least relevant n-gram is 0 % rele-vant; finally, we scale the relevances of the othern-grams according to rank.When scoring the remaining n-grams, linearscaling is avoided, because the majority of the n-grams are irrelevant or neutral with respect to ourdomain of interest, and many of them would ob-tain fairly high relevance values.
Instead, we fixthe relevance value of the ?most domain-neutral?n-gram (the one with the relative probability valueclosest to one); we might assume that only 5 % ofall documents containing this n-gram are indeedrelevant.
We then fit a polynomial curve throughthe three points with known values (0, 0.05, and 1)to get the missing ?(?)
values for all qs.Decay factor ?
(s |Q).
We noticed that if con-stant relevance values are used, the top rankedqueries will consist of a rather small set of topranked n-grams that are paired with each other inall possible combinations.
However, it is likelythat each time an n-gram is used in a query, theneed for finding more occurrences of this partic-ular n-gram decreases.
Therefore, we introduceda decay factor ?
(s |Q), by which the initial ?(?
)value, written ?0(qs), is multiplied:?
(qs|Q) = ?0(qs) ?
?
(s |Q), (5)The decay is exponential:?
(s |Q) = (1 ?
)P?s?Q1.
(6) is a small value between zero and one (for in-stance 0.05), and ?
?s?Q1 is the number of timesthe n-gram s has occurred in past queries.Overlap with previous queries.
Some queriesare likely to retrieve the same set of documentsas other queries.
This occurs if two queries shareone n-gram and there is strong correlation be-tween the second n-grams (for instance, ?we wishyou?+?Merry Christmas?
vs. ?we wish you?+?and a Happy New Year?).
In principle, when as-sessing the relevance of a query, one should esti-mate the overlap of that query with all past queries.We have tested an approximate solution that al-lows for fast computing.
However, the real effectof this addition was insignificant, and a further de-scription is omitted in this paper.Optimal order of the queries.
We want to max-imize the expected number of retrieved relevantdocuments while keeping the number of submittedqueries as low as possible.
Therefore we sort thequeries best first and submit as many queries wecan afford from the top of the list.
However, therelevance of a query is dependent on the sequenceof past queries (because of the decay factor).
Find-ing the optimal order of the queries takes O(n2)operations, if n is the total number of queries.A faster solution is to apply an iterative algo-rithm: All queries are put in some initial order.
For160each query, its r(qs?t) value is computed accord-ing to Equation 1.
The queries are then rearrangedinto the order defined by the new r(?)
values, bestfirst.
These two steps are repeated until conver-gence.Repeated focused querying.
Focused queryingcan be run multiple times.
Some ten thousands ofthe top ranked queries are submitted to the searchengine and the documents matching the queriesare downloaded.
A new background LM is trainedusing the new web data, and a new round of fo-cused querying can take place.2.2.3 Comparison of the linguistic andstatistical focused querying schemesOn one language (German), the statical focusedquerying algorithm (Section 2.2.2) was shownto retrieve 50 % more unique web pages and70 % more words than the linguistic scheme (Sec-tion 2.2.1) for the same number of queries.
Alsoresults from language modeling and speech recog-nition experiments favored statistical querying.2.3 Web collections obtainedFor the speech recognition experiments describedin the current paper, we have collected web textsfor three languages: US English, European Span-ish, and Canadian French.As in-domain data we used 230,000 Englishtext messages (4 million words), 65,000 Spanishmessages (2 million words), and 60,000 Frenchmessages (1 million words).
These text messageswere obtained in data collection projects involvingthousand of participants, who used a web interfaceto enter messages according to different scenariosof personal communication situations.3 A few ex-ample messages are shown in Figure 1.The queries were submitted to Yahoo!
?s websearch engine.
The web pages that were retrievedby the queries were filtered and cleaned and di-vided into chunks consisting of single paragraphs.For English, we obtained 210 million paragraphsand 13 billion words, for Spanish 160 millionparagraphs and 12 billion words, and for French44 million paragraphs and 3 billion words.3Real messages sent from mobile phones would be thebest data, but are hard to get because of privacy protection.The postprocessing of authentic messages would, however,require proper handling of artifacts resulting from the limitedinput capacities on keypads of mobile devices, such as spe-cific acronyms: i?ll c u l8er.
In our setup, we did not have toface such issues.I hope you have a long and happy marriage.Congratulations!Remember to pick up Billy at practice at fiveo?clock!Hey Eric, how was the trip with the kids overwinter vacation?
Did you go to Texas?Figure 1: Example text messages (US English).The linguistic focused querying method was ap-plied in the US English task (because the statisti-cal method did not yet exist).
The Spanish andCanadian French web collections were obtainedusing statistical querying.
Since the French setwas smaller than the other sets (?only?
3 billionwords), web crawling was performed, such thatthose web sites that had provided us with the mostvaluable data (measured by relative perplexity)were downloaded entirely.
As a result, the num-ber of paragraphs increased to 110 million and thenumber of words to 8 billion.3 Speech Recognition ExperimentsWe have trained language models on the in-domain data together with web data, and thesemodels have been used in speech recognition ex-periments.
Two kinds of experiments have beenperformed: (1) the in-domain LM is augmentedwith web data, and (2) the LM is adapted to a user-specific vocabulary utilizing web data as an addi-tional data source.One hundred native speakers for each languagewere recorded reading held-out subsets of the in-domain text data.
The speech data was partitionedinto training and test sets, such that around onefourth of the speakers were reserved for testing.We use a continuous speech recognizer opti-mized for low memory footprint and fast recog-nition (Olsen et al, 2008).
The recognizerruns on a server (Core2 2.33 GHz) in aboutone fourth of real time.
The LM probabilitiesare quantized and precompiled together with thespeaker-independent acoustic models (intra-wordtriphones) into a finite state transducer (FST).3.1 Language model augmentationEach paragraph in the web data is treated as a po-tential text message and scored according to itssimilarity to the in-domain data.
Relative perplex-ity is used as the similarity measure.
The para-graphs are sorted, lowest relative perplexity first,161US EnglishFST size [MB] 10 20 40 70In-domain 42.7 40.1 39.1 ?Web mixture 42.0 37.6 35.7 33.8Ppl reduction [%] 1.6 6.2 8.7 13.6European SpanishFST size [MB] 10 20 25 40In-domain 68.0 64.6 64.3 ?Web mixture 63.9 58.4 55.0 52.1Ppl reduction [%] 6.0 9.6 14.5 19.0Canadian FrenchFST size [MB] 10 20 25 50In-domain 57.6 ?
?
?Web mixture 51.7 47.9 45.9 44.6Ppl reduction [%] 10.2 16.8 20.3 22.6Table 1: Perplexities.In the tables, the perplexity and word error rate reductions of the web mixtures are computed withrespect to the in-domain models of the same size, if such models exist; otherwise the comparison ismade to the largest in-domain model available.and the highest ranked paragraphs are used as LMtraining data.
The optimal size of the set dependson the test, but the largest chosen set contains 15million paragraphs and 500 million words.Separate LMs are trained on the in-domain dataand web data.
The two LMs are then linearlyinterpolated into a mixture model.
Roughly thesame interpolation weights (0.5) are obtained forthe LMs, when the optimal value is chosen basedon a held-out in-domain development test set.3.1.1 Test set perplexitiesIn Table 1, the prediction abilities of the in-domainand web mixture language models are compared.As an evaluation measure we use perplexity cal-culated on test sets consisting of in-domain text.The comparison is performed on FSTs of differ-ent sizes.
The FSTs contain the acoustic models,language model and lexicon, but the LM makes upfor most of the size.
The availability of data variesfor the different languages, and therefore the FSTsizes are not exactly the same across languages.The LMs have been created using the SRI LMtoolkit (Stolcke, 2002).
Good-Turing smoothingwith Katz backoff (Katz, 1987) has been used, andthe different model sizes are obtained by pruningdown the full models using entropy-based prun-ing (Stolcke, 1998).
N-gram orders up to five havebeen tested: 5-grams always work best on the mix-US EnglishFST size [MB] 10 20 40 70In-domain 17.9 17.5 17.3 ?Web mixture 17.5 16.7 16.4 15.8WER reduction 2.2 4.4 5.2 8.4European SpanishFST size [MB] 10 20 25 40In-domain 18.9 18.7 18.6 ?Web mixture 18.7 17.9 17.4 16.8WER reduction 1.4 4.1 6.6 9.7Canadian FrenchFST size [MB] 10 20 25 50In-domain 22.6 ?
?
?Web mixture 22.1 21.7 21.3 20.9WER reduction 2.3 4.1 5.8 7.5Table 2: Word error rates [%].ture models, whereas the best in-domain modelsare 4- or 5-grams.For every language and model size, the webmixture model performs better than the corre-sponding in-domain model.
The perplexity reduc-tions obtained increase with the size of the model.Since it is possible to create larger mixture mod-els than in-domain models, there are no in-domainresults for the largest model sizes.Especially if large models can be afforded, theperplexity reductions are considerable.
The largestimprovements are observed for French (between10.2 % and 22.6 % relative).
This is not surprising,as the French in-domain set is the smallest, whichleaves much room for improvement.3.1.2 Word error ratesSpeech recognition results for the different LMsare given in Table 2.
The results are consistent inthe sense that the web mixture models outperformthe in-domain models, and augmentation helpsmore with larger models.
The largest word errorrate reduction is observed for the largest Span-ish model (9.7 % relative).
All WER reductionsare statistically significant (one-sided Wilcoxonsigned-rank test; level 0.05) except the 10 MBSpanish setup.Although the observed word error rate reduc-tions are mostly smaller than the corresponding162perplexity reductions, the results are actually verygood, when we consider the fact that consider-able reductions in perplexity may typically trans-late into meager word error reductions; see, for in-stance, Rosenfeld (2000), Goodman (2001).
Thissuggests that the web texts are very welcome com-plementary data that improve on the robustness ofthe recognition.3.1.3 Modified Kneser-Ney smoothingIn the above experiments, Good-Turing (GT)smoothing with Katz backoff was used, althoughmodified Kneser-Ney (KN) interpolation has beenshown to outperform other smoothing methods(Chen and Goodman, 1999).
However, as demon-strated by Siivola et al (2007), KN smoothingis not compatible with simple pruning methodssuch as entropy-based pruning.
In order to makea meaningful comparison, we used the revisedKneser pruning and Kneser-Ney growing tech-niques proposed by Siivola et al (2007).
For thethree languages, we built KN models that resultedin FSTs of the same sizes as the largest GT in-domain models.
The perplexities decreased 4?8%,but in speech recognition, the improvements weremostly negligible: the error rates were 17.0 for En-glish, 18.7 for Spanish, and 22.5 for French.For English, we also created web mixture mod-els with KN smoothing.
The error rates were 16.5,15.9 and 15.7 for the 20 MB, 40 MB and 70 MBmodels, respectively.
Thus, Kneser-Ney outper-formed Good-Turing, but the improvements weresmall, and a statistically significant difference wasmeasured only for the 40 MB LMs.
This was ex-pected, as it has been observed before that verysimple smoothing techniques can perform well onlarge data sets, such as web data (Brants et al,2007).For the purpose of demonstrating the usefulnessof our web data retrieval system, we concludedthat there was no significant difference betweenGT and KN smoothing in our current setup.3.2 Language model adaptationIn the second set of experiments we envisage asystem that adapts to the user?s own vocabulary.Some words that the user needs may not be in-cluded in the built-in vocabulary of the device,such as names in the user?s contact list, names ofplaces or words related to some specific hobby orother focus of interest.Two adaptation techniques have been tested:(1) Unigram adaptation is a simple technique, inwhich user-specific words (for instance, namesfrom the contact list) are added to the vocabulary.No context information is available, and thus onlyunigram probabilities are created for these words.
(2) In message adaptation, the LM is augmentedselectively with paragraphs of web data that con-tain user-specific words.
Now, higher order n-grams can be estimated, since the words occurwithin passages of running text.
This idea is notnew: information retrieval has been suggested as asolution by Bigi et al (2004) among others.In our message adaptation, we have not createdweb queries dynamically on demand.
Instead, weused the large web collections described in Sec-tion 2.3, from which we selected paragraphs con-taining user-specific words.
We have tested bothadaptation by pooling (adding the paragraphs tothe original training data), and adaptation by in-terpolation (using the new data to train a sepa-rate LM, which is interpolated with the originalLM).
One million words from the web data wereselected for each language.
The adaptation wasthought to take place off-line on a server.3.2.1 Data setsFor each language, the adaptation takes place ontwo baseline models, which are the in-domainand web mixture LMs of Section 3.1; however,the amount of in-domain training data is reducedslightly (as explained below).In order to evaluate the success of the adapta-tion, a simulated user-specific test set is created.This set is obtained by selecting a subset of alarger potential test set.
Words that occur both inthe training set and the potential test set and thatare infrequent in the training set are chosen as theuser-specific vocabulary.
For Spanish and French,a training set frequency threshold of one is used,resulting in 606 and 275 user-specific words, re-spectively.
For English the threshold is 5, whichresults in 99 words.
All messages in the potentialtest set containing any of these words are selectedinto the user-specific test set.
Any message con-taining user-specific words is removed from thein-domain training set.
In this manner, we obtaina test set with a certain over-representation of aspecific vocabulary, without biasing the word fre-quency distribution of the training set to any no-ticeable degree.For comparison, performance is additionallycomputed on a generic in-domain test set, as be-163US English, 23 MB modelsModel WER (reduction)user-specific in-domainIn-domain 29.1 (?)
17.9 (?
)+unigram adapt.
24.4 (16.3) 17.1 (4.7)+message adapt.
21.6 (26.0) 16.8 (6.0)Web mixture 25.7 (11.8) 16.9 (5.9)+unigram adapt.
23.1 (20.6) 16.3 (8.8)+message adapt.
22.2 (23.8) 16.4 (8.5)European Spanish, 23 MB modelsModel WER (reduction)user-specific in-domainIn-domain 25.3 (?)
18.6 (?
)+unigram adapt.
23.4 (7.7) 18.5 (0.3)+message adapt.
21.7 (14.4) 18.0 (3.2)Web mixture 21.9 (13.7) 17.5 (5.8)+unigram adapt.
21.5 (15.3) 17.7 (5.0)+message adapt.
21.2 (16.5) 17.7 (4.7)Canadian French, 21 MB modelsModel WER (reduction)user-specific in-domainIn-domain 30.3 (?)
22.6 (?
)+unigram adapt.
28.3 (6.4) 22.5 (0.4)+message adapt.
26.6 (12.1) 22.2 (1.8)Web mixture 26.7 (11.8) 21.4 (5.1)+unigram adapt.
26.0 (14.3) 21.4 (5.4)+message adapt.
26.0 (14.2) 21.6 (4.3)Table 3: Adaptation, word error rates [%].
Sixmodels have been evaluated on two types of testsets: a user-specific test set with a higher numberof user-specific words and a generic in-domain testset.
The numbers in brackets are relative WER re-ductions [%] compared to the in-domain model.WER values for the unigram adaptation are ren-dered in italics, if the improvement obtained is sta-tistically significant compared to the correspond-ing non-adapted model.
WER values for the mes-sage adaptation are in italics, if there is a statisti-cally significant reduction with respect to unigramadaptation.fore.
User-specific and generic development testsets are used for the estimation of optimal interpo-lation weights.3.2.2 ResultsThe adaptation experiments are summarized in Ta-ble 3.
Only medium sized FSTs (21?23 MB)have been tested.
The two baseline models havebeen adapted using the simple unigram reweight-ing scheme and using selective web message aug-mentation.
For the in-domain baseline, poolingworks the best, that is, adding the web messagesto the original in-domain training set.
For the webmixture baseline, a mixture model is the only op-tion; that is, one more layer of interpolation isadded.In the adaptation of the in-domain LMs, mes-sage selection is almost twice as effective as uni-gram adaptation for all data sets.
Also the perfor-mance on the generic in-domain test set is slightlyimproved, because more training data is available.Except for English, the best results on the user-specific test sets are produced by the adaptation ofthe web mixture models.
The benefit of using mes-sage adaptation instead of simple unigram adapta-tion is smaller when we have a web mixture modelas a baseline rather than an in-domain-only LM.On the generic test sets, the adaptation of theweb mixture makes a difference only for English.Since there were practically no singleton wordsin the English in-domain data, the user-specificvocabulary consists of words occurring at mostfive times.
Thus, the English user-specific wordsare more frequent than their Spanish and Frenchequivalents, which shows in larger WER reduc-tions for English in all types of adaptation.4 Discussion and conclusionMobile applications need to run in small memory,but not much attention is usually paid to memoryconsumption in related LM work.
We have shownthat LM augmentation using web data can be suc-cessful, even when the resulting mixture model isnot allowed to grow any larger than the initial in-domain model.
Yet, the benefit of the web data islarger, the larger model can be used.The largest WER reductions were observed inthe adaptation to a user-specific vocabulary.
Thiscan be compared to Misu and Kawahara (2006),who obtained similar accuracy improvements withclever selection of web data, when there was ini-tially no in-domain data available with both thecorrect topic and speaking style.We used relative perplexity ranking to filter thedownloaded web data.
More elaborate algorithmscould be exploited, such as the one proposed bySethy et al (2007).
Initially, we have experi-mented along those lines, but it did not pay off;maybe future refinements will be more successful.164ReferencesAdam Berger and Robert Miller.
1998.
Just-in-timelanguage modeling.
In In ICASSP-98, pages 705?708.Brigitte Bigi, Yan Huang, and Renato De Mori.
2004.Vocabulary and language model adaptation using in-formation retrieval.
In Proc.
Interspeech 2004 ?
IC-SLP, pages 1361?1364, Jeju Island, Korea.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 858?867.Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.2003.
Getting more mileage from web text sourcesfor conversational speech language modeling usingclass-dependent mixtures.
In NAACL ?03: Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Lin-guistics on Human Language Technology, pages 7?9, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng,Andreas Stolcke, and ?Ozgu?r C?etin.
2007.
Webresources for language modeling in conversationalspeech recognition.
ACM Trans.
Speech Lang.
Pro-cess., 5(1):1?25.
?Ozgu?r C?etin and Andreas Stolcke.
2005.
Lan-guage modeling in the ICSI-SRI spring 2005 meet-ing speech recognition evaluation system.
TechnicalReport 05-006, International Computer Science In-stitute, Berkeley, CA, USA, July.S.
F. Chen and J. Goodman.
1999.
An empiricalstudy of smoothing techniques for language model-ing.
Computer Speech and Language, 13:359?394.Joshua T. Goodman.
2001.
A bit of progress in lan-guage modeling.
Computer Speech and Language,15:403?434.Slava M. Katz.
1987.
Estimation of probabilitiesfrom sparse data for the language model compo-nent of a speech recognizer.
IEEE Transactionson Acoustics, Speech and Signal Processing, ASSP-35(3):400?401, March.Teruhisa Misu and Tatsuya Kawahara.
2006.
A boot-strapping approach for developing language modelof new spoken dialogue systems by selecting webtexts.
In Proc.
INTERSPEECH ?06, pages 9?13,Pittsburgh, PA, USA, September, 17?21.Jesper Olsen, Yang Cao, Guohong Ding, and XinxingYang.
2008.
A decoder for large vocabulary contin-uous short message dictation on embedded devices.In Proc.
ICASSP 2008, Las Vegas, Nevada.Ronald Rosenfeld.
2000.
Two decades of languagemodeling: Where do we go from here?
Proceedingsof the IEEE, 88(8):1270?1278.Ruhi Sarikaya, Augustin Gravano, and Yuqing Gao.2005.
Rapid language model development using ex-ternal resources for new spoken dialog domains.
InProc.
IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP ?05), vol-ume I, pages 573?576.Abhinav Sethy, Shrikanth Narayanan, and BhuvanaRamabhadran.
2007.
Data driven approach for lan-guage model adaptation using stepwise relative en-tropy minimization.
In Proc.
IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing (ICASSP ?07), volume IV, pages 177?180.Vesa Siivola, Teemu Hirsima?ki, and Sami Virpi-oja.
2007.
On growing and pruning Kneser-Ney smoothed n-gram models.
IEEE Transac-tions on Audio, Speech and Language Processing,15(5):1617?1624.A.
Stolcke.
1998.
Entropy-based pruning of backofflanguage models.
In Proc.
DARPA BNTU Work-shop, pages 270?274, Lansdowne, VA, USA.A.
Stolcke.
2002.
SRILM ?
an extensiblelanguage modeling toolkit.
In Proc.
ICSLP,pages 901?904.
http://www.speech.sri.com/projects/srilm/.Vincent Wan and Thomas Hain.
2006.
Strategies forlanguage model web-data collection.
In Proc.
IEEEInternational Conference on Acoustics, Speech, andSignal Processing (ICASSP ?06), volume I, pages1069?1072.Karl Weilhammer, Matthew N. Stuttle, and SteveYoung.
2006.
Bootstrapping language models fordialogue systems.
In Proc.
INTERSPEECH 2006- ICSLP Ninth International Conference on Spo-ken Language Processing, Pittsburgh, PA, USA,September 17?21.Xiaojin Zhu and R. Rosenfeld.
2001.
Improving tri-gram language modeling with the world wide web.In Proc.
IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP ?01).,volume 1, pages 533?536.165
