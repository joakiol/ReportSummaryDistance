Search Algorithms for Software-Only Real-Time Recognitionwith Very Large VocabulariesLong Nguyen, Richard Schwartz, Francis Kubala, Paul PlacewayBBN Systems & Techno log ies70 Fawcet t  Street, Cambr idge ,  MA 02138ABSTRACTThis paper deals with search algorithms for real-time speech recog-nition.
We argue that software-only speech recognition has severalcritical advantages over using special or parallel hardware.
Wepresent a history of several advances in search algorithms, whichtogether, have made it possible to implement real-time recogni-tion of large vocabularies on a single workstation without he needfor any hardware accelerators.
We discuss the Forward-BackwardSearch algorithm in detail, as this is the key algorithm that hasmade possible recognition of very large vocabularies in real-time.The result is that we can recognize continuous speech with a vocab-ulary of 20,000 words strictly in real-time ntirely in software ona high-end workstation with large memory.
We demonstrate hatthe computation needed grows as the cube root of the vocabularysize.1.
IntroductionThe statistical approach to speech recognition requires thatwe compare the incoming speech signal to our model ofspeech and choose as our recognized sentence that wordstring that has the highest probability, given our acousticmodels of speech and our statistical models of language.The required computation is fairly large.
When we realizedthat we needed to include a model of understanding, ourestimate of the computational requirement was increased,because we assumed that it was necessary for all of theknowledge sources in the speech recognition search to betightly coupled.Over the years DARPA has funded major programs inspecial-purpose VLSI and parallel computing environmentsspecifically for speech recognition, because it was taken forgranted that this was the only way that real-time speechrecognition would be possible.
However, these directionsbecame major efforts in themselves.
Using a small num-ber of processors in parallel was easy, but efficient use ofa large number of processors required a careful redesign ofthe recognition algorithms.
By the time high efficiency wasobtained, there were often faster uniprocessors available.Design of special-purpose VLSI obviously requires consid-erable effort.
Often by the time the design is completed, thealgorithms implemented are obsolete and much faster gen-eral purpose processors are available in workstations.
Theresult is that neither of these approaches has resulted in real-time recognition with vocabularies of 1,000 words or more.Another approach to the speech recognition search problemis to reduce the computation needed by changing the searchalgorithm.
For example, IBM has developed a flexible stack-based search algorithm and several fast match algorithmsthat reduce the search space by quickly eliminating a largefraction of the possible words at each point in the search.In 1989 we, at BBN \[1\], and others \[2, 3\] developed theN-best Paradigm, in which we use a powerful but inexpen-sive model for speech to find the top N sentence hypothesesfor an utterance, and then we rescore ach of these hypothe-ses with more complex models.
The result was that thehuge search space described by the complex models couldbe avoided, since the space was constrained to the list ofN hypotheses.
Even so, an exact algorithm for the N-bestsentence hypotheses required about 100 times more com-putation than the simple Viterbi search for the most likelysentence.In 1990 we realized that we could make faster advancesin the algorithms using off-the-shelf hardware than by us-ing special hardware.
Since then we have gained orders ofmagnitude in speed in a short time by changing the searchalgorithms in some fundamental ways, without the need foradditional or special hardware other than a workstation.
Thishas resulted in a major paradigm shift.
We no longer thinkin terms of special-purpose hardware - we take it for grantedthat recognition of any size problem will be possible with asoftware-only solution.There are several obvious advantages to software-based rec-ognizers: greater flexibility, lower cost, and the opportunityfor large gains in speed due to clever search algorithms.1.
Since the algorithms are in a constant state of flux,any special-purpose hardware is obsolete before it isfinished.2.
Software-only systems are key to making the technol-ogy broadly usable.- Many people will simply not purchase xtra hardware.- Integration is much easier.91- 'Iqae systems are more flexible.3.
For those people who already have workstations, oft-ware is obviously less expensive.4.
Most importantly, it is possible to obtain much largergains in speed ue to clever search algorithms than fromfaster hardware.We have previously demonstrated real-time software-onlyrecognition for the ATIS task with over 1,000 words.
Morerecently, we have developed new search algorithms that per-form recognition of 20,000 words with fully-connnected bi-gram and trigram statistical grammars in strict real-time withlittle loss in recognition accuracy relative to research levels.First, we will very briefly review some of the search algo-rithms that we have developed.
Then we will explain howthe Forward-Backward Search can be used to achieve real-time 20,000-word continuous peech recognition.2.
Previous Algor i thmsThe two most commonly used algorithms for speech recog-nition search are the time-synchronous beam search \[4\] andthe best-first stack search \[5\].
(We do not consider "island-driven" searches here, since they have not been shown to beeffective.)2.1.
T ime-Synchronous  SearchIn the time-synchronous Viterbi beam search, all the statesof the model are updated in lock step frame-by-frame asthe speech is processed.
The computation required for thissimple method is proportional to the number of states in themodel and the number of frames in the input.
If we discardany state whose score is far below the highest score in thatframe we can reduce the computation by a large factor.There are two important advantages of a time-synchronoussearch.
First, it is necessary that the search be time-synchronous in order for the computation to be finished atthe same time that the speech is finished.
Second, sinceall of the hypotheses are of exactly the same length, it ispossible to compare the scores of different hypotheses in or-der to discard most hypotheses.
This technique is called thebeam search.
Even though the beam search is not theoreti-cally admissible, it is very easy to make it arbitrarily closeto optimal simply by increasing the size of the beam.
Thecomputational properties are fairly well-behaved with minordifferences in speech quality.One minor disadvantage of the Viterbi search is that it findsthe state sequence with the highest probability rather thanthe word sequence with the highest probability.
This is onlya minor disadvantage b cause the most likely state sequencehas been empirically shown to be highly correlated to themost likely word sequence.
(We have shown in \[6\] that aslight modification to the Viterbi computation removes thisproblem, albeit with a slight approximation.
When two pathscome to the same state at the same time, we add the prob-abilities instead of taking the maximum.)
A much moreserious problem with the time-synchronous search is that itmust follow a very large number of theories in parallel eventhough only one of them will end up scoring best.
This canbe viewed as wasted computation.We get little benefit from using a fast match algorithm withthe time-synchronous search because we consider starting allpossible words at each frame.
Thus, it would be necessaryto run the fast match algorithm at each frame, which wouldbe too expensive for all but the least expensive of fast matchalgorithms.2.2.
Best-First Stack SearchThe true best-first search keeps a sorted stack of the highestscoring hypotheses.
At each iteration, the hypothesis withthe highest score is advanced by all possible next words,which results in more hypotheses on the stack.
The best-firstsearch has the advantage that it can theoretically minimizethe number of hypotheses considered if there is a good func-tion to predict which theory to follow next.
In addition, itcan take very good advantage of a fast match algorithm atthe point where it advances the best hypothesis.The main disadvantage is that there is no guarantee as towhen the algorithm will finish, since it may keep backingup to shorter theories when it hits a part of the speech thatdoesn't match well.
In addition it is very hard to comparetheories of different length.2.3.
Pseudo T ime-Synchronous  Stack  SearchA compromise between the strict time-synchronous searchand the best-first stack search can be called the Pseudo Time-Synchronous Stack Search.
In this search, the shortest hy-pothesis (i.e.
the one that ends earliest in the signal) isupdated first.
Thus, all of the active hypotheses are withina short time delay of the end of the speech signal.
To keepthe algorithm from requiring exponential time, a beam-typepruning is applied to all of the hypotheses that end at thesame time.
Since this method advances one hypothesis ata time, it can take advantage of a powerful fast match al-gorithm.
In addition, it is possible to use a higher orderlanguage model without he computation growing with thenumber of states in the language model.922.4.
N-best ParadigmThe N-best Paradigm was introduced in 1989 as a way tointegrate speech recognition with natural anguage process-ing.
Since then, we have found it to be useful for applyingthe more expensive speech knowledge sources as well, suchas cross-word models, tied-mixture densities, and trigrarnlanguage models.
We also use it for parameter and weightoptimization.
The N-best Paradigm is a type of fast matchat the sentence level.
This reduces the search space to ashort list of likely whole-sentence hypotheses.The Exact N-best Algorithm \[1\] has the side benefit hat itis also the only algorithm that guarantees finding the mostlikely sequence of words.
Theoretically, the computationrequired for this algorithm cannot be proven to be less thanexponential with the length of the utterance.
However, thiscase only exists when all the models of all of the phonemesand words are identical (which would present a more seri-ous problem than large computation).
In practice, we findthat the computation required can be made proportional tothe number of hypotheses desired, by the use of techniquessimilar to the beam search.Since the development of the exact algorithm, there havebeen several approximations developed that are much faster,with varying degrees of accuracy \[2, 3, 7, 8\].
The mostrecent algorithm \[9\] empirically retains the accuracy of theexact algorithm, while requiring little more computation thanthat of a simple 1-best search.The N-best Paradigm has the potential problem that if aknowledge source is not used to find the N-best hypothe-ses, the answer that would ultimately have the highest scoreincluding this knowledge source may be missing from thetop N hypotheses.
This becomes more likely as the errorrate becomes higher and the utterances become longer.
Wehave found empirically that this problem does not occur forsmaller vocabularies, but it does occur when we use vocab-ularies of 20,000 words and trigram language models in therescoring pass.This problem can be avoided by keeping the lattice of allsentence hypotheses generated by the algorithm, rather thanenumerating independent sentence hypotheses.
Then the lat-tice is treated as a grammar and used to rescore all the hy-potheses with the more powerful knowledge sources \[10\].2.5.
Forward-Backward Search ParadigmThe Forward-Backward Search algorithm is a generalparadigm in which we use some inexpensive approximatetime-synchronous search in the forward direction to speed upa more complex search in the backwards direction.
This al-gorithm generally results in tw o orders of magnitude speedupfor the backward pass.
Since it was the key mechanism thatmade it possible to perform recognition with a 20,000-wordvocabulary in real time, we discuss it in more detail in thenext section.3.
The  Forward-Backward  Search  AlgorithmWe developed the Forward-Backward Search (FBS) algo-rithm in 1986 as a way to greatly reduce the computationneeded to search a large language model.
While many siteshave adopted this paradigm for computation of the N-bestsentence hypotheses, we feel that its full use may not befully understood.
Therefore, we will discuss the use of theFBS at some length in this section.The basic idea in the FBS is to perform a search in theforward direction to compute the probability of each wordending at each frame.
Then, a second more expensive searchin the backward irection can use these word-ending scoresto speed up the computation immensely.
If we multiply theforward score for a path by the backward score of anotherpath ending at the same frame, we have an estimate of thetotal score for the combined path, given the entire utterance.In a sense, the forward search provides the ideal fast matchfor the backward pass, in that it gives a good estimate of thescore for each of the words that can follow in the backwarddirection, including the effect of all of the remaining speech.When we first introduced the FBS to speed up the N-bestsearch algorithm, the model used in the forward and back-ward directions were identical.
So the estimate of the back-ward scores provided by the forward pass were exact.
Thismethod has also been used in a best-first stack search \[8\], inwhich it is very effective, since the forward-backward scorefor any theory covers the whole utterance.
The forward-backward score solves the primary problem with the besst-first search, which is that different hypotheses don't span thesame amount of speech.However, the true power of this algorithm is revealed whenwe use different models in the forward and backward i-rections.
For example, in the forward direction we can useapproximate acoustic models with a bigram language model.Then, in the backward pass we can use detailed HMM mod-els with a trigram language model.
In this case, the forwardscores still provide an excellent (although not exact) esti-mate of the ranking of different word end scores.
Becauseboth searches are time-synchronous, it does not matter thatthe forward and backward passes do not get the same score.
(This is in contrast o a backward best-first or A* search,which depends on the forward scores being an accurate pre-diction of the actual scores that will result in the backwardpass.
)In order to use these approximate scores, we need to rood-93ify the algorithm slightly.
The forward scores are normal-ized relative to the highest forward score at that frame.
(This happens automatically in the BYBLOS decoder, sincewe normalized the scores in each frame in order to pre-vent undertow.)
We multiply the normalized forward scoreby the normalized backward score to produce a normalizedforward-backward score.
We can compare these normalizedforward-I)ackward scores to the normalized backward scoresusing the usual beam-type threshold.
This causes us to con-sider more than one path in the backwards direction.
Thebest path (word sequence) associated with each word endmay not turn out to be the highest, but this does not mat-ter, because the backward search will rescore all the allowedpaths anyway.We find that the backward pass can run about 1000 timesfaster than it would otherwise, with the same accuracy.
Forexample, when using a vocabulary of 20,000 words a typicalbeam search that allows for only a small error rate due topruning requires about 20 times real time.
In contrast, wefind that the backward pass runs at about 1/60 real time!This makes it fast enough so that it can be performed at theend of the utterance with a delay that is barely noticeable.But the FBS also speeds up the forward pass indirectly!Since we know there will be a detailed backward search, weneed not worry about the accuracy of the forward pass tosome extent.
This allows us the freedom to use powerfulapproximate methods to speed up the forward pass, eventhough they may not be as accurate as we would like for afinal score.4.
Subl inear  Computat ionFast match methods require much less computation for eachword than a detailed match.
But to reduce the computationfor speech recognition significantly for very large vocabu-lary problems, we must change the computation from onethat is linear with the vocabulary to one that is essentiallyindependent of the vocabulary size.4.1.
Memory  vs Speed TradeoffsOne of the classical methods for saving computation is totrade increased memory for reduced computation.
Now thatmemory is becoming large and inexpensive, there are severalmethods open to us.
The most obvious is various forms offast match.
We propose one such memory-intensive fastmatch algorithm here.
Many others could be developed.Given an unknown word, we can make several orthogonalmeasures on the word to represent the acoustic realizationof that word as a single point in a multi-dimensional space.If we quantize ach dimension independently, we determinea single (quantized) cell in this space.
We can associateinformation with this cell that gives us a precomputed es-timate of the HMM score of each word.
The computationis performed only once, and is therefore very small and in-dependent of the size of the vocabulary.
(Of course theprecompilation of the scores of each of the words given acell in the space can be large.)
The precision of the fastmatch score is limited only by the amount of memory thatwe have, and our ability to represent the scores efficiently.4.2.
Computat ion  vs Vocabu lary  S izeTo learn how the computation of our real-time search al-gorithm grows with vocabulary size we measured the com-putation required at three different vocabulary sizes: 1,500words, 5,000 words, and 20,000 words.
The time required,as a fraction of real time, is shown plotted against he vo-cabulary size in Figure !.Time (x RT)1.0 1.00.5 ~ 0.50.0 I I I I 0.01.5 5 20Vocabulary (xlO00)liar)l .
o  ..... : i .
.
.
.
!i!
: 'rI.0o .5  i -0.5"To.z  L JELi: ti :l-o.,t -o.I.1.5 5 goFigure 1: Run time vs vocabulary size.
Plotted on a linearand a log-log scale.As can be seen, the computation i creases very slowly withincreased vocabulary.
To understand the behavior better weplotted the same numbers on a log-log scale as shown above.94Here we can see that the three points fall neatly on a straightline, leading us to the conclusion that the computation growsas a power of the vocabulary size, V. Solving the equationgives us the formulat ime= 0.04 V 1/3 (1)This is very encouraging, since it means that if we can de-crease the computation needed by a small factor it wouldbe feasible to increase the vocabulary size by a much largerfactor, making recognition with extremely large vocabulariespossible.5.
SummaryWe have discussed the search problem in speech recognitionand concluded that, in our opinion, it is no longer worth con-sidering parallel or special propose hardware for the speechproblem, because we have been able to make faster progressby modifying the basic search algorithm in software.
Atpresent, the fastest recognition systems are based entirelyon software implementations.
We reviewed several searchalgorithms briefly, and discussed the advantage of time-synchronous earch algorithms over other basic strategies.The Forward-Backward Search algorithm has turned out tobe an algorithm of major importance in that it has madepossible the first real-time recognition of 20,000-word vo-cabularies in continuous peech.
Finally, we demonstratedthat the computation required by this algorithm grows as thecube root of the vocabulary size, which means that real-timerecognition with extremely large vocabularies i feasible.AcknowledgementSome of this work was supported by the Defense AdvancedResearch Projects Agency and monitored by the Office ofNaval Research under Contract Nos.
N00014-91-C-0115,and N00014-92-C-0035.Re ferences1.
Schwartz, R. and Y.L.
Chow (1990) "The N-Best Algorithm:An Efficient and Exact Procedure for Finding the N MostLikely Sentence Hypotheses", ICASSP-90, April 1990, Albu-querque $2.12, pp.
81-84.
Also in Proceedings of the DARPASpeech and Natural Language Workshop, Cape Cod, Oct.1989.2.
V. Steinbiss (1989) "Sentence-Hypotheses Generation in aContinuous-Speech Recognition System," Proc.
of the Euro-pean Conf.
on Speech Communication a d Technology, Paris,Sept.
1989, Vol.
2, pp.
51-543.
Mari~o, J. and E. Monte (1989) "Generation of Multiple Hy-pothesis in Connected Phonetic-Unit Recognition by a Mod-ified One-Stage Dynamic Programming Algorithm", Proc.
ofthe European Cor~.
on Speech Communication a d Technol-ogy, Paris, Sept. 1989, Vol.
2, pp.
408-4114.
Lowerre, B.
(1977) "The Harpy Speech Recognition System",Doctoral Thesis CMU 1977.5.
Bald, L.R., de Souza, P., Gopalalaishnan, P.S., Kanevsky,D., and D. Nahamoo (1990) "Constructing Groups of Acous-tically Confusable Words".
Proceedings of the ICASSP 90,April, 1990.6.
Schwartz, R.M., Chow, Y., Kimball, O., Roucos, S., Krasner,M., and L Makhoul (1985) "Context-Dependent Modelingfor Acoustic-Phonetic Recognition of Continuous Speech",Proceedings of the ICASSP 85, pp.
1205-1208, March, 1985.7.
R.M.
Schwartz and S.A. Austin, "Efficient, High-PerformanceAlgorithms for N-Best Search," Proc.
DARPA Speech andNatural Language Workshop, Hidden Valley, PA, MorganKaufmann Publishers, pp.
6-11, June 1990.8.
Soong, F., Huang, E., "A Tree-Trellis Based Fast Searchfor Finding the N Best Sentence Hypotheses in ContinuousSpeech Recognition".
Proceedings of the DARPA Speech andNatural Language Workshop, Hidden Valley, June 1990.9.
Alleva, F., Huang, X., Hwang, M-Y., Rosenfeld, R., "AnImproved Search Algorithm Using Incremental Knowledgefor Continuous Speech Recognition and An Overview ofthe SPHINX-II Speech Recognition System", DARPA HumanLanguage Technology Workshop, Princeton, NJ, March, 1993.10.
Murveit, H., Butzberger, J., Digalakis, V., Weintraub, M.,"Progressive-Search Algorithms for Large Vocabulary SpeechRecognition", DARPA Human Language Technology Work-shop, Princeton, NJ, March, 1993.95
