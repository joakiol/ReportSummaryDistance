Proceedings of the 7th Workshop on Statistical Machine Translation, pages 410?421,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsTwitter Translation using Translation-Based Cross-Lingual RetrievalLaura Jehl and Felix Hieber and Stefan RiezlerDepartment of Computational LinguisticsHeidelberg University69120 Heidelberg, Germany{jehl,hieber,riezler}@cl.uni-heidelberg.deAbstractMicroblogging services such as Twitter havebecome popular media for real-time user-created news reporting.
Such communica-tion often happens in parallel in different lan-guages, e.g., microblog posts related to thesame events of the Arab spring were writtenin Arabic and in English.
The goal of thispaper is to exploit this parallelism in orderto eliminate the main bottleneck in automaticTwitter translation, namely the lack of bilin-gual sentence pairs for training SMT systems.We show that translation-based cross-lingualinformation retrieval can retrieve microblogmessages across languages that are similarenough to be used to train a standard phrase-based SMT pipeline.
Our method outper-forms other approaches to domain adaptationfor SMT such as language model adaptation,meta-parameter tuning, or self-translation.1 IntroductionAmong the various social media platforms, mi-croblogging services such as Twitter1 have becomepopular communication tools.
This is due to the easyaccessibility of microblogging platforms via inter-net or mobile phones, and due to the need for a fastmode of communication that microblogging satis-fies: Twitter messages are short (limited to 140 char-acters) and simultaneous (due to frequent updates byprolific microbloggers).
Twitter users form a socialnetwork by ?following?
the updates of other users,either reciprocal or one-way.
The topics discussedin Twitter messages range from private chatter to im-portant real-time witness reports.1http://twitter.com/Events such as the Arab spring have shown thepower and also the shortcomings of this new modeof communication.
Microblogging services played acrucial role in quickly spreading the news about im-portant events, furthermore they were useful in help-ing organizers plan their protest.
The fact that newson microblogging platforms is sometimes ahead ofnewswire is one of the most interesting facets ofthis new medium.
However, while Twitter messag-ing is happening in multiple languages, most net-works of ?friends?
and ?followers?
are monolingualand only about 40% of all messages are in English2.One solution to sharing news quickly and interna-tionally was crowdsourcing manual translations, forexample at Meedan3, a nonprofit organization builtto share news and opinion between the Arabic andEnglish speaking world, by translating articles andblogs, using machine translation and human expertcorrections.The goal of our research is to automate this trans-lation process, with a further aim of providing rapidcrosslingual data access for downstream applica-tions.
The automated translation of microbloggingmessages is facing two main problems.
First, thereare no bilingual sentence pair data from microblog-ging domains available.
Second, the colloquial, non-standard language of many microblogging messagesmakes it very difficult to adapt a machine translationsystem trained on any of the available bilingual re-sources such as transcriptions from political organi-zations or news text.The approach presented in this paper aims to ex-ploit the fact that microblogging often happens in2http://semiocast.com/publications/2011_11_24_Arabic_highest_growth_on_Twitter3http://news.meedan.net410parallel in different languages, e.g., microblog postsrelated to the same events of the Arab spring werepublished in parallel in Arabic and in English.
Thecentral idea is to crawl a large set of topically relatedArabic and English microblogging messages, anduse Arabic microblog messages as search queries ina cross-lingual information retrieval (CLIR) setup.We use the probabilistic translation-based retrievaltechnique of Xu et al (2001) that naturally inte-grates translation tables for cross-lingual retrieval.The retrieval results are then used as input to a stan-dard SMT pipeline to train translation models, start-ing from unsupervised induction of word alignments(Och and Ney, 2000) to phrase-extraction (Och andNey, 2004) and phrase-based decoding (Koehn et al,2007).
We investigate several filtering techniquesfor retrieval and phrase extraction (Munteanu andMarcu, 2006; Snover et al, 2008) and find a straight-forward application of phrase extraction from sym-metrized alignments to be optimal.
Furthermore, wecompare our approach to related domain adaptationtechniques for SMT and find our approach to yieldlarge improvements over all related techniques.Finally, a side-product of our research is a cor-pus of around 1,000 Arabic Twitter messages with3 manual English translations each, which were cre-ated using crowdsourcing techniques.
This corpusis used for development and testing in our experi-ments.2 Related WorkSMT for user-generated noisy data has been pio-neered at the 2011 Workshop on Statistical Ma-chine Translation that featured a translation task ofHaitian Creole emergency SMS messages4.
Thistask is very similar to the problem of Twitter transla-tion since SMS contain noisy, abbreviated language.The research papers related to the featured transla-tion task deploy several approaches to domain adap-tation, including crowdsourcing (Hu et al, 2011)or extraction of parallel sentences from comparabledata (Hewavitharana et al, 2011).The use of crowdsourcing to evaluate machinetranslation and to build development sets was pi-oneered by Callison-Burch (2009) and Zaidan and4http://www.statmt.org/wmt11/featured-translation-task.htmlCallison-Burch (2009).
Crowdsourcing has its lim-its when it comes to generating parallel training dataon the scale of millions of parallel sentences.
Inour work, we use crowdsourcing via Amazon Me-chanical Turk5 to create a development and test cor-pus that includes 3 English translations for each ofaround 1,000 Arabic microblog messages.There is a substantial amount of previous work onextracting parallel sentences from comparable datasuch as newswire text (Fung and Cheung, 2004;Munteanu and Marcu, 2005; Tillmann and ming Xu,2009) and on finding parallel phrases in non-parallelsentences (Munteanu and Marcu, 2006; Quirk et al,2007; Cettolo et al, 2010; Vogel and Hewavitha-rana, 2011).
The approach that is closest to ourwork is that of Munteanu and Marcu (2006): Theyuse standard information retrieval together with sim-ple word-based translation for CLIR, and extractphrases from the retrieval results using a clean bilin-gual lexicon and an averaging filter.
In this ap-proach, filtering and cleaning techniques in align-ment and phrase extraction have to compensate forlow-quality retrieval results.
In our approach, the fo-cus is on high-quality retrieval.As our experimental results show, the main im-provement of our technique is a decrease in out-of-vocabulary (OOV) rate at an increase of the per-centage of correctly translated unigrams and bi-grams.
Similar work on solving domain adaptationfor SMT by mining unseen words has been pre-sented by Snover et al (2008) and Daum?
and Ja-garlamudi (2011).
Both approaches show improve-ments by adding new phrase tables; however, bothapproaches rely on techniques that require largercomparable texts for mining unseen words.
Sincein our case documents are very short (they consistof 140 character sequences), these techniques arenot applicable.
However, the advantage of the factthat microblog messages resemble sentences is thatwe can apply standard word- and phrase-alignmenttechniques directly to the retrieval results.Further approaches to domain adaptation for SMTinclude adaptation using in-domain language mod-els (Bertoldi and Federico, 2009), meta-parametertuning on in-domain development sets (Koehn andSchroeder, 2007), or translation model adaptation5http://www.turk.com411using self-translations of in-domain source languagetexts (Ueffing et al, 2007).
In our experiments wecompare our approach to these domain adaptationtechniques.3 Cross-Lingual Retrieval via StatisticalTranslation3.1 Retrieval ModelIn our approach, comparable candidates for domainadaptation are selected via cross-lingual retrieval.In a probabilistic retrieval framework, we estimatethe probability of a relevant document microblogmessage D given a query microblog message Q,P (D|Q).
Following Bayes rule, this can be sim-plified to ranking documents according to the like-lihood P (Q|D) if we assume a uniform prior overdocuments.score(Q,D) = P (D|Q) = P (D)P (Q|D)P (Q) (1)Our model is defined as follows:score(Q,D) = P (Q|D) =?q?QP (q|D) (2)P (q|D) = ?Pmix(q|D)?
??
?mixture model+(1?
?)
PML(q|C)?
??
?query collection backoff(3)Pmix(q|D) = ?
?d?DT (q|d)PML(d|D)?
??
?translation model(4)+(1?
?)PML(q|D)?
??
?self-translationOur retrieval model is related to monolingual re-trieval models such as the language-modeling ap-proach of Ponte and Croft (1998) and the monolin-gual statistical translation approach of Berger andLafferty (1999).
Xu et al (2001) extend the formerapproaches to the cross-lingual setting by adding aterm translation table.
They describe their model interms of a Hidden Markov Model with two statesthat generate query terms: First, a document stategenerates terms d in the document language and thentranslates them into a query term q.
Second, a back-off state generates query terms q directly in the querylanguage.
In the document state the probability ofemitting q depends on all d that translate to q, ac-cording to a translation distribution T .
This is esti-mated by marginalizing out d as?d T (q|d)P (d|D).In the backoff state the probability PML(q|C) ofemitting a query term is estimated as the relativefrequency of this term within a corpus in the querylanguage.
The probability of transitioning into thedocument state or the backoff state is given by ?
and1?
?.We view this model from a smoothing perspectivewhere the backoff state is linearly interpolated withthe translation probability using a mixture weight?
to control the weighting between both terms.Furthermore, we expand Xu et al (2001)?s gen-erative model to incorporate the concept of ?self-translation?, introduced by Xue et al (2008) in amonolingual question-answering context: Twittermessages across languages usually share relevantterms such as hashtags, named entities or user men-tions.
Therefore, we model the event of a queryterm literally occurring in the document in a sepa-rate model that is itself linearly interpolated with aparameter ?
with the translation model.We implemented the model based on a Lucene6index, which allows efficient storage of term-document and document-term vectors.
To mini-mize retrieval time, we consider only those doc-uments as retrieval candidates where at least oneterm translates to a query term, according to thetranslation table T .
Stopwords were removed forboth queries and documents.
Compared to com-mon inverted index retrieval implementations, ourmodel is quite slow since the document-term vectorshave to be loaded.
However, multi-threading sup-port and batch retrieval on a Hadoop cluster madethe model tractable.
On the upside, the translation-based model allows greater precision in findingthe candidates for comparable microblog messagesthan simpler approaches that use a combination oftfidf matching and n-best query term expansion:The translation-based retrieval exploits all possi-ble alignments between query and document termswhich is particularly important for short documentssuch as microblog messages.3.2 In-Domain Phrase ExtractionTo prepare the extraction of phrases from retrievalresults, we conducted cross-lingual retrieval in bothdirections: retrieving Arabic documents using En-glish microblog messages as queries and vice versa.6http://lucene.apache.org/core/412For each run we kept the top N retrieved documents.Each document was then paired with its query togenerate pseudo-parallel data.We tried two approaches for using this data toimprove our translations.
The first, more restric-tive method makes use of the word alignments weobtained from 5.8 million clean parallel trainingdata from the NIST evaluation campaign.
The re-trieval step generates word-alignments in the direc-tion D ?
Q.
After retrieval, the reverse alignmentfor each query-document pair is also generated byusing a translation table in the direction Q ?
D. Analignment point between a query term q and a docu-ment term d is created, iff T (q|d) or T (d|q) exist inthe translation tables D ?
Q or Q ?
D. Based onthese word-alignments, we extract phrases by apply-ing the grow-diag-final-and heuristic and using Ochand Ney (2004)?s phrase extraction algorithm as im-plemented in Moses7 (Koehn et al, 2007).
We con-ducted experiments using different constraints onthe number of alignment points required for a pairto be considered as well as the value of N .
Our firsttechnique resembles the technique of Munteanu andMarcu (2006) who also perform phrase extractionby combining clean alignment lexica for initial sig-nals with heuristics to smooth alignments for finalfragment extraction.While we obtained some gains using our heuris-tics, we are aware that our method is severely re-stricted in that it only learns new words which arein the vicinity of known words.
We therefore alsotried the bolder approach of treating our data asparallel and running unsupervised word alignment8(Och and Ney, 2000) directly on the query-documentpairs to obtain new world alignments and build aphrase table.
In contrast to previous work (Snoveret al, 2008; Daum?
and Jagarlamudi, 2011), we cantake advantage of the sentence-like character of mi-croblog messages and treat queries and retrieval re-sults similar to sentence aligned data.For both extraction methods, the standard fivetranslation features from the new phrase table(phrase translation probability and lexical weight-ing in both directions, phrase penalty) were added tothe translation features in Moses.
We tried different7http://statmt.org/moses/8http://code.google.com/p/giza-pp/al-Gaddafi, al-Qaddhafi, assad, babrain, bahrain,egypt, gadaffi, gaddaffi, gaddafi, Gheddafi, homs,human rights, human-rights, humanrights, libia, li-bian, libya, libyan, lybia, lybian, lybya, lybyan,manama, Misrata, nabeelrajab, nato, oman, Pos-itiveLibyaTweets, Qaddhafi, sirte, syria, tripoli,tripolis, yemen;Table 1: Keywords used for Twitter crawl.modes of combining new and original phrase table,namely using either one or using the new phrase ta-ble as backoff in case no phrase translation is foundin the original phrase table.4 Data4.1 Twitter CrawlWe crawled Twitter messages from September 20,2011 until January 23, 2012 via the Streaming API9in keyword-tracking mode, obtaining 25.5M Twit-ter messages (tweets) in various languages.
Table 1shows the list of keywords that were chosen to re-trieve microblog messages related to the events ofthe Arab spring.10In order to separate the microblog message cor-pus by languages, we applied a Naive Bayes lan-guage identifier11.
This yielded a distribution withthe six most common languages (of 52) being Ara-bic (57%), English (33%), Somali (2%), Spanish(2%), Indonesian (1.5%), German (0.7%).
We keptonly microblog messages classified as English orArabic with confidence greater 0.9.
Keyword-basedcrawling creates a strong bias towards the domainof the keywords and it does not guarantee that allmicroblog messages regarding a certain topic or re-gion are retrieved or that all retrieved messages arerelated to the Arab Spring and human righs in themiddle east.
Additionally, retweets artificially in-9https://dev.twitter.com/docs/streaming-api/10The Twitter Streaming API allows up to 400 tracking key-words that are matched to uppercase, lowercase and quotedvariations of the keywords.
Partial matching such as ?tripolis?matching ?tripoli?
as well as Arabic Unicode characters are notsupported.
We extended our keywords over time by analyzingthe crawl, e.g., by introducing spelling variants and hashtags.11Language Detection Library for Java, byShuyo Nakatani (http://code.google.com/p/language-detection/).413Arabic Englishtweets + retweets 14,565,513 8,501,788tweets 6,614,126 5,129,829avg.
retweet/tweet 11.62 7.27unique users 180,271 865,202avg.
tweets/user 36.6 5.9Table 2: Twitter corpus statisticsflate the size of the data, although there are no newterms added.
Therefore, we removed all duplicateretweets that did not introduce additional terms tothe original tweet.
Table 2 explains the shrinkageof the dataset after removing retweets - comparedto English users, a smaller number of Arabic usersproduced a much larger number of retweets.
Inter-estingly, 56,087 users tweet a substantial amount inboth languages.
This suggests that users spread mes-sages simultaneously in Arabic and English.4.2 Creating a Small Parallel Twitter Corpususing CrowdsourcingFor the evaluation of our method, a small amountof parallel in-domain data was required.
Since thereare no corpora of translated microblog messages, wedecided to use Amazon Mechanical Turk12 to cre-ate our own evaluation set, following the exploratorywork of Zaidan and Callison-Burch (2011b).
Werandomly selected 2,000 Arabic microblog mes-sages.
Hashtags, user mentions and URLs were re-moved from each microblog message beforehand,because they do not need to be translated and wouldjust artificially inflate scores at test time.
The mi-croblog messages were then manually cleaned andpruned.
We discarded messages which containedalmost no text or large portions of other languagesand removed remaining Twitter markup.
In the end,1,022 microblog messages were used in the Me-chanical Turk task.
We split the data into batchesof ten sentences which comprised one HIT (humanintelligence task).
Each HIT had to be completed bythree workers.
In order to have some control overtranslation quality, we inserted one control sentenceper HIT, taken from the LDC-GALE Phase 1 ArabicBlog Parallel Text.
Turkers were rewarded 10 centsper translation.
Following Zaidan and Callison-Burch (2011b), all Arabic sentences were converted12http://www.turk.cominto images in order to prevent turkers from past-ing them into online machine translation engines.Our final corpus consists of 1,022 translated mi-croblog messages with three translations each.
Anexample containing translations for one of the sen-tences which we inserted for quality checking pur-poses, along with the reference translation, is givenin table 3.
It can be seen that translators sometimesmade grammar mistakes or odd word choices.
Theyalso tended to omit punctuation marks.
However,translations also contained reasonable translation al-ternatives (such as ?gathered?
or ?collected?).
Wealso asked translators to insert an ?unknown?
tokenwhenever they were unable to translate a word.
OurHIT setup did not allow workers to skip a sentence,forcing them to complete an entire batch.
In order toaccount for translation variants we decided to use allthree translations obtained via Mechanical Turk asmultiple references instead of just keeping the toptranslation.
We randomly split our small parallelcorpus, using half of the microblog messages for de-velopment and half for testing.4.3 PreprocessingBesides removal of Twitter markup, several addi-tional preprocessing steps such as digit normaliza-tion were applied to the data.
We also decided to ap-ply the Buckwalter Arabic transliteration scheme13to avoid encoding difficulties.
Habash and Sadat(2006) have shown that tokenization is helpful fortranslating Arabic.
We therefore decided to ap-ply a more involved tokenization scheme than sim-ple whitespace splitting to our data.
As the re-trieval relies on translation tables, all data needto be tokenized the same way.
We are awareof the MADA+TOKAN Arabic morphological an-alyzer and tokenizer (Habash and Rambow, 2005),however, this toolkit produces very in-depth analy-ses of the data and thus led to difficulties when wetried to scale it to millions of sentences/microblogmessages.
That is why we only used MADA fortransliteration and chose to implement the simplerapproach by Lee et al (2003) for tokenization.
Thisapproach only requires a small set of annotated datato obtain a list of prefixes and suffixes and uses n-13http://www.qamus.org/transliteration.htm414REFERENCE breaking the silence, a campaign group made up of israeli soldiers, gathered anonymous accounts from 26 soldiers.TRANSLATION1 and breaking silence is a group of israeli soldiers that had unknown statistics from 26 soldiers israeliTRANSLATION2 breaking the silence by a group of israeli soldiers who gathered unidentified statistics from 26 israeli soldier.TRANSLATION3 breaking the silence is a group of israeli soldiers that collected unknown statistics of 26 israeli soldiersTable 3: Example turker translations.gram-models to determine the most likely prefix?-stem-suffix?
split of a word.145 Twitter Translation ExperimentsWe conducted a series of experiments to evaluateour strategy of using CLIR and phrase-extraction toextract comparable data in the Twitter domain.
Wealso explored more standard ways of domain adap-tation such as using English microblog messages tobuild an in-domain language model, or generatingsynthetic bilingual corpora from monolingual data.All experiments were conducted using the Mosesmachine translation system15 (Koehn et al, 2007)with standard settings.
Language models werebuilt using the SRILM toolkit16 (Stolcke, 2002).For all experiments, we report lowercased BLEU-4 scores (Papineni et al, 2001) as calculated byMoses?
multi-bleu script.
For assessing signifi-cance, we apply the approximate randomization test(Noreen, 1989; Riezler and Maxwell, 2005).
Weconsider pairwise differing results scoring a p-value< 0.05 as significant.Our baseline model was trained using 5,823,363million parallel sentences in Modern StandardArabic (MSA) (198,500,436 tokens) and English(193,671,201 tokens) from the NIST evaluationcampaign.
This data contains parallel text from dif-ferent domains, including UN reports, newsgroups,newswire, broadcast news and weblogs.5.1 Domain Adaption using MonolingualResourcesAs a first step, we used the available in-domaindata for a combination of domain adaptation tech-14The n-gram-model required for tokenization was trained on5.8 million Modern Standard Arabic sentences from the NISTevaluation campaign.
This data had previously been tokenizedwith the same method, trained to match the Penn Arabic Tree-bank, v3.15http://statmt.org/moses/16http://www.speech.sri.com/projects/srilm/niques similar to Bertoldi and Federico (2009).There were three different adaptation measures:First, the turker-generated development set was usedfor optimizing the weights of the decoding meta-parameters, as introduced by Koehn and Schroeder(2007).
Second, the English microblog messages inour crawl were used to build an in-domain languagemodel.
This adaptation technique was first proposedby Zhao et al (2004).
Third, the Arabic portion ofour crawl was used to synthetically generate addi-tional parallel training data.
This was accomplishedby machine-translating the Arabic microblog mes-sages with the best system after performing the firsttwo adaptation steps.
Since decoding is very time-intensive, only 1 million randomly selected Ara-bic microblog messages were used to generate syn-thetic parallel data.
This new data was then usedto train another phrase table.
Such self-translationtechniques have been introduced by Ueffing et al(2007).
All results were evaluated against a base-line of using only NIST data for translation model,language model and weight optimization.Our results are shown in table 4.
Using an in-domain development set while leaving everythingelse untouched led to an improvement of approxi-mately 1 BLEU point.
Three experiments involv-ing the Twitter language model confirm Bertoldiand Federico (2009)?s findings that the languagemodel was most helpful.
The BLEU-score couldbe improved by 1.5 to 2 points in all experiments.When using an in-domain language model, therewas no significant difference between deploying anin-domain or out-of-domain development set.
Wealso compared the effect of using only the in-domainlanguage model to that of adding the in-domainlanguage model as an extra feature while keepingthe NIST language model.17 There was no signif-17The weights for both language models were optimizedalong with all other translation feature weights, rather than run-ning an extra optimization step to interpolate between both lan-guage models, since Koehn and Schroeder (2007) showed that415Run Translation Model Language Model Dev Set BLEU %1 NIST NIST NIST 13.902 NIST NIST Twitter 14.83?3 NIST Twitter NIST 15.98?4 NIST Twitter Twitter 15.68?5 NIST Twitter & NIST Twitter 16.04?6 self-train Twitter & NIST Twitter 15.79?7 self-train & NIST Twitter & NIST Twitter 15.94?Table 4: Domain adaptation experiments.
Asterisks indicate significant improvements over baseline (1).Run Twitter Phrases extraction method # sentence pairs # extracted phrases BLEU %8 top 3 retrieval results heuristics 14,855,985 6,508,141 17.04?9 top 1 retrieval results GIZA++ 5,141,065 54,260,537 18.73?
?10 retrieval intersection GIZA++ 3,452,566 29,091,009 18.85?
?11 retrieval intersection as backoff GIZA++ 3,452,566 29,091,009 18.93?
?Table 5: CLIR domain adaptation experiments.
All weights were optimized on the Twitter dev set and usedthe Twitter and NIST language models.
One Asterisk indicates a significant improvement over baseline run(5) from table 4.
Two Asterisks indicate a significant improvement over run (8).icant difference between both runs.
However, forfurther adaptation experiments we used the systemwith the highest absolute BLEU score.
In our case,using synthetically generated data was not help-ful, yielding similar results as the language modelexperiments above.
As has been observed beforeby Bertoldi and Federico (2009), it did not matterwhether the synthetic data were used on their ownor in addition to the original training data.5.2 Domain Adaptation usingTranslation-based CLIRMeta-parameters ?, ?
?
[0, 1] of the retrieval modelwere tuned in a mate-finding experiment: Mate-finding refers to the task of retrieving the single rel-evant document for a query.
In our case, each sourcetweet in the crowdsourced development set had ex-actly one ?mate?, namely the crowdsourced transla-tion that was ranked best in a further crowdsourcedranking task.
Using the retrieval model describedin section 3 we achieved precision@1 scores above95% in finding the translations of a tweet when ?and ?
were set to 0.9.
We fixed these parameter set-tings for all following experiments.
The translationtable was taken from the baseline experiments in ta-ble 4.
During retrieval, we kept up to 10 highestscoring documents per query.both strategies yielded the same results.We first employed heuristic phrase extractionbased on the word alignments generated from theNIST data as described above.
To avoid learningtoo much noise, maximum phrase length was re-stricted to 3 (the default is 7).
To evaluate the effectsof choosing more restrictive or more lax settings,we ran experiments varying the following configu-rations:1.
Constraints on alignment points:?
no constraints,?
3+ alignment points in each direction,?
3+ alignment points in both directions,?
5+ alignment points in both directions.2.
Constraints on retrieval ranking:?
top 10 results,?
top 3 results,?
top 1 results,?
retrieval intersection (results found in bothretrieval directions)We obtained improvements for all combinationsof these configurations.
However, we observed thatrequiring 5 common alignment points was too strict,since few pairs met this constraint.
We also noticedthat using only the top 3 retrieval results was benefi-cial to performance, suggesting that more compara-ble microblog messages were indeed ranked higher.416Using extraction heuristics we gained maximally 1.0BLEU using the top 3 retrieval results and requiringat least 3 alignment points in both alignment direc-tions (see first line in table 5).
However, other con-figurations produced very similar results.While heuristics led to small incremental im-provements, we achieved a much larger improve-ment by training a new phrase table from scratch us-ing GIZA++.
Again, we restricted maximum phraselength to 3 words.
In order to keep phrase tablesize manageable, we had to restrict retrieval to top-1 results or only use retrieval results in the inter-section of retrieval directions.
Best results are ob-tained when combining phrase tables extracted fromGIZA++ alignments in the intersection of retrievalresults with NIST phrase tables in backoff mode (seelast line in table 5).6 Error AnalysisOur cross-lingual retrieval approach succeeded infinding nearly parallel tweets, confirming our hy-pothesis that such data actually exists.
Examples aregiven in table 6.Table 7 shows a more detailed breakdown of ourtranslation scores.
First, standard adaptation meth-ods increased n-gram precision, suggesting that us-ing in-domain adaptation data caused the system tochoose more suitable words.
As expected, there wasno reduction in OOVs, since using an in-domainlanguage model and development set does not in-troduce new vocabulary.
Heuristic phrase extrac-tion again produced small improvements in n-gramprecision while reducing the number of unknownwords.
Learning a new phrase table with GIZA++produced substantial improvements both in OOV-rate and in n-gram precision.Nevertheless, even the scores of the adapted sys-tem are still fairly low and translation quality asjudged by inspection of the output can be very poor.This suggests that the language used on Twitter stillposes a great challenge, due to its variety of stylesas well as the users?
tendency to use non-standardspelling and colloquial or dialectal expressions.
Ourdevelopment set contained many different genres,from Qu?ran verses over news headlines to personalchatter.
Another difficulty was posed by dialectalArabic content.
To gain an impression of the amountof dialectal content in our data, we used the ArabicOnline Commentary Dataset created by Zaidan andCallison-Burch (2011a) to classify our test set.
Ta-ble 8 shows the distribution of dialects in our testdata according to language model probability.
Thisdistribution should be viewed with a grain of salt,since the shortness of tweets might cause unreliableresults when using a model based on word frequen-cies for classification.
Still, the results suggest thatthere is a high proportion of dialectal content andspelling variation in our data, causing a large num-ber of OOVs.
For example, the preposition ?
?,meaning ?in?
is often written as Y?.
Our phrasetable trained only on standard Arabic data as well asour extraction heuristic failed to translate this fre-quently occurring word.
Only when retraining aphrase table with GIZA++ did we translate it cor-rectly.Dialect # SentencesEgyptian 141Levantine 147Gulf 78Modern Standard Arabic 145Table 8: Dialectal content in our test set as classifiedby the AOC dataset.Table 9 gives examples of translations generatedusing different adaptation methods in comparison tothe references and the Google translation service toillustrate strengths and weaknesses of our approach.Example 1 shows a case where unknown words werelearned through translation model adaptation.
Notethat even the Google translator did not recognizethe word ?ys?
which was transliterated as?Msellat?.
Zaidan and Callison-Burch (2011a) pointout that dialectal variants are often transliteratedby Google.
Note also, that the unadapted transla-tion erroneously translated the place name ?sitra?
as?jacket?, a mistake which was also made in two ofthe references and by Google.
The same happenedto the place name ?wadyan?, which could also betaken as meaning ?and religions?.
This error wasenforced by our preprocessing step incorrectly split-ting off the prefix ?w?
which often carries the mean-ing ?and?.
In addition to that, the two runs whichused translation model adaptation each dropped apart of the input sentence (?in sitra?, ?firing?).
We417ARABIC TWEET fO?  Y?  ?yybyl?   w?d??
?
?AyF ?
? @q?  ?  d???
?s?rf?  Hy?r? ?
 GOOGLE TRANSLATION AFP confirms that the French President Gaddafi Libyans tried to call and forgivenessENGLISH TWEET french president assures that will be taken to court and tells the libyans to forgive each otherARABIC TWEET Hym?  ??
  ?
rO?
Y?
?wmm?  A?rJ ?ym ?
?C ?
A?E Crq?
?AO?  ?y\n EAhGOOGLE TRANSLATION NTRA decide to increase the number of all mobile operators in Egypt a commencement from ThursdayENGLISH TWEET ntra decide to increase the number of all mobile operators in starting from thursdayARABIC TWEET ?CA?
?lV ?
?rV ??
r?An?
?w?
dm  Yl?
?y?  dyhK? GOOGLE TRANSLATION Shahid Amin AA Day January through gunshotENGLISH TWEET martyr amin ali ahmed on jan by gunshotTable 6: Examples of nearly parallel tweets found by our retrieval method.Adaptation method OOV-rate %/absolute unigram precision %/absolute bigram precision %/absolute output length (words)None 22.56/2216 51.1/5020 20.2/1882 9832LM and Dev 20.05/2220 51.4/5442 22.1/2227 10595Retrieval (heuristic) 17.47/1790 53.5/5484 23.6/2299 10246Retrieval (GIZA++) 4.22/439 56.1/5834 26.1/2575 10395Table 7: OOV-rate and precision for different adaptation methods.attribute this to that fact that the phrase table extrac-tion often produced one-to-many alignments whenonly one alignment point was known.
In Example 2GIZA++ extraction clearly outperformed heuristicphrase extraction.
This example also shows that ourmethod is good at learning proper names.
Whilethe first two examples resemble news text, Exam-ple 3 is a more informal message.
It is particularlyinteresting to note that with GIZA++ extraction theterm ?shabiha?
is learned, which is commonly usedin Syria to mean ?thugs?
and specifically refers toarmed civilians who assault protesters against BashirAl-Assad?s regime.
Example 4 also shows substan-tial OOV reduction.
However, the term ? rtns r?
?  (?in Opera Central?, the location of TelecomEgypt) is incorrectly translated as ?really opera?.7 ConclusionWe presented an approach to translation of mi-croblog messages from the Twitter domain.
Themain obstacle to state-of-the-art SMT of such datais the complete lack of sentence-parallel trainingdata.
We presented a technique that uses translation-based CLIR to find relevant Arabic Twitter messagesgiven English Twitter queries, and applies a standardpipeline for unsupervised training of phrase-basedSMT to retrieval results.
We found this straight-forward technique to outperform more conservativetechniques to extract phrases from comparable dataand also to outperform techniques using monolin-gual resources for language model adaptation, meta-parameter tuning, or self-translation.The greatest benefit of our approach is a signifi-cant reduction of OOV terms at a simultaneous im-provement of correct unigram and bigram transla-tions.
Despite this positive net effect, we still finda considerable amount of noise in the automati-cally extracted phrase tables.
Noise reduction byimproved pre-processing and by more sophisticatedtraining will be subject to future work.
Furthermore,we would like to investigate a tighter integration ofCLIR and SMT training by using forced decodingtechniques for CLIR and by a integrating a feedbackloop into retrieval and training.AcknowledgmentsWe would like to thank Julia Ostertag for several it-erations of manual error analysis of Arabic transla-tion output.418EXAMPLE 1SRC ?w?d?  ?ys?
?lW?
Tlrt?
?A?
 ?
?tq 	?K?   w?
?rtFGOOGLE Riot troops stormed the jacket and religions foot and launches Msellat tearsNO ADAPTATION jacket riot forces storm and religions foot ?ys?
?lW?
tearsLM AND DEV sitra and religions of the foot of the riot forces storm ?ys?
?lW?
tearsRETRIEVAL (HEURISTIC) in sitra riot police storming and religions of tear gas on footRETRIEVAL (GIZA++) the riot police stormed and religions of the foot firing tear gasREF0 vest riot forces break into wadyan by foot and trough gas tearREF1 sotra the riot forces enter on foot and shoot tear bombsREF2 the cover for riot police enters wadian walking and shoot tear bombsEXAMPLE 2SRC Yq?w`?  ?tq?
??
?wy?  dtyF A?A?GOOGLE Obama will speak today the death of al-AwlakiNO ADAPTATION dtyF A?A? today killed Yq?w`? LM AND DEV dtyF A?A? friday for the killing of Yq?w`? RETRIEVAL (HEURISTIC) A?A? today on the killing ofRETRIEVAL (GIZA++) obama today on the al awlaki killingREF0 obama will talk today about the killing of al - awlakiREF1 obama is talking today about el awlaqi deathREF2 obama will speak today about the killing of al - awlaqiEXAMPLE 3SRC (: ?wy?ts?
?Am ??
TybK? GOOGLE Cbihh in Hama are crying :)NO ADAPTATION TybK?  mired in calling for help : )LM AND DEV TybK?  in hama calling for help : )RETRIEVAL (HEURISTIC) inside the protectors of the calling for help : )RETRIEVAL (GIZA++) shabiha in hama calling for help : )REF0 the gangsters in hama are asking for helpREF1 the gangs in hamah are peading :)REF2 the thugs in hama are calling for help :)EXAMPLE 4SRC  r?
?  ? rtns T?r?
Y?
T?rK?  Hy?C ??zt?
?AO??
T?rOm?A ?wl?A?
:: ??
?r?GOOGLE Freedom :: Telecom Egypt workers holding company?s president in a room Psontral OperaNO ADAPTATION : : free workers ?AO??
T?rOm?A holding company chairman  r?
?  ? rtns Y?
chamberLM AND DEV : : workers free ?AO??
T?rOm?A holding company chairman Y? r?
?  ? rtns roomRETRIEVAL (HEURISTIC) free : : afcd T?rOm?A hold ceo hostage ppl is the president of the chamber of  r?
?  ? rtnsRETRIEVAL (GIZA++) egypt : : workers telecom workers are holding the head of the company in the chamber of really operaREF0 freedom :: workers in the egyptian for communication are holding the company president in a room in the opera centralREF1 freedom , workers in egypt for calls detain the head of the company in a room in opera centralREF2 hurriya :: workers in telecom egypt detaining the president of the company in a room in the opera centralTable 9: Example output using different adaptation methods.ReferencesAdam Berger and John Lafferty.
1999.
Information re-trieval as statistical translation.
In Proceedings of the22nd ACM SIGIR Conference on Research and Devel-opment in Information Retrieval (SIGIR?99), Berkeley,CA.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation withmonolingual resources.
In Proceedings of the 4thEACL Workshop on Statistical Machine Translation,Athens, Greece.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: Evaluating translation quality using amazon?smechanical turk.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP?09), Singapore.M.
Cettolo, M. Federico, and N. Bertoldi.
2010.
Min-ing parallel fragments from comparable texts.
In Pro-ceedings of the 7th International Workshop on Spoken419Language Translation, Paris, France.Hal Daum?
and Jagadeesh Jagarlamudi.
2011.
Domainadaptation for machine translation by mining unseenwords.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies (ACL-HLT?11), Portland,OR.Pascale Fung and Percy Cheung.
2004.
Mining very-non-parallel corpora: Parallel sentence and lexicon ex-traction via bootstrapping and EM.
In Proceedings ofthe 2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?04), Barcelona, Spain.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics (ACL?05), Ann Arbor, MI.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.
InProceedings of the Human Language Technology Con-ference - North American Chapter of the Associationfor Computational Linguistics annual meeting (HLT-NAACL?06), New York, NY.Sanjika Hewavitharana, Nguyen Bach, Qin Gao, VamshiAmbati, and Stephan Vogel.
2011.
CMU haitiancreole-english translation system for WMT 2011.
InProceedings of the 6th Workshop on Statistical Ma-chine Translation, Edinburgh, Scotland, UK.Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-delman, Olivia Buzek, and Benjamin B. Bederson.2011.
The value of monolingual crowdsourcing ina real-world translation scenario: Simulation usinghaitian creole emergency SMS messages.
In Proceed-ings of the 6th Workshop on Statistical Machine Trans-lation, Edinburgh, Scotland, UK.Philipp Koehn and Josh Schroeder.
2007.
Experiments indomain adaptation for statistical machine translation.In Proceedings of the Second Workshop on StatisticalMachine Translation, Prague, Czech Republic.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Birch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the ACL 2007 Demo and Poster Sessions,Prague, Czech Republic.Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-sama Emam, and Hany Hassan.
2003.
Languagemodel based arabic word segmentation.
In Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics (ACL?03), Sapporo, Japan.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Compu-tational Linguistics (COLING-ACL?06), Sydney, Aus-tralia.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses.
An Introduction.
Wiley, NewYork.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics (ACL?00), Hongkong, China.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
Technical ReportIBM Research Division Technical Report, RC22176(W0190-022), Yorktown Heights, N.Y.Jay M. Ponte and Bruce W. Croft.
1998.
A languagemodeling approach to information retrieval.
Proceed-ings of the 21st annual international ACM SIGIR con-ference on Research and development in informationretrieval (SIGIR?98).Chris Quirk, Raghavendra Udupa U, and Arul Menezes.2007.
Generative models of noisy translations withapplications to parallel fragment extraction.
In Pro-ceedings of MT Summit XI, Copenhagen , Denmark.Stefan Riezler and John Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Proceedings of the ACL-05 Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization, Ann Arbor, MI.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptation us-ing comparable corpora.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP?08), Honolulu, Hawaii.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, Denver,CO.Christoph Tillmann and Jian ming Xu.
2009.
A sim-ple sentence-level extraction algorithm for comparabledata.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-420can Chapter of the Association for Computational Lin-guistic (NAACL-HLT?09), Boulder, CO.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Transductive learning for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics(ACL?07), Prague, Czech Republic.Stephan Vogel and Sanjika Hewavitharana.
2011.
Ex-tracting parallel phrases from comparable data.
InProceedings of the 4th Workshop on Building and Us-ing Comparable Corpora: Comparable Corpora andthe Web, Portland, OR.Jinxi Xu, Ralph Weischedel, and Chanh Nguyen.
2001.Evaluating a probabilistic model for cross-lingual in-formation retrieval.
In Proceedings of the 24th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR?01),New York, NY.Xiaobing Xue, Jiwoon Jeon, and Bruce Croft.
2008.
Re-trieval models for question and answer archives.
InProceedings of the 31st Annual International ACM SI-GIR Conference on Research and Development in In-formation Retrieval (SIGIR?08), Singapore.Omar F. Zaidan and Chris Callison-Burch.
2009.
Feasi-bility of human-in-the-loop minimum error rate train-ing.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP?09), Singapore.Omar F. Zaidan and Chris Callison-Burch.
2011a.The arabic online commentary dataset: an annotateddataset of informal arabic with high dialectal content.In Proceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?11), Port-land, OR.Omar F. Zaidan and Chris Callison-Burch.
2011b.Crowdsourcing translation: Professional quality fromnon-professionals.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics (ACL?11), Portland, OR.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Proceed-ings of the 20th International Conference on Compu-tational Linguistics (COLING?04), Geneva, Switzer-land.421
