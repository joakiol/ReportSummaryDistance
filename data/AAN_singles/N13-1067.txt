Proceedings of NAACL-HLT 2013, pages 596?606,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsPurpose and Polarity of Citation: Towards NLP-based BibliometricsAmjad Abu-JbaraDepartment of EECSUniversity of MichiganAnn Arbor, MI, USAamjbara@umich.eduJefferson EzraDepartment of EECSUniversity of MichiganAnn Arbor, MI, USAjezra@umich.eduDragomir RadevDepartment of EECSand School of InformationUniversity of MichiganAnn Arbor, MI, USAradev@umich.eduAbstractBibliometric measures are commonly used toestimate the popularity and the impact of pub-lished research.
Existing bibliometric mea-sures provide ?quantitative?
indicators of howgood a published paper is.
This does not nec-essarily reflect the ?quality?
of the work pre-sented in the paper.
For example, when h-index is computed for a researcher, all incom-ing citations are treated equally, ignoring thefact that some of these citations might be neg-ative.
In this paper, we propose using NLPto add a ?qualitative?
aspect to biblometrics.We analyze the text that accompanies citationsin scientific articles (which we term citationcontext).
We propose supervised methods foridentifying citation text and analyzing it to de-termine the purpose (i.e.
author intention) andthe polarity (i.e.
author sentiment) of citation.1 IntroductionAn objective and fair evaluation of the impactof published research requires both quantitativeand qualitative assessment.
Existing bibliometricmeasures such as H-Index (Hirsch, 2005; Hirsch,2010), G-index (Egghe, 2006), and Impact Fac-tor (Garfield, 1994) focus on the quantitative aspectof this evaluation which dose not always correlatewith the qualitative aspect.For example, the number of papers published bya researcher only tells how productive she or he is.It does not say anything about the quality or the im-pact of the work.
Similarly, the number of citationsthat a paper receives should not be used to gaugethe quality of the work as it really only measuresthe popularity of the work and the interest of otherresearchers in it (Garfield, 1979).
Controversial pa-pers or those based on fabricated data or experimentsmay receive a large number of citations.
A popularexample of fraudulent research that deceived manyresearchers and caught media attention was the caseof a South Korean research scientist, Hwang Woo-suk, who was found to have faked his research re-sults in the area of human stem cell cloning.
His re-search was published in Science and received closeto 200 citations after the fraud was discovered.
Thevast majority of those citations were negative.This suggests that the purpose of citation shouldbe taken into consideration when biblometric mea-sures are computed.
Negative citations should beweighted less than positive or neutral citations.
Thismotivates the need to automatically distinguish be-tween positive, negative, and neutral citations and toidentify the purpose of a citation; i.e.
the author?s in-tention behind choosing a published article and cit-ing it.This analysis of citation purpose and polarity canbe useful for many applications.
For example, it canbe used to build systems that help funding agenciesand hiring committees at universities and researchinstitutions evaluate researchers?
work more accu-rately.
It can also be used as a preprocessing step insystems that process scholarly data.
For example,citation-based summarization systems (Qazvinianand Radev, 2008; Qazvinian et al 2010; Abu-Jbara and Radev, 2011) and survey generation sys-tems (Mohammad et al 2009; Qazvinian et al2013) can benefit from citation purpose and polar-ity analysis to improve paper and content selection.In this paper, we investigate the use of linguis-tic analysis techniques to automatically identify thepurpose of citing a paper and the polarity of this cita-tion.
We first present a sequence labeling method forextracting the text that cites a given target reference;i.e.
the text that appears in a scientific article andrefers to another article and comments on it.
We usethe term citation context to refer to this text.
Next,596we use supervised classification techniques to ana-lyze this text and identify the purpose and polarityof citation.The rest of this paper is organized as follows.
Sec-tion 2 reviews the related work.
We present our ap-proach in Section 3.
We then describe the data andexperiments in Section 4.
Finally, Section 5 con-cludes the paper and suggests directions for futurework.2 Related WorkOur work is related to a large body of researchon citations.
Studying citation patterns and ref-erencing practices has interested researchers formany years (Hodges, 1972; Garfield et al 1984).White (2004) provides a good survey of the differ-ent research directions that study or use citations.
Inthe following subsections, we review three lines ofresearch that are closely related to our work.2.1 Citation Context IdentificationThe first line of related research addresses the prob-lem of identifying citation context.
The context of acitation that cites a given target paper can be a set ofsentences, one sentence, or a fragment of a sentence.Nanba and Okumura (1999) use the term citingarea to refer to the same concept.
They define theciting area as the succession of sentences that ap-pear around the location of a given reference in ascientific paper and have connection to it.
Their al-gorithm starts by adding the sentence that containsthe target reference as the first member sentence inthe citing area.
Then, they use a set of cue wordsand hand-crafted rules to determine whether the sur-rounding sentences should be added to the citingarea or not.
In (Nanba et al 2000), they use theiralgorithm to improve citation type classification andautomatic survey generation.Qazvinian and Radev (2010) addressed a simi-lar problem.
They proposed a method based onprobabilistic inference to extract non-explicit cit-ing sentences; i.e., sentences that appear aroundthe sentence that contains the target reference andare related to it.
They showed experimentally thatcitation-based survey generation produces better re-sults when using both explicit and non-explicit cit-ing sentences rather than using the explicit onesalone.In previous work, we addressed the issue of iden-tifying the scope of a given target reference in citingsentences that contain multiple references (2012).Our definition of reference scope was limited tofragments of the explicit citing sentence (i.e.
thesentence in which actual citation appears).
Thatmethod does not identify related text in surroundingsentences.In this work, we propose a supervised sequencelabeling method for identifying the citation contextof given reference which includes the explicit citingsentence and the related surrounding sentences.2.2 Citation Purpose ClassificationSeveral research efforts have focused on studyingthe different purposes for citing a paper (Garfield,1964; Weinstock, 1971; Moravcsik and Muruge-san, 1975; and Moitra, 1975; Bonzi, 1982).Bonzi (1982) studied the characteristics of citingand cited works that may aid in determining the re-latedness between them.
Garfield (1964) enumer-ated several reasons why authors cite other publi-cations, including ?alerting researchers to forthcom-ing work?, paying homage to the leading scholarsin the area, and citations which provide pointers tobackground readings.
Weinstock (1971) adopted thesame scheme that Garfield proposed in her study ofcitations.Spiegel-Rosing (1977) proposed 13 categories forcitation purpose based on her analysis of the firstfour volumes of Science Studies.
Some of them are:Cited source is the specific point of departure forthe research question investigated, Cited source con-tains the concepts, definitions, interpretations used,Cited source contains the data used by the citing pa-per.
Nanba and Okumura (1999) came up with asimple schema composed of only three categories:Basis, Comparison, and other Other.
They pro-posed a rule-based method that uses a set of statis-tically selected cue words to determine the categoryof a citation.
They used this classification as a firststep for scientific paper summarization.
Teufel etal.
(2006), in their work on citation function classifi-cation, adopted 12 categories from Spiegel-Rosing?staxonomy.
They trained an SVM classifier and usedit to label each citing sentence with exactly one cat-egory.
Further, they mapped the twelve categories tofour top level categories namely: weakness, contrast597(4 categories), positive (6 categories) and neutral.The taxonomy that we use in this work is basedon previous work.
We adopt a scheme that containssix categories.
We selected the six categories afterstudying all the previously used citation taxonomies.We included the ones we believed are important forimproving bibliometric measures and for the appli-cations that we are planning to pursue in the future(Section 5).2.3 Citation Polarity ClassificationThe polarity (or sentiment) of a citation has alsobeen studied previously.
Previous work showedthat positive and negative citations are common, al-though negative citations might be expressed indi-rectly or in an implicit way (Ziman, 1968; Mac-Roberts and MacRoberts, 1984; THOMPSON andYIYUN, 1991).
Athar (2011) addressed the prob-lem of identifying sentiment in citing sentences.
Heused a set of structure-based features to train a ma-chine learning classifier using annotated data.
Thiswork uses the citing sentence only to predict senti-ment.
Context sentences were ignored.
Athar andTeufel (2012a) observed that taking the context intoconsideration when judging sentiment in citationsincreases the number of negative citations by a fac-tor of 3.
They proposed two methods for utilizingthe context.
In the first method, they treat the citingsentence and a fixed context (a window of four sen-tences around the citing sentence) as if they werea single sentence.
They extract features from themerged text and train a classifier similar to what theydid in their 2011 paper.
In the second method, theyuse a four-class annotation scheme.
Each sentencein a window of four sentences around the citationis labeled as positive, negative, neutral, or excluded(unrelated to the cited work).
There experimentssurprisingly gave negative results and showed thatclassifying sentiment without considering the con-text achieves better results.
They attributed this tothe small size of their training data and to the noisethat including the context text introduces to the data.In (Athar and Teufel, 2012b), the authors present amethod for automatically identifying all the men-tions of the cited paper in the citing paper.
Theyshow that considering all the mentions improves theperformance of detecting sentiment in citations.In our work, we propose a sequence labelingmethod for identifying the citation context first, andthen use a supervised approach to determine the po-larity of a given citation.3 ApproachIn this section, we describe our approach to threetasks: citation context identification, citation pur-pose classification, and citation polarity identifica-tion.
We also describe a preprocessing stage that isapplied to the citation text before performing any ofthe three tasks.3.1 PreprocessingThe goal of the preprocessing stage is to clean andprepare the citation text for part-of-speech taggingand parsing.
The available POS taggers and parsersare not trained on citation text.
Citation text is dif-ferent from normal text in that it contains referenceswritten in a special format (e.g., author names andpublication year written in parentheses; or referenceindices written in square brackets).
Many citing sen-tences contain multiple references, some of whichmight be grouped together in a pair of parenthesesand separated by a comma or a semi-colons.
Thesereferences are usually not syntactic nor semanticconstituents of the sentences they appear in.
Thisresults in many POS tagging and parsing errors.
Weaddress this issue in the pre-processing stage to im-prove the performance of the feature extraction com-ponent.
We perform three pre-processing steps:a.
Reference Tagging: In the first step, we findand tag all the references that appear in the text.
Weuse a regular expression to find references and re-place each reference with a placeholder.
The ref-erence to the target paper is replaced by the place-holder TREF.
Each other reference is replaced byREF.b.
Reference Grouping: In this step, we identifygrouped references (i.e.
multiple references listedbetween one pair of parentheses separated by semi-colons).
Each such group is replaced by a place-holder, GREF.
If the target reference is a member ofthe group, we use a different placeholder: GTREF.c.
Non-syntactic Reference Removal: A refer-ence or a group of references could either be a syn-tactic constituent and has a semantic role in the sen-tence or not (Whidby, 2012; Abu Jbara and Radev,2012).
If the reference is not a syntactic compo-598Feature DescriptionDemonstrative determiners Takes a value of 1 if the current sentence contains contains a demonstrative determiner (this, these,etc.
), and 0 otherwise.Conjunctive adverbs Takes a value of 1 if the current sentence starts with a conjunctive adverb (However, Furthermore,Accordingly, etc.
), and 0 otherwise.Position Position of the current sentence with respect to the citing sentence.
This feature takes one of fourvalues: -1, 0, 1, and 2.Contains Closest Noun Phrase Takes a value of 1 if the current sentence contains closest noun phrase (if any) immediately beforethe reference position in the citing sentence, and 0 otherwise.
This noun phrase often is the name ofa method, a tool, or corpus originating from the cited reference.2-3 grams The first bigram and trigram in the sentence (This approach, One problem with, etc.
).Contains Other references Takes a value of 1 if the current sentence contains references other than the target, and 0 otherwise.Contains a Mention of target reference Takes a value of 1 if the current sentence contains a mention (explicit or anaphoric) of the targetreference, and 0 otherwise.Multiple references Takes a value of 1 if the citing sentence contains multiple references, and 0 otherwise.
If the cit-ing sentence contains multiple references, it becomes less likely that the surrounding sentences arerelated.Table 1: Features used for citation context identificationnent in the sentence, we remove it to reduce pars-ing errors.
Following our previous work (Abu Jbaraand Radev, 2012), we use a rule-based algorithm todetermine whether a reference should be removedfrom the sentence or kept.
The algorithm uses stylis-tic and linguistic features such as the style of thereference, the position of the reference, and the sur-rounding words to make the decision.
When a ref-erence is removed, the head of the closest nounphrase (NP) immediately before the position of theremoved reference is used as a representative of thereference.
This is needed for feature extraction asshown later in the paper.3.2 Citation Context IdentificationThe task of identifying the citation context of a giventarget reference can be formally defined as follows.Given a scientific article A that cites another articleB, find a set of sentences in A that talk about thework done in B such that at least one of these sen-tences contains an explicit reference to B.We treat this problem as a sequence labeling prob-lem.
The goal is to find the globally best sequenceof labels for all the sentences that appear within awindow around the citing sentence.
The citing sen-tence is the one that contains an explicit referenceto the cited paper.
Each sentence within the windowis labeled as INCLUDED or EXCLUDED from thecitation context of the given target paper.
To deter-mine the size of the window, we examined a devel-opment set of 300 sentences.
We noticed that the re-lated context almost always falls within a window offour sentences.
The window includes the citing sen-tence, one sentence before the citing sentence, andtwo sentences after the citing sentence.We use Conditional Random Fields (CRFs) forsequence labeling.
In particular, we use a first-orderchain-structured CRF.
The chain consists of two setsof nodes: 1) a set of hidden nodes Y which representthe context labels of sentences (INCLUDED or EX-CLUDED), and 2) a set of observed nodes X whichrepresent the features extracted from the sentences.The task is to estimate the probability of a sequenceof labels Y given the sequence of observed featuresX: P (Y|X)Lafferty et al(2001) define this probability to bea normalized product of potential functions ?
:P (y|x) =?t?k(yt, yt?1, x) (1)Where ?k(yt, yt?1, x) is defined as?k(yt, yt?1, x) = exp(?k?kf(yt, yt?1, x)) (2)where f(yt, yt?1, x) is a transition feature func-tion of the label at positions i ?
1 and i and the ob-servation sequence x; and ?j is a parameter that thealgorithm estimates from training data.The features we use to train the CRF model in-clude structural and lexical features that attempt tocapture indicators of relatedness to the given targetreference.
The features that we used and their de-scriptions are listed in table 1.599Category Description ExampleCriticizing Criticism can be positive or negative.
A citing sentence is classi-fied as ?criticizing?
when it mentions the weakness/strengths ofthe cited approach, negatively/positively criticizes the cited ap-proach, negatively/positively evaluates the cited source.Chiang (2005) introduced a constituent feature to rewardphrases that match a syntactic tree but did not yield signif-icant improvement.Comparison A citing sentence is classified as ?comparison?
when it comparesor contrasts the work in the cited paper to the author?s work.
Itoverlaps with the first category when the citing sentence says oneapproach is not as good as the other approach.
In this case we usethe first category.Our approach permits an alternative to minimum error-ratetraining (MERT; Och, 2003);Use A citing sentence is classified as ?use?
when the citing paper usesthe method, idea or tool of the cited paper.We perform the MERT training (Och, 2003) to tune theoptimal feature weights on the development set.Substantiating A citing sentence is classified as ?substantiating?
when the re-sults, claims of the citing work substantiate, verify the cited paperand support each other.It was found to produce automated scores, which stronglycorrelate with human judgements about translation flu-ency (Papineni et al, 2002).Basis A citing sentence is classified as ?basis?
when the author uses thecited work as starting point or motivation and extends on the citedwork.Our model is derived from the hidden-markov model forword alignment (Vogel et al 1996; Och and Ney, 2000).Neutral (Other) A citing sentence is classified as ?neutral?
when it is a neutraldescription of the cited work or if it doesn?t come under any ofthe above categories.The solutions of these problems depend heavily on thequality of the word alignment (Och and Ney, 2000).Table 2: Annotation scheme for citation purpose.
Motivated by the work of (Spiegel-Ro?sing, 1977) and (Teufel et al2006)3.3 Citation Purpose ClassificationIn this section, we describe the citation purpose clas-sification task.
Given a target paper B and its cita-tion context (extracted using the method describedabove) in a given article A, we want to determinethe purpose of citing B by A.
The purpose is de-fined as intention behind selecting B and citing it bythe author of A (Garfield, 1964).We use a taxonomy that consists of six categories.We designed this taxonomy based on our study ofsimilar taxonomies proposed in previous work.
Weselected the categories that we believe are more im-portant and useful from a bibliometric point of view,and the ones that can be detected through citationtext analysis.
We also tried to limit the number ofcategories by grouping similar categories proposedin previous work under one category.
The six cate-gories, their descriptions, and an example for eachcategory are listed in Table 2.We use a supervised approach whereby a classifi-cation model is trained on a number of lexical andstructural features extracted from a set of labeled ci-tation contexts.
Some of the features that we use totrain the classifier are listed in table 3.3.4 Citation Polarity IdentificationIn this section, we describe the citation polarity iden-tification task.
Given a target paper B and its citationcontext in a given article A, we want to determinethe polarity of the citation text with respect to B.The polarity can be: positive, negative, or neutral(objective).
Positive, negative, and neutral in thiscontext are defined in a slightly different way thantheir usual sense.
A citation is marked positive if iteither explicitly states a strength of the target paperor indicates that the work done in the target paperhas been used either by the author or a third-party.
Itis also marked as positive if it is compared to anotherpaper (possibly by the same authors) and deemedbetter in some way.
A citation is marked negativeif it explicitly points to a weakness of the target pa-per.
It is also marked as negative if it is comparedto another paper and deemed worse in some way.
Acitation is marked as neutral if it is only descriptive.Similar to citation purpose classification, we usea supervised approach for this problem.
We train aclassification model using the same features listed inTable 3.
Due to the high skewness in the data (morethan half of the citations are neutral), we use twosetups for binary classification.
In the first setup,the citation is classified as Polarized (Subjective) or(Neutral) Objective.
In the second one, Subjectivecitations are classified as Positive or Negative.
Wefind that this method gives more intuitive results thanusing a 3-way classifier.600Feature DescriptionReference count The number of references that appear in the citation context.Is Separate Whether the target reference appears within a group of references or separate (i.e.
single reference).Closest Verb / Adjective / Adverb The lemmatized form of the closest verb/adjective/adverb to the target reference or its representative or any mentionof it.
Distance is measure based on the shortest path in the dependency tree.Self Citation Whether the citation from the source paper to the target reference is a self citation.Contains 1st/3rd PP Whether the citation context contains a first/third person pronoun.Negation Whether the citation context contains a negation cue.
The list of negation cues is taken from the training data ofthe *SEM 2012 negation detection shared task (Morante and Blanco, 2012).Speculation Whether the citation context contains a speculation cue.
The list is taken from Quirk et al(1985)Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it.
The list ofcues is taken from OpinionFinder (Wilson et al 2005)Contrary Expressions Whether the citation context contains a contrary expression.
The list is taken from Biber (1988)Section The headline of the section in which the citation appears.
We identify five title categorizes: 1) Introduction,Motivation, etc.
2) Background, Prior Work, Previous Work, etc.
3) Experiments, Data, Results, Evaluation, etc.4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines.
Headlines are identified using regularexpressions.Dependency Relations All the dependency relations that appear in the citation context.
For example, nsubj(outperform, algorithm)is one of the relations extracted from ?This algorithm outperforms the one proposed by...?.
The arguments of thedependency relation are replaced by their lemmatized forms.
This type of features has been shown to give goodresults in similar tasks (Athar and Teufel, 2012a).Table 3: The features used for citation purpose and polarity classification4 EvaluationIn this section, we describe the data that we used forevaluation and the experiments that we conducted.4.1 DataWe use the ACL Anthology Network corpus(AAN) (Radev et al 2009; Radev et al 2013) inour evaluation.
AAN is a publicly available collec-tion of more than 19,000 NLP papers.
It includesa manually curated citation network of its papersas well as the full text of the papers and the cit-ing sentences associated with each edge in the ci-tation network.
From this set, we selected 30 pa-pers that have different numbers of incoming cita-tions and that were consistently cited since they werepublished.
These 30 papers received a total of about3,500 citations from within AAN (average = 115 ci-tation/paper, Min = 30, and Max = 338).
These ci-tations come from 1,493 unique papers.
For eachof these citations, we extracted a window of 4 sen-tences around the reference position.
This bringsthe number of sentences in our dataset to a total ofroughly 14,000 sentences.
We refer to this dataset astraining/testing dataset.In addition to this dataset, we created anotherdataset that contains 300 citations that cite 5 papersfrom AAN.
We refer to this dataset as the develop-ment dataset.
This dataset was used to determine thesize of the citation context window, and to developthe feature sets used in the three tasks described inSection 3 above.4.2 AnnotationIn this section, we describe the annotation process.We asked graduate students with good backgroundin NLP (the topic of the annotated sentences) to pro-vide three annotations for each citation example (awindow of 4 sentences around the reference anchor)in the training/testing dataset.
We asked them tomark the sentences that are related to a given tar-get reference.
In addition, we asked them to deter-mine the purpose of citing the target reference bychoosing from the six purpose categories that wedescribed earlier.
We also asked them to determinewhether the citation is negative, positive, or neutral.To estimate the inter-annotator agreement, wepicked 400 sentences from the training/testingdataset and assigned them to two different annota-tors.
We use the Kappa coefficient (Cohen, 1968)to measure the agreement.
The Kappa coefficient isdefined as follows:K =P (A)?
P (E)1?
P (E)(3)where P(A) is the relative observed agreementamong annotators and P(E) is the hypothetical prob-601ability of chance agreement.
The agreement be-tween the two annotators on the context identifica-tion task wasK = 0.89.
On Landis and Kochs (Lan-dis and Koch, 1977) scale, this value indicates al-most perfect agreement.
The agreement on the pur-pose and the polarity classification task were K =0.61 and K = 0.66, respectively; which indicatessubstantial agreement on the same scale.The annotation shows that in 22% of the citationexamples, the citation context consists of 2 or moresentences.
The distribution of the purpose categoriesin the data was: 14.7% criticism, 8.5% comparison,17.7% use, 7% substantiation, 5% basis, and 47%other.
The distribution of the polarity categorieswas: 30% positive, 12% negative, and 58% neutral.4.3 Experimental SetupWe use the CRF++1 toolkit for CRF training andtesting.
We use the Stanford parser to parse the ci-tation text and generate the dependency parse treesof sentences.
We use Weka for classification experi-ments.
We experimented with several classifiers in-cluding: SVM, Logistic Regression (LR), and NaiveBayes.
All the experiments that we conducted usedthe training/testing dataset in a 10-fold cross vali-dation mode.
All the results have been tested forstatistical significance using a 2-tailed paired t-test.4.4 Evaluation of Citation ContextIdentificationWe compare the CRF approach to three baselines.The first baseline (ALL) labels all the sentences inthe citation window of size 4 as INCLUDED in thecitation context.
The second baseline (CS-ONLY)labels the citing sentence only as INCLUDED in thecitation context.
In the third baseline, we use a su-pervised classification method instead of sequencelabeling.
We use Support Vector Machines (SVM)to train a model using the same set of features as inthe CRF approach.Table 4 shows the precision, recall, and F1 scoreof the CRF approach and the baselines.
The re-sults show that our CRF approach outperforms allthe baselines.
It also asserts our expectation that ad-dressing this problem as a sequence labeling prob-lem leads to better performance than individual sen-1http://crfpp.googlecode.com/svn/trunk/doc/index.htmlPrecision Recall F1CRFs 98.5% 82.0% 89.5%ALL 30.7% 100.0% 46.9%CS-ONLY 88.0% 74.0% 80.4%SVM 92.0% 76.4% 83.5%Table 4: Results of citation context identificationtence classification, which is also clear from the na-ture of the task.Feature Analysis: We evaluated the importanceof the features listed in Table 1 by computing thechi-squared statistic for every feature with respect tothe class.
We found that the lexical features (such asdeterminers and conjunction adverbs) are generallymore important than the structural features (such asposition and reference count).
The features shownin Table 1 are listed in the order of their importancebased on this analysis.4.5 Evaluation of Citation PurposeClassificationOur experiments with several classification algo-rithms showed that the SVM classifier outperformsLogistic Regression and Naive Bayes classifiers.Due to space limitations, we only show the resultsfor SVM.
Table 5 shows the precision, recall, andF1 for each of the six categories.
It also shows theoverall accuracy and the Macro-F measure.Feature Analysis: The chi-squared evaluation ofthe features listed in Table 3 shows that both lexicaland structural features are important.
It also showsthat among lexical features, the ones that are limitedto the existence of a direct relation to the target ref-erence (such as closest verb, adjective, adverb, sub-jective cue, etc.)
are most useful.
This can be ex-plained by the fact that the restricting the features tohaving direct dependency relation introduces muchless noise than other features (such as DependencyTriplets).
Among the structural features, the num-ber of references in the citation context showed tobe more useful.4.6 Evaluation of Citation PolarityIdentificationSimilar to the case of citation purpose classification,our experiments showed that the SVM classifier out-performs the other classifiers that we experimentedwith.
Table 6 shows the precision, recall, and F1 for602Criticism Comparison Use Substantiating Basis OtherPrecision 53.0% 55.2% 60.0% 50.1% 47.3% 64.0%Recall 77.4% 43.1% 73.0% 57.3% 39.1% 85.1%F1 63.0% 48.4% 66.0% 53.5% 42.1% 73.1%Accuracy: 70.5%Macro-F: 58.0%Table 5: Summary of Citation Purpose Classification Results (10-fold cross validation, SVM: Linear Kernel, c = 1.0)each of the three categories.
It also shows the over-all accuracy and the Macro-F measure.
The analysisof the features used to train this classifier using chi-squared analysis leads to the same conclusions aboutthe relative importance of the features as describedin the previous subsection.
However, we noticed thatfeatures that are related to subjectivity (Subjectiv-ity Cues, Negation, Speculation) are ranked higherwhich makes sense in the case of polarity classifica-tion.4.7 Impact of Context on ClassificationAccuracyTo study the impact of using citation context in ad-dition to the citing sentence on classification per-formance, we ran two polarity classification exper-iments.
In the first experiment, we used the citingsentence only to extract the features that are usedto train the classifiers.
In the second experiment,we used the gold context sentences (the ones la-beled INCLUDED by human annotators).
Table 6shows the results of the first experiment betweenrounded parentheses and the results of the secondexperiments in square brackets.
The results showthat adding citation context improves the classifica-tion accuracy especially in the subjective categories,specially in the negative category if we want to bemore specific.
This supports our intuition about po-larized citations that authors start their review of thecited work with an objective (neutral) sentence andthen follow it with their criticism if they have any.We also reached to similar conclusions with purposeclassification, but we are not showing the numbersdue to space limitations.4.8 Other Experiments4.8.1 Can We Do Better?In this section, we investigate whether it is possi-ble to improve the performance in the two classifica-tion tasks.
One factor that we believe could have anNegative % Positive % Neutral %Precision 68.7 (66.4) [69.8] 54.9 (52.1) [55.4] 83.6 (82.8) [84.2]Recall 79.2 (71.1) [81.1] 48.1 (45.6) [46.3] 95.5 (95.1) [95.3]F1 73.6 (68.7) [75.0] 51.3 (48.6) [50.4] 89.1 (88.5) [89.4]Accuracy: 81.4 (74.2) [84.2] %Macro-F: 71.3 (62.1) [74.2] %Table 6: Summary of Citation Polarity Classification Re-sults (10-fold cross validation, SVM: Linear Kernel, c =1.0).
Numbers between rounded parentheses are whenonly the explicit citing sentence is used (i.e.
no context).Numbers in square brackets are when the gold standardcontext is used.impact on the result is the size of the training data.To examine this hypothesis, we ran the experimenton different sizes of data.
Figure 1 shows the learn-ing curve of the two classifiers for different sizes oftraining data.
The accuracy increases as more train-ing data is available so we can expect that with evenmore data, we can do even better.4.8.2 Relation Between CitationPurpose/Polarity and Citation CountThe main motivation of this work is our hypothet-ical assumption that using NLP for analyzing cita-tions gives a clearer picture of the impact of the citedwork.
As a way to check the validity of this assump-tion, we study the correlation between the counts ofthe different purpose and polarity categories.
Wealso study the correlation between these categoriesand the total number of citations that a paper re-ceived since it was published.
We use the train-ing/testing dataset and the gold annotations for thisstudy.We compute the Pearson correlation coefficientbetween the counts of citations from the differentcategories that a paper received per year since itspublication.
We found that, on average, the correla-tion between positive and negative citations is neg-ative (AVG P = -0.194) and that the correlation be-603253545556575850 500 1000 1500 2000 2500 3000AccuracyDataset SizePurpose AccuracyPolarity AccuracyFigure 1: The effect of size of the data set size on theclassifiers accuracy.tween the count of positive citations and the totalnumber of citations is higher than the correlation be-tween negative citations and total citations (AVG P =0.531 for positive vs. AVG P = 0.054 for negative).Similarly, we noticed that there is a higher posi-tive correlation between Use citations and total ci-tations than in the case of both Substantiation andBasis.
This can be explained by the intuition thatpublications that present new algorithms, tools, orcorpora that are used by the research community be-come more and more popular with time and thus re-ceive more and more citations.Figure 2 shows the result of running our pur-pose classifier on all the citations to Papineni etal.
?s (2002) paper about Bleu, an automatic metricfor evaluating Machine Translation (MT) systems.The figure shows that this paper receives a highnumber of Use citations.
This makes sense for a pa-per that describes an evaluation metric that has beenwidely used in the MT area.
The figure also showsthat in the recent years, this metric started to receivesome Criticizing citations that resulted in a slight de-crease in the number of Use citations.
Such a tempo-ral analysis of citation purpose and polarity is usefulfor studying the dynamics of research.
It can alsobe used to detect the emergence or de-emergence ofresearch techniques.010203040506070802001 2003 2005 2007 2009 2011CriticizingComparisonUseSubstantiatingBasisOtherFigure 2: Change in the purpose of the citations to Pap-ineni et al(2002)5 ConclusionIn this paper, we presented methods for three tasks:citation context identification, citation purpose clas-sification, and citation polarity classification.
Thiswork is motivated by the need for more accuratebibliometric measures that evaluates the impact ofresearch both qualitatively and quantitatively.
Ourexperiments showed that we can classify the pur-pose and polarity of citation with a good accuracy.
Italso showed that using the citation context improvesthe classification accuracy and increases the num-ber of polarized citations detected.
For future work,we plan to use the output of this research in severalapplications such as predicting future prominence ofpublications, studying the dynamics of research, anddesigning more accurate bibliometric measures.AcknowledgementThis research is supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) viaDepartment of Interior National Business Center(DoI/NBC) contract number D11PC20153.
TheU.S.
Government is authorized to reproduce and dis-tribute reprints for Governmental purposes notwith-standing any copyright annotation thereon.
Dis-claimer: The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of IARPA, DoI/NBC, or the U.S. Government.604ReferencesDaryl E. and Soumyo D. Moitra.
1975.
Content analysisof references: Adjunct or alternative to citation count-ing?
Social Studies of Science, 5(4):pp.
423?441.Amjad Abu-Jbara and Dragomir Radev.
2011.
Coherentcitation-based summarization of scientific papers.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 500?509, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Amjad Abu Jbara and Dragomir Radev.
2012.
Refer-ence scope identification in citing sentences.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 80?90, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Awais Athar and Simone Teufel.
2012a.
Context-enhanced citation sentiment detection.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 597?601, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Awais Athar and Simone Teufel.
2012b.
Detection ofimplicit citations for sentiment detection.
In Proceed-ings of the Workshop on Detecting Structure in Schol-arly Discourse, pages 18?26, Jeju Island, Korea, July.Association for Computational Linguistics.Awais Athar.
2011.
Sentiment analysis of citations us-ing sentence structure-based features.
In Proceedingsof the ACL 2011 Student Session, pages 81?87, Port-land, OR, USA, June.
Association for ComputationalLinguistics.Douglas Biber.
1988.
Variation across speech and writ-ing.
Cambridge University Press, Cambridge.Susan Bonzi.
1982.
Characteristics of a literature as pre-dictors of relatedness between cited and citing works.Journal of the American Society for Information Sci-ence, 33(4):208?216.J.
Cohen.
1968.
Weighted kappa: Nominal scale agree-ment with provision for scaled disagreement or partialcredit.
Psychological Bulletin, 70:213?220.Leo Egghe.
2006.
Theory and practise of the g-index.Scientometrics, 69:131?152.E.
Garfield, Irving H. Sher, and R. J. Torpie.
1984.
TheUse of Citation Data in Writing the History of Science.Institute for Scientific Information Inc., Philadelphia,Pennsylvania, USA.Eugene Garfield.
1964.
Can citation indexing be auto-mated?E.
Garfield.
1979.
Is citation analysis a legitimate evalu-ation tool?
Scientometrics, 1(4):359?375.Eugene Garfield.
1994.
The thomson reuters impact fac-tor.J.
E. Hirsch.
2005.
An index to quantify an individual?sscientific research output.
Proceedings of the NationalAcademy of Sciences, 102(46):16569?16572, Novem-ber.J.
E. Hirsch.
2010.
An index to quantify an individ-ual?s scientific research output that takes into accountthe effect of multiple coauthorship.
Scientometrics,85(3):741?754, December.T.
L. Hodges.
1972.
Citation indexing-its theoryand application in science, technology, and humani-ties.
Ph.D. thesis, University of California at Berke-ley.Ph.D.
thesis, University of California at Berkeley.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.J.
Richard Landis and Gary G. Koch.
1977.
The Mea-surement of Observer Agreement for Categorical Data.Biometrics, 33(1):159?174, March.Michael H. MacRoberts and Barbara R. MacRoberts.1984.
The negational reference: Or the art of dissem-bling.
Social Studies of Science, 14(1):pp.
91?94.Saif Mohammad, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Using ci-tations to generate surveys of scientific paradigms.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 584?592, Boulder, Colorado, June.
Associationfor Computational Linguistics.Roser Morante and Eduardo Blanco.
2012.
*sem 2012shared task: resolving the scope and focus of nega-tion.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics - Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, SemEval?12, pages 265?274, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.M.
J. Moravcsik and P. Murugesan.
1975.
Some resultson the function and quality of citations.
Social Studiesof Science, 5:86?92.Hidetsugu Nanba and Manabu Okumura.
1999.
To-wards multi-paper summarization using reference in-formation.
In IJCAI ?99: Proceedings of the Six-605teenth International Joint Conference on Artificial In-telligence, pages 926?931, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Hidetsugu Nanba, Noriko Kando, Manabu Okumura, andOf Information Science.
2000.
Classification of re-search papers using citation links and citation types:Towards automatic review article generation.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Vahed Qazvinian and Dragomir R. Radev.
2008.
Scien-tific paper summarization using citation summary net-works.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 689?696, Manchester, UK, August.
Coling 2008Organizing Committee.Vahed Qazvinian and Dragomir R. Radev.
2010.
Identi-fying non-explicit citing sentences for citation-basedsummarization.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 555?564, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Vahed Qazvinian, Dragomir R. Radev, and ArzucanOzgur.
2010.
Citation summarization throughkeyphrase extraction.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(Coling 2010), pages 895?903, Beijing, China, Au-gust.
Coling 2010 Organizing Committee.Vahed Qazvinian, Dragomir R. Radev, Saif Mohammad,Bonnie Dorr, David Zajic, Michael Whidby, and Tae-sun Moon.
2013.
Generating extractive summaries ofscientific paradigms.
Journal of Artificial IntelligenceResearch.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Grammarof the English Language.
Longman, London.Dragomir R. Radev, Pradeep Muthukrishnan, and VahedQazvinian.
2009.
The acl anthology network corpus.In NLPIR4DL ?09: Proceedings of the 2009 Workshopon Text and Citation Analysis for Scholarly Digital Li-braries, pages 54?61, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Dragomir R. Radev, Pradeep Muthukrishnan, VahedQazvinian, and Amjad Abu-Jbara.
2013.
The aclanthology network corpus.
Language Resources andEvaluation, pages 1?26.Ina Spiegel-Ro?sing.
1977.
Science Studies: Bibliomet-ric and Content Analysis.
Social Studies of Science,7(1):97?113, February.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.
InIn Proc.
of EMNLP-06.GEOFF THOMPSON and YE YIYUN.
1991.
Evalu-ation in the reporting verbs used in academic papers.Applied Linguistics, 12(4):365?382.Melvin Weinstock.
1971.
Citation Indexes.
Encyclope-dia of Library and Information Science.Michael Alan Whidby.
2012.
Citation handling: Pro-cessing citation text in scientific documents.
In MasterThesis.Howard D. White.
2004.
Citation analysis and discourseanalysis revisited.
Applied Linguistics, 25(1):89?116.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi, ClaireCardie, Ellen Riloff, and Siddharth Patwardhan.
2005.Opinionfinder: a system for subjectivity analysis.
InProceedings of HLT/EMNLP on Interactive Demon-strations, HLT-Demo ?05, pages 34?35, Stroudsburg,PA, USA.
Association for Computational Linguistics.J.
M. Ziman.
1968.
Public knowledge: An essay con-cerning the social dimension of science.
CambridgeU.P., London.606
