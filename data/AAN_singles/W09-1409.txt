Proceedings of the Workshop on BioNLP: Shared Task, pages 68?76,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExtraction of biomedical events using case-based reasoningMariana L. Neves Jos?
M. CarazoBiocomputing Unit Biocomputing UnitCentro Nacional de Biotecnolog?a - CSIC Centro Nacional de Biotecnolog?a - CSICC/ Darwin 3, Campus de Cantoblanco,28049, Madrid, SpainC/ Darwin 3, Campus de Cantoblanco,28049, Madrid, Spainmlara@cnb.csic.es carazo@cnb.csic.esAlberto Pascual-MontanoDepartamento de Arquitectura de ComputadoresUniversidad Complutense de Madrid, Facultad deCiencias F?sicas28040, Madrid, Spainpascual@fis.ucm.esAbstractThe BioNLP?09 Shared Task on Event Extrac-tion presented an evaluation on the extractionof biological events related to genes/proteinsfrom the literature.
We propose a system thatuses the case-based reasoning (CBR) machinelearning approach for the extraction of the enti-ties (events, sites and location).
The mappingof the proteins in the texts to the previously ex-tracted entities is carried out by some simplemanually developed rules for each of the argu-ments under consideration (cause, theme, siteor location).
We have achieved an f-measure of24.15 and 21.15 for Task 1 and 2, respectively.1 IntroductionThe increasing amount of biological data gener-ated by the high throughput experiments haslead to a great demand of computational tools toprocess and interpret such amount of informa-tion.
The protein-protein interactions, as well asmolecular events related to one entity only, arekey issues as they take part in many biologicalprocesses, and many efforts have been dedicateto this matter.
For example, databases are avail-able for the storage of such interaction pairs,such as the Molecular INTeraction Database(Chatr-aryamontri et al, 2007) and IntAct(Kerrien et al, 2007).In the field of text mining solutions, many ef-forts have been made.
For example, the Bio-Creative II protein-protein interaction (PPI) task(Krallinger, Leitner, Rodriguez-Penagos, & Va-lencia, 2008) consists of four sub-tasks, includ-ing the extraction of the protein interaction pairsin full-text documents, achieving an f-measureof up to 0.30.
The initiative of annotation ofboth Genia corpus (J. D. Kim, Ohta, & Tsujii,2008) and BioInfer (Pyysalo et al, 2007) is an-other good example.The BioNLP?09 Shared Task on Event Ex-traction (J.-D. Kim, Ohta, Pyysalo, Kano, &Tsujii, 2009) proposes a comparative evaluationfor the extraction of biological events related toone or more gene/protein and even other typesof entities related to the localization of the re-ferred event in the cell.
The types of events thathave been considered in the shared task werelocalization, binding, gene expression, transcrip-tion, protein catabolism, phosphorylation, regu-lation, positive regulation and negativeregulation.
A corpus that consisted of 800, 150and 260 PubMed documents (title and abstracttext only) was made available for the training,development test and testing datasets, respec-tively.
For all documents, the proteins that tookpart in the events were provided.The shared task organization proposed threetasks.
Task 1 (Event detection and characteriza-tion) required the participants to extract theevents from the text and map them to its respec-68tive theme(s), as an event may be associated toone or more themes, e.g.
binding.
Also, someevents may have only a gene/protein as theme,e.g.
protein catabolism, while some other maybe also associated to another event, e.g.
regula-tion events.
Task 2 (Event argument recognition)asked the participants to provide the many ar-guments that may be related to the extractedevent, such as its cause, that may be an anno-tated or one of the previously extracted events.Other arguments include site and localization,which should be first extracted from the texts bythe system, as they do not come annotated in thedocuments.
Task 3 (Recognition of negation andspeculations) evaluates the presence of negationsand speculation related to the previously ex-tracted events.Our group has participated in this shared taskwith a system implemented with the case-basedreasoning (CBR) machine learning techniqueas well as some manual rules.
We have pre-sented results for tasks 1 and 2 exclusively.
Thesystem described here is part of the Moara pro-ject1 and was developed in Java programminglanguage and use MySQL database.2 MethodsCase-based reasoning (CBR) (Aamodt & Plaza,1994) is the machine learning method that wasused for extracting the terms and events hereproposed and consists of first learning casesfrom the training documents, by means of savingthem in a base of case, and further retrieving acase the most similar to a given problem duringthe testing step, from which will be given thefinal solution, hereafter called ?case-solution?.One of the advantages of the CBR algorithm isthe possibility of getting an explanation of whyto a given token has been attributed a certaincategory, by means of checking the features thatcompose the case-solution.
Additionally, anddue to the complexity of the tasks, a rule-basedpost-processing step was built in order to mapthe previously extracted terms and events amongthemselves.1 http://moara.dacya.ucm.es2.1 Retaining the casesIn this first step, documents of the training data-set are tokenized according to spaces and punc-tuations.
The resulting tokens are represented inthe CBR approach as cases composed of somepredefined features that take into account themorphology and grammatical function of thetokens in the text as well as specific featuresrelated to the problem under consideration.
Theresulting cases are then stored in a base of caseto be further retrieved (Figure 1).Figure 1: Training step in which cases are repre-sented by some pre-defined features and furthersaved to a base.Regarding the features that compose a case,these were the ones that were considered duringthe training and development phases: the tokenitself (token); the token in lower case (lower-case); the stem of the token (stem); the shape ofthe token (shape); the part-of-speech tag(posTag); the chunk tag (chunkTag); a biomedi-cal entity tag (entityTag); the type of the term(termType); the type of the event (eventType);and the part of the term in the event (eventPart).The stem of a token was extracted using anavailable Java implementation2 of the Porter al-gorithm (Porter, 1980), while the part-of-speech,chunk and bio-entity tags were taken from theGENIA Tagger (Tsuruoka et al, 2005).The shape of a token is given by a set of char-acters that represent its morphology: ?a?
forlower case letters, ?A?
for upper case letters, ?1?for numbers, ?g?
for Greek letters, ?p?
for stop-2 http://www.tartarus.org/~martin/PorterStemmer69words3, ?$?
for identifying 3-letters prefixes orsuffixes or any other symbol represented by it-self.
Here are some few example for the shapefeature: ?Dorsal?
would be represented by ?Aa?,?Bmp4?
by ?Aa1?, ?the?
by ?p?, ?cGKI(alpha)?by ?aAAA(g)?, ?patterning?
by ?pat$a?
(?$?symbol separating the 3-letters prefix) and ?ac-tivity?
by ?a$vity?
(?$?
symbol separating the 4-letters suffix).
No repetition is allowed in thecase of the ?a?
symbol for the lower case letters.Figure 2: Example of the termType, eventType andpartEvent features.The last three features listed above are specificto the event detection task and were extractedfrom the annotation files (.a1 and .a2) that arepart of the corpus.
The termType feature is usedto identify the type of the term in the event prob-lem, and it is extracted from the term lines ofboth annotation files .a1 and .a2, i.e.
the oneswhich the identifiers starts with a ?T?.
TheeventType features represent the event itself andit is extracted from the event lines of .a2 annota-tion file, i.e.
the ones that starts with an ?E?.
Fi-nally, eventPart represents the token accordingto its role, i.e.
entity, theme, cause, site and loca-tion.
The termType, eventType and eventPartfeatures are the hereafter called ?feature-problem?, the features that are unknown to thesystem in the testing phase and which values areto be given by the case-solution.
Figure 2 illus-trate one example of these features for an extractof the annotation of the document ?1315834?from the training dataset.Usually, one case corresponds for each tokenof the documents in the training dataset.
How-ever, more than one case may be created from atoken, as well as none at all, depending on thepredefined features.
For example, some tokensmay derive in more than one case due to theshape feature, as for example, ?patterning?3http://www.dcs.gla.ac.uk/idom/ir_resources/linguistic_utils/(?pat$a?, ?a$ing?, ?a?).
Also, according to theretaining strategy, some tokens may be associ-ated to no case at all, for example, by restrictingthe value of a determined feature as the retainingstrategy.
In order to reduce the number of re-tained cases, and consequently reduce the furtherretrieving time, only those tokens related to anevent are retained, i.e., tokens with not nullvalue for the termType feature.The text of a document may be read in theforward or backward direction during the train-ing step, and even combining both of them(Neves, Chagoyen, Carazo, & Pascual-Montano,2008).
Here, we have considered the forwarddirection exclusively.
Also, another importantpoint is the window of tokens under considera-tion when setting the features of a case, if takinginto account only the token itself or also the sur-rounding tokens, the ones which come before orafter it.
Here we consider a window of (-1,0),i.e., for each token, we get the feature of the to-ken itself and of the preceding one, exclusively.Training Testing Features / Tokens-1 0 -1 0stem 9 9 9 9shape  9  9posTag 9 9 9 9chunkTagentityTag 9 9 9 9termType 9 9 9 9eventType 9 9 9partEvent 9 9 9Table 1: Selected features in the training and testingsteps for the tokens ?0?
and ?-1?.
The last three fea-tures are the ones to be inferred.Many experiments have been carried out in or-der to choose the best set of features (Table 1).The higher the number of features under consid-eration, the greater is the number of cases to beretained and the higher is the time needed tosearch for the case-solution.
He relies thereforethe importance of choosing a small an efficientset of features.
For this reason, the shape fea-tures has not been considered for the precedingtoken (-1) in order to reduce the number ofcases, as this shape usually result in more thanone case per token.
The termType feature is atthe same time known and unknown in the testingstep.
It is know for the protein terms but is un-70known for the remaining entities (events, sitesand locations).By considering these features for the 800documents in the training set, about 26,788unique cases were generated.
It should be notedthat no repetition of cases with the same valuesfor the features are allowed, instead a field forthe frequency of the case is incremented to keeptrack of the number of times that it has appearedduring the training phase.
The frequency rangegoes from 1 (more than 22,000 cases) to 238(one case only).2.2 Retrieving a caseWhen a new document is presented to the sys-tem, it is first read in the forward direction andtokenized according to space and punctuationand the resulting tokens are mapped to cases offeatures, exactly as discussed in the retainingstep.
The only difference here is the set of fea-ture (cf.
Table 1), as some of them are unknownto the system and are the ones to be inferredfrom the cases retained during the training step.Figure 3: Retrieval procedure to choose the mostcase-solution with higher frequency and based onMMF and MFC parameters.For each token, the system first creates a case(hereafter called ?case-problem?)
based on thetesting features and proceeds to search the baseof cases for the case-solution the most similar tothis case-problem (Figure 3).
It should be notedthat a token may have more than one case-problem, depending of the values of the shapefeature.
The best case-solution among the onesfound by the system will be the one with thehigher frequency.
The system always tries tofind a case-solution with the higher number offeatures that have exactly the same value of thecase-problem?s respective features.
The stem isthe only mandatory feature which value must bealways matched between the case-problem andthe case-solution.
The value of the two features-problem (eventType and partEvent) will begiven by the values of the case-solution?s re-spective features.
If no case solution is found,the token is considered of not being related tothe event domain in none of its parts (entity,theme, cause, etc.
).Two parameters have been taken into consid-eration in the retaining strategy: the minimummatching feature (MMF) and the minimum fre-quency of the case (MFC).
The first one set theminimum features that should be matched be-tween the case-problem and the case-solution, asthe higher the number of equal features betweentheses cases, the more precise is the decisioninferred from the case-solution.On the other hand, the MFC parameter re-stricts the cases that are to be considered by thesearch strategy, the ones with frequency higherthan the value specified by this parameter.
Thehigher the minimum frequency asked for a case,the lower is the number of cases under consid-eration and the lower is the time for obtainingthe case-solution.
From the 26,788 cases wehave retained during the training phase, about22,389 of them appeared just once and wouldnot be considered by the searching procedure ifthe MFC parameter was set to 2, for example,therefore reducing the searching time.Experiments have been carried out in order todecide the values for both parameters and it re-sulted that a better performance is achieved (cf.3) by setting the MFC to a value higher than 1.On the other hand, experiments have shown thatthe recall may decrease considerably when re-stricting the MMF parameter.By repeating this procedure for all the tokensof the document, the latter may be then consid-ered as being tagged with the event entities.However, in order to construct the output filerequired by the shared task organization, somemanual rules have been created in order to mapthe events mapped to its respective arguments,as described in the next section.2.3 Post-processing rulesFor the tasks 1 and 2, the participants wereasked to output the events present in the pro-vided texts along with their respective argu-ments.
The events have been already extractedin the previous step; the tokens that were taggedas ?Entity?
for the ?partEvent?
feature (cf.
Fig-71ure 2), hereafter called ?event-entity?.
This en-tity is the start point from which to search for thearguments which are incrementally extractedfrom the text in the following order: theme,theme 2, cause, site and location.
Figure 4 re-sumes the rules for each of the arguments.Figure 4: Resume of the post-processing rules foreach type of argument.Themes: The theme-candidates for an event-entity are the annotated proteins (.a1 file) as wellas the events themselves, in the case of the regu-lation, positive regulation and negative regula-tion events.
The first step is then to try to mapeach event to its theme and in case that no themeis found, the event is not considered anymore bythe system and it is not printed to the output file.The theme searching strategy starts from theevent-entity and consists of reading the text inboth directions alternatively, one token in theforward direction followed by one token in thebackward direction until a theme-candidate isfound (Figure 5).
The system halts if the end ofthe sentence is found or if the specified numberof tokens in each direction is reached, 20 for thetheme.
By analyzing some of the false negativesreturned from the experiments with the devel-opment dataset, we have learned that few eventsare associated to themes present in a differentsentence and although aware these cases, wehave decided to restrict the searching to the sen-tence boundaries in order to avoid a high num-ber of false positives.In the case of a second theme, allowed forbinding events only, a similar searching strategyis carried out, except that here the system readsup of 10 tokens in each direction, starting fromthe theme entity previously extracted.Cause: The cause-candidates are also the an-notated proteins and, starting from the event-entity, a similar search is carried out, restrictedup to 30 tokens in each direction and to theboundaries of the same sentence.
This procedureis carried out for the regulation, positive regula-tion and negative regulation events only and theonly extra restriction is that the candidate shouldnot be the protein already assigned as theme.
Ifno candidate is found, the system considers thatthere is no cause associated to the event underconsideration.Site and Location: Here the candidates arethe tokens tagged with the values of ?Entity?
forthe termType feature, and ?Site?
and ?Location?for the partEvent feature, respectively.
Thesearch for the site is carried out for the bindingand phosphorylation events and the locationsearch for the localization event only.
The pro-cedure is restricted to the sentence boundariesand up to 20 and 30 tokens, respectively, startingfrom the event-entity.
Once again, if not candi-date is found, the system consider that there isno site or location associated to the event underconsideration.Figure 5: Contribution of each class of error to the275 false positives analyzed here.3 ResultsThis section presents the results of the experi-ments carried out with the development and theblind test datasets as well as an analysis of thefalse negatives and false positives.
Results herewill be presented for tasks 1 and 2 in terms ofprecision, recall and f-measure.Experiments have been carried out with thedevelopment dataset in order to decide the bestvalue of the MMF and MFC parameters (cf.2.2).
Figure 6 shows the variation of the F-measure according to both parameters for thevalues of 1, 3, 4, 5, 6, 7 and 8 for MMF; and 1,2, 5, 10, 15, 20 and 50 for MFC.Usually, recall is higher for a low value ofMFC, as the searching for the case-solution is72carried out over a greater number of cases andthe possibility of finding a good case-problem ishigher.
On the other hand, precision increaseswhen few cases are under considered by thesearch strategy, as fewer decisions are taken andthe cases-solution have usually a high frequency,avoiding decision based on ?weak?
cases of fre-quency 1, for example.Figure 6 shows that the best value for MFCranges from 2 to 20 and for MMF from 5 to 7and the best f-measure result is found for thevalues of 2 and 6 for these parameters (f2m6),respectively.
As these experiments have beencarried out after the deadline of the test dataset,the run that was submitted as the final solutionwas the one with the values of 2 and 1 for theMFC and MMF parameters (f2m1), respectively.Table 3 and 4 resumes the results obtained forthe test dataset with the configuration that wassubmitted (f2m1), and the best one (f2m6) afteraccomplishing the experiments above described.Results have slightly improved by only trying tochoose the best values for the parameters hereconsidered.F-Measure79111315171921231 3 4 5 6 7 8Minimum matching features1 2 5 10 15 20 50Figure 6: F-Measure for the development dataset interms of the MFC (curves) and the MMF (x-axis).An automatic analysis of the false positives andfalse negatives has been performed for the de-velopment dataset and for the results obtainedwith the final submission (f2m1), a total of 2502false positives and 1300 false negatives.
Wehave found out that the mistakes are relatedmainly to the retrieving of the case-solution andto the mapping of an event to its arguments.
Themistakes have been classified in seven groupsdescribed below and figures 7 and 8 show thepercent contribution of each class for the falsepositives and false negatives, respectively.Events composed of more than one token(1): this mistake happens when the system isable to find the event with its correct type andarguments but with only part of its tokens, suchas ?regulation?
instead of ?up-regulation?
and?reduced?
or ?levels?
instead of ?reduced lev-els?, both in document 10411003.
This is mainlydue to our tokenization strategy of separating thetokens according to all punctuation and symbols(including hyphens) and also due to the evalua-tion method that seems not consider alternativesto the text of an event.
This mistake always re-sults in one false positive and one false negative.tasks /results recall precision f-measure(f2m1) 28.63 20.88 24.15 task 1(f2m6) 27.18 23.92 25.45(f2m1) 25.02 18.32 21.15 task 2(f2m6) 24.49 21.63 22.97Table 3: Results for the test dataset (tasks 1 and 2).
(f2m1) (f2m6) Results /Events p r fm p r fmprot.
catab.
78.6 55.0 64.7 71.4 55.6 65.5phosphoryl.
49.6 56.1 52.7 46.0 55.2 50.2transcript.
48.9 19.8 28.1 38.7 29.6 33.5neg.
reg.
9.8 7.9 8.8 7.9 7.7 7.8pos.
reg.
10.0 6.6 7.9 10.2 8.0 9.0regulation 8.6 4.5 5.9 7.5 5.3 6.3localizat.
28.2 42.9 34.0 23.3 48.9 33.3gene expr.
51.8 55.1 53.4 52.6 61.2 56.6binding 19.5 12.1 14.9 22.4 14.4 17.5Table 4: Results by event for Task 2 on test dataset.Events and arguments in different sentencesof the text (2):  as we already discussed in sec-tion 2.3, our arguments searching strategy is re-stricted to the boundaries of the sentence.
Someexamples of this mistake may be found indocument 10395645 in which two events of thetoken ?activation [1354-1364]?
is mapped to thethemes ?caspase-6 [1190-1199]?
and ?CPP32[1165-1170]?, both located in a different sen-tence.
This mistake usually affects only the falsenegatives but may cause also a false positive ifthe system happens to find a valid (wrong) ar-73gument in the same sentences for the event un-der consideration.False Positivescasedecision (3);74,3composedtokens (1);5,2site/locationdetection(7); 1,6themedetection(5); 14,6causedetection(6); 1,6event type(4); 2,7Figure 7: Percent contribution of each error to thefalse positives.False Negativesthemedetection(5); 56,2site/locationdetection(7); 0,7causedetection(6); 4,2event type(4); 10,0differentsentences(2); 1,4composedtokens (1);10,4casedecision (3);17,2Figure 8: Percent contribution of each error to thefalse negatives.Decision for a case (3): this class of error is dueto the selection of a wrong case-solution and weinclude in this class mistakes due to two situa-tions: when the system fails to find any case-solution for an event token (false negative) orwhen a case-solution is found for a non-eventtoken (false positive).
The first situation is onlydependent of the searching strategy and its twoparameters (MMF and MFC) while the secondone is also related to the post-processing step, ifthe latter succeeds to find a theme for the incor-rectly extracted event.
An example of a falsenegative that falls in this group is ?dysregulation[727-740]?
from document 10229231 that failedto be mapped to a case-solution.
Regarding thefalse positives, this class of mistake is the major-ity of them and it is due to the low precision ofthe system that frequently is able to find cases-solution associated to tokens that are not eventsat all, such as the token ?transcript [392-402]?
ofdocument 10229231.
It should be noted that theincorrect association of a token to a case-solution does not result in a false positive a pri-ori, but only if the post-processing step happento find a valid theme to it, a mistake further de-scribed in group 5.Wrong type of the event (4): this class ofmistake is also due to the wrong selection of acase-solution, but the difference here is that thetoken is really an event, but the case-solution isof the wrong type, i.e.
it has a wrong value forthe eventType feature.
The causes of this mis-take are many, such as, the selection of features(cf.
Table 1) or the value of the MFC parameterthat may lead to the selection of a wrong butmore frequent case.
We also include in thisgroup the few false negatives mistakes in whicha token is associated to more than one type ofevent in the gold-standard, such as the token?Overexpression [475-489]?
from document10229231 that is associated both to a Gene Ex-pression and to a Positive Regulation event.
Oneway of overcome it would be to allow the sys-tem to associated more than one case to a token,taking the risk of decreasing the precision.Theme detection (5): in this group falls morethan half of the false negatives and we includehere only those mistakes in which the token wascorrectly associated to a case-solution of the cor-rect type.
These mistakes may be due to a vari-ety of situations related to the theme detection,such as: the association of the event to anotherevent when it should have been done to a proteinor vice-versa (for the regulation events); themapping of a binding event to one theme onlywhen it should have been two theme or vice-versa; the association of the event to the wrongprotein theme, especially when there is morethan one nearby; and even not being able to findany theme at all.
Also, half of theses mistakeshappen when an event is associated to more thanone theme separately, not as a second theme.
Forexample, the token ?associated [278-288]?, fromdocument 10196286, is associated in the goldstandard to three themes ?
?tumor necrosis fac-tor receptor-associated factor (TRAF) 1 [294-351]?, ?2 [353-354]?
and ?3 [359-360]?
?
and74we were only able to extract the first of them.This is due to the fact that we restrict the systemto search only one ?first?
and one ?second?theme for each event.Cause detection (6): similar to the previousclass, these mistakes happens when associating acause to an event (regulation events only) whenthere is no cause related to it or vice-versa.
Forexample, in document 10092805, the system hascorrectly mapped the token ?decreases [1230-1239]?
to the theme ?4E-BP1 [1240-1246]?
butalso associated to it an inexistent cause ?4E-BP2[1315-1321]?.
The evaluation of Task 2 does notallow the partial evaluation of an event andtherefore a false positive and a false negativewould be returned for the example above.Site/Location detection (7): this error issimilar to the previous one but related only tobinding, phosphorylation and localizationevents, when the system fails to associate a siteor a location to an event or vice-versa.
For ex-ample, in document 10395671, the token ?phos-phorylation [1091-1106]?
was correctly mappedto the theme ?Janus kinase 3 [1076-1090]?
butwas also associated to an inexistent site ?DNA[1200-1203]?.
Once again, the evaluation ofTask 2 does not allow the partial evaluation ofthe event and a false positive and a false nega-tive would be returned.We have also carried out an evaluation of ourown in order to check the performance of oursystem only on the extraction the entities (event,site and location), not taking into account theassociation to the arguments.
Table 5 resumesthe values of precision, recall and f-measure foreach type of term.
The high recall confirm thatmost of the entities were successful extractedalthough the precision is not always satisfactory,proving that the tagging of the entities is not ashard a task as it is the mapping of the arguments.Additional results and more a detailed analysisof the errors may be found at Moara page4.4 ConclusionsResults show that our system has performedrelatively well using a simple methodology of amachine learning based extraction of the entitiesand manual rules developed for the post-4 http://moara.dacya.ucm.es/results_shared_task.htmlprocessing step.
The analysis of the mistakespresented here confirms the complexity of thetasks proposed but not the extraction of theevent terms (cf.
Table 5).We consider that the part of our system thatrequires most our attention is the retrieval of thecase-solution and the theme detection of thepost-processing step, in order to increase theprecision and recall, respectively.
The decisionof searching for a second theme and of associat-ing a single event separately to more than onetheme is hard to be accomplished by manualrules and could better be learned automaticallyusing a machine learning algorithm.
(f2m1) (f2m6) Eventsp r fm p r fmprot.
catab.
70.8 89.5 79.1 69.6 84.2 76.2phosphoryl.
75.0 94.7 83.7 79.1 89.5 84.0transcript.
22.7 75.9 34.9 36.4 74.6 48.9neg.
reg.
26.4 56.5 36.0 25.3 43.5 32.0pos.
reg.
24.3 63.7 35.2 26.5 59.1 36.6regulation 20.8 65.9 31.7 22.1 52.5 31.1localizat.
47.7 79.5 59.6 49.1 66.7 56.5gene expr.
46.5 83.4 59.7 50.8 80.2 62.2binding 29.7 71.1 41.9 29.7 64.4 40.7entity 12.5 55.3 20.4 16.8 50.0 25.1TOTAL 27.5 69.2 39.4 30.9 62.9 41.4Table 5: Evaluation of the extraction of the event andsite/location entities for the development dataset.The automatic analysis of the false positive andfalse negative mistakes is a hard task since nohint is given for the reason of the mistake by theevaluation system, if due to the event type or towrong theme, an incorrectly association to anevent or even a missing cause or site.AcknowledgmentsThis work has been partially funded by theSpanish grants BIO2007-67150-C03-02, S-Gen-0166/2006, PS-010000-2008-1, TIN2005-5619.APM acknowledges the support of the SpanishRam?n y Cajal program.
The authors acknowl-edge support from Integromics, S.L.ReferencesAamodt, A., & Plaza, E. (1994).
Case-Based Reason-ing: Foundational Issues, Methodological Varia-tions, and System Approaches.
AICommunications, 7(1), 39-59.75Chatr-aryamontri, A., Ceol, A., Palazzi, L. M.,Nardelli, G., Schneider, M. V., Castagnoli, L., etal.
(2007).
MINT: the Molecular INTeraction da-tabase.
Nucleic Acids Res, 35(Database issue),D572-574.Kerrien, S., Alam-Faruque, Y., Aranda, B., Bancarz,I., Bridge, A., Derow, C., et al (2007).
IntAct--open source resource for molecular interactiondata.
Nucleic Acids Res, 35(Database issue),D561-565.Kim, J.-D., Ohta, T., Pyysalo, S., Kano, Y., & Tsujii,J.
i.
(2009).
Overview of BioNLP'09 Shared Taskon Event Extraction.
Paper presented at the Pro-ceedings of Natural Language Processing in Bio-medicine (BioNLP) NAACL 2009 Workshop,Boulder, CO, USA.Kim, J. D., Ohta, T., & Tsujii, J.
(2008).
Corpus an-notation for mining biomedical events from litera-ture.
BMC Bioinformatics, 9, 10.Krallinger, M., Leitner, F., Rodriguez-Penagos, C., &Valencia, A.
(2008).
Overview of the protein-protein interaction annotation extraction task ofBioCreative II.
Genome Biol, 9 Suppl 2, S4.Neves, M., Chagoyen, M., Carazo, J. M., & Pascual-Montano, A.
(2008).
CBR-Tagger: a case-basedreasoning approach to the gene/protein mentionproblem.
Paper presented at the Proceedings ofthe BioNLP 2008 Workshop at ACL 2008, Co-lumbus, OH, USA.Porter, M. (1980).
An algorithm for suffix stripping.Program, 14(3), 130-137.Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J.,Boberg, J., Jarvinen, J., et al (2007).
BioInfer: acorpus for information extraction in the biomedi-cal domain.
BMC Bioinformatics, 8, 50.Tsuruoka, Y., Tateishi, Y., Kim, J.-D., Ohta, T.,McNaught, J., Ananiadou, S., et al (2005).
De-veloping a Robust Part-of-Speech Tagger forBiomedical Text.
Paper presented at the Advancesin Informatics - 10th Panhellenic Conference onInformatics.76
