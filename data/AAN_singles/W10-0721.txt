Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 139?147,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCollecting Image Annotations Using Amazon?s Mechanical TurkCyrus Rashtchian Peter Young Micah Hodosh Julia HockenmaierDepartment of Computer ScienceUniversity of Illinois at Urbana-Champaign201 North Goodwin Ave, Urbana, IL 61801-2302{crashtc2, pyoung2, mhodosh2, juliahmr}@illinois.eduAbstractCrowd-sourcing approaches such as Ama-zon?s Mechanical Turk (MTurk) make it pos-sible to annotate or collect large amounts oflinguistic data at a relatively low cost and highspeed.
However, MTurk offers only limitedcontrol over who is allowed to particpate ina particular task.
This is particularly prob-lematic for tasks requiring free-form text en-try.
Unlike multiple-choice tasks there is nocorrect answer, and therefore control itemsfor which the correct answer is known can-not be used.
Furthermore, MTurk has no ef-fective built-in mechanism to guarantee work-ers are proficient English writers.
We de-scribe our experience in creating corpora ofimages annotated with multiple one-sentencedescriptions on MTurk and explore the effec-tiveness of different quality control strategiesfor collecting linguistic data using Mechani-cal MTurk.
We find that the use of a qualifi-cation test provides the highest improvementof quality, whereas refining the annotationsthrough follow-up tasks works rather poorly.Using our best setup, we construct two imagecorpora, totaling more than 40,000 descriptivecaptions for 9000 images.1 IntroductionAlthough many generic NLP applications can be de-veloped by using existing corpora or text collectionsas test and training data, there are many areas whereNLP could be useful if there was a suitable corpusavailable.
For example, computer vision researchersare becoming interested in developing methods thatcan predict not just the presence and location of cer-tain objects in an image, but also the relations be-tween objects, their attributes, or the actions andevents they participate in.
Such information canneither be obtained from standard computer visiondata sets such as the COREL collection nor fromthe user-provided keyword tag annotations or cap-tions on photo-sharing sites such as Flickr.
Simi-larly, although the text near an image on a websitemay provide cues about the entities depicted in theimage, an explicit description of the image contentitself is typically only provided if it is not immedi-ately obvious to a human what is depicted (in whichcase we may not expect a computer vision systemto be able to recognize the image content either).We therefore set out to collect a corpus of imagesannotated with simple full-sentence descriptions oftheir content.
To obtain these descriptions, we usedAmazon?s Mechanical Turk (MTurk).1 MTurk isan online framework that allows researchers to postannotation tasks, called HITs (?Human IntelligenceTask?
), then, for a small fee, be completed by thou-sands of anonymous non-expert users (Turkers).
Al-though MTurk has been used for a variety of tasks inNLP, our use of MTurk differs from other researchin NLP that uses MTurk mostly for annotation ofexisting text.
Similar to crowdsourcing-based an-notation, quality control is an essential componentof crowdsourcing-based data collection efforts, andneeds to be factored into the overall costs.
For us,the quality of the text produced by the Turkers isparticularly important since we are interested in us-1All of our experiments on Mechanical Turk were adminis-tered and paid for through the services offered by Dolores Labs.139ing this corpus for future research at the intersectionof computer vision and natural language processing.However, MTurk provides limited ways to imple-ment such quality control directly.
For example, ourinitial experiments yielded a data set that containedmany sentences that were clearly not written by na-tive speakers.
We learned that several steps must betaken to ensure that Turkers both understand the taskand produce quality data.This paper describes our experiences with Turk(based on data collection efforts in spring and sum-mer 2009), comparing two different approaches toquality control.
Although we did not set out to run ascientific experiment comparing different strategiesof how to collect linguistic data on Turk, our expe-rience points towards certain recommendations forhow to collect linguistic data on Turk.2 The core task: image annotationThe PASCAL Data Set Every year, the Pat-tern Analysis, Statistical Modeling, and Computa-tional Learning (PASCAL) organization hosts theVisual Object Classes Challenge (Everingham et al,2008).
This is a competition similar to the sharedtasks familiar to the ACL community, where a com-mon data set of images with classification and de-tection information is released, and computer visionresearchers compete to create the best classification,detection, and segmentation systems.
We chose touse this collection of images because it is a standardresource for computer vision, and will therefore fa-ciliate further research.The VOC2008 development and training set con-tains around 6000 images.
It is categorized by ob-jects that appear in the image, with some images ap-pearing in multiple categories.2.
The images con-tain a wide variety of actions and scenery.
Our cor-pus consists of 1000 of these images, fifty randomlychosen from each of the twenty categories.MTurk setup We asked Turkers to write one de-scriptive sentence for each of ten images.
An ex-ample annotation screen is shown in Figure 1.
We2The twenty categories include people, various animals,vehicles and other objects: person, bird, cat, cow,dog, horse, sheep, aeroplane, bicycle,boat, bus, car, motorbike, train, bottle,chair, dining table, potted plant, sofa,tv/monitorFigure 1: Screenshot of the image annotation task.first showed the Turkers a list of instructive guide-lines describing the task (Figure 6).
The instruc-tions told them to write ten complete but simple sen-tences, to include adjectives if possible, to describethe main characters, the setting, or the relation ofthe objects in the image, to pay attention to gram-mar and spelling, and to try to be concise.
Theseinstructions were meant to both explain the task andto prepare Turkers to write quality sentences.
Wethen showed each Turker a set of ten images, chosenrandomly from the 1000 total images, and displayedone at a time.
The Turkers navigated using ?Next?buttons through the ten annotation screens, each dis-playing one image and one text-box.
We allowedTurkers ten minutes to complete one task.3 We re-stricted the task to Turkers who have previously hadat least 95% of their results approved.
We paid $0.10to complete one task.
The total cost for all 5000 de-scriptions was $50 (plus Amazon?s 10% fee).2.1 ResultsOn average, Turkers wrote the ten sentences in a to-tal of four minutes.
The average pay rate was $1.30per hour, and the whole experiment finished in undertwo days.
Five different people described each im-age, and in the end, most of the Turkers completedthe task successfully, although 2.5% of the 5000 sen-tences were empty strings.
Turkers varied in the timethey took to complete the experiment, in the lengthof their sentences, and in the level of detail they in-cluded about the image.
An example captioned im-age is shown in Figure 2.Problems with the data The quality of descrip-tions varied greatly.
We were hoping to collect sim-ple sentences, written in correct English, describ-ing the entities and actions in the images.
More-3This proved to be more than enough time for the task.140Figure 2: An image along with the five captions that were written by Turkers.over, these are explicitly the types of descriptions weasked for in the MTurk task instructions.
Althoughwe found the descriptions acceptable more than halfof the time, a large number of the remaining descrip-tions had at least one of the following two problems:1.
Some descriptions did not mention the saliententities in the image, some were simply nounphrases (or less), and some were humorous orspeculative.4 We find all of these to be prob-lems because future computer vision and nat-ural language processing research will requireaccurate and consistent image captions.2.
A number of Turkers were not sufficiently pro-ficient in English.
Many descriptions containedgrammar and spelling errors, and some in-cluded very awkward constructions.
For exam-ple, the phrase ?X giving pose?
showed up sev-eral times in descriptions of images containingpeople (e.g.
?The lady and man giving pose.?
).Such spelling and grammar errors will pose dif-ficulties for any standard text-processing algo-rithms trained on native English.Spell checking Due to the large number of mis-spellings in in the initial data set, we first ran the sen-tences first through our spell checker before puttingthem up on Turk to assess their quality.
We tok-enized the captions with OpenNLP, and first checkeda manually created list of spelling corrections foreach token.
These included canonicalizations (cor-recting ?surf board?
as ?surfboard?
), words our au-tomatic spell checker did not recognize (?mown?
),and the most common misspellings in our data set4For example, some Turkers commented on the feelings ofanimals (e.g.
?the dog is not very happy next to the dumpster?
),and others made jokes about the content of the image (e.g.
?Thegoat is ready for hair cut?)(?shepard?
to ?shepherd?).
If the token was not inour manual list, we passed the word to aspell.
Fromaspell?s candidate corrections, we selected the mostfrequent word that appeared either in other captionsof the same image, of images of the same topic, orany caption in our data set.3 Post-hoc quality controlBecause our initial data collection efforts resulted inrelatively noisy data, we created a new set of MTurktasks designed to provide post-hoc quality control.Our aim was to filter out captions containing mis-spellings and incorrect grammar.MTurk setup Each HIT consisted of fifty differ-ent image descriptions and asked Turkers to decidefor each of them whether they contained correctgrammar and spelling or not.
At the beginning ofeach HIT, we included a brief training phase, wherewe showed the Turkers five example descriptions la-beled as ?correct?
or ?incorrect?
(Figure 7).
In theHIT itself, the fifty descriptions were displayed inblocks of five (albeit not for the same image) , andeach description was followed by two radio buttonslabeled ?correct?
and ?incorrect?.
We did not showthe corresponding images.
A screenshot is shown inFigure 3.
Each block of five captions contained onecontrol item that we use for later assessment of theTurkers?
spell-checking ability.
We wrote these con-trol captions ourselves, modeling them after actualimage descriptions.
We paid $0.08 for one task, andthree people completed each task.3.1 ResultsOn average, Turkers completed a HIT (judging fiftysentences) in four minutes, at an average hourly rateof $1.04.
Each sentence in our data set was judgedby three Turkers.
The whole experiment finished141Figure 3: Screenshot from the grammar/spelling checking task.
This is a block of five sentences that Turkers hadto label as using correct or incorrect grammar and spelling.
The first sentence is a control item that we included tomonitor the Turkers?
performance, and the other four are captions generated by other Turkers in a previous task.Data set Quality control % Votes for ?correct English?produced by... performed by... 0 1 2 3Unqualified writers three Turkers 18.9% 31.2% 26.4% 23.5%Unqualified writers three experts 11.8% 12.7% 15.3% 60.2%Qualified writers three experts 0.5% 2.5% 15.0% 82.0%Table 1: Quality control by Turkers and Experts.
The three experts judged 600 sentences from each data set.
565sentences produced by unqualified workers were also judged by three Turkers.in under two days, at a total cost of $28.80 (plusAmazon?s 10% fee).
We also selected randomly600 spell-checked sentences for expert annotation.Three members of our team (all native speakers ofEnglish) judged each of these sentences in the samemanner as the Turkers.
Each sentence could there-fore get between 0 and 3 Turker votes and between0 and 3 expert votes for good English.
The top tworows of Table 1 show the distribution of votes ineach of the two groups.
We also assess whether thejudgments of the Turkers correlate with our own ex-pert judgments.
Table 2(a) shows the overall agree-ment between Turkers and expert annotators.
Therest of Table 2 shows how performance of the Turk-ers on the control items affected agreement with ex-pert judgments.
We define the performance of aTurker in terms of the average the number of con-trol items that they got right in each HIT they took.For each threshold in Tables 2(a)-(d), we consideredonly those images for which we have three qualityjudgments by workers whose performance is abovethe specified threshold.Our results show that the effectiveness of usingTurkers to filter for grammar and spelling issues islimited.
Overall, the Turker judgments were overlyharsh.
The majority Turker vote agrees with the ma-jority vote of the trained annotators on only 65.1%of the sentences.
Manual inspection of the differ-ences reveals that the Turkers marked many per-fectly grammatical English sentences as incorrect(although they also marked a few which we hadmissed).
Agreement with experts decreases amongthose Turkers that performed better on the controlsentences, with only 56.7% agreement for Turkersthat got all the controls right.
In addition, the Turk-ers are significantly more likely to report false nega-tives over false positives and this also increases withperformance on the control sentences.
(Overall, theTurkers marked 29.9% of the sentences as false neg-atives, whereas the Turkers that scored perfectly onthe controls marked 39.3% as false negatives.)
Ex-amination of the areas of high disagreement revealthat the Turkers were much more likely to vote downnoun phrases than the experts were.
The correct ex-ample captions provided in the instructions of thequality control test were complete sentences.
Someof the control captions were noun phrases, but allof the noun phrase controls had some other errorin them.
Thus it was possible to either believe thatnoun phrases were correct or incorrect, and still beconsistent with the provided examples, and providecorrect judgments on the control sentences.142(a) ?
0 controls correct: 565 sentencesTurk Expert votesvotes 0 1 2 30 6.9% 4.4% 3.7% 3.9%1 3.2% 5.7% 5.0% 17.3%2 1.8% 2.8% 3.5% 18.2%3 0.0% 0.4% 2.5% 20.7%(b) ?
5 controls correct: 553 sentencesTurk Expert votesvotes 0 1 2 30 6.9% 4.5% 3.8% 4.0%1 3.1% 5.4% 5.1% 17.5%2 1.8% 2.7% 3.6% 18.4%3 0.0% 0.4% 2.5% 20.3%(c) ?
7 controls correct: 331 sentencesTurk Expert votesvotes 0 1 2 30 6.9% 6.3% 3.9% 5.1%1 3.0% 4.5% 5.1% 24.5%2 1.8% 1.8% 2.4% 15.1%3 0.0% 0.0% 2.1% 17.2%(d) ?
9 controls correct: 127 sentencesTurk Expert votesvotes 0 1 2 30 7.9% 6.3% 3.1% 6.3%1 1.6% 4.7% 6.3% 23.6%2 0.8% 3.1% 1.6% 15.7%3 0.0% 0.0% 1.6% 17.3%Table 2: Quality control: Agreement between Turker and Expert votes, depending on the average number of controlitems the Turker voters got right.4 Quality control through pre-screeningQuality control can also be imposed through a pre-screening of the Turkers allowed to take the HIT.
Wecollected another set of five descriptions per image,but restricted participation to Turkers residing in theUS5, and created a brief qualification test to checktheir English.
We would like to be able to restrict ourtasks to Turkers who are native speakers and com-petent spellers and writers of English, regardless oftheir country of residence.
However, this seems tobe difficult to verify within the current MTurk setup.Qualification Test Design The qualification testconsists of forty binary questions: fifteen testingspelling, fifteen testing grammar, and ten testing theability to identify good image descriptions.In all three cases, we started the section with aset of instructions displaying examples of positiveand negative answers to the tasks.
Each spellingquestion consisted of a single sentence, and Turk-ers were asked to determine if all of the words inthe sentence were spelled correctly and if the correctword was being used (?lose?
versus ?loose?).
Eachgrammar question consisted of a single sentence thatwas either correct or included a grammatical error.Both spelling and grammar checking questions werebased on common mistakes made by foreign English5As of March 2010, 46.80% of Turkers reside in the U.S(http://behind-the-enemy-lines.blogspot.com/ 03/09/2010)Figure 4: Average caption length (5000 images)speakers and on grammatical or spelling errors thatoccurred in our initial set of image captions.
Thegrammar and spelling questions are listed in Table3.
The image description questions consisted of oneimage shown with two actual captions, and the Turk-ers were asked which caption better described theimage.
In order to pass the qualification test, werequired each annotator to correctly answer at leasttwenty-four spelling and grammar questions and atleast eight image description questions.
To preventTurkers from using the number of question they gotcorrect to do a brute force search for the correct an-swers, we simply told them if they passed (?1?)
orfailed (?0?).
Currently, 1504 people have taken thequalification test, with a 67.2% passing rate.
Sincethis qualification test was only required for our HITsthat were restricted to US residents, we assume (butare not able to verify) that most, if not all, of thepeople who took this test are actually US residents.143MTurk Set-up We use the same MTurk set-up asbefore, but to encourage Turkers to complete thetask even though they first have to pass a qualifica-tion test, we pay them $0.10 to annotate five images.4.1 ResultsWe found that the Turkers who passed the qualifica-tion provided much better captions for the images.The average time spent on each image was longer(four minutes per ten images for the non-qualifiedworkers versus five minutes per ten images for thequalified workers).
On average, qualified Turk-ers produced slightly longer sentences (avg.
10.7words) than non-qualified workers (avg.
10.0 words)(Figure 4), and the awkward constructions producedby the unqualified workers were mostly absent.
Theentire corpus was annotated in 253 hours at a cost of$100.00 (plus Amazon?s 10% fee).We also looked at the rate of misspellings (ap-proximated by how often our spell-checker indicateda misspelling).
Without the qualification test, Outof the 600 sentences produced without the qualifica-tion test, 78 contained misspellings, whereas only 25sentences out of the 600 produced by the qualifiedworkers contained misspellings.
Furthermore, mis-spellings in the no-qualification group include manygenuine errors (?the boys are playing in tabel?,?bycycles?, ?eatting?
), whereas misspellings in thequalification group are largely typos (e.g.
Ywo forTwo, tableclothe, chari for chair).
Furthermore, thespell checker corrected all 25 misspellings in thequalified data set to the intended word, but 27 out ofthe 78 misspellings in the data produced by the un-qualified workers got changed to some other word.The same three members of our team rated againthe English of 600 randomly selected sentences writ-ten by Turkers residing in the US who passed ourtest.
We found a significant improvement in quality(Table 1, bottom row), with the majority expert voteaccepting over 97% of the sentences.
This is alsocorroborated by qualitative analysis of the data (seeFigure 5 for examples).
Inspection reveals that sen-tences that are deemed ungrammatical by the expertstypically contain some undetected typo, and wouldbe correct if these typos could be fixed.
Without aqualification test, there is a significantly greater per-centage of nonsensical responses such as: ?Is this abird squirrel??
and ?thecentury?.
In addition, gram-matically correct but useless fragments such as ?verydark?
and ?peace?
only appear without a test.
Afterrequiring the qualification test, the major reasons forrejection by Turkers are typos such as in ?The twodogs blend in with the stuff animals?
or missing de-terminers such as in ?a train on tracks in town?.Overall cost effectiveness Using the no qualifica-tion test approach, we first paid $50.00 to get 5000sentences written by unqualified Turkers (which re-sulted in 4851 non-empty sentences).
This resultedin low-quality data which required further verifica-tion.
Since this is too time-consuming for expert an-notators, we then paid another $28.80 to get each ofthese sentences subsequently checked by three Turk-ers for grammaticality, resulting in 2222 sentenceswhich received at least two positive votes for gram-maticality.
With the qualification test approach, wepaid $100.00 to get 5000 sentences written.
Basedon our experiments on the set of 600 sentences, ex-perts would judge over 97% of these sentences ascorrect, thus obviating the immediate need for fur-ther control.
That is, it effectively costs more fornon-qualified Turkers to produce sentences that arejudged to be good than for qualified Turkers.
Fur-thermore, their sentences will probably be of lowerquality even after they have been judged acceptable.5 A corpus of captions for Flickr photosEncouraged by the success of the qualification testapproach, we extended our corpus to contain 8000images collected from Flickr.
We again paid theTurkers $0.10 to annotate five images.
Our data setconsists of 8108 hand-selected images from Flickr,depicting actions and events (rather than images de-picting scenery and mood).
These images are morelikely to require full sentence descriptions than thePASCAL images.
We chose six large Flickr groups6and downloaded a few thousand images from each,giving us a total of 15,000 candidate images.
We re-moved all black and white or sepia images as well asimages containing photographer signatures or seals.Next, we manually identified pictures that depictedthe actions of people or animals.
For example, wekept images of people walking in parks, but not of6The groups: strangers!, Wild-Child (Kids in Action), Dogsin Action (Read the Rules), Outdoor Activities, Action Photog-raphy and Flickr-Social (two or more people in the photo)144Without qualification test(1) lady with birds(2) Some parrots are have speaking skill.
(3) A lady in their dining table with birds on her shoulder and head.
(4) Asian woman with two cockatiels, on shoulderhead, room with oak cabinets.,(5) The lady loves the parrotWith qualification test(1) A woman has a bird on her shoulder, and another bird on her head(2) A woman with a bird on her head and a bird on her shoulder.
(3) A women sitting at a dining table with two small birds sitting on her.
(4) A young Asian woman sitting at a kitchentable with a bird on her head and another on her shoulder.
(5) Two birds are perched on a woman sitting in a kitchen.Figure 5: Comparison of captions written by Turkers with and without qualification testempty parks; we kept several people posing, but nota close-up of a single person.7 Each HIT askedTurkers to describe five images.
We required thequalification test and US residency.
Average com-pletion time was a little above 3 minutes for 5 sen-tences.
The corpus was annotated in 284 hours8, ata total cost of $812.00 (plus Amazon?s 10% fee).6 Related work and conclusionsRelated work MTurk has been used for many dif-ferent NLP and vision tasks (Tietze et al, 2009;Zaidan and Callison-Burch, 2009; Snow et al, 2008;Sorokin and Forsyth, 2008).
Due to the noise in-herent in non-expert annotations, many other at-tempts at quality control have been made.
Kit-tur et al (2008) solicit ratings about different as-pects of Wikipedia articles.
At first they receivevery noisy results, due to Turkers?
not paying at-tention when completing the task or specifically try-ing to cheat the requester.
They remade the task,this time starting by asking the Turkers verifiablequestions, speculating that the users would producebetter quality responses when they suspect their an-swers will be checked.
They also added a questionthat required the Turkers to comprehend the con-tent of the Wikipedia article.
With this new set-up, they find that the quality greatly increases andcarelessness is reduced.
Kaisser and Lowe (2008)7Our final data set consists of 1482 pictures from action pho-tography, 1904 from dogs, 776 from flickr-social, 916 from out-door, 1257 from strangers and 1773 from wild-child.8Note that the annotation process scaled pretty well, con-sidering that annotating more than eight times the number ofimages took only 31 hours longer.collected question and answer pairs by presentingTurkers with a question and telling them to copy andpaste from a document of text they know to containthe answer.
They achieve a good but far from per-fect interannotator agreement based on the extractedanswers.
We speculate that the quality would bemuch worse if the Turkers wrote the sentences them-selves.
Callison-Burch (2009) asks Turkers to pro-duce translations when given reference sentences inother languages.
Overall, he finds find that Turk-ers produce better translations than machine transla-tion systems.
To eliminate translations from Turkerswho simply put the reference sentence into an onlinetranslation website, he performs a follow-up task,where he asks other Turkers to vote on if they believethat sentences were generated using an online trans-lation system.
Mihalcea and Strapparava (2009) askTurkers to produce 4-5 sentence opinion paragraphsabout the death penalty, about abortion and describ-ing a friend.
They report that aside from a smallnumber of invalid responses, all of the paragraphswere of good quality and followed their instructions.Their success is surprising to us because they do notreport using a qualification test, and when we didthis our responses contained a large amount of in-correct English spelling and grammar.The TurKit toolkit (Little et al, 2009) providesanother approach to improving the quality of MTurkannotations.
Their iterative framework allows therequester to set up a series of tasks that first solic-its text annotations from Turkers and then asks otherTurkers to improve the annotations.
They report suc-cessful results using this methodology, but we chose145to stick with simply using the qualification test be-cause it achieves the desired results already.
Fur-thermore, although using TurKit would have proba-bly done away with our few remaining grammar andspelling mistakes, it may have caused the captionsfor an image to be a little too similar, and we valuea diversity in the use of words and points of view.Our experiences We have described our experi-ences in using Amazon?s Mechanical Turk in thefirst half of 2009 to create a corpus of images anno-tated with descriptive sentences.
We implementedtwo different approaches to quality control: first, wedid not impose any restrictions on who could writeimage descriptions.
This was then followed by a sec-ond set of MTurk tasks where Turkers had to judgethe quality of the sentences generated in our initialTurk experiments.
This approach to quality controlwould be cost-effective if the initial data were nottoo noisy and the subsequent judgments were ac-curate and cheap.
However, this was not the case,and quality control on the judgments in the form ofcontrol items turned out to result in even lower ac-curacy.
We then repeated our data collection effort,but required that Turkers live in the US and take abrief qualification test that we created to test theirEnglish.
This is cost-effective if English proficiencycan be accurately assessed in such a brief qualifica-tion test.
We found that the latter approach was in-deed far cheaper, and produced significantly betterdata.
We did not set out to run a scientific experi-ment comparing different strategies of how to col-lect linguistic data on Turk, and therefore there maybe multiple explanations for the effects we observe.Nevertheless, our experience indicates strongly thateven very simple prescreening measures can providevery effective quality control.We also extended our corpus to include 8000 im-ages collected from Flickr.
We hope to release thisdata to the public for future natural language pro-cessing and computer vision research.Recommended practices for usingMTurk in NLPOur experience indicates that with simple prescreen-ing, linguistic data can be elicited fairly cheaply andrapidly from crowd-sourcing services such as Me-chanical Turk.
However, many applications may re-quire more control over where the data comes from.Even though NLP data collection differs fundamen-tally from psycholinguistic experiments that mayelicit production data, our community will typicallyalso need to know whether data was produced by na-tive speakers or not.
Until MTurk provides a bettermechanism to check the native language of its work-ers, linguistic data collection on MTurk will have torely on potentially very noisy input.AcknowledgementsThis research was funded by NSF grant IIS 08-03603 INT2-Medium: Understanding the Meaningof Images.
We are grateful for David Forsyth?s ad-vice and for Alex Sorokin?s support with MTurk.ReferencesChris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proceedings of EMNLP 2009.M.
Everingham, L. Van Gool, C. K. I. Williams,J.
Winn, and A. Zisserman.
2008.
ThePASCAL Visual Object Classes Challenge2008 (VOC2008) Results.
http://www.pascal-network.org/challenges/VOC/voc2008/workshop/.Michael Kaisser and John Lowe.
2008.
Creating a re-search collection of question answer sentence pairswith amazons mechanical turk.
In LREC 2008.Aniket Kittur, Ed H. Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with mechanical turk.
InProceedings of SIGCHI 2008.Greg Little, Lydia B. Chilton, Max Goldman, andRobert C. Miller.
2009.
Turkit: tools for iterative taskson mechanical turk.
In HCOMP ?09: Proceedings ofthe ACM SIGKDD Workshop on Human Computation.Rada Mihalcea and Carlo Strapparava.
2009.
The liedetector: Explorations in the automatic recognitionof deceptive language.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?evaluating non-expert annotations for natural languagetasks.
In Proceedings of EMNLP 2008.Alexander Sorokin and David Forsyth.
2008.
Utility dataannotation with amazon mechanical turk.
In Com-puter Vision and Pattern Recognition Workshop.Martin I. Tietze, Andi Winterboer, and Johanna D.Moore.
2009.
The effect of linguistic devices in infor-mation presentation messages on comprehension andrecall.
In Proceedings of ENLG 2009.Omar F. Zaidan and Chris Callison-Burch.
2009.
Feasi-bility of human-in-the-loop minimum error rate train-ing.
In Proceedings of EMNLP 2009.146Are all of the words correctly spelled and correctly used?
Is the sentence grammatically correct?A group of children playing with thier toys (N) A man giving pose to camera.
(N)He accepts the crowd?s praise graciously.
(Y) The white sheep walks on the grass.
(Y)The coffee is kept at a very hot temperture.
(N) She is good woman.
(N)A green car is parked in front of a resturant.
(N) He should have talk to him.
(N)An orange cat sleeping with a dog that is much larger then it.
(N) He has many wonderful toy.
(N)I ate a tasty desert after lunch.
(N) He sended the children home to their parents.
(N)A group of people getting ready for a surprise party.
(Y) The passage through the hills was narrow.
(Y)A small refrigerator filled with colorful fruits and vegetables.
(Y) A sleeping dog.
(Y)Two men fly by in a red plain.
(N) The questions on the test was difficult.
(N)A causal picture of a man and a woman.
(N) In Finland, we are used to live in a cold climate.
(N)Three men are going out for a special occasion.
(Y) Three white sheeps graze on the grassy field.
(N)Woman eatting lots of food.
(N) Between you and me, this is wrong.
(Y)Dyning room with chairs.
(N) They are living there during six months.
(N)A woman recieving a package.
(N) I was given lots of advices about buying new furnitures.
(N)This is a relatively uncommon occurance.
(Y) A horse being led back to it?s stall.
(N)Table 3: The spelling and grammar portions of the qualification test.
The test may be found on MTurk by searchingfor the qualification entitled ?Image Annotation Qualification?.Figure 6: Screenshot of the image annotation instruc-tions: guidelines (top) and examples (bottom).Figure 7: Screenshot of the quality control test instruc-tions: guidelines (top) and examples (bottom).147
