Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 452?461,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsInferring Temporally-Anchored Spatial Knowledge from Semantic RolesEduardo Blanco and Alakananda VempalaHuman Intelligence and Language Technologies LabUniversity of North TexasDenton, TX, 76203eduardo.blanco@unt.edu, AlakanandaVempala@my.unt.eduAbstractThis paper presents a framework to infer spa-tial knowledge from verbal semantic role rep-resentations.
First, we generate potential spa-tial knowledge deterministically.
Second, wedetermine whether it can be inferred and adegree of certainty.
Inferences capture thatsomething is located or is not located some-where, and temporally anchor this informa-tion.
An annotation effort shows that infer-ences are ubiquitous and intuitive to humans.1 IntroductionExtracting semantic relations from text is at the coreof text understanding.
Semantic relations encode se-mantic connections between words.
For example,from (1) Bill couldn?t handle the pressure and quityesterday, one could extract that the CAUSE of quitwas the pressure.
Doing so would help answeringquestion Why did Bill quit?
and determining that thepressure started before Bill quit.In the past years, computational semantics has re-ceived a significant boost.
But extracting all seman-tic relations in text?even in single sentences?isstill an elusive goal.
Most existing approaches targeteither a single relation, e.g., PART-WHOLE (Girju etal., 2006), or relations that hold between argumentsfollowing some syntactic construction, e.g., posses-sives (Tratz and Hovy, 2013).
Among the latter kind,the task of verbal semantic role labeling focuses onextracting semantic links exclusively between verbsand their arguments.
PropBank (Palmer et al, 2005)is a popular corpus for this task, and tools to ex-tract verbal semantic roles have been proposed foryears (Carreras and Ma`rquez, 2005).Some semantic relations hold forever, e.g., theCAUSE of event quit in example (1) above is pres-sure.
Discussing when this CAUSE holds is some-what artificial: at some point Bill quit, and he did soSNP VPNNP AUX VPJohn wasVBN PPincarceratedTHEMELOCATIONat ShawshankprisonFigure 1: Semantic roles (solid arrows) and addi-tional spatial knowledge (discontinuous arrow).because of the pressure.
But LOCATION and othersemantic relations often do not hold forever.
For ex-ample, while buildings typically have one locationduring their existence, people and objects such ascars and books do not: they participate in events andas a result their locations change.This paper presents a framework to infertemporally-anchored spatial knowledge from verbalsemantic roles.
Specifically, our goal is to inferwhether something is located somewhere or not lo-cated somewhere, and temporally anchor this spa-tial information.
Consider sentence (2) John wasincarcerated at Shawshank prison and its semanticroles (Figure 1, solid arrows).
Given these roles,we aim at inferring that John had LOCATION Shaw-shank prison during event incarcerated, and that he(probably) did not have this LOCATION before andafter (discontinuous arrow).
Our intuition is thatknowing that incarcerated has THEME John and LO-CATION Shawshank prison will help making theseinferences.
As we shall discuss, sometimes we haveevidence that something is (or is not) located some-where, but cannot completely commit.We target temporally-anchored spatial knowledgebetween intra-sentential arguments of verbs, notonly between arguments of the same verb as ex-emplified in Figure 1.
The main contributions are:452(1) analysis of spatial knowledge inferable fromPropBank-style semantic roles; (2) annotations oftemporally-anchored LOCATION relations on top ofOntoNotes;1 (3) supervised models to infer the ad-ditional spatial knowledge; and (4) experiments de-tailing results using lexical, syntactic and semanticfeatures.
The framework presented here infers over44% spatial knowledge on top of the PropBank-stylesemantic roles annotated in OntoNotes (certYESand certNO labels, Section 3.3).2 Semantic Roles and Additional SpatialKnowledgeWe denote a semantic relation R between x and yas R(x, y).
R(x, y) could be read ?x has R y?,e.g., AGENT(moved, John) could be read ?movedhas AGENT John?.
Semantic roles2 are semantic re-lations R(x, y) such that x is a verb and y is an ar-gument of x.
We refer to any spatial relation LO-CATION(x, y) where (1) x is not a verb, or (2) x isa verb but y is not a argument of x, as additionalspatial knowledge.
As we shall see, we target addi-tional spatial knowledge beyond plain LOCATION(x,y) relations, which only specify the location y of x.Namely, we consider polarity, i.e., whether some-thing is or is not located somewhere, and temporallyanchor this information.This paper complements semantic role represen-tations with additional spatial knowledge.
We fol-low a practical approach by inferring spatial knowl-edge from PropBank-style semantic roles.
We be-lieve this is an advantage since PropBank is well-known in the field and several tools to predict Prop-Bank roles are documented and publicly available.3The work presented here could be incorporated intoany NLP pipeline after role labeling without modifi-cations to other components.2.1 PropBank and OntoNotesPropBank (Palmer et al, 2005) adds semantic roleannotations on top of the parse trees of the Penn1Available at http://hilt.cse.unt.edu/2We use semantic role to refer to PropBank-style (verbal)semantic roles.
NomBank (Meyers et al, 2004) and FrameNet(Baker et al, 1998) also annotate semantic roles.3E.g., http://cogcomp.cs.illinois.edu/page/software, http://ml.nec-labs.com/senna/;[Mr. Cray]ARG0[will]ARGM-MOD [work]verb [for theColorado Springs CO company]ARG2[as an indepen-dent contractor]ARG1.
[I]ARG0?d [slept]verb [through my only previous brushwith natural disaster]ARG2, [.
.
.
]Table 1: Examples of PropBank annotations.ARGM-LOC: location ARGM-CAU: causeARGM-EXT: extent ARGM-TMP: timeARGM-DIS: discourse connective ARGM-PNC: purposeARGM-ADV: general-purpose ARGM-MNR: mannerARGM-NEG: negation marker ARGM-DIR: directionARGM-MOD: modal verbTable 2: Argument modifiers in PropBank.Treebank.
It uses a set of numbered arguments4(ARG0, ARG1, etc.)
and modifiers (ARGM-TMP,ARGM-MNR, etc.).
Numbered arguments do notshare a common meaning across verbs, they are de-fined on verb-specific framesets.
For example, ARG2is used to indicate ?employer?
with verb work.01and ?expected terminus of sleep?
with verb sleep.01(Table 1).
Unlike numbered arguments, modifiershave the same meaning across verbs (Table 2).The original PropBank corpus consists of (1)3,327 framesets, each frameset defines the num-bered roles for a verb, and (2) actual semantic roleannotations (numbered arguments and modifiers) for112,917 verbs.
On average, each verb has 1.93 num-bered arguments and 0.66 modifiers annotated.
Only7,198 verbs have an ARGM-LOC annotated, i.e., lo-cation information is present in 6.37% of verbs.
Formore information about PropBank and examples, re-fer to the annotation guidelines.5OntoNotes (Hovy et al, 2006) is a more re-cent corpus that includes POS tags, word senses,parse trees, speaker information, named entities,PropBank-style semantic roles and coreference.While the original PropBank annotations were doneexclusively in the news domain, OntoNotes includesother genres as well: broadcast and telephone con-versations, weblogs, etc.
Because of the addi-tional annotation layers and genres, we work withOntoNotes instead of PropBank.4Numbered arguments are also referred to as core.5http://verbs.colorado.edu/?mpalmer/projects/ace/PBguidelines.pdf453SSBAR NP VPNP INafterS NBC News has learnt .
.
.Exactlya monthNP VPtwenty-sixyear oldGeorge SmithVBDvanishedARG1ARGM-DIR PPINfromNPNP VPa RoyalCaribbean shipVBGcruisingARG0ARGM-LOC PPin the MediterraneanFigure 2: Semantic roles (solid arrows) and additional spatial knowledge (discontinuous arrow) of type (1b).The additional LOCATION(a Royal Caribbean ship, in the Mediterranean) of type (1a) is not shown.2.2 Additional Spatial KnowledgeSentences contain spatial information beyondARGM-LOC semantic role, i.e., beyond links be-tween verbs and their arguments.
There are twomain types of additional LOCATION(x, y) relations:6(1) those whose arguments x and y are semanticroles of a verb, and (2) those whose arguments x andy are not semantic roles of a verb.The first kind can be further divided into (1a)those whose arguments are semantic roles of thesame verb (Figure 1), and (1b) those whose argu-ments are semantic roles of different verbs.
Fig-ure 2 illustrates type (1b).
Semantic roles indicateARG1and ARGM-DIR of vanished, and ARG0andARGM-LOC of cruising.
In this example, one caninfer that twenty-six year old George Smith (ARG1of vanished) has LOCATION in the Mediterranean(ARGM-LOC of cruising) during the cruising event.The second kind of additional LOCATION(x, y) isexemplified in the following sentence: [Residentsof Biddeford apartments]ARG0can [enjoy]verb [therecreational center]ARG1[free of charge]MANNER.LOCATION(recreational center, Biddeford apart-ments) could be inferred yet Biddeford apartmentsis not a semantic role of a verb.7 Inferring this kindof relations would require splitting semantic roles;6Both ARGM-LOC(x, y) and LOCATION(x, y) encode thesame meaning, but we use ARGM-LOC for the PropBank se-mantic role and LOCATION for additional spatial knowledge.7Note that the head of ARG0is residents, not the apartments.one could also extract that the residents have LOCA-TION Biddeford apartments.In this paper, we focus on extracting additionalspatial knowledge of type (1), and reserve type (2)for future work.
More specifically, we infer spa-tial knowledge between x and y, where the follow-ing semantic roles exist: ARGi(xpred, x) and ARGM-LOC(ypred, y).
ARGiindicates any numbered argu-ment (ARG0, ARG1, ARG2, etc.)
and xpred(ypred) in-dicates the verbal predicate to which x (y) attaches.Targeting additional spatial knowledge exclusivelyfor numbered arguments is not a significant limita-tion: most semantic roles annotated in OntoNotes(75%) are numbered arguments, and it is pointlessto infer spatial knowledge for most modifiers, e.g.,ARGM-EXT, ARGM-DIS, ARGM-ADV, ARGM-MOD,ARGM-NEG, ARGM-DIR.3 Annotating Spatial KnowledgeAnnotating all additional spatial knowledge inOntoNotes inferable from semantic roles is a daunt-ing task.
OntoNotes is a large corpus with 63,918sentences and 9,924 ARGM-LOC semantic roles an-notated.
Our goal is not to present an extensiveannotation effort, but rather show that additionaltemporally-anchored spatial knowledge can be (1)annotated reliably by non-experts following simpleguidelines, and (2) inferred automatically using su-pervised machine learning.
Thus, we focus on 200sentences from OntoNotes that have at least oneARGM-LOC role annotated.454foreach sentence s doforeach sem.
role ARGM-LOC(ypred, y) ?
s doforeach sem.
role ARGi(xpred, x) ?
s doif is valid(x, y) thenIs x located at y before ypred?Is x located at y during ypred?Is x located at y after ypred?Algorithm 1: Procedure to generate potential addi-tional spatial knowledge of type (1) (Section 2.2).Obviously, [the pilot]ARG0, v1did[n?t]ARGM-NEG, v1[think]v1[too much]ARGM-EXT, v1[about [what]ARG1, v2was[happening]v2[on the ground]ARGM-LOC, v2, or .
.
.
]ARG1, v1Figure 3: Sample sentence and semantic roles.
Pair(x: about what was happening on the ground, y: onthe ground) is invalid because x contains y.All potential additional spatial knowledge is gen-erated with Algorithm 1, and a manual annotationeffort determines whether spatial knowledge shouldbe inferred.
Algorithm 1 loops over all ARGM-LOCroles, and generates questions regarding whetherspatial knowledge can be inferred for any numberedargument within the same sentence.
is valid(x, y)returns True if (1) x is not contained in y and (2) y isnot contained in x.
Considering invalid pairs wouldbe trivial or nonsensical, e.g., pair (x: about whatwas happening on the ground, y: on the ground) isinvalid in the sentence depicted in Figure 3.3.1 Annotation Process and GuidelinesIn a first batch of annotations, two annotators wereasked questions generated by Algorithm 1 and re-quired to answer YES or NO.
The only informationthey had available was the source sentence withoutsemantic role information.
Feedback from this firstattempt revealed that (1) because of the nature of xor y, sometimes questions are pointless, and (2) be-cause of uncertainty, sometimes it is not correct toanswer YES or NO, even tough there is some evidencethat makes either answer likely.Based on this feedback, and inspired by previousannotation guidelines (Saur??
and Pustejovsky, 2012),in a second batch we allowed five answers:?
certYES: I am certain that the answer is yes.?
probYES: It is probable that the answer is yes,but it is not guaranteed.?
certNO: I am certain that the answer is no.?
probNO: It is probable that the answer is no, butit is not guaranteed.?
UNK: There is not enough information to an-swer, I can?t tell the location of x.The goal is to infer spatial knowledge as gath-ered by humans when reading text.
Thus, annotatorswere encouraged to use commonsense and worldknowledge.
While simple and somewhat open tointerpretation, these guidelines allowed as to gatherannotations with ?good reliability?
(Section 3.3.1).3.2 Annotation ExamplesIn this section, we present annotation examples af-ter resolving conflicts (Figure 4).
These examplesshow that ambiguity is common and sentences mustbe fully interpreted before annotating.Sentence 4(a) has four semantic roles for verb col-lecting (solid arrows), and annotators are asked todecide whether ARG0and ARG1of collecting arelocated at the ARGM-LOC before, during or aftercollecting (discontinuous arrows).
Annotators inter-preted that the FBI agents and divers (ARG0) and ev-idence (ARG1) were located at Lake Logan (ARGM-LOC) during collecting (certYES).
They also anno-tated that the FBI agents and divers were likely to belocated at Lake Logan before and after (probYES).Finally, they determined that the evidence was lo-cated at Lake Logan before the collecting (certYES),but probably not after (probNO).
These annotationsreflect the natural reading of sentence 4(a): (1) peo-ple and whatever they collect are located where thecollecting takes place during the event, (2) peoplecollecting are likely to be at that location before andafter (i.e., presumably they do not arrive immedi-ately before and leave immediately after), and (3)the objects being collected are located at that loca-tion before collecting, but probably not after.Sentence 4(b) is more complex.
First, potentialrelation LOCATION(in sight, at the intersection) isannotated UNK: it is nonsensical to ask for the loca-tion of sight.
Second, the Disney symbols are neverlocated at the intersection (certNO).
Third, both thecar and security guard were located at the intersec-tion during the stop for sure (certYES).
Fourth, an-notators interpreted that the car was not at the in-tersection before (certNO), but they were not sureabout after (probNO).
Fifth, they considered that thesecurity guard was probably located at the intersec-455Today FBI agents and divers were collectingARG0ARGM-TMPARG1ARGM-LOCevidence at Lake Logan .
.
.
(a)However, before[any of theDisney symbols]ARG1, v1[were]v1[in sight]ARG2, v1the car was stoppedARGM-DISARGM-TMPARG1ARG0ARGM-LOCby asecurity guardat the intersectionof the roadstowards Disney(b)x y ypredBefore During AfterFBI agents and divers at Lake Logan collecting probYES certYES probYESevidence at Lake Logan collecting certYES certYES probNOany of the Disney symbols at the intersection of the roads .
.
.
stopped certNO certNO certNOin sight at the intersection of the roads .
.
.
stopped UNK UNK UNKthe car at the intersection of the roads .
.
.
stopped certNO certYES probNOby a security guard at the intersection of the roads .
.
.
stopped probYES certYES probYESFigure 4: Examples of semantic role representations (solid arrows), potential additional spatial knowledge(discontinuous arrows) and annotations with respect to the verb to which y attaches (collecting or stopped).LabelcertYES probYES certNO probNO UNK# % # % # % # % # %Before 100 15.04 225 33.83 57 8.57 248 37.29 35 5.26During 477 71.51 36 5.40 60 9.00 59 8.85 35 5.25After 140 21.12 344 51.89 57 8.60 87 13.12 35 5.28All 717 35.94 605 30.33 174 8.72 394 19.75 105 5.26Table 3: Annotation counts.
Over 44% of potential spatial knowledge can be inferred (certYES and certNO).tion before and after.
In other words, annotators un-derstood that (1) the car was moving down a roadand arrived at the intersection; (2) then, it was pulledover by a security guard who is probably stationed atthe intersection; and (3) after the stop, the car prob-ably continued with its route but the guard probablystayed at the intersection.3.3 Annotation AnalysisEach annotator answered 1,995 questions generatedwith Algorithm 1.
Basic label counts after resolvingconflicts are shown in Table 3.
First, it is worth not-ing that annotators used UNK to answer only 5.26%of questions.
Thus, over 94% of times ARGM-LOCsemantic role is found, additional spatial knowledgecan be inferred with some degree of certainty.
Sec-ond, annotators were certain about the additionalspatial knowledge, i.e., labels certYES and certNO,35.94% and 8.72% of times respectively.
Thus,44% of times one encounters ARGM-LOC seman-Observed Cohen KappaBefore 89.0% 0.845During 91.2% 0.848After 87.8% 0.814All 89.8% 0.862Table 4: Inter-annotation agreements.
Kappa scoresindicate ?good reliability?.tic role, additional spatial knowledge can be inferredwith certainty.
Finally, annotators answered around50% of questions with probYES or probNO.
In otherwords, they found it likely that spatial informationcan be inferred, but were not completely certain.3.3.1 Inter-Annotator AgreementsTable 4 presents observed agreements, i.e., raw per-centage of equal annotations, and Cohen Kappascores (Cohen, 1960) per temporal anchor and forall questions.
Kappa scores are above 0.80, indicat-ing ?good reliability?
(Artstein and Poesio, 2008).456No.
Name Description0 temporal anchor are we predicting LOCATION(x, y) before, during or after ypred?lexical1?4 first word, POS tag first word and POS tag in x and y5?8 last word, POS tag last word and POS tag in x and y9,10 num tokens number of tokens in x and y11,12 subcategory concatenation of (1) x?s children and (2) y?s children13 direction whether x occurs before or after ysyntactic14,15 syntactic node syntactic node of x and y16?19 head word, POS tag head word and POS tag of x and y20?23 left and right sibling syntactic nodes of the left and right siblings of x and y24?27 parent node and index syntactic nodes and child indices of parents of x and y28 common subsumer syntactic node subsuming x and y29 syntactic path syntactic path between x and ysemantic30?33 word, POS tag predicate and POS tag of xpredand ypred34 isRole semantic role label between xpredand x35 same predicate whether xpredand ypredare the same token36?39 firstRole, lastRole the first and last semantic roles of xpredand ypred40?59 hasRole flags indicating whether xpredand ypredhave each semantic role60?99 role index and node for each semantic role, the order of appearance and syntactic node100 x containedIn y role semantic role of ypredthat fully contains x101 y containedIn x role semantic role of xpredthat fully contains yTable 5: Feature set to infer temporally-anchored spatial knowledge from semantic role representations.We believe the high Kappa scores are due to thefact that we start from PropBank-style roles insteadof plain text, and questions asked are intuitive.
Notethat not all disagreements are equal, e.g., the differ-ence between certYES and certNO is much largerthan the difference between certYES and probYES.4 Inferring Spatial KnowledgeWe follow a standard supervised machine learningapproach.
The 200 sentences were divided intotrain (80%) and test (20%), and the correspondinginstances assigned to the train and test sets.8 Wetrained an SVM with RBF kernel using scikit-learn(Pedregosa et al, 2011).
Parameters C and ?
weretuned using 10-fold cross-validation with the train-ing set, and results are calculated with test instances.4.1 Feature selectionSelected features (Table 5) are a mix of lexical, syn-tactic and semantic features, and are extracted fromtokens (words and POS tags), full parse trees and se-mantic roles.
Lexical and syntactic features are stan-dard in semantic role labeling (Gildea and Jurafsky,2002) and we do not elaborate on them.
Hereafter8Splitting instances randomly would be unfair, as instancesfrom the same sentence would be assigned to the train and testsets.
Thank you to an anonymous reviewer for pointing this out.Sentence: [In this laboratory]ARGM-LOC, v1[I]ARG0, v1?m[surrounded]v1[by the remains of [20 service memberswho]ARG1, v2are in the process of being [identified]v2]ARG1, v1Potential additional spatial knowledge: x: 20 service mem-bers who, y: In this laboratory; x containedIn y role = ARG1Sentence: [Children]ARG0, v1can get to [know]v1[dif-ferent animals and plants, and [even some cropsthat]ARG1, v2are [rarely]ARGM-ADV, v2[seen]v2[in our dailylife]ARGM-LOC, v2]ARG1, v1.Potential additional spatial knowledge: x: Children, y: inour daily life; y containedIn x role = ARG1Figure 5: Pairs (x, y) for which x containedIn y roleand y containedIn x role features have a value.we describe semantic features, which include anyfeature derived from semantic role representations.Features 30?33 correspond to the surface formand POS tag of the verbs to which x and y attach to.Feature 34 indicates the semantic role between xpredand x; note that the semantic role between ypredandy is always ARGM-LOC (Algorithm 1).
Feature 35distinguishes inferences of type (1a) from (1b) (Sec-tion 2.2): it indicates whether both x and y attach tothe same verb, as in Figure 1, or not, as in Figure2.
Features 36?39 encode the first and last seman-tic role of xpredand ypredby order of appearance.Features 40?59 are binary flags signalling which se-457Before During After AllP R F P R F P R F P R Fmost frequentbaselinecertYES 0.11 1.00 0.20 0.74 1.00 0.85 0.26 1.00 0.42 0.37 1.00 0.54other labels 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00weighted avg.
0.01 0.11 0.02 0.54 0.74 0.63 0.07 0.26 0.11 0.14 0.37 0.20most frequentper temporalanchorbaselinecertYES 0.00 0.00 0.00 0.75 1.00 0.86 0.00 0.00 0.00 0.75 0.62 0.68probYES 0.00 0.00 0.00 0.00 0.00 0.00 0.45 1.00 0.62 0.45 0.56 0.50probNO 0.38 1.00 0.55 0.00 0.00 0.00 0.00 0.00 0.00 0.38 0.62 0.47other labels 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00weighted avg.
0.14 0.38 0.21 0.57 0.75 0.65 0.20 0.45 0.28 0.50 0.53 0.50lexicalfeaturescertYES 0.13 0.20 0.16 0.74 1.00 0.85 0.53 0.29 0.37 0.63 0.75 0.69probYES 0.39 0.34 0.36 0.00 0.00 0.00 0.56 0.90 0.69 0.51 0.63 0.56certNO 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00probNO 0.39 0.53 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.39 0.37 0.38UNK 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00weighted avg.
0.31 0.35 0.32 0.54 0.74 0.63 0.44 0.56 0.47 0.47 0.55 0.50lexical +syntacticfeaturescertYES 0.41 0.47 0.44 0.74 0.99 0.85 0.27 0.09 0.13 0.67 0.72 0.70probYES 0.53 0.34 0.41 0.00 0.00 0.00 0.54 0.90 0.67 0.54 0.63 0.58certNO 0.33 0.10 0.15 0.00 0.00 0.00 0.00 0.00 0.00 0.25 0.04 0.06probNO 0.38 0.64 0.48 0.00 0.00 0.00 0.00 0.00 0.00 0.38 0.44 0.41UNK 1.00 0.12 0.22 1.00 0.12 0.22 1.00 0.12 0.22 1.00 0.12 0.22weighted avg.
0.48 0.43 0.41 0.61 0.74 0.64 0.42 0.51 0.41 0.57 0.56 0.53lexical +semanticfeaturescertYES 0.18 0.20 0.19 0.74 1.00 0.85 0.65 0.31 0.42 0.67 0.76 0.71probYES 0.48 0.42 0.44 0.00 0.00 0.00 0.57 0.92 0.70 0.54 0.66 0.60certNO 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00probNO 0.35 0.51 0.41 0.00 0.00 0.00 0.00 0.00 0.00 0.35 0.35 0.35UNK 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00weighted avg.
0.33 0.37 0.34 0.54 0.74 0.63 0.47 0.57 0.49 0.49 0.56 0.52all featurescertYES 0.50 0.20 0.29 0.76 0.97 0.85 0.50 0.14 0.22 0.73 0.70 0.71probYES 0.51 0.36 0.42 0.50 0.14 0.22 0.56 0.93 0.70 0.55 0.66 0.60certNO 0.33 0.10 0.15 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.04 0.05probNO 0.40 0.72 0.51 0.00 0.00 0.00 0.00 0.00 0.00 0.39 0.50 0.44UNK 1.00 0.12 0.22 0.33 0.12 0.18 0.50 0.12 0.20 0.50 0.12 0.20weighted avg.
0.49 0.44 0.41 0.61 0.73 0.65 0.46 0.54 0.45 0.56 0.57 0.55Table 6: Results obtained with two baselines, and training with several feature combinations.
Models aretrained with all instances (before, during and after).mantic roles xpredand ypredhave, and features 60?99 capture the index of each role (first, second, third,etc.)
and its syntactic node (NP, PP, SBAR, etc.
).Finally, features 100 and 101 capture the semanticrole of xpredand ypredwhich fully contain y and xrespectively, if such roles exists.
These features areespecially designed for our inference task and areexemplified in Figure 5.5 Experiments and ResultsResults obtained with the test set using two base-lines and models trained with several feature com-binations are presented in Table 6.
The most fre-quent baseline always predicts certYES, and themost frequent per temporal anchor baseline pre-dicts probNO, certYES and probYES for instanceswith temporal anchor before, during and after re-spectively.
The most frequent baseline obtains aweighted F-measure of 0.20, and most frequent pertemporal anchor baseline 0.50.
Results with su-pervised models are better, but we note that alwayspredicting certYES for during instances obtains thesame F-measure than using all features (0.65).The bottom block of Table 6 presents results us-ing all features.
The weighted F-measure is 0.55,and the highest F-measures are obtained with labelscertYES (0.71) and probYES (0.60).
Results withcertNO and probNO are lower (0.05 and 0.44), webelieve this is due to the fact that few instances areannotated with this labels (8.72% and 19.75%, Ta-458ble 3).
Results are higher (0.65) with during in-stances than with before and after instances (0.41and 0.45).
These results are intuitive: certain eventssuch as press and write require participants to be lo-cated where the event occurs only during the event.5.1 Feature Ablation and Detailed ResultsThe weighted F-measure using lexical features is thesame than with the most frequent per temporal an-chor baseline (0.50).
F-measures go up with before(0.21 vs. 0.32, 52.38%) and after (0.28 vs. 0.47,67.85%) instances, but slightly down with during in-stances (0.65 vs. 0.63, ?3.08%).Complementing lexical features with syntacticand semantic features brings the overall weighted F-measure slightly up: 0.53 with syntactic and 0.52with semantic features (+0.03 and +0.02, 6% and4%).
Before instances benefit the most from syn-tactic features (0.32 vs. 0.41, 28.13%), and afterinstances benefit from semantic features (0.47 vs.0.49, 4.26%).
During instances do not benefit fromsemantic features, and only gain 0.01 F-measure(1.59%) with syntactic features.Finally, combining lexical, syntactic and seman-tic features obtains the best overall results (weightedF-measure: 0.55 vs. 0.53 and 0.52, 3.77% and5.77%).
We note, however, that before instances donot benefit from including semantic features (sameF-measure, 0.41), and the best results for after in-stances are obtained with lexical and semantic fea-tures (0.49 vs. 0.45, 8.16%),6 Related WorkTools to extract the PropBank semantic roles we in-fer from have been studied for years (Carreras andMa`rquez, 2005; Hajic?
et al, 2009; Lang and Lapata,2010).
These systems only extract semantic linksbetween predicates and their arguments, not be-tween arguments of predicates.
In contrast, this pa-per complements semantic role representations withspatial knowledge for numbered arguments.There have been several proposals to extract se-mantic links not annotated in well-known corporasuch as PropBank (Palmer et al, 2005), FrameNet(Baker et al, 1998) or NomBank (Meyers et al,2004).
Gerber and Chai (2010) augment Nom-Bank annotations with additional numbered argu-ments appearing in the same or previous sentences;posterior work obtained better results for the sametask (Gerber and Chai, 2012; Laparra and Rigau,2013).
The SemEval-2010 Task 10: Linking Eventsand their Participants in Discourse (Ruppenhoferet al, 2009) targeted cross-sentence missing num-bered arguments in PropBank and FrameNet.
Wehave previously proposed an unsupervised frame-work to compose semantic relations out of previ-ously extracted relations (Blanco and Moldovan,2011; Blanco and Moldovan, 2014a), and a super-vised approach to infer additional argument mod-ifiers (ARGM) for verbs in PropBank (Blanco andMoldovan, 2014b).
Unlike the current work, theseprevious efforts (1) improve the semantic represen-tation of verbal and nominal predicates, or (2) in-fer relations between arguments of the same predi-cate.
None of them target temporally-anchored spa-tial knowledge or account for uncertainty.Attaching temporal information to semantic rela-tions is uncommon.
In the context of the TAC KBPtemporal slot filling track (Garrido et al, 2012; Sur-deanu, 2013), relations common in information ex-traction (e.g., SPOUSE, COUNTRY OF RESIDENCY)are assigned a temporal interval indicating whenthey hold.
The task proved very difficult, andthe best system achieved 48% of human perfor-mance.
Unlike this line of work, the approach pre-sented in this paper starts from semantic role repre-sentations, targets temporally-anchored LOCATIONrelations, and accounts for degrees of uncertainty(certYES / certNO vs. probYES / probNO).The task of spatial role labeling (Hajic?
et al,2009; Kolomiyets et al, 2013) aims at thoroughlyrepresenting spatial information with so-called spa-tial roles, i.e., trajector, landmark, spatial and motionindicators, path, direction, distance, and spatial rela-tions.
Unlike us, the task does not consider temporalspans nor certainty.
But as the examples through-out this paper show, doing so is useful because (1)spatial information for most objects changes overtime, and (2) humans sometimes can only state thatan object is probably located somewhere.
In con-trast to this task, we infer temporally-anchored spa-tial knowledge as humans intuitively understand it,and purposely avoid following any formalism.4597 ConclusionsSemantic roles encode semantic links between averb and its arguments.
Among other role labels,PropBank uses numbered arguments (ARG0, ARG1,etc.)
to encode the core arguments of a verb, andARGM-LOC to encode the location.
This paper ex-ploits these numbered arguments and ARGM-LOCin order to infer temporally-anchored spatial knowl-edge.
This knowledge encodes whether a numberedargument x is or is not located in a location y, andtemporally anchors this information with respect tothe verb to which y attaches.An annotation effort with 200 sentences fromOntoNotes has been presented.
First, potential addi-tional spatial knowledge is generated automatically(Algorithm 1).
Then, annotators following straight-forward guidelines answer questions asking for intu-itive spatial information, including uncertainty.
Theresult is annotations with high inter-annotator agree-ments that encode spatial knowledge as understoodby humans when reading text.Experimental results show that inferring addi-tional spatial knowledge can be done with a mod-est weighted F-measure of 0.55.
Results are higherfor certYES and probYES (0.71 and 0.60), the labelsthat indicate that something is certainly or probablylocated somewhere.
Simple majority baselines pro-vide strong results, but combining lexical, syntacticand semantic features yields the best results (0.50vs.
0.55).
Inferring spatial knowledge for numericarguments before and after an event occurs is harderthan during the event (0.41 and 0.45 vs. 0.65).The most important conclusion of this work isthe fact that given an ARGM-LOC semantic role,temporally-anchored spatial knowledge can be in-ferred for numbered arguments in the same sen-tence.
Indeed, annotators answered 44% of ques-tions with certYES or certNO, and 50% of questionswith probYES or probNO.
Another important obser-vation is that spatial knowledge can be inferred frommost verbs, not only motion verbs.
While it is fairlyobvious to infer from John went to Paris that he hadLOCATION Paris after went but not before or dur-ing, we have shown that verbs such as incarcerated(Figure 1) also grant spatial inferences.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596, December.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of the 17th international conference on Computa-tional Linguistics, Montreal, Canada.Eduardo Blanco and Dan Moldovan.
2011.
Unsuper-vised learning of semantic relation composition.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics (ACL 2011),pages 1456?1465, Portland, Oregon.Eduardo Blanco and Dan Moldovan.
2014a.
Compo-sition of semantic relations: Theoretical frameworkand case study.
ACM Trans.
Speech Lang.
Process.,10(4):17:1?17:36, January.Eduardo Blanco and Dan Moldovan.
2014b.
Leveragingverb-argument structures to infer semantic relations.In Proceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL 2014), pages 145?154, Gothenburg, Swe-den.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 shared task: semantic role label-ing.
In CONLL ?05: Proceedings of the Ninth Confer-ence on Computational Natural Language Learning,pages 152?164.J.
Cohen.
1960.
A Coefficient of Agreement for NominalScales.
Educational and Psychological Measurement,20(1):37.Guillermo Garrido, Anselmo Pen?as, Bernardo Cabaleiro,and ?Alvaro Rodrigo.
2012.
Temporally anchored re-lation extraction.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics: Long Papers - Volume 1, ACL ?12, pages 107?116.Matthew Gerber and Joyce Chai.
2010.
Beyond Nom-Bank: A Study of Implicit Arguments for NominalPredicates.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 1583?1592, Uppsala, Sweden, July.Matthew Gerber and Joyce Chai.
2012.
Semantic rolelabeling of implicit arguments for nominal predicates.Computational Linguistics, 38:755?798, 2012.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288, September.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic discovery of part-whole relations.Computational Linguistics, 32(1):83?135, March.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?s460Ma`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, CoNLL ?09, pages 1?18.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% Solution.
In NAACL?06: Proceedings ofthe Human Language Technology Conference of theNAACL, pages 57?60, Morristown, NJ, USA.Oleksandr Kolomiyets, Parisa Kordjamshidi, Marie-Francine Moens, and Steven Bethard.
2013.
Semeval-2013 task 3: Spatial role labeling.
In Second JointConference on Lexical and Computational Semantics(*SEM), Volume 2: Proceedings of the Seventh Inter-national Workshop on Semantic Evaluation (SemEval2013), pages 255?262.Joel Lang and Mirella Lapata.
2010.
Unsupervised in-duction of semantic roles.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, HLT ?10, pages 939?947.Egoitz Laparra and German Rigau.
2013.
Impar: Adeterministic algorithm for implicit semantic role la-belling.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1180?1189, Sofia, Bul-garia, August.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The Nom-Bank Project: An Interim Report.
In A. Meyers, ed-itor, HLT-NAACL 2004 Workshop: Frontiers in Cor-pus Annotation, pages 24?31, Boston, Massachusetts,USA, May.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2009.
SemEval-2010 Task 10: Linking Events and Their Participantsin Discourse.
In Proceedings of the Workshop on Se-mantic Evaluations: Recent Achievements and FutureDirections (SEW-2009), pages 106?111, Boulder, Col-orado, June.Roser Saur??
and James Pustejovsky.
2012.
Are you surethat this happened?
assessing the factuality degree ofevents in text.
Computational Linguistics, 38(2):261?299, June.Mihai Surdeanu.
2013.
Overview of the tac2013 knowl-edge base population evaluation: English slot fillingand temporal slot filling.
In Proceedings of the TAC-KBP 2013 Workshop.Stephen Tratz and Eduard Hovy.
2013.
Automatic inter-pretation of the english possessive.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), pages372?381.
Association for Computational Linguistics.461
