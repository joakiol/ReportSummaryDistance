Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1245?1255,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsAddressing Limited Data for Textual Entailment Across DomainsChaitanya Shivade?Preethi Raghavan?and Siddharth Patwardhan?
?Department of Computer Science and Engineering,The Ohio State University,Columbus, OH 43210shivade@cse.ohio-state.edu?IBM T. J. Watson Research Center,1101 Kitchawan Road,Yorktown Heights, NY 10598{praghav,siddharth}@us.ibm.comAbstractWe seek to address the lack of labeled data(and high cost of annotation) for textualentailment in some domains.
To that end,we first create (for experimental purposes)an entailment dataset for the clinical do-main, and a highly competitive supervisedentailment system, ENT, that is effective(out of the box) on two domains.
Wethen explore self-training and active learn-ing strategies to address the lack of la-beled data.
With self-training, we success-fully exploit unlabeled data to improveover ENT by 15% F-score on the newswiredomain, and 13% F-score on clinical data.On the other hand, our active learning ex-periments demonstrate that we can match(and even beat) ENT using only 6.6% ofthe training data in the clinical domain,and only 5.8% of the training data in thenewswire domain.1 IntroductionTextual entailment is the task of automatically de-termining whether a natural language hypothesiscan be inferred from a given piece of natural lan-guage text.
The RTE challenges (Bentivogli etal., 2009; Bentivogli et al, 2011) have spurredconsiderable research in textual entailment overnewswire data.
This, along with the availabilityof large-scale datasets labeled with entailment in-formation (Bowman et al, 2015), has resulted in avariety of approaches for textual entailment recog-nition.
?This work was conducted during an internship at IBMA variation of this task, dubbed textual entail-ment search, has been the focus of RTE-5 and sub-sequent challenges, where the goal is to find allsentences in a corpus that entail a given hypoth-esis.
The mindshare created by those challengesand the availability of the datasets has spurredmany creative solutions to this problem.
How-ever, the evaluations have been restricted primarilyto these datasets, which are in the newswire do-main.
Thus, much of the existing state-of-the-artresearch has focused on solutions that are effectivein this domain.It is easy to see though, that entailment searchhas potential applications in other domains too.For instance, in the clinical domain we imagineentailment search can be applied for clinical trialmatching as one example.
Inclusion criteria fora clinical trial (for e.g., patient is a smoker) be-come the hypotheses, and the patient?s electronichealth records are the text for entailment search.Clearly, an effective textual entailment search sys-tem could possibly one day fully automate clinicaltrial matching.Developing an entailment system that workswell in the clinical domain and, thus, automatesthis matching process, requires lots of labeleddata, which is extremely scant in the clinical do-main.
Generating such a dataset is tedious andcostly, primarily because it requires medical do-main expertise.
Moreover, there are always pri-vacy concerns in releasing such a dataset to thecommunity.
Taking this into consideration, we in-vestigate the problem of textual entailment in alow-resource setting.We begin by creating a dataset in the clinicaldomain, and a supervised entailment system that1245is competitive on multiple domains ?
newswire aswell as clinical.
We then present our work on self-training and active learning to address the lack of alarge-scale labeled dataset.
Our self-training sys-tem results in significant gains in performance onclinical (+13% F-score) and on newswire (+15%F-score) data.
Further, we show that active learn-ing with uncertainty sampling reduces the numberof required annotations for the entailment searchtask by more than 90% in both domains.2 Related workRecognizing Textual Entailment (RTE) sharedtasks (Dagan et al, 2013) conducted annuallyfrom 2006 up until 2011 have been the primarydrivers of textual entailment research in recentyears.
Initially the task was defined as thatof entailment recognition.
RTE-5 (Bentivogli etal., 2009) then introduced the task of entailmentsearch as a pilot.
Subsequently, RTE-6 (Bentivogliet al, 2010) and RTE-7 (Bentivogli et al, 2011)featured entailment search as the primary task, butconstrained the search space to only those candi-date sentences that were first retrieved by Lucene,an open source search engine1.
Based on the80% recall from Lucene in RTE-5, the organizersof RTE-6 and RTE-7 deemed this filter to be anappropriate compromise between the size of thesearch space and the cost and complexity of thehuman annotation task.Annotating data for these tasks has remaineda challenge since they were defined in the RTEchallenges.
Successful approaches for entailment(Mirkin et al, 2009; Jia et al, 2010; Tsuchidaand Ishikawa, 2011) have relied on annotated datato either train classifiers, or to develop rules fordetecting entailing sentences.
Operating underthe assumption that more labeled data would im-prove system performance, some researchers havesought to augment their training data with auto-matically or semi-automatically obtained labeledpairs (Burger and Ferro, 2005; Hickl et al, 2006;Hickl and Bensley, 2007; Zanzotto and Pennac-chiotti, 2010; Celikyilmaz et al, 2009).Burger and Ferro (2005) automatically createan entailment recognition corpus using the newsheadline and the first paragraph of a news article asnear-paraphrases.
Their approach has an estimatedaccuracy of 70% on a held out set of 500 pairs.The primary limitation of the approach is that it1http://lucene.apache.orgonly generates positive training examples.
Hicklet al (2006) improves upon this work by includingnegative examples selected using heuristic rules(e.g., sentences connected by although, otherwise,and but).
On RTE-2 their method achieves accu-racy improvements of upto 10%.
However, Hickland Bensley (2007) achieves only a 1% accuracyimprovement on RTE-3 using the same method,suggesting that it is not always as beneficial.Recent work by Bowman et al (2015) describesa method for generating large scale annotateddatasets, viz., the Stanford Natural Language In-ference (SNLI) Corpus, for the problem of entail-ment recognition.
They use Amazon MechanicalTurk to very inexpensively produce a large entail-ment annotated data set from image captions.Zanzotto and Pennacchiotti (2010) create an en-tailment corpus using Wikipedia data.
They hand-annotate original Wikipedia entries, and their as-sociated revisions for entailment recognition.
Us-ing a previously published system for RTE (Zan-zotto and Moschitti, 2006), they show that theirexpanded corpus does not result in improvementfor RTE-1, RTE-2 or RTE-3.Similarly, Celikyilmaz et al (2009) address thelack of labeled data by semi-automatically creat-ing an entailment corpus, which they use withintheir question answering system.
They reuse text-hypothesis pairs from RTE challenges in additionto manually annotated pairs from a newswire cor-pus (with pairs for annotation obtained through aLucene search over the corpus).Note that all of the above research on expand-ing the labeled data for entailment has focused onentailment recognition.
Our focus in this paper ison improving entailment search by exploiting un-labeled data with self-training and active learning.3 DatasetsIn this section, we describe the data sets from twodomains, newswire and clinical, that we use in thedevelopment and evaluation of our work.3.1 Newswire DomainFor the newswire domain, we use entailmentsearch data from the PASCAL RTE-5, RTE-6 andRTE-7 challenges (Bentivogli et al, 2009; Ben-tivogli et al, 2010; Bentivogli et al, 2011).
Thedataset consists of a corpus of news documents,along with a set of hypotheses.
The hypothesescome from a separate summarization task, where1246Dataset Size EntailingNewswire-train 20,104 810 (4.0%)Newswire-dev 35,927 1,842 (5.1%)Newswire-test 17,280 800 (4.6%)Newswire-unlabeled 43,485 -Clinical-train 7,026 293 (4.1%)Clinical-dev 8,092 324 (4.0%)Clinical-test 10,466 596 (5.6%)Clinical-unlabeled 623,600 -Table 1: Summary of datasets**NAME[XX (YY) ZZ] has no liverproblems.PAST MEDICAL HISTORY1.
HtnWell controlled2.
Diabetes mellitusOn regular dose of insulin.FAMILY HISTORY:Father with T2DM age unknownFigure 1: Excerpt from a sample clinical notethe summary sentences about a news story (givena topic) were manually created by human anno-tators.
These summary sentences are used as hy-potheses in the dataset.
Entailment annotations arethen provided for a subset of sentences from thedocument corpus, based on a Lucene filter for eachhypothesis.In this work, we use the RTE-5 developmentdata to train our system (Newswire-train), RTE-5test data for evaluation of our systems (Newswire-test), and we use the combined RTE-6 develop-ment and test data for our system developmentand parameter estimation (Newswire-dev).
We useall of the development and test data from RTE-7,without the human annotation labels, as our unla-beled data (Newswire-unlabeled) for self-trainingand active learning experiments.
A summary ofthe newswire data is shown in Table 1.3.2 Clinical DomainThere are no public datasets available for textualentailment search in the clinical domain.
In cre-ating this dataset, we imagine a real-world clin-ical situation where hypotheses are facts about apatient that a physician seeing the patient mightwant to learn (e.g., The patient underwent a surgi-cal procedure within the last three months.).
Theunstructured notes in the patients electronic med-ical record (EMR) is the text against which a sys-tem would determine the entailment status of thegiven hypotheses.Observe that the aforementioned real-worldclinical scenario is very closely related to a ques-tion answering problem, where instead of hy-potheses a physician may pose natural languagequestions seeking information about the patient(e.g., Has this patient undergone a surgical pro-cedure within the past three months?).
Answersto such questions are words, phrases or passagesfrom the patient?s EMR.
Since we have access toa patient-specific question answering dataset overEMRs2(henceforth, referred to as the QA dataset),we use it here as our starting point in constructingthe clinical domain textual entailment dataset.Given a question answering dataset, how mightone go about creating a dataset on textual entail-ment?
We follow a methodology similar to that ofRTE-1 through RTE-5 for entailment set derivedfrom question answering data.
The text corpus inour entailment dataset is the set of de-identifiedpatient records associated with the QA dataset.
Togenerate hypotheses, human annotators convertedquestions into multiple assertive sentences, whichis somewhat similar to what was done in the firstfive RTE challenges (RTE-1 through RTE-5).
Fora given question, the human annotators pluggedin clinically-plausible answers to convert the ques-tion into a statement that may or may not be trueabout a given patient.
Table 2 shows examplehypotheses and their source questions.
Note thatthis procedure for hypothesis generation divergesslightly from the RTE procedure, where answersfrom a question answering system were pluggedinto the questions to produce assertive sentences.To generate entailment annotations, we paired ahypothesis with every sentence in a subset of clini-cal notes of the EHR, and asked human annotatorsto determine if the note sentence enabled them toconclude an entailment relationship with the hy-pothesis.
For example, the text: ?The appearanceis felt to be classic for early MS.?
entails the hy-pothesis: ?She has multiple sclerosis?.
While inthe RTE procedure, a Lucene search was used asa filter to limit the number of hypothesis-sentencepairs that are annotated, in our clinical dataset we2a publication describing the question-answering datasetis currently under review at another venue1247Question HypothesesWhen was the patient diagnosed withdermatomyositis?The patient was diagnosed with dermatomyositistwo years ago.Any creatinine elevation?
Creatinine is elevated.Creatinine is normal.Why were xrays done on the forearmand hand?Xrays were done on the forearm and hand forsuspected fracture.Table 2: Example question?
hypotheses mappingslimit the number of annotations by pairing eachhypothesis only with sentences from EMR notescontaining an answer to the original question inthe QA dataset.The entailment annotations were generated bytwo medical students with the help of the annota-tions generated for QA.
11 medical students cre-ated our QA dataset of 5696 questions over 71 pa-tient records, of which 1747 questions have cor-responding answers.
This was generated intermit-tently over a period of 11 months.
Given the QAdataset, the time taken to generate entailment an-notations includes conversion of questions to hy-potheses, and annotating entailment.
While con-version of questions to hypotheses took approx.
2hours for 20 questions, generating about 3000 hy-pothesis and text pairs took approx.
16 hours.At the end of this process, we had a total of 243hypotheses annotated against sentences from 380clinical notes, to generate 25,584 text-hypothesispairs.
We split this into train, development and testsets, summarized in Table 1.
Although we havea fairly limited number of labeled text-hypothesispairs, we do have a large number of patient healthrecords (besides the ones in the annotated set).
Wegenerated unlabeled data in the clinical domain, bypairing the hypotheses from our training data withsentences from a set of randomly sampled subsetof health records outside of the annotated data.Datasets for the textual entailment search taskare highly skewed towards the non-entailmentclass.
Note that our clinical data, while smallerin size than the newswire data, maintains a similarclass imbalance.4 Supervised Entailment SystemWe begin by defining, in this section, our super-vised entailment system (called ENT) that is usedas the basis of our self-training and active learn-ing experiments.
Our system draws upon charac-teristics and features of systems that have previ-ously been successful in the RTE challenges in thenewswire domain.
We further enhance this sys-tem with new features targeting the clinical do-main.
The purpose of this section is to demon-strate, through an experimental comparison withother entailment systems, that ENT is competi-tive on both domains, and is a reasonable super-vised system to use in our investigations into self-training and active learning.4.1 System DescriptionTop systems (Tsuchida and Ishikawa, 2011;Mirkin et al, 2009) in the RTE challengeshave used various types of passage matching ap-proaches in combination with machine learningfor entailment.
We follow along these lines, anddesign a classifier-based entailment system.
Forevery text-hypothesis pair in the dataset we ex-tract a feature vector representative of that pair.Then, using the training data, we train a classi-fier to make entailment decisions on unseen exam-ples.
In our system, we employ a logistic regres-sion with ridge estimator (the Weka implementa-tion (Hall et al, 2009)), powered by a variety ofpassage matching features described below.Underlying many of our passage match featuresis a more fine-grained notion of ?term match?.Term matchers are a set of algorithms that at-tempt to match tokens (including multi-word to-kens, such as New York or heart attack) acrossa pair of passages.
One of the simplest exam-ples of these is exact string matcher.
A token inone text passage that matches exactly, character-for-character, with a token in another text passagewould be considered a term match by this sim-ple term matcher.
However, these term match-ers could be more sophisticated and match pairsof terms that are synonyms, or paraphrases, or1248Exact String match, ignore caseMulti-word Overlapping terms in multi-word tokenHead String match head of multi-word tokenWikipedia Wikipedia redirects and disamb.
pagesMorphology Derivational morphology, e.g.
archaeo-logical?
archaeologyDate+Time Match normalized dates and timesVerb resource Match verbs using WordNet, Moby the-saurus, manual resourcesUMLS Medical concept match using UMLSTranslation Affix-rule-based translation of medicalterms to layman termsTable 3: ENT term matchersequivalent to one another according to other crite-ria.
ENT employs a series of term matchers listedin Table 3.
Each of these may also produce aconfidence score for every match they find.
Be-cause we are working with clinical data, we addedsome medical domain term matchers as well ?
us-ing UMLS (Bodenreider, 2004) and a rule-based?translator?
of medical terms to layman terms3.Listed below are all of our features used in theENT?s classifier.
Most passage match features ag-gregate the output of the term matchers along var-ious linguistic dimensions ?
lexical, syntactic, se-mantic, and document/passage characteristics.Lexical: This set includes a feature aggregatingexact string matches across text-hypothesis, oneaggregating all term matchers, a feature count-ing skip-bigram matches (using all matchers), ameasure of matched term coverage of text (ratioof matched terms to unmatched terms).
Addi-tionally, we have some medical domain features,viz.
UMLS concept overlap, and a measure ofUMLS-based similarity (Shivade et al, 2015; Ped-ersen et al, 2007) using the UMLS::Similaritytool (McInnes et al, 2009).Syntactic: Following the lead of several ap-proaches textual entailment (Wang and Zhang,2009; Mirkin et al, 2009; Kouylekov and Negri,2010) we have a features measuring the similar-ity of parse trees.
Our rule-based syntactic parser(McCord, 1989) produces dependency parses thetext-hypothesis pair, whose nodes are aligned us-ing all of the term matchers.
The tree match fea-ture is an aggregation of the aligned subgraphs inthe tree (somewhat similar to a tree kernel (Mos-chitti, 2004)).3Rules for medical term translator were derived fromhttp://www.globalrph.com/medterm.htmSemantic: We apply open domain as well as medi-cal entity and relation detectors (Wang et al, 2011;Wang et al, 2012) to the texts, and post featuresmeasuring overlap in detected entities and overlapin the detected relations across the text-hypothesispair.
We also have a rule-based semantic frame de-tector for a ?medical finding?
frame (patient pre-senting with symptom or disease).
We post a fea-ture that aggregates matched elements of detectedframes.Passage Characteristics: Clinical notes typicallyhave a structure and the content is often orga-nized in sections (e.g.
History of Illness followedby Physical Examination and ending with Assess-ment and Plan).
We identified the section in whicheach note sentence was located and used them asfeatures in the classifier.
Clinical notes are alsoclassified into many different categories (e.g., dis-charge summary, radiology report, etc.
), whichwe generate features from.
We also generate sev-eral features capturing the ?readability?
of the textsegments ?
parse failure, list detector, numberof verbs, word capitalization, no punctuation andsentence size.
We also have a measure of passagetopic relevance based on medical concepts in thepair of texts.4.2 System PerformanceTo compare effectiveness of ENT on the entail-ment task, we chose two publicly available sys-tems ?
EDITS and TIE ?
for comparison.
Boththese system are available under the ExcitementOpen Platform (EOP), an initiative (Magnini et al,2014) to make tools for textual entailment freelyavailable4to the NLP community.
EDITS (EditDistance Textual Entailment Suite) by Kouylekovand Negri (2010) is an open source textual entail-ment system that uses a set of rules and resourcesto perform ?edit?
operations on the text to con-vert it into the hypothesis.
There are costs as-sociated with the operations, and an overall costis computed for the text-hypothesis pair, whichdetermines the decision for that pair.
This sys-tem has placed third (out of eight teams) in RTE-5, and seventh (out of thirteen teams) in RTE-7.The Textual Inference Engine (TIE) (Wang andZhang, 2009) is a maximum entropy based entail-ment system relying on predicate argument struc-ture matching.
While this system did not partici-4http://hltfbk.github.io/Excitement-Open-Platform/1249Newswire ClinicalSystem Precision Recall F-score Precision Recall F-scoreLucene 0.47 0.48 0.47?0.16 0.22 0.19EDITS 0.22 0.57 0.32 0.23 0.21 0.20TIE 0.66 0.21 0.31 0.43 0.01 0.02ENT 0.77 0.26 0.39 0.42 0.15 0.23?Table 4: System performance on test data (* indicates statistical significance)pate in the RTE challenges, it has been shown to beeffective on the RTE datasets.
In our experiments,we trained the EDITS system optimizing for F-score (the default optimization criterion is accu-racy) and TIE with its default settings.
We alsoused a Lucene baseline similar to the one used inRTE-5, RTE-6 and RTE-7 entailment challenges.We trained the systems on the training set ofeach domain and tested on the test set.
The Lucenebaseline considers the first N sentences (where Nis 5, 10, 15 or 20) top-ranked by the search engineto be entailing the hypothesis.
The configurationwith the top 10 sentences performed the best, andis reported in the results.
Note that this baseline isa strong one, and none of the systems participatingin RTE-5 could beat it.Table 4 summarizes the system performance onnewswire and clinical data.
We observe that sys-tems that did well on RTE datasets, were mediocreon the clinical dataset.
We did not, however, putany effort into adaption of TIE and EDITS to theclinical data.
So the mediocre performance onclinical is understandable.
It is interesting to seethough that ENT did well (comparatively) on bothdomains.We note that our problem setting is most similarto the RTE-5 entailment search task.
Of the 20runs across eight teams that participated in RTE-5,the median F-Score was 0.30 and the best system(Mirkin et al, 2009) achieved an F-Score of 0.46.EDITS and TIE perform slightly above the medianand ENT (with 0.39 F-score) would have rankedthird in the challenge.The performance of all systems on the clin-ical data is noticeably low as compared to thenewswire data.
An obvious difference in the twodomains is the training data size (see Table 1).However, obtaining annotations for textual entail-ment search is expensive, particularly in the clin-ical domain.
The remaining sections present ourinvestigations into self-training and active learn-ing, to overcome the lack of training data.5 Self-TrainingOur goal is to exploit unlabeled data, with thehope of augmenting the limited annotated datain a given domain.
Self-training is a methodthat has been successfully used to address limitedtraining data on many NLP tasks, such as pars-ing (McClosky et al, 2006), information extrac-tion (Huang and Riloff, 2012; Patwardhan andRiloff, 2007), word sense disambiguation (Mi-halcea, 2004), etc.
Self-training iteratively in-creases the size of the training set, by automati-cally assigning labels to unlabeled examples, us-ing a model trained in a previous iteration of theself-training regime.For our newswire and clinical datasets, usingthe set of unlabeled text-hypothesis pairs U , weran the following training regime: A model wascreated using the training data Ln, and applied itto the unlabeled data U .
From U , all such pairsthat were classified by the model as entailing pairswith high confidence (above a threshold ? )
wereadded to the labeled training data Lnto generateLn+1.
Non-entailing pairs were ignored.
A newmodel is trained on data Ln+1, and the above pro-cess repeated iteratively, until a stopping criteriais reached (in our case, all pairs from U are ex-hausted).The threshold ?
determines the confidence ofour model for a text-hypothesis pair being classi-fied to the entailment class.
This threshold wastuned by varying it incrementally from 0.1 to 0.9in steps of 0.1.
The best ?
was determined on thedevelopment set, and chosen for the self-trainingsystem.
Figure 2 shows the effect of ?
on the de-velopment data.As such, we see that the F-score of the self-trained model is always above that of the baselineENT system.
The F-score increases upto a peakof 0.33 at threshold ?
of 0.2 before dropping athigher thresholds.
Using this tuned threshold ontest set, the comparitive performance on the testset is outlined in Table 5.
We observe an F-score1250(a) Precision and recall for clinical data(b) F-Score for clinical data(c) Precision and recall for newswire data(d) F-Score for newswire dataFigure 2: Self-training on development dataNewswire ClinicalSystem Precision Recall F-score Precision Recall F-scoreENT 0.77 0.26 0.39 0.42 0.15 0.23ENT + Self-Training 0.62 0.48 0.54?0.34 0.39 0.36?Table 5: Self-training results on test data (* indicates statistical significance)of 0.36, which is significantly greater than that ofthe vanilla ENT system (0.23).The effect of the threshold on performance cor-relates with the number of instances added to thetraining set.
When the threshold is low, there aremore instances being added (10,799 at thresholdof 0.1) into the training set.
Therefore, recall islikely to benefit, since the model is exposed to alarger variety of text-hypothesis pairs.
However,the precision is low since noisy pairs are likely tobe added.
When the threshold is high, fewer in-stances are added (316 at threshold of 0.9).
Theseare the ones that the model is most certain about,suggesting that these are likely to be less noisy.Therefore, the precision is comparatively high.We also ran our self-training approach on theNewswire datasets.
We observed similar varia-tions in performance with newswire data as withthe clinical data.
At threshold of 0.9, fewer in-stances (49) are added to the training set from theunlabeled data, while a large number of instances(2,861) are added at a lower threshold ?
of 0.1.The best performance (F-score of 0.52) was ob-tained at threshold of 0.3, on the development set.This threshold also resulted in the best perfor-mance (0.54) on the test set.
Similar to the clinicaldomain, precision increased but recall decreasedas the threshold increased.
Again, it is evidentfrom Table 5 that gains obtained from self-trainingare due to recall.
It should be noted that the self-trained system achieves an F-score of 0.54 ?
sub-stantially better than the best performing systemof Mirkin et al (2009) (F-score, 0.46) in RTE-5.6 Active LearningActive learning is a popular training paradigm inmachine learning (Settles, 2012) where a learningagent interacts with its environment in acquiring atraining set, rather than passively receiving inde-pendent samples from an underlying distribution.This is especially pertinent in the clinical domain,where input from a medical professional should besought only when really necessary, because of thehigh cost of such input.
The purpose of exploring1251(a) Newswire Data (b) Clinical DataFigure 3: Learning curves for uncertainty sampling and random sampling on test datathis paradigm is to achieve the best possible gen-eralization performance at the lowest cost.Active learning is an iterative process, and typi-cally works as follows: a modelM is trained usinga minimal training dataset L. A query frameworkis used to identify an instance from an unlabeledset U that, if added to L, will result in maximumexpected benefit.
Gold standard annotations areobtained for this instance and added to the originaltraining set L to generate a new training set L?.
Inthe next iteration, a new modelM?is trained usingL?and used to identify the next most beneficial in-stance for the training set L?.
This is repeated untila stopping criterion is met.
This approach is oftensimulated using a training dataset L of reasonablesize.
The initial modelM is created using a subsetA of L. Further, instead of querying a large unla-beled set U , the remaining training data (L ?
A)is treated as an unlabeled dataset and queried forthe most beneficial addition.We carried out active learning in this setting us-ing a querying framework known as uncertaintysampling (Lewis and Gale, 1994).
Here, the modelM trained usingA, queries the instances in (L?A)for instance(s) it is least certain for a predictionlabel.
For probabilistic classifiers the most uncer-tain instance is the one where posterior probabilityfor a given class is nearest to 0.5.
To estimate theeffectiveness of this framework, it is always com-pared with a random sampling framework, whererandom instances from the training data are incre-mentally added to the model.Starting with a model trained using a singlerandomly chosen instance, we carried out activelearning using uncertainty sampling, adding oneinstance at a time.
After the addition of each in-stance, the model was retrained and tested on aheld out set.
To minimize the effect of randomiza-tion associated with the first instance, we repeatedthe experiment ten times and averaged the perfor-mance scores across the ten runs.Following previous work (Settles and Craven,2008; Reichart et al, 2008) we evaluate activelearning using learning curves on the test set.
Fig-ure 3 shows the learning curves for newswire andclinical data.On clinical data, uncertainty sampling achievesa performance equal to the baseline ENT with only470 instances.
With random sampling, over 2,200instances are required.
The active learner matchesthe performance of the ENT with only 6.6% oftraining data.
Newswire shows a similar trend,with both sampling strategies outperforming ENT,using less than half the training, and uncertaintysampling learning faster than random.
While un-certainty sampling matches ENT F-score with only1,169 instances, random sampling requires 2,305.Here, the active learner matches ENT performanceusing only 5.8% of the training data.7 Effect of Class DistributionAfter analyzing our experimental results, we con-sidered that one possible explanation for the im-provements over baseline ENT could plausibly bebecause of changes in the class distribution.
FromTable 1, we observe that the distribution of classesin both domains is highly skewed (only 4-5% pos-itive instances).
Self-training and active learn-ing dramatically change the class distribution intraining.
To assess the effect of class distributionchanges on performance, we ran additional exper-iments, described here.We first investigated sub-sampling (Japkowicz,2000) the training data to address class imbalance.This includes down-sampling the majority class orup-sampling the minority class until the classes are1252Figure 4: Comparison of SMOTE and self-training (on newswire development set)balanced.
We found no significant gains over thevanilla ENT baseline with both strategies.
Specif-ically, down-sampling resulted in gains of only0.002 and 0.001 F-score and up-sampling resultedin a drop of 0.011 and 0.013 F-score on clinical-dev and newswire-dev, respectively.Another approach to addressing class imbal-ance is to apply Synthetic Minority Oversam-pling Technique (SMOTE) (Chawla et al, 2002).SMOTE creates instances of the minority class bytaking a minority class sample and introducingsynthetic examples between its k nearest neigh-bors.
Using SMOTE on newswire and clinicaldatasets resulted in improvements over baselineENT in both domains.
The improvements us-ing self-training, however, are significantly higherthan SMOTE.
Figure 4 shows a comparison ofSMOTE and self-training on newswire data, whereequal number of instances are added to the trainingset by both techniques.Finally, for active learning, we consider randomsampling as a competing approach to uncertaintysampling.
Figure 5 illustrates the percentage ofpositive and negative instances that get includedin the training set for both sampling strategies,as active learning proceeds.
The blue solid lineshows that positive instances are consumed fasterthan the negative instances with uncertainty sam-pling.
Thus, a higher percentage of positive in-stances (that approximately equals the number ofnegative instances getting added) get added andthis helps maintain a balanced class distribution.Once the positive instances are exhausted, morenegative instances are added, resulting in someclass imbalance that hurts performance (eventhough more training data is being added over-all).
In contrast, random sampling does not changethe class balance, as it consumes a proportionalnumber of positive and negative instances (result-Figure 5: Comparison of sampling strategies foractive learning (on newswire development set)ing in more negative than positive instances).
Theplot indicates that when using uncertainty sam-pling 80% of the positive examples are added tothe training set with less than 50% of the data.This also explains how the active learner matchesthe performance of the model using the entire la-beled set, but with fewer training examples.8 ConclusionWe explored the problem of textual entailmentsearch in two domains ?
newswire and clinical ?and focused a spotlight on the cost of obtaininglabeled data in certain domains.
In the process,we first created an entailment dataset for the clin-ical domain, and a highly competitive supervisedentailment system, called ENT, which is effective(out of the box) on two domains.
We then exploredtwo strategies ?
self-training and active learning ?to address the lack of labeled data, and observedsome interesting results.
Our self-training sys-tem substantially improved over ENT, achievingan F-score gain of 15% on newswire and 13% onclinical, using only additional unlabeled data.
Onthe other hand, our active learning experimentsdemonstrated that we could match (and even beat)the baseline ENT system with only 6.6% of thetraining data in the clinical domain, and only 5.8%of the training data in the newswire domain.AcknowledgmentsWe thank our in-house medical expert, JenniferLiang, for guidance on the data annotation task,our medical annotators for annotating clinical datafor us, and Murthy Devarakonda for valuable in-sights during the project.
We also thank EricFosler-Lussier and Albert M. Lai for their help inconceptualizing this work.1253ReferencesLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
TheFifth PASCAL Recognizing Textual EntailmentChallenge.
In Proceedings of the Second Text Anal-ysis Conference, Gaithersburg, MD.Luisa Bentivogli, Peter Clark, Ido Dagan, and DaniloGiampiccolo.
2010.
The Sixth PASCAL Recogniz-ing Textual Entailment Challenge.
In Proceedingsof the Third Text Analysis Conference, Gaithersburg,MD.Luisa Bentivogli, Peter Clark, Ido Dagan, and DaniloGiampiccolo.
2011.
The Seventh PASCAL Rec-ognizing Textual Entailment Challenge.
In Pro-ceedings of the Fourth Text Analysis Conference,Gaithersburg, MD.Olivier Bodenreider.
2004.
The Unified Medical Lan-guage System (UMLS): Integrating Biomedical Ter-minology.
Nucleic Acids Research, 32(DatabaseIssue):D267?D270.Samuel Bowman, Gabor Angeli, Christopher Potts, andChristopher Manning.
2015.
A large annotated cor-pus for learning natural language inference.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 632?642, Lisbon, Portugal.John Burger and Lisa Ferro.
2005.
Generating an En-tailment Corpus from News Headlines.
In Proceed-ings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment, pages 49?54,Ann Arbor, MI.Asli Celikyilmaz, Marcus Thint, and Zhiheng Huang.2009.
A Graph-based Semi-Supervised Learningfor Question-Answering.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages719?727, Singapore.Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.Hall, and W. Philip Kegelmeyer.
2002.
SMOTE:Synthetic Minority Over-sampling Technique.
Jour-nal of Artificial Intelligence Research, 16:321?357.Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-simo Zanzotto.
2013.
Recognizing Textual Entail-ment: Models and Applications.
Synthesis Lectureson Human Language Technologies, 6(4):1?220.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software.
ACMSIGKDD Explorations Newsletter, 11(1):10.Andrew Hickl and Jeremy Bensley.
2007.
A Dis-course Commitment-based Framework for Recog-nizing Textual Entailment.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 171?176, Prague, Czech Re-public.Andrew Hickl, Jeremy Bensley, John Williams, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recog-nizing Textual Entailment with LCC?s GROUND-HOG System.
In The Second PASCAL RecognizingTextual Entailment Challenge: Proceedings of theChallenges Workshop, Venice, Italy.Ruihong Huang and Ellen Riloff.
2012.
BootstrappedTraining of Event Extraction Classifiers.
In Pro-ceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 286?295, Avignon, France.Nathalie Japkowicz.
2000.
Learning from ImbalancedData Sets: A Comparison of Various Strategies.In Proceedings of the AAAI Workshop on Learningfrom Imbalanced Data Sets, pages 10?15, Austin,TX.Houping Jia, Xiaojiang Huang, Tengfei Ma, XiaojunWan, and Jianguo Xiao.
2010.
PKUTM Participa-tion at TAC 2010 RTE and Summarization Track.
InProceedings of the Third Text Analysis Conference,Gaithersburg, MD.Milen Kouylekov and Matteo Negri.
2010.
AnOpen-Source Package for Recognizing Textual En-tailment.
In Proceedings of the ACL 2010 SystemDemonstrations, Uppsala, Sweden.David D. Lewis and William A. Gale.
1994.
A Se-quential Algorithm for Training Text Classifiers.
InProceedings of the 17th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 3?12, Dublin, Ireland.Bernardo Magnini, Roberto Zanoli, Ido Dagan, KathrinEichler, Neumann Guenter, Tae-Gil Noh, SebastianPad?o, Asher Stern, and Omer Levy.
2014.
The Ex-citement Open Platform for Textual Inferences.
InProceedings of 52nd Annual Meeting of the Associa-tion for Computational Linguistics: System Demon-strations, pages 43?48, Baltimore, MD.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective Self-Training for Parsing.
InProceedings of Human Language Technology Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, pages 152?159, New York City, NY.Michael McCord.
1989.
Slot Grammar: A Systemfor Simpler Construction of Practical Natural Lan-guage Grammars.
In Proceedings of the Interna-tional Symposium on Natural Language and Logic,pages 118?145, Hamburg, Germany.Bridget T McInnes, Ted Pedersen, and Serguei V. S.Pakhomov.
2009.
UMLS-Interface and UMLS-Similarity : Open Source Software for MeasuringPaths and Semantic Similarity.
In Proceedings ofthe Annual Symposium of the American Medical In-formatics Association, San Francisco, CA.1254Rada Mihalcea.
2004.
Co-Training and Self-Trainingfor Word Sense Disambiguation.
In Proceedings ofthe Eighth Conference on Natural Language Learn-ing, pages 33?40, Boston, MA.Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, IdoDagan, Eyal Shnarch, Asher Stern, and Idan Szpek-tor.
2009.
Addressing Discourse and DocumentStructure in the RTE Search Task.
In Proceedings ofthe Second Text Analysis Conference, Gaithersburg,MD.Alessandro Moschitti.
2004.
A Study on Convolu-tion Kernels for Shallow Statistic Parsing.
In Pro-ceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 335?342,Barcelona, Spain.Siddharth Patwardhan and Ellen Riloff.
2007.
Ef-fective Information Extraction with Semantic Affin-ity Patterns and Relevant Regions.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, pages 717?727,Prague, Czech Republic.Ted Pedersen, Serguei V. S. Pakhomov, Siddharth Pat-wardhan, and Christopher G Chute.
2007.
Mea-sures of Semantic Similarity and Relatedness in theBiomedical Domain.
Journal of Biomedical Infor-matics, 40(3):288?99.Roi Reichart, Katrin Tomanek, Udo Hahn, and AriRappoport.
2008.
Multi-Task Active Learning forLinguistic Annotations.
In Proceedings of ACL-08:HLT, pages 861?869, Columbus, OH.Burr Settles and Mark Craven.
2008.
An Analysis ofActive Learning Strategies for Sequence LabelingTasks.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 1070?1079, Honolulu, HI.Burr Settles.
2012.
Active Learning.
Synthesis Lec-tures on Artificial Intelligence and Machine Learn-ing, 6(1):1?114.Chaitanya Shivade, Courtney Hebert, MarceloLoptegui, Marie-Catherine de Marneffe, EricFosler-Lussier, and Albert M. Lai.
2015.
TextualInference for Eligibility Criteria Resolution inClinical trials.
Journal of Biomedical Informatics,58:S211?S218.Masaaki Tsuchida and Kai Ishikawa.
2011.
IKOMA atTAC2011 : A Method for Recognizing Textual En-tailment using Lexical-level and Sentence Structure-level Features.
In Proceedings of the Fourth TextAnalysis Conference, Gaithersburg, MD.Rui Wang and Yi Zhang.
2009.
Recognizing TextualRelatedness with Predicate-Argument Structures.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages784?792, Singapore.Chang Wang, James Fan, Aditya Kalyanpur, and DavidGondek.
2011.
Relation Extraction with RelationTopics.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1426?1436, Edinburgh, UK.Chang Wang, Aditya Kalyanpur, James Fan, Bran-imir K. Boguraev, and David Gondek.
2012.
Rela-tion Extraction and Scoring in DeepQA.
IBM Jour-nal of Research and Development, 56(3.4):9:1?9:12.Fabio M. Zanzotto and Alessandro Moschitti.
2006.Automatic Learning of Textual Entailments withCross-Pair Similarities.
In Proceedings of the 21stInternational Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 401?408, Syd-ney, Australia.Fabio M. Zanzotto and Marco Pennacchiotti.
2010.Expanding Textual Entailment Corpora fromWikipedia using Co-training.
In Proceedings of the2nd Workshop on The People?s Web Meets NLP:Collaboratively Constructed Semantic Resources,pages 28?36, Beijing, China.1255
