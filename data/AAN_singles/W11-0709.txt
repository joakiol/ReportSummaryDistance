Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 66?75,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsWhy is ?SXSW?
trending?
Exploring Multiple Text Sources forTwitter Topic SummarizationFei Liu1 Yang Liu1 Fuliang Weng21Computer Science Department, The University of Texas at Dallas2Research and Technology Center, Robert Bosch LLC{feiliu, yangl}@hlt.utdallas.edu1fuliang.weng@us.bosch.com2AbstractUser-contributed content is creating a surge onthe Internet.
A list of ?buzzing topics?
caneffectively monitor the surge and lead peopleto their topics of interest.
Yet a topic phrasealone, such as ?SXSW?, can rarely presentthe information clearly.
In this paper, wepropose to explore a variety of text sourcesfor summarizing the Twitter topics, includ-ing the tweets, normalized tweets via a ded-icated tweet normalization system, web con-tents linked from the tweets, as well as inte-gration of different text sources.
We employthe concept-based optimization framework fortopic summarization, and conduct both au-tomatic and human evaluation regarding thesummary quality.
Performance differences areobserved for different input sources and typesof topics.
We also provide a comprehensiveanalysis regarding the task challenges.1 IntroductionUser contributed content has become a major sourceof information in the Web 2.0 era.
People followtheir topics of interest, share their experience oropinions on a variety of interactive platforms, in-cluding forums, blogs, microblogs, social network-ing sites, etc.
To keep track of the trends onlineand suggest topics of interest to the general public,many leading websites provide a ?buzzing?
serviceby publishing the current most popular topics ontheir entrance page and update them regularly, suchas the ?popular now?
column on Bing.com, ?trend-ing topics?
on Twitter.com, ?trending now?
on Ya-hoo.com, Google Trends, and so forth.
Often pop-ular topics are in the form of a list of keywords orphrases1.
Take Twitter.com as an example.
Clickingon a trending topic phrase will return a set of relevantTwitter posts (tweets) or web pages.
Nonetheless,whether this is a convenient way for users to navi-gate through the popular topic information is still ar-guable.
For example, when ?SXSW?
was listed as atrending topic, it seems difficult to understand at thefirst glance.
A condensed topic summary would beextremely helpful for the users before diving into themassive search results to figure out what this topicphrase is about and why it is trending.
In this paper,our goal is to generate a short text summary for anygiven topic phrase.
Note that the proposed approachis not limited to trending topics, but can be appliedto arbitrary Twitter topics.There are a lot of differences between tweets andtraditional written text that has been widely usedfor automatic summarization.
In Table 1, we showexample tweets for the topic ?SXSW?.
The tweetswere extracted by searching the Twitter site usingthe topic phrase as a query.
We also provide an ex-cerpt of the linked web content to help understandthe topic.
The tweets present some unique charac-teristics:?
All tweets are limited to 140 characters.
Sometweets are news headlines from the official me-dia, others are generated by users with vari-ous degrees of familiarity with the social me-dia.
The resulting tweets can be very differentregarding the text quality and word usage.1They are referred to as topic phrases hereafter, with no dis-tinction between keywords and key phrases.66Twitter Topic: ?SXSW?TwtsI wish I could go to SXSW...
I will, one day!http://sxsw.com/RT @user123: SXSW FilmRound-Up: Documentaries http://bit.ly/fg033b@user456 yo.whats good,i met u at sxsw, talkinbout that feature.I was gonna see about sendingu a few beats.u lookin for only original?The South by Southwest (SXSW) ConferencesWeb & Festivals offer the unique convergence ofCont original music, independent films, andemerging technologies...(http://sxsw.com/)Table 1: Example tweets and an excerpt of the linked webcontent for Twitter topic ?SXSW?.?
Tweets lack structure information, contain var-ious ill-formed sentences and grammatical er-rors.
There are lots of noisy nonstandard to-kens, such as abbreviations (?feelin?
for ?feel-ing?
), substitutions (?Pr1mr0se?
for ?Prim-rose?
), emoticons, etc.?
Twitter invented its own markup language.?@user?
is used to reply to a specific user orcall for attentions.
The hashtag ?#topic?
aimsto assign a topic label to the tweet, and is fre-quently employed by the twitter users.?
Tweets frequently contain embedded URLsthat direct users to other online content, suchas news web pages, blogs, organization home-pages (Wu et al, 2011).
According to Twitter?snews release in September 2010 (Rao, 2010),25% of tweets contain an URL.
These linkedweb pages provide a much richer source of in-formation than is possible in the 140-charactertweet.These Twitter-specific characteristics may posechallenges to the automatic summarization systemsfor identifying the essential information.
In this pa-per, we focus on two such characteristics that arenot studied in previous literature, the web contentlink and the non-standard tokens in tweets.
Specif-ically, we ask two questions: (1) Is the web contentlinked from the tweets useful for summarization?Can we integrate different text sources, includingthe tweets and linked web pages, to generate moreinformative Twitter topic summaries?
(2) what isthe effect of nonstandard tokens on summarizationperformance?
Will the summaries be improved ifthe noisy tweets were pre-normalized into standardEnglish sentences?
We investigate these two ques-tions under a concept-based summarization frame-work using integer linear programming (ILP).
Weutilize text input that has various quality and is orig-inated from multiple sources, and thoroughly ana-lyze the resulting summaries using both automaticand human evaluation metrics.2 Related WorkThere is not much previous work on summarizingthe Twitter topics.
Most previous summarization lit-erature focused on the written text domain, as drivenby the annual evaluation tracks of the DUC (Doc-ument Understanding Conference) and TAC (TextAnalysis Conference).
To some extent, Twitter topicsummarization is related to spoken document sum-marization, since both tasks deal with the conver-sational text that is contributed by multiple par-ticipants and contains lots of ill-formed sentences,colloquial expressions, nonstandard word tokens orhigh word error rate, etc.
To summarize the spo-ken text, (Zechner, 2002) aimed to address prob-lems related to disfluencies, extraction units, cross-speaker coherence, etc.
(Maskey and Hirschberg,2005; Murray et al, 2006; Galley, 2006; Xie etal., 2008; Liu and Liu, 2010a) incorporated lexical,structural, speaker, and discourse cues to generatetextual summaries for broadcast news and meetingconversations.For microblog summarization, (Sharifi et al,2010a) proposed a phrase reinforcement (PR) algo-rithm to summarize the Twitter topic in one sen-tence.
The algorithm builds a word graph using thetopic phrase as the root node; each word node isweighted in proportion to its distance to the root andthe corresponding phrase frequency.
The summarysentence is selected as one of the highest weightedpaths in the graph.
(Sharifi et al, 2010b; Inouye,2010) introduced a hybrid TF-IDF approach to ex-tract one- or multiple-sentence summary for eachtopic.
Sentences were ranked according to the av-erage TF-IDF score of the consisting words; topweighted sentences were iteratively extracted, butexcluding those that have high cosine similarity withthe existing summary sentences.
They showed theHybrid TF-IDF approach performs constantly bet-67ter than the PR algorithm and other traditional sum-marization systems.
Our approach of summarizingthe Twitter topics is different from the above stud-ies in that, we focus on exploring richer informa-tion sources (such as the online web content) and in-vestigating effect of non-standard tokens.
There arealso studies working on visualizing Twitter topicsby identifying a set of topic phrases and presentingthe related tweets to users (O?Connor et al, 2010;Marcus et al, 2011).
Our proposed approach can bebeneficial to these systems by providing informativetopic summaries generated from rich text sources.3 Data CollectionWe collected 5,537 topic phrases and the referencetopic descriptions by crawling the Twitter.com andWhatTheTrend.com simultaneously during the pe-riod of Aug 22th, 2010 to Oct 30th, 2010 (about 70days).
The Twitter API was queried every 5 min-utes for the current top ten trending topics.
For eachof these topics, a search query was submitted to theTwitter Search API to retrieve only English tweetsrelated to this topic.
If any tweet contains embeddedURLs linked to the other web pages, the contentsof these web pages were retrieved.
For each topic,we limit the maximum number of retrieved tweets to5,000 and webpages to 100.
An example is shown inTable 1 for a topic phrase, some related tweets, andan excerpt of the linked webpage.
WhatTheTrendAPI provides short topic descriptions contributedand constantly updated by the Twitter users.
Thereis also a manually assigned category tag for eachtopic phrase.
We found the top categories amongthe collected topics are ?Entertainment (29.26%)?,?Sports (25.58%)?, and ?Meme (15.69%, pointlessbabble)?.
We divided the collected topics into twogroups: the general topics (e.g., ?Chilean miners?,?MTV VMA?)
and the hashtag topics that start withthe ?#?
(e.g., ?#top10rappers?, ?#octoberwish?
).To generate reference summaries for the Twit-ter topics, two human annotators were asked topick the topic descriptions/sentences (collected fromWhatTheTrend.com) that are appropriate and valu-able to be included in the summary.
This is per-formed on a selected set of 1,511 topics with bothtrending duration and number of tweets greater thanour predefined thresholds.
For each of the topic sen-tences, we ask the annotators to label its category:(1) the sentence is a general description of the topic;(2) the sentence is trying to explain why the topic istrending; (3) it is hard to tell the difference.
Over-all, the two annotators have good agreement (Kappa= 0.67) regarding whether or not to include a sen-tence in the summary.
Among the selected summarysentences, 22.58% of them were assigned with con-flicting purpose tags such as (1) or (2).
To forma reference summary, we concatenate all the topicsentences selected by both annotators.
Since somereference descriptions are simply repetition of oth-ers with very minor changes, we reduce the dupli-cates by iteratively removing the oldest sentences ifall the consisting words are covered by the remain-ing sentence collection, until no sentence can be re-moved.
On average, the reference summary for gen-eral and hashtag topics contains 44 and 40 wordsrespectively.4 Summarization SystemFor each of the topic phrases, our goal is to gener-ate a short textual summary that can best convey themain ideas of the topic contents.
We explore andcompare multiple text sources as summarization in-put, including the user-contributed tweets, web con-tents linked from the tweets, as well as combinationof the two sources.
The concept-based optimizationapproach (Gillick et al, 2009; Xie et al, 2009; Mur-ray et al, 2010) was employed for selecting informa-tive summary sentences and minimizing the redun-dancy.
Note that our focus of this paper is not devel-oping new summarization systems, but rather utiliz-ing and integrating different text sources for gener-ating more informative Twitter topic summaries.4.1 Concept-based Optimization FrameworkConcept-based summarization approach first ex-tracts a set of important concepts for each topic, thenselects a collection of sentences that can cover asmany important concepts as possible, while withinthe specified length limit.
This idea is realized us-ing the integer linear programming-based (ILP) op-timization framework, with objective function set tomaximize the sum of the weighted concepts:max?iwici68where ci is a binary variable indicating whether theconcept i is covered by the summary; wi is theweight assigned to ci.We enforce two sets of length constraints to thesummary: sentence- or word-based.
Sentence con-straint requires the total number of selected sum-mary sentences to not to exceed a length limit L1;while word constraint requires the total words ofselected sentences not to exceed length limit L2.These two constraints are:?jsj < L1 or?jljsj < L2where sj is a binary variable indicating whether sen-tence j was selected in the summary; lj representsthe number of words in sj .Further, we connect concept i with sentence j us-ing two sets of constraints.
For all the sentences thatcontain concept i, if any sentence was selected inthe summary, the concept i should be covered by thesummary; reversely, if concept i was covered by thesummary, at least one of the sentences containingconcept i should be selected.
?i ci ?
?joijsj?i, j ci ?
oijsjwhere the binary variable oij is used to indicatewhether concept i exists in sentence j.The concepts are selected by extracting n-grams(n=1, 2, 3) from the input documents correspondingto each topic.
Similar to (Xie et al, 2009), we re-move (1) n-grams that appear only once in the docu-ments; (2) n-grams that have a consisting word withinverse document frequency (IDF) value lower thana threshold; (3) n-grams that are enclosed by higherorder n-grams with the same frequency.
These fil-ters are designed to exclude insignificant n-gramsfrom the concept set.
The IDF scores were calcu-lated from a large background corpus correspondingto the input text source, using individual sentencesor tweets as pseudo-documents; words with low IDFscores (such as stopwords) tend to appear in manysentences and therefore should be removed from theconcept set.
We assign a weight wi to an n-gramconcept as follows:wi = tf(ngrami)?
n?maxjidf(wij)where tf(ngrami) is the term frequency of ngramiin the input document of the topic; n denotes theorder of ngrami; wij are the consisting words ofngrami; idf(wij) represents IDF value of wordwij .
This approach aims to extract n-grams that ap-pear frequently in each topic, but do not appear fre-quently in a large background corpus.
The weightsare also biased towards longer n-grams since theycarry more information.4.2 Summarization InputIn this section, we explore different text sourcesas input to the summarization system.
Differentfrom previous studies that take input from a sin-gle text source, we propose to utilize both theuser-contributed tweets and the linked web con-tents for Twitter topic summarization, since thesetwo sources provide very different text quality andmay contain complementary information regardingthe topic.
These text sources also pose great chal-lenges to the summarization system: the tweets areshort and extremely noisy; while the online contentslinked from the tweets may have vastly different lay-outs and contain a variety of information.4.2.1 Original TweetsAs shown in Table 1, the initially collected tweetsare very noisy.
They are passed through a set of pre-processors to remove non-ascii characters, HTMLspecial characters, URLs, emoticons, punctuationmarks, retweet tags (RT @user), etc.
We also re-move the reply (@) and hashtag (#) tokens that donot carry important syntactic roles (such as in thesubject or object position) by using a set of regularexpressions.
These preprocessed tweets are sortedby date and taken as the first input source to the sum-marization system (denoted by ?OrigTweets?
).4.2.2 Normalized TweetsThe original tweets contain various nonstandardword tokens.
In Table 2, we list the possible to-ken categories and corresponding examples.
We hy-pothesize that normalizing these nonstandard tokensinto standard English words and using the normal-ized tweets as input can help boost the summariza-tion performance.We developed a twitter message normalizationsystem based on the noisy-channel framework anda proposed letter transformation model (Liu et al,69Category Example(1) abbreviation tgthr, weeknd, shudnt(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq(4) typographic error thimg, macam(5) stylistic variation betta, hubbie, cutie(6) letter repetition pleeeaas, togtherrr(7) any combination of (1) to (6) luvvvin, 2moro, m0rninTable 2: Nonstandard token categories and examples.2011).
Given a noisy tweet T , our goal is to nor-malize it into a standard English word sequence S.Under the noisy channel model, this is equivalent tofinding the sequence S?
that maximizes p(S|T ):S?
= argmaxS p(S|T ) = argmaxS(?ip(Ti|Si))p(S)where we assume that each non-standard token Tiis dependent on only one English word Si, that is,we are not considering acronyms (e.g., ?bbl?
for ?beback later?)
in this study.
p(S) can be calculatedusing a language model (LM).
We formulate theprocess of generating a nonstandard token Ti fromdictionary word Si using a letter transformationmodel, and use the model confidence as the prob-ability p(Ti|Si).
This transformation process will belearned automatically through a sequence labelingframework.
To form a nonstandard token, each let-ter in the dictionary word can be labeled with: (a)one of the 0-9 digits; (b) one of the 26 charactersincluding itself; (c) the null character ?-?
; (d) a let-ter combination.
We integrate character-, phonetic-,and syllable-level features in the model that can ef-fectively characterize the formation process of non-standard tokens.
In general, the letter transforma-tion approach will handle the nonstandard tokenslisted in Table 2 yet without explicitly categorizingthem.
The proposed system also achieved robustperformance using the automatically collected train-ing word pairs.
On a test set of 3,802 distinct non-standard tokens collected from Twitter, our systemachieved 68.88% 1-best normalization word accu-racy and 78.27% 3-best accuracy.We identify the nonstandard tokens that need tobe normalized using the following criteria: (1) it isnot in the CMU dictionary2; (2) it does not containcapitalized letter; (3) it appears infrequently in the2http://www.speech.cs.cmu.edu/cgi-bin/cmudicttopic (less than a threshold); (4) it is not a popularchat acronyms (such as ?lol?, ?omg?
); (5) it containsletters/digits/apostrophe, but should not be numbersonly.
These criteria are designed to avoid normaliz-ing the named entities, frequently appearing out-of-vocabulary terms (such as ?itunes?
), chat acronyms,usernames, and hashtags.
The selected nonstandardtokens in the original tweets will be replaced by thesystem generated 1-best candidate word.
Note thatwe do not discriminate the context when replacingeach nonstandard token.
This will be addressed inthe future work.
We use these normalized tweets asa second source of summarization input and namethem ?NormTweets?.4.2.3 Linked Web ContentsFor each Twitter topic, we collect a set of webpages linked by the topic tweets and use them asanother source of summarization input.
For eachtopic, we select up to n (n = 10) URLs that appearmost frequently in the topic tweets and infrequentlyacross different Twitter topics.
This scheme is sim-ilar to the TF-IDF measure.
This way we can se-lect the salient URLs for each topic while avoidingthe spam URLs.
The contents of these URLs werecollected and only distinct web pages were retained.We use an HTML parser3 to extract the textual con-tents, and perform sentence segmentation (Reynarand Ratnaparkhi, 1997) on the parsed web pages.All the pages corresponding to the same topic weresorted by the date they were first cited in the tweets.These web pages were taken as another input textsource for the summarization system, denoted as?Web?.4.2.4 Combining Tweets and Web ContentsWe expect that taking advantage of both tweetsand linked web contents would benefit the topicsummarization system.
Consolidating the distincttext sources may help boost the weight of key con-cepts and eliminate the spam information.
As a pre-liminary study, we investigate concatenating eitherthe original tweets or the normalized tweets withthe linked web pages as input to the concept-basedsummarization system.
This results in two inputs?Web + OrigTweets?
and ?Web + NormTweets?.
Wewill explore other ways of combining the two text3http://jericho.htmlparser.net/docs/index.html70sources in future work.5 Experiments5.1 Experimental SetupAmong the collected topics, we select 500 generaltopics (such as ?Chilean miners?)
and 50 hashtagtopics (such as ?#octoberwish?, ?#wheniwasakid?
)for experimentation.
On average, a general topiccontains 1673 tweets and 3.43 extracted linked webpages; while a hashtag topic contains 3316 tweetsbut does not have meaningful linked web pages.The concept-based optimization system was con-figured to extract a collection of sentences/tweetsfor each topic, using either the sentence- or word-constraint (denoted as ?#Sent?
and ?#Word?).
Weopt to set individual length constraint for each topicrather than using a uniform length limit for all thetopics, since the topics can be very different inlength and duration.
We use the number of sen-tences/words in the reference summary as the sen-tence/word constraint for each topic.
Note that inpractice this reference summary length informationmay not be available.
We use the length constraintsobtained from the reference summary in this ex-ploratory study, since our focus is to first evaluate iftwitter trending summarization is feasible, and whatare the effects of different information sources andnon-standard tokens.
For a comparison to our ap-proach, we implement the Hybrid TF-IDF approachin (Sharifi et al, 2010b; Inouye, 2010) as a baselineusing ?OrigTweets?
as input.
For the baseline, thesummary length is altered according to the sentence-or word-constraint.
The last summary tweet is cut inthe middle if it exceeds the word limit.The ROUGE-1 F-scores (Lin, 2004) are used tomeasure the n-gram (n=1) overlap between the sys-tem summaries and reference summaries.
Since theROUGE scores may not correlate well with the hu-man judgments (Liu and Liu, 2010b), we also per-formed human evaluation by asking annotators toscore both the system and reference summaries re-garding the linguistic quality and content respon-siveness, in the hope this will benefit future researchin this direction.5.2 Automatic EvaluationWe present the results (ROUGE-1 F-measure) forthe general topics in Table 3.
ROUGE-2 andGeneral Topics R-1 F(%) RefSumInput Source Render #Sent #Word Cov(%)OrigTweetsOrig 29.53 30.21 94.81Norm 29.41 30.21 94.81NormTweets Norm 29.69 30.35 94.60Web 24.32 25.07 63.74Web + OrigTweets 29.58 30.44 95.37Web + NormTweets 29.66 30.54 95.16OrigTweets(Sharifi et al, 2010b) 24.37 25.68 94.81Table 3: ROUGE-1 F-measure and reference summarycoverage scores for general topics.ROUGE-4 scores show similar trends and thus arenot presented.
Five different text sources were ex-ploited as the system inputs, as described in Sec-tion 4.2.
To measure the quality of the input forsummarization, we also include reference summarycoverage score in the table, defined as the percent-age of words in the reference summary that are cov-ered by the input text source.
When using tweetsas input, we also investigate whether we should ap-ply tweet normalization before or after the summa-rization process, that is ?pre-normalization?
(using?NormTweets?
as input), or ?post-normalization?
(using ?OrigTweets?
as input, and rendering the nor-malized summary tweets).Compared to the Hybrid TF-IDF approach (Shar-ifi et al, 2010b; Inouye, 2010), our system per-forms significantly better (p < 0.05) accordingto the paired t-test; however, we also notice theROUGE scores are lower compared to summariza-tion in other text domains.
This indicates that Twit-ter topic summarization is very challenging.
Com-paring the two constraints used in the concept-basedoptimization framework, we found that the wordconstraint performs constantly better for the gen-eral topics.
This is natural since the word constrainttightly bounds the length of the system output, whilethe sentence constraint is relatively loose.
For thedifferent sources, we notice using linked web pagesalone yields worse summarization performance, aswell as lower reference summary coverage; how-ever, when combined with the tweets, there is aslight increase in the coverage scores, and some-times improved summarization results.
This sug-gests that the linked web pages can contain extra71useful information for generating summaries.
Re-garding normalization, results show that the ?pre-normalization?
(using normalized tweets as input)can generally improve the summary tweet selec-tion.
For general topics, the best performance wasachieved by combining the normalized tweets andlinked web pages as input source and using theword-level constraint.Hashtag Topics R-1 F(%) RefSumInput Source Render #Sent #Word Cov(%)OrigTweetsOrig 9.08 7.19 93.93Norm 9.09 7.16 93.93NormTweets Norm 9.35 7.14 93.71OrigTweets(Sharifi et al, 2010b) 7.03 7.72 93.93Table 4: ROUGE-1 F-measure and reference summarycoverage scores for hashtag topics.Results for hashtag topics were shown in Table4 using tweets as input (there are no linked web-pages for these topics).
We notice the reference cov-erage scores are satisfying, yet the system outputbarely matches the reference summaries (very lowROUGE-1 scores).
Looking at the reference andsystem generated summaries for the hashtag top-ics, we found the system output is more specific(e.g., ?#octoberwish everything goes well.?
), whilethe reference summaries are often very general (e.g.,?people tweeting about their wishes for October.?
).The human annotators also noted that most hashtagtopics (such as ?#octoberwish?, ?#wheniwasakid?
)are self-explainable and may require special atten-tion to redefine an appropriate summary.
Usingsentence constraints yields better performance thanword-based one, with larger performance differencethan that for the general topics.
We found theword-constraint summaries tend to include tweetsthat are very short and noisy.
Our system withsentence-based length constraint also significantlyoutperforms the Hybrid TF-IDF approach (Sharifiet al, 2010b; Inouye, 2010).
For hashtag topics,the best performance was achieved using the ?pre-normalization?
with sentence constraint.For an analysis, we generate oracle system per-formance by using the reference summaries to ex-tract a set of unweighted concepts to use in the ILPoptimization framework for sentences/tweets selec-tion.
This results in 61.76% ROUGE-1 F-score forthe general topics and 40.34% for the hashtag topics,indicating abundant space for future improvement.We also notice that though there is some perfor-mance gain using normalized tweets and linked webcontents, the improvement is not statistically signifi-cant as compared to using the original tweets.
Uponcloser examination, we found the normalization sys-tem replaced 1.08% and 1.8% of the total word to-kens for the general and hashtag topics respectively;these tokens spread in 13.12% and 16.85% of thetotal tweets.
The relatively small percentage of thenormalized tokens partly explains the marginal per-formance gain when using the normalized tweets asinput.
Similarly for linked web content, though itcontains some sentences that can provide more de-tails of the topic, but they can also take more spacein the summary as compared to the short and con-densed tweets.
Therefore using the combined tweetsand linked webpages does not significantly outper-form using just the tweets.5.3 Human EvaluationGeneral HashtagTweet Web Ref Tweet RefGram.
3.13 3.42 4.52 3.04 4.24NRedun.
3.93 4.64 4.30 4.82 3.62Clarity 4.07 3.91 4.77 4.06 4.60Focus 3.64 3.03 4.75 3.22 4.72Content 2.82 2.55 n/a 2.60 n/aExtraInfo n/a 2.63 n/a n/a n/aTable 5: Linguistic quality, content coverage, and useful-ness scores judged by human assessors.We ask two human annotators to manually evalu-ate the system and reference summaries regardingthe readability and content coverage.
Readabilityincludes grammaticality, non-redundancy, referen-tial clarity, and focus; content coverage was eval-uated for system summaries against the referencesummary.
The annotators were also asked to ratethe ?Web?
summaries regarding whether they pro-vided extra useful topic information on top of the?Tweet?
summary.
50 general topics and 25 hash-tag topics were randomly selected for assessment.The ?Tweet?
and ?Web?
summaries were generatedusing the original tweets and linked web pages withword constraint for general topics, and sentence con-straint for hashtag topics.
Each of the assessors was72General Topic: ?3PAR?RefSumDell Inc. and Hewlett-Packard Co. are both bidding for storage device maker 3Par Inc.3Par jumped 21 percent after Hewlett- Packard Co. offered $30 a share for the company.TweetSumDell ups 3Par offer yet again, to $27 per shareDell Raises 3par Offer to Match HP BidDell Matches HP?s Offer for 3Par, Boosting Bid to $1.8 BillionWebSumDell Matches HP?s $27 Offer, Is Accepted by 3PAR.3PAR has accepted an increased acquisition offer from Dell of US$27 per share, matchingHewlett-Packard?s earlier raised bid.Hashtag Topic: ?#wheniwasakid?RefSumwhen i was a kid.... people are sharing there best (good or bad) memories from childhood.People reminise the wonderful times about being a kid.TweetSum#whenIwasakid getting wasted meant eating all the ice cream and candy you could until you puked!#whenIWasAKid Apple & Blackberry were fruits not phones.Table 6: Example system and reference summaries for both general and hashtag topics.asked to judge all the summaries and assign a scorefor each criterion on a 1 to 5 Likert scale (5 beingthe best quality).
The average scores of the two as-sessors were presented in Table 5.For general topics, the ?Web?
summaries outper-form the ?Tweet?
summaries on both grammatical-ity and non-redundancy, confirming the advantageof using the high-quality linked web pages.
Thereferential clarity and focus scores of the ?Web?summaries are not very high, since the summarysentences were extracted simultaneously from sev-eral web pages, and the system subjects to simi-lar challenges as in multi-document summarization.The content coverage scores of both system sum-maries seem to correlate well with the ROUGE-1F-measure, with a higher score for ?Tweet?
sum-maries.
The assessors also rated that 48% of the?Web?
summaries contain ?Somewhat Useful?
ex-tra topic information, and 21% are ?Very Useful?.Note that this could be just because of the inherentdifference of the two summaries, regardless of theinput source, but in general we believe the linkedweb pages (such as the news documents) can pro-vide more detailed and coherent stories as comparedto the 140-character tweets.
For hashtag topics, the?Tweet?
summaries yield worse grammaticality andfocus scores, but have very high non-redundancyscore.
On the contrary, the reference summariesoften contain redundant information.
The contentmatch score between the system and reference sum-maries (2.6) does not seem to reflect the ROUGEscores.
We hypothesize that even though the speci-ficity of the two summaries is different, the asses-sors may still think the system summaries match thereference ones to some extent.
A larger scale humanevaluation is needed to study the correlation betweenhuman and automatic evaluation.5.4 DiscussionsWe show an example of reference and system gen-erated summaries for a general and a hashtag topicin Table 6, and summarize some challenges for thissummarization task below:?
Gold standard summaries are difficult andtime-consuming to obtain.
The reference de-scriptions from WhatTheTrend.com were cre-ated by Twitter users, which vary a lot inword usage and would be unavoidably biasedto the information available in Twitter.
Theuser-contributed descriptions may also containspam descriptions, repetitions, nonstandard to-kens, etc.
It would be better to have a con-cise non-redundant sentence collection for de-veloping future summarization systems.
Inparticular, hashtag topics need special atten-tion.
They account for 40% of the total trend-ing topics in 2010 according to the statisticsin WhatTheTrend.com4.
Yet there still lacksstandard definition regarding a good hashtagsummary.
From the example topic ?#wheni-wasakid?
in Table 6, we can see they are verydifferent in nature from general topics, thus fu-ture efforts are needed to define an appropriatesummary.4http://yearinreview.whatthetrend.com/73?
Evaluation issues.
Word based evaluationmeasures will rarely consider semantic relat-edness between concepts, or name entity vari-ations, such as ?Hewlett-Packard?
vs.
?HP?,?Dell ups 3Par offer?
vs. ?Dell Raises 3parOffer?, etc.
When comparing the systemsummaries with short human-written referencesummaries, the word overlap varies a lot fordifferent human summarizers.?
Dynamically changing topics/events.
Somegeneral topics are related to events that are con-stantly changing.
Take the ?3PAR?
topic inTable 6 as an example, where two companiestake turns to raise the bid for 3Par Inc. A goodtopic summary should be able to develop a se-ries of sub-events and show the topic evolvingprocess.6 ConclusionIn this paper, we proposed to explore a variety of textsources for summarizing the Twitter topics.
We em-ployed the concept-based optimization frameworkwith multiple input text sources to generate the sum-maries.
We conducted both automatic and humanevaluation regarding the summary quality.
Betterperformance is observed when using the normalizedtweets as input, indicating special treatment shouldbe performed before feeding the noisy tweets to thesummarization system.
We also found the linkedweb contents can provide extra useful topic infor-mation.
In future work, we will compare our sys-tem with other dedicated microblog summarizationsystems, as well as address some of the challengesidentified in this study.AcknowledgmentsThis work is partly supported by NSF award IIS-0845484.
Any opinions expressed in this work arethose of the authors and do not necessarily reflect theviews of NSF.ReferencesMichel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.
InProc.
of EMNLP.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-Tu?r.
2009.
A global optimizationframework for meeting summarization.
In Proc.
ofICASSP.David Inouye.
2010.
Multiple post microblog summa-rization.
REU Research Final Report.Chin-Yew Lin.
2004.
ROUGE: A package for automaticevaluation of summaries.
In Workshop on Text Sum-marization Branches Out.Fei Liu and Yang Liu.
2010a.
Exploring speaker char-acteristics for meeting summarization.
In Proc.
of IN-TERSPEECH.Feifan Liu and Yang Liu.
2010b.
Exploring correlationbetween ROUGE and human evaluation on meetingsummaries.
IEEE Transactions on Audio, Speech, andLanguage Processing, 18(1):187?196.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011.
Insertion, deletion, or substitution?
Normaliz-ing text messages without pre-categorization nor su-pervision.
In Proc.
of ACL-HLT.Adam Marcus, Michael S. Bernstein, Osama Badar,David R. Karger, Samuel Madden, and Robert C.Miller.
2011.
TwitInfo: Aggregating and visualizingmicroblogs for event exploration.
In Proc.
of CHI.Sameer Maskey and Julia Hirschberg.
2005.
Compar-ing lexical, acoustic/prosodic, structural and discoursefeatures for speech summarization.
In Proc.
of Eu-rospeech.Gabriel Murray, Steve Renals, Jean Carletta, and JohannaMoore.
2006.
Incorporating speaker and discoursefeatures into speech summarization.
In Proc.
of HLT-NAACL.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2010.
Interpretation and transformation for abstract-ing conversations.
In Proc.
of NAACL.Brendan O?Connor, Michel Krieger, and David Ahn.2010.
Tweetmotif: Exploratory search and topic sum-marization for twitter.
In Proc.
of the InternationalAAAI Conference on Weblogs and Social Media.Leena Rao.
2010.
Twitter seeing 90 mil-lion tweets per day, 25 percent contain links.http://techcrunch.com/2010/09/14/twitter-seeing-90-million-tweets-per-day/.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proc.
of the Fifth Conference on Ap-plied Natural Language Processing.Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.2010a.
Summarizing microblogs automatically.
InProc.
of HLT/NAACL.Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.Kalita.
2010b.
Experiments in microblog summariza-tion.
In Proc.
of IEEE Second International Confer-ence on Social Computing.74Shaomei Wu, Jake M. Hofman, Winter A. Mason, andDuncan J. Watts.
2011. Who says what to whom ontwitter.
In Proc.
of WWW.Shasha Xie, Yang Liu, and Hui Lin.
2008.
Evaluatingthe effectiveness of features and sampling in extractivemeeting summarization.
In Proc.
of IEEE Workshopon Spoken Language Technology.Shasha Xie, Benoit Favre, Dilek Hakkani-Tu?r, and YangLiu.
2009.
Leveraging sentence weights in a concept-based optimization framework for extractive meetingsummarization.
In Proc.
of INTERSPEECH.Klaus Zechner.
2002.
Automatic summarization ofopen-domain multiparty dialogues in diverse genres.Computational Linguistics, 28(4):447?485.75
