Eff ic ient A lgor i thms for Pars ing the  DOP Mode l  *Joshua GoodmanHarvard University33 Oxford St.Cambridge, MA 02138goodman@das.harvard.eduAbstractExcellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts(Bod, 1993c).
Unfortunately, existing algorithmsare both computationally intensive and difficultto implement.
Previous algorithms are expen-sive due to two factors: the exponential numberof rules that must be generated and the use ofa Monte Carlo parsing algorithm.
In this paperwe solve the first problem by a novel reduction ofthe DOP model to:a small, equivalent probabilisticcontext-free grammar.
We solve the second prob-lem by a novel deterministic parsing strategy thatmaximizes the expected number of correct con-stituents, rather than the probability of a correctparse tree.
Using ithe optimizations, experimentsyield a 97% crossing brackets rate and 88% zerocrossing brackets rate.
This differs significantlyfrom the results reported by Bod, and is compara-ble to results from a duplication of Pereira andSchabes's (1992) experiment on the same data.We show that Bod's results are at least partiallydue to an extremely fortuitous choice of test data,and partially due to using cleaner data than otherresearchers.In t roduct ionThe Data-Oriented Parsing (DOP) model has ashort, interesting, and controversial history.
Itwas introduced by Remko Scha (1990), and wasthen studied by Rens Bod.
Unfortunately, Bod(1993c, 1992) was not able to find an efficient exact* I would like to acknowledge support from Na-tional Science Foundation Grant IRI-9350192 and aNational Science Foundation Graduate Student Fel-lowship.
I would also like to thank Rens Bod, StanChen, Andrew Kehler, David Magerman, WheelerRural, Stuart Shieber, and Khalil Sima'an for help-ful discussions, and comments on earlier drafts, andthe comments of the anonymous reviewers.algorithm for parsing using the model; however hedid discover and implement Monte Carlo approxi-mations.
He tested these algorithms on a cleanedup version of the ATIS corpus, and achieved somevery exciting results, reportedly getting 96% of histest set exactly correct, a huge improvement overprevious results.
For instance, Bod (1993b) com-pares these results to Schabes (1993), in which,for short sentences, 30% of the sentences have nocrossing brackets (a much easier measure than ex-act match).
Thus, Bod achieves an extraordinary&fold error rate reduction.Not surprisingly, other researchers attemptedto duplicate these results, but due to a lack of de-tails of the parsing algorithm in his publications,these other researchers were not able to confirmthe results (Magerman, Lalferty, personal commu-nication).
Even Bod's thesis (Bod, 1995a) doesnot contain enough information to replicate his re-sults.Parsing using the DOP model is especially dif-ficult.
The model can be summarized as a spe-cial kind of Stochastic Tree Substitution Grammar(STSG): given a bracketed, labelled training cor-pus, let every subtree of that corpus be an elemen-tary tree, with a probability proportional to thenumber of occurrences ofthat subtree in the train-ing corpus.
Unfortunately, the number of trees isin general exponential in the size of the trainingcorpus trees, producing an unwieldy grammar.In this paper, we introduce a reduction of theDOP model to an exactly equivalent ProbabilisticContext Free Grammar (PCFG) that is linear inthe number of nodes in the training data.
Next,we present an algorithm for parsing, which returnsthe parse that is expected to have the largest num-ber of correct constituents.
We use the reductionand algorithm to parse held out test data, com-paring these results to a replication of Pereira and143SNP VPPN PN V NPDET NFigure 1: Training corpus tree for DOP exampleSchabes (1992) on the same data.
These resultsare disappointing: the PCFG implementation ofthe DOP model performs about the same as thePereira and Schabes method.
We present an anal-ysis of the runtime of our algorithm and Bod's.Finally, we analyze Bod's data, showing that someof the difference between our performance and hisis due to a fortuitous choice of test data.This paper contains the first published repli-cation of the full DOP model, i.e.
using a parserwhich sums over derivations.
It also contains algo-rithms implementing the model with significantlyfewer resources than previously needed.
Further-more, for the first time, the DOP model is com-pared on the same data to a competing model.P rev ious  ResearchThe DOP model itself is extremely simple and canbe described as follows: for every sentence in aparsed training corpus, extract every subtree.
Ingeneral, the number of subtrees will be very large,typically exponential in sentence l ngth.
Now, usethese trees to form a Stochastic Tree Substitu-tion Grammar (STSG).
There are two ways to de-fine a STSG: either as a Stochastic Tree Adjoin-ing Grammar (Schabes, 1992) restricted to sub-stitution operations, or as an extended PCFG inwhich entire trees may occur on the right handside, instead of just strings of terminals and non-terminals.Given the tree of Figure 1, we can use theDOP model to convert it into the STSG of Figure2.
The numbers in parentheses represent the prob-abilities.
These trees can be combined in variousways to parse sentences.In theory, the DOP model has several advan-tages over other models.
Unlike a PCFG, the useof trees allows capturing large contexts, makingthe model more sensitive.
Since every subtree isincluded, even trivial ones corresponding to rulesin a PCFG, novel sentences with unseen contexts144s (3) s s = s (~)_~ ~ ~ B (1)A C A D E B E B II I I I I I IX X X X ~rFigure 3: Simple Example STSGcan still be parsed.Unfortunately, the number of subtrees i  huge;therefore Bod randomly samples 5% of the sub-trees, throwing away the rest.
This significantlyspeeds up parsing.There are two existing ways to parse using theDOP model.
First, one can find the most proba-ble derivation.
That is, there can be many waysa given sentence could be derived from the STSG.Using the most probable derivation criterion, onesimply finds the most probable way that a sentencecould be produced.
Figure 3 shows a simple ex-ample STSG.
For the string xx,  what is the mostprobable derivation?
The parse treeSA CI Ix xhas probability ~ of being generated by the trivialderivation containing a single tree.
This tree cor-responds to the most probable derivation of xx.One could try to find the most probable parsetree.
For a given sentence and a given parse tree,there are many different derivations that couldlead to that parse tree.
The probability of theparse tree is the sum of the probabilities of thederivations.
Given our example, there are two dif-ferent ways to generate the parse treeSE BI Ix xeach with probability -~, so that the parse tree hasprobability -~.
This parse tree is most probable.Bod (1993c) shows how to approximate thismost probable parse using a Monte Carlo algo-rithm.
The algorithm randomly samples possiblederivations, then finds the tree with the most sam-pled derivations.
Bod shows that the most proba-ble parse yields better performance than the mostprobable derivation on the exact match criterion.NP (?
)DET NVP (?
)V NPsNP VPV NP//'......DET NvP (?
)NP (?
)V NPPN PNDET Ns sNP VP  NP  VPPN PN PN PN V NPssNP VPNP  VPV NPs ({)NP VPPN PN V NPDET NFigure 2: Sample STSG Produced from DOP ModelKhalil Sima'an (1996) implemented a versionof the DOP model, which parses efficiently by lim-iting the number of trees used and by using anefficient most probable derivation model.
His ex-periments differed from ours and Bod's in manyways, including his use of a ditferent version of theATIS corpus; the use of word strings, rather thanpart of speech strings; and the fact that he did notparse sentences containing unknown words, effec-tively throwing out the most difficult sentences.Furthermore, Sim a'an limited the number of sub-stitution sites for his trees, effectively using a sub-set of the DOP model.Reduct ion  o f  DOP to  PCFGUnfortunately, Bod's reduction to a STSG is ex-tremely expensive, even when throwing away 95%of the grammar.
Fortunately, it is possible to findan equivalent PCFG that contains exactly eightPCFG rules for each node in the training data;thus it is O(n).
Because this reduction is so muchsmaller, we do not discard any of the grammarwhen using it.
The PCFG is equivalent in twosenses: first it generates the same strings with thesame probabilities; second, using an isomorphismdefined below, it generates the same trees with thesame probabilities, although one must sum overseveral PCFG trees for each STSG tree.To show this reduction and equivalence, wemust first define some terminology.
We assign ev-ery node in every tree a unique number, which wewill call its address.
Let A@k denote the node ataddress k, where A is the non-terminal labelingthat node.
We will need to create one new non-terminal for each node in the training data.
Wewill call this non-terminal Ak.
We will call non-terminals of this form "interior" non-terminals,and the original non-terminals in the parse trees145"exterior".Let aj represent he number of subtreesheaded by the node A@j.
Let a represent the num-ber of subtrees headed by nodes with non-terminalA, that is a = ~j  aj.Consider a node A~j  of the form:A@jB@k C@lHow many subtrees does it have?
Consider firstthe possibilities on the left branch.
There are bknon-trivial subtrees headed by B@k, and there isalso the trivial case where the left node is sim-ply B.
Thus there are bk ?
1 different possibil-ities on the left branch.
Similarly, for the rightbranch there are cl + 1 possibilities.
We can cre-ate a subtree by choosing any possible left subtreeand any possible right subtree.
Thus, there areaj = (bk + 1)(c~ + 1) possible subtrees headed byA@j.
In our example tree of Figure 1, both nounphrases have exactly one subtree: np4 -- nl>z -- 1;the verb phrase has 2 subtrees: vp3 = 2; and thesentence has 6: sl = 6.
These numbers correspondto the number of subtrees in Figure 2.We will call a PCFG subderivation isomor-phic to a STSG tree if the subderivation beginswith an external non-terminal, uses internal non-terminals for intermediate steps, and ends withexternal non-terminals.
For instance, consider thetreeNP VPPN PN V NPtaken from Figure 2.
The following PCFG sub-derivation is isomorphic: S ~ NP@I VP@2PN PN VP@2 =~ PN PN V NP.
We saythat a PCFG derivation is isomorphic to a STSGderivation if there is a corresponding PCFG sub-derivation for every step in the STSG derivation.We will give a simple small PCFG with thefollowing surprising property: for every subtree inthe training corpus headed by A, the grammar willgenerate an isomorphic subderivation with proba-bility 1/a.
In other words, rather than using thelarge, explicit STSG, we can use this small PCFGthat generates i omorphic derivations, with iden-tical probabilities.The construction is as follows.
For a node suchasA@jB@k C@lwe will generate the following eight PCFG rules,where the number in parentheses following a ruleis its probability.Aj  --~ SC (1 /a j )  A ~ BC ( l /a)Aj ~ BkC (bh/aj) A ~ BkC (bk/a)Aj ~ BCi (ci/aj) A ~ BCz (cJa)Aj ~ B~Ci (bkcl/aj) A ~ BkCl (bkcl/a)(1)We will show that subderivations headedby A with external non-terminals at the rootsand leaves, internal non-terminals elsewhere haveprobability 1/a.
Subderivations headed by Ajwith external non-terminals only at the leaves, in-ternal non-terminals elsewhere, have probability1/aj.
The proof is by induction on the depth ofthe trees.For trees of depth 1, there are two cases:A A@jB C B CTrivially, these trees have the required probabili-ties.Now, assume that the theorem is true for treesof depth n or less.
We show that it holds for treesof depth n + 1.
There are eight cases, one for eachof the eight rules.
We show two of them.
LetB@k?
represent a tree of at most depth n withexternal leaves, headed by B@k, and with internalintermediate non-terminals.
Then, for trees suchas146PCFG derivation4 productionsSNP@3 VP@IPN PN V NPDET NSTSG derivation2 subtreesSNP VPPN PN V NPNPDET NFigure 4: Example of Isomorphic DerivationB@k C@l: :1 1 bhc !
~ 1 the probability of the tree is ~ ~ ai ~ .
Simi-larly, for another case, trees headed byAB@k Cthe probability of the tree is b~ b~a = ~'1 The othersix cases follow trivially with similar reasoning.We call a PCFG derivation isomorphic to aSTSG derivation if for every substitution in theSTSG there is a corresponding subderivation ithe PCFG.
Figure 4 contains an example of iso-morphic derivations, using two subtrees in theSTSG and four productions in the PCFG.We call a PCFG tree isomorphic to a STSGtree if they are identical when internal non-terminals are changed to external non-terminals.Our main theorem is that this construction pro-duces PCFG trees isomorphic to the STSG treeswith equal probability.
If every subtree in thetraining corpus occurred exactly once, this wouldbe trivial to prove.
For every STSG subderiva-tion, there would be an isomorphic PCFG sub-derivation, with equal probability.
Thus for everySTSG derivation, there would be an isomorphicPCFG derivation, with equal probability.
Thusevery STSG tree would be produced by the PCFGwith equal probability.However, it is extremely likely that some sub-trees, especially trivial ones likeSNP VPwill occur repeatedly.If the STSG formalism were modified slightly,so that trees could occur multiple times, then ourrelationship could be made one to one.
Consider amodified form of the DOP model, in which whensubtrees occurred multiple times in the trainingcorpus, their counts were not merged: both iden-tical trees are added to the grammar.
Each ofthese trees will have a lower probability than iftheir counts were merged.
This would change theprobabilities of the derivations; however the prob-abilities of parse trees would not change, sincethere would be correspondingly more derivationsfor each tree.
Now, the desired one to one relation-ship holds: for every derivation in the new STSGthere is an isomorphic derivation in the PCFGwith equal probability.
Thus, summing over allderivations of a tree in the STSG yields the sameprobability as summing over all the isomorphicderivations in the PCFG.
Thus, every STSG treewould be produced by the PCFG with equal prob-ability.It follows trivially from this that no extra treesare produced by the PCFG.
Since the total prob-ability of the trees produced by the STSG is 1,and the PCFG produces these trees with the sameprobability, no probability is "left over" for anyother trees.Parsing AlgorithmThere are several different evaluation metrics onecould use for finding the best parse.
In the sec-tion covering previous research, we considered themost probable derivation and the most probableparse tree.
There is one more metric we could con-sider.
If our performance evaluation were based onthe number of constituents correct, using measuressimilar to the crossing brackets measure, we wouldwant the parse tree that was most likely to havethe largest number of correct constituents.
Withthis criterion and the example grammar of Figure3, the best parse tree would be147SAA BI Ix ~gThe probability that the S constituent is correct is1.0, while the probability that the A constituent iscorrect is ~, and the probability that the B con-stituent is correct is }.
Thus, this tree has onaverage 2 constituents correct.
All other trees willhave fewer constituents correct on average.
Wecall the best parse tree under this criterion theMaximum Constituents Parse.
Notice that thisparse tree cannot even be produced by the gram-mar: each of its constituents i  good, but it is notnecessarily good when considered as a full tree.Bod (1993a, 1995a) shows that the most prob-able derivation does not perform as well as themost probable parse for the DOP model, getting65% exact match for the most probable deriva-tion, versus 96% correct for the most probableparse.
This is not surprising, since each parsetree can be derived by many different deriva-tions; the most probable parse criterion takesall possible derivations into account.
Similarly,the Maximum Constituents Parse is also derivedfrom the sum of many different derivations.
Fur-thermore, although the Maximum ConstituentsParse should not do as well on the exact matchcriterion, it should perform even better on thepercent constituents correct criterion.
We havepreviously performed a detailed comparison be-tween the most likely parse, and the MaximumConstituents Parse for Probabilistic Context FreeGrammars (Goodman, 1996); we showed that thetwo have very similax performance on a broadrange of measures, with at most a 10% differencein error rate (i.e., a change from 10% error rate to9% error rate.)
We therefore think that it is rea-sonable to use a Maximum Constituents Parser toparse the DOP model.The parsing algorithm is a variation on theInside-Outside algorithm, developed by Baker(1979) and discussed in detail by Lari and Young(1990).
However, while the Inside-Outside algo-rithm is a grammar e-estimation algorithm, thealgorithm presented here is just a parsing algo-rithm.
It is closely related to a similar algorithmused for Hidden Markov Models (Rabiner, 1989)for finding the most likely state at each time.
How-ever, unlike in the HMM case where the algorithmproduces a simple state sequence, in the PCFGcase a parse tree is produced, resulting in addi-for length := 2 to nfor s := 1 to n- length+lt := s + length -  I;for all non- termina ls  Xsum\[X\] := g(s ,  t ,  X);loop over addresses klet X := non- termina l  at k;l e t  sum\[X\] := sum\[X\] + g(s , t ,X_k) ;loop over non- termina ls  Xl e t  max_X := art max of sum IX\]loop over  r such that s <= r < tlet best_split  :=max of maxc\[s,r\] + maxc\[r+l,t\];maxc\[s,t\] := sum\[max_X\] + best_split;Figure 5: Maximum Constituents Data-OrientedParsing Algorithmtional constraints.A formal derivation of a very similar algorithmis given elsewhere (Goodman, 1996); only the in-tuition is given here.
The algorithm can be sum-marized as follows.
First, for each potential con-stituent, where a constituent is a non-terminal, astart position, and an end position, find the prob-ability that that constituent is in the parse.
Afterthat, put the most likely constituents ogether toform a parse tree, using dynamic programming.The probability that a potential constituentoccurs in the correct parse tree, P(X *ws...wtlS ~ wl...wn), will be called g(s,t,X).In words, it is the probability that, given thesentence wl...w,, a symbol X generates ws...wt.We can compute this probability using elementsof the Inside-Outside algorithm.
First, computethe inside probabilities, e(s, t, X) = P (X  =~w,...wt).
Second, compute the outside probabil-ities, / ( s , t ,X)  = P(S ~ wl...w~-lXwt+l...wn).Third, compute the matrix g(s, t, X):g(s,t,X)P(S ::~ wI...w,-1Xwt+I...w,)P(X ~ w,...wt)P(S ~ wl...wn)= f(s, t, X) x e(s, t, X)/e(1, n, S)Once the matrix g(s, t, X) is computed, a dy-namic programming algorithm can be used to de-termine the best parse, in the sense of maximizingthe number of constituents expected correct.
Fig-ure 5 shows pseudocode for a simplified form of148this algorithm.For a grammar with g nonterminals and train-ing data of size T, the run time of the algorithmis O(Tn 2 + gn 3 + n a) since there are two layersof outer loops, each with run time at most n, andinner loops, over addresses (training data), non-terminals and n. However, this is dominated bythe computation of the Inside and Outside prob-abilities, which takes time O(rna), for a grammarwith r rules.
Since there are eight rules for everynode in the training data, this is O(Tn3).By modifying the algorithm slightly to recordthe actual split used at each node, we can recoverthe best parse.
The entry maxc\[1, n\] containsthe expected number of correct constituents, giventhe model.Exper imenta l  Resu l ts  andD iscuss ionWe are grateful to Bod for supplying the datathat he used for his experiments (Bod, 1995b,Bod, 1995a, Bod, 1993c).
The original ATIS datafrom the Penn Tree Bank, version 0.5, is verynoisy; it is difficult to even automatically read thisdata, due to inconsistencies between files.
Re-searchers are thus left with the difficult decisionas to how to clean the data.
For this paper, weconducted two sets of experiments: one using aminimally cleaned set of data, 1 making our resultscomparable to previous results; the other usingthe ATIS data prepared by Bod, which containedmuch more significant revisions.Ten data sets were constructed by randomlysplitting minimally edited ATIS (Hemphill et al,1990) sentences into a 700 sentence training set,and 88 sentence test set, then discarding sentencesof length > 30.
For each of the ten sets, both theDOP algorithm outlined here and the grammarinduction experiment of Pereira and Schabes wererun.
Crossing brackets, zero crossing brackets, andthe paired differences are presented in Table 1.All sentences output by the parser were made bi-nary branching (see the section covering analysisof Bod's data), since otherwise the crossing brack-ets measures are meaningless (Magerman, 1994).1A diff file between the original ATIS data andthe cleaned up version, in a form usable by the"eft' program, is available by anonymous FTP fromftp://ftp.das.harvard.edu/pub/goodman/atis-ed/tLtb.par-ed and ti_tb.pos-ed.
Note that the numberof changes made was small.
The diff files sum to 457bytes, versus 269,339 bytes for the original files, or lessthan 0.2%.CriteriaCross Brack DOPCross Brack P&SCross Brack DOP-P&SZero Cross Brack DOPZero Cross Brack P&SMin86.53%86.99%-3.79%60.23%54.02%Max96.O6%94.41%2.87%75.86%78.16%Range9.53%7.42%6.66%15.63%24.14%Mean90.15%90.18%-0.03%66.11%63.94%2.17%StdDev2.65%2.59%2.34%5.56%7.34%Zero Cross Brack DOP-P&S -5.68% 11.36% 17.05% 5.57%Table 1: DOP versus Pereira and Schabes on Minimally Edited ATISCriteria Min Max Range Mean StdDevCross Brack DOP 95.63% 98.62% 2.99% 97.16% 0.93%Cross Brack P&S 94.08% 97.87% 3.79% 96.11% 1.14%Cross Brack DOP-P&SZero Cross Brack DOP-0.16%78.67%3.03%90.67%3.19%12.00%Zero Cross Brack P&S 70.67% 88.00% 17.33%Zero Cross Brack DOP-P&S -1.33% 20.00% 21.33%Exact Match DOP 58.67% 68.00% 9.33%1.05%86.13%79.20%6.93%63.33%1.04%3.99%5.97%5.65%3.22%Table 2: DOP versus Pereira and Schabes on Bod's DataA few sentences were not parsable; these were as-signed right branching period high structure, agood heuristic (Brill, 1993).We also ran experiments using Bod's data,75 sentence test sets, and no limit on sentencelength.
However, while Bod provided us with hisdata, he did not provide us with the split intotest and training that he used; as before we usedten random splits.
The results are disappointing,as shown in Table 2.
They are noticeably worsethan those of Bod, and again very comparableto those of Pereira and Schabes.
Whereas Bodreported 96% exact match, we got only 86% us-ing the less restriCtive zero crossing brackets cri-terion.
It is not clear what exactly accounts forthese differences.
2 It is also noteworthy that theresults are much better on Bod's data than on theminimally edited data: crossing brackets rates of96% and 97% on Bod's data versus 90% on min-imally edited data.
Thus it appears that part ofBod's extraordinary performance an be explainedby the fact that his data is much cleaner than thedata used by other researchers.DOP does do slightly better on most mea-sures.
We performed a statistical analysis using at-test on the paired differences between DOP andPereira nd Schabes performance on each run.
On~Ideally, we would exactly reproduce these exper-iments using Bod's algorithm.
Unfortunately, it wasnot possible to get a full specification ofthe algorithm.149the minimally edited ATIS data, the differenceswere statistically insignificant, while on Bod's datathe differences were statistically significant beyondthe 98'th percentile.
Our technique for finding sta-tistical significance is more strenuous than most:we assume that since all test sentences were parsedwith the same training data, all results of a sin-gle run are correlated.
Thus we compare paireddifferences of entire runs, rather than of sentencesor constituents.
This makes it harder to achievestatistical significance.Notice also the minimum and maximumcolumns of the "DOP-P&S" lines, constructed byfinding for each of the paired runs the differencebetween the DOP and the Pereira and Schabesalgorithms.
Notice that the minimum is usuallynegative, and the maximum is usually positive,meaning that on some tests DOP did worse thanPereira and Schabes and on some it did better.
Itis important to run multiple tests, especially withsmall test sets like these, in order to avoid mis-leading results.T iming  Ana lys i sIn this section, we examine the empirical runtimeof our algorithm, and analyze Bod's.
We also notethat Bod's algorithm will probably be particularlyinefficient on longer sentences.It takes about 6 seconds per sentence to runour algorithm on an HP 9000/715, versus 3.5 hoursto run Bod's algorithm on a Sparc 2 (Bod, 1995b).Factoring in that the HP is roughly four timesfaster than the Sparc, the new algorithm is about500 times faster.
Of course, some of this differencemay be due to differences in implementation, sothis estimate is fairly rough.Furthermore, we believe Bod's analysis of hisparsing algorithm is flawed.
Letting G representgrammar size, and e represent maximum estima-tion error, Bod correctly analyzes his runtime asO(Gn3e-2).
However, Bod then neglects analy-sis of this e -~ term, assuming that it is constant.Thus he concludes that his algorithm runs in poly-nomial time.
However, for his algorithm to havesome reasonable chance of finding the most proba-ble parse, the number of times he must sample hisdata is at least inversely proportional to the con-ditional probability of that parse.
For instance,if the maximum probability parse had probability1/50, then he would need to sample at least 50times to be reasonably sure of finding that parse.Now, we note that the conditional probabil-ity of the most probable parse tree will in generaldecline xponentially with sentence l ngth.
We as-sume that the number of ambiguities in a sentencewill increase linearly with sentence l ngth; if a fiveword sentence has on average one ambiguity, thena ten word sentence will have two, etc.
A linearincrease in ambiguity will lead to an exponentialdecrease in probability of the most probable parse.Since the probability of the most probableparse decreases exponentially in sentence length,the number of random samples needed to findthis most probable parse increases exponentiallyin sentence length.
Thus, when using the MonteCarlo algorithm, one is left with the uncomfortablechoice of exponentially decreasing the probabilityof finding the most probable parse, or exponen-tially increasing the runtime.We admit that this is a somewhat informalargument.
Still, the Monte Carlo algorithm hasnever been tested on sentences longer than thosein the ATIS corpus; there is good reason to believethe algorithm will not work as well on longer sen-tences.
Note that our algorithm has true runtimeO(Tn3), as shown previously.Ana lys i s  o f  Bod 's  DataIn the DOP model, a sentence cannot be given anexactly correct parse unless all productions in thecorrect parse occur in the training set.
Thus, wecan get an upper bound on performance by ex-150amining the test corpus and finding which parsetrees could not be generated using only produc-tions in the training corpus.
Unfortunately, whileBod provided us with his data, he did not specifywhich sentences were test and which were training.We can however find an upper bound on averagecase performance, as well as an upper bound onthe probability that any particular level of perfor-mance could be achieved.Bod randomly split his corpus into test andtraining.
According to his thesis (Bod, 1995a,page 64), only one of his 75 test sentences hada correct parse which could not be generated fromthe training data.
This turns out to be very sur-prising.
An analysis of Bod's data shows that atleast some of the difference in performance be-tween his results and ours must be due to anextraordinarily fortuitous choice of test data.
Itwould be very interesting to see how our algorithmperformed on Bod's split into test and training,but he has not provided us with this split.
Bod didexamine versions of DOP that smoothed, allowingproductions which did not occur in the trainingset; however his reference to coverage is with re-spect to a version which does no smoothing.In order to perform our analysis, we must de-termine certain details of Bod's parser which af-fect the probability of having most sentences cor-rectly parsable.
When using a chart parser, asBod did, three problematic ases must be han-dled: e productions, unary productions, and n-ary(n > 2) productions.
The first two kinds of pro-ductions can be handled with a probabilistic hartparser, but large and difficult matrix manipula-tions are required (Stolcke, 1993); these manipu-lations would be especially difficult given the sizeof Bod's grammar.
Examining Bod's data, we findhe removed e productions.
We also assume thatBod made the same choice we did and eliminatedunary productions, given the difficulty of correctlyparsing them.
Bod himself does not know whichtechnique he used for n-ary productions, ince thechart parser he used was written by a third party(Bod, personal communication).The n-ary productions can be parsed in astraightforward manner, by converting them to bi-nary branching form; however, there are at leastthree different ways to convert them, as illus-trated in Table 3.
In method "Correct", the n-ary branching productions are converted in such away that no overgeneration is introduced.
A set ofspecial non-terminals i  added, one for each partialright hand side.
In method "Continued", a singleOriginal Correct Continued SimpleAB C D EAB *_CDEC *_DED EAB A_*C A_*D EAB AC AAD ETable 3: Transformations from N-ary to Binary Branching StructuresCorrect Continued Simpleno unary 0.78 0.0000002 0.88 0.0009484 0.90 0.0041096unary 0.80 0.0000011 0.90 0.0037355 0.92 0.0150226Table 4: Probabilities of Sentences with Unique Productions/Test Data with Ungeneratable S ntencesnew non-terminal is introduced for each originalnon-terminal.
Because these non-terminals occurin multiple contexts, some overgeneration is in-troduced.
However, this overgeneration is con-strained, so that elements that tend to occur onlyat the beginning, middle, or end of the right handside of a production cannot occur somewhere else.If the "Simple" method is used, then no new non-terminals are introduced; using this method, it isnot possible to recover the n-ary branching struc-ture from the resulting parse tree, and significantovergeneration ccurs.Table 4 shows the undergeneration probabili-ties for each of these possible techniques for han-dling unary productions and n-ary productions.
3The first number in each column is the probabil-ity that a sentence in the training data will havea production that occurs nowhere else.
The sec-ond number is the probability that a test set of 75sentences drawn from this database will have oneungeneratable s ntence: 75p~4(1 - p).4The table is arranged from least generous tomost generous: in the upper left hand corner isa technique Bod might reasonably have used; inthat case, the probability of getting the test sethe described is lessthan one in a million.
In theaA perl script for analyzing Bod's data is availableby anonymous FTP  fromftp://ftp.das.harvard,edu/pub/goodman/analyze.perl4Actually, this is a slight overestimate for a fewreasons, including the fact that the 75 sentences aredrawn without replacement.
Also, consider a sentencewith a production that occurs only in one other sen-tence in the corpus; there is some probability that bothsentences will end up fin the test data, causing both tobe ungeneratable.151lower right corner we give Bod the absolute max-imum benefit of the doubt: we assume he useda parser capable of parsing unary branching pro-ductions, that he used a very overgenerating gram-mar, and that he used a loose definition of "ExactMatch."
Even in this case, there is only about a1.5% chance of getting the test set Bod describes.ConclusionWe have given efficient echniques for parsing theDOP model.
These results are significant since theDOP model has perhaps the best reported parsingaccuracy; previously the full DOP model had notbeen replicated ue to the difficulty and computa-tional complexity of the existing algorithms.
Wehave also shown that previous results were par-tially due to an unlikely choice of test data, andpartially due to the heavy cleaning of the data,which reduced the difficulty of the task.Of course, this research raises as many ques-tions as it answers.
Were previous results due onlyto the choice of test data, or are the differences inimplementation partly responsible?
In that case,there is significant future work required to under-stand which differences account for Bod's excep-tional performance.
This will be complicated bythe fact that sufficient details of Bod's implemen-tation are not available.This research also shows the importance oftesting on more than one small test set, as wellas the importance of not making cross-corpus com-parisons; if a new corpus is required, then previousalgorithms hould be duplicated for comparison.References\[Baker, 1979\] J.K. Baker.
1979.
Trainable gram-mars for speech recognition.
In Proceedings ofthe Spring Conference of the Acoustical Societyof America, pages 547-550, Boston, MA, June.\[Bod, 1992\] Rens Bod.
1992.
Mathematical prop-erties of the data oriented parsing model.
Paperpresented at the Third Meeting on Mathematicsof Language (MOL3), Austin Texas.\[Bod, 1993a\] Rens Bod.
1993a.
Data-orientedparsing as a general framework for stochas-tic language processing.
In K. Sikkel andA.
Nijholt, editors, Parsing Natural Language.Twente, The Netherlands.\[Bod, 1993b\] Rens Bod.
1993b.
MonteCarlo parsing.
In Proceedings Third Inter-national Workshop on Parsing Technologies,Tilburg/Durbury.\[Bod, 1993c\] Rens Bod.
1993c.
Using an anno-tated corpus as a stochastic grammar.
In Pro-ceedings of the Sixth Conference of the EuropeanChapter of the ACL, pages 37-44.\[Bod, 1995a\] Rens Bod.
1995a.
Enriching Lin-guistics with Statistics: Performance Models ofNatural Language.
University of AmsterdamILLC Dissertation Series 1995-14.
AcademischePers, Amsterdam.\[Bod, 1995b\] Rens Bod.
1995b.
The problemof computing the most probable tree in data-oriented parsing and stochastic tree grammars.In Proceedings of the Seventh Conference of theEuropean Chapter of the ACL.\[Brill, 1993\] Eric Brill.
1993.
A Corpus-Based Ap-proach to Language Learning.
Ph.D. thesis, Uni-versity of Pennsylvania.\[Goodman, 1996\] Joshua Goodman.
1996.
Pars-ing algorithms and metrics.
In Proceedings ofthe 34th Annual Meeting of the ACL.
To ap-pear.\[Hemphill et al, 1990\] Charles T. Hemphill,John J. Godfrey, and George R. Doddington.1990.
The ATIS spoken language systems pilotcorpus.
In DARPA Speech and Natural Lan-guage Workshop, Hidden Valley, Pennsylvania,June.
Morgan Kaufmann.\[Lari and Young, 1990\] K. Lari and S.J.
Young.1990.
The estimation of stochastic context-free152grammars using the inside-outside algorithm.Computer Speech and Language, 4:35-56.\[Magerman, 1994\] David Magerman.
1994.
Nat-ural Language Parsing as Statistical PatternRecognition.
Ph.D. thesis, Stanford UniversityUniversity, February.\[Pereira and Schabes, 1992\] Fernando Pereiraand Yves Schabes.
1992.
Inside-Outside r es-timation from partially bracketed corpora.
InProceedings of the 30th Annual Meeting of theACL, pages 128-135, Newark, Delaware.\[Rabiner, 1989\] L.R.
Rabiner.
1989.
A tutorialon hidden Markov models and selected applica-tions in speech recognition.
Proceedings of theIEEE, 77(2), February.\[Scha, 1990\] R. Scha.
1990.
Language theory andlanguage technology; competence and perfor-mance.
In Q.A.M.
de Kort and G.L.J.
Leerdam,editors, Computertoepassingen in de Neerlan-distiek.
Landelijke Vereniging van Neerlandici(LVVN-jaarboek), Almere.
In Dutch.\[Schabes tal., 1993\] Yves Schabes, Michal Roth,and Randy Osborne.
1993.
Parsing the WallStreet Journal with the Inside-Outside algo-rithm.
In Proceedings of the Sixth Conference ofthe European Chapter of the ACL, pages 341-347.\[Schabes, 1992\] Y. Schabes.
1992.
Stochastic lexi-calized tree-adjoining grammars.
In Proceedingsof the l$th International Conference on Compu-tational Linguistics.\[Sima'an, 1996\] Khalil Sima'an.
1996.
Efficientdisambiguation bymeans of stochastic tree sub-stitution grammars.
In R. Mitkov and N. Ni-colov, editors, Recent Advances in NLP 1995,volume 136 of Current Issues in Linguistic The-ory.
John Benjamins, Amsterdam.\[Stolcke, 1993\] Andreas Stolcke.
1993.
An ef-ficient probabilistic ontext-free parsing algo-rithm that computes prefix probabilities.
Tech-nical Report TR-93-065, International Com-puter Science Institute, Berkeley, CA.
