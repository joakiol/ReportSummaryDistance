Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394?402,Beijing, August 2010Better Arabic Parsing: Baselines, Evaluations, and AnalysisSpence Green and Christopher D. ManningComputer Science Department, Stanford University{spenceg,manning}@stanford.eduAbstractIn this paper, we offer broad insightinto the underperformance of Arabic con-stituency parsing by analyzing the inter-play of linguistic phenomena, annotationchoices, and model design.
First, we iden-tify sources of syntactic ambiguity under-studied in the existing parsing literature.Second, we show that although the PennArabic Treebank is similar to other tree-banks in gross statistical terms, annotationconsistency remains problematic.
Third,we develop a human interpretable gram-mar that is competitive with a latent vari-able PCFG.
Fourth, we show how to buildbetter models for three different parsers.Finally, we show that in application set-tings, the absence of gold segmentationlowers parsing performance by 2?5% F1.1 IntroductionIt is well-known that constituency parsing mod-els designed for English often do not generalizeeasily to other languages and treebanks.1 Expla-nations for this phenomenon have included therelative informativeness of lexicalization (Dubeyand Keller, 2003; Arun and Keller, 2005), insensi-tivity to morphology (Cowan and Collins, 2005;Tsarfaty and Sima?an, 2008), and the effect ofvariable word order (Collins et al, 1999).
Cer-tainly these linguistic factors increase the diffi-culty of syntactic disambiguation.
Less frequentlystudied is the interplay among language, annota-tion choices, and parsing model design (Levy andManning, 2003; Ku?bler, 2005).1The apparent difficulty of adapting constituency mod-els to non-configurational languages has been one motivationfor dependency representations (Hajic?
and Zema?nek, 2004;Habash and Roth, 2009).To investigate the influence of these factors,we analyze Modern Standard Arabic (henceforthMSA, or simply ?Arabic?)
because of the unusualopportunity it presents for comparison to Englishparsing results.
The Penn Arabic Treebank (ATB)syntactic guidelines (Maamouri et al, 2004) werepurposefully borrowed without major modifica-tion from English (Marcus et al, 1993).
Further,Maamouri and Bies (2004) argued that the Englishguidelines generalize well to other languages.
ButArabic contains a variety of linguistic phenom-ena unseen in English.
Crucially, the conventionalorthographic form of MSA text is unvocalized, aproperty that results in a deficient graphical rep-resentation.
For humans, this characteristic canimpede the acquisition of literacy.
How do addi-tional ambiguities caused by devocalization affectstatistical learning?
How should the absence ofvowels and syntactic markers influence annotationchoices and grammar development?
Motivated bythese questions, we significantly raise baselinesfor three existing parsing models through bettergrammar engineering.Our analysis begins with a description of syn-tactic ambiguity in unvocalized MSA text (?2).Next we show that the ATB is similar to other tree-banks in gross statistical terms, but that annotationconsistency remains low relative to English (?3).We then use linguistic and annotation insights todevelop a manually annotated grammar for Arabic(?4).
To facilitate comparison with previous work,we exhaustively evaluate this grammar and twoother parsing models when gold segmentation isassumed (?5).
Finally, we provide a realistic eval-uation in which segmentation is performed bothin a pipeline and jointly with parsing (?6).
Wequantify error categories in both evaluation set-tings.
To our knowledge, ours is the first analysisof this kind for Arabic parsing.3942 Syntactic Ambiguity in ArabicArabic is a morphologically rich language with aroot-and-pattern system similar to other Semiticlanguages.
The basic word order is VSO, butSVO, VOS, and VO configurations are also pos-sible.2 Nouns and verbs are created by selectinga consonantal root (usually triliteral or quadrilit-eral), which bears the semantic core, and addingaffixes and diacritics.
Particles are uninflected.Diacritics can also be used to specify grammaticalrelations such as case and gender.
But diacriticsare not present in unvocalized text, which is thestandard form of, e.g., news media documents.3Let us consider an example of ambiguity causedby devocalization.
Table 1 shows four wordswhose unvocalized surface forms ? an are indis-tinguishable.
Whereas Arabic linguistic theory as-signs (1) and (2) to the class of pseudo verbs ?Ahw?
inna and her sisters since they can beinflected, the ATB conventions treat (2) as a com-plementizer, which means that it must be the headof SBAR.
Because these two words have identicalcomplements, syntax rules are typically unhelp-ful for distinguishing between them.
This is es-pecially true in the case of quotations?which arecommon in the ATB?where (1) will follow a verblike (2) (Figure 1).Even with vocalization, there are linguistic cat-egories that are difficult to identify without se-mantic clues.
Two common cases are the attribu-tive adjective and the process nominal CdOm?maSdar, which can have a verbal reading.4 At-tributive adjectives are hard because they are or-thographically identical to nominals; they are in-flected for gender, number, case, and definiteness.Moreover, they are used as substantives much2Unlike machine translation, constituency parsing is notsignificantly affected by variable word order.
However, whengrammatical relations like subject and object are evaluated,parsing performance drops considerably (Green et al, 2009).In particular, the decision to represent arguments in verb-initial clauses as VP internal makes VSO and VOS configu-rations difficult to distinguish.
Topicalization of NP subjectsin SVO configurations causes confusion with VO (pro-drop).3Techniques for automatic vocalization have been studied(Zitouni et al, 2006; Habash and Rambow, 2007).
However,the data sparsity induced by vocalization makes it difficult totrain statistical models on corpora of the size of the ATB, sovocalizing and then parsing may well not help performance.4Traditional Arabic linguistic theory treats both of thesetypes as subcategories of noun ?F?.Word Head Of Complement POS1 ?
? inna ?Indeed, truly?
VP Noun VBP2 ?
?anna ?That?
SBAR Noun IN3 ? in ?If?
SBAR Verb IN4 ?an ?to?
SBAR Verb INTable 1: Diacritized particles and pseudo-verbs that, afterorthographic normalization, have the equivalent surface form? an.
The distinctions in the ATB are linguistically justified,but complicate parsing.
Table 8a shows that the best modelrecovers SBAR at only 71.0% F1.VPVBD?ARshe addedSVPPUNC?VBP?IndeedNPNN?d}Saddam.
.
.
(a) ReferenceVPVBD?ARshe addedSBARPUNC?IN?IndeedNPNN?d}Saddam.
.
.
(b) StanfordFigure 1: The Stanford parser (Klein and Manning, 2002)is unable to recover the verbal reading of the unvocalizedsurface form ? an (Table 1).more frequently than is done in English.Process nominals name the action of the tran-sitive or ditransitive verb from which they derive.The verbal reading arises when the maSdar has anNP argument which, in vocalized text, is markedin the accusative case.
When the maSdar lacksa determiner, the constituent as a whole resem-bles the ubiquitous annexation construct T?AR?iDafa.
Gabbard and Kulick (2008) show thatthere is significant attachment ambiguity associ-ated with iDafa, which occurs in 84.3% of thetrees in our development set.
Figure 4 showsa constituent headed by a process nominal withan embedded adjective phrase.
All three mod-els evaluated in this paper incorrectly analyze theconstituent as iDafa; none of the models attach theattributive adjectives properly.For parsing, the most challenging form of am-biguity occurs at the discourse level.
A definingcharacteristic of MSA is the prevalence of dis-course markers to connect and subordinate wordsand phrases (Ryding, 2005).
Instead of offsettingnew topics with punctuation, writers of MSA in-sert connectives such as ?
wa and ?
fa to linknew elements to both preceding clauses and thetext as a whole.
As a result, Arabic sentences areusually long relative to English, especially after395Length English (WSJ) Arabic (ATB)?
20 41.9% 33.7%?
40 92.4% 73.2%?
63 99.7% 92.6%?
70 99.9% 94.9%Table 2: Frequency distribution for sentence lengths in theWSJ (sections 2?23) and the ATB (p1?3).
English parsingevaluations usually report results on sentences up to length40.
Arabic sentences of up to length 63 would need to beevaluated to account for the same fraction of the data.
Wepropose a limit of 70 words for Arabic parsing evaluations.Part of Speech Tag Freq.?
wa?and?conjunction CC 4256preposition IN 6abbreviation NN 6?
fa?so, then?conjunction CC 160connective particle RP 67abbreviation NN 22response conditioning particle RP 11subordinating conjunction IN 3Table 3: Dev set frequencies for the two most significant dis-course markers in Arabic are skewed toward analysis as aconjunction.segmentation (Table 2).
The ATB gives severaldifferent analyses to these words to indicate dif-ferent types of coordination.
But it conflates thecoordinating and discourse separator functions ofwa (?W`? ??)
into one analysis: conjunction(Table 3).
A better approach would be to distin-guish between these cases, possibly by drawingon the vast linguistic work on Arabic connectives(Al-Batal, 1990).
We show that noun-noun vs.discourse-level coordination ambiguity in Arabicis a significant source of parsing errors (Table 8c).3 Treebank Comparison3.1 Gross StatisticsLinguistic intuitions like those in the previous sec-tion inform language-specific annotation choices.The resulting structural differences between tree-banks can account for relative differences in pars-ing performance.
We compared the ATB5 to tree-banks for Chinese (CTB6), German (Negra), andEnglish (WSJ) (Table 4).
The ATB is disadvan-taged by having fewer trees with longer average5LDC A-E catalog numbers: LDC2008E61 (ATBp1v4),LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).We map the ATB morphological analyses to the shortened?Bies?
tags for all experiments.ATB CTB6 Negra WSJTrees 23449 28278 20602 43948Word Typess 40972 45245 51272 46348Tokens 738654 782541 355096 1046829Tags 32 34 499 45Phrasal Cats 22 26 325 27Test OOV 16.8% 22.2% 30.5% 13.2%Per SentenceDepth (?
/ ?2) 3.87 / 0.74 5.01 / 1.44 3.58 / 0.89 4.18 / 0.74Breadth (?
/ ?2) 14.6 / 7.31 10.2 / 4.44 7.50 / 4.56 12.1 / 4.65Length (?
/ ?2) 31.5 / 22.0 27.7 / 18.9 17.2 / 10.9 23.8 / 11.2Constituents (?)
32.8 32.5 8.29 19.6?
Const.
/ ?
Length 1.04 1.18 0.482 0.820Table 4: Gross statistics for several different treebanks.
Testset OOV rate is computed using the following splits: ATB(Chiang et al, 2006); CTB6 (Huang and Harper, 2009); Ne-gra (Dubey and Keller, 2003); English, sections 2-21 (train)and section 23 (test).yields.6 But to its great advantage, it has a highratio of non-terminals/terminals (?
Constituents /?
Length).
Evalb, the standard parsing metric, isbiased toward such corpora (Sampson and Babar-czy, 2003).
Also surprising is the low test set OOVrate given the possibility of morphological varia-tion in Arabic.
In general, several gross corpusstatistics favor the ATB, so other factors must con-tribute to parsing underperformance.3.2 Inter-annotator AgreementAnnotation consistency is important in any super-vised learning task.
In the initial release of theATB, inter-annotator agreement was inferior toother LDC treebanks (Maamouri et al, 2008).
Toimprove agreement during the revision process,a dual-blind evaluation was performed in which10% of the data was annotated by independentteams.
Maamouri et al (2008) reported agree-ment between the teams (measured with Evalb) at93.8% F1, the level of the CTB.
But Rehbein andvan Genabith (2007) showed that Evalb shouldnot be used as an indication of real difference?or similarity?between treebanks.Instead, we extend the variation n-grammethod of Dickinson (2005) to compare annota-tion error rates in the WSJ and ATB.
For a corpusC, let M be the set of tuples ?n, l?, where n is ann-gram with bracketing label l. If any n appears6Generative parsing performance is known to deterioratewith sentence length.
As a result, Habash et al (2006) devel-oped a technique for splitting and chunking long sentences.In application settings, this may be a profitable strategy.396Corpus Sample Error %Trees Nuclei n-grams Type n-gramWSJ 2?23 43948 25041 746 12.0% 2.10%ATB 23449 20292 2100 37.0% 1.76%Table 5: Evaluation of 100 randomly sampled variation nu-clei types.
The samples from each corpus were indepen-dently evaluated.
The ATB has a much higher fraction ofnuclei per tree, and a higher type-level error rate.in a corpus position without a bracketing label,then we also add ?n,NIL?
to M. We call the setof unique n-grams with multiple labels in M thevariation nuclei of C.Bracketing variation can result from either an-notation errors or linguistic ambiguity.
Humanevaluation is one way to distinguish between thetwo cases.
Following Dickinson (2005), we ran-domly sampled 100 variation nuclei from eachcorpus and evaluated each sample for the presenceof an annotation error.
The human evaluators werea non-native, fluent Arabic speaker (the first au-thor) for the ATB and a native English speaker forthe WSJ.7Table 5 shows type- and token-level error ratesfor each corpus.
The 95% confidence intervals fortype-level errors are (5580, 9440) for the ATB and(1400, 4610) for the WSJ.
The results clearly in-dicate increased variation in the ATB relative tothe WSJ, but care should be taken in assessing themagnitude of the difference.
On the one hand,the type-level error rate is not calibrated for thenumber of n-grams in the sample.
At the sametime, the n-gram error rate is sensitive to sampleswith extreme n-gram counts.
For example, one ofthe ATB samples was the determiner???
dhalik?that.?
The sample occurred in 1507 corpus po-sitions, and we found that the annotations wereconsistent.
If we remove this sample from theevaluation, then the ATB type-level error rises toonly 37.4% while the n-gram error rate increasesto 6.24%.
The number of ATB n-grams also fallsbelow the WSJ sample size as the largest WSJsample appeared in only 162 corpus positions.7Unlike Dickinson (2005), we strip traces and only con-sider POS tags when pre-terminals are the only interveningnodes between the nucleus and its bracketing (e.g., unaries,base NPs).
Since our objective is to compare distributions ofbracketing discrepancies, we do not use heuristics to prunethe set of nuclei.NPNNTm?summitNPNNP?rJSharmDTNNP%yK?Al-Sheikh(a)NPNNTm?summitNPNNP?rJSharmNPDTNNP%yK?Al-Sheikh(b)Figure 2: An ATB sample from the human evaluation.
TheATB annotation guidelines specify that proper nouns shouldbe specified with a flat NP (a).
But the city name Sharm Al-Sheikh is also iDafa, hence the possibility for the incorrectannotation in (b).4 Grammar DevelopmentWe can use the preceding linguistic and annota-tion insights to build a manually annotated Ara-bic grammar in the manner of Klein and Manning(2003).
Manual annotation results in human in-terpretable grammars that can inform future tree-bank annotation decisions.
A simple lexicalizedPCFG with second order Markovization gives rel-atively poor performance: 75.95% F1 on the testset.8 But this figure is surprisingly competitivewith a recent state-of-the-art baseline (Table 7).In our grammar, features are realized as annota-tions to basic category labels.
We start with nounfeatures since written Arabic contains a very highproportion of NPs.
genitiveMark indicates recur-sive NPs with a indefinite nominal left daughterand an NP right daughter.
This is the form of re-cursive levels in iDafa constructs.
We also add anannotation for one-level iDafa (oneLevelIdafa)constructs since they make up more than 75% ofthe iDafa NPs in the ATB (Gabbard and Kulick,2008).
For all other recursive NPs, we add acommon annotation to the POS tag of the head(recursiveNPHead).Base NPs are the other significant category ofnominal phrases.
markBaseNP indicates thesenon-recursive nominal phrases.
This feature in-cludes named entities, which the ATB marks witha flat NP node dominating an arbitrary number ofNNP pre-terminal daughters (Figure 2).For verbs we add two features.
First we markany node that dominates (at any level) a verb8We use head-finding rules specified by a native speakerof Arabic.
This PCFG is incorporated into the StanfordParser, a factored model that chooses a 1-best parse from theproduct of constituency and dependency parses.397Feature States Tags F1 Indiv.
?F1?
3208 33 76.86 ?recursiveNPHead 3287 38 77.46 +0.60genitiveMark 3471 38 77.88 +0.42splitPUNC 4221 47 77.98 +0.10markContainsVerb 5766 47 79.16 +1.18markBaseNP 6586 47 79.5 +0.34markOneLevelIdafa 7202 47 79.83 +0.33splitIN 7595 94 80.48 +0.65containsSVO 9188 94 80.66 +0.18splitCC 9492 124 80.87 +0.21markFem 10049 141 80.95 +0.08Table 6: Incremental dev set results for the manually anno-tated grammar (sentences of length ?
70).phrase (markContainsVerb).
This feature has alinguistic justification.
Historically, Arabic gram-mar has identified two sentences types: those thatbegin with a nominal (TymF? Tlm)?), and thosethat begin with a verb (Tyl`f? Tlm)?).
But for-eign learners are often surprised by the verblesspredications that are frequently used in Arabic.Although these are technically nominal, they havebecome known as ?equational?
sentences.
mark-ContainsVerb is especially effective for distin-guishing root S nodes of equational sentences.
Wealso mark all nodes that dominate an SVO con-figuration (containsSVO).
In MSA, SVO usuallyappears in non-matrix clauses.Lexicalizing several POS tags improves perfor-mance.
splitIN captures the verb/preposition id-ioms that are widespread in Arabic.
Althoughthis feature helps, we encounter one consequenceof variable word order.
Unlike the WSJ corpuswhich has a high frequency of rules like VP ?VB PP, Arabic verb phrases usually have lexi-calized intervening nodes (e.g., NP subjects anddirect objects).
For example, we might haveVP?VB NP PP, where the NP is the subject.This annotation choice weakens splitIN.The ATB gives all punctuation a single tag.
Forparsing, this is a mistake, especially in the caseof interrogatives.
splitPUNC restores the conven-tion of the WSJ.
We also mark all tags that dom-inate a word with the feminine ending ?
taa mar-buuTa (markFeminine).To differentiate between the coordinating anddiscourse separator functions of conjunctions (Ta-ble 3), we mark each CC with the label of itsright sister (splitCC).
The intuition here is thatthe role of a discourse marker can usually be de-termined by the category of the word that followsit.
Because conjunctions are elevated in the parsetrees when they separate recursive constituents,we choose the right sister instead of the categoryof the next word.
We create equivalence classesfor verb, noun, and adjective POS categories.5 Standard Parsing ExperimentsWe compare the manually annotated grammar,which we incorporate into the Stanford parser, toboth the Berkeley (Petrov et al, 2006) and Bikel(Bikel, 2004) parsers.
All experiments use ATBparts 1?3 divided according to the canonical splitsuggested by Chiang et al (2006).
Preprocessingthe raw trees improves parsing performance con-siderably.9 We first discard all trees dominated byX, which indicates errors and non-linguistic text.At the phrasal level, we remove all function tagsand traces.
We also collapse unary chains withidentical basic categories like NP ?
NP.
The pre-terminal morphological analyses are mapped tothe shortened ?Bies?
tags provided with the tree-bank.
Finally, we add ?DT?
to the tags for definitenouns and adjectives (Kulick et al, 2006).The orthographic normalization strategy we useis simple.10 In addition to removing all diacrit-ics, we strip instances of taTweel ?
?wW, col-lapse variants of alif  to bare alif,11 and map Ara-bic punctuation characters to their Latin equiva-lents.
We retain segmentation markers?whichare consistent only in the vocalized section of thetreebank?to differentiate between e.g.
??
?they?and ?
?+ ?their.?
Because we use the vocalizedsection, we must remove null pronoun markers.In Table 7 we give results for several evalua-tion metrics.
Evalb is a Java re-implementationof the standard labeled precision/recall metric.129Both the corpus split and pre-processing code are avail-able at http://nlp.stanford.edu/projects/arabic.shtml.10Other orthographic normalization schemes have beensuggested for Arabic (Habash and Sadat, 2006), but we ob-serve negligible parsing performance differences betweenthese and the simple scheme used in this evaluation.11taTweel (?)
is an elongation character used in Arabicscript to justify text.
It has no syntactic function.
Variantsof alif are inconsistently used in Arabic texts.
For alif withhamza, normalization can be seen as another level of devo-calization.12For English, our Evalb implementation is identical to themost recent reference (EVALB20080701).
For Arabic we398Leaf Ancestor Evalb TagModel System Length Corpus Sent Exact LP LR F1 %Stanford (v1.6.3)Baseline 70 0.791 0.825 358 80.37 79.36 79.86 95.58all 0.773 0.818 358 78.92 77.72 78.32 95.49GoldPOS 70 0.802 0.836 452 81.07 80.27 80.67 99.95Bikel (v1.2)Baseline (Self-tag) 70 0.770 0.801 278 77.92 76.00 76.95 94.64all 0.752 0.794 278 76.96 75.01 75.97 94.63Baseline (Pre-tag) 70 0.771 0.804 295 78.35 76.72 77.52 95.68all 0.752 0.796 295 77.31 75.64 76.47 95.68GoldPOS 70 0.775 0.808 309 78.83 77.18 77.99 96.60Berkeley (Sep. 09)(Petrov, 2009) all ?
?
?
76.40 75.30 75.85 ?Baseline 70 0.809 0.839 335 82.32 81.63 81.97 95.07all 0.796 0.834 336 81.43 80.73 81.08 95.02GoldPOS 70 0.831 0.859 496 84.37 84.21 84.29 99.87Table 7: Test set results.
Maamouri et al (2009b) evaluated the Bikel parser using the same ATB split, but only reported devset results with gold POS tags for sentences of length ?
40.
The Bikel GoldPOS configuration only supplies the gold POStags; it does not force the parser to use them.
We are unaware of prior results for the Stanford parser.7580855000 10000 15000BerkeleyStanfordBikeltraining treesF1Figure 3: Dev set learning curves for sentence lengths ?
70.All three curves remain steep at the maximum training setsize of 18818 trees.The Leaf Ancestor metric measures the cost oftransforming guess trees to the reference (Samp-son and Babarczy, 2003).
It was developed in re-sponse to the non-terminal/terminal bias of Evalb,but Clegg and Shepherd (2005) showed that it isalso a valuable diagnostic tool for trees with com-plex deep structures such as those found in theATB.
For each terminal, the Leaf Ancestor metricextracts the shortest path to the root.
It then com-putes a normalized Levenshtein edit distance be-tween the extracted chain and the reference.
Therange of the score is between 0 and 1 (higher isbetter).
We report micro-averaged (whole corpus)and macro-averaged (per sentence) scores alongadd a constraint on the removal of punctuation, which has asingle tag (PUNC) in the ATB.
Tokens tagged as PUNC arenot discarded unless they consist entirely of punctuation.with the number of exactly matching guess trees.5.1 Parsing ModelsThe Stanford parser includes both the manuallyannotated grammar (?4) and an Arabic unknownword model with the following lexical features:1.
Presence of the determiner ? Al2.
Contains digits3.
Ends with the feminine affix ?
p4.
Various verbal (e.g., ?, 1) and adjectivalsuffixes (e.g., T?
)Other notable parameters are second order verticalMarkovization and marking of unary rules.Modifying the Berkeley parser for Arabic isstraightforward.
After adding a ROOT node toall trees, we train a grammar using six split-and-merge cycles and no Markovization.
We use thedefault inference parameters.Because the Bikel parser has been parameter-ized for Arabic by the LDC, we do not change thedefault model settings.
However, when we pre-tag the input?as is recommended for English?we notice a 0.57% F1 improvement.
We use thelog-linear tagger of Toutanova et al (2003), whichgives 96.8% accuracy on the test set.5.2 DiscussionThe Berkeley parser gives state-of-the-art perfor-mance for all metrics.
Our baseline for all sen-tence lengths is 5.23% F1 higher than the best pre-vious result.
The difference is due to more careful399S-NOMVPVBG?2A`tFrestoringNPNPNNC?2roleNPPRP?itsADJPDTJJ?Anb?constructiveDTJJ?
?Af?effective(a) ReferenceNPNN?2A`tFNPNPNNC?2NPPRP?ADJPDTJJ?Anb?ADJPDTJJ?
?Af?(b) StanfordNPNPNN?2A`tFNPNPNNC?2NPPRP?ADJPDTJJ?Anb?ADJPDTJJ?
?Af?(c) BerkeleyNPNN?2A`tFNPNPNPNNC?2NPPRP?ADJPDTJJ?Anb?ADJPDTJJ?
?Af?(d) BikelFigure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmen-tation).
The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals.
Like verbs,maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., takingdeterminers and heading iDafa (Fassi Fehri, 1993).
In the ATB, ?2A`tF asta?adah is tagged 48 times as a noun and 9 timesas verbal noun.
Consequently, all three parsers prefer the nominal reading.
Table 8b shows that verbal nouns are the hardestpre-terminal categories to identify.
None of the models attach the attributive adjectives correctly.pre-processing.
However, the learning curves inFigure 3 show that the Berkeley parser does notexceed our manual grammar by as wide a mar-gin as has been shown for other languages (Petrov,2009).
Moreover, the Stanford parser achieves themost exact Leaf Ancestor matches and tagging ac-curacy that is only 0.1% below the Bikel model,which uses pre-tagged input.In Figure 4 we show an example of variationbetween the parsing models.
We include a listof per-category results for selected phrasal labels,POS tags, and dependencies in Table 8.
The er-rors shown are from the Berkeley parser output,but they are representative of the other two pars-ing models.6 Joint Segmentation and ParsingAlthough the segmentation requirements for Ara-bic are not as extreme as those for Chinese, Ara-bic is written with certain cliticized prepositions,pronouns, and connectives connected to adjacentwords.
Since these are distinct syntactic units,they are typically segmented.
The ATB segmen-tation scheme is one of many alternatives.
Untilnow, all evaluations of Arabic parsing?includingthe experiments in the previous section?have as-sumed gold segmentation.
But gold segmentationis not available in application settings, so a seg-menter and parser are arranged in a pipeline.
Seg-mentation errors cascade into the parsing phase,placing an artificial limit on parsing performance.Lattice parsing (Chappelier et al, 1999) is analternative to a pipeline that prevents cascadingerrors by placing all segmentation options intothe parse chart.
Recently, lattices have been usedsuccessfully in the parsing of Hebrew (Tsarfaty,2006; Cohen and Smith, 2007), a Semitic lan-guage with similar properties to Arabic.
We ex-tend the Stanford parser to accept pre-generatedlattices, where each word is represented as a finitestate automaton.
To combat the proliferation ofparsing edges, we prune the lattices according toa hand-constructed lexicon of 31 clitics listed inthe ATB annotation guidelines (Maamouri et al,2009a).
Formally, for a lexicon L and segmentsI ?
L, O /?
L, each word automaton accepts thelanguage I?
(O+ I)I?.
Aside from adding a simplerule to correct alif deletion caused by the prepo-sition ?, no other language-specific processing isperformed.Our evaluation includes both weighted and un-weighted lattices.
We weight edges using aunigram language model estimated with Good-Turing smoothing.
Despite their simplicity, uni-gram weights have been shown as an effective fea-ture in segmentation models (Dyer, 2009).13 Thejoint parser/segmenter is compared to a pipelinethat uses MADA (v3.0), a state-of-the-art Arabicsegmenter, configured to replicate ATB segmen-tation (Habash and Rambow, 2005).
MADA usesan ensemble of SVMs to first re-rank the output ofa deterministic morphological analyzer.
For each13Of course, this weighting makes the PCFG an improperdistribution.
However, in practice, unknown word modelsalso make the distribution improper.400Label # gold F1ADJP 1216 59.45SBAR 2918 69.81FRAG 254 72.87VP 5507 78.83S 6579 78.91PP 7516 80.93NP 34025 84.95ADVP 1093 90.64WHNP 787 96.00(a) Major phrasalcategoriesTag # gold % Tag # gold %VBG 182 48.84 JJR 134 92.83VN 163 60.37 DTNNS 1069 94.29VBN 352 72.42 DTJJ 3361 95.07DTNNP 932 83.48 NNP 4152 95.09JJ 1516 86.09 NN 10336 95.23ADJ NUM 277 88.93 DTNN 6736 95.78VBP 2139 89.94 NOUN QUANT 352 98.16RP 818 91.23 PRP 1366 98.24NNS 907 91.75 CC 4076 98.92DTJJR 78 92.41 IN 8676 99.07VBD 2580 92.42 DT 525 99.81(b) Major POS categoriesParent Head Modifer Dir # gold F1NP NP TAG R 946 0.54S S S R 708 0.57NP NP ADJP R 803 0.64NP NP NP R 2907 0.66NP NP SBAR R 1035 0.67NP NP PP R 2713 0.67VP TAG PP R 3230 0.80NP NP TAG L 805 0.85VP TAG SBAR R 772 0.86S VP NP L 961 0.87(c) Ten lowest scoring (Collins,2003)-style dependencies occur-ring more than 700 timesTable 8: Per category performance of the Berkeley parser on sentence lengths ?
70 (dev set, gold segmentation).
(a) Ofthe high frequency phrasal categories, ADJP and SBAR are the hardest to parse.
We showed in ?2 that lexical ambiguityexplains the underperformance of these categories.
(b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN)and adjectives (e.g., JJ).
Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007),but we find that linguistically rich tag sets do not help parsing.
(c) Coordination ambiguity is shown in dependency scores bye.g., ?S S S R?
and ?NP NP NP R?.
?NP NP PP R?
and ?NP NP ADJP R?
are both iDafa attachment.input token, the segmentation is then performeddeterministically given the 1-best analysis.Since guess and gold trees may now have dif-ferent yields, the question of evaluation is com-plex.
Cohen and Smith (2007) chose a metric likeSParseval (Roark et al, 2006) that first aligns thetrees and then penalizes segmentation errors withan edit-distance metric.
But we follow the moredirect adaptation of Evalb suggested by Tsarfaty(2006), who viewed exact segmentation as the ul-timate goal.
Therefore, we only score guess/goldpairs with identical character yields, a conditionthat allows us to measure parsing, tagging, andsegmentation accuracy by ignoring whitespace.Table 9 shows that MADA produces a highquality segmentation, and that the effect of cas-cading segmentation errors on parsing is only1.92% F1.
However, MADA is language-specificand relies on manually constructed dictionaries.Conversely, the lattice parser requires no linguis-tic resources and produces segmentations of com-parable quality.
Nonetheless, parse quality ismuch lower in the joint model because a latticeis effectively a long sentence.
A cell in the bottomrow of the parse chart is required for each poten-tial whitespace boundary.
As we have said, parsequality decreases with sentence length.
Finally,we note that simple weighting gives nearly a 2%F1 improvement, whereas Goldberg and Tsarfaty(2008) found that unweighted lattices were moreeffective for Hebrew.LP LR F1 Seg F1 Tag F1 CoverageSTANFORD (Gold) 81.64 80.55 81.09 100.0 95.81 100.0%MADA ?
?
?
97.67 ?
96.42%MADA+STANFORD 79.44 78.90 79.17 97.67 94.27 96.42%STANFORDJOINT 76.13 72.61 74.33 94.12 90.13 94.73%STANFORDJOINT+UNI 77.09 74.97 76.01 96.26 92.23 95.87%Table 9: Dev set results for sentences of length ?
70.
Cov-erage indicates the fraction of hypotheses in which the char-acter yield exactly matched the reference.
Each model wasable to produce hypotheses for all input sentences.
In theseexperiments, the input lacks segmentation markers, hence theslightly different dev set baseline than in Table 6.7 ConclusionBy establishing significantly higher parsing base-lines, we have shown that Arabic parsing perfor-mance is not as poor as previously thought, butremains much lower than English.
We have de-scribed grammar state splits that significantly im-prove parsing performance, catalogued parsing er-rors, and quantified the effect of segmentation er-rors.
With a human evaluation we also showedthat ATB inter-annotator agreement remains lowrelative to the WSJ corpus.
Our results suggestthat current parsing models would benefit frombetter annotation consistency and enriched anno-tation in certain syntactic configurations.Acknowledgments We thank Steven Bethard, Evan Rosen,and Karen Shiells for material contributions to this work.
Weare also grateful to Markus Dickinson, Ali Farghaly, NizarHabash, Seth Kulick, David McCloskey, Claude Reichard,Ryan Roth, and Reut Tsarfaty for constructive discussions.The first author is supported by a National Defense Scienceand Engineering Graduate (NDSEG) fellowship.
This paperis based on work supported in part by DARPA through IBM.The content does not necessarily reflect the views of the U.S.Government, and no official endorsement should be inferred.401ReferencesAl-Batal, M. 1990.
Connectives as cohesive elements in amodern expository Arabic text.
In Eid, Mushira and JohnMcCarthy, editors, Perspectives on Arabic Linguistics II.John Benjamins.Arun, A and F Keller.
2005.
Lexicalization in crosslinguisticprobabilistic parsing: The case of French.
In ACL.Bikel, D M. 2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30:479?511.Chappelier, J-C, M Rajman, R Arages, and A Rozenknop.1999.
Lattice parsing for speech recognition.
In TALN.Chiang, D, M Diab, N Habash, O Rambow, and S Shareef.2006.
Parsing Arabic dialects.
In EACL.Clegg, A and A Shepherd.
2005.
Evaluating and integratingtreebank parsers on a biomedical corpus.
In ACL Work-shop on Software.Cohen, S and N A Smith.
2007.
Joint morphological andsyntactic disambiguation.
In EMNLP.Collins, M, J Hajic, L Ramshaw, and C Tillmann.
1999.
Astatistical parser for Czech.
In ACL.Collins, M. 2003.
Head-Driven statistical models for naturallanguage parsing.
Computational Linguistics, 29(4):589?637.Cowan, B and M Collins.
2005.
Morphology and rerankingfor the statistical parsing of Spanish.
In NAACL.Diab, M. 2007.
Towards an optimal POS tag set for ModernStandard Arabic processing.
In RANLP.Dickinson, M. 2005.
Error Detection and Correction in An-notated Corpora.
Ph.D. thesis, The Ohio State University.Dubey, A and F Keller.
2003.
Probabilistic parsing for Ger-man using sister-head dependencies.
In ACL.Dyer, C. 2009.
Using a maximum entropy model to buildsegmentation lattices for MT.
In NAACL.Fassi Fehri, A.
1993.
Issues in the structure of Arabic clausesand words.
Kluwer Academic Publishers.Gabbard, R and S Kulick.
2008.
Construct state modificationin the Arabic treebank.
In ACL.Goldberg, Y and R Tsarfaty.
2008.
A single generative modelfor joint morphological segmentation and syntactic pars-ing.
In ACL.Green, S, C Sathi, and C D Manning.
2009.
NP subjectdetection in verb-initial Arabic clauses.
In Proc.
of theThird Workshop on Computational Approaches to ArabicScript-based Languages (CAASL3).Habash, N and O Rambow.
2005.
Arabic tokenization, part-of-speech tagging and morphological disambiguation inone fell swoop.
In ACL.Habash, N and O Rambow.
2007.
Arabic diacritizationthrough full morphological tagging.
In NAACL.Habash, N and R Roth.
2009.
CATiB: The Columbia ArabicTreebank.
In ACL, Short Papers.Habash, N and F Sadat.
2006.
Arabic preprocessing schemesfor statistical machine translation.
In NAACL.Habash, N, B Dorr, and C Monz.
2006.
Challenges in build-ing an Arabic-English GHMT system with SMT compo-nents.
In EAMT.Hajic?, J and P Zema?nek.
2004.
Prague Arabic dependencytreebank: Development in data and tools.
In NEMLAR.Huang, Z and M Harper.
2009.
Self-training PCFGgrammars with latent annotations across languages.
InEMNLP.Klein, D and C D Manning.
2002.
Fast exact inference witha factored model for natural language parsing.
In NIPS.Klein, D and C D Manning.
2003.
Accurate unlexicalizedparsing.
In ACL.Kulick, S, R Gabbard, and M Marcus.
2006.
Parsing theArabic Treebank: Analysis and improvements.
In TLT.Ku?bler, S. 2005.
How do treebank annotation schemes influ-ence parsing results?
Or how not to compare apples andoranges.
In RANLP.Levy, R and C D Manning.
2003.
Is it harder to parse Chi-nese, or the Chinese treebank?
In ACL.Maamouri, M and A Bies.
2004.
Developing an ArabicTreebank: Methods, guidelines, procedures, and tools.
InProc.
of the Workshop on Computational Approaches toArabic Script-based Languages (CAASL1).Maamouri, M, A Bies, T Buckwalter, and W Mekki.
2004.The Penn Arabic Treebank: Building a large-scale anno-tated Arabic corpus.
In NEMLAR.Maamouri, M, A Bies, and S Kulick.
2008.
Enhancing theArabic Treebank: A collaborative effort toward new an-notation guidelines.
In LREC.Maamouri, M, A Bies, S Krouna, F Gaddeche, andB Bouziri.
2009a.
Penn Arabic Treebank guidelinesv4.92.
Technical report, Linguistic Data Consortium, Uni-versity of Pennsylvania, August 5.Maamouri, M, A Bies, and S Kulick.
2009b.
Creating amethodology for large-scale correction of treebank anno-tation: The case of the Arabic Treebank.
In MEDAR.Marcus, M, M A Marcinkiewicz, and B Santorini.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19:313?330.Petrov, S, L Barrett, R Thibaux, and D Klein.
2006.
Learningaccurate, compact, and interpretable tree annotation.
InACL.Petrov, S. 2009.
Coarse-to-Fine Natural Language Process-ing.
Ph.D. thesis, University of California-Berkeley.Rehbein, I and J van Genabith.
2007.
Treebank annotationschemes and parser evaluation for German.
In EMNLP-CoNLL.Roark, B, M Harper, E Charniak, B Dorr, M Johnson, J GKahne, Y Liuf, Mari Ostendorf, J Hale, A Krasnyanskaya,M Lease, I Shafran, M Snover, R Stewart, and L Yung.2006.
SParseval: Evaluation metrics for parsing speech.In LREC.Ryding, K. 2005.
A Reference Grammar of Modern StandardArabic.
Cambridge University Press.Sampson, G and A Babarczy.
2003.
A test of the leaf-ancestor metric for parse accuracy.
Natural Language En-gineering, 9:365?380.Toutanova, K, D Klein, C D Manning, and Y Singer.
2003.Feature-rich part-of-speech tagging with a cyclic depen-dency network.
In NAACL.Tsarfaty, R and K Sima?an.
2008.
Relational-realizationalparsing.
In COLING.Tsarfaty, R. 2006.
Integrated morphological and syntacticdisambiguation for Modern Hebrew.
In ACL.Zitouni, I, J S Sorensen, and R Sarikaya.
2006.
Maximumentropy based restoration of Arabic diacritics.
In ACL.402
