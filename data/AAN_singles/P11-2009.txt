Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 48?52,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSemisupervised condensed nearest neighbor for part-of-speech taggingAnders S?gaardCenter for Language TechnologyUniversity of CopenhagenNjalsgade 142, DK-2300 Copenhagen Ssoegaard@hum.ku.dkAbstractThis paper introduces a new training set con-densation technique designed for mixturesof labeled and unlabeled data.
It finds acondensed set of labeled and unlabeled datapoints, typically smaller than what is obtainedusing condensed nearest neighbor on the la-beled data only, and improves classificationaccuracy.
We evaluate the algorithm on semi-supervised part-of-speech tagging and presentthe best published result on the Wall StreetJournal data set.1 IntroductionLabeled data for natural language processing taskssuch as part-of-speech tagging is often in short sup-ply.
Semi-supervised learning algorithms are de-signed to learn from a mixture of labeled and un-labeled data.
Many different semi-supervised algo-rithms have been applied to natural language pro-cessing tasks, but the simplest algorithm, namelyself-training, is the one that has attracted most atten-tion, together with expectation maximization (Ab-ney, 2008).
The idea behind self-training is simplyto let a model trained on the labeled data label theunlabeled data points and then to retrain the modelon the mixture of the original labeled data and thenewly labeled data.The nearest neighbor algorithm (Cover and Hart,1967) is a memory-based or so-called lazy learn-ing algorithm.
It is one of the most extensivelyused nonparametric classification algorithms, sim-ple to implement yet powerful, owing to its theo-retical properties guaranteeing that for all distribu-tions, its probability of error is bound by twice theBayes probability of error (Cover and Hart, 1967).Memory-based learning has been applied to a widerange of natural language processing tasks includingpart-of-speech tagging (Daelemans et al, 1996), de-pendency parsing (Nivre, 2003) and word sense dis-ambiguation (Ku?bler and Zhekova, 2009).
Memory-based learning algorithms are said to be lazy be-cause no model is learned from the labeled datapoints.
The labeled data points are the model.
Con-sequently, classification time is proportional to thenumber of labeled data points.
This is of course im-practical.
Many algorithms have been proposed tomake memory-based learning more efficient.
Theintuition behind many of them is that the set of la-beled data points can be reduced or condensed, sincemany labeled data points are more or less redundant.The algorithms try to extract a subset of the overalltraining set that correctly classifies all the discardeddata points through the nearest neighbor rule.
Intu-itively, the model finds good representatives of clus-ters in the data or discards the data points that are farfrom the decision boundaries.
Such algorithms arecalled training set condensation algorithms.The need for training set condensation is partic-ularly important in semi-supervised learning wherewe rely on a mixture of labeled and unlabeled datapoints.
While the number of labeled data pointsis typically limited, the number of unlabeled datapoints is typically high.
In this paper, we intro-duce a new semi-supervised learning algorithm thatcombines self-training and condensation to producesmall subsets of labeled and unlabeled data pointsthat are highly relevant for determining good deci-48sion boundaries.2 Semi-supervised condensed nearestneighborThe nearest neighbor (NN) algorithm (Cover andHart, 1967) is conceptually simple, yet very pow-erful.
Given a set of labeled data points T , label anynew data point (feature vector) x with y where x?is the data point in T most similar to x and ?x?, y?.Similarity is usually measured in terms of Euclideandistance.
The generalization of the nearest neighboralgorithm, k nearest neighbor, finds the k most simi-lar data points Tk to x and assigns x the label y?
suchthat:y?
= arg maxy???Y??x?,y???TkE(x,x?)||y?
= y?
?||with E(?, ?)
Euclidean distance and || ?
|| = 1 if theargument is true (else 0).
In other words, the k mostsimilar points take a weighted vote on the class of x.Naive implementations of the algorithm store allthe labeled data points and compare each of them tothe data point that is to be classified.
Several strate-gies have been proposed to make nearest neighborclassification more efficient (Angiulli, 2005).
Inparticular, training set condensation techniques havebeen much studied.The condensed nearest neighbor (CNN) algorithmwas first introduced in Hart (1968).
Finding a sub-set of the labeled data points may lead to fasterand more accurate classification, but finding the bestsubset is an intractable problem (Wilfong, 1992).CNN can be seen as a simple technique for approxi-mating such a subset of labeled data points.The CNN algorithm is defined in Figure 1 with Tthe set of labeled data points and T (t) is label pre-dicted for t by a nearest neighbor classifier ?trained?on T .Essentially we discard all labeled data pointswhose label we can already predict with the cur-rent subset of labeled data points.
Note that wehave simplified the CNN algorithm a bit comparedto Hart (1968), as suggested, for example, in Alpay-din (1997), iterating only once over data rather thanwaiting for convergence.
This will give us a smallerset of labeled data points, and therefore classifica-tion requires less space and time.
Note that whilethe NN rule is stable, and cannot be improved byT = {?x1, y1?, .
.
.
, ?xn, yn?
}, C = ?for ?xi, yi?
?
T doif C(xi) 6= yi thenC = C ?
{?xi, yi?
}end ifend forreturn CFigure 1: CONDENSED NEAREST NEIGHBOR.T = {?x1, y1?, .
.
.
, ?xn, yn?
}, C = ?for ?xi, yi?
?
T doif C(xi) 6= yi or PC(?xi, yi?|xi) < 0.55 thenC = C ?
{?xi, yi?
}end ifend forreturn CFigure 2: WEAKENED CONDENSED NEAREST NEIGH-BOR.techniques such as bagging (Breiman, 1996), CNNis unstable (Alpaydin, 1997).We also introduce a weakened version of the al-gorithm which not only includes misclassified datapoints in the classifier C , but also correctly classi-fied data points which were labeled with relativelylow confidence.
So C includes all data points thatwere misclassified and those whose correct labelwas predicted with low confidence.
The weakenedcondensed nearest neighbor (WCNN) algorithm issketched in Figure 2.C inspects k nearest neighbors when labelingnew data points, where k is estimated by cross-validation.
CNN was first generalized to k-NN inGates (1972).Two related condensation techniques, namely re-moving typical elements and removing elements byclass prediction strength, were argued not to beuseful for most problems in natural language pro-cessing in Daelemans et al (1999), but our experi-ments showed that CNN often perform about as wellas NN, and our semi-supervised CNN algorithmleads to substantial improvements.
The condensa-tion techniques are also very different: While re-moving typical elements and removing elements byclass prediction strength are methods for removingdata points close to decision boundaries, CNN ide-49Figure 3: Unlabeled data may help find better representa-tives in condensed training sets.ally only removes elements close to decision bound-aries when the classifier has no use of them.Intuitively, with relatively simple problems,e.g.
mixtures of Gaussians, CNN and WCNN try tofind the best possible representatives for each clus-ter in the distribution of data, i.e.
finding the pointsclosest to the center of each cluster.
Ideally, CNNreturns one point for each cluster, namely the cen-ter of each cluster.
However, a sample of labeleddata may not include data points that are near thecenter of a cluster.
Consequently, CNN sometimesneeds several points to stabilize the representation ofa cluster; e.g.
the two positives in Figure 3.When a large number of unlabeled data pointsthat are labeled according to nearest neighbors pop-ulates the clusters, chances increase that we find datapoints near the centers of our clusters, e.g.
the ?goodrepresentative?
in Figure 3.
Of course the centers ofour clusters may move, but the positive results ob-tained experimentally below suggest that it is morelikely that labeling unlabeled data by nearest neigh-bors will enable us to do better training set conden-sation.This is exactly what semi-supervised condensednearest neighbor (SCNN) does.
We first run aWCNN C and obtain a condensed set of labeled datapoints.
To this set of labeled data points we add alarge number of unlabeled data points labeled by aNN classifier T on the original data set.
We use asimple selection criterion and include all data points1: T = {?x1, y1?, .
.
.
, ?xn, yn?
}, C = ?, C ?
= ?2: U = {?x?1?, .
.
.
, ?x?m?}
# unlabeled data3: for ?xi, yi?
?
T do4: if C(xi) 6= yi or PC(?xi, yi?|xi) < 0.55then5: C = C ?
{?xi, yi?
}6: end if7: end for8: for ?x?i?
?
U do9: if PT (?x?i, T (x?i)?|wi) > 0.90 then10: C = C ?
{?x?i, T (x?i)?
}11: end if12: end for13: for ?xi, yi?
?
C do14: if C ?
(xi) 6= yi then15: C ?
= C ?
?
{?xi, yi?
}16: end if17: end for18: return C ?Figure 4: SEMI-SUPERVISED CONDENSED NEARESTNEIGHBOR.that are labeled with confidence greater than 90%.We then obtain a new WCNN C ?
from the new dataset which is a mixture of labeled and unlabeled datapoints.
See Figure 4 for details.3 Part-of-speech taggingOur part-of-speech tagging data set is the standarddata set from Wall Street Journal included in Penn-III (Marcus et al, 1993).
We use the standard splitsand construct our data set in the following way, fol-lowing S?gaard (2010): Each word in the data wiis associated with a feature vector xi = ?x1i , x2i ?where x1i is the prediction on wi of a supervised part-of-speech tagger, in our case SVMTool1 (Gimenezand Marquez, 2004) trained on Sect.
0?18, and x2iis a prediction on wi from an unsupervised part-of-speech tagger (a cluster label), in our case Unsu-pos (Biemann, 2006) trained on the British NationalCorpus.2 We train a semi-supervised condensednearest neighbor classifier on Sect.
19 of the devel-opment data and unlabeled data from the Brown cor-pus and apply it to Sect.
22?24.
The labeled data1http://www.lsi.upc.es/?nlp/SVMTool/2http://wortschatz.uni-leipzig.de/?cbiemann/software/50points are thus of the form (one data point or wordper line):JJ JJ 17*NNS NNS 1IN IN 428DT DT 425where the first column is the class labels or thegold tags, the second column the predicted tags andthe third column is the ?tags?
provided by the unsu-pervised tagger.
Words marked by ?*?
are out-of-vocabulary words, i.e.
words that did not occur inthe British National Corpus.
The unsupervised tag-ger is used to cluster tokens in a meaningful way.Intuitively, we try to learn part-of-speech tagging bylearning when to rely on SVMTool.The best reported results in the literature on WallStreet Journal Sect.
22?24 are 97.40% in Suzuki etal.
(2009) and 97.44% in Spoustova et al (2009);both systems use semi-supervised learning tech-niques.
Our semi-supervised condensed nearestneighbor classifier achieves an accuracy of 97.50%.Equally importantly it condensates the available datapoints, from Sect.
19 and the Brown corpus, thatis more than 1.2M data points, to only 2249 datapoints, making the classifier very fast.
CNN alone isa lot worse than the input tagger, with an accuracyof 95.79%.
Our approach is also significantly betterthan S?gaard (2010) who apply tri-training (Li andZhou, 2005) to the output of SVMTool and Unsu-pos.acc (%) data points err.redCNN 95.79 3,811SCNN 97.50 2,249 40.6%SVMTool 97.15 -S?gaard 97.27 -Suzuki et al 97.40 -Spoustova et al 97.44 -In our second experiment, where we vary theamount of unlabeled data points, we only train ourensemble on the first 5000 words in Sect.
19 andevaluate on the first 5000 words in Sect.
22?24.The derived learning curve for the semi-supervisedlearner is depicted in Figure 5.
The immediate dropin the red scatter plot illustrates the condensation ef-fect of semi-supervised learning: when we begin toadd unlabeled data, accuracy increases by more than1.5% and the data set becomes more condensed.Semi-supervised learning means that we populateFigure 5: Normalized accuracy (range: 92.62?94.82) andcondensation (range: 310?512 data points).clusters in the data, making it easier to identify rep-resentative data points.
Since we can easier identifyrepresentative data points, training set condensationbecomes more effective.4 ImplementationThe implementation used in the experiments buildson Orange 2.0b for Mac OS X (Python and C++).In particular, we made use of the implementationsof Euclidean distance and random sampling in theirpackage.
Our code is available at:cst.dk/anders/sccn/5 ConclusionsWe have introduced a new learning algorithm thatsimultaneously condensates labeled data and learnsfrom a mixture of labeled and unlabeled data.
Wehave compared the algorithm to condensed nearestneighbor (Hart, 1968; Alpaydin, 1997) and showedthat the algorithm leads to more condensed models,and that it performs significantly better than con-densed nearest neighbor.
For part-of-speech tag-ging, the error reduction over condensed nearestneighbor is more than 40%, and our model is 40%smaller than the one induced by condensed nearestneighbor.
While we have provided no theory forsemi-supervised condensed nearest neighbor, we be-lieve that these results demonstrate the potential ofthe proposed method.51ReferencesSteven Abney.
2008.
Semi-supervised learning for com-putational linguistics.
Chapman & Hall.Ethem Alpaydin.
1997.
Voting over multiple con-densed nearest neighbors.
Artificial Intelligence Re-view, 11:115?132.Fabrizio Angiulli.
2005.
Fast condensed nearest neigh-bor rule.
In Proceedings of the 22nd InternationalConference on Machine Learning.Chris Biemann.
2006.
Unsupervised part-of-speechtagging employing efficient graph clustering.
InCOLING-ACL Student Session.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.T.
Cover and P. Hart.
1967.
Nearest neighbor patternclassification.
IEEE Transactions on Information The-ory, 13(1):21?27.Walter Daelemans, Jakub Zavrel, Peter Berck, and StevenGillis.
1996.
MBT: a memory-based part-of-speechtagger generator.
In Proceedings of the 4th Workshopon Very Large Corpora.Walter Daelemans, Antal Van Den Bosch, and Jakub Za-vrel.
1999.
Forgetting exceptions is harmful in lan-guage learning.
Machine Learning, 34(1?3):11?41.W Gates.
1972.
The reduced nearest neighbor rule.IEEE Transactions on Information Theory, 18(3):431?433.Jesus Gimenez and Lluis Marquez.
2004.
SVMTool: ageneral POS tagger generator based on support vectormachines.
In LREC.Peter Hart.
1968.
The condensed nearest neighbor rule.IEEE Transactions on Information Theory, 14:515?516.Sandra Ku?bler and Desislava Zhekova.
2009.
Semi-supervised learning for word-sense disambiguation:quality vs. quantity.
In RANLP.Ming Li and Zhi-Hua Zhou.
2005.
Tri-training: ex-ploiting unlabeled data using three classifiers.
IEEETransactions on Knowledge and Data Engineering,17(11):1529?1541.Mitchell Marcus, Mary Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Lin-guistics, 19(2):313?330.Joakim Nivre.
2003.
An efficient algorithm for projec-tive dependency parsing.
In Proceedings of the 8th In-ternational Workshop on Parsing Technologies, pages149?160.Anders S?gaard.
2010.
Simple semi-supervised trainingof part-of-speech taggers.
In ACL.Drahomira Spoustova, Jan Hajic, Jan Raab, and MiroslavSpousta.
2009.
Semi-supervised training for the aver-aged perceptron POS tagger.
In EACL.Jun Suzuki, Hideki Isozaki, Xavier Carreras, and MichaelCollins.
2009.
An empirical study of semi-supervisedstructured conditional models for dependency parsing.In EMNLP.G.
Wilfong.
1992.
Nearest neighbor problems.
Interna-tional Journal of Computational Geometry and Appli-cations, 2(4):383?416.52
