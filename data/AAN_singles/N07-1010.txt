Proceedings of NAACL HLT 2007, pages 73?80,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsCoreference or Not: A Twin Model for Coreference ResolutionXiaoqiang LuoIBM T.J. Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598, U.S.A.{xiaoluo}@us.ibm.comAbstractA twin-model is proposed for coreference res-olution: a link component, modeling the coref-erential relationship between an anaphor anda candidate antecedent, and a creation com-ponent modeling the possibility that a phraseis not coreferential with any candidate an-tecedent.
The creation model depends on allcandidate antecedents and is often expensiveto compute; Therefore constraints are imposedon feature forms so that features in the cre-ation model can be efficiently computed fromfeature values in the link model.
The pro-posed twin-model is tested on the data fromthe 2005 Automatic Content Extraction (ACE)task and the proposed model performs bet-ter than a thresholding baseline without tuningfree parameter.1 IntroductionCoreference resolution aims to find multiple mentionsof an entity (e.g., PERSON, ORGANIZATION) in adocument.
In a typical machine learning-based coref-erence resolution system (Soon et al, 2001; Ng andCardie, 2002b; Yang et al, 2003; Luo et al, 2004), astatistical model is learned from training data and isused to measure how likely an anaphor 1 is corefer-ential to a candidate antecedent.
A related, but oftenoverlooked, problem is that the anaphor may be non-coreferential to any candidate, which arises from sce-narios such as an identified anaphor is truly generic and1In this paper, ?anaphor?
includes all kinds of phrases tobe resolved, which can be named, nominal or pronominalphrases.there does not exist an antecedent in the discourse con-text, or an anaphor is the first mention (relative to pro-cessing order) in a coreference chain.In (Soon et al, 2001; Ng and Cardie, 2002b),the problem is treated by thresholding the scores re-turned by the coreference model.
That is, if the max-imum coreference score is below a threshold, then theanaphor is deemed non-referential to any candidate an-tecedent.
The threshold approach does not model non-coreferential events directly, and is by no means the op-timal approach to the problem.
It also introduces a freeparameter which has to be set by trial-and-error.
As animprovement, Ng and Cardie (2002a) and Ng (2004)train a separate model to classify an anaphor as eitheranaphoric or non-anaphoric.
The output of this clas-sifier can be used either as a pre-filter (Ng and Cardie,2002a) so that non-anaphoric anaphors will not be pre-cessed in the coreference system, or as a set of featuresin the coreference model (Ng, 2004).
By rejecting anyanaphor classified as non-anaphoric in coreference res-olution, the filtering approach is meant to handle non-anaphoric phrases (i.e., no antecedent exists in the dis-course under consideration), not the first mention in acoreference chain.In this paper, coreference is viewed as a process ofsequential operations on anaphor mentions: an anaphorcan either be linked with its antecedent if the antecedentis available or present.
If the anaphor, on the otherhand, is discourse new (relative to the process order),then a new entity is created.
Corresponding to the twotypes of operations, a twin-model is proposed to re-solve coreferential relationships in a document.
Thefirst component is a statistical model measuring howlikely an anaphor is coreferential to a candidate an-tecedent; The second one explicitly models the non-73coreferential events.
Both models are trained automat-ically and are used simultaneously in the coreferencesystem.
The twin-model coreference system is testedon the 2005 ACE (Automatic Content Extraction, see(NIST, 2005)) data and the best performance underboth ACE-Value and entity F-measure can be obtainedwithout tuning a free parameter.The rest of the paper is organized as follows.
Thetwin-model is presented in Section 2.
A maximum-entropy implementation and features are then presentedin Section 3.
The experimental results on the 2005ACE data is presented in Section 4.
The proposed twin-model is compared with related work in Section 5 be-fore the paper is concluded.2 Coreference ModelA phrasal reference to an entity is called a mention.
Aset of mentions referring to the same physical object issaid to belong to the same entity.
For example, in thefollowing sentence:(I) John said Mary was his sister.there are four mentions: John, Mary, his, andsister.
John and his belong to the same entitysince they refer to the same person; So do Mary andsister.
Furthermore, John and Mary are namedmentions, sister is a nominal mention and his is apronominal mention.In our coreference system, mentions are processedsequentially, though not necessarily in chronologicalorder.
For a document with n mentions {mi : 1 ?
i ?n}, at any time t(t > 1), mention m1 through mt?1have been processed and each mention is placed in oneof Nt(Nt ?
(t?1)) entities: Et = {ej : 1 ?
j ?
Nt}.Index i in mi indicates the order in which it is pro-cessed, not necessarily the order in which it appears ina document.
The basic step is to extend Et to Et+1with mt.Let us use the example in Figure 1 to illustrate howthis is done.
Note that Figure 1 contains one possibleprocessing order for the four mentions in Example (I):first name mentions are processed, followed by nom-inal mentions, followed by pronominal mentions.
Attime t = 1, there is no existing entity and the mentionm1=John is placed in an initial entity (entity is signi-fied by a solid rectangle).
At time t = 2, m2=Maryis processed and a new entity containing Mary is cre-ated.
At time t = 3, the nominal mention m3=sisteris processed.
At this point, the set of existing entitiesE3 ={{John}, {Mary}}.m3 is linked with the existing entity {Mary}.
At thelast step t = 4, the pronominal mention his is linkedwith the entity {John}.The above example illustrates how a sequence ofcoreference steps lead to a particular coreference result.Conversely, if the processing order is known and fixed,every possible coreference result can be decomposedand mapped to a unique sequence of such coreferencesteps.
Therefore, if we can score the set of coreferencesequences, we can score the set of coreference resultsas well.In general, when determining if a mention mt iscoreferential with any entity in Et, there are two typesof actions: one is that mt is coreferential with one ofthe entities; The other is that mt is not coreferentialwith any.
It is important to distinguish the two casesfor the following reason: if mt is coreferential with anentity ej , in most cases it is sufficient to determine therelationship by examining mt and ej , and their localcontext; But if mt is not coreferential with any existingentities, we need to consider mt with all members inEt.
This observation leads us to propose the followingtwin-model for coreference resolution.The first model, P (L|ej , mt), is conditioned on anentity ej and the current mention mt and measure howlikely they are coreferential.
L is a binary variable, tak-ing value 1 or 0, which represents positive and nega-tive coreferential relationship, respectively.
The secondmodel, on the other hand, P (C|Et, mt), is conditionedon the past entities Et and the current mention mt.
Therandom variable C is also binary: when C is 1, it meansthat a new entity {mt} will be created.
In other words,the second model measures the probability that mt isnot coreferential to any existing entity.
To avoid con-fusion in the subsequent presentation, the first modelwill be written as Pl(?|ej , mt) and called link model;The second model is written as Pc(?|Et, mt) and calledcreation model.For the time being, let?s assume that we have the linkand creation model at our disposal, and we will showhow they can be used to score coreference decisions.Given a set of existing entities Et = {ej}Nt1 , formedby mentions {mi}t?1i=1, and the current mention mt,there are Nt + 1 possible actions: we can either linkmt with an existing entity ej (j = 1, 2, ?
?
?
, Nt), orcreate a new entity containing mt.
The link action be-tween ej and mt can be scored by Pl(1|ej , mt) whilethe creation action can be measured by Pc(1|Et, mt).Each possible coreference outcome consists of n suchactions {at : t = 1, 2, ?
?
?
, n}, each of which can bescored by either the link model Pl(?|ej , mt) or the cre-74JohnJohnMaryJohnMarysisterJohnhisMarysisterJohn Mary sister hist=1 t=2 t=3 t=4E1={}m3 m4m2m1E3E2 E4Figure 1: Coreference process for the four mentions in Example (I).
Mentions in a document are processed se-quentially: first name mentions, then nominal mentions, and then pronominal mentions.
A dashed arrow signifiesthat a new entity is created, while a solid arrow means that the current mention is linked with an existing entity.ation model Pc(?|Et, mt).
Denote the score for ac-tion at by S(at|at?11 ), where dependency of at ona1 through at?1 is emphasized.
The coreference re-sult corresponding to the action sequence is written asEn({ai}ni=1).
When it is clear from context, we willdrop {ai}ni=1 and write En only.With this notation, the score for a coreference out-come En({ai}ni=1) is the product of individual scoresassigned to the corresponding action sequence {ai}ni=1,and the best coreference result is the one with the high-est score:E?n = arg maxEnS(En)= arg max{at}n1n?t=1S(at|at?11 ).
(1)Given n mentions, the number of all possibleentity outcomes is the Bell Number (Bell, 1934):B(n) = 1e??k=0knk!
.
Exhaustive search is out of thequestion.
Thus, we organize hypotheses into a BellTree (Luo et al, 2004) and use a beam search with thefollowing pruning strategy: first, a maximum beam size(typically 20) S is set, and we keep only the top S hy-potheses; Second, a relative threshold r (we use 10?5)is set to prune any hypothesis whose score divided bythe maximum score falls below the threshold.To give an concrete example, we use the examplein Figure 1 again.
The first step at t = 1 creates anew entity and is therefore scored by Pc(1|{},John);the second step also creates an entity and is scoredby Pc(1|{John},Mary); the step t = 3, how-ever, links sister with {Mary} and is scored byPl(1|{Mary},sister); Similarly, the last step isscored by Pl(1|{John},his).
The score for thiscoreference outcome is the product of the four num-bers:S({{John,his}, {Mary,sister}})=Pc(1|{},John)Pc(1|{John},Mary)?Pl(1|{Mary},sister)?Pl(1|{John},his).
(2)Other coreference results for these four mentions canbe scored similarly.
For example, if his at the laststep is linked with {Mary,sister}, the score wouldbe:S({{John}, {Mary,sister,his}})=Pc(1|{},John)Pc(1|{John},Mary)?Pl(1|{Mary},sister)?Pl(1|{Mary,sister},his).
(3)At testing time, (2) and (3), among other possible out-comes, will be searched and compared, and the onewith the highest score will be output as the coreferenceresult.Examples in (2) and (3) indicate that the link modelPl(?|ej , mt) and creation model Pc(?|Et, mt) form anintegrated coreference system and are applied simul-taneously at testing time.
As will be shown in the nextsection, features in the creation model Pc(?|Et, mt) canbe computed from their counterpart in the link modelPl(?|ej , mt) under some mild constraints.
So the twomodels?
training procedures are tightly coupled.
Thisis different from (Ng and Cardie, 2002a; Ng, 2004)where their anaphoricty models are trained indepen-dently of the coreference model, and it is either usedas a pre-filter, or its output is used as features in thecoreference model.
The creation model Pc(?|Et, mt)proposed here bears similarity to the starting model75in (Luo et al, 2004).
But there is a crucial differ-ence: the starting model in (Luo et al, 2004) is anad-hoc use of the link scores and is not learned auto-matically, while Pc(?|Et, mt) is fully trained.
TrainingPc(?|Et, mt) is covered in the next section.3 Implementation3.1 Feature StructureTo implement the twin model, we adopt the log linearor maximum entropy (MaxEnt) model (Berger et al,1996) for its flexibility of combining diverse sources ofinformation.
The two models are of the form:Pl(L|ej , mt) =exp(?k ?kgk(ej , mt, L))Y (ej , mt)(4)Pc(C|Et, mt) =exp(?i ?ihi(Et, mt, C))Z(Et, mt), (5)where L and C are binary variables indicating eithermt is coreferential with ej , or mt is used to create anew entity.
Y (ej , mt) and Z(ej , mt) are normalizationfactors to ensure that Pl(?|ej , mt) and Pc(?|Et, mt) areprobabilities; ?k and ?i are the weights for featuregk(ej , mt, L) and hi(Et, mt, C), respectively.
Oncethe set of features functions are selected, algorithmsuch as improved iterative scaling (Berger et al, 1996)or sequential conditional generalized iterative scal-ing (Goodman, 2002) can be used to find the optimalparameter values of {?k} and {?i}.Computing features {gk(ej , mt, ?)}
for the linkmodel Pl(L|ej , mt) 2 is relatively straightforward:given an entity ej and the current mention mt, wejust need to characterize things such as lexical similar-ity, syntactic relationship, and/or semantic compatibil-ity of the two.
It is, however, very challenging to com-pute the features {hi(Et, mt, ?)}
for the creation modelPc(?|Et, mt) since its conditioning includes a set of en-tities Et, whose size grows as more and more mentionsare processed.
The problem exists because the decisionof creating a new entity with mt has to be made afterexamining all preceding entities.
There is no reason-able modeling assumption one can make to drop someentities in the conditioning.To overcome the difficulty, we impose the follow-ing constraints on the features of the link and creation2The link model is actually implemented as:Pl(L|ej , mt) ?
maxm?
?ej P?l(L|ej , m?, mt).
Somefeatures are computed on a pair of mentions (m?, mt) whilesome are computed at entity level.
See (Luo and Zitouni,2005) and (Daume?
III and Marcu, 2005).model:gk(ej , mt, L) =g(1)k (ej , mt)g(2)k (L) (6)hi(Et, mt, C) =h(1)i({g(1)k (e, mt) : e ?
Et})?h(2)i (C), for some k. (7)(6) states that a feature in the link model is separableand can be written as a product of two functions: thefirst one, g(1)k (?, ?
), is a binary function depending onthe conditioning part only; the second one, g(2)k (?
), isan indicator function depending on the prediction partL only.
Like g(2)k (?
), h(2)i (?)
is also a binary indicatorfunction.
(7) implies that features in the creation modelare also separable; Moreover, the conditioning parth(1)i({g(1)k (e, mt) : e ?
Et}), also a binary function,only depends on the function values of the set of linkfeatures {g(1)k (e, mt) : e ?
Et} (for some k).
In otherwords, once {g(1)k (e, mt) : e ?
Et} and C are known,we can compute hi(Et, mt, C) without actually com-paring mt with any entity in Et.
Using binary featuresis a fairly mild constraint as non-binary features can bereplaced by a set of binary features through quantiza-tion.How fast h(1)i({g(1)k (e, mt) : e ?
Et})can be com-puted depends on how h(1)i is defined.
In most cases?
as will be shown in Section 3.2, it boils down test-ing if any member in {g(1)k (e, mt) : e ?
Et} is non-zero; or counting how many non-zero members thereare in {g(1)k (e, mt) : e ?
Et}.
Both are simple op-erations that can be carried out quickly.
Thus, the as-sumption (7) makes it possible to compute efficientlyhi(Et, mt, C).3.2 Features in the Creation ModelWe describe features used in our coreference system.We will concentrate on features used in the creationmodel since those in the link model can be found inthe literature (Soon et al, 2001; Ng and Cardie, 2002b;Yang et al, 2003; Luo et al, 2004).
In particular,we show how features in the creation model can becomputed from a set of feature values from the linkmodel for a few example categories.
Since g(2)k (?)
andh(2)i (?)
are simple indicator functions, we will focus ong(1)k (?, ?)
and h(1)i (?
).3.2.1 Lexical FeaturesThis set of features computes if two surface strings(spellings of two mentions) match each other, and are76applied to name and nominal mentions only.
For thelink model, a lexical feature g(1)k (ej , mt) is 1 if ej con-tains a mention matches mt, where a match can be ex-act, partial, or one is an acronym of the other.Since gk(ej , mt) is binary, one corresponding fea-ture used in the creation model is the disjunction of thevalues in the link model, orh(1)i (Et, mt) = ?e?Et{g(1)k (e, mt)}, (8)where ?
is a binary ?or?
operator.
The intuition is thatif there is any mention in Et matching mt, then theprobability to create a new entity with mt should below; Conversely, if none of the mentions in Et matchesmt, then mt is likely to be the first mention of a newentity.Take t = 2 in Figure 1 as an example.
There isonly one partially-established entity {John}, so E2 ={John}, and m2 = Mary.
The exact string matchfeature g(1)em(?, ?)
would beg(1)em({John},Mary) = 0,and the corresponding string match feature in the cre-ation model ish(1)em({John},Mary) = ?e?Et{g(1)em(e,Mary)}= 0.Disjunction is not the only operation we can use.Another possibility is counting how many times mtmatches mentions in Et, so (8) becomes:h(1)i (Et, mt) = Q[?e?Et{g(1)k (e, mt)}], .
(9)where Q[?]
quantizes raw counts into bins.3.2.2 Attribute FeaturesIn the link model, features in this category comparethe properties of the current mention mt with that of anentity ej .
Properties of a mention or an entity, when-ever applicable, include gender, number, entity type,reflexivity of pronouns etc.
Similar to what done inthe lexical feature, we again synthesize a feature in thecreation model by taking the disjunction of the corre-sponding set of feature values in the link model, orh(1)i (Et, mt) = ?e?Et{g(1)k (e, mt)},where g(1)k (e, mt) takes value 1 if entity e and mentionmt share the same property; Otherwise its value is 0.The intuition is that if there is an entity having the sameproperty as the current mention, then the probability forthe current mention to be linked with the entity shouldbe higher than otherwise; Conversely, if none of the en-tities in Et shares a property with the current mention,the probability for the current mention to create a newentity ought to be higher.Consider the gender attribute at t = 4 in Fig-ure 1.
Let g(1)gender(?, ?)
be the gender feature in thelink model, assume that we know the gender of John,Mary and his.
Then g(1)gender({{John},his) is 1,while g(1)gender({Mary, sister},his) is 0.
There-fore, the gender feature for the creation model wouldbeh(1)gender({{John},{Mary, sister}}, his)=0 ?
1 = 1,which means that there is at least one mention whichhas the same the gender of the current mention mt.3.2.3 Distance FeatureDistance feature needs special treatment: while itmakes sense to talk about the distance between a pairof mentions, it is not immediately clear how to computethe distance between a set of entities Et and a mentionmt.
To this end, we compute the minimum distance be-tween the entities and the current mention with respectto a ?fired?
link feature, as follows.For a particular feature g(1)k (?, ?)
in the link model,define the minimum distance to bed?
(Et, mt; gk) = min{d(m, mt) : m ?
Et,and g(1)k (m, mt) = 1}, (10)where d(m, mt) is the distance between mention mandmt.
The distance itself can be the number of tokens,or the number of intervening mentions, or the numberof sentences.
The minimum distance d?
(Et, mt; gk) isquantized and represented as binary feature in the cre-ation model.
The idea here is to encode what is thenearest place where a feature fires.Again as an example, consider the gender attribute att = 4 in Figure 1.
Assuming that d(m, mt) is the num-ber of tokens.
Since only John matches the gender ofhis,d?
(E4, m4; ggender) = 3.The number is then quantized and used as a binary fea-ture to encode the information that ?there is a mentionwhose gender matches the current mention within in atoken distance range including 3.?77In general, binary features in the link model whichmeasure the similarity between an entity and a mentioncan be turned into features in the creation model in thesame manner as described in Section 3.2.1 and 3.2.2.For example, syntactic features (Ng and Cardie, 2002b;Luo and Zitouni, 2005) can be computed this way andare used in our system.4 Experiments4.1 Data and Evaluation MetricWe report the experimental results on ACE 2005data (NIST, 2005).
The dataset consists of 599 doc-uments from a rich and diversified sources, which in-clude newswire articles, web logs, and Usenet posts,transcription of broadcast news, broadcast conversa-tions and telephone conversations.
We reserve the last16% documents of each source as the test set and usethe rest of the documents as the training set.
Statisticssuch as the number of documents, words, mentions andentities of this data split is tabulated in Table 1.DataSet #Docs #Words #Mentions #EntitiesTraining 499 253771 46646 16102Test 100 45659 8178 2709Total 599 299430 54824 18811Table 1: Statistics of ACE 2005 data: number of docu-ments, words, mentions and entities in the training andtest set.The link and creation model are trained at the sametime.
Besides the basic feature categories described inSection 3.2, we also compute composite features bytaking conjunctions of the basic features.
Features areselected by their counts with a threshold of 8.ACE-Value is the official score reported in the ACEtask and will be used to report our coreference system?sperformance.
Its detailed definition can be found in theofficial evaluation document 3.
Since ACE-Value is aweighted metric measuring a coreference system?s rel-ative value, and it is not sensitive to certain type oferrors (e.g., false-alarm entities if these entities con-tain correct mentions), we also report results using un-weighted entity F-measure.4.2 ResultsTo compare the proposed twin model with simplethresholding (Soon et al, 2001; Ng and Cardie, 2002b),3The official evaluation document can be found at:www.nist.gov/speech/tests/ace/ace05/doc/ace05-evalplan.v3.pdf.5560657075808590950.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9PerformanceThresholdPerformances vs. ThresholdBaseline-EntFTwin-EntFBaseline-ACEValTwin-ACEValFigure 2: Performance comparison between a thresh-olding baseline and the twin-model: lines with squarepoints are the entity F-measure (x100) results; lineswith triangle points are ACE-Value (in %).
Solid linesare baseline while dashed lines are twin-model.we first train our twin model.
To simulate the thresh-olding approach, a baseline coreference system is cre-ated by replacing the creation model with a constant,i.e.,Pc(1|Et, mt) = ?, (11)where ?
is a number between 0 and 1.
At testing time,a new entity is created with score ?
whenPl(1|ej , mt) < ?, ?ej ?
Et.The decision rule simply implies that if the scores be-tween the current mention mt and all candidate entitiesej ?
Et are below the threshold ?, a new entity will becreated.Performance comparison between the baseline andthe twin-model is plotted in Figure 2.
X-axis is thethreshold varying from 0.1 to 0.9 with a step size 0.1.Two metrics are used to compare the results: two lineswith square data points are the entity F-measure results,and two lines with triangle points are ACE-Value.
Notethat performances for the twin-model are constant sinceit does not use thresholding.As shown in the graph, the twin-model (two dashedlines) always outperforms the baseline (two solidlines).
A ?bad?
threshold impacts the entity F-measuremuch more than ACE-Value, especially in the regionwith high threshold value.
Note that a large ?
will leadto more false-alarm entities.
The graph suggests thatACE-Value is much less sensitive than the un-weightedF-measure in measuring false-alarm errors.
For exam-ple, at ?
= 0.9, the baseline F-measure is 0.591 while78the twin model F-measure is 0.848, a 43.5% difference;On the other hand, the corresponding ACE-Values are84.5% (baseline) vs. 88.4% (twin model), a mere 4.6%relative difference.
There are at least two reasons: first,ACE-Value discounts importance of nominal and pro-noun entities, so more nominal and pronoun entity er-rors are not reflected in the metric; Second, ACE-Valuedoes not penalize false-alarm entities if they containcorrect mentions.
The problem associated with ACE-Value is the reason we include the entity F-measure re-sults.Another interesting observation is that an optimalthreshold for the entity F-measure is not necessarily op-timal for ACE-Value, and vice versa: ?
= 0.3 is thebest threshold for the entity F-measure, while ?
= 0.5is optimal for ACE-Value.
This is highlighted in Ta-ble 2, where row ?B-opt-F?
contains the best results op-timizing the entity F-measure (at ?
= 0.3), row ?B-opt-AV?
contains the best results optimizing ACE-Value (at?
= 0.5), and the last line ?Twin-model?
contains theresults of the proposed twin-model.
It is clear fromTable 2 that thresholding cannot be used to optimizethe entity F-measure and ACE-Value simultaneously.A sub-optimal threshold could be detrimental to an un-weighted metric such as the entity F-measure.
The pro-posed twin model eliminates the need for threshold-ing, a benefit of using the principled creation model.In practice, the optimal threshold is a free parameterthat has to be tuned every time when a task, dataset andmodel changes.
Thus the proposed twin model is moreportable when a task or dataset changes.System F-measure ACE-ValueB-opt-F 84.7 87.5B-opt-AV 81.1 88.0Twin-model 84.8 88.4Table 2: Comparison between the thresholding base-line and the twin model: optimal threshold depends onperformance metric.
The proposed twin-model outper-forms the baseline without tuning the free parameter.5 Related WorkSome earlier work (Lappin and Leass, 1994; Kennedyand Boguraev, 1996) use heuristic to determinewhether a phrase is anaphoric or not.
Bean and Riloff(1999) extracts rules from non-anaphoric noun phrasesand noun phrases patterns, which are then applied totest data to identify existential noun phrases.
It is in-tended as as pre-filtering step before a coreference res-olution system is run.
Ng and Cardie (2002a) trains aseparate anaphoricity classifier in addition to a corefer-ence model.
The anaphoricity classifier is applied as afilter and only anaphoric mentions are later consideredby the coreference model.
Ng (2004) studies what isthe best way to make use of anaphoricity informationand concludes that the constrained-based and globally-optimized approach works the best.
Poesio et al (2004)contains a good summary of recent research work ondiscourse new or anaphoricity.
Luo et al (2004) usesa start model to determine whether a mention is thefirst one in a coreference chain, but it is computed adhoc without training.
Nicolae and Nicolae (2006) con-structs a graph where mentions are nodes and an edgerepresents the likelihood two mentions are in an entity,and then a graph-cut algorithm is employed to producefinal coreference results.We take the view that determining whether ananaphor is coreferential with any candidate antecedentis part of the coreference process.
But we do recog-nize that the disparity between the two types of events:while a coreferential relationship can be resolved byexamining the local context of the anaphor and its an-tecedent, it is necessary to compare the anaphor withall the preceding candidates before it can be declaredthat it is not coreferential with any.
Thus, a creationcomponent Pc(?|Et, mt) is needed to model the secondtype of events.
A problem arising from the adoption ofthe creation model is that it is very expensive to havea conditional model depending on all preceding enti-ties Et.
To solve this problem, we adopt the MaxEntmodel and impose some reasonable constraints on thefeature functions, which makes it possible to synthe-size features in the creation model from those of thelink model.
The twin model components are intimatelytrained and used simultaneously in our coreference sys-tem.6 ConclusionsA twin-model is proposed for coreference resolution:one link component computes how likely a mention iscoreferential with a candidate entity; the other compo-nent, called creation model, computes the probabilitythat a mention is not coreferential with any candidateentity.
Log linear or MaxEnt approach is adopted forbuilding the two components.
The twin componentsare trained and used simultaneously in our coreferencesystem.The creation model depends on all preceding enti-ties and is often expensive to compute.
We imposesome reasonable constraints on feature functions which79makes it feasible to compute efficiently the features inthe creation model from a subset of link feature val-ues.
We test the proposed twin-model on the ACE 2005data and the proposed model outperforms a threshold-ing baseline.
Moreover, it is observed that the optimalthreshold in the baseline depends on performance met-ric, while the proposed model eliminates the need oftuning the optimal threshold.AcknowledgmentsThis work was partially supported by the Defense Ad-vanced Research Projects Agency under contract No.HR0011-06-2-0001.
The views and findings containedin this material are those of the authors and do notnecessarily reflect the position or policy of the U.S.government and no official endorsement should be in-ferred.I would like to thank Salim Roukos for helping toimprove the writing of the paper.
Suggestions and com-ments from three anonymous reviewers are also grate-fully acknowledged.ReferencesDavid L. Bean and Ellen Riloff.
1999.
Corpus-basedidentification of non-anaphoric noun phrases.
InProc.
ACL.E.T.
Bell.
1934.
Exponential numbers.
Amer.
Math.Monthly, pages 411?419.Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71, March.Hal Daume?
III and Daniel Marcu.
2005.
A large-scale exploration of effective global features for ajoint entity detection and tracking model.
In Proc.
ofHLT and EMNLP, pages 97?104, Vancouver, BritishColumbia, Canada, October.
Association for Com-putational Linguistics.Joshua Goodman.
2002.
Sequential conditional gener-alized iterative scaling.
In Pro.
of the 40th ACL.Christopher Kennedy and Branimir Boguraev.
1996.Anaphora for everyone: Pronominal anaphora reso-lution without a parser.
In Proceedings of COLING-96 (16th International Conference on Computa-tional Linguistics), Copenhagen,DK.Shalom Lappin and Herbert J. Leass.
1994.
An algo-rithm for pronominal anaphora resolution.
Compu-tational Linguistics, 20(4), December.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-lingual coreference resolution with syntactic fea-tures.
In Proc.
of Human Language Technology(HLT)/Empirical Methods in Natural Language Pro-cessing (EMNLP).Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the bell tree.
In Proc.
of ACL.Vincent Ng and Claire Cardie.
2002a.
Identifyinganaphoric and non-anaphoric noun phrases to im-prove coreference resolution.
In Proceedings ofCOLING.Vincent Ng and Claire Cardie.
2002b.
Improving ma-chine learning approaches to coreference resolution.In Proc.
of ACL, pages 104?111.Vincent Ng.
2004.
Learning noun phrase anaphoric-ity to improve conference resolution: Issues in rep-resentation and optimization.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 151?158,Barcelona, Spain, July.Cristina Nicolae and Gabriel Nicolae.
2006.
BEST-CUT: A graph algorithm for coreference resolution.In Proceedings of the 2006 Conference on Empiri-cal Methods in Natural Language Processing, pages275?283, Sydney, Australia, July.
Association forComputational Linguistics.NIST.
2005.
ACE 2005 evaluation.www.nist.gov/speech/tests/ace/ace05/index.htm.M.
Poesio, O. Uryupina, R. Vieira, M. Alexandrov-Kabadjov, and R. Goulart.
2004.
Discourse-new de-tectors for definite description resolution: A surveyand a preliminary proposal.
In ACL 2004: Workshopon Reference Resolution and its Applications, pages47?54, Barcelona, Spain, July.Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguis-tics, 27(4):521?544.Xiaofeng Yang, Guodong Zhou, Jian Su, andChew Lim Tan.
2003.
Coreference resolution us-ing competition learning approach.
In Proc.
of the41st ACL.80
