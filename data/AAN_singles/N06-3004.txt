Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223?226,New York, June 2006. c?2006 Association for Computational LinguisticsEfficient Algorithms for Richer Formalisms:Parsing and Machine TranslationLiang HuangDepartment of Computer and Information ScienceUniversity of Pennsylvanialhuang3@cis.upenn.eduMy PhD research has been on the algorithmic andformal aspects of computational linguistics, esp.
inthe areas of parsing and machine translation.
I aminterested in developing efficient algorithms for for-malisms with rich expressive power, so that we canhave a better modeling of human languages withoutsacrificing efficiency.
In doing so, I hope to help in-tegrating more linguistic and structural knowledgewith modern statistical techniques, and in particular,for syntax-based machine translation (MT) systems.Among other projects, I have been working on k-best parsing, synchronous binarization, and syntax-directed translation.1 k-best Parsing and HypergraphsNLP systems are often cascades of several modules,e.g., part-of-speech tagging, then syntactic parsing,and finally semantic interpretation.
It is often thecase that the 1-best output from one module is notalways optimal for the next module.
So one mightwant to postpone some disambiguation by propa-gating k-best lists (instead of 1-best solutions) tosubsequent phases, as in joint parsing and seman-tic role-labeling (Gildea and Jurafsky, 2002).
Thisis also true for reranking and discriminative train-ing, where the k-best list of candidates serves as anapproximation of the full set (Collins, 2000; Och,2003; McDonald et al, 2005).
In this way we canoptimize some complicated objective function onthe k-best set, rather than on the full search spacewhich is usually exponentially large.Previous algorithms for k-best parsing (Collins,2000; Charniak and Johnson, 2005) are either sub-optimal or slow and rely significantly on prun-ing techniques to make them tractable.
So I co-developed several fast and exact algorithms for k-best parsing in the general framework of directedmonotonic hypergraphs (Huang and Chiang, 2005).This formulation extends and refines Klein andManning?s work (2001) by introducing monotonic1.52.53.54.55.56.57.51  10  100  1000  10000AverageParsingTime(seconds)kAlgorithm 0Algorithm 1Algorithm 3Figure 1: Average parsing speed on the Section 23of Penn Treebank (Algorithms 0, 1, and 3, log-log).weight functions, which is closely related to the opti-mal subproblem property in dynamic programming.We first generalize the classical 1-best Viterbi al-gorithm to hypergraphs, and then present four k-bestalgorithms, each improving its predessor by delay-ing more work until necessary.
The final one, Al-gorithm 3, starts with a normal 1-best search foreach vertex (or item, as in deductive frameworks),and then works backwards from the target vertex (fi-nal item) for its 2nd, 3rd, .
.
., kth best derivations,calling itself recursively only on demand, being thelaziest of the four algorithms.
When tested on topof two state-of-the-art systems, the Collins/Bikelparser (Bikel, 2004) and Chiang?s CKY-based Hierodecoder (Chiang, 2005), this algorithm is shown tohave very little overhead even for quite large k (say,106) (See Fig.
1 for experiments on Bikel parser).These algorithms have been re-implemented byother researchers in the field, including EugeneCharniak for his n-best parser, Ryan McDonald forhis dependency parser (McDonald et al, 2005), Mi-crosoft Research NLP group (Simon Corston-Oliverand Kevin Duh, p.c.)
for a similar model, JonathanGraehl for the ISI syntax-based MT decoder, DavidA.
Smith for the Dyna language (Eisner et al, 2005),223and Jonathan May for ISI?s tree automata packageTiburon.
All of these experiments confirmed thefindings in our work.2 Synchronous Binarization for MTMachine Translation has made very good progressin recent times, especially, the so-called ?phrase-based?
statistical systems (Och and Ney, 2004).
Inorder to take a substantial next-step it will be neces-sary to incorporate several aspects of syntax.
Manyresearchers have explored syntax-based methods,for instance, Wu (1996) and Chiang (2005) both usesbinary-branching synchronous context-free gram-mars (SCFGs).
However, to be more expressiveand flexible, it is often easier to start with a gen-eral SCFG or tree-transducer (Galley et al, 2004).In this case, binarization of the input grammar isrequired for the use of the CKY algorithm (in or-der to get cubic-time complexity), just as we converta CFG into the Chomsky Normal Form (CNF) formonolingual parsing.
For synchronous grammars,however, different binarization schemes may resultin very different-looking chart items that greatly af-fect decoding efficiency.
For example, consider thefollowing SCFG rule:(1) S ?
NP(1) VP(2) PP(3), NP(1) PP(3) VP(2)We can binarize it either left-to-right or right-to-left:S ?
VNP-PP VPVNP-PP?
NP PPorS ?
NP VPP-VPVPP-VP ?
PP VPThe intermediate symbols (e.g.
VPP-VP) are called vir-tual nonterminals.
We would certainly prefer theright-to-left binarization because the virtual nonter-minal has consecutive span (see Fig.
2).
The left-to-right binarization causes discontinuities on the targetside, which results in an exponential time complex-ity when decoding with an integrated n-gram model.We develop this intuition into a technique calledsynchronous binarization (Zhang et al, 2006)which binarizes a synchronous production or tree-tranduction rule on both source and target sides si-multaneously.
It essentially converts an SCFG intoan equivalent ITG (the synchronous extension ofCNF) if possible.
We reduce this problem to thebinarization of the permutation of nonterminal sym-bols between the source and target sides of a syn-chronous rule and devise a linear-time algorithmNPNPPPVPVPPPtarget (English)source (Chinese)VPP-VPNPPPVPChinese indicesEnglishboundarywords 1 2 4 7PowellPowellheldmeetingwithSharonVPP-VPFigure 2: The alignment pattern (left) and alignmentmatrix (right) of the SCFG rule.system BLEUmonolingual binarization 36.25synchronous binarization 38.44Table 1: Synchronous vs. monolingual binarizationin terms of translation quality (BLEU score).for it.
Experiments show that the resulting rule setsignificantly improves the speed and accuracy overmonolingual binarization (see Table 1) in a state-of-the-art syntax-based machine translation system(Galley et al, 2004).
We also propose another trick(hook) for further speeding up the decoding with in-tegrated n-gram models (Huang et al, 2005).3 Syntax-Directed TranslationSyntax-directed translation was originally proposedfor compiling programming languages (Irons, 1961;Lewis and Stearns, 1968), where the source pro-gram is parsed into a syntax-tree that guides thegeneration of the object code.
These translationshave been formalized as a synchronous context-freegrammar (SCFG) that generates two languages si-multaneously (Aho and Ullman, 1972), and equiv-alently, as a top-down tree-to-string transducer(Ge?cseg and Steinby, 1984).
We adapt this syntax-directed transduction process to statistical MT byapplying stochastic operations at each node of thesource-language parse-tree and searching for thebest derivation (a sequence of translation steps) thatconverts the whole tree into some target-languagestring (Huang et al, 2006).3.1 Extended Domain of LocalityFrom a modeling perspective, however, the struc-tural divergence across languages results in non-isomorphic parse-trees that are not captured by224SCFGs.
For example, the S(VO) structure in En-glish is translated into a VSO order in Arabic, aninstance of complex re-ordering (Fig.
4).To alleviate this problem, grammars with richerexpressive power have been proposed which cangrab larger fragments of the tree.
Following Galleyet al (2004), we use an extended tree-to-string trans-ducer (xRs) with multi-level left-hand-side (LHS)trees.1 Since the right-hand-side (RHS) string canbe viewed as a flat one-level tree with the same non-terminal root from LHS (Fig.
4), this framework isclosely related to STSGs in having extended domainof locality on the source-side except for remain-ing a CFG on the target-side.
These rules can belearned from a parallel corpus using English parse-trees, Chinese strings, and word alignment (Galleyet al, 2004).3.2 A Running ExampleConsider the following English sentence and its Chi-nese translation (note the reordering in the passiveconstruction):(2) the gunman was killed by the police .qiangshou[gunman]bei[passive]jingfang[police]jibi[killed]?.Figure 3 shows how the translator works.
The En-glish sentence (a) is first parsed into the tree in (b),which is then recursively converted into the Chinesestring in (e) through five steps.
First, at the rootnode, we apply the rule r1 which preserves the top-level word-order and translates the English periodinto its Chinese counterpart:(r1) S (x1:NP-C x2:VP PUNC (.)
) ?
x1 x2 ?Then, the rule r2 grabs the whole sub-tree for ?thegunman?
and translates it as a phrase:(r2) NP-C ( DT (the) NN (gunman) ) ?
qiangshouNow we get a ?partial Chinese, partial English?
sen-tence ?qiangshou VP ??
as shown in Fig.
3 (c).
Ourrecursion goes on to translate the VP sub-tree.
Herewe use the rule r3 for the passive construction:1we will use LHS and source-side interchangeably (so areRHS and target-side).
In accordance with our experiments, wealso use English and Chinese as the source and target languages,opposite to the Foreign-to-English convention of Brown et al(1993).
(a) the gunman was killed by the police .parser ?
(b)SNP-CDTtheNNgunmanVPVBDwasVP-CVBNkilledPPINbyNP-CDTtheNNpolicePUNC.r1, r2 ?
(c) qiangshouVPVBDwasVP-CVBNkilledPPINbyNP-CDTtheNNpolice?r3 ?
(d) qiangshou beiNP-CDTtheNNpoliceVBNkilled?r5 ?
r4 ?
(e) qiangshou bei jingfang jibi ?Figure 3: A synatx-directed translation process.SNP(1)?
VPVB(2)?
NP(3)?, SVB(2)?
NP(1)?
NP(3)?Figure 4: An example of complex re-ordering.225(r3)VPVBDwasVP-Cx1:VBN PPINbyx2:NP-C?
bei x2 x1which captures the fact that the agent (NP-C, ?thepolice?)
and the verb (VBN, ?killed?)
are alwaysinverted between English and Chinese in a passivevoice.
Finally, we apply rules r4 and r5 which per-form phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chi-nese string in (e).3.3 Translation AlgorithmGiven a fixed parse-tree ?
?, the search for the bestderivation (as a sequence of conversion steps) canbe done by a simple top-down traversal (or depth-first search) from the root of the tree.
With memo-izationm, we get a dynamic programming algorithmthat is guaranteed to run in O(n) time where n is thelength of the input string, since the size of the parse-tree is proportional to n. Similar algorithms havealso been proposed for dependency-based transla-tion (Lin, 2004; Ding and Palmer, 2005).I am currently performing large-scale experi-ments on English-to-Chinese translation using thexRs rules.
We are not doing the usual direction ofChinese-to-English partly due to the lack of a suf-ficiently good Chinese parser.
Initial results showpromising translation quality (in terms of BLEUscores) and fast translation speed.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theory ofParsing, Translation, and Compiling, volume I: Parsing ofSeries in Automatic Computation.
Prentice Hall, EnglewoodCliffs, New Jersey.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479?511, December.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLinguistics, 19:263?311.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminative reranking.
In Pro-ceedings of the 43rd ACL.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of the 43rd ACL.Michael Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of ICML, pages 175?182.Yuan Ding and Martha Palmer.
2005.
Machine translationusing probablisitic synchronous dependency insertion gram-mars.
In Proceedings of the 43rd ACL.Jason Eisner, Eric Goldlust, and Noah A. Smith.
2005.
Com-piling comp ling: Weighted dynamic programming and thedyna language.
In Proceedings of HLT-EMNLP.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL.F.
Ge?cseg and M. Steinby.
1984.
Tree Automata.
Akade?miaiKiado?, Budapest.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labelingof semantic roles.
Computational Linguistics, 28(3):245?288.Liang Huang and David Chiang.
2005.
Better k-best Pars-ing.
In Proceedings of 9th International Workshop of Pars-ing Technologies (IWPT).Liang Huang, Hao Zhang, and Daniel Gildea.
2005.
Machinetranslation as lexicalized parsing with hooks.
In Proceed-ings of 9th International Workshop of Parsing Technologies(IWPT).Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Syntax-directed translation with extended domain of locality.
Insubmission.E.
T. Irons.
1961.
A syntax-directed compiler for ALGOL 60.Comm.
ACM, 4(1):51?55.Dan Klein and Christopher D. Manning.
2001.
Parsing andHypergraphs.
In Proceedings of the Seventh InternationalWorkshop on Parsing Technologies (IWPT-2001), 17-19 Oc-tober 2001, Beijing, China.P.
M. Lewis and R. E. Stearns.
1968.
Syntax-directed transduc-tion.
Journal of the ACM, 15(3):465?488.Dekang Lin.
2004.
A path-based transfer model for machinetranslation.
In Proceedings of the 20th COLING.Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005.Online large-margin training of dependency parsers.
In Pro-ceedings of the 43rd ACL.F.
J. Och and H. Ney.
2004.
The alignment template approachto statistical machine translation.
Computational Linguis-tics, 30:417?449.Franz Och.
2003.
Minimum error rate training for statisticalmachine translation.
In Proc.
of ACL.Dekai Wu.
1996.
A polynomial-time algorithm for statisticalmachine translation.
In Proceedings of the 34th ACL.Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.2006.
Synchronous binarization for machine translation.
InProceedings of HLT-NAACL.226
