Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 193?196,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPA Rose is a Roos is a Ruusu: Querying Translations for Web Image SearchJanara Christensen Mausam Oren EtzioniTuring CenterDept.
of Computer Science and EngineeringUniversity of Washington, Seattle, WA 98105 USA{janara, mausam, etzioni}@cs.washington.eduAbstractWe query Web Image search engines withwords (e.g., spring) but need images thatcorrespond to particular senses of the word(e.g., flexible coil).
Querying with poly-semous words often yields unsatisfactoryresults from engines such as Google Im-ages.
We build an image search engine,IDIOM, which improves the quality of re-turned images by focusing search on thedesired sense.
Our algorithm, instead ofsearching for the original query, searchesfor multiple, automatically chosen trans-lations of the sense in several languages.Experimental results show that IDIOM out-performs Google Images and other com-peting algorithms returning 22% more rel-evant images.1 IntroductionOne out of five Web searches is an image search(Basu, 2009).
A large subset of these searchesis subjective in nature, where the user is lookingfor different images for a single concept (Linsley,2009).
However, it is a common user experiencethat the images returned are not relevant to the in-tended concept.
Typical reasons include (1) exis-tence of homographs (other words that share thesame spelling, possibly in another language), and(2) polysemy, several meanings of the query word,which get merged in the results.For example, the English word ?spring?
has sev-eral senses ?
(1) the season, (2) the water body, (3)spring coil, and (4) to jump.
Ten out of the first fif-teen Google images for spring relate to the seasonsense, three to water body, one to coil and none tothe jumping sense.
Simple modifications to querydo not always work.
Searching for spring waterresults in many images of bottles of spring waterand searching for spring jump returns only threeimages (out of fifteen) of someone jumping.Polysemous words are common in English.
Itis estimated that average polysemy of English ismore than 2 and average polysemy of commonEnglish words is much higher (around 4).
Thus,it is not surprising that polysemy presents a signif-icant limitation in the context of Web Search.
Thisis especially pronounced for image search wherequery modification by adding related words maynot help, since, even though the new words mightbe present on the page, they may not be all associ-ated with an image.Recently Etzioni et al (2007) introduced PAN-IMAGES, a novel approach to image search, whichpresents the user with a set of translations.
E.g., itreturns 38 translations for the coil sense of spring.The user can query one or more translations to getthe relevant images.
However, this method putsthe onus of choosing a translation on the user.
Atypical user is unaware of most properties of lan-guages and has no idea whether a translation willmake a good query.
This results in an added bur-den on the user to try different translations beforefinding the one that returns the relevant images.Our novel system, IDIOM, removes this addi-tional burden.
Given a desired sense it automati-cally picks the good translations, searches for as-sociated images and presents the final images tothe user.
For example, it automatically queries theFrench ressort when looking for images of springcoil.
We make the following contributions:?
We automatically learn a predictor for "good"translations to query given a desired sense.
Agood translation is one that is monosemousand is in a major language, i.e., is expected toyield a large number of images.?
Given a sense we run our predictor on all itstranslations to shortlist a set of three transla-tions to query.?
We evaluate our predictor by comparing theimages that its shortlists return against the193images that several competing methods re-turn.
Our evaluation demonstrates that ID-IOM returns at least one good image for 35%more senses (than closest competitor) andoverall returns 22% better images.2 BackgroundIDIOM makes heavy use of a sense disambiguated,vastly multilingual dictionary called PANDIC-TIONARY (Mausam et al, 2009).
PANDIC-TIONARY is automatically constructed by prob-abilistic inference over a graph of translations,which is compiled from a large number of multi-lingual and bilingual dictionaries.
For each sensePANDICTIONARY provides us with a set of trans-lations in several languages.
Since it is gener-ated by inference, some of the asserted transla-tions may be incorrect ?
it additionally associatesa probability score with each translation.
Forour work we choose a probability threshold suchthat the overall precision of the dictionary is 0.9(evaluated based on a random sample).
PANDIC-TIONARY has about 80,000 senses and about 1.8million translations at precision 0.9.We use Google Image Search as our underlyingimage search engine, but our methods are indepen-dent of the underlying search engine used.3 The IDIOM AlgorithmAt the highest level IDIOM operates in three mainsteps: (1) Given a new query q it looks up its vari-ous senses in PANDICTIONARY.
It displays thesesenses and asks the user to select the intendedsense, sq.
(2) It runs Algorithm 1 to shortlist threetranslations of sqthat are expected to return highquality images.
(3) It queries Google Images us-ing the three shortlisted translations and displaysthe images.
In this fashion IDIOM searches forimages that are relevant to the intended conceptas opposed to using a possibly ambiguous query.The key technical component is the second step?
shortlisting the translations.
We first use PAN-DICTIONARY to acquire a set of high probabilitytranslations of sq.
We run each of these transla-tions through a learned classifier, which predictswhether it will make a good query, i.e., whetherwe can expect images relevant to this sense ifqueried using this translation.
The classifier ad-ditionally outputs a confidence score, which weuse to rank the various translations.
We pick thetop three translations, as long as they are above aminimum confidence score, and return those as theshortlisted queries.
Algorithm 1 describes this asa pseudo-code.Algorithm 1 findGoodTranslationsToQuery(sq)1: translations = translations of sqin PANDICTIONARY2: for all w ?
translations do3: pd = getPanDictionaryFeatures(w, sq)4: g = getGoogleFeatures(w, sq)5: conf[w] = confidence in Learner.classify(pd, g)6: sort all words w in decreasing order of conf scores7: return top three w from the sorted list3.1 Features for ClassifierWhat makes a translation w good to query?
Adesired translation is one that (1) is in a high-coverage language, so that the number of imagesreturned is large, (2) monosemously expresses theintended sense sq, or at least has this sense asits dominant sense, and (3) does not have homo-graphs in other languages.
Such a translation isexpected to yield images relevant to only the in-tended sense.
We construct several features thatprovide us evidence for these desired characteris-tics.
Our features are automatically extracted fromPANDICTIONARY and Google.For the first criterion we restrict the transla-tions to a set of high-coverage languages includ-ing English, French, German, Spanish, Chinese,Japanese, Arabic, Russian, Korean, Italian, andPortuguese.
Additionally, we include the lan-guage as well as number of documents returned byGoogle search of w as features for the classifier.To detect if w is monosemous we add a featurereflecting the degree of polysemy of w: the num-ber of PANDICTIONARY senses thatw belongs to.The higher this number the more polysemous wis expected to be.
We also include the number oflanguages that have w in their vocabulary, thus,adding a feature for the degree of homography.PANDICTIONARY is arranged such that eachsense has an English source word.
If the sourceword is part of many senses but sqis much morepopular than others or sqis ordered before theother senses then we can expect sqto be the dom-inant sense for this word.
We include features likesize of the sense and order of the sense.Part of speech of sqis another feature.
Finallywe also add the probability score that w is a trans-lation of sqin our feature set.3.2 Training the ClassifierTo train our classifier we used Weka (Witten andFrank, 2005) on a hand labeled dataset of 767 ran-1940 100 200 300 4000.000.100.20Number of Good Images ReturnedPrecision IDIOMSWSW+GRSW+RIDIOM SW SW+G SW+R RPercentage Correct0204060IDIOM SW SW+G SW+R RPercentage Correct0204060Figure 1: (a): Precision of images vs. the number of relevant images returned.
IDIOM covers the maximum area.
(b,c) Thepercentage of senses for which at least one relevant result was returned, for (b) all senses and (c) for minor senses of the queries.domly chosen word sense pairs (e.g., pair of ?pri-mavera,?
and ?the season spring?).
We labeled apair as positive if googling the word returns at leastone good image for the sense in the top three.
Wecompared performance among a number of ma-chine learning algorithms and found that RandomForests (Breiman, 2001) performed the best over-all with 69% classification accuracy using ten foldcross validation versus 63% for Naive Bayes and62% for SVMs.
This high performance of Ran-dom Forests mirrors other past experiments (Caru-ana and Niculescu-Mizil, 2006).Because of the ensemble nature of RandomForests it is difficult to inspect the learned clas-sifier for analysis.
Still, anecdotal evidence sug-gests that the classifier is able to learn an effectivemodel of good translations.
We observe that it fa-vors English whenever the English word is part ofone or few senses ?
it picks out auction when thequery is ?sale?
in the sense of ?act of putting upfor auction to highest bidder".
In cases where En-glish is more ambiguous it chooses a relatively lessambiguous word in another language.
It choosesthe French word ressort for finding ?spring?
in thesense of coil.
For the query ?gift?
we notice that itdoes not choose the original query.
This matchesour intuition, since gift has many homographs ?the German word ?Gift?
means poison or venom.4 ExperimentsCan querying translations instead of the originalquery improve the quality of image search?
If so,then how much does our classifier help comparedto querying random translations?
We also analyzeour results and study the variation of image qual-ity along various dimensions, like part of speech,abstractness/concreteness of the sense, and ambi-guity of the original query.As a comparison, we are interested in how ID-IOM performs in relation to other methods forquerying Google Images.
We compare IDIOM toseveral methods.
(1) Source Word (SW): Queryingwith only the source word.
This comparison func-tions as our baseline.
(2) Source Word + Gloss(SW+G): Querying with the source word and thegloss for the sense1.
This method is one way to fo-cus the source word towards the desired sense.
(3)Source Word + Random (SW+R): Querying withthree pairs of source word and a random transla-tion.
This is another natural way to extend thebaseline for the intended sense.
(4) Random (R):Querying with three random translations.
Thistests the extent to which our classifier improvesour results compared to randomly choosing trans-lations shown to the user in PANIMAGES.We randomly select fifty English queries fromPANDICTIONARY and look up all senses contain-ing these in PANDICTIONARY, resulting in a totalof 134 senses.
These queries include short wordsequences (e.g., ?open sea?
), mildly polysemousqueries like ?pan?
(means Greek God and cookingvessel) and highly polysemous ones like ?light?.For each sense of each word, we query GoogleImages with the query terms suggested by eachmethod and evaluate the top fifteen results.
Formethods in which we have three queries, we eval-uate the top five results for each query.
We evalu-ate a total of fifteen results because Google Imagesfits fifteen images on each page for our screen size.Figure 1(a) compares the precision of the fivemethods with the number of good images re-turned.
We vary the number of images in con-sideration from 1 to 15 to generate various pointsin the graph.
IDIOM outperforms the others bywide margins overall producing a larger number ofgood images and at higher precision.
Surprisingly,the closest competitor is the baseline method asopposed to other methods that try to focus thesearch towards the intended sense.
This is prob-ably because the additional words in the query (ei-ther from gloss or a random translation) confuseGoogle Images rather than focusing the search.IDIOM covers 41% more area than SW. Overall1PANDICTIONARY provides a gloss (short explanation)for each sense.
E.g., a gloss for ?hero?
is ?role model.
?1951 sense 2 or 3 senses >3 sensesPercentage Correct020406080 IDIOMSWSW+GSW+RRNoun Verb AdjectivePercentage Correct020406080 IDIOMSWSW+GSW+RRConcrete AbstractPercentage Correct020406080 IDIOMSWSW+GSW+RRFigure 2: The percentage of senses for which at least one relevant result was returned varied along several dimensions: (a)polysemy of original query, and (b) part of speech of the sense, (c) abstractness/concreteness of the sense.IDIOM produces 22% better images compared toSW (389 vs 318).We also observe that random translations returnmuch worse images than IDIOM suggesting that aclassifier is essential for high quality images.Figure 1(b) compares the percentage of sensesfor which at least one good result was returned inthe fifteen.
Here IDIOM performs the best at 51%.Each other method performs at about 40%.
The re-sults are statistically highly significant (p < 0.01).Figure 1(c) compares the performance just onthe subset of the non-dominant senses of the querywords.
All methods perform worse than in Figure1(b) but IDIOM outperforms the others.We also analyze our results across several di-mensions.
Figure 2(a) compares the performanceas a function of polysemy of the original query.
Asexpected, the disparity in methods is much morefor high polysemy queries.
Most methods performwell for the easy case of unambiguous queries.Figure 2(b) compares along the different partsof speech.
For nouns and verbs, IDIOM returns thebest results.
For adjectives, IDIOM and SW per-form the best.
Overall, nouns are the easiest forfinding images and we did not find much differ-ence between verbs and adjectives.Finally, Figure 2(c) reports how the methodsperform on abstract versus concrete queries.
Wedefine a sense as abstract if it does not have a nat-ural physical manifestation.
For example, we clas-sify ?nest?
(a bird built structure) as concrete, and?confirm?
(to strengthen) as abstract.
IDIOM per-forms better than the other methods, but the resultsvary massively between the two categories.Overall, we find that our new system consis-tently produces better results across the several di-mensions and various metrics.5 Related Work and ConclusionsRelated Work: The popular paradigm for imagesearch is keyword-based, but it suffers due to pol-ysemy and homography.
An alternative paradigmis content based (Datta et al, 2008), which is veryslow and works on simpler images.
The fieldof cross-lingual information retrieval (Ballesterosand Croft, 1996) often performs translation-basedsearch.
Other than PANIMAGES (which we out-perform), no one to our knowledge has used thisfor image search.Conclusions: The recent development of PAN-DICTIONARY (Mausam et al, 2009), a sense-distinguished, massively multilingual dictionary,enables a novel image search engine called ID-IOM.
We show that querying unambiguous trans-lations of a sense produces images for 35% moreconcepts compared to querying just the Englishsource word.
In the process we learn a classi-fier that predicts whether a given translation is agood query for the intended sense or not.
Weplan to release an image search website basedon IDIOM.
In the future we wish to incorporateknowledge from WordNet and cross-lingual linksin Wikipedia to increase IDIOM?s coverage beyondthe senses from PANDICTIONARY.ReferencesL.
Ballesteros and B. Croft.
1996.
Dictionary methods forcross-lingual information retrieval.
In DEXA Conferenceon Database and Expert Systems Applications.Dev Basu.
2009.
How To Leverage Rich Me-dia SEO for Small Businesses.
In Search En-gine Journal.
http://www.searchenginejournal.com/rich -media-small-business-seo/9580.L.
Breiman.
2001.
Random forests.
Machine Learning,45(1):5?32.R.
Caruana and A. Niculescu-Mizil.
2006.
An empiri-cal comparison of supervised learning algorithms.
InICML?06, pages 161?168.R.
Datta, D. Joshi, J. Li, and J. Wang.
2008.
Image retrieval:Ideas, influences, and trends of the new age.
ACM Com-puting Surveys, 40(2):1?60.O.
Etzioni, K. Reiter, S. Soderland, and M. Sammer.
2007.Lexical translation with application to image search on theWeb.
In Machine Translation Summit XI.Peter Linsley.
2009.
Google Image Search.
In SMX West.Mausam, S. Soderland, O. Etzioni, D. Weld, M. Skinner, andJ.
Bilmes.
2009.
Compiling a massive, multilingual dic-tionary via probabilistic inference.
In ACL?09.I.
Witten and E. Frank.
2005.
Data Mining: Practical Ma-chine Learning Tools and Techniques.
Morgan Kaufmann.196
