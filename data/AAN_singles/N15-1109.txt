Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1036?1041,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUnsupervised Code-Switching forMultilingual Historical Document TranscriptionDan Garrette?Hannah Alpert-Abrams?Taylor Berg-Kirkpatrick?Dan Klein?
?Department of Computer Science, University of Texas at Austin, dhg@cs.utexas.edu?Comparative Literature Program, University of Texas at Austin, halperta@gmail.com?Computer Science Division, University of California at Berkeley, {tberg,klein}@cs.berkeley.eduAbstractTranscribing documents from the printingpress era, a challenge in its own right, ismore complicated when documents interleavemultiple languages?a common feature of16th century texts.
Additionally, many ofthese documents precede consistent ortho-graphic conventions, making the task evenharder.
We extend the state-of-the-art his-torical OCR model of Berg-Kirkpatrick et al(2013) to handle word-level code-switchingbetween multiple languages.
Further, we en-able our system to handle spelling variabil-ity, including now-obsolete shorthand systemsused by printers.
Our results show average rel-ative character error reductions of 14% acrossa variety of historical texts.1 IntroductionTranscribing documents printed on historical print-ing presses poses a number of challenges for OCRtechnology.
Berg-Kirkpatrick et al (2013) presentedan unsupervised system, called Ocular, that han-dles the types of noise that are characteristic of pre-20th century documents and uses a fixed monolin-gual language model to guide learning.
While thisapproach is highly effective on English documentsfrom the 18th and 19th centuries, problems arisewhen it is applied to older documents that featurecode-switching between multiple languages and ob-solete orthographic characteristics.In this work, we address these issues by devel-oping a new language model for Ocular.
First, tohandle multilingual documents, we replace Ocular?ssimple n-gram language model with an unsuper-vised model of intrasentential code-switching thatallows joint transcription and word-level languageidentification.
Second, to handle orthographic vari-ation, we provide an interface that allows individ-uals familiar with relevant languages to guide thelanguage model with targeted orthographic informa-tion.
As a result, our system handles inconsistentspelling, punctuation, and diacritic usage, as well asnow-obsolete shorthand conventions used by print-ers.We evaluate our model using documents from thePrimeros Libros project, a digital archive of booksprinted in the Americas prior to 1601 (Dolan, 2012).These texts, written in European and indigenous lan-guages, often feature as many as three languageson a single page, with code-switching occurring onthe chapter, sentence, and word level.
Orthographicvariations are pervasive throughout, and are particu-larly difficult with indigenous languages, for whichwriting systems were still being developed.Our results show improvements across a range ofdocuments, yielding an average 14% relative charac-ter error reduction over the previous state-of-the-art,with reductions as high as 27% on particular texts.2 DataWriting during the early modern period in Europewas characterized by increasing use of vernacu-lar languages alongside Latin, Greek, and Hebrew.In the colonies, this was matched by the develop-ment of grammars and alphabetic writing systemsfor indigenous languages (see Eisenstein (1979) andMignolo (1995)).
In all cases, orthographies wereregionally variable and subject to the limited re-sources of the printing houses; this is particularlytrue in the Americas, where resources were scarce,1036ligature diacriticnon-standard character obsoletespelling elision characterInput:Language model: praesertim urgente causaFigure 1: An example OCR input showing the origi-nal image and an example of an equivalent modern-ized text similar to data used to train the LM.and where indigenous-language orthographies werefirst being developed (Baddeley and Voeste, 2013).The 349 digital facsimiles in the Primeros Li-bros collection are characteristic of this trend.
Pro-duced during the first century of Spanish coloniza-tion, they represent the introduction of printing tech-nology into the Americas, and reflect the (sometimesconflicted) priorities of the nascent colony, from re-ligious orthodoxy to conversion and education.For our experiments, we focus on multilingualdocuments in three languages: Spanish, Latin, andNahuatl.
As Berg-Kirkpatrick et al (2013) show, alanguage model built on contemporaneous data willperform better than modern data.
For this reason, wecollected 15?17th century texts from Project Guten-berg,1producing Spanish and Latin corpora of morethan one million characters each.
Due to its relativescarcity, we augmented the Nahuatl corpus with aprivate collection of transcribed colonial documents.3 Baseline SystemThe starting point for our work is the Ocular systemdescribed by Berg-Kirkpatrick et al (2013).
Thefonts used in historical documents are usually un-known and can vary drastically from document todocument.
Ocular deals with this problem by learn-ing the font in an unsupervised fashion ?
directlyfrom the input historical document.
In order to ac-complish this, the system uses a specialized gener-ative model that reasons about the main sources ofvariation and noise in historical printing.
These in-clude the shapes of the character glyphs, the hor-izontal spacing between characters, and the verti-1http://www.gutenberg.org/cal offset of each character from a common base-line.
Additionally, since documents exhibit variableinking levels (where individual characters are oftenfaded or smeared with blotched ink) the system alsomodels the amount of ink applied to each type piece.The generative process operates as follows.
First,a sequence of character tokens is generated by acharacter n-gram language model.
Then, boundingboxes for each character token are generated, con-ditioned on the character type, followed by verti-cal offsets and inking levels.
Finally, the pixels ineach bounding box are generated, conditioned on thecharacter types, vertical offsets, and inking levels.In this work, we focus on improving the languagemodel, and leave the rest of the generative processuntouched.4 Language ModelWe present a new language model for Ocular that isdesigned to handle issues that are characteristic ofolder historical documents: code-switching and or-thographic variability.
We extend the conventionalcharacter n-gram language model and its trainingprocedure to deal with each of these problems inturn.4.1 Code-SwitchingBecause Ocular?s character n-gram languagemodel (LM) is fixed and monolithic, even when it istrained on corpora from multiple languages, it treatsall text as a single ?language?
?a multilingual blurat best.
As a result, the system cannot model the factthat different contiguous blocks of text correspondto specific languages and thus follow specific statis-tical patterns.
In order to transcribe documents thatfeature intrasentential code-switching, we replaceOcular?s simple n-gram LM with one that directlymodels code-switching by representing languagesegmentation as a latent variable.Our code-switching LM generates a sequence ofpairs (ei,`i) where eiis the current character and `iisthe current language.
The sequence of languages `ispecifies the segmentation of generated text into lan-guage regions.
Our LM is built from several compo-nent models: First, for each language type `, we in-corporate a standard monolingual character n-grammodel trained on data from language `.
The com-1037ponent model corresponding to language ` is calledPCHAR`.
Second, our LM also incorporates a modelthat governs code-switching between languages.
Wecall this model PLANG.
The generative process forour LM works as follows.
For the ith character posi-tion, we first generate the current language `icondi-tioned on the previous character ei?1and the previ-ous language `i?1using PLANG.
Then, conditionedon the current language liand the previous n ?
1characters, we generate the current character eius-ing PCHAR`i.
This means that the probability of pair(ei,`i) given its context is computed as:PLANG(`i| ei?1, `i?1) ?
PCHAR`i(ei| ei?1... ei?n+1)We parameterize PLANGin a way that enforcestwo constraints.
First, to ensure that each word isassigned a single language, we only allow languagetransitions for characters directly following whites-pace (a space character or a page margin, unless thecharacter follows a line-end hyphen).
Second, toresist overly frequent code-switching (and encour-age longer spans), we let a Bernoulli parameter ?specify the probability of choosing to draw a newlanguage at a word boundary (instead of determin-istically staying in the same language).
By setting?
low, we indicate a strong belief that languageswitches should be infrequent, while still allowing aswitch if sufficient evidence is found in the image.2Finally, we parameterize the frequency of each lan-guage in the text.
Specifically, for each language `,a multinomial parameter ?`specifies the probabilityof transitioning to ` when you draw a new language.We learn this group of multinomial parameters, ?`for each language, in an unsupervised fashion, andin doing so, adapt to the proportions of languagesfound in the particular input document.
Thus, usingour parameterization, the probability of transitioningfrom language ` to `?, given previous character e, is:PLANG(`?| e, `) =???????????(1?
?)
+ ?
?
?`?if e = space and ` = `??
?
?`?if e = space and ` 6= `?1 if e 6= space and ` = `?0 if e 6= space and ` 6= `?2We use ?
= 10?6across all experiments.original ?
replacement`a a?a aque ?qper ?pce zex jj xan ?a?space?h ?space?be veu vv uoracion o?ronTable 1: An example subset of the orthographic re-placement rules for Spanish.Finally, because our code-switching LM usesmultiple separate language-specific n-gram modelsPCHAR`, we are able to maintain a distinct set of validcharacters for each language.
By restricting eachlanguage?s model to the set of characters in the cor-pus for that language, we can push the model awayfrom incompatible languages during transcription ifit is confident about certain rare characters, and limitthe search space by reducing the number of charac-ter combinations considered for any given position.We also include, for all languages, a set of punctua-tion symbols such as ?
and ?
that appear in printedbooks but not in the LM training data.4.2 Orthographic VariabilityThe component monolingual n-gram LMs must betrained on monolingual corpora in their respectivelanguages.
However, due to the lack of codifiedorthographic conventions concerning spelling, dia-critic usage, and spacing, compounded by the liberaluse of now-obsolete shorthand notations by print-ers, statistics gleaned from available modern corporaprovide a poor representation of the language used inthe printed documents.
Even 16th century texts onProject Gutenberg tend to be written, for the benefitof the reader, using modern spellings.
The discon-nect between the orthography of the original docu-ments and modern texts can be seen in Figure 1.
Toaddress these issues, we introduced an interface for1038author Gante Anunciaci?on Sahag?un Rinc?on Bautista Macro Averagepub.
year 1553 1565 1583 1595 1600 WERCER WER CER WER CER WER CER WER CER WER CER WER w.p.Ocular 13.7 55.9 15.7 53.6 10.8 44.3 11.6 38.4 9.7 25.7 12.3 43.6 56.6+code-switch 12.8 55.0 14.6 53.8 9.6 38.7 10.7 35.4 8.8 24.5 11.3 41.5 53.5+orth.
var.
13.5 55.3 14.1 51.6 8.4 34.9 9.5 31.0 7.1 18.2 10.5 38.2 51.0Table 2: Experimental results for each book, and average across all books.
Columns show Character ErrorRate (CER) or Word Error Rate (WER; excluding punctuation).
The final column gives the average WERincluding punctuation (w.p.).
The Ocular row is the previous state-of-the-art: Berg-Kirkpatrick et al (2013).The second row uses our code-switching model, and the third additionally handles orthographic variability.Gan.
(1553)Anu.
(1565)Sah.
(1583)Rin.
(1595)Bau.
(1600)Table 3: An example line from each test book.incorporating orthographic variability into the train-ing procedure for the component LMs.For our experiments, we built Latin, Nahuatl, andSpanish variability rulebanks by asking language ex-perts to identify spelling anomalies from among sev-eral sample pages from Primeros Libros documents,and specify rewrite rules that map modern spellingsback to variant spellings; we also drew on data frompaleographic textbooks.
Example rules can be seenin Table 1.
These rules are used to rewrite corpustext before the LMs are trained; for instance, everynth occurrence of en in the Spanish corpus might berewritten as ?e.
This approach reintroduces histori-cally accurate orthographic variability into the LM.5 ExperimentsWe compare to Ocular, the state of the art for his-torical OCR.3Since Ocular only supports monolin-gual English OCR, we added support for alterna-tive alphabets, including diacritics and ligatures, andtrained a single mixed-language model on a com-bined Spanish/Latin/Nahuatl corpus.We evaluate our model on five different booksfrom the Primeros Libros collection, representing avariety of printers, presses, typefaces, and authors(Table 3).
Each book features code-switching be-3http://nlp.cs.berkeley.edu/projects/ocular.shtmltween Spanish, Latin, and Nahuatl.
For each book, afont was trained on ten (untranscribed) pages usingunsupervised learning procedure described by Berg-Kirkpatrick et al (2013).
The font was evaluated ona separate set of ten pages, manually transcribed.46 Results and AnalysisOur overall results (Table 2) show improvements onevery book in our evaluation, achieving as high as29% relative word-error (WER) reduction.Replacing Ocular?s single mixed-language LMwith our unsupervised code-switch model results inimmediate improvements.
An example of transcrip-tion output, including the language-assignmentsmade by the model, can be seen in Figure 2.Further improvements are seen by handling ortho-graphic variation.
Figure 3 gives an example of howa single spelling variation can lead to a cascade oftranscription errors.
Here, the baseline system, con-fused by the elision of the letter n in the word m?etira(from mentira, ?lie?
), transcribed it with an entirelydifferent word (merita, ?merit?).
When our handlingof alternate spellings is employed, the LM has goodstatistics for character sequences including the char-acter ?e, and is able to decode the word correctly.There are several explanations for the differencesin results among the five evaluation books.
First, thetwo oldest texts, Gante and Anunciaci?on, use Gothicfonts that are more difficult to read and feature capi-tal letters that are nearly impossible for the model torecognize (see Table 3).
This contributes to the highcharacter error rates for those books.Second, the word error rate metric is complicatedby the inconsistent use of spaces in Nahuatl writ-4Hyperparameters were set to be consistent with Berg-Kirkpatrick et al (2013).1039Ay proprio vocablo de logr?o, que es tetech -tlaixtlapanaliztli, tetechtla miec caquixtiliztli,y para dezir di te a logro?
Cuix tetech otitlaix-Figure 2: A passage with Spanish/Nahuatl code-switching, and our model?s language-coded output.
(Spanish in blue; Nahuatl in red/italics.
)no variation handling mentira meritahandling variation mentira m?etiraFigure 3: Two variants of the same word (mentira),pulled from the same page of text.
The form men-tira appears in the LM training corpus, but the short-hand m?etira does not.
Without special handling, themodel does not know that m?etira is valid.ing, falsely claiming ?word?
errors when all charac-ters are correct.
Use of spaces is not standardizedacross the printed books, or across the digitized LMtraining corpora, and is still in fact a contested is-sue among modern Nahuatl scholars.
While it is im-portant for the transcription process to insert spacesappropriately into the Spanish and Latin text (evenwhen the printer left little, as with ypara in Figure 2),it is difficult to assess what it means for a space tobe ?correctly?
inserted into Nahuatl text.
Rinc?on andBautista contain relatively less Nahuatl text and areaffected less by this problem.A final source of errors arises when our model?corrects?
the original document to match modernconventions, as with diacritics, whose usages wereless conventionalized at the time these books wereprinted.
For example, the string numero is oftentranscribed as n?umero, the correct modern spelling.7 Conclusions and Future WorkWe have demonstrated an unsupervised OCR modelthat improves upon Berg-Kirkpatrick et al (2013)?sstate-of-the-art Ocular system in order to effectivelyhandle the code-switching and orthographic vari-ability prevalent in historical texts.
In addition totranscribing documents, our system also implicitlyassigns language labels to words, allowing their us-age in downstream tasks.
We have also presented anew corpus, with transcriptions, for the evaluationof multilingual historical OCR systems.Our system, as currently designed, attempts tofaithfully transcribe text.
However, for the purposesof indexability and searchability of these documents,it may be desirable to also produce canonicalizedtranscriptions, for example collapsing spelling vari-ants to their modern forms.
Fortunately, this canbe done in our approach by running the variabilityrewrite rules ?backward?
as a post-processing step.Further technical improvements may be made byhaving the system automatically attempt to boot-strap the identification of spelling variants, a pro-cess that could complement our approach throughan active learning setup.
Additionally, since evenour relatively simple unsupervised code-switch lan-guage modeling approach yielded improvements toOCR performance, it may be justified to attempt theadaptation of more complex code-switch recogni-tion techniques (Solorio et al, 2014).The automatic transcription of the Primeros Li-bros collection has significant implications forscholars of the humanities interested in the role thatinscription and transmission play in colonial history.For example, there are parallels between the waythat the Spanish transformed indigenous languagesinto Latin-like writing systems (removing ?noise?like phonemes that do not exist in Latin), and theway that the OCR tool transforms historical printeddocuments into unicode (removing ?noise?
like arti-facts of the printing process and physical changesto the pages); in both instances, arguably impor-tant information is lost.
We present some of theseideas at the American Comparative Literature As-sociation?s annual meeting, where we discuss therelationship between sixteenth century indigenousorthography and Ocular?s code-switching languagemodels (Alpert-Abrams and Garrette, 2015).AcknowledgementsWe would like to thank Stephanie Wood, Kelly Mc-Donough, Albert Palacios, Adam Coon, and SergioRomero, as well as Kent Norsworthy for their input,advice, and assistance on this project.1040ReferencesHannah Alpert-Abrams and Dan Garrette.
2015.
Read-ing Primeros Libros: From archive to OCR.
In Pro-ceedings of The Annual Meeting of the American Com-parative Literature Association.Susan Baddeley and Anja Voeste.
2013.
Orthographiesin Early Modern Europe.
De Gruyter.Taylor Berg-Kirkpatrick and Dan Klein.
2014.
Improvedtypesetting models for historical OCR.
In Proceedingsof ACL.Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.2013.
Unsupervised transcription of historical docu-ments.
In Proceedings of ACL.Thomas G. Dolan.
2012.
The Primeros Libros Project.The Hispanic Outlook in Higher Education, 22:20?22,March.Elizabeth L. Eisenstein.
1979.
The printing press as anagent of change.
Cambridge University Press.Walter Mignolo.
1995.
The Darker Side of the Renais-sance.
University of Michigan Press.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, StevenBethard, Mona Diab, Mahmoud Gohneim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirschberg, AlisonChang, and Pascale Fung.
2014.
Overview for thefirst shared task on language identification in code-switched data.
In Proceedings of The First Workshopon Computational Approaches to Code Switching.1041
