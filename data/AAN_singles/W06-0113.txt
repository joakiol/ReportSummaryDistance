Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 94?101,Sydney, July 2006. c?2006 Association for Computational LinguisticsA SVM-based Model for Chinese Functional Chunk ParsingYingze ZhaoState Key Laboratory of Intelligent Technol-ogy and SystemsDept.
of Computer Science and Technology,Tsinghua UniversityBeijing 100084, P. R. Chinazhaoyingze@gmail.comQiang ZhouState Key Laboratory of Intelligent Technol-ogy and SystemsDept.
of Computer Science and Technology,Tsinghua UniversityBeijing 100084, P. R. Chinazq-lxd@mail.tsinghua.edu.cnAbstractFunctional chunks are defined as a seriesof non-overlapping, non-nested segmentsof text in a sentence, representing the im-plicit grammatical relations between thesentence-level predicates and their argu-ments.
Its top-down scheme and com-plexity of internal constitutions bring in anew challenge for automatic parser.
Inthis paper, a new parsing model is pro-posed to formulate the complete chunk-ing problem as a series of boundary de-tection sub tasks.
Each of these sub tasksis only in charge of detecting one type ofthe chunk boundaries.
As each sub taskcould be modeled as a binary classifica-tion problem, a lot of machine learningtechniques could be applied.In our experiments, we only focus onthe subject-predicate (SP) and predicate-object (PO) boundary detection sub tasks.By applying SVM algorithm to these subtasks, we have achieved the best F-Scoreof 76.56% and 82.26% respectively.1 IntroductionParsing is a basic task in natural language proc-essing; however, it has not been successful inachieving the accuracy and efficiency requiredby real world applications.
As an alternative,shallow parsing or partial parsing has been pro-posed to meet the current needs by obtainingonly a limited amount of syntactic informationneeded by the application.
In recent years, therehas been an increasing interest in chunk parsing.From CoNLL-2000 to CoNLL-2005, a lot of ef-forts have been made in the identification of ba-sic chunks and the methods of combining themfrom bottom-up to form large, complex units.
Inthis paper, we will apply functional chunks toChinese shallow parsing.Functional chunks are defined as a series ofnon-overlapping, non-nested functional units in asentence, such as subjects, predicates, objects,adverbs, complements and so on.
These unitsrepresent the implicit grammatical relations be-tween the sentence-level predicates and their ar-guments.
Different from the basic chunks de-fined by Abney (1991), functional chunks aregenerated from a top-down scheme, and thustheir constitutions may be very complex.
In addi-tion, the type of a functional chunk could not besimply determined by its constitution, but de-pends heavily on the context.
Therefore, we willhave new challenges in the functional chunkparsing.Ramshaw and Marcus (1995) first introducedthe machine learning techniques to chunkingproblem.
By formulating the NP-chunking taskas a tagging process, they marked each wordwith a tag from set {B, I, O}, and successfullyapplied TBL to it.
Inspired by their work, weintroduce SVM algorithm to our functionalchunking problem.
Instead of using the BIO tag-ging system, we propose a new model for solv-ing this problem.
In this model, we do not tag thewords with BIO tags, but directly discover thechunk boundaries between every two adjacentfunctional chunks.
Each of these chunk bounda-ries will be assigned a type to it, which containsthe information of the functional chunk typesbefore and after it.
Then we further decomposethis model into a series of sub modules, each ofwhich is in charge of detecting only one type of94the chunk boundaries.
As each sub module canbe modeled as a binary classifier, various ma-chine learning techniques could be applied.In our experiments, we focus on the subject-predicate (SP) and predicate-object (PO) bound-ary detection tasks, which are the most difficultbut important parts in our parsing model.
By ap-plying SVM algorithm to these tasks, we achievethe best F-Score of 76.56% and 82.26% respec-tively.This paper is organized as follows.
In section2, we give a brief introduction to the concept ofour functional chunks.
In section 3, we proposethe parsing model for Chinese functional chunkparsing.
In section 4, we compare SVM with sev-eral other machine learning techniques, and illus-trate how competitive SVM is in our chunkingtask.
In section 5, we build 2 sub modules basedon SVM algorithm for SP and PO boundary de-tection tasks.
In section 6, some related work onfunctional chunk parsing is introduced.
Section 7is the conclusion.2 Functional Chunk SchemeFunctional chunks are defined as a series ofnon-overlapping, non-nested segments of text atthe sentence level without leaving any wordsoutside.
Each chunk is labeled with a functionaltag, such as subject, predicate, object and so on.These functional chunks in the sentence form alinear structure within which the grammaticalrelations between sentence-level predicates andtheir arguments or adjuncts are kept implicitly.Table 1 lists all the tags used in our functionalchunk scheme:Table 1.
Functional Chunk Tag Set.Chunk Tag Basic Function DescriptionS SubjectP PredicateO ObjectJ Raised ObjectD Adverbial adjunctC ComplementT Independent constituentY Modal particleHere, we list some examples to illustrate howthese functional tags are used in Chinese sen-tences.1.
?
[D ??
/t (afternoon)  ?
/?
[D ?
/p(when)  ?/rN (I) ?
?/v (come to)  ???
?/nS (Xi Bai Po village) ?
?/s (eastern entrance)?/n  ?/?
[D ?/d (already) [P ?/v (there is)[J ?/m  ?/qN (a) ?
?/n (brainman) [D ?/p?
?/rS (there) [P ?
?/v (waiting) [Y ?/y ?/?
?2.
?
[T ??
?/l (frankly speaking)  ?/?
[S?/rN (that) [P ?/vC (was) [O ?/rN (I) ?
?/d(lifetime) ?/dN  ?/vM (can?t) ?
?/v (forget)?/u  ?/?
?3.
?
[S ?
?/n (time) [P ?
?/v  ?/u (schedule)[C ?/dD (very) ?/a (tight) ?/?
?Compared with the basic chunk scheme de-fined by Abney (1991), our functional chunkscheme has the following two main differences:(1) Functional chunks are not constituted frombottom-up, but generated from top-down, thussome functional chunks are usually longer andmore complex than the basic chunks.We have a collection of 185 news files as ourfunctional chunk corpus.
Each file is manuallyannotated with functional chunks.
There areabout 200,000 Chinese words in the corpus.
Toinvestigate the complex constitutions of func-tional chunks, we list the average chunk lengths(ACL) of different types in Table 2:Table 2.
Average Chunk Lengths of DifferentTypes.Chunk Type Count Word Sum ACLP 21988 27618 1.26D 19795 46919 2.37O 14289 61401 4.30S 11920 34479 2.89J 855 2083 2.44Y 594 604 1.02T 407 909 2.23C 244 444 1.82From the table above, we can find that Ochunk has the longest average length of 4.30words, and S chunk has the second longest aver-age length of 2.89 words, and D chunk has anaverage length of 2.37 words.
Although the aver-age length doesn?t seem so long, the length of aspecific chunk varies greatly.In Table 3, we list some detailed length distri-butional data of three chunks.Table 3.
Length Distribution of S, O and DChunks.Chunk Length # of S # of O # of D1 5322 3537 121472 2093 2228 24993 1402 2117 14314 917 1624 10105 627 1108 696>5 1559 3675 2013Sum 11920 14289 1979695From the table above, we can find that thereare totally 1559 S chunks with a length of morethan 5 words which takes up 13.08% of the totalnumber.
And when we refer to the S chunks withmore than 3 words, the percentage will increaseto 26.03%.
These long chunks are usually consti-tuted with several complex phrases or clauses asthe modifiers of a head word.
Among the Ochunks, 25.72% of them have a length of morethan 5 words, and 44.84% of them are longerthan 3 words.
The reason why O chunks have alonger length may be that many of them containthe entire clauses.
Although most of the Dchunks are less than 5 words, some constitutedwith complex preposition phrases can still bevery long.The complex constitutions of S, O, D chunksare the main parsing difficulties.
(2) The type of functional chunks can?t besimply determined by their constitutions, but de-pends heavily on their contexts.As the constitution of a basic chunk is verysimple, its type can be largely determined by itshead word, but in the case of functional chunks,the relationships between the functional chunksplay an important role.
For example, a NP phrasebefore a P chunk can be identified as a subjectchunk, but in other sentences, when it followsanother P chunk, it will be recognized as an ob-ject chunk.
Thus we can?t determine the type of afunctional chunk simply by its constitution.The context dependencies of functionalchunks bring a new challenge for our chunkparser.In the next section, we will propose a top-down model for Chinese functional chunk pars-ing.
Since the functional chunk boundaries havethe information of linking two adjacent chunks,they will be very helpful in the determination ofchunk types.3 Parsing ModelThe Chinese functional chunk parser takes astream of segmented and tagged words as its in-put, and outputs all the functional chunk bounda-ries in a sentence.
In this section, we will presenta parsing model which formulates the functionalchunk parsing problem as a boundary detectiontask, and then decompose this model into a seriesof sub modules that are easy to build.3.1 FormulationFunctional chunks have the property of exhaust-ibility and no words will be left outside thechunks.
Thus we don?t need to find the end posi-tion for a functional chunk as it could be identi-fied by the start of the next one.
In this case, wecan simply regard the chunking task as a processof cutting the input sentence into several seg-ments of words, each of which is labeled with afunctional tag.
Based on this idea, we can modelthe functional chunk parsing problem as aboundary detection task.Let S=<W, T> denote the input sentence to beparsed by the functional chunk parser, whereW=w1w2w3?wn is the sequence of words in S,and T=t1t2t3?tn is sequence of the POS tags as-signed to each word in W. If wi is a punctuationmark, ti will be equal to wi.A chunk boundary is defined as a pair <C1,C2> where  C1 ,C2 ?
{S, P, O, J, D, C, T, Y}, C1is the chunk type before this boundary and C2 isthe chunk type following it.
The output of thechunk parser is denoted as O=<B, P> whereB=b1b2b3?bm is the sequence of chunk bounda-ries generated by the parser, and P=p1p2p3?pm isthe corresponding positions of b1b2b3?bm in thesentence.Chinese functional chunk parser can be con-sidered as a function h(S) which maps the inputsentence S to the chunk boundary sequence O.Take the following sentence for example:?14  ?
?/n(Nuclear electricity) 1 ?/vC(is) 2?/m(a)  3 ?/qN(kind) 4 ?
?/a(safe) 5 ?/?
6?
?/a(safe) 7 ?/?
8 ?
?/a(economical) 9 ?/u10  ?
?/n(energy) 11 ?/??
?Nuclear electricity is a kind of safe, clean andeconomical energy.
?In this sentence, there are totally 12 Chinesewords (punctuation marks are treated the sameway as words) with 11 numbers falling betweenthem indicating the positions where a functionalchunk boundary may appear.
If the input sen-tence is parsed correctly by the functional chunkparser, a series of boundaries will arise at posi-tion 1 and 2, which are illustrated as below:?14  ?
?/n <S, P> ?/vC <P, O> ?/m ?/qN ?
?/a ?/?
?
?/a ?/?
?
?/a ?/u  ?
?/n ?/?
?From the information provided by theseboundaries, we can easily identify the functionalchunks in the sentence:?14  [S ?
?/n  [P ?/vC  [O ?/m  ?/qN  ?
?/a  ?/?
?
?/a  ?/?
?
?/a  ?/u  ?
?/n  ?/?
?963.2 Decomposition of Parsing ModelThe functional chunk parser presented abovecould be further divided into several sub modules,each of which is only in charge of detecting onetype of the chunk boundaries in a sentence.
Thesub module in charge of detecting boundary bcould be formulated as a Boolean function hb(S, i)where S is the input sentence and i is the positionbetween word wi and wi+1.
Function hb(S, i) willtake true if there is a chunk boundary of type b atposition i, and it will take false if there?s not.Since the Boolean function hb(S, i) can be treatedas a binary classifier, many machine learningtechniques could be applied.If we combine every two chunk types in thetag set, we can make a total number of 8*8=64boundary types in our chunking task.
However,not all of them appear in the natural languagetext, for example, we don?t have any SO bounda-ries in our corpus as S and O chunks can?t be-come neighbors in a sentence without any Pchunks between them.
In our corpus, we couldfind 43 boundary types, but only a small numberof them are used very frequently.
In table 4, welist the 5 most frequently used boundaries in ourcorpus:Table 4.
The 5 Most Frequently Used Bounda-ries in the Corpus.Boundary Type CountPO 14209DP 11459SD 6156DD 5238SP 5233The top 5 boundaries take up 67.76% of all the62418 boundaries in our corpus.
If we furtherinvestigate the chunk types associated with theseboundaries, we can find that only four types areinvolved: P, D, O and S. Referred to Table 2, wecan find that these chunks are also the 4 mostfrequently used chunks in our corpus.In most cases, S, P, and O chunks constitutethe backbone of a Chinese sentence, and theyusually contain the most useful information weneed.
Therefore, we are more concerned about S,P and O chunks.
In the following sections, wewill focus on the construction of sub modules forSP and PO boundary detection tasks.4 Statistical Model SelectionAfter decomposing the parsing model into sev-eral sub modules, a lot of machine learning tech-niques could be applied to the constructions ofthese sub modules.SVM 1  is a machine learning technique forsolving the binary classification problems.
It iswell known for its good generalization perform-ance and high efficiency.
In this section, we willmake a performance comparison between SVM(Vapnik, 1995) and several other machine learn-ing techniques including Na?ve Bayes, ID3 2(Quinlan, 1986) and C4.53 (Quinlan, 1993), andthen illustrates how competitive SVM is in theboundary detection tasks.4.1 Experimental DataThe corpus we use here is a collection of 185news files which are manually corrected afterautomatic sentence-split, word segmentation andpart-of-speech tagging.
After these processes,they have been manually annotated with func-tional chunks.
Among the 185 files, 167 of themare taken as the training data and the remaining18 are left as the test data, which takes up ap-proximately 10% of all the data.In our experiments, we will use feature tem-plates to describe which features are to be usedin the generation of feature vectors.
For example,if the current feature template we use is w-1t2,then the feature vector generated at position iwill take the first word on the left and the secondword tag on the right as its features.Before we perform any experiments, all thedata have been converted to the vectors that areacceptable by different machine learning algo-rithms.
Thus we have a total number of 199268feature vectors generated from the 185 files.Among them, 172465 vectors are in the trainingdata and 26803 vectors are in the test data.
Twosets of training and test data are prepared respec-tively for the SP and PO boundary detectiontasks.The performance of each experiment is meas-ured with 3 rates: precision, recall and F?=1,where precision is the percentage of detectedboundaries that are correct, recall is the percent-age of boundaries in the test data that are foundby the parser, and F?=1 is defined asF?=(?2+1)*precision*recall/(?2*precision + recall)with ?=1.1 The software package we use is SVMlight v6.00, it is avail-able at http://svmlight.joachims.org/.
We use linear kernelfunction and other default parameters in our experiments.2 We use the weka?s implementation of Na?ve Bayes andID3 algorithms.
Weak 3.4 is available athttp://www.cs.waikato.ac.nz/ml/weka/.3 We use Quinlan?s C4.5 software package with its defaultparameters in our experiments.974.2 Algorithm ComparisonWe first use t-3t-2t-1t1t2 as the feature tem-plate, and list all the experimental results in Ta-ble 5 and Table 6.
From these results, we canfind that SVM has achieved the best precision,recall and F-Score in SP boundary detection task,while C4.5 has an overwhelming advantage inPO boundary detection task.
In both tasks, Na?veBayes algorithm performs the worst, whichmakes us very disappointed.Table 5.
Results of Different Algorithms in SPBoundary Detection Task.Algorithm Precision Recall F?=1SVM 82.21% 57.10% 67.39%ID3 67.60% 50.70% 57.94%C4.5 81.10% 44.60% 57.55%Na?ve Bayes 47.90% 51.00% 49.40%Table 6.
Results of Different Algorithms inPO Boundary Detection Task.Algorithm Precision Recall F?=1C4.5 72.00% 74.70% 73.33%SVM 67.27% 64.96% 66.09%ID3 70.70% 59.90% 64.85%Na?ve Bayes 48.10% 60.10% 53.43%As the feature template we use here is too sim-ple, the results we have got may not seem so per-suasive.
Therefore we decide to conduct anotherexperiment using a more complex feature tem-plate.In the following experiments, we will use w-2w-1w1w2t-2t-1t1t2 as the feature template.
Theexperimental results are listed in Table 7 and Ta-ble 8.After adding the word information to the fea-ture template, the dimensions of feature vectorsused by some algorithms increase dramatically.We remove Na?ve Bayes algorithm from the fol-lowing experiments, as it fails to deal with suchhigh dimensional data.Table 7.
Results of Different Algorithms in SPBoundary Detection Task.Algorithm Precision Recall F?=1SVM 82.25% 61.22% 70.19%ID3 64.70% 51.70% 57.47%C4.5 79.70% 37.40% 50.91%Table 8.
Results of Different Algorithms inPO Boundary Detection Task.Algorithm Precision Recall F?=1SVM 74.83% 86.99% 80.45%C4.5 67.90% 79.90% 73.41%ID3 75.10% 57.70% 65.26%After applying the complex feature template,SVM still keeps the first place in SP boundarydetection task.
In PO boundary detection task,SVM successfully takes the place of C4.5, andachieves the best recall and F-Score among allthe algorithms.
Although the precision of ID3 isa little better than SVM, we still prefer SVM toID3.
It seems that the word information in thefeature vectors is not so beneficial to decisiontree algorithms as to SVM.We also notice that SVM can perform very ef-ficiently even with a large number of features.
Inthe second set experiments, it usually takes sev-eral hours to train a decision tree model, but forSVM, the time cost is no more than 20 minutes.In addition, we can expect a better result by add-ing more information to SVM algorithm withoutworrying about the dimension disaster problemin other algorithms.
Therefore, we decide to baseour parsing model on SVM algorithm.5 The SVM-based Parsing Model5.1 Baseline ModelsIn this section, we will build 2 baseline modelsbased on SVM for SP and PO boundary detec-tion tasks respectively.
By comprising the resultsof two different feature templates, we will illus-trate how useful the word information is in ourSVM based models.One feature template we use here is the simpletemplate which only takes the POS tag informa-tion as its features.
The other one is the complextemplate which takes both word and tag informa-tion as its features.
To make sure the results arecomparable, we restrict the context window to 4words.In the SP boundary detection sub task, we gotthe following results:Table 9.
SP Boundary Detection Results.Feature template Precision Recall F?=1t-2t-1t1t2 76.25% 51.99% 61.83%w-2w-1w1w2t-2t-1t1t282.25% 61.22% 70.19%In the PO boundary detection sub task, we gotthe following results:Table 10.
PO Boundary Detection Results.Feature template Precision Recall F?=1t-2t-1t1t2 66.42% 65.27% 65.84%w-2w-1w1w2t-2t-1t1t274.83% 86.99% 80.45%By taking the complex feature template, wehave achieved the best F?=1 value of 70.19% inSP boundary detection experiment and 80.45%in PO experiment, both of which are muchhigher than those of the simple feature templates.From these results we can conclude that wordinformation is very helpful in our SVM based98models.
Thus we will only use the feature tem-plates with word information in the succeedingexperiments.5.2 Expanding the Context WindowIn the previous section, the feature templates weuse are restricted to a context window of 4 words,which might not be large enough to detect theboundaries between complex chunks.
For exam-ple, when parsing the sentence ?
[P ?
?/v 1 [O?
?/a 2 ?/u 3 ?
?/n 4 ?
?/n 5 ?
?/vN 6 ?
?/n?, the algorithm fails to detect the PO boundaryat position 1.
If we expand the context window tothe noun word ??
?/n?, some of these errorsmay disappear.
In the following experiments, wewill expand the context window from a size of 4words to 10 words, and make a comparison be-tween the different results.The 4 feature templates used here are listedbelow:T1: w-2w-1w1w2t-2t-1t1t2,T2: w-3w-2w-1w1w2w3t-3t-2t-1t1t2t3,T3: w-4w-3w-2w-1w1w2w3w4t-4t-3t-2t-1t1t2t3t4T4: w-5w-4w-3w-2w-1w1w2w3w4w5t-5t-4t-3t-2t-1t1t2t3t4t5.SP Boundary Detection Results82.25% 83.84%86.44% 86.15%61.22%66.34% 67.90%68.89%70.19%74.07% 76.06%76.56%55.00%60.00%65.00%70.00%75.00%80.00%85.00%T1 T2 T3 T4Precision Recall F-ScoreFigure 1.
SP Boundary Detection Results.As we have expected, the performance of SPboundary detection experiment has been im-proved as the context window expands from asize of 4 words to 8 words.
However, the preci-sion value meets its turning point at T3 afterwhich it goes down, while F-Score and recallvalue still keep rising.
From the curves shown infigure 1, we can find that the expansion of con-text window size from 4 words to 6 words has anobvious improvement for performance, and afterthat only F-Score and recall could be improved.PO Boundary Detection Results74.87%77.17% 78.16%78.74%86.83% 86.42% 86.17% 86.12%80.41%81.53% 81.97% 82.26%72.00%74.00%76.00%78.00%80.00%82.00%84.00%86.00%88.00%T1 T2 T3 T4Precision Recall F-ScoreFigure 2.
SP Boundary Detection Results.In contrast to the significant improvement wehave achieved in the SP experiments, the resultsof PO experiments are not so exciting.
As thecontext window expands, the precision valuekeeps rising while the recall value keeps declin-ing.
Fortunately, we have obtained a very slightincrease of F-Score from these efforts.Although it is very difficult to improve theperformance of PO boundary detection by simplyexpanding the context window, we?ve still got abetter result than that of SP.
If we examine theresults of the two tasks carefully, we can find avery interesting difference between them: in SPboundary detection task, it?s very easier to get abetter precision than recall, but in PO experiment,as the O chunks have a longer length, they aremore likely to be cut into small pieces, and thusit?s easier to get a better recall than precision.5.3 Error AnalysisIn our experiments, the recall value can be sim-ply raised by adding a positive bias value to theSVM classifier.
However, we can?t do the samething to improve the precision value.
Thus, in thefollowing analysis, we are only focus on the er-rors that deter the improvement of precisionvalue.There are 2 kinds of errors influencing theprecision value of the test results: One is thewrongly detected chunk boundaries (WDB)within chunks (these chunk boundaries are de-tected by the program, but they don?t exist in thetraining data).
This kind of error tends to cut alarge chunk into several small pieces.
The otheris the misclassification of chunk boundary types(MBT) at the chunk boundaries (There exists a99chunk boundary at that position, but chunkboundary type labeled by the program is wrong).In the following analysis, by comparing thenumbers of errors in the test results of T1 (w-2w-1w1w2t-2t-1t1t2) and T4 (w-5w-4w-3w-2w-1w1w2w3w4w5t-5t-4t-3t-2t-1t1t2t3t4t5), wewill point out which kind of errors could be ef-fectively eliminated by the expansion of contextwindow and which of them couldn?t.
Throughthis analysis, we hope to get some knowledge ofwhat efforts should be made in our further study.In SP boundary detection task, we list thenumber of wrongly detected chunk boundaries(#WDB) and the corresponding chunk types (CT)where WDB arises in the following table.Table 11.
Wrongly Detected Chunk Bounda-ries in the Test Results of T1 and T4.CT #WDB of T1 #WDB of T4 T4-T1O 17 18 1S 17 18 1D 7 6 -1C 0 1 1P 2 1 -1T 1 1 0Sum 44 45 1From the above table, we find that the numberof wrongly detected boundaries seems to be un-changed during the expansion of context window.But when we refer to the second type of errors,the expansion of context window does help.
Welist the misclassified boundary types (MBT) andthe error numbers (#MB) in the below table.
InSP boundary detection task, MBT is wronglyrecognized as boundary type SP.Table 12.
Misclassified Chunk Boundaries inthe Test Results of T1 and T4.MBT #MB of T1 #MB of T4 T4-T1OP 9 3 -6JP 8 2 -6DP 23 20 -3SD 6 6 0DS 1 1 0Sum 47 32 -15From the above table, we can find that themisclassifications of OP, JP and DP as SP havebeen largely reduced by expanding the contextwindow, but the misclassifications of DS and SDremain the same.
Therefore, we should try someother methods for D chunks in our future work.In PO boundary detection task, the expansionof context window seems to be very effective.We list all the results in the below table:Table 14.
Wrongly Detected Chunk Bounda-ries in the Test Results of T1 and T4.CT #WDB of T1 #WDB of T4 T4-T1O 251 196 -55S 106 76 -30D 92 55 -37P 56 64 8T 4 4 0C 1 1 0J 0 1 1Sum 510 397 -113It?s very exciting to see that by expanding thewindow size, the number of WDB decreases dra-matically from 510 to 397.
But it fails to elimi-nate the WDB errors within P, T, C, and Jchunks.In PO boundary detection task, MBT iswrongly recognized as boundary type PO.
Welist the error data of T1 and T4 in the below table.Table 13.
Misclassified Chunk Boundaries inthe Test Results of T1 and T4.MBT #MB of T1 #MB of T4 T4-T1PJ 17 18 1PD 9 9 0PC 8 8 0SP 6 6 0PS 5 5 0SD 5 4 -1DP 3 2 -1TS 3 3 0OD 1 0 -1PY 1 1 0Sum 58 56 -2In contrast to the results of SP boundary detec-tion task, the MBT errors could not be largelyreduced by simply expanding the context win-dow.
Therefore, we need to pay more attention tothese problems in our future work.6 Related worksAfter the work of Ramshaw and Marcus (1995) ,many machine learning techniques have beenapplied to the basic chunking task, such as Sup-port Vector Machines (Kudo and Matsumoto,2001), Hidden Markov Model(Molina and Pla2002), Memory Based Learning (Sang, 2002),Conditional Random Fields (Sha and Pereira,2003), and so on.
But only a small amount ofattention has been paid to the functional chunkparsing problem.Sandra and Erhard (2001) tried to constructthe function-argument structures based on thepre-chunked input.
They proposed a similaritybased algorithm to assign the functional labels tocomplete syntactic structures, and achieved a100precision of 89.73% and 90.40% for German andEnglish respectively.
Different from our top-down scheme, their function-argument structuresare still constituted from bottom-up, and the pre-chunked input helps simplify the chunking proc-ess.Elliott and Qiang Zhou (2001) used the BIOtagging system to identify the functional chunksin a sentence.
In their experiments, they usedC4.5 algorithm to build the parsing model, andfocused their efforts on the selection of featuresets.
After testing 5 sets of features, they haveachieved the best f-measure of 0.741 by usingfeature set E which contains all the features inother feature sets.
Instead of using BIO tags inour chunking task, we introduced chunk bounda-ries to help us identify the functional chunks,which could provide more relational informationbetween the functional chunks.7 Conclusions and Future WorksIn this paper, we have applied functional chunksto Chinese shallow parsing.
Since the functionalchunks have the properties of linearity and ex-haustibility, we can formulate the functionalchunk parsing problem as a boundary detectiontask.
By applying the divide-and-conquer strat-egy, we have further decomposed the parsingmodel into a series of sub modules, each ofwhich is only in charge of one boundary type.
Inthis way, we provide a very flexible frameworkwithin which different machine learning tech-niques could be applied.
In our experiments, webuild two sub modules based on SVM for solv-ing the SP and PO boundary detection tasks.Thanks to the good generalization performanceand high efficiency of SVM, we can successfullydeal with a large number of features.
By expand-ing the context window, we have achieved thebest F-Score of 76.56% and 82.26 for SP and POboundary detection tasks.The 2 sub modules we have built are onlyparts of the Chinese functional chunk parser.
Al-though the results we have got here seem some-what coarse, they could already be used in somesimple tasks.
In the future, we will build theother sub modules for the remaining types of thechunk boundaries.
After all these work, theremay be some inconsistent chunk boundaries inthe results, thus we need to solve the inconsis-tency problems and try to identify all the func-tional chunks in a sentence by combining thesechunk boundaries.AcknowledgementsThis work was supported by the Chinese NationalScience Foundation (Grant No.
60573185,60520130299).ReferencesElliott Franco Dr?bek and Qiang Zhou.
2001.
Ex-periments in Learning Models for FunctionalChunking of Chinese Text.
IEEE InternationalWorkshop on Natural Language processing andKnowledge Engineering, Tucson, Arizona, pages859-864.E.F.
Tjong Kim Sang.
2002.
Memory-based shallowparsing, Journal of Machine Learning Research 2,pages 559-594.F.
Sha and F. Pereira.
2003.
Shallow parsing withconditional random fields.
In Proceedings of Hu-man Language Technology Conference / NorthAmerican Chapter of the Association for Computa-tional Linguistics annual meeting.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques,2nd Edition, Morgan Kaufmann, San Francisco.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
Proceedings of theSecond Meeting of the North American Chapter ofthe Association for Computational Linguistics.Pittsburgh, PA.Lance Ramshaw and Mitch Marcus.
1995.
Textchunking using transformation-based learning.
InProceedings of the Third Workshop on Very LargeCorpora, pages 82?94.Quinlan, J. Ross.
1986.
Induction of decision trees.Machine Learning, 1(1), pages 81-106.Quinlan, J. Ross.
1993.
C4.5: Programs for MachineLearning.
San Mateo, CA: Morgan Kaufmann.Steven Abney.
1991.
Parsing by chunks.
In Principle-Based Parsing, Kluwer Academic Publishers,Dordrecht, pages 257?278.Sandra K?bler and Erhard W. Hinrichs.
2001.
Fromchunks to function-argument structure: A similar-ity-based approach.
In Proceedings of ACL/EACL2001, Toulouse, France, 2001, pages 338 - 345.Thorsten Joachims.
1999.
Advances in Kernel Meth-ods - Support Vector Learning, chapter Makinglarge-Scale SVM Learning Practical.
MIT-Press.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, New York.101
