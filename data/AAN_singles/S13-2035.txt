Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 193?201, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 11: Word Sense Induction & Disambiguationwithin an End-User ApplicationRoberto Navigli and Daniele VannellaDipartimento di InformaticaSapienza Universita` di RomaViale Regina Elena, 295 ?
00161 Roma Italy{navigli,vannella}@di.uniroma1.itAbstractIn this paper we describe our Semeval-2013task on Word Sense Induction and Dis-ambiguation within an end-user application,namely Web search result clustering and diver-sification.
Given a target query, induction anddisambiguation systems are requested to clus-ter and diversify the search results returned bya search engine for that query.
The task en-ables the end-to-end evaluation and compari-son of systems.1 IntroductionWord ambiguity is a pervasive issue in Natural Lan-guage Processing.
Two main techniques in compu-tational lexical semantics, i.e., Word Sense Disam-biguation (WSD) and Word Sense Induction (WSI)address this issue from different perspectives: theformer is aimed at assigning word senses from a pre-defined sense inventory to words in context, whereasthe latter automatically identifies the meanings of aword of interest by clustering the contexts in whichit occurs (see (Navigli, 2009; Navigli, 2012) for asurvey).Unfortunately, the paradigms of both WSD andWSI suffer from significant issues which hampertheir success in real-world applications.
In fact, theperformance of WSD systems depends heavily onwhich sense inventory is chosen.
For instance, themost popular computational lexicon of English, i.e.,WordNet (Fellbaum, 1998), provides fine-graineddistinctions which make the disambiguation taskquite difficult even for humans (Edmonds and Kil-garriff, 2002; Snyder and Palmer, 2004), althoughdisagreements can be solved to some extent withgraph-based methods (Navigli, 2008).
On the otherhand, although WSI overcomes this issue by allow-ing unrestrained sets of senses, its evaluation is par-ticularly arduous because there is no easy way ofcomparing and ranking different representations ofsenses.
In fact, all the proposed measures in the lit-erature tend to favour specific cluster shapes (e.g.,singletons or all-in-one clusters) of the senses pro-duced as output.
Indeed, WSI evaluation is actuallyan instance of the more general and difficult problemof evaluating clustering algorithms.Nonetheless, many everyday tasks carried out byonline users would benefit from intelligent systemsable to address the lexical ambiguity issue effec-tively.
A case in point is Web information retrieval, atask which is becoming increasingly difficult giventhe continuously growing pool of Web text of themost wildly disparate kinds.
Recent work has ad-dressed this issue by proposing a general evaluationframework for injecting WSI into Web search resultclustering and diversification (Navigli and Crisa-fulli, 2010; Di Marco and Navigli, 2013).
In thistask the search results returned by a search enginefor an input query are grouped into clusters, and di-versified by providing a reranking which maximizesthe meaning heterogeneity of the top ranking results.The Semeval-2013 task described in this paper1adopts the evaluation framework of Di Marco andNavigli (2013), and extends it to both WSD and WSIsystems.
The task is aimed at overcoming the well-known limitations of in vitro evaluations, such asthose of previous SemEval tasks on the topic (Agirre1http://www.cs.york.ac.uk/semeval-2013/task11/193and Soroa, 2007; Manandhar et al 2010), and en-abling a fair comparison between the two disam-biguation paradigms.
Key to our framework is theassumption that search results grouped into a givencluster are semantically related to each other andthat each cluster is expected to represent a specificmeaning of the input query (even though it is possi-ble for more than one cluster to represent the samemeaning).
For instance, consider the target queryapple and the following 3 search result snippets:1.
Apple Inc., formerly Apple Computer, Inc., is...2.
The science of apple growing is called pomology...3.
Apple designs and creates iPod and iTunes...Participating systems were requested to produce aclustering that groups snippets conveying the samemeaning of the input query apple, i.e., ideally {1, 3}and {2} in the above example.2 Task setupFor each ambiguous query the task required partic-ipating systems to cluster the top ranking snippetsreturned by a search engine (we used the GoogleSearch API).
WSI systems were required to iden-tify the meanings of the input query and cluster thesnippets into semantically-related groups accordingto their meanings.
Instead, WSD systems were re-quested to sense-tag the given snippets with the ap-propriate senses of the input query, thereby implic-itly determining a clustering of snippets (i.e., onecluster per sense).2.1 DatasetWe created a dataset of 100 ambiguous queries.The queries were randomly sampled from the AOLsearch logs so as to ensure that they had been used inreal search sessions.
Following previous work on thetopic (Bernardini et al 2009; Di Marco and Navigli,2013) we selected those queries for which a senseinventory exists as a disambiguation page in the En-glish Wikipedia2.
This guaranteed that the selectedqueries consisted of either a single word or a multi-word expression for which we had a collaboratively-edited list of meanings, including lexicographic andencyclopedic ones.
We discarded all queries made2http://en.wikipedia.org/wiki/Disambiguation pageFigure 1: An example of search result for the apple query,including: page title, URL and snippet.query length 1 2 3 4AOL logs 45.89 40.98 10.98 2.32our dataset 40.00 40.00 15.00 5.00Table 1: Percentage distribution of AOL query lengths(first row) vs. the queries sampled for our task (secondrow).up of > 4 words, since the length of the great ma-jority of queries lay in the range [1, 4].
In Table1 we compare the percentage distribution of 1- to4-word queries in the AOL query logs against ourdataset of queries.
Note that we increased the per-centage of 3- and 4-word queries in order to havea significant coverage of those lengths.
Anyhow,in both cases most queries contained from 1 to 2words.
Note that the reported percentage distribu-tions of query length is different from recent statis-tics for two reasons: first, over the years users haveincreased the average number of words per query inorder to refine their searches; second, we selectedonly queries which were either single words (e.g.,apple) or multi-word expressions (e.g., mortal kom-bat), thereby discarding several long queries com-posed of different words (such as angelina jolie ac-tress).Finally, we submitted each query to Googlesearch and retrieved the 64 top-ranking results re-turned for each query.
Therefore, overall the datasetconsists of 100 queries and 6,400 results.
Eachsearch result includes the following information:page title, URL of the page and snippet of the pagetext.
We show an example of search result for theapple query in Figure 1.2.2 Dataset AnnotationFor each query q we used Amazon Mechani-cal Turk3 to annotate each query result with the3https://www.mturk.com194most suitable sense.
The sense inventory for qwas obtained by listing the senses available in theWikipedia disambiguation page of q augmentedwith additional options from the classes obtainedfrom the section headings of the disambiguationpage plus the OTHER catch-all meaning.
For in-stance, consider the apple query.
We show its disam-biguation page in Figure 2.
The sense inventory forapple was made up of the senses listed in that page(e.g., MALUS, APPLE INC., APPLE BANK, etc.
)plus the set of generic classes OTHER PLANTS ANDPLANT PARTS, OTHER COMPANIES, OTHER FILMS,plus OTHER.For each query we ensured that three annotatorstagged each of the 64 results for that query withthe most suitable sense among those in the senseinventory (selecting OTHER if no sense was appro-priate).
Specifically, each Turker was provided withthe following instructions: ?The goal is annotatingthe search result snippets returned by Google for agiven query with the appropriate meaning amongthose available (obtained from the Wikipedia disam-biguation page for the query).
You have to selectthe meaning that you consider most appropriate?.No constraint on the age, gender and citizenship ofthe annotators was imposed.
However, in order toavoid random tagging of search results, we provided3 gold-standard result annotations per query, whichcould be shown to the Turker more than once duringthe annotation process.
In the case (s)he failed toannotate the gold items, the annotator was automat-ically excluded.2.3 Inter-Annotator Agreement andAdjudicationIn order to determine the reliability of the Turkers?annotations, we calculated the individual values ofFleiss?
kappa ?
(Fleiss, 1971) for each query q andthen averaged them:?
=?q?Q ?q|Q|, (1)where ?q is the Fleiss?
kappa agreement of the threeannotators who tagged the 64 snippets returned bythe Google search engine for the query q ?
Q, andQ is our set of 100 queries.
We obtained an averagevalue of ?
= 0.66, which according to Landis andFigure 2: The Wikipedia disambiguation page of Apple.Koch (1977) can be seen as substantial agreement,with a standard deviation ?
= 0.185.In Table 2 we show the agreement distributionof our 6400 snippets, distinguishing between fullagreement (3 out of 3), majority agreement (2 out of3), and no agreement.
Most of the items were anno-tated with full or majority agreement, indicating thatthe manual annotation task was generally doable forthe layman.
We manually checked all the cases ofmajority agreement, correcting only 7.92% of themajority adjudications, and manually adjudicatedall the snippets for which there was no agreement.We observed during adjudication that in many casesthe disagreement was due to the existence of sub-tle sense distinctions, like between MORTAL KOM-BAT (VIDEO GAME) and MORTAL KOMBAT (2011VIDEO GAME), or between THE DA VINCI CODEand INACCURACIES IN THE DA VINCI CODE.The average number of senses associated withthe search results of each query was 7.69(higher than in previous datasets, such as AMBI-ENT4+MORESQUE5, which associates 5.07 senses4http://credo.fub.it/ambient5http://lcl.uniroma1.it/moresque195Full agr.
Majority Disagr.% snippets 66.70 25.85 7.45Table 2: Percentage of snippets with full agreement, ma-jority agreement and full disagreement.per query on average).3 ScoringFollowing Di Marco and Navigli (2013), we eval-uated the systems?
outputs in terms of the snippetclustering quality (Section 3.1) and the snippet di-versification quality (Section 3.2).
Given a queryq ?
Q and the corresponding set of 64 snippet re-sults, let C be the clustering output by a given systemand let G be the gold-standard clustering for thoseresults.
Each measure M(C,G) presented below iscalculated for the query q using these two cluster-ings.
The overall results on the entire set of queriesQ in the dataset is calculated by averaging the val-ues of M(C,G) obtained for each single test queryq ?
Q.3.1 Clustering QualityThe first evaluation concerned the quality of theclusters produced by the participating systems.Since clustering evaluation is a difficult issue, wecalculated four distinct measures available in the lit-erature, namely:?
Rand Index (Rand, 1971);?
Adjusted Rand Index (Hubert and Arabie,1985);?
Jaccard Index (Jaccard, 1901);?
F1 measure (van Rijsbergen, 1979).The Rand Index (RI) of a clustering C is a mea-sure of clustering agreement which determines thepercentage of correctly bucketed snippet pairs acrossthe two clusterings C and G. RI is calculated as fol-lows:RI(C,G) =TP + TNTP + FP + FN + TN, (2)where TP is the number of true positives, i.e., snip-pet pairs which are in the same cluster both in C andHHHHHHGCC1 C2 ?
?
?
Cm SumsG1 n11 n12 ?
?
?
n1m a1G2 n21 n22 ?
?
?
n2m a2.......... .
.......Gg ng1 ng2 ?
?
?
ngm agSums b1 b2 ?
?
?
bm NTable 3: Contingency table for the clusterings G and C.G, TN is the number of true negatives, i.e., pairswhich are in different clusters in both clusterings,and FP and FN are, respectively, the number of falsepositives and false negatives.
RI ranges between 0and 1, where 1 indicates perfect correspondence.Adjusted Rand Index (ARI) is a development ofRand Index which corrects the RI for chance agree-ment and makes it vary according to expectaction:ARI(C,G) =RI(C,G)?
E(RI(C,G))maxRI(C,G)?
E(RI(C,G)).
(3)where E(RI(C,G)) is the expected value of the RI.Using the contingency table reported in Table 3 wecan quantify the degree of overlap between C and G,where nij denotes the number of snippets in com-mon between Gi and Cj (namely, nij = |Gi ?
Cj |),ai and bj represent, respectively, the number of snip-pets inGi and Cj , andN is the total number of snip-pets, i.e., N = 64.
Now, the above equation can bereformulated as:ARI(C,G)=?ij (nij2 )?
[?i (ai2 )?j (bj2 )]/(N2 )12 [?i (ai2 )+?j (bj2 )]?
[?i (ai2 )?j (bj2 )]/(N2 ).
(4)The ARI ranges between ?1 and +1 and is 0when the index equals its expected value.Jaccard Index (JI) is a measure which takes intoaccount only the snippet pairs which are in the samecluster both in C and G, i.e., the true positives (TP),while neglecting true negatives (TN), which are thevast majority of cases.
JI is calculated as follows:JI(C,G) =TPTP + FP + FN.
(5)Finally, the F1 measure calculates the harmonicmean of precision (P) and recall (R).
Precision de-termines how accurately the clusters of C represent196the query meanings in the gold standard G, whereasrecall measures how accurately the different mean-ings in G are covered by the clusters in C. We followCrabtree et al(2005) and define the precision of acluster Cj ?
C as follows:P (Cj) =|Csj ||Cj |, (6)whereCsj is the intersection betweenCj ?
C and thegold cluster Gs ?
G which maximizes the cardinal-ity of the intersection.
The recall of a query sense sis instead calculated as:R(s) =|?Cj?Cs Csj |ns, (7)where Cs is the subset of clusters of C whose ma-jority sense is s, and ns is the number of snippetstagged with query sense s in the gold standard.
Thetotal precision and recall of the clustering C are thencalculated as:P =?Cj?C P (Cj)|Cj |?Cj?C |Cj |; R =?s?S R(s)ns?s?S ns(8)where S is the set of senses in the gold standard Gfor the given query (i.e., |S| = |G|).
The two valuesof P and R are then combined into their harmonicmean, namely the F1 measure:F1(C,G) =2PRP +R.
(9)3.2 Clustering DiversityOur second evaluation is aimed at determining theimpact of the output clustering on the diversifica-tion of the top results shown to a Web user.
Tothis end, we applied an automatic procedure for flat-tening the clusterings produced by the participatingsystems to a list of search results.
Given a clus-tering C = (C1, C2, .
.
.
, Cm), we add to the ini-tially empty list the first element of each cluster Cj(j = 1, .
.
.
,m); then we iterate the process by se-lecting the second element of each cluster Cj suchthat |Cj | ?
2, and so on.
The remaining elements re-turned by the search engine, but not included in anycluster of C, are appended to the bottom of the listin their original order.
Note that systems were askedto sort snippets within clusters, as well as clustersthemselves, by relevance.Since our goal is to determine how many differ-ent meanings are covered by the top-ranking searchresults according to the output clustering, we usedthe measures of S-recall@K (Subtopic recall at rankK) and S-precision@r (Subtopic precision at recallr) (Zhai et al 2003).S-recall@K determines the ratio of differentmeanings for a given query q in the top-K resultsreturned:S-recall@K =|{sense(ri) : i ?
{1, .
.
.
,K}}|g,(10)where sense(ri) is the gold-standard sense associ-ated with the i-th snippet returned by the system,and g is the total number of distinct senses for thequery q in our gold standard.S-precision@r instead determines the ratio of dif-ferent senses retrieved for query q in the first Krsnippets, where Kr is the minimum number of topresults for which the system achieves recall r. Themeasure is defined as follows:S-precision@r =| ?Kri=1 sense(ri)|Kr.
(11)3.3 BaselinesWe compared the participating systems with twosimple baselines:?
SINGLETONS: each snippet is clustered as aseparate singleton cluster (i.e., |C| = 64).?
ALL-IN-ONE: all snippets are clustered into asingle cluster (i.e., |C| = 1).These baselines are important in that they makeexplicit the preference of certain quality measurestowards clusterings made up with a small or largenumber of clusters.4 Systems5 teams submitted 10 systems, out of which 9 wereWSI systems, while 1 was a WSD system, i.e., us-ing the Wikipedia sense inventory for performingthe disambiguation task.
All systems could exploitthe information provided for each search result, i.e.,URL, page title and result snippet.
WSI systemswere requested to use unannotated corpora only.197System URLs Snippets Wikipedia YAGO Hierarchy Distr.
Thesaurus OtherWSIHDP-CLUSTERS-LEMMA X XHDP-CLUSTERS-NOLEMMA X XDULUTH.SYS1.PK2 XDULUTH.SYS7.PK2 XDULUTH.SYS9.PK2 GigawordUKP-WSI-WP-LLR2 X X X WaCkyUKP-WSI-WP-PMI X X X WaCkyUKP-WSI-WACKY-LLR X X X WaCkySATTY-APPROACH1 XWSD RAKESH X DBPediaTable 4: Resources used for WSI/WSD.We asked each team to provide information abouttheir systems.
In Table 4 we report the resourcesused by each system.
The HDP and UKP systemsuse Wikipedia as raw text for sampling word counts;DULUTH-SYS9-PK2 uses the first 10,000 paragraphsof the Associated Press wire service data from theEnglish Gigaword Corpus (Graff, 2003, 1st edition),whereas DULUTH-SYS1-PK2 and DULUTH-SYS7-PK2 both use the snippets for inducing the querysenses.
Finally, the UKP systems were the only onesto retrieve the Web pages from the correspondingURLs and exploit them for WSI purposes.
Theyalso use WaCky (Baroni et al 2009) and a distri-butional thesaurus obtained from the Leipzig Cor-pora Collection6 (Biemann et al 2007).
SATTY-APPROACH1 just uses snippets.The only participating WSD system, RAKESH,uses the YAGO hierarchy (Suchanek et al 2008) to-gether with DBPedia abstracts (Bizer et al 2009).5 ResultsWe show the results of RI and ARI in Table 5.
Thebest performing systems are those from the HDPteam, with considerably higher RI and ARI.
Thenext best systems are SATTY-APPROACH1, whichuses only the words in the snippets, and the onlyWSD system, i.e., RAKESH.
SINGLETONS performwell with RI, but badly when chance agreement istaken into account.As for F1 and JI, whose values are shown in Table6, the two HDP systems again perform best in termsof F1, and are on par with UKP-WSI-WACKY-LLR interms of JI.
The third best approach in terms of F1is again SATTY-APPROACH1, which however per-6http://corpora.uni-leipzig.de/System RI ARIWSIHDP-CLUSTERS-LEMMA 65.22 21.31HDP-CLUSTERS-NOLEMMA 64.86 21.49SATTY-APPROACH1 59.55 7.19DULUTH.SYS9.PK2 54.63 2.59DULUTH.SYS1.PK2 52.18 5.74DULUTH.SYS7.PK2 52.04 6.78UKP-WSI-WP-LLR2 51.09 3.77UKP-WSI-WP-PMI 50.50 3.64UKP-WSI-WACKY-LLR 50.02 2.53WSD RAKESH 58.76 8.11BL SINGLETONS 60.09 0.00ALL-IN-ONE 39.90 0.00Table 5: Results for Rand Index (RI) and Adjusted RandIndex (ARI), sorted by RI.forms badly in terms of JI.
The SINGLETONS base-line clearly obtains the best F1 performance, but theworst JI results.
The ALL-IN-ONE baseline outper-forms all other systems with the JI measure, becauseTN are not considered, which favours large clusters.To get more insights into the performance of thevarious systems, we calculated the average numberof clusters per clustering produced by each systemand compared it with the gold standard average.
Wealso computed the average cluster size, i.e., the aver-age number of snippets per cluster.
The statistics areshown in Table 7.
Interestingly, the best performingsystems are those with the cluster number and aver-age number of clusters closest to the gold standardones.
This finding is also confirmed by Figure 3,where we draw each system according to its averagevalues regarding cluster number and size: again thedistance from the gold standard is meaningful.We now move to the diversification perfor-198System JI F1WSIUKP-WSI-WACKY-LLR 33.94 58.26HDP-CLUSTERS-NOLEMMA 33.75 68.03HDP-CLUSTERS-LEMMA 33.02 68.30DULUTH.SYS1.PK2 31.79 56.83UKP-WSI-WP-LLR2 31.77 58.64DULUTH.SYS7.PK2 31.03 58.78UKP-WSI-WP-PMI 29.32 60.48DULUTH.SYS9.PK2 22.24 57.02SATTY-APPROACH1 15.05 67.09WSD RAKESH 30.52 39.49BL SINGLETONS 0.00 100.00ALL-IN-ONE 39.90 54.42Table 6: Results for Jaccard Index (JI) and F1 measure.System # cl.
ACSGOLD STANDARD 7.69 11.56WSIHDP-CLUSTERS-LEMMA 6.63 11.07HDP-CLUSTERS-NOLEMMA 6.54 11.68SATTY-APPROACH1 9.90 6.46UKP-WSI-WP-PMI 5.86 30.30DULUTH.SYS7.PK2 3.01 25.15UKP-WSI-WP-LLR2 4.17 21.87UKP-WSI-WACKY-LLR 3.64 32.34DULUTH.SYS9.PK2 3.32 19.84DULUTH.SYS1.PK2 2.53 26.45WSD RAKESH 9.07 2.94Table 7: Average number of clusters (# cl.)
and averagecluster size (ACS).5101520253035402 4 6 8 10 12average number of clustersaverage cluster size (ACS)gold-standardhdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2rakeshsatty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmiFigure 3: Average cluster size (ACS) vs. average numberof clusters.mance, calculated in terms of S-recall@K and S-precision@r, whose results are shown in Tables 8System K5 10 20 40WSIHDP-CL.-NOLEMMA 50.80 63.21 79.26 92.48HDP-CL.-LEMMA 48.13 65.51 78.86 91.68UKP-WACKY-LLR 41.19 55.41 68.61 83.90UKP-WP-LLR2 41.07 53.76 68.87 85.87UKP-WP-PMI 40.45 56.25 68.70 84.92SATTY-APPROACH1 38.97 48.90 62.72 82.14DULUTH.SYS7.PK2 38.88 53.79 70.38 86.23DULUTH.SYS9.PK2 37.15 49.90 68.91 83.65DULUTH.SYS1.PK2 37.11 53.29 71.24 88.48WSD RAKESH 46.48 62.36 78.66 90.72Table 8: S-recall@K.System r50 60 70 80WSIHDP-CL.-LEMMA 48.85 42.93 35.19 27.62HDP-CL.-NOLEMMA 48.18 43.88 34.85 29.30UKP-WP-PMI 42.83 33.40 26.63 22.92UKP-WACKY-LLR 42.47 31.73 25.39 22.71UKP-WP-LLR2 42.06 32.04 26.57 22.41DULUTH.SYS1.PK2 40.08 31.31 26.73 24.51DULUTH.SYS7.PK2 39.11 30.42 26.54 23.43DULUTH.SYS9.PK2 35.90 29.72 25.26 21.26SATTY-APPROACH1 34.94 26.88 23.55 20.40WSD RAKESH 48.00 39.04 32.72 27.92Table 9: S-precision@r.and 9, respectively.
Here we find that, again, theHDP team obtains the best performance, followed byRAKESH.
We note however that not all systems op-timized the order of clusters and cluster snippets byrelevance.We also graph the diversification performancetrend of S-recall@K and S-precision@r in Fig-ures 4 and 5 for K = 1, .
.
.
, 25 and r ?
{40, 50, .
.
.
, 100}.6 Conclusions and Future DirectionsOne of the aims of the SemEval-2013 task on WordSense Induction & Disambiguation within an EndUser Application was to enable an objective compar-ison of WSI and WSD systems when integrated intoWeb search result clustering and diversification.
Thetask is a hard one, in that it involves clustering, butprovides clear-cut evidence that our end-to-end ap-plication framework overcomes the limits of previ-ous in-vitro evaluations.
Indeed, the systems whichcreate good clusters and better diversify search re-sults, i.e., those from the HDP team, achieve goodperformance across all the proposed measures, withno contradictory evidence.1990.10.20.30.40.50.60.70.80.95 10 15 20 25S-recall-at-KKhdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2satty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmirakeshFigure 4: S-recall@K.0.00.10.20.30.40.50.640 50 60 70 80 90 100S-precision-at-rrhdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2satty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmirakeshFigure 5: S-precision@r.Our annotation experience showed that theWikipedia sense inventory, augmented with ourgeneric classes, is a good choice for semanticallytagging search results, in that it covers most of themeanings a Web user might be interested in.
In fact,only 20% of the snippets was annotated with theOTHER class.Future work might consider large-scale multilin-gual lexical resources, such as BabelNet (Navigliand Ponzetto, 2012), both as sense inventory and forperforming the search result clustering and diversi-fication task.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.We thank Antonio Di Marco and David A. Jur-gens for their help.200ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 task02: Evaluating word sense induction and discrimina-tion systems.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007),pages 7?12, Prague, Czech Republic.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky Wide Web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Andrea Bernardini, Claudio Carpineto, and Massimil-iano D?Amico.
2009.
Full-subtopic retrieval withkeyphrase-based search results clustering.
In Proceed-ings of Web Intelligence 2009, volume 1, pages 206?213, Los Alamitos, CA, USA.Chris Biemann, Gerhard Heyer, Uwe Quasthoff, andMatthias Richter.
2007.
The Leipzig corpora collec-tion - monolingual corpora of standard size.
In Pro-ceedings of Corpus Linguistic 2007, Birmingham, UK.Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?renAuer, Christian Becker, Richard Cyganiak, and Sebas-tian Hellmann.
2009.
Dbpedia - a crystallization pointfor the web of data.
J.
Web Sem., 7(3):154?165.Daniel Crabtree, Xiaoying Gao, and Peter Andreae.2005.
Improving web clustering by cluster selection.In Proceedings of the 2005 IEEE/WIC/ACM Interna-tional Conference on Web Intelligence, pages 172?178, Washington, DC, USA.Antonio Di Marco and Roberto Navigli.
2013.
Clus-tering and diversifying web search results with graph-based word sense induction.
Computational Linguis-tics, 39(4).Philip Edmonds and Adam Kilgarriff.
2002.
Introduc-tion to the special issue on evaluating word sense dis-ambiguation systems.
Journal of Natural LanguageEngineering, 8(4):279?291.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA, USA.Joseph L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
In Psychological Bulletin,volume 76, page 378?382.David Graff.
2003.
English Gigaword.
In TechnicalReport, LDC2003T05, Linguistic Data Consortium,Philadelphia, PA, USA.Lawrence Hubert and Phipps Arabie.
1985.
ComparingPartitions.
Journal of Classification, 2(1):193?218.Paul Jaccard.
1901.
E?tude comparative de la distributionflorale dans une portion des alpes et des jura.
In Bul-letin de la Socie?te?
Vaudoise des Sciences Naturelles,volume 37, page 547?579.J Richard Landis and Gary G Koch.
1977.
The mea-surement of observer agreement for categorical data.biometrics, pages 159?174.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
SemEval-2010task 14: Word sense induction & disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 63?68, Uppsala, Sweden.Roberto Navigli and Giuseppe Crisafulli.
2010.
Inducingword senses to improve web search result clustering.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 116?126, Boston, USA.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193:217?250.Roberto Navigli.
2008.
A structural approach to theautomatic adjudication of word sense disagreements.Journal of Natural Language Engineering, 14(4):293?310.Roberto Navigli.
2009.
Word Sense Disambiguation: asurvey.
ACM Computing Surveys, 41(2):1?69.Roberto Navigli.
2012.
A quick tour of word sensedisambiguation, induction and related approaches.
InProceedings of the 38th Conference on Current Trendsin Theory and Practice of Computer Science (SOF-SEM), pages 115?129.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the AmericanStatistical association, 66(336):846?850.Benjamin Snyder and Martha Palmer.
2004.
The En-glish all-words task.
In Proceedings of the 3rd Inter-national Workshop on the Evaluation of Systems forthe Semantic Analysis of Text (Senseval-3), pages 41?43, Barcelona, Spain.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
YAGO: A large ontology fromwikipedia and wordnet.
Journal of Web Semantics,6(3):203?217.Cornelis Joost van Rijsbergen.
1979.
Information Re-trieval.
Butterworths, second edition.ChengXiang Zhai, William W. Cohen, and John Lafferty.2003.
Beyond independent relevance: Methods andevaluation metrics for subtopic retrieval.
In Proceed-ings of the 26th annual international ACM SIGIR con-ference on Research and development in informaionretrieval, pages 10?17, Toronto, Canada.201
