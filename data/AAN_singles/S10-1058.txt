Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 260?263,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUvT: Memory-based pairwise ranking of paraphrasing verbsSander WubbenTilburg centre for Cognition and CommunicationTilburg UniversityThe Netherlandss.wubben@uvt.nlAbstractIn this paper we describe Mephisto, oursystem for Task 9 of the SemEval-2 work-shop.
Our approach to this task is to de-velop a machine learning classifier whichdetermines for each verb pair describinga noun compound which verb should beranked higher.
These classifications arethen combined into one ranking.
Our clas-sifier uses features from the Google N-gram Corpus, WordNet and the providedtraining data.1 IntroductionWe interpret the task of ranking a set of givenparaphrasing verbs as described by Butnariu etal (2010) as a competition between these verbs.Each verb competes with every other verb in theset and receives a positive score if it is more likelyto describe the given noun compound (NC) thanthe other verb and a negative score if it is lesslikely to describe the NC.
In line with this ap-proach we regard the task as a classification prob-lem where for each comparison our classificationalgorithm picks the paraphrasing verb that is morelikely to describe the NC.
This brings the clas-sification problem down to three classes: higher,equal or lower.
Sometimes the paraphrasing verbsare accompanied by a preposition.
In this paper wewill simply refer to all verbs and verb-prepositionsas verbs.The distribution of the verbs in the training dataprovides us already with valuable information.
Weincorporate basic features describing this distribu-tion to train our classifier.
We also need addi-tional semantic features that provide us with in-sight into the relation between the NC and theverb, therefore we use features constructed fromWordNet and the Google N-gram Corpus to trainour Memory-based paraphrase interpretation scor-ing tool (Mephisto).2 System DescriptionThe system consists of three components: the fea-ture extraction component, the classification com-ponent and the ranking component.
We will de-scribe all three components.2.1 Feature ExtractionFor each verb describing an NC we try to extractthose features that describe the probability that thisverb is a good interpretation of the NC.
We assumethat given a NC N1N2and a verb V , the NC inter-pretation should be N2V N1.
The phrase ?Buttermade from peanuts?
adequately describes peanutbutter.The training data provides us with a total of17,727 instances of NC verb pairs scored by hu-man judges.
This can be broken down into 4,360unique verb phrases describing 250 NCs.
Thisdistribution already gives us a good clue when weare generating new rankings.
The following arethe features we used:Weighted mean in training data For each NCthat has to be ranked we find the most similar NCin the training data by measuring the overlap inverb phrases between the two NCs.
We do this bycalculating the Jaccard coefficient over the sets ofverbs associated with the NCs.
We adapt the high-est ranking NC as most similar to our candidateNC (the NC with most matching verbs).
For eachverb V we then calculate the score as follows:Score = J ?
Ssim+ (1 ?
J) ?Mwhere J is the Jaccard score, Ssimis theassigned score of the verb in the most similarset and M is the mean score for the verb in thetraining data.Rank in training data For this feature wedirectly compare the two verbs V1and V2.
We just260feature values info gain gain ratioverb1 4,093 0.24 0.02verb2 4,093 0.24 0.02verb1-verb2 768,543 1.06 0.06verb1-verb2-LCS 986,031 1.29 0.07n-gram score1 7 0.07 0.02n-gram score2 7 0.01 0.08weighted mean 7 0.29 0.12rank 3 0.68 0.43Table 1: Features used in our systemcount the number of times that V1is ranked higherthan V2and vice versa for every NC where bothverbs occur.
We end up with a positive, equal ornegative class.WordNet Least Common Subsumer In orderto distinguish between different kinds of NCs weuse WordNet (Fellbaum, 1998) to determine thekind of relation between the nouns.
This idea issupported by work by Levi (1978), Warren (1978)and Nastase & Szpakowicz (2003).
Our intuitionis that the ranking of verb phrases is very depen-dent on this relation between the nouns.
To deter-mine this we use the WordNet::QueryData (Ren-nie, 2000) module.
In the WordNet graph we lookfor the Least Common Subsumer (LCS) of thetwo nouns.
The LCS is the lowest parent node ofboth nouns.
We combine the LCS with both verbphrases into one feature.Google N-gram features We use the Google N-gram corpus to count co-occurence frequencies ofcertain n-grams.
An NC occurring often togetherwith a certain verb should indicate that that verbis a good paraphrase for the NC.
Using web textfor various NLP-tasks has been proven to be use-ful (Lapata and Keller, 2005), also for NC inter-pretation (Nakov and Hearst, 2005).
Because ofdata sparseness and the unlikelihood of finding aperfect match for a certain n-gram, we adopt dif-ferent strategies for constructing features.
First ofall, we try to relax the matching conditions by ap-plying certain regular expression.
Given the NC?abortion problem?
and the paraphrasing verb ?berelated to?
, it seems unlikely you will ever en-counter the n-gram ?problem be related to abor-tion?, yet in the training data ?be related to?
is thenumber three verb for ?abortion problem?.
There-fore, we first apply some simple inflection.
Insteadof ?be?
we match on ?is/are/being?.
and we do acomparable inflection for other verbs transforming+up+ -dwn- =eq=+up+ 23,494 7,099 8,912-dwn- 7,168 23,425 8,912=eq= 22,118 22,084 22,408Table 2: Confusion matrix of the classes, with hor-izontally the output classes and vertically the tar-get classesa verb such as ?involve?
into ?involves/involving?.Additionally we also match on singular and pluralnouns.
We then use two different techniques tofind the n-gram frequencies:N ?
gram1=f(N2V ) + f(V N1)f(V )N ?
gram2=f(N2V N1)f(V )where f stands for the occurrences of the givensequences of nouns and verb.
We do not divide bynoun occurrences because they are constant forevery pair of verbs we compare.Pairwise comparison of features For eachverb pair in an NC set we compare all numericfeatures and assign one of the following symbolsto characterize the relation of the two verbs:+++: V1score is more than 10 times V2score++: V1score is between 2 and 10 times V2score+: V1score is between 1 and 2 times verb2 score=: scores are equal-: V2score is between 1 and 2 times V1score- -: V2score is between 2 and 10 times V1score- - -: V2score is more than 10 times V1scoreAn overview of the features is displayed in Ta-ble 1.2.2 ClassificationOur system makes use of Memory-Based Learn-ing (MBL) for classification.
MBL stores featurerepresentations of training instances in memorywithout abstraction and classifies unseen instancesby matching their feature representation to all in-stances in memory, finding the most similar in-stances.
The class of these most similar instancesis then copied to the new instance The learningalgorithm our system uses is the IB1 classier asimplemented in TiMBL (version 6.1.5).
IB1 is asupervised decision-tree-based implementation of261Settings TiMBL F-score Spearman ?
Pearson r KullbackLeibler div.k=3 all features 0.48 0.50 0.44 1.91k=3 no external features 0.53 0.48 0.41 2.05k=11 all features 0.51 0.50 0.42 1.97k=11 no external features 0.20 - - -Table 3: Results for different settings on the development setthe k-nearest neighbor algorithm for learning clas-sification tasks (Aha et al, 1991).
The TiMBL pa-rameters we used in the Mephisto system for theIB1 classifier are the overlap metric, weighting us-ing GainRatio, and k=3, taking into account theinstances on the 3 most similar positions to extrap-olate the class of the instance.
More informationabout these settings can be found in the TiMBLreference guide (Daelemans et al, 2009).
We trainour classifier on the provided training data to clas-sify instances into one of three classes; +up+ ifV1ranks higher than V2, =eq= if both verbs rankequally and -dwn- if V1ranks lower than V2.2.3 RankingThe final step is to combine all the classificationinto one score per verb.
This is done in a verystraight forward way: a verb receives one pointevery time it is classified as +up+.
This results inscores for each verb paraphrasing an NC.
We thenperform a simple post processing step: we reas-sign classes to each verb based on the final scoresthey have received and recalculate their scores.
Werepeat this process until the scores converge.3 ResultsFor development the original training set was di-vided in a development training set of 15,966 linesand a development test set of 1,761 lines, whichcontains 23 NCs.
The distribution and ranking fea-tures were calculated using only the developmenttraining set.
Because we compare for each NC ev-ery verb to every other verb the TiMBL traininginstance-base contains 1,253,872 lines, and the de-velopment test set 145,620.
The results for differ-ent settings are in Table 3.
Although the TiMBLF-score (macro-averaged) of using all features isactually lower than using only semantic features atk=3, the final correlations are in favor of using allfeatures.
There does not seem to be an improve-ment when extrapolating from 11 neighbouring in-stances in the instance-base over 3.
In fact, whenusing no external features and k=11, the classifierovergeneralizes and classifies every instance as=eq= and consequently does not provide a rankingSystem Spearman ?
Pearson r CosineUvT-MEPHISTO 0.450 0.411 0.635UCD-PN 0.441 0.361 0.669UCD-GOGGLE-III 0.432 0.395 0.652UCD-GOGGLE-II 0.418 0.375 0.660UCD-GOGGLE-I 0.380 0.252 0.629UCAM 0.267 0.219 0.374NC-INTERP 0.186 0.070 0.466Baseline 0.425 0.344 0.524Table 4: Final results for SemEval-2 Task 9at all.
Additionally, classifying with k=11 takesconsiderably longer than with k=3.
The settingswe use for our final system are k=3 and we use allfeatures.
Table 2 displays a confusion matrix ofthe classification on the development test set.
Notsurprisingly the classifier is very bad at recogniz-ing the =eq= class.
These mistakes are not as badas miss-classifying a +up+ instance as -dwn- andvice versa, and fortunately these mistakes happenless often.The official test set contains 32,830 instances,almost twice as many as the training set.
Thisbreaks down into 2,837,226 cases to classify.
InTable 4 are the final results of the task with allparticipating systems and their macro-averagedSpearman, Pearson and Cosine correlation.
Alsoshown is the baseline, which involves scoring agiven verb paraphrase by its frequency in the train-ing set.
The final results are quite a bit lower thanthe results on the development set.
This couldbe coincidence (the final test set is about twentytimes larger than our development test set), but itcould also be due to overfitting on the developmentset.
The ten best and worst scoring compounds areshown in Table 5 with their Least Common Sub-sumer as taken from WordNet.
The best-scoringNC ?jute products?
achieves a Spearman ?
of 0.75while the worst-scoring compound, ?electron mi-croscope?
only achieves 0.12.4 ConclusionWe have shown that a Memory-based pairwiseapproach to ranking with features taken fromWordNet and the Google N-gram corpus achieves262Best scoring NCs LCS Spearman ?jute products physical entity 0.75ceramics products artifact 0.75steel frame physical entity 0.74cattle population entity 0.74metal body physical entity 0.74winter blooming entity 0.73warbler family entity 0.72wool scarf artifact 0.71fiber optics physical entity 0.70petroleum products physical entity 0.70Worst scoring NCs LCS Spearman ?electron microscope whole 0.12light bulb physical entity 0.15yesterday evening measure 0.16student loan entity 0.16theater orchestra entity 0.17sunday restrictions abstraction 0.20yesterday afternoon measure 0.20relations agency abstraction 0.21crime novelist entity 0.21office buildings structure 0.21Table 5: Best and worst scoring noun compoundswith their Least Common Subsumer and Spear-man ?
correlationgood results on the task of ranking verbs para-phrasing noun compounds.
We outperform thestrong baseline and also systems using an unsuper-vised approach.
If we analyse our results we seethat our system scores particularly well on nouncompounds describing materials: in Table 5 wesee that all top ten compounds are either ?arti-facts?, ?physical entities?
or ?entities?
accordingto WordNet and the relation is quite direct: gen-erally a made of relation seems appropriate.
If welook at the bottom ten on the other hand, we see re-lations such as ?abstraction?
and ?measure?
: theseare harder to qualify.
Also, an ?electron micro-scope?
will generally not be perceived as a micro-scope made of electrons.
We can conclude that forNCs where the relation between the nouns is moreobscure the verbs are harder to rank.If we look at the Information Gain Ratio, ofall features the rank difference of the verbs in thetraining data seems to be the strongest feature, andof the external features the frequency difference ofthe entire phrase containing the NC and the verb.A lot more investigations could be made into theviability of using large n-gram collections such asthe Google N-gram corpus for paraphrase tasks.It might also be interesting to explore a some-what more challenging variant of this task by notproviding the verbs to be ranked a priori.
Thiswould probably be more interesting for real worldapplications because often the task is not onlyranking but finding the verbs in the first place.
Oursystem should be able to handle this task with mi-nor modifications: we simply regards all verbs inthe training-data candidates to be ranked.
Then,a pre-filtering step should take place to weed outirrelevant verbs based on an indicator such as theLCS of the nouns.
In addition a threshold could beimplemented to only accept a (further) limited setof verbs in the final ranking.ReferencesDavid W. Aha, Dennis Kibler, and Marc K. Albert.1991.
Instance-based learning algorithms.
Mach.Learn.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-armuid?O S?eaghdha, Stan Szpakowicz, and TonyVeale.
2010.
Semeval-2 task 9: The interpreta-tion of noun compounds using paraphrasing verbsand prepositions.
In Proceedings of the 5th SIGLEXWorkshop on Semantic Evaluation.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2009.
Timbl: Tilburgmemory-based learner - version 6.2 - referenceguide.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACMTrans.
Speech Lang.
Process.Judith N. Levi.
1978.
The Syntax and Semantics ofComplex Nominals.Preslav Nakov and Marti Hearst.
2005.
Search enginestatistics beyond the n-gram: Application to nouncompound bracketing.
In Proceedings of the 9thConference on Computational Natural LanguageLearning.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Proceedingsof the 5th International Workshop on ComputationalSemantics.Jason Rennie.
2000.
Wordnet::querydata: a perl mod-ule for accessing the wordnet database.Beatrice Warren.
1978.
Semantic Patterns of Noun-Noun Compounds.263
