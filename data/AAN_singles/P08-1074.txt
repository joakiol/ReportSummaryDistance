Proceedings of ACL-08: HLT, pages 647?655,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsPhrase Chunking using Entropy Guided Transformation LearningRuy L. Milidiu?Departamento de Informa?ticaPUC-RioRio de Janeiro, Brazilmilidiu@inf.puc-rio.brC?
?cero Nogueira dos SantosDepartamento de Informa?ticaPUC-Rionogueira@inf.puc-rio.brJulio C. DuarteCentro Tecnolo?gico do Exe?rcitoRio de Janeiro, Braziljduarte@ctex.eb.brAbstractEntropy Guided Transformation Learning(ETL) is a new machine learning strategythat combines the advantages of decisiontrees (DT) and Transformation Based Learn-ing (TBL).
In this work, we apply the ETLframework to four phrase chunking tasks: Por-tuguese noun phrase chunking, English basenoun phrase chunking, English text chunkingand Hindi text chunking.
In all four tasks,ETL shows better results than Decision Treesand also than TBL with hand-crafted tem-plates.
ETL provides a new training strat-egy that accelerates transformation learning.For the English text chunking task this corre-sponds to a factor of five speedup.
For Por-tuguese noun phrase chunking, ETL shows thebest reported results for the task.
For the otherthree linguistic tasks, ETL shows state-of-the-art competitive results and maintains the ad-vantages of using a rule based system.1 IntroductionPhrase Chunking is a Natural Language Processing(NLP) task that consists in dividing a text into syn-tactically correlated parts of words.
Theses phrasesare non-overlapping, i.e., a word can only be a mem-ber of one chunk (Sang and Buchholz, 2000).
It pro-vides a key feature that helps on more elaboratedNLP tasks such as parsing and information extrac-tion.Since the last decade, many high-performancechunking systems were proposed, such as, SVM-based (Kudo and Matsumoto, 2001; Wu et al,2006), Winnow (Zhang et al, 2002), voted-perceptrons (Carreras and Ma`rquez, 2003),Transformation-Based Learning (TBL) (Ramshawand Marcus, 1999; Megyesi, 2002) and HiddenMarkov Model (HMM) (Molina and Pla, 2002),Memory-based (Sang, 2002).
State-of-the-artsystems for English base noun phrase chunking andtext chunking are based in statistical techniques(Kudo and Matsumoto, 2001; Wu et al, 2006;Zhang et al, 2002).TBL is one of the most accurate rule-based tech-niques for phrase chunking tasks (Ramshaw andMarcus, 1999; Ngai and Florian, 2001; Megyesi,2002).
On the other hand, TBL rules must followpatterns, called templates, that are meant to cap-ture the relevant feature combinations.
The processof generating good templates is highly expensive.It strongly depends on the problem expert skills tobuild them.
Even when a template set is availablefor a given task, it may not be effective when wechange from a language to another (dos Santos andOliveira, 2005).In this work, we apply Entropy Guided Transfor-mation Learning (ETL) for phrase chunking.
ETL isa new machine learning strategy that combines theadvantages of Decision Trees (DT) and TBL (dosSantos and Milidiu?, 2007a).
The ETL key idea is touse decision tree induction to obtain feature com-binations (templates) and then use the TBL algo-rithm to generate transformation rules.
ETL pro-duces transformation rules that are more effectivethan decision trees and also eliminates the need ofa problem domain expert to build TBL templates.We evaluate the performance of ETL over four647phrase chunking tasks: (1) English Base NounPhrase (NP) chunking; (2) Portuguese NP chunk-ing; (3) English Text Chunking; and (4) Hindi TextChunking.
Base NP chunking consists in recogniz-ing non-overlapping text segments that contain NPs.Text chunking consists in dividing a text into syn-tactically correlated parts of words.
For these fourtasks, ETL shows state-of-the-art competitive resultsand maintains the advantages of using a rule basedsystem.The remainder of the paper is organized as fol-lows.
In section 2, the ETL strategy is described.In section 3, the experimental design and the corre-sponding results are reported.
Finally, in section 4,we present our concluding remarks.2 Entropy Guided TransformationLearningEntropy Guided Transformation Learning (ETL)is a new machine learning strategy that com-bines the advantages of Decision Trees (DT) andTransformation-Based Learning (TBL) (dos Santosand Milidiu?, 2007a).
The key idea of ETL is to usedecision tree induction to obtain templates.
Next,the TBL strategy is used to generate transformationrules.
The proposed method is illustrated in the Fig.1.Figure 1: ETL - Entropy Guided Transformation Learn-ing.A combination of DT and TBL is presented in(Corston-Oliver and Gamon, 2003).
The main dif-ference between Corston-Oliver & Gamon work andthe ETL strategy is that they extract candidate rulesdirectly from the DT, and then use the TBL strategyto select the appropriate rules.
Another difference isthat they use a binary DT, whereas ETL uses a DTthat is not necessarily binary.An evolutionary approach based on Genetic Al-gorithms (GA) to automatically generate TBL tem-plates is presented in (Milidiu?
et al, 2007).
Us-ing a simple genetic coding, the generated templatesets have efficacy near to the handcrafted templatesfor the tasks: English Base Noun Phrase Identifica-tion, Text Chunking and Portuguese Named EntitiesRecognition.
The main drawback of this strategy isthat the GA step is computationally expensive.
If weneed to consider a large context window or a largenumber of features, it can be infeasible.The remainder of this section is organized as fol-lows.
In section 2.1, we describe the DT learningalgorithm.
In section 2.2, the TBL algorithm is de-picted.
In section 2.3, we depict the process of ob-taining templates from a decision tree decomposi-tion.
Finally, in section 2.4, we present a templateevolution scheme that speeds up the TBL step.2.1 Decision TreesDecision tree learning is one of the most widely usedmachine learning algorithms.
It performs a parti-tioning of the training set using principles of Infor-mation Theory.
The learning algorithm executes ageneral to specific search of a feature space.
Themost informative feature is added to a tree structureat each step of the search.
Information Gain Ratio,which is based on the data Entropy, is normally usedas the informativeness measure.
The objective is toconstruct a tree, using a minimal set of features, thatefficiently partitions the training set into classes ofobservations.
After the tree is grown, a pruning stepis carried out in order to avoid overfitting.One of the most used algorithms for induction ofa DT is the C4.5 (Quinlan, 1993).
We use Quinlan?sC4.5 system throughout this work.2.2 Transformation-Based LearningTransformation Based error-driven Learning (TBL)is a successful machine learning algorithm intro-duced by Eric Brill (Brill, 1995).
It has since beenused for several Natural Language Processing tasks,such as part-of-speech (POS) tagging (Brill, 1995),English text chunking (Ramshaw and Marcus, 1999;dos Santos and Milidiu?, 2007b), spelling correc-648tion (Mangu and Brill, 1997), Portuguese appos-itive extraction (Freitas et al, 2006), Portuguesenamed entity extraction (Milidiu?
et al, 2006) andPortuguese noun-phrase chunking (dos Santos andOliveira, 2005), achieving state-of-the-art perfor-mance in many of them.TBL uses an error correcting strategy.
Its mainscheme is to generate an ordered list of rules thatcorrect classification mistakes in the training set,which have been produced by an initial classifier.The requirements of the algorithm are:?
two instances of the training set, one that hasbeen correctly labeled, and another that re-mains unlabeled;?
an initial classifier, the baseline system, whichclassifies the unlabeled training set by tryingto apply the correct class for each sample.
Ingeneral, the baseline system is based on simplestatistics of the labeled training set; and?
a set of rule templates, which are meant tocapture the relevant feature combinations thatwould determine the sample?s classification.Concrete rules are acquired by instantiation ofthis predefined set of rule templates.?
a threshold value, that is used as a stopping cri-teria for the algorithm and is needed to avoidoverfitting to the training data.The learning method is a mistake-driven greedyprocedure that iteratively acquires a set of transfor-mation rules.
The TBL algorithm can be depicted asfollows:1.
Starts applying the baseline system, in order toguess an initial classification for the unlabeledversion of the training set;2.
Compares the resulting classification with thecorrect one and, whenever a classification erroris found, all the rules that can correct it are gen-erated by instantiating the templates.
This tem-plate instantiation is done by capturing somecontextual data of the sample being corrected.Usually, a new rule will correct some errors, butwill also generate some other errors by chang-ing correctly classified samples;3.
Computes the rules?
scores (errors repaired - er-rors created).
If there is not a rule with a scoreabove an arbitrary threshold, the learning pro-cess is stopped;4.
Selects the best scoring rule, stores it in the setof learned rules and applies it to the trainingset;5.
Returns to step 2.When classifying a new sample item, the resultingsequence of rules is applied according to its genera-tion order.2.3 DT Template ExtractionThere are many ways to extract feature combinationsfrom decision trees.
In an path from the root to theleaves, more informative features appear first .
Sincewe want to generate the most promising templatesonly, we just combine the more informative ones.The process we use to extract templates from aDT includes a depth-first traversal of the DT.
Foreach visited node, we create a new template thatcombines its parent node template with the featureused to split the data at that node.
This is a verysimple decomposition scheme.
Nevertheless, it re-sults into extremely effective templates.
We also usepruned trees in all experiments shown in section 3.Fig.
2 shows an excerpt of a DT generated for theEnglish text chunking task1.
Using the describedmethod to extract templates from the DT shown inFig.
2, we obtain the template set listed in the leftside of Table 1.
In order to generate more featurecombinations, without largely increasing the num-ber of templates, we extend the template set by in-cluding templates that do not have the root node fea-ture.
The extended template set for the DT shown inFig.
2 is listed in the right side of the Table 1.We have also tried some other strategies that ex-tract a larger number of templates from a DT.
How-ever, the efficacy of the learned rules is quite similarto the one generated by the first method.
This rein-forces the conjecture that a DT generates informa-tive feature combinations.1CK[0] = Chunk tag of the current word (initial classifierresult); CK[?1] = previous word Chunk tag; CK[1] = next wordChunk tag; POS[0] = current word POS tag; WRD[0] = currentword.649Table 1: Text chunking DT Template set exampleTemplate set Extended template setCK[0] CK[0]CK[0] CK[1] CK[0] CK[1] CK[1]CK[0] CK[1] WRD[0] CK[0] CK[1] WRD[0] CK[1] WRD[0]CK[0] CK[1] WRD[0] CK[?1] CK[0] CK[1] WRD[0] CK[?1] CK[1] WRD[0] CK[?1]CK[0] CK[1] POS[0] CK[0] CK[1] POS[0] CK[1] POS[0]CK[0] CK[?1] CK[0] CK[?1] CK[?1]Figure 2: Text chunking decision tree excerpt.2.4 Template Evolution SpeedupTBL training time is highly sensitive to the numberand complexity of the applied templates.
In (Cur-ran and Wong, 2000), it is argued that we can bet-ter tune the training time vs. templates complex-ity trade-off by using an evolutionary template ap-proach.
The main idea is to apply only a small num-ber of templates that evolve throughout the training.When training starts, templates are short, consistingof few feature combinations.
As training proceeds,templates evolve to more complex ones that containmore feature combinations.
In this way, only a fewtemplates are considered at any point in time.
Nev-ertheless, the descriptive power is not significantlyreduced.The template evolution approach can be easily im-plemented by using template sets extracted from aDT.
We implement this idea by successively trainingTBL models.
Each model uses only the templatesthat contain feature combinations up to a given treelevel.
For instance, using the tree shown in Fig.
2,we have the following template sets for the three firsttraining rounds2:1.
CK[0] CK[1];CK[0] CK[?1]2.
CK[0] CK[1] WRD[0];CK[0] CK[1] POS[0]3.
CK[0] CK[1] WRD[0] CK[?1]Using the template evolution strategy, the trainingtime is decreased by a factor of five for the Englishtext chunking task.
This is a remarkable reduction,since we use an implementation of the fastTBL algo-rithm (Ngai and Florian, 2001) that is already a veryfast TBL version.
The efficacy of the rules gener-ated by the sequential training is quite similar to theone obtained by training with all the templates at thesame time.3 ExperimentsThis section presents the experimental setup and re-sults of the application of ETL to four phrase chunk-ing tasks.
ETL results are compared with the resultsof DT and TBL using hand-crafted templates.In the TBL step, for each one of the four chunkingtasks, the initial classifier assigns to each word thechunk tag that was most frequently associated withthe part-of-speech of that word in the training set.The DT learning works as a feature selector andis not affected by irrelevant features.
We have triedseveral context window sizes when training the clas-sifiers.
Some of the tested window sizes would bevery hard to be explored by a domain expert using2We ignore templates composed of only one feature test.650TBL alone.
The corresponding huge number of pos-sible templates would be very difficult to be man-aged by a template designer.For the four tasks, the following experimentalsetup provided us our best results.ETL in the ETL learning, we use the features word,POS and chunk.
In order to overcome the spar-sity problem, we only use the 200 most fre-quent words to induce the DT.
In the DT learn-ing, the chunk tag of the word is the one appliedby the initial classifier.
On the other hand, thechunk tag of neighbor words are the true ones.We report results for ETL trained with all thetemplates at the same time as well as using tem-plate evolution.TBL the results for the TBL approach refers to TBLtrained with the set of templates proposed in(Ramshaw and Marcus, 1999).DT the best result for the DT classifier is shown.The features word, POS and chunk are used togenerate the DT classifier.
The chunk tag of aword and its neighbors are the ones guessed bythe initial classifier.
Using only the 100 mostfrequent words gives our best results.In all experiments, the term WS=X subscriptmeans that a window of size X was used for thegiven model.
For instance, ETLWS=3 correspondsto ETL trained with window of size three, that is,the current token, the previous and the next one.3.1 Portuguese noun phrase chunkingFor this task, we use the SNR-CLIC corpus de-scribed in (Freitas et al, 2005).
This corpus istagged with both POS and NP tags.
The NP tagsare: I, for in NP; O, for out of NP; and B for theleftmost word of an NP beginning immediately af-ter another NP.
We divided the corpus into 3514-sentence (83346 tokens) training set and a 878-sentence (20798 tokens) test set.In Table 2 we compare the results3 of ETL withDT and TBL.
We can see that ETL, even with asmall window size, produces better results than DTand TBL.
The F?=1 of the ETLWS=7 classifier is1.8% higher than the one of TBL and 2.6% higherthan the one of the DT classifier.3#T = Number of templates.Table 2: Portuguese noun phrase chunking.Acc.
Prec.
Rec.
F?=1 # T(%) (%) (%) (%)BLS 96.57 62.69 74.45 68.06 ?DTWS=13 97.35 83.96 87.27 85.58 ?TBL 97.45 85.48 87.32 86.39 100ETLWS=3 97.61 86.12 87.24 86.67 21ETLWS=5 97.68 86.85 87.49 87.17 35ETLWS=7 97.82 88.15 88.20 88.18 34ETLWS=9 97.82 88.02 88.34 88.18 40Table 3 shows the results4 of ETL using templateevolution.
As we can see, for the task of Portuguesenoun phrase chunking, the template evolution strat-egy reduces the average training time in approxi-mately 35%.
On the other hand, there is a decreaseof the classifier efficacy in some cases.Table 3: Portuguese noun phrase chunking using ETLwith template evolution.Acc.
Prec.
Rec.
F?=1 TTR(%) (%) (%) (%) (%)ETLWS=3 97.61 86.22 87.27 86.74 20.7ETLWS=5 97.56 86.39 87.10 86.74 38.2ETLWS=7 97.69 87.35 87.89 87.62 37.0ETLWS=9 97.76 87.55 88.14 87.85 41.9In (dos Santos and Oliveira, 2005), a special setof six templates is shown.
These templates aredesigned to reduce classification errors of prepo-sition within the task of Portuguese noun phrasechunking.
These templates use very specific do-main knowledge and are difficult to DT and TBLto extract.
Table 4 shows the results of an experi-ment where we include these six templates into theRamshaw&Marcus template set and also into thetemplate sets generated by ETL.
Again, ETL pro-duces better results than TBL.Table 5 shows the results of using a committeecomposed by the three best ETL classifiers.
Theclassification is done by selecting the most populartag among all the three committee members.
Theachieved F?=1, 89.14% is the best one ever reportedfor the SNR-CLIC corpus.4TTR = Training time reduction.651Table 4: Portuguese noun phrase chunking using six ad-ditional hand-crafted templates.Acc.
Prec.
Rec.
F?=1 # T(%) (%) (%) (%)BLS 96.57 62.69 74.45 68.06 ?TBL 97.60 86.79 88.12 87.45 106ETLWS=3 97.73 86.95 88.40 87.67 27ETLWS=5 97.87 88.35 89.02 88.68 41ETLWS=7 97.91 88.12 89.22 88.67 40ETLWS=9 97.93 88.53 89.11 88.82 46Table 5: Committee with the classifiers ETLWS=5,ETLWS=7 and ETLWS=9, shown in Table 4.Results (%)Accuracy 97.97Precision 88.62Recall 89.67F?=1 89.143.2 English base noun phrase chunkingThe data used in the base NP chunking experimentsis the one by Ramshaw & Marcus (Ramshaw andMarcus, 1999).
This corpus contains sections 15-18 and section 20 of the Penn Treebank, and is pre-divided into 8936-sentence (211727 tokens) trainingset and a 2012-sentence (47377 tokens) test.
Thiscorpus is tagged with both POS and chunk tags.Table 6 compares the results of ETL with DTand TBL for the base NP chunking.
We can seethat ETL, even using a small window size, producesbetter results than DT and TBL.
The F?=1 of theETLWS=9 classifier is 0.87% higher than the one ofTBL and 2.31% higher than the one of the DT clas-sifier.Table 7 shows the results of ETL using templateevolution.
The template evolution strategy reducesthe average training time in approximately 62%.Differently from the Portuguese NP chunking, weobserve an increase of the classifier efficacy in al-most all the cases.Table 8 shows the results of using a committeecomposed by the eight ETL classifiers reported inthis section.
Table 8 also shows the results for acommittee of SVM models presented in (Kudo andMatsumoto, 2001).
SVM?s results are the state-of-Table 6: Base NP chunking.Acc.
Prec.
Rec.
F?=1 # T(%) (%) (%) (%)BLS 94.48 78.20 81.87 79.99 ?DTWS=11 97.03 89.92 91.16 90.53 ?TBL 97.42 91.68 92.26 91.97 100ETLWS=3 97.54 91.93 92.78 92.35 68ETLWS=5 97.55 92.43 92.77 92.60 85ETLWS=7 97.52 92.49 92.70 92.59 106ETLWS=9 97.63 92.62 93.05 92.84 122Table 7: Base NP chunking using ETL with template evo-lution.Acc.
Prec.
Rec.
F?=1 TTR(%) (%) (%) (%) (%)ETLWS=3 97.58 92.07 92.74 92.41 53.9ETLWS=5 97.63 92.66 93.16 92.91 57.9ETLWS=7 97.61 92.56 93.04 92.80 65.1ETLWS=9 97.59 92.50 93.01 92.76 69.4the-art for the Base NP chunking task.
On the otherhand, using a committee of ETL classifiers, we pro-duce very competitive results and maintain the ad-vantages of using a rule based system.Table 8: Base NP chunking using a committee of eightETL classifiers.Accuracy Precision Recall F?=1(%) (%) (%) (%)ETL 97.72 92.87 93.34 93.11SVM ?
94.15 94.29 94.223.3 English text chunkingThe data used in the English text chunking exper-iments is the CoNLL-2000 corpus, which is de-scribed in (Sang and Buchholz, 2000).
It is com-posed by the same texts as the Ramshaw & Marcus(Ramshaw and Marcus, 1999) corpus.Table 9 compares the results of ETL with DTs andTBL for English text chunking.
ETL, even using asmall window size, produces better results than DTsand TBL.
The F?=1 of the ETLWS=3 classifier is0.28% higher than the one of TBL and 2.17% higherthan the one of the DT classifier.
It is an interestinglinguistic finding that the use of a window of size 3652(the current token, the previous token and the nexttoken) provides the current best results for this task.Table 9: English text Chunking.Acc.
Prec.
Rec.
F?=1 # T(%) (%) (%) (%)BLS 77.29 72.58 82.14 77.07 ?DTWS=9 94.29 89.55 91.00 90.27 ?TBL 95.12 92.05 92.28 92.16 100ETLWS=3 95.24 92.32 92.56 92.44 105ETLWS=5 95.12 92.19 92.27 92.23 167ETLWS=7 95.13 92.24 92.32 92.28 183ETLWS=9 95.07 92.10 92.27 92.19 205Table 10 shows the results of ETL using templateevolution.
The template evolution strategy reducesthe average training time by approximately 81%.
Onthe other hand, there is a small decrease of the clas-sifier efficacy in all cases.Table 10: English text chunking using ETL with templateevolution.Acc.
Prec.
Rec.
F?=1 TTR(%) (%) (%) (%) (%)ETLWS=3 95.21 92.14 92.53 92.34 77.2ETLWS=5 94.98 91.84 92.25 92.04 80.8ETLWS=7 95.03 91.89 92.28 92.09 83.0ETLWS=9 95.01 91.87 92.21 92.04 84.5Table 11 shows the results of using a committeecomposed by the eight ETL classifiers reported inthis section.
Table 11 also shows the results for aSVM model presented in (Wu et al, 2006).
SVM?sresults are the state-of-the-art for the Text chunkingtask.
On the other hand, using a committee of ETLclassifiers, we produce very competitive results andmaintain the advantages of using a rule based sys-tem.Table 11: English text Chunking using a committee ofeight ETL classifiers.Accuracy Precision Recall F?=1(%) (%) (%) (%)ETL 95.50 92.63 92.96 92.79SVM ?
94.12 94.13 94.12Table 12 shows the results, broken down by chunktype, of using a committee composed by the eightETL classifiers reported in this section.Table 12: English text chunking results, broken down bychunk type, for the ETL committee.Precision Recall F?=1(%) (%) (%)ADJP 75.59 72.83 74.19ADVP 82.02 79.56 80.77CONJP 35.71 55.56 43.48INTJ 00.00 00.00 00.00LST 00.00 00.00 00.00NP 92.90 93.08 92.99PP 96.53 97.63 97.08PRT 66.93 80.19 72.96SBAR 86.50 85.05 85.77VP 92.84 93.58 93.21Overall 92.63 92.96 92.793.4 Hindi text chunkingThe data used in the Hindi text chunking exper-iments is the SPSAL-2007 corpus, which is de-scribed in (Bharati and Mannem, 2007).
This cor-pus is pre-divided into a 20000-tokens training set, a5000-tokens development set and a 5000-tokens testset.
This corpus is tagged with both POS and chunktags.To fairly compare our approach with the onespresented in the SPSAL-2007, the POS tags of thetest corpus were replaced by the ones predicted byan ETL-based Hindi POS Tagger.
The descriptionof our ETL pos tagger is beyond the scope of thiswork.
Since the amount of training data is very small(20000 tokens), the accuracy of the ETL Hindi POStagger is low, 77.50% for the test set.The results are reported in terms of chunking ac-curacy, the same performance measure used in theSPSAL-2007.
Table 13 compares the results of ETLwith DT and TBL for Hindi text chunking.
ETL pro-duces better results than DT and achieves the sameperformance of TBL using 60% less templates.
Webelieve that ETL performance is not as good as inthe other tasks mainly because of the small amountof training data, which increases the sparsity prob-lem.We do not use template evolution for Hindi text653chunking.
Since the training corpus is very small,the training time reduction is not significant.Table 13: Hindi text Chunking.Accuracy # Templates(%)BLS 70.05 ?DTWS=5 78.20 ?TBL 78.53 100ETLWS=5 78.53 30Table 14 compares the results of ETL with the twobest Hindi text chunkers at SPSAL-2007 (Bharatiand Mannem, 2007).
The first one is a combinationof Hidden Markov Models (HMM) and ConditionalRandom Fields (CRF) (PVS and Gali, 2007).
Thesecond is based in Maximum Entropy Models (Max-Ent) (Dandapat, 2007).
ETL performs better thanMaxEnt and worst than HMM+CRF.
It is importantto note that the accuracy of the POS tagger used by(PVS and Gali, 2007) (78.66%) is better than ours(77.50%).
The POS tagging quality directly affectsthe chunking accuracy.Table 14: Comparison with best systems of SPSAL-2007Accuracy(%)HMM + CRF 80.97ETLWS=5 78.53MaxEnt 74.924 ConclusionsIn this paper, we approach the phrase chunkingtask using Entropy Guided Transformation Learning(ETL).
We carry out experiments with four phrasechunking tasks: Portuguese noun phrase chunking,English base noun phrase chunking, English textchunking and Hindi text chunking.
In all four tasks,ETL shows better results than Decision Trees andalso than TBL with hand-crafted templates.
ETLprovides a new training strategy that acceleratestransformation learning.
For the English text chunk-ing task this corresponds to a factor of five speedup.For Portuguese noun phrase chunking, ETL showsthe best reported results for the task.
For the otherthree linguistic tasks, ETL shows competitive resultsand maintains the advantages of using a rule basedsystem.ReferencesAkshar Bharati and Prashanth R. Mannem.
2007.
In-troduction to shallow parsing contest on south asianlanguages.
In Proceedings of the IJCAI and the Work-shop On Shallow Parsing for South Asian Languages(SPSAL), pages 1?8.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Comput.
Linguistics,21(4):543?565.Xavier Carreras and Llu?
?s Ma`rquez.
2003.
Phrase recog-nition by filtering and ranking with perceptrons.
InProceedings of RANLP-2003, Borovets, Bulgaria.Simon Corston-Oliver and Michael Gamon.
2003.
Com-bining decision trees and transformation-based learn-ing to correct transferred linguistic representations.
InProceedings of the Ninth Machine Tranlsation Sum-mit, pages 55?62, New Orleans, USA.
Association forMachine Translation in the Americas.J.
R. Curran and R. K. Wong.
2000.
Formalisationof transformation-based learning.
In Proceedings ofthe Australian Computer Science Conference - ACSC,pages 51?57, Canberra, Australia.Sandipan Dandapat.
2007.
Part of speech tagging andchunking with maximum entropy model.
In Proceed-ings of the IJCAI and the Workshop On Shallow Pars-ing for South Asian Languages (SPSAL), pages 29?32.C?
?cero N. dos Santos and Ruy L. Milidiu?.
2007a.
En-tropy guided transformation learning.
Technical Re-port 29/07, Departamento de Informtica, PUC-Rio.C?
?cero N. dos Santos and Ruy L. Milidiu?.
2007b.
Prob-abilistic classifications with tbl.
In Proceedings ofEighth International Conference on Intelligent TextProcessing and Computational Linguistics ?
CICLing,pages 196?207, Mexico City, Mexico, February.C?
?cero N. dos Santos and Claudia Oliveira.
2005.
Con-strained atomic term: Widening the reach of rule tem-plates in transformation based learning.
In EPIA,pages 622?633.M.
C. Freitas, M. Garrao, C. Oliveira, C. N. dos Santos,and M. Silveira.
2005.
A anotac?a?o de um corpus parao aprendizado supervisionado de um modelo de sn.
InProceedings of the III TIL / XXV Congresso da SBC,Sa?o Leopoldo - RS - Brasil.M.
C. Freitas, J. C. Duarte, C. N. dos Santos, R. L.
Mi-lidiu?, R. P. Renteria, and V. Quental.
2006.
A ma-chine learning approach to the identification of appos-654itives.
In Proceedings of Ibero-American AI Confer-ence, Ribeira?o Preto, Brazil, October.T.
Kudo and Y. Matsumoto.
2001.
Chunking with sup-port vector machines.
In Proceedings of the NAACL-2001.Lidia Mangu and Eric Brill.
1997.
Automatic rule ac-quisition for spelling correction.
In Proceedings ofThe Fourteenth International Conference on MachineLearning, ICML 97.
Morgan Kaufmann.Bea?ta Megyesi.
2002.
Shallow parsing with pos taggersand linguistic features.
Journal of Machine LearningResearch, 2:639?668.Ruy L.
Milidiu?, Julio C. Duarte, and Roberto Cavalcante.2006.
Machine learning algorithms for portuguesenamed entity recognition.
In Proceedings of FourthWorkshop in Information and Human Language Tech-nology (TIL?06), Ribeira?o Preto, Brazil.Ruy L.
Milidiu?, Julio C. Duarte, and C?
?cero N. dos San-tos.
2007.
Tbl template selection: An evolutionaryapproach.
In Proceedings of Conference of the Span-ish Association for Artificial Intelligence - CAEPIA,Salamanca, Spain.Antonio Molina and Ferran Pla.
2002.
Shallow parsingusing specialized hmms.
J. Mach.
Learn.
Res., 2:595?613.Grace Ngai and Radu Florian.
2001.
Transformation-based learning in the fast lane.
In Proceedings ofNorth Americal ACL, pages 40?47, June.Avinesh PVS and Karthik Gali.
2007.
Part-of-speechtagging and chunking using conditional random fieldsand transformation based learning.
In Proceedings ofthe IJCAI and the Workshop On Shallow Parsing forSouth Asian Languages (SPSAL), pages 21?24.J.
Ross Quinlan.
1993.
C4.5: programs for machinelearning.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.Lance Ramshaw and Mitch Marcus.
1999.
Text chunk-ing using transformation-based learning.
In S. Arm-strong, K.W.
Church, P. Isabelle, S. Manzi, E. Tzouk-ermann, and D. Yarowsky, editors, Natural LanguageProcessing Using Very Large Corpora.
Kluwer.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the conll-2000 shared task: chunking.In Proceedings of the 2nd workshop on Learning lan-guage in logic and the 4th CONLL, pages 127?132,Morristown, NJ, USA.
Association for ComputationalLinguistics.Erik F. Tjong Kim Sang.
2002.
Memory-based shallowparsing.
J. Mach.
Learn.
Res., 2:559?594.Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee.
2006.A general and multi-lingual phrase chunking modelbased on masking method.
In Proceedings of 7th In-ternational Conference on Intelligent Text Processingand Computational Linguistics, pages 144?155.Tong Zhang, Fred Damerau, and David Johnson.
2002.Text chunking based on a generalization of winnow.
J.Mach.
Learn.
Res., 2:615?637.655
