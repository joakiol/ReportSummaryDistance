Introduction to the 3rd ROMAND workshop on Robust Methods inAnalysis of Natural Language DataVincenzo PallottaSwiss Federal Institute of Technology ?
Lausanne, CHUniversity of California ?
Berkeley, CAVincenzo.Pallotta@epfl.chAmalia TodirascuUniversity of Troyes - FranceUniversity "I.A.
Cuza" of Iasi - RomaniaAmalia.Todirascu@utt.frRobustness in Computational Linguistics has been recently recognized as a central issue for thedesign of interactive applications based on natural language communication.
If a failure of thesystem can be acceptable in batch applications requiring a human intervention, an on-line systemshould be capable of dealing with unforeseen situations in a more flexible way.
When we talk aboutsystem failure we do not think at inherent program failures like infinite loops or system exception,we consider, rather, failures related to the processing of the input and its assimilation in the system'sknowledge base.
A failure of this kind means simply that the system does not "understand" theinput.
The automated analysis of natural language data has become a central issue in the design ofIntelligent Information Systems.
Processing unconstrained natural language data is still consideredan AI-hard task.
However, various analysis techniques have been proposed in order to addressspecific aspects of natural language.
In particular, recent interest has been focused on providingapproximate analysis techniques, assuming that when perfect analysis is not possible, partial resultsmay be still very useful.Interpretation of natural language data is a subjective cognitive process.
Its formalisation could beeither a straightforward or a hard task, depending on the perspective taken.
The human ability tointerpret language is the result of thousands of years of evolution and cultural development.
We, ashumans, are capable of mapping surface language forms into meaning representations, but we canonly observe the final result of this process without exactly knowing what happens in our brain.
Incontrast, we can model understanding in an abstract domain where the process of reaching themeaning is decomposed at least in two parts.
We are interested in establishing to what degree:?
the mapping is sound (i.e.
if the competence model enables us to extract correct meanings);?
the mapping is complete (i.e.
if the competence model enables us to deal with all thelanguage phenomena).These two measures are fairly general, but in specific applications they might correspond to theclassic evaluation metrics of Information Retrieval (i.e.
precision and recall).Even human interpretation of language is not infallible.
For instance, in areas where people lackcontext, or have different views on the context, people can fail to understand each other and canhave different opinions on utterances' interpretation.
Apparently, it is always possible to provide aninterpretation of any kind of data.
Actually, one not always provides the right or the bestinterpretation among the possible ones.
This happens for humans and, why not, for artificialsystems.
When switching to artificial systems, what is worth considering is the human ability toprovide different degrees of approximate interpretations, ranging from the full understanding to thecomplete ignorance.
In addition, humans are able to overcome their limitations by their learningcapabilities.
In the interpretation process, the lack of knowledge, the uncertainty, vagueness,ambiguity, and misconception should be explicitly represented and considered at a meta-level inorder to handle linguistic applications 'robustly'.There are many ways in which the topic of robustness may be tackled: as a competency problem, asa problem of achieving interesting partial results, as a shallow analysis method, etc.
What theseapproaches have in common is that the simple (rigid) combination of 'complete' analysis modules atdifferent linguistic levels does not guarantees the system's robustness.
Rather, robustness must beconsidered as a system-wide concern.
We believe that the problem of robustness in NLP may betackled by adopting the following two complementary approaches:1. as an engineering `add-on': completing an existing system with additional features in orderto overcome the problem of  its inability to cope with real-world data;2. as a basic element of the underlying language theories: extending them by assuming that theunderstanding of the domain can be incomplete.Both approaches may be effective under certain circumstances.
We thus propose to consider twodifferent perspectives on the role of robustness in software architectures for natural languageprocessing and understanding, namely: robustness "in the small" and "in the large".With Robustness "in the small" we mean the robustness in language analysis is achieved at theindividual linguistic levels, such as the morpho-syntactic analysis, semantic interpretation,conversational analysis, dialogue acts recognition, anaphora resolution, and discourse analysis.With Robustness "in the large" we mean the robustness achieved in integrated NLP/NLUarchitectures possibly implementing hybrid approaches to language analysis, and incorporating thedifferent methods into a competitive/cooperative system.ROMAND 2004 is the third of a series of workshops aimed at bringing together researchers thatwork in fields like artificial intelligence, computational linguistics, human-computer interaction,cognitive science and are interested in robust methods in natural language processing andunderstanding.
Theoretical aspects of robustness in Natural Language Processing (NLP) andUnderstanding (NLU) are concerned by the workshop?s theme, as well as engineering and industrialexperiences.This volume contains an extended abstract of the invited talk and 11 papers selected by peer reviewout of 16 submissions.
The accepted papers cover topics related to robust syntactic parsing, robustsemantic parsing and applications using robust analysis methods (semantic tagging, informationextraction, question answering, document clustering).The third edition of ROMAND workshop features an exceptional invited speaker.
Frank Kellerfrom Edinburgh University accepted to talk about robustness aspects in cognitive, computationaland stochastic models of human parsing surveying and discussing the weaknesses and strengths ofmost recent advanced theories.The papers dedicated to robust syntactic parsing methods cover topics as combinations of statisticaland deep-linguistic syntactic analysis, as well as a parser's evaluation.
The paper proposed by G.Schneider, J. Dowdall and F. Rinaldi, ?A Robust and Hybrid Deep-Linguistic Theory Applied toLarge-Scale Parsing?, presents an efficient state-of-the-art hybrid parser combining statistical andrule-based parsing as well as shallow and deep parsing using a combination of phrase-structure andfunctional dependency grammars.
The paper ?Syntactic parser combination for improveddependency analysis?
describes how F. Brunet-Manquant improves parsing efficiency in buildingcomplex dependency structures by combining the results of the three concurrent parsers: theIncremental Finite-State Parser, the GREYC parser combining tagging methods to build non-recursive chunks, and the Xerox Incremental Parser.
A difficult problem of evaluation as well as theevaluation of the GETARUNS system is proposed and discussed in Delmonte?s paper ?EvaluatingGETARUNS Parser with GREVAL Test Suite?.Robustness plays an important role in semantic interpretation.
Semantic interpretations aregenerated by robust syntactic parsers output in specific representation languages: as logicalformulae in Minimal Recursion Semantics or as semantic hypergraphs in Unified NetworkLanguage respectively in the papers ?A step towards incremental generation of logical forms?
by L.Coheur, N. Mamede, G. B?s, and ?Using an incremental robust parser to automatically generatesemantic UNL graphs?
by N. Gala.
Existing knowledge bases (FrameNet and WordNet) areexploited to build complex semantic structures directly from free texts, as proposed in ?AnAlgorithm for Open Text Semantic Parsing?
by L. Shi and R. Mihalcea.
J. Bryant in his paper?Recovering Coherent Interpretations Using Semantic Integration of Partial Parses?
presents arobust semantic parser for Embodied Construction Grammars used to reconstruct full semanticinterpretations from semantic chunks in the framework of psycholinguistics studies on languageacquisition.A direct use of robust analysis methods is featured by several Information Extraction applications:Part of Speech tagging, building knoweldge maps from texts, Question-Answering, and documentclustering.
Robust morphological analysis for POS and restricted semantic tagging in Bulgarian isachieved by for learning robust ending guessing rules in the P. Nakov and E. Paskaleva?s paper?Robust Ending Guessing Rules with Application to Slavonic Languages?.
Extraction ofAssociative Term Networks from texts including co-occurences of several content words sharingsimilar contexts is the goal of the paper ?Knowledge Extraction Using Dynamical Updating ofRepresentation?
by A. Dragoni, L. Lella, G. Tascini, and W. Giordano.
Answer Validation is animportant module of a Question-Answering system for which a method exploiting co-occurrencefrequencies of keywords extracted from Web documents is proposed in the paper ?AnswerValidation by Keyword Association?
by M. Tonoike, T. Utsuro, and S. Sato.
Document clusteringalgorithms could help the users for Web browsing, where several document representations arecompared (as POS or as WordNet synsets) and exploited by an efficient clustering algorithmdiscussed in the paper ?WordNet-based text document clustering?
by J. Sedding and D. Kazakov.We believe that the output of the ROMAND 2004 workshop will contribute to a betterunderstanding of various aspects of robust analysis in Natural Language Processing andUnderstanding by presenting relevant advances in morphology, syntax, semantics, pragmatics, andevaluation, as well as examples of large-scale Information-Extraction applications relying on robustNLP/NLU techniques and architectures.We would like to thank all the people who have supported the 3rd edition of ROMAND, inparticular the authors who submitted their works, the members of the scientific program committee,the COLING workshop program committee, and the local organizing staff.
