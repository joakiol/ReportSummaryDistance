Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 290?300,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTwo Discourse Driven Language Models for SemanticsHaoruo Peng and Dan RothUniversity of Illinois, Urbana-ChampaignUrbana, IL, 61801{hpeng7,danr}@illinois.eduAbstractNatural language understanding often re-quires deep semantic knowledge.
Ex-panding on previous proposals, we suggestthat some important aspects of semanticknowledge can be modeled as a languagemodel if done at an appropriate level of ab-straction.
We develop two distinct mod-els that capture semantic frame chainsand discourse information while abstract-ing over the specific mentions of predi-cates and entities.
For each model, we in-vestigate four implementations: a ?stan-dard?
N-gram language model and threediscriminatively trained ?neural?
languagemodels that generate embeddings for se-mantic frames.
The quality of the se-mantic language models (SemLM) is eval-uated both intrinsically, using perplexityand a narrative cloze test and extrinsically?
we show that our SemLM helps improveperformance on semantic natural languageprocessing tasks such as co-reference res-olution and discourse parsing.1 IntroductionNatural language understanding often necessitatesdeep semantic knowledge.
This knowledge needsto be captured at multiple levels, from wordsto phrases, to sentences, to larger units of dis-course.
At each level, capturing meaning fre-quently requires context sensitive abstraction anddisambiguation, as shown in the following exam-ple (Winograd, 1972):Ex.1 [Kevin] was robbed by [Robert].
[He] wasarrested by the police.Ex.2 [Kevin] was robbed by [Robert].
[He] wasrescued by the police.In both cases, one needs to resolve the pronoun?he?
to either ?Robert?
or ?Kevin?.
To makethe correct decisions, one needs to know that thesubject of ?rob?
is more likely than the objectof ?rob?
to be the object of ?arrest?
while theobject of ?rob?
is more likely to be the object of?rescue?.
Thus, beyond understanding individualpredicates (e.g., at the semantic role labelinglevel), there is a need to place them and theirarguments in a global context.However, just modeling semantic frames is notsufficient; consider a variation of Ex.1:Ex.3 Kevin was robbed by Robert, but the policemistakenly arrested him.In this case, ?him?
should refer to ?Kevin?
asthe discourse marker ?but?
reverses the meaning,illustrating that it is necessary to take discoursemarkers into account when modeling semantics.In this paper we propose that these aspects ofsemantic knowledge can be modeled as a Seman-tic Language Model (SemLM).
Just like the ?stan-dard?
syntactic language models (LM), we de-fine a basic vocabulary, a finite representation lan-guage, and a prediction task, which allows us tomodel the distribution over the occurrence of el-ements in the vocabulary as a function of their(well-defined) context.
In difference from syn-tactic LMs, we represent natural language at ahigher level of semantic abstraction, thus facilitat-ing modeling deep semantic knowledge.We propose two distinct discourse driven lan-guage models to capture semantics.
In our first se-mantic language model, the Frame-Chain SemLM,we model all semantic frames and discourse mark-ers in the text.
Each document is viewed as a sin-gle chain of semantic frames and discourse mark-ers.
Moreover, while the vocabulary of discoursemarkers is rather small, the number of differentsurface form semantic frames that could appear inthe text is very large.
To achieve a better level ofabstraction, we disambiguate semantic frames andmap them to their PropBank/FrameNet represen-290tation.
Thus, in Ex.3, the resulting frame chainis ?rob.01 ?
but ?
arrest.01?
(?01?
indicates thepredicate sense).Our second semantic language model is calledEntity-Centered SemLM.
Here, we model a se-quence of semantic frames and discourse mark-ers involved in a specific co-reference chain.
Foreach co-reference chain in a document, we firstextract semantic frames corresponding to eachco-referent mention, disambiguate them as be-fore, and then determine the discourse markersbetween these frames.
Thus, each unique framecontains both the disambiguated predicate and theargument label of the mention.
In Ex.3, the re-sulting sequence is ?rob.01#obj ?
but ?
ar-rest.01#obj?
(here ?obj?
indicates the argument la-bel for ?Kevin?
and ?him?
respectively).
Whilethese two models capture somewhat different se-mantic knowledge, we argue later in the paper thatboth models can be induced at high quality, andthat they are suitable for different NLP tasks.For both models of SemLM, we study fourlanguage model implementations: N-gram, skip-gram (Mikolov et al, 2013b), continuous bag-of-words (Mikolov et al, 2013a) and log-bilinearlanguage model (Mnih and Hinton, 2007).
Eachmodel defines its own prediction task.
In total, weproduce eight different SemLMs.
Except for N-gram model, others yield embeddings for semanticframes as they are neural language models.In our empirical study, we evaluate both thequality of all SemLMs and their application to co-reference resolution and shallow discourse parsingtasks.
Following the traditional evaluation stan-dard of language models, we first use perplexityas our metric.
We also follow the script learningliterature (Chambers and Jurafsky, 2008b; Cham-bers and Jurafsky, 2009; Rudinger et al, 2015) andevaluate on the narrative cloze test, i.e.
randomlyremoving a token from a sequence and test the sys-tem?s ability to recover it.
We conduct both eval-uations on two test sets: a hold-out dataset fromthe New York Times Corpus and gold sequencedata (for frame-chain SemLMs, we use Prop-Bank (Kingsbury and Palmer, 2002); for entity-centered SemLMs, we use Ontonotes (Hovy etal., 2006) ).
By comparing the results on thesetest sets, we show that we do not incur noticeabledegradation when building SemLMs using prepro-cessing tools.
Moreover, we show that SemLMsimproves the performance of co-reference resolu-tion, as well as that of predicting the sense of dis-course connectives for both explicit and implicitones.The main contributions of our work can besummarized as follows: 1) The design of twonovel discourse driven Semantic Language mod-els, building on text abstraction and neural em-beddings; 2) The implementation of high qualitySemLMs that are shown to improve state-of-the-art NLP systems.2 Related WorkOur work is related to script learning.
Earlyworks (Schank and Abelson, 1977; Mooneyand DeJong, 1985) tried to construct knowledgebases from documents to learn scripts.
Recentwork focused on utilizing statistical models toextract high-quality scripts from large amountsof data (Chambers and Jurafsky, 2008a; Bejan,2008; Jans et al, 2012; Pichotta and Mooney,2014; Granroth-Wilding et al, 2015; Pichotta andMooney, 2016).
Other works aimed at learninga collection of structured events (Chambers, 2013;Cheung et al, 2013; Cheung et al, 2013; Balasub-ramanian et al, 2013; Bamman and Smith, 2014;Nguyen et al, 2015), and several works haveemployed neural embeddings (Modi and Titov,2014b; Modi and Titov, 2014a; Frermann et al,2014; Titov and Khoddam, 2015).In our work, the semantic sequences in theentity-centered SemLMs are similar to narrativeschemas (Chambers and Jurafsky, 2009).
How-ever, we differ from them in the following aspects:1) script learning does not generate a probabilis-tic model on semantic frames1; 2) script learningmodels semantic frame sequences incompletely asthey do not consider discourse information; 3)works in script learning rarely show applicationsto real NLP tasks.Some prior works have used scripts-relatedideas to help improve NLP tasks (Irwin et al,2011; Rahman and Ng, 2011; Peng et al, 2015b).However, since they use explicit script schemaseither as features or constraints, these works suf-fer from data sparsity problems.
In our work, theSemLM abstract vocabulary ensures a good cov-erage of frame semantics.1Some works may utilize a certain probabilistic frame-work, but they mainly focus on generating high-qualityframes by filtering.291Table 1: Comparison of vocabularies betweenframe-chain (FC) and entity-centered (EC)SemLMs.
?F-Sen?
stands for frames with pred-icate sense information while ?F-Arg?
standsfor frames with argument role label information;?Conn?
means discourse marker and ?Per?
meansperiod.
?Seq/Doc?
represents the number of se-quence per document.F-Sen F-Arg Conn Per Seq/DocFC YES NO YES YES SingleEC YES YES YES NO Multiple3 Two Models for SemLMIn this section, we describe how we capture se-quential semantic information consisted of seman-tic frames and discourse markers as semantic units(i.e.
the vocabulary).3.1 Semantic Frames and Discourse MarkersSemantic Frames A semantic frame is composedof a predicate and its corresponding argument par-ticipants.
Here we require the predicate to be dis-ambiguated to a specific sense, and we need a cer-tain level of abstraction of arguments so that wecan assign abstract labels.
The design of Prop-Bank frames (Kingsbury and Palmer, 2002) andFrameNet frames (Baker et al, 1998) perfectly fitsour needs.
They both have a limited set of frames(in the scale of thousands) and each frame can beuniquely represented by its predicate sense.
Theseframes provide a good level of generalization aseach frame can be instantiated into various surfaceforms in natural texts.
We use these frames as partof our vocabulary for SemLMs.
Formally, we usethe notation f to represent a frame.
Also, we de-note fa , f#Arg when referring to an argumentrole label (Arg) inside a frame (f).Discourse Markers We use discourse markers(connectives) to model discourse relationships be-tween frames.
There is only a limited number ofunique discourse markers, such as and, but, how-ever, etc.
We get the full list from the Penn Dis-course Treebank (Prasad et al, 2008) and includethem as part of our vocabulary for SemLMs.
For-mally, we use dis to denote the discourse marker.Note that discourse relationships can exist with-out an explicit discourse marker, which is also achallenge for discourse parsing.
Since we cannotreliably identify implicit discourse relationships,we only consider explicit ones here.
More impor-tantly, discourse markers are associated with ar-guments (Wellner and Pustejovsky, 2007) in text(usually two sentences/clauses, sometimes one).We only add a discourse marker in the semanticsequence when its corresponding arguments con-tain semantic frames which belong to the same se-mantic sequence.
We call them frame-related dis-course markers.
Details on generating semanticframes and discourse markers to form semantic se-quences are discussed in Sec.
5.3.2 Frame-Chain SemLMFor frame-chain SemLM, we model all seman-tic frames and discourse markers in a document.We form the semantic sequence by first includ-ing all semantic frames in the order they appearin the text: [f1, f2, f3, .
.
.].
Then we add frame-related discourse markers into the sequence byplacing them in their order of appearance.
Thuswe get a sequence like [f1, dis1, f2, f3, dis2, .
.
.
].Note that discourse markers do not necessarilyexist between all semantic frames.
Additionally,we treat the period symbol as a special discoursemarker, denoted by ?o?.
As some sentences con-tain more than one semantic frame (situations likeclauses), we get the final semantic sequence likethis:[f1, dis1, f2, o, f3, o, dis2, .
.
.
, o]3.3 Entity-Centered SemLMWe generate semantic sequences according toco-reference chains for entity-centered SemLM.From co-reference resolution, we can get a se-quence like [m1,m2,m3, .
.
.
], where mentions ap-pear in the order they occur in the text.
Eachmention can be matched to an argument inside asemantic frame.
Thus, we replace each mentionwith its argument label inside a semantic frame,and get [fa1, fa2, fa3, .
.
.].
We then add discoursemarkers exactly in they way we do for frame-chainSemLM, and get the following sequence:[fa1, dis1, fa2, fa3, dis2, .
.
.
]The comparison of vocabularies betweenframe-chain and entity-centered SemLMs is sum-marized in Table 1.4 Implementations of SemLMIn this work, we experiment with four languagemodel implementations: N-gram (NG), Skip-Gram (SG), Continuous Bag-of-Words (CBOW)and Log-bilinear (LB) language model.
For ease292of explanation, we assume that a semantic unit se-quence is s = [w1, w2, w3, .
.
.
, wk].4.1 N-gram ModelFor an n-gram model, we predict each token basedon its n?1 previous tokens, i.e.
we directly modelthe following conditional probability (in practice,we choose n = 3, Tri-gram (TRI) ):p(wt+2|wt, wt+1).Then, the probability of the sequence isp(s) = p(w1)p(w2|w1)k?2?t=1p(wt+2|wt, wt+1).To compute p(w2|w1) and p(w1), we need toback off from Tri-gram to Bi-gram and Uni-gram.4.2 Skip-Gram ModelThe SG model was proposed in Mikolov et al(2013b).
It uses a token to predict its context, i.e.we model the following conditional probability:p(c ?
c(wt)|wt, ?
).Here, c(wt) is the context for wtand ?
denotes thelearned parameters which include neural networkstates and embeddings.
Then the probability of thesequence is computed ask?t=1?c?c(wt)p(c|wt, ?
).4.3 Continuous Bag-of-Words ModelIn contrast to skip-gram, CBOW (Mikolov et al,2013a) uses context to predict each token, i.e.
wemodel the following conditional probability:p(wt|c(wt), ?
).In this case, the probability of the sequence isk?t=1p(wt|c(wt), ?
).4.4 Log-bilinear ModelLB was introduced in Mnih and Hinton (2007).Similar to CBOW, it also uses context to predicteach token.
However, LB associates a token withthree components instead of just one vector: a tar-get vector v(w), a context vector v?
(w) and a biasb(w).
So, the conditional probability becomes:p(wt|c(wt)) =exp(v(wt)?u(c(wt)) + b(wt))?w?Vexp(v(w)?u(c(wt)) + b(w)).Here, V denotes the vocabulary and we defineu(c(wt)) =?ci?c(wt)qiv?(ci).
Note thatrepresents element-wise multiplication and qiis avector that depends only on the position of a tokenin the context, which is a also a model parameter.So, the overall sequence probability isk?t=1p(wt|c(wt)).5 Building SemLMs from ScratchIn this section, we explain how we build SemLMsfrom un-annotated plain text.5.1 Dataset and PreprocessingDataset We use the New York Times Corpus2(from year 1987 to 2007) for training.
It containsa bit more than 1.8M documents in total.Preprocessing We pre-process all documents withsemantic role labeling (Punyakanok et al, 2004)and part-of-speech tagger (Roth and Zelenko,1998).
We also implement the explicit dis-course connective identification module in shal-low discourse parsing (Song et al, 2015).
Ad-ditionally, we utilize within document entity co-reference (Peng et al, 2015a) to produce co-reference chains.
To obtain all annotations, weemploy the Illinois NLP tools3.5.2 Semantic Unit GenerationFrameNet Mapping We first directly derive se-mantic frames from semantic role labeling anno-tations.
As the Illinois SRL package is built uponPropBank frames, we do a mapping to FrameNetframes via VerbNet senses (Schuler, 2005), thusachieving a higher level of abstraction.
The map-ping file4defines deterministic mappings.
How-ever, the mapping is not complete and there areremaining PropBank frames.
Thus, the generatedvocabulary for SemLMs contains both PropBankand FrameNet frames.
For example, ?place?
and2https://catalog.ldc.upenn.edu/LDC2008T193http://cogcomp.cs.illinois.edu/page/software/4http://verbs.colorado.edu/verb-index/fn/vn-fn.xml293?put?
with the VerbNet sense id ?9.1-2?
are con-verted to the same FrameNet frame ?Placing?.Augmenting to Verb Phrases We apply threeheuristic modifications to augment semanticframes defined in Sec.
3.1: 1) if a prepositionimmediately follows a predicate, we append thepreposition to the predicate e.g.
?take over?
; 2)if we encounter the semantic role label AM-PRDwhich indicates a secondary predicate, we also ap-pend this secondary predicate to the main predi-cate e.g.
?be happy?
; 3) if we see the semantic rolelabel AM-NEG which indicates negation, we ap-pend ?not?
to the predicate e.g.
?not like?.
Thesethree augmentations can co-exist and they allow usto model more fine-grained semantic frames.Verb Compounds We have observed that if twopredicates appear very close to each other, e.g.
?eat and drink?, ?decide to buy?, they actually rep-resent a unified semantic meaning.
Thus, we con-struct compound verbs to connect them together.We apply the rule that if the gap between two pred-icates is less than two tokens, we treat them asa unified semantic frame defined by the conjunc-tion of the two (augmented) semantic frames, e.g.?eat.01-drink.01?
and ?decide.01-buy.01?.Argument Labels for Co-referent Mentions Toget the argument role label information for co-referent mentions, we need to match each mentionto its corresponding semantic role labeling argu-ment.
If a mention head is inside an argument, weregard it as a match.
We do not consider singletonmentions.Vocabulary Construction After generating all se-mantic units for (augmented and compounded) se-mantic frames and discourse markers, we mergethem together as a tentative vocabulary.
In orderto generate a sensible SemLM, we filter out raretokens which appear less than 20 times in the data.We add the Unknown token (UNK) and End-of-Sequence token (EOS) to the eventual vocabulary.Statistics on the eventual SemLM vocabular-ies and semantic sequences are shown in Table 2.We also compare frame-chain and entity-centeredSemLMs to the usual syntactic language modelsetting.
The statistics in Table 2 shows that theyare comparable both in vocabulary size and in thetotal number of tokens for training.
Moreover,entity-centered SemLMs have shorter sequencesthen frame-chain SemLMs.
We also provide sev-eral examples of high-frequency augmented com-pound semantic frames in our generated SemLMTable 2: Statistics on SemLM vocabularies andsequences.
?F-s?
stands for single frame while?F-c?
stands for compound frame; ?Conn?
meansdiscourse marker.
?#seq?
is the number of se-quences, and ?#token?
is the total number of to-kens (semantic units).
We also compute the av-erage token in a sequence i.e.
?#t/s?.
We com-pare frame-chain (FC) and entity-centered (EC)SemLMs to the usual syntactic language modelsetting i.e.
?LM?.Vocabulary Size Sequence SizeF-s F-c Conn #seq #token #t/sFC 14857 7269 44 1.2M 25.4M 21EC 8758 2896 44 3.4M 18.6M 5LM ?20k ?3M ?38M 10-15vocabularies.
All are very intuitive:want.01-know.01, agree.01-pay.01,try.01-get.01, decline.02-comment.01,wait.01-see.01, make.02-feel.01,want.01(not)-give.08(up)5.3 Language Model TrainingNG We implement the N-gram model using theSRILM toolkit (Stolcke, 2002).
We also employthe well-known KneserNey Smoothing (Kneserand Ney, 1995) technique.SG&CBOW We utilize the word2vec package toimplement both SG and CBOW.
In practice, we setthe context window size to be 10 for SG while setthe number as 5 for CBOW (both are usual settingsfor syntactic language models).
We generate 300-dimension embeddings for both models.LB We use the OxLM toolkit (Paul et al, 2014)with Noise-Constrastive Estimation (Gutmann andHyvarinen, 2010) for the LB model.
We setthe context window size to 5 and produce 150-dimension embeddings.6 EvaluationIn this section, we first evaluate the quality ofSemLMs through perplexity and a narrative clozetest.
More importantly, we show that the proposedSemLMs can help improve the performance of co-reference resolution and shallow discourse pars-ing.
This further proves that we successfully cap-ture semantic sequence information which can po-tentially benefit a wide range of semantic relatedNLP tasks.We have designed two models for SemLM:frame-chain (FC) and entity-centered (EC).
Bytraining on both types of sequences respectively,we implement four different language models:294TRI, SG, CBOW, LB.
We focus the evaluationefforts on these eight SemLMs.6.1 Quality Evaluation of SemLMsDatasets We use three datasets.
We first randomlysample 10% of the New York Times Corpus doc-uments (roughly two years of data), denoted theNYT Hold-out Data.
All our SemLMs are trainedon the remaining NYT data and tested on thishold-out data.
We generate semantic sequencesfor the training and test data using the methodol-ogy described in Sec.
5.We use PropBank data with gold frame annota-tions as another test set.
In this case, we only gen-erate frame-chain SemLM sequences by apply-ing semantic unit generation techniques on goldframes, as described in Sec 5.2.
When we test onGold PropBank Data with Frame Chains, we useframe-chain SemLMs trained from all NYT data.Similarly, we use Ontonotes data (Hovy et al,2006) with gold frame and co-reference annota-tions as the third test set, Gold Ontonotes Datawith Coref Chains.
We only generate entity-centered SemLMs by applying semantic unit gen-eration techniques on gold frames and gold co-reference chains, as described in Sec 5.2.Baselines We use Uni-gram (UNI) and Bi-gram(BG) as two language model baselines.
In ad-dition, we use the point-wise mutual informa-tion (PMI) for token prediction.
Essentially, PMIscores each pair of tokens according to their co-occurrences.
It predicts a token in the sequence bychoosing the one with the highest total PMI withall other tokens in the sequence.
We use the or-dered PMI (OP) as our baseline, which is a vari-ation of PMI by considering asymmetric count-ing (Jans et al, 2012).6.1.1 PerplexityAs SemLMs are language models, it is natural toevaluate the perplexity, which is a measurement ofhow well a language model can predict sequences.Results for SemLM perplexities are presentedin Table 3.
They are computed without consider-ing end token (EOS).
We apply tri-gram Kneser-Ney Smoothing to CBOW, SG and LB.
LB con-sistently shows the lowest perplexities for bothframe-chain and entity-centered SemLMs acrossall test sets.
Similar to syntactic language mod-els, perplexities are fast decreasing from UNI, BIto TRI.
Also, CBOW and SG have very close per-plexity results which indicate that their languageTable 3: Perplexities for SemLMs.
UNI, BG,TRI, CBOW, SG, LB are different language modelimplementations while ?FC?
and ?EC?
stand forthe two SemLM models studied, respectively.?FC-FM?
and ?EC-FM?
indicate that we removedthe ?FrameNet Mapping?
step (Sec.
5.2).
LB con-sistently produces the lowest perplexities for bothframe-chain and entity-centered SemLMs.Baselines SemLMsUNI BG TRI CBOW SG LBNYT Hold-out DataFC 952.1 178.3 119.2 115.4 114.1 108.5EC 914.7 154.4 114.9 111.8 113.8 109.7Gold PropBank Data with Frame ChainsFC-FM 992.9 213.7 139.1 135.6 128.4 121.8FC 970.0 191.2 132.7 126.4 123.5 115.4Gold Ontonotes Data with Coref ChainsEC-FM 956.4 187.7 121.1 115.6 117.2 113.7EC 923.8 163.2 120.5 113.7 115.0 109.3modeling abilities are at the same level.We can compare the results of our frame-chainSemLM on NYT Hold-out Data and Gold Prop-Bank Data with Frame Chains, and our entity-centered SemLM on NYT Hold-out Data and GoldOntonotes Data with Coref Chains.
While we seedifferences in the results, the gap is narrow andthe relative ranking of different SemLMs does notchange.
This indicates that the automatic SRL andCo-reference annotations added some noise but,more importantly, that the resulting SemLMs arerobust to this noise as we still retain the languagemodeling ability for all methods.Additionally, our ablation study removes the?FrameNet Mapping?
step in Sec.
5.2 (?FC-FM?and ?EC-FM?
rows), resulting in only using Prop-Bank frames in the vocabulary.
The increase inperplexities shows that ?FrameNet Mapping?
doesproduce a higher level of abstraction, which is use-ful for language modeling.6.1.2 Narrative Cloze TestWe follow the Narrative Cloze Test idea used inscript learning (Chambers and Jurafsky, 2008b;Chambers and Jurafsky, 2009).
As Rudinger etal.
(2015) points out, the narrative cloze test canbe regarded as a language modeling evaluation.
Inthe narrative cloze test, we randomly choose andremove one token from each semantic sequencein the test set.
We then use language models topredict the missing token and evaluate the correct-ness.
For all SemLMs, we use the conditionalprobabilities defined in Sec.
4 to get token predic-tions.
We also use ordered PMI as an additionalbaseline.
The narrative cloze test is conducted on295Table 4: Narrative cloze test results for SemLMs.
UNI, BG, TRI, CBOW, SG, LB are different lan-guage model implementations while ?FC?
and ?EC?
stand for our two SemLM models, respectively.?FC-FM?
and ?EC-FM?
mean that we remove the FrameNet mappings.
?w/o DIS?
indicates the removalof discourse makers in SemLMs.
?Rel-Impr?
indicates the relative improvement of the best performingSemLM over the strongest baseline.
We evaluate on two metrics: mean reciprocal rank (MRR)/recall at30 (Recall@30).
LB outperforms other methods for both frame-chain and entity-centered SemLMs.Baselines SemLMsRel-ImprOP UNI BG TRI CBOW SG LBMRRNYT Hold-out DataFC 0.121 0.236 0.225 0.249 0.242 0.247 0.276 8.5%EC 0.126 0.235 0.210 0.242 0.249 0.249 0.261 5.9%EC w/o DIS 0.092 0.191 0.188 0.212 0.215 0.216 0.227 18.8%Rudinger et al (2015)?0.083 0.186 0.181 ??
??
??
0.223 19.9%Gold PropBank Data with Frame ChainsFC 0.106 0.215 0.212 0.232 0.228 0.229 0.254 18.1%FC-FM 0.098 0.201 0.204 0.223 0.218 0.220 0.243 ??
?Gold Ontonotes Data with Coref ChainsEC 0.122 0.228 0.213 0.239 0.247 0.246 0.257 12.7%EC-FM 0.109 0.215 0.208 0.230 0.237 0.239 0.254 ??
?Recall@30NYT Hold-out DataFC 33.2 46.8 45.3 47.3 46.6 47.5 55.4 18.4%EC 29.4 43.7 41.6 44.8 46.5 46.6 52.0 19.0%Gold PropBank Data with Frame ChainsFC 26.3 39.5 38.1 45.5 43.6 43.8 53.9 36.5%FC-FM 24.4 37.3 37.3 42.8 41.9 42.1 48.2 ??
?Gold Ontonotes Data with Coref ChainsEC 30.6 42.1 39.7 46.4 48.3 48.1 51.5 22.3%EC-FM 26.6 39.9 37.6 45.4 46.7 46.2 49.8 ??
?the same test sets as the perplexity evaluation.
Weuse mean reciprocal rank (MRR) and recall at 30(Recall@30) to evaluate.Results are provided in Table 4.
Consistent withthe results in the perplexity evaluation, LB out-performs other methods for both frame-chain andentity-centered SemLMs across all test sets.
It isinteresting to see that UNI performs better thanBG in this prediction task.
This finding is alsoreflected in the results reported in Rudinger et al(2015).
Though CBOW and SG have similar per-plexity results, SG appears to be stronger in thenarrative cloze test.
With respect to the strongestbaseline (UNI), LB achieves close to 20% rela-tive improvement for Recall@30 metric on NYThold-out data.
On gold data, the frame-chainSemLMs get a relative improvement of 36.5%for Recall@30 while entity-centered SemLMs get22.3%.
For MRR metric, the relative improvementis around half that of the Recall@30 metric.In the narrative cloze test, we also carry out anablation study to remove the ?FrameNet Mapping?step in Sec.
5.2 (?FC-FM?
and ?EC-FM?
rows).The decrease in MRR and Recall@30 metricsfurther strengthens the argument that ?FrameNetMapping?
is important for language modeling asit improves the generalization on frames.We cannot directly compare with other re-lated works (Rudinger et al, 2015; Pichotta andMooney, 2016) because of the differences in dataand evaluation metrics.
Rudinger et al (2015) alsouse the NYT portion of the Gigaword corpus, butwith Concrete annotations; Pichotta and Mooney(2016) use the English Wikipedia as their data, andStanford NLP tools for pre-processing while weuse the Illinois NLP tools.
Consequently, the even-tual chain statistics are different, which leads todifferent test instances.5We counter this difficulty5Rudinger et al (2015) is similar to our entity-centeredSemLM without discourse information.
So, in Table 4, we296Table 5: Co-reference resolution results withentity-centered SemLM features.
?EC?
standsfor the entity-centered SemLM.
?TRI?
is the tri-gram model while ?LB?
is the log-bilinear model.?pc?
means conditional probability features and?em?
represents frame embedding features.
?w/oDIS?
indicates the ablation study by removing alldiscourse makers for SemLMs.
We conduct theexperiments by adding SemLM features into thebase system.
We outperform the state-of-art sys-tem (Wiseman et al, 2015), which reports the bestresults on CoNLL12 dataset.
The improvementachieved by ?EC LB (pc+em)?
over the base sys-tem is statistically significant.ACE04 CoNLL12Wiseman et al (2015) ??
63.39Base (Peng et al, 2015a) 71.20 63.03Base+EC-TRI (pc) 71.31 63.14Base+EC-TRI w/o DIS 71.08 62.99Base+EC-LB (pc) 71.71 63.42Base+EC-LB (pc+ em) 71.79 63.46Base+EC-LB w/o DIS 71.12 63.00by reporting results on ?Gold PropBank Data?
and?Gold Ontonotes Data?.
We hope that these twogold annotation datasets can become standard testsets.
Rudinger et al (2015) does share a commonevaluation metric with us: MRR.
If we ignore thedata difference and make a rough comparison, wefind that the absolute values of our results are bet-ter while Rudinger et al (2015) have higher rela-tive improvement (?Rel-Impr?
in Table 4).
Thismeans that 1) the discourse information is verylikely to help better model semantics 2) the dis-course information may boost the baseline (UNI)more than it does for the LB model.6.2 Evaluation of SemLM Applications6.2.1 Co-reference ResolutionCo-reference resolution is the task of identifyingmentions that refer to the same entity.
To help im-prove its performance, we incorporate SemLM in-formation as features into an existing co-referenceresolution system.
We choose the state-of-art Illi-nois Co-reference Resolution system (Peng et al,2015a) as our base system.
It employs a su-pervised joint mention detection and co-referenceframework.
We add additional features into themention-pair feature set.Given a pair of mentions (m1,m2) where m1make a rough comparison between them.appears beforem2, we first extract the correspond-ing semantic frame and the argument role label ofeach mention.
We do this by following the proce-dures in Sec.
5.
Thus, we can get a pair of semanticframes with argument information (fa1, fa2).
Wemay also get an additional discourse marker be-tween these two frames, e.g.
(fa1, dis, fa2).
Now,we add the following conditional probability as thefeature from SemLMs:pc= p(fa2|fa1, dis).We also add p2c,?pcand 1/pcas features.
To getthe value of pc, we follow the definitions in Sec.
4,and we only use the entity-centered SemLM hereas its vocabulary covers frames with argument la-bels.
For the neural language model implementa-tions (CBOW, SG and LB), we also include frameembeddings as additional features.We evaluate the effect of the added SemLMfeatures on two co-reference benchmark datasets:ACE04 (NIST, 2004) and CoNLL12 (Pradhan etal., 2012).
We use the standard split of 268 train-ing documents, 68 development documents, and106 testing documents for ACE04 data (Culottaet al, 2007; Bengtson and Roth, 2008).
ForCoNLL12 data, we follow the train and test doc-ument split from CoNLL-2012 Shared Task.
Wereport CoNLL AVG for results (average of MUC,B3, and CEAFemetrics), using the v7.0 scorerprovided by the CoNLL-2012 Shared Task.Co-reference resolution results with entity-centered SemLM features are shown in Table 5.Tri-grams with conditional probability featuresimprove the performance by a small margin, whilethe log-bilinear model achieves a 0.4-0.5 F1 pointsimprovement.
By employing log-bilinear modelembeddings, we further improve the numbers andwe outperform the best reported results on theCoNLL12 dataset (Wiseman et al, 2015).In addition, we carry out ablation studies to re-move all discourse makers during the languagemodeling process.
We re-train our models andstudy their effects on the generated features.
Ta-ble 5 (?w/o DIS?
rows) shows that without dis-course information, the SemLM features wouldhurt the overall performance, thus proving the ne-cessity of considering discourse for semantic lan-guage models.6.2.2 Shallow Discourse ParsingShallow discourse parsing is the task of identi-fying explicit and implicit discourse connectives,297Table 6: Shallow discourse parsing results with frame-chain SemLM features.
?FC?
stands for theframe-chain SemLM.
?TRI?
is the tri-gram model while ?LB?
is the log-bilinear model.
?pc?, ?em?are conditional probability and frame embedding features, resp.
?w/o DIS?
indicates the case where weremove all discourse makers for SemLMs.
We do the experiments by adding SemLM features to the basesystem.
The improvement achieved by ?FC-LB (pc+ em)?
over the baseline is statistically significant.CoNLL16 Test CoNLL16 BlindExplicit Implicit Overall Explicit Implicit OverallBase (Song et al, 2015) 89.8 35.6 60.4 75.8 31.9 52.3Base + FC-TRI (qc) 90.3 35.8 60.7 76.4 32.5 52.9Base + FC-TRI w/o DIS 89.2 35.3 60.0 75.5 31.6 52.0Base + FC-LB (qc) 90.9 36.2 61.3 76.8 32.9 53.4Base + FC-LB (qc+ em) 91.1 36.3 61.4 77.3 33.2 53.8Base + FC-LB w/o DIS 90.1 35.7 60.6 76.9 33.0 53.5determine their senses and their discourse argu-ments.
In order to show that SemLM can help im-prove shallow discourse parsing, we evaluate onidentifying the correct sense of discourse connec-tives (both explicit and implicit ones).We choose Song et al (2015), which uses a su-pervised pipeline approach, as our base system.The system extracts context features for potentialdiscourse connectives and applies the discourseconnective sense classifier.
Consider an explicitconnective ?dis?
; we extract the semantic framesthat are closest to it (left and right), resulting in thesequence [f1, dis, f2] by following the proceduresdescribed in Sec.
5.
We then add the followingconditional probabilities as features.
Computeqc= p(dis|f1, f2).and, similar to what we do for co-reference resolu-tion, we add qc, q2c,?qc, 1/qcas conditional prob-ability features, which can be computed followingthe definitions in Sec.
4.
We also include frameembeddings as additional features.
We only useframe-chain SemLMs here.We evaluate on CoNLL16 (Xue et al, 2015)test and blind sets, following the train and devel-opment document split from the Shared Task, andreport F1 using the official shared task scorer.Table 6 shows the results for shallow discourseparsing with SemLM features.
Tri-gram with con-ditional probability features improve the perfor-mance for both explicit and implicit connectivesense classifiers.
Log-bilinear model with condi-tional probability features achieves even better re-sults, and frame embeddings further improve thenumbers.
SemLMs improve relatively more on ex-plicit connectives than on implicit ones.We also show an ablation study in the samesetting as we did for co-reference, i.e.
removingdiscourse information (?w/o DIS?
rows).
Whileour LB model can still exhibit improvement overthe base system, its performance is lower than theproposed discourse driven version, which meansthat discourse information improves the expres-siveness of semantic language models.7 ConclusionThe paper builds two types of discourse driven se-mantic language models with four different lan-guage model implementations that make use ofneural embeddings for semantic frames.
We useperplexity and a narrative cloze test to prove thatthe proposed SemLMs have a good level of ab-straction and are of high quality, and then ap-ply them successfully to the two challenging tasksof co-reference resolution and shallow discourseparsing, exhibiting improvements over state-of-the-art systems.
In future work, we plan to applySemLMs to other semantic related NLP tasks e.g.machine translation and question answering.AcknowledgmentsThe authors would like to thank ChristosChristodoulopoulos and Eric Horn for commentsthat helped to improve this work.
This work issupported by Contract HR0011-15-2-0025 withthe US Defense Advanced Research ProjectsAgency (DARPA).
Approved for Public Release,Distribution Unlimited.
The views expressed arethose of the authors and do not reflect the officialpolicy or position of the Department of Defense orthe U.S. Government.
This material is also basedupon work supported by the U.S. Department ofHomeland Security under Award Number 2009-ST-061-CCI002-07.298ReferencesC.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
Theberkeley framenet project.
In COLING/ACL, pages86?90.N.
Balasubramanian, S. Soderland, Mausam, andO.
Etzioni.
2013.
Generating coherent eventschemas at scale.
In EMNLP, pages 1721?1731.D.
Bamman and N. A. Smith.
2014.
Unsuperviseddiscovery of biographical structure from text.
TACL,2:363?376.C.
A. Bejan.
2008.
Unsupervised discovery of eventscenarios from texts.
In FLAIRS Conference, pages124?129.E.
Bengtson and D. Roth.
2008.
Understanding thevalue of features for coreference resolution.
InEMNLP.N.
Chambers and D. Jurafsky.
2008a.
Jointly combin-ing implicit constraints improves temporal ordering.In EMNLP.N.
Chambers and D. Jurafsky.
2008b.
Unsupervisedlearning of narrative event chains.
In ACL, volume94305, pages 789?797.N.
Chambers and D. Jurafsky.
2009.
Unsupervisedlearning of narrative schemas and their participants.In ACL, volume 2, pages 602?610.N.
Chambers.
2013.
Event schema induction with aprobabilistic entity-driven model.
In EMNLP, vol-ume 13, pages 1797?1807.J.
C. K. Cheung, H. Poon, and L. Vanderwende.
2013.Probabilistic frame induction.
arXiv:1302.4813.A.
Culotta, M. Wick, R. Hall, and A. McCallum.
2007.First-order probabilistic models for coreference res-olution.
In NAACL.L.
Frermann, I. Titov, and Pinkal.
M. 2014.
A hierar-chical bayesian model for unsupervised induction ofscript knowledge.
In EACL.M.
Granroth-Wilding, S. Clark, M. T. Llano, R. Hep-worth, S. Colton, J. Gow, J. Charnley, N.
Lavra?c,M.
?Znidar?si?c, and M. Perov?sek.
2015.
What hap-pens next?
event prediction using a compositionalneural network model.M.
Gutmann and A. Hyvarinen.
2010.
Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models.
In AISTATS.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90% solu-tion.
In Proceedings of HLT/NAACL.J.
Irwin, M. Komachi, and Y. Matsumoto.
2011.
Nar-rative schema as world knowledge for coreferenceresolution.
In CoNLL Shared Task, pages 86?92.B.
Jans, S. Bethard, I. Vuli?c, and M. F. Moens.
2012.Skip n-grams and ranking functions for predictingscript events.
In EACL, pages 336?344.P.
Kingsbury and M. Palmer.
2002.
From Treebank toPropBank.
In Proceedings of LREC-2002.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In ICASSP.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013a.Efficient estimation of word representations in vec-tor space.
arXiv:1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In NAACL.A.
Mnih and G. Hinton.
2007.
Three new graphicalmodels for statistical language modelling.
In ICML,pages 641?648.A.
Modi and I. Titov.
2014a.
Inducing neural modelsof script knowledge.
In CoNLL.A.
Modi and I. Titov.
2014b.
Learning semantic scriptknowledge with event embeddings.
In ICLR Work-shop.R.
Mooney and G. DeJong.
1985.
Learning schematafor natural language processing.K.-H. Nguyen, X. Tannier, O. Ferret, and R. Besanc?on.2015.
Generative event schema induction with en-tity disambiguation.
In ACL.US NIST.
2004.
The ace evaluation plan.
US NationalInstitute for Standards and Technology (NIST).B.
Paul, B. Phil, and H. Hieu.
2014.
Oxlm: A neurallanguage modelling framework for machine transla-tion.
The Prague Bulletin of Mathematical Linguis-tics, 102(1):81?92.H.
Peng, K. Chang, and D. Roth.
2015a.
A joint frame-work for coreference resolution and mention headdetection.
In CoNLL.H.
Peng, D. Khashabi, and D. Roth.
2015b.
Solvinghard coreference problems.
In NAACL.K.
Pichotta and R. J. Mooney.
2014.
Statistical scriptlearning with multi-argument events.
In EACL, vol-ume 14, pages 220?229.K.
Pichotta and R. J. Mooney.
2016.
Learning statis-tical scripts with lstm recurrent neural networks.
InAAAI.S.
Pradhan, A. Moschitti, N. Xue, O. Uryupina, andY.
Zhang.
2012.
CoNLL-2012 shared task:Modeling multilingual unrestricted coreference inOntoNotes.
In CoNLL.299Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC 2008).V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic role labeling via integer linear program-ming inference.
In COLING.A.
Rahman and V. Ng.
2011.
Coreference resolutionwith world knowledge.
In ACL.D.
Roth and D. Zelenko.
1998.
Part of speech taggingusing a network of linear separators.
In COLING-ACL.R.
Rudinger, P. Rastogi, F. Ferraro, and B.
Van Durme.2015.
Script induction as language modeling.
InEMNLP.R.
C. Schank and R. P. Abelson.
1977.
Scripts, plans,goals, and understanding: An inquiry into humanknowledge structures.
In JMZ.K.
K. Schuler.
2005.
Verbnet: A broad-coverage, com-prehensive verb lexicon.Y.
Song, H. Peng, P. Kordjamshidi, M. Sammons, andD.
Roth.
2015.
Improving a pipeline architecturefor shallow discourse parsing.
In CoNLL SharedTask.A.
Stolcke.
2002.
Srilm-an extensible language mod-eling toolkit.
In INTERSPEECH, volume 2002,page 2002.I.
Titov and E. Khoddam.
2015.
Unsupervised induc-tion of semantic roles within a reconstruction-errorminimization framework.
In NAACL.Ben Wellner and James Pustejovsky.
2007.
Automati-cally identifying the arguments of discourse connec-tives.
In Proceedings of the 2007 Joint Conferenceof EMNLP-CoNLL.T.
Winograd.
1972.
Understanding natural language.Cognitive psychology, 3(1):1?191.S.
Wiseman, A. M. Rush, S. M. Shieber, and J. Weston.2015.
Learning anaphoricity and antecedent rankingfeatures for coreference resolution.
In ACL.N.
Xue, H. T. Ng, S. Pradhan, R. P. C. Bryant, andA.
T. Rutherford.
2015.
The conll-2015 shared taskon shallow discourse parsing.
In CoNLL.300
