Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54?58,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 11: English Lexical Sample Taskvia English-Chinese Parallel TextHwee Tou Ng and Yee Seng ChanDepartment of Computer ScienceNational University of Singapore3 Science Drive 2, Singapore 117543{nght, chanys}@comp.nus.edu.sgAbstractWe made use of parallel texts to gather train-ing and test examples for the English lexi-cal sample task.
Two tracks were organizedfor our task.
The first track used examplesgathered from an LDC corpus, while thesecond track used examples gathered froma Web corpus.
In this paper, we describethe process of gathering examples from theparallel corpora, the differences with similartasks in previous SENSEVAL evaluations,and present the results of participating sys-tems.1 IntroductionAs part of the SemEval-2007 evaluation exercise, weorganized an English lexical sample task for wordsense disambiguation (WSD), where the sense-annotated examples were semi-automatically gath-ered from word-aligned English-Chinese paralleltexts.
Two tracks were organized for this task, eachgathering data from a different corpus.
In this paper,we describe our motivation for organizing the task,our task framework, and the results of participants.Past research has shown that supervised learningis one of the most successful approaches to WSD.However, this approach involves the collection ofa large text corpus in which each ambiguous wordhas been annotated with the correct sense to serve astraining data.
Due to the expensive annotation pro-cess, only a handful of manually sense-tagged cor-pora are available.An effort to alleviate the training data bottle-neck is the Open Mind Word Expert (OMWE)project (Chklovski and Mihalcea, 2002) to collectsense-tagged data from Internet users.
Data gath-ered through the OMWE project were used in theSENSEVAL-3 English lexical sample task.
In thattask, WordNet-1.7.1 was used as the sense inven-tory for nouns and adjectives, while Wordsmyth1was used as the sense inventory for verbs.Another source of potential training data is par-allel texts.
Our past research in (Ng et al, 2003;Chan and Ng, 2005) has shown that examples gath-ered from parallel texts are useful for WSD.
Briefly,after manually assigning appropriate Chinese trans-lations to each sense of an English word, the Englishside of a word-aligned parallel text can then serve asthe training data, as they are considered to have beendisambiguated and ?sense-tagged?
by the appropri-ate Chinese translations.Using the above approach, we gathered the train-ing and test examples for our task from parallel texts.Note that our examples are collected without manu-ally annotating each individual ambiguous word oc-currence, allowing us to gather our examples in amuch shorter time.
This contrasts with the setting ofthe English lexical sample task in previous SENSE-VAL evaluations.
In the English lexical sample taskof SENSEVAL-2, the sense tagged data were cre-ated through manual annotation by trained lexicog-raphers.
In SENSEVAL-3, the data were gatheredthrough manual sense annotation by Internet users.In the next section, we describe in more detailthe process of gathering examples from parallel textsand the two different parallel corpora we used.
Wethen give a brief description of each of the partici-1http://www.wordsmyth.net54pating systems.
In Section 4, we present the resultsobtained by the participants, before concluding inSection 5.2 Gathering Examples from ParallelCorporaTo gather examples from parallel corpora, we fol-lowed the approach in (Ng et al, 2003).
Briefly, af-ter ensuring the corpora were sentence-aligned, wetokenized the English texts and performed word seg-mentation on the Chinese texts (Low et al, 2005).We then made use of the GIZA++ software (Och andNey, 2000) to perform word alignment on the paral-lel corpora.
Then, we assigned some possible Chi-nese translations to each sense of an English wordw.
From the word alignment output of GIZA++, weselected those occurrences of w which were alignedto one of the Chinese translations chosen.
The En-glish side of these occurrences served as trainingdata for w, as they were considered to have been dis-ambiguated and ?sense-tagged?
by the appropriateChinese translations.
The English half of the par-allel texts (each ambiguous English word and its 3-sentence context) were used as the training and testmaterial to set up our English lexical sample task.Note that in our approach, the sense distinctionis decided by the different Chinese translations as-signed to each sense of a word.
This is thussimilar to the multilingual lexical sample task inSENSEVAL-3 (Chklovski et al, 2004), except thatour training and test examples are collected with-out manually annotating each individual ambiguousword occurrence.
The average time needed to assignChinese translations for one noun and one adjectiveis 20 minutes and 25 minutes respectively.
This isa relatively short time, compared to the effort other-wise needed to manually sense annotate individualword occurrences.
Also, once the Chinese transla-tions are assigned, more examples can be automat-ically gathered as more parallel texts become avail-able.We note that frequently occurring words are usu-ally highly polysemous and hard to disambiguate.To maximize the benefits of our work, we gatheredtraining data from parallel texts for a set of most fre-quently occurring noun and adjective types in theBrown Corpus.
Also, similar to the SENSEVAL-3Dataset Avg.
no.
Avg.
no.
of examplesof senses Training TestLDC noun 5.2 197.6 98.5LDC adjective 3.9 125.6 62.9Web noun 3.5 182.0 91.3Web adjective 2.8 88.8 44.6Table 1: Average number of senses, training exam-ples, and test examples per word.English lexical sample task, we used WordNet-1.7.1as our sense inventory.2.1 LDC CorpusWe have two tracks for this task, each track using adifferent corpus.
The first corpus is the Chinese En-glish News Magazine Parallel Text (LDC2005T10),which is an English-Chinese parallel corpus avail-able from the Linguistic Data Consortium (LDC).From this parallel corpus, we gathered examplesfor 50 English words (25 nouns and 25 adjectives)using the method described above.
From the gath-ered examples of each word, we randomly selectedtraining and test examples, where the number oftraining examples is about twice the number of testexamples.The rows LDC noun and LDC adjective in Table1 give some statistics about the examples.
For in-stance, each noun has an average of 197.6 trainingand 98.5 test examples and these examples repre-sent an average of 5.2 senses per noun.2 Participantstaking part in this track need to have access to thisLDC corpus in order to access the training and testmaterial in this track.2.2 Web CorpusSince not all interested participants may have accessto the LDC corpus described in the previous sub-section, the second track of this task makes use ofEnglish-Chinese documents gathered from the URLpairs given by the STRAND Bilingual Databases.3STRAND (Resnik and Smith, 2003) is a system thatacquires document pairs in parallel translation auto-matically from the Web.
Using this corpus, we gath-ered examples for 40 English words (20 nouns and2Only senses present in the examples are counted.3http://www.umiacs.umd.edu/?resnik/strand5520 adjectives).The rows Web noun and Web adjective in Table 1show that we selected an average of 182.0 trainingand 91.3 test examples for each noun and these ex-amples represent an average of 3.5 senses per noun.We note that the average number of senses per wordfor the Web corpus is slightly lower than that of theLDC corpus.2.3 Annotation AccuracyTo measure the annotation accuracy of examplesgathered from the LDC corpus, we examined a ran-dom selection of 100 examples each from 5 nounsand 5 adjectives.
From these 1,000 examples, wemeasured a sense annotation accuracy of 84.7%.These 10 words have an average of 8.6 senses perword in the WordNet-1.7.1 sense inventory.
As de-scribed in (Ng et al, 2003), when several sensesof an English word are translated by the same Chi-nese word, we can collapse these senses to obtain acoarser-grained, lumped sense inventory.
If we dothis and measure the sense annotation accuracy withrespect to a coarser-grained, lumped sense inventory,these 10 words will have an average of 6.5 senses perword and an annotation accuracy of 94.7%.For the Web corpus, we similarly examined a ran-dom selection of 100 examples each from 5 nounsand 5 adjectives.
These 10 words have an average of6.5 senses per word in WordNet-1.7.1 and the 1,000examples have an average sense annotation accuracyof 85.0%.
After sense collapsing, annotation ac-curacy is 95.3% with an average of 4.8 senses perword.2.4 Training and Test Data from DifferentDocumentsIn our previous work (Ng et al, 2003), we conductedexperiments on the nouns of SENSEVAL-2 Englishlexical sample task.
We found that there were caseswhere the same document contributed both trainingand test examples and this inflated the WSD accu-racy figures.
To avoid this, during our preparationof the LDC and Web data, we made sure that a doc-ument contributed only either training or test exam-ples, but not both.3 Participating SystemsThree teams participated in the Web corpus trackof our task, with each team employing one system.There were no participants in the LDC corpus track,possibly due to the licensing issues involved.
Allparticipating systems employed supervised learningand only used the training examples provided by us.3.1 CITYU-HIFThe CITYU-HIF team from the City University ofHong Kong trained a naive Bayes (NB) classifierfor each target word to be disambiguated, usingknowledge sources such as parts-of-speech (POS) ofneighboring words and single words in the surround-ing context.
They also experimented with using dif-ferent sets of features for each target word.3.2 HIT-IR-WSDThe system submitted by the HIT-IR-WSD teamfrom Harbin Institute of Technology used SupportVector Machines (SVM) with a linear kernel func-tion as the learning algorithm.
Knowledge sourcesused included POS of surrounding words, local col-locations, single words in the surrounding context,and syntactic relations.3.3 PKUThe system submitted by the PKU team from PekingUniversity used a combination of SVM and maxi-mum entropy classifiers.
Knowledge sources usedincluded POS of surrounding words, local colloca-tions, and single words in the surrounding context.Feature selection was done by ignoring word fea-tures with certain associated POS tags and by se-lecting the subset of features based on their entropyvalues.4 ResultsAs all participating systems gave only one answerfor each test example, recall equals precision andwe will only report micro-average recall on the Webcorpus track in this section.Table 2 gives the overall results obtained by eachof the systems when evaluated on all the test exam-ples of the Web corpus.
We note that all the par-ticipants obtained scores which exceed the baselineheuristic of tagging all test examples with the most56System ID Contact author Learning algorithm ScoreHIT-IR-WSD Yuhang Guo, <astronaut@ir.hit.edu.cn> SVM 0.819PKU Peng Jin, <jandp@pku.edu.cn> SVM and maximum entropy 0.815CITYU-HIF Oi Yee Kwong, <rlolivia@cityu.edu.hk> NB 0.753MFS ?
Most frequent sense baseline 0.689Table 2: Overall micro-average scores of the participants and the most frequent sense (MFS) baseline.Noun MFS CITYU-HIF HIT-IR-WSD PKUage 0.486 0.643 0.743 0.700area 0.480 0.693 0.773 0.773body 0.872 0.897 0.910 0.923change 0.411 0.400 0.578 0.611director 0.580 0.890 0.960 0.960experience 0.830 0.830 0.880 0.840future 0.889 0.889 0.990 0.990interest 0.308 0.165 0.813 0.780issue 0.651 0.711 0.892 0.855life 0.820 0.830 0.860 0.740material 0.719 0.719 0.781 0.641need 0.907 0.907 0.918 0.918performance 0.410 0.570 0.690 0.700program 0.590 0.590 0.730 0.690report 0.870 0.840 0.880 0.870system 0.510 0.700 0.610 0.730time 0.455 0.673 0.733 0.693today 0.800 0.750 0.800 0.780water 0.882 0.921 0.868 0.895work 0.644 0.743 0.842 0.891Micro-avg 0.656 0.719 0.813 0.802Table 3: Micro-average scores of the most frequentsense baseline and the various participants on eachnoun.frequent sense (MFS) in the training data.
This sug-gests that the Chinese translations assigned to sensesof the ambiguous words are appropriate and providesense distinctions which are clear enough for effec-tive classifiers to be learned.In Table 3 and Table 4, we show the scores ob-tained by each system on each of the 20 nouns and20 adjectives.
For comparison purposes, we alsoshow the corresponding MFS score of each word.Paired t-test on the results of the top two systemsshow no significant difference between them.5 ConclusionWe organized an English lexical sample task usingexamples gathered from parallel texts.
Unlike theEnglish lexical task of previous SENSEVAL evalua-tions where each example is manually annotated, weAdjective MFS CITYU-HIF HIT-IR-WSD PKUancient 0.778 0.667 0.778 0.741bad 0.857 0.857 0.905 0.905common 0.533 0.567 0.533 0.633early 0.769 0.846 0.769 0.769educational 0.911 0.911 0.911 0.911free 0.760 0.792 0.854 0.917high 0.630 0.926 0.815 0.852human 0.872 0.987 0.962 0.962little 0.450 0.750 0.650 0.650long 0.667 0.690 0.786 0.714major 0.870 0.902 0.880 0.913medical 0.738 0.787 0.800 0.725national 0.267 0.467 0.667 0.700new 0.441 0.441 0.529 0.559present 0.875 0.917 0.875 0.875rare 0.727 0.818 0.727 0.909serious 0.879 0.879 0.879 0.879simple 0.795 0.818 0.864 0.864small 0.714 0.929 0.893 0.929third 0.888 0.988 0.963 0.963Micro-avg 0.757 0.823 0.831 0.842Table 4: Micro-average scores of the most frequentsense baseline and the various participants on eachadjective.only need to assign appropriate Chinese translationsto each sense of a word.
Once this is done, we auto-matically gather training and test examples from theparallel texts.
All the participating systems of ourtask obtain results that are significantly better thanthe most frequent sense baseline.6 AcknowledgementsYee Seng Chan is supported by a Singapore Millen-nium Foundation Scholarship (ref no.
SMF-2004-1076).ReferencesYee Seng Chan and Hwee Tou Ng.
2005.
Scalingup word sense disambiguation via parallel texts.
InProceedings of AAAI05, pages 1037?1042, Pittsburgh,Pennsylvania, USA.57Timothy Chklovski and Rada Mihalcea.
2002.
Buildinga sense tagged corpus with Open Mind Word Expert.In Proceedings of ACL02 Workshop on Word SenseDisambiguation: Recent Successes and Future Direc-tions, pages 116?122, Philadelphia, USA.Timothy Chklovski, Rada Mihalcea, Ted Pedersen, andAmruta Purandare.
2004.
The SENSEVAL-3 multi-lingual English-Hindi lexical sample task.
In Proceed-ings of SENSEVAL-3, pages 5?8, Barcelona, Spain.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.
Amaximum entropy approach to Chinese word segmen-tation.
In Proceedings of the Fourth SIGHAN Work-shop on Chinese Language Processing, pages 161?164, Jeju Island, Korea.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Ex-ploiting parallel texts for word sense disambiguation:An empirical study.
In Proceedings of ACL03, pages455?462, Sapporo, Japan.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceeedings of ACL00,pages 440?447, Hong Kong.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.58
