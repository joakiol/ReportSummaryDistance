Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 780?788,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLetter-Phoneme Alignment: An ExplorationSittichai Jiampojamarn and Grzegorz KondrakDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, T6G 2E8, Canada{sj,kondrak}@cs.ualberta.caAbstractLetter-phoneme alignment is usually gen-erated by a straightforward application ofthe EM algorithm.
We explore several al-ternative alignment methods that employphonetics, integer programming, and setsof constraints, and propose a novel ap-proach of refining the EM alignment byaggregation of best alignments.
We per-form both intrinsic and extrinsic evalua-tion of the assortment of methods.
Weshow that our proposed EM-Aggregationalgorithm leads to the improvement of thestate of the art in letter-to-phoneme con-version on several different data sets.1 IntroductionLetter-to-phoneme (L2P) conversion (also calledgrapheme-to-phoneme conversion) is the task ofpredicting the pronunciation of a word given itsorthographic form by converting a sequence ofletters into a sequence of phonemes.
The L2Ptask plays a crucial role in speech synthesis sys-tems (Schroeter et al, 2002), and is an importantpart of other applications, including spelling cor-rection (Toutanova and Moore, 2001) and speech-to-speech machine translation (Engelbrecht andSchultz, 2005).
Many data-driven techniques havebeen proposed for letter-to-phoneme conversionsystems, including neural networks (Sejnowskiand Rosenberg, 1987), decision trees (Black et al,1998), pronunciation by analogy (Marchand andDamper, 2000), Hidden Markov Models (Taylor,2005), and constraint satisfaction (Bosch and Can-isius, 2006).Letter-phoneme alignment is an important stepin the L2P task.
The training data usually consistsof pairs of letter and phoneme sequences, whichare not aligned.
Since there is no explicit infor-mation indicating the relationships between indi-vidual letter and phonemes, these must be inferredby a letter-phoneme alignment algorithm beforea prediction model can be trained.
The qualityof the alignment affects the accuracy of L2P con-version.
Letter-phoneme alignment is closely re-lated to transliteration alignment (Pervouchine etal., 2009), which involves graphemes representingdifferent writing scripts.
Letter-phoneme align-ment may also be considered as a task in itself; forexample, in the alignment of speech transcriptionwith text in spoken corpora.Most previous L2P approaches induce the align-ment between letters and phonemes with the ex-pectation maximization (EM) algorithm.
In thispaper, we propose a number of alternative align-ment methods, and compare them to the EM-based algorithms using both intrinsic and extrin-sic evaluations.
The intrinsic evaluation is con-ducted by comparing the generated alignments toa manually-constructed gold standard.
The extrin-sic evaluation uses two different generation tech-niques to perform letter-to-phoneme conversionon several different data sets.
We discuss the ad-vantages and disadvantages of various methods,and show that better alignments tend to improvethe accuracy of the L2P systems regardless of theactual technique.
In particular, one of our pro-posed methods advances the state of the art in L2Pconversion.
We also examine the relationship be-tween alignment entropy and alignment quality.This paper is organized as follows.
In Sec-tion 2, we enumerate the assumptions that thealignment methods commonly adopt.
In Section 3,we review previous work that employs the EM ap-proach.
In Sections 4, 5 and 6, we describe alter-native approaches based on phonetics, manually-constructed constraints, and Integer Programming,respectively.
In Section 7, we propose an algo-rithm to refine the alignments produced by EM.Sections 8 and 9 are devoted to the intrinsic andextrinsic evaluation of various approaches.
Sec-tion 10 concludes the paper.7802 BackgroundWe define the letter-phoneme alignment task asthe problem of inducing links between units thatare related by pronunciation.
Each link is an in-stance of a specific mapping between letters andphonemes.
The leftmost example alignment of theword accuse [@kjuz] below includes 1-1, 1-0, 1-2, and 2-1 links.
The letter e is considered to belinked to special null phoneme.Figure 1: Two alignments of accuse.The following constraints on links are assumedby some or all alignment models:?
the monotonicity constraint prevents linksfrom crossing each other;?
the representation constraint requires eachphoneme to be linked to at least one letter,thus precluding nulls on the letter side;?
the one-to-one constraint stipulates that eachletter and phoneme may participate in at mostone link.These constraints increasingly reduce the searchspace and facilitate the training process for theL2P generation models.We refer to an alignment model that assumes allthree constraints as a pure one-to-one (1-1) model.By allowing only 1-1 and 1-0 links, the align-ment task is thus greatly simplified.
In the sim-plest case, when the number of letters is equal tothe number of phonemes, there is only one pos-sible alignment that satisfies all three constraints.When there are more letters than phonemes, thesearch is reduced to identifying letters that mustbe linked to null phonemes (the process referredto as ?epsilon scattering?
by Black et al (1998)).In some words, however, one letter clearly repre-sents more than one phoneme; for example, u inFigure 1.
Moreover, a pure 1-1 approach cannothandle cases where the number of phonemes ex-ceeds the number of letters.
A typical solution toovercome this problems is to introduce so-calleddouble phonemes by merging adjacent phonemesthat could be represented as a single letter.
Forexample, a double phoneme U would replace a se-quence of the phonemes j and u in Figure 1.
Thissolution requires a manual extension of the set ofphonemes present in the data.
By convention, weregard the models that include a restricted set of1-2 mappings as 1-1 models.Advanced L2P approaches, including the jointn-gram models (Bisani and Ney, 2008) and thejoint discriminative approach (Jiampojamarn etal., 2007) eliminate the one-to-one constraint en-tirely, allowing for linking of multiple letters tomultiple phonemes.
We refer to such models asmany-to-many (M-M) models.3 EM AlignmentEarly EM-based alignment methods (Daelemansand Bosch, 1997; Black et al, 1998; Damper etal., 2005) were generally pure 1-1 models.
The1-1 alignment problem can be formulated as a dy-namic programming problem to find the maximumscore of alignment, given a probability table ofaligning letter and phoneme as a mapping func-tion.
The dynamic programming recursion to findthe most likely alignment is the following:Ci,j = max??
?Ci?1,j?1 + ?
(xi, yj)Ci?1,j + ?
(xi, ?
)Ci,j?1 + ?
(?, yj)(1)where ?
(xi, ?)
denotes a probability that a let-ter xi aligns with a null phoneme and ?
(?, yj) de-notes a probability that a null letter aligns with aphoneme yj .
In practice, the latter probability isoften set to zero in order to enforce the represen-tation constraint, which facilitates the subsequentphoneme generation process.
The probability ta-ble ?
(xi, yj) can be initialized by a uniform dis-tribution and is iteratively re-computed (M-step)from the most likely alignments found at each it-eration over the data set (E-step).
The final align-ments are constructed after the probability tableconverges.M2M-aligner (Jiampojamarn et al, 2007) is amany-to-many (M-M) alignment algorithm basedon EM that allows for mapping of multiple let-ters to multiple phonemes.
Algorithm 1 describesthe E-step of the many-to-many alignment algo-rithm.
?
represents partial counts collected overall possible mappings between substrings of let-ters and phonemes.
The maximum lengths of let-ter and phoneme substrings are controlled by the781Algorithm 1: Many-to-many alignmentInput: x, y,maxX,maxY, ?Output: ??
:= FORWARD-M2M (x, y,maxX,maxY )1?
:= BACKWARD-M2M (x, y,maxX,maxY )2T = |x| + 1 , V = |y| + 13if (?T,V = 0) then4return5for t = 1..T , v = 1..V do6for i = 1..maxX st t ?
i ?
0 do7?
(xtt?i+1, ?)
+=?t?i,v?(xtt?i+1,?
)?t,v?T,V8for i = 1..maxX st t ?
i ?
0 do9for j = 1..maxY st v ?
j ?
0 do10?
(xtt?i+1, yvv?j+1) +=?t?i,v?j?
(xtt?i+1,yvv?j+1)?t,v?T,V11maxX and maxY parameters.
The forward prob-ability ?
is estimated by summing the probabilitiesfrom left to right, while the backward probabil-ity ?
is estimated in the opposite direction.
TheFORWARD-M2M procedure is similar to line 3 to10 of Algorithm 1, except that it uses Equation 2in line 8 and 3 in line 11.
The BACKWARD-M2Mprocedure is analogous to FORWARD-M2M.
?t,v += ?
(xtt?i+1, ?
)?t?i,v (2)?t,v += ?
(xtt?i+1, yvv?j+1)?t?i,v?j (3)In M-step, the partial counts are normalizedby using a conditional distribution to create themapping probability table ?.
The final many-to-many alignments are created by finding the mostlikely paths using the Viterbi algorithm based onthe learned mapping probability table.
The sourcecode of M2M-aligner is publicly available.1Although the many-to-many approach tends tocreate relatively large models, it generates moreintuitive alignments and leads to improvement inthe L2P accuracy (Jiampojamarn et al, 2007).However, since many links involve multiple let-ters, it also introduces additional complexity in thephoneme prediction phase.
One possible solutionis to apply a letter segmentation algorithm at testtime to cluster letters according to the alignmentsin the training data.
This is problematic becauseof error propagation inherent in such a process.A better solution is to combine segmentation anddecoding using a phrasal decoder (e.g.
(Zens andNey, 2004)).1http://code.google.com/p/m2m-aligner/4 Phonetic alignmentThe EM-based approaches to L2P alignment treatboth letters and phonemes as abstract symbols.A completely different approach to L2P align-ment is based on the phonetic similarity betweenphonemes.
The key idea of the approach is to rep-resent each letter by a phoneme that is likely to berepresented by the letter.
The actual phonemes onthe phoneme side and the phonemes representingletters on the letter side can then be aligned on thebasis of phonetic similarity between phonemes.The main advantage of the phonetic alignment isthat it requires no training data, and so can be read-ily be applied to languages for which no pronunci-ation lexicons are available.The task of identifying the phoneme that is mostlikely to be represented by a given letter may seemcomplex and highly language-dependent.
For ex-ample, the letter a can represent no less than 12different English vowels.
In practice, however, ab-solute precision is not necessary.
Intuitively, theletters that had been chosen (often centuries ago)to represent phonemes in any orthographic systemtend to be close to the prototype phoneme in theoriginal script.
For example, the letter ?o?
rep-resented a mid-high rounded vowel in ClassicalLatin and is still generally used to represent simi-lar vowels.The following simple heuristic works well for anumber of languages: treat every letter as if it werea symbol in the International Phonetic Alphabet(IPA).
The set of symbols employed by the IPA in-cludes the 26 letters of the Latin alphabet, whichtend to correspond to the phonemes that they rep-resent in the Latin script.
For example, the IPAsymbol [m] denotes a voiced bilabial nasal con-sonant, which is the phoneme represented by theletter m in most languages that utilize Latin script.ALINE (Kondrak, 2000) performs phoneticalignment of two strings of phonemes.
It combinesa dynamic programming alignment algorithm withan appropriate scoring scheme for computing pho-netic similarity on the basis of multivalued fea-tures.
The example below shows the alignment ofthe word sheath to its phonetic transcription [S iT].ALINE correctly links the most similar pairs ofphonemes (s:S, e:i, t:T).22ALINE can also be applied to non-Latin scripts by re-placing every grapheme with the IPA symbol that is phoneti-cally closest to it.782s h e a t h| | | | | |S - i - T -Since ALINE is designed to align phonemeswith phonemes, it does not incorporate the repre-sentation constraint.
In order to avoid the prob-lem of unaligned phonemes, we apply a post-processing algorithm, which also handles 1-2links.
The algorithm first attempts to remove 0-1links by merging them with the adjacent 1-0 links.If this is not possible, the algorithm scans a list ofvalid 1-2 mappings, attempting to replace a pair of0-1 and 1-1 links with a single 1-2 link.
If this alsofails, the entire entry is removed from the trainingset.
Such entries often represent unusual foreign-origin words or outright annotation errors.
Thenumber of unaligned entries rarely exceeds 1% ofthe data.The post-processing algorithm produces analignment that contains 1-0, 1-1, and 1-2 links.The list of valid 1-2 mappings must be preparedmanually.
The length of such lists ranges from 1for Spanish and German (x:[ks]) to 17 for English.This approach is more robust than the double-phoneme technique because the two phonemes areclustered only if they can be linked to the corre-sponding letter.5 Constraint-based alignmentOne of the advantages of the phonetic alignmentis its ability to rule out phonetically implausibleletter-phoneme links, such as o:p. We are in-terested in establishing whether a set of allow-able letter-phoneme mappings could be derived di-rectly from the data without relying on phoneticfeatures.Black et al (1998) report that constructing listsof possible phonemes for each letter leads to L2Pimprovement.
They produce the lists in a ?semi-automatic?, interactive manner.
The lists constrainthe alignments performed by the EM algorithmand lead to better-quality alignments.We implement a similar interactive programthat incrementally expands the lists of possiblephonemes for each letter by refining alignmentsconstrained by those lists.
However, instead ofemploying the EM algorithm, we induce align-ments using the standard edit distance algorithmwith substitution and deletion assigned the samecost.
In cases when there are multiple alternativealignments that have the same edit distance, werandomly choose one of them.
Furthermore, weextend this idea also to many-to-many alignments.In addition to lists of phonemes for each letter (1-1 mappings), we also construct lists of many-to-many mappings, such as ee:i, sch:S, and ew:ju.
Intotal, the English set contains 377 mappings, ofwhich more than half are of the 2-1 type.6 IP AlignmentThe process of manually inducing allowable letter-phoneme mappings is time-consuming and in-volves a great deal of language-specific knowl-edge.
The Integer Programming (IP) frameworkoffers a way to induce similar mappings without ahuman expert in the loop.
The IP formulation aimsat identifying the smallest set of letter-phonememappings that is sufficient to align all instances inthe data set.Our IP formulation employs the three con-straints enumerated in Section 2, except that theone-to-one constraint is relaxed in order to identifya small set of 1-2 mappings.
We specify two typesof binary variables that correspond to local align-ment links and global letter-phoneme mappings,respectively.
We distinguish three types of localvariables, X , Y , and Z, which correspond to 1-0,1-1, and 1-2 links, respectively.
In order to min-imize the number of global mappings, we set thefollowing objective that includes variables corre-sponding to 1-1 and 1-2 mappings:minimize :?l,pG(l, p) +?l,p1,p2G(l, p1p2) (4)We adopt a simplifying assumption that any let-ter can be linked to a null phoneme, so no globalvariables corresponding to 1-0 mappings are nec-essary.In the lexicon entry k, let lik be the letter at po-sition i, and pjk the phoneme at position j.
In or-der to prevent the alignments from utilizing letter-phoneme mappings which are not on the globallist, we impose the following constraints:?i,j,kY (i, j, k) ?
G(lik, pjk) (5)?i,j,kZ(i, j, k) ?
G(lik, pjkp(j+1)k) (6)For example, the local variable Y (i, j, k) is set iflik is linked to pjk.
A corresponding global vari-able G(lik, pjk) is set if the list of allowed letter-phoneme mappings includes the link (lik, pjk).Activating the local variable implies activating thecorresponding global variable, but not vice versa.783Figure 2: A network of possible alignment links.We create a network of possible alignment linksfor each lexicon entry k, and assign a binary vari-able to each link in the network.
Figure 2 shows analignment network for the lexicon entry k: wriggle[r I g @ L].
There are three 1-0 links (level), three1-1 links (diagonal), and one 1-2 link (steep).
Thelocal variables that receive the value of 1 are thefollowing: X(1,0,k), Y(2,1,k), Y(3,2,k), Y(4,3,k),X(5,3,k), Z(6,5,k), and X(7,5,k).
The correspond-ing global variables are: G(r,r), G(i,I), G(g,g), andG(l,@L).We create constraints to ensure that the linkvariables receiving a value of 1 form a left-to-rightpath through the alignment network, and that allother link variables receive a value of 0.
We ac-complish this by requiring the sum of the linksentering each node to equal the sum of the linksleaving each node.
?i,j,k X(i, j, k) + Y (i, j, k) + Z(i, j, k) =X(i + 1, j, k) + Y (i + 1, j + 1, k)+Z(i + 1, j + 2, k)We found that inducing the IP model with thefull set of variables gives too much freedom to theIP program and leads to inferior results.
Instead,we first run the full set of variables on a subset ofthe training data which includes only the lexiconentries in which the number of phonemes exceedsthe number of letters.
This generates a small setof plausible 1-2 mappings.
In the second pass, werun the model on the full data set, but we allowonly the 1-2 links that belong to the initial set of1-2 mappings induced in the first pass.6.1 Combining IP with EMThe set of allowable letter-phoneme mappings canalso be used as an input to the EM alignment algo-rithm.
We call this approach IP-EM.
After induc-ing the minimal set of letter-phoneme mappings,we constrain EM to use only those mappings withthe exclusion of all others.
We initialize the prob-ability of the minimal set with a uniform distribu-tion, and set it to zero for other mappings.
We trainthe EM model in a similar fashion to the many-to-many alignment algorithm presented in Section 3,except that we limit the letter size to be one letter,and that any letter-phoneme mapping that is not inthe minimal set is assigned zero count during theE-step.
The final alignments are generated afterthe parameters converge.7 Alignment by aggregationDuring our development experiments, we ob-served that the technique that combines IP withEM described in the previous section generallyleads to alignment quality improvement in com-parison with the IP alignment.
Nevertheless, be-cause EM is constrained not to introduce any newletter-phoneme mappings, many incorrect align-ments are still proposed.
We hypothesized that in-stead of pre-constraining EM, a post-processing ofEM?s output may lead to better results.M2M-aligner has the ability to create preciselinks involving more than one letter, such as ph:f.However, it also tends to create non-intuitive linkssuch as se:z for the word phrase [f r e z], where eis clearly a case of a ?silent?
letter.
We proposean alternative EM-based alignment method thatinstead utilizes a list of alternative one-to-manyalignments created with M2M-aligner and aggre-gates 1-M links into M-M links in cases whenthere is a disagreement between alignments withinthe list.
For example, if the list contains the twoalignments shown in Figure 3, the algorithm cre-ates a single many-to-many alignment by merg-ing the first pair of 1-1 and 1-0 links into a singleph:f link.
However, the two rightmost links are notmerged because there is no disagreement betweenthe two initial alignments.
Therefore, the resultingalignment reinforces the ph:f mapping, but avoidsthe questionable se:z link.p h r a s e p h r a s e| | | | | | | | | | | |f - r e z - - f r e z -Figure 3: Two alignments of phrase.In order to generate the list of best alignments,we use Algorithm 2, which is an adaptation of thestandard Viterbi algorithm.
Each cell Qt,v con-tains a list of n-best scores that correspond to al-784Algorithm 2: Extracting n-best alignmentsInput: x, y, ?Output: QT,VT = |x| + 1 , V = |y| + 11for t = 1..T do2Qt,v = ?3for v = 1..V do4for q ?
Qt?1,v do5append q ?
?
(xt, ?)
to Qt,v6for j = 1..maxY st v ?
j ?
0 do7for q ?
Qt?1,v?j do8append q ?
?
(xt, yvv?j+1) to Qt,v9sort Qt,v10Qt,v = Qt,v[1 : n]11ternative alignments during the forward pass.
Inline 9, we consider all possible 1-M links betweenletter xt and phoneme substring yvv?j+1.
At theend of the main loop, we keep at most n best align-ments in each Qt,v list.Algorithm 2 yields n-best alignments in theQT,V list.
However, in order to further restrictthe set of high-quality alignments, we also dis-card the alignments with scores below thresholdR with respect to the best alignment score.
Basedon the experiments with the development set, weset R = 0.8 and n = 10.8 Intrinsic evaluationFor the intrinsic evaluation, we compared the gen-erated alignments to gold standard alignments ex-tracted from the the core vocabulary of the Com-bilex data set (Richmond et al, 2009).
Combilexis a high quality pronunciation lexicon with ex-plicit expert manual alignments.
We used a sub-set of the lexicon composed of the core vocabu-lary containing 18,145 word-phoneme pairs.
Thealignments contain 550 mappings, which includecomplex 4-1 and 2-3 types.Each alignment approach creates alignmentsfrom unaligned word-phoneme pairs in an un-supervised fashion.
We distinguish between the1-1 and M-M approaches.
We report the align-ment quality in terms of precision, recall and F-score.
Since the gold standard includes many linksthat involve multiple letters, the theoretical up-per bound for recall achieved by a one-to-one ap-proach is 90.02%.
However, it is possible to obtainthe perfect precision because we count as correctall 1-1 links that are consistent with the M-M linksin the gold standard.
The F-score correspondingto perfect precision and the upper-bound recall is94.75%.Alignment entropy is a measure of alignmentquality proposed by Pervouchine et al (2009) inthe context of transliteration.
The entropy indi-cates the uncertainty of mapping between letterl and phoneme p resulting from the alignment:We compute the alignment entropy for each of themethods using the following formula:H = ?
?l,pP (l, p) logP (l|p) (7)Table 1 includes the results of the intrinsic eval-uation.
(the two rightmost columns are discussedin Section 9).
The baseline BaseEM is an im-plementation of the one-to-one alignment methodof (Black et al, 1998) without the allowable list.ALINE is the phonetic method described in Sec-tion 4.
SeedMap is the hand-seeded method de-scribed in Section 5.
M-M-EM is the M2M-aligner approach of Jiampojamarn et al (2007).1-M-EM is equivalent to M-M-EM but with therestriction that each link contains exactly one let-ter.
IP-align is the alignment generated by theIP formulation from Section 6.
IP-EM is themethod that combines IP with EM described inSection 6.1.
EM-Aggr is our final many-to-manyalignment method described in Section 7.
Oraclecorresponds to the gold-standard alignments fromCombilex.Overall, the M-M models obtain lower preci-sion but higher recall and F-score than 1-1 models,which is to be expected as the gold standard is de-fined in terms of M-M links.
ALINE produces themost accurate alignments among the 1-1 methods,with the precision and recall values that are veryclose to the theoretical upper bounds.
Its preci-sion is particularly impressive: on average, onlyone link in a thousand is not consistent with thegold standard.
In terms of word accuracy, 98.97%words have no incorrect links.
Out of 18,145words, only 112 words contain incorrect links, andfurther 75 words could not be aligned.
The rank-ing of the 1-1 methods is quite clear: ALINE fol-lowed by IP-EM, 1-M-EM, IP-align, and BaseEM.Among the M-M methods, EM-Aggr has slightlybetter precision than M-M-EM, but its recall ismuch worse.
This is probably caused by the ag-gregation strategy causing EM-Aggr to ?lose?
asignificant number of correct links.
In general, theentropy measure does not mirror the quality of thealignment.785Aligner Precision Recall F1 score Entropy L2P 1-1 L2P M-MBaseEM 96.54 82.84 89.17 0.794 50.00 65.38ALINE 99.90 89.54 94.44 0.672 54.85 68.741-M-EM 99.04 89.15 93.84 0.636 53.91 69.13IP-align 98.30 88.49 93.14 0.706 52.66 68.25IP-EM 99.31 89.40 94.09 0.651 53.86 68.91M-M-EM 96.54 97.13 96.83 0.655 ?
68.52EM-Aggr 96.67 93.39 95.00 0.635 ?
69.35SeedMap 97.88 97.44 97.66 0.634 ?
68.69Oracle 100.0 100.0 100.0 0.640 ?
69.35Table 1: Alignment quality, entropy, and L2P conversion accuracy on the Combilex data set.Aligner Celex-En CMUDict NETtalk OALD BrulexBaseEM 75.35 60.03 54.80 67.23 81.33ALINE 81.50 66.46 54.90 72.12 89.371-M-EM 80.12 66.66 55.00 71.11 88.97IP-align 78.88 62.34 53.10 70.46 83.72IP-EM 80.95 67.19 54.70 71.24 87.81Table 2: L2P word accuracy using the TiMBL-based generation system.9 Extrinsic evaluationIn order to investigate the relationship betweenthe alignment quality and L2P performance, wefeed the alignments to two different L2P systems.The first one is a classification-based learning sys-tem employing TiMBL (Daelemans et al, 2009),which can utilize either 1-1 or 1-M alignments.The second system is the state-of-the-art onlinediscriminative training for letter-to-phoneme con-version (Jiampojamarn et al, 2008), which ac-cepts both 1-1 and M-M types of alignment.
Ji-ampojamarn et al (2008) show that the online dis-criminative training system outperforms a num-ber of competitive approaches, including joint n-grams (Demberg et al, 2007), constraint satisfac-tion inference (Bosch and Canisius, 2006), pro-nunciation by analogy (Marchand and Damper,2006), and decision trees (Black et al, 1998).
Thedecoder module uses standard Viterbi for the 1-1case, and a phrasal decoder (Zens and Ney, 2004)for the M-M case.
We report the L2P performancein terms of word accuracy, which rewards onlythe completely correct output phoneme sequences.The data set is randomly split into 90% for trainingand 10% for testing.
For all experiments, we holdout 5% of our training data to determine when tostop the online training process.Table 1 includes the results on the Combilexdata set.
The two rightmost columns correspondto our two test L2P systems.
We observe that al-though better alignment quality does not alwaystranslate into better L2P accuracy, there is never-theless a strong correlation between the two, espe-cially for the weaker phoneme generation system.Interestingly, EM-Aggr matches the L2P accuracyobtained with the gold standard alignments.
How-ever, there is no reason to claim that the gold stan-dard alignments are optimal for the L2P genera-tion task, so that result should not be considered asan upper bound.
Finally, we note that alignmententropy seems to match the L2P accuracy betterthan it matches alignment quality.Tables 2 and 3 show the L2P results on sev-eral evaluation sets: English Celex, CMUDict,NETTalk, OALD, and French Brulex.
The train-ing sizes range from 19K to 106K words.
We fol-low exactly the same data splits as in Bisani andNey (2008).The TiMBL L2P generation method (Table 2)is applicable only to the 1-1 alignment models.ALINE produces the highest accuracy on four outof six datasets (including Combilex).
The perfor-mance of IP-EM is comparable to 1-M-EM, butnot consistently better.
IP-align does not seem tomeasure up to the other algorithms.The discriminative approach (Table 3) is flexi-ble enough to utilize all kinds of alignments.
How-ever, the M-M models perform clearly better than1-1 models.
The only exception is NetTalk, which786Aligner Celex-En CMUDict NETTalk OALD BrulexBaseEM 85.66 71.49 68.60 80.76 88.41ALINE 87.96 75.05 69.52 81.57 94.561-M-EM 88.08 75.11 70.78 81.78 94.54IP-EM 88.00 75.09 70.10 81.76 94.96M-M-EM 88.54 75.41 70.18 82.43 95.03EM-Aggr 89.11 75.52 71.10 83.32 95.07joint n-gram 88.58 75.47 69.00 82.51 93.75Table 3: L2P word accuracy using the online discriminative system.Figure 4: L2P word accuracy vs. alignment en-tropy.can be attributed to the fact that NetTalk alreadyincludes double-phonemes in its original formu-lation.
In general, the 1-M-EM method achievesthe best results among the 1-1 alignment methods,Overall, EM-Aggr achieves the best word accuracyin comparison to other alignment methods includ-ing the joint n-gram results, which are taken di-rectly from the original paper of Bisani and Ney(2008).
Except the Brulex and CMUDict datasets, the differences between EM-Aggr and M-M-EM are statistically significant according to Mc-Nemar?s test at 90% confidence level.Figure 4 contains a plot of alignment entropyvalues vs. L2P word accuracy.
Each point rep-resent an application of a particular alignmentmethod to a different data sets.
It appears thatthere is only weak correlation between alignmententropy and L2P accuracy.
So far, we have beenunable to find either direct or indirect evidence thatalignment entropy is a reliable measure of letter-phoneme alignment quality.10 ConclusionWe investigated several new methods for gener-ating letter-phoneme alignments.
The phoneticalignment is recommended for languages with lit-tle or no training data.
The constraint-based ap-proach achieves excellent accuracy at the costof manual construction of seed mappings.
TheIP alignment requires no linguistic expertise andguarantees a minimal set of letter-phoneme map-pings.
The alignment by aggregation advancesthe state-of-the-art results in L2P conversion.
Wethoroughly evaluated the resulting alignments onseveral data sets by using them as input to two dif-ferent L2P generation systems.
Finally, we em-ployed an independently constructed lexicon todemonstrate the close relationship between align-ment quality and L2P conversion accuracy.One open question that we would like to investi-gate in the future is whether L2P conversion accu-racy could be improved by treating letter-phonemealignment links as latent variables, instead of com-mitting to a single best alignment.AcknowledgmentsThis research was supported by the Alberta In-genuity, Informatics Circle of Research Excel-lence (iCORE), and Natural Science of Engineer-ing Research Council of Canada (NSERC).ReferencesMaximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451.Alan W. Black, Kevin Lenzo, and Vincent Pagel.
1998.Issues in building general letter to sound rules.
InThe Third ESCA Workshop in Speech Synthesis,pages 77?80.Antal Van Den Bosch and Sander Canisius.
2006.Improved morpho-phonological sequence process-ing with constraint satisfaction inference.
Proceed-ings of the Eighth Meeting of the ACL Special Inter-est Group in Computational Phonology, SIGPHON?06, pages 41?49.787Walter Daelemans and Antal Van Den Bosch.
1997.Language-independent data-oriented grapheme-to-phoneme conversion.
In Progress in Speech Synthe-sis, pages 77?89.
New York, USA.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2009.
TiMBL: Tilburg Mem-ory Based Learner, version 6.2, Reference Guide.ILK Research Group Technical Report Series no.09-01.Robert I. Damper, Yannick Marchand, John DS.Marsters, and Alexander I. Bazin.
2005.
Align-ing text and phonemes for speech technology appli-cations using an EM-like algorithm.
InternationalJournal of Speech Technology, 8(2):147?160.Vera Demberg, Helmut Schmid, and Gregor Mo?hler.2007.
Phonological constraints and morphologi-cal preprocessing for grapheme-to-phoneme conver-sion.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages96?103, Prague, Czech Republic.Herman Engelbrecht and Tanja Schultz.
2005.
Rapiddevelopment of an afrikaans-english speech-to-speech translator.
In International Workshop of Spo-ken Language Translation (IWSLT), Pittsburgh, PA,USA.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden markov models to letter-to-phonemeconversion.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 372?379, Rochester, New York, USA.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discriminativetraining for letter-to-phoneme conversion.
In Pro-ceedings of ACL-08: HLT, pages 905?913, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Grzegorz Kondrak.
2000.
A new algorithm for thealignment of phonetic sequences.
In Proceedings ofNAACL 2000: 1st Meeting of the North AmericanChapter of the Association for Computational Lin-guistics, pages 288?295.Yannick Marchand and Robert I. Damper.
2000.
Amultistrategy approach to improving pronunciationby analogy.
Computational Linguistics, 26(2):195?219.Yannick Marchand and Robert I. Damper.
2006.
Cansyllabification improve pronunciation by analogy ofEnglish?
Natural Language Engineering, 13(1):1?24.Vladimir Pervouchine, Haizhou Li, and Bo Lin.
2009.Transliteration alignment.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages136?144, Suntec, Singapore, August.
Associationfor Computational Linguistics.Korin Richmond, Robert A. J. Clark, and Sue Fitt.2009.
Robust LTS rules with the Combilex speechtechnology lexicon.
In Proceedings od Interspeech,pages 1295?1298.Juergen Schroeter, Alistair Conkie, Ann Syrdal, MarkBeutnagel, Matthias Jilka, Volker Strom, Yeon-JunKim, Hong-Goo Kang, and David Kapilow.
2002.A perspective on the next challenges for TTS re-search.
In IEEE 2002 Workshop on Speech Synthe-sis.Terrence J. Sejnowski and Charles R. Rosenberg.1987.
Parallel networks that learn to pronounce En-glish text.
In Complex Systems, pages 1:145?168.Paul Taylor.
2005.
Hidden Markov Models forgrapheme to phoneme conversion.
In Proceedingsof the 9th European Conference on Speech Commu-nication and Technology.Kristina Toutanova and Robert C. Moore.
2001.
Pro-nunciation modeling for improved spelling correc-tion.
In ACL ?02: Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 144?151, Morristown, NJ, USA.Richard Zens and Hermann Ney.
2004.
Improvementsin phrase-based statistical machine translation.
InHLT-NAACL 2004: Main Proceedings, pages 257?264, Boston, Massachusetts, USA.788
