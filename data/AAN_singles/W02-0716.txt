Towards a  Speech-to-Speech Machine Translation Quality MetricKurt GoddenLockheed Martin Advanced Technology Laboratories1 Federal StreetCamden, New Jersey   08102kgodden@atl.lmco.comAbstractGeneral characteristics of a pragmaticmetric for the production evaluation ofspeech-to-speech translations are dis-cussed.
While these characteristics con-strain the space of allowable metrics,infinite definition space remains fromwhich to select and define any particularmetric.
The recommended characteisticsare drawn from the author?s experienceas primary developer of a text-basedtranslation quality metric used in a pro-duction environment.
The primary contri-bution is that of strict category orderingand two meta-rules that reduce thevariance in assignment of errors tocategories.1  IntroductionWhen we consider speech-to-speech (S2S)translation systems, several abstract models arepossible.In Model 1 (Figure 1) we treat the entiresoftware system as a ?black box,?
just recognizingthat the input is a source language utterance (SLU)and the output is a target language utterance(TLU).SLUS2STLUFigure 1.
Model 1.In Model 2 (Figure 2) we break the previousblack box into several traditional components,reflecting typical language processing modules.The source language utterance is transformed to asource language text (SLT) by an automatic speechrecognition (ASR) system.
The SLT is thentranslated by a machine translation (MT) system toa target language text (TLT), which is in turnconverted to the target language utterance by atext-to-speech (TTS) system.SLUASRMTTTSSLTTLUTLTFigure 2.
Model 2.Model 3 (Figure 3) illustrates how the sourcelanguage text and MT component may be replacedby a natural language generation (NLG) system,given a rich enough semantic representation.
Othermodels are certainly possible, depending upon howthe various processing tasks are subdivided.SLUASRNLG TLUSemanticRepFigure 3.
Model 3.Regardless of how many levels andcomponents there are in a given implementation,different metrics could be applied around anyinput-output pair of interest to help drive qualityimprovements.
In Model 2 above, for example, wecould have a metric around each processingAssociation for Computational Linguistics.Algorithms and Systems, Philadelphia, July 2002, pp.
117-120.Proceedings of the Workshop on Speech-to-Speech Translation:module; that is, one metric for the mapping SLU toSLT, another metric for SLT to TLT, and a thirdfrom TLT to TLU.
Each metric would be used tostudy the effectiveness of the system module ofinterest.However, since the only guaranteed input-output pair regardless of the particular combinationof technologies used would be SLU to TLU, andsince all systems can be abstracted into Model 1above, let us focus on an abstract metric which wewill call the utterance-to-utterance (U2U) Metric.What do we require of our abstract U2U metric?2  Metric CharacteristicsI will not take a position on a particular metric,since metrics may vary depending upon thepurpose of the people using them.
However, I willtake a position on various general characteristics ofa metric for production use.These arguments are based on my experienceas principal author of the J2450 translation qualitymetric that has been adopted by the Society ofAutomotive Engineers as a recommended practicefor evaluation of service information translations.
[SAE]In production environments?as opposed tosystem development?translation metrics aretypically applied to random samples of source andtarget language translations.
Metrics based onstatic reference translations for the automaticevaluation of system quality during systemdevelopment [Doddington] are thus not in thedomain of this discussion.Evaluation is usually performed by a qualifiedtranslator with domain knowledge, who is general-ly employed by a translation agency, though clientcompanies sometimes perform their own internalevaluations.2.1  Primary CategoriesFirst, a U2U metric for production use shouldconsist of approximately seven categories oferrors, plus or minus two.
Seven categories areenough to provide adequate linguistic coverage yetare few enough to be usable by people in the realworld.
This is consistent with most metrics  usedby translation agencies.For example, one category may refer to theappropriate mapping of source language words totarget language words, where appropriate  isdefined by the category description.
Anothercategory may refer to word order.
Given that weare discussing speech-to-speech systems, acategory may be reserved for the intelligibility ofthe target language audio.
Again, the particularcategories will be dictated by the purpose of thepeople employing the metric.
System applicationdevelopers will have different interests fromresearchers working on the base technologies, andclients will have different interests than suppliersof the technologies.With respect to categories, I would suggestthat any particular U2U metric not include aprimary category called mistranslation.
Whilemany translators use this term, and many transla-tion agencies employ their own proprietary metricsthat have an error category by this name, it israrely, if ever, defined with any precision.
Theword mistranslation itself tends to evoke strongemotions in the translation industry, and for thatreason alone it is not a good term to use for what ishoped will be a relatively objective, scientificallymotivated metric.2.2  Numeric ScoresThe next characteristic of a U2U metric should bethat it produces numeric scores for the utteranceevaluations.
That is, each category should itselfproduce a numeric score, and each category can beweighted or not, according to the goals of themetric?s users.
The presence and categorization ofthe errors are generally matters of human judge-ment, but once the error is recognized andcategorized, a numeric result has numerous advan-tages.
Given appropriate sample sizes it allowsreasonable comparisons across different translationsystems.
It also allows easy use of statisticalcontrol charts for quality control [Godden 1996].Employing a numeric score as the basis for aquality evaluation does not ipso facto allow theidentification of a translation as good, bad orindifferent.
Two different people may use the samemetric, producing the same evaluation scores andyet define the notion of acceptable quality entirelydifferently.
One person may define acceptablequality as a normalized quality score of .80 orhigher, while another person may defineacceptable quality only with a score of .90.
Thethreshold of acceptance is independent of themetric, and is a business, not a technical decision.2.3  Major and Minor SubcategoriesAnother necessary characteristic for a productionmetric encompasses the notion of a major vs. aminor error.
An example will clarify both theconcept and its utility.
Suppose that a sourcelanguage utterance contained the phrase a door,but that the S2S system translated it as a window.This is a lexical translation error that is major.
Anexample of a lexical error that is minor would be atarget language utterance of an door.
It is ill-advised to penalize both errors with the samenumeric score, which would happen if a ?wrongword?
category always resulted in a single numericvalue.
Adding the major vs. minor distinction withdifferent numeric scores allows the evaluator topenalize the first error more than the second.Thus we have now constrained our metrics toinclude approximately seven primary errorcategories and two secondary categories (majorversus minor) for a total of roughly fourteendistinct classifications of any given error.
When anerror in a translation is detected, the evaluatortherefore has two assignments to make, theprimary category and one of its two secondarycategories.
These primary and secondary categoryassignments are not always clear.
Since translationquality judgements are generally human judge-ments, there will be evaluation variance acrossevaluators.Is an incorrect gender on an article an exampleof an incorrect term or a syntax error?
If the twocategories have different penalty scores, then thecategory assignment can be a significant source ofvariance.
Is the translation of a definite article asan indefinite article a minor error or a major one?That will depend upon the context of course, but itmay also depend upon the person performing theevaluation.2.4  Reducing VarianceTo the extent that this human variance can bereduced, then the metric used by that evaluator willbecome more valuable.The most effective way in which variance canbe reduced is to give as precise a definition aspossible of each error category, both primary andsecondary.
If a category of wrong term is to beused, then the notions of both wrong and term needto be defined precisely.
Is ?gas pedal?
one term ortwo?
Are function words regarded as terms?
If thesource language term is ambiguous, then whatconstitutes a wrong term in the target language?Definitions of error categories should be amplyillustrated with examples.The second most effective way to reducevariance is to provide training for evaluators.Sample utterances and translations with deliberateerrors should be prepared in advance, offeringseveral examples of each error category.
Ideally,an entire training course would be designed aroundthese examples and no person would performworking evaluations without taking the course.Also, evaluators should only be drawn from theranks of qualified translators.2.5  Ordering and Meta-RulesBut there is an additional way to reduce thevariance in category assignment that can beincorporated into the metric itself.
This can bedone by employing rule ordering coupled with twometa-rules.
The seven (plus or minus two) primarycategories should be totally ordered by the numericdemerit penalty values referenced in the twosubcategories.For example, if primary category X has amajor penalty demerit of five and a minor demeritof two, then it should be ordered before anotherprimary category Y with major and minor demeritsof four and three.
If primary categories X and Yhave major/minor demerits of three/four andthree/five, respectively, then Y should be orderedbefore X.Any potential ambiguities may be resolved byarbitrary sort order rules.
The important concept isthat each primary category be ordered with respectto every other primary category.
Within a category,the major subtype is always ordered before theminor subtype.Once the ordering is determined, then twometa-rules may be used to reduce error categoryassignment variance.
The first meta-rule states thatif the evaluator is unsure which primary categoryto assign to an error, then he or she shouldautomatically assign it to that primary categoryhighest in the sort order.
Thus, if two evaluatorsare both unsure about which of two primarycategories X or Y to assign to a given error, thisfirst meta-rule forces them both to make the samedecision.
They will both select X, if X precedes Yin the sort order.Similarly, the second meta-rule states that oncethe primary error category is assigned, if anevaluator is unsure whether the error constitutes amajor or a minor instance of that error, then theevaluator should automatically regard it as a majorerror.In this way, the metric itself?which nowcontains the two meta-rules?is removing some ofthe decision-making authority from the humanevaluators, with the effect of reducing the variancein quality score demerit assignments.
We mustassume, of course, the honest and unbiasedapplication of the metric by the evaluator.
We alsoassume that both the metric definition as well asthe training course and materials clearly emphasizethe application and importance of the meta-rules.The meta-rules impose a bias toward higherdemerits, which is somewhat arbitrary.
We couldas easily have made the bias favor lower demerits.Any definition of acceptable quality, i.e., anacceptance threshold, based on the numeric scorescan be adjusted up or down, according to the needsof the organization employing the metric.
Aspreviously stated, such acceptance criteria arebusiness decisions, not technical ones.
Theimportant effect of the meta-rules is to reduce thevariance in assignment of errors to categories.3  Summary and ConclusionsThe recommended U2U metric characteristics aresummarized in Table 1.Table 1.
Characteristics of Metric.Seven (+/- two) primary categoriesTwo secondary categories (major/minor) for eachprimary category.Numeric demerits for each major and minorcategory.Primary categories sorted by demerits.Meta-Rule 1 removes ambiguity of primarycategory assignment.Meta-Rule 2 removes ambiguity of secondarycategory assignment.While I have discussed several characteristicsof what I consider to be required elements of anyadequate U2U translation quality metric forproduction use, these constraints still permitinfinite variation in the definition of any particularmetric.
Primary categories may be drawn from anynumber of classifications that divide the errorspace, e.g.
lexical, syntactic, semantic, phonetic,etc.
Numeric demerits may be taken from anydesired range, be it 0-1 or 1-1000.Finally, let me say that just because a U2Umetric conforms to the characteristics discussed inthis paper, that metric does not automaticallybecome a good metric.
As previously discussed,the category definitions are of extreme impor-tance, as are the examples used to illustrate thedefinitions and the training materials created forevaluators.
Without clear, unambiguous andprecise error definitions no metric will be of anypractical value.ReferencesDoddington, G. 2002.
?Automatic Evaluation ofMachine Translation Quality Using N-gramCo-Ocurrence Statistics.?
Notebook Proceed-ings.
Human Language Technology.
SanDiego, CA.
pp.
128-132.Godden, K. 1996.
?Statistical Control Charts inNatural Language Processing.?
Proceedings.Natural Language Processing and IndustrialApplications.
Moncton, NB, Canada.
pp.
111-117.SAE.
2001.
?Translation Quality Metric.?
Docu-ment Number J2450.
Available throughwww.sae.org/technicalcommittees/j2450p1.htm
