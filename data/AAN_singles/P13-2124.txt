Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 708?713,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Learner Corpus-based Approach to Verb Suggestion for ESLYu Sawai Mamoru Komachi?Graduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192, Japan{yu-s, komachi, matsu}@is.naist.jpYuji MatsumotoAbstractWe propose a verb suggestion methodwhich uses candidate sets and domainadaptation to incorporate error patternsproduced by ESL learners.
The candi-date sets are constructed from a large scalelearner corpus to cover various error pat-terns made by learners.
Furthermore, themodel is trained using both a native cor-pus and the learner corpus via a domainadaptation technique.
Experiments on twolearner corpora show that the candidatesets increase the coverage of error patternsand domain adaptation improves the per-formance for verb suggestion.1 IntroductionIn this study, we address verb selection errors inthe writing of English learners.
Selecting the rightverb based on the context of a sentence is difficultfor the learners of English as a Second Language(ESL).
This error type is one of the most commonerrors in various learner corpora ranging from ele-mentary to proficient levels1.They ?connect/communicate with otherbusinessmen and do their jobs with thehelp of computers.2This sentence is grammatically acceptable witheither verb.
However, native speakers of En-glish would less likely use ?connect?, whichmeans ?forming a relationship (with other busi-nessmen)?, whereas ?communicate?
means ?ex-changing information or ideas?, which is what thesentence is trying to convey.
?Now at Tokyo Metropolitan University.1For example, in the CLC-FCE dataset, the replacementerror of verbs is the third most common out of 75 error types.In the KJ corpus, lexical choice of verb is the sixth most com-mon out of 47 error types.2This sentence is taken from the CLC-FCE dataset.Previous work on verb selection usually treatsthe task as a multi-class classification problem(Wu et al 2010; Wang and Hirst, 2010; Liu etal., 2010; Liu et al 2011).
In this formaliza-tion, it is important to restrict verbs by a candi-date set because verb vocabulary is more numer-ous than other classes, such as determiners.
Can-didate sets for verb selection are often extractedfrom thesauri and/or round-trip translations.
How-ever, these resources may not cover certain errorpatterns found in actual learner corpora, and sufferfrom low-coverage.
Furthermore, all the existingclassifier models are trained only using a nativecorpus, which may not be adequate for correctinglearner errors.In this paper, we propose to use error patternsin ESL writing for verb suggestion task by usingcandidate sets and a domain adaptation technique.First, to increase the coverage, candidate sets areextracted from a large scale learner corpus derivedfrom a language learning website.
Second, a do-main adaptation technique is applied to the modelto fill the gap between two domains: native cor-pus and ESL corpus.
Experiments are carried outon publicly available learner corpora, the Cam-bridge Learner Corpus First Certificate of Englishdataset (CLC-FCE) and the Konan JIEM corpus(KJ).
The results show that the proposed candidatesets improve the coverage, compared to the base-line candidate sets derived from the WordNet anda round-trip translation table.
Domain adaptationalso boosts the suggestion performance.To our knowledge, this is the first work forverb suggestion that uses (1) a learner corpus asa source of candidate sets and (2) the domainadaptation technique to take learner errors into ac-count.7082 Verb Suggestion Considering ErrorPatternsThe proposed verb suggestion system follows thestandard approach in related tasks (Rozovskayaand Roth, 2011; Wu et al 2010), where the candi-date selection is formalized as a multi-class classi-fication problem with predefined candidate sets.2.1 Candidate SetsFor reflecting tendency of learner errors to the can-didate sets, we use a large scale corpus obtainedfrom learners?
writing on an SNS (Social Net-working Service), Lang-83.
An advantage of usingthe learner corpus from such website is the size ofannotated portion (Mizumoto et al 2011).
ThisSNS has over 1 million manually annotated En-glish sentences written by ESL learners.
We havecollected the learner writings on the site, and re-leased the dataset for research purpose4.First, we performed POS tagging for the datasetusing the treebank POS tagger in the NLTK toolkit2.10.
Second, we extracted the correction pairswhich have ?VB*?
tag.
The set of correction pairsgiven an incorrect verb is considered as a candi-date set for the verb.We then performed the following preprocessingfor the dataset because we focus on lexical selec-tion of verbs:?
Lemmatize verbs to reduce data sparseness.?
Remove non-English verbs using WordNet.?
Remove incorrect verbs which occur onlyonce in the dataset.The target verbs are limited to the 500 mostcommon verbs in the CLC-FCE corpus5.
There-fore, verbs that do not appear in the target list arenot included in the candidate sets.
The topmost500 verbs cover almost 90 percent of the vocabu-lary of verbs in the CLC-FCE corpus6.The average number of candidates in a set is20.37.
Note that the number of candidates variesacross each target verb8.3http://lang-8.com4Further details can be found at http://cl.naist.jp/nldata/lang-8/.
Candidate sets will also be avail-able at the same URL.5They are extracted from all ?VB?
tagged tokens, andthey contain 1,292 unique verbs after removing non-Englishwords.6This number excludes ?be?.7In this paper, we limit the maximum number of candi-dates in each set to 50.8For instance, the candidate set for ?get?
has 315 correc-tion pairs, whereas ?refund?
has only 4.2.2 Suggestion ModelThe verb suggestion model consists of multi-classclassifiers for each target verb; and based on theclassifiers?
output, it suggests alternative verbs.Instances are in a fill-in-the-blank format, wherethe labels are verbs.
Features in this format areextracted from the surrounding context of a verb.When testing on the learner corpus, the model sug-gests a ranking of the possible verbs for the blankcorresponding to a given context.
Note that un-like the fill-in-the-blank task, the candidate setsand domain adaptation can be applied to this taskto take the original word into account.The model is trained on a huge native corpus,namely the ukWaC corpus, because the data-sizeof learner corpora is limited compared to nativecorpora.
It is then adapted to the target domain,i.e., learner writing.
In our experiment, the Lang-8 corpus is used as the target domain corpus, sincewe assume that it shares the same characteristicswith the CLC-FCE and the KJ corpora used fortesting.2.3 Domain AdaptationTo adapt the models to the learner corpus, we em-ploy a domain adaptation technique to emphasizethe importance of learner domain information.
Al-though there are many studies on domain adap-tation, we chose to use Feature Augmentationtechnique introduced by (Daume?
III, 2007) for itssimplicity.
Recently, (Imamura et al 2012) pro-posed to apply this method to grammatical errorcorrection for writings of Japanese learners andconfirmed that this is more effective for correct-ing learner errors than simply adding the target do-main instances.In this study, the source domain is the nativewriting, and the target domain is the ESL writing.Our motivation is to use the ESL corpus togetherwith the huge native corpus to employ both an ad-vantage of the size of training data and the ESLwriting specific features.In this method, adapting a model to anothermodel is achieved by extending the feature space.Given a feature vector of F dimensions as x ?RF(F > 0), using simple mapping, the aug-mented feature vectors for source and target do-mains are obtained as follows,Source domain: < xS, xS, 0 > (1)Target domain: < xT, 0, xT > (2)709where 0 denotes a zero-vector of F dimensions.The three partitions mean a common, a source-specific, and a target-specific feature space.
Whentesting on the ESL corpora, the target-specific fea-tures are emphasized.2.4 FeaturesIn previous work, various features were used: lex-ical and POS n-grams, dependencies, and argu-ments in the verb context.
(Liu et al 2011) hasshown that shallow parse features, such as lexi-cal n-grams and chunks, work well in realistic set-tings, in which the input sentence may not be cor-rectly parsed.
Considering this, we use shallowparse features as context features for robustness.The features include lexical and POS n-grams,and lexical head words of the nearest NPs, andclustering features of these head words.
An ex-ample of extracted features is shown in Table 2.4.Note that those features are also used when ex-tracting examples from the target domain dataset(the learner domain corpus).
As shown in Table2.4, the n-gram features are 3-gram and extractedfrom ?2 context window.
The nearest NP?s headfeatures are divided into two (Left, Right).The additional clustering features are used forreducing sparseness, because the NP?s head wordsare usually proper nouns.
To create the word clus-ters, we employ Brown clustering, a hierarchicalclustering algorithm proposed by (Brown et al1992).
The structure of clusters is a complete bi-nary tree, in which each node is represented as abit-string.
By varying the length of the prefix ofbit-string, it is possible to change the granularityof cluster representation.
As illustrated in Table2.4, we use the clustering features with three lev-els of granularity: 256, 128, and 64 dimensions.We used Percy Liang?s implementation9 to create256 dimensional model from the ukWaC corpus,which is used as the native corpus.3 ExperimentsPerformance of verb suggestion is evaluated ontwo error-tagged learner corpora: CLC-FCE andKJ.
In the experiments, we assume that the tar-get verb and its context for suggestion are alreadygiven.For the experiment on the CLC-FCE dataset,the targets are all words tagged with ?RV?
(re-9https://github.com/percyliang/brown-clusterFeature Examplen-grams they-*V*-with(surface) <S>-they-*V**V*-with-othern-grams PRP-*V*-IN(POS) <S>-PRP-*V**V*-IN-JJNP head L they, L PRP(Left, Right) R businessmen, R NNSNP head cluster L 01110001, L 0111000, L 011100(Left, Right) R 11011001, R 1101100, R 110110(e.g., They (communicate) with other businessmen and dotheir jobs with the help of computers.)?<S>?
denotes the beginning of the sentence, ?
*V*?denotes the blanked out verb.Table 1: Example of extracted features as the fill-in-the-blank form.placement error of verbs).
We assume that all theverb selection errors are covered with this errortag.
All error tagged parts with nested correctionor multi-word expressions are excluded.
The re-sulting number of ?true?
targets is 1,083, whichamounts to 4% of all verbs.
Therefore the datasetis highly skewed to correct usages, though this set-ting expresses well the reality of ESL writing, asshown in (Chodorow et al 2012).We carried out experiments with a variety of re-sources used for creating candidate sets.?
WordNetCandidates are retrieved from the synsets andverbs sharing the same hypernyms in theWordNet 3.0.?
LearnerSmallCandidates are retrieved from followinglearner corpora: NUS corpus of learnerEnglish (NUCLE), Konan-JIEM (KJ), andNICT Japanese learner English (JLE) corpus.?
RoundtripCandidates are collected by performing?round-trip?
translation, which is similar to(Bannard and Callison-Burch, 2005) 10.?
WordNet+RoundtripA combination of the thesaurus-based and thetranslation table-based candidate sets, similarto (Liu et al 2010) and (Liu et al 2011).?
Lang-8The proposed candidate sets obtained from alarge scale learner corpus.?
Lang-8+DALang-8 candidate sets with domain adapta-10Our roundtrip translation lexicons are built using a subsetof the WIT3 corpus (Cettolo et al 2012), which is availableat http://wit3.fbk.eu.710Settings Candidates/set (Avg.
)WordNet 14.8LearnerSmall 5.1Roundtrip 50Roundtrip (En-Ja-En) 50WordNet+Roundtrip 50Lang-8 20.3Table 2: Comparison of candidate set size for eachsetting.tion via feature augmentation.Table 3 shows a comparison of the averagenumber of candidates in each setting.
In all config-urations above, the parameters of the models un-derlying the system are identical.
We used a L2-regularized generalized linear model with log-lossfunction via Scikit-learn ver.
0.13.Inter-corpus EvaluationWe also evaluate the suggestion performance onthe KJ corpus.
The corpus contains diary-stylewriting by Japanese university students.
The pro-ficiency of the learners ranges from elementary tointermediate, so it is lower than that of the CLC-FCE learners.
The targets are all verbs tagged with?v lxc?
(lexical selection error of verbs).To see the effect of L1 on the verb sugges-tion task, we added an alternative setting forthe Roundtrip using only the English-Japaneseand Japanese-English round-trip translation tables(En-Ja-En).
For the experiment on this test-corpus, the LearnerSmall is not included.DatasetsThe ukWaC web-corpus (Ferraresi et al 2008) isused as a native corpus for training the suggestionmodel.
Although this corpus consists of over 40million sentences, 20,000 randomly selected sen-tences are used for each verb11.The Lang-8 learner corpus is used for domainadaptation of the model in the Lang-8+DA config-uration.
The portion of data is the same as thatused for constructing candidate sets.MetricsMean Reciprocal Rank (MRR) is used for evalu-ating the performance of alternative suggestions.The mean reciprocal rank is calculated by taking11e.g., a classifier with a candidate set containing 50 verbsis trained with 1 million sentences in total.the average of the reciprocal ranks for each in-stance.
Given r goldi as the position of the goldcorrection candidate in the suggestion list Si for i-th checkpoint, the reciprocal rank RRi is definedas,RRi =??
?1r goldi (goldi ?
Si)0 (otherwise)(3)4 ResultsTables 5 and 5 show the results of suggestion per-formance on the CLC-FCE dataset and the KJ cor-pus, respectively.
In both cases, the Lang-8 and itsdomain adaptation variant outperformed the oth-ers.
The coverage of error patterns in the tablesis the percentage of the cases where the sugges-tion list includes the gold correction.
Generally,the suggestion performance and the coverage im-prove as the size of the candidate sets increases.5 DiscussionsAlthough the expert-annotated learner corporacontain candidates which are more reliable thana web-crawled Lang-8 corpus, the Lang-8 settingperformed better as shown in Table 5.
This can beexplained by the broader coverage by the Lang-8candidate sets than that of the LearnerSmall.
Sim-ilarly, the WordNet performed the worst becauseit contains only synonym-like candidates.
We canconclude that, for the verb suggestion task, thecoverage (recall) of candidate sets is more impor-tant than the quality (precision).We see little influence of learners?
L1 in the re-sults of Table 5, since the Roundtrip performedbetter than the Roundtrip (En-Ja-En).
As alreadymentioned, the number of error patterns containedin the candidate sets seems to have more impor-tance than the quality.As shown in Tables 5 and 5, a positive ef-fect of domain adaptation technique appeared inboth test-corpora.
In the case of the CLC-FCE,280 out of 624 suggestions were improved com-pared to the setting without domain adaptation.For instance, confusions between synonyms suchas ?
?live/stay?, ?
?say/tell?, and ?
?solve/resolve?are improved, because sentences containing theseconfusions appear more frequently in the Lang-8 corpus.
Although the number of test-cases forthe KJ corpus is smaller than the CLC-FCE, wecan see the improvements for 33 out of 66 sug-711Settings MRR CoverageWordNet 0.066 14.0 %LearnerSmall 0.128 23.5 %Roundtrip 0.185 48.1 %WordNet+Roundtrip 0.173 48.1 %Lang-8 0.220 57.6 %Lang-8+DA 0.269* 57.6 %The value marked with the asterisk indicates statistically sig-nificant improvement over the baselines, where p < 0.05bootstrap test.Table 3: Suggestion performance on the CLC-FCE dataset.Settings MRR CoverageWordNet 0.044 5.0 %Roundtrip 0.241 53.8 %Roundtrip (En-Ja-En) 0.188 38.8 %WordNet+Roundtrip 0.162 53.8 %Lang-8 0.253 68.9 %Lang-8+DA 0.412* 68.9 %The value marked with the asterisk indicates statistically sig-nificant improvement over the baselines, except ?Roundtrip?,where p < 0.05 bootstrap test.Table 4: Suggestion performance on the KJ cor-pus.gestions.
The improvements appeared for fre-quent confusions of Japanese ESL learners suchas ??see/watch?
and ?
?tell/teach?.Comparing the results of the Lang-8+DA onboth test-corpora, the domain adaptation tech-nique worked more effectively on the KJ cor-pus than on the CLC-FCE.
This can be explainedby the fact that the style of writing of the addi-tional data, i.e., the Lang-8 corpus, is closer toKJ than it is to CLC-FCE.
More precisely, unlikethe examination-type writing style of CLC-FCE,the KJ corpus consists of diary writing similar instyle to the Lang-8 corpus, and it expresses moreclosely the proficiency of the learners.We think that the next step is to refine the sug-gestion models, since we currently take a simplefill-in-the-blank approach.
As future work, weplan to extend the models as follows: (1) use bothincorrect and correct sentences in learner corporafor training, and (2) employ ESL writing specificfeatures such as learners?
L1 for domain adapta-tion.AcknowledgmentsWe thank YangYang Xi of Lang-8, Inc. for kindlyallowing us to use the Lang-8 learner corpus.
Wealso thank the anonymous reviewers for their in-sightful comments.
This work was partially sup-ported by Microsoft Research CORE Project.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 597?604.Peter F Brown, Vincent J Della Pietra, Peter V DeS-ouza, Jenifer C Lai, Robert L Mercer, and VincentJ Della Pietra.
1992.
Class-Based n-gram Modelsof Natural Language.
Computational Linguistics,18(4):467?479, December.M Cettolo, Christian Girardi, and Marcello Federico.2012.
WIT3: Web Inventory of Transcribed andTranslated Talks.
In Proceedings of the 16th Con-ference of the European Association for MachineTranslation (EAMT), pages 28?30.Martin Chodorow, Markus Dickinson, Ross Israel, andJoel Tetreault.
2012.
Problems in Evaluating Gram-matical Error Detection Systems.
In Proceedings ofthe 24th International Conference on ComputationalLinguistics (Coling2012), pages 611?628.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263.Adriano Ferraresi, Eros Zanchetta, and Marco Baroni.2008.
Introducing and evaluating ukWaC, a verylarge web-derived corpus of English.
In Proceed-ings of the 4th Web as Corpus Workshop (WAC-4),pages 45?54.Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, andHitoshi Nishikawa.
2012.
Grammar error correc-tion using pseudo-error sentences and domain adap-tation.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics, pages 388?392.Xiaohua Liu, Bo Han, Kuan Li, Stephan HyeonjunStiller, and Ming Zhou.
2010.
SRL-based verb se-lection for ESL.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1068?1076.Xiaohua Liu, Bo Han, and Ming Zhou.
2011.
Cor-recting verb selection errors for ESL with the per-ceptron.
In 12th International Conference on Intel-ligent Text Processing and Computational Linguis-tics, pages 411?423.712Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-gata, and Yuji Matsumoto.
2011.
Mining revi-sion log of language learning SNS for automatedJapanese error correction of second language learn-ers.
In Proceedings of the 5th International JointConference on Natural Language Processing, pages147?155.Alla Rozovskaya and Dan Roth.
2011.
Algorithmselection and model adaptation for ESL correctiontasks.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics, pages 924?933.Tong Wang and Graeme Hirst.
2010.
Near-synonymlexical choice in latent semantic space.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (Coling 2010), pages 1182?1190.Jian-Cheng Wu, Yu-Chia Chang, Teruko Mitamura,and Jason S Chang.
2010.
Automatic collocationsuggestion in academic writing.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics Short Papers, pages 115?119.713
