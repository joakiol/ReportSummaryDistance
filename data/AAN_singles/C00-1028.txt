Explaining away ambiguity: Learning verb selectional preferencewith Bayesian networks*Mass imi l iano  C iarami ta  and Mark  JohnsonCognit ive and l , inguist ic  Sci('.n(:esBox \]978, Brown Univers i tyPr()vi(h;n(:e, l{,I 02912, USAmas s ?m?
:l J_ano_ciaramita@broun.
edu mj @cs.
broun, eduAbst ractThis t)at)er presents a Bayesian lnodel for unsu-t)(;rvised learning of v(;rb s(;le(-t;ional t)refer(;nc('s.For each vert) the model creates a 13~Wo.siann(~twork whose archii;e(:lan'(~ is (l(fl;(wmin(;d t).5'the h',xical hicr~m:hy of W()r(hmt mtd whos('~1)ar~mmt;(;rs are ('~sl;im;~l:(~d from a list; of v('d'l)-ol)je('t pairs \]'cram from a tort)us.
"lgxl)lainingaway", t~ well-known t)rop('xi;y of Baycsi~m net-works, helps the moth;1 (teal in a natural  fash-ion with word sense aml)iguity in tlw, training(tat~L.
()n a, word sense disamt)igu;~tion Lest ourmodel t)erformed l)ctl;c,r than ot;h(',r stal;(~ of timart systems for unSUl)crvis(~d learning ()t7 seh'x:-tionM t)r(d'er(mces.
Coml)utational (:Oml)lcxityl)rol)lems, wn.ys of improving tiffs ;tl)l)roa(:h mMmethods for iml)h'menting "('xt)laining away" inoth(;r graphical frameworks are dis('ussed.1 Se lec t iona l  p re ference  and senseambigu i tyR('.gularil;i('~s of avcrt) with rcsl)e(:t o t;lw.
seman-tic class of its m:guments (sul)j('.cl:, ol)j('.
(:l; mMindirect o\])je(:l;) arc called selectional prefer-enees (S1)) (Katz and Fodor, 1964; Chomsky,1965; Johnson-Laird, 1983).
The verb pilot car-ries the information thal; its ol)jecl; will likely 1)esome kind of veh, icle; sut)jects of tim vert) th, int,:t(md to 1)e h,'uman; ;rod sul)jects of the verb barkl;end l;o l)c, dogs.
For the sake of simt)licity wewill locus on the verl)-ot)je(:t relation all;houghthe techniques we will describe can be at)t)li(;dto other verb-argument pairs.
* We wouhl like to ttmnk the Brown Lal)orat;ory for Lin-guistic Inibrmation Processing; Thomas IIoflnann; ElieBienenstoek; 1)hilip Resnik, who provided us with train-ing and test data; and Daniel Oarcia for his hel ) with theSMILE lil)rary of (:lasses tbr Bayesian etworks that weused for our exl)eriments.
This research was SUl)l)orted1)y NSF awards 9720368, 9870676 and 9812169.. EN77TY._,vomet\]tiug - " ~ , " - .
.  "
~ ,~ .. _ " " CtHHyFOOl )  I , IQUl l )  PIISI:h'ICAI.
O l t J I iCT..... "?
/ '  i " \  , I , \  \ \  .
.
.
.
; -  " \ \ ,  ICK .lbod/ I I .
IM l iNT III'VI;'RA(;I" WAT/ : 'R  liqltid lAND olejeet ":?
/71<4 bel'i'rat~e ('OI"H'.
'I'; dr ink carlh land ISLAND ~ro l l ( 'APE<'q~lFe I;'S1't?1:'S.
';0 .IA VA- I ,I/'~ VA-2 IIAI,I i.~hmdt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ie.v~rcs.~o .\]av~ baliFigm'e 1: A 1)ortion of Wordnet.Models of the acquisition of SP arc impor-|;ant in their own right and h;w(', at)plic~tt.ions inN~tural ,anguage l~ro(:essing (NIA').
The selcc-tional l)rclhr(;nc(;s of ~L verb can b(; used t;o inti;rI;he l)ossil)\]c meanings of an mlknown re'gum(mrof a known verb; e.g., it; might be possibh; toinfer that  xzzz  is ~ kind ot!
dog front the tbllow-ing sentence: "The :rzJ:z barked all night".
Inp~rsing ;~ sentence seh,ctional l)refe, rcn(:es can 1)(;used to rank competing parses, providing a par-tim nlt',asur(; of scmmlt;ic well-forlnedness, in-v('stigating SI ) might hel l) us to understand thestructure of the mental lexicon.Systems for mlsupervised learning of SP usu-ally combine statistical aim knowledge-1)asedapproaches.
The knowledge-base componentis typicMly a database that  groups words intoclasses.
In the models w(' will see.
the knowl-edge base is Wordnet (Miller, 1990).
Word-net groups nouns into c, lasses of synonymsret)resenting concel)ts , called synsets ,  e.g.,{car,,,'ld, o,a, 'utomobilc,.
.
.}.
A noun that lm-hmgs to sew:ral synsets is ambiguous .
A t ran-187sitive and asymmetrical relation, hyponymy,is defined between synsets.
A synset is a hy-ponym of another synset if the tbrmer has thelatter as a broader concept; for example, BEV-ERAGE is a hyponym of LIQ UID.
Figure 1 de-picts a portion of the hierarchy.The statistical component consists ofpredicate-argument pairs extracted from acorpus in which the semantic lass of the wordsis not indicated.
A trivial algorithm mightget a list of words that occurred as objectsof the verb and output the semantic classesthe words belong to according to Wordnet.For example, if the verb drink occurred with'water and water E L IQUID,  the nlodel wouldlearn that drink selects tbr LIQUID.
As Resnik(1997) and abney and Light (1999) have found,the main problem these systems face is thepresence of ambiguous words in the trainingdata.
If the word java also occurred as anobject of drink, since java C BEVERAGEand java C ISLAND,  this model would learnthat drink selects tbr both BEVERAGE andISLAND.More complex models have been t)roposed.These models, though, deal with word senseambiguity by applying an unselective strategysimilar to the one above; i.e., they assmne thatanfl)iguous words provide equal evidence tbr alltheir senses.
These models choose as the con-eepts the verb selects tbr those that are in com-mon among several words (e.g., BEVERAGEabove).
This strategy works to the extent thatthese overlapping senses are also the conceptsthe verb selects tbr.2 Prev ious  approaches to learn ingse lect iona l  preference2.1 P~esnik's modelOurs system is closely related to those proposedin (Resnik, 1997) and (Atmey and Light, 1999).The fact; that a predicate p selects for a classc, given a syntactic relation r, can be repre-sented as a relation, selects(p, r, c); e.g., thateat selects for FOOD in object position canbe represented as selects(eat, object, FOOD).In (Resnik, 1997) selectional preference is quan-tiffed by comparing the prior distribution ofa given class c appearing as an argument,P(c)~ and the conditional probability of |;liesame class given a predicate and a syntac-COGNI770N 1/4 FOOD 7/4ESSENCE 1/4 FLESH 1/4 FRUH" 1/2 BREAD I/2 DAIRY I/2l' J l l lidea(O) meal(l) apple(I) bagel(l) cheese(l)Figure 2: Simplified Wordnet.
The numbersnext to the synsets represent he values offreq(p, r, c) estimated using (3), the nmnbers inl)arentheses represent the values of frcq(p, r, w).tic relation P(c\[p,r),  e.g., P(FOOD) andJ ( OODleat, object), The relative ntropy be-tween P(c) and P(clp, r) measures how nmchthe predicate constrains its arguments:S(p,r) = D(P(clp, r) II P(c)) (1)Resnik defines the se lect ional  assoc iat ion ofa predicate for a particular class c to be the por-tion of the selectional preference strength due tothat class:P(clP, ~') 1 P(clp , r) log (2) A(p,, .
,  c) - S(v, , ' )  P(c)Here the main problem is the estimation ofP(clp, r).
Resnik suggests  as  a plausil)le esti-mator /5(clp, r) de=r freq(p, r, c)/freq(p, r).
13utsince the model is trained on data that are notsense-tagged, there is no obvious way to esti-mate freq(p, r, c).
Resnik suggests consideringeach observation of a word as evidence tbr eachof the classes |;lie word belongs to,count(p, r, w)c) aasses(w) (3)~uc:ewhere count(p, r, w) is the nmnber of timesthe word w occurred as an argument of p inrelation r, and classes(w) is the number ofclasses w belongs to.
For exmnple, supposethe system is trained on (eat, object) pairs andthe verb occurred once each with meat, ap-ple, bagel, and cheese, and Wordnet is simpli-fied as in Figure 2.
An ambiguous word likemeat provides evidence also tbr classes that ap-pear unrelated to those selected by the verb.Resnik's assumption is that only the classes e-lected by the verb will be associated with each188( )  ,-<.,/ / - "  1\8 \ \ - .~SCOGNIllON ( ) FOOl)1\7 " - .  "
- - -( - - .
1 1 ' I ' I 'idea meat apph' bagel cheeaeFigure 3: The HMM version of the siml)le ex-ample.of the observed words, and hence will re(:eivethe highest values for l'(clp, r).
Using (3) wefill(t that the highesl; fre(luen(;y is in t'~t(;t as-social;ed with FOOD: .f'req(ea, I, objecl,, food)I + ~ + 7 _1_ 2 1  ~ ~ -- 717 an(t I'(leOODie.,l, ) = 0.44.H()wever, some eviden('e is tomM also for COG-NIT ION:  .fr(:q(cat,, obj,cl,, co9'~,,it, io'~,,) ~ ?
andI ' (COGNIT ' IONieat)  = 0.06.2.2 Abney  and  L ight ' s  approachAtmey and Light (1999) pointed ouI; l;h~tt l;hedistri l)ution of se.llSeS of all anfl)iguous word isno|; unifornl.
'\['tiey not;iced also l:hai; it is nol;clear how the 1)rol)ability l'((:\[p, 7") is t;o 1)e ini;er-t)reted sin(:e there is no exl)lieit sto(:llasl, i(: geil-(',ration nlodel involved.They \])rol)ose(t a syst;enl l;hat; ass()(',ial;esa Hidden Markov Model (\[IMM) wii;h 0,a(:hl)re(lic~te-re, lal;ion 1)air (p,r).
'l~'ansil;ions be-tween synsel; states rel)resent he hyt)onynly re-lation, and c, the empty word, is emitted withprobal)ility 1; transit ions to ~t tinal sta|;e enli|; aword w with 1)rot)al)ility 0 < P('m) < I.
35an-sit|on and emission t)rol)al)ilities are est imatedusing the \]'DM ;dgorittnn on i;raining data I;hatconsist of the lieu|is that o(;clirrext wi th  the w;rb.Abney and Ligh|,'s mo(lel e, stintates i '(clp, r )ti'om the model trained tbr (p,r); the disl, ri-l int|on P(c) (:ml 1)e calculatxxt from a modeltrained for all n(mns in the (:orpus.This model did not perti)rm as well as ex-pected.
All amt)iguous word ill the nlodel callbe generated 1)y more than one sl;;~te sequence.Atmey and Light dis(:overed that the EM al-goril;hm tinds t)arameter wflues that associatesome t)rol)ability mass with all the trmlsitionsin the lnultil)le paths that lead to an ambigu-ous word.
In other words, when there are sev-eral state sequences fi)r the same word, \]DM doesnot select one of l;hen: over the others.
I Figure 3shows the I)arameters esl;imated by EM for thesame examt)le as above.
The transit ion to theCOGNITION sl;ate has \])een assigne(t a t)rol)a-1)|lily of 1/8 because it is part of ~t possible l)athto meat.
The I IMM nlodel does not solve t, hel)roblent of the unselective distr ibution of thefrequen(:y of oceurren(:e of an aml)iguous wordto all its senses.
A1)ney and Light claimed that;this is a serious l)roblem, par|;i(:iilarly when l;heaml)iguous word is a ti'equent one, and cruisedthe model to learn the wrong seleel;ional pref-eren('es.
To (:orre(:i; this undesiral)le outcomethey introduced some smoothing and t)alam:ingte, chniques.
Howe, ver, even with these modiliea-tions their sysl;em's l)erfornlance, was t)elow fllal;a(:hieved l)y Resnik's.3 Bayes ian  networksA Bayes ian  network  (Pearl, 1988), orBayesian 1)el|el nel;work (BBN),  eonsisi;s of a sol;of var iab les  and a sel; of d i rec ted  edges  (:on-neel;ing the w~riat)les.
The variables and tileedges detine, a dire,(:te, d acyclic graph (DAG)where each wtrial)le is rei)resented 1)y ~ node.Ea(:h vmi~d)le is asso(:iated with a finite numberof (nmi;u;dly ex('lusive) sl;ates.
'1)) each wu:ial)leA with \]);n'eni;s \]31,..., I7~ is ;t|;l;;mll(',(l ;t condi-tio'n, al probability tabh', (CPT) l'(A\[131, ...,Hn).Given a BBN, Bayesiml int~rence (:~m 1)e used1;o esi;ilm~l;e marg ina l  and poster io r  p roba-b i l i t ies  given the evidence at hand ~md (;It(', in-fornlation six)red in the CPTs,  the pr io r  prob-ab i l i t ies ,  by means of B~yes' rule, P(H IE  ) =l'(H)P(s~ln) P(E) , where It stands fi)r hyt)othesis andE tbr eviden(:e.Baye, sian nel;works display ml exl;remely inter-eslfing t)roi)ert;y called exp la in ing  away.
Wordsense mnbiguity in the 1)recess of learning SP de,-tines a 1)rot)lem that nlight, l)e solved by a modelthat imt)lements an explaining away strategy.Sul)t)ose we ;~re learning the, selectional 1)refer-en(:e of drink, and the network ill Figure 4 is the'As a nmtt;er of fi*cl;, for this HMM there are (in-finitely) many i )a ramel ;e r  vahles that nmxinfize the like-lihood of t;he training data; i.e., l;he i)arame, l;ers are, not;idenl;ifiable.
The intuil;ively correct solution is one ofl;helll, \])ILL SO are infinitely lilalty ()|;her, intuitively incor-re(:t; ones.
Thus il, is no surprise l;hat the EM algorithmemmet; lind the intuitively correct sohlt;ion.189I ISIAND ?> BEVERAGE.\](11'(1 0 WllICFFigure 4: A Bayesian network for word ainbigu-ity.knowledge base.
The verb occurred with javaand water.
This situation can be ret)resentedas a Bayesian network.
The variables ISLANDand BEVERAGE represent concepts in a se-mantic hierarchy.
The wtriables java and waterstand for I)ossible instantiations of the concet)ts.All the w,riables are Boolean; i.e., they are as-sociated with two states, true or false.
Supposethe tbllowing CPTs define the priors associatedwith each node.
2 The unconditional probabili-ties are P ( I  = t,'~,,e) = P (B  = t,'~,,~0 = 0.01 andP( I  = false) = I"(13 = false) = 0.99, and theCPTs for the child nodes areI =>)  I1,13 1, ~13 ~I,13 -~I, ~f3j = true 0.99 (I.99 0.99 0.01j = false 0.01 0.01 0.01 0.99w = true 0.99 0.99 0.01 (I.01w false.
0.01 0.01 0.99 0.99These vahms mean that the occu,'rence of eitherconcept is a priori unlikely.
If either concept istrue the word java is likely to occur.
Similarly,if BE VERA GE occurs it; is likely to observe alsothe word water.
As the posterior probabilitiesshow, if java occurs, the belief~ in both conceptsincrease: P(II.j) = P(B I j  ) = 0.3355.
However,'water provides evidence for BEVERAGE only.Overall there is more evidence for the hypoth-esis that the concept being expressed is BEV-ERAGE and not ISLAND.
Bayesian networksimplement his inference scheme; if we computethe conditional probabilities given that bothwords occurred, we obtain P(BI j  , w) = 0.98 andP(I\[ j ,  w) = 0.02.
The new evidence caused the"island" hyt)othesis to be explained away!3.1 The relevance of priorsExplaining away seems to depend on the spec-ification of the prior prolmt)ilities.
The priors2I, 13, j and w abbrev ia te  ISLAND, 1lEVERAGE,java and water, respect ively.
(-)coaNmo,v ) roe, \[0 ii~\] L_ mFigm'e 5: A Bayesian network for the simpleexample.define the background knowledge awdlable tothe model relative to the conditional probabili-ties of the events represented by the variables,but also about the joint distributions of severalevents.
In the simple network above, we de-fined the probat)ility that either concept is se-lected (i.e., that the correst)onding variable istrue) to be extremely small.
Intuitively, thereare many concepts and the probability of ob-serving any particular one is small.
This meansthat the joint probability of the two events ismuch higher in the case in which only one ofthem is true (0.0099) than in the case in whichthey are both true (0.0001).
Therefore, via thepriors, we introduced a bias according to whichthe hypothesis that one concept is selected willbe t/wored over two co-occurring ones.
This is ageneral pattern of Bayesian networks; the priorcauses impler explanations to be preferred overmore complex ones, and thereby the explainingaway effect.4 A Bayesian network approach tolearning select ional  preference4.1 Structure and parameters of themodelThe hierarchy of nouns in Wordnet defines aDAG.
Its mapping into a BBN is straightibr-ward.
Each word or synset in Wordnet is anode in the network.
If A is a hyponym of Bthere is an are in the network from B to A. Allthe variables are Boolean.
A synset node is trueif the verb selects for that class.
A word node istrue if the word can appear as an argument ofthe verb.
The priors are defined tbllowing twointuitive principles.
First, it is unlikely that averb a priori selects for troy particular synset.Second, if a verb does select for a synset, sayFOOD, then it; is likely that it also selects tbr190i(;s hyl)onyms, say FR.UFI'.
The sam(', l>rin(:il)les;H)t)Iy (;o words: it is likely l;h~t a wor(t ;q)I)earsas an m:gmnent of t,h(; verl) if the vert) seh;(:l;s forany of il;s possible senses.
On l;he other h~m(t,if (,he verb does nol; selecl; for a synsel;, it; is'u, nlikely that  the words insl;anl;b~l;ing (;he synse(;occur ~s its ;~rgumen~s.
"Likely" and "unlikely"are given mml(;rical values l;h~l; Sllill 1l t) 1;o 1.The following l;at)le defines l;\]te scheme for theCPTs  associated wil;h each node in the nel;work;p i (X)  (lcnot;cs 1;12(: il.h, t2ar(:n(, of (;h(: no(t(: X.F__ \ ] .
P(X = xlI, I(X)V,... , VI,,(X) = t,",,(~) \]z .lalsc, unl ihc ly\[_ T l " (X = : , : lp , (X )a , .
.
.
, /> , , (X )= .l',,.Z.~(:) \]:t; .fitls(: like.l?/For (;h(; rool; nod(:s, the l;;d)le r(:(hl(:es 1;o (;h(:uncondil;ion~d t)rol)al/iliI;y of (;h(: node.
NowWO, (;}UI I;(:Si; (;he mo(M on (;h(: siml)l(: ex~mq)\](:seen (:~rli(:r. W + is th(: set of wor(ls l;ha( o(:-(:m'rc(t with I;he v(:rl).
'l.
'\]l(: \]2o(l(:s (:orr(:st)on(l-ing (;o |;he wor(ls in l/l/q ;w(: s(:l, 1;o lr,,,c ;rod(;h(: o(;hers l(:f(; mls(:l;.
For 1;12(: l)revious ex-ample W -I = {mca/,~ apph'., bagel, ch, c~c'.sc'.
}, mt(\]l;h(; (:orrest)on(ting no(t(:s m:c sol; (;o I/rue, as (t(:-t)i(:l;e(t in Figm'(; 5.
Wil;h lil,:(dy ;m(t ',.nhT~:c'.lyr(:sl)(:(:l;iv(:ly equal Ix) 0.9.0 mM 0.01, tit(: i)osl;(:-rior l)rol);d)ilil;i(:s are a /)(/i'\[m, a., b, c.) = 0.9899mM .P(Cl,m,, a, b, c) = 0.0101.
Expla.ining awayworks.
Th(; t)osl;(;rior 1)rol);fl)ilil;y of COGNI -T ION g(;l;s as low as i(;s prior, whcr(',as l;h(;1)rol)al)ilil;y of FOOD goes u t) to almost; 1.
A13~y(;sim~ n(;l;work ~q)t/roa(:\]~ seems 1;o ;l(:l;u~dl.yimt)leancn(; he.
conse.rva/,'ive, stral;egy w(: l;hough(;(;o 1)e (;he corr(;(:I; one.
for unsupervisc(t l(;~mfingof sehx-t, ional resi;ri(:tions.4.2 Computat iona l  i ssues  in bu i ld ingBBNs  based  on  WordnetTh(: imt)l(:m(:ni;~d;ion f a BBN for (;h(; whole ofW()r(lnel; fax:as (:oml)ul;al;ional (:oml)lexi|;y pro|)-l(:ms (;ypi(:al of graphi(:al too(Ms. A (l(:ns(:ly(:ommcI;ed \]IBN presents (;wo kinds of l)rol)l(:ms.The tirst is (;he storage of the CPTs.
The sizeof a CPT  grows extIonenl;ially with the nmnberof parents of the node.
4 This prol)lem can lieaF, C, m, a, b and c respec(;ively stand for FOOD,COGNITION, meat, apph'., bagd and chces(:4Some words in \Vordnet have mor(: than 20 senses.For (,Xaml)h: , line in \'Vordne(: is asso(:ia(,ed with 25EN77TY?
j?
"~.- /I"0()1) I,IQUID\- ..//B E VERA GE('OFI"EI?
drinkI,IA VA - II>IISYSIC'AL OB,II'2CT1ANDtISIANI)>/.
// -JAVA -2X.
/ : /j<,raFigure.
6: Th(; sulmel;work for d'riuJ<.solved 1)y ot)i;infizing the r(:l)r(:s(;nl;~ti;ion f thes(:(;;d)l(:s. In our case most of l,h(: (:ntri(:s h~w(: l;hes~tln('~ v;~luos~ ~ttl(l ~t COlll\])a,(:(; re l )resenl ;a l ; ion forl ; lmln ( ;&n l)(; fOllll(\] (ltlll(;h l ike l;h(', ( )he  llS(Xl in(;h(', no isy -OR too(tel (Pearl, 1{)88)).A h;~r(l(:r lirol)lem is lmrforming inf(;rc\]me.The gr;q)hi(:al sl;rlt('i;llr( ~,of & BBN r(;t)resenl;sl;h(: (l(:t)(:n(len(:y r(:lal;ions among th(: rml(lomwtrial)lcs of the, nel;work, r\]?he ~dgoril;hlns use(twil;lt B\]INs usmdly l)(M'orm inference t)y (ty-mmfi(" t)rogrmmning on the tri~mgul~d;e(t lnor~dgr~q)h. A low(n" 1)(mn(t on l, he mmfl)er of (:om-l)ul;al;ions l;h;~(; are n(:(:(:ssa.ry I;() mo(t(:I l;h(; joint(lisl;ritmi;ion ov(:r l;h(: wn'bd)h:s using su(:h ~dgo-ril;hms is 21"1t I wh(:r(: 'r~, is t;\]m size of (:h(; ma.x-imal l)(mn(tary s(:l; a(:(:or(ling (;o t;hc visil;a,tions(:h(:(lul(:.4.3 Subnetworks  and  ba lanc ingB(,(:mls(; of l;h(:s(, 1)rol)h:ms w(: (:(told not t)uihl asingl(: BBN for Wor(hmI;.
Insl;e~M w(: simt/litie(t(;he sl;rll('l;ur(: of 1;12(: model by building a smallersutmei;work for each 1)re(ticate-argumenl; pair.
Asulm(:twork consis(;s of (;he mlion of the s(:ts ofml(:(;sl;ors of the words in W +.
Figure.
6 pro-vid(:s ml example of the union of these :%nces-tral sul)grat)hs" of Wordne(; for (;he words java~m(l drink (COml)~We i(; wil;h Figure 1).This siml)liti(:ation (toes not atfe(:t the (:om-pul;~tion of the (tistril)ui;ions we are inl;(;resl;edin; l;h;fl; is, the marginals of the synset nodes.A BBN provi(tes a coral)act representation tbrthe joinI; disl;rit)ution over the set of variablessenses.
The size of its OPT is therefor('.
2 2(~.
Six)ring a (:a-1)Ie of tloa(; numbers tbr l;his node alone requires around(2'-)~)8 = 537 MBytes of memory.191in the network.
If N = X1, ..., Xn  i8 a Bayesiannetwork with variables X1,..., Xn, its joint dis-tribution P(N) is the product of all the condi-tional probabilities pecified in the network,P (N)  = I I  P(XJl p,,(Xj)) (4)Jwhere pa(X) is the set of parents of X.
A BBNgenerates a factorization of the joint distribu-tion over its variables.
Consider a network ofthree nodes A, B~ C with arcs fl'om A to \]5 andC.
Its joint distribution can be characterized asP(A, B, C) = P(A)P(BIA)P(CIA ).
If there isno evidence for C the joint distribution isP(A ,B ,C)  = P(A)P(BIA ) ~ P(CIA )c= P(A)P(B IA  )= P(A, B)The node C gets marginalized out.
Marginaliz-ing over a childless node is equivalent o remov-ing it with its connections from the network.Therefore the subnetworks are equivalent to thewhole network; i.e., they have the same jointdistribution.Our model comtmtes the value of P(c\[p,r),lint we did not compute the prior P(c) for alln(mns in the cortms.
We assumed this to bea constant, equal to the 'u'nlihcly wflue, for allclasses.
In a BBN the wdues of the marginalsincrease with their distance fl'om the root nodes.To avoid undesired bias (see table of results) wedefined a balancing formula that adjusted theconditional probabilities of the CPTs in such away that we got; all tim marginals to have ap-proximately the same wdue)5 Exper iments  and  resu l t s  a5.1 Learn ing of  se leet iona l  pre ferencesWhen trained on t)redicate-argument pairs ex-tracted from a large corpus, the San .Jose Mer-cury Corpus, the model gave very good results.The corpus contains about 1.a million verb-object tokens.
The obtained rankings of classesaccording to their posterior marginal probabili-ties were good.
Table 1 shows the top and the'~More details can be found in an extended version ofthe paper: www.cog.brown.edu/~massi/.6For these experimc'nts we used values for the likelyand unlikely 1)arameters of 0.9 and 0.1~ respectively.Ranking Synset P(clp, r)1 VEIIICLE 0.99952 VESSEL 0.98933 AIRCRAFT 0.99374 AIRPLANE 0.95005 SHIP 0.9114255 CONCEPT 0.1002256 LAW 0.1001257 PIIILOSOPIIY 0.1000258 ,IUI?,ISPRUDENCE 0.1000Table 1: Results tbr (maneuver, object).bottom of the list of synsets for the verb ma-neuver.
Tile model learned that maneuver "se-lects" for melnbers of the class VEttlCLE andof other plausible classes, hyponynls of I/EHI -CLE.
It also learned that the verb does notselect for direct; objects that are inembers of(:lasses, like CONCEPT or PItILOSOPltY.5.2 Word  sense d i sambiguat ion  testA direct ewfluation measure for unsupervisedlearning of SP models does not exist.
Theselnodels are instead evaluated on a word-sensedisambiguation test (WSD).
The idea is thatsystems that learn SP produce word sense dis-amt)iguation as a side-effect.
Java might be in-terl)reted as the island or the beverage, but in acontext like "the tourists flew to Java" the for-mer seems more correct, because fly could selectfor geographic locations but not for beverages.A system trained on a predicate p should heable to disambiguate arguments of p if it haslearned its selectional restrictions.We tested our model using the test andtraining data developed by Resnik (see Resnik,1997).
The same test was used in (Almeyand Light, 1999).
The training data consistsof predicate-object ounts extracted fl'oln 4/5of the Brown corpus (at)out 1M words).
Thetest set consists of predicate-object pairs fromthe remaining 1/5 of the corpus, which hasbeen manually sense-annotated by Wordnet re-searchers.
The results are shown in Table 2.The baseline algorithm chooses at random oneof the multiple senses of an ambiguous word.The "first sense" method always chooses themost frequent sense (such a system should betrained on sense-tagged data).
Our model per-192Method ResultBaseline 28.5%Abney mM Light (HMM smootlm(l) 35.6%Abney and Light (ItMM 1)alan(:ed) 42.3%Resnik 44.3%BBN (without bM~mcing) 45.6%BBN (with bMancing) 51.4(/(/First Sense 82.5(/0Table 2: R,esultsformed 1)etter th;m the state of tlm art mo(telsfor mlSUl)ervised le~n'ning of SP.
It seems to de-fine i~ l)ett(;r esi;ima, l;or for \])(ell), 'r).It is remnrkabh: fliat the model ~mhi(:ved thisresult making only a limi(;(:(t use of distribu-tional informal;ion.
A n(mn is in 14 f+ if it oe-('urred at h;ast once in th(: tra,ining set, 1)ut thesysi;em does not know if i(; o(:(:urre(l once or sev-eral times; either it oc(:urred or it didn't.
Thenl()(te, l did not; suffer too mu(:h froxn this limi-(;ntion (htril~g (;his task.
This is 1)rol)~d)ly (tucto the Sl)arsencss of the (;rltining (t;tl;a for (;tietest.
For each verb (;he average mmfl)er ()f el)-\]eel; tyl)es is 3.3, for each of them tim :werng(:lxumber of (;ok(ms is 1.3; i.e., most of the, wordsin the training data.
only ()(:(:urred once.
Pin"this training set we ~dso t(:sted a version of (;hem()del that |rail(; it word node ti)r each el)servedol)je(:l; token ;~n(| (;here, fore inl,(~gral;e(t the (listri-lmtional informntion.
()n (;\]m WSI) test it; per-ti)rmed exactly the stone its the simph~r version.When trained on the, San .lose Mercury Cort)usthe model l)erfornle(t worse on the WSI) t:esl;(35.8%).
This is not too surprising considering(;he diftbxelmes beA;ween tim SJM and the \]~rown(:ort)or~: the former, i~ re(:ent newswire cortms;the llg;ter, &Xl older, |)alml(:ed ('orl)uS.
Allot;herilnportmlt factor is the different relevance of dis-tributional informntion.
The training (tatn fl'omthe SJM Corlms are nnlch ri(:her and noisierthan the Brown data.
Here the, fl'equen(:y in-tbrm~tion is probably crucbfl; however, in thiscase we could not imt)lement the silnl)le s('hem(~;tb ove.5.3 Conc lus ionExplaining away imt)lements ;~ (:ognitively ~tt-tractive and successful strategy.
A straighttbr-ward lint)rove, men( would lm tbr the me(tel tomake flfll use of the distrilmtional ildbrmtLtionpresent in the training data; we only partiallyachieved this.
B~yesian networks are usuallyconfronted with a single present~Ltion of evi-den('e.
Their exi;ension to multil)le evidence isnot triviM.
We believe the model can be ex-tended in this direction.
Possibly there m'e sev-erM ways to do so (muir(hernial Saml)ling , ded-i(:ated implementations, etc.).
However, we be-lieve that the most relevmit finding of this re-search mighl; t)e (;h~t(; %xplnining itww" is notonly a 1)roperl;y of Bayesian networks but of\]3ayesimx infe, rence in general and that it mighttm imt)lemental)le in other kinds of graphicalmodels.
We, observed that (;he prol)erty seems todel)end on the specification of the prior proba-bilities.
We t'omld (;lm(; (;he HMM model of (Ab-hey mid Ligh(;, 1999) was 'unidentifiable; that is,(;here are several sohli;iolxs tbr the \])~tra, xnel;els ofthe lno(te,1, including the desired Olle.
OHr intu-ition is (;hat it shouht l)e possible to imt)lemelxt"exl)laining awlty" in a HMM with 1)tiers, so(;hit(; il; wouh\[ 1)rethr only ()lie or ~t ti~w solu(;ions()ver lmuiy.
This model would have also the a(t-wmtttgc of t)eing (:Onll)U|;~d;ionally silnt)ler.ReferencesS.
Almey told M. Light.
1999.
Hiding a serum>tic hierarchy in ~ Mm'kov model.
In P'm(:e,d-ings q/' the Workshop o'n Uns'~q)ervi,scd Le,,'r'n,-ing in Nat'wra,1 Lang,uagc Proeessin9, A CL.N.
Chomsky.
1965.
Aspcct.s of th, e Theory ofSy'nta,:r. MIT Press, Cambridge, MA.P.
N. Johnson-Laird.
1983.
Mental Models: 2b-'wards a Cognitive Sciev, ce of Lang'uage , Pn:fi',r-c'nce, a'n,d Con.sciousncss.
ilm'w~rd UniversityPress.J .
.
\] .
Kntz and .\].
A. Fodor.
1964.
The.
struc-ture of ;~ semmiti(: theory.
In ,\].
,J. KaI;z mMJ.
A. l/odor, editors, The St'ructure of Lan-g'uage.
Prentice-Ilall.G.
Miller.
299(I. Wordnet: An on-line lexicaldatabase, btternational Journal of Lexicog-raphy, 3(4).J.
Pearl.
\] 988.
Probabilistic R, easo'n, ing in Intel-ligent, Systems: Net'worl?s of Plausible l~l:fer-e'n, ee.
Morgml Kimthmn.P.
Resnik.
2997.
Sele(:tiona\] prefi:ren(:e andsens(; disamt)iguation.
In Proceedings of th.eANLP-97 Workshop: Taggin 9 Text with Lex-ical Semantics: Wll.y, What, and Itow?193
