Proceedings of NAACL-HLT 2013, pages 127?137,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTraining Parsers on Incompatible TreebanksRichard JohanssonSpra?kbanken, Department of Swedish, University of GothenburgBox 200, SE-40530 Gothenburg, Swedenrichard.johansson@gu.seAbstractWe consider the problem of training a sta-tistical parser in the situation when there aremultiple treebanks available, and these tree-banks are annotated according to different lin-guistic conventions.
To address this problem,we present two simple adaptation methods:the first method is based on the idea of usinga shared feature representation when parsingmultiple treebanks, and the second method onguided parsing where the output of one parserprovides features for a second one.To evaluate and analyze the adaptation meth-ods, we train parsers on treebank pairs in fourlanguages: German, Swedish, Italian, and En-glish.
We see significant improvements forall eight treebanks when training on the fulltraining sets.
However, the clearest benefitsare seen when we consider smaller trainingsets.
Our experiments were carried out withunlabeled dependency parsers, but the meth-ods can easily be generalized to other feature-based parsers.1 IntroductionWhen developing a data-driven syntactic parser, weneed to fit the parameters of its statistical model ona collection of syntactically annotated sentences ?
atreebank.
Generally speaking, a larger collection ofexamples in the training treebank will give a higherquality of the resulting parser, but the cost in timeand effort of annotating training sentences is fairlyhigh.
Most existing treebanks are in the range of afew thousand sentences.However, there is an abundance of theoreticalmodels of syntax and there is no consensus on howtreebanks should be annotated.
For some languages,there exist multiple treebanks annotated accordingto different syntactic theories.
Apart from German,Swedish, and Italian, which will be considered inthis paper, there are important examples among theworld?s major languages, such as Arabic and Chi-nese.To exemplify how syntactic annotation conven-tions may differ in even such a simple case as un-labeled dependency annotation, consider the Italiansentence fragment la sospensione o l?interruzione(?the suspension or the interruption?)
in Figure 1.
Aswe will see in detail in ?3.1.3, there are two Ital-ian treebanks: the ISST and TUT.
If annotating asin the ISST treebank (drawn above the sentence)determiners (la, l?)
are annotated as dependents ofthe following nouns (sospensione, interruzione); inTUT (drawn below the sentence), we have the re-verse situation.
There are also differences in howcoordinate structures are represented: in ISST, thetwo conjuncts are directly conjoined and the con-junction attached to the first of them, while in TUTthe conjunction acts as a link between the conjuncts.osospensionela interruzionel?Figure 1: Differences in dependency annotation styles.Given the high cost of treebank annotation and theimportance of a proper amount of data for parser de-velopment, this situation is frustrating.
How couldwe then make use of multiple treebanks when train-ing a parser?
A na?
?ve way would be simply to con-catenate them, but as we will see this results in aparser that performs badly on all the treebanks.In this paper, we investigate two simple adapta-tion methods to bridge the gap between differing127syntactic annotation styles, allowing us to use moredata for parser training.
The first approach treatsthe problem of parsing with multiple syntactic an-notation styles as a multiview learning problem andaddresses it by using feature representation that ispartly shared between the views.
In the second onewe use a parser trained on one treebank to guide anew parser trained on another treebank.
We evaluatethese methods as well as their combination on fourlanguages: German, Swedish, Italian, and English.In all four languages, we see a similar picture: theshared features approach is generally better whenone of the treebanks is very small, while the guidedparsing approach is better when the treebanks aremore similar in size.
However, for most trainingset sizes the combination of the the two methodsachieves a higher performance than either of themindividually.2 Methods for Training Parsers onMultiple TreebanksWe now describe the two adaptation methods toleverage multiple treebanks for parser training.
Forclarity of presentation, we assume that there aretwo treebanks, although we can easily generalize tomore.
We use a common graph-based parsing tech-nique (Carreras, 2007); the approaches describedhere could be used in transition-based parsing aswell.In a graph-based parser, for a given sentence xthe task of finding the top-scoring parse y?
is statedas an optimization problem of maximizing a linearobjective function:y?
= argmaxyw ?
f(x, y).Here w is a weight vector produced by some learn-ing algorithm and f(x, y) a feature representationthat maps the sentence x with a parse tree y toa high-dimensional vector; the adaptation methodspresented in this work is implemented as modifica-tions of the feature representation function f .
Sincethe search space is too large to be enumerated, themaximization must be handled carefully, and howthis is done determines the expressivity of the fea-ture representation f .
In the parser by Carreras(2007) the maximization is carried out by a dynamicprogramming procedure relying on crucial indepen-dence assumptions to break down the search spaceinto tractable parts.
The factorization used in thisapproach allows f to express features extracted notonly from single edges, as McDonald et al(2005),but also from sibling and grandchild edges.To understand the machine learning problem oftraining parsers on incompatible treebanks, we com-pare it to the related problem of domain adapta-tion: training a system for a target domain, usinga large collection of training data from a source do-main combined with a small labeled or large unla-beled set from the target domain.
Some algorithmsfor domain adaptation rely on the assumption thatthe differences between source and target distribu-tions Ps and Pt can be explained in terms of a co-variate shift: Ps(y|x) = Pt(y|x) for all x, y, butPs(x) 6= Pt(x) for some x.
In our case, we have thereverse situation: the input distribution is at least intheory unchanged between the two treebanks, whilethe input?output relation (i.e.
the treebank annota-tion style) is different.
However, domain adaptationand cross-treebank training can be seen as instancesof the more general problem of multitask learning(Caruana, 1997).
Indeed, one of the simplest andmost well-known approaches to domain adaptation(Daume?
III, 2007), which will also be considered inthis paper, should more correctly be seen as a trickto handle multitask learning with any machine learn-ing algorithm.
On the other hand, there is no pointin trying to use domain adaptation methods assum-ing a covariate shift, e.g.
instance weighting, or anymethod in which the target data is unlabeled (Blitzeret al 2007; Ben-David et al 2010).2.1 Sharing Feature RepresentationsOur first adaptation method relies on the intuitionthat some properties of two treebanks are shared,while others are unique to each of them.
For in-stance, as we have seen in Figure 1 the two Ital-ian treebanks annotate coordination differently; onthe other hand, these treebanks also annotate sev-eral other linguistic phenomena in the same way.This observation can then be used to devise a modelwhere we train two parsers at the same time and usea feature representation that is partly shared betweenthe two models, allowing the machine learning algo-rithm to automatically determine which properties128of the two datasets are common and which are dif-ferent.
The idea of using features that are shared be-tween the source and target training sets is a slightgeneralization of a well-known method for super-vised domain adaptation (Daume?
III, 2007).In practice, this is implemented as follows.
As-sume that originally a sentence x with a parse tree ywas represented as f1(x, y) if it came from the firsttreebank, and f2(x, y) if from the second treebank.We then add a shared feature representation fs to f1and f2, and embed them into a single feature space.The resulting feature vectors then becomef1(x, y) ?
02 ?
fs(x, y) (1)for a sentence from the first treebank, and01 ?
f2(x, y) ?
fs(x, y) (2)for the second treebank.
Here, 01 means an all-zerovector with the dimensionality of the feature spaceof f1, and ?
is vector concatenation.
Using this newrepresentation, the two datasets are combined and asingle model trained.
The hope is then that the learn-ing algorithm will store the information about the re-spective particularities in the weights for f1 and f2,and about the commonalities in the weights for fs.The result of this process is a symmetric parser thatcan handle both treebank formats: when we parsea sentence at test time, we just use the representa-tion (1) if we want an output according to the firsttreebank and (2) for the second treebank.In this work, f1, f2, and fs are identical: all ofthem correspond to the feature set described by Car-reras (2007).
However, it is certainly imaginablethat fs could consist of specially tailored featuresthat make generalization easier.
In particular, usinga generalized fs would allow us to use this approachin more complex cases than considered here, for in-stance if the dependencies would be labeled withtwo different sets of grammatical function labels, orif one of the treebanks would use constituents ratherthan dependencies.2.2 Using One Parser to Guide AnotherThe second method is inspired by work in parsercombination, an idea that has been applied success-fully several times and relies on the fact that dif-ferent parsing methods have different strengths andweaknesses (McDonald and Nivre, 2007), so thatcombining them may result in a better overall pars-ing accuracy.
There are several ways to combineparsers; one of the simplest and most successfulmethods of parsing combination uses one parser asa guide for a second parser.
This is normally im-plemented as a pipeline where the second parser ex-tracts features based on the output of the first parser.Nivre and McDonald (2008) used this approachfor combining a graph-based and a transition-basedparser and achieved excellent results on test sets forseveral languages, and similar ideas were proposedby Martins et al(2008).We added guide features to the parser feature rep-resentation.
However, the features by Nivre andMcDonald (2008) are slightly too simple since theyonly describe whether two words are directly con-nected or not.
That makes sense if the two parsersare trying to predict the same type of representation,but will not help us if there are systematic annota-tion differences between the two treebanks, for in-stance in whether to annotate a function word or alexical word as the head.
Instead, following workin semantic role labeling and similar areas, we use ageneralized notion of syntactic relationship that weencode by determining a path between two nodesin a syntactic tree.
We defined the function Path(x,y) as a representation describing the steps requiredto traverse the parse tree from x to y, first the stepsup from x to the common ancestor a and then downfrom a to y.
Since we are working with unlabeledtrees, the path can be represented as just two inte-gers; to generalize to labeled dependency parsing,we could have used a full path representation ascommonly used in dependency-based semantic rolelabeling (Johansson and Nugues, 2008).We added the following path-based feature tem-plates, assuming we have a potential head h withdependent d, a sibling dependent s and grandchild(dependent-of-dependent) g:?
POS(h)+POS(d)+Path(h, d)?
POS(h)+POS(s)+Path(h, s)?
POS(h)+POS(d)+POS(s)+Path(h, s)?
POS(h)+POS(g)+Path(h, g)?
POS(h)+POS(d)+POS(g)+Path(h, g)To exemplify, consider again the example lasospensione o l?interruzione shown in Figure 1.
As-129sume that we are parsing according to the ISST rep-resentation (drawn above the sentence) and we con-sider adding an edge with sospensione as head andla as dependent, and another parser following theTUT representation (below the sentence) has cre-ated an edge in the opposite direction.
The firstfeature template above would then result in a fea-ture NOUN+DET+(1,0), where (1,0) represents thepath relationship between the two words in the TUTtree (one step up, no step down).
Similarly, whenthe ISST parser adds the coordination edge betweensospensione and interruzione, it can make use ofthe information that these two nouns are indirectlyconnected in the output by the TUT parser; this isrepresented as a path (1,3).
This is an example ofa situation where we have a systematic correspon-dence where a single edge in one representation cor-responds to several edges in the other.Like the multiview approach described above, thismethod is trivially adaptable to more complex situ-ations such as labeled dependency parsers with dif-fering label sets, or dependency/constituent parsing.2.3 Combining MethodsThe two adaptation methods are orthogonal and caneasily be combined.
When trying to improve the per-formance of a parser trained on the primary treebankT1 by leveraging a supporting treebank T2, we thenuse T2 in two different ways: first by training a guideparser, and secondly by concatenating it to T1 usinga shared feature representation.3 ExperimentsWe carried out experiments to evaluate the cross-framework adaptation methods.
The evaluationswere carried out using the official CoNLL-X eval-uation script using the default parameters.
Since ourparsers do not predict edge labels, we report unla-beled attachment scores in all tables and plots.3.1 Treebanks Used in the ExperimentsIn our experiments, we used four languages: Ger-man, Swedish, Italian, and English.
For each lan-guage, we had two treebanks.
Our approaches cur-rently require that the treebanks use the same tok-enization conventions, so for Italian and Swedish weautomatically retokenized the treebanks.
We alsomade sure that the two treebanks for one languageused the same part-of-speech tag sets, by applyingan automatic tagger when necessary.3.1.1 German: Tiger and Tu?Ba-D/ZFor German, there are two treebanks available:Tiger (Brants et al 2002) and Tu?Ba-D/Z (Telljo-hann et al 2004).
These treebanks are constituenttreebanks, but dependency versions are available:Tu?Ba-D/Z (version 7.0) includes the dependencyversion in the distribution, while for Tiger we usedthe version from CoNLL-X (Buchholz and Marsi,2006).
The constituent annotation styles in the twotreebanks are radically different: Tiger uses a veryflat structure with a minimal amount of intermediatenodes, while Tu?Ba-D/Z uses a more elaborate struc-ture including topological field information.
How-ever, the dependency versions are actually quite sim-ilar, at least with respect to attachment.
The mostcommon systematic difference we observed is in theannotation of coordination.Both treebanks are large: for Tiger, the trainingset was 31,243 sentences and the test set 7,973 sen-tences, and for Tu?Ba-D/Z 40,000 and 11,428 sen-tences respectively.
We did not use the Tiger test setfrom the CoNLL-X shared task since it is very small.We applied the TreeTagger POS tagger (Schmid,1994) to both treebanks, using the pre-trained Ger-man model.3.1.2 Swedish: Talbanken05 and SyntagAs previously noted by Nivre (2002) inter alia,Swedish has a venerable tradition in treebanking:there are not only one but two treebanks which mustbe counted among the earliest efforts of that kind.The oldest one is the Talbanken or MAMBA tree-bank (Einarsson, 1976), which has later been repro-cessed for modern use (Nilsson et al 2005).
Theoriginal annotation is a function-tagged constituentsyntax without phrase labels, but the reprocessed re-lease includes a version converted to dependencysyntax.
The dependency treebank was used in theCoNLL-X Shared Task (Buchholz and Marsi, 2006),and we used that version version in this work.The second treebank is called Syntag (Ja?rborg,1986).
Similar to Talbanken, its representation usesfunction-tagged constituents but no phrase labels.We developed a conversion to dependency trees,which was straightforward since many constituents130have explicitly defined heads (Johansson, 2013).The two treebank annotation styles have signifi-cant differences.
Most prominently, the Syntag an-notation is fairly semantically oriented in its treat-ment of function words such as prepositions andsubordinating conjunctions: in Talbanken, a prepo-sition is the head of a prepositional phrase, whilein Syntag the head is the prepositional complement.There are also some domain differences: Talbankenconsists of student essays and public information,while Syntag consists of news text.To make the two treebanks compatible on the to-ken level, we retokenized Syntag ?
which handlespunctuation in an idiosyncratic way ?
and applied aPOS tagger trained on the Stockholm?Umea?
Corpus(Gustafson-Capkova?
and Hartmann, 2006) to bothtreebanks.
For Talbanken, we used 7,362 sentencesfor training and set aside a new test set of 3,680 sen-tences since the CoNLL-X test set is too small forserious experimental purposes ?
only 389 sentences.For Syntag, we split the treebank into 3,524 sen-tences for training and 1,763 sentences for testing.3.1.3 Italian: ISST and TUTThere are two Italian treebanks.
The first is theItalian Syntactic?Semantic Treebank or ISST (Mon-temagni et al 2003).
Here, we used the version thatwas prepared (Montemagni and Simi, 2007) for theCoNLL-2007 Shared Task (Nivre et al 2007).The TUT treebank1 is a more recent effort.
Thistreebank is available in multiple constituent and de-pendency formats, and we have used the CoNLL-formatted dependency version in this work.
Therepresentation used in TUT is inspired by the WordGrammar theory (Hudson, 1984) and tends to bemore surface-oriented than that of ISST.
For in-stance, as pointed out above in the discussion ofFigure 1, TUT differs from ISST in its treatment ofdeterminer?noun constructions and coordination.
Ithas been noted (Bosco and Lavelli, 2010; Bosco etal., 2010) that the TUT representation is easier toparse than the ISST representation.We simplified the tokenization of both treebanks.In ISST, we split multiwords into separate tokensand reattached clitics to nonfinite verb forms.
For in-stance, a single token a causa di was converted into1http://www.di.unito.it/?tutreeb/three tokens a, causa, di, and the three tokens trovar-se-lo into a single token trovarselo.
In TUT, weapplied the same conversions and also recomposedpreposition?article and multiple-clitic contractionsthat had been split by the annotators, e.g.
della,glielo etc.2 After changing the tokenization, we ap-plied the TreeTagger POS tagger (Schmid, 1994) toboth treebanks, using the pre-trained Italian modelwith the Baroni tagset3.After preprocessing the data, we created trainingand test sets.
For ISST, the training set was 2,239and the test set 1,120 sentences, while for TUT thetraining set was 1,906 and the test set 954 sentences.3.1.4 English: Two Different Conversions ofthe Penn TreebankFor English, there is no significant dependencytreebank so we followed most previous work in us-ing dependency trees automatically derived fromconstituent trees in the large Penn Treebank WSJcorpus (Marcus et al 1993).
Due to the factthat there is a highly parametrizable constituent-to-dependency conversion tool available (Johanssonand Nugues, 2007), we could create two dependencytreebanks with very different annotation styles.The first training set was created from sections02?12 of the WSJ corpus.
By default, the conversiontool outputs a treebank using the annotation styleof the CoNLL-2008 Shared Task (Surdeanu et al2008); however we wanted to create a more surface-oriented style for this treebank, so we turned on op-tions to make wh-words heads of relative clauses,and possessive markers heads of noun phrases.
Thiscorpus had 20,706 sentences, and will be referred toas WSJ Part 1 in the experimental section.The second training treebank was built from sec-tions 13?22.
For this treebank, we inverted thevalue of most options in order to get a more seman-tically oriented treebank where content words areconnected directly.
In this treebank, we also used?Prague-style?
annotation of coordination: the con-juncts are annotated as dependents of the conjunc-tion.
This set contained 20,826 sentences, and will2It should be noted that these conversions also make sensefrom a practical NLP point of view, since a number of contrac-tions are homonymic with other words.3http://sslmit.unibo.it/?baroni/collocazioni/itwac.tagset.txt131be called WSJ Part 2.We finally applied both conversion methods tosections 24 and 23 to create development and testsets.
The development set contained 1,346 and thetest set 2,416 sentences.
We did not change the tok-enization or part-of-speech tags of the WSJ corpora.Here, we should note that we have a slightly moresynthetic and controlled experimental setting thanfor Swedish and German: the parsers are evaluatedon the same test set, so we know that there is nodifference in test set difficulty.
We also know a pri-ori that performance differences are not due to anysignificant differences in genre, since all texts comefrom the same source (the Wall Street Journal) andtend to focus on business-oriented news.3.2 Baseline Parsing PerformanceAs a starting point, we trained parsers on all tree-banks.
In addition, we created a parser using a na?
?veadaptation method by combining the training sets foreach language, and training parsers on those threesets.
We then applied all three parsers for every lan-guage on both test sets for that language.
The re-sults for German, Swedish, Italian, and English arepresented in Table 1.Every parser performed well on the test set anno-tated in the same annotation style as its training set.As has been observed previously, surface-orientedstyles are easier to parse than semantically orientedstyles: The Talbanken and WSJ Part 1 parsers allachieve much higher performance on their respec-tive test sets than the Syntag and WSJ Part 2 parsers.The better performance of the Talbanken parser isalso partly explainable by the fact that its trainingset is more than twice as large as the Syntag trainingset.
Similarly for German, we see slightly higherperformance for Tu?Ba-D/Z than for Tiger.However, as can be expected every parser per-formed very poorly when applied to the test set us-ing the annotation style it was not trained on.
ForSwedish and English, the accuracy figures are in therange of 50-60, while the figure are a bit less poorfor German since the two treebanks are more simi-lar.
We also see, again unsurprisingly, that the na?
?vecombination baseline performs poorly in all situa-tions: we just get a ?worst-of-both-worlds?
parserthat performs badly on both test sets.GERMAN Acc.
on Tiger Acc.
on TBDZTiger 87.8 72.0Tu?Ba-D/Z 71.8 89.4Tiger+TBDZ 77.7 87.7SWEDISH Acc.
on ST Acc.
on TBSyntag 81.4 52.6Talbanken 50.3 88.2Syntag+Talbanken 61.8 82.7ITALIAN Acc.
on ISST Acc.
on TUTISST 81.1 57.4TUT 55.9 84.0ISST+TUT 73.9 71.6ENGLISH Acc.
on WSJ 1 Acc.
on WSJ 2WSJ part 1 92.6 57.4WSJ part 2 57.4 89.5WSJ parts 1+2 75.3 72.1Table 1: Baseline performance figures.3.3 Evaluation on the Full Training SetsWe trained new parsers using the shared features andguided parsing adaptation methods described in ?2.Additionally, we trained parsers using both methodsat the same time; we refer to these parsers as com-bined.
Including the baseline parsers, this gave us24 parsers to evaluate on their respective test sets.The results for German are given in Table 2.
Here,we see that all three adaptation methods give statis-tically significant4 improvements over the baselinewhen parsing the Tiger treebank.
In particular, thecombined method gives a strong 0.7-point improve-ment, a 6% error reduction.
For Tu?Ba-D/Z, the im-provements are smaller, although still significant ex-cept for the guided parsing method.Method Acc.
on Tiger Acc.
on Tu?Ba-D/ZBaseline 87.8 89.4Shared 88.1 89.6Guided 88.4 89.5Combined 88.5 89.6Table 2: Performance figures for the German adaptedparsers.
Results that are significantly different from thebaseline performances are written in boldface.4At the 95% level.
The significance levels of differenceswere computed using permutation tests.132Method Acc.
on ST Acc.
on TBBaseline 81.4 88.2Shared 81.3 88.3Guided 82.5 88.4Combined 82.5 88.5Table 3: Performance of the Swedish adapted parsers.For Swedish, we have a similar story: we seestronger improvements in the weak parser.
Sincethe Talbanken treebank is twice as large as the Syn-tag treebank and has a surface-oriented representa-tion that is easier to parse, this parser is useful asa guide for the Syntag parser: the improvements ofthe guided and combined Syntag parsers are statis-tically significant.
However, it is harder to improvethe Talbanken parser, for which the baseline is muchstronger.
3 shows the results for the Swedish parsers.Method Acc.
on ISST Acc.
on TUTBaseline 81.1 84.0Shared 81.5 84.4Guided 81.7 84.3Combined 81.8 84.7Table 4: Performance of the Italian adapted parsers.When we turn to the English corpora, the adapta-tion methods again gave us a number of very largeimprovements.
The results are shown in Table 5.The shared features and combined methods gave sta-tistically significant improvements for the WSJ Part1 parser, and the guided parsing method an improve-ment that is nearly significant.
However the mostdramatic change is the 1.2-point improvement of theWSJ Part 2 parser, given by the guided parsing andcombined methods.
It is possible that this resultpartly can be explained by the fact that this exper-iment is a bit cleaner: in particular, as outlined in?3.1.4, there are no domain differences.Method Acc.
on WSJ 1 Acc.
on WSJ 2Baseline 92.6 89.5Shared 92.8 89.5Guided 92.8 90.7Combined 92.9 90.7Table 5: Performance of the English adapted parsers.For WSJ Part 2, we analyzed the differencesbetween the baseline and the best adapted parser.While there were improvements for all POS tags, themost notable one was in the attachment of conjunc-tions, where we got an increase from 69% to 75%in attachment accuracy, an 18% relative error reduc-tion.
Here we saw a very clear benefit of guidedparsing: since this treebank uses ?Prague-style?
co-ordination annotation (i.e.
the conjunction governsthe conjuncts), it is hard for the parser to handle va-lencies and selectional preferences when there is aconjunction involved.
It has been noted (Nilsson etal., 2007) that this style of annotating coordinationis hard to parse.
Since the WSJ Part 1 parser usesa coordination style that is easier to parse, the WSJPart 2 parser can rely on its judgment.Although conclusions must be very tentative sincewe are testing on just four languages, we can makea few general observations.?
The largest improvements (absolute and rela-tive) all happen in treebanks that are harder toparse.
In particular, Syntag and WSJ Part 2 areharder to parse due to their representation, andto some extent this may be true for Tiger as well?
its learning curve rises more slowly than forTu?Ba-D/Z.
Of course, in some cases (in partic-ular Syntag, but also Tiger) this may partly beexplained by the training set being smaller, butnot for WSJ Part 2.
In these cases, the guidedparsing method seems to be more effective.?
The languages where the shared featuresmethod gives significant improvement for bothtreebanks are German and Italian, where we donot have the situation that one treebank is muchlarger or much easier to parse.?
The combination of the two methods gave sig-nificant improvements in all eight cases, andhad the highest performance in six cases.3.4 The Effect of the Training Set SizeIn order to better understand the differences betweenthe adaptation methods, we analyzed the impact oftraining set size on the improvement given by therespective methods.
Let us refer to the training tree-bank annotated according to the same style as thetest set as the primary treebank, and the other oneas the supporting treebank.
We carried out the ex-periments in this section by varying the number of133101102103104105Training set size (sentences)01020304050Errorreduction(percent)Tiger error reductionSharedGuidedCombined101102103104105Training set size (sentences)01020304050Errorreduction(percent)T Ba-D/Z error reductionSharedGuidedCombinedFigure 2: Error reduction by training set size, German.training sentences in the primary treebank and keep-ing the size of the supporting treebank constant.In order to highlight the differences between thethree adaptation methods, we show error reductionplots in Figures 2, 3, 4, and 5 for German, Swedish,Italian, and English respectively.
For each trainingset size on the x axis, the plot shows the reductionin relative error with respect to the baseline.We note that every single one of the 24 adaptedparsers learns faster than the corresponding baselineparser.
While we saw a number of significant im-provements in ?3.3 when using the full training sets,the relative improvements are much stronger whenthe training sets are small- and medium-sized.These plots illustrate the different properties ofthe two methods.
Using a shared feature represen-tation tends to be very effective when the primarytreebank is small: the error reductions are over 40percent for German and over 25 percent for English.Guided parsing works best for mid-sized sets, andthe relative effectiveness of both methods decreasesas the size of the primary treebank increases.
Again,we see that guided parsing is less effective if theguide uses an annotation style that is hard to parse.101102103104Training set size (sentences)024681012141618Errorreduction(percent)Syntag error reductionSharedGuidedCombined101102103104Training set size (sentences)0510152025Errorreduction(percent)Talbanken error reductionSharedGuidedCombinedFigure 3: Error reduction by training set size, Swedish.100101102103104Training set size (sentences)024681012141618Errorreduction(percent)ISST error reductionSharedGuidedCombined100101102103104Training set size (sentences)051015202530Errorreduction(percent)TUT error reductionSharedGuidedCombinedFigure 4: Error reduction by training set size, Italian.134In particular, for Swedish the Syntag parser nevergives a very large improvement when guiding theTalbanken parser, and this is also true of both Italianparsers.
To a smaller extent, this also holds for En-glish and German: the WSJ Part 2 and Tiger parsersare less useful as guides than their counterparts.The combination method generally performs verywell: in all eight experiments, it outperforms theother two for almost every training set size.
Its per-formance is very close to that of the guided parsingmethod for larger training sets, when the effect ofthe shared features method is less pronounced.101102103104105Training set size (sentences)0510152025303540Errorreduction(percent)WSJ part 1 error reductionSharedGuidedCombined101102103104105Training set size (sentences)0510152025303540Errorreduction(percent)WSJ part 2 error reductionSharedGuidedCombinedFigure 5: Error reduction by training set size, English.4 ConclusionWe have considered the problem of training a de-pendency parser on incompatible treebanks, and westudied two very simple methods for addressing thisproblem, the shared features and guided parsingmethods.
These methods allow us to use more thanone treebank when training dependency parsers.
Weevaluated the methods on eight treebanks in fourlanguages, and had statistically significant improve-ments in all eight cases.
In particular, for Englishwe saw a strong 1.2-point absolute improvement (an11% relative error reduction) in the performance of asemantically oriented parser when trained on the fulltraining set.
For German, we also had very strongresults for the Tiger treebank: a 6% error reduction.For Swedish, the parser trained on the small Syntagtreebank got a boost from a guide parser trained onthe larger Talbanken.
In general, it seems to be eas-ier to improve parsers that use representations thatare harder to parse.For all eight treebanks, both methods achievedlarge improvements for small training set sizes,while the effect gradually diminished as the trainingset size increased.
The shared features method wasthe most effective for very small training sets, whileguided parsing surpassed it when training sets gotlarger.
The combination of the twomethods was alsoeffective, in most cases outperforming both methodson their own.
In particular, when using the full train-ing sets, this was the only method that had statisti-cally significant improvements for all treebanks.While this work used an unlabeled graph-baseddependency parser, our methods generalize naturallyto other parsing approaches, including transition-based dependency parsing.
Labeled parsing withincompatible label sets is easy to implement in theshared features framework by removing the label in-formation from the shared feature representation fs,and similar modifications of fs could be carried outto handle more complex situations such as combinedconstituent and dependency parsing.
Furthermore,the paths used by the feature extractor in the guidedparser can be extended without much effort as well.The models presented here are very simple, and infuture work we would like to explore more com-plex approaches such as quasi-synchronous gram-mars (Smith and Eisner, 2009; Li et al 2012) or au-tomatic treebank transformation (Niu et al 2009).AcknowledgementsI am grateful to the anonymous reviewers, whosefeedback has helped to clarify the description of themethods.
This research was supported by Universityof Gothenburg through its support of the Centre forLanguage Technology and Spra?kbanken.
It has beenpartly funded by the Swedish Research Council un-der grant number 2012-5738.135ReferencesShai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2010.
A theory of learning from differentdomains.
Machine Learning, 2010(79):151?175.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, Bollywood, Boom-boxes and Blenders:Domain adaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 440?447,Prague, Czech Republic.Cristina Bosco and Alberto Lavelli.
2010.
Annota-tion schema oriented validation for dependency pars-ing evaluation.
In Proceedings of the Ninth Workshopon Treebanks and Linguistic Theories (TLT9), Tartu,Estonia.Cristina Bosco, Simonetta Montemagni, AlessandroMazzei, Vincenzo Lombardo, Felice Dell?Orletta,Alessandro Lenci, LeonardoLesmo, GiuseppeAttardi,Maria Simi, Alberto Lavelli, Johan Hall, Jens Nils-son, and Joakim Nivre.
2010.
Comparing the influ-ence of different treebank annotations on dependencyparsing.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), pages 1794?1801, Valletta, Malta.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theory, pages 24?41, Sozopol, Bul-garia.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning (CoNLL-X), pages149?164, New York City, United States.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings theCoNLL Shared Task, pages 957?961, Prague, CzechRepublic.Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28(1):41?75.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 256?263, Prague, Czech Republic.Jan Einarsson.
1976.
Talbankens skrift-spra?kskonkordans.
Department of ScandinavianLanguages, Lund University.Sofia Gustafson-Capkova?
and Britt Hartmann.
2006.Manual of the Stockholm Umea?
Corpus version 2.0.Stockholm University.Richard Hudson.
1984.
Word Grammar.
Blackwell.Jerker Ja?rborg.
1986.
Manual fo?r syntaggning.
De-partment of Linguistic Computation, University ofGothenburg.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for En-glish.
In NODALIDA 2007 Conference Proceedings,pages 105?112, Tartu, Estonia.Richard Johansson and Pierre Nugues.
2008.
The ef-fect of syntactic representation on semantic role label-ing.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 393?400, Manchester, United Kingdom.Richard Johansson.
2013.
Bridging the gap betweentwo Swedish treebanks.
Northern European Journalof Language Technology.
Submitted.Zhenghua Li, Ting Liu, and Wanxiang Che.
2012.
Ex-ploiting multiple treebanks for parsing with quasi-synchronous grammars.
In Proceedings of the 50thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 675?684,Jeju Island, Korea.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 157?166, Honolulu, United States.Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency parsingmodels.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 122?131, Prague, Czech Re-public.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 91?98, Ann Arbor, United States.Simonetta Montemagni and Maria Simi.
2007.
The Ital-ian dependency annotated corpus developed for theCoNLL-2007 shared task.
Technical report, ILC-CNR.Simonetta Montemagni, Francesco Barsotti, Marco Bat-tista, Nicoletta Calzolari, Ornella Corazzari, Alessan-dro Lenci, Antonio Zampolli, Francesca Fanciulli,Maria Massetani, Remo Raffaelli, Roberto Basili,Maria Teresa Pazienza, Dario Saracino, Fabio Zan-zotto, Nadia Mana, Fabio Pianesi, and Rodolfo Del-monte.
2003.
Building the Italian Syntactic?Semantic136Treebank.
In Anne Abeille?, editor, Building and UsingSyntactically Annotated Corpora.
Kluwer, Dordrecht.Jens Nilsson, Johan Hall, and Joakim Nivre.
2005.MAMBA meets TIGER: Reconstructing a Swedishtreebank from antiquity.
In Proceedings of NODAL-IDA Special Session on Treebanks.Jens Nilsson, Joakim Nivre, and Johan Hall.
2007.Generalizing tree transformations for inductive depen-dency parsing.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 968?975, Prague, Czech Republic.Zheng-Yu Niu, Haifeng Wang, and Hua Wu.
2009.
Ex-ploiting heterogeneous treebanks for parsing.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 46?54, Suntec, Singapore.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of ACL-08: HLT, pages 950?958,Columbus, United States.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on depen-dency parsing.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL 2007, pages 915?932,Prague, Czech Republic.Joakim Nivre.
2002.
What kinds of trees growin Swedish soil?
A comparison of four annota-tion schemes for Swedish.
In Proceedings of theFirst Workshop on Treebanks and Linguistic Theories(TLT2002), Sozopol, Bulgaria.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of Interna-tional Conference on New Methods in Language Pro-cessing, Manchester, United Kingdom.David A. Smith and Jason Eisner.
2009.
Parser adapta-tion and projection with quasi-synchronous grammarfeatures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 822?831, Suntec, Singapore.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and JoakimNivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
InCoNLL 2008: Proceedings ofthe Twelfth Conference on Natural Language Learn-ing, pages 159?177, Manchester, United Kingdom.Heike Telljohann, Erhard Hinrichs, and Sandra Kbler.2004.
The Tu?ba-D/Z treebank: Annotating Germanwith a context-free backbone.
In In Proceedings ofthe Fourth International Conference on Language Re-sources and Evaluation (LREC 2004), pages 2229?2235.137
