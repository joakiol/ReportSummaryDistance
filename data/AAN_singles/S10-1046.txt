Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 210?213,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsTUD: semantic relatedness for relation classificationGy?orgy Szarvas?and Iryna GurevychUbiquitous Knowledge Processing (UKP) LabComputer Science DepartmentTechnische Universit?at DarmstadtHochschulstra?e 10., D-64289 Darmstadt, Germanyhttp://www.ukp.tu-darmstadt.de/AbstractIn this paper, we describe the system sub-mitted by the team TUD to Task 8 atSemEval 2010.
The challenge focused onthe identification of semantic relations be-tween pairs of nominals in sentences col-lected from the web.
We applied max-imum entropy classification using bothlexical and syntactic features to describethe nominals and their context.
In addi-tion, we experimented with features de-scribing the semantic relatedness (SR) be-tween the target nominals and a set of cluewords characteristic to the relations.
Ourbest submission with SR features achieved69.23% macro-averaged F-measure, pro-viding 8.73% improvement over our base-line system.
Thus, we think SR can serveas a natural way to incorporate externalknowledge to relation classification.1 IntroductionAutomatic extraction of typed semantic relationsbetween sentence constituents is an important steptowards deep semantic analysis and understand-ing the semantic content of natural language texts.Identification of relations between a nominal andthe main verb, and between pairs of nominals areimportant steps for the extraction of structured se-mantic information from text, and can benefit vari-ous applications ranging from Information Extrac-tion and Information Retrieval to Machine Trans-lation or Question Answering.The Multi-Way Classification of Semantic Re-lations Between Pairs of Nominals challenge(Hendrickx et al, 2010) focused on the identi-fication of specific relation types between nomi-nals (nouns or base noun phrases) in natural lan-guage sentences collected from the web.
The main?On leave from the Research Group on Artificial Intelli-gence of the Hungarian Academy of Sciencestask of the challenge was to identify and clas-sify instances of 9 abstract semantic relations be-tween noun phrases, i.e.
Cause-Effect, Instrument-Agency, Product-Producer, Content-Container,Entity-Origin, Entity-Destination, Component-Whole, Member-Collection, Message-Topic.
Thatis, given two nominals (e1 and e2) in a sentence,systems had to decide whether relation(e1,e2), re-lation(e2,e1) holds for one of the relation types orthe nominals?
relation is other (falls to a categorynot listed above or they are unrelated).
In thissense, the challenge was an important pilot tasktowards large scale semantic processing of text.In this paper, we describe the system we sub-mitted to Semeval 2010, Task 8.
We applied max-imum entropy classification to the problem usingboth lexical and contextual features to describethe nominals themselves and their context (i.e.the sentence).
In addition, we experimented withfeatures exploiting the strength of association be-tween the target nominals and a predefined set ofclue words characteristic to the nine relation types.In order to measure the semantic relatedness (SR)of targets and clues, we used the Explicit Seman-tic Analysis (Gabrilovich and Markovitch, 2007)SR measure (based on Wikipedia, Wiktionary andWordNet).
Our best submission, benefiting fromSR features, achieved 69.23% macro-averaged F-measure for the 9 relation types used.
Providing8.73% improvement over our baseline system, wefound the SR-based features to be beneficial forthe classification of semantic relations.2 Experimental setup2.1 Feature set and selectionFeature set In our system, we used both lexical(1-3) and contextual features (4-8) to describe thenominals and their context (i.e.
the sentence).
Ad-ditionally, we experimented with a set of features(9) that exploit the co-occurrence statistics of the210nominals and a set of clue words chosen manu-ally, examining the relation definitions and exam-ples provided by the organizers.
The clues char-acterize the relations addressed in the task (e.g.cargo, goods, content, box, bottle characterize theContent-Container relation)1.
Each feature typewas distinguished from the others using a prefix.All but the semantic relatedness features we usedwere binary, denoting whether a specific word,lemma, POS tag, etc.
is found in the example sen-tence, or not.
SR features were real valued, scaledto [0, 1] for each clue word separately (on train,and the same scaling factores were applied on thetest data).
The feature types used:1.
Token: word unigrams in the sentence in theirinflected form.
2.
Lemma: word uni- and bigramsin the sentence in their lemmatized form.
3.
Tar-get Nouns: the syntactic head words of the targetnouns.
4.
POS: the part of speech uni- and bi-and trigrams in the sentence.
5.
Between POS: thepart of speech sequence between the target nouns.6.
Dependency Path: the dependency path (syn-tactic relations and directions) between the targetnouns.
The whole path constituted a single fea-ture.
7.
Target Distance: the distance betweenthe target nouns (in tokens).
8.
Sentence Length:the length of the sentence (in tokens).
9.
Seman-tic Relatedness: the semantic relatedness scoresmeasuring the strength of association between thetarget nominals and the set of clue words we col-lected.
In order to measure the semantic related-ness (SR) of targets and clues, we used the ExplicitSemantic Analysis (ESA) SR measure.Feature selection In order to discard uninforma-tive features automatically, we performed featureselection on the binary features.
We kept featuresthat satisfied the following three conditions:freq(x) > 3 (1)p = argmaxyP (y|x) > t1(2)p5?
freq(x) > t2(3)where freq(x) denotes the frequency of feature xobserved in the training dataset, y denotes a classlabel, p denotes the highest posterior probability(for feature x) over the nine relations (undirected)and the other class.
Finally, t1, t2are filteringthresholds chosen arbitrarily.
We used t1= 0.25for all features but the dependency path, where we1The clue list is available at:http://www.ukp.tu-darmstadt.de/research/data/relation-classification/relation type size c4.5 SMO maxentcause-effect 1003 75.2% 78.9% 78.2%component-whole 941 46.7% 53.0% 54.7%content-container 540 72.9% 78.1% 75.1%entity-destination 845 77.6% 82.3% 82.0%entity-origin 716 61.8% 65.0% 68.7%instrument-agency 534 40.7% 42.7% 47.6%member-collection 690 68.2% 72.1% 75.3%message-topic 634 41.3% 47.3% 56.4%product-producer 717 43.8% 50.3% 53.4%macro AVG F1 6590 58.7% 63.3% 65.7%Table 1: Performance of different learning meth-ods on train (10-fold).used t1= 0.2.
We set the thresold t2to 1.9 forlexical features (i.e.
token and lemma features),to 0.3 for dependency path features and to 0.9 forall other features.
All parameters for the featureselection process were chosen manually (cross-validating the parameters was omitted due to lackof time during the challenge development period).The higher t2value for lexical features was moti-vated by the aim to avoid overfitting, and the lowerthresholds for dependency-based features by thehypothesis that these can be most efficient to deter-mine the direction of relationships (c.f.
we disre-garded direction during feature selection).
As thenumeric SR features were all bound to clue wordsselected specifically for the task, we did not per-form any feature selection for that feature type.2.2 Learning modelsWe compared three learning algorithms, usingthe baseline feature types (1-8), namely a C4.5decision tree learner, a support vector classifier(SMO), and a maximum entropy (logistic regres-sion) classifier, all implemented in the Weka pack-age (Hall et al, 2009).
We trained the SMO modelwith polynomial kernel of degree 2, fitting logisticmodels to the output to get valid probability esti-mates and the C4.5 model with pruning confidencefactor set to 0.33.
All other parameters were set totheir default values as defined in Weka.
We foundthe maxent model to perform best in 10-fold crossvalidation on the training set (see Table 1).
Thus,we used maxent in our submissions.3 ResultsWe submitted 4 runs to the challenge.
Table2 shows the per-class and the macro average F-measures of the 9 relation classes and the accu-racy over all classes including other, on the train(10-fold) and the test sets (official evaluation):211Train Testrelation type Base WP cSR cSR-t Base WP cSR cSR-tcause-effect 78.17% 78.25% 79.42% 79.10% 80.69% 81.90% 83.76% 83.38%component-whole 54.68% 58.71% 60.18% 60.79% 50.52% 57.90% 61.67% 62.15%content-container 75.09% 77.55% 78.26% 78.11% 75.27% 78.96% 78.33% 78.87%entity-destination 81.99% 82.97% 83.12% 82.90% 77.59% 82.86% 81.54% 81.12%entity-origin 68.74% 70.39% 71.14% 71.18% 67.08% 72.05% 71.03% 70.36%instrument-agency 47.59% 56.71% 59.60% 59.80% 31.09% 44.06% 46.78% 46.91%member-collection 75.27% 79.43% 80.71% 80.89% 66.37% 71.24% 72.65% 72.65%message-topic 56.40% 62.68% 64.77% 65.15% 49.88% 65.06% 68.15% 69.83%product-producer 53.36% 57.98% 59.97% 60.70% 46.04% 57.94% 56.00% 57.85%macro AVG F1 65.70% 69.40% 70.80% 70.96% 60.50% 68.00% 68.88% 69.23%accuracy (incl.
other) 62.10% 65.42% 66.83% 67.12% 56.13% 63.49% 64.63% 65.37%Table 2: Performance of 4 submissions on train (10-fold) and test.Baseline (Base) As our baseline system, we usedthe information extracted from the sentence itself(i.e.
lexical and contextual features, types 1-8).Wikipedia (WP) As a first extension, we addedSR features (9) exploiting term co-occurrence in-formation, using the ESA model with Wikipedia.Combined Semantic Relatedness (cSR) Second,we replaced the ESA measure with a combinedmeasure developed by us, exploiting term co-occurrence not only in Wikipedia, but also inWordNet and Wiktionary glosses.
We found thismeasure to perform better than the Wikipedia-based ESA in earlier experiments.cSR threshold (cSR-t) We submitted the predic-tions of the cSR system, with less emphasis on theother class: we predicted other label only whenthe following held for the posteriors predicted bycSR:argmaxyP (y|x)p(other)< 0.7.
The threshold 0.7 waschosen based on the training dataset.First, the SR features improved the performanceof our system by a wide marging (see Table2).
The difference in performance is even moreprominent on the Test dataset, which suggests thatthese features efficiently incorporated useful ex-ternal evidence on the relation between the nomi-nals and this not just improved the accuracy of thesystem, but also helped to avoid overfitting.
Thuswe conclude that the SR features with the encodedexternal knowledge helped the maxent model tolearn a hypothesis that clearly generalized better.Second, we notice that the combined SR mea-sure proved to be more useful than the standardESA measure (Gabrilovich and Markovitch, 2007)improving the performance by approximately 1percent over ESA, both in terms of macro aver-aged F-measure and overall accuracy.
This con-firms our hypothesis that the combined measure ismore robust than ESA with just Wikipedia.prediction category cSR cSR-ttrue positive relation (TP) 1555 1612true positive other (TN) 201 164wrong relation type (FP & FN) 291 341wrong relation direction (FP & FN) 50 58relation classified as other (FN) 367 252other classified as relation (FP) 253 290total 2717 2717Table 3: Prediction error statistics.3.1 Error AnalysisTable 3 shows the breakdown of system predic-tions to different categories, and their contributionto the official ranking as true/false positives andnegatives.
The submission that manipulated thedecision threshold for the other class improvedthe overall performance by a small margin.
Thisfact, and Table 3 confirm that our approach hadmajor difficulties in correctly discriminating the 9relation categories from other.
Since this class isan umbrella class for unrelated nominals and thenumerous semantic relations not considered in thechallenge, it proved to be extremely difficult to ac-curately characterize this class.
On the other hand,the confusion of the 9 specified relations (betweeneach other) and directionality were less prominenterror types.
The most frequent cross-relationconfusion types were the misclassificationof Component-Whole as Instrument-Agencyand Member-Collection; Content-Containeras Component-Whole; Instrument-Agency asProduct-Producer and vice versa.
Interestingly,Component-Whole and Cause-Effect relationswere the most typical sources for wrong directionerrors.
Lowering the decision threshold for otherin our system naturally resulted in more truepositive relation classifications, but unfortunatelynot only raised the number of other instancesfalsely classified as being one of the valuable re-212lations, but also introduced several wrong relationclassification errors (see Table 3).
That is whythis step resulted only in marginal improvement.4 Conclusions & Future WorkIn this paper, we presented our system submittedto the Multi-Way Classification of Semantic Re-lations Between Pairs of Nominals challenge atSemEval 2010.
We submitted 4 different systemruns.
Our first submission was a baseline system(Base) exploiting lexical and contextual informa-tion collected solely from the sentence to be classi-fied.
A second run (WP) complemented this base-line configuration with a set of features that usedExplicit Semantic Analysis (Wikipedia) to modelthe SR of the nominals to be classified and a setof clue words characteristic of the relations usedin the challenge.
Our third run (cSR) used a com-bined semantic relatedness measure that exploitsmultiple lexical semantic resources (Wikipedia,Wiktionary and WordNet) to provide more reliablerelatedness estimates.
Our final run (cSR-t) ex-ploited that our system in general was inaccuratein predicting instances of the other class.
Thus,it used the same predictions as cSR, but favoredthe prediction of one of the 9 specified classes in-stead of other, when a comparably high posteriorfor such a class was predicted by the system.Our approach is fairly simple, in the sense that itused mostly just local information collected fromthe sentence.
It is clear though that encoding asmuch general world knowledge to the representa-tion as possible is crucial for efficient classifica-tion of semantic relations.
In the light of the abovefact, the results we obtained are reasonable.As the main goal of our study, we attemptedto use semantic relatedness features that exploittexts in an external knowledge source (Wikipedia,Wiktionary or WordNet in our case) to incorpo-rate some world knowledge in the form of term co-occurrence scores.
We found that our SR featuressignificantly contribute to system performance.Thus, we think this kind of information is usefulin general for relation classification.
The experi-mental results showed that our combined SR mea-sure performed better than the standard ESA usingWikipedia.
This confirms our hypothesis that ex-ploiting multiple resources for modeling term re-latedness is beneficial in general.Obviously, our system leaves much space forimprovement ?
the feature selection parametersand the clue word set for the SR features werechosen manually, without any cross-validation (onthe training set), due to lack of time.
One of theparticipating teams used an SVM-based systemand gained a lot from manipulating the decisionthresholds.
Thus, despite our preliminary results,it is also an interesting option to use SVMs.In general, we think that more features areneeded to achieve significantly better performancethan we reported here.
Top performing systemsin the challenge typically exploited web frequencyinformation (n-gram data) and manually encodedrelations from an ontology (mainly WordNet).Thus, future work is to incorporate such features.We demonstrated that SR features are helpful tomove away from lexicalized systems using token-or lemma-based features.
Probably the same holdsfor web-based and ontology-based features exten-sively used by top performing systems.
This sug-gests that experimenting with all these to see iftheir value is complementary is an especially in-teresting piece of future work.AcknowledgmentsThis work was supported by the German Ministryof Education and Research (BMBF) under grant?Semantics- and Emotion-Based ConversationManagement in Customer Support (SIGMUND)?,No.
01ISO8042D, and by the Volkswagen Foun-dation as part of the Lichtenberg-ProfessorshipProgram under the grant No.
I/82806.ReferencesEvgeniy Gabrilovich and Shaul Markovitch.
2007.Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.
In Proceedingsof The 20th International Joint Conference on Arti-ficial Intelligence, pages 1606?1611.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Up-date.
SIGKDD Explorations, 11(1):10?18.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,Preslav Nakov, Diarmuid?O S?eaghdha, SebastianPad?o, Marco Pennacchiotti, Lorenza Romano, andStan Szpakowicz.
2010.
Semeval-2010 task 8:Multi-way classification of semantic relations be-tween pairs of nominals.
In Proceedings of the 5thSIGLEX Workshop on Semantic Evaluation.213
