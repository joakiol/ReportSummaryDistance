Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 98?106,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLabeling Documents with Timestamps:Learning from their Time ExpressionsNathanael ChambersDepartment of Computer ScienceUnited States Naval Academynchamber@usna.eduAbstractTemporal reasoners for document understand-ing typically assume that a document?s cre-ation date is known.
Algorithms to groundrelative time expressions and order events of-ten rely on this timestamp to assist the learner.Unfortunately, the timestamp is not alwaysknown, particularly on the Web.
This pa-per addresses the task of automatic documenttimestamping, presenting two new models thatincorporate rich linguistic features about time.The first is a discriminative classifier withnew features extracted from the text?s timeexpressions (e.g., ?since 1999?).
This modelalone improves on previous generative mod-els by 77%.
The second model learns prob-abilistic constraints between time expressionsand the unknown document time.
Imposingthese learned constraints on the discriminativemodel further improves its accuracy.
Finally,we present a new experiment design that facil-itates easier comparison by future work.1 IntroductionThis paper addresses a relatively new task inthe NLP community: automatic document dating.Given a document with unknown origins, what char-acteristics of its text indicate the year in which thedocument was written?
This paper proposes a learn-ing approach that builds constraints from a docu-ment?s use of time expressions, and combines themwith a new discriminative classifier that greatly im-proves previous work.The temporal reasoning community has long de-pended on document timestamps to ground rela-tive time expressions and events (Mani and Wilson,2000; Llido?
et al, 2001).
For instance, considerthe following passage from the TimeBank corpus(Pustejovsky et al, 2003):And while there was no profit this year fromdiscontinued operations, last year they con-tributed 34 million, before tax.Reconstructing the timeline of events from this doc-ument requires extensive temporal knowledge, mostnotably, the document?s creation date to ground itsrelative expressions (e.g., this year = 2012).
Notonly did the latest TempEval competitions (Verha-gen et al, 2007; Verhagen et al, 2009) includetasks to link events to the (known) document cre-ation time, but state-of-the-art event-event orderingalgorithms also rely on these timestamps (Chambersand Jurafsky, 2008; Yoshikawa et al, 2009).
Thisknowledge is assumed to be available, but unfortu-nately this is not often the case, particularly on theWeb.Document timestamps are growing in importanceto the information retrieval (IR) and managementcommunities as well.
Several IR applications de-pend on knowledge of when documents were posted,such as computing document relevance (Li andCroft, 2003; Dakka et al, 2008) and labeling searchqueries with temporal profiles (Diaz and Jones,2004; Zhang et al, 2009).
Dating documents is sim-ilarly important to processing historical and heritagecollections of text.
Some of the early work that moti-vates this paper arose from the goal of automaticallygrounding documents in their historical contexts (deJong et al, 2005; Kanhabua and Norvag, 2008; Ku-mar et al, 2011).
This paper builds on their work98by incorporating more linguistic knowledge and ex-plicit reasoning into the learner.The first part of this paper describes a novel learn-ing approach to document dating, presenting a dis-criminative model and rich linguistic features thathave not been applied to document dating.
Further,we introduce new features specific to absolute timeexpressions.
Our model outperforms the generativemodels of previous work by 77%.The second half of this paper describes a novellearning algorithm that orders time expressionsagainst the unknown timestamp.
For instance, thephrase the second quarter of 1999 might be labeledas being before the timestamp.
These labels imposeconstraints on the possible timestamp and narrowdown its range of valid dates.
We combine theseconstraints with our discriminative learner and seeanother relative improvement in accuracy by 9%.2 Previous WorkMost work on dating documents has come from theIR and knowledge management communities inter-ested in dating documents with unknown origins.de Jong et al (2005) was among the first to auto-matically label documents with dates.
They learnedunigram language models (LMs) for specific timeperiods and scored articles with log-likelihood ra-tio scores.
Kanhabua and Norvag (2008; 2009) ex-tended this approach with the same model, but ex-panded its unigrams with POS tags, collocations,and tf-idf scores.
They also integrated search engineresults as features, but did not see an improvement.Both works evaluated on the news genre.Recent work by Kumar et al (2011) focused ondating Gutenberg short stories.
As above, theylearned unigram LMs, but instead measured the KL-divergence between a document and a time period?sLM.
Our proposed models differ from this workby applying rich linguistic features, discriminativemodels, and by focusing on how time expressionsimprove accuracy.
We also study the news genre.The only work we are aware of within the NLPcommunity is that of Dalli and Wilks (2006).
Theycomputed probability distributions over differenttime periods (e.g., months and years) for each ob-served token.
The work is similar to the above IRwork in its bag of words approach to classification.They focused on finding words that show periodicspikes (defined by the word?s standard deviation inits distribution over time), weighted with inversedocument frequency scores.
They evaluated on asubset of the Gigaword Corpus (Graff, 2002).The experimental setup in the above work (exceptKumar et al who focus on fiction) all train on newsarticles from a particular time period, and test on ar-ticles in the same time period.
This leads to possi-ble overlap of training and testing data, particularlysince news is often reprinted across agencies thesame day.
In fact, one of the systems in Kanhabuaand Norvag (2008) simply searches for one trainingdocument that best matches a test document, and as-signs its timestamp.
We intentionally deviate fromthis experimental design and instead create tempo-rally disjoint train/test sets (see Section 5).Finally, we extend this previous work by focusingon aspects of language not yet addressed for docu-ment dating: linguistic structure and absolute timeexpressions.
The majority of articles in our datasetcontain time expressions (e.g., the year 1998), yetthese have not been incorporated into the models de-spite their obvious connection to the article?s times-tamp.
This paper first describes how to includetime expressions as traditional features, and thendescribes a more sophisticated temporal reasoningcomponent that naturally fits into our classifier.3 Timestamp ClassifiersLabeling documents with timestamps is similar totopic classification, but instead of choosing fromtopics, we choose the most likely year (or othergranularity) in which it was written.
We thus beginwith a bag-of-words approach, reproducing the gen-erative model used by both de Jong (2005) and Kan-habua and Norvag (2008; 2009).
The subsequentsections then introduce our novel classifiers andtemporal reasoners to compare against this model.3.1 Language ModelsThe model of de Jong et al (2005) uses the nor-malized log-likelihood ratio (NLLR) to score doc-uments.
It weights tokens by the ratio of their prob-ability in a specific year to their probability over theentire corpus.
The model thus requires an LM foreach year and an LM for the entire corpus:99NLLR(D,Y ) =?w?DP (w|D) ?
log(P (w|Y )P (w|C)) (1)where D is the target document, Y is the time span(e.g., a year), and C is the distribution of words inthe corpus across all years.
A document is labeledwith the year that satisfies argmaxYNLLR(D,Y ).They adapted this model from earlier work in theIR community (Kraaij, 2004).
We apply Dirichlet-smoothing to the language models (as in de Jong etal.
), although the exact choice of ?
did not signifi-cantly alter the results, most likely due to the largesize of our training corpus.
Kanhabua and Norvagadded an entropy factor to the summation, but wedid not see an improvement in our experiments.The unigrams w are lowercased tokens.
We willrefer to this de Jong et al model as the UnigramNLLR.
Follow-up work by Kanhabua and Norvag(2008) applied two filtering techniques to the uni-grams in the model:1.
Word Classes: include only nouns, verbs, andadjectives as labeled by a POS tagger2.
IDF Filter: include only the top-ranked termsby tf-idf scoreWe also tested with these filters, choosing a cut-off for the top-ranked terms that optimized perfor-mance on our development data.
We also stemmedthe words as Kanhabua and Norvag suggest.
Thismodel is the Filtered NLLR.Kanhabua and Norvag also explored what theytermed collocation features, but lacking details onhow collocations were included (or learned), wecould not reproduce this for comparison.
How-ever, we instead propose using NER labels to ex-tract what may have counted as collocations in theirdata.
Named entities are important to document dat-ing due to the nature of people and places coming inand out of the news at precise moments in time.
Wecompare the NER features against the Unigram andFiltered NLLR models in our final experiments.3.2 Discriminative ModelsIn addition to reproducing the models from previouswork, we also trained a new discriminative versionwith the same features.
We used a MaxEnt modeland evaluated with the same filtering methods basedon POS tags and tf-idf scores.
The model performedbest on the development data without any filteringor stemming.
The final results (Section 6) only usethe lowercased unigrams.
Ultimately, this MaxEntmodel vastly outperforms these NLLR models.3.3 Models with Time ExpressionsThe above language modeling and MaxEnt ap-proaches are token-based classifiers that one couldapply to any topic classification domain.
Barringother knowledge, the learners solely rely on the ob-served frequencies of unigrams in order to decidewhich class is most likely.
However, document dat-ing is not just a simple topic classification applica-tion, but rather relates to temporal phenomena thatis often explicitly described in the text itself.
Lan-guage contains words and phrases that discuss thevery time periods we aim to recover.
These expres-sions should be better incorporated into the learner.3.3.1 MotivationLet the following snippet serve as a text examplewith an ambiguous creation time:Then there?s the fund-raiser at the AmericanMuseum of Natural History, which plans towelcome about 1,500 guests paying $1,000 to$5,000.
Their tickets will entitle them to a pre-view of...the new Hayden Planetarium.Without extremely detailed knowledge about theAmerican Museum of Natural History, the eventsdiscussed here are difficult to place in time, let alnewhen the author reported it.
However, time expres-sions are sometimes included, and the last sentencein the original text contains a helpful relative clause:Their tickets will entitle them to a previewof...the new Hayden Planetarium, which doesnot officially open until February 2000.This one clause is more valuable than the rest ofthe document, allowing us to infer that the docu-ment?s timestamp is before February, 2000.
An ed-ucated guess might surmise the article appeared inthe year prior, 1999, which is the correct year.
Atthe very least, this clause should eliminate all yearsafter 2000 from consideration.
Previous work ondocument dating does not integrate this informationexcept to include the unigram ?2000?
in the model.100This paper discusses two complementary ways tolearn and reason about this information.
The firstis to simply add richer time-based features into themodel.
The second is to build separate learners thatcan assign probabilities to entire ranges of dates,such as all years following 2000 in the exampleabove.
We begin with the feature-based model.3.3.2 Time FeaturesTo our knowledge, the following time featureshave not been used in a document dating setting.We use the freely available Stanford Parser and NERsystem1 to generate the syntactic interpretation forthese features.
We then train a MaxEnt classifier andcompare against previous work.Typed Dependency: The most basic time feature isincluding governors of year mentions and the rela-tion between them.
This covers important contextsthat determine the semantics of the time frame, likeprepositions.
For example, consider the followingcontext for the mention 1997:Torre, who watched the Kansas City Royalsbeat the Yankees, 13-6, on Friday for the firsttime since 1997.The resulting feature is ?since pobj 1997?.Typed Dependency POS: Similar to Typed Depen-dency, this feature uses POS tags of the dependencyrelation?s governor.
The feature from the previousexample is now ?PP pobj 1997?.
This generalizesthe features to capture time expressions with prepo-sitions, as noun modifiers, or other constructs.Verb Tense: An important syntactic feature for tem-poral positioning is the tense of the verb that domi-nates the time expression.
A past tense verb situatesthe phrase in 2003 differently than one in the future.We traverse the sentence?s parse tree until a gover-nor with a VB* tag is found, and determine its tensethrough hand constructed rules based on the struc-ture of the parent VP.
The verb tense feature takes avalue of past, present, future, or undetermined.Verb Path: The verb path feature is the dependencypath from the nearest verb to the year expression.The following snippet will include the feature, ?ex-pected prep in pobj 2002?.1http://nlp.stanford.edu/softwareFinance Article from Jan. 2002Text Snippet Relation to 2002...started a hedge fund before themarket peaked in 2000.beforeThe peak in economic activity wasthe 4th quarter of 1999.before...might have difficulty in the latterpart of 2002.simultaneousFigure 1: Three year mentions and their relation to thedocument creation year.
Relations can be correctly iden-tified for training using known document timestamps.Supervising them is Vice President Hu Jintao,who appears to be Jiang?s favored successor ifhe retires from leadership as expected in 2002.Named Entities: Although not directly related totime expressions, we also include n-grams of tokensthat are labeled by an NER system using Person, Or-ganization, or Location.
People and places are oftendiscussed during specific time periods, particularlyin the news genre.
Collecting named entity mentionswill differentiate between an article discussing a billand one discussing the US President, Bill Clinton.We extract NER features as sequences of uninter-rupted tokens labeled with the same NER tag, ignor-ing unigrams (since unigrams are already includedin the base model).
Using the Verb Path exampleabove, the bigram feature Hu Jintao is included.4 Learning Time ConstraintsThis section departs from the above document clas-sifiers and instead classifies individual emphyearmentions.
The goal is to automatically learn tem-poral constraints on the document?s timestamp.Instead of predicting a single year for a document,a temporal constraint predicts a range of years.
Eachtime mention, such as ?not since 2009?, is a con-straint representing its relation to the document?stimestamp.
For example, the mentioned year ?2009?must occur before the year of document creation.This section builds a classifier to label time mentionswith their relations (e.g., before, after, or simultane-ous with the document?s timestamp), enabling thesementions to constrain the document classifiers de-scribed above.
Figure 1 gives an example of timementions and the desired labels we wish to learn.To better motivate the need for constraints, let1011995 1996 1997 1998 1999 2000 2001 2004 200500.050.10.150.2ProbabilityYear ClassFigure 2: Distribution over years for a single documentas output by a MaxEnt classifier.Figure 2 illustrate a typical distribution output by adocument classifier for a training document.
Twoof the years appear likely (1999 and 2001), how-ever, the document contains a time expression thatseems to impose a strict constraint that should elim-inate 2001 from consideration:Their tickets will entitle them to a previewof...the new Hayden Planetarium, which doesnot officially open until February 2000.The clause until February 2000 in a present tensecontext may not definitively identify the document?stimestamp (1999 is a good guess), but as discussedearlier, it should remove all future years beyond2000 from consideration.
We thus want to imposea constraint based on this phrase that says, loosely,?this document was likely written before 2000?.The document classifiers described in previoussections cannot capture such ordering information.Our new time features in Section 3.3.2 add richertime information (such as until pobj 2000 and openprep until pobj 2000), but they compete with manyother features that can mislead the final classifica-tion.
An independent constraint learner may pushthe document classifier in the right direction.4.1 Constraint TypesWe learn several types of constraints between eachyear mention and the document?s timestamp.
Yearmentions are defined as tokens with exactly fourdigits, numerically between 1900 and 2100.
Let Tbe the document timestamp?s year, and M the yearmention.
We define three core relations:1.
Before Timestamp: M < T2.
After Timestamp: M > T3.
Same as Timestamp: M == TWe also experiment with 7 fine-grained relations:1.
One year Before Timestamp: M == T ?
12.
Two years Before Timestamp: M == T ?
23.
Three+ years Before Timestamp: M < T ?
24.
One year After Timestamp: M == T + 15.
Two years After Timestamp: M == T + 26.
Three+ years After Timestamp: M > T + 27.
Same Year and Timestamp: M == TObviously the more fine-grained a relation, the bet-ter it can inform a classifier.
We experiment withthese two granularities to compare performance.The learning process is a typical training envi-ronment where year mentions are treated as labeledtraining examples.
Labels for year mentions areautomatically computed by comparing the actualtimestamp of the training document (all documentsin Gigaword have dates) with the integer value ofthe year token.
For example, a document written in1997 might contain the phrase, ?in the year 2000?.The year token (2000) is thus three+ years after thetimestamp (1997).
We use this relation for the yearmention as a labeled training example.Ultimately, we want to use similar syntactic con-structs in training so that ?in the year 2000?
and ?inthe year 2003?
mutually inform each other.
We thuscompute the label for each time expression, and re-place the integer year with the generic YEAR tokento generalize mentions.
The text for this example be-comes ?in the year YEAR?
(labeled as three+ yearsafter).
We train a MaxEnt model on each year men-tion, to be described next.
Table 2 gives the overallcounts for the core relations in our training data.
Thevast majority of year mentions are references to thefuture (e.g.
after the timestamp).4.2 Constraint LearnerThe features we use to classify year mentions aregiven in Table 1.
The same time features in the docu-ment classifier of Section 3.3.2 are included, as wellas several others specific to this constraint task.We use a MaxEnt classifier trained on the individ-ual year mentions.
Documents often contain multi-ple (and different) year mentions; all are included intraining and testing.
This classifier labels mentionswith relations, but in order to influence the documentclassifier, we need to map the relations to individual102Time Constraint FeaturesTyped Dep.
Same as Section 3.3.2Verb Tense Same as Section 3.3.2Verb Path Same as Section 3.3.2Decade The decade of the year mentionBag of Words Unigrams in the year?s sentencen-gram The 4-gram and 3-gram that endwith the yearn-gram POS The 4-gram and 3-gram of POS tagsthat end with the yearTable 1: Features used to classify year expressions.Constraint CountAfter Timestamp 1,203,010Before Timestamp 168,185Same as Timestamp 141,201Table 2: Training size of year mentions (and their relationto the document timestamp) in Gigaword?s NYT section.year predictions.
Let Td be the set of mentions indocument d. We represent a MaxEnt classifier byPY (R|t) for a time mention t ?
Td and possible re-lations R. We map this distribution over relations toa distribution over years by defining Pyear(Y |d):Pyear(y|d) =1Z(Td)?t?TdPY (rel(val(t)?
y)|t) (2)rel(x) =??
?before if x < 0after if x > 0simultaneous otherwise(3)where val(t) is the integer year of the year mentionandZ(Td) is the partition function.
The rel(val(t)?y) function simply determines if the year mention t(e.g., 2003) is before, after, or overlaps the year weare predicting for the document?s unknown times-tamp y.
We use a similar function for the seven fine-grained relations.
Figure 3 visually illustrates howPyear(y|d) is constructed from three year mentions.4.3 Joint ClassifierFinally, given the document classifiers of Section 3and the constraint classifier just defined in Section 4,we create a joint model combining the two with thefollowing linear interpolation:P (y|d) = ?Pdoc(y|d) + (1?
?
)Pyear(y|d) (4)where y is a year, and d is the document.
?
was setto 0.35 by maximizing accuracy on the dev set.
See0 0.2 0.4 0.6 0.8 10.5150.520.5250.530.5350.540.545Lambda ValueAccuracyLambda Parameter AccuracyFigure 4: Development set accuracy and ?
values.Figure 4.
This optimal ?
= .35 weights the con-straint classifier higher than the document classifier.5 DatasetsThis paper uses the New York Times section of theGigaword Corpus (Graff, 2002) for evaluation.
Mostprevious work on document dating evaluates on thenews genre, so we maintain the pattern for consis-tency.
Unfortunately, we cannot compare to theseprevious experiments because of differing evalua-tion setups.
Dalli and Wilks (2006) is most similar intheir use of Gigaword, but they chose a random setof documents that cannot be reproduced.
We insteaddefine specific segments of the corpus for evaluation.The main goal for this experiment setup was to es-tablish specific training, development, and test sets.One of the potential difficulties in testing with newsarticles is that the same story is often reprinted withvery minimal (or no) changes.
Over 10% of the doc-uments in the New York Times section of the Giga-word Corpus are exact or approximate duplicates ofanother document in the corpus2.
A training set fordocument dating must not include duplicates fromthe test set.We adopt the intuition behind the experimen-tal setup used in other NLP domains, like parsing,where the entire test set is from a contiguous sec-tion of the corpus (as opposed to randomly selectedexamples across the corpus).
As the parsing com-munity trains on sections 2-21 of the Penn Treebank(Marcus et al, 1993) and tests on section 23, we cre-ate Gigaword sections by isolating specific months.2Approximate duplicate is defined as an article whose firsttwo sentences exactly match the first two of another article.Only the second matched document is counted as a duplicate.103Year Distributions for Three Time Expressions97 98 99 00 01 02 03 04 0596PY(y | "peaked in 2000")PY(y | "was the quarter of 1999")PY(y | "will have difficulty in part of 2003")Final Distribution  -  Pyear(y|d)0.20.00.20.00.20.00.20.0Figure 3: Three year mentions in a document and the distributions output by the learner.
The document is from 2002.The dots indicate the before, same, and after relation probabilities.
The combination of three constraints results in afinal distribution that gives the years 2001 and 2002 the highest probability.
This distribution can help a documentclassifier make a more informed final decision.Training Jan-May and Sep-DecDevelopment JulyTesting June and AugustIn other words, the development set includes docu-ments from July 1995, July 1996, July 1997, etc.
Wechose the dev/test sets to be in the middle of the yearso that the training set includes documents on bothtemporal sides of the test articles.
We include years1995-2001 and 2004-2006, but skip 2002 and 2003due to their abnormally small size compared to theother years.Finally, we experiment in a balanced data set-ting, training and testing on the same numberof documents from each year.
The test set in-cludes 11,300 documents in each year (monthsJune and August) for a total of 113,000 test doc-uments.
The development set includes 7,300from July of each year.
Training includes ap-proximately 75,000 documents in each year withsome years slightly less than 75,000 due to theirsmaller size in the corpus.
The total number oftraining documents for the 10 evaluated years is725,468.
The full list of documents is online atwww.usna.edu/Users/cs/nchamber/data/timestamp.6 Experiments and ResultsWe experiment on the Gigaword corpus as describedin Section 5.
Documents are tokenized and parsedwith the Stanford Parser.
The year in the times-tamp is retrieved from the document?s Gigaword IDwhich contains the year and day the article was re-trieved.
Year mentions are extracted from docu-ments by matching all tokens with exactly four digitswhose integer is in the range of 1900 and 2100.The MaxEnt classifiers are also from the Stanfordtoolkit, and both the document and year mentionclassifiers use its default settings (quadratic prior).The ?
factor in the joint classifier is optimized onthe development set as described in Section 4.3.
Wealso found that dev results improved when trainingignores the border months of Jan, Feb, and Dec. Thefeatures described in this paper were selected solelyby studying performance on the development set.The final reported results come from running on thetest set once at the end of this study.Table 3 shows the results on the Test set for alldocument classifiers.
We measure accuracy to com-pare overall performance since the test set is a bal-anced set (each year has the same number of testdocuments).
Unigram NLLR and Filtered NLLRare the language model implementations of previ-ous work as described in Section 3.1.
MaxEnt Un-igram is our new discriminative model for this task.MaxEnt Time is the discriminative model with richtime features (but not NER) as described in Section3.3.2 (Time+NER includes NER).
Finally, the Jointmodel is the combined document and year mentionclassifiers as described in Section 4.3.
Table 4 showsthe F1 scores of the Joint model by year.Our new MaxEnt model outperforms previouswork by 55% relative accuracy.
Incorporating timefeatures further improves the relative accuracy by104Model Overall AccuracyRandom Guess 10.0%Unigram NLLR 24.1%Filtered NLLR 29.1%MaxEnt Unigram 45.1%MaxEnt Time 48.3%MaxEnt Time+NER 51.4%Joint 53.4%Table 3: Performance as measured by accuracy.
The pre-dicted year must exactly match the actual year.95 96 97 98 99 00 01 02P .57 .49 .52 .48 .47 .51 .51 .59R .54 .56 .62 .44 .48 .48 .46 .57F1 .55 .52 .57 .46 .48 .49 .48 .58Table 4: Yearly results for the Joint model.
2005/06 areomitted due to space, with F1 .56 and .63, respectively.7%, and adding NER by another 6%.
Total relativeimprovement in accuracy is thus almost 77% fromthe Time+NER model over Filtered NLLR.
Further,the temporal constraint model increases this bestclassifier by another 3.9%.
All improvements arestatistically significant (p < 0.000001, McNemar?stest, 2-tailed).
Table 6 shows that performance in-creased most on the documents that contain at leastone year mention (60% of the corpus).Finally, Table 5 shows the results of the tempo-ral constraint classifiers on year mentions.
Not sur-prisingly, the fine-grained performance is quite a bitlower than the core relations.
The full Joint resultsin Table 3 use the three core relations, but the sevenfine-grained relations give approximately the sameresults.
Its lower accuracy is mitigated by the finergranularity (i.e., the majority class basline is lower).7 DiscussionThe main contribution of this paper is the discrimi-native model (54% improvement) and a new set ofP R F1Before Timestamp .95 .98 .96Same as Timestamp .73 .57 .64After Timestamp .84 .81 .82Overall Accuracy 92.2%Fine-Grained Accuracy 70.1%Table 5: Precision, recall, and F1 for the core relations.Accuracy for both core and fine-grained.All With Year MentionsMaxEnt Unigram 45.1% 46.1%MaxEnt Time+NER 51.4% 54.3%Joint 53.4% 57.7%Table 6: Accuracy on all documents and documents withat least one year mention (about 60% of the corpus).features for document dating (14% improvement).Such a large performance boost makes clear that thelog likelihood and entropy approaches from previ-ous work are not as effective as discriminative mod-els on a large training corpus.
Further, token-basedfeatures do not capture the implicit references totime in language.
Our richer syntax-based featuresonly apply to year mentions, but this small textualphenomena leads to a surprising 13% relative im-provement in accuracy.
Table 6 shows that a signif-icant chunk of this improvement comes from docu-ments containing year mentions, as expected.The year constraint learner also improved perfor-mance.
Although most of its features are in the doc-ument classifier, by learning constraints it captures adifferent picture of time that a traditional documentclassifier does not address.
Combining this picturewith the document classifier leads to another 3.9%relative improvement.
Although we focused on yearmentions here, there are several avenues for futurestudy, including explorations of how other types oftime expressions might inform the task.
These con-straints might also have applications to the orderingtasks of recent TempEval competitions.Finally, we presented a new evaluation setup forthis task.
Previous work depended on having train-ing documents in the same week and day of the testdocuments.
We argued that this may not be an ap-propriate assumption in some domains, and particu-larly problematic for the news genre.
Our proposedevaluation setup instead separates training and test-ing data across months.
The results show that log-likelihood ratio scores do not work as well in thisenvironment.
We hope our explicit train/test envi-ronment encourages future comparison and progresson document dating.AcknowledgmentsMany thanks to Stephen Guo and Dan Jurafsky forearly ideas and studies on this topic.105ReferencesNathanael Chambers and Dan Jurafsky.
2008.
Jointlycombining implicit constraints improves temporal or-dering.
In Proceedings of the Conference on Em-pirical Methods on Natural Language Processing(EMNLP), Hawaii, USA.W.
Dakka, L. Gravano, and P. G. Ipeirotis.
2008.
An-swering general time sensitive queries.
In Proceedingsof the 17th International ACM Conference on Informa-tion and Knowledge Management, pages 1437?1438.Angelo Dalli and Yorick Wilks.
2006.
Automatic dat-ing of documents and temporal text classification.
InProceedings of the Workshop on Annotating and Rea-soning about Time and Events, pages 17?22.Franciska de Jong, Henning Rode, and Djoerd Hiemstra.2005.
Temporal language models for the disclosure ofhistorical text.
In Humanities, computers and culturalheritage: Proceedings of the XVIth International Con-ference of the Association for History and Computing(AHC 2005).Fernando Diaz and Rosie Jones.
2004.
Using temporalprofiles of queries for precision prediction.
In Pro-ceedings of the 27th Annual International ACM Spe-cial Interest Group on Information Retrieval Confer-ence.David Graff.
2002.
English Gigaword.
Linguistic DataConsortium.Nattiya Kanhabua and Kjetil Norvag.
2008.
Improv-ing temporal language models for determining time ofnon-timestamped documents.
In Proceedings of the12th European conference on Research and AdvancedTechnology for Digital Libraries.Nattiya Kanhabua and Kjetil Norvag.
2009.
Using tem-poral language models for document dating.
LectureNotes in Computer Science: machine learning andknowledge discovery in databases, 5782.W.
Kraaij.
2004.
Variations on language modelingfor information retrieval.
Ph.D. thesis, University ofTwente.Abhimanu Kumar, Matthew Lease, and Jason Baldridge.2011.
Supervised language modeling for temporal res-olution of texts.
In Proceedings of CIKM.Xiaoyan Li and W. Bruce Croft.
2003.
Time-based lan-guage models.
In Proceedings of the twelfth interna-tional conference on Information and knowledge man-agement.Dolores M.
Llido?, Rafael Llavori, and Maria?
J. Aram-buru.
2001.
Extracting temporal references to assigndocument event-time periods.
In Proceedings of the12th International Conference on Database and Ex-pert Systems Applications.Inderjeet Mani and George Wilson.
2000.
Robust tempo-ral processing of news.
In Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19.James Pustejovsky, Patrick Hanks, Roser Sauri, AndrewSee, David Day, Lisa Ferro, Robert Gaizauskas, Mar-cia Lazo, Andrea Setzer, and Beth Sundheim.
2003.The timebank corpus.
Corpus Linguistics, pages 647?656.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporal re-lation identification.
In Workshop on Semantic Evalu-ations.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Jessica Moszkowicz, and James Puste-jovsky.
2009.
The tempeval challenge: identifyingtemporal relations in text.
Special Issue: Computa-tional Semantic Analysis of Language: SemEval-2007and Beyond, 43(2):161?179.Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-hara, and Yuji Matsumoto.
2009.
Jointly identify-ing temporal relations with markov logic.
In Proceed-ings of the Association for Computational Linguistics(ACL).Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, DonaldMetzler, and Jian yun Nie.
2009.
Search resultre-ranking by feedback control adjustment for time-sensitive query.
In Proceedings of the 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics.106
