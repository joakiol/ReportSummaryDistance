Integrated Control of Chart Items for Error RepairKyongho MIN and William H. WILSONSchool of Computer Science & EngineeringUniversity of New South WalesSydney NSW 2052 Australia{ min,billw }@cse.unsw.edu.auAbstractThis paper describes a system thatperforms hierarchical error repair for ill-formed sentences, with heterarchicalcontrol of chart items produced at thelexical, syntactic, and semantic levels.
Thesystem uses an augmented context-freegrammar and employs a bidirectional chartparsing algorithm.
The system iscomposed of four subsystems: for lexical,syntactic, surface case, and semanticprocessing.
The subsystems are controlledby an integrated-agenda system.
Thesystem employs a parser for well-formedsentences and a second parser for repairingsingle error sentences.
The system rankspossible repairs by penalty scores whichare based on both grammar-dependentfactors (e.g.
the significance of therepaired constituent in a local tree) andgrammar-independent factors (e.g.
errortypes).
This paper focuses on theheterarchical processing of integrated-agenda items (i.e.
chart items) at threelevels, in the context of single errorrecovery.IntroductionWeischedel and Sondheimer (1983) describedtwo types of ill-formedness: relative (i.e.limitations of the computer system) andabsolute (e.g.
misspel l ings, mistyping,agreement violation etc).
These two types ofproblem cause ill-formedness of a sentence atvarious levels, including typographical,orthographical, morphological, phonological,syntactic, semantic, and pragmatic levels.Typographical spelling errors have beenstudied by many people (Damerau, 1964;Peterson, 1980; Pollock and Zamora, 1983).Mitton (1987) found a large proportion ofreal-word errors were orthographical: to-->too, were  ---> where .
At the sentential level,types of syntactic errors such as co-occurrenceviolations, ellipsis, conjunction errors, andextraneous terms have been studied (Young,Eastman, and Oakman, 1991).
In addition,Min (1996) found 0.6% of words misspelt(447/68966) in 300 email messages, leading toabout 12.0% of the 3728 sentences having-errors.Various systems have focused on therecovery of ill-formed text at the morpho-syntactic level (Vosse, 1992), the syntacticlevel (Irons, 1963; Lyon, 1974), and thesemantic level (Fass and Wilks, 1983;Carbonell and Hayes, 1983).
Those systemsidentified and repaired errors in various ways,including using grammar-specific rules (meta-rules) (Weischedel and Sondheimer, 1983),least-cost error recovery based on chartparsing (Lyon, 1974; Anderson andBackhouse, 1981), semantic preferences (Fassand Wilks, 1983), and heuristic approachesbased on a shift-reduce parser (Vosse, 1992).Systems that focus on a particular level misserrors that can only be detected using higherlevel knowledge.
For example, at the lexicallevel, in I saw a man i f  the park, the misspeltword i f  is undetected.
At the syntactic level, inI saw a man in the pork ,  the misspelling ofpork  can only be detected using semanticinformation.This paper describes the automaticcorrection of ill-formed sentences by usingintegrated information from three levels(lexical, syntactic, and semantic).
TheCHAPTER system (CHArt Parser for Two-stage Error Recovery), performs two-stageerror recovery using generalised top-downchart parsing for the syntax phase (cf.
Mellish,1989; Kato, 1994).
It uses an augmentedcontext-free grammar, which covers verbsubcategorisations, passives, yes/no and WH-questions, f inite relative clauses, andEQUI/SOR phenomena.The semantic processing uses a conceptualhierarchy and act templates (Fass and Wilks,1983), that express semantic restrictions.Surface case processing is used to help extractmeaning (Grishman and Peng, 1988) bymapping surface cases to their correspondingconceptual cases.
Unlike other systems that862have focused on error recovery at a particularlevel (Damerau, 1964; Mellish, 1989; Fass andWilks, 1983), CHAPTER uses an integratedagenda system, which integrates lexical,syntactic,  surface case, and semanticprocessing.
CHAPTER uses syntactic andsemantic information to correct spelling errorsdetected, including real-word errors.Section 1 treats methodology.
Section 2 givestest results for CHAPTER.
Section 3 describesproblems with CHAPTER and section 4contains conclusions.1 MethodologyThe system uses a hierarchical approach andan integrated-agenda system, for efficiency inan environment where most sentences do nothave errors.
The first stage parses an inputsentence using a bottom-up left-to-right chartparsing algorithm incorporating surface caseand semantic processing.
If no parse is found,the second stage tries to repair a single error:either at the lexical or syntactic level (?1.1) orat the semantic level (?
1.2).
The second parseruses generalised top-down strategies (Mellish,1989) and a restricted bidirectional algorithm(Satta and Stock, 1994) for error detection andcorrection.Errors at the syntactic level are assumed toarise from replacement of a word by a knownor unknown word, addition of a known orunknown word, or deletion of a word.
Real-word replacement errors may occur because ofsimple misspellings, or agreement violations.
Asemantic error is signalled if a filler conceptviolates the semantic onstraints of the conceptframe for a sentence.1 .1  Syntact i c  RecoveryCHAPTER's syntactic error recovery systememploys  genera l i sed  top-down andbidirectional bottom-up chart parsing (cf.Mellish, 1989) using an augmented context-free grammar.
The system is composed of twophases: error detection and error correction(see section 4 in Min, 1996).
A single syntacticerror is detected by the fo l lowing twoprocesses:(1) top-down expectation: expands a goalusing an augmented context- f reegrammar.
(A goal is a partial tree, whichmay contain one or more syntacticcategories, specifically a subtree of asyntax tree corresponding to a singlecontext-free rule, and which mightcontain syntactic errors.
For example,the first goal for the ill-formed sentenceI have a bif book is <S needs from 0 to5 with penalty score 4>.
)(2) bottom-up satisfaction: searches for anerror using a goal and inactive arcsmade by the first-stage parser, andproduces a need-chart network;The error detected by this process is correctedby the following two processes:(3) a constituent reconstruction engine:repairs the error and reconstructs localtrees by retracing the need-chartnetwork; and(4) spelling correction corrects spellingerrors (see Min and Wilson, 1995).Because of space limitations, this paper focuseson (3) and (4).Consider the sentence I saw a man i f  thepark.
The top-down expectation phase wouldproduce the initial goal for the sentence, <goalS is needed from 0 to 7>, and expand it usinggrammar rules, <(S ---> NP VP) is needed from0 to 7>.
Next, a bottom-up satisfaction phaseuses the inactive arcs left behind by the first-stage parser to refine and localise the error bylooking for the leftmost or r ightmostconstituent of the expanded goal in abidirectional mode.For example, given an inactive arc, <NPCI")from 0 to 1>, the left-to-right process isapplied: for the expanded goal S, NP ('T') isfound from 0 to 1 and VP is needed from 1 to7: or, more briefly, <S ---> NP('T') ?
VP isneeded from 1 to 7>.
This data structure iscalled a need-arc.
A need-arc is similar to anactive arc, and it includes the followinginformation: which constituents are alreadyfound and which constituents are needed forthe recovery of a local tree between twopositions, together with the arc's penalty score.From this need-arc, another goal, <goal VP isneeded from 1 to 7>, is produced.After detecting an error using the top-downexpectation and bottom-up satisfaction phases,the detected error is corrected using two typesof chart item: a goal and a need-arc, and thetypes of the goal's or need-arc's constituentand its penalty score.
The penalty score(PS(G)) of a goal (or need-arc) G, whosesyntactic category is L and whose twopositions are FROM and TO, is computed asfollows:PS(G) = RW(G) - MEL(L)where RW(G) is the number of remainingwords to be processed, (ie.
TO - FROM),and MEL(L) is the minimal extensionlength of the category L.863MEL (Minimal Extension Length) is theminimum number of preterminals necessaryto produce the rule's LHS category.
Forexample, the MEL of S is 2, because ofexamples like "I go".Using the penalty scores, three errorcorrection conditions are as follows:?
The substitution correction condition is:the goal's label is a single lexical category,and its penalty score is 0 (there is areplaced word)?
The addition correction condition is:the goal's label is a single lexical category,and its penalty score is -1 (there is anomitted word).?
The deletion correction condition is:there is no constituent needed for repair,and the penalty score of the need-arc is 1(there is an extra word).The repaired constituent produced with theseconditions is used to repair constituents all theway up to the original S goal via the need-chart network.
This process is performed bythe constituent reconstruction engine.At the syntactic level, the choice of the bestcorrection relies on two penalty schemes:error-type penalties and penalties based on theweight (or importance) of the repairedconstituent in its local tree.
The error-typepenalties are 0.5 for substitution errors, and 1for deletion or addition errors 1.
The weightpenalty of a repaired constituent in a local treeis either 0.1 for a head daughter, 0.5 for anon-head daughter, or 0.3 for a recursivehead-daughter (e.g.
NP in the right-hand sideof the rule NP ---> NP PP).
The weight penaltyis accumulated while retracing the need-chartnetwork.
In effect, the system seeks a bestrepair with minimal length path from node Sto the error location in the syntax tree.Often more than one repair is suggested.
Therepaired syntactic structures are subject tosurface case and semantic processing duringsyntactic reconstruction.
If the syntactic repairdoes not violate selectional restrictions, it isacceptable.1 .2  Semant ic  RecoveryCHAPTER maps syntactic parses into surfacecase frames.
These are interpreted by amapping procedure and a pattern matchingalgorithm.
The mapping procedure usessemantic selectional restrictions based on acttemplates and a concept hierarchy andconverts the surface case slots into conceptIThe~ penalties are somewhat rbitrary.
Corpus-basedprobability estimates would be preferable.slots, while the pattern matching algorithmconstrains filler concepts using ACT templateswhich represents semant ic  select ionalrestrictions.
Selectional restr ict ions arerepresented by a expressions like ANIMATE,or (NOT HUMAN).
The latter represents anyconcept hat is not a sub-concept of HUMAN.Surface cases are mapped to concept slots:subject ---> agent, verb ---> act, direct objecttheme.
Consider the sentence "I parked a car".The mapping of SENTI into PARK1 is asfollows:SENTI: (subj (value 'T'))(verb (value "parked"))(dobj (value "a car"))PARKI: (agent (SPEAKER 'T'))(act (PARK "parked"))(theme (CAR "a car"))Semantic errors may be of two types:(1) there may be no full parse tree, sosemantic interpretation is impossible;(2) the sentence may be syntact ical lyacceptable, but semantically i l l-formed(e.g.
I parked a bud (bus)).The first type of error is repaired from thespelling level up to semantic level (if a spellingerror is detected).
For errors of a semanticnature, semantic selectional restrictions may beforced onto the error concept o make it fit thetemplate.
For example, the sentence "I parkeda bud" violates the semantic selectionalrestriction on the theme slot of park .
Thetemplate of the verb park is (HUMAN PARKVEHICLE).
However, the concept BUD,associated with 'bud', is not consistent with therestriction, VEHICLE, on the theme slot.
As aresult, the sentence is semantically ill-formed,with a semantic penalty o f -1  (one slot violatesa restriction).
To correct the error, the fillerconcept BUD is forced to satisfy the templateconcept VEHICLE by invoking the spellingcorrector with the word 'bud' and the conceptVEHICLE.
Thus the real word error budwould be corrected to bus.The filler concept may itself be internallyinconsistent.
Consider the sentence I saw apregnant man.
The theme slot of SEE satisfiesits restriction.
However, the filler concept ofthe theme slot is inconsistent.
In CHAPTER,the attribute concept pregnant is identified asthe error rather than the head concept man.
Tocorrect it, the attribute concept is relaxed toany attribute concept that can qualify theMAN concept.
It would also be possible toforce man to fit to the attribute concept (e.g.by changing it to woman).
There seems to beno general method to pick the correctcomponent to modify with this type of error:864we chose to relax the attribute concept.
Thisproblem might be resolved by pragmaticprocessing.1 .3  Integrated-Agenda ManagerCHAPTER is composed of four subsystems forparsing well-formed sentences and repairingill-formed sentences: lexical, syntactic, surfacecase, and semantic processing.
Each subsystemuses relevant chart items from othersubsystems as its input and is invoked in aheterarchical mode byan  agenda scheme,which is called the integrated-agenda m nager.The manager controls and integrates all levelsof information to parse well-formed sentencesand repair ill-formed sentences (Min, 1996).Thus the integrated-agenda managerdistributes agenda items to relevant subsystems(see Figure 1).~ genda items t.~I syntactic sem,~nuc item I I surlace ' caseiteml\[  item \]~syntac~c Isurface casel I semanticocesslng I I processing I \[processing\ [ - - -~ew ch!rt item ~Figure 1.
Integrated agenda managerFor example, if an agenda item is a repairedsyntactic item, then it is distributed to syntacticprocessing for recovery, then to surface caseand semantic processing.
The invocation ofthe relevant subsystem depends on thecharacteristics of the chart item.
Consider anagenda item which is a syntactic NP node.Syntact ic and subsequent ly  semanticprocessing are invoked.
Surface caseprocessing is not appropriate for an NP node.If an agenda item is a syntactic VP node, thensyntactic, surface case, and semanticprocessing are all invoked.
After subsystemprocessing of the item, the new chart itembecomes an agenda item in turn.
Thiscontinues until the integrated agenda is empty.The data structures of CHAPTER are based ona network-like structure that allows access to alllevels of information (syntactic, surface case,and semantics).
Some of the data are storedusing associative structures (e.g.
grammarrules, active arcs, and inactive arcs) that allowdirect access to structures most likely to beneeded uring processing.2 Experimental Resul tsThe test data included syntactic errorsintroduced by substitution of an unknown orknown word, addition of an unknown orknown word, deletion of a word, segmentationand punctuation problems, and semanticerrors.
Data sets we used are identified as:NED (a mix of errors from Novels, Electronicmail, and an (electronic) Diary); Applingl,and Peters2 (the Birkbeck data from OxfordText Archive (Mitton, 1987)); and Thesprev.Thesprev was a scanned version of ananonymous humorous article titled "ThesisPrevention: Advice to PhD Supervisors: TheSiblings of Perpetual Prototyping".In all, 258 ill-formed sentences were tested:153 from the NED data, 13 from Thesprev, 74from Applingl, and 18 from Peters2.
Thesyntactic grammar covered 166 (64.3%) of themanually corrected versions of the 258sentences.
The average parsing time was 3.2seconds.
Syntactic processing produced onaverage 1.7 parse trees 2, of which 0.4 syntacticparse trees were filtered out by semanticprocessing.
Semantic processing produced 9.3concepts on average per S node, and 7.3 ofthem on average were ill-formed.
So manywere produced because CHAPTER generated asemantic oncept whether it was semanticallyill-formed or not, to assist with the repair of ill-formed sentences (Fass and Wilks, 1983).Across the 4 data sets, about one-third ofthe (manually-corrected) sentences wereoutside the coverage of the grammar and lex-icon.
The most common reasons were that thesentences included a conjunction ("He placesthem face down so that they are a surprise"), aphrasal verb CI called out to Fred and wentinside"), or a compound noun ("P Cdevelopment ools are far ahead of Unixdevelopment ools").
The remaining 182sentences were used for testing: NED (98/153);Thesprev (12/13); Appl ingl (55/74); andPeters2 (17/18).
Compound and compound-complex sentences in NED were split intosimple sentences to collect 13 more ill-formedsentences for testing.2There are so few parse trees because of the use ofsubcategorisation and the augmented context-freegrammar (the number of parse trees ranges from 1 to7).865Table 1 shows that 89.9% of these ill-formed sentences were repaired.
Among these,CHAPTER ranked the correct repair first orsecond in 79.3% of cases (see 'best repair'column in Table 1).
The ranking was based onpenalty schemes at three levels: lexical,syntactic, and semantic.
If the correct repairwas ranked lower than second among therepairs suggested, then it is counted under'other repairs' in Table 1.
In the case of theNED data, the 'other repairs' include 11 casesof incorrect repairs introduced by:segmentation errors, apostrophe errors,semantic errors, and phrasal verbs.
Thus forabout 71% of all ill-formed sentences tested,the correct repair ranked first or secondamong the repairs suggested.
For 19% of thesentences tested, incorrect repairs were rankedas the best repairs.
A sentence was consideredto be "correctly repaired" if any of thesuggested corrections was the same as the oneobtained by manual correctionTable 2 shows further statistics onCHAPTER's performance.
CHAPTER took18.8 seconds on average 3 to repair an ill-formed sentence; suggested an average of 6.4repaired parse trees; an average of 3 repairswere filtered out by semantic processing.During semantic processing, an average of40.3 semantic concepts were suggested foreach S node.
An average 34.3 concepts per Snode were classified as ill-formed.
Twentyseven percent of the 'best' parse trees suggestedby CHAPTER's ranking strategy at thesyntactic level were filtered out by semanticprocessing.
The remaining 73% of the 'best'parse trees were judged semantically well-formed.In the case of the NED data set, 90 ill-formed sentences were repaired.
On average:recovery time per sentence was 23.9 seconds;9.8 repaired S trees per sentence wereproduced; 4.5 of the 9.8 repaired S trees weresemantically well-formed; 95.1 repairedconcepts (ill-formed and well-formed) wereproduced; 8.5 of 95.1 repaired concepts werewell-formed; and semantic processing filteredsyntactically best repairs, removing 22% ofrepaired sentences.
The number of repairedconcepts for S is very large because semanticprocessing at present supports interpretation fonly a single verbal (or verb phrasal) adjuncts.For example, the template of the verb GOallows either a temporal or destination adjunctat present and ignores any second or lateradjunct.
Thus a GO sentence would beinterpreted using both \[THING GO DEST\]and \[THING GO TIME\].3 Discussion3.1 Syntactic Level ProblemsThe grammar ules need extension to coverthe following grammatical phenomena:compound nouns and adjectives, gerunds,TO+VP, conjunctions, comparatives, phrasalverbs and idiomatic sentences.
For example,'in the morning' and 'at midnight' are well-formed phrases.
However, CHAPTERcurrently also parses 'in morning', 'in themidnight', and 'at morning' as well-formed.CHAPTER uses prioritised search to detect andcorrect syntactic errors using the penaltyscores of goals.
However, the scheme forselecting the best repair did not uncritically usethe first detected error found by the prioritisedsearch at the syntactic level, because the bestrepair might be ill-formed at the semanticlevel.
In fact, the prioritised search strategy didnot contribute to the selection scheme, whichdepended solely on the error type and theimportance of the repaired constituent in itslocal tree.3 .2  Semant ic  Level P rob lemsAt present in CHAPTER's semantic system,the most complex problem is the processing ofprepositions, and their conceptual definition.For example, the preposition 'for' canindicate at least three major concepts: timeduration (for a week), beneficiary (for hismother), and purpose (for digging holes).
Iffor takes a gerund object, then the concept willspecify a purpose or reason (e.g.
It is amachine for slicing bread).In addition, the act templates do not allowmultiple optional conceptual cases (i.e.relational conceptual cases - LOC forIdeational concepts, and DEST for destinationconcepts, etc.)
for prepositional nd adverbialphrases.
This would increase the number oftemplates and the computational cost.
If thereis more than one verbal adjunct (PPs andADVPs) in a sentence, then CHAPTER doesnot interpret al adjuncts.3Running under Macintosh Common Lisp v 2.0 on aMacintosh II fx with 10 MB for Lisp866Data SetNED (%)Appling 1 (%)Peters2 (%)Thesprev (%)Average (%)Sentencestested98551712Number ofrepairs90 (91.8)52 (94.5)17 (100)10 (83.3)* 89.9%Best repairs64/90 (71.1)40/52 (76,9)14/17 (82.4)9/10 (90.0)79.3%Other repairs26/90 (28.9)12/52 (23.1)3/17 (17.6)1/10 (10.0)20.7%No repairssuggested8 (8.2)3 (5.5)02 (16.7)10.1%Table 1.
Performance of CHAPTER on ill-formed sentences*Peters2 data are not considered in the averages because Peters2 consists of only the sentences that were covered byCHAPTER's grammar, selected from more than 300 sentence fragments (simple sentences and phrases.
)Data set \[ Sentences I Time I RepairedI repaired \[ (sec) I S treesar r'ami  1?i ISemantically Repaired Repaired well- % of syntactic-well-formed concepts formed ally-best parsesfor S ~ filtered4.5 95.1 8.57"TTZr-.7----Z3----- " - -3 - i f - - -'77Z- - - -  7 " " - ' fTF" - -3.4 40.3 6.0 46/169 (27%)Table 2.
Results on CHAPTER's performance (average values per sentence)Conc lus ionThis paper has presented a hierarchical errorrecovery system, CHAPTER, based on a chartparsing algorithm using an augmentedcontext-free grammar.
CHAPTER uses anintegrated-agenda manager that invokessubsystems incremental ly at four levels:lexical, syntactic, surface case, and semantic.
Asentence has been confirmed as well-formedor repaired when it has been processed at alllevels.Semantic processing performs patternmatching using a concept hierarchy and verbtemplates (which specify semantic selectionalrestrictions).
In addition, procedural semanticconstraints have been used to improve theefficiency of semantic processing based on aconcept hierarchy.
However, it increasescomputational cost.CHAPTER repaired 89.9% of the ill-formedsentences on which it was tested, and in 79.3%of cases suggested the correct repair (as judgedby a human) as the best of its alternatives.CHAPTER's semantic processing rejected 27%of the repairs judged "best" by the syntacticsystem.ReferencesAnderson, S. and Backhouse, R. (1981).
LocallyLeast-cost Error Recovery in Earley's Algorithm.ACM Transactions on Programming I_zmguages andSystems, 3(3) 318-347.Carbonell, J. and Hayes, P. (1983).
RecoveryStrategies for Parsing ExtragrammaticalLanguage.
American Journal of ComputationalLinguistics, 9(3-4) 123-146.Damerau, F. (1964).
A Technique for ComputerDetection and Correction of Spelling Errors.Communications ofthe ACM, 7(3) 171-176.Fass, D. and Wilks, Y.
(1983).
Preference Semantics,Ill-formedness, and Metaphor.
American Journalof Computational Linguistics, 9(3-4) 178-187.Grishman, R. and Peng, P. (1988).
Responding toSemantically Ill-Formed Input.
The 2ndConference of Applied Natural LanguageProcessing, 65-70.Irons, E. (1963).
An Error-Correcting ParseAlgorithm.
Communications of the ACM, 6(11)669-673.Kato, T. (1994).
Yet Another Chart-Based Techniquefor Parsing Ill-formed Input.
The Fourth867Conference on Applied Natural LanguageProcessing, 107-112.Lyon, G. (1974).
Syntax-Directed Least-ErrorsAnalysis for Context-Free Languages: A PracticalApproach.
Convnunications of the A CM, 17(1)3-14.Mellish, C. (1989).
Some Chart-Based Techniques forParsing Ill-Formed Input.
ACL Proceedings, 27thAnnual Meeting, 102-109.Min.
(1996).
Hierarchical error recovery based onbidirectional chart parsing techniques.
PhDdissertation, University of UNSW, Sydney,Australia.Min, K. and Wilson, W. H. (1995).
Are EfficientNatural Language Parsers Robust?.
EighthAustralian Joint Conference on ArtificialIntelligence; 283-290Mitton, R. (1987).
Spelling Checkers, SpellingCorrectors and the Misspellings of Poor Spellers.Information Processing and Management, 23(5)495-505.Peterson, J.
(1980).
Computer Programs for Det-ecting and Correcting Spelling Errors.
Com-munications of the ACM, 23(12) 676-687.Pollock and Zamora (1983).
Collection andcharacterisation of spelling errors in scientific andscholarly text.
Journal of the American Society forInformation Science.
34(1) 51-58.Satta and Stock (1994).
Bidirectional context-freegrammar parsing for natural language processing.Artificial Intelligence 69 123 -164.Vosse, T. (1992).
Detecting and Correcting Morpho-Syntactic Errors in Real Texts.
The ThirdConference on Applied Natural LanguageProcessing, 111-118.Weischedel, R. and Sondheimer, N. (1983).
Meta-rules as Basis for Processing Ill-formed Input.American Journal of Computational Linguistics,9(3-4) 161-177.Young, C., Eastman, C., and Oakman, R. (1991).
AnAnalysis of Ill-formed Input in Natural LanguageQueries to Document Retrieval Systems.Information Processing and Management, 27(6)615-622.868
