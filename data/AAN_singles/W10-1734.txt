Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 224?234,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsHow to Avoid Burning Ducks:Combining Linguistic Analysis and Corpus Statisticsfor German Compound ProcessingFabienne Fritzinger and Alexander FraserInstitute for Natural Language ProcessingUniversity of Stuttgart{fritzife, fraser}@ims.uni-stuttgart.deAbstractCompound splitting is an important prob-lem in many NLP applications which mustbe solved in order to address issues of datasparsity.
Previous work has shown that lin-guistic approaches for German compoundsplitting produce a correct splitting moreoften, but corpus-driven approaches workbest for phrase-based statistical machinetranslation from German to English, aworrisome contradiction.
We address thissituation by combining linguistic analysiswith corpus-driven statistics and obtain-ing better results in terms of both produc-ing splittings according to a gold standardand statistical machine translation perfor-mance.1 IntroductionCompounds are highly productive in German andcause problems of data sparsity in data-driven sys-tems.
Compound splitting is an important com-ponent of German to English statistical machinetranslation systems.
The central result of work by(Koehn and Knight, 2003) is that corpus-drivenapproaches to compound splitting perform betterthan approaches based on linguistic analysis, andthis result has since been confirmed by other re-searchers (Popovic?
et al, 2006; Stymne, 2008).This is despite the fact that linguistic analysis per-forms better in terms of matching a gold standardsplitting.
Our work shows that integrating thesetwo approaches, by employing high-recall lin-guistic analysis disambiguated using corpus statis-tics, effectively combines the benefits of both ap-proaches.
This is important due to the wide us-age of the Koehn and Knight approach in statisti-cal machine translation systems.The splittings we produce are best in terms ofboth end-to-end machine translation performance(resulting in an improvement of 0.59 BLEU and0.84 METEOR over the corpus-driven approachof Koehn and Knight on the development test setused for WMT 20091) and two gold standard eval-uations (see section 4).
We provide an exten-sive analysis of the improvements of our approachover the corpus-driven approach.
The approachwe have developed may help show how to im-prove previous approaches to handling compoundsin such applications as speech recognition (e.g.,(Larson et al, 2000)) or information retrieval (e.g.,(Braschler and Ripplinger, 2004)).The organization of the paper is as follows.
Sec-tion 2 discusses previous work on compound split-ting for statistical machine translation.
Section 3presents approaches for compound splitting andalso presents SMOR, the morphological analyzerthat is a key knowledge source for our approach.Section 4 presents a comparison of compoundsplitting techniques using two gold standard cor-pora and an error analysis.
Section 5 presentsphrase-based statistical machine translation (SMT)results.
Section 6 concludes.2 Related Work on German CompoundSplittingRule-based compound splitting for SMT has beenaddressed by Nie?en and Ney (2000), whereGERTWOL was used for morphological analysisand the GERCG parser for lexical analysis and dis-ambiguation.
Their results showed that morpho-syntactic analysis could reduce the subjective sen-tence error rate.The empirical approach of Koehn and Knight(2003) splits German compounds into wordsfound in a training corpus.
A minimal amountof linguistic knowledge is included in that thefiller letters ?s?
and ?es?
are allowed to be intro-duced between any two words while ?n?
might be1See Table 6 in section 5 for details.224dropped.
A scoring function based on the aver-age log frequency of the resulting words is usedto find the best splitting option, see section 3.2 fordetails.
SMT experiments with additional knowl-edge sources (parallel corpus, part-of-speech tag-ger) for compound splitting performed worse thanusing only the simple frequency metric.
Stymne(2008) varies the Koehn and Knight approach byexamining the effect of a number of parameters:e.g.
word length, scoring method, filler letters.Popovic?
et al (2006), compared the approachof Nie?en and Ney (2000) with the corpus-drivensplitting of Koehn and Knight (2003) in terms ofperformance on an SMT task.
Both systems yieldsimilar results for a large training corpus, whilethe linguistic-based approach is slightly superiorwhen the amount of training data is drastically re-duced.There has recently been a large amount of in-terest in the use of input lattices in SMT.
One useof lattices is to defer disambiguation of word-levelphenomena such as inflection and compounds todecoding.
Dyer (2009) applied this to German us-ing a lattice encoding different segmentations ofGerman words.
The work is evaluated by using the1-best output of a weak segmenter2 on the trainingdata and then using a lattice of the N-best outputof the same segmenter on the test set to decode,which was 0.6 BLEU better than the unsegmentedbaseline.
It would be of interest to test whether de-ferral of disambiguation to decoding still producesan improvement when used in combination with ahigh-performance segmenter such as the one wepresent, an issue we leave for future work.3 Compound ProcessingPrevious work has shown a positive impact ofcompound splitting on translation quality of SMTsystems.
The splitting reduces data sparsity andenhances word alignment performance.
An exam-ple is given in Figure 1.Previous approaches for compound splittingcan be characterized as following two basic ap-proaches: the use of morphological analyzers tofind split points based on linguistic knowledgeand corpus-driven approaches combining large2The use of the 1-best output of the segmenter for Germanto English decoding results in a degradation of 0.3 BLEU,showing that it is worse in performance than the corpus-driven method of Koehn and Knight, which improves perfor-mance (see the evaluation section).
However, this segmenteris interesting because it is language neutral.InflationsratenEnglish translationunsplit compoundinflation ratesInflation Ratensplit compound1?to?n alignment1?to?1 alignmentFigure 1: Compound splitting enhances the num-ber of 1-to-1 word alignments.amounts of data and scoring metrics.We briefly introduce the computational mor-phology SMOR (section 3.1) and the corpus-driven approach of Koehn and Knight (2003) (sec-tion 3.2), before we present our hybrid approachthat combines the benefits of both in section 3.3.3.1 SMOR Morphological AnalyzerSMOR is a finite-state based morphological ana-lyzer covering the productive word formation pro-cesses of German, namely inflection, derivationand compounding (Schmid et al, 2004).
Word for-mation is implemented as a concatenation of mor-phemes filtered according to selectional restric-tions.
These restrictions are based on feature deco-rations of stems and affixes encoded in the lexicon.Inflection is realized using inflection classes.An abbreviated3 SMOR analysis of the wordDurchschnittsauto (?standard car?
)4 is given inFigure 2 (a).
The hierarchical structure of the wordformation process is given in Figure 2 (b).
Imple-mented with finite-state technology, SMOR is notable to produce this hierarchy: in our example itoutputs two (correct) analyses of different depthsand does not perform disambiguation.3.2 Corpus-Driven ApproachKoehn and Knight (2003) describe a method re-quiring no linguistically motivated morphologicalanalysis to split compounds.
Instead, a compoundis broken into parts (words) that are found in alarge German monolingual training corpus.We re-implemented this approach with an ex-tended list of filler letters that are allowed to oc-3We show analyses for nominative, and analyses for theother cases genitive, ,dative, accusative are left out as theyare identical.4durch = ?through?, schneiden = ?to cut?, Schnitt = ?
(the)cut?, Durchschnitt = ?average?, Auto = ?car?part-of-speech: <NN>/<V> (noun/verb)gender: <Neu> (neutrum)case: <Nom> (nominative)number: <Sg> (singular)suffixation: <SUFF> (suffix)prefixation: <VPART> (verb particle)225analyze> DurchschnittsautoDurchschnitt<NN>Auto<+NN><Neut><Nom><Sg>durch<VPART>schneiden<V><NN><SUFF>Auto<+NN><Neut><Nom><Sg>(a) SMOR output formatDurchschnittsautoDurchschnittschneidenAutodurch<NN><NN><NN>durchschneiden<V><V><VPART>(b) Morphological analysisFigure 2: Morphological analysis of Durchschnittsauto (?standard car?
).cur between any two parts (nen, ien, en, es, er, s,n) such as s in Inflationsrate (cf.
Figure 1) anddeletable letters (e, n), required for compoundssuch as Kirchturm = Kirche+Turm (?steeple?,?church+tower?).
Filler letters are dropped onlyin cases where the part is more frequent withoutthe letter than with it (an example is that the fre-quency of the word Inflation is greater than thefrequency of the word Inflations); the same holdsfor deletable letters and hyphens (?-?).
The min-imal part size was set to 3 characters.
Word fre-quencies are derived from the true-cased corpususing case insensitive matching.
In order to reducewrong splittings, infrequent words (frequency ?3) are removed from the training corpus and a stoplist was used5.
These are similar choices to thosefound to be best in work by Stymne (2008).The splitting that maximizes the geometricmean of part frequencies using the following for-mula6 is chosen:argmaxS(?pi?Scount(pi))1nFigure 3 contains all splitting options ofthe corpus-driven approach for Ministerpra?sident(?prime minister?).
As can be seen, thedesired splitting Minister|Pra?sident is amongthe options, but in the end Min|ist|Pra?sident(?Min|is|president?)
is picked by the corpus-driven approach because this splitting maximizesthe geometric mean score (mainly due to thehighly frequent verb ist ?is?).
This is linguisti-cally implausible, and the system we introduce inthe next section splits this correctly.Even though this corpus-driven approach tendsto oversplit it works well for phrase-based SMTbecause adjacent words (or word parts) are likely5The stop list contains the following units, which occur inthe corpus as separate words (e.g., as names, function words,etc.
), and frequently occur in incorrect splittings: adr, and,bes, che, chen, den, der, des, eng, ein, fue, ige, igen, iger,kund, sen, ses, tel, ten, trips, ung, ver.6Taken from (Koehn and Knight, 2003):S = split, pi = part, n = number of parts.
The original wordis also considered, it has 1 part and a minimal count of 1.Ministerpr?Ministerpr?sidMinisterMiniMinisMin isteristterPr?Pr?sidPr?sidePr?sidententsidsidentFigure 3: Corpus-driven splittings of Minis-terpra?sident (?prime minister?
).to be learned as phrases.
We will refer to thecorpus-driven approach using the abbreviation cd.3.3 Hybrid ApproachWe present a novel approach to compound split-ting: based on linguistically motivated split pointsgained from SMOR, we search word frequenciesin a large training corpus (the same corpus as wewill use for the corpus-driven approach) in orderto determine the best splitting option for a word(or to leave it unsplit).
This approach needs no ex-plicit definition of filler letters or deletable letters,as this knowledge is encoded in SMOR.In contrast to the corpus-driven approach de-scribed in the previous section, the hybrid ap-proach uses neither a minimal part size constraint,nor a stop-list.
Instead, we make use of the linguis-tic knowledge encoded in SMOR, i.e.
we allow thehybrid approach to split only into parts that canappear as free morphemes, such as stems and sep-aratable particles.
An example is auf|gibt (?to giveup?
), where the particle auf may occur separatedfrom the verb, as in Er gibt nicht auf (?he givesnot up?).
Bound morphemes, such as prefixes andsuffixes cannot be split from the stem, e.g.
verhan-delbar (?negotiable?)
which consists of the prefixver-, the stem handeln and the suffix -bar, is leftunsplit by the hybrid approach.For N-ary compounds (with N>2), we use notonly the split points proposed by SMOR, but wealso search the training corpus for recombinationsof the compound parts: e.g.
SMOR provides theparts A|B|C for the compound ABC, and we addi-226(a) SMOR splitting optionsMinisterpr?sidentw?hlen KampfWahlkampfWahl KampfMinister Pr?sident(b) Part frequenciesword part frequencyKampf 30,546Minister 12,742Ministerpra?sident 22,244Ministerpra?sidentwahl 111Ministerpra?sidentwahlkampf 1Pra?sident 125,747Pra?sidentenwahl 2,482Pra?sidentenwahlkampf 25Wahl 29,255Wahlkampf 23,335(c) Log-based geometric mean scoressplitting option scoreMinisterpra?sidentenwahlkampf 0Ministerpra?sident|Wahlkampf 10.04Ministerpra?sident|Wahl|Kampf 10.21Ministerpra?sident|wa?hlen|Kampf 9.85Minister|Pra?sident|Wahlkampf 10.38Minister|Pra?sident|Wahl|Kampf 10.42Minister|Pra?sident|wa?hlen|Kampf 10.15Ministerpra?sidentenwahl|Kampf 7.52Minister|Pra?sidentenwahl|Kampf 9.19Minister|Pra?sidentenwahlkampf 6.34Table 1: Splitting options for Ministerpra?sidentenwahlkampf (?election campaign of the prime minis-ter?)
(a) with part frequencies derived from the corpus (b) and log-based geometric mean scores (c).tionally search for AB|C and A|BC.Even though SMOR lemmatizes along withcompound splitting, only the information aboutpossible split points is used in our splitting ap-proach.
The compound Beitrittsla?nder (?ac-cession countries?
), for example, is reduced toBeitritt|Land by SMOR, but is retransformed toBeitritt|La?nder in our approach.
This holds alsofor adjectives, e.g.
firmeninterne ?company-internal?
which is split to firma|interne (interne isthe female form of the adjective intern) and verbs,such as the participle wasser|gebunden ?waterbound?, where the lemma is Wasser|binden.Hyphenated words can also be split with SMOR,as long as the rightmost part of the word is in itslexicon.
However, the word parts which are to theleft of hyphen(s) are left unanalyzed.
The SMORanalyses for NATO-Berichts (?NATO report?)
andthe nonsense XYZabc-Berichts (?XYZabc report?
)are given below:analyze> NATO-BerichtsNATO-<TRUNC>Bericht<+NN><Masc><Gen><Sg>analyze> XYZabc-BerichtsXYZabc-<TRUNC>Bericht<+NN><Masc><Gen><Sg>Such Words where the rightmost part is unknownto SMOR are left completely unanalyzed bySMOR.
Examples include NATO-Berxchts (whichis a typo of NATO-Berichts) or al-Qaeda (a propername).
If such words occurred less than 5 times inthe training corpus, they were split at the hyphens.This procedure splits NATO|Berxchts, while itleaves al-Qaeda unsplit.Table 1(a) shows the different splittings7 thatSMOR returns for the ambiguous ad-hoc com-pound Ministerpra?sidentenwahlkampf (?electioncampaign of the prime minister?).
All of them aremorphologically sound compounds of German.The corpus frequencies of the parts provided bySMOR (and their recombinations) are given in Ta-ble 1 (b).
The average natural log frequencies ofthe SMOR splittings in Table 1 (c), with the recom-binations of their parts in the last three rows.
Weset the minimal frequency for each part to 1 (whichgives a log frequency of 0) even if it was not seenin the training corpus.Even though ?prime?
is not a literal transla-tion of Pra?sident, the best splitting (out of thegiven options) is Minister|Pra?sident|Wahl|Kampf(?minister|president|election|campaign?).
It isscored highest and thus chosen by the hybrid ap-proach.For the purpose of SMT, we want to split com-pounds into parts that have a translational cor-respondent in the target language.
To accom-plish that, it is often sufficient to consider thesplit at the highest linguistic analysis level.
For7Ministerpra?sident = ?prime minister?, Wahlkampf =?election campaign?, Minister = ?minister?, Pra?sident =?president?, Wahl = ?election?, wa?hlen = ?to elect?, Kampf= ?fight?227the example Durchschnittsauto (?standard car?)(cf.
Figure 2 above), where the ideal splitis Durchschnitt|Auto (?average|car?).
Here, thedeeper analysis of Durchschnitt as a nominalisa-tion of the particle verb durch|schneiden (?to cutthrough?)
is not relevant.
The same holds for Min-isterpra?sidentenwahlkampf of Table 1, where inone of the splittings Wahl is further reduced to theverb wa?hlen.In order to prevent such analyses from be-ing picked, we investigate the use of restrictingSMOR?s splitting options to analyses having a min-imal number of component parts.
On the otherhand, there are many lexicalized compounds inGerman, that, besides being analyzed as a com-pound also appear as a free word stem in SMOR?slexicon (e.g.
both Gela?ndewagen ?all-terrain vehi-cle?
and Gela?nde|wagen ?terrain vehicle?
are re-turned by SMOR).
Therefore, we keep both vari-ants for our subsequent experiments: the con-strained version that uses only analyses with aminimal number of parts (and thus performs amore conservative splitting) is referred to as smc,while using all of SMOR?s analyses is named sm.In addition to these, we use a constraint that splitsonly nouns.
To do so, the text to be split was POS-tagged with TreeTagger (Schmid, 1994) to deter-mine the nouns in the context of the whole sen-tence.
Splitting only nouns will be referred to as@nn in the remainder of this paper.Compared to the purely corpus-driven ap-proach, hybrid compound splitting substantiallyreduces the number of false splitting options, be-cause only splittings that are linguistically moti-vated are looked up in the training corpus.
Wewill show that this restriction of splitting optionsenhances the number of correct splittings beingpicked.
The purely corpus-driven approach con-siders the correct splitting in most cases, but oftendoes not choose it because there is another higherscoring splitting option (cf.
section 4.3).The main shortcoming of the hybrid approachis its dependence on SMOR?s lexical coverage.SMOR incorporates numerous word formationrules and thousands of word stems (e.g.
over16,000 noun base stems), but our approach willleave all words unsplit that cannot be analyzedwith SMOR.
However, we will show in both thegold standard evaluations (section 4) and the SMTevaluation (section 5) that the recall of SMOR issufficient to result in substantial gains over thecorpus-driven approach.4 Gold Standard EvaluationThe accuracies of the compound splitting ap-proaches are evaluated against two hand-craftedgold standards: one that includes linguisticallymotivated split points (section 4.1), and one indi-cating compounds that were translated composi-tionally by a human translator (section 4.2).
Wefound that the hybrid approach performs best forboth.
In section 5, we will show the impact of thedifferent splitting approaches on translation per-formance, with the result that the hybrid approachoutperforms the corpus-driven approach even fortranslation quality (in contrast to previous work,where the best system according to the gold stan-dard was not the best system for translation qual-ity).
In order to better understand the divergentresults of the splitting approaches, we perform adetailed error analysis in section 4.3.The accuracy of compound splitting is mea-sured using the same terminology and metrics asdescribed in (Koehn and Knight, 2003):correct split: should be split and was split correctlycorrect not: should not be split and was notwrong split: should not be split but was splitwrong not: should be split but was notwrong faulty (fty): should be split, but was split wronglyprecision: correctsplitcorrectsplit+wrongfaulty+wrongsplitrecall: correctsplitcorrectsplit+wrongfaulty+wrongnotaccuracy: correctcorrect+wrongThe results of the following splitting approacheswere investigated:raw = baseline without splittingcd = corpus-driven splittingsm = hybrid approach using all SMOR analysessmc = hybrid approach using the SMOR analysiswith the minimal number of parts@nn = split only nounsThe word frequencies required for all splitting ap-proaches were derived from the German monolin-gual language model training data (?
225 milliontokens) of the shared task of the 2009 ACL work-shop on machine translation.4.1 Linguistically Motivated Gold StandardIn the course of developing the hybrid approach,we used a hand-crafted gold standard for testing,which contains 6,187 distinct word types extracted228Correct Wrong Metricssplit not split not fty prec.
recall acc.raw 0 5073 0 1114 0 - 0.00% 81.99%cd 679 4192 883 120 313 36.21% 61.06% 78.73%sm 912 4534 541 35 165 56.37% 82.01% 88.02%sm@nn 628 4845 230 337 147 62.49% 56.73% 88.46%smc 884 4826 249 135 93 72.10% 79.50% 92.29%smc@nn 648 4981 94 380 84 78.45% 58.27% 90.98%Table 2: Linguistically motivated gold standard:6,187 distinct word types.
Bold-face font indi-cates the best result of each column.from the development set of the 2009 shared MTtask.
The most plausible split points were anno-tated by a native speaker of German, allowing forsplits into word stems or particles, but not intobound morphemes such as prefixes or suffixes.Splits were annotated at the highest wordformation level only, see also Durchschnittsautoin Figure 2 (section 3.1 above), where only thesplit point Durchschnitt|Auto would be annotatedin the gold standard.
Another example is thecomplex derivative Untersuchungsha?ftling(?person being imprisoned on remand?
),where the inherent word structure looks asfollows: [Untersuchung+Haft]+ling (?
[investi-gation+imprisonment]+being a person?).
Thesplitting into Untersuchung|Ha?ftling is semanti-cally not correct and the word is thus left unsplitin the gold standard.
Finally, particles are onlysplit if these can be used separately from the verbin a grammatically sound sentence, as is the casein the example mentioned in section 3.3, auf|gibt:Er gibt nicht auf (?he gives not up?).
In contrast,the particle cannot be separated in a past participleconstruction like aufgegeben: *Er gegeben nichtauf (?he given not up?
), because in this example,-ge- is an infix introduced between the particleand the verb in order to form the past participleform.
Constructions of this kind are thus leftunsplit in the gold standard.We found that 1,114 of the 6,187 types we in-vestigated were compounds, of which 837 werenouns.
The detailed results are given in Table 2.Due to the fact that the majority of words shouldnot be split, the raw method reaches a considerableaccuracy of 81.99%.As can be seen from Table 2, 679 of the 1,114compounds are split correctly by the corpus-drivenapproach (cd).
However, the high number ofwrong splits (883), which is the main shortcomingof the corpus-driven approach, leads to an accu-racy below the raw system (78.73% vs. 81.99%).Out of the variants of the hybrid approach,the less constrained one, sm achieves the high-est recall (82.01%), while the most constrainedone smc@nn has the highest precision (78.45%).The smc variant yields the most accurate splitting92.29%.
The higher precision of the @nn-variantscomes from the fact that most of the compoundsare nouns (837 of 1,114) and that these approaches(sm@nn, smc@nn) leave more words incorrectlyunsplit than oversplit.Note that the gold standard we presented in thissection was measured on a few times during devel-opment of the hybrid approach and there might besome danger of overfitting.
Therefore, we used an-other gold standard based on human translations toconfirm the high accuracy of the hybrid approach.We introduce it in the next section.4.2 One-to-one CorrespondenceGold StandardThe one-to-one correspondence gold standard(Koehn and Knight, 2003) indicates only com-pounds that were translated compositionally by ahuman translator.
Such translations need not al-ways be consistent: the human translator mightdecide to translate a compound compositionally inone sentence and using a different concept in an-other sentence.
As a consequence, a linguisticallycorrect split might or might not be considered cor-rect, depending on how it was translated.
This istherefore a harsh metric.We used data from the 2009 shared MT task8for this evaluation.
The first 5,000 words of thetest text (news-dev2009b) were annotated manu-ally with respect to compounds that are translatedcompositionally into more than one English word.This is the same data set as used for the evalu-ation of SMT performance in section 5, but thecompound annotation was done only after all SMTexperiments were completed, to ensure unbiasedtranslation results.
The use of the same data set fa-cilitates the comparison of the splitting approachesin terms of the one-to-one gold standard vs. trans-lation quality.The results are given in Table 3.
In this set, only155 compounds with one-to-one correspondencesare found amongst the 5,000 word tokens, whichleads to a very high accuracy of 96.90% with nosplitting (raw).8http://www.statmt.org/wmt09/translation-task.html229Correct Wrong Metricssplit not split not fty prec.
recall acc.raw 0 4,845 0 155 0 ??
0.00% 96.90%cd 81 4,435 404 14 59 14,89% 52.60% 90.32%sm 112 4,563 283 8 34 26.11% 72.73% 93.50%sm@nn 107 4,677 169 15 32 34.74% 69.48% 95.68%smc 128 4,666 180 12 14 39,75% 83,12% 95,88%smc@nn 123 4,744 102 18 13 51.68% 79.87% 97.34%Table 3: Evaluation of splitting approaches withrespect to one-to-one correspondences.
Bold-facefont indicates the best result of each column.The corpus-driven approach (cd) splits 81 of the155 compounds correctly (52.60% recall), but alsosplits 404 words that should have been left unsplit,which leads to a low precision of only 14.89%.As can be seen from Table 3, all variants of thehybrid splitting approach, reach higher accuraciesthan the corpus-driven approach, and again, themost restrictive one (smc@nn) performs best: it isthe only one that achieves a slightly higher accu-racy than raw (97.34% vs. 96.90%).
Even thoughthe number of correct splits of smc@nn (123) islower than for e.g.
smc (with 128, the highest re-call 83.12%), the number of correct not splittingsis higher (4,744 vs. 4,666).Generally speaking, the results of both goldstandards show that linguistic knowledge en-hances the number of correct splits, while at thesame time it considerably reduces oversplitting,which is the main shortcoming of the corpus-driven approach.
A detailed error analysis is pro-vided in the following section 4.3.4.3 Error Analysis4.3.1 Errors of the Corpus-Driven ApproachIn gold standard evaluation, the purely corpus-driven approach exhibited a number of erroneoussplits.
These splits are not linguistically motivatedand are thus filtered out a priori by the SMOR-based systems.
In the following, we give someexamples for wrong splits that are typical for thecorpus-driven approach.In Table 4 we divide typical errors into two cat-egories: frequency-based where wrong splitting issolely due to higher frequencies of the parts fromthe wrong splitting and insertions/deletions wherefiller letters or deletions of letters lead to wrongsplittings of which the parts are again more fre-quent than for the correct splitting.The adjective lebenstreuen (?true-to-life?)
is theonly true compound of Table 4.
Its correct splitis Leben|treuen (?life|true?).
All other words inTable 4 should be left unsplit.error type word splittingfrequency basedlebenstreuen Leben|streuentrue-to-life life|spreadtraumatisch Trauma|Tischtraumatic trauma|tableThemen the|menthemes the|meninsertions/deletionsentbrannte Ente|branntebroke out duck|burnedBelangen Bela|Genaspect Bela|geneToynbeesche toy|been|scheToynbeean toy|been|*scheTable 4: Typical errors of the corpus-driven ap-proach.
The only true compound in this table isLeben|treuen (?life|true?
).The lookup of word frequencies is done case-insensitively, i.e.
the casing variant with thehighest frequency is chosen.
This leads tocases like traumatisch (?traumatic?
), where adjec-tives are split into nominal head words (namelyTrauma|Tisch = ?trauma|table?
), which is impos-sible from a linguistic point of view.
If, how-ever, Traumatisch occurs uppercased and is thusto be interpreted as a noun, the splitting intoTrauma|Tisch is correct.The splitting accuracy of the corpus-drivenmethod is highly dependent on the quality of themonolingual training corpus from which wordfrequencies are derived.
The examples Themen(?themes?)
and Toynbeesche (?Toynbeean?)
in Ta-ble 4 show how foreign language material from alanguage like English in the training corpus canlead to severe splitting errors.In order to account for the lack of linguisticknowledge, the corpus-driven approach has to al-low for a high flexibility of filler letters, dele-tion of letters and combinations of both.
The ex-amples in the lower part of Table 4 show thatthis flexibility often leads to erroneous splits thatcompletely modify the semantic content of theoriginal word.
For example, the verb partici-ple form of ?to break out?, entbrannte is splitinto Ente|brannte (?duck|burned?
), because thecorpus-driven approach allows to add an ?e?
at theend of each but the rightmost part.
This transfor-mation is required to cover compounds like Kirch-turm (?church tower?
(or also ?steeple?))
that arecomposed of the words Kirche (?church?)
andTurm (?tower?
).Often, one high frequent part of the (possible)230compound determines the split of a word, eventhough the other part(s) are much less frequent.This is the case for Belangen (442 occurrences),where the high frequent Gen (?gene?, 1,397 oc-currences) leads to a splitting of the word, eventhough the proper name Bela is much less frequent(165 occurrences).The case of Toynbeesche (which is a propernoun used as an adjective) shows that the corpus-driven approach splits everything into parts, aslong as they are more frequent than the unsplitword.
In contrast, all words that are unknown toSMOR are left unsplit by the hybrid approach.Finally, the corpus-driven approach often iden-tifies content-free syllables such as -sche (see lastrow of Table 4) as compound parts.
These sylla-bles frequently occur in the training corpus due tosyllabification, making them a prevalent source forcorpus-driven splitting errors.
Such wrong split-tings could be blocked by extending the stopwordlist of the corpus-driven approach.
See footnote 5in section 3.2, for the list of stopwords we used inour implementation.Previous approaches to corpus-driven com-pound splitting used a part-of-speech (POS) taggerto reduce the number of erroneous analyses (e.g.
(Koehn and Knight, 2003), (Stymne, 2008)): theword class of the rightmost (possible) part of thecompound is restricted to match the word class ofthe whole compound, which is coherent to Ger-man compositional morphology.
This constraintlead to higher accuracies in gold standard evalu-ations, but it did not improve translation qualityin the experiments of Koehn and Knight (2003)and Stymne (2008), and therefore, we did not re-implement the corpus-driven approach with thisPOS-constraint.
However, some of the errors pre-sented in this section could have been prevented ifthe POS-constraint was used: the erroneous splitsof lebenstreuen and traumatisch were avoided, butfor the splittings of Belangen and entbrannte, thePOS-constraint would not help.
A more restrictivePOS-constraint proposed by Stymne (2008), al-lows splitting only into parts belonging to content-bearing word classes.
This works for Belangen,but not for entbrannte.
In the case of Themen andToynbeesche, the output of a POS-tagger for thelast part are not trustworthy, as these are not cor-rect German words: men belongs to foreign lan-guage material or it is a content-free syllable, suchas sche.4.3.2 Errors of the Hybrid ApproachDuring the development of the hybrid splittingapproach, we did an extensive gold standard eval-uation along the way, as described in section 4.1above.
The performance of the hybrid approachis limited by the performance of its constituents,namely the coverage of SMOR and the qualityof the corpus from which part frequencies arederived.
In the gold standard evaluation, wedistinguished three error categories: wrong split(should not be split but was), wrong not (shouldbe split but was not) and wrong faulty (shouldbe split, and was split, but wrongly).
Table 2 (cf.Section 4.1) contains the results of the gold stan-dard we used as development set for our approach.In Table 5, we give a detailed distribution of thewrong splittings of the less constrained hybridapproach sm, into the following categories:frequency-based: SMOR found the correct split, buta wrong split was scored higherunknown to SMOR: lexeme or rule missing in SMORlexicalized in SMOR: lexeme exists in SMOR, but fullylexicalized (no splitting possible)It can be seen from Table 5 that most of the errorsare due to corpus frequencies of the componentparts.
An example is Nachteil (?disadvantage?
),which is lexicalized in German, but can also becorrectly divided (even though it is semanticallyless plausible) into nach|Teil (?after|part?
), and asboth of these parts are high frequent, Nachteil issplit.As the corpus-driven approach uses the samedisambiguation component, there must be an over-lap of the frequency-based errors of the two ap-proaches.error typeWrongsplit not faultyfrequency-based 538 26 155unknown to SMOR 3 7 0lexicalized in SMOR 0 2 10total number of errors 541 35 165Table 5: Error analysis of sm with respect to thegold standard in Table 2 above.The remaining two categories contain errorsthat are attributed to wrong or missing analysesin SMOR.
Compared to the total number of er-rors, there are very few such errors.
Most of theunknown words are proper names or compoundswith proper names, such as Petrischale (?petridish?).
Here, the corpus-driven approach is able231to correctly the compound into Petri|Schale.There are a number of compounds in Germanthat originally consisted of two words, but arenow lexicalized.
For some of them SMOR doesnot provide any splitting option.
An example isSackgasse (?dead end street?)
which contains thewords Sack (?sack?)
and Gasse (?narrow street?
),where SMOR leaves the word unsplit (but not un-analyzed: it is encoded as one lexeme), while thecorpus-driven approach correctly splits it.5 Translation Performance5.1 System DescriptionThe Moses toolkit (Koehn et al, 2007) was usedto construct a baseline PBSMT system (with de-fault parameters), following the instructions of theshared task9.
The baseline system is Moses builtexactly as described for the shared task baseline.Contrastive systems are also built identically, ex-cept for the use of preprocessing on the Germantraining, tuning and testing data; this ensures thatall measured effects on translation quality are at-tributable to the preprocessing.
We used data fromthe EACL 2009 workshop on statistical machinetranslation10.
The data include ?1.2 million par-allel sentences for training (EUROPARL and news),1,025 sentences for tuning and 1,026 sentencesfor testing.
All data was lowercased and tok-enized, using the shared task tokenizer.
We usedthe English side of the parallel data for the lan-guage model.
As specified in the instructions, sen-tences longer than 40 words were removed fromthe bilingual training corpus, but not from the lan-guage model corpus.
The monolingual languagemodel training data (containing roughly 227 mil-lion words11) was used to derive corpus frequen-cies for the splitting approaches.For tuning of feature weights we ran Mini-mum Error Rate Training (Och, 2003) until con-vergence, individually for each system (optimiz-ing BLEU).
The experiments were evaluated usingBLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007)12.
Tuning scores are calcu-lated on lowercased, tokenized text; all test scoresare case sensitive and performed on automatically9http://www.statmt.org/wmt09/baseline.html10http://www.statmt.org/wmt09/translation-task.html11http://www.statmt.org/wmt09/training-monolingual.tar12The version of METEOR used is 0.7, we use ?exactporter-stem wn-synonmy?, weights are ?0.8 0.83 0.28?.systemtuning test testBLEU BLEU METEORraw 18.10 15.72 47.65cd 18.52 16.17 49.29sm 19.47 16.59 49.98sm@nn 19.42 16.76 49.77smc 19.53 16.63 50.13smc@nn 19.61 16.40 49.64Table 6: Effects of compound splitting:raw = without preprocessing, cd = corpus-driven,sm = hybrid approach using all SMOR analyses,smc = hybrid approach with minimal SMOR splits*@nn = split only nouns.bold-face = significant wrt.
rawunderlined = significant wrt.
cdrecapitalized, detokenized text.5.2 Translation ResultsThe BLEU and METEOR scores of our experi-ments are summarized in Table 6.
Results thatare significantly better than the baseline are bold-faced13.
Underlining indicates that a result is sig-nificantly better than corpus-driven.Compared to not-splitting (raw), the corpus-driven approach (cd) gains 0.45 BLEU points and+1.64 in METEOR for testing.
All variants of thehybrid approach (sm*) score higher than cd, reach-ing up to +0.59 BLEU compared to cd and +1.04BLEU compared to raw for sm@nn.
In terms ofMETEOR, gains of up to +0.84 compared to cd and+2.48 compared to raw are observable for smc, allof them being significant with respect to both, rawand cd.
The smc variant of the hybrid approachyielded the highest METEOR score and it was alsofound to be the most accurate one when evaluatedagainst the linguistic gold standard in section 4.1.The restriction to split only nouns (@nn) leadsto a slightly improved performance of sm (+0.17)BLEU, while METEOR is slightly worse when the@nn constraint is used: -0.21.
Despite the fact thatit had a high precision in the gold standard evalu-ation of section 4.1 above, smc, when used withthe @nn constraint, decreases in performance ver-sus smc without the constraint, because the @nnvariant leaves many compounds unsplit (cf.
row?Wrong not?, Table 2), Secion 4.1).13We used pair-wise bootstrap resampling using samplesize 1,000 and p-value 0.05, code obtained from http://www.ark.cs.cmu.edu/MT2325.3 Vocabulary Reduction ThroughCompound SplittingOne of the main issues in translating from a com-pounding and/or highly inflected language intoa morphologically less complex language is datasparsity: many source words occur very rarely,which makes it difficult to learn the correct transla-tions.
Compound splitting aims at making the vo-cabulary as small as possible but at the same timekeeping as much of the morphological informationas necessary to ensure translation quality.
Table 7shows the vocabulary sizes of our translation ex-periments, where ?types?
and ?singles?
refer tothe training data and ?unknown?
refers to the testset.
It can be seen that the vocabulary is smallestfor the corpus-driven approach (cd).
However, asthe translation experiments in the previous sectionhave shown, the cd approach was outperformed bythe hybrid approaches, despite their larger vocab-ularies.system types singles unknownraw 267,392 135,328 1,032cd 97,378 36,928 506sm 100,836 37,433 593sm@nn 130,574 51,799 644smc 109,837 39,908 608smc@nn 133,755 52,505 650Table 7: Measuring Vocabulary Reduction forCompound Splitting.6 ConclusionWe combined linguistic analysis with corpus-based statistics and obtained better results in termsof both producing splittings and statistical ma-chine translation performance.
We provided an ex-tensive analysis showing where our approach im-proves on corpus-driven splitting.We believe that our work helps to validate theutility of SMOR.
The unsupervised morphologyinduction community has already begun to evalu-ate using SMT (Viripioja et al, 2007).
Developersof high recall hand-crafted morphologies shouldalso consider statistical machine translation as auseful extrinsic evaluation.AcknowledgmentsThis work was supported by DeutscheForschungsgemeinschaft grant ?Models of Mor-phosyntax for Statistical Machine Translation?.We would like to thank Helmut Schmid.ReferencesMartin Braschler and Ba?rbel Ripplinger.
2004.
Howeffective is stemming and decompounding for Ger-man text retrieval?
Information Retrieval, 7(3-4):291?316.Chris Dyer.
2009.
Using a maximum entropymodel to build segmentation lattices for MT.
InHLT-NAACL?09: Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In EACL ?03: Pro-ceedings of the 10th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 187?193, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In ACL?07: Proceedings of the 45th AnnualMeeting of the Association for Computational Lin-guistics, Demonstration Session, pages 177?180.Martha Larson, Daniel Willett, Joachim Ko?hler, andGerhard Rigoll.
2000.
Compound splitting and lexi-cal unit recombination for improved performance ofa speech recognition system for German parliamen-tary speeches.
In ICSLP?00: Proceedings of the 6thInternational Conference on Spoken Language Pro-cessing, pages 945?948.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: Anautomatic metric for MT evaluation with high levelsof correlation with human judgements.
In ACL?07:Proceedings of the 2nd Workshop on Statistical Ma-chine Translation within the 45th Annual Meetingof the Association for Computational Linguistics,pages 228?231.Sonja Nie?en and Hermann Ney.
2000.
ImprovingSMT quality with morpho-syntactic analysis.
InCOLING?00: Proceedings of the 18th InternationalConference on Computational Linguistics, pages1081?1085.
Morgan Kaufmann.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL?03: Proceed-ings of the 41st Annual Meeting of the Associationfor Compuational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In ACL?02: Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 311?318.Maja Popovic?, Daniel Stein, and Hermann Ney.
2006.Statistical machine translation of German compound233words.
In FinTAL?06: Proceedings of the 5th Inter-national Conference on Natural Language Process-ing, pages 616?624.
Springer Verlag.Helmut Schmid, Arne Fitschen, and Ulrich Heid.2004.
Smor: A German computational morphologycovering derivation, composition and inflection.
InLREC ?04: Proceedings of the 4th Conference onLanguage Resources and Evaluation, pages 1263?1266.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49.Sara Stymne.
2008.
German compounds in factoredstatistical machine translation.
In GoTAL ?08: Pro-ceedings of the 6th International Conference on Nat-ural Language Processing, pages 464?475.
SpringerVerlag.Sami Viripioja, Jaakko J. Va?yrynen, Mathias Creutz,and Markus Sadeniemi.
2007.
Morphology-awarestatistical machine translation based on morphs in-duced in an unsupervised manner.
In MT Summit?07: Proceedings of the 11th Machine TranslationSummit, pages 491?498.234
