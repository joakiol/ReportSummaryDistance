A Global Joint Model for SemanticRole LabelingKristina Toutanova?Microsoft ResearchAria Haghighi?
?University of California BerkeleyChristopher D. Manning?Stanford UniversityWe present a model for semantic role labeling that effectively captures the linguistic intuitionthat a semantic argument frame is a joint structure, with strong dependencies among thearguments.
We show how to incorporate these strong dependencies in a statistical joint modelwith a rich set of features over multiple argument phrases.
The proposed model substantiallyoutperforms a similar state-of-the-art local model that does not include dependencies amongdifferent arguments.We evaluate the gains from incorporating this joint information on the Propbank corpus,when using correct syntactic parse trees as input, and when using automatically derived parsetrees.
The gains amount to 24.1% error reduction on all arguments and 36.8% on core argumentsfor gold-standard parse trees on Propbank.
For automatic parse trees, the error reductions are8.3% and 10.3% on all and core arguments, respectively.
We also present results on the CoNLL2005 shared task data set.
Additionally, we explore considering multiple syntactic analyses tocope with parser noise and uncertainty.1.
IntroductionSince the release of the FrameNet (Baker, Fillmore, and Lowe 1998) and Propbank(Palmer, Gildea, and Kingsbury 2005) corpora, there has been a large amount of workon statistical models for semantic role labeling.
Most of this work relies heavily on localclassifiers: ones that decide the semantic role of each phrase independently of the rolesof other phrases.However, linguistic theory tells us that a core argument frame is a joint struc-ture, with strong dependencies between arguments.
For instance, in the sentence?
One Microsoft Way, Redmond, WA 98052, USA.
E-mail: kristout@microsoft.com.??
Department of Electrical Engineering and Computer Sciences, Soda Hall, Berkeley, CA 94720, USA.E-mail: aria42@cs.berkeley.edu.?
Department of Computer Science, Gates Building 1A, 353 Serra Mall, Stanford CA 94305, USA.
E-mail:manning@cs.stanford.edu.Submission received: 15 July 2006; Revised submission received: 1 May 2007; Accepted for publication:19 June 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 2[Final-hour trading]THEME accelerated [to 108.1 million shares]TARGET [yesterday]ARGM-TMP, thefirst argument is the subject noun phrase final-hour trading of the active verb accelerated.If we did not consider the rest of the sentence, it would look more like an AGENT argu-ment, but when we realize that there is no other good candidate for a THEME argument,because to 108.1 million sharesmust be a TARGET and yesterday is most likely ARGM-TMP,we can correctly label it THEME.Even though previous work has modeled some correlations between the labels ofparse tree nodes (see Section 2), many important phenomena have not been modeled.The key properties needed to model this joint structure are: (1) no finite Markov horizonassumption for dependencies among node labels, (2) features looking at the labels ofmultiple argument nodes and internal features of these nodes, and (3) a statistical modelcapable of incorporating these long-distance dependencies and generalizing well.
Weshow how to build a joint model of argument frames, incorporating novel features intoa discriminative log-linear model.
This system achieves an error reduction of 24.1%on ALL arguments and 36.8% on CORE arguments over a state-of-the-art independentclassifier for gold-standard parse trees on Propbank.If we consider the linguistic basis for joint modeling of a verb?s arguments (includ-ing modifiers), there are at least three types of information to be captured.
The mostbasic is to limit occurrences of each kind of argument.
For instance, there is usuallyat most one argument of a verb that is an ARG0 (agent), and although some modifierroles such as ARGM-TMP can fairly easily be repeated, others such as ARGM-MNR alsogenerally occur at most once.1 The remaining two types of information apply mainly tocore arguments (the strongly selected arguments of a verb: ARG0?ARG5 in Propbank),which in most linguistic theories are modeled as belonging together in an argumentframe (set of arguments).
The information is only marginally useful for adjuncts (theARGM arguments of Propbank), which are usually treated as independent realizationalchoices not included in the argument frame of a verb.Firstly, many verbs take a number of different argument frames.
Previous workhas shown that these are strongly correlated with the word sense of the verb (Rolandand Jurafsky 2002).
If verbs were disambiguated for sense, the semantic roles ofphrases would be closer to independent given the sense of the verb.
However, be-cause in almost all semantic role labeling work (including ours), the word sense isunknown and the model conditions only on the lemma, there is much joint informa-tion between arguments when conditioning only on the verb lemma.
For example,compare:(1) Britain?s House of Commons passed a law on steroid use.
(2) The man passed the church on his way to the factory.In the first case the noun phrase after passed is an ARG1, whereas in the second case it is aARGM-LOC, with the choice governed by the sense of the verb pass.
Secondly, even withsame sense of a verb, different patterns of argument realization lead to joint informationbetween arguments.
Consider:(3) The day that the ogre cooked the children is still remembered.1 The Propbank semantic role names which we use here are defined in Section 3.162Toutanova, Haghighi, and Manning A Global Joint Model for SRL(4) The meal that the ogre cooked the children is still remembered.Despite both examples having an identical surface syntax, knowing that the ARG1 ofcook is expressed by the initial noun meal in the second example gives evidence that thechildren is the ARG2 (beneficiary), not the ARG1 in this case.Let us think of a graphical model over a set of m variables, one for each node inthe parse tree t, representing the labels of the nodes and the dependencies betweenthem.
In order for a model over these variables to capture, for example, the statisticaltendency of some semantic roles to occur at most once (e.g., that there is usually at mostone constituent labeled AGENT), there must be a dependency link between any twovariables.
To estimate the probability that a certain node gets the role AGENT, we needto know if any of the other nodes were labeled with this role.We propose such a model, with a very rich graphical model structure, which isglobally conditioned on the observation (the parse tree).2 Such a model is formallya Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001).
However,note that in practice this term has previously been used almost exclusively to describethe restricted case of linear chain Conditional Markov Random Fields (sequence mod-els) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least modelsthat have strong Markov properties, which allow efficient dynamic programming al-gorithms (Cohn and Blunsom 2005).
Instead, we consider a densely connected CRFstructure, with no Markov properties, and use approximate inference by re-ranking then-best solutions of a simpler model with stronger independence assumptions (for whichexact inference is possible).Such a rich graphical model can represent many dependencies but there are twodangers?one is that the computational complexity of training the model and search-ing for the most likely labeling given the tree can be prohibitive, and the other isthat if too many dependencies are encoded, the model will over-fit the trainingdata and will not generalize well.
We propose a model which circumvents thesetwo dangers and achieves significant performance gains over a similar local modelthat does not add any dependency arcs among the random variables.
To tackle theefficiency problem, we adopt dynamic programming and re-ranking algorithms.
Toavoid overfitting we encode only a small set of linguistically motivated dependenciesin features over sets of the random variables.
Our re-ranking approach, like the ap-proach to parse re-ranking of Collins (2000), employs a simpler model?a local semanticrole labeling algorithm?as a first pass to generate a set of n likely complete assign-ments of labels to all parse tree nodes.
The joint model is restricted to these n assign-ments and does not have to search the exponentially large space of all possible jointlabelings.2.
Related WorkThere has been a substantial amount of work on automatic semantic role labeling,starting with the statistical model of Gildea and Jurafsky (2002).
Researchers haveworked on defining new useful features, and different system architectures andmodels.Here we review the work most closely related to ours, concentrating on methods forincorporating joint information and for increasing robustness to parser error.2 That is, it defines a conditional distribution of labels of all nodes given the parse tree.163Computational Linguistics Volume 34, Number 22.1 Methods for Incorporating Joint InformationGildea and Jurafsky (2002) propose a method to model global dependencies by includ-ing a probability distribution over multi-sets of semantic role labels given a predicate.In this way the model can consider the assignment of all nodes in the parse tree andevaluate whether the set of realized semantic roles is likely.
If a necessary role is missingor if an unusual set of arguments is assigned by the local model, this additional factorcan correct some of the mistakes.
The distribution over label multi-sets is estimatedusing interpolation of a relative frequency and a back-off distribution.
The back-offdistribution assumes each argument label is present or absent independently of theother labels, namely, it assumes a Bernoulli Naive Bayes model.The most likely assignment of labels according to such a joint model is found ap-proximately using re-scoring of the top k = 10 assignments according to a local model,which does not include dependencies among arguments.
Using this model improvesthe performance of the system in F-measure from 59.2 to 62.85.
This shows that addingglobal information improves the performance of a role labeling system considerably.However, the type of global information in this model is limited to label multi-sets.We will show that much larger gains are possible from joint modeling, adding richersources of joint information using a more flexible statistical model.The model of Pradhan, Hacioglu, et al (2004, 2005) is a state-of-the-art model, basedon Support Vector Machines, and incorporating a large set of structural and lexicalfeatures.
At the heart of the model lies a local classifier, which labels each parse treenode with one of the possible argument labels or NONE.
Joint information is integratedinto the model in two ways:Dynamic class context: Using the labels of the two nodes to the left as features forclassifying the current node.
This is similar to the Conditional Markov Models (CMM)often used in information extraction (McCallum, Freitag, and Pereira 2000).
Noticethat here the previous two nodes classified are not in general the previous two nodesassigned non-NONE labels.
If a linear order on all nodes is imposed, then the previoustwo nodes classified most likely bear the label NONE.Language model lattice re-scoring: Re-scoring of an N-best lattice with a trigramlanguage model over semantic role label sequences.
The target predicate is also part ofthe sequence.These ways of incorporating joint information resulted in small gains over a base-line system using only the features of Gildea and Jurafsky (2002).
The performancegain due to joint information over a system using all features was not reported.
Thejoint information captured by this model is limited by the n-gram Markov assumptionof the language model over labels.
In our work, we improve the modeling of jointdependencies by looking at longer-distance context, by defining richer features overthe sequence of labels and input features, and by estimating the model parametersdiscriminatively.A system which can integrate longer-distance dependencies is that of Punyakanoket al (2004) and Punyakanok, Roth, and Yih (2005).
The idea is to build a semantic rolelabeling system that is based on local classifiers but also uses a global component thatensures that several linguistically motivated global constraints on argument framesare satisfied.
The constraints are categorical and specified by hand.
For example, oneglobal constraint is that the argument phrases cannot overlap?that is, if a node islabeled with a non-NONE label, all of its descendants have to be labeled NONE.
Theproposed framework is integer linear programming (ILP), which makes it possibleto find the most likely assignment of labels to all nodes of the parse tree subject to164Toutanova, Haghighi, and Manning A Global Joint Model for SRLspecified constraints.
Solving the ILP problem is NP-hard but it is very fast in practice(Punyakanok et al 2004).
The authors report substantial gains in performance dueto these global consistency constraints.
This method was applied to improve theperformance both of a system based on labeling syntactic chunks and one based onlabeling parse tree nodes.
Our work differs from that work in that our constraints arenot categorical (either satisfied or not), but are rather statistical preferences, and thatthey are learned automatically based on features specified by the knowledge engineer.On the other hand, we solve the search/estimation problem through re-ranking andn-best search only approximately, not exactly.So far we have mainly discussed systems which label nodes in a parse tree.
Manysystems that only use shallow syntactic information have also been presented (Hacioglu2004; Punyakanok et al 2004); using full syntactic parse information was not allowed inthe CoNLL 2004 shared task on Semantic Role Labeling and description of such systemscan be found in (Carreras and Ma`rquez 2004).
Most systems which use only shallowsyntactic information represent the input sentence as a sequence of tokens (words orphrases), which they label with a BIO tagging representation (beginning, inside, andoutside argument labels) (Hacioglu 2004).
Limited joint information is used by such sys-tems, provided as a fixed size context of tags on previous tokens; for example, a lengthfive window is used in the chunk-based system in (Pradhan, Hacioglu et al 2005).A method that models joint information in a different way was proposed by Cohnand Blunsom (2005).
It uses a tree-structured CRF, where the statistical dependencystructure is exactly defined by the edges in the syntactic parse tree.
The only depen-dencies captured are between the label of a node and the label of each of its chil-dren.
However, the arguments of a predicate can be arbitrarily far from each otherin the syntactic parse tree and therefore a tree-CRF model is limited in its ability tomodel dependencies among different arguments.
For instance, the dependency betweenthe meal and the children for the sentence in example (4) will not be captured becausethese phrases are not in the same local tree according to Penn Treebank syntax.2.2 Increasing Robustness to Parser ErrorThere have been multiple approaches to reducing the sensitivity of semantic rolelabeling systems to syntactic parser error.
Promising approaches have been to considermultiple syntactic analyses?the top k parses from a single or multiple full parsers(Punyakanok, Roth, and Yih 2005), or a shallow parse and a full parse (Ma`rquez et al2005; Pradhan et al 2005), or several types of full syntactic parses (Pradhan, Wardet al 2005).
Such techniques are important for achieving good performance: The topfour systems in the CoNLL 2005 shared task competition all used multiple syntacticanalyses (Carreras and Ma`rquez 2005).These previous methods develop special components to combine the labeling deci-sions obtained using different syntactic annotation.
The method of Punyakanok, Roth,and Yih (2005) uses ILP to derive a consistent set of arguments, each of which couldbe derived using a different parse tree.
Pradhan, Ward et al (2005) use stacking totrain a classifier which combines decisions based on different annotations, andMa`rquezet al (2005) use special-purpose filtering and inference stages which combine argumentsproposed by systems using shallow and full analyses.Our approach to increasing robustness uses the top k parses from a single parserand is a simple general method to factor in the uncertainty of the parser by apply-165Computational Linguistics Volume 34, Number 2ing Bayesian inference.
It is most closely related to the method described in Finkel,Manning, and Ng (2006) and can be seen as an approximation of that method.We describe our system in detail by first introducing simpler local semantic rolelabeling models in Section 4, and later building on them to define joint models in Sec-tion 5.
Before we start presenting models, we describe the data and evaluation measuresused in Section 3.
Readers can skip the next section and continue on to Section 4 if theyare not interested in the details of the evaluation.3.
Data and Evaluation Measures3.1 DataFor most of our experiments we used the February 2004 release of Propbank.
We alsoreport results on the CoNLL 2005 shared task data (Propbank I) in Section 6.2.
Forthe latter, we used the standard CoNLL evaluation measures, and we refer readersto the description of that task for details of the evaluation (Carreras and Ma`rquez2005).
In this section we describe the data and evaluation measures we used for theFebruary 2004 data.
We use our own set of measures on the February 2004 data forthree reasons.
Firstly, we wish to present a richer set of measures, which can betterillustrate the performance of the system on core arguments as against adjuncts andthe performance on identifying versus classifying arguments.
Secondly, we technicallycould not use the CoNLL measure on the February 2004 data, because this earlierdata was not available in a format which specifies which arguments should have theadditional R-ARGX labels used in the CoNLL evaluation.3 Finally, these measures arebetter for comparison with early papers, because most research before 2005 did notdistinguish referring arguments.
We describe our argument-based measures in detailhere in case researchers are interested in replicating our results for the February 2004data.For the February 2004 data, we used the standard split into training, development,and test sets?the annotations from sections 02?21 formed the training set, section 24 thedevelopment, and section 23 the test set.
The set of argument labels considered is theset of core argument labels (ARG0 through ARG5) plus the modifier labels (see Figure 1).The training set contained 85,392 propositions, the test set 4,615, and the developmentset 2,626.We evaluate semantic role labeling models on gold-standard parse trees and parsetrees produced by Charniak?s automatic parser (Charniak 2000).
For gold-standardparse trees, we preprocess the trees to discard empty constituents and strip functionaltags.
Using the trace information provided by empty constituents is very useful forimproving performance (Palmer, Gildea, and Kingsbury 2005; Pradhan, Ward et al2005), but we have not used this information so that we can compare our results toprevious work and since automatic systems that recover it are not widely available.3.2 Evaluation MeasuresSince 2004, there has been a precise, standard evaluation measure for semantic role la-beling, formulated by the organizers of the CoNLL shared tasks (Carreras and Ma`rquez3 Currently, a script which can convert original Propbank annotations to CoNLL format is available as partof the CoNLL software distribution.166Toutanova, Haghighi, and Manning A Global Joint Model for SRLFigure 1Labels of modifying arguments occurring in Propbank.2004, 2005).
An evaluation script is also distributed as part of the provided software forthe shared task and can be used to evaluate systems on Propbank I data.For papers published between 2000 and 2005, there are several details of the eval-uation measures for semantic role labeling that make it difficult to compare resultsobtained by different researchers, because researchers use their own implementationsof evaluation measures, without making all the exact details clear in their papers.
Thefirst issue is the existence of arguments consisting of multiple constituents.
In this caseit is not clear whether partial credit is to be given for guessing only some of the con-stituents comprising the argument correctly.
The second issue is whether the bracketingof constituents should be required to be recovered correctly, in other words, whetherpairs of labelings, such as [the]ARG0 [man]ARG0 and [the man]ARG0 are to be considered thesame or not.
If they are considered the same, there are multiple labelings of nodes ina parse tree that are equivalent.
The third issue is that when using automatic parsers,some of the constituents that are fillers of semantic roles are not recovered by the parser.In this case it is not clear how various research groups have scored their systems (usingheadword match, ignoring these arguments altogether, or using exact match).
If wevary the choice taken for these three issues, we can come up with many (at least eight)different evaluationmeasures, and these details are important, because different choicescan lead to rather large differences in reported performance.Here we describe in detail our evaluation measures for the results on the February2004 data reported in this article.
The measures are similar to the CoNLL evaluationmeasure, but report a richer set of statistics; the exact differences are discussed at theend of this section.For both gold-standard and automatic parses we use one evaluation measure,which we call argument-based evaluation.
To describe the evaluation measure, we willuse as an example the correct and guessed semantic role labelings shown in Figures 2(a)and 2(b).
Both are shown as labelings on parse tree nodes with labels of the form ARGXand C-ARGX.
The label C-ARGX is used to represent multi-constituent arguments.
A con-stituent labeled C-ARGX is assumed to be a continuation of the closest constituent to theleft labeled ARGX.
Our semantic role labeling system produces labelings of this form andthe gold standard Propbank annotations are converted to this form as well.4 The evalua-tion is carried out individually for each predicate and its associated argument frame.
If asentence contains several clauses, the several argument frames are evaluated separately.4 This representation is not powerful enough to represent all valid labelings of multi-constituentarguments, because it cannot represent the case where a new argument with label ARGX starts beforea previous multi-constituent argument with the same label ARGX has finished.
This case, however, isvery rare.167Computational Linguistics Volume 34, Number 2Our argument-based measures do not require exact bracketing (if the set of wordsconstituting an argument is correct, there is no need to know how this set is brokeninto constituents) and do not give partial credit for labeling correctly only some ofseveral constituents in a multi-constituent argument.
They are illustrated in Figure 2.For these measures, a semantic role labeling of a sentence is viewed as a labeling on setsof words.
These sets can encompass several non-contiguous spans.
Figure 2(c) gives therepresentation of the correct and guessed labelings shown in Figures 2(a) and 2(b), in thefirst and second rows of the table, respectively.
To convert a labeling on parse tree nodesto this form, we create a labeled set for each possibly multi-constituent argument.
Allremaining sets of words are implicitly labeled with NONE.
We can see that, in this way,exact bracketing is not necessary and also no partial credit is given when only some ofseveral constituents in a multi-constituent argument are labeled correctly.We will refer to word sets as ?spans.?
To compute the measures, we are comparinga guessed set of labeled spans to a correct set of labeled spans.
We briefly define thevarious measures of comparison used herein, using the example guessed and correctFigure 2Argument-based scoring measures for the guessed labeling.168Toutanova, Haghighi, and Manning A Global Joint Model for SRLlabelings shown in Figure 2(c).
All spans not listed explicitly are assumed to have labelNONE.
The scoring measures are illustrated in Figure 2(d).The figure shows performance measures?F-Measure (F1) and Whole Frame Accu-racy (Acc.
)?across nine different conditions.
When the sets of labeled spans are com-pared directly, we obtain the complete taskmeasures, corresponding to the ID&CLS rowand ALL column in Figure 2(d).
We also define several other measures to understand theperformance of the system on different types of labels.
We measure the performance onidentification (ID), classification (CLS), and the complete task (ID&CLS), when consider-ing only the core arguments (CORE), all arguments but with a single ARGM label for themodifier arguments (COARSEARGM), and all arguments (ALL).
This defines nine sub-tasks,which we now describe.
For each of them, we compute the Whole Frame Accuracy andF-Measure as follows:Whole Frame Accuracy (Acc.).
This is the percentage of propositions for whichthere is an exact match between the proposed and correct labelings.
For example, thewhole frame accuracy for ID&CLS and ALL is 0, because the correct and guessed setsof labeled spans shown in Figure 2(c) do not match exactly.
In the figures, ?Acc.?
isalways an abbreviation for this whole frame accuracy.
Even though this measure hasnot been used extensively in previous work, we find it useful to track.
Most importantly,potential applications of role labeling may require correct labeling of all (or at least thecore) arguments in a sentence in order to be effective, and partially correct labelings maynot be very useful.
Moreover, a joint model for semantic role labeling optimizes WholeFrame Accuracy more directly than a local model does.F-Measure (F1).
Because there may be confusion about what wemean by F-Measurein this multi-class setting, we define it here.
F-Measure is defined as the harmonic meanof precision and recall: f = 2?p?rp+r ; p =true positivetrue positive+false positive ; r =true positivetrue positve+false negative .This formula uses the number of true positive, false positive, and false nega-tive spans in a given guessed labeling.
True positive is the number of spans whose correctlabel is one of the core or modifier argument labels (not NONE) and whose guessed labelis the same as the correct label.
False positive is the number of spans whose guessedlabel is non-NONE and whose correct label is different from the guessed label (possiblyNONE).
False negative is the number of spans whose correct label is non-NONE andwhoseguessed label is not the same as the correct one (possibly NONE).
In the figures in thispaper we show F-Measure multiplied by 100 so that it is in the same range as WholeFrame Accuracy.Core Argument Measures (CORE).
These measures score the system on corearguments only, without regard to modifier arguments.
They can be obtained by firstmapping all non-core argument labels in the guessed and correct labelings to NONE.Coarse Modifier Argument Measures (COARSEARGM).
Sometimes it is sufficient toknow a given span has a modifier role, without knowledge of the specific role label.
Inaddition, deciding exact modifier argument labels was one of the decisions with highestdisagreement among annotators (Palmer, Gildea, and Kingsbury 2005).
To estimateperformance under this setting, we relabel all ARGM-X arguments to ARGM in theproposed and correct labeling.
Such a performance measure was also used by Xue andPalmer (2004).
Note that these measures do not exclude the core arguments but insteadconsider the core plus a coarse version of the modifier arguments.
Thus for COARSEARGM169Computational Linguistics Volume 34, Number 2ALL we count {0} as a true positive span, {1, 2} , {3, 4}, and {7, 8, 9} as false positive,and {1, 2, 3, 4} and {7, 8, 9} as false negative.Identification Measures (ID).
These measure how well we do on the ARG vs. NONEdistinction.
For the purposes of this evaluation, all spans labeled with a non-NONE labelare considered to have the generic label ARG.
For example, to compute CORE ID, wecompare the following sets of labeled spans:Correct: {0}-ARG, {7, 8, 9}-ARGGuessed: {0}-ARG, {7, 8, 9}-ARGThe F-Measure is 1.0 and the Whole Frame Accuracy is 100%.Classification Measures (CLS).
These are performance on argument spans whichwere also guessed to be argument spans (but possibly the exact label was wrong).
Inother words, thesemeasures ignore the ARG vs. NONE confusions.
They ignore all spans,which were incorrectly labeled NONE, or incorrectly labeled with an argument label,when the correct label was NONE.
This is different from ?classification accuracy?
usedin previous work to mean the accuracy of the system in classifying spans when thecorrect set of argument spans is given.
To compute CLS measures, we remove all spansfrom Sguessed and Scorrect that do not occur in both sets, and compare the resulting sets.For example, to compute the ALL CLS measures, we need to compare the following setsof labeled spans:Correct: {0}-ARG0, {7, 8, 9}-ARG2Guessed: {0}-ARG0, {7, 8, 9}-ARG3The rest of the spans were removed from both sets because they were labeled NONEaccording to one of the labelings and non-NONE according to the other.
The F-Measureis .50 and the Whole Frame Accuracy is 0%.As we mentioned before, we label and evaluate the semantic frame of everypredicate in the sentence separately.
It is possible for a sentence to contain severalpropositions?annotations of predicates occurring in the sentence.
For example, in thesentence The spacecraft faces a six-year journey to explore Jupiter, there are two propositions,for the verbs faces and explore.
These are:[The spacecraft]ARG0 [faces]PRED [a six-year journey to explore Jupiter]ARG1.
[The spacecraft]ARG0 faces a six-year journey to [explore]PRED [Jupiter]ARG1.Our evaluation measures compare the guessed and correct set of labeled spans for eachproposition.3.3 Relation to the CoNLL Evaluation MeasureThe CoNLL evaluation measure (Carreras and Ma`rquez 2004, 2005) is almost the sameas our argument-based measure.
The only difference is that the CoNLL measure intro-duces an additional label type for arguments, of the form R-ARGX, used for referring ex-170Toutanova, Haghighi, and Manning A Global Joint Model for SRLpressions.
The Propbank distribution contains a specification of whichmulti-constituentarguments are in a coreference chain.
The CoNLL evaluation script considers thesemulti-constituent arguments as several separate arguments having different labels,where one argument has an ARGX label and the others have R-ARGX labels.
The decisionof which constituents were to be labeled with referring labels was made using a set ofrules expressed with regular expressions.5 A script that converts Propbank annotationsto CoNLL format is available as part of the shared task software.For example, in the following sentence, the CoNLL specification annotates thearguments of began as follows:[The deregulation]ARG1 of railroads [that]R-ARG1 [began]PRED enabled shippers to bargainfor transportation.In contrast, we treat all multi-constituent arguments in the same way, and do notdistinguish coreferential versus non-coreferential split arguments.
According to ourargument-based evaluation, the annotation of the arguments of the verb began is:[The deregulation]ARG1 of railroads [that]C-ARG1 [began]PRED enabled shippers to bargainfor transportation.The difference between our argument based measure and the CoNLL evaluationmeasure is such that we cannot say that the value of one is always higher than the valueof the other.
Either measure could be higher depending on the kinds of errors made.For example, if the guessed labeling is: [The deregulation]ARG0 of railroads [that]R-ARG1[began]PRED enabled shippers to bargain for transportation, the CoNLL script wouldcount the argument that as correct and report precision and recall of .5, whereas ourargument-based measure would not count any argument correct and report precisionand recall of 0.
On the other hand, if the guessed labeling is [The deregulation]ARG1 ofrailroads [that]C-ARG1 [began]PRED enabled shippers to bargain for transportation, the CoNLLmeasure would report a precision and recall of 0, whereas our argument-based measurewould report precision and recall of 1.
If the guessed labeling is [The deregulation]ARG1of railroads [that]R-ARG1 [began]PRED enabled shippers to bargain for transportation, bothmeasures would report precision and recall of 1.
(For our argument-based measureit does not make sense to propose R-ARGX labels and we assume such labels wouldbe converted to C-ARGX labels if they are after the phrase they refer to.)
Nevertheless,overall we expect the two measures to yield very similar results.4.
Local ClassifiersA classifier is local if it assigns a probability (or score) to the label of an individual parsetree node ni independently of the labels of other nodes.In defining our models, we use the standard separation of the task of semantic rolelabeling into identification and classification phases.
Formally, let L denote a mappingof the nodes in a tree t to a label set of semantic roles (including NONE) with respect toa predicate v. Let Id(L) be the mapping which collapses L?s non-NONE values into ARG.5 The regular expressions look for phrases containing pronouns with part-of-speech tags WDT, WRB, WP,or WP$ (Xavier Carreras, personal communication).171Computational Linguistics Volume 34, Number 2Then, like the Gildea and Jurafsky (2002) system, we decompose the probability of alabeling L into probabilities according to an identification model PID and a classificationmodel PCLS.PSRL(L|t, v) = PID(Id(L)|t, v)PCLS(L|t, v, Id(L)) (1)This decomposition does not encode any independence assumptions, but is a use-ful way of thinking about the problem.
Our local models for semantic role labelinguse this decomposition.
We use the same features for local identification and classi-fication models, but use the decomposition for efficiency of training.
The identifica-tion models are trained to classify each node in a parse tree as ARG or NONE, andthe classification models are trained to label each argument node in the training setwith its specific label.
In this way the training set for the classification models issmaller.
Note that we do not do any hard pruning at the identification stage in testingand can find the exact labeling of the complete parse tree, which is the maximizer ofEquation (1).We use log-linear models for multi-class classification for the local models.
Becausethey produce probability distributions, identification and classification models can bechained in a principled way, as in Equation (1).
The baseline features we used for thelocal identification and classification models are outlined in Figure 3.
These features area subset of the features used in previous work.
The standard features at the top of thefigure were defined by Gildea and Jurafsky (2002), and the rest are other useful lexicaland structural features identified in more recent work (Surdeanu et al 2003; Pradhanet al 2004; Xue and Palmer 2004).
We also incorporated several novel features whichwe describe next.Figure 3Baseline features.172Toutanova, Haghighi, and Manning A Global Joint Model for SRLFigure 4Example of displaced arguments.4.1 Additional Features for Displaced ConstituentsWe found that a large source of errors for ARG0 and ARG1 stemmed from cases suchas those illustrated in Figure 4, where arguments were dislocated by raising or controlverbs.
Here, the predicate, expected, does not have a subject in the typical position?indicated by the empty NP?because the auxiliary is has raised the subject to its currentposition.
In order to capture this class of examples, we use a binary feature, MISSINGSUBJECT, indicating whether the predicate is ?missing?
its subject, and use this featurein conjunction with the PATH feature, so that we learn typical paths to raised subjectsconditioned on the absence of the subject in its typical position.6In the particular case of Figure 4, there is another instance of an argument beingquite far from its predicate.
The predicate widen shares the phrase the trade gap withexpect as an ARG1 argument.
However, as expect is a raising verb, widen?s subject is notin its typical position either, and we should expect to find it in the same position asexpected?s subject.
This indicates it may be useful to use the path relative to expectedto find arguments for widen.
In general, to identify certain arguments of predicatesembedded in auxiliary and infinitival VPs we expect it to be helpful to take the pathfrom the maximum extended projection of the predicate?the highest VP in the chainof VPs dominating the predicate.
We introduce a new path feature, PROJECTED PATH,which takes the path from the maximal extended projection to an argument node.
Thisfeature applies only when the argument is not dominated by the maximal projection(e.g., direct objects).
These features also handle other cases of discontinuous and non-local dependencies, such as those arising due to control verbs.
The performance gainfrom these new features was notable, especially in identification.
The performance onALL arguments for the model using only the features in Figure 3, and the model usingthe additional features as well, are shown in Figure 5.
For these results, the constraintthat argument phrases do not overlap was enforced using the algorithm presented inSection 4.2.4.2 Enforcing the Non-Overlapping ConstraintThe most direct way to use trained local identification and classification models intesting is to select a labeling L of the parse tree that maximizes the product of the6 We consider a verb to be missing its subject if the highest VP in the chain of VPs dominating the verbdoes not have an NP or S(BAR) as its immediate left sister.173Computational Linguistics Volume 34, Number 2Figure 5Performance of local classifiers on ALL arguments, using the features in Figure 3 only and usingthe additional local features.
Using gold standard parse trees on Section 23.probabilities according to the two models, as in Equation (1).
Because these models arelocal, this is equivalent to independently maximizing the product of the probabilities ofthe twomodels for the label li of each parse tree node ni as shown below in Equation (2).PSRL(L|t, v) =?ni?tPID(Id(li)|t, v)?ni?tPCLS(li|t, v, Id(li)) (2)A problem with this approach is that a maximizing labeling of the nodes could possiblyviolate the constraint that argument nodes should not overlap with each other.
There-fore, to produce a consistent set of arguments with local classifiers, we must have a wayof enforcing the non-overlapping constraint.When labeling parse tree nodes, previous work has either used greedy algorithmsto find a non-overlapping assignment, or the general-purpose ILP approach ofPunyakanok et al (2004).
For labeling chunks an exact algorithm based on shortestpaths was proposed in Punyakanok and Roth (2001).
Its complexity is quadratic in thelength of the sentence.Here we describe a faster exact dynamic programming algorithm to find the mostlikely non-overlapping (consistent) labeling of all nodes in the parse tree, accordingto a product of probabilities from local models, as in Equation (2).
For simplicity, wedescribe the dynamic program for the case where only two classes are possible: ARG andNONE.
The generalization to more classes is straightforward.
Intuitively, the algorithmis similar to the Viterbi algorithm for context-free grammars, because we can describethe non-overlapping constraint by a ?grammar?
that disallows ARG nodes having ARGdescendants.Subsequently, we will talk about maximizing the sum of the logs of local proba-bilities rather than the product of local probabilities, which is equivalent.
The dynamicprogram works from the leaves of the tree up and finds a best assignment for eachsubtree, using already computed assignments for its children.
Suppose we want themost likely consistent assignment for subtree t with child trees t1, .
.
.
, tk each storingthe most likely consistent assignment of its nodes, as well as the log-probability of theALLNONE assignment: the assignment of NONE to all nodes in the tree.
The most likelyassignment for t is the one that corresponds to the maximum of: The sum of the log-probabilities of the most likely assignments of the childsubtrees t1, .
.
.
, tk plus the log-probability of assigning the node t to NONE. The sum of the log-probabilities of the ALLNONE assignments of t1, .
.
.
, tkplus the log-probability of assigning the node t to ARG.174Toutanova, Haghighi, and Manning A Global Joint Model for SRLFigure 6Performance of local model on ALL arguments when enforcing the non-overlapping constraintor not.The log-probability of the ALLNONE assignment for a tree t is the log-probabilityof assigning the root node of t to NONE plus the sum of the log-probabilities of theALLNONE assignments of the child subtrees of t.Propagating this procedure from the leaves to the root of t we have our mostlikely non-overlapping assignment.
By slightly modifying this procedure, we obtain themost likely assignment according to a product of local identification and classificationmodels.
We use the local models in conjunction with this search procedure to select amost-likely labeling in testing.The complexity of this algorithm is linear in the number of nodes in the parse tree,which is usually much less than the square of the number of words in the sentence (l2),the complexity of the Punyakanok and Roth (2001) algorithm.
For example, for a binary-branching parse tree, the number of nodes is approximately 2l.
The speedup is due tothe fact that when we label parse tree nodes, we make use of the bracketing constraintsimposed by the parse tree.
The shortest path algorithm proposed by Punyakanok andRoth can also be adapted to achieve this lower computational complexity.It turns out that enforcing the non-overlapping constraint does not lead to largegains in performance.
The results in Figure 5 are frommodels that use the dynamic pro-gram for selecting non-overlapping arguments.
To evaluate the gain from enforcing theconstraint, Figure 6 shows the performance of the same local model using all features,when the dynamic program is used versus when a most likely possibly overlappingassignment is chosen in testing.The local model with basic plus additional features is our first pass model usedin re-ranking.
The non-overlapping constraint is enforced using the dynamic program.This is a state-of-the-art model.
Its F-Measure on ALL arguments is 88.4 according to ourargument-based scoring measure.
This is very similar to the best reported results (as of2004) using gold-standard parse trees without null constituents and functional tags: 89.4F-Measure reported for the Pradhan et al (2004) model.7A more detailed analysis of the results obtained by the local model is given in Fig-ure 7(a), and the two confusion matrices in Figures 7(b) and 7(c), which display thenumber of errors of each type that the model made.
The first confusion matrix con-centrates on CORE arguments and merges all modifying argument labels into a singleARGM label.
The second concentrates on confusions among modifying arguments.From the confusion matrix in Figure 7(b), we can see that the largest number oferrors are confusions of argument labels with NONE.
The number of confusions betweenpairs of core arguments is low, as is the number of confusions between core andmodifierlabels.
If we ignore the column and row corresponding to NONE in Figure 7(b), thenumber of off-diagonal entries is very small.
This corresponds to the high F-Measures7 The results in Pradhan et al (2004) are based on a measure which is more lenient than ourargument-based scoring (Sameer Pradhan, personal communication, July 2005).
Our estimatedperformance using his measure is 89.9.175Computational Linguistics Volume 34, Number 2Figure 7Performance measures for local model using all local features and enforcing thenon-overlapping constraint.
Results are on Section 23 using gold standard parse trees.on COARSEARGM CLS and CORE CLS, 98.1 and 98.0 respectively, shown in Figure 7(a).The number of confusions of argument labels with NONE, shown in the NONE column,is larger than the number of confusions of NONE with argument labels, shown in theNONE row.
This shows that the model generally has higher precision than recall.
Weexperimented with the precision?recall tradeoff but this did not result in an increase inF-Measure.From the confusion matrix in Figure 7(c) we can see that the number of confusionsbetween modifier argument labels is higher than the number of confusions betweencore argument labels.
This corresponds to the ALL CLS F-Measure of 95.7 versus theCORE CLS F-Measure of 98.0.
The per-label F-Measures in the last column show that theperformance on some very frequent modifier labels is in the low sixties or seventies.The confusions between modifier labels and NONE are quite numerous.Thus, to improve the performance on CORE arguments, we need to improve recallwithout lowering precision.
In particular, when the model is uncertain which of severallikely CORE labels to assign, we need to find additional sources of evidence to improveits confidence.
To improve the performance on modifier arguments, we also need tolower the confusions among different modifier arguments.
We will see that our jointmodel improves the overall performance mainly by improving the performance on176Toutanova, Haghighi, and Manning A Global Joint Model for SRLCORE arguments, through increasing recall and precision by looking at wider sentencecontext.4.3 On Split ConstituentsAs discussed in Section 3, multiple constituents can be part of the same semanticargument as specified by Propbank.
An automatic system that has to recover suchinformation needs to have a way of indicating when multiple constituents labeled withthe same semantic role are a part of the same argument.
Some researchers (Pradhan et al2004; Punyakanok et al 2004) have chosen to make labels of the form C-ARGX distinctargument labels that become additional classes in a multi-class constituent classifier.These C-ARGX are used to indicate continuing arguments as illustrated in the two treesin Figure 2.
We chose to not introduce additional labels of this form, because they mightunnecessarily fragment the training data.
Our automatic classifiers label constituentswith one of the core or modifier semantic role labels, and a simple post-processingrule is applied to the output of the system to determine which constituents that arelabeled the same are to be merged as the same argument.
The post-processing rule isthe following: For every constituent that bears a core argument label ARGX, if there isa preceding constituent with the same label, re-label the current constituent C-ARGX.Therefore, according to our algorithm, all constituents having the same core argumentlabel are part of the same argument, and all constituents having the samemodifier labelsare separate arguments by themselves.
This rule is fairly accurate for core arguments butis not always correct; it fails more often on modifier arguments.
An evaluation of thisrule using the CoNLL data set and evaluation measure shows that our upper bound inperformance because of this rule is approximately 99.0 F-Measure on ALL arguments.5.
Joint ClassifiersWe proceed to describe our models incorporating dependencies between labels of nodesin the parse tree.
As we discussed briefly before, the dependencies we would like tomodel are highly non-local.
A factorized sequence model that assumes a finite Markovhorizon, such as a chain CRF (Lafferty, McCallum, and Pereira 2001), would not beable to encode such dependencies.
We define a CRF with a much richer dependencystructure.5.1 Form of the Joint ClassifiersMotivation for Re-Ranking.
For argument identification, the number of possible as-signments for a parse tree with n nodes is 2n.
This number can run into the hundredsof billions for a normal-sized tree.
For argument labeling, the number of possibleassignments is ?
20m, if m is the number of arguments of a verb (typically between2 and 5), and 20 is the approximate number of possible labels if considering both coreand modifying arguments.
Training a model which has such a huge number of classesis infeasible if the model does not factorize due to strong independence assumptions.Therefore, in order to be able to incorporate long-range dependencies in our models,we chose to adopt a re-ranking approach (Collins 2000), which selects from likely as-signments generated by a model which makes stronger independence assumptions.
Weutilize the top n assignments of our local semantic role labeling model PSRL to generatelikely assignments.
As can be seen from Figure 8(a), for relatively small values of n, our177Computational Linguistics Volume 34, Number 2re-ranking approach does not present a serious bottleneck to performance.
We used avalue of n = 10 for training.
In Figure 8(a) we can see that if we could pick, using anoracle, the best assignment out of the top 10 assignments according to the local model,we would achieve an F-Measure of 97.3 on all arguments.
Increasing the number of n to30 results in a very small gain in the upper bound on performance and a large increasein memory requirements.
We therefore selected n = 10 as a good compromise.Generation of top n Most Likely Joint Assignments.
We generate the top n mostlikely non-overlapping joint assignments of labels to nodes in a parse tree accordingto a local model PSRL, using an exact dynamic programming algorithm, which is adirect generalization of the algorithm for finding the top non-overlapping assignmentdescribed in Section 4.2.Parametric Models.We learn log-linear re-ranking models for joint semantic role label-ing, which use feature maps from a parse tree and label sequence to a vector space.
Theform of the models is as follows.
Let ?
(t, v,L) ?
Rs denote a feature map from a tree t,target verb v, and joint assignment L of the nodes of the tree, to the vector space Rs.
LetL1,L2, ?
?
?
,LN denote the top N possible joint assignments.
We learn a log-linear modelwith a parameter vectorW, with one weight for each of the s dimensions of the featurevector.
The probability (or score) of an assignment L according to this re-ranking modelis defined asPrSRL(L|t, v) =e<?
(t,v,L),W>?Nj=1 e<?
(t,v,Lj ),W>(3)The score of an assignment L not in the top n is zero.
We train the model to maximize thesum of log-likelihoods of the best assignments minus a quadratic regularization term.In this framework, we can define arbitrary features of labeled trees that capture generalproperties of predicate?argument structure.5.2 Joint Model FeaturesWewill introduce the features of the joint re-rankingmodel in the context of the exampleparse tree shown in Figure 9.
We model dependencies not only between the label of aFigure 8Oracle upper bounds for top n non-overlapping assignments from local model on CORE and ALLarguments, using gold-standard parse trees.178Toutanova, Haghighi, and Manning A Global Joint Model for SRLFigure 9An example tree from Propbank with semantic role annotations, for the sentence Final-hourtrading accelerated to 108.1 million shares yesterday.node and the labels of other nodes, but also dependencies between the label of a nodeand input features of other argument nodes.
The features are specified by instantiationof templates and the value of a feature is the number of times a particular pattern occursin the labeled tree.For a tree t, predicate v, and joint assignment L of labels to the nodes of the tree, wedefine the candidate argument sequence as the sequence of non-NONE labeled nodes[n1, l1, .
.
.
, vPRED, .
.
.
,nm, lm] (li is the label of node ni).
A reasonable candidate argumentsequence usually contains very few of the nodes in the tree?about 2 to 7?as this isthe typical number of arguments for a verb.
To make it more convenient to expressour feature templates, we include the predicate node v in the sequence.
This sequenceof labeled nodes is defined with respect to the left-to-right order of constituents in theparse tree.
Because non-NONE labeled nodes do not overlap, there is a strict left-to-rightorder among these nodes.
The candidate argument sequence that corresponds to thecorrect assignment in Figure 9 is then:[NP1-ARG1, VBD1-PRED, PP1-ARG4, NP3-ARGM-TMP]Features from Local Models.All features included in the local models are also includedin our joint models.
In particular, each template for local features is included as a jointtemplate that concatenates the local template and the node label.
For example, for thelocal feature PATH, we define a joint feature template that extracts PATH from everynode in the candidate argument sequence and concatenates it with the label of thenode.
Both a feature with the specific argument label and a feature with the genericback-off ARG label are created.
This is similar to adding features from identificationand classification models.
In the case of the example candidate argument sequenceprovided, for the node NP1 we have the features:{(NP?S?VP?VBD)-ARG1, (NP?S?VP?VBD)-ARG}When comparing a local and a joint model, we use the same set of local featuretemplates in the two models.
If these were the only features that a joint model used,we would expect its performance to be roughly the same as the performance of alocal model.
This is because the two models will in fact be in the same parametricfamily but will only differ slightly in the way the parameters are estimated.
Inparticular, the likelihood of an assignment according to the joint model with localfeatures will differ from the likelihood of the same assignment according to the localmodel only in the denominator (the partition function).
The joint model sums over179Computational Linguistics Volume 34, Number 2a few likely assignments in the denominator, whereas the local model sums over allassignments; also, the joint model does not treat the decomposition into identificationand classification models in exactly the same way as the local model.Whole Label Sequence Features.
As observed in previous work (Gildea and Jurafsky2002; Pradhan et al 2004), including information about the set or sequence of labelsassigned to argument nodes should be very helpful for disambiguation.
For example,including such information will make the model less likely to pick multiple nodes tofill the same role or to come up with a labeling that does not contain an obligatoryargument.
We added a whole label sequence feature template that extracts the labelsof all argument nodes, and preserves information about the position of the predicate.Two templates for whole label sequences were added: one having the predicate voiceonly, and another also including the predicate lemma.
These templates are instantiatedas follows for the example candidate argument sequence:[voice:active, ARG1, PRED, ARG4, ARGM-TMP][voice:active, lemma:accelerate, ARG1, PRED, ARG4, ARGM-TMP]We also add variants of these templates that use a generic ARG label instead ofspecific labels for the arguments.
These feature templates have the effect of counting thenumber of arguments to the left and right of the predicate, which provides useful globalinformation about argument structure.
A local model is not able to represent the countof arguments since the label of each node is decided independently.
This feature canvery directly and succinctly encode preferences for required arguments and expectednumber of arguments.As previously observed (Pradhan et al 2004), including modifying arguments insequence features is not helpful.
This corresponds to the standard linguistic understand-ing that there are no prevalent constraints on the position or presence of adjuncts in anargument frame, and was confirmed in our experiments.
We redefined the whole labelsequence features to exclude modifying arguments.The whole label sequence features are the first type of features we add to relaxthe independence assumptions of the local model.
Because these features look atthe sequence of labels of all arguments, they capture joint information.
There is nolimit on the length of the label sequence and thus there is no n-gram Markov orderindependence assumption (in practice the candidate argument sequences in the topn complete assignments are rarely more than 7 nodes long).
Additionally, the nodesin the candidate argument sequences are in general not in the same local tree in thesyntactic analysis and a tree-CRF model (Cohn and Blunsom 2005) would not be ableto encode these dependencies.Joint Syntactic?Semantic Features.
This class of features is similar to the whole labelsequence features, but in addition to labels of argument nodes, it includes syntacticfeatures of the nodes.
These features can capture the joint mapping from the syntacticrealization of the predicate?s arguments to its semantic frame.
The idea of these featuresis to capture knowledge about the label of a constituent given the syntactic realizationand labels of all other arguments of the verb.
This is helpful in capturing syntacticalternations, such as the dative alternation.
For example, consider the sentence (i) [ShawPublishing]ARG0 [offered]PRED [Mr. Smith]ARG2 [a reimbursement]ARG1 and the alternative re-alization (ii) [Shaw Publishing]ARG0 [offered]PRED [a reimbursement]ARG1 [to Mr. Smith]ARG2.180Toutanova, Haghighi, and Manning A Global Joint Model for SRLWhen classifying the NP in object position, it is useful to know whether the followingargument is a PP.
If it is, the NP will more likely be an ARG1, and if not, it will morelikely be an ARG2.
A feature template that captures such information extracts, for eachcandidate argument node, its phrase type and label.
For example, the instantiations ofsuch templates in (ii), including only the predicate voice or also the predicate lemma,would be:[voice:active, NP-ARG0, PRED, NP-ARG1, PP-ARG2][voice:active,lemma:offer, NP-ARG0, PRED, NP-ARG1, PP-ARG2]We experimented with extracting several kinds of features from each argumentnode and found that the phrase type and the head of a directly dominating PP?if oneexists?were most helpful.Local models normally consider only features of the phrase being classified inaddition to features of the predicate.
They cannot take into account the features of otherargument nodes, because they are only given the input (parse tree), and the identity ofthe argument nodes is unknown.
It is conceivable that a local model could condition onthe features of all nodes in the tree but the number of parameters (features) would beextremely large.
The joint syntactic?semantic features proposed here encode importantdependencies using a very small number of parameters, as we will show in Section 5.4.We should note that Xue and Palmer (2004) define a similar feature template, calledsyntactic frame, which often captures similar information.
The important difference isthat their template extracts contextual information from noun phrases surrounding thepredicate, rather than from the sequence of argument nodes.
Because we use a jointmodel, we are able to use information about other argument nodes when labeling anode.Repetition Features.
We also add features that detect repetitions of the same label ina candidate argument sequence, together with the phrase types of the nodes labeledwith that label.
For example, (NP-ARG0, WHNP-ARG0) is a common pattern of this form.Variants of this feature template also indicate whether all repeated arguments are sistersin the parse tree, or whether all repeated arguments are adjacent in terms of word spans.These features can provide robustness to parser errors, making it more likely to assignthe same label to adjacent phrases that may have been incorrectly split by the parser.
InSection 5.4 we report results from the joint model and an ablation study to determinethe contribution of each of the types of joint features.5.3 Applying Joint Models in TestingHere we describe the application in testing of a joint model for semantic role labeling,using a local model PSRL and a joint re-ranking model PrSRL.
The local model PSRL is usedto generate N non-overlapping joint assignments L1, .
.
.
,LN.One option is to select the best Li according to PrSRL, as in Equation (3), ignoring thescore from the local model.
In our experiments, we noticed that for larger values of N,the performance of our re-ranking model PrSRL decreased.
This was probably due to thefact that at test time the local classifier produces very poor argument frames near thebottom of the top n for large n. Because the re-ranking model is trained on relatively181Computational Linguistics Volume 34, Number 2few good argument frames, it cannot easily rule out very bad frames.
It makes sensethen to incorporate the local model into our final score.
Our final score is given by:PSRL(L|t, v) = (PSRL(L|t, v))?
PrSRL(L|t, v)where ?
is a tunable parameter determining the amount of influence the local score hason the final score (we found ?
= 1.0 to work best).
Such interpolation with a score froma first-pass model was also used for parse re-ranking in (Collins 2000).
Given this score,at test time we choose among the top n local assignments L1, .
.
.
,Ln according to:argmaxL?L1,...,Ln ?
logPSRL(L|t, v)+ logPrSRL(L|t, v) (4)5.4 Joint Model ResultsWe compare the performance of joint re-ranking models and local models.
We usedn = 10 joint assignments for training re-ranking models, and n = 15 for testing.
Theweight ?
of the local model was set to 1.
Using different numbers of joint assignmentsin training and testing is in general not ideal, but due to memory requirements, wecould not experiment with larger values of n for training.Figure 10 shows the summary performance of the local model (LOCAL), repeatedfrom earlier figures, a joint model using only local features (JOINTLOCAL), a joint modelusing local + whole label sequence features (LABELSEQ), and a joint model using alldescribed types of features (ALLJOINT).
The evaluation is on gold-standard parse trees.In addition to performance measures, the figure shows the number of binary featuresincluded in the model.
The number of features is a measure of the complexity of thehypothesis space of the parametric model.We can see that a joint model using only local features outperforms a local modelby .5 points of F-Measure.
The joint model using local features estimates the featureweights only using the top n consistent assignments, thus making the labels of differentnodes non-independent according to the estimation procedure, which may be a causeof the improved performance.
Another factor could be that the model JOINTLOCAL is acombination of two models as specified in Equation (4), which may lead to gains (as isusual for classifier combination).The label sequence features added in Model LABELSEQ result in another 1.5 pointsjump in F-Measure on all arguments.
An additional .8 gain results from the inclusionof syntactic?semantic and repetition features.
The error reduction of model ALLJOINTFigure 10Performance of local and joint models on ID&CLS on Section 23, using gold-standard parse trees.The number of features of each model is shown in thousands.182Toutanova, Haghighi, and Manning A Global Joint Model for SRLover the local model is 36.8% in CORE arguments F-Measure, 33.3% in CORE argumentswhole frame accuracy, 24.1% in ALL arguments F-Measure, and 21.7% in ALL argumentswhole frame accuracy.
All differences in ALL arguments F-Measure are statisticallysignificant according to a paired Wilcoxon signed rank test.
JOINTLOCAL is significantlybetter than LOCAL (p < .001), LABELSEQ is significantly better than JOINTLOCAL (p <.001), and ALLJOINT is significantly better than LABELSEQ (p < .001).
We performed theWilcoxon signed rank test on per-proposition ALL arguments F-Measure for all models.We also note that the joint models have fewer features than the local model.
This isdue to the fact that the local model has seenmanymore negative examples and thereforemore unique features.
The joint features are not very numerous compared to the localfeatures in the joint models.
The ALLJOINT model has around 30% more features thanthe JOINTLOCAL model.These experiments showed that the label sequence features were very useful, es-pecially on CORE arguments, increasing the F-Measure on these arguments by twopoints when added to the JOINTLOCAL model.
This shows that even though thelocal model is optimized to use a large set of features and achieve state-of-the-artperformance, it is still advantageous to model the joint information in the sequenceof labels in a predicate?s argument frame.
Additionally, the joint syntactic?semanticfeatures improved performance further, showing that when predicting the label of anargument, it is useful to condition on the features of other arguments, in addition totheir labels.A more detailed analysis of the results obtained by the joint model ALLJOINTis given in Figure 11(a) (Summary results), and the two confusion matrices in Fig-ures 11(b) and 11(c), which display the number of errors of each type that the modelmade.
The first confusion matrix concentrates on CORE arguments and merges allmodifying argument labels into a single ARGM label.
The second confusion matrixconcentrates on confusions among modifying arguments.
This figure can be comparedto Figure 7, which summarizes the results for the local model in the same form.
Thebiggest differences are in the performance on CORE arguments, which can be seen bycomparing the confusion matrices in Figures 7(b) and 11(b).
The F-Measure on eachof the core argument labels has increased by at least three points: the F-Measure onARG2 by 5.7 points, and the F-Measure on ARG3 by eight points.
The confusions ofcore argument labels with NONE have gone down significantly, and also there is alarge decrease in the confusions of NONE with ARG1.
There is generally a slight increasein F-Measure on modifier labels as well, but the performance on some of the modifierlabels has gone down.
This makes sense because our joint features are targeted atcapturing the dependencies among core arguments.
There may be useful regularitiesfor modifier arguments as well, but capturing them may require different joint featuretemplates.Figure 12 lists the frequency with which each of the top k assignments from theLOCAL model was ranked first by the re-ranking model ALLJOINT.
For example, for84.1% of the propositions, the re-ranking model chose the same assignment that theLOCAL model would have chosen.
The second best assignment according to the LOCALmodel was promoted to first 8.6% of the time.
The figure shows statistics for the top tenassignments only.
The rest of the assignments, ranked 11 through 15, were chosen asbest by the re-ranking model for a total of 0.3% of the propositions.The labeling of the tree in Figure 9 is a specific example of the kind of errorsfixed by the joint models.
The local classifier labeled the first argument in the tree asARG0 instead of ARG1, probably because an ARG0 label is more likely for the subjectposition.183Computational Linguistics Volume 34, Number 26.
Semantic Role Labeling of Automatic ParsesWe now evaluate our models when trained and tested using automatic parses producedby Charniak?s parser.
The Propbank training set Sections 2?21 is also the training set ofthe parser.
The performance of the parser is therefore better on the training set.When theconstituents of an argument do not have corresponding constituents in an automaticallyproduced parse tree, it will be very hard for a model to get the semantic role labelingcorrect.
However, this is not impossible and systems which are more robust to parsererror have been proposed (Pradhan et al 2005; Ma`rquez et al 2005).
Our system can alsotheoretically guess the correct set of words by labeling a set of constituents that coverFigure 11Performance measures for joint model using all features (AllJoint).
Results are on Section 23using gold-standard parse trees.Figure 12Percentage of test set propositions for which each of the top ten assignments from the Localmodel was selected as best by the joint model AllJoint.184Toutanova, Haghighi, and Manning A Global Joint Model for SRLFigure 13Percentage of argument constituents that are not present in the automatic parses of Charniak?sparser.
Constituents shows the percentage of missing constituents and Propositions shows thepercentage of propositions that have missing constituents.the argument words, but we found that this rarely happens in practice.
Figure 13 showsthe percentage of argument constituents that are missing in the automatic parse treesproduced by Charniak?s parser.
We can see that the percentage of missing constituentsis quite high.We report local and joint model results in Figures 14(a) and 14(b), respectively.
Asfor gold-standard parses, we test on all arguments regardless of whether they corre-spond to constituents that have been recovered by the parser and use the samemeasuresdetailed in Section 3.2.
We also compare the confusion matrices for the local and jointmodels, ignoring the confusions among modifier argument labels (COARSEARGM setting)in Figure 15.
The error reduction of the joint over the local model is 10.3% in COREarguments F-Measure and 8.3% in ALL arguments F-Measure.6.1 Using Multiple Automatic Parse GuessesSemantic role labeling is very sensitive to the correctness of the given parse tree, as theresults show.
If an argument does not correspond to any constituent in a parse tree, ora constituent exists but is not attached or labeled correctly, our model will have a veryhard time guessing the correct labeling.Thus, if the syntactic parser makes errors, these errors influence directly the seman-tic role labeling system.
The theoretically correct way to propagate the uncertainty ofthe syntactic parser is to consider (sum over) multiple possible parse trees, weighted bytheir likelihood.
In Finkel, Manning, and Ng (2006), this is approximated by samplingFigure 14Comparison of local and joint model results on Section 23 using Chaniak?s automatic parser.185Computational Linguistics Volume 34, Number 2Figure 15COARSEARGM argument confusion matrices for local and joint model using Charniak?sautomatic parses.parse trees.
We implement this idea by an argmax approximation, using the top k parsetrees from the parser of Charniak (2000).We use these alternative parses as follows: Suppose t1, .
.
.
, tk are trees for sentenceswith probabilities P(ti|s) given by the parser.
Then for a fixed predicate v, let Li denotethe best joint labeling of tree ti, with score scoreSRL(Li|ti) according to our final jointmodel.
Then we choose the labeling Lwhich maximizesargmaxi?{1,...,k}?
logP(ti|S)+ scoreSRL(Li|ti)This method of using multiple parse trees is very simple to implement and factorsin the uncertainty of the parser to some extent.
However, according to this method (dueto the argmax operation) we are choosing a single parse and a complete semantic framederived from that parse.
Other methods are able to derive different arguments of thesemantic frame from different syntactic annotations which maymake themmore robust(Ma`rquez et al 2005; Pradhan, Ward et al 2005; Punyakanok, Roth, and Yih 2005).Figure 16 shows summary results for the test set when using the top ten parses andthe joint model.
The weighting parameter for the parser probabilities was ?
= 1.
We didnot experiment extensively with different values of ?.
Preliminary experiments showedthat considering 15 parses was a bit better, and considering the top 20 was a bit worse.6.2 Evaluation on the CoNLL 2005 Shared TaskThe CoNLL 2005 data is derived from Propbank version I, which is the first officialrelease in 2005, whereas the results we have been reporting in the previous sections usedthe pre-final February 2004 data.
Using the CoNLL 2005 evaluation standard ensuresthat results obtained by different groups are evaluated in exactly the same way.
In186Toutanova, Haghighi, and Manning A Global Joint Model for SRLFigure 16Performance of the joint model using the top ten parses from Charniak?s parser.
Results are onSection 23.Propbank I, there have been several changes in the annotation conventions, as well aserror fixes and addition of new propositions.
There was also a change in the way PParguments are annotated: In the February 2004 data some PP arguments are annotatedat the head NP child, but in Propbank I all PP arguments are annotated at the PP nodes.In order to achieve maximal performance with respect to these annotations, it wouldprobably be best to change the feature definitions to account for the changes.
However,we did no adaptation of the features.The training set consists of the annotations in Sections 2 to 21, the development setis section 24 (Devset), and one of the test sets is section 23 (Test WSJ).
The other test setis from the Brown corpus (Test Brown).
The CoNLL annotations distinguish referringarguments, of the form R-ARGX, as discussed in Section 3.Our approach to dealing with referring arguments and deciding when multipleidentically labeled constituents are part of the same argument was to label constituentswith only the set of argument labels and NONE and then map some of these labelsinto referring or continuation labels.
We converted an ARGX into a R-ARGX if andonly if the label of the constituent began with ?WH?.
The rule for deciding when toadd continuation labels was the same as for our systems for the February 2004 datadescribed in Section 4.3: A constituent label becomes continuing if and only if it is acore argument label and there is another constituent with the same core argument labelto the left.
Therefore, for the CoNLL 2005 shared task we employ the same semanticrole labeling system, just using a different post-processing rule to map to CoNLL-stylelabelings of sets of words.We tested the upper bound in performance due to our conversion scheme in thefollowing way: Take the gold-standard CoNLL annotations for the development set(including referring and continuing labels), convert these to basic argument labels of theform ARGX, then convert the resulting labeling to CoNLL-style labeling using our rulesto recover the referring and continuing annotations.
The F-Measure obtained was 99.0.Figure 17 shows the performance of the local and joint model on one of the CoNLLtest sets?Test WSJ (Section 23)?when using gold-standard parse trees.
Performanceon gold-standard parse trees was not measured in the CoNLL 2005 shared task, but wereport it here to provide a basis for comparison with the results of other researchers.Figure 17Results on the CoNLL WSJ Test set, when using gold-standard parse trees.187Computational Linguistics Volume 34, Number 2Figure 18Results on the CoNLL data set, when using Charniak automatic parse trees as provided in theCoNLL 2005 shared task data.Figure 19Results on the CoNLL data set, using automatic parse trees from the May 2005 version of theCharniak parser with correct treatment of forward quotes.Next we present results using Charniak?s automatic parses on the developmentand two test sets.
We present results for the local and joint models using the max-scoring Charniak parse tree.
Additionally, we report results for the joint model usingthe top five Charniak parse trees according to the algorithm described in Section 6.1.The performance measures reported here are higher than the results of our submissionin the CoNLL 2005 shared task (Haghighi, Toutanova, and Manning 2005), because oftwo changes.
One was changing the rule that produces continuing arguments to onlyadd continuation labels to core argument labels; in the previous version the rule addedcontinuation labels to all repeated labels.
Another was fixing a bug in the way thesentences were passed in as input to Charniak?s parser, leading to incorrect analysesof forward quotes.8We first present results of our local and joint model using the parses provided aspart of the CoNLL 2005 data (and having wrong forward quotes) in Figure 18.
Wethen report results from the same local and joint model, and the joint model using thetop five Charniak parses, where the parses have correct representation of the forwardquotes in Figure 19.
For these results we used the version of the Charniak parser from4 May 2005.
The results were very similar to the results we obtained with the versionfrom 18March 2005.
We did not experiment with the new re-ranking model of Charniakand Johnson (2005), even though it improves upon Charniak (2000) significantly.For comparison, the system we submitted to CoNLL 2005 had an F-Measure of78.45 on the WSJ Test set.
The winning system (Punyakanok, Roth, and Yih 2005) hadan F-Measure of 79.44 and our current system has an F-Measure of 80.32.
For the BrownTest set, our submitted version had an F-Measure of 67.71, the winning system had67.75, and our current system has 68.81.Figure 20 shows the per-label performance of our joint model using the top fiveCharniak parse trees on the Test WSJ test set.
The columns show the Precision, Recall,F-Measure, and the total number of arguments for each label.8 The Charniak parses provided as part of the CoNLL shared task data uniformly ignore the distinctionbetween forward and backward quotes and all quotes are backward.
We re-ran the parser and obtainedanalyses with correct treatment of quotes.188Toutanova, Haghighi, and Manning A Global Joint Model for SRL7.
ConclusionsIn accord with standard linguistic assumptions, we have shown that there are sub-stantial gains to be had by jointly modeling the argument frames of verbs.
This isespecially true when we model the dependencies with discriminative models capableof incorporating non-local features.
We incorporated joint information by using twotypes of features: features of the complete sequence of argument labels and featuresmodeling dependencies between the labels of arguments and syntactic features of otherarguments.We showed that both types of features yielded significant performance gainsover a state-of-the-art local model.For further improving performance in the presence of perfect syntactic parses, wesee at least three promising avenues for improvement.
First, one could improve theidentification of argument nodes, by better handling of long-distance dependencies; forexample, by incorporatingmodels which recover the trace and null element informationin Penn Treebank parse trees, as in Levy andManning (2004).
Second, it may be possibleto improve the accuracy on modifier labels, by enhancing the knowledge about thesemantic characteristics of specific words and phrases, such as by improving lexicalstatistics; for instance, our performance on ARGM-TMP roles is rather worse than thatof some other groups.
Finally, it is worth exploring alternative handling of multi-constituent arguments; our current model uses a simple rule in a post-processing stepFigure 20Per-label performance of joint model using the top five Charniak automatic parse trees on theTest WSJ test set.189Computational Linguistics Volume 34, Number 2to decide which constituents given the same label are part of the same argument.
Thiscould be done more intelligently by the machine learning model.Because perfect syntactic parsers do not yet exist and the major bottleneck to theperformance of current semantic role labeling systems is syntactic parser performance,the more important question is how to improve performance in the presence of parsererrors.
We explored a simple approach of choosing from among the top k parses fromCharniak?s parser, which resulted in an improvement.
Other methods have also beenproposed, as we discussed in Section 2 (Ma`rquez et al 2005; Pradhan, Ward et al 2005;Punyakanok, Roth, and Yih 2005; Yi and Palmer 2005; Finkel, Manning, and Ng 2006).This is a very promising line of research.AcknowledgmentsThis research was carried out while all theauthors were at Stanford University.
Wethank the journal reviewers and thereviewers and audience at ACL 2005 andCoNLL 2005 for their helpful comments.
Wealso thank Dan Jurafsky for his insightfulcomments and useful discussions.
This workwas supported in part by the DisruptiveTechnology Organization (DTO)?s AdvancedQuestion Answering for Intelligence(AQUAINT) Program.ReferencesBaker, Collin, Charles Fillmore, and JohnLowe.
1998.
The Berkeley Framenetproject.
In Proceedings of COLING-ACL,pages 86?90, San Francisco, CA.Carreras, Xavier and Lu?
?s Ma`rquez.
2004.Introduction to the CoNLL-2004 sharedtask: Semantic role labeling.
In Proceedingsof CoNLL, pages 89?97, Boston, MA.Carreras, Xavier and Lu?
?s Ma`rquez.
2005.Introduction to the CoNLL-2005 sharedtask: Semantic role labeling.
In Proceedingsof CoNLL, pages 152?164, Ann Arbor, MI.Charniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
InProceedings of NAACL, pages 132?139,Seattle, WA.Charniak, Eugene and Mark Johnson.
2005.Coarse-to-fine n-best parsing and MaxEntdiscriminative reranking.
In Proceedings ofACL, pages 173?180, Ann Arbor, MI.Cohn, Trevor and Philip Blunsom.
2005.Semantic role labelling with treeconditional random fields.
In Proceedings ofCoNLL, pages 169?172, Ann Arbor, MI.Collins, Michael.
2000.
Discriminativereranking for natural language parsing.In Proceedings of ICML, pages 175?182,Stanford, CA.Finkel, Jenny, Christopher Manning, andAndrew Ng.
2006.
Solving the problem ofcascading errors: Approximate bayesianinference for linguistic annotationpipelines.
In Proceedings of EMNLP,pages 618?626, Sydney, Australia.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Hacioglu, Kadri.
2004.
A lightweightsemantic chunking model based ontagging.
In Proceedings of HLT-NAACL:Short Papers, pages 145?148, Boston, MA.Haghighi, Aria, Kristina Toutanova, andChristopher D. Manning.
2005.
A jointmodel for semantic role labeling.
InProceedings of CoNLL, pages 173?176,Ann Arbor, MI.Lafferty, John, Andrew McCallum, andFernando Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.In Proceedings of ICML, pages 282?289,Williamstown, MA.Levy, Roger and Chris Manning.
2004.Deep dependencies from context-freestatistical parsers: correcting thesurface dependency approximation.In Proceedings of ACL, pages 327?334,Barcelona, Spain.Ma`rquez, Lu?
?s, Mihai Surdeanu, PereComas, and Jordi Turmo.
2005.
A robustcombination strategy for semantic rolelabeling.
In Proceedings of EMNLP,pages 644?651, Vancouver, Canada.McCallum, Andrew, Dayne Freitag, andFernando Pereira.
2000.
Maximum entropyMarkov models for information extractionand segmentation.
In Proceedings of ICML,pages 591?598, Stanford, CA.Palmer, Martha, Dan Gildea, and PaulKingsbury.
2005.
The proposition bank:An annotated corpus of semanticroles.
Computational Linguistics,31(1):71?105.Pradhan, Sameer, Kadri Hacioglu, ValerieKrugler, Wayne Ward, James Martin,and Dan Jurafsky.
2005.
Supportvector learning for semantic argument190Toutanova, Haghighi, and Manning A Global Joint Model for SRLclassification.Machine Learning Journal,60(1):11?39.Pradhan, Sameer, Wayne Ward, KadriHacioglu, James Martin, and DanJurafsky.
2004.
Shallow semantic parsingusing support vector machines.
InProceedings of HLT-NAACL, pages 233?240,Boston, MA.Pradhan, Sameer, Wayne Ward, KadriHacioglu, James Martin, and DanielJurafsky.
2005.
Semantic role labeling usingdifferent syntactic views.
In Proceedings ofACL, pages 581?588, Ann Arbor, MI.Punyakanok, Vasin and Dan Roth.
2001.
Theuse of classifiers in sequential inference.In Proceedings of NIPS, pages 995?1001,Vancouver, Canada.Punyakanok, Vasin, Dan Roth, and Wen-tauYih.
2005.
The necessity of syntacticparsing for semantic role labeling.
InProceedings of IJCAI, pages 1117?1123,Acapulco, Mexico.Punyakanok, Vasin, Dan Roth, Wen-tau Yih,Dav Zimak, and Yuancheng Tu.
2004.Semantic role labeling via generalizedinference over classifiers.
In Proceedingsof CoNLL, pages 130?133, Boston, MA.Roland, Douglas and Daniel Jurafsky.
2002.Verb sense and verb subcategorizationprobabilities.
In Paola Merlo andSuzanne Stevenson, editors, The LexicalBasis of Sentence Processing: Formal,Computational, and ExperimentalIssues.
John Benjamins, Amsterdam,pages 325?345.Sha, Fei and Fernando Pereira.
2003.Shallow parsing with conditionalrandom fields.
In Proceedings ofHLT-NAACL, pages 134?141,Edmonton, Canada.Surdeanu, Mihai, Sanda Harabagiu,John Williams, and Paul Aarseth.2003.
Using predicate-argumentstructures for information extraction.In Proceedings of ACL, pages 8?15,Sapporo, Japan.Xue, Nianwen and Martha Palmer.
2004.Calibrating features for semantic rolelabeling.
In Proceedings of EMNLP,pages 88?94, Barcelona, Spain.Yi, Szu-ting and Martha Palmer.
2005.
Theintegration of syntactic parsing andsemantic role labeling.
In Proceedings ofCoNLL, pages 237?240, Ann Arbor, MI.191
