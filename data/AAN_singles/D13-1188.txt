Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1828?1839,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOpinion Mining in Newspaper Articles by Entropy-basedWord ConnectionsThomas Scholz and Stefan ConradHeinrich-Heine-UniversityInstitute of Computer ScienceD-40225 Du?sseldorf, Germany{scholz,conrad}@cs.uni-duesseldorf.deAbstractA very valuable piece of information in news-paper articles is the tonality of extracted state-ments.
For the analysis of tonality of newspa-per articles either a big human effort is needed,when it is carried out by media analysts, or anautomated approach which has to be as accu-rate as possible for a Media Response Anal-ysis (MRA).
To this end, we will compareseveral state-of-the-art approaches for Opin-ion Mining in newspaper articles in this pa-per.
Furthermore, we will introduce a newtechnique to extract entropy-based word con-nections which identifies the word combina-tions which create a tonality.
In the evalua-tion, we use two different corpora consistingof news articles, by which we show that thenew approach achieves better results than thefour state-of-the-art methods.1 IntroductionThe Web keeps many potentially valuable opinionsin news articles which are partly new online articlesor uploaded print media articles.
Many companiesor organisations such as political parties or even dis-tinguished public figures perform a Media ResponseAnalysis (MRA) (Watson and Noble, 2007) in orderto analyse the output of their effort in public rela-tions.
So, an opinion-oriented analysis of news arti-cles is important, because the tonality (Watson andNoble, 2007; Scholz et al 2012a) is the key indi-cator of a MRA.
A purely manual solution impliesa big human effort for so-called media analysts, be-cause they have to read and rate approx.
200 to 800news articles each week.As a consequence, an automated Opinion Miningsolution is very attractive.
At the same time, Opin-ion Mining in newspaper articles appears to be dif-ficult, because not all parts of news articles are assubjective (Balahur et al 2010) as reviews, for ex-ample.
Also, different parts of one article can con-tain different opinions (Watson and Noble, 2007).Therefore, we work with extracted statements ofnews articles, in which a sequence of consecutivesentences has the same tonality value.
At the sametime, some approaches focus more on differentiatingonly between positive and negative news and leaveout neutral examples (Taboada et al 2011; Scholz etal., 2012b).
Conversely, we have noticed that even ifthe used words in the news domain are quite similar,the tonality which the words express can be differ-ent, especially if neutral examples are involved (cf.section 3.1).
We propose this task formulation:Problem definition: Let s ?
d be a statement anddocument d represents a newspaper article.
The taskis to determine the tonality y for a given statements, consisting of k words:t : s = (w1, w2, ..., wk) 7?y ?
{positive,neutral,negative}(1)Normally, a statement consists of one up to foursentences.
But also longer statements are possible,but they appear less frequently in a MRA.
An au-tomated approach (Scholz and Conrad, 2013) forthe extraction of statements already exists.
The ap-proach applies machine learning to extract relevantsentences from a collection of news articles andcombine them to statements.
So, we concentrate onthe tonality classification, which is not provided by1828the approach for the statements extraction (Scholzand Conrad, 2013).
Furthermore, we define the po-larity of sentiment as the distinction between posi-tive and negative sentiment and the subjectivity asthe distinction between subjective (positive and neg-ative) statements and neutral statements.The following example is a positive statementfrom an article in The Telegraph (8th Aug 2012)which deals with the prospects of British companiesin Africa:?
Example statement (positive): There arestructural factors behind the African growthstory: a growing and sizeable population whichis increasingly urbanised with disposable in-come; growing political stability; and a finan-cial services industry that is still in its infancy.The so-called pressrelations dataset (Scholz et al2012a), which represents a publicly available cor-pus1 of a MRA on German news articles, contains1,521 annotated statements.
Since this is the onlypublicly available corpus of a MRA as far as weknow, we perform our experiments in German.
Weare aware of the fact that viewpoints play a signif-icant role in a newspaper, but since we concentrateon the determination of the tonality, the extraction ofviewpoints can be solved in a separate step (Scholzand Conrad, 2012).
This is possible, because thetonality of a statement can be determined withoutknowledge of the viewpoint in almost all cases.
Theonly exception is a statement with multiple view-points and different tonalities, but these statementsare very rare (cf.
also section 4.1).Our approach learns a graph from an annotatedcollection of statements, in which nodes and edgesmodel tonality-bearing word connections.
For un-seen statements, we recognize subgraphs of thelearned graph, compare two weighting methods forextracting different tonality features, and classify thestatements by a support vector machine.In this paper, we describe four state-of-the-arttechniques for Opinion Mining in the next sectionabout related work.
In the third section, we intro-duce our graph-based and entropy-based approachto calculate the tonality features T .
We will evaluateour approach against the state-of-the-art methods insection 4, before we conclude in the last section.1http://www.pressrelations.de/research/2 Related WorkOpinion Mining and Sentiment Analysis represent abroad subject area (Pang and Lee, 2008).The different contributions reach from applyingOpinion Mining in reviews and recommending newmultimedia products for individuals (Qumsiyeh andNg, 2012) to sentiment analyses for different topicsin social media (Wang et al 2011) or the creationof sentiment dictionaries (Baccianella et al 2009).In this paper, we focus on state-of-the-art methodsfor Opinion Mining which differ from each other intheir methodology.In the news domain, Wilson et al(2009) de-veloped a word-based classification approach whichcan extract contextual polarity.
This method (de-noted as Wilson) uses a lexicon and POS-taggingto generate word features and sentence features.Moreover, it also uses deep natural language anal-yses with dependency parse trees in order to cal-culate (general and polarity) modification featuresand structure features.
Finally, they compute 32 fea-tures for neutral-polar classification and 10 featuresfor the polarity classification.
These features can beused by different kinds of machine learning tech-niques such as Ripper (Cohen, 1996) or BoosTexter(Schapire and Singer, 2000).Based on a sentiment lexicon, Taboada et al(2011) calculate the semantic orientation of opinion-bearing words (SO-CAL).
They begin with a fine-grained dictionary of adjectives, adverbs, verbs, andnouns which have a score from -5 to +5.
?Master-piece?
has a score of +5 and ?monstrosity?
of -5,for example.
In addition, the approach takes inten-sifiers, negations, and irrealis (Taboada et al 2011)into account and thereby modifies the score of thewords through rules and formulas.
SO-CAL iden-tifies some special expressions and constructions,which tell the reader, that this text part does not re-ally contain an actual opinion or sentiment.
The lin-guistic term for this situation is called irrealis.
Also,text-level features weight the final score by merepresence of the words.In the field of customer reviews, Ding et al(2008) also work with a dictionary, which evenincludes context-dependent words (positive, neu-tral, and negative words) as well as rules to iden-tify the sentiment orientation of words (Opinion1829Observer).
The rules deal with negations, inter-sentence conjunctions, but-clauses, and the modifier?too?.
Furthermore, they extract relations betweenopinion words and corresponding product features.Thereby, a detailed analysis of product reviews ispossible.Sarvabhotla et al(2011) propose to extract thesubjective excerpt of a text (RSUMM).
They con-struct two word-vectors: An average document fre-quency vector represents the most important andmost specific word features for the given domain.Subsequently, an average subjective measure vectorselects the most subjective terms.
As a result, theyrequire hardly any natural language preprocessingexcept a sentence splitter and a tokenizer.
The fi-nal classification is accomplished by a SVM (SVM-Light (Joachims, 1999)).For product reviews, graph-based approaches(Goldberg and Zhu, 2006; Wu et al 2009) canincrease the performance in cross-domain tasks(Ponomareva and Thelwall, 2012).
By contrast, ourgraph nodes do not represent documents, but wordsto overcome the problem of similar bag-of-wordsrepresentations (cf.
next section).One important resource for Opinion Mining innews is the MPQA corpus (Wiebe et al 2005)which contains word and phrase-based annotationfor 523 news articles.
Unfortunately, since the cor-pus does not have statements and a statement-basedtonality, it is not designed as a MRA.
A slightlylarger corpus, the pressrelations dataset (Scholz etal., 2012a) with 617 articles, is the result of a MRAin German.
We use this corpus as one part of ourevaluation.3 Learning Tonality with Entropy-basedWord Connections3.1 Graph Model for Word ConnectionsTo solve the Opinion Mining task for a MRA,we propose a graph-based approach to capturethe opinion-bearing words and modifiers such asnegations.
In this way, our approach is able torecognize tonality-indicating structures (subgraphs)which provide precise information about the tonal-ity, even if statements have a very similar bag-of-words representation and at the same time differenttonalities.
One could also say that we create a graphinstead of a sentiment dictionary from training ex-amples, as other approaches (Kaji and Kitsuregawa,2007; Du et al 2010) proceed.In figure 1, simple examples are shown with apossible graph (the nodes and edges are taken fromthe given statements; of course, the graphs andweights become larger in practice).
These simpleexamples are concentrated on nouns, verbs, and ad-verbs, but also examples with combinations of othercategories are possible, such as, for example, dif-ferent combinations of adjectives, nouns, and verbs:?This is a black day for the company?, ?The com-pany is in the black?, ?The company is in the red?and ?The company prevents to be in the red?.
Thus,even though the word representation is quite similar,the tonality can be different.For opinion-bearing words, we use adjectives,nouns, verbs, and adverbs, which are widely ac-knowledged as opinion-bearing word categories(Bollegala et al 2011; Remus et al 2010; Taboadaet al 2011).
Furthermore, we also include negationparticles.
Therefore, the vocabulary V is the set ofwords in lemma for one set of statements S. Thus,for every lemma w ?
V , the approach creates onenode ?
in the graph.
A node ?
also contains the typeinformation (adjective, noun, verb, adverb, or nega-tion).The edge eij shows the appearance of node ?i and?j in combination with tonality y by means of aweight ?i,j (the sequence of the values in equation2 is also used in figure 1 and 2).
?ij = (yijpi, yijo, yij?)
(2)yijpi is the number of co-occurrences of node ?iand ?j in positive statements within the same sen-tence.
In analogy, yijo belongs to sentences of neu-tral statements and yij?
to sentences of negativestatements.
Figure 1 shows a small example for thiscalculation, too.3.2 Generating Features for LearningFrom a learned graph, we can combine differentedges to calculate tonality features for an unseenstatement s. An unseen statement is a statement,which is of course not used to learn the graph.
Weuse all edges of the subgraph Gsl which contains thenodes for every lemma wi in the l-th sentence of s.18301) This solves the crisis.
(positive)2) This solves the crisis slowly.
(neutral)3) This intensifies the crisis.
(negative)1) This solves the crisis.
(positive)2) This solves the crisis slowly.
(neutral)3) This intensifies the crisis.
(negative) solvecrisisintensify slowly(0,0,1) (0,1,0)(0,1,0)(1,1,0)Figure 1: An example for different statements and a graph: The weights base on the three examples and their notationis (positive,neutral,negative).1)ThisTiholshv v eosTthcr.(.p2cw.r.
(2cy.r.y2cn.r.r2uhtaT3  Tthf T)llc(.r.r2cg.y.y2 cy.0.y2c(.g.r02 c(.(.
(2Figure 2: An example of a learned graph: The nodes andedges, which are drawn in solid lines, represent the recog-nized subgraph Gsl for the sentence ?There are structuralfactors behind the African growth story.
?.We explain this with an example.
Assuming thatour learned graph is shown in figure 2.
It con-tains seven nodes and nine edges (also the nodes andedges in dashed lines).
If we further assume that anunseen statement is the example of section 1.
Tokeep this example short, we take the part until thecolon as the first sentence of the statement: ?Thereare structural factors behind the African growthstory.
?Our approach recognizes the nodes for ?be?,?structural?, ?factors?, ?growth?, and ?story?.
Thus,the subgraphGsl for the first sentence (l = 0) wouldbe the graph which is drawn in solid lines in figure 2.In this example, it is a connected graph, but it doesnot have to be.We could also look for complete or connectedgraphs in the statement instead of using all edges.The largest complete graph would consist of thenodes ?structural?, ?factor?, and ?be?
in our ex-ample.
But using all edges achieves better results,because this method provides all information.
Inaddition, this method is quicker (search for largestcomplete or connected graph can be omitted, whichwould be an additional check).If we have found our subgraphs Gsl, we can thencompute the vectorial sum of all edges for one node?i and we get the probability for a tonality y, if weobserve ?i in the l-th sentence:P (pos|?i) =?eij?Gslyijpi?eij?Gslyijpi + yij?
(3)P (neg|?i) =?eij?Gslyij?
?eij?Gslyijpi + yij?
(4)P (sub|?i) =?eij?Gslyijpi + yij?
?eij?Gslyijpi + yijo + yij?
(5)P (neu|?i) =?eij?Gslyijo?eij?Gslyijpi + yijo + yij?
(6)For the subjective class (sub), we add the appear-ance in positive statements (yijpi) and negative state-ments (yij?).
Otherwise we take the appearances instatements of the same class.
The denominators ofthe polarity refer only to positive and negative ap-pearances, while the denominators for the subjectiv-ity refer to every tonality.By calculating the vectorial sum, we combineseveral edges in order to estimate precise tonalityscores.
In this way, we can get the correct tonal-ity score for the noun ?crisis?, if a sentence con-tains also ?solve?
and ?slowly?
(?
more neutral) or?intensify?
(?
more negative) (cf.
figure 1).
Andwe get the correct tonality score for the adjective?structural?, if a sentence includes also ?crisis?
(?negative) or the nodes ?factor?, ?be?, ?growth?, and?story?
(?
positive) (cf.
figure 2).We distinguish between different word categories(we have noticed that this creates better results than1831just having a single feature for one statement).
Thus,every category gets its own feature and every nodeonly has a tonality value, if it belongs to the categoryof the feature.
This does not mean that we only con-sider edges which connect two nodes with the samecategory; we divide the influence of different cate-gories into different features:Tcat,z(?i) ={fz(?i) if ?i ?
cat0 if ?i /?
cat(7)cat ?
{adj, adv, n, v} indicates the category ofthe node (adjectives, adverbs, nouns, or verbs) andz specifies the type of feature.
One type showsthe difference between positive and negative polarity(z = pol), for the other type we replace the positiveclass by the subjective one (the sum of positive andnegative) and the negative by a neutral one in orderto differentiate between neutral and non-neutral ex-amples (z = sub).
As a result, we calculate eightfeatures (see table 1) for the tonality, two for eachimportant word category.
For the weighting, we ap-ply and compare two methods, presented in the nextsections.3.2.1 Kullback-Leibler WeightingFor the final score, we can use the Kullback-Leibler divergence (relative entropy) (Kullback andLeibler, 1951) of P2 from P1:DKL(P1||P2) =?x?XP1(x) logP1(x)P2(x)(8)To measure the information about tonality, we candefine our tonality scores based on the divergencebetween the two category pairs:fpol(?i) = DKL(P (pos|?i)||P (neg|?i)) (9)fsub(?i) = DKL(P (sub|?i)||P (neu|?i)) (10)Here, we measure the information lost, ifP (neg|?i) approximates P (pos|?i), for example.The Kullback-Leibler is an asymmetric measure, soa switch of the distributions would give a differentresult.
This is one reason why we prefer our secondmethod, but we evaluate both in order to find outhow important the choice of the weighting methodis.3.2.2 Entropy-summand WeightingAlso, the basic idea of the entropy (Shannon,1948) can be applied to extract the importance ofthe edges for the tonality.H(X) = ?n?i=1p(xi) log2(p(xi)) (11)Here, the p(xi) refer to the probabilities in theequations 3 to 6.
We add or subtract the entropy-summand of the assumed tonality class for one node?i to/from a perfect state (normalized to 1 and -1):fpol(?i) =??????
?1 + P (pos|?i) ?
log2(P (pos|?i))if P (neg|?i) ?
P (pos|?i)?1?
P (neg|?i) ?
log2(P (neg|?i))otherwise(12)fsub(?i) =??????
?1 + P (sub|?i) ?
log2(P (sub|?i))if P (neu|?i) ?
P (sub|?i)?1?
P (neu|?i) ?
log2(P (neu|?i))otherwise(13)In this way, we measure how much disorder onenode ?i provides for a certain tonality class.
For aclearly positive node (appears only in positive state-ments), e.g., the disorder will be 0 and so fpol(?i) =1 and also fsub(?i) = 1.3.3 Final Scores and ClassificationTo compute the eight final features values (fourfor each z-class), we calculate the average scoresof all nodes, which share the same category, overall sentences of the statement.
If no nodes/edgescould be recognized in an unseen statement, all fea-tures would be zero.
We use a SVM2 to clas-sify the statements by the extracted features.
Thisworks according to the one-versus-all strategy fora non-binary classification, which achieved slightlybetter results than a one-versus-one strategy or asubjective-objective classification first and then apositive-negative classification.
Linear kernels areused and the parameters are the default ones.
This2Rapidminer standard implementation (http://rapid-i.com/)1832Polarity Features Subjectivity FeaturesTv,pol: polarity for edges with verbs Tv,sub: subjectivity for edges with verbsTn,pol: polarity for edges with nouns Tn,sub: subjectivity for edges with nounsTadv,pol: polarity for edges with adverbs Tadv,sub: subjectivity for edges with adverbsTadj,pol: polarity for edges w. adjectives Tadj,sub: subjectivity for edges w. adjectivesTable 1: Polarity and subjectivity features based on word connectionsmeans that every class has the same priority, for in-stance.By using only 8 features, we actually achieve bet-ter results if compared with the use of one edge asa feature, because we abstract from individual wordcombinations in order to prevent overfitting.
We willdemonstrate that in section 4, where this method ofusing all edges as features is denoted as the graphedges method.
Another positive aspect of restrict-ing the number of features to a constant limit is thatwe save computing time (for the calculation of dis-tances within machine learning, e.g.
), because thegraphs can be large (cf.
section 4).4 Experiments4.1 Data and Experimental SetupWe use two different datasets for our evaluation: Thepressrelations dataset3 (called PDS) contains 1,521statements (446 positive, 492 neutral, 583 negative),and a real world dataset contains 8,500 statements(2,125 positive, 2,125 negative, 4,250 neutral) from5,352 news items about a financial service provider,the so-called Finance dataset.
Up to ten media ana-lysts (professional experts in the field of MRA) an-notate the extracted statements with a tonality.
Wehave investigated their inter-annotator agreement.So, four analysts annotate the same statements froma small part of the statements.
They achieve anagreement of 81.8% by using the simple accuracymetric.
The PDS has an inter-annotator agreementof 88.06% (Cohen?s kappa) (Scholz et al 2012a).We do not use the viewpoint information containedin the PDS.
This is not a problem, because the tonal-ity of statements can be estimated without knowl-edge of the viewpoint in the most cases.Nevertheless, a statement can have two differentviewpoints in a MRA.
This is the case for 116 state-ments (approx.
7.62%) of the pressrelations dataset3http://www.pressrelations.de/research/and 279 statements of the Finance dataset (approx.3.28%).
Statements can have two different tonal-ities for different viewpoints, but this is rarely thecase (for less than 3.56% of the pressrelations state-ments and less than 0.17% of the statements of theFinance dataset).
One of these examples is the fol-lowing statement, which is a translated statement ofthe PDS:?
Example: The logical consequence would bea substantial increase of the subsidies, whichthe SPD fraction has demanded several times.But the government has limited the fundingfor 2011 and a too slight rise is planned for2012.
(Code A: positive, SPD; Code B: nega-tive, CDU)At the time of the creation of this dataset, theSPD is the biggest opposing party of the CDU inGermany.
The CDU is the governing party underits chairwoman Chancellor Merkel.
We keep thesestatements within the dataset, because this case canoccur in a MRA.
However, we will show that thissituation does not irritate our approach too much.We use approx.
30% of the statements, that is 420statements (the first 140 positive, neutral, or nega-tive statements) or 2,500 statements (the first 625positive or negative and the first 1,250 neutral state-ments) in order to create our graph (the graph has41,470 or 154,001 edges, resp.).
For POS-tagging,identification of negations, and lemmatisation, weapply the TreeTagger (Schmid, 1995).
Unless oth-erwise stated, 20% of the remaining statements (220and 1,200 statements) are the training set for theSVM and the rest is test set.
The size of the testis so large, because we are aiming at a real signifi-cance of the solution which can actually be operatedin practice.18334.2 Adapting the State-of-the-Art Approachesfor a German MRAFor the approaches of Ding et al(2008), Wilson etal.
(2009), and Taboada et al(2011) we need a sen-timent dictionary.
Thus, we use the same statementswhich we use for the creation of our graphs for thecreation of a dictionary as one variant.To create the lexicon of subjectivity clues for themethod of Wilson et al(2009), all words which ap-pear more often in neutral statements get the priorpolarity neutral.
For all other words, we calculatethe number of appearances in positive statementsminus the appearances in negative statements di-vided by all appearances.
A positive word has avalue of over 0.2, a negative word has a value ofless than -0.2 and the rest has the prior polarity both.A positive word with a value above 0.6 belongs tothe reliability class strongsubj, the other positivewords are weaksubj.
We treat the negative wordsanalogously.
We use the Stanford Parser for Ger-man (Rafferty and Manning, 2008) to calculate thedependency trees for the sentences (Wilson et al2009), in order to extract the General ModificationFeatures, the Polarity Modification Features and theStructure Features.
The lists of intensifiers, copu-lar verbs, modals, negations, and polarity shiftersare translated by a domain expert, who also addedsuch elements which are not direct translations, buthave the same function.
The result of this methodis a classification of words and phrases.
Thus, fora statement classification, we classify the words ofthe statements and the class of the most frequentlyused words is the class of the statement (ambigu-ous statements are classified as the most frequentclass).
According to the authors, we apply the bestmachine learning techniques for the word classifica-tion (BoosTexter for tonality classification and Rip-per for Subjectivity Analysis with parameters as in(Wilson et al 2009)).For Opinion Observer (Ding et al 2008), wealso identify neutral words if they appear more oftenin neutral than in subjective statements and subjec-tive words are positive if they appear more often inpositive than in negative statements and vice versafor negative words.
In contrast to Opinion Miningin customer reviews, we exchange product featuresthrough statements and calculate the orientation ofopinions for all statements with their opinion ori-entation algorithm.
For this purpose, we adapt thenegation rules, the but-clause rule, the inter-sentenceconjunction rule, and the ?too?
rules for German(by translating important words such as ?but?
or thenegations).SO-CAL (Taboada et al 2011) needs dictionar-ies with sentiment values from -5 to +5 with inter-vals of one.
Thus, we use the same scores as theWilson method and a word with a value above 0.818to 1 gets a sentiment score of +5 and so on.
Thismeans, that neutral words also exist.
Our domain ex-pert translated the list of intensifiers (amplifiers anddowntoners) and negations, as well as the expert alsoadded missing elements.
The authors propose twoapproaches for the negation search.
We use the sec-ond, more conservative approach, because this ap-proach works better according to the authors.
Also,we use the value 4 for the negation shift.
Further-more, we implement the algorithm of irrealis block-ing and translate the list of irrealis markers (modalverbs, conditional markers, negative polarity items,private-state verbs (Taboada et al 2011)).For all dictionary-based methods (Wilson, Opin-ion Observer, SO-CAL), we also evaluate an addi-tional variant which use a sentiment dictionary andnot the statements which we use to construct thegraphs on each fold.
We apply the SentiWS (Remuset al 2010) for this purpose.
As the SentiWS hassentiment values between ?1 and 1, we apply simi-lar procedures to construct the method-specific dic-tionaries as described above: For SO-CAL, it is thesame procedure by using the SentiWS values, pos-itive words has a score above 0.33 for Wilson andOpinion Observer, strongsubj words have an abso-lute value above 0.66 and so on.
The methods aredenoted as method (dictionary).RSUMM (Sarvabhotla et al 2011) needs lessspecific adaptation, because only a sentence split-ter and a tokenizer are needed.
So, RSUMMis very language-independent.
We test two ver-sions of this method: one includes the optimiza-tion step to estimate the best values for X and Y(notated as RSUMM(X%, Y%)) and the other ver-sion (RSUMM(100%)) does without this step, be-cause we believe that every sentence is important inthe statements and also because more words meanmore information about the tonality in our domain.1834We use the sets for the creation of the graphs andlexicons as the validation dataset (VDS) (Sarvab-hotla et al 2011) and the subjectivity dataset (SDS)(Sarvabhotla et al 2011).
As in (Sarvabhotla et al2011), we apply the SVMLight package4 for classi-fication.Opinion Observer (Ding et al 2008) and SO-CAL (Taboada et al 2011) do not use supervisedlearning.
Therefore, we have also added our SVM inorder to classify the statements based on the scoresof Opinion Observer and SO-CAL (as shown in ta-bles with (+ SVM)).4.3 ResultsTable 2 and 4 (left side) show the results on thepressrelations dataset (PDS) and table 3 and 4 (rightside) show the results on Finance.
Table 2 and 3present the tonality classification (positive, neutral,negative) and table 4 displays the Subjectivity Anal-ysis (subjective, neutral).Word connections (Entropy-summand) achievethe best results with 63.45% accuracy on PDS (morethan 15% better than Wilson, which is the best of the?classical?
state-of-the-art methods) and best resultson Finance with 65.17% (more than 4% better thanRSUMM, which comes in second).
The weight-ing of the edges through the Entropy-summand per-forms better than the Kullback-Leibler weightingon both datasets, so we use the Entropy-summandweighting for all further experiments.Also, the improved methods (RSUMM(100%),Opinion Observer (+ SVM), and SO-CAL(+ SVM))get better results in the majority of cases (the im-provement of SO-CAL is more than 13% on PDSand more than 4% on Finance, e.g.).
Furthermore,the variants of the methods, which are expanded by ageneral sentiment dictionary, perform rather worse.The ?classical?
Opinion Observer performs betterwith a general sentiment dictionary, while Wilsontends to achieve worse results in this variant.Wilson (without an additional dictionary)achieves an accuracy of 42.91% on PDS (Subjec-tivity Analysis 69.36%) and 48.67% on Finance(Subjectivity Analysis 60.96%) for their word clas-sification.
The accuracy of the dictionary variant is43.44% on PDS and 40.12% on Finance.
Therefore,4http://svmlight.joachims.org/the tonality classification by the most frequent wordclass seems appropriate for this task and method,because this method achieves better results in theclassification of statements than on the word level.The findings of RSUMM are ambiguous.
The?classical?
RSUMM with parameter optimizationdoes not perform very well on PDS, but it performswell on Finance with a high proportion of sentencesand words (RSUMM(90%,95%)).
Also, if we useall sentences and all features (RSUMM(100%)) weobtain better results on Finance and PDS.
This fits inwith our assumption that every sentence of a state-ment is important and that more words lead to moretonality information.
The number of word featuresfor RSUMM(100%) is 4,985 features for one state-ment on PDS and 13,608 features on Finance.
Af-ter the parameter optimization the size is 974 wordfeatures on PDS (RSUMM(80%,20%)) and 12,248features on Finance (RSUMM(90%,95%)).The outcomes of this study suggest that methodswhich include machine learning techniques tend toperform better than unsupervised techniques.
Theresults of the approaches which we expand with aSVM support this conclusion.
As mentioned before,only the graph edges obtain a not so high accuracy.This shows the importance of the aggregation of theedges and entropy-based weighting.We evaluate the influence of the different inputsizes and so we performed experiments with 5%,10%, 40%, and 80% training for machine learningas well as 210 and 840 statements for the creation ofdictionaries/graphs on PSD (0.17% training for 210statements and 0.32% training for 840 statements inorder to create the same size of training according tothe results of 420 statements).
The results are shownin table 5.
Opinion Observer and SO-CAL are writ-ten in italics, because the results on the left side (sizeof the training set) belongs to their (+ SVM) variantsand the results on the right side are the ?classical?methods with no supervised learning.
These exper-iments show that our word connections remain verystable if the training set is decreased.
However, itdoes not benefit from more training, especially whenthe training set is very large (80%).
Opinion Ob-server and RSUMM(80%,20%) has the same prob-lem.
Nevertheless, it still receives the second-bestresults, even if another method gets a higher accu-racy.
However, in our opinion, it is more important1835Method AccuracyPositive Neutral Negativeprec rec prec rec prec recWilson 0.4784 0.358 0.5 0.5423 0.5054 0.5540 0.4444Wilson (dictionary) 0.4609 0.377 0.3366 0.3664 0.2963 0.5346 0.6223Opinion Observer 0.3806 0.3732 0.1732 0.3481 0.8267 0.6098 0.1693Opinion Observer (dictionary) 0.4468 0.5083 0.1993 0.4005 0.8693 0.576 0.2822RSUMM(80%,20%) 0.403 - 0.0 - 0.0 0.403 1.0SO-CAL 0.3279 0.3676 0.7353 0.2626 0.3551 0.8461 0.0248SO-CAL (dictionary) 0.2852 0.2987 0.8464 0.2072 0.1307 0.0075 0.0002Opinion Observer (+ SVM) 0.3825 - 0.0 0.252 0.1084 0.4037 0.8743Opinion Observer (dictionary + SVM) 0.3235 0.52 0.2122 0.1322 0.0804 0.346 0.6RSUMM(100%) 0.4801 0.4586 0.3025 0.8298 0.1354 0.4609 0.8789SO-CAL (+ SVM) 0.4608 0.463 0.3061 0.3543 0.5699 0.6486 0.48SO-CAL (dictionary + SVM) 0.3995 0.8235 0.0571 0.3559 0.9371 0.6306 0.2graph edges 0.5482 0.4313 0.551 0.6578 0.5175 0.5831 0.5714our approach (Kullback-Leibler) 0.5778 0.5 0.302 0.6642 0.6154 0.5534 0.74our approach (Entropy-summand) 0.6345 0.5346 0.4735 0.6989 0.6818 0.6442 0.7086Table 2: Results of the experiments on the PDSMethod AccuracyPositive Neutral Negativeprec rec prec rec prec recWilson 0.5602 0.4206 0.188 0.6358 0.7329 0.4706 0.5872Wilson (dictionary) 0.4088 0.3678 0.3291 0.5618 0.339 0.3367 0.6132Opinion Observer 0.4357 0.3641 0.0947 0.5033 0.713 0.2449 0.222Opinion Observer (dictionary) 0.4583 0.3275 0.186 0.5325 0.664 0.3404 0.3193RSUMM(90%,95%) 0.6092 0.4433 0.4840 0.731 0.6145 0.5866 0.7233SO-CAL 0.3478 0.2992 0.5993 0.384 0.373 0.8519 0.046SO-CAL (dictionary) 0.2905 0.2669 0.9207 0.4429 0.1203 0.001 0.0007Opinion Observer (+ SVM) 0.4852 0.3384 0.0914 0.496 0.9269 - 0.0Opinion Observer (dictionary + SVM) 0.4577 0.3299 0.187 0.5118 0.6649 0.3384 0.3177RSUMM(100%) 0.6088 0.4428 0.4823 0.731 0.6145 0.5854 0.7233SO-CAL (+ SVM) 0.3921 0.2986 0.7479 0.4573 0.1074 0.599 0.601SO-CAL (dictionary + SVM) 0.4762 0.3862 0.341 0.544 0.6206 0.3878 0.3244graph edges 0.5875 0.4437 0.3633 0.6444 0.7096 0.5816 0.5708our approach (Kullback-Leibler) 0.561 0.3868 0.5445 0.7659 0.5524 0.5201 0.5951our approach (Entropy-summand) 0.6517 0.53 0.5675 0.7714 0.6527 0.5946 0.7351Table 3: Results of the experiments on FinanceMethod AccuracySubjective ObjectiveAccuracySubjective Objectiveprec rec prec rec prec rec prec recWilson 0.6818 0.7251 0.8602 0.4970 0.2975 0.6307 0.6228 0.6649 0.6399 0.5966Wilson (dictionary) 0.7029 0.7742 0.8636 0.2871 0.179 0.5247 0.5296 0.7944 0.5069 0.2305Opinion Observer 0.4496 0.7698 0.2724 0.3481 0.8267 0.5047 0.508 0.2963 0.5033 0.713Opinion Observer (dictionary) 0.5422 0.8635 0.3885 0.4005 0.8693 0.5405 0.5538 0.417 0.5325 0.664RSUMM(80%,20%)/(90%,95%) 0.3269 - 0.0 0.3269 1.0 0.6919 0.7307 0.6170 0.6630 0.7682SO-CAL 0.5250 0.7373 0.4686 0.3632 0.6449 0.6127 0.616 0.5983 0.6095 0.627SO-CAL (dictionary) 0.4378 0.7928 0.235 0.3481 0.8693 0.5155 0.5571 0.1513 0.509 0.8797Opinion Observer (+ SVM) 0.6061 0.6636 0.8454 0.252 0.1084 0.494 0.4665 0.0636 0.496 0.9269Opinion Observer (dictionary + SVM) 0.4109 0.88 0.1479 0.3508 0.958 0.5327 0.5732 0.2667 0.5204 0.8003RSUMM(100%) 0.7083 0.7014 0.9865 0.8298 0.1354 0.6975 0.7424 0.6137 0.6654 0.7829SO-CAL (+ SVM) 0.5153 0.7485 0.4252 0.3702 0.7028 0.6231 0.7415 0.3814 0.582 0.8663SO-CAL (dictionary + SVM) 0.3598 0.878 0.0605 0.3345 0.9825 0.511 0.5481 0.1421 0.5055 0.8822graph edges 0.7037 0.6983 0.9882 0.8205 0.1119 0.6302 0.7821 0.3639 0.5840 0.898our approach (Kullback-Leibler) 0.7662 0.8215 0.8353 0.6449 0.6224 0.7006 0.6753 0.7761 0.735 0.6247our approach (Entropy-summand) 0.7707 0.8478 0.8050 0.6329 0.6993 0.739 0.7179 0.7898 0.7649 0.6878Table 4: Subjectivity Analysis on PDS (left side) and on Finance (right side)1836Method 0.05 0.1 0.2 0.4 0.8 210 420 840Wilson 0.4388 0.4743 0.4784 0.5514 0.5795 0.5275 0.4784 0.5553Opinion Observer 0.3403 0.3683 0.3825 0.3979 0.3591 0.3585 0.3806 0.3822SO-CAL 0.4579 0.439 0.4608 0.4402 0.4818 0.3509 0.3279 0.2702RSUMM(80%,20%) 0.4063 0.4046 0.403 0.3949 0.3636 0.3226 0.403 0.4557RSUMM(100%) 0.2964 0.448 0.4801 0.5265 0.6318 0.489 0.4801 0.5529our approach (Entropy-summand) 0.5717 0.5883 0.6345 0.6278 0.5818 0.5224 0.6345 0.6452Table 5: Different sizes of the training set and the dictionaries/graphsFeatures Level(Wilson) Level(SO-CAL) Features Level(Wilson) Level(SO-CAL)Tv,pol ?????
nsc Tv,sub nsc +++++Tn,pol ?????
???
Tn,sub ?????
++Tadv,pol ?????
?
Tadv,sub ?????
nscTadj,pol ?????
?????
Tadj,sub ?????
nscTcat,pol ?????
nsc Tcat,sub ??
+++++Tcat,z(all) +++++ +++++Table 6: Significance of the tonality features T to the baselines Wilson and SO-CALto obtain good results on small training sizes, be-cause over 75% for training would mean that a pos-sible practical implementation would not save muchhuman effort.4.4 Statistical Significance of the FeaturesWe perform a 10-fold cross validation with ourmethod, Wilson (as the best ?classical?
state-of-the-art-method) and SO-CAL (+ SVM) on the pressre-lations dataset in order to evaluate the contributionof single tonality features.
Our approach (Entropy-summand with all features) achieves an accuracy of61.94%, while Wilson gets 56.36% and SO-CAL46.68%.
As an analogy to Wilson et al(2009),we carry out a two-sided t-test with Wilson and SO-CAL (+ SVM) as baselines.
The results are shownin table 6.
The pluses indicate a significant increaseto the baseline, the minuses show a significant de-crease.
For one sign, changes are significant at thelevel p ?
0.1, two signs mean p ?
0.05, three signsp ?
0.025, four signs p ?
0.01 and five signs in-dicate p ?
0.005.
?nsc?
stands for no significantchange.As shown in table 6, the features with type z =sub are more important than the polarity features.
Inthe categories, the nouns and verbs are more signifi-cant than adjectives and adverbs (adverbs are a littlestronger in the polarity difference).
Combining allfeatures produces a very significant increase againstboth baselines.5 ConclusionWe have shown that the word connections outper-form state-of-the-art-methods in most cases of tonal-ity classification for a MRA.
As a major advantage,our approach does not need much training data.
Thecombination of all tonality features is a significantincrease against both baselines, too.
The findingsshow that the word connections in combination withthe entropy weighting allow to learn the tonalitystructure of different word combinations accurately,even though the training size is small.
This is a ma-jor advantage for a solution, which operates in prac-tice for media analysts, which have to analyse arti-cles for a MRA.So, this approach in combination with an extrac-tion of statements (Scholz and Conrad, 2013) andthe determination of viewpoints (Scholz and Con-rad, 2012) represents a fully automated solution inorder to perform Opinion Mining for a MRA.Acknowledgments.This work is funded by the German Federal Min-istry of Economics and Technology under the ZIM-program (Grant No.
KF2846501ED1).
The authorswant to thank the anonymous reviewers for theirvery helpful comments.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2009.
Multi-facet rating of product reviews.
In1837Proc.
of the 31th European Conf.
on IR Research onAdvances in Information Retrieval, ECIR ?09, pages461?472.Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov,Vanni Zavarella, Erik van der Goot, Matina Halkia,Bruno Pouliquen, and Jenya Belyaeva.
2010.
Senti-ment analysis in the news.
In Proc.
of the 7th intl.
conf.on Language Resources and Evaluation (LREC?10),pages 2216?2220.Danushka Bollegala, David Weir, and John Carroll.2011.
Using multiple sources to construct a sentimentsensitive thesaurus for cross-domain sentiment classi-fication.
In Proc.
of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 132?141.William W. Cohen.
1996.
Learning trees and rules withset-valued features.
In Proc.
of the 13th national con-ference on Artificial intelligence - Volume 1, AAAI?96,pages 709?716.
AAAI Press.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
Aholistic lexicon-based approach to opinion mining.
InProc.
of the Intl.
Conf.
on Web search and web datamining, WSDM ?08, pages 231?240.Weifu Du, Songbo Tan, Xueqi Cheng, and Xiaochun Yun.2010.
Adapting information bottleneck method forautomatic construction of domain-oriented sentimentlexicon.
In Proc.
of the 3rd ACM intl.
conf.
on Websearch and data mining, WSDM ?10, pages 111?120.Andrew B. Goldberg and Xiaojin Zhu.
2006.
Seeingstars when there aren?t many stars: graph-based semi-supervised learning for sentiment categorization.
InProc.
of the 1st Workshop on Graph Based Methods forNatural Language Processing, TextGraphs-1, pages45?52.Thorsten Joachims.
1999.
Advances in kernel meth-ods.
chapter Making large-scale support vector ma-chine learning practical, pages 169?184.
MIT Press,Cambridge, MA, USA.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing lexicon for sentiment analysis from massive col-lection of html documents.
In Proc.
of the 2007 JointConf.
on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 1075?1083.Solomon Kullback and Richard A. Leibler.
1951.
Oninformation and sufficiency.
Annals of MathematicalStatistics, 22(1):79?86.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Natalia Ponomareva and Mike Thelwall.
2012.
Doneighbours help?
: an exploration of graph-based al-gorithms for cross-domain sentiment classification.
InProc.
of the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, EMNLP-CoNLL?12, pages 655?665.Rani Qumsiyeh and Yiu-Kai Ng.
2012.
Predicting theratings of multimedia items for making personalizedrecommendations.
In Proc.
of the 35th intl.
ACM SI-GIR conf.
on Research and development in informationretrieval, SIGIR ?12, pages 475?484.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing three german treebanks: lexicalized and unlex-icalized baselines.
In Proc.
of the Workshop on Pars-ing German, PaGe ?08, pages 40?46.Robert Remus, Uwe Quasthoff, and Gerhard Heyer.2010.
Sentiws ?
a publicly available german-languageresource for sentiment analysis.
In Proc.
of the 7thIntl.
Conf.
on Language Resources and Evaluation(LREC?10), pages 1168?1171.Kiran Sarvabhotla, Prasad Pingali, and Vasudeva Varma.2011.
Sentiment classification: a lexical similaritybased approach for extracting subjectivity in docu-ments.
Inf.
Retr., 14(3):337?353.Robert E. Schapire and Yoram Singer.
2000.
Boostex-ter: A boosting-based systemfor text categorization.Mach.
Learn., 39(2-3):135?168.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to german.
In Proc.
of theACL SIGDAT-Workshop, pages 47?50.Thomas Scholz and Stefan Conrad.
2012.
Integratingviewpoints into newspaper opinion mining for a mediaresponse analysis.
In Proc.
of the 11th conf.
on Nat-ural Language Processing (KONVENS 2012), pages30?38.Thomas Scholz and Stefan Conrad.
2013.
Extraction ofstatements in news for a media response analysis.
InProc.
of the 18th Intl.
conf.
on Applications of Natu-ral Language Processing to Information Systems 2013(NLDB 2013), pages 1?12.Thomas Scholz, Stefan Conrad, and Lutz Hillekamps.2012a.
Opinion mining on a german corpus of a me-dia response analysis.
In Proc.
of the 15th Interna-tional Conference on Text, Speech and Dialogue (TSD2012), pages 39?46.Thomas Scholz, Stefan Conrad, and Isabel Wolters.2012b.
Comparing different methods for opinion min-ing in newspaper articles.
In Proc.
of the 17th Intl.conf.
on Applications of Natural Language Process-ing to Information Systems 2012 (NLDB 2012), pages259?264.Claude E. Shannon.
1948.
A mathematical theory ofcommunication.
The Bell system technical journal,27:379?423.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based1838methods for sentiment analysis.
Comput.
Linguist.,37(2):267?307.Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, andMing Zhang.
2011.
Topic sentiment analysis in twit-ter: a graph-based hashtag sentiment classification ap-proach.
In Proc.
of the 20th ACM intl.
conf.
on Infor-mation and knowledge management, CIKM ?11, pages1031?1040.Tom Watson and Paul Noble, 2007.
Evaluating public re-lations: a best practice guide to public relations plan-ning, research & evaluation, chapter 6, pages 107?138.
PR in practice series.
Kogan Page.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational Linguistics, 35(3):399?433.Qiong Wu, Songbo Tan, and Xueqi Cheng.
2009.
Graphranking for sentiment transfer.
In Proc.
of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort?09, pages 317?320.1839
