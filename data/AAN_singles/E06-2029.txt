Adaptivity in Question Answeringwith User Modelling and a Dialogue InterfaceSilvia Quarteroni and Suresh ManandharDepartment of Computer ScienceUniversity of YorkYork YO10 5DDUK{silvia,suresh}@cs.york.ac.ukAbstractMost question answering (QA) and infor-mation retrieval (IR) systems are insensi-tive to different users?
needs and prefer-ences, and also to the existence of multi-ple, complex or controversial answers.
Weintroduce adaptivity in QA and IR by cre-ating a hybrid system based on a dialogueinterface and a user model.
Keywords:question answering, information retrieval,user modelling, dialogue interfaces.1 IntroductionWhile standard information retrieval (IR) systemspresent the results of a query in the form of aranked list of relevant documents, question an-swering (QA) systems attempt to return them inthe form of sentences (or paragraphs, or phrases),responding more precisely to the user?s request.However, in most state-of-the-art QA systemsthe output remains independent of the questioner?scharacteristics, goals and needs.
In other words,there is a lack of user modelling: a 10-year-old anda University History student would get the sameanswer to the question: ?When did the MiddleAges begin??.
Secondly, most of the effort of cur-rent QA is on factoid questions, i.e.
questions con-cerning people, dates, etc., which can generally beanswered by a short sentence or phrase (Kwok etal., 2001).
The main QA evaluation campaign,TREC-QA 1, has long focused on this type ofquestions, for which the simplifying assumption isthat there exists only one correct answer.
Even re-cent TREC campaigns (Voorhees, 2003; Voorhees,2004) do not move sufficiently beyond the factoidapproach.
They account for two types of non-factoid questions ?list and definitional?
but not fornon-factoid answers.
In fact, a) TREC defines listquestions as questions requiring multiple factoid1http://trec.nist.govanswers, b) it is clear that a definition questionmay be answered by spotting definitional passages(what is not clear is how to spot them).
However,accounting for the fact that some simple questionsmay have complex or controversial answers (e.g.
?What were the causes of World War II??)
remainsan unsolved problem.
We argue that in such situa-tions returning a short paragraph or text snippet ismore appropriate than exact answer spotting.
Fi-nally, QA systems rarely interact with the user:the typical session involves the user submitting aquery and the system returning a result; the sessionis then concluded.To respond to these deficiencies of existing QAsystems, we propose an adaptive system where aQA module interacts with a user model and a di-alogue interface (see Figure 1).
The dialogue in-terface provides the query terms to the QA mod-ule, and the user model (UM) provides criteriato adapt query results to the user?s needs.
Givensuch information, the goal of the QA module is tobe able to discriminate between simple/factoid an-swers and more complex answers, presenting themin a TREC-style manner in the first case and moreappropriately in the second.DIALOGUEINTERFACEQUESTIONPROCESSINGDOCUMENTRETRIEVALANSWEREXTRACTIONUSERMODELQuestionAnswerQA MODULEFigure 1: High level system architectureRelated work To our knowledge, our system isamong the first to address the need for a differentapproach to non-factoid (complex/controversial)199answers.
Although the three-tiered structure ofour QA module reflects that of a typical web-based QA system, e.g.
MULDER (Kwok et al,2001), a significant aspect of novelty in our archi-tecture is that the QA component is supported bythe user model.
Additionally, we drastically re-duce the amount of linguistic processing appliedduring question processing and answer generation,while giving more relief to the post-retrieval phaseand to the role of the UM.2 User modelDepending on the application of interest, the UMcan be designed to suit the information needs ofthe QA module in different ways.
As our currentapplication, YourQA2, is a learning-oriented, web-based system, our UM consists of the user?s:1) age range, a ?
{7 ?
11, 11 ?
16, adult};2) reading level, r ?
{poor,medium, good};3) webpages of interest/bookmarks, w.Analogies can be found with the SeAn (Ardissonoet al, 2001) and SiteIF (Magnini and Strapparava,2001) news recommender systems where age andbrowsing history, respectively, are part of the UM.In this paper we focus on how to filter and adaptsearch results using the reading level parameter.3 Dialogue interfaceThe dialogue component will interact with boththe UM and the QA module.
From a UM point ofview, the dialogue history will store previous con-versations useful to construct and update a modelof the user?s interests, goals and level of under-standing.
From a QA point of view, the main goalof the dialogue component is to provide users witha friendly interface to build their requests.
A typi-cal scenario would start this way:?
System: Hi, how can I help you??
User: I would like to know what books Roald Dahl wrote.The query sentence ?what books Roald Dahl wrote?, isthus extracted and handed to the QA module.
In asecond phase, the dialogue module is responsiblefor providing the answer to the user once the QAmodule has generated it.
The dialogue managerconsults the UM to decide on the most suitableformulation of the answer (e.g.
short sentences)and produce the final answer accordingly, e.g.:?
System: Roald Dahl wrote many books for kids and adults,including: ?The Witches?, ?Charlie and the Chocolate Fac-tory?, and ?James and the Giant Peach".2http://www.cs.york.ac.uk/aig/aqua4 Question Answering ModuleThe flow between the three QA phases ?
questionprocessing, document retrieval and answer gener-ation ?
is described below (see Fig.
2).4.1 Question processingWe perform query expansion, which consists increating additional queries using question wordsynonyms in the purpose of increasing the recallof the search engine.
Synonyms are obtained viathe WordNet 2.0 3 lexical database.Question QUERYEXPANSIONDOCUMENTRETRIEVALKEYPHRASEEXTRACTIONESTIMATIONOF READINGLEVELSCLUSTERINGLanguageModelsUM-BASEDFILTERINGSEMANTICSIMILARITYRANKINGUser ModelReadingLevelRankedAnswerCandidatesFigure 2: Diagram of the QA module4.2 RetrievalDocument retrieval We retrieve the top 20 doc-uments returned by Google4 for each query pro-duced via query expansion.
These are processedin the following steps, which progressively narrowthe part of the text containing relevant informa-tion.Keyphrase extraction Once the documents areretrieved, we perform keyphrase extraction to de-termine their three most relevant topics using Kea(Witten et al, 1999), an extractor based on Na?veBayes classification.Estimation of reading levels To adapt the read-ability of the results to the user, we estimatethe reading difficulty of the retrieved documentsusing the Smoothed Unigram Model (Collins-Thompson and Callan, 2004), which proceeds in3http://wordnet.princeton.edu4http://www.google.com200two phases.
1) In the training phase, sets of repre-sentative documents are collected for a given num-ber of reading levels.
Then, a unigram languagemodel is created for each set, i.e.
a list of (wordstem, probability) entries for the words appearingin its documents.
Our models account for the fol-lowing reading levels: poor (suitable for ages 7?11), medium (ages 11?16) and good (adults).
2)In the test phase, given an unclassified documentD, its estimated reading level is the model lmimaximizing the likelihood that D ?
lmi5.Clustering We use the extracted topics and es-timated reading levels as features to apply hierar-chical clustering on the documents.
We use theWEKA (Witten and Frank, 2000) implementationof the Cobweb algorithm.
This produces a treewhere each leaf corresponds to one document, andsibling leaves denote documents with similar top-ics and reading difficulty.4.3 Answer extractionIn this phase, the clustered documents are filteredbased on the user model and answer sentences arelocated and formatted for presentation.UM-based filtering The documents in the clus-ter tree are filtered according to their reading diffi-culty: only those compatible with the UM?s read-ing level are retained for further analysis6.Semantic similarity Within each of the retaineddocuments, we seek the sentences which are se-mantically most relevant to the query by applyingthe metric in (Alfonseca et al, 2001): we rep-resent each document sentence p and the queryq as word sets P = {pw1, .
.
.
, pwm} and Q ={qw1, .
.
.
, qwn}.
The distance from p to q is thendistq(p) =?1?i?m minj [d(pwi, qwj)], whered(pwi, qwj) is the word-level distance betweenpwi and qwj based on (Jiang and Conrath, 1997).Ranking Given the query q, we thus locatein each document D the sentence p?
such thatp?
= argminp?D[distq(p)]; then, distq(p?)
be-comes the document score.
Moreover, each clus-5The likelihood is estimated using the formula:Li,D =?w?D C(w, D) ?
log(P (w|lmi)), where w is aword in the document, C(w, d) is the number of occurrencesof w in D and P (w|lmi) is the probability with which woccurs in lmi6However, if their number does not exceed a given thresh-old, we accept in our candidate set part of the documents hav-ing the next lowest readability ?
or a medium readability if theuser?s reading level is lowter is assigned a score consisting in the maximalscore of the documents composing it.
This allowsto rank not only documents, but also clusters, andpresent results grouped by cluster in decreasing or-der of document score.Answer presentation We present our answersin an HTML page, where results are listed follow-ing the ranking described above.
Each result con-sists of the title and clickable URL of the originat-ing document, and the passage where the sentencewhich best answers the query is located and high-lighted.
Question keywords and potentially usefulinformation such as named entities are in colour.5 Sample resultWe have been running our system on a rangeof queries, including factoid/simple, complex andcontroversial ones.
As an example of the latter, wereport the query ?Who wrote the Iliad?
?, which isa subject of debate.
These are some top results:?
UMgood: ?Most Classicists would agree that, whetherthere was ever such a composer as "Homer" or not, theHomeric poems are the product of an oral tradition [.
.
.
]Could the Iliad and Odyssey have been oral-formulaic po-ems, composed on the spot by the poet using a collection ofmemorized traditional verses and phases???
UMmed: ?No reliable ancient evidence for Homer ?[.
.
. ]
General ancient assumption that same poet wrote Il-iad and Odyssey (and possibly other poems) questioned bymany modern scholars: differences explained biographi-cally in ancient world (e g wrote Od.
in old age); but simi-larities could be due to imitation.??
UMpoor: ?Homer wrote The Iliad and The Odyssey(at least, supposedly a blind bard named "Homer" did).
?In the three results, the problem of attribution ofthe Iliad is made clearly visible: document pas-sages provide a context which helps to explain thecontroversy at different levels of difficulty.6 EvaluationSince YourQA does not single out one correct an-swer phrase, TREC evaluation metrics are not suit-able for it.
A user-centred methodology to assesshow individual information needs are met is moreappropriate.
We base our evaluation on (Su, 2003),which proposes a comprehensive search engineevaluation model, defining the following metrics:1.
Relevance: we define strict precision (P1) asthe ratio between the number of results rated asrelevant and all the returned results, and loose pre-201cision (P2) as the ratio between the number of re-sults rated as relevant or partially relevant and allthe returned results.2.
User satisfaction: a 7-point Likert scale7 is usedto assess the user?s satisfaction with loose preci-sion of results (S1) and query success (S2).3.
Reading level accuracy: given the set R of re-sults returned for a reading level r, Ar is the ratiobetween the number of results ?
R rated by theusers as suitable for r and |R|.4.
Overall utility (U ): the search session as awhole is assessed via a 7-point Likert scale.We performed our evaluation by running 24queries (some of which in Tab.
2) on Google andYourQA and submitting the results ?i.e.
Googleresult page snippets and YourQA passages?
ofboth to 20 evaluators, along with a questionnaire.The relevance results (P1 and P2) in Tab.
1 show aP1 P2 S1 S2 UGoogle 0,39 0,63 4,70 4,61 4,59YourQA 0,51 0,79 5,39 5,39 5,57Table 1: Evaluation results10-15% difference in favour of YourQA for bothstrict and loose precision.
The coarse seman-tic processing applied and context visualisationthus contribute to creating more relevant passages.Both user satisfaction results (S1 and S2) in Tab.1 also denote a higher level of satisfaction tributedto YourQA.
Tab.
2 shows that evaluators found ourQuery Ag Am ApWhen did the Middle Ages begin?
0,91 0,82 0,68Who painted the Sistine Chapel?
0,85 0,72 0,79When did the Romans invade Britain?
0,87 0,74 0,82Who was a famous cubist?
0,90 0,75 0,85Who was the first American in space?
0,94 0,80 0,72Definition of metaphor 0,95 0,81 0,38average 0,94 0,85 0,72Table 2: Sample queries and accuracy valuesresults appropriate for the reading levels to whichthey were assigned.
The accuracy tended to de-crease (from 94% to 72%) with the level: it isindeed more constraining to conform to a lowerreading level than to a higher one.
Finally, the7This measure ?
ranging from 1= ?extremely unsatisfac-tory?
to 7=?extremely satisfactory?
?
is particularly suitableto assess how well a system meets user?s search needs.general satisfaction values for U in Tab.
1 showan improved preference for YourQA.7 ConclusionA user-tailored QA system is proposed where auser model contributes to adapting answers to theuser?s needs and presenting them appropriately.A preliminary evaluation of our core QA moduleshows a positive feedback from human assessors.Our short term goals involve performing a moreextensive evaluation and implementing a dialogueinterface to improve the system?s interactivity.ReferencesE.
Alfonseca, M. DeBoni, J.-L. Jara-Valencia, andS.
Manandhar.
2001.
A prototype question answer-ing system using syntactic and semantic informationfor answer retrieval.
In Text REtrieval Conference.L.
Ardissono, L. Console, and I. Torre.
2001.
An adap-tive system for the personalized access to news.
AICommun., 14(3):129?147.K.
Collins-Thompson and J. P. Callan.
2004.
A lan-guage modeling approach to predicting reading dif-ficulty.
In Proceedings of HLT/NAACL.J.
J. Jiang and D. W. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proceedings of the International Conference Re-search on Computational Linguistics (ROCLING X).C.
C. T. Kwok, O. Etzioni, and D. S. Weld.
2001.
Scal-ing question answering to the web.
In World WideWeb, pages 150?161.Bernardo Magnini and Carlo Strapparava.
2001.
Im-proving user modelling with content-based tech-niques.
In UM: Proceedings of the 8th Int.
Confer-ence, volume 2109 of LNCS.
Springer.L.
T. Su.
2003.
A comprehensive and systematicmodel of user evaluation of web search engines: Ii.an evaluation by undergraduates.
J.
Am.
Soc.
Inf.Sci.
Technol., 54(13):1193?1223.E.
M. Voorhees.
2003.
Overview of the TREC 2003question answering track.
In Text REtrieval Confer-ence.E.
M. Voorhees.
2004.
Overview of the TREC 2004question answering track.
In Text REtrieval Confer-ence.H.
Witten and E. Frank.
2000.
Data Mining: PracticalMachine Learning Tools and Techniques with JavaImplementation.
Morgan Kaufmann.I.
H. Witten, G. W. Paynter, E. Frank, C. Gutwin, andC.
G. Nevill-Manning.
1999.
KEA: Practical au-tomatic keyphrase extraction.
In ACM DL, pages254?255.202
