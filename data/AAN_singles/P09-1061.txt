Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 540?548,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSemi-supervised Learning for Automatic Prosodic Event DetectionUsing Co-training AlgorithmJe Hun Jeon and Yang LiuComputer Science DepartmentThe University of Texas at Dallas, Richardson, TX, USA{jhjeon,yangl}@hlt.utdallas.eduAbstractMost of previous approaches to automaticprosodic event detection are based on su-pervised learning, relying on the avail-ability of a corpus that is annotated withthe prosodic labels of interest in order totrain the classification models.
However,creating such resources is an expensiveand time-consuming task.
In this paper,we exploit semi-supervised learning withthe co-training algorithm for automatic de-tection of coarse level representation ofprosodic events such as pitch accents, in-tonational phrase boundaries, and breakindices.
We propose a confidence-basedmethod to assign labels to unlabeled dataand demonstrate improved results usingthis method compared to the widely usedagreement-based method.
In addition, weexamine various informative sample selec-tion methods.
In our experiments on theBoston University radio news corpus, us-ing only a small amount of the labeled dataas the initial training set, our proposed la-beling method combined with most confi-dence sample selection can effectively useunlabeled data to improve performanceand finally reach performance closer tothat of the supervised method using all thetraining data.1 IntroductionProsody represents suprasegmental information inspeech since it normally extends over more thanone phoneme segment.
Prosodic phenomena man-ifest themselves in speech in different ways, in-cluding changes in relative intensity to emphasizespecific words or syllables, variations of the fun-damental frequency range and contour, and subtletiming variations, such as syllable lengthening andinsertion of pause.
In spoken utterances, speakersuse prosody to convey emphasis, intent, attitude,and emotion.
These are important cues to aid thelistener for interpretation of speech.
Prosody alsoplays an important role in automatic spoken lan-guage processing tasks, such as speech act detec-tion and natural speech synthesis, because it in-cludes aspect of higher level information that isnot completely revealed by segmental acoustics orlexical information.To represent prosodic events for the categoricalannotation schemes, one of the most popular label-ing schemes is the Tones and Break Indices (ToBI)framework (Silverman et al, 1992).
The most im-portant prosodic phenomena captured within thisframework include pitch accents (or prominence)and prosodic phrase boundaries.
Within the ToBIframework, prosodic phrasing refers to the per-ceived grouping of words in an utterance, andaccent refers to the greater perceived strength oremphasis of some syllables in a phrase.
Cor-pora annotated with prosody information can beused for speech analysis and to learn the relation-ship between prosodic events and lexical, syntac-tic and semantic structure of the utterance.
How-ever, it is very expensive and time-consuming toperform prosody labeling manually.
Therefore,automatic labeling of prosodic events is an attrac-tive alternative that has received attention over thepast decades.
In addition, automatically detectingprosodic events also benefits many other speechunderstanding tasks.Many previous efforts on prosodic event de-tection were supervised learning approaches thatused acoustic, lexical, and syntactic cues.
How-ever, the major drawback with these methods isthat they require a hand-labeled training corpusand depend on specific corpus used for training.Limited research has been conducted using unsu-pervised and semi-supervised methods.
In this pa-per, we exploit semi-supervised learning with the540Figure 1: An example of ToBI annotation on a sentence ?Hennessy will be a hard act to follow.
?co-training algorithm (Blum and Mitchell, 1998)for automatic prosodic event labeling.
Two dif-ferent views according to acoustic and lexical-syntactic knowledge sources are used in the co-training framework.
We propose a confidence-based method to assign labels to unlabeled datain training iterations and evaluate its performancecombined with different informative sample se-lection methods.
Our experiments on the BostonRadio News corpus show that the use of unla-beled data can lead to significant improvementof prosodic event detection compared to usingthe original small training set, and that the semi-supervised learning result is comparable with su-pervised learning with similar amount of trainingdata.The remainder of this paper is organized as fol-lows.
In the next section, we provide details ofthe corpus and the prosodic event detection tasks.Section 3 reviews previous work briefly.
In Sec-tion 4, we describe the classification method forprosodic event detection, including the acousticand syntactic prosodic models, and the featuresused.
Section 5 introduces the co-training algo-rithm we used.
Section 6 presents our experimentsand results.
The final section gives a brief sum-mary along with future directions.2 Corpus and tasksIn this paper, our experiments were carried outon the Boston University Radio News Corpus(BU) (Ostendorf et al, 2003) which consistsof broadcast news style read speech and hasToBI-style prosodic annotations for a part of thedata.
The corpus is annotated with orthographictranscription, automatically generated and hand-corrected part-of-speech (POS) tags, and auto-matic phone alignments.The main prosodic events that we are concernedto detect automatically in this paper are phrasingand accent (or prominence).
Prosodic phrasingrefers to the perceived grouping of words in an ut-terance, and prominence refers to the greater per-ceived strength or emphasis of some syllables ina phrase.
In the ToBI framework, the pitch accenttones (*) are marked at every accented syllable andhave five types according to pitch contour: H*, L*,L*+H, L+H*, H+!H*.
The phrase boundary tonesare marked at every intermediate phrase boundary(L-, H-) or intonational phrase boundary (L-L%,L-H%, H-H%, H-L%) at certain word boundaries.There are also the break indices at every wordboundary which range in value from 0 through4, where 4 means intonational phrase boundary, 3means intermediate phrase boundary, and a valueunder 3 means phrase-medial word boundary.
Fig-ure 1 shows a ToBI annotation example for a sen-tence ?Hennessy will be a hard act to follow.?
Thefirst and second tiers show the orthographic infor-mation such as words and syllables of the utter-ance.
The third tier shows the accents and phraseboundary tones.
The accent tone is located on eachaccented syllable, such as the first syllable of word?Hennessy.?
The boundary tone is marked on ev-ery final syllable if there is a prosodic boundary.For example, there are intermediate phrase bound-aries after words ?Hennessy?
and ?act?, and thereis an intonational phrase boundary after word ?fol-low.?
The fourth tier shows the break indices at theend of every word.The detailed representation of prosodic eventsin the ToBI framework creates a serious sparsedata problem for automatic prosody detection.This problem can be alleviated by grouping ToBIlabels into coarse categories, such as presence orabsence of pitch accents and phrasal tones.
Thisalso significantly reduces ambiguity of the task.
Inthis paper, we thus use coarse representation (pres-ence versus absence) for three prosodic event de-tection tasks:541?
Pitch accents: accent mark (*) means pres-ence.?
Intonational phrase boundaries (IPB): all ofthe IPB tones (%) are grouped into one cate-gory.?
Break indices: value 3 and 4 are grouped to-gether to represent that there is a break.
Thistask is equivalent to detecting the presence ofintermediate and intonational phrase bound-aries.These three tasks are binary classification prob-lems.
Similar setup has also been used in otherprevious work.3 Previous workMany previous efforts on prosodic event detec-tion used supervised learning approaches.
In thework by Wightman and Ostendorf (1994), binaryaccent, IPB, and break index were assigned tosyllables based on posterior probabilities com-puted from acoustic evidence using decision trees,combined with a bigram model of accent andboundary patterns.
Their method achieved anaccuracy of 84% for accent, 71% for IPB, and84% for break index detection at the syllablelevel.
Chen et al (2004) used a Gaussian mix-ture model for acoustic-prosodic information andneural network based syntactic-prosodic modeland achieved pitch accent detection accuracy of84% and IPB detection accuracy of 90% at theword level.
The experiments of Ananthakrish-nan and Narayanan (2008) with neural networkbased acoustic-prosodic model and a factored n-gram syntactic model reported 87% accuracy onaccent and break index detection at the syllablelevel.
The work of Sridhar et al (2008) using amaximum entropy model achieved accent and IPBdetection accuracies of 86% and 93% on the wordlevel.Limited research has been done in prosodicdetection using unsupervised or semi-supervisedmethods.
Ananthakrishnan and Narayanan (2006)proposed an unsupervised algorithm for prosodicevent detection.
This algorithm was based on clus-tering techniques to make use of acoustic and syn-tactic cues and achieved accent and IPB detec-tion accuracies of 77.8% and 88.5%, comparedwith the accuracies of 86.5% and 91.6% with su-pervised methods.
Similarly, Levow (2006) triedclustering based unsupervised approach on ac-cent detection with only acoustic evidence andreported accuracy of 78.4% for accent detectioncompared with 80.1% using supervised learning.She also exploited a semi-supervised approach us-ing Laplacian SVM classification on a small set ofexamples.
This approach achieved 81.5%, com-pared to 84% accuracy for accent detection in afully supervised fashion.Since Blum and Mitchell (1998) proposed co-training, it has received a lot of attention in the re-search community.
This multi-view setting applieswell to learning problems that have a natural wayto divide their features into subsets, each of whichare sufficient to learn the target concept.
Theo-retical and empirical analysis has been performedfor the effectiveness of co-training such as Blumand Mitchell (1998), Goldman and Zhou (2000),Nigam and Ghani (2000), and Dasuta et al (2001).More recently, researchers have begun to exploreways of combing ideas from sample selection withthat of co-training.
Steedman et al (2003) ap-plied co-training method to statistical parsing andintroduced sample selection heuristics.
Clark etal.
(2003) and Wang et al (2007) applied co-training method in POS tagging using agreement-based selection strategy.
Co-testing (Muslea etal., 2000), one of active learning approaches, hasa similar spirit.
Like co-training, it consists oftwo classifiers with redundant views and comparestheir outputs for an unlabeled example.
If theydisagree, then the example is considered as a con-tention point, and therefore a good candidate forhuman labeling.In this paper, we apply co-training algorithmto automatic prosodic event detection and proposemethods to better select samples to improve semi-supervised learning performance for this task.4 Prosodic event detection methodWe model the prosody detection problem as a clas-sification task.
We separately develop acoustic-prosodic and syntactic-prosodic models accord-ing to information sources and then combine thetwo models.
Our previous supervised learning ap-proach (Jeon and Liu, 2009) showed that a com-bined model using Neural Network (NN) classifierfor acoustic-prosodic evidence and Support VectorMachine (SVM) classifier for syntactic-prosodicevidence performed better than other classifiers.We therefore use NN and SVM in this study.
Note542that our feature extraction is performed at the syl-lable level.
This is straightforward for accent de-tection since stress is defined associated with syl-lables.
In the case of IPB and break index detec-tion, we use only the features from the final syl-lable of a word since those events are associatedwith word boundaries.4.1 The acoustic-prosodic modelThe most likely sequence of prosodic events P ?
={p?1, .
.
.
, p?n} given the sequence of acoustic evi-dences A = {a1, .
.
.
, an} can be found as follow-ing:P ?
= arg maxPp(P |A)?
arg maxPn?i=1p(pi|ai) (1)where ai = {a1i , .
.
.
, ati} is the acoustic featurevector corresponding to a syllable.
Note that thisassumes that the prosodic events are independentand they are only dependent on the acoustic obser-vations in the corresponding locations.The primary acoustic cues for prosodic eventsare pitch, energy and duration.
In order to reducethe effect by both inter-speaker and intra-speakervariation, both pitch and energy values were nor-malized (z-value) with utterance specific meansand variances.
The acoustic features used in ourexperiments are listed below.
Again, all of the fea-tures are computed for a syllable.?
Pitch range (4 features): maximum pitch,minimum pitch, mean pitch, and pitch range(difference between maximum and minimumpitch).?
Pitch slope (5 features): first pitch slope, lastpitch slope, maximum plus pitch slope, max-imum minus pitch slope, and the number ofchanges in the pitch slope patterns.?
Energy range (4 features): maximum en-ergy, minimum energy, mean energy, andenergy range (difference between maximumand minimum energy).?
Duration (3 features): normalized vowel du-ration, pause duration after the word final syl-lable, and the ratio of vowel durations be-tween this syllable and the next syllable.Among the duration features, the pause dura-tion and the ratio of vowel durations are only usedto detect IPB and break index, not for accent de-tection.4.2 The syntactic-prosodic modelThe prosodic events P ?
given the sequence of lex-ical and syntactic evidences S = {s1, .
.
.
, sn} canbe found as following:P ?
= arg maxPp(P |S)?
arg maxPn?i=1p(pi|?
(si)) (2)where ?
(si) is chosen such that it contains lexi-cal and syntactic evidence from a fixed window ofsyllables surrounding location i.There is a very strong correlation between theprosodic events in an utterance and its lexical andsyntactic structure.
Previous studies have shownthat for pitch accent detection, the lexical featuressuch as the canonical stress patterns from the pro-nunciation dictionary perform better than the syn-tactic features, while for IPB and break index de-tection, the syntactic features such as POS workbetter than the lexical features.
We use differentfeature types for each task and the detailed fea-tures are as follows:?
Accent detection: syllable identity, lexicalstress (exist or not), word boundary informa-tion (boundary or not), and POS tag.
Wealso include syllable identity, lexical stress,and word boundary features from the previ-ous and next context window.?
IPB and Break index detection: POS tag, theratio of syntactic phrases the word initiates,and the ratio of syntactic phrases the wordterminates.
All of these features from the pre-vious and next context windows are also in-cluded.4.3 The combined modelThe two models above can be coupled as a classi-fier for prosodic event detection.
If we assume thatthe acoustic observations are conditionally inde-pendent of the syntactic features given the prosodylabels, the task of prosodic detection is to find theoptimal sequence P ?
as follows:P ?
= arg maxPp(P |A,S)543?
arg maxPp(P |A)p(P |S)?
arg maxPn?i=1p(pi|ai)?p(pi|?
(si)) (3)where ?
is a parameter that can be used to adjustthe weighting between syntactic and the acousticmodel.
In our experiments, the value of ?
is esti-mated based on development data.5 Co-training strategy for prosodic eventdetectionCo-training (Blum and Mitchell, 1998) is a semi-supervised multi-view algorithm that uses the ini-tial training set to learn a (weak) classifier in eachview.
Then each classifier is applied to all theunlabeled examples.
Those examples that eachclassifier makes the most confident predictions areselected and labeled with the estimated class la-bels and added to the training set.
Based on thenew training set, a new classifier is learned in eachview, and the whole process is repeated for someiterations.
At the end, a final hypothesis is cre-ated by combining the predictions of the classifierslearned in each view.As described in Section 4, we use two classi-fiers for the prosodic event detection task basedon two different information sources: one is theacoustic evidence extracted from the speech signalof an utterance; the other is the lexical and syn-tactic evidence such as syllables, words, POS tagsand phrasal boundary information.
These are twodifferent views for prosodic event detection and fitthe co-training framework.The general co-training algorithm we used isdescribed in Algorithm 1.
Given a set L of labeleddata and a set U of unlabeled data, the algorithmfirst creates a smaller pool U?
containing u unla-beled data.
It then iterates in the following proce-dure.
First, we use L to train two distinct classi-fiers: the acoustic-prosodic classifier h1, and thesyntactic classifier h2.
These two classifiers areused to examine the unlabeled set U?
and assign?possible?
labels.
Then we select some samplesto add to L. Finally, the pool U?
is recreated fromU at random.
This iteration continues until reach-ing the defined number of iterations or U is empty.The main issue of co-training is to select train-ing samples for next iteration so as to minimizenoise and maximize training utility.
There are twoissues: (1) the accurate self-labeling method forunlabeled data and (2) effective heuristics to se-Algorithm 1 General co-training algorithm.Given a set L of labeled training data and a setU of unlabeled dataRandomly select U?
from U, |U?|=uwhile iteration < k doUse L to train classifiers h1 and h2Apply h1 and h2 to assign labels for all ex-amples in U?Select n self-labeled samples and add to LRemove these n samples from URecreate U?
by choosing u instances ran-domly from Uend whilelect more informative examples.
We investigatedifferent approaches to address these issues forthe prosodic event detection task.
The first is-sue is how to assign possible labels accurately.The general method is to let the two classifierspredict the class for a given sample, and if theyagree, the hypothesized label is used.
However,when this agreement-based approach is used forprosodic event detection, we notice that there isnot only difference in the labeling accuracy be-tween positive and negative samples, but also animbalance of the self-labeled positive and negativeexamples (details in Section 6).
Therefore we be-lieve that using the hard decisions from the twoclassifiers along with the agreement-based rule isnot enough to label the unlabeled samples.
To ad-dress this problem, we propose an approximatedconfidence measure based on the combined classi-fier (Equation 3).
First, we take a squared root ofthe classifier?s posterior probabilities for the twoclasses, denoted as score(pos) and score(neg),respectively.
Our proposed confidence is the dis-tance between these two scores.
For example, ifthe classifier?s hypothesized label is positive, then:Positive confidence=score(pos)-score(neg)Similarly if the classifier?s hypothesis is negative,we calculate a negative confidence:Negative confidence=score(neg)-score(pos)Then we apply different thresholds of confi-dence level for positive and negative labeling.
Thethresholds are chosen based on the accuracy distri-bution obtained on the labeled development dataand are reestimated at every iteration.
Figure 2shows the accuracy distribution for accent detec-tion according to different confidence levels in thefirst iteration.
In Figure 2, if we choose 70% label-ing accuracy, the positive confidence level is about5440 0.2 0.4 0.6 0.8 10.20.40.60.81Confidence levelAccuracyFigure 2: Approximated confidence level and la-beling accuracy on accent detection task.0.1 and the negative confidence level is about 0.8.In our confidence-based approach, the sampleswith a confidence level higher than these thresh-olds are assigned with the classifier?s hypothesizedlabels, and the other samples are disregarded.The second problem in co-training is how toselect informative samples.
Active learning ap-proaches, such as Muslea et al (2000), can gener-ally select more informative samples, for example,samples for which two classifiers disagree (sinceone of two classifiers is wrong) and ask for humanlabels.
Co-training approaches cannot, however,use this selection method since there is a risk tolabel the disagreed samples.
Usually co-trainingselects samples for which two classifiers have thesame prediction but high difference in their con-fidence measures.
Based on this idea, we appliedthree sampling strategies on top of our confidence-based labeling method:?
Random selection: randomly select samplesfrom those that the two classifiers have dif-ferent posterior probabilities.?
Most confident selection: select samples thathave the highest posterior probability basedon one classifier, and at the same time thereis certain posterior probability difference be-tween the two classifiers.?
Most different selection: select samples thathave the most difference between the twoclassifiers?
posterior probabilities.The first strategy is appropriate for base classi-fiers that lack the capability of estimating the pos-terior probability of their predictions.
The secondis appropriate for base classifiers that have highclassification accuracy and also with high poste-rior probability.
The last one is also appropriatefor accurate classifiers and expected to convergeutter.
word syll SpeakerTest Set 102 5,448 8,962 f1a, m1bDevelopment Set 20 1,356 2,275 f2b, f3bLabeled set L 5 347 573 m2b, m3bUnlabeled set U 1,027 77,207 129,305 m4bTable 1: Training and test sets.faster since big mistakes of one of the two classi-fiers can be fixed.
These sample selection strate-gies share some similarity with those in previouswork (Steedman et al, 2003).6 Experiments and resultsOur goal is to determine whether the co-trainingalgorithm described above could successfully usethe unlabeled data for prosodic event detection.
Inour experiment, 268 ToBI labeled utterances and886 unlabeled utterances in BU corpus were used.Among labeled data, 102 utterances of all f1a andm1b speakers are used for testing, 20 utterancesrandomly chosen from f2b, f3b, m2b, m3b, andm4b are used as development set to optimize pa-rameters such as ?
and confidence level thresh-old, 5 utterances are used as the initial trainingset L, and the rest of the data is used as unlabeledset U, which has 1027 unlabeled utterances (weremoved the human labels for co-training exper-iments).
The detailed training and test setting isshown in Table 1.First of all, we compare the learning curves us-ing our proposed confidence-based method to as-sign possible labels with the simple agreement-based random selection method.
We expect that ifself-labeling is accurate, adding new samples ran-domly drawn from these self-labeled data gener-ally should not make performance worse.
For thisexperiment, in every iteration, we randomly se-lect the self-labeled samples that have at least 0.1difference between two classifiers?
posterior prob-abilities.
The number of new samples added totraining is 5% of the size of the previous trainingdata.
Figure 3 shows the learning curves for accentdetection.
The number of samples in the x-axisis the number of syllables.
The F-measure scoreusing the initial training data is 0.69.
The darksolid line in Figure 3 is the learning curve of thesupervised method when varying the size of thetraining data.
Compared with supervised method,our proposed relative confidence-based labelingmethod shows better performance when there is5455,000 10,000 15,0000.550.60.650.70.750.80.85# of samplesF?measureSupervisedAgreement basedConfidence basedFigure 3: The learning curve of agreement-basedand our proposed confidence-based random selec-tion methods for accent detection.Confidence AgreementAccentdetection% of P samples 47% 38%P sample error 0.17 0.09N sample error 0.12 0.22IPBdetection% of P samples 46% 19%P sample error 0.12 0.01N sample error 0.18 0.53Breakdetection% of P samples 50% 25%P sample error 0.15 0.03N sample error 0.17 0.42Table 2: Percentage of positive samples, andaveraged error rate for positive (P) and nega-tive (N) samples for the first 20 iterations usingthe agreement-based and our confidence labelingmethods.less data, but after some iteration, the performanceis saturated earlier.
However, the agreement-basedmethod does not yield any performance gain, in-stead, its performance is much worse after someiteration.
The other two prosodic event detectiontasks also show similar patterns.To analyze the reason for this performancedegradation using the agreement-based method,we compare the labels of the newly added samplesin random selection with the reference annotation.Table 2 shows the percentage of the positive sam-ples added for the first 20 iterations, and the av-erage labeling error rate of those samples for theself-labeled positive and negative classes for twomethods.
The agreement-based random selectionadded more negative samples that also have highererror rate than the positive samples.
Adding thesesamples has a negative impact on the classifier?sperformance.
In contrast, our confidence-basedapproach balances the number of positive and neg-ative samples and significantly reduces the error5,000 10,000 15,0000.650.70.750.8# of samplesF?measureSupervisedRandomMost confidentMost differentFigure 4: The learning curve of 3 sample selectionmethods for accent detection.rates for the negative samples as well, thus leadingto performance improvement.Next we evaluate the efficacy of the three sam-ple selection methods described in Section 5,namely, random, most confident, and most dif-ferent selections.
Figure 4 shows the learningcurves for the three selection methods for accentdetection.
The same configuration is used as inthe previous experiment, i.e., at least 0.1 posteriorprobability difference between the two classifiers,and adding 5% of new samples in each iteration.All of these sample selection approaches use theconfidence-based labeling.
For comparison, Fig-ure 4 also shows the learning curve for supervisedlearning when varying the training size.
We cansee from the figure that compared to random selec-tion, the most confident selection method showssimilar performance in the first few iterations, butits performance continues to increase and the sat-uration point is much later than random selection.Unlike the other two sample selection methods,most different selection results in noticeable per-formance degradation after some iteration.
Thisdifference is caused by the high self-labeling er-ror rate of selected samples.
Both random andmost confident selections perform better than su-pervised learning at the first few iterations.
This isbecause the new samples added have different pos-terior probabilities by the two classifiers, and thusone of the classifiers benefits from these samples.Learning curves for the other two tasks (breakindex and IPB detection) show similar pattern forthe random and most different selection methods,but some differences in the most confident selec-tion results.
For the IPB task, the learning curve ofthe most confident selection fluctuates somewhatin the middle of the iterations with similar per-formance to random selection, however, afterwardthe performance is better than random selection.5465,000 10,000 15,000 20,000 25,0000.680.70.720.740.760.780.8# of samplesF?measureSupervised5 utterances10 utterances20 utterances5 utterances10 utterances20 utterancesFigure 5: The learning curves for accent detectionusing different amounts of initial labeled trainingdata.For the break index detection, the learning curveof most different selection increases more slowlythan random selection at the beginning, but the sat-uration point is much later and therefore outper-forms the random selection at the later iterations.We also evaluated the effect of the amount ofinitial labeled training data.
In this experiment,most confident selection is used, and the other con-figurations are the same as the previous experi-ment.
The learning curve for accent detection isshown in Figure 5 using different numbers of utter-ances in the initial training data.
The arrow marksindicate the start position of each learning curve.As we can see, the learning curve when using 20utterances is slightly better than the others, butthere is no significant performance gain accordingto the size of initial labeled training data.Finally we compared our co-training perfor-mance with supervised learning.
For supervisedlearning, all labeled utterances except for the testset are used for training.
We used most confi-dent selection with proposed self-labeling method.The initial training data in co-training is 3% ofthat used for supervised learning.
After 74 iter-ations, the size of samples of co-training is similarto that in the supervised method.
Table 3 presentsthe results of three prosodic event detection tasks.We can see that the performance of co-training forthese three tasks is slightly worse than supervisedlearning using all the labeled data, but is signifi-cantly better than the original performance using3% of hand labeled data.Most of the previous work for prosodic eventdetection reported their results using classificationaccuracy instead of F-measure.
Therefore to bet-ter compare with previous work, we present be-low the accuracy results in our approach.
The co-training algorithm achieves the accuracy of 85.3%,Accent IPB BreakSupervised 0.82 0.74 0.77Co-trainingInitial training (3%) 0.69 0.59 0.62After 74 iterations 0.80 0.71 0.75Table 3: The results (F-measure) of prosodicevent detection for supervised and co-training ap-proaches.90.1%, and 86.7% respectively for accent, intona-tional phrase boundary, and break index detection,compared with 87.6%, 92.3%, and 88.9% in su-pervised learning.
Although the test condition isdifferent, our result is significantly better than thatof other semi-supervised approaches of previouswork and comparable with supervised approaches.7 ConclusionsIn this paper, we exploit the co-training methodfor automatic prosodic event detection.
We intro-duced a confidence-based method to assign possi-ble labels to unlabeled data and evaluated the per-formance combined with informative sample se-lection methods.
Our experimental results usingco-training are significantly better than the origi-nal supervised results using the small amount oftraining data, and closer to that using supervisedlearning with a large amount of data.
This sug-gests that the use of unlabeled data can lead to sig-nificant improvement for prosodic event detection.In our experiment, we used some labeled dataas development set to estimate some parameters.For the future work, we will perform analysisof loss function of each classifier in order to es-timate parameters without labeled developmentdata.
In addition, we plan to compare this to othersemi-supervised learning techniques such as ac-tive learning.
We also plan to use this algorithmto annotate different types of data, such as sponta-neous speech, and incorporate prosodic events inspoken language applications.AcknowledgmentsThis work is supported by DARPA under ContractNo.
HR0011-06-C-0023.
Distribution is unlim-ited.ReferencesA.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
Proceedings of547the Workshop on Computational Learning Theory,pp.
92-100.C.
W. Wightman and M. Ostendorf.
1994.
Automaticlabeling of prosodic patterns.
IEEE Transactions onSpeech and Audio Processing, Vol.
2(4), pp.
69-481.G.
Levow.
2006.
Unsupervised and semi-supervisedlearning of tone and pitch accent.
Proceedings ofHLT-NAACL, pp.
224-231.I.
Muslea, S. Minton and C. Knoblock.
2000.
Selec-tive sampling with redundant views.
Proceedings ofthe 7th International Conference on Artificial Intel-ligence, pp.
621-626.J.
Jeon and Y. Liu.
2009.
Automatic prosodic eventdetection using syllable-base acoustic and syntacticfeatures.
Proceeding of ICASSP, pp.
4565-4568.K.
Chen, M. Hasegawa-Johnson, and A. Cohen.
2004.An automatic prosody labeling system using ANN-based syntactic-prosodic model and GMM-basedacoustic prosodic model.
Proceedings of ICASSP,pp.
509-512.K.
Nigam and R. Ghani.
2000 Analyzing the effec-tiveness and applicability of Co-training Proceed-ings 9th International Conference on Informationand Knowledge Management, pp.
86-93.K.
Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,C.
Wightman, P. Price, J. Pierrehumbert, and J.Hirschberg.
1992.
ToBI: A standard for labelingEnglish prosody.
Proceedings of ICSLP, pp.
867-870.M.
Steedman, S. Baker, S. Clark, J. Crim, J. Hocken-maier, R. Hwa, M. Osborne, P. Ruhlen, A. Sarkar2003.
CLSP WS-02 Final Report: Semi-SupervisedTraining for Statistical Parsing.M.
Ostendorf, P. J.
Price and S. Shattuck-Hunfnagel.1995.
The Boston University Radio News Corpus.Linguistic Data Consortium.S.
Ananthakrishnan and S. Narayanan.
2006.
Com-bining acoustic, lexical, and syntactic evidence forautomatic unsupervised prosody labeling.
Proceed-ings of ICSLP, pp.
297-300.S.
Ananthakrishnan and S. Narayanan.
2008.
Auto-matic prosodic event detection using acoustic, lex-ical and syntactic evidence.
IEEE Transactions onAudio, Speech and Language Processing, Vol.
16(1),pp.
216-228.S.
Clark, J. Currant, and M. Osborne.
2003.
Bootstrap-ping POS taggers using unlabeled data.
Proceedingsof CoNLL, pp.
49-55.S.
Dasupta, M. L. Littman, and D. McAllester.
2001.PAC generalization bounds for co-training.
Ad-vances in Neural Information Processing Systems,Vol.
14, pp.
375-382.S.
Goldman and Y. Zhou.
2000.
Enhancing supervisedlearning with unlabeled data.
Proceedings of theSeventeenth International Conference on MachineLearning, pp.
327-334.V.
K. Rangarajan Sridhar, S. Bangalore, and S.Narayanan.
2008.
Exploiting acoustic and syntacticfeatures for automatic prosody labeling in a maxi-mum entropy framework.
IEEE Transactions on Au-dio, Speech, and Language processing, pp.
797-811.W.
Wang, Z. Huang, and M. Harper.
2007.
Semi-supervised learning for part-of-speech tagging ofMandarin transcribed speech.
Proceeding ofICASSP, pp.
137-140.548
