Proceedings of ACL-08: HLT, pages 281?289,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsHedge classification in biomedical texts with a weakly supervised selection ofkeywordsGyo?rgy SzarvasResearch Group on Artificial IntelligenceHungarian Academy of Sciences / University of SzegedHU-6720 Szeged, Hungaryszarvas@inf.u-szeged.huAbstractSince facts or statements in a hedge or negatedcontext typically appear as false positives, theproper handling of these language phenomenais of great importance in biomedical text min-ing.
In this paper we demonstrate the impor-tance of hedge classification experimentallyin two real life scenarios, namely the ICD-9-CM coding of radiology reports and genename Entity Extraction from scientific texts.We analysed the major differences of specu-lative language in these tasks and developeda maxent-based solution for both the free textand scientific text processing tasks.
Based onour results, we draw conclusions on the pos-sible ways of tackling speculative language inbiomedical texts.1 IntroductionThe highly accurate identification of several regu-larly occurring language phenomena like the specu-lative use of language, negation and past tense (tem-poral resolution) is a prerequisite for the efficientprocessing of biomedical texts.
In various naturallanguage processing tasks, relevant statements ap-pearing in a speculative context are treated as falsepositives.
Hedge detection seeks to perform a kindof semantic filtering of texts, that is it tries to sep-arate factual statements from speculative/uncertainones.1.1 Hedging in biomedical NLPTo demonstrate the detrimental effects of specula-tive language on biomedical NLP tasks, we will con-sider two inherently different sample tasks, namelythe ICD-9-CM coding of radiology records and geneinformation extraction from biomedical scientifictexts.
The general features of texts used in thesetasks differ significantly from each other, but bothtasks require the exclusion of uncertain (or specula-tive) items from processing.1.1.1 Gene Name and interaction extractionfrom scientific textsThe test set of the hedge classification dataset 1(Medlock and Briscoe, 2007) has also been anno-tated for gene names2.Examples of speculative assertions:Thus, the D-mib wing phenotype may result from de-fective N inductive signaling at the D-V boundary.A similar role of Croquemort has not yet been tested,but seems likely since the crq mutant used in thisstudy (crqKG01679) is lethal in pupae.After an automatic parallelisation of the 2 annota-tions (sentence matching) we found that a significantpart of the gene names mentioned (638 occurencesout of a total of 1968) appears in a speculative sen-tence.
This means that approximately 1 in every 3genes should be excluded from the interaction detec-tion process.
These results suggest that a major por-tion of system false positives could be due to hedg-ing if hedge detection had been neglected by a geneinteraction extraction system.1.1.2 ICD-9-CM coding of radiology recordsAutomating the assignment of ICD-9-CM codesfor radiology records was the subject of a shared task1http://www.cl.cam.ac.uk/?bwm23/2http://www.cl.cam.ac.uk/?nk304/281challenge organised in Spring 2007.
The detaileddescription of the task, and the challenge itself canbe found in (Pestian et al, 2007) and online3.
ICD-9-CM codes that are assigned to each report afterthe patient?s clinical treatment are used for the reim-bursement process by insurance companies.
Thereare official guidelines for coding radiology reports(Moisio, 2006).
These guidelines strictly state thatan uncertain diagnosis should never be coded, henceidentifying reports with a diagnosis in a specula-tive context is an inevitable step in the developmentof automated ICD-9-CM coding systems.
The fol-lowing examples illustrate a typical non-speculativecontext where a given code should be added, anda speculative context where the same code shouldnever be assigned to the report:non-speculative: Subsegmental atelectasis in theleft lower lobe, otherwise normal exam.speculative: Findings suggesting viral or reactiveairway disease with right lower lobe atelectasis orpneumonia.
In an ICD-9 coding system developedfor the challenge, the inclusion of a hedge classi-fier module (a simple keyword-based lookup methodwith 38 keywords) improved the overall system per-formance from 79.7% to 89.3%.1.2 Related workAlthough a fair amount of literature on hedging inscientific texts has been produced since the 1990s(e.g.
(Hyland, 1994)), speculative language from aNatural Language Processing perspective has onlybeen studied in the past few years.
This phe-nomenon, together with others used to express formsof authorial opinion, is often classified under the no-tion of subjectivity (Wiebe et al, 2004), (Shana-han et al, 2005).
Previous studies (Light et al,2004) showed that the detection of hedging can besolved effectively by looking for specific keywordswhich imply that the content of a sentence is spec-ulative and constructing simple expert rules that de-scribe the circumstances of where and how a key-word should appear.
Another possibility is to treatthe problem as a classification task and train a sta-tistical model to discriminate speculative and non-speculative assertions.
This approach requires theavailability of labeled instances to train the models3http://www.computationalmedicine.org/challenge/index.phpon.
Riloff et al (Riloff et al, 2003) applied boot-strapping to recognise subjective noun keywordsand classify sentences as subjective or objective innewswire texts.
Medlock and Briscoe (Medlock andBriscoe, 2007) proposed a weakly supervised settingfor hedge classification in scientific texts where theaim is to minimise human supervision needed to ob-tain an adequate amount of training data.Here we follow (Medlock and Briscoe, 2007) andtreat the identification of speculative language as theclassification of sentences for either speculative ornon-speculative assertions, and extend their method-ology in several ways.
Thus given labeled sets Sspecand Snspec the task is to train a model that, for eachsentence s, is capable of deciding whether a previ-ously unseen s is speculative or not.The contributions of this paper are the following:?
The construction of a complex feature selectionprocedure which successfully reduces the num-ber of keyword candidates without excludinghelpful keywords.?
We demonstrate that with a very limitedamount of expert supervision in finalising thefeature representation, it is possible to build ac-curate hedge classifiers from (semi-) automati-cally collected training data.?
The extension of the feature representationused by previous works with bigrams and tri-grams and an evaluation of the benefit of usinglonger keywords in hedge classification.?
We annotated a small test corpora of biomed-ical scientific papers from a different sourceto demonstrate that hedge keywords are highlytask-specific and thus constructing models thatgeneralise well from one task to another is notfeasible without a noticeable loss in accuracy.2 Methods2.1 Feature space representationHedge classification can essentially be handled byacquiring task specific keywords that trigger specu-lative assertions more or less independently of eachother.
As regards the nature of this task, a vectorspace model (VSM) is a straightforward and suit-able representation for statistical learning.
As VSM282is inadequate for capturing the (possibly relevant) re-lations between subsequent tokens, we decided toextend the representation with bi- and trigrams ofwords.
We chose not to add any weighting of fea-tures (by frequency or importance) and for the Max-imum Entropy Model classifier we included binarydata about whether single features occurred in thegiven context or not.2.2 Probabilistic training data acquisitionTo build our classifier models, we used the datasetgathered and made available by (Medlock andBriscoe, 2007).
They commenced with the seed setSspec gathered automatically (all sentences contain-ing suggest or likely ?
two very good speculativekeywords), and Snspec that consisted of randomlyselected sentences from which the most probablespeculative instances were filtered out by a patternmatching and manual supervision procedure.
Withthese seed sets they then performed the followingiterative method to enlarge the initial training sets,adding examples to both classes from an unlabelledpool of sentences called U :1.
Generate seed training data: Sspec and Snspec2.
Initialise: Tspec ?
Sspec and Tnspec ?
Snspec3.
Iterate:?
Train classifier using Tspec and Tnspec?
Order U by P (spec) values assigned bythe classifier?
Tspec ?
most probable batch?
Tnspec ?
least probable batchWhat makes this iterative method efficient is that,as we said earlier, hedging is expressed via key-words in natural language texts; and often severalkeywords are present in a single sentence.
Theseed set Sspec contained either suggest or likely,and due to the fact that other keywords cooccurwith these two in many sentences, they appearedin Sspec with reasonable frequency.
For example,P (spec|may) = 0.9985 on the seed sets createdby (Medlock and Briscoe, 2007).
The iterative ex-tension of the training sets for each class furtherboosted this effect, and skewed the distribution ofspeculative indicators as sentences containing themwere likely to be added to the extended training setfor the speculative class, and unlikely to fall into thenon-speculative set.We should add here that the very same feature hasan inevitable, but very important side effect that isdetrimental to the classification accuracy of mod-els trained on a dataset which has been obtainedthis way.
This side effect is that other words (oftencommon words or stopwords) that tend to cooccurwith hedge cues will also be subject to the same it-erative distortion of their distribution in speculativeand non-speculative uses.
Perhaps the best exam-ple of this is the word it.
Being a stopword in ourcase, and having no relevance at all to speculativeassertions, it has a class conditional probability ofP (spec|it) = 74.67% on the seed sets.
This is dueto the use of phrases like it suggests that, it is likely,and so on.
After the iterative extension of trainingsets, the class-conditional probability of it dramati-cally increased, to P (spec|it) = 94.32%.
This is aconsequence of the frequent co-occurence of it withmeaningful hedge cues and the probabilistic modelused and happens with many other irrelevant terms(not just stopwords).
The automatic elimination ofthese irrelevant candidates is one of our main goals(to limit the number of candidates for manual con-sideration and thus to reduce the human effort re-quired to select meaningful hedge cues).This shows that, in addition to the desired ef-fect of introducing further speculative keywords andbiasing their distribution towards the speculativeclass, this iterative process also introduces signifi-cant noise into the dataset.
This observation led usto the conclusion that in order to build efficient clas-sifiers based on this kind of dataset, we should fil-ter out noise.
In the next part we will present ourfeature selection procedure (evaluated in the Resultssection) which is capable of underranking irrelevantkeywords in the majority of cases.2.3 Feature (or keyword) selectionTo handle the inherent noise in the training datasetthat originates from its weakly supervised construc-tion, we applied the following feature selection pro-cedure.
The main idea behind it is that it is unlikelythat more than two keywords are present in the text,which are useful for deciding whether an instance isspeculative.
Here we performed the following steps:2831.
We ranked the features x by frequency andtheir class conditional probability P (spec|x).We then selected those features that hadP (spec|x) > 0.94 (this threshold was cho-sen arbitrarily) and appeared in the trainingdataset with reasonable frequency (frequencyabove 10?5).
This set constituted the 2407 can-didates which we used in the second analysisphase.2.
For trigrams, bigrams and unigrams ?
pro-cessed separately ?
we calculated a new class-conditional probability for each feature x, dis-carding those observations of x in speculativeinstances where x was not among the two high-est ranked candidate.
Negative credit was givenfor all occurrences in non-speculative contexts.We discarded any feature that became unreli-able (i.e.
any whose frequency dropped be-low the threshold or the strict class-conditionalprobability dropped below 0.94).
We did thisseparately for the uni-, bi- and trigrams to avoidfiltering out longer phrases because more fre-quent, shorter candidates took the credit for alltheir occurrences.
In this step we filtered out85% of all the keyword candidates and kept 362uni-, bi-, and trigrams altogether.3.
In the next step we re-evaluated all 362 candi-dates together and filtered out all phrases thathad a shorter and thus more frequent substringof themselves among the features, with a sim-ilar class-conditional probability on the specu-lative class (worse by 2% at most).
Here wediscarded a further 30% of the candidates andkept 253 uni-, bi-, and trigrams altogether.This efficient way of reranking and selecting po-tentially relevant features (we managed to discard89.5% of all the initial candidates automatically)made it easier for us to manually validate the re-maining keywords.
This allowed us to incorporatesupervision into the learning model in the featurerepresentation stage, but keep the weakly supervisedmodelling (with only 5 minutes of expert supervi-sion required).2.4 Maximum Entropy ClassifierMaximum Entropy Models (Berger et al, 1996)seek to maximise the conditional probability ofclasses, given certain observations (features).
Thisis performed by weighting features to maximise thelikelihood of data and, for each instance, decisionsare made based on features present at that point, thusmaxent classification is quite suitable for our pur-poses.
As feature weights are mutually estimated,the maxent classifier is capable of taking feature de-pendence into account.
This is useful in cases likethe feature it being dependent on others when ob-served in a speculative context.
By downweightingsuch features, maxent is capable of modelling to acertain extent the special characteristics which arisefrom the automatic or weakly supervised trainingdata acquisition procedure.
We used the OpenNLPmaxent package, which is freely available4 .3 ResultsIn this section we will present our results for hedgeclassification as a standalone task.
In experimentswe made use of the hedge classification dataset ofscientific texts provided by (Medlock and Briscoe,2007) and used a labeled dataset generated automat-ically based on false positive predictions of an ICD-9-CM coding system.3.1 Results for hedge classification inbiomedical textsAs regards the degree of human intervention needed,our classification and feature selection model fallswithin the category of weakly supervised machinelearning.
In the following sections we will evalu-ate our above-mentioned contributions one by one,describing their effects on feature space size (effi-ciency in feature and noise filtering) and classifi-cation accuracy.
In order to compare our resultswith Medlock and Briscoe?s results (Medlock andBriscoe, 2007), we will always give the BEP (spec)that they used ?
the break-even-point of precisionand recall5.
We will also present F?=1(spec) values4http://maxent.sourceforge.net/5It is the point on the precision-recall curve of spec classwhere P = R. If an exact P = R cannot be realised due tothe equal ranking of many instances, we use the point closestto P = R and set BEP (spec) = (P + R)/2.
BEP is an284which show how good the models are at recognisingspeculative assertions.3.1.1 The effects of automatic feature selectionThe method we proposed seems especially effec-tive in the sense that we successfully reduced thenumber of keyword candidates from an initial 2407words having P (spec|x) > 0.94 to 253, whichis a reduction of almost 90%.
During the pro-cess, very few useful keywords were eliminated andthis indicated that our feature selection procedurewas capable of distinguishing useful keywords fromnoise (i.e.
keywords having a very high specula-tive class-conditional probability due to the skewedcharacteristics of the automatically gathered train-ing dataset).
The 2407-keyword model achieved aBEP (spec) os 76.05% and F?=1(spec) of 73.61%,while the model after feature selection performedbetter, achieving a BEP (spec) score of 78.68%and F?=1(spec) score of 78.09%.
Simplifying themodel to predict a spec label each time a keywordwas present (by discarding those 29 features thatwere too weak to predict spec alone) slightly in-creased both the BEP (spec) and F?=1(spec) val-ues to 78.95% and 78.25%.
This shows that theMaximum Entropy Model in this situation couldnot learn any meaningful hypothesis from the cooc-curence of individually weak keywords.3.1.2 Improvements by manual featureselectionAfter a dimension reduction via a strict rerankingof features, the resulting number of keyword candi-dates allowed us to sort the retained phrases manu-ally and discard clearly irrelevant ones.
We judgeda phrase irrelevant if we could consider no situationin which the phrase could be used to express hedg-ing.
Here 63 out of the 253 keywords retained bythe automatic selection were found to be potentiallyrelevant in hedge classification.
All these featureswere sufficient for predicting the spec class alone,thus we again found that the learnt model reducedto a single keyword-based decision.6 These 63 key-interesting metric as it demonstrates how well we can trade-offprecision for recall.6We kept the test set blind during the selection of relevantkeywords.
This meant that some of them eventually proved tobe irrelevant, or even lowered the classification accuracy.
Ex-amples of such keywords were will, these data and hypothesis.words yielded a classifier with a BEP (spec) scoreof 82.02% and F?=1(spec) of 80.88%.3.1.3 Results obtained adding externaldictionariesIn our final model we added the keywords used in(Light et al, 2004) and those gathered for our ICD-9-CM hedge detection module.
Here we decided notto check whether these keywords made sense in sci-entific texts or not, but instead left this task to themaximum entropy classifier, and added only thosekeywords that were found reliable enough to predictspec label alone by the maxent model trained on thetraining dataset.
These experiments confirmed thathedge cues are indeed task specific ?
several cuesthat were reliable in radiology reports proved to beof no use for scientific texts.
We managed to in-crease the number of our features from 63 to 71 us-ing these two external dictionaries.These additional keywords helped us to increasethe overall coverage of the model.
Our final hedgeclassifier yielded a BEP (spec) score of 85.29%and F?=1(spec) score of 85.08% (89.53% Preci-sion, 81.05% Recall) for the speculative class.
Thismeant an overall classification accuracy of 92.97%.Using this system as a pre-processing module fora hypothetical gene interaction extraction system,we found that our classifier successfully excludedgene names mentioned in a speculative sentence (itremoved 81.66% of all speculative mentions) andthis filtering was performed with a respectable pre-cision of 93.71% (F?=1(spec) = 87.27%).Articles 4Sentences 1087Spec sentences 190Nspec sentences 897Table 1: Characteristics of the BMC hedge dataset.3.1.4 Evaluation on scientific texts from adifferent sourceFollowing the annotation standards of Medlockand Briscoe (Medlock and Briscoe, 2007), we man-ually annotated 4 full articles downloaded from theWe assumed that these might suggest a speculative assertion.285BMC Bioinformatics website to evaluate our finalmodel on documents from an external source.
Thechief characteristics of this dataset (which is avail-able at7) is shown in Table 1.
Surprisingly, the modellearnt on FlyBase articles seemed to generalise tothese texts only to a limited extent.
Our hedge clas-sifier model yielded a BEP (spec) = 75.88% andF?=1(spec) = 74.93% (mainly due to a drop in pre-cision), which is unexpectedly low compared to theprevious results.Analysis of errors revealed that some keywordswhich proved to be very reliable hedge cues in Fly-Base articles were also used in non-speculative con-texts in the BMC articles.
Over 50% (24 out of47) of our false positive predictions were due tothe different use of 2 keywords, possible and likely.These keywords were many times used in a mathe-matical context (referring to probabilities) and thusexpressed no speculative meaning, while such useswere not represented in the FlyBase articles (other-wise bigram or trigram features could have capturedthese non-speculative uses).3.1.5 The effect of using 2-3 word-long phrasesas hedge cuesOur experiments demonstrated that it is indeed agood idea to include longer phrases in the vectorspace model representation of sentences.
One thirdof the features used by our advanced model were ei-ther bigrams or trigrams.
About half of these werethe kind of phrases that had no unigram componentsof themselves in the feature set, so these could be re-garded as meaningful standalone features.
Examplesof such speculative markers in the fruit fly datasetwere: results support, these observations, indicatethat, not clear, does not appear, .
.
.
The majority ofthese phrases were found to be reliable enough forour maximum entropy model to predict a specula-tive class based on that single feature.Our model using just unigram features achieveda BEP (spec) score of 78.68% and F?=1(spec)score of 80.23%, which means that using bigramand trigram hedge cues here significantly improvedthe performance (the difference in BEP (spec) andF?=1(spec) scores were 5.23% and 4.97%, respec-tively).7http://www.inf.u-szeged.hu/?szarvas/homepage/hedge.html3.2 Results for hedge classification in radiologyreportsIn this section we present results using the above-mentioned methods for the automatic detection ofspeculative assertions in radiology reports.
Here wegenerated training data by an automated procedure.Since hedge cues cause systems to predict false pos-itive labels, our idea here was to train MaximumEntropy Models for the false positive classificationsof our ICD-9-CM coding system using the vectorspace representation of radiology reports.
That is,we classified every sentence that contained a medi-cal term (disease or symptom name) and caused theautomated ICD-9 coder8 to predict a false positivecode was treated as a speculative sentence and allthe rest were treated as non-speculative sentences.Here a significant part of the false positive predic-tions of an ICD-9-CM coding system that did nothandle hedging originated from speculative asser-tions, which led us to expect that we would havethe most hedge cues among the top ranked keywordswhich implied false positive labels.Taking the above points into account, we usedthe training set of the publicly available ICD-9-CMdataset to build our model and then evaluated eachsingle token by this model to measure their predic-tivity for a false positive code.
Not surprisingly,some of the best hedge cues appeared among thehighest ranked features, while some did not (theydid not occur frequently enough in the training datato be captured by statistical methods).For this task, we set the initial P (spec|x) thresh-old for filtering to 0.7 since the dataset was gener-ated by a different process and we expected hedgecues to have lower class-conditional probabilitieswithout the effect of the probabilistic data acqui-sition method that had been applied for scientifictexts.
Using all 167 terms as keywords that hadP (spec|x) > 0.7 resulted in a hedge classifier withan F?=1(spec) score of 64.04%After the feature selection process 54 keywordswere retained.
This 54-keyword maxent classifiergot an F?=1(spec) score of 79.73%.
Plugging thismodel (without manual filtering) into the ICD-9 cod-ing system as a hedge module, the ICD-9 coder8Here the ICD-9 coding system did not handle the hedgingtask.286yielded an F measure of 88.64%, which is much bet-ter than one without a hedge module (79.7%).Our experiments revealed that in radiology re-ports, which mainly concentrate on listing the iden-tified diseases and symptoms (facts) and the physi-cian?s impressions (speculative parts), detectinghedge instances can be performed accurately usingunigram features.
All bi- and trigrams retained byour feature selection process had unigram equiva-lents that were eliminated due to the noise presentin the automatically generated training data.We manually examined all keywords that had aP (spec) > 0.5 given as a standalone instance forour maxent model, and constructed a dictionary ofhedge cues from the promising candidates.
Here wejudged 34 out of 54 candidates to be potentially use-ful for hedging.
Using these 34 keywords we got anF?=1(spec) performance of 81.96% due to the im-proved precision score.Extending the dictionary with the keywords wegathered from the fruit fly dataset increased theF?=1(spec) score to 82.07% with only one out-domain keyword accepted by the maxent classifier.Biomedical papers Medical reportsBEP (spec) F?=1(spec) F?=1(spec)Baseline 1 60.00 ?
48.99Baseline 2 76.30 ?
?All features 76.05 73.61 64.04Feature selection 78.68 78.09 79.73Manual feat.
sel.
82.02 80.88 81.96Outer dictionary 85.29 85.08 82.07Table 2: Summary of results.4 ConclusionsThe overall results of our study are summarised ina concise way in Table 2.
We list BEP (spec)and F?=1(spec) values for the scientific text dataset,and F?=1(spec) for the clinical free text dataset.Baseline 1 denotes the substring matching system ofLight et al (Light et al, 2004) and Baseline 2 de-notes the system of Medlock and Briscoe (Medlockand Briscoe, 2007).
For clinical free texts, Baseline1 is an out-domain model since the keywords werecollected for scientific texts by (Light et al, 2004).The third row corresponds to a model using all key-words P (spec|x) above the threshold and the fourthrow a model after automatic noise filtering, while thefifth row shows the performance after the manual fil-tering of automatically selected keywords.
The lastrow shows the benefit gained by adding reliable key-words from an external hedge keyword dictionary.Our results presented above confirm our hypothe-sis that speculative language plays an important rolein the biomedical domain, and it should be han-dled in various NLP applications.
We experimen-tally compared the general features of this task intexts from two different domains, namely medicalfree texts (radiology reports), and scientific articleson the fruit fly from FlyBase.The radiology reports had mainly unambiguoussingle-term hedge cues.
On the other hand, it provedto be useful to consider bi- and trigrams as hedgecues in scientific texts.
This, and the fact that manyhedge cues were found to be ambiguous (they ap-peared in both speculative and non-speculative as-sertions) can be attributed to the literary style of thearticles.
Next, as the learnt maximum entropy mod-els show, the hedge classification task reduces to alookup for single keywords or phrases and to theevaluation of the text based on the most relevant cuealone.
Removing those features that were insuffi-cient to classify an instance as a hedge individuallydid not produce any difference in the F?=1(spec)scores.
This latter fact justified a view of ours,namely that during the construction of a statisticalhedge detection module for a given application themain issue is to find the task-specific keywords.Our findings based on the two datasets employedshow that automatic or weakly supervised data ac-quisition, combined with automatic and manual fea-ture selection to eliminate the skewed nature of thedata obtained, is a good way of building hedge clas-sifier modules with an acceptable performance.The analysis of errors indicate that more com-plex features like dependency structure and clausalphrase information could only help in allocating thescope of hedge cues detected in a sentence, not thedetection of any itself.
Our finding that token uni-gram features are capable of solving the task accu-rately agrees with the the results of previous workson hedge classification ((Light et al, 2004), (Med-287lock and Briscoe, 2007)), and we argue that 2-3word-long phrases also play an important role ashedge cues and as non-speculative uses of an oth-erwise speculative keyword as well (i.e.
to resolvean ambiguity).
In contrast to the findings of Wiebeet al ((Wiebe et al, 2004)), who addressed thebroader task of subjectivity learning and found thatthe density of other potentially subjective cues inthe context benefits classification accuracy, we ob-served that the co-occurence of speculative cues ina sentence does not help in classifying a term asspeculative or not.
Realising that our learnt mod-els never predicted speculative labels based on thepresence of two or more individually weak cues anddiscarding such terms that were not reliable enoughto predict a speculative label (using that term aloneas a single feature) slightly improved performance,we came to the conclusion that even though specu-lative keywords tend to cooccur, and two keywordsare present in many sentences; hedge cues have aspeculative meaning (or not) on their own withoutthe other term having much impact on this.The main issue thus lies in the selection of key-words, for which we proposed a procedure that iscapable of reducing the number of candidates to anacceptable level for human evaluation ?
even in datacollected automatically and thus having some unde-sirable properties.The worse results on biomedical scientific papersfrom a different source also corroborates our find-ing that hedge cues can be highly ambiguous.
Inour experiments two keywords that are practicallynever used in a non-speculative context in the Fly-Base articles we used for training were responsi-ble for 50% of false positives in BMC texts sincethey were used in a different meaning.
In our case,the keywords possible and likely are apparently al-ways used as speculative terms in the FlyBase arti-cles used, while the articles from BMC Bioinformat-ics frequently used such cliche phrases as all possi-ble combinations or less likely / more likely .
.
.
(re-ferring to probabilities shown in the figures).
Thisshows that the portability of hedge classifiers is lim-ited, and cannot really be done without the examina-tion of the specific features of target texts or a moreheterogenous corpus is required for training.
Theconstruction of hedge classifiers for each separatetarget application in a weakly supervised way seemsfeasible though.
Collecting bi- and trigrams whichcover non-speculative usages of otherwise commonhedge cues is a promising solution for addressing thefalse positives in hedge classifiers and for improvingthe portability of hedge modules.4.1 Resolving the scope of hedge keywordsIn this paper we focused on the recognition of hedgecues in texts.
Another important issue would be todetermine the scope of hedge cues in order to lo-cate uncertain sentence parts.
This can be solved ef-fectively using a parser adapted for biomedical pa-pers.
We manually evaluated the parse trees gen-erated by (Miyao and Tsujii, 2005) and came to theconclusion that for each keyword it is possible to de-fine the scope of the keyword using subtrees linkedto the keyword in the predicate-argument syntac-tic structure or by the immediate subsequent phrase(e.g.
prepositional phrase).
Naturally, parse errorsresult in (slightly) mislocated scopes but we hadthe general impression that state-of-the-art parserscould be used efficiently for this issue.
On the otherhand, this approach requires a human expert to de-fine the scope for each keyword separately using thepredicate-argument relations, or to determine key-words that act similarly and their scope can be lo-cated with the same rules.
Another possibility issimply to define the scope to be each token up tothe end of the sentence (and optionally to the previ-ous punctuation mark).
The latter solution has beenimplemented by us and works accurately for clinicalfree texts.
This simple algorithm is similar to NegEx(Chapman et al, 2001) as we use a list of phrasesand their context, but we look for punctuation marksto determine the scopes of keywords instead of ap-plying a fixed window size.AcknowledgmentsThis work was supported in part by the NKTH grantof Jedlik ?Anyos R&D Programme 2007 of the Hun-garian government (codename TUDORKA7).
Theauthor wishes to thank the anonymous reviewers forvaluable comments and Veronika Vincze for valu-able comments in linguistic issues and for help withthe annotation work.288ReferencesAdam L. Berger, Stephen Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-gory F. Cooper, and Bruce G. Buchanan.
2001.
Asimple algorithm for identifying negated findings anddiseases in discharge summaries.
Journal of Biomedi-cal Informatics, 5:301?310.Ken Hyland.
1994.
Hedging in academic writing and eaptextbooks.
English for Specific Purposes, 13(3):239?256.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The language of bioscience: Facts, spec-ulations, and statements in between.
In LynetteHirschman and James Pustejovsky, editors, HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Bi-ological Literature, Ontologies and Databases, pages17?24, Boston, Massachusetts, USA, May 6.
Associa-tion for Computational Linguistics.Ben Medlock and Ted Briscoe.
2007.
Weakly supervisedlearning for hedge classification in scientific literature.In Proceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 992?999,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage HPSG pars-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 83?90, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Marie A. Moisio.
2006.
A Guide to Health InsuranceBilling.
Thomson Delmar Learning.John P. Pestian, Chris Brew, Pawel Matykiewicz,DJ Hovermale, Neil Johnson, K. Bretonnel Cohen, andWlodzislaw Duch.
2007.
A shared task involvingmulti-label classification of clinical free text.
In Bi-ological, translational, and clinical language process-ing, pages 97?104, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of the Seventh Com-putational Natural Language Learning Conference,pages 25?32, Edmonton, Canada, May-June.
Associa-tion for Computational Linguistics.James G. Shanahan, Yan Qu, and Janyce Wiebe.
2005.Computing Attitude and Affect in Text: Theoryand Applications (The Information Retrieval Series).Springer-Verlag New York, Inc., Secaucus, NJ, USA.Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce,Matthew Bell, and Melanie Martin.
2004.
Learn-ing subjective language.
Computational Linguistics,30(3):277?308.289
