Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 608?616, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsCoMeT: Integrating different levels of linguistic modeling formeaning assessmentNiels Ott Ramon Ziai Michael Hahn Detmar MeurersSonderforschungsbereich 833Eberhard Karls Universita?t Tu?bingen{nott,rziai,mhahn,dm}@sfs.uni-tuebingen.deAbstractThis paper describes the CoMeT system, ourcontribution to the SemEval 2013 Task 7 chal-lenge, focusing on the task of automaticallyassessing student answers to factual questions.CoMeT is based on a meta-classifier that usesthe outputs of the sub-systems we developed:CoMiC, CoSeC, and three shallower bag ap-proaches.
We sketch the functionality of allsub-systems and evaluate their performanceagainst the official test set of the challenge.CoMeT obtained the best result (73.1% accu-racy) for the 3-way unseen answers in Beetleamong all challenge participants.
We also dis-cuss possible improvements and directions forfuture research.1 IntroductionOur contribution to the SemEval 2013 Task 7 chal-lenge (Dzikovska et al 2013) presented here is basedon our research in the A4 project1 of the SFB 833,which is dedicated to the question how meaning canbe computationally compared in realistic situations.In realistic situations, utterances are not necessarilywell-formed or complete, there may be individualdifferences in situative and world knowledge amongthe speakers.
This can complicate or even precludea complete linguistic analysis, leading us to the fol-lowing research question: Which linguistic repre-sentations can be used effectively and robustly forcomparing the meaning of sentences and text frag-ments computationally?1http://purl.org/dm/projects/sfb833-a4In order to work on effective and robust processing,we base our work on reading comprehension exer-cises for foreign language learners, of which we arealso collecting a large corpus (Ott et al 2012).
Ourfirst system, CoMiC, is an alignment-based approachwhich exists in English and German variants (Meur-ers et al 2011a; Meurers et al 2011b).
CoMiCuses various levels of linguistic abstraction from sur-face tokens to dependency parses.
Further work thatwe are starting to tackle includes the utilization ofInformation Structure (Krifka, 2007) in the system.The second approach emerging from the researchproject is CoSeC (Hahn and Meurers, 2011; Hahnand Meurers, 2012), a semantics-based system formeaning comparison that was developed for Germanfrom the start and was ported to operate on Englishfor this shared task.
As a novel contribution in thispaper, we present CoMeT (Comparing Meaning inTu?bingen), a system that employs a meta-classifierfor combining the output of CoMiC and CoSeC andthree shallower bag approaches.In terms of the general context of our work, shortanswer assessment essentially comes in the two fla-vors of meaning comparison and grading, the firsttrying to determine whether or not two utterancesconvey the same meaning, the latter aimed at gradingthe abilities of students (cf.
Ziai et al 2012).
Shortanswer assessment is also closely related to the fieldof Recognizing Textual Entailment (RTE, Dagan etal., 2009), which this year is directly reflected bythe fact that SemEval 2013 Task 7 is the Joint Stu-dent Response Analysis and 8th Recognizing TextualEntailment Challenge.608Turning to the organization of this paper, section 2introduces the three types of sub-systems and themeta-classifier.
In section 3, we report on the evalu-ation results of each sub-system both for our devel-opment set as well as for the official test set of theshared task.
We then discuss possible causes andimplications of the findings we made by participatingin the shared task.2 SystemsThe CoMeT system that we describe in this paperis a combination of three types of sub-systems inone meta-classifier.
CoSeC and CoMiC are sys-tems that align linguistic units in the student answerto those in the reference answer.
In contrast, thebag-based approaches employ a vocabulary of words,lemmas, and Soundex hashes constructed from allof the student answers in the training data.
In themeta-classifier, we tried to combine the benefits of thenamed sub-systems into one large system that eventu-ally computed our submission to the SemEval 2013Task 7 challenge.2.1 CoMiCCoMiC (Comparing Meaning in Context) is analignment-based system, i.e., it operates on a map-ping of linguistic units found in a student answer tothose given in a reference answer.
CoMiC started offas a re-implementation of the Content AssessmentModule (CAM) of Bailey and Meurers (2008).
Itexists in two flavors: CoMiC-DE for German, de-scribed in Meurers et al(2011b), and CoMiC-EN forEnglish, described in Meurers et al(2011a).
Bothsystems are positioned in the landscape of the shortanswer assessment field in Ziai et al(2012).
In thispaper, we refer to CoMiC-EN simply as CoMiC.Sketched briefly, CoMiC operates in three stages:1.
Annotation uses various NLP modules to equipstudent answers and reference answers with lin-guistic abstractions of several types.2.
Alignment creates links between these linguisticabstractions from the reference answer to thestudent answer.3.
Classification uses summary statistics of thesealignment links in machine learning in order toassign labels to each student answer.Automatic annotation and alignment are imple-mented in the Unstructured Information ManagementArchitecture (UIMA, Ferrucci and Lally, 2004).
OurUIMA modules mainly wrap around standard NLPtools of which we provide an overview in Table 1.We used the standard statistical models which areprovided with the NLP tools.Annotation Task NLP ComponentSentence Detection OpenNLP2Tokenization OpenNLPLemmatization morpha (Minnen et al 2001)Spell Checking Edit distance (Levenshtein, 1966),SCOWL word list3Part-of-speech Tagging TreeTagger (Schmid, 1994)Noun Phrase Chunking OpenNLPSynonyms and WordNet (Fellbaum, 1998)Semantic TypesSimilarity Scores PMI-IR (Turney, 2001)on UkWaC (Baroni et al 2009)Dependency Relations MaltParser (Nivre et al 2007)Keyword extraction Heads from dependency parseTable 1: NLP tools used for CoMiC and Bag ApproachesAnnotation ranges from very basic linguistic unitssuch as sentences and tokens with POS and lemmas,over NP chunks, up to full dependency parses ofthe input.
For distributional semantic similarity viaPMI-IR (Turney, 2001), a local search engine basedon Lucene (Gospodnetic?
and Hatcher, 2005) queryingthe UkWaC corpus (Baroni et al 2009) was used,since all major search engines meanwhile have shutdown their APIs.After the annotation of linguistic units has takenplace, candidate alignment links are created withinUIMA.
In a simple example case, a candidate align-ment link is a pair of tokens that is token identicalin the student answer and in the reference answer.The same token in the student answer may also bepart of a candidate alignment link that maps to an-other token in the reference answer that, e.g., has thesame lemma, or is a possible synonym, or again istoken identical.
Other possible links are based onspelling-corrected tokens, semantic types, or highvalues of the PMI-IR similarity measure.Words that are present in the reading comprehen-sion question and that are also found in the student an-swer are excluded from alignment, resulting in a very2http://incubator.apache.org/opennlp3http://wordlist.sourceforge.net609basic implementation of an approach to givenness(cf.
Halliday, 1967, p. 204 and many others since).Subsequently, a globally optimal alignment of lin-guistic units in the reference answer and student an-swer is determined using the Traditional MarriageAlgorithm (Gale and Shapley, 1962).At this point, processing within UIMA comes toan end with an output module that generates the filescontaining the features for machine learning.
Thesefeatures basically are summary statistics of the typesof alignment links.
An overview of these numericfeatures used is given in Table 2.Feature Description1.
Keyword Overlap Percent of keywords aligned(relative to target)2./3.
Token Overlap Percent of alignedtarget/learner tokens4./5.
Chunk Overlap Percent of alignedtarget/learner chunks6./7.
Triple Overlap Percent of alignedtarget/learner triples8.
Token Match Percent of token alignmentsthat were token-identical9.
Similarity Match Percent of token alignmentsthat were similarity-resolved10.
Type Match Percent of token alignmentsthat were type-resolved11.
Lemma Match Percent of token alignmentsthat were lemma-resolved12.
Synonym Match Percent of token alignmentsthat were synonym-resolved13.
Variety of Match Number of kinds of(0-5) token-level alignmentsTable 2: Features used in CoMiC?s classification phaseCurrent versions of CoMiC use the WEKA toolkit(Hall et al 2009), allowing us to experiment withdifferent machine learning strategies.
In general, anytype of classification can be trained in this machinelearning phase, a binary correct vs. incorrect de-cision as in the 2-way task being the simplest case.The best results with CoMiC on our held-out develop-ment set were achieved using WEKA?s J48 classifier,which is an implementation of decision tree based onQuinlan (1993).In terms of linguistic abstractions, CoMiC leavesthe choice of representations used to its alignmentstep.
However, in the final machine learning step, noconcrete information about linguistic units is presentany more.
The machine learning component onlysees alignment configurations which are indepen-dent of concrete words, phrases, or any other lin-guistic information.
This high level of abstractionsuggests that CoMiC should perform better than otherapproaches on unseen topics and unseen questions,since it does not rely on concrete units as, e.g., abag-of-words approach does.2.2 CoSeCCoSeC (Comparing Semantics in Context) performsmeaning comparison on the basis of an underspec-ified semantic representation robustly derived fromthe learner and the reference answers.
The sys-tem was developed for German (Hahn and Meurers,2012), on the basis of which we created the EnglishCoSeC-EN for the SemEval 2013 Task 7 challenge.Using an explicit semantic formalism in principlemakes it possible to precisely represent meaning dif-ferences.
It also supports a direct representation ofInformation Structure as a structuring of semanticsrepresentations (Krifka, 2007).CoSeC is based on Lexical Resource Semantics(LRS, Richter and Sailer, 2004).
Being an under-specified semantic formalism, LRS avoids the costlycomputation of all readings and provides access tothe building blocks of the semantic representation,while additional constraints provide the informationabout their composition.As described in Hahn and Meurers (2011), LRSrepresentations can be derived automatically usinga two-step approach based on part-of-speech tagsassigned by TreeTagger (Schmid, 1994) and depen-dency parses by MaltParser (Nivre et al 2007).
First,the dependency structure is transformed into a com-pletely lexicalized syntax-semantics interface rep-resentation, which abstracts away from some formvariation at the surface.
These representations arethen mapped to LRS representations.
The approachis robust in that it always results in an LRS structure,even for ill-formed sentences.CoSeC then aligns the LRS representations of thereference answer and the student answer to each otherand also to the representation of the question.
Thealignment approach takes into account local criteria,namely the semantic similarity of pairs of elementsthat are linked by the alignment, as well as globalcriteria measuring the extent to which the alignment610preserves structure at the levels of variables and thesubterm structure of the semantic formulas.Local similarity of semantic expressions is esti-mated using WordNet (Fellbaum, 1998), FrameNet(Baker et al 1998), PMI-IR (Turney, 2001) on theUkWaC (Baroni et al 2009) as used in CoMiC, theMinimum Edit Distance (Levenshtein, 1966), andspecial parameters for comparing functional elementssuch as quantifiers and grammatical function labels.Based on the alignments, the system marks ele-ments which are not linked to elements in the ques-tion or which are linked to the semantic contributionof an alternative in an alternative question as ?fo-cused?.
This is intended as a first approximation ofthe concept of focus in the sense of Information Struc-ture (von Heusinger, 1999; Kruijff-Korbayova?
andSteedman, 2003; Krifka, 2007), an active field of re-search in linguistics addressing the question how theinformation in sentences is packaged and integratedinto discourse.
Focus elements are expected to beparticularly relevant for determining the correctnessof an answer (Meurers et al 2011b).Overall meaning comparison is then done basedon a set of numerical scores computed from the align-ments and their quality.
For each of these scores, athreshold is empirically determined, over which thestudent answer is considered to be correct.
Amongthe scores discussed by Hahn and Meurers (2011),weighted-target focus, consistently scored best in thedevelopment set.
This score measures the percent-age of terms in the semantic representation of thereference answer which are linked to elements ofthe student answer in relation to the number of allelements in the representation of the reference an-swer.
Only terms that were marked as focused inthe preceding step are counted.
Functional elements,i.e., quantifiers, predicates representing grammaticalfunction labels, or the lambda operator, are weighteddifferently from other elements.This threshold method can only be used to perform2-way classification.
Unlike the machine learningstep in CoMiC, it does not generalize to 3-way or5-way classification.The alignment algorithm uses several numericalparameters, such as weights for the different compo-nents measuring semantic similarities, weights forthe different overall local and global criteria, andthe weight of the weighted-target focus score.
Theseparameters are optimized using Powells algorithmcombined with grid-based line optimization (Press etal., 2002).
To avoid overfitting, the parameters andthe threshold are determined on disjoint partitions ofthe training set.In terms of linguistic abstractions, meaning assess-ment in CoSeC is based entirely on underspecifiedsemantic representations.
Surface forms are indi-rectly encoded by the structure of the representationand the predicate names, which are usually derivedfrom the lemmas.
As with CoMiC, parameter opti-mization and the determination of the thresholds forthe numerical scores do not involve concrete infor-mation about linguistic objects.
Again, the high levelof abstraction suggests that CoSeC should performbetter than other approaches on unseen topics andunseen questions.2.3 The Bag ApproachesInspired by the bag-of-words concept that emergedfrom information retrieval (Salton and McGill, 1983),we designed a system that uses bag representationsof student answers.
For each student answer, thereare three bags, each containing one of the followingrepresentations: words, lemmas and Soundex hashesof that answer.
The question ID corresponding tothe answer is added to each bag as a pseudo-word,allowing the machine learner to adjust to question-specific properties.
Based on the bag representations,the approach compares a given student answer to amodel trained on all other known student answers.On the one hand, this method ignores the presence ofreference answers (although they could be added tothe training set as additional correct answers), on theother hand it makes use of information not taken intoaccount by alignment-based systems such as CoMiCor CoSeC.Concerning pre-processing, the linguistic anal-yses such as tokenization and lemmatization areidentical to those of CoMiC, since the bag gener-ator technically is just another output module of theUIMA-based pipeline used there.
No stop-word listis used.
The bags are fed into a support vector-basedmachine learner.
We used WEKA?s Sequential Min-imal Optimization (SMO, Platt, 1998) implementa-tion with the radial basis function (RBF) kernel, sinceit yielded good results on our development set andsince it supports output of the estimated probabilities611for each class.
The optimal gamma parameter andcomplexity constant were estimated via 10-fold gridsearch.In terms of abstractions, all bag-based approachessimply disregard word order and in case of binarybags even word frequency.
Still, a bit of the relationbetween words is essentially encoded in their mor-phology.
This piece of information is discarded inthe bags of lemmas, eventually, e.g., putting wordslike ?bulb?
and ?bulbs?
in the same vector slot.
Fur-ther away from the surface are the Soundex hashes,a phonetic representation of English words patentedby Russell (1918).
The well-known algorithm trans-forms similar-sounding English words into the samerepresentation of characters and numbers, therebyironing out many spelling mistakes and commonconfusion cases of homophones such as ?there?
vs.?their?.
The MorphAdorner4 implementation we usedreturns empty Soundex hashes for input tokens thatdo not start with a letter of the alphabet.
However,we found in our experiments, that the presence ofthese empty hashes in the bags has a positive impacton performance.
This is most likely due to the factthat it discriminates answers containing punctuation(not a letter of the alphabet) from those which do not.Since the bag approaches use Soundex as pho-netic equivalence classes, but no semantic equiva-lence classes, they should perform best on the unseenanswers data in which most lexical material from thetest set is likely to already be present in the trainingset.2.4 CoMeT: A Meta-ClassifierAs described in the previous sections, our sub-systems perform short answer evaluation on differ-ent representations and at different levels of abstrac-tion.
The bag approaches are very surface-oriented,whereas CoSeC uses a semantic formalism to com-pare answers to each other.
We expected each systemto show its strengths in different test scenarios, so away was needed to combine the predictions of differ-ent systems into the final result.CoMeT (Comparing Meaning in Tu?bingen) is ameta-classifier which builds on the predictions ofour individual systems (feature stacking, see Wolpert,1992).
The rationale is that if systems are comple-4http://morphadorner.northwestern.edumentary, their combination will perform better (or atleast as good) than any individual system on its own.The design is as follows:Each system produces predictions on the trainingset, using 10-fold cross-validation, and on the test set.In addition to the predicted class, each system wasalso made to output probabilities for each possibleclass (cf., e.g., Tetreault et al 2012a).
The classprobabilities were then used as features in the metaclassifier to train a model for the test data.
In additionto the probabilities, we also used the question ID andmodule ID in the meta-classifier, in the hope that theywould allow differentiation between scenarios.
Forexample, an unseen question ID means that we arenot testing on unseen answers and thus predictionsfrom systems with more abstraction from the surfacemay be preferred.The class probabilities come from differentsources, depending on the system.
In the case ofCoMiC, they are extracted directly from the decisiontrees.
For the bag approaches, we used WEKA?s op-tion to fit logistic models to the SVM output afterclassification in order to estimate probabilities.
Fi-nally, the CoSeC probabilities are derived directlyfrom its final score.
As mentioned in section 2.2,CoSeC only does binary classification, so those prob-abilities are used in the meta-classifier for all tasks.Based on the results on our internal developmentset (see section 3.1), we chose different system com-binations for different scenarios.
For unseen topicsand unseen questions, we used only CoMiC in com-bination with CoSeC, since the inclusion of the bagapproaches had a negative impact on results.
For un-seen answers, we additionally included the bag mod-els.
All meta-classification was done using WEKA?sLogistic Regression implementation.
The results arediscussed in section 3.3 EvaluationIn this section, we present the results for each of thesub-systems, both on the custom-made split of thetraining data we used in our development, as well ason the official test data of the SemEval 2013 Task 7challenge.
Subsequently, we discuss possible causesfor issues raised by our evaluation results.6123.1 Development SetIn order to be as close as possible to the final testsetting, we replicated the official test scenarios onthe training set, resulting in a train/dev/test split foreach of the corpora.
For Beetle, we held out all an-swers to two random questions for each module toform the unseen questions scenario, and five randomanswers from each remaining question to form theunseen answers scenario.
For SciEntsBank, we heldout module LF for dev and module VB for test toform the unseen topics scenario, because they havean average number of questions (11).
The LF moduleturned out to be far more skewed towards incorrectanswers (76.8%) than the training set on average(57.5%).
While this skewedness needs to be takeninto account for the interpretation of the developmentresults, it did not have a negative effect on our fi-nal test results.
Furthermore, analogous to Beetle,we held out all answers to one random question foreach remaining module for unseen-questions, andtwo random answers from each remaining questionfor unseen answers.The dev set was used for tuning and design deci-sions concerning which individual systems to com-bine in the stacked classifier, while we envisagedthe test set to be used as a final checkpoint beforesubmission.The accuracy results for all sub-systems on thedevelopment set are reported in detail in Table 3.The majority baseline reflects the accuracy a systemwould achieve by always labelling any student answeras ?incorrect?, hence it is equivalent to the percentageof incorrect answers in the data.
The lexical baselineis the performance of the system provided by thechallenge organizers.Beetle SciEntsBankSystem d-uA d-uQ d-uA d-uQ d-uTMaj.
Baseline 57.14% 59.28% 54.30% 60.70% 76.84%Lex.
Baseline 75.43% 71.10% 63.44% 66.05% 59.54%CoMiC 76.57% 71.52% 67.20% 70.23% 64.63%Bag of Words 85.14% 62.03% 80.65% 54.65% 73.79%?
of Lemmas 85.71% 58.02% 80.11% 52.33% 74.55%?
of Soundex 86.86% 60.76% 81.18% 53.95% 72.77%CoSeC 76.00% 74.89% 64.52% 73.49% 68.96%CoMeT 88.00% 75.95% 81.18% 66.74% 68.45%Table 3: Development set: accuracy for 2-way task (uA:unseen answers, uQ: unseen questions, uT: unseen topics)The systems presented in section 2 performed asexpected: The Bag-of-Soundex system achieved itsbest scores on the unseen answers where overlap ofvocabulary was most likely, outperforming CoMiCand CoSeC with accuracy values as high as 86.86%.For Beetle unseen answers, the meta-classifier op-erated as expected and improved the overall resultsto 88.86%.
For SciEntsBank unseen answers, it re-mained stable at 81.18%.As expected, CoMiC and CoSeC with their align-ment not depending on vocabulary outperformed thebag approaches in the other scenarios, in which thequestion or even the domain were not known duringtraining.
However, both alignment-based systemsfailed on SciEntsBank?s unseen topics in comparisonto the rather high majority baseline.3.2 Official Test SetFor our submission to the SemEval 2013 Task 7 chal-lenge, we trained our sub-systems on the entire of-ficial training set.
The overall performance of theCoMeT system on all sub-tasks is shown in Table 4.Beetle SciEntsBankuA uQ uA uQ uTLexical 2-way 79.7% 74.0% 66.1% 67.4% 67.6%Overlap 3-way 59.5% 51.2% 55.6% 54.0% 57.7%Baseline 5-way 51.9% 48.0% 43.7% 41.3% 41.5%Best 2-way 84.5% 74.1% 77.6% 74.5% 71.1%System 3-way 73.1% 59.6% 72.0% 66.3% 63.7%5-way 71.5% 62.1% 64.3% 53.2% 51.2%CoMeT 2-way 83.8% 70.2% 77.4% 60.3% 67.6%3-way 73.1% 51.8% 71.3% 54.6% 57.9%5-way 68.8% 48.8% 60.0% 43.7% 42.1%Table 4: Official test set: overall accuracy of CoMeT (uA:unseen answers, uQ: unseen questions, uT: unseen topics)While CoMeT won the Beetle 3-way task in unseenanswers, our main focus is on the 2-way task.
Theresults for the 2-way task of our sub-systems on theofficial test set are shown in Table 5.The first row of the table reports the results of thewinning system of the challenge; the two baselinesare computed as before.
In general, the accuracy val-ues of CoMeT exhibit a drop of around 5% fromour development set to the official test set.
Themeta-classifier was unable to benefit from the dif-ferent sub-systems except for the unseen answers inSciEntsBank that slightly outperformed the best bagapproach.613Beetle SciEntsBankSystem uA uQ uA uQ uTBest 84.50% 74.10% 77.60% 74.50% 71.10%Maj.
Baseline 59.91% 58.00% 56.85% 58.94% 57.98%Lex.
Baseline 79.70% 74.00% 66.10% 67.40% 67.60%CoMiC 76.08% 70.57% 67.96% 66.30% 67.97%Bag of Words 83.14% 67.52% 75.93% 57.84% 59.84%?
of Lemmas 83.60% 67.16% 76.67% 58.25% 58.81%?
of Soundex 84.05% 68.38% 75.93% 57.57% 58.02%CoSeC 62.19% 63.61% 67.22% 58.94% 62.36%CoMeT 83.83% 70.21% 77.41% 60.30% 67.62%CoSeC* 75.40% 70.82% 72.04% 64.94% 70.60%CoMeT* 84.51% 71.43% 79.26% 65.35% 69.53%Table 5: Official test set: accuracy for 2-way task (uA:unseen answers, uQ: unseen questions, uT: unseen topics)Even though it does not live up to the standards ofthe bag approaches in their area of expertise (unseenanswers), the CoMiC systems outperforms the bagson the unseen question and unseen topic sub-sets asexpected.
Note that on unseen topics, CoMiC stillscores 10% above the majority baseline on the officialtest set, in contrast to the drop of more than 10%below the baseline for the corresponding (skewed)development set.However, the results for CoSeC are around 10%lower on the unseen questions, and almost 7% loweron the unseen topics of the test data than on the de-velopment set, a drop that the overall meta-classifier(CoMeT) was unable to catch.
Investigating this dropin comparison to our development set, we checkedthe correctness of the training script and discovered abug in the CoSeC setup that led to the parameters andthe thresholds being computed on the same partitionof the training set, i.e., the system overfitted to thispartition, while the remainder of the training set wasnot used for training.
Correcting the bug resulted inCoSeC accuracy values broadly comparable to thoseof CoMiC, as was the case on the development set.This confirms that the reason for the drop in the sub-mission was not a flaw in the CoSeC system as such,but a programming bug in a peripheral component.With this bug fixed, CoSeC performs 5%?13%better on the test set, and the meta-classifier wouldhave been able to benefit from the regularly perform-ing CoSeC, improving in performance up to 5%.These two amended systems are listed as CoSeC*and CoMeT* in Table 5.
For the two unseen an-swers scenarios, CoMeT* would outperform the bestscoring systems of the challenge in the 2-way task.3.3 DiscussionIn this section, we try to identify some general ten-dencies from studying the results.
Firstly, we canobserve that due to the strong performance of the bagmodels, unseen answers scores are generally higherthan their counterparts.
It seems that if questionshave been seen before, surface-oriented methods out-perform more abstract approaches.
However, thepicture is different for unseen domains and unseenquestions.
We are generally puzzled by the fact thatmany systems in the shared task scored worse onunseen questions, where in-domain training data isavailable, than on unseen domains, where this is notthe case.
The CoMeT classifier suffered especially inunseen questions of SciEntsBank, scoring lower thanour best system would have on its own (see Table 5);even after the CoSeC bug was fixed, CoMeT* stillscored worse there than CoMiC on its own.In general, we likely would have benefited fromdomain adaptation, as described in, e.g., Daume III(2007).
Consider that the input for the meta-classifieralways consists of the same set of features producedvia standard cross-validation, regardless of the testscenario.
Instead, the trained model should have dif-ferent feature weights depending on what the modelwill be tested on.4 Conclusion and OutlookWe presented our approach to Task 7 of SemEval2013, consisting of a combination of surface-orientedbag models and the increasingly abstract alignment-based systems CoMiC and CoSeC.
Predictions ofall systems were combined using a meta classifier inorder to produce the final result for CoMeT.The results presented show that our approach per-forms competitively, especially in the unseen answerstest scenarios, where we obtained the best result of allparticipants in the 3-way task with the Beetle corpus(73.1% accuracy).
As expected, the unseen topicsscenario proved to be more challenging, with resultsat 67.6% accuracy in the 2-way task for CoMeT.
Sur-prisingly, CoMeT performed consistently worse inthe unseen questions scenarios, which we attributeto rather low CoSeC results there and to the way themeta classifier is trained, which currently does nottake into account the test scenario it is trained forand instead uses the module and question IDs as fea-614tures, which turned out not to be an effective domainadaptation approach.In our future research, work on CoMiC will con-centrate on integrating two aspects of the context:First, we are planning to develop an automatic ap-proach to focus identification in order to pinpoint theessential parts of the student answers.
Second, fordata sets where a reading text is available, we willtry to automatically determine the location of the rel-evant source information given the question, whichcan then be used as alternative or additional referencematerial for answer evaluation.The CoMiC system currently also relies on theTraditional Marriage Algorithm to select the optimalglobal alignment between student answer and refer-ence answer.
We plan to replace this algorithm bya machine learning component that can handle thisselection in a data-driven way.For CoSeC, we plan to develop an extension thatallows for n-to-m mappings, hence improving thealignment performance for multi-word units such as,e.g., phrasal verb constructions.The bag approaches could be augmented by explor-ing additional levels of abstractions, e.g., semanticequivalence classes constructed via WordNet lookup.In sum, while we will also plan to explore opti-mizations to the training setup of the meta-classifier(e.g., domain adaptation along the lines of DaumeIII, 2007), the main focus of our further research liesin improving the individual sub-systems, which thenagain are expected to push the overall performanceof the CoMeT meta-classifier system.AcknowledgementsWe are thankful to Sowmya Vajjala and Serhiy Bykhfor their valuable advice on meta-classifiers and othermachine learning techniques.
We also thank the re-viewers for their comments; in consultation with theSemEval organizers we kept the length at 8 pagesplus references, the page limit for papers describingmultiple systems.ReferencesStacey Bailey and Detmar Meurers.
2008.
Diagnosingmeaning errors in short answers to reading compre-hension questions.
In Joel Tetreault, Jill Burstein,and Rachele De Felice, editors, Proceedings of the3rd Workshop on Innovative Use of NLP for BuildingEducational Applications (BEA-3) at ACL?08, pages107?115, Columbus, Ohio.
http://aclweb.org/anthology/W08-0913.pdf.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceedings ofthe 36th Annual Meeting of the Association for Compu-tational Linguistics and 17th International Conferenceon Computational Linguistics, volume 1, pages 86?90,Montreal, Quebec, Canada.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: A collec-tion of very large linguistically processed web-crawledcorpora.
Journal of Language Resources and Evalua-tion, 3(43):209?226.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch, 2007.
TiMBL: TilburgMemory-Based Learner Reference Guide, ILK Techni-cal Report ILK 07-03.
Induction of Linguistic Knowl-edge Research Group Department of Communicationand Information Sciences, Tilburg University, Tilburg,The Netherlands, July 11.
Version 6.0.Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.2009.
Recognizing textual entailment: Rational, evalu-ation and approaches.
Natural Language Engineering,15(4):i?xvii, 10.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.2013.
Semeval-2013 task 7: The joint student responseanalysis and 8th recognizing textual entailment chal-lenge.
In *SEM 2013: The First Joint Conference onLexical and Computational Semantics, Atlanta, Geor-gia, USA, 13-14 June.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, Mas-sachusetts.David Ferrucci and Adam Lally.
2004.
UIMA: An ar-chitectural approach to unstructured information pro-cessing in the corporate research environment.
NaturalLanguage Engineering, 10(3?4):327?348.David Gale and Lloyd S. Shapley.
1962.
College admis-sions and the stability of marriage.
American Mathe-matical Monthly, 69:9?15.Otis Gospodnetic?
and Erik Hatcher.
2005.
Lucene inAction.
Manning, Greenwich, CT.Michael Hahn and Detmar Meurers.
2011.
On deriv-ing semantic representations from dependencies: A615practical approach for evaluating meaning in learnercorpora.
In Proceedings of the Intern.
Confer-ence on Dependency Linguistics (DEPLING 2011),pages 94?103, Barcelona.
http://purl.org/dm/papers/hahn-meurers-11.html.Michael Hahn and Detmar Meurers.
2012.
Evaluat-ing the meaning of answers to reading comprehen-sion questions: A semantics-based approach.
In Pro-ceedings of the 7th Workshop on Innovative Use ofNLP for Building Educational Applications (BEA-7) atNAACL-HLT 2012, pages 94?103, Montreal.
http://aclweb.org/anthology/W12-2039.pdf.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.
2009.The WEKA data mining software: An update.
In TheSIGKDD Explorations, volume 11, pages 10?18.Michael Halliday.
1967.
Notes on Transitivity and Themein English.
Part 1 and 2.
Journal of Linguistics, 3:37?81, 199?244.Manfred Krifka.
2007.
Basic notions of information struc-ture.
In Caroline Fery, Gisbert Fanselow, and ManfredKrifka, editors, The notions of information structure,volume 6 of Interdisciplinary Studies on InformationStructure (ISIS).
Universita?tsverlag Potsdam, Potsdam.Ivana Kruijff-Korbayova?
and Mark Steedman.
2003.
Dis-course and information structure.
Journal of Logic,Language and Information (Introduction to the SpecialIssue), 12(3):249?259.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-ley.
2011a.
Integrating parallel analysis modules toevaluate the meaning of answers to reading comprehen-sion questions.
IJCEELL.
Special Issue on AutomaticFree-text Evaluation, 21(4):355?369.
http://purl.org/dm/papers/meurers-ea-11.html.Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.2011b.
Evaluating answers to reading comprehen-sion questions in context: Results for German andthe role of information structure.
In Proceedings ofthe TextInfer 2011 Workshop on Textual Entailment,pages 1?9, Edinburgh, Scotland, UK, July.
http://aclweb.org/anthology/W11-2401.pdf.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
NaturalLanguage Engineering, 7(3):207?233.Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(1):1?41.Niels Ott, Ramon Ziai, and Detmar Meurers.
2012.
Cre-ation and analysis of a reading comprehension exercisecorpus: Towards evaluating meaning in context.
InThomas Schmidt and Kai Wo?rner, editors, Multilin-gual Corpora and Multilingual Corpus Analysis, Ham-burg Studies in Multilingualism (HSM), pages 47?69.Benjamins, Amsterdam.
http://purl.org/dm/papers/ott-ziai-meurers-12.html.John C. Platt.
1998.
Sequential minimal optimization:A fast algorithm for training support vector machines.Technical Report MSR-TR-98-14, Microsoft Research.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2002.
Numerical Recipesin C++.
Cambridge University Press, Cambridge, UK.J.R.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann Publishers.Frank Richter and Manfred Sailer.
2004.
Basic conceptsof lexical resource semantics.
In Arnold Beckmann andNorbert Preining, editors, European Summer School inLogic, Language and Information 2003.
Course Mate-rial I, volume 5 of Collegium Logicum, pages 87?143.Publication Series of the Kurt Go?del Society, Wien.Robert C. Russell.
1918.
US patent number 1.261.167, 4.Gerard Salton and Michael J. McGill.
1983.
Introductionto modern information retrieval.
McGraw-Hill, NewYork.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, pages 44?49, Manchester, UK.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost and found:Resources and empirical evaluations in native languageidentification.
In Proceedings of the 24th InternationalConference on Computational Linguistics (COLING),pages 2585?2602, Mumbai, India.Peter Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe Twelfth European Conference on Machine Learn-ing (ECML-2001), pages 491?502, Freiburg, Germany.Klaus von Heusinger.
1999.
Intonation and InformationStructure.
The Representation of Focus in Phonologyand Semantics.
Habilitationssschrift, Universita?t Kon-stanz, Konstanz, Germany.David H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5(2):241?259.Ramon Ziai, Niels Ott, and Detmar Meurers.
2012.
Shortanswer assessment: Establishing links between re-search strands.
In Joel Tetreault, Jill Burstein, andClaudial Leacock, editors, Proceedings of the 7th Work-shop on Innovative Use of NLP for Building Edu-cational Applications (BEA-7) at NAACL-HLT 2012,pages 190?200, Montreal, June.
http://aclweb.org/anthology/W12-2022.pdf.616
