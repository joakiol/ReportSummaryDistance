Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 9?17,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsDid Social Networks Shape Language Evolution?A Multi-Agent Cognitive SimulationDavid ReitterDepartment of PsychologyCarnegie Mellon UniversityPittsburgh, PA, USAreitter@cmu.eduChristian LebiereDepartment of PsychologyCarnegie Mellon UniversityPittsburgh, PA, USAcl@cmu.eduAbstractNatural language as well as other commu-nication forms are constrained by cogni-tive function and evolved through a socialprocess.
Here, we examine whether hu-man memory may be uniquely adapted tothe social structures prevalent in groups,specifically small-world networks.
Theemergence of domain languages is simu-lated using an empirically evaluated ACT-R-based cognitive model of agents in anaming game played within communi-ties.
Several community structures are ex-amined (grids, trees, random graphs andsmall-world networks).
We present pre-liminary results from small-scale simula-tions, showing relative robustness of cog-nitive models to network structure.1 IntroductionA language, even if shared among the membersof a community, is hardly static.
It is constantlyevolving and adapting to the needs of its speak-ers.
Adaptivity in natural language has been foundat various linguistic levels.
Models of dialoguedescribe how interlocutors develop representationsystems in order to communicate; such systemscan, for instance, be observed using referring ex-pressions such as the wall straight ahead that iden-tify locations in a maze.
Experiments have shownthat communities converge on a common standardfor such expressions (Garrod and Doherty, 1994).Models of the horizontal transmission of cul-tural information within generations show on amuch larger scale how beliefs or communicativestandards spread within a single generation of hu-mans.
Recently, language change has acceleratedthrough the use of communication technologies,achieving changes that used to take generationsin years or even months or weeks.
However, thestructure of electronic networks mimics that ofmore traditional social networks, and even com-munication via mass media follows a power-law-driven network topology.The individual agents that are effecting the lan-guage change depend on their cognitive abilitiessuch as memory retrieval and language processingto control and accept novel communication stan-dards.
Do the local, cognitive constraints at theindividual level interact with the structure of large-scale networks?
Both social structure and individ-ual cognitive systems have evolved over a long pe-riod of time, leading to the hypothesis that certainnetwork structures are more suitable than others toconvergence, given the specific human cognitiveapparatus.
Some properties of human cognitionare well established, e.g., in cognitive frameworks(Anderson et al, 2004).
Was human cognitionshaped by social networks?
Why are memory pa-rameters the way they are?
Social network struc-tures may hold an answer to this question.
If so,we should find that naturally occurring networksstructures are uniquely suited to human learning,while others will perform less well when humanlearners are present.The environment may have been influenced byindividual cognition as well.
Why are social net-works structured the way they are?
Human mem-ory and possibly human learning strategies arethe result of an evolutionary process.
Social net-work structures can be explained by models suchas Preferential Attachment (Barabasi and Albert,1999), yet, even that is tied to evolved distribu-tions of preferences in human agents.
Dall?Astaet al (2006) argue that the dynamic of agreementin small-world networks shows, at times, proper-ties that ease the (cognitive) memory burden onthe individuals.
It is possible that the human mem-ory apparatus and social preferences governingnetwork structures have co-evolved.
Such a the-ory would, again, suggest the hypothesis underly-9ing this study: that network structure and humanmemory are co-dependent.2 Modeling Language ChangeNetwork structure, on a small scale, does influ-ence the evolving patterns of communication.
Thedichotomy between individual and community-based learning motivated experiments by Garrodet al (2007) and Fay et al (2010), where partic-ipants played the Pictionary game.
In each trialof this naming game, each participant is paired upwith another participant.
One of them is then tomake a drawing to convey a given concept out ofa small set of known concepts; the other one is toselect the concept from that list without engagingin verbal communication.
Over time, participantsdevelop common standards codifying those con-cepts: they develop a system of meaning-symbolpairs, or, signs.
We take this system as the lex-ical core of the shared language.
The conver-gence rate and the actual language developed dif-fered as a function of the structure of the smallparticipant communities: Fay (2010) either askedthe same pairs of participants to engage in theactivity repeatedly, or matched up different pairsof participants over time.
Fay and Garrod?s Pic-tionary experiments served as the empirical basisfor a cognitive process model developed by (Reit-ter and Lebiere, 2009).
Our model has agents pro-pose signs by combining more elementary signsfrom their divergent knowledge bases, and alsoadopt other agent?s proposals of signs for later re-use.
The model, designed to match Fay?s com-munities, was studied in a condition involvinggroups of eight agents, with two network struc-tures: maximally disjoint with the same pairs ofagents throughout the simulation, and maximallyconnected, with interactions between all possiblepairs of agents.Reitter and Lebiere?s (2009) cognitive model re-flects the Pictionary game.
The model explainsthe convergence as a result of basic learning andmemory retrieval processes, which have been wellunderstood and made available for simulation in acognitive modeling framework, ACT-R Andersonet al (2004).
Thus, properties of human memoryand of the agent?s learning strategies dictate howquickly they adopt signs or establish new signs:processes such as learning, forgetting and noise to-gether with their fundamental parameters that arewithin well-established ranges provide strong con-straints on the behavior of each agent and in turnthe evolution of their communication within thenetwork.
This approach acknowledges that cul-tural evolution is constrained by individual learn-ing; each agent learns according to their cognitivefaculty (cf., Christiansen and Chater, 2008).
Withnon-cognitive models, language change has beensimulated on a larger scale as well (e.g., Kirby andHurford, 2002; Brighton et al, 2005).It is because adaptation according to experi-ence is determined by human learning behav-ior that simulation in validated learning frame-works is crucial.
Griffiths and Kalish (2007)for instance model language evolution throughiteration among rational learners in a Bayesianframework; the purpose of the present project isto tie the simulation of language evolution to aconcrete experiment and a more process-orientedcognitive architecture than the Bayesian frame-work.
ACT-R?s learning mechanisms extend theBayesian view with at least a notion of recency.Work on language processing has pointed out itsrelationship to memory retrieval from within theACT-R framework, both for language comprehen-sion (Budiu and Anderson, 2002; Lewis and Va-sishth, 2005; Crescentini and Stocco, 2005; Ballet al, 2007) and for language production (Reitter,2008).
The individual language faculty as a resultof biological evolution and adaptation to culturallanguage has been the focus of psycholinguis-tic models proposing specialized mechanisms (theChomskian viewpoint); our model does not pro-pose a specialized mechanism but rather declara-tive memory as store for lexical information, andprocedural cognitive processes as regulators ofcertain communicative functions.
Our multi-agentmodel sees part of the linguistic process as an in-stantiation of general cognition: the compositionand retrieval of signs follows general cognitivemechanisms and can be formulated within cogni-tive frameworks such as ACT-R (Anderson et al,2004) or SOAR (Laird and Rosenbloom, 1987).In this study, we adapted the 2009 model andsimulated language convergence in several larger-scale networks.
We investigate the relationshipbetween human memory function in the retrievalof linguistic items and the structure of social net-works on which humans depend to communicate.103 Network structuresDifferences in naturally occurring social networksare hardly as extreme as in Fay?s experiment.Some agents will be connected to a large numberof other ones, while many agents will have just afew connections each.
Concretely, the number ofinteraction partners of a randomly chosen commu-nity member is not normally distributed and cen-tered around a mean.
It shows a (Zipfian) powerlaw distribution, with a number of hubs attractingmany network neighbors, and a long tail of sub-jects interacting with just a few other ones each.Social networks are small world networks: the av-erage distance between any two nodes in the net-works is low, since many of them are connected tohubs.
Non-organically connected communicationand command networks follow other normals?treegraphs for instance.
However, natural communica-tion standards develop in networks that have veryspecific properties that can be observed in most or-ganically developed networks.Realistic social networks commonly show veryspecific properties.
Social networks, in whichlinks symbolize communication pathways or someform of social acquaintance, frequently exhibit thesmall world property.
The mean minimum dis-tance between any two nodes is relatively low, andthe clustering coefficient is high (Watts and Stro-gatz, 1998).Other forms of networks include tree hierar-chies with a constant or variable branching factor(directed acyclic graphs).
Such networks ressem-ble communication and command hierarchies inmilitary or business organizations.
N-dimensionalgrid networks have nodes with constant degrees,which are connected to each of their two neigh-bors along each dimension in a lattice.Much work on information or belief propaga-tion, or decision-making in networks has usedlarge artificial networks modeled after social ones;nodes in such networks are commonly simpleagents that make decisions based on input fed tothem by their neighbor nodes and pass on infor-mation.
These often state-less agents do not nec-essarily employ learning or adaptivity, and whenthey do, learning does not reflect known cognitiveproperties of human memory.
The mechanismsgoverning learning and retrieval in human mem-ory have been studied in detail, leading to formalmodels of process that detail the units that may bestored in and retrieved from memory, the retrievaltime and accuracy depending on the frequency andrecency of prior rehearsals, on contextual cues thatmay facilitate retrieval, and on individual differ-ences.
Cognitive agents can serve as a more real-istic basis for network simulations (Sun, 2001).Frequency, recency, contextual cues and chunk-ing of the stored information determine retrievalprobability, which is crucial when novel idiomsare required to express meaning in communica-tion.
The process leads to the choice of one ofseveral available synonyms.
Our model sees thisdecision-making process as a matter of memoryretrieval: given the desired meaning, which sign(word or drawing, compound noun or drawings)can be used to express it.
This process is implicit(not consciously controlled), and it follows re-cent suggestions from cognitive psychology: Pick-ering and Garrod?s (2004) Interactive AlignmentModel proposes that explicit negotiation and sepa-rate models of the interlocutor?s mental state aren?tnecessary, as long as each speaker is coherent andadapts to their interlocutors, as speakers are knownto do on even simple, linguistic levels (lexical,syntactic).
This shifts the weight of the task froma sophisticated reasoning device to the simpler,more constrained implicit learning mechanism ofthe individual.The social network controls the interactions thatthe agents can experience.
Each interaction is anopportunity to develop new signs and adapt the ex-isting communication systems.
It can be shownthat even separate pairs of agents develop spe-cialized communication systems, both empirically(Garrod and Doherty, 1994; Reitter and Moore,2007; Kirby and Hurford, 2002) and in the specificmodel used here.When communication partnerschange, convergence towards a common systemand the final transmission accuracy is slower (Fayet al, 2008).
At this point it is unclear how thestructure of the communication network and thelearning process interact.
Given that some typesof networks show a wide distribution of degrees,where some nodes communicate much more oftenand with a wide variety of neighbors, while otherscommunicate less often, recency and frequency ofmemory access will vary substantially.
Other com-munication networks may reflect command hier-archies in organizations, which are constructed toensure, among other things, more predictable in-formation propagation.We hypothesize that the human memory ap-11paratus and preferred social network structureshave co-evolved to be uniquely suited to createa macro-organism that adapts its communicationstructures and reasoning mechanisms to novel sit-uations.
There is limited opportunity to test such ahypothesis under controlled conditions with a suf-ficiently large human network; however, cognitivemodels that have been developed to explain andpredict human performance in isolated cognitivesituations can be leveraged to study the develop-ment of sign systems.In a simulated network with cognitive mod-els representing agents at the network nodes,and communication between agents along networklinks, we expect that the social network structureslead to better, if not optimal, adaptivity during theestablishment of a communication system.
We ex-pect that scale-free small world networks do best,outperforming tree hierarchies, random networksand regular grids (lattices).3.1 ArchitectureACT-R?s memory associates symbolic chunks ofinformation (sets of feature-value pairs) with sub-symbolic, activation values.
Learning occursthrough the creation of such a chunk, which isthen reinforced through repeated presentation, andforgotten through decay over time.
The symbolicinformation stored in chunks is available for ex-plicit reasoning, while the subsymbolic informa-tion moderates retrieval, both in speed and in re-trieval probability.
The assumption of rationalityin ACT-R implies that retrievability is governedby the expectation to make use of a piece of in-formation at a later point.
Important to our ap-plication, retrieval is further aided by contextualcues.
When other chunks are in use (e.g., parlia-ment), they support the retrieval of related chunks(building).The properties of memory retrieval in terms oftime and of retrieval success are governed by theactivation of a chunk that is to be retrieved.
Threecomponents of activation are crucial in the contextof this model: base-level activation, spreading ac-tivation and transient noise ().
Base-level activa-tion is predictive of retrieval probability indepen-dent of the concurrent context.
It is determined bythe frequency and recency of use of the particularchunk, with tj indicating the time elapsed sinceuse k of the chunk.
d indicates a base-level decayparameter, usually 0.5):HOSPITALPARAMEDICFIRE STATIONFigure 1: Example of a small ontology with ab-stract concepts (spelled-out words) and concreteones (drawings).Ai = logpres?k=1t?dk +cues?jwjSji + Retrieval is contextualized by cues availablethrough spreading activation.
It is proportionalto the strengths of association (Sji) of all of thecues with the target chunk.
While the base-levelterm (first term of the sum) can be seen as a prior,spreading activation models the conditional proba-bility of retrieval given the available cues.
Finally, is sampled from a logistic distribution shaped bycanonical parameters.
Ai must surpass a minimumretrieval threshold.The model is implemented using the ACT-UPtoolbox, which makes the components of the ACT-R theory are directly accessible.
The cognitivemodel does not specify other model components(perceptual, manual, procedural), as they are nei-ther subject to evaluation nor considered to make asignificant contribution to learning or convergenceeffects.3.2 Communication modelWe assume that the communication system, orlanguage, is a system of signs.
Concretely, it isa set of tuples (signs), each associating a mean-ing with a set of up to three symbols (a simpli-fying assumption).
If the communication systemuses natural language, symbols consist of spokenor written words.
The communication system es-tablished by the participants of Garrod?s and Fay?s12experiments uses drawings as symbols?the princi-ple stays the same.
Agents start out with a knowl-edge base containing signs for concrete conceptsthat are immediately representable as drawings ornouns; the target concepts to be conveyed by theparticipants, however, are more abstract and re-quire the combination of such concrete concepts.A concept such as hospital, for instance, could in-volve the drawings for house, ambulance, and asad face.
A participant could choose among manyways to express hospital.The goal of our cognitive models is to com-municate meaning from one agent to another one.Put in natural language-oriented terminology, thedirector role is the speaker, a role that involvesselecting the right concrete concepts that can ex-press a given target concepts; the matcher role (lis-tener) involves decoding the concrete drawings (orwords) to retrieve the target.A single ACT-R model implements the directorand matcher roles.
As a director, the model es-tablishes new combinations of drawings for giventarget concepts.
As a matcher, the model makesguesses.
In each role, the model revises its internalmappings between drawings and target concepts.The model is copied to instantiate a community ofagents, one for each node in the network.The simplest form of representing a communi-cation system in ACT-R memory chunks is as a setof signs.
Each sign pairs a concept with a set ofdrawings.
Competing signs can be used to assignmultiple drawings for one conceptTo reflect se-mantic relationships, we need to introduce a sub-symbolic notion of relatedness.
We use ACT-R?sspreading activation mechanism and weights be-tween concepts to reflect relatedness.
Spreadingactivation facilitates retrieval of a chunk if the cur-rent context offers cues related to the chunk.
Re-latedness is expressed as a value in log-odds space(Sji values).When the model is faced with the task to drawa given concept such as Russell Crowe (one of theconcepts in the experiment) or Hospital (as in Fig-ure 1) that has no canonical form as a drawing,a related but concrete concept is retrieved fromdeclarative memory (such as Syringe in the exam-ple).
In drawing-based communication, this wouldbe a concept that can be drawn, while in natural-language based communication, this is an existingdrawing expressing a similar, partial or otherwiserelated concept.
We request two other such con-cepts, reflecting the desire of the communicatorto come up with a distinctive rather than just fit-ting depiction of the target concept.
The case of amodel recognizing a novel combination of draw-ings is similar; we retrieve the concept using thedrawings as cues that spread activation, makingthe target concept the one that is the most relatedone to the drawings.After drawings have been produced or recog-nized and mapped to a target, the target or guessedconcept, along with the component drawings, isstored symbolically in memory as a chunk forlater reuse (domain sign).
These signs differ fromthe pre-existing concepts in the network, althoughthey also allow for the retrieval of suitable draw-ings given a concept, and for a concept given somedrawings.
When drawing or recognizing at a laterstage, the memorized domain signs are strictlypreferred as a strategy over the retrieval of relatedconcepts.
The system of domain signs encodeswhat is agreed upon as a language system betweentwo communicators; they will be reused readilyduring drawing when interacting with a new part-ner, but they will be of only limited use when at-tempting to recognize a drawing combination thatadheres to somebody else?s independently devel-oped communication system.Thus, the model has two avenues to express andrecognize an abstract concept: by associative re-trieval and by idiomatic domain concept.
A mes-sage constructed by domain concept retrieval isoften decoded by the matcher by association, andvice versa.The identification accuracy of the model showscharacteristics observed in empirical work (Fay etal.
2008).
See Reitter and Lebiere (subm) for a de-tailed description of the model and its evaluation.3.3 KnowledgeAgents start out with shared world knowledge.This is expressed as a network of concepts, con-nected by weighted links (Sji).
The distributionof link strengths is important in this context, as itdetermines how easily we can find drawing combi-nations that reliably express target concepts.
Thus,the Sji were sampled randomly from an empir-ical distribution: log-odds derived from the fre-quencies of collocations found in text corpus data.From the Wall Street Journal corpus we extractedand counted pairs of nouns that co-occurred in thesame sentence (e.g., ?market?, ?plunge?).
As ex-13ID accuracy (empirical)42 Games over 7 roundsIdentification accuracy0.750.800.850.900.950 10 20 30 40CommunitiesIsolated Pairs42 Games over 7 roundsIdentificationaccuracy0.650.700.750.800.8510 20 30 40CommunitiesIsolated PairsFigure 2: Identification accuracy for isolatedpairs and communities: (a) human data as pro-vided by Fay (p.c.
), (b) simulation.
One-tailedstandard-error based 95% confidence intervals(upper bounds for communities, lower bounds forpairs) for human data; two-tailed 95% via boot-strapping for simulations.
As in the human data,both community pairs and isolated pairs convergemost in the early rounds, but community pairs losemuch accuracy when switching partners.pected, the frequencies of such collocations aredistributed according to a power law.Such knowledge is, however, not fully sharedbetween agents.
Each agent has their own knowl-edge network resulting from life experience.
Thisdifference is essential to the difficulty of the task:if all agents came to the same conclusions aboutthe strongest representation of target concepts,there would be little need to establish the domainlanguage.
We control the noise applied to thelink strengths between concepts j and i for agentM (SMji ) by combining the common ground Sji(shared between all agents) with a random sampleNMji in a mixture model: SMji = (1 ?
n)Sji +nNMji ; sign identification accuracy was found tobe stable for n up to about 0.4; we set it to 0.3 forSimulation 1.4 Simulation 1Networks of individual cognitive agents were cre-ated to differentiate performance between four dif-ferent network structures.
Random networkscontain N nodes with randomly assigned linksbetween them, on average d links for each node(Erdo?s and Re?nyi, 1959).
n-dimensional Gridscontain N nodes with a constant numer of linksd per node, with links between neighbors alongeach dimension.
The width w is kept the samealong each dimension, i.e.
there are w nodes perrow.
We use 6-dimensional lattices.
Trees are di-rected acyclic graphs with 1 link leading up, andd ?
1 links (branching factor) leading down thehierarchy of a total of N nodes.
Scale-free net-works are constructed using the preferential at-tachment method as follows (Barabasi and Albert,1999).
N nodes are created and each is connectedto one randomly selected other node.
Then, twolinks< a, b > and< a?, b?
> are chosen randomlyout of the existing set of links, and a new link< a, b?
> is added, until the mean degree d (linksper node) is reached.
Preferential attachment en-sures that nodes with a high number of links ac-quire further links more quickly than other nodes(the rich get richer).
This yields a power-law dis-tribution of degrees.
Our scale-free networks dis-play small world properties.For the first Simulation, we control N at 85 andd at 5 1.
35 iterations were simulated in each trial;20 trials were run.
During each round, each agent(network node) plays one game (16 concepts) withone of its neighbors.
The order of neighbors isshuffled initially, but constant across the rounds.A variable Round coded iterations from 1to35.Results Figure 3 shows the learning curve foragent pairs in the four networks.
Agents in all net-works converge.
Confidence intervals obtained viabootstrapping indicated no apparent differences atany specific iteration.
A linear model was fit-ted estimating the effects of network type over-all (as a baseline) for each of the four types.
Italso fitted interactions of iteration (1?35) with thenetwork types, which indicate significant learn-ing effects as follows.
For each network type,we found a significant learning effect (effect ofRound) (?
0.002, p < 0.001).Planned comparisons of the learning rate inSmall World networks revealed no difference witheither of the other three network types (p > 0.3).1We found that networks need to be sufficiently large todisplay meaningful differences in community structure.
Thesizes were chosen to be computationally feasible (4h/CPUcore per network).14iterationIdentificationaccuracy0.60.70.80 10 20 30gridsmallworldtreerandomFigure 3: Identification accuracy between con-nected agents for communities of different net-work structures.5 Simulation 2The success of a community is not only deter-mined by how successfully individuals communi-cate in their local environment, that is, with theirnetwork neighbors.
Communities require commu-nicative success outside of well-acquainted agents.Agents?
languages would ideally converge on aglobal scale.
One way to test this is to have ran-domly paired agents play the Pictionary game atregular intervals throughout the game and thusmeasure identification accuracy outside of the net-work that defines the social structure.This simulation was identical to Simulation 1,except that we scaled up the simulation to examinewhether the lack of effect was possibly due to sizeor density of the nodes (N = 512, d = 6, noiselevel: 0.2, repetitions: 20).
In this simulation, wemeasured ID accuracy between pairs of randomlychosen agents after each round.
For three networktypes, Grid, Small World and Random we foundsignificant interactions with round, i.e.
significantconvergence, (all ?
> 0.016, z > 2.1, p < 0.05).For the network type Tree we found no significantinteraction (?
= 0.012, z = 1.55, p = 0.12).22All regressions in this simulation where (generalized)mixed-effects models, with ID accuracy as response via logitlink, Round as predictor, and Condition as factor for four net-work types.
A random intercept was fitted, grouped by repeti-tion (1?20), to account for repeated measures.
The predictorwas centered; no substantial collinearity remained.
The anal-ysis of Simulation 1 was a simple linear model; ID accuracyiterationID accuracyof randomly pairedagents0.600.650.700 10 20 30gridsmallworld treerandomFigure 4: (Aggregate) Identification accuracy be-tween random agent pairs for communities of dif-ferent network structures.To test the initial hypothesis, we re-coded theconditions with a SmallWorld factor, contrastingthe small world networks with all other conditions.We found an effect of Round (?
= 0.017, z =3.66, p < 0.001), indicating convergence, but nointeraction with SmallWorld (?
= ?0.00027, z =?0.03, p = 0.98).3Results Figure 4 shows network-global conver-gence.
Again, a linear model was fitted to estimatethe learning rate in different network types (inter-action of network type and iteration) (baseline in-tercepts were fitted for each network type).
Wefound significant interactions with iteration for thefollowing network types: Grid (?
= 0.004, p <0.001), Small World (?
= 0.003, p < 0.01), andRandom (?
= 0.003, p < 0.005), but not for Tree(p = 0.991).Planned comparisons revealed an interaction ofnetwork type and iteration for Tree compared toSmall World (?
= ?0.003, p < 0.05), but notfor Grid nor Random compared to Small World(p > 0.35).
This indicates slower across-networkconvergence for trees than for small worlds.
It alsosuggests that convergence across the network doesnot differ much between grids, random networksand small worlds.was, for all levels, not near either extreme (?
= 0.77).3Further, unreported, experiments, showed a similar pic-ture with a smaller network as in Simulation 1.156 DiscussionWe find that convergence is relatively stable acrossthe four network types.
Analyzing the differencesbetween the networks, we find that the average de-gree, which was controlled for grids, random net-works and small worlds, was substantially lowerfor trees (d = 1.9) due to the large number ofleaves with degree 1.
This (or the correlated al-gebraic connectivity of the network) may prove tobe a deciding correlate with cross-network conver-gence.
Other metrics, such as the clustering coef-ficient (Watts and Strogatz, 1998), which gives anindication of the degree of neighborhood cohesionWe see these results still as preliminary.
Morework needs to be done to investigate how welllearning scales with network growth, and how net-work analytics such as clustering coefficients af-fect the dispersion of information.Further work will explore range of networksand the possibly unique suitability of human learn-ing mechanisms to succeed in such networks.
Wewill explore the (subsymbolic) parameters govern-ing adaptation, and to what extend the quantitativeparameters we find universal to humans are sub-stantially optimized to deal with the small-worldnetworks and pareto degree-distributions found inhuman communities.7 ConclusionCognition may appear to be adapted to the so-cial structures prevalent in communities of flocks,packs and human teams.
There are many reasonswhy such social structures themselves could haveevolved; if cognitive constraints play a role, we ex-pect it to be only a small factor among many.
Thepresent simulation results certainly do not supportthis view: they are much more compatible witha humans-as-generalists theory that proposes thathumans have evolved to handle a variety of net-work structures well, or that their recency- andfrequency-based learning mechanism is not spe-cialized.Learning, if adapted to social structure in anyway, may go beyond the current, mechanisticand implicit mechanisms implemented in ACT-Rand comparable theories: learning may rely onmore explicit strategies, analyzing one?s interac-tion partners and their current knowledge, and itneeds to judge information according to its sources(trust).
Meta-cognition could also play a role indetermining when a set of signs is substantiallynovel and better than the current system, and thusworth enduring the cost of switching from a settledset of language conventions.We have evaluated only a small, initial part of aco-evolution theory we proposed.
Also, the prob-lem we describe may be best operationalized ata higher abstraction level: Consensus problemsand information spread have been intensively stud-ied (e.g., Latora and Marchiori, 2001; Wu et al,2004).
Comparing community convergence in anumber of differently-structured networks, so farwe see little evidence supporting our hypothesis,namely that cognition (memory) has specialized toaccommodate social structures as defined by con-temporary network science, and that those struc-tures accommodate cognitive properties.
Instead,we find that the simulated cognitive agents con-verge in their communication systems quite wellregardless of the network structures, at least aslong as those networks are relatively small and ofsimilar average degrees.AcknowledgmentsThis work was funded by the Air ForceOffice of Scientific Research (MURI grantFA95500810356).ReferencesAnderson, J. R., Bothell, D., Byrne, M. D., Dou-glass, S., Lebiere, C., and Quin, Y.
(2004).
Anintegrated theory of mind.
Psychological Re-view, 111:1036?1060.Ball, J., Heiberg, A., and Silber, R. (2007).
Towarda large-scale model of language comprehensionin act-r 6.
In Proceedings of the 8th Interna-tional Conference on Cognitive Modeling, AnnArbor, MI.Barabasi, A. L. and Albert, R. (1999).
Emer-gence of scaling in random networks.
Science,286(5439):509?512.Brighton, H., Smith, K., and Kirby, S. (2005).Language as an evolutionary system.
Physicsof Life Reviews, 2(3):177?226.Budiu, R. and Anderson, J. R. (2002).
Compre-hending anaphoric metaphors.
Memory & Cog-nition, 30:158?165.Christiansen, M. H. and Chater, N. (2008).
Lan-guage as shaped by the brain.
Behavioral andBrain Sciences, 31(5):489?509.16Crescentini, C. and Stocco, A.
(2005).
Agramma-tism as a failure in the lexical activation process.In Proceedings of the 27th Annual Conferenceof the Cognitive Science Society.Dall?Asta, L., Baronchelli, A., Barrat, A., andLoreto, V. (2006).
Agreement dynamics onsmall-world networks.
EPL (Europhysics Let-ters), 73(6):969.Erdo?s, P. and Re?nyi, A.
(1959).
On randomgraphs.
I. Publ.
Math.
Debrecen, 6:290?297.Fay, N., Garrod, S., and Roberts, L. (2008).
Thefitness and functionality of culturally evolvedcommunication systems.
Philosophical Trans-actions of the Royal Society B: Biological Sci-ences, 363(1509):3553?3561.Fay, N., Garrod, S., Roberts, L., and Swoboda,N.
(2010).
The interactive evolution of hu-man communication systems.
Cognitive Sci-ence, 34(3):351?386.Garrod, S. and Doherty, G. M. (1994).
Conversa-tion, co-ordination and convention: An empir-ical investigation of how groups establish lin-guistic conventions.
Cognition, 53:181?215.Garrod, S., Fay, N., Lee, J., Oberlander, J., andMacleod, T. (2007).
Foundations of represen-tation: Where might graphical symbol systemscome from?
Cognitive Science, 31(6):961?987.Griffiths, T. L. and Kalish, M. L. (2007).Language evolution by iterated learning withBayesian agents.
Cognitive Science, 31(3):441?480.Kirby, S. and Hurford, J.
(2002).
The emergenceof linguistic structure: An overview of the it-erated learning model.
In Cangelosi, A. andParisi, D., editors, Simulating the Evolution ofLanguage, chapter 6, pages 121?148.
SpringerVerlag, London.Laird, J. E. and Rosenbloom, P. S. (1987).
Soar:An architecture for general intelligence.
Artifi-cial Intelligence, 33(1):1?64.Latora, V. and Marchiori, M. (2001).
Efficientbehavior of small-world networks.
Phys.
Rev.Lett., 87(19):198701.Lewis, R. L. and Vasishth, S. (2005).
Anactivation-based model of sentence processingas skilled memory retrieval.
Cognitive Science,29:1?45.Pickering, M. J. and Garrod, S. (2004).
Towarda mechanistic psychology of dialogue.
Behav-ioral and Brain Sciences, 27:169?225.Reitter, D. (2008).
Context Effects in LanguageProduction: Models of Syntactic Priming in Di-alogue Corpora.
PhD thesis, University of Ed-inburgh.Reitter, D. and Lebiere, C. (2009).
Towards ex-plaining the evolution of domain languages withcognitive simulation.
In Proceedings of the 9thInternational Conference on Cognitive Model-ing (ICCM), Manchester, UK.Reitter, D. and Lebiere, C.
(subm.).
Towards ex-plaining the evolution of domain languages withcognitive simulation.
Cognitive Systems Re-search.Reitter, D. and Moore, J. D. (2007).
Predict-ing success in dialogue.
In Proceedings of the45th Annual Meeting of the Association of Com-putational Linguistics (ACL), pages 808?815,Prague, Czech Republic.Steedman, M. (2000).
The Syntactic Process.
MITPress, Cambridge, MA.Sun, R. (2001).
Cognitive science meets multi-agent systems: A prolegomenon.
PhilosophicalPsychology, 14(1):5?28.Watts, D. J. and Strogatz, S. H. (1998).
Collectivedynamics of /?small-world/?
networks.
Nature,393(6684):440?442.Wu, F., Huberman, B.
A., Adamic, L. A., andTyler, J. R. (2004).
Information flow in socialgroups.
Physica A: Statistical and TheoreticalPhysics, 337(1-2):327 ?
335.17
