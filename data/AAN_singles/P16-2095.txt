Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 585?590,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMetrics for Evaluation of Word-Level Machine Translation QualityEstimationVarvara Logacheva, Michal Lukasik and Lucia SpeciaDepartment of Computer ScienceUniversity of Sheffield, UK{v.logacheva, m.lukasik, l.specia}@sheffield.ac.ukAbstractThe aim of this paper is to investigate suit-able evaluation strategies for the task ofword-level quality estimation of machinetranslation.
We suggest various metricsto replace F1-score for the ?BAD?
class,which is currently used as main metric.We compare the metrics?
performance onreal system outputs and synthetically gen-erated datasets and suggest a reliable alter-native to the F1-BAD score ?
the multi-plication of F1-scores for different classes.Other metrics have lower discriminativepower and are biased by unfair labellings.1 IntroductionQuality estimation (QE) of machine translation(MT) is a task of determining the quality of an au-tomatically translated text without any oracle (ref-erence) translation.
This task has lately been re-ceiving significant attention: from confidence es-timation (i.e.
estimation of how confident a partic-ular MT system is on a word or a phrase (Gan-drabur and Foster, 2003)) it evolved to system-independent QE and is performed at the word level(Luong et al, 2014), sentence level (Shah et al,2013) and document level (Scarton et al, 2015).The emergence of a large variety of approachesto QE led to need for reliable ways to com-pare them.
The evaluation metrics that havebeen used to compare the performance of systemsparticipating in QE shared tasks1have receivedsome criticisms.
Graham (2015) shows that Pear-son correlation better suits for the evaluation ofsentence-level QE systems than mean absolute er-ror (MAE), often used for this purpose.
Pearsoncorrelation evaluates how well a system captures1http://statmt.org/wmt15/quality-estimation-task.htmlthe regularities in the data, whereas MAE essen-tially measures the difference between the true andthe predicted scores and in many cases can be min-imised by always predicting the average score asgiven by the training set labels.Word-level QE is commonly framed as a bi-nary task, i.e., the classification of every translatedword as ?OK?
or ?BAD?.
This task has been eval-uated in terms of F1-score for the ?BAD?
class,a metric that favours ?pessimistic?
systems ?
i.e.systems that tend to assign the ?BAD?
label tomost words.
A trivial baseline strategy that assignsthe label ?BAD?
to all words can thus receive ahigh score while being completely uninformative(Bojar et al, 2014).
However, no analysis of theword-level metrics?
performance has been doneand no alternative metrics have been proposed thatare more reliable than the F1-BAD score.In this paper we compare existing evaluationmetrics for word-level QE, suggest a number of al-ternatives, and show that one of these alternativesleads to more objective and reliable results.2 MetricsOne of the reasons word-level QE is a challeng-ing problem is the fact that ?OK?
and ?BAD?
la-bels are not equally important: we are generallymore interested in finding incorrect words than inassigning a suitable category to every single word.An ideal metric should be oriented towards the re-call for the ?BAD?
class.
However, the case ofF1-BAD score shows that this is not the only re-quirement: in order to be useful the metric shouldnot favour pessimistic labellings, i.e., all or mostwords labelled as ?BAD?.
Below we describe pos-sible alternatives to the F1-BAD score.2.1 F1-score variantsWord-level F1-scores.
Since F1-BAD score istoo pessimistic, an obvious solution would be to585balance it with F1-score for the ?OK?
class.
How-ever, the widely used weighted average of F1-scores for the two classes is not suitable as it willbe dominated by F1-OK due to labels imbalance.Any reasonable MT system will nowadays gener-ate texts where most words are correct, so the la-bel distribution is very skewed towards the ?OK?class.
Therefore, we suggest instead the multi-plication of F1-scores for individual classes: it isequal to zero if one of the components is zero, andsince both are in the [0,1] range, the overall resultwill not exceed the value of any of the multipliers.Phrase-level F1-scores.
One of the features ofMT errors is their phrase-level nature.
Errors arenot independent: one incorrect word can influencethe classification of its neighbours.
If several ad-jacent words are tagged as ?BAD?, they are likelyto be part of an error which spans over a phrase.Therefore, we also evaluate word-level F1-scores and alternative metrics which are based oncorrectly identified erroneous or error-free spansof words.
The phrase-level F1-score we suggestis similar to the one used for the evaluation ofnamed entity recognition (NER) systems (TjongKim Sang and De Meulder, 2003).
There, pre-cision is the percentage of named entities foundby a system that are correct, recall is the percent-age of named entities present in the corpus thatare found by a system.
For the QE task, insteadof named entities we have spans of erroneous (orcorrect) words.
Precision is the percentage of cor-rectly identified spans among all the spans foundby a system, recall is the percentage of correctlyidentified spans among the spans in the test data.However, in NER the correct borders of anamed entity are of big importance, because fail-ure to identify them results in an incorrect entity.On the other hand, the actual borders of an errorspan in QE are not as important: the primary goalis to identify the erroneous region in the sentence,the task of finding the exact borders of an errorcannot be solved unambiguously even by humanannotators (Wisniewski et al, 2013).
In order totake into account partially correct phrases (e.g.
a4-word ?BAD?
phrase where the first word wastagged as ?OK?
by a system and the remainingwords were correctly tagged as ?BAD?
), we com-pute the number of true positives as the sum ofpercentages of words with correctly predicted tagsfor every ?OK?
phrase.
The number of true nega-tives is defined analogously.2.2 Other metricsMatthews correlation coefficient.
MCC (Pow-ers, 2011) was used as a secondary metric inWMT14 word-level QE shared task (Bojar et al,2014).
It is determined as follows:MCC =TP ?
TN + FP ?
FN?
(TP + FP )(TP + FN)(TN + FP )(TN + FN)where TP , TN , FP and FN are true positive,true negative, false positive and false negative val-ues, respectively.This coefficient results in values in the [-1, 1]range.
If the reference and hypothesis labellingsagree on the majority of the examples, the final fig-ure is dominated by the TP ?TN quantity, whichgets close to the value of the denominator.
Themore false positives and false negatives the predic-tor produces, the lower the value of the numerator.Sequence correlation.
The sequence correla-tion score was used as a secondary evaluation met-ric in the QE shared task at WMT15 (Bojar et al,2015).
Analogously to the phrase-level F1-score,it is based on the intersection of spans of correctand incorrect words.
It also weights the phrasesto give them equal importance and penalises thedifference in the number of phrases between thereference and the hypothesis.3 Metrics comparisonOne of the most reliable ways of comparing met-rics is to measure their correlation with humanjudgements.
However, for the word-level QE task,asking humans to rate a system labelling or tocompare the outputs of two or more QE systemsis a very expensive process.
A practical way ofgetting the human judgements is the use of qual-ity labels in downstream human tasks ?
i.e.
taskswhere quality labels can be used as additional in-formation and where they can influence human ac-curacy or speed.
One such a downstream task canbe computer-assisted translation, where the usertranslates a sentence having automatic translationas a draft, and word-level quality labels can high-light incorrect parts in a sentence.
Improvementsin productivity could show the degree of useful-ness of the quality labels in this case.
However,such an experiment is also very expensive to beperformed.
Therefore, we consider indirect waysof comparing the metrics?
reliability based on pre-labelled gold-standard test sets.5863.1 Comparison on real systemsOne of the purposes of system comparison is toidentify the best-performing system.
Therefore,we expect a good metric to be able to distinguishbetween systems as well as possible.
One of thequality criteria for a metric will thus be the num-ber of significantly different groups of systems themetric can identify.
Another criterion to evalu-ate metrics is to compare the real systems?
perfor-mance with synthetic datasets for which we knowthe desirable behaviour of the metrics.
If a metricgives the expected scores to all artificially gener-ated datasets, it detects some properties of the datawhich are relevant to us, so we can expect it towork adequately also on real datasets.Here we compare the performance of six met-rics:?
F1-BAD ?
F1-score for the ?BAD?
class.?
F1-mult ?
multiplication of F1-scores for?BAD?
and ?OK?
classes.?
phr F1-BAD ?
phrase-level F1-score for the?BAD?
class.?
phr F1-mult ?
multiplication of phrase-level F1-scores.?
MCC ?
Matthews Correlation Coefficient.?
SeqCor ?
Sequence Correlation.We used these metrics to rank all systems sub-mitted to the WMT15 QE shared task 2 (word-level QE).2In addition to that, we test the per-formance of the metrics on a number of syntheti-cally created labellings that should be ranked lowin comparison to real system labellings:?
all-bad ?
all words are tagged as ?BAD?.?
all-good ?
all words are tagged as ?OK?.?
optimistic ?
98% words are tagged as?OK?, with only a small number of ?BAD?labels generated: this system should havehigh precision (0.9) and low recall (0.1) forthe ?BAD?
label.?
pessimistic ?
90% words are tagged as?BAD?
: this system should have high recall(0.9) for the ?BAD?
label, but low recall (0.1)for the ?OK?
label.?
random ?
labels are drawn randomly fromthe label probability distribution.We rank the systems according to all the met-rics and compute the level of significance for every2Systems that took part in the shared task are listed anddescribed in (Bojar et al, 2015).pair of systems with randomisation tests (Yeh,2000) with Bonferroni correction (Abdi, 2007).In order to evaluate the metrics?
performance wecompute the system distinction coefficient d ?
theprobability of two systems being significantly dif-ferent, which is defined as the ratio between thenumber of significantly different pairs of systemsand all pairs of systems.
We also compute d forthe top half and for the bottom half of the rankedsystems list separately in order to check how welleach metric can discriminate between better per-forming and worse performing systems.3The results are shown in Table 1.
For everysynthetic dataset we show the number of real sys-tem outputs that were rated lower than this dataset,with the rightmost column showing the sum of thisfigure across all the synthetic sets.We can see that three metrics are better at distin-guishing synthetic results from real systems: Se-qCor and both multiplied F1-scores.
In the caseof SeqCor this result is explained by the fact thatit favours longer spans of ?OK?
and ?BAD?
la-bels and thus penalises arbitrary labellings.
Themultiplications of F1-scores have two componentswhich penalise different labellings and balanceeach other.
This assumption is confirmed by thefact that F1-BAD scores become too pessimisticwithout the ?OK?
component: they both favoursynthetic systems with prevailing ?BAD?
labels.Phrase-F1-BAD ranks these systems the highest:all-bad and pessimistic outperform 16 out of 17systems according to this metric.MCC is, in contrast, too ?optimistic?
: the opti-mistic dataset is rated higher than most of systemoutputs.
In addition to that, it is not good at distin-guishing different systems: its system distinctioncoefficient is the lowest among all metric.
SeqCorand phrase-F1-multiplied, despite identifying ar-tificial datasets, cannot discriminate between realsystems: SeqCor fails with the top half systems,phrase-F1-multiplied is bad at finding differencesin the bottom half of the list.Overall, F1-multiplied is the only metric thatperforms well both in the task of distinguishing3dbottomis always greater than dtopin our experimentsbecause better performing systems tend to have closer scoresunder all metrics and more often are not significantly differ-ent from one another.
When comparing two metrics, greaterd does not imply greater dtopand dbottom: we use Bonfer-roni correction for which the significance level depends onthe number of compared values, so a difference which is sig-nificant when comparing eight systems, for example, can be-come insignificant when comparing 16 systems.587d dtopdbottomall-bad all-good optimistic pessimistic random totalF1-BAD 0.79 0.61 0.81 4 - 1 4 1 10F1-mult 0.81 0.57 0.75 - - 2 - 2 4phr F1-BAD 0.86 0.61 0.78 16 - 1 16 - 33phr F1-mult 0.75 0.54 0.47 - - 1 - - 1MCC 0.63 0.61 0.34 - - 15 - - 15SeqCor 0.77 0.39 0.75 - - 1 1 2 4Table 1: Results for all metrics.
Numbers in synthetic dataset columns denote the number of systemsubmissions that were rated lower than the corresponding synthetic dataset.synthetic systems from real ones and in the taskof discriminating among real systems, despite thefact that its d scores are not the best.
However,F1-BAD is not far behind: it has high values ford scores and can identify synthetic datasets quiteoften.3.2 Comparison on synthetic datasetsThe experiment described above has a notabledrawback: we evaluated metrics on the outputs ofsystems which had been tuned to maximise the F1-BAD score.
This means that the system rankingsproduced by other metrics may be unfairly consid-ered inaccurate.Therefore, we suggest a more objective met-ric evaluation procedure which uses only syntheticdatasets.
We generate datasets with different pro-portion of errors, compute the metrics?
values andtheir statistical significance and then compare themetrics?
discriminative power.
This procedure isfurther referred to as repeated sampling, becausewe sample artificial datasets multiple times.Our goal is for the synthetic datasets to simulatereal systems?
output.
We achieve this by using thefollowing procedure for synthetic data generation:?
Choose the proportion of errors to introducein the synthetic data.?
Collect all sequences that contain incorrectlabels from the outputs of real systems.?
Randomly choose the sequences from this setuntil the overall number of errors reaches thechosen threshold.?
Take the rest of segments from the gold-standard labelling (so that they contain no er-rors).Thus our artificial datasets contain a specificnumber of errors, and all of them come from realsystems.
We can generate datasets with very smalldifferences in quality and identify metrics accord-ing to which this difference is more significant.Let us compare the discriminative power ofmetrics m1and m2.
We choose two error thresh-olds e1and e2.
Then we sample a relatively smallnumber (e.g.
100) of random datasets with e1er-rors.
Then ?
100 random datasets with e2er-rors.
We compute the values for both metrics onthe two sets of random samples and for each met-ric we test if the difference between the results forthe two sets is significant (we compute the statisticsignificance using non-paired t-test with Bonfer-roni correction).
Since we sampled the syntheticdatasets a small number of times it is likely thatthe metrics will not detect any significant differ-ences between them.
In this case we repeat theprocess with a larger (e.g.
200) number of samplesand compare the p-values for two metrics again.By gradually increasing the number of samplesat some point we will find that one of the met-rics recognises the differences in scores as statisti-cally significant, while another one does not.
Thismeans that this metric has higher discriminativepower: it needs less samples to determine that thesystems they are different.
The procedure is out-lined in Algorithm 1.In our experiments in order to make p-valuesmore stable we repeat each sampling round (sam-pling of a set with eierrors 100, 200, etc.
times)1,000 times and use the average of p-values.
Weused fixed sets of sample numbers: [100, 200, 500,1000, 2000, 5000, 10,000] and error thresholds:[30%, 30.01%, 30.05%, 30.1%, 30.2%].
The sig-nificance level ?
is 0.05.Since we compare all six metrics on five er-ror thresholds, we have 10 p-values for each met-ric at every sampling round.
We analyse the re-sults in the following way: for every difference inthe percentage of errors (e.g.
thresholds of 30%and 30.01% give 0.01% difference, thresholds of30% and 30.2% ?
0.2% difference), we definethe minimum number of samplings that a metric5880.01 0.04 0.05 0.1 0.15 0.2F1-mult 10000 2000 2000 500 200 100MCC 10000 2000 2000 500 200 100F1-BAD 10000 5000 2000 1000 500 200phr F1-mult 10000 5000 5000 1000 500 200SeqCor 10000 5000 5000 1000 500 500phr F1-BAD 10000 10000 5000 1000 500 500Table 2: Repeated sampling: the minimum number of samplings required to discriminate between sam-ples with a different proportions of errors.Result: mx?
{m1, m2}, where mx?
metricwith the highest discriminative poweron error thresholds e1and e2N ?
100??
significance levelwhile p-valm1> ?
and p-valm2> ?
dos1?
N random samples with e1errorss2?
N random samples with e2errorsp-valm1?
t-test(m1(s1),m1(s2))p-valm2?
t-test(m2(s1),m2(s2))if p-valm1< ?
and p-valm2> ?
thenreturn m1else if p-valm1> ?
and p-valm2< ?thenreturn m2elseN ?
N + 100endAlgorithm 1: Repeated sampling for metricsm1,m2and error thresholds e1, e2.needs to observe significant differences betweendatasets which differ in this number of errors.
Ta-ble 2 shows the results.
Numbers in cells are min-imum numbers of samplings.
We do not show er-ror differences greater than 0.2 because all metricsidentify them well.
All metrics are sorted by dis-criminative power from best to worst, i.e.
metricsat the top of the table require less samplings to tellone synthetic dataset from another.As in the previous experiment, here the discrim-inative power of the multiplication of F1-scores isthe highest.
Surprisingly, MCC performs equallywell.
Similarly to the experiment with real sys-tems, the F1-BAD metric performs worse thanthe F1-multiply metric, but here their difference ismore salient.
All phrase-motivated metrics showworse results.4 ConclusionsThe aim of this paper was to compare evaluationmetrics for word and phrase-level quality estima-tion and find an alternative for F1-BAD score,which has been used as primary metric in recentresearch but has a number of drawbacks, in partic-ular tendency to overrate labellings with predomi-nantly?BAD?
instances.We found that the multiplication of F1-BADand F1-OK scores is more stable against ?pes-simistic?
labellings and has bigger discrimina-tive power when comparing synthetic datasets.However, other tested metrics, including advancedphrase-based scores, could not outperform F1-BAD.This work should be seen as a proxy forreal user evaluation of word-level QE metrics,which could be done on downstream tasks (e.g.computer-assisted translation).ReferencesHerv?e Abdi.
2007.
The bonferroni and ?sid?ak cor-rections for multiple comparisons.
Encyclopedia ofmeasurement and statistics, 3:103?107.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?sTamchyna.
2014.
Findings of the 2014 workshopon statistical machine translation.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, pages 12?58, Baltimore, Maryland, USA, June.Association for Computational Linguistics.Ond?rej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp,Philipp Koehn, Varvara Logacheva, Christof Monz,Matteo Negri, Matt Post, Carolina Scarton, LuciaSpecia, and Marco Turchi.
2015.
Findings of the2015 workshop on statistical machine translation.In Proceedings of the Tenth Workshop on StatisticalMachine Translation, pages 1?46, Lisbon, Portugal.589Simona Gandrabur and George Foster.
2003.
Confi-dence estimation for translation prediction.
In HLT-NAACL-2003, pages 95?102, Edmonton, Canada.Yvette Graham.
2015.
Improving evaluation of ma-chine translation quality estimation.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 1804?1813.Ngoc Quang Luong, Laurent Besacier, and BenjaminLecouteux.
2014.
Lig system for word level qetask at wmt14.
In WMT-2014, pages 335?341, Bal-timore, USA, June.David M.W.
Powers.
2011.
Evaluation: from preci-sion, recall and F-measure to ROC, informedness,markedness and correlation.
Journal of MachineLearning Technologies, 2(1):37?63.Carolina Scarton, Liling Tan, and Lucia Specia.
2015.Ushef and usaar-ushef participation in the wmt15 qeshared task.
In Proceedings of the Tenth Workshopon Statistical Machine Translation, pages 336?341,Lisbon, Portugal, September.Kashif Shah, Trevor Cohn, and Lucia Specia.
2013.An investigation on the effectiveness of features fortranslation quality estimation.
In MT Summit XIV,pages 167?174, Nice, France.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InProceedings of CoNLL-2003, pages 142?147.Guillaume Wisniewski, Anil Kumar Singh, Natalia Se-gal, and Franc?ois Yvon.
2013.
Design and Anal-ysis of a Large Corpus of Post-Edited Translations:Quality Estimation, Failure Analysis and the Vari-ability of Post-Edition.
In MT Summit XIV: 14thMachine Translation Summit, pages 117?124, Nice,France.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Coling-2000: the 18th Conference on Computational Lin-guistics, pages 947?953, Saarbr?ucken, Germany.590
