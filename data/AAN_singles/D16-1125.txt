Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1173?1182,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsReasoning about Pragmatics with Neural Listeners and SpeakersJacob Andreas and Dan KleinComputer Science DivisionUniversity of California, Berkeley{jda,klein}@cs.berkeley.eduAbstractWe present a model for contrastively describ-ing scenes, in which context-specific behav-ior results from a combination of inference-driven pragmatics and learned semantics.
Likeprevious learned approaches to language gen-eration, our model uses a simple feature-driven architecture (here a pair of neural ?lis-tener?
and ?speaker?
models) to ground lan-guage in the world.
Like inference-driven ap-proaches to pragmatics, our model activelyreasons about listener behavior when select-ing utterances.
For training, our approach re-quires only ordinary captions, annotated with-out demonstration of the pragmatic behaviorthe model ultimately exhibits.
In human eval-uations on a referring expression game, ourapproach succeeds 81% of the time, comparedto 69% using existing techniques.1 IntroductionWe present a model for describing scenes and ob-jects by reasoning about context and listener behav-ior.
By incorporating standard neural modules forimage retrieval and language modeling into a prob-abilistic framework for pragmatics, our model gen-erates rich, contextually appropriate descriptions ofstructured world representations.This paper focuses on a reference game RGplayed between a listener L and a speaker S.1.
Reference candidates r1 and r2 are re-vealed to both players.2.
S is secretly assigned a random target t ?
{1, 2}.3.
S produces a description d = S(t, r1, r2),which is shown to L.4.
L chooses c = L(d, r1, r2).5.
Both players win if c = t.(RG)(a) target (b) distractorthe owl is sitting in the tree(c) descriptionFigure 1: Sample output from our model.
When presented witha target image (a) in contrast with a distractor image (b), themodel generates a description (c).
This description mentions atree, the distinguishing object present in (a) but not in (b), andsituates it with respect to other objects and events in the scene.Figure 1 shows an example drawn from a standardcaptioning dataset (Zitnick et al, 2014).In order for the players to win, S?s description dmust be pragmatic: it must be informative, fluent,concise, and must ultimately encode an understand-ing of L?s behavior.
In Figure 1, for example, theowl is wearing a hat and the owl is sitting in the treeare both accurate descriptions of the target image,but only the second allows a human listener to suc-ceed with high probability.
RG is the focus of manypapers in the computational pragmatics literature: itprovides a concrete generation task while eliciting abroad range of pragmatic behaviors, including con-versational implicature (Benotti and Traum, 2009)and context dependence (Smith et al, 2013).
Exist-ing computational models of pragmatics can be di-vided into two broad lines of work, which we termthe direct and derived approaches.Direct models (see Section 2 for examples) arebased on a representation of S. They learn prag-matic behavior by example.
Beginning with datasetsannotated for the specific task they are trying to1173solve (e.g.
examples of humans playing RG), directmodels use feature-based architectures to predict ap-propriate behavior without a listener representation.While quite general in principle, such models re-quire training data annotated specifically with prag-matics in mind; such data is scarce in practice.Derived models, by contrast, are based on a repre-sentation of L. They first instantiate a base listenerL0 (intended to simulate a na?
?ve, non-pragmaticlistener).
They then form a reasoning speakerS1, which chooses a description that causes L0to behave correctly.
Existing derived models cou-ple hand-written grammars and hand-engineered lis-tener models with sophisticated inference proce-dures.
They exhibit complex behavior, but are re-stricted to small domains where grammar engineer-ing is practical.The approach we present in this paper aims tocapture the best aspects of both lines of work.
Likedirect approaches, we use machine learning to ac-quire a complete grounded generation model fromdata, without domain knowledge in the form of ahand-written grammar or hand-engineered listenermodel.
But like derived approaches, we use thislearning to construct a base model, and embed itwithin a higher-order model that reasons about lis-tener responses.
As will be seen, this reasoning stepallows the model to make use of weaker supervisionthan previous data-driven approaches, while exhibit-ing robust behavior in a variety of contexts.Our goal is to build a derived model that scales toreal-world datasets without domain engineering.
In-dependent of the application to RG, our model alsobelongs to the family of neural image captioningmodels that have been a popular subject of recentstudy (Xu et al, 2015).
Nevertheless, our approachappears to be:?
the first such captioning model to reasonexplicitly about listeners?
the first learned approach to pragmatics that re-quires only non-pragmatic training dataFollowing previous work, we evaluate our modelon RG, though the general architecture could be ap-plied to other tasks where pragmatics plays a corerole.
Using a large dataset of abstract scenes likethe one shown in Figure 1, we run a series of gameswith humans in the role of L and our system in therole of S. We find that the descriptions generated byour model result in correct interpretation 17% moreoften than a recent learned baseline system.
We usethese experiments to explore various other aspectsof computational pragmatics, including tradeoffs be-tween adequacy and fluency, and between computa-tional efficiency and expressive power.12 Related WorkDirect pragmatics As an example of the directapproach mentioned in the introduction, FitzGeraldet al (2013) collect a set of human-generated re-ferring expressions about abstract representations ofsets of colored blocks.
Given a set of blocks todescribe, their model directly learns a maximum-entropy distribution over the set of logical expres-sions whose denotation is the target set.
Other re-search, focused on referring expression generationfrom a computer vision perspective, includes that ofMao et al (2015) and Kazemzadeh et al (2014).Derived pragmatics Derived approaches, some-times referred to as ?rational speech acts?
models,include those of Smith et al (2013), Vogel et al(2013), Golland et al (2010), and Monroe and Potts(2015).
These couple template-driven language gen-eration with probabilistic or game-theoretic reason-ing frameworks to produce contextually appropriatelanguage: intelligent listeners reason about the be-havior of reflexive speakers, and even higher-orderspeakers reason about these listeners.
Experiments(Frank et al, 2009) show that derived approaches ex-plain human behavior well, but both computationaland representational issues restrict their applicationto simple reference games.
They require domain-specific engineering, controlled world representa-tions, and pragmatically annotated training data.An extensive literature on computational prag-matics considers its application to tasks other thanRG, including instruction following (Anderson etal., 1991) and discourse analysis (Jurafsky et al,1997).1Models, human annotations, and code to generate all tablesand figures in this paper can be found at http://github.com/jacobandreas/pragma.1174Representing language and the world In addi-tion to the pragmatics literature, the approach pro-posed in this paper relies extensively on recently de-veloped tools for multimodal processing of languageand unstructured representations like images.
Theseincludes both image retrieval models, which selectan image from a collection given a textual descrip-tion (Socher et al, 2014), and neural conditional lan-guage models, which take a content representationand emit a string (Donahue et al, 2015).3 ApproachOur goal is to produce a model that can play therole of the speaker S in RG.
Specifically, given atarget referent (e.g.
scene or object) r and a dis-tractor r?, the model must produce a description dthat uniquely identifies r. For training, we have ac-cess to a set of non-contrastively captioned referents{(ri, di)}: each training description di is generatedfor its associated referent ri in isolation.
There isno guarantee that di would actually serve as a goodreferring expression for ri in any particular context.We must thus use the training data to ground lan-guage in referent representations, but rely on reason-ing to produce pragmatics.Our model architecture is compositional and hi-erarchical.
We begin in Section 3.2 by describing acollection of ?modules?
: basic computational prim-itives for mapping between referents, descriptions,and reference judgments, here implemented as lin-ear operators or small neural networks.
While thesemodules appear as substructures in neural architec-tures for a variety of tasks, we put them to novel usein constructing a reasoning pragmatic speaker.Section 3.3 describes how to assemble two basemodels: a literal speaker, which maps from refer-ents to strings, and a literal listener, which mapsfrom strings to reference judgments.
Section 3.4 de-scribes how these base models are used to imple-ment a top-level reasoning speaker: a learned, prob-abilistic, derived model of pragmatics.3.1 PreliminariesFormally, we take a description d to consist of a se-quence of words d1, d2, .
.
.
, dn, drawn from a vo-cabulary of known size.
For encoding, we also as-sume access to a feature representation f(d) of thesentenceFCFCReLUSum FC SoftmaxReLUFC SoftmaxFCngram	features descref	features referentreferentwordnword<n wordn+1choicereferentdesc(d) referent describer D(a) referent encoder Er (b) description encoder Ed(c) choice ranker RFigure 2: Diagrams of modules used to construct speaker andlistener models.
?FC?
is a fully-connected layer (a matrix multi-ply) and ?ReLU?
is a rectified linear unit.
The encoder modules(a,b) map from feature representations (in gray) to embeddings(in black), while the ranker (c) and describer modules (d) re-spectively map from embeddings to decisions and strings.sentence (for purposes of this paper, a vector of in-dicator features on n-grams).
These two views?asa sequence of words di and a feature vector f(d)?form the basis of module interactions with language.Referent representations are similarly simple.
Be-cause the model never generates referents?onlyconditions on them and scores them?a vector-valued feature representation of referents suffices.Our approach is completely indifferent to the na-ture of this representation.
While the experimentsin this paper use a vector of indicator features onobjects and actions present in abstract scenes (Fig-ure 1), it would be easy to instead use pre-trainedconvolutional representations for referring to naturalimages.
As with descriptions, we denote this featurerepresentation f(r) for referents.3.2 ModulesAll listener and speaker models are built from a kitof simple building blocks for working with multi-modal representations of images and text:1. a referent encoder Er2.
a description encoder Ed3.
a choice ranker R4.
a referent describer D1175These are depicted in Figure 2, and specified moreformally below.
All modules are parameterized byweight matrices, written with capital lettersW1,W2,etc.
; we refer to the collection of weights for allmodules together as W .Encoders The referent and description encodersproduce a linear embedding of referents and descrip-tions in a common vector space.Referent encoder: Er(r) = W1f(r) (1)Description encoder: Ed(d) = W2f(d) (2)Choice ranker The choice ranker takes a stringencoding and a collection of referent encodings, as-signs a score to each (string, referent) pair, and thentransforms these scores into a distribution over ref-erents.
We write R(ei|e?i, ed) for the probability ofchoosing i in contrast to the alternative; for exam-ple, R(e2|e1, ed) is the probability of answering ?2?when presented with encodings e1 and e2.s1 = w>3 ?
(W4e1 +W5ed)s2 = w>3 ?
(W4e2 +W5ed)R(ei|e?i, ed) =esies1 + es2 (3)(Here ?
is a rectified linear activation function.
)Referent describer The referent describer takesan image encoding and outputs a description us-ing a (feedforward) conditional neural languagemodel.
We express this model as a distributionp(dn+1|dn, d<n, er), where dn is an indicator fea-ture on the last description word generated, d<n is avector of indicator features on all other words pre-viously generated, and er is a referent embedding.This is a ?2-plus-skip-gram?
model, with local posi-tional history features, global position-independenthistory features, and features on the referent beingdescribed.
To implement this probability distribu-tion, we first use a multilayer perceptron to com-pute a vector of scores s (one si for each vocabularyitem): s = W6?
(W7[dn, d<n, ei]).
We then normal-ize these to obtain probabilities: pi = esi/?j esj .Finally, p(dn+1|dn, d<n, er) = pdn+1 .3.3 Base modelsFrom these building blocks, we construct a pair ofbase models.
The first of these is a literal listenerDesc.
encoderRef.
encoderRef.
encoderRankerRef.
decoderRef.
encoderLiteral listener (L0) Literal speaker (S0)Reasoning speaker (S1)S0L0SamplerFigure 3: Schematic depictions of models.
The literal listenerL0 maps from descriptions and reference candidates to ref-erence decisions.
The literal speaker S0 maps directly fromscenes to descriptions, ignoring context, while the reasoningspeaker uses samples from S0 and scores from both L0 and S0to produce contextually-appropriate captions.L0, which takes a description and a set of referents,and chooses the referent most likely to be described.This serves the same purpose as the base listener inthe general derived approach described in the intro-duction.
We additionally construct a literal speakerS0, which takes a referent in isolation and outputs adescription.
The literal speaker is used for efficientinference over the space of possible descriptions, asdescribed in Section 3.4.
L0 is, in essence, a retrievalmodel, and S0 is neural captioning model.Both of the base models are probabilistic: L0 pro-duces a distribution over referent choices, and S0produces a distribution over strings.
They are de-picted with shaded backgrounds in Figure 3.Literal listener Given a description d and a pair ofcandidate referents r1 and r2, the literal listener em-beds both referents and passes them to the rankingmodule, producing a distribution over choices i.ed = Ed(d)e1 = Er(r1)e2 = Er(r2)pL0(i|d, r1, r2) = R(ei|e?i, ed) (4)That is, pL0(1|d, r1, r2) = R(e1|e2, ed) and vice-versa.
This model is trained contrastively, by solvingthe following optimization problem:maxW?jlog pL0(1|dj , rj , r?)
(5)1176Here r?
is a random distractor chosen uniformlyfrom the training set.
For each training exam-ple (ri, di), this objective attempts to maximize theprobability that the model chooses ri as the referentof di over a random distractor.This contrastive objective ensures that our ap-proach is applicable even when there is not anaturally-occurring source of target?distractor pairs,as previous work (Golland et al, 2010; Monroe andPotts, 2015) has required.
Note that this can also beviewed as a version of the loss described by Smithand Eisner (2005), where it approximates a likeli-hood objective that encourages L0 to prefer ri to ev-ery other possible referent simultaneously.Literal speaker As in the figure, the literalspeaker is obtained by composing a referent encoderwith a describer, as follows:e = Er(f(r))pS0(d|r) = Dd(d|e)As with the listener, the literal speaker should be un-derstood as producing a distribution over strings.
Itis trained by maximizing the conditional likelihoodof captions in the training data:maxW?ilog pS0(di|ri) (6)These base models are intended to be the minimallearned equivalents of the hand-engineered speak-ers and hand-written grammars employed in previ-ous derived approaches (Golland et al, 2010).
Theneural encoding/decoding framework implementedby the modules in the previous subsection providesa simple way to map from referents to descriptionsand descriptions to judgments without worrying toomuch about the details of syntax or semantics.
Pastwork amply demonstrates that neural conditionallanguage models are powerful enough to generatefluent and accurate (though not necessarily prag-matic) descriptions of images or structured represen-tations (Donahue et al, 2015).3.4 Reasoning modelAs described in the introduction, the general derivedapproach to pragmatics constructs a base listenerand then selects a description that makes it behavecorrectly.
Since the assumption that listeners willbehave deterministically is often a poor one, it iscommon for such derived approaches to implementprobabilistic base listeners, and maximize the prob-ability of correct behavior.The neural literal listener L0 described in the pre-ceding section is such a probabilistic listener.
Givena target i and a pair of candidate referents r1 and r2,it is natural to specify the behavior of a reasoningspeaker as simply:maxdpL0(i|d, r1, r2) (7)At a first glance, the only thing necessary to im-plement this model is the representation of the literallistener itself.
When the set of possible utterancescomes from a fixed vocabulary (Vogel et al, 2013)or a grammar small enough to exhaustively enumer-ate (Smith et al, 2013) the operation maxd in Equa-tion 7 is practical.For our purposes, however, we would like themodel to be capable of producing arbitrary utter-ances.
Because the score pL0 is produced by adiscriminative listener model, and does not factoralong the words of the description, there is no dy-namic program that enables efficient inference overthe space of all strings.We instead use a sampling-based optimizationprocedure.
The key ingredient here is a good pro-posal distribution from which to sample sentenceslikely to be assigned high weight by the model lis-tener.
For this we turn to the literal speaker S0described in the previous section.
Recall that thisspeaker produces a distribution over plausible de-scriptions of isolated images, while ignoring prag-matic context.
We can use it as a source of candi-date descriptions, to be reweighted according to theexpected behavior of L0.
The full specification of asampling neural reasoning speaker is as follows:1.
Draw samples d1, .
.
.
dn ?
pS0(?|ri).2.
Score samples: pk = pL0(i|dk, r1, r2).3.
Select dk with k = argmax pk.While primarily to enable efficient inference, wecan also use the literal speaker to serve a differ-ent purpose: ?regularizing?
model behavior towardschoices that are adequate and fluent, rather than ex-ploiting strange model behavior.
Past work has re-1177stricted the set of utterances in a way that guaran-tees fluency.
But with an imperfect learned listenermodel, and a procedure that optimizes this listener?sjudgments directly, the speaker model might acci-dentally discover the kinds of pathological optimathat neural classification models are known to ex-hibit (Goodfellow et al, 2014)?in this case, sen-tences that cause exactly the right response from L0,but no longer bear any resemblance to human lan-guage use.
To correct this, we allow the model toconsider two questions: as before, ?how likely isit that a listener would interpret this sentence cor-rectly?
?, but additionally ?how likely is it that aspeaker would produce it?
?Formally, we introduce a parameter ?
that tradesoff between L0 and S0, and take the reasoning modelscore in step 2 above to be:pk = pS0(dk|ri)?
?
pL0(i|dk, r1, r2)1??
(8)This can be viewed as a weighted joint probabilitythat a sentence is both uttered by the literal speakerand correctly interpreted by the literal listener, or al-ternatively in terms of Grice?s conversational max-ims (Grice, 1970): L0 encodes the maxims of qual-ity and relation, ensuring that the description con-tains enough information for L to make the rightchoice, while S0 encodes the maxim of manner, en-suring that the description conforms with patterns ofhuman language use.
Responsibility for the maximof quantity is shared: L0 ensures that the modeldoesn?t say too little, and S0 ensures that the modeldoesn?t say too much.4 EvaluationWe evaluate our model on the reference game RGdescribed in the introduction.
In particular, we con-struct instances of RG using the Abstract ScenesDataset introduced by Zitnick and Parikh (2013).Example scenes are shown in Figure 1 and Figure4.
The dataset contains pictures constructed by hu-mans and described in natural language.
Scene rep-resentations are available both as rendered imagesand as feature representations containing the identityand location of each object; as noted in Section 3.1,we use this feature set to produce our referent rep-resentation f(r).
This dataset was previously usedfor a variety of language and vision tasks (e.g.
Or-tiz et al (2015), Zitnick et al (2014)).
It consists of10,020 scenes, each annotated with up to 6 captions.The abstract scenes dataset provides a more chal-lenging version of RG than anything we are aware ofin the existing computational pragmatics literature,which has largely used the TUNA corpus of isolatedobject descriptions (Gatt et al, 2007) or small syn-thetic datasets (Smith et al, 2013).
By contrast, theabstract scenes data was generated by humans look-ing at complex images with numerous objects, andfeatures grammatical errors, misspellings, and a vo-cabulary an order of magnitude larger than TUNA.Unlike previous work, we have no prespecified in-domain grammar, and no direct supervision of therelationship between scene features and lexemes.We perform a human evaluation using AmazonMechanical Turk.
We begin by holding out a de-velopment set and a test set; each held-out set con-tains 1000 scenes and their accompanying descrip-tions.
For each held-out set, we construct two sets of200 paired (target, distractor) scenes: All, with up tofour differences between paired scenes, and Hard,with exactly one difference between paired scenes.
(We take the number of differences between scenesto be the number of objects that appear in one scenebut not the other.
)We report two evaluation metrics.
Fluency isdetermined by showing human raters isolated sen-tences, and asking them to rate linguistic quality ona scale from 1?5.
Accuracy is success rate at RG:as in Figure 1, humans are shown two images and amodel-generated description, and asked to select theimage matching the description.In the remainder of this section, we measure thetradeoff between fluency and accuracy that resultsfrom different mixtures of the base models (Sec-tion 4.1), measure the number of samples neededto obtain good performance from the reasoning lis-tener (Section 4.2), and attempt to approximate thereasoning listener with a monolithic ?compiled?
lis-tener (Section 4.3).
In Section 4.4 we report finalaccuracies for our approach and baselines.# samples 1 10 100 1000Accuracy (%) 66 75 83 85Table 1: S1 accuracy vs. number of samples.1178Figure 5: Tradeoff between speaker and listener models, con-trolled by the parameter ?
in Equation 8.
With ?
= 0, all weightis placed on the literal listener, and the model produces highlydiscriminative but somewhat disfluent captions.
With ?
= 1, allweight is placed on the literal speaker, and the model producesfluent but generic captions.4.1 How good are the base models?To measure the performance of the base models,we draw 10 samples djk for a subset of 100 pairs(r1,j , r2,j) in the Dev-All set.
We collect human flu-ency and accuracy judgments for each of the 1000total samples.
This allows us to conduct a post-hocsearch over values of ?
: for a range of ?, we com-pute the average accuracy and fluency of the high-est scoring sample.
By varying ?, we can view thetradeoff between accuracy and fluency that resultsfrom interpolating between the listener and speakermodel?setting ?
= 0 gives samples from pL0, and?
= 1 gives samples from pS0.Figure 5 shows the resulting accuracy and fluencyfor various values of ?.
It can be seen that relyingentirely on the listener gives the highest accuracybut degraded fluency.
However, by adding only avery small weight to the speaker model, it is possibleto achieve near-perfect fluency without a substantialdecrease in accuracy.
Example sentences for an in-dividual reference game are shown in Figure 5; in-creasing ?
causes captions to become more generic.For the remaining experiments in this paper, we take?
= 0.02, finding that this gives excellent perfor-mance on both metrics.On the development set, ?
= 0.02 results in anaverage fluency of 4.8 (compared to 4.8 for the lit-eral speaker ?
= 1).
This high fluency can be con-firmed by inspection of model samples (Figure 4).We thus focus on accuracy or the remainder of theevaluation.4.2 How many samples are needed?Next we turn to the computational efficiency of thereasoning model.
As in all sampling-based infer-ence, the number of samples that must be drawnfrom the proposal is of critical interest?if too manysamples are needed, the model will be too slow touse in practice.
Having fixed ?
= 0.02 in the pre-ceding section, we measure accuracy for versions ofthe reasoning model that draw 1, 10, 100, and 1000samples.
Results are shown in Table 1.
We find thatgains continue up to 100 samples.4.3 Is reasoning necessary?Because they do not require complicated inferenceprocedures, direct approaches to pragmatics typi-cally enjoy better computational efficiency than de-rived ones.
Having built an accurate derived speaker,can we bootstrap a more efficient direct speaker?To explore this, we constructed a ?compiled?speaker model as follows: Given reference candi-dates r1 and r2 and target t, this model producesembeddings e1 and e2, concatenates them togetherinto a ?contrast embedding?
[et, e?t], and then feedsthis whole embedding into a string decoder mod-ule.
Like S0, this model generates captions withoutthe need for discriminative rescoring; unlike S0, thecontrast embedding means this model can in prin-ciple learn to produce pragmatic captions, if givenaccess to pragmatic training data.
Since no suchtraining data exists, we train the compiled model on(a) target (b) distractor(prefer L0) 0.0 a hamburger on the ground0.1 mike is holding the burger(prefer S0) 0.2 the airplane is in the skyFigure 5: Captions for the same pair with varying ?.
Changing?
alters both the naturalness and specificity of the output.1179(a) the sun is in the sky (d) the plane is flying in the sky[contrastive] [contrastive](c) the dog is standing beside jenny (b) mike is wearing a chef?s hat[contrastive] [non-contrastive]Figure 4: Figure 4: Four randomly-chosen samples from our model.
For each, the target image is shown on the left, the distractorimage is shown on the right, and description generated by the model is shown below.
All descriptions are fluent, and generallysucceed in uniquely identifying the target scene, even when they do not perfectly describe it (e.g.
(c)).
These samples are broadlyrepresentative of the model?s performance (Table 2).Dev acc.
(%) Test acc.
(%)Model All Hard All HardLiteral (S0) 66 54 64 53Contrastive 71 54 69 58Reasoning (S1) 83 73 81 68Table 2: Success rates at RG on abstract scenes.
?Literal?
isa captioning baseline corresponding to the base speaker S0.?Contrastive?
is a reimplementation of the approach of Maoet al (2015).
?Reasoning?
is the model from this paper.
Alldifferences between our model and baselines are significant(p < 0.05, Binomial).captions sampled from the reasoning speaker itself.This model is evaluated in Table 3.
While thedistribution of scores is quite different from thatof the base model (it improves noticeably over S0on scenes with 2?3 differences), the overall gain isnegligible (the difference in mean scores is not sig-nificant).
The compiled model significantly under-performs the reasoning model.
These results sug-gest either that the reasoning procedure is not easilyapproximated by a shallow neural network, or thatexample descriptions of randomly-sampled trainingpairs (which are usually easy to discriminate) do notprovide a strong enough signal for a reflex learner torecover pragmatic behavior.# of differences1 2 3 4 MeanLiteral (S0) 50 66 70 78 66 (%)Reasoning 64 86 88 94 83Compiled (S1) 44 72 80 80 69Table 3: Comparison of the ?compiled?
pragmatic speakermodel with literal and explicitly reasoning speakers.
The mod-els are evaluated on subsets of the development set, arranged bydifficulty: column headings indicate the number of differencesbetween the target and distractor scenes.4.4 Final evaluationBased on the following sections, we keep ?
= 0.02and use 100 samples to generate predictions.
Weevaluate on the test set, comparing this Reason-ing model S1 to two baselines: Literal, an imagecaptioning model trained normally on the abstractscene captions (corresponding to our L0), and Con-trastive, a model trained with a soft contrastive ob-jective, and previously used for visual referring ex-pression generation (Mao et al, 2015).Results are shown in Table 2.
Our reasoningmodel outperforms both the literal baseline and pre-vious work by a substantial margin, achieving an im-provement of 17% on all pairs set and 15% on hard1180(a) (b) (c)(b vs. a) mike is holding a baseball bat(b vs. c) the snake is slithering away from mike and jennyFigure 6: Descriptions of the same image in different contexts.When the target scene (b) is contrasted with the left (a), thesystem describes a bat; when the target scene is contrasted withthe right (c), the system describes a snake.pairs.2 Figures 4 and 6 show various representativedescriptions from the model.5 ConclusionWe have presented an approach for learning to gen-erate pragmatic descriptions about general referents,even without training data collected in a pragmaticcontext.
Our approach is built from a pair of sim-ple neural base models, a listener and a speaker, anda high-level model that reasons about their outputsin order to produce pragmatic descriptions.
In anevaluation on a standard referring expression game,our model?s descriptions produced correct behaviorin human listeners significantly more often than ex-isting baselines.It is generally true of existing derived approachesto pragmatics that much of the system?s behavior re-quires hand-engineering, and generally true of di-rect approaches (and neural networks in particular)that training is only possible when supervision isavailable for the precise target task.
By synthesiz-ing these two approaches, we address both prob-lems, obtaining pragmatic behavior without domainknowledge and without targeted training data.
Webelieve that this general strategy of using reasoningto obtain novel contextual behavior from neural de-coding models might be more broadly applied.2 For comparison, a model with hand-engineered pragmaticbehavior?trained using a feature representation with indicatorson only those objects that appear in the target image but not thedistractor?produces an accuracy of 78% and 69% on all andhard development pairs respectively.
In addition to perform-ing slightly worse than our reasoning model, this alternativeapproach relies on the structure of scene representations andcannot be applied to more general pragmatics tasks.ReferencesAnne H. Anderson, Miles Bader, Ellen Gurman Bard,Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,Stephen Isard, Jacqueline Kowtko, Jan McAllister, JimMiller, et al 1991.
The HCRC map task corpus.
Lan-guage and speech, 34(4):351?366.Luciana Benotti and David Traum.
2009.
A computa-tional account of comparative implicatures for a spo-ken dialogue agent.
In Proceedings of the Eighth In-ternational Conference on Computational Semantics,pages 4?17.
Proceedings of the Annual Meeting of theAssociation for Computational Linguistics.Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2015.
Long-termrecurrent convolutional networks for visual recogni-tion and description.
In Proceedings of the Conferenceon Computer Vision and Pattern Recognition, pages2625?2634.Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer.2013.
Learning distributions over logical forms forreferring expression generation.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Michael C Frank, Noah D Goodman, Peter Lai, andJoshua B Tenenbaum.
2009.
Informative communi-cation in word production and word learning.
In Pro-ceedings of the 31st annual conference of the cognitivescience society, pages 1228?1233.Albert Gatt, Ielka Van Der Sluis, and Kees Van Deemter.2007.
Evaluating algorithms for the generation of re-ferring expressions using a balanced corpus.
In Pro-ceedings of the Eleventh European Workshop on Nat-ural Language Generation, pages 49?56.
Proceedingsof the Annual Meeting of the Association for Compu-tational Linguistics.Dave Golland, Percy Liang, and Dan Klein.
2010.
Agame-theoretic approach to generating spatial descrip-tions.
In Proceedings of the 2010 conference onEmpirical Methods in Natural Language Processing,pages 410?419.
Association for Computational Lin-guistics.Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.2014.
Explaining and harnessing adversarial exam-ples.
arXiv preprint arXiv:1412.6572.Herbert P Grice.
1970.
Logic and conversation.Daniel Jurafsky, Rebecca Bates, Noah Coccaro, RachelMartin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,Audreas Stolcke, Paul Taylor, Van Ess-Dykema, et al1997.
Automatic detection of discourse structure forspeech recognition and understanding.
In IEEE Work-shop on Automatic Speech Recognition and Under-standing, pages 88?95.
IEEE.1181Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, andTamara L Berg.
2014.
Referitgame: Referring to ob-jects in photographs of natural scenes.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 787?798.Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan Yuille, and Kevin Murphy.
2015.Generation and comprehension of unambiguous objectdescriptions.
arXiv preprint arXiv:1511.02283.Will Monroe and Christopher Potts.
2015.
Learning inthe Rational Speech Acts model.
In Proceedings of20th Amsterdam Colloquium, Amsterdam, December.ILLC.Luis Gilberto Mateos Ortiz, Clemens Wolff, and MirellaLapata.
2015.
Learning to interpret and describe ab-stract scenes.
In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 1505?1515.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics.Nathaniel J Smith, Noah Goodman, and Michael Frank.2013.
Learning and using language via recursive prag-matic reasoning about other agents.
In Advances inNeural Information Processing Systems, pages 3039?3047.Richard Socher, Andrej Karpathy, Quoc V Le, Christo-pher D Manning, and Andrew Y Ng.
2014.
Groundedcompositional semantics for finding and describingimages with sentences.
Transactions of the Associa-tion for Computational Linguistics, 2:207?218.Adam Vogel, Max Bodoia, Christopher Potts, and DanielJurafsky.
2013.
Emergence of Gricean maxims frommulti-agent decision theory.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 1072?1081.Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,Ruslan Salakhutdinov, Richard Zemel, and YoshuaBengio.
2015.
Show, attend and tell: neural im-age caption generation with visual attention.
arXivpreprint arXiv:1502.03044.C Zitnick and Devi Parikh.
2013.
Bringing semanticsinto focus using visual abstraction.
In Proceedingsof the Conference on Computer Vision and PatternRecognition, pages 3009?3016.C Lawrence Zitnick, Ramakrishna Vedantam, and DeviParikh.
2014.
Adopting abstract images for semanticscene understanding.1182
