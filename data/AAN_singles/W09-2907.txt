Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 47?54,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPImproving Statistical Machine Translation UsingDomain Bilingual Multiword ExpressionsZhixiang Ren1 Yajuan Lu?1 Jie Cao1 Qun Liu1 Yun Huang21Key Lab.
of Intelligent Info.
Processing 2Department of Computer ScienceInstitute of Computing Technology School of ComputingChinese Academy of Sciences National University of SingaporeP.O.
Box 2704, Beijing 100190, China Computing 1, Law Link, Singapore 117590{renzhixiang,lvyajuan huangyun@comp.nus.edu.sgcaojie,liuqun}@ict.ac.cnAbstractMultiword expressions (MWEs) havebeen proved useful for many natural lan-guage processing tasks.
However, how touse them to improve performance of statis-tical machine translation (SMT) is not wellstudied.
This paper presents a simple yeteffective strategy to extract domain bilin-gual multiword expressions.
In addition,we implement three methods to integratebilingual MWEs to Moses, the state-of-the-art phrase-based machine translationsystem.
Experiments show that bilingualMWEs could improve translation perfor-mance significantly.1 IntroductionPhrase-based machine translation model has beenproved a great improvement over the initial word-based approaches (Brown et al, 1993).
Recentsyntax-based models perform even better thanphrase-based models.
However, when syntax-based models are applied to new domain with fewsyntax-annotated corpus, the translation perfor-mance would decrease.
To utilize the robustnessof phrases and make up the lack of syntax or se-mantic information in phrase-based model for do-main translation, we study domain bilingual mul-tiword expressions and integrate them to the exist-ing phrase-based model.A multiword expression (MWE) can be consid-ered as word sequence with relatively fixed struc-ture representing special meanings.
There is nouniform definition of MWE, and many researchersgive different properties of MWE.
Sag et al (2002)roughly defined MWE as ?idiosyncratic interpre-tations that cross word boundaries (or spaces)?.Cruys and Moiro?n (2007) focused on the non-compositional property of MWE, i.e.
the propertythat whole expression cannot be derived from theircomponent words.
Stanford university launcheda MWE project1, in which different qualities ofMWE were presented.
For bilingual multiwordexpression (BiMWE), we define a bilingual phraseas a bilingual MWE if (1) the source phrase is aMWE in source language; (2) the source phraseand the target phrase must be translated to eachother exactly, i.e.
there is no additional (boundary)word in target phrase which cannot find the corre-sponding word in source phrase, and vice versa.In recent years, many useful methods have beenproposed to extract MWEs or BiMWEs automati-cally (Piao et al, 2005; Bannard, 2007; Fazly andStevenson, 2006).
Since MWE usually constrainspossible senses of a polysemous word in context,they can be used in many NLP applications suchas information retrieval, question answering, wordsense disambiguation and so on.For machine translation, Piao et al (2005) havenoted that the issue of MWE identification andaccurate interpretation from source to target lan-guage remained an unsolved problem for existingMT systems.
This problem is more severe whenMT systems are used to translate domain-specifictexts, since they may include technical terminol-ogy as well as more general fixed expressions andidioms.
Although some MT systems may employa machine-readable bilingual dictionary of MWE,it is time-consuming and inefficient to obtain thisresource manually.
Therefore, some researchershave tried to use automatically extracted bilingualMWEs in SMT.
Tanaka and Baldwin (2003) de-scribed an approach of noun-noun compound ma-chine translation, but no significant comparisonwas presented.
Lambert and Banchs (2005) pre-sented a method in which bilingual MWEs wereused to modify the word alignment so as to im-prove the SMT quality.
In their work, a bilin-gual MWE in training corpus was grouped as1http://mwe.stanford.edu/47one unique token before training alignment mod-els.
They reported that both alignment qualityand translation accuracy were improved on a smallcorpus.
However, in their further study, they re-ported even lower BLEU scores after groupingMWEs according to part-of-speech on a large cor-pus (Lambert and Banchs, 2006).
Nonetheless,since MWE represents liguistic knowledge, therole and usefulness of MWE in full-scale SMTis intuitively positive.
The difficulty lies in howto integrate bilingual MWEs into existing SMTsystem to improve SMT performance, especiallywhen translating domain texts.In this paper, we implement three methods thatintegrate domain bilingual MWEs into a phrase-based SMT system, and show that these ap-proaches improve translation quality significantly.The main difference between our methods andLambert and Banchs?
work is that we directly aimat improving the SMT performance rather than im-proving the word alignment quality.
In detail, dif-ferences are listed as follows:?
Instead of using the bilingual n-gram trans-lation model, we choose the phrase-basedSMT system, Moses2, which achieves sig-nificantly better translation performance thanmany other SMT systems and is a state-of-the-art SMT system.?
Instead of improving translation indirectlyby improving the word alignment quality,we directly target at the quality of transla-tion.
Some researchers have argued that largegains of alignment performance under manymetrics only led to small gains in translationperformance (Ayan and Dorr, 2006; Fraserand Marcu, 2007).Besides the above differences, there are someadvantages of our approaches:?
In our method, automatically extractedMWEs are used as additional resources ratherthan as phrase-table filter.
Since bilingualMWEs are extracted according to noisy au-tomatic word alignment, errors in word align-ment would further propagate to the SMT andhurt SMT performance.?
We conduct experiments on domain-specificcorpus.
For one thing, domain-specific2http://www.statmt.org/moses/corpus potentially includes a large numberof technical terminologies as well as moregeneral fixed expressions and idioms, i.e.domain-specific corpus has high MWE cov-erage.
For another, after the investigation,current SMT system could not effectivelydeal with these domain-specific MWEs es-pecially for Chinese, since these MWEs aremore flexible and concise.
Take the Chi-nese term ?^ j ?
(?
for example.
Themeaning of this term is ?soften hard massand dispel pathogenic accumulation?.
Ev-ery word of this term represents a specialmeaning and cannot be understood literallyor without this context.
These terms are dif-ficult to be translated even for humans, letalone machine translation.
So, treating theseterms as MWEs and applying them in SMTsystem have practical significance.?
In our approach, no additional corpus is intro-duced.
We attempt to extract useful MWEsfrom the training corpus and adopt suitablemethods to apply them.
Thus, it benefitsfor the full exploitation of available resourceswithout increasing great time and space com-plexities of SMT system.The remainder of the paper is organized as fol-lows.
Section 2 describes the bilingual MWE ex-traction technique.
Section 3 proposes three meth-ods to apply bilingual MWEs in SMT system.Section 4 presents the experimental results.
Sec-tion 5 draws conclusions and describes the futurework.
Since this paper mainly focuses on the ap-plication of BiMWE in SMT, we only give a briefintroduction on monolingual and bilingual MWEextraction.2 Bilingual Multiword ExpressionExtractionIn this section we describe our approach of bilin-gual MWE extraction.
In the first step, we obtainmonolingual MWEs from the Chinese part of par-allel corpus.
After that, we look for the translationof the extracted MWEs from parallel corpus.2.1 Automatic Extraction of MWEsIn the past two decades, many different ap-proaches on automatic MWE identification werereported.
In general, those approaches can beclassified into three main trends: (1) statisti-cal approaches (Pantel and Lin, 2001; Piao et48al., 2005), (2) syntactic approaches (Fazly andStevenson, 2006; Bannard, 2007), and (3) seman-tic approaches (Baldwin et al, 2003; Cruys andMoiro?n, 2007).
Syntax-based and semantic-basedmethods achieve high precision, but syntax or se-mantic analysis has to be introduced as preparingstep, so it is difficult to apply them to domains withfew syntactical or semantic annotation.
Statisticalapproaches only consider frequency information,so they can be used to obtain MWEs from bilin-gual corpora without deeper syntactic or semanticanalysis.
Most statistical measures only take twowords into account, so it not easy to extract MWEscontaining three or more than three words.Log Likelihood Ratio (LLR) has been proved agood statistical measurement of the association oftwo random variables (Chang et al, 2002).
Weadopt the idea of statistical approaches, and pro-pose a new algorithm named LLR-based Hierar-chical Reducing Algorithm (HRA for short) to ex-tract MWEs with arbitrary lengths.
To illustrateour algorithm, firstly we define some useful items.In the following definitions, we assume the givensentence is ?A B C D E?.Definition 1 Unit: A unit is any sub-string of thegiven sentence.
For example, ?A B?, ?C?, ?C D E?are all units, but ?A B D?
is not a unit.Definition 2 List: A list is an ordered sequence ofunits which exactly cover the given sentence.
Forexample, {?A?,?B C D?,?E?}
forms a list.Definition 3 Score: The score function only de-fines on two adjacent units and return the LLRbetween the last word of first unit and the firstword of the second unit3.
For example, the scoreof adjacent unit ?B C?
and ?D E?
is defined asLLR(?C?,?D?
).Definition 4 Select: The selecting operator is tofind the two adjacent units with maximum scorein a list.Definition 5 Reduce: The reducing operator is toremove two specific adjacent units, concatenatethem, and put back the result unit to the removedposition.
For example, if we want to reduce unit?B C?
and unit ?D?
in list {?A?,?B C?,?D?,?E?
},we will get the list {?A?,?B C D?,?E?
}.Initially, every word in the sentence is consid-ered as one unit and all these units form a initiallist L. If the sentence is of length N , then the3we use a stoplist to eliminate the units containing func-tion words by setting their score to 0list contains N units, of course.
The final set ofMWEs, S, is initialized to empty set.
After initial-ization, the algorithm will enter an iterating loopwith two steps: (1) select the two adjacent unitswith maximum score in L, naming U1 and U2; and(2) reduce U1 and U2 in L, and insert the reducingresult into the final set S. Our algorithm termi-nates on two conditions: (1) if the maximum scoreafter selection is less than a given threshold; or (2)if L contains only one unit.c1(??
p ??
w  ?
)c1(??
p ??
w )c1(??
p ??
w)c1(??
)c2(p ??
w)c2(p ??
)c2(p) c3(??)
c4(w) c5()c6(?
)c6(?)
c7()??
p ??
w  ?
147.1 6755.2 1059.6 0 0 809.6Figure 1: Example of Hierarchical Reducing Al-gorithmLet us make the algorithm clearer with an ex-ample.
Assume the threshold of score is 20, thegiven sentence is ???
p ??
w  ?
?4.Figure 1 shows the hierarchical structure of givensentence (based on LLR of adjacent words).
Inthis example, four MWEs (?p ??
?, ?p ?
?w?, ??
?, ???
p ??
w?)
are extractedin the order, and sub-strings over dotted line in fig-ure 1 are not extracted.From the above example, we can see that theextracted MWEs correspond to human intuition.In general, the basic idea of HRA is to reflectthe hierarchical structure pattern of natural lan-guage.
Furthermore, in the HRA, MWEs are mea-sured with the minimum LLR of adjacent wordsin them, which gives lexical confidence of ex-tracted MWEs.
Finally, suppose given sentencehas length N , HRA would definitely terminatewithin N ?
1 iterations, which is very efficient.However, HRA has a problem that it would ex-tract substrings before extracting the whole string,even if the substrings only appear in the particu-lar whole string, which we consider useless.
Tosolve this problem, we use contextual features,4The whole sentence means ?healthy tea for preventinghyperlipidemia?, and we give the meaning for each Chi-nese word: ??
(preventing), p(hyper-), ??
(-lipid-), w(-emia), (for),?
(healthy),(tea).49contextual entropy (Luo and Sun, 2003) and C-value (Frantzi and Ananiadou, 1996), to filter outthose substrings which exist only in few MWEs.2.2 Automatic Extraction of MWE?sTranslationIn subsection 2.1, we described the algorithm toobtain MWEs, and we would like to introduce theprocedure to find their translations from parallelcorpus in this subsection.For mining the English translations of ChineseMWEs, we first obtain the candidate translationsof a given MWE from the parallel corpus.
Stepsare listed as follows:1.
Run GIZA++5 to align words in the trainingparallel corpus.2.
For a given MWE, find the bilingual sentencepairs where the source language sentences in-clude the MWE.3.
Extract the candidate translations of theMWE from the above sentence pairs accord-ing to the algorithm described by Och (2002).After the above procedure, we have alreadyextracted all possible candidate translations of agiven MWE.
The next step is to distinguish rightcandidates from wrong candidates.
We constructperceptron-based classification model (Collins,2002) to solve the problem.
We design twogroups of features: translation features, whichdescribe the mutual translating chance betweensource phrase and target phrase, and the languagefeatures, which refer to how well a candidateis a reasonable translation.
The translation fea-tures include: (1) the logarithm of source-targettranslation probability; (2) the logarithm of target-source translation probability; (3) the logarithmof source-target lexical weighting; (4) the loga-rithm of target-source lexical weighting; and (5)the logarithm of the phrase pair?s LLR (Dunning,1993).
The first four features are exactly the sameas the four translation probabilities used in tradi-tional phrase-based system (Koehn et al, 2003).The language features include: (1) the left entropyof the target phrase (Luo and Sun, 2003); (2) theright entropy of the target phrase; (3) the first wordof the target phrase; (4) the last word of the targetphrase; and (5) all words in the target phrase.5http://www.fjoch.com/GIZA++.htmlWe select and annotate 33000 phrase pairs ran-domly, of which 30000 pairs are used as trainingset and 3000 pairs are used as test set.
We use theperceptron training algorithm to train the model.As the experiments reveal, the classification preci-sion of this model is 91.67%.3 Application of Bilingual MWEsIntuitively, bilingual MWE is useful to improvethe performance of SMT.
However, as we de-scribed in section 1, it still needs further researchon how to integrate bilingual MWEs into SMTsystem.
In this section, we propose three methodsto utilize bilingual MWEs, and we will comparetheir performance in section 4.3.1 Model Retraining with Bilingual MWEsBilingual phrase table is very important forphrase-based MT system.
However, due to the er-rors in automatic word alignment and unalignedword extension in phrase extraction (Och, 2002),many meaningless phrases would be extracted,which results in inaccuracy of phrase probabilityestimation.
To alleviate this problem, we take theautomatically extracted bilingual MWEs as paral-lel sentence pairs, add them into the training cor-pus, and retrain the model using GIZA++.
Byincreasing the occurrences of bilingual MWEs,which are good phrases, we expect that the align-ment would be modified and the probability es-timation would be more reasonable.
Wu et al(2008) also used this method to perform domainadaption for SMT.
Different from their approach,in which bilingual MWEs are extracted from ad-ditional corpus, we extract bilingual MWEs fromthe original training set.
The fact that additionalresources can improve the domain-specific SMTperformance was proved by many researchers(Wu et al, 2008; Eck et al, 2004).
However,our method shows that making better use of theresources in hand could also enhance the qualityof SMT system.
We use ?Baseline+BiMWE?
torepresent this method.3.2 New Feature for Bilingual MWEsLopez and Resnik (2006) once pointed out thatbetter feature mining can lead to substantial gainin translation quality.
Inspired by this idea, weappend one feature into bilingual phrase table toindicate that whether a bilingual phrase containsbilingual MWEs.
In other words, if the source lan-guage phrase contains a MWE (as substring) and50the target language phrase contains the translationof the MWE (as substring), the feature value is 1,otherwise the feature value is set to 0.
Due to thehigh reliability of bilingual MWEs, we expect thatthis feature could help SMT system to select bet-ter and reasonable phrase pairs during translation.We use ?Baseline+Feat?
to represent this method.3.3 Additional Phrase Table of bilingualMWEsWu et al (2008) proposed a method to construct aphrase table by a manually-made translation dic-tionary.
Instead of manually constructing transla-tion dictionary, we construct an additional phrasetable containing automatically extracted bilingualMWEs.
As to probability assignment, we just as-sign 1 to the four translation probabilities for sim-plicity.
Since Moses supports multiple bilingualphrase tables, we combine the original phrase ta-ble and new constructed bilingual MWE table.
Foreach phrase in input sentence during translation,the decoder would search all candidate transla-tion phrases in both phrase tables.
We use ?Base-line+NewBP?
to represent this method.4 Experiments4.1 DataWe run experiments on two domain-specific patentcorpora: one is for traditional medicine domain,and the other is for chemical industry domain.
Ourtranslation tasks are Chinese-to-English.In the traditional medicine domain, table 1shows the data statistics.
For language model, weuse SRI Language Modeling Toolkit6 to train a tri-gram model with modified Kneser-Ney smoothing(Chen and Goodman, 1998) on the target side oftraining corpus.
Using our bilingual MWE ex-tracting algorithm, 80287 bilingual MWEs are ex-tracted from the training set.Chinese EnglishTraining Sentences 120,355Words 4,688,873 4,737,843Dev Sentences 1,000Words 31,722 32,390Test Sentences 1,000Words 41,643 40,551Table 1: Traditional medicine corpus6http://www.speech.sri.com/projects/srilm/In the chemical industry domain, table 2 givesthe detail information of the data.
In this experi-ment, 59466 bilingual MWEs are extracted.Chinese EnglishTraining Sentences 120,856Words 4,532,503 4,311,682Dev Sentences 1,099Words 42,122 40,521Test Sentences 1,099Words 41,069 39,210Table 2: Chemical industry corpusWe test translation quality on test set and use theopen source tool mteval-vllb.pl7 to calculate case-sensitive BLEU 4 score (Papineni et al, 2002) asour evaluation criteria.
For this evaluation, there isonly one reference per test sentence.
We also per-form statistical significant test between two trans-lation results (Collins et al, 2005).
The mean ofall scores and relative standard deviation are calcu-lated with a 99% confidence interval of the mean.4.2 MT SystemsWe use the state-of-the-art phrase-based SMT sys-tem, Moses, as our baseline system.
The featuresused in baseline system include: (1) four transla-tion probability features; (2) one language modelfeature; (3) distance-based and lexicalized distor-tion model feature; (4) word penalty; (5) phrasepenalty.
For ?Baseline+BiMWE?
method, bilin-gual MWEs are added into training corpus, as aresult, new alignment and new phrase table areobtained.
For ?Baseline+Feat?
method, one ad-ditional 0/1 feature are introduced to each entry inphrase table.
For ?Baseline+NewBP?, additionalphrase table constructed by bilingual MWEs isused.Features are combined in the log-linear model.To obtain the best translation e?
of the source sen-tence f , log-linear model uses following equation:e?
= arg maxep(e|f)= arg maxeM?m=1?mhm(e, f) (1)in which hm and ?m denote the mth feature andweight.
The weights are automatically turned byminimum error rate training (Och, 2002) on devel-opment set.7http://www.nist.gov/speech/tests/mt/resources/scoring.htm514.3 ResultsMethods BLEUBaseline 0.2658Baseline+BiMWE 0.2661Baseline+Feat 0.2675Baseline+NewBP 0.2719Table 3: Translation results of using bilingualMWEs in traditional medicine domainTable 3 gives our experiment results.
Fromthis table, we can see that, bilingual MWEsimprove translation quality in all cases.
TheBaseline+NewBP method achieves the most im-provement of 0.61% BLEU score comparedwith the baseline system.
The Baseline+Featmethod comes next with 0.17% BLEU score im-provement.
And the Baseline+BiMWE achievesslightly higher translation quality than the baselinesystem.To our disappointment, however, none of theseimprovements are statistical significant.
Wemanually examine the extracted bilingual MWEswhich are labeled positive by perceptron algorithmand find that although the classification precisionis high (91.67%), the proportion of positive exam-ple is relatively lower (76.69%).
The low positiveproportion means that many negative instanceshave been wrongly classified to positive, which in-troduce noises.
To remove noisy bilingual MWEs,we use the length ratio x of the source phrase overthe target phrase to rank the bilingual MWEs la-beled positive.
Assume x follows Gaussian distri-butions, then the ranking score of phrase pair (s, t)is defined as the following formula:Score(s, t) = log(LLR(s, t))?
1?2pi??e?(x??
)22?2(2)Here the mean ?
and variance ?2 are estimatedfrom the training set.
After ranking by score, weselect the top 50000, 60000 and 70000 bilingualMWEs to perform the three methods mentioned insection 3.
The results are showed in table 4.From this table, we can conclude that: (1) Allthe three methods on all settings improve BLEUscore; (2) Except the Baseline+BiMWE method,the other two methods obtain significant improve-ment of BLEU score (0.2728, 0.2734, 0.2724)over baseline system (0.2658); (3) When the scaleof bilingual MWEs is relatively small (50000,60000), the Baseline+Feat method performs betterMethods 50000 60000 70000Baseline 0.2658Baseline+BiMWE 0.2671 0.2686 0.2715Baseline+Feat 0.2728 0.2734 0.2712Baseline+NewBP 0.2662 0.2706 0.2724Table 4: Translation results of using bilingualMWEs in traditional medicine domainthan others; (4) As the number of bilingual MWEsincreasing, the Baseline+NewBP method outper-forms the Baseline+Feat method; (5) Comparingtable 4 and 3, we can see it is not true that themore bilingual MWEs, the better performance ofphrase-based SMT.
This conclusion is the same as(Lambert and Banchs, 2005).To verify the assumption that bilingual MWEsdo indeed improve the SMT performance not onlyon particular domain, we also perform some ex-periments on chemical industry domain.
Table 5shows the results.
From this table, we can see thatthese three methods can improve the translationperformance on chemical industry domain as wellas on the traditional medicine domain.Methods BLEUBaseline 0.1882Baseline+BiMWE 0.1928Baseline+Feat 0.1917Baseline+Newbp 0.1914Table 5: Translation results of using bilingualMWEs in chemical industry domain4.4 DiscussionIn order to know in what respects our methods im-prove performance of translation, we manually an-alyze some test sentences and gives some exam-ples in this subsection.
(1) For the first example in table 6, ??
?
?is aligned to other words and not correctly trans-lated in baseline system, while it is aligned to cor-rect target phrase ?dredging meridians?
in Base-line+BiMWE, since the bilingual MWE (??
?
?,?dredging meridians?)
has been added into train-ing corpus and then aligned by GIZA++.
(2) For the second example in table 6, ???
has two candidate translation in phrase table:?tea?
and ?medicated tea?.
The baseline systemchooses the ?tea?
as the translation of ??
?,while the Baseline+Feat system chooses the ?med-52Src T ??
?
k ??
!
?
?
!
?
?
! )
9 !
| Y !
S  _?
? , ?
?
 ?
E 8 "Ref the obtained product is effective in tonifying blood , expelling cold , dredging meridians, promoting production of body fluid , promoting urination , and tranquilizing mind ;and can be used for supplementing nutrition and protecting health .Baseline the food has effects in tonifying blood , dispelling cold , promoting salivation and water, and tranquilizing , and tonic effects , and making nutritious health .+Bimwe the food has effects in tonifying blood , dispelling cold , dredging meridians , promotingsalivation , promoting urination , and tranquilizing tonic , nutritious pulverizing .Src ?
?
??
?J !
J !
?J !?!
5J "Ref the product can also be made into tablet , pill , powder , medicated tea , or injection .Baseline may also be made into tablet , pill , powder , tea , or injection .+Feat may also be made into tablet , pill , powder , medicated tea , or injection .Table 6: Translation exampleicated tea?
because the additional feature giveshigh probability of the correct translation ?medi-cated tea?.5 Conclusion and Future WorksThis paper presents the LLR-based hierarchicalreducing algorithm to automatically extract bilin-gual MWEs and investigates the performance ofthree different application strategies in applyingbilingual MWEs for SMT system.
The translationresults show that using an additional feature to rep-resent whether a bilingual phrase contains bilin-gual MWEs performs the best in most cases.
Theother two strategies can also improve the qualityof SMT system, although not as much as the firstone.
These results are encouraging and motivatedto do further research in this area.The strategies of bilingual MWE application isroughly simply and coarse in this paper.
Com-plicated approaches should be taken into accountduring applying bilingual MWEs.
For example,we may consider other features of the bilingualMWEs and examine their effect on the SMT per-formance.
Besides application in phrase-basedSMT system, bilingual MWEs may also be inte-grated into other MT models such as hierarchicalphrase-based models or syntax-based translationmodels.
We will do further studies on improvingstatistical machine translation using domain bilin-gual MWEs.AcknowledgmentsThis work is supported by National Natural Sci-ence Foundation of China, Contracts 60603095and 60873167.
We would like to thank the anony-mous reviewers for their insightful comments onan earlier draft of this paper.ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
Goingbeyond aer: an extensive analysis of word align-ments and their impact on mt.
In Proceedings of the44th Annual Meeting of the Association for Compu-tational Linguistics, pages 9?16.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL-2003 Workshop on MultiwordExpressions: Analysis, Acquisiton and Treatment,pages 89?96.Colin Bannard.
2007.
A measure of syntactic flex-ibility for automatically identifying multiword ex-pressions in corpora.
In Proceedings of the ACLWorkshop on A Broader Perspective on MultiwordExpressions, pages 1?8.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Baobao Chang, Pernilla Danielsson, and WolfgangTeubert.
2002.
Extraction of translation unit fromchinese-english parallel corpora.
In Proceedings ofthe first SIGHAN workshop on Chinese languageprocessing, pages 1?5.Stanley F. Chen and Joshua Goodman.
1998.
Am em-pirical study of smoothing techniques for languagemodeling.
Technical report.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual53Meeting on Association for Computational Linguis-tics, pages 531?540.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Empirical Methods in Natural Language Pro-cessing Conference, pages 1?8.Tim Van de Cruys and Begon?a Villada Moiro?n.
2007.Semantics-based multiword expression extraction.In Proceedings of the Workshop on A Broader Per-spective on Multiword Expressions, pages 25?32.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Matthias Eck, Stephan Vogel, and Alex Waibel.
2004.Improving statistical machine translation in the med-ical domain using the unified medical language sys-tem.
In Proceedings of the 20th international con-ference on Computational Linguistics table of con-tents, pages 792?798.Afsaneh Fazly and Suzanne Stevenson.
2006.
Auto-matically constructing a lexicon of verb phrase id-iomatic combinations.
In Proceedings of the EACL,pages 337?344.Katerina T. Frantzi and Sophia Ananiadou.
1996.
Ex-tracting nested collocations.
In Proceedings of the16th conference on Computational linguistics, pages41?46.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine trans-lation.
Computational Linguistics, 33(3):293?303.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology, pages48?54.Patrik Lambert and Rafael Banchs.
2005.
Data in-ferred multi-word expressions for statistical machinetranslation.
In Proceedings of Machine TranslationSummit X, pages 396?403.Patrik Lambert and Rafael Banchs.
2006.
Groupingmulti-word expressions according to part-of-speechin statistical machine translation.
In Proceedings ofthe Workshop on Multi-word-expressions in a multi-lingual context, pages 9?16.Adam Lopez and Philip Resnik.
2006.
Word-basedalignment, phrase-based translation: What?s thelink?
In proceedings of the 7th conference of the as-sociation for machine translation in the Americas:visions for the future of machine translation, pages90?99.Shengfen Luo and Maosong Sun.
2003.
Two-characterchinese word extraction based on hybrid of internaland contextual measures.
In Proceedings of the sec-ond SIGHAN workshop on Chinese language pro-cessing, pages 24?30.Franz Josef Och.
2002.
Statistical Machine Transla-tion: From Single-Word Models to Alignment Tem-plates.
Ph.d. thesis, Computer Science Department,RWTH Aachen, Germany.Patrick Pantel and Dekang Lin.
2001.
A statistical cor-pus based term extractor.
In AI ?01: Proceedings ofthe 14th Biennial Conference of the Canadian Soci-ety on Computational Studies of Intelligence, pages36?46.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Conference of the Association for Com-putational Linguistics, pages 311?318.Scott Songlin Piao, Paul Rayson, Dawn Archer, andTony McEnery.
2005.
Comparing and combining asemantic tagger and a statistical tool for mwe extrac-tion.
Computer Speech and Language, 19(4):378?397.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multi-word expressions: A pain in the neck for nlp.
InProceedings of the 3th International Conferenceon Intelligent Text Processing and ComputationalLinguistics(CICLing-2002), pages 1?15.Takaaki Tanaka and Timothy Baldwin.
2003.
Noun-noun compound machine translation: A feasibilitystudy on shallow processing.
In Proceedings ofthe ACL-2003 Workshop on Multiword Expressions:Analysis, Acquisition and Treatment, pages 17?24.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine transla-tion with domain dictionary and monolingual cor-pora.
In Proceedings of Conference on Computa-tional Linguistics (COLING), pages 993?1000.54
