Automatic Arabic Document Categorization Based on the Na?ve Bayes AlgorithmMohamed EL KOURDI Amine BENSAID?
Tajje-eddine RACHIDISchool of Science & EngineeringAlakhawayn UniversityP.O.
Box 104, Ifrane 53000, Morocco[M.Elkourdi, A.Bensaid, T.Rachidi]@alakhawayn.ma?Corresponding AuthorAbstractThis paper deals with automatic classification ofArabic web documents.
Such a classification is veryuseful for affording directory search functionality,which has been used by many web portals andsearch engines to cope with an ever-increasingnumber of documents on the web.
In this paper,Naive Bayes (NB) which is a statistical machinelearning algorithm, is used to classify non-vocalizedArabic web documents (after their words have beentransformed to the corresponding canonical form,i.e., roots) to one of five pre-defined categories.Cross validation experiments are used to evaluatethe NB categorizer.
The data set used during theseexperiments consists of 300 web documents percategory.
The results of cross validation in theleave-one-out experiment show that, using 2,000terms/roots, the categorization accuracy varies fromone category to another with an average accuracyover all categories of 68.78 %.
Furthermore, thebest categorization performance by category duringcross validation experiments goes up to 92.8%.Further tests carried out on a manually collectedevaluation set which consists of 10 documents fromeach of the 5 categories, show that the overallclassification accuracy achieved over all categoriesis 62%,  and that the best result by category  reaches90%.Keywords: Na?ve Bayes, Arabic documentcategorization, cross validation, TF-IDF.1 IntroductionWith the explosive growth of text documents onthe web, relevant information retrieval has becomea crucial task to satisfy the needs of different endusers.
To this end, automatic text categorization hasemerged as a way to cope with such a problem.Automatic text (or document) categorizationattempts to replace and save human effort requiredin performing manual categorization.
It consists ofassigning and labeling documents using a set of pre-defined categories based on document contents.
Assuch, one of the primary objectives of automatictext categorization has been the enhancement andthe support of information retrieval tasks to tackleproblems, such as information filtering and routing,clustering of related documents, and theclassification of documents into pre-specifiedsubject themes.
Automatic text categorization hasbeen used in search engines, digital library systems,and document management systems (Yang, 1999).Such applications have included electronic emailfiltering, newsgroups classification, and survey datagrouping.
Barq for instance uses automaticcategorization to provide similar documents feature(Rachidi et al, 2003).
In this paper, NB which is astatistical machine learning algorithm is used tolearn to classify non-vocalized1 Arabic web textdocuments.This paper is organized as follows.
Section 2,briefly describe related works in the area ofautomatic text categorization.
Section 3 describesthe preprocessing undergone by documents for thepurpose of categorization; it describes in particularthe preprocessing specific to the Arabic language.In section 4 Na?ve Bayes (NB), the learningalgorithm used in this paper for documentcategorization is presented.
Section 5 outlines theexperimental setting, as well as the experimentscarried out to evaluate the performance of the NBclassifier.
It also gives the numerical results withtheir analysis and interpretation.
Section 6summarizes the work and suggests some ideas forfuture works.2 Related WorksMany machine learning algorithms have beenapplied for many years to text categorization.
They1 Most modern Arabic writing (web, novels, articles) arewritten without vowels.include decision tree learning and Bayesianlearning, nearest neighbor learning, and artificialneural networks, early such works may be found in(Lewis and Ringnette, 1994), (Creecy and Masand,1992) and (Wiene and Pedersen, 1995),respectively.The bulk of the text categorization work has beendevoted to cope with automatic categorization ofEnglish and Latin character documents.
Forexample, (Fang et al, 2001) discusses theevaluation of two different text categorizationstrategies with several variations of their featurespaces.
A good study comparing documentcategorization algorithms can be found in  (Yangand Liu, 1999).
More recently, (Sebastiani, 2002)has performed a good survey of documentcategorization; recent works can also be found in(Joachims, 2002), (Crammer and Singer, 2003), and(Lewis et al, 2004).Concerning Arabic, one automatic categorizer hasbeen reported to have been put under operationaluse to classify Arabic documents; it is referred to as"Sakhr's categorizer" (Sakhr, 2004).
Unfortunately,there is no technical documentation or specificationconcerning this Arabic categorizer.
Sakhr'smarketing literature claims that this categorizer isbased on Arabic morphology and some research thathas been carried out on natural language processing.The present work evaluates the performance onArabic documents of the Na?ve Bayes algorithm(NB) - one of the simplest algorithms applied toEnglish document categorization (Mitchell, 1997).The aim of this work is to gain some insight as towhether Arabic document categorization (using NB)is sensitive to the root extraction algorithm used orto different data sets.
This work is a continuation ofthat initiated in (Yahyaoui, 2001), which reports anoverall NB classification correctness of 75.6%, incross validation experiments, on a data set thatconsists of 100 documents for each of 12 categories(the data set is collected from different Arabicportals).
A 50% overall classification accuracy isalso reported when testing with a separatelycollected evaluation set (3 documents for each ofthe 12 categories).
The present work expands thework in (Yahyaoui, 2001) by experimenting withthe use of a better root extraction algorithm (ElKourdi, 2004) for document preprocessing, andusing a different data set, collected from the largestArabic site on the web: aljazeera.net.3 Preprocessing of documentPrior to applying document categorizationtechniques to an Arabic document, the latter istypically preprocessed: it is parsed, in order toremove stopwords (these are conjunction anddisjunction words etc.).
In addition, at this stage inthis work, vowels are stripped from the full textrepresentation when the document is (fully orpartially) voweled/vocalized.
Then roots areextracted for words in the document.In Arabic, however, the use of stems will notyield satisfactory categorization.
This is mainly dueto the fact that Arabic is a non-concatenativelanguage (Al-Shalabi and Evens, 1998), and that thestem/infix obtained by suppression of infix andprefix add-ons is not the same for words derivedfrom the same origin called the root.
The infix form(or stem) needs further to be processed in order toobtain the root.
This processing is notstraightforward: it necessitates expert knowledge inArabic language word morphology (Al-Shalabi andEvens, 1998).
As an example, two close roots (i.e.,roots made of the same letters), but semanticallydifferent, can yield the same infix form thuscreating ambiguity.The root extraction process is concerned with thetransformation of all Arabic word derivatives totheir single common root or canonical form.
Thisprocess is very useful in terms of reducing andcompressing the indexing structure, and in takingadvantage of the semantic/conceptual relationshipsbetween the different forms of the same root.
In thiswork, we use the Arabic root extraction technique in(El Kourdi, 2004).
It compares favorably to otherstemming or root extraction algorithms (Yates andNeto, 1999; Al-Shalabi and Evens, 1998; andHoumame, 1999), with a performance of over 97%for extracting the correct root in web documents,and it addresses the challenge of the Arabic brokenplural and hollow verbs.
In the remainder of thispaper, we will use the term "root" and "term"interchangeably to refer to canonical forms obtainedthrough this root extraction process.4 NB for document categorization4.1 The classifier moduleThe classifier module is considered to be the corecomponent of the document categorizer.
It isresponsible for classifying given Arabic documentsto their target class.
This is performed using theNaive Bayes (NB) algorithm.
The NB classifiercomputes a posteriori probabilities of classes, usingestimates obtained from a training set of labeleddocuments.
When an unlabeled document ispresented, the a posteriori probability is computedfor each class using (1) in Figure 1; and theunlabeled document is then assigned to the classwith the largest a posteriori probability.A posteriori probability computationLet D be a document represented as a set of finiteterms D={w1, w2,..., w3}.Let C be the number of target classes.Let docsi be the number of documents in categoryC,i and |Examples| be the number of documents inthe training set of labeled documents.Let  n be the total number of distinct stems in CiLet  Nk be the number of times wk occurs in CiThen the a posteriori probability as given byBayes theorem is:P(Ci|D)=[P(Ci)*P(D| Ci)]/P(D).
i=1,2,...C( 1)When comparing a posteriori probabilities for thesame document D, P(D) is the same for allcategories and will not affect the comparison.The other quantities in (1) are estimated from thetraining set using  NB  learning (see  Figure 2).The assigned class AC(D) to document D is theclass with largest a posteriori probability given by(1):AC(D)=argmaxCi { P(Ci|D).
i=1,2,...C}Figure 1.
A posteriori probability reduction.4.2 The learning moduleThe main task of the learning module is to learnfrom a set of labeled documents with predefinedcategories in order to allow the categorizer toclassify the newly encountered documents D and toassign them to each of the predefined targetcategories Ci.
This module is based on the NBlearning algorithm given in Figure 2.
The learningmodule is one way of estimating the neededquantities in (1) by learning from a training set ofdocuments.NB learning algorithmLet D be a document represented as a set of  finiteterms/roots D={w1, w2,..., wn}.Let docsi be the number of documents in categoryCi , and |Examples| be the number of documents inthe training set of labeled documents.Step 1: collect the vocabulary, which is defined asthe set of distinct words in the whole training setStep2: For each category Ci  do the followingCompute  P(Cj) = | docsj |/|Examples|(2)where docsj is the number of training documentsfor the category is Cj.For each root wk in VocabularyCompute  P(wk/Cj)= (Nk,j +1)/( nj +| Textj |)( 3)where Nk,j is the number of times wk occurs in Cj,nj is the total number of distinct terms in all trainingdocuments labeled Cj, and Textj is a singledocuments generated by concatenating all thetraining documents for category Cj .Equation (2) and (3) make use of the followingtwo assumptions:1) Assuming that the order of the words in adocument does not affect the classification of thedocument:P(D|Cj)=P({w1, w2,..., wn}|Cj)( 4)2) Assuming that the occurrence of each word isindependent of the occurrence of other words in thedocument then:P(w1,...,wn|Cj)=P(w1|Cj)*P(w2|Cj)*...*P(wn|Cj)  (5)Figure 2.
The Na?ve Bayes (supervised) learningalgorithm for document categorizationThe m-estimate method (with m equal to the sizeof word vocabulary) (Cestink, 1990) is used tocompute the probability terms and handle zerocount probabilities (smoothing).
Equation (3) givesan estimate for P(wk/Cj).Various assumptions are needed in order tosimplify Equation (1), whose computations areotherwise expensive.
These assumptions areapplied in Figure 2 to obtain the needed quantitiesfor the class-conditional probabilities (Equations (4)and (5)).
These assumptions are:1.
The probability of encountering a specific wordwithin a document is the same regardless the wordposition.
In other words, P(wi=w|Cj)= P(wm= w|Cj)for every i, j, and m where i and m are differentpossible positions of the same word within thedocument.
This assumption allows representing adocument as a bag of word (Equation (4) in Figure2).2.
The probability of occurrence of a word isindependent of the occurrence of other words in thesame document.
This is reflected in Equation (5):P(w1,...,wn|Cj)=P(w1|Cj)*P(w2|Cj)*...*P(wn|Cj).
It isin fact a na?ve assumption, but it significantlyreduces computation costs, since the number ofprobabilities that should be computed is decreased.Even though this assumption does not hold inreality, NB performs surprisingly well for textclassification (Mitchell, 1997).5 Experiments and resultsFor classification problems, it is customary tomeasure a classifier?s performance in terms ofclassification error rate.
A data set of documents isused with known category/class label L(Dk) for eachdocument Dk.
The set is split into two subsets: atraining set and a testing set.
The trained classifier isused to assign a class AC(Dk) using Equation (3) toeach document (Dk) in the test set, as if its true classlabel were not known.
If AC(Dk) matches L(Dk), theclassification is considered correct; otherwise, it iscounted as an error:Errorik= ??????
?= ii C  )AC(D and ,C  )L(D iff         1otherwise        0kk  (6)For a given class, the error rate is computed as theratio of the number of errors made on the whole testset of unlabeled documents (Xu)  to the cardinality|Xu| of this set.
For a given class Ci, the error rate iscomputed as:ClassErrori =  ?
=  |X| 1k iku Error / |Xu|   (7)In order to measure the performance of the NBalgorithm on Arabic document classification, weconducted several experiments: we performed crossvalidation using the original space (using all thewords in the documents), cross validationexperiments based on feature selection (using asubset of terms/roots only), and experiments basedon an independently constructed evaluation set.
Thefollowing paragraphs describe the data set used, andthe experiments.5.1 The data setWe have collected 300 web documents for eachof five categories from the websitewww.aljazeera.net, which is the website ofAljazeera (the Qatari television news channel inArabic).
This site contains over seven million(7,000,000) documents corresponding to theprograms broadcast on the television channel; it isarguably the most visited Arabic web site.Aljazeera.net presents documents in (manuallyconstructed) categories.
The five (5) categoriesused for this work are: sports, business, culture andart, science, and health.5.2 Cross validationIn cross validation, a fixed number of documentsis reserved for testing (as if they were unlabeleddocuments) and the remainder are used for training(as labeled documents).
Several such partitions ofthe data set are constructed, by making randomsplits of the data set.
NB's performance is evaluatedseveral times, using the different random partitions.Then the error statistics are aggregated.
The steps ofthe cross validation experiments are delineated inFigure 3 next:Cross validation stepsLet X be the entire data seto f N=1500 documentsc =5 is the number of different categoriesEr,i will store the error rate for category i duringtrial r.1) Fix the size s of the training set for (s=N/3, N/2,2N/3, or N-1) to perform 1/3-2/3, 50/50, 2/3-1/3 orleave-one-out cross validation.2) Set the number of trials T. If s=N-1, fix thenumber of trials T=N; else, T=40.3) For trial r=1 to T3.1 Select randomly s documents from X aslabeled documents into training set X lr .3.2 Store the remaining documents (X- X lr ) asunlabeled documents into X ur  (as if they wereunlabeled).3.3 Train NB using X lr .
(Compute Equation (2)and Equation (4))3.4 Use trained NB to compute the class of eachelement in X ur using Equation (4)3.5 Compute error rate Er,i , on Xur  for eachcategory (i=1,2...,c) using Equation (7):Er,i  =  ?
=  |X| 1k u ikError /|Xu|    i=1,2,?,cNext r (return to step 3).4.1 Compute the average error rate for each classover all trials:AvgErrori,s.= ?
= T 1r ir,    /TE  i=1,2,?,c4.2 Compute the maximum error rate for eachclass over all trials:MaxErrori,s = Max Tr ...,2,1= {Er,i}  i=1,2,?,c4.3 Get the minimum error rate for each classover all trials:MinErrori,s = Min Tr ...,2,1= {Er,i}  i=1,2,?,c.Next s (return to step 1)Figure 3.
Cross validation experiments.5.2.1.
Experiments without feature extractionIn these experiments, each document in data set Xis represented by all word roots in the document.The cross validation experiments described inFigure 3, is conducted.
Table 1 reports the errorrates obtained over all categories during the crossvalidation experiments.
The smallest error rate isobtained in the leave-one-out experiment (asillustrated in Table 1).
Table 2, Table 3, Table 4,and Table 5 represent, respectively, the confusionmatrices of the cross validation experiments.
Thepercentages reported in an entry of a confusionmatrix correspond to the percentage of documentsthat are known to actually belong to the categorygiven by the row header of the matrix, but that areassigned by NB to the category given by the columnheader.Cross-validation Experiments1/3-2/3 1/2-1/2 2/3-1/3 Leave-one-outAvg 67% 55% 46% 32.1%Max 69.9% 56.5% 49% 100%ErrorRate Min 62% 48.1% 42% 0%Table 1.
The error rates of NB over all categories incross validation experiments (with feature extraction)Category Health Business  Culture Science SportHealth 22% 27% 3% 8% 40%Business 7% 39% 10% 18% 26%Culture 13% 18% 27% 7% 35%Science 14% 15% 8% 30% 33%Sport 16% 12% 17% 8% 47%Table 2.
Confusion Matrix results for crossvalidation, with no feature extraction (1/3-2/3).CategoryhealthBusinessCultureScienceSportHealth 32% 22.5% 3.2% 8% 34.3%Business8.2% 50% 10.7% 13.3% 17.8%Culture 8% 20% 39% 3% 30%Science 16% 9.8% 7.2% 46% 21%Sport 12% 8% 16% 4% 60%Table 3.
Confusion Matrix results for crossvalidation, with no feature extraction (1/2-1/2).CategoryHealthBusinessCultureScienceSportHealth 46% 12% 6% 8% 28%Business 4.8% 63% 7% 9.2% 16%Culture 7.1% 16.8% 42% 6.1% 28%Science 8.1% 10.8% 9.1% 46% 26%Sport 7.2% 5% 6.8% 5% 76%Table 4.
Confusion Matrix results for crossvalidation, with no feature extraction (2/3-1/3).CategorynameHealth Business Culture Science SportHealth 58.0% 13% 4% 3.7% 21.3%Business 4.6% 73.5% 5.3% 4.6% 12%Culture 2.3% 10% 57.0% 0.7% 30%Science 13.3% 5.3% 2.3% 59.1% 20%Sport 2.0% 1.3% 3.6% 1.3% 91.8%Table 5.
Confusion Matrix results for cross validation,with no feature extraction (Leave-one-out)The diagonals in tables 2-5 indicate higherclassification performance for categories: Sport andBusiness than for the categories: Culture, Science,and health.
Moreover, the leave-one-out experimentyields the best result by category as illustrated inTable 5 compared to the error rates reported intables 2-4.
Tables 2-5 revealed that error rates bycategory decrease from experiment to experiment.In other words, the error rates recorded in 1/3-2/3experiment are higher than those in 1/2-1/2experiment, those in 1/2-1/2 experiment are higherthan those in 2/3-1/3 experiment, and those obtainedin the 2/3-1/3 experiment are higher than those inthe leave-one-out experiment.
Thus, larger trainingsets yield higher accuracy when all the data setterms are used.When investigating some of themisclassifications/confusions made by NB, we havenoticed that misclassified documents, in fact,contain large number of words that arerepresentative of other categories.
In other words,documents that are known to belong to a categorycontain numerous words that have higher frequencyin other categories.
Therefore, these words havehigher influence on the prediction that will be madeby the classifier.
For instance, the confusion matrixin Table 5 shows that  30% of Culture documentshave been misclassified in the Sports category.
Themisclassified documents contain words that aremore frequent in the Sports category such as ?????
(Arabic for prize and for trophy), ???
(Arabic forchampion and for lead character), and ?????
(Arabicfor scoring and for recording).5.2.2.
Cross-validation, using feature selectionFeature selection techniques have been widelyused in information retrieval as a means for copingwith the large number of words in a document; aselection is made to keep only the more relevantwords.
Various feature selection techniques havebeen used in automatic text categorization; theyinclude document frequency (DF), information gain(IG) (Tzeras and Hartman, 1993), minimumdescription length principal (Lang, 1995), and the ?2statistic.
(Yang and Pedersen, 1997) has foundstrong correlations between DF, IG and the ?2statistic for a term.
On the other hand, (Rogati andYang, 2002) reports the ?2 to produce bestperformance.
In this paper, we use TF-IDF (a kindof augmented DF) as a feature selection criterion, inorder to ensure results are comparable with those in(Yahyaoui, 2001).TF-IDF (term frequency-inverse documentfrequency) is one of the widely used featureselection techniques in information retrieval (Yatesand Neto, 1999).
Specifically, it is used as a metricfor measuring the importance of a word in adocument within a collection, so as to improve therecall and the precision of the retrieved documents.While the TF measurement concerns the importanceof a term in a given document, IDF seeks tomeasure the relative importance of a term in acollection of documents.
The importance of eachterm is assumed to be inversely proportional to thenumber of documents that contain that term.
TF isgiven by TFD,t, and it denotes frequency of term t indocument D. IDF is given by IDFt = log(N/dft),where N is the number of documents in thecollection, and dft is the number of documentscontaining the term t. (Salton and Yang, 1973)proposed the combination of  TF and IDF asweighting schemes, and it has been shown that theirproduct gave better performance.
Thus, the weightof each term/root in a document is given by wD,t =TFD,t * IDFt.We have conducted five cross validationexperiments based on TF-IDF.
Experiments arebased on selecting, in turn, 50, 100, 500, 1000, and2000 terms that best represent the predefined 5categories.
We have repeated the experiments inFigure 3 for each number of terms.
A summary ofthe results is presented in Table 6.
The performancelevels obtained are comparable to those obtainedwithout feature selection.
Figure 4 plots averagecategorization error rates versus the number ofterms used for different trials.Experiments#terms/roots1/3-2/31/2-1/22/3-1/3Leave-one-out50 75.2(69.92,77.42)  64.88(60.32,68.4)  53.48(49.62,56.14)  36.9(0,100)100 73.44(67.2,77) 62.58(59,66.7) 49.44(46.62,53.96)  33.7(0,100)500 71.82(65.94,75.5)  60.32(55.9,64.24)  48.96(45.66,52.3)  33.16(0,100)1000 69.54(64.06,72.12)  57.08(52.58,62.1)  46.96(42.84,50.76)  32.18(0,100)2000 66.18(61.3,69) 53.96(46.9,66) 44.38(40.8,47.58)  31.22(0,100)5000 67(62,69.9) 55(48.1,56.5) 46(42,49) 32.1(0,100)Table 6.
The overall error rate of NB in crossvalidation experiments using feature selection, informat: Avg(Min, Max)Category NB accuracyHealth 50%Business 70%Culture 40%Science 60%Sport 90%Table 7.
Classification accuracy on the evaluation setusing Leave-one-out and TF-IDF with 2,000 roots/termsCategorization error rates versusnumber of roots in vocabulary0102030405060708050 100 500 1000 2000 5000Number of roots in vocabularyCategorization error rates 1/3-2/31/2-1/22/3-1/3Leave-one-outFigure 4.
Categorization error rates versusnumber of terms.5.3 Experiments using an evaluation setCross validation has been used to determine theaverage performance of NB for Arabic textcategorization, and to design training sets thatproduce the best performance.
This experiment,based on a separately and independently constructedevaluation set, is designed to evaluate theperformance of NB on a set of documents that havenever been submitted to the classifier.
For thispurpose, we further carefully collected manually 10documents from Aljazeera.net for each of the 5predefined categories.
For each category, we haveselected documents that best represent thevariability in the category.
We refer to thiscollection of documents as the evaluation set.
Thisset is presented to the classifier for categorization.For testing on the evaluation set, trained NBclassifiers are used.
For each category, we use theNB classifier that has been trained using the trainingset that produced the best category classificationaccuracy in cross validation experiments.
In ourcase, we have used the whole set as a training set(1,500) represented by 2,000 terms since the bestcross validation accuracy was obtained in leave-one-out experiment with 2,000 terms.
Table 7summarizes NB?s performance results when testedusing the evaluation set.
The results obtained haveshown higher performance for the Sports and theBusiness categories with a classification accuracythat is higher than 70%.
The performance of othercategories ranges from 40% to 60%.
The averageaccuracy over all categories is 62%.The results obtained in the evaluation setexperiment are very consistent with theperformance obtained in cross validationexperiments.6 ConclusionsTo sum up, this work has been carried out toautomatically classify Arabic documents using theNB algorithm, with the use of a different data set, adifferent number of categories, and a different rootextraction algorithm from those used in (Yahyaoui,2001).
In this work, the average accuracy over allcategories is: 68.78% in cross validation and 62% inevaluation set experiments.
The correspondingperformances in (Yahyaoui, 2001) are 75.6% and50%, respectively.
Thus, the overall performance(including cross validation and evaluation setexperiments) in this work is comparable to that in(Yahyaoui, 2001).
This offers some indication thatthe performance of NB algorithm in classifyingArabic documents is not sensitive to the Arabic rootextraction algorithm.
Future work will be directed atexperimenting with other root extraction algorithms.Further improvement of NB?s performance may beeffected by using unlabeled documents; e.g., EMhas been used successfully for this purpose in(Nigam et al, 200), where EM has increased theclassification accuracy by 30% for classifyingEnglish documents.
Two (English) documentcategorization algorithms have been reported toproduce best results: Support Vector Machines(SVM) and AdaBoost.
If the similarity betweenNB?s performance for English and Arabic is anyindication, SVM and AdaBoost should be the nextcandidates for application to Arabic Documentcategorization.ReferencesR.
Al-Shalabi, and M. Evens, "A computationalmorphology system for Arabic,?
In Workshop onComputational Approaches to Semitic Languages,COLING-ACL98, 1998.B.
Cestink, "Estimating probabilities: A crucial taskin machine learning," Proceedings of the NinthEuropean Conference on Artificial Intelligence, pp.147--149, London, 1990.K.
Crammer and Y.
Singer, ?A Family of AdditiveOnline Algorithms for Category Ranking,?
JMLR,v.
3, pp.
1025-1058, Feb. 2003.R.
H. Creecy, B. M. Masand, S. J. Smith, and D. L.Waltz, ?Trading mips and memory for knowledgeengineering,?
Communication of the ACM, Vol.
35,No.
8, pp.
48--64, August 1992.M.
El Kourdi, T. Rachidi, and A. Bensaid, "Aconcatenative approach to Arabic word rootextraction," in progress, 2004.Y.C.
Fang, S. Parthasarathy and F. Schwartz,"Using clustering to boost text classification,"ICDM Workshop on Text Mining (TextDM'01),2001.Y.
Houmame, Towards an Arabic InformationRetrieval System, MS thesis, AlAkhawaynUniversity, Morocco, 1999.T.
Joachims, Learning to classify text using SVM,Kluwer Academic Publishers, 2002.K.
Lang, "Newsweeder: Learning to filter netnews,"Proceedings of the Twelfth InternationalConference on Machine Learning, 1995.D.
Lewis, M. Ringnette, "Comparison of twolearning algorithms for text categorization,"Proceedings of the Third Annual Symposium onDocument Analysis and Information Retrieval(SDAIR'94), 1994.D.
Lewis, Yiming Yang, Tony G. Rose, Fan Li, ?ANew Benchmark Collection for Text CategorizationResearch,?
JMLR, v. 5, pp.
361-397, Apr.
2004.T.
Mitchell.
Machine learning.
McGraw Hill, 1997.K.
Nigam, A. K. McCallum, S. Thrun, andT.Mitchell, "Text classification from labeled andunlabeled documents using EM," MachineLearning, vol.
39, pp.
103--134, 2000.T.
Rachidi, O. Iraqi, M. Bouzoubaa, A. Ben AlKhattab, M. El Kourdi, A. Zahi, and A. Bensaid,?Barq: distributed multilingual Internet searchengine with focus on Arabic language,?Proceedings of IEEE Conf.
on Sys., Man andCyber., Washington DC, October 5-8, pp.
, 2003.M.
Rogati and Y. Yang.
?High-performing featureselection for text classification,?
ACM CIKM 2002.Sakhr software company's website:www.sakhrsoft.com, 2004.G.
Salton and C. S. Yang, "On the specification ofterm values in automatic indexing", Journal ofDocumentation, Vol.
29, No.
4, pp.
351--372, 1973.F.
Sebastiani, ?Machine learning in automated textcategorization,?
ACM Computing Surveys, v.34 n.1,p.1-47, March 2002.K.
Tzeras and S. Hartman, "Automatic indexingbased on Bayesian inference networks," Proc 16thAnn Int ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR'93),pp.
22--34, 1993.
(Wiene and Pedersen, 1995) E. Wiener, J. O.Pedersen, and A. S. Zeigend, "A neural networkapproach to topic spotting," Proceedings of theFourth Symposium on Document Analysis andInformation Retrieval (SDAIR'95), 1995.M.
Yahyaoui, "Toward an Arabic web pageclassifier," Master project.
AUI.
2001.Y.
Yang, ?An evaluation of statistical approaches totext categorization,?
Journal of InformationRetrieval, Vol.
1, Number 1-2, pp.
69--90, 1999.Y.
Yang and X. Liu, ?A re-examination of textcategorization methods,?
Proceedings of ACMSIGIR Conference on Research and Development inInformation Retrieval (SIGIR'99), pp 42--49, 1999.R.
B. Yates, and B. R. Neto,  Modern informationretrieval.
Addison-Wesley ISBN 0-201-39829-X,1999.Yang, Y., Pedersen J.P. A Comparative Study onFeature Selection in Text CategorizationProceedings of the 14th International Conferenceon Machine Learning, pp.
412-420, 1997.
