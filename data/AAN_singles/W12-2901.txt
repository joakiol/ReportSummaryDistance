NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 1?4,Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational LinguisticsA free and open-source tool that reads movie subtitles aloudPeter Ljungl?fComputer Science and EngineeringUniversity of GothenburgGothenburg, Swedenpeter.ljunglof@gu.seSandra Derbring and Maria OlssonDART: Centre for AAC and ATGothenburg, Swedensandra.derbring@vgregion.semaria.in.olsson@vgregion.seAbstractWe present a simple tool that enables the com-puter to read subtitles of movies and TV showsaloud.
The tool extracts information from sub-title files, which can be freely downloadedfrom the Internet, and reads the text aloudthrough a speech synthesizer.
There are threeversions of the tool, one for Windows andLinux, another for Mac OS X, and the third isa browser-based HTML5 prototype.
The toolsare freely available and open-source.The target audience is people who have trou-ble reading subtitles while watching a movie,including elderly, people with visual impair-ments, people with reading difficulties andpeople who wants to learn a second language.The application is currently being evaluatedtogether with user from these groups.1 Background1.1 Why read subtitles aloud?Spoken subtitles could be a solution if, due to sightdisorder or poor reading skills, a person is unable toread subtitles and the language spoken in the movieis unknown, or not known well enough.Swedish Association of the Visually Impaired1has around 12,000 members but there are most likelymany more people with poor eyesight.
The num-ber of people with reading disabilities is unknown,but according to the Swedish dyslexia association?Dyslexif?reningen?2 between 5 and 8 percent ofthe population have significant difficulties to readand write.
A survey by OECD (Organisation for1Synskadades riksf?rbund, http://www.srfriks.org/2Dyslexif?reningen, http://dyslexiforeningen.se/Economic Co-operation and Development) in 1996showed that ?8 per cent of the adult population [inSweden] encounters a severe literacy deficit in ev-eryday life and at work?
(OECD, 2000, p. xiii).
Forother countries, the problems were even bigger: ?In14 out of 20 countries, at least 15 per cent of alladults have literacy skills at only the most rudimen-tary level?
(OECD, 2000, p. xiii).To hear the subtitles along with the original au-dio track of the movie may not suit everyone, butmaking these movies and TV shows accessible couldbring a huge value for people who would use it.1.2 Related workThe idea of automatic reading of movie and TV sub-titles is not new.
It is implemented in regular pub-lic service TV broadcasts in at least Sweden andthe Netherlands, and probably also in more coun-tries.
In 2002, the Dutch national broadcasting com-pany NOS started regular broadcasts of automaticsubtitles reading (Verboom et al, 2002), and Swe-den?s public service TV company SVT followed in2005 (A-focus, 2010, p. 20).
In both these cases,the speech signal is transmitted through a secondchannel, which means that the user needs two digi-tal boxes.
Naturally, this solution only works for theprograms that the company itself is broadcasting.Other projects have been trying to use OCR (op-tical character recognition) to interpret the subtitleson the TV or computer screen.
In 2002, a projectby the Swedish Association of the Visually Impaireddeveloped a prototype that used OCR to Interpretsubtitles, which then were spoken aloud using TTS(Eliasson, 2005, pp.
63?64).
The project estimatedthat a mass-produced product would cost around2500C, which they concluded would be too much1for ordinary users.
In 2007, a similar Danish projectdescribed a tool that reads the composite video sig-nal, performs OCR on the subtitles and then speaksthem using TTS (Nielson and Bothe, 2007).
Theyalso developed a specialised OCR algorithm for sub-title detection (J?nsson and Bothe, 2007).
However,both systems have remained prototypes and have notbeen released as publicly available tools.A similar Czech project has investigated how tominimise speech overlap and how to get better syn-chronisation by using techniques such as time com-pression and text simplification (Hanzl?c?ek et al,2008; Matou?ek et al, 2010).
Their evaluation ispurely technical, where they count the number ofoverlapped subtitles and the number of subtitles thatrequire different compression factors, but they havenot evaluated their prototype system on actual users.Finally, there is an ongoing Swedish project bythe Swedish dyslexia association ?Dyslexif?rbundetFMLS?
where they aim to make cinemas more ac-cessible by transmitting spoken subtitles via Wi-Fiwhich the users can listen to via their own mobilephone.1.3 Issues with existing solutionsCurrently there are two kinds of spoken subtitlessystems, and both of them have different problems:?
TV broadcasting systems that transmit the spo-ken subtitles in a separate audio stream.
It isan important addition to the TV infrastructure,but it is by nature closed to one media channeland cannot be used for users who want to watchmovies or TV shows on their computer or fromthe Internet.?
Systems that use OCR to interpret movie subti-tles have a great potential, but they are currentlyno publicly available systems.
There are stillsome technological problems left to be solveduntil OCR based systems can be released to thepublic.None of the existing systems are freely available, letalone open-source products.
Furthermore, we havenot found any studies that evaluate these systems onreal users, to find out how useful they are in practice.The systems we describe in this paper are allfreely available and open-source.
They are focusedon personal computer use, not TV or cinemas, andare meant to be usable and easily installable to thosewith basic computer skills.2 ImplementationThe idea behind all our implementations is very sim-ple.
The program reads the subtitles into an internaldatabase.
When the movie starts playing, the pro-gram communicates with the movie to get the cur-rent time position, and calls a speech synthesiserwhen it is time to show the next subtitle.
The pro-gram does not include a speech synthesiser, but as-sumes that it is already installed on the computer.Alternatively, the program can call an online webservice-based TTS.We have developed three systems which workin different ways and on different operating sys-tems.
Some of them are still in prototype/demo state,whereas others are almost finished products.
Allsystems are free and open-source and can be down-loaded from the project website:http://code.google.com/p/subtts2.1 Windows/Linux media playerThe Windows/Linux client has been developed bythe company STTS.3 It is implemented in Pythonand the wxWidgets GUI toolkit.4 The video play-back interface uses a Python backend that comeswith the VLC Media Player.5 This means that theclient can play all media formats that the VLC playercan handle, including DVD movies.2.2 Mac OS X menuletThe Mac OS X client uses the AppleScript Eventmodel to communicate with the active media player.The program is developed in Objective-C and re-sides in the menu bar as a global ?menulet?6.When the user starts watching a movie, themenulet repeatedly queries the media player forthe current time, and calls the speech synthesiserwhenever a new subtitle is about to be shown.The menulet currently supports the following me-dia players: VLC, QuickTime Player (versions 7 andX), and Apple DVD Player.3S?dermalms Talteknologiservice, http://stts.se/4wxWidgets, http://wxwidgets.org/5VLC, http://videolan.org/6http://en.wikipedia.org/wiki/Menulet22.3 Browser-based HTML5 media playerWe have also developed a prototype browser-basedmedia player written in Javascript, that uses HTML5video and audio elements to support spoken subti-tles.
This has the potential to be very useful, but iscurrently limited since current browsers do not sup-port HTML5 video and audio in full.We estimate that, in a few years time, the mainbrowsers will support all HTML5 features,as well asoffline TTS.
Then this kind of HTML5 media playercould have a big impact on movie and TV accessi-bility.2.4 Subtitle filesThe system does not extract the subtitles from themovie file or the DVD.
Instead the user has to pro-vide it with a text file with the movie subtitles.
Sub-titles are available from several sites on the Internet,7both in the original language and in translations intodifferent other languages.The subtitle format that we support is SRT, whichis the de-facto standard for movie subtitiles and avery simple text format.
Each subtitle is in a separateparagraph on the following form:2600:03:05,083 --> 00:03:09,417You, I mean we,we could easily die out here.The above example means that the 26th subtitle con-sists of two lines of text, and should be displayed3 minutes 5.083 seconds into the movie and disap-pear 4.334 seconds later.Both the Windows and the Mac OS X clients canshow DVD movies, but they cannot use the subtitlesthat are provided with the movie.
DVD subtitles arepre-rendered into separate video tracks.
To accessthem we would have to use OCR which was not inthe scope of this project.One serious drawback with existing subtitles isthat they do not store meta-information about thespeaker.
Useful meta-information would be gender,age and dialect of the speaker, or even a unique iden-tifier for each person in the movie.
With this infor-mation the system could use different TTS voices fordifferent characters.7E.g., http://opensubtitles.org/ and http://undertexter.se/.2.5 Speech synthesisWe are only using existing speech synthesisers,which means that the user either has to have a TTSvoice installed on his/her computer, or constant ac-cess to the Internet since the system can call exist-ing online TTS engines.
The only problem with on-line TTS systems is that almost all of them are fordemonstration purposes only and therefore cannotbe used in day-to-day work.
We have been usingan online Swedish open-source voice being devel-oped by the company STTS8 using the OpenMaryTTS platform (Schr?der and Trouvain, 2003).Here is the current status of speech synthesis forour different systems:?
The Windows client can use any SAPI voice in-stalled on the system.
It can also use an onlinevoice, as an alternative.?
The Mac OS X client can use any voice in-stalled on the system.
The latest version ofOS X (10.7) includes high-quality voices for 22different languages, so there is no need for on-line voices on this platform.?
The HTML5 browser client cannot use system-installed voices, since that functionality is notincluded in HTML5.
There is a current W3Cdraft proposal for how to use TTS from withinHTML (Bringert, 2010), but it is not decidedupon and no browsers support this yet.
Un-til TTS becomes a HTML standard we have torely on online voices, which unfortunately is ascarce resource.3 Discussion3.1 Social and pedagogical advantagesPeople with visually impairments and/or readingdifficulties often use text-to-speech to cope withschool work, and to keep up with society.
Spokensubtitles further increase the accessibility of foreignmovies and TV shows for these people.Hopefully, spoken subtitles can help improve thereading skills for people with reading difficulties.The theory is that listening to the spoken subtitlesat the same time as reading the text may benefit thereading process, but this has yet to be tested.8S?dermalms Talteknologiservice, http://stts.se/33.2 EvaluationWe are currently, during spring 2012, evaluating theapplications together with different users in the tar-get groups.
Initially we will only be evaluating usersatisfaction and whether this approach could be anaccepted solution to the need of text interpretationduring movie playback.If this initial evaluation is positive, we are veryinterested in continuing by evaluating specific fac-tors that might or might not improve user satisfac-tion.
Such factors could be: using different TTSvoices, using different speech rates, reducing speechoverlap, having the speech coming from another di-rection, lowering the movie volume while speaking,using advanced audio techniques for filtering awaymovie speech, etc.Another interesting evaluation would be to en-code speaker meta-information into movie subtitles,and test how different TTS voices for different char-acters can improve the user?s satisfaction and com-prehension.3.3 Future workTo further ease the user friendliness and the avail-ability, it would be desirable to have the func-tionality built into an existing media player, suchas the open-source and cross-platform VLC MediaPlayer.9 If more users request this functionality, thedevelopers will have to catch on and include it intonew releases.According to (Hanzl?c?ek et al, 2008), 44 per-cent of the Czech subtitles had overlaps when spo-ken with TTS.
Even though we have no figures forSwedish, some overlap is to be expected also here,which is an issue that should be addressed.
One pos-sible simple solution is to modify the speech rate.An important factor for the experience of thespeech synthesizer together with a video playbackwould be the settings of the audio channels.
Hy-pothetically, a listener would want to keep both theoriginal background cues, like music, and the orig-inal voices.
However, these sounds must not inter-fere with the speech synthesizer that is the sourceof information for the listener.
Balancing these twocriteria to get the optimized result is of great interest.9VLC Media Player, http://www.videolan.org/vlc/If the program would be used for language learn-ing, or to help slow readers to comprehend, the fea-ture of highlighting the word that is spoken could bea very useful additional feature.AcknowledgementsThe SubTTS project is funded by the Swedish Postand Telecom Authority (PTS).
We are grateful tofour anonymous referees for their comments.ReferencesA-focus.
2010.
Utredning avseende TV-tillg?nglighetf?r personer med funktionsneds?ttning.
Myndighetenf?r radio och TV, Stockholm, Sweden.Bj?rn Bringert.
2010.
HTML text to speech (TTS) APIspecification.
W3c editor?s draft, W3C.Folke Eliasson.
2005.
IT i praktiken ?
slutrapport.Hj?lpmedelsinstitutet, Sweden.Zdene?k Hanzl?c?ek, Jindr?ich Matou?ek, and Daniel Ti-helka.
2008.
Towards automatic audio track gener-ation for Czech TV broadcasting: Initial experimentswith subtitles-to-speech synthesis.
In ICSP ?08, 9thInternational Conference on Signal Processing, Bei-jing, China.Morten J?nsson and Hans Heinrich Bothe.
2007.
OCR-algorithm for detection of subtitles in television andcinema.
In CVHI?07, 5th Conference and Workshopon Assistive Technology for People with Vision andHearing Impairments, Granada, Spain.Jindr?ich Matou?ek, Zdene?k Hanzl?c?ek, Daniel Tihelka,and Martin M?ner.
2010.
Automatic dubbing of TVprogrammes for the hearing impaired.
In 10th IEEEInternational Conference on Signal Processing, Bei-jing, China.Simon Nielson and Hans Heinrich Bothe.
2007.
SubPal:A device for reading aloud subtitles from televisionand cinema.
In CVHI?07, 5th Conference and Work-shop on Assistive Technology for People with Visionand Hearing Impairments, Granada, Spain.OECD.
2000.
Literacy in the Information Age: Final Re-port of the International Adult Literacy Survey.
OECDPublications, Paris.Marc Schr?der and J?rgen Trouvain.
2003.
The Ger-man text-to-speech synthesis system MARY: A toolfor research, development and teaching.
InternationalJournal of Speech Technology, 6:365?377.Maarten Verboom, David Crombie, Evelien Dijk, andMildred Theunisz.
2002.
Spoken subtitles: Makingsubtitled TV programmes accessible.
In ICCHP?02,8th International Conference on Computers HelpingPeople with Special Needs, Linz, Austria.4
