Proceedings of the 14th European Workshop on Natural Language Generation, pages 172?177,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsOn the Feasibility of Automatically Describing n-dimensional ObjectsPablo Ariel DuboueLes Laboratoires Foulab999 du CollegeMontreal, Quebe?cpablo.duboue@gmail.comAbstractThis paper introduces the problem of gen-erating descriptions of n-dimensional spa-tial data by decomposing it via model-based clustering.
I apply the approachto the error function of supervised clas-sification algorithms, a practical problemthat uses Natural Language Generation forunderstanding the behaviour of a trainedclassifier.
I demonstrate my system on adataset taken from CoNLL shared tasks.1 IntroductionMy focus is the generation of textual descriptionsfor n-dimensional data.
At this early stage inthis research, I introduce the problem, describe apotential application and source of interesting n-dimensional objects and show preliminary workon a traditional NLG system built on off-the-shelftext planning and surface realization technologyplus a customized sentence planner.This work was inspired by a talk by KathleenMcCoy in which she described a system that pro-duces Natural Language explanations of maga-zine infographics for the blind by combining Com-puter Vision techniques with NLG (Carberry et al2013).
She mentioned an anecdote in which sheasked a blind user of the system what would theuser would want added to the text description andthe user replied ?I don?t know, I have never seenan infographic.?
I found the comment very inspir-ing and it led to the realization that n-dimensionalobjects (for n > 3) were also something whichwe, as humans, have never seen before and whichwe will profit from having a computer system todescribe to us.A type of n-dimensional objects that are of par-ticular practical interest are the error function for amachine learning algorithm for particular trainingdata.
That is the case because, for NLP practition-ers using supervised classification, the task of de-bugging and improving their classifiers at times in-volves repeated steps of training with different pa-rameters.
Usually, at each stage the trained modelis kept as an opaque construct of which only ag-gregate statistics (precision, recall, etc) are inves-tigated.
My technology improves this scenario bygenerating Natural Language descriptions for theerror function of trained machine learning models.My system, Thoughtland,1 (Fig.
1) is a pipelinewith four stages, accessed through a Web-basedinterface (Duboue, 2013), further discussed in thenext section.This early prototype is already able to tackle de-scriptions of existing, non-trivial data.
These re-sults are very encouraging and the problem meritsattention from other NLG researchers.
To furtherbroad interest in this problem, I am distributing myprototype under a Free Software license,2 whichshould encourage extensions and classroom use.
Ihave already found the current descriptions usefulfor telling apart the output of two different algo-rithms when run on the same data.I will now describe the algorithm and then diveinto the NLG details.
I conclude with related andfuture work discussions.2 AlgorithmThoughtland?s architecture is shown in Fig.
1.While the first stage lies clearly outside the in-terest of NLG practitioners, the next two stages(Clustering and Analysis) are related to the mes-sage generation aspect of content planning (Reiterand Dale, 2000),3 as they seek to transform thedata into units that can be communicated verbally(the last stage is the more traditional NLG systemitself).1http://thoughtland.duboue.net2https://github.com/DrDub/Thoughtland3pages 61-63.172TrainingDataFigure 1: Thoughtland?s architecture.2.1 Cross-ValidationThe error function is computed as the error foreach point in the input data.
For a numeric tar-get class, that would mean that for every traininginstance (~x, y), e =??
?f( ~x)?
y??
?, where the erroris computed using f trained on the folds that donot contain (~x, y).4 This stage produces a cloud ofpoints in n-dimensions, for n = F + 1, where Fis the number of features in the training data (theextra dimension is the error value).2.2 ClusteringThe cloud of error points obtained in the previousstep is then clustered using a mixture of Dirich-let models (McCullagh and Yang, 2008) as imple-mented by Apache Mahout (Owen et al 2011).5I choose this clustering approach because eachof the obtained clusters has a geometrical rep-resentation in the form of n-balls, which are n-dimensional spheres.
These representations areimportant later on for the natural language gener-ation approach.Some input features present a natural geomet-ric groupings which will interfere with a clusteringset to elucidate the error function.
To make the er-ror coordinate the most prominent coordinate forclustering, I re-scale the error coordinate using theradius of an n-ball that encompasses all the inputfeatures.2.3 AnalysisIn Fig.
1, the Analysis Stage involves determin-ing the overall size, density, distances to the othern-balls and extension in each dimension for eachn-ball.
These numbers are put into perspectivewith respect to the n-ball encompassing the wholecloud of points.
The distance between two n-balls,for example, is said to be big if in any dimension4The error is different if the target class is not numeric(nominal target classes).
In that case the error is 1.0 if theclass is different from the target or 0 if it the same.5See Section 9.4.2, ?Dirichlet clustering.
?it is above half the radius of the large n-ball inthat particular dimension.
Each n-ball is also com-pared to each other in terms of distance.I have so far determined these thresholds byworking on the mileage data discussed elsewhere(Duboue, 2013).
Objective-function optimization-based techniques (discussed in the next section)might prove useful here.This stage is at its infancy, in future work Iwant to analyze the pairs of n-balls in terms ofrotations as they are particularly important to de-termine how many dimensions are actually beingused by the sets of n-balls.3 Natural Language GenerationAs I go exploring the different aspects of the prob-lem, I opt for a very traditional generation systemand architecture.
Approaches based on learning(Mairesse et al 2010; Varges and Mellish, 2010;Oh and Rudnicky, 2000) are not particularly easyto apply to this problem as I am producing a textfor which there are no available examples.
I dohope to explore objective-function optimization-based techniques such as Lemon (2011) or Deth-lefs and Cuaya?huitl (2011) in the near future.The NLG system is thus implemented ontop of McKeown?s (1985) Document Structur-ing Schemata (using the recent implementationOpenSchema6) and SimpleNLG (Gatt and Reiter,2009).
I use two schemata, in one the n-balls arepresented in order while in the other the attributesare presented in order.
One of the schemata Iam using is shown in Fig.
2.
Document structur-ing schemata are transition networks of rhetoricalpredicates that can contain free and bound vari-ables, with restrictions on each variable.
The sys-tem presents the user the shorter description.Either strategy should emphasize similarities,simplifying aggregation (Reape and Mellish,1999).
I employ some basic aggregation rules, that6http://openschema.sf.net173is, for each aggregation segment I assemble alln-balls with the same property together to makecomplex sentences.
That works well for size anddensity.
To verbalize distances, I group the dif-ferent pairs by distance value and then look forcliques using the Bron-Kerbosch clique-finding al-gorithm (Bron and Kerbosch, 1973), as imple-mented in JGraphT.7 I also determine the mostcommon distance and verbalize it as a defeasiblerule (Knott et al 1997), which significantly short-ens the text.This pipeline presents a non-trivial NLG appli-cation that is easy to improve upon and can be useddirectly in a classroom setting.3.1 Case StudyI will now illustrate Thoughtland by virtue ofan example with training data from the CoNLLShared Task for the year 2000 (Sang and Buch-holz, 2000).
The task involved splitting a sentenceinto syntactically related segments of words:(NP He) (VP reckons) (NP the current accountdeficit) (VP will narrow) (PP to) (NP only # 1.8billion) (PP in) (NP September) .The training contains for each word its POS andits Beginning/Inside/Outside chunk information:He PRP B-NPreckons VBZ B-VPthe DT B-NPcurrent JJ I-NPaccount NN I-NPdeficit NN I-NPwill MD B-VPnarrow VB I-VPI transformed the data into a classification problembased on the current and previous POS, renderingit a two dimensional problem.
The provided dataconsists of 259,104 training instances.
Over thisdata Na?
?ve Bayes produces an accuracy of 88.9%and C4.5, 89.8%.
These numbers are very close,but do the two algorithms produce similar errorfunction?
Looking at Thoughtland?s descriptions(Fig.
3) we can see that is not the case.In later runs I add the current and previouswords, to make for a three and fourth dimensionalproblem.
These are extra dimensions with a nomi-nal class with 20,000 distinct values (one for eachword).
Interestingly, when the classifiers becomegood enough, there is no discriminating informa-tion left to verbalize.
A similar situation happenswhen the classifiers have poor accuracy.7http://jgrapht.sourceforge.net/schema by-attribute(whole: c-full-cloud); first sentence, overall numberspred-intro(cloud|whole)aggregation-boundarystarpred-size()aggregation-boundarystarpred-density()aggregation-boundarystarpred-distance()predicate pred-densityvariablesreq def component : c-n-ballreq attribute : c-densitypropertiescomponent == attribute.componentoutputpred has-attributepred0 componentpred1 attributepred2 magnitudeFigure 2: One of the two schemata employed byThoughtland.
This schema produces descriptionsfocusing on the similar attributes of each of the n-balls.
I include one of the predicates for reference.4 Related WorkThe problem of describing n-dimensional objectsis a fascinating topic which Thoughtland just startsto address.
It follows naturally the long term inter-est in NLG for describing 3D scenes (Blocher etal., 1992), spatial/GIS data (De Carolis and Lisi,2002) or just numerical data (Reiter et al 2008).In the more general topic of explaining machinelearning decisions, ExOpaque (Guo and Selman,2007) takes a trained system and uses it to pro-duce training data for an Inductive Logic Program-ming (Muggleton and Raedt., 1994) system, pre-senting the resulting Horn-clauses directly to theuser.
Focusing on explaining the impact of specificattributes in the prediction outcome of a particularinstance, Robnik-Sikonja and Kononenko (2008)analyze changes to the classification outcome un-der different input variations, weighted by theirpriors, an idea explored early on in agent-basedsystems (Johnson, 1994).
In general, systemsbased on Bayesian networks seem to have astronger probabilistic framework that facilitatesexplanations (Lacave and Diez, 2000).By far, most of the attention in understandingthe error function for machine learning algorithmshas come from the graphical visualization commu-174THREE DIMENSIONSNaive Bayes C4.5Accuracy 88.9% Accuracy 89.8%There are five components and three dimensions.
ComponentOne is big and components Two, Three and Four are small.Component Four is dense and components Two and Three arevery dense.
Components Three and Five are at a good distancefrom each other.
The rest are all far from each other.There are six components and three dimensions.
ComponentOne is big, components Two, Three and Four are small andcomponent Five is giant.
Component Five is sparse and com-ponents Two, Three and Four are very dense.
Components Oneand Two are at a good distance from each other.
The rest are allfar from each other.FOUR DIMENSIONSAccuracy 90.4% Accuracy 91.4%There are six components and four dimensions.
ComponentsOne, Two and Three are big and components Four and Five aresmall.
Component Three is dense, component One is sparseand components Four and Five are very dense.
ComponentsTwo and Three are at a good distance from each other.
The restare all far from each other.There are six components and four dimensions.
ComponentsOne, Two and Three are big and components Four and Five aresmall.
Component One is dense, component Three is sparse andcomponents Four and Five are very dense.
Components Threeand Four are at a good distance from each other.
ComponentsSix and Four are also at a good distance from each other.
Therest are all far from each other.FIVE DIMENSIONSAccuracy 91.6% Accuracy 91.6%There is one component and five dimensions.
There is one component and five dimensions.Figure 3: Example generated descriptions.nities.
However, as stated by Janert (2010):8As soon as we are dealing withmore than two variables simultaneously,things become much more complicated ?in particular, graphical methods quicklybecome impractical.The focus is then in dimensionality reduction9and projection (Kaski and Peltonen, 2011), usuallyas part of an integrated development environment(Kapoor et al 2012; Patel et al 2010).
The usualdiscussion regarding the complementary role oftext and graphics, as studied for a long time inNLG (McKeown et al 1997), applies also here:there are things like generalizations and excep-tions that are easier to express in text.
We lookforward for NLG-based approaches to be includedin future versions of ML IDEs such as Gestalt.Finally, Thoughtland uses the error function foran ML algorithm as applied to training data.
Asimilarly worded term which should not be con-fused is error surface (Reed and Marks, 1999),10which refers to the space of possible ML models.Error surfaces are particularly important for train-ing algorithms that explore the said surface, for ex-ample by gradient descent.8Chapter 5, page 99.9A reviewer suggested combining dimensionality reduc-tion and NLG, an idea most definitely worth exploring.10Chapter 8.5 Final RemarksI have presented Thoughtland, a working proto-type addressing the problem of describing cloudsof points in n-dimensional space.
In this paper Ihave identified the problem and shown it to be ap-proachable with a solution based on model-basedclustering.For future work, I want to enrich the analysiswith positional information: I want to find planeson which a majority of the n-balls lie so as to de-scribe their location relative to them.
I am alsoconsidering hierarchical decomposition in up tofive to seven n-balls (to make it cognitively ac-ceptable (Miller, 1956)) as it will translate well totextual descriptions.My preliminary experiments suggest there isvalue in generating comparisons for two errorfunctions.
I can therefore employ the existingbody of work in NLG for generating comparisons(Milosavljevic, 1999).While the pilot might speak of the feasibility ofthe task, Thoughtland still needs to be evaluated.For this, I want to start with simple cases such asoverfitting or feature leaks and see if the descrip-tions help humans detect such cases faster.AcknowledgementsThe author would like to thank Annie Ying, OrBiran, Samira Ebrahimi Kahou and David Racca.175ReferencesA.
Blocher, E. Stopp, and T. Weis.
1992.
ANTLIMA-1: Ein System zur Generierung von Bildvorstel-lungen ausgehend von Propositionen.
Techni-cal Report 50, University of Saarbru?cken, Sonder-forschungsbereich 314, Informatik.Coenraad Bron and Joep Kerbosch.
1973.
Findingall cliques of an undirected graph (algorithm 457).Commun.
ACM, 16(9):575?576.Sandra Carberry, Stephanie Elzer Schwartz, KathleenMccoy, Seniz Demir, Peng Wu, Charles Green-backer, Daniel Chester, Edward Schwartz, DavidOliver, and Priscilla Moraes.
2013.
Access to mul-timodal articles for individuals with sight impair-ments.
ACM Trans.
Interact.
Intell.
Syst., 2(4):21:1?21:49, January.Berardina De Carolis and Francesca A Lisi.
2002.A NLG-based presentation method for supportingKDD end-users.
In Foundations of Intelligent Sys-tems, pages 535?543.
Springer.Nina Dethlefs and Heriberto Cuaya?huitl.
2011.
Hier-archical reinforcement learning and hidden markovmodels for task-oriented natural language genera-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: short papers-Volume2, pages 654?659.
Association for ComputationalLinguistics.P.A.
Duboue.
2013.
Thoughtland: Natural LanguageDescriptions for Machine Learning n-dimensionalError Functions.
In Proceedings of ENLG?13.Albert Gatt and Ehud Reiter.
2009.
SimpleNLG: arealisation engine for practical applications.
In Pro-ceedings of the 12th European Workshop on Natu-ral Language Generation, ENLG ?09, pages 90?93,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Yunsong Guo and Bart Selman.
2007.
ExOpaque:A framework to explain opaque machine learningmodels using Inductive Logic Programming.
In IC-TAI (2), pages 226?229.
IEEE Computer Society.Philipp K. Janert.
2010.
Data Analysis with OpenSource Tools.
O?Reilly.W Lewis Johnson.
1994.
Agents that learn to ex-plain themselves.
In Proceedings of the twelfthnational conference on Artificial intelligence, vol-ume 2, pages 1257?1263.Ashish Kapoor, Bongshin Lee, Desney Tan, and EricHorvitz.
2012.
Performance and preferences: In-teractive refinement of machine learning procedures.In Twenty-Sixth AAAI Conference on Artificial Intel-ligence.Samuel Kaski and Jaakko Peltonen.
2011.
Dimen-sionality reduction for data visualization [applica-tions corner].
Signal Processing Magazine, IEEE,28(2):100?104.Alistair Knott, Mick O?Donnell, Jon Oberlander, andChris Mellish.
1997.
Defeasible rules in con-tent selection and text structuring.
In Proceed-ings of the Sixth European Workshop on NaturalLanguage Generation, pages 50?60, Duisburg, Ger-many, March.Carmen Lacave and Francisco J. Diez.
2000.
A re-view of explanation methods for bayesian networks.Knowledge Engineering Review, 17:2002.Oliver Lemon.
2011.
Learning what to say and how tosay it: Joint optimisation of spoken dialogue man-agement and natural language generation.
Com-puter Speech & Language, 25(2):210?221.Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc??
?c?ek, SimonKeizer, Blaise Thomson, Kai Yu, and Steve Young.2010.
Phrase-based statistical language generationusing graphical models and active learning.
In Pro-ceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1552?1561.
Association for Computational Linguistics.Peter McCullagh and Jie Yang.
2008.
How many clus-ters?
Bayesian Analysis, 3(1):101?120.Kathleen McKeown, Shimei Pan, James Shaw, JordanDesmond, and Barry Allen.
1997.
Language gener-ation for multimedia healthcare briefings.
In Pro-ceedings of the Fifth Conference on Applied Nat-ural Language Processing (ANLP-97), Washington(DC), USA, April.Kathleen Rose McKeown.
1985.
Text Generation: Us-ing Discourse Strategies and Focus Constraints toGenerate Natural Language Text.
Cambridge Uni-versity Press, Cambridge, England.George Miller.
1956.
The magical number seven,plus or minus two: Some limits on our capacity forprocessing information.
The psychological review,63:81?97.Maria Milosavljevic.
1999.
Maximising the Coher-ence of Descriptions via Comparison.
Ph.D. thesis,Macquarie University, Sydney, Australia.S.
Muggleton and L. D. Raedt.
1994.
Inductive logicprogramming: Theory and methods.
Journal ofLogic Programming, (19/20):629?679.Alice Oh and A. Rudnicky.
2000.
Stochastic languagegeneration for spoken dialogue systems.
In Pro-ceedings of the ANLP/NAACL 2000 Workshop onConversational Systems, pages 27?32, Seattle, WA,May.Sean Owen, Robin Anil, Ted Dunning, and Ellen Fried-man.
2011.
Mahout in Action.
Manning Publi-cations Co., Manning Publications Co. 20 Baldwin176Road PO Box 261 Shelter Island, NY 11964, firstedition.Kayur Patel, Naomi Bancroft, Steven M Drucker,James Fogarty, Andrew J Ko, and James Landay.2010.
Gestalt: integrated support for implemen-tation and analysis in machine learning.
In Pro-ceedings of the 23nd annual ACM symposium onUser interface software and technology, pages 37?46.
ACM.Mike Reape and Chris Mellish.
1999.
Just whatis aggregation anyway?
In Proceedings of theEuropean Workshop on Natural Language Genera-tion (EWNLG?99), pages 20 ?
29, Toulouse, France,May.Russell D. Reed and Robert J.
Marks.
1999.
NeuralSmithing: Supervised Learning in Feedforward Ar-tificial Neural Networks.
MIT Press.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press.Ehud Reiter, Albert Gatt, Franc?ois Portet and Marianvan der Meulen 2008.
The importance of narrativeand other lessons from an evaluation of an NLG sys-tem that summarises clinical data.
In INLG ?08.Marko Robnik-Sikonja and Igor Kononenko.
2008.Explaining classifications for individual instances.IEEE Trans.
Knowl.
Data Eng., 20(5):589?600.Tjong Kim Sang and Sabine Buchholz.
2000.
Intro-duction to the CoNLL-2000 shared task: Chunking.In Proceedings of the 2nd workshop on Learninglanguage in logic and the 4th conference on Com-putational natural language learning, September,pages 13?14.Sebastian Varges and Chris Mellish.
2010.
Instance-based natural language generation.
Natural Lan-guage Engineering, 16(3):309.177
