Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1?8,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 1: Evaluation of Compositional DistributionalSemantic Models on Full Sentences through Semantic Relatedness andTextual EntailmentMarco Marelli(1)Luisa Bentivogli(2)Marco Baroni(1)Raffaella Bernardi(1)Stefano Menini(1,2)Roberto Zamparelli(1)(1)University of Trento, Italy(2)FBK - Fondazione Bruno Kessler, Trento, Italy{name.surname}@unitn.it, {bentivo,menini}@fbk.euAbstractThis paper presents the task on the evalu-ation of Compositional Distributional Se-mantics Models on full sentences orga-nized for the first time within SemEval-2014.
Participation was open to systemsbased on any approach.
Systems were pre-sented with pairs of sentences and wereevaluated on their ability to predict hu-man judgments on (i) semantic relatednessand (ii) entailment.
The task attracted 21teams, most of which participated in bothsubtasks.
We received 17 submissions inthe relatedness subtask (for a total of 66runs) and 18 in the entailment subtask (65runs).1 IntroductionDistributional Semantic Models (DSMs) approx-imate the meaning of words with vectors sum-marizing their patterns of co-occurrence in cor-pora.
Recently, several compositional extensionsof DSMs (CDSMs) have been proposed, with thepurpose of representing the meaning of phrasesand sentences by composing the distributional rep-resentations of the words they contain (Baroni andZamparelli, 2010; Grefenstette and Sadrzadeh,2011; Mitchell and Lapata, 2010; Socher et al.,2012).
Despite the ever increasing interest in thefield, the development of adequate benchmarks forCDSMs, especially at the sentence level, is stilllagging.
Existing data sets, such as those intro-duced by Mitchell and Lapata (2008) and Grefen-stette and Sadrzadeh (2011), are limited to a fewhundred instances of very short sentences with afixed structure.
In the last ten years, several largeThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/data sets have been developed for various com-putational semantics tasks, such as Semantic TextSimilarity (STS)(Agirre et al., 2012) or Recogniz-ing Textual Entailment (RTE) (Dagan et al., 2006).Working with such data sets, however, requiresdealing with issues, such as identifying multiwordexpressions, recognizing named entities or access-ing encyclopedic knowledge, which have little todo with compositionality per se.
CDSMs shouldinstead be evaluated on data that are challengingfor reasons due to semantic compositionality (e.g.context-cued synonymy resolution and other lexi-cal variation phenomena, active/passive and othersyntactic alternations, impact of negation at vari-ous levels, operator scope, and other effects linkedto the functional lexicon).
These issues do not oc-cur frequently in, e.g., the STS and RTE data sets.With these considerations in mind, we devel-oped SICK (Sentences Involving CompositionalKnowledge), a data set aimed at filling the void,including a large number of sentence pairs thatare rich in the lexical, syntactic and semantic phe-nomena that CDSMs are expected to account for,but do not require dealing with other aspects ofexisting sentential data sets that are not withinthe scope of compositional distributional seman-tics.
Moreover, we distinguished between genericsemantic knowledge about general concept cate-gories (such as knowledge that a couple is formedby a bride and a groom) and encyclopedic knowl-edge about specific instances of concepts (e.g.,knowing the fact that the current president of theUS is Barack Obama).
The SICK data set containsmany examples of the former, but none of the lat-ter.2 The TaskThe Task involved two subtasks.
(i) Relatedness:predicting the degree of semantic similarity be-tween two sentences, and (ii) Entailment: detect-ing the entailment relation holding between them1(see below for the exact definition).
Sentence re-latedness scores provide a direct way to evalu-ate CDSMs, insofar as their outputs are able toquantify the degree of semantic similarity betweensentences.
On the other hand, starting from theassumption that understanding a sentence meansknowing when it is true, being able to verifywhether an entailment is valid is a crucial chal-lenge for semantic systems.In the semantic relatedness subtask, given twosentences, systems were required to produce a re-latedness score (on a continuous scale) indicatingthe extent to which the sentences were expressinga related meaning.
Table 1 shows examples of sen-tence pairs with different degrees of semantic re-latedness; gold relatedness scores are expressed ona 5-point rating scale.In the entailment subtask, given two sentencesA and B, systems had to determine whether themeaning of B was entailed by A.
In particular, sys-tems were required to assign to each pair eitherthe ENTAILMENT label (when A entails B, viz.,B cannot be false when A is true), the CONTRA-DICTION label (when A contradicted B, viz.
B isfalse whenever A is true), or the NEUTRAL label(when the truth of B could not be determined onthe basis of A).
Table 2 shows examples of sen-tence pairs holding different entailment relations.Participants were invited to submit up to fivesystem runs for one or both subtasks.
Developersof CDSMs were especially encouraged to partic-ipate, but developers of other systems that couldtackle sentence relatedness or entailment taskswere also welcome.
Besides being of intrinsic in-terest, the latter systems?
performance will serveto situate CDSM performance within the broaderlandscape of computational semantics.3 The SICK Data SetThe SICK data set, consisting of about 10,000 En-glish sentence pairs annotated for relatedness inmeaning and entailment, was used to evaluate thesystems participating in the task.
The data setcreation methodology is outlined in the followingsubsections, while all the details about data gen-eration and annotation, quality control, and inter-annotator agreement can be found in Marelli et al.
(2014).3.1 Data Set CreationSICK was built starting from two existing datasets: the 8K ImageFlickr data set1and theSemEval-2012 STS MSR-Video Descriptions dataset.2The 8K ImageFlickr dataset is a dataset ofimages, where each image is associated with fivedescriptions.
To derive SICK sentence pairs werandomly chose 750 images and we sampled twodescriptions from each of them.
The SemEval-2012 STS MSR-Video Descriptions data set is acollection of sentence pairs sampled from the shortvideo snippets which compose the Microsoft Re-search Video Description Corpus.
A subset of 750sentence pairs were randomly chosen from thisdata set to be used in SICK.In order to generate SICK data from the 1,500sentence pairs taken from the source data sets, a 3-step process was applied to each sentence compos-ing the pair, namely (i) normalization, (ii) expan-sion and (iii) pairing.
Table 3 presents an exampleof the output of each step in the process.The normalization step was carried out on theoriginal sentences (S0) to exclude or simplify in-stances that contained lexical, syntactic or seman-tic phenomena (e.g., named entities, dates, num-bers, multiword expressions) that CDSMs are cur-rently not expected to account for.The expansion step was applied to each of thenormalized sentences (S1) in order to create up tothree new sentences with specific characteristicssuitable to CDSM evaluation.
In this step syntac-tic and lexical transformations with predictable ef-fects were applied to each normalized sentence, inorder to obtain (i) a sentence with a similar mean-ing (S2), (ii) a sentence with a logically contradic-tory or at least highly contrasting meaning (S3),and (iii) a sentence that contains most of the samelexical items, but has a different meaning (S4) (thislast step was carried out only where it could yielda meaningful sentence; as a result, not all normal-ized sentences have an (S4) expansion).Finally, in the pairing step each normalizedsentence in the pair was combined with all thesentences resulting from the expansion phase andwith the other normalized sentence in the pair.Considering the example in Table 3, S1a and S1bwere paired.
Then, S1a and S1b were each com-bined with S2a, S2b,S3a, S3b, S4a, and S4b, lead-1http://nlp.cs.illinois.edu/HockenmaierGroup/data.html2http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=data2Relatedness score Example1.6A: ?A man is jumping into an empty pool?B: ?There is no biker jumping in the air?2.9A: ?Two children are lying in the snow and are making snow angels?B: ?Two angels are making snow on the lying children?3.6A: ?The young boys are playing outdoors and the man is smiling nearby?B: ?There is no boy playing outdoors and there is no man smiling?4.9A: ?A person in a black jacket is doing tricks on a motorbike?B: ?A man in a black jacket is doing tricks on a motorbike?Table 1: Examples of sentence pairs with their gold relatedness scores (on a 5-point rating scale).Entailment label ExampleENTAILMENTA: ?Two teams are competing in a football match?B: ?Two groups of people are playing football?CONTRADICTIONA: ?The brown horse is near a red barrel at the rodeo?B: ?The brown horse is far from a red barrel at the rodeo?NEUTRALA: ?A man in a black jacket is doing tricks on a motorbike?B: ?A person is riding the bicycle on one wheel?Table 2: Examples of sentence pairs with their gold entailment labels.ing to a total of 13 different sentence pairs.Furthermore, a number of pairs composed ofcompletely unrelated sentences were added to thedata set by randomly taking two sentences fromtwo different pairs.The result is a set of about 10,000 new sen-tence pairs, in which each sentence is contrastedwith either a (near) paraphrase, a contradictory orstrongly contrasting statement, another sentencewith very high lexical overlap but different mean-ing, or a completely unrelated sentence.
The ra-tionale behind this approach was that of buildinga data set which encouraged the use of a com-positional semantics step in understanding whentwo sentences have close meanings or entail eachother, hindering methods based on individual lex-ical items, on the syntactic complexity of the twosentences or on pure world knowledge.3.2 Relatedness and Entailment AnnotationEach pair in the SICK dataset was annotated tomark (i) the degree to which the two sentencemeanings are related (on a 5-point scale), and (ii)whether one entails or contradicts the other (con-sidering both directions).
The ratings were col-lected through a large crowdsourcing study, whereeach pair was evaluated by 10 different subjects,and the order of presentation of the sentences wascounterbalanced (i.e., 5 judgments were collectedfor each presentation order).
Swapping the orderof the sentences within each pair served a two-fold purpose: (i) evaluating the entailment rela-tion in both directions and (ii) controlling pos-sible bias due to priming effects in the related-ness task.
Once all the annotations were collected,the relatedness gold score was computed for eachpair as the average of the ten ratings assigned byparticipants, whereas a majority vote scheme wasadopted for the entailment gold labels.3.3 Data Set StatisticsFor the purpose of the task, the data set was ran-domly split into training and test set (50% and50%), ensuring that each relatedness range and en-tailment category was equally represented in bothsets.
Table 4 shows the distribution of sentencepairs considering the combination of relatednessranges and entailment labels.
The ?total?
column3Original pairS0a: A sea turtle is hunting for fish S0b: The turtle followed the fishNormalized pairS1a: A sea turtle is hunting for fish S1b: The turtle is following the fishExpanded pairsS2a: A sea turtle is hunting for food S2b: The turtle is following the red fishS3a: A sea turtle is not hunting for fish S3b: The turtle isn?t following the fishS4a: A fish is hunting for a turtle in the sea S4b: The fish is following the turtleTable 3: Data set creation process.indicates the total number of pairs in each rangeof relatedness, while the ?total?
row contains thetotal number of pairs in each entailment class.SICK Training Setrelatedness CONTRADICT ENTAIL NEUTRAL TOTAL1-2 range 0 (0%) 0 (0%) 471 (10%) 4712-3 range 59 (1%) 2 (0%) 638 (13%) 6993-4 range 498 (10%) 71 (1%) 1344 (27%) 19134-5 range 155 (3%) 1344 (27%) 352 (7%) 1851TOTAL 712 1417 2805 4934SICK Test Setrelatedness CONTRADICT ENTAIL NEUTRAL TOTAL1-2 range 0 (0%) 1 (0%) 451 (9%) 4522-3 range 59 (1%) 0 (0%) 615(13%) 6743-4 range 496 (10%) 65 (1%) 1398 (28%) 19594-5 range 157 (3%) 1338 (27%) 326 (7%) 1821TOTAL 712 1404 2790 4906Table 4: Distribution of sentence pairs across theTraining and Test Sets.4 Evaluation Metrics and BaselinesBoth subtasks were evaluated using standard met-rics.
In particular, the results on entailment wereevaluated using accuracy, whereas the outputs onrelatedness were evaluated using Pearson correla-tion, Spearman correlation, and Mean Squared Er-ror (MSE).
Pearson correlation was chosen as theofficial measure to rank the participating systems.Table 5 presents the performance of 4 base-lines.
The Majority baseline always assignsthe most common label in the training data(NEUTRAL), whereas the Probability baselineassigns labels randomly according to their rela-tive frequency in the training set.
The Overlapbaseline measures word overlap, again withparameters (number of stop words and EN-TAILMENT/NEUTRAL/CONTRADICTIONthresholds) estimated on the training part of thedata.Baseline Relatedness EntailmentChance 0 33.3%Majority NA 56.7%Probability NA 41.8%Overlap 0.63 56.2%Table 5: Performance of baselines.
Figure of meritis Pearson correlation for relatedness and accuracyfor entailment.
NA = Not Applicable5 Submitted Runs and ResultsOverall, 21 teams participated in the task.
Partici-pants were allowed to submit up to 5 runs for eachsubtask and had to choose the primary run to be in-cluded in the comparative evaluation.
We received17 submissions to the relatedness subtask (for atotal of 66 runs) and 18 for the entailment subtask(65 runs).We asked participants to pre-specify a pri-mary run to encourage commitment to atheoretically-motivated approach, rather thanpost-hoc performance-based assessment.
Inter-estingly, some participants used the non-primaryruns to explore the performance one could reachby exploiting weaknesses in the data that are notlikely to hold in future tasks of the same kind(for instance, run 3 submitted by The MeaningFactory exploited sentence ID ordering informa-tion, but it was not presented as a primary run).Participants could also use non-primary runs totest smart baselines.
In the relatedness subtasksix non-primary runs slightly outperformed theofficial winning primary entry,3while in theentailment task all ECNU?s runs but run 4 werebetter than ECNU?s primary run.
Interestingly,the differences between the ECNU?s runs were3They were: The Meaning Factory?s run3 (Pearson0.84170) ECNU?s runs2 (0.83893) run5 (0.83500) and Stan-fordNLP?s run4 (0.83462) and run2 (0.83103).4due to the learning methods used.We present the results achieved by primary runsagainst the Entailment and Relatedness subtasks inTable 6 and Table 7, respectively.4We witnesseda very close finish in both subtasks, with 4 moresystems within 3 percentage points of the winnerin both cases.
4 of these 5 top systems were thesame across the two subtasks.
Most systems per-formed well above the best baselines from Table5.The overall performance pattern suggests that,owing perhaps to the more controlled nature ofthe sentences, as well as to the purely linguisticnature of the challenges it presents, SICK entail-ment is ?easier?
than RTE.
Considering the firstfive RTE challenges (Bentivogli et al., 2009), themedian values ranged from 56.20% to 61.75%,whereas the average values ranged from 56.45%to 61.97%.
The entailment scores obtained onthe SICK data set are considerably higher, being77.06% for the median system and 75.36% forthe average system.
On the other hand, the re-latedness task is more challenging than the onerun on MSRvid (one of our data sources) at STS2012, where the top Pearson correlation was 0.88(Agirre et al., 2012).6 ApproachesA summary of the approaches used by the sys-tems to address the task is presented in Table 8.In the table, systems in bold are those for whichthe authors submitted a paper (Ferrone and Zan-zotto, 2014; Bjerva et al., 2014; Beltagy et al.,2014; Lai and Hockenmaier, 2014; Alves et al.,2014; Le?on et al., 2014; Bestgen, 2014; Zhao etal., 2014; Vo et al., 2014; Bic?ici and Way, 2014;Lien and Kouylekov, 2014; Jimenez et al., 2014;Proisl and Evert, 2014; Gupta et al., 2014).
For theothers, we used the brief description sent with thesystem?s results, double-checking the informationwith the authors.
In the table, ?E?
and ?R?
referto the entailment and relatedness task respectively,and ?B?
to both.Almost all systems combine several kinds offeatures.
To highlight the role played by com-position, we draw a distinction between compo-sitional and non-compositional features, and di-vide the former into ?fully compositional?
(sys-4ITTK?s primary run could not be evaluated due to tech-nical problems with the submission.
The best ITTK?s non-primary run scored 78,2% accuracy in the entailment task and0.76 r in the relatedness task.ID Compose ACCURACYIllinois-LH run1 P/S 84.6ECNU run1 S 83.6UNAL-NLP run1 83.1SemantiKLUE run1 82.3The Meaning Factory run1 S 81.6CECL ALL run1 80.0BUAP run1 P 79.7UoW run1 78.5Uedinburgh run1 S 77.1UIO-Lien run1 77.0FBK-TR run3 P 75.4StanfordNLP run5 S 74.5UTexas run1 P/S 73.2Yamraj run1 70.7asjai run5 S 69.8haLF run2 S 69.4RTM-DCU run1 67.2UANLPCourse run2 S 48.7Table 6: Primary run results for the entailmentsubtask.
The table also shows whether a sys-tem exploits composition information at either thephrase (P) or sentence (S) level.tems that compositionally computed the meaningof the full sentences, though not necessarily by as-signing meanings to intermediate syntactic con-stituents) and ?partially compositional?
(systemsthat stop the composition at the level of phrases).As the table shows, thirteen systems used compo-sition in at least one of the tasks; ten used compo-sition for full sentences and six for phrases, only.The best systems are among these thirteen sys-tems.Let us focus on such compositional methods.Concerning the relatedness task, the fine-grainedanalyses reported for several systems (Illinois-LH, The Meaning Factory and ECNU) shows thatpurely compositional systems currently reach per-formance above 0.7 r. In particular, ECNU?scompositional feature gives 0.75 r, The MeaningFactory?s logic-based composition model 0.73 r,and Illinois-LH compositional features combinedwith Word Overlap 0.75 r. While competitive,these scores are lower than the one of the best5ID Compose r ?
MSEECNU run1 S 0.828 0.769 0.325StanfordNLP run5 S 0.827 0.756 0.323The Meaning Factory run1 S 0.827 0.772 0.322UNAL-NLP run1 0.804 0.746 0.359Illinois-LH run1 P/S 0.799 0.754 0.369CECL ALL run1 0.780 0.732 0.398SemantiKLUE run1 0.780 0.736 0.403RTM-DCU run1 0.764 0.688 0.429UTexas run1 P/S 0.714 0.674 0.499UoW run1 0.711 0.679 0.511FBK-TR run3 P 0.709 0.644 0.591BUAP run1 P 0.697 0.645 0.528UANLPCourse run2 S 0.693 0.603 0.542UQeResearch run1 0.642 0.626 0.822ASAP run1 P 0.628 0.597 0.662Yamraj run1 0.535 0.536 2.665asjai run5 S 0.479 0.461 1.104Table 7: Primary run results for the relatednesssubtask (r for Pearson and ?
for Spearman corre-lation).
The table also shows whether a system ex-ploits composition information at either the phrase(P) or sentence (S) level.purely non-compositional system (UNAL-NLP)which reaches the 4th position (0.80 r UNAL-NLPvs.
0.82 r obtained by the best system).
UNAL-NLP however exploits an ad-hoc ?negation?
fea-ture discussed below.In the entailment task, the best non-compositional model (again UNAL-NLP)reaches the 3rd position, within close reach of thebest system (83% UNAL-NLP vs. 84.5% obtainedby the best system).
Again, purely compositionalmodels have lower performance.
haLF CDSMreaches 69.42% accuracy, Illinois-LH WordOverlap combined with a compositional featurereaches 71.8%.
The fine-grained analysis reportedby Illinois-LH (Lai and Hockenmaier, 2014)shows that a full compositional system (basedon point-wise multiplication) fails to capturecontradiction.
It is better than partial phrase-basedcompositional models in recognizing entailmentpairs, but worse than them on recognizing neutralpairs.Given our more general interest in the distri-butional approaches, in Table 8 we also classifythe different DSMs used as ?Vector Space Mod-els?, ?Topic Models?
and ?Neural Language Mod-els?.
Due to the impact shown by learning methods(see ECNU?s results), we also report the differentlearning approaches used.Several participating systems deliberately ex-ploit ad-hoc features that, while not helping a trueunderstanding of sentence meaning, exploit somesystematic characteristics of SICK that should becontrolled for in future releases of the data set.In particular, the Textual Entailment subtask hasbeen shown to rely too much on negative wordsand antonyms.
The Illinois-LH team reports that,just by checking the presence of negative words(the Negation Feature in the table), one can detect86.4% of the contradiction pairs, and by combin-ing Word Overlap and antonyms one can detect83.6% of neutral pairs and 82.6% of entailmentpairs.
This approach, however, is obviously verybrittle (it would not have been successful, for in-stance, if negation had been optionally combinedwith word-rearranging in the creation of S4 sen-tences, see Section 3.1 above).Finally, Table 8 reports about the use of externalresources in the task.
One of the reasons we cre-ated SICK was to have a compositional semanticsbenchmark that would not require too many ex-ternal tools and resources (e.g., named-entity rec-ognizers, gazetteers, ontologies).
By looking atwhat the participants chose to use, we think wesucceeded, as only standard NLP pre-processingtools (tokenizers, PoS taggers and parsers) and rel-atively few knowledge resources (mostly, Word-Net and paraphrase corpora) were used.7 ConclusionWe presented the results of the first task on theevaluation of compositional distributional seman-tic models and other semantic systems on full sen-tences, organized within SemEval-2014.
Two sub-tasks were offered: (i) predicting the degree of re-latedness between two sentences, and (ii) detect-ing the entailment relation holding between them.The task has raised noticeable attention in thecommunity: 17 and 18 submissions for the relat-edness and entailment subtasks, respectively, for atotal of 21 participating teams.
Participation wasnot limited to compositional models but the major-ity of systems (13/21) used composition in at leastone of the subtasks.
Moreover, the top-rankingsystems in both tasks use compositional features.However, it must be noted that all systems also ex-6Participant ID Non composition features Comp features Learning Methods External ResourcesVector Semantics ModelTopicModelNeural Language ModelDenotationalModelWordOverlapWordSimilaritySyntactic FeaturesSentence differenceNegation FeaturesSentence CompositionPhrase compositionSVMand KernelmethodsK-NearestNeighboursClassifier CombinationRandomForestFoL/ProbabilisticFoLCurriculumbased learningOtherWordNetParaphrases DBOther CorporaImageFlickerSTS MSR-VideoDescriptionASAP R R R R R R R R RASJAI B B B B B B B B E B R BBUAP B B B B E B E BUEdinburgh B B B B B E R BCECL B B B B B BECNU B B B B B B B B B B B B BFBK-TR R R R E B E E B R E R R EhaLF E E E EIITK B B B B B B B B BIllinois-LH B B B B B B B B B B B BRTM-DCU B B B B BSemantiKLUE B B B B B B B BStandfordNLP B B R R R B EThe Meaning Factory R R R R R R B E R E B B RUANLPCourse B B B B BUIO-Lien E EUNAL-NLP B B B B R B BUoW B B B B B BUQeRsearch R R R R R R RUTexas B B B B B B BYamarj B B B BTable 8: Summary of the main characteristics of the participating systems on R(elatedness), E(ntailment)or B(oth)ploit non-compositional features and most of themuse external resources, especially WordNet.
Al-most all the participating systems outperformedthe proposed baselines in both tasks.
Further anal-yses carried out by some participants in the taskshow that purely compositional approaches reachaccuracy above 70% in entailment and 0.70 r forrelatedness.
These scores are comparable with theaverage results obtained in the task.AcknowledgmentsWe thank the creators of the ImageFlickr, MSR-Video, and SemEval-2012 STS data sets for grant-ing us permission to use their data for the task.
TheUniversity of Trento authors were supported byERC 2011 Starting Independent Research Grantn.
283554 (COMPOSES).ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A pi-lot on semantic textual similarity.
In Proceedings ofthe Sixth International Workshop on Semantic Eval-uation (SemEval 2012), volume 2.Ana O. Alves, Adirana Ferrugento, Mariana Lorenc?o,and Filipe Rodrigues.
2014.
ASAP: Automatica se-mantic alignment for phrases.
In Proceedings of Se-mEval 2014: International Workshop on SemanticEvaluation.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Islam Beltagy, Stephen Roller, Gemma Boleda, KatrinErk, and Raymon J. Mooney.
2014.
UTexas: Nat-ural language semantics using distributional seman-tics and probablisitc logic.
In Proceedings of Se-mEval 2014: International Workshop on SemanticEvaluation.7Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-ampiccolo, and Bernardo Magnini.
2009.
The fifthPASCAL recognizing textual entailment challenge.In The Text Analysis Conference (TAC 2009).Yves Bestgen.
2014.
CECL: a new baseline and a non-compositional approach for the Sick benchmark.
InProceedings of SemEval 2014: International Work-shop on Semantic Evaluation.Ergun Bic?ici and Andy Way.
2014.
RTM-DCU: Ref-erential translation machines for semantic similar-ity.
In Proceedings of SemEval 2014: InternationalWorkshop on Semantic Evaluation.Johannes Bjerva, Johan Bos, Rob van der Goot, andMalvina Nissim.
2014.
The Meaning Factory: For-mal Semantics for Recognizing Textual Entailmentand Determining Semantic Similarity.
In Proceed-ings of SemEval 2014: International Workshop onSemantic Evaluation.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Machine learning challenges.
Evalu-ating predictive uncertainty, visual object classifica-tion, and recognising textual entailment, pages 177?190.
Springer.Lorenzo Ferrone and Fabio Massimo Zanzotto.
2014.haLF:comparing a pure CDSM approach and a stan-dard ML system for RTE.
In Proceedings of Se-mEval 2014: International Workshop on SemanticEvaluation.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of EMNLP, pages 1394?1404, Edinburgh, UK.Rohit Gupta, Ismail El Maarouf Hannah Bechara, andCostantin Oras?an.
2014.
UoW: NLP techniques de-veloped at the University of Wolverhampton for Se-mantic Similarity and Textual Entailment.
In Pro-ceedings of SemEval 2014: International Workshopon Semantic Evaluation.Sergio Jimenez, George Duenas, Julia Baquero, andAlexander Gelbukh.
2014.
UNAL-NLP: Combin-ing soft cardinality features for semantic textual sim-ilarity, relatedness and entailment.
In Proceedingsof SemEval 2014: International Workshop on Se-mantic Evaluation.Alice Lai and Julia Hockenmaier.
2014.
Illinois-lh: Adenotational and distributional approach to seman-tics.
In Proceedings of SemEval 2014: InternationalWorkshop on Semantic Evaluation.Sa?ul Le?on, Darnes Vilarino, David Pinto, Mireya To-var, and Beatrice Beltr?an.
2014.
BUAP:evaluatingcompositional distributional semantic models on fullsentences through semantic relatedness and textualentailment.
In Proceedings of SemEval 2014: Inter-national Workshop on Semantic Evaluation.Elisabeth Lien and Milen Kouylekov.
2014.
UIO-Lien: Entailment recognition using minimal recur-sion semantics.
In Proceedings of SemEval 2014:International Workshop on Semantic Evaluation.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zam-parelli.
2014.
A SICK cure for the evaluation ofcompositional distributional semantic models.
InProceedings of LREC, Reykjavik.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL, pages 236?244, Columbus, OH.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Thomas Proisl and Stefan Evert.
2014.
SemantiK-LUE: Robust semantic similarity at multiple levelsusing maximum weight matching.
In Proceedings ofSemEval 2014: International Workshop on SemanticEvaluation.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.An N. P. Vo, Octavian Popescu, and Tommaso Caselli.2014.
FBK-TR: SVM for Semantic Relatedness andCorpus Patterns for RTE.
In Proceedings of Se-mEval 2014: International Workshop on SemanticEvaluation.Jiang Zhao, Tian Tian Zhu, and Man Lan.
2014.ECNU: One Stone Two Birds: Ensemble of Het-erogenous Measures for Semantic Relatedness andTextual Entailment.
In Proceedings of SemEval2014: International Workshop on Semantic Evalu-ation.8
