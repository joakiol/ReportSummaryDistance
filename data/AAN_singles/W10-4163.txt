Overview of the Chinese Word Sense Induction Task at CLP2010Le SunInstitute of SoftwareChinese Academy ofSciencessunle@iscas.ac.cnZhenzhong ZhangInstitute of Software, GraduateUniversity Chinese Academy ofScienceszhenzhong@nfs.iscas.ac.cnQiang DongCanada Keentime Inc.dongqiang@keenage.comAbstractIn this paper, we describe the Chineseword sense induction task at CLP2010.Seventeen teams participated in this taskand nineteen system results weresubmitted.
All participant systems areevaluated on a dataset containing 100target words and 5000 instances usingthe standard cluster evaluation.
We willdescribe the participating systems andthe evaluation results, and then find themost suitable method by comparing thedifferent Chinese word sense inductionsystems.1 IntroductionWord Sense Disambiguation (WSD) is animportant task in natural language proceedingresearch and is critical to many applicationswhich require language understanding.
Intraditional evaluations, the supervised methodsusually can achieve a better WSD performancethan the unsupervised methods.
But thesupervised WSD methods have some drawbacks:Firstly, they need large annotated dataset whichis expensive to manually annotate (Agirre andAitor, 2007).
Secondly, the supervised WSDmethods   are based on the ?fixed-list ofsenses?
paradigm, i.e., the senses of a targetword are represented as a closed list comingfrom a manually constructed dictionary (Agirreet al, 2006).
Such a ?Fixed-list of senses?paradigm suffers from the lack of explicit andtopic relations between word senses, are usuallycannot reflect the exact context of the targetword (Veronis, 2004).
Furthermore, because the?fixed-list of senses?
paradigm make the fixgranularity assumption of the senses distinction,it may not be suitable in different situations(Samuel and Mirella, 2009).
Thirdly, since mostsupervised WSD methods assign senses basedon dictionaries or other lexical resources, it willbe difficult to adapt them to new domains orlanguages when such resources are scare(Samuel and Mirella, 2009).To overcome the deficiencies of thesupervised WSD methods, many unsupervisedWSD methods have been developed in recentyears, which can induce word senses directlyfrom the unannotated dataset, i.e., Word SenseInduction (WSI).
In this sense, WSI could betreat as a clustering task, which groups theinstances of the target word according to theircontextual similarity, with each resulting clustercorresponding to a specific ?word sense?
or?word use?
of the target word (in the task ofWSI, the term ?word use?
is more suitable than?word sense?
(Agirre and Aitor, 2007)).Although traditional clustering techniques canbe directly employed in WSI, in recent yearssome new methods have been proposed toenhance the WSI performance, such as theBayesian approach (Samuel and Mirella, 2009)and the collocation graph approach (Ioannis andSuresh, 2008).
Both the traditional and the newmethods can achieve a good performance in thetask of English word sense induction.
However,the methods work well in English may not besuitable for Chinese due to the differencebetween Chinese and English.
So it is bothimportant and critical to provide a standardtestbed for the task of Chinese word senseinduction (CWSI), in order to compare theperformance of different Chinese WSI methodsand find the methods which are suitable for theChinese word sense induction task.In this paper, we describe the Chinese wordsense induction task at CLP2010.
The goal ofthis task is to provide a standard testbed forChinese WSI task.
By comparing the differentChinese WSI methods, we can find the suitablemethods for the Chinese word sense inductiontask.This paper is organized as follow.
Section 2describes the evaluation dataset in detail.
Section3 demonstrates the evaluation criteria.
Section 3describes the participated systems and theirresults.
The conclusions are drawn in section 4.2 DatasetTwo datasets are provided to the participants:the trial dataset and the test dataset.The trial dataset contains 50 Chinese words,and for each Chinese word, a set of 50 wordinstances are provided.
All word instances areextracted from the Web and the newspapers likethe Xinhua newspaper and the Renminnewspaper, and the HowNet senses of targetwords were manually annotated (Dong).
Figure1 shows an example of the trial data withouthand-annotated tag.
Figure 2 shows an exampleof the trial data with hand-annotated tag.
InFigure 1, the tag ?snum=2?
indicates that thetarget word ????
has two different senses inthis dataset.
In each instance, the target word ismarked between the tag ?<head>?
and the tag?</head>?.
In Figure 2, all instances between thetag ?<sense s=S0>?
and the tag ?</sense>?
arebelong to the same sense class.Figure 1: Example of the trial data withouthand-annotated tag.The case of the test dataset is similar to thetrial dataset, but with little different in thenumber of target words.
The test dataset contains100 target words (22 Chinese words containingone Chinese character and 78 Chinese wordscontaining two or more Chinese ideographs).Figure 3 shows an example of a system?s output.In Figure 3, the first column represents theidentifiers of target word, the second columnrepresents the identifiers of instances, and thethird column represents the identifiers of theresulting clusters and their weight (1.0 by default)generated by Chinese WSI systems.Figure 2: Example of the trial data withhand-annotated tag.Figure 3: Example of the output format.3 Evaluation MetricAs described in Section 1, WSI could beconceptualized as a clustering problem.
So wecan measure the performance of WSI systemsusing the standard cluster evaluation metrics.
Asthe same as Zhao and Karypis(2005), we use theFScore measure as the primary measure forassessing different WSI methods.
The FScore isused in a similar way as at Information Retrievalfield.In this case, the results of the WSI systems aretreated as clusters of instances and the goldstandard senses are classes.
Then the precisionof a class with respect to a cluster is defined asthe number of their mutual instances divided bythe total cluster size, and the recall of a classwith respect to a cluster is defined as the numberof their mutual instances divided by the totalclass size.
The detailed definition is as bellows.Let the size of a particular class sr is nr, thesize of a particular cluster hj is nj and the size oftheir common instances set is nr,j.,then theprecision can be defined as:,( , ) r jr jjnP s hn=The recall can be defined as:,( , ) r jr jrnR s hn=Then FScore of this class and cluster is definedto be:2 ( , ) ( , )( , )( , ) ( , )r j r jr jr j r jP s h R s hF s hP s h R s h?
?= +The FScore of a class sr, F(sr), is the maximumF(sr, hj) value attained by any cluster, and it isdefined as:( ) max( ( , ))jr r jhF s F s h=Finally, the FScore of the entire clusteringsolution is defined as the weighted averageFScore of all class:1( )q r rrn F sFScoren=?=?where q is the number of classes and n is the sizeof the instance set for particular target word.Table 1 shows an example of a contingencytable of classes and clusters, which can be usedto calculate FScore.Cluster 1 Cluster 2Class 1 100 500Class 2 400 200Table 1: A contingency table of classes andclustersUsing this contingency table, we can calculatethe FScore of this example is 0.7483.
It is easyto know the FScore of a perfect clusteringsolution will be equal to one, where each clusterhas exactly the same instances as one of theclasses, and vice versa.
This means that thehigher the FScore, the better the clusteringperformance.Purity and entropy (Zhao and Karypis, 2005)are also used to measure the performance of theclustering solution.
Compared to FScore, theyhave some disadvantages.
FScore uses twocomplementary concepts, precision and recall, toassess the quality of a clustering solution.Precision indicates the degree of the instancesthat make up a cluster, which belong to a singleclass.
On the other hand, recall indicates thedegree of the instances that make up a class,which belong to a single cluster.
But purity andentropy only consider one factor and discardanother.
So we use FScore measure to assess aclustering solution.For the sake of completeness, we also employthe V-Measure to assess different clusteringsolutions.
V-Measure assesses a cluster solutionby considering its homogeneity and itscompleteness (Rosenberg and Hirschberg, 2007).Homogeneity measures the degree that eachcluster contains data points which belong to asingle Gold Standard class.
And completenessmeasures the degree that each Gold Standardclass contains data points assigned to a singlecluster (Rosenberg and Hirschberg, 2007).
Ingeneral, the larger the V-Measure, the better theclustering performance.
More details can bereferred to (Rosenberg and Hirschberg, 2007).4 ResultsIn this section we describe the participantsystems and present their results.Since the size of test data may not be largeenough to distinguish word senses, participantswere provided the total number of the targetword?s senses.
And participants were alsoallowed to use extra resources withouthand-annotated.4.1 Participant teams and systemsThere were 17 teams registered for the WSI taskand 12 teams submitted their results.
Totally 19participant system results were submitted (Onewas submitted after the deadline).
10 teamssubmitted their technical reports.
Table 2demonstrates the statistics of the participantinformation.The methods used by the participated systemswere described as follows:FDU: This system first extracted the tripletsfor target word in each instance and got theintersection of all related words of these tripletsusing Baidu web search engine.
Then the tripletsand their corresponding intersections were usedto construct feature vectors of the target word?sinstances.
After that, sequential InformationBottleneck algorithm was used to groupinstances into clusters.BUPT: Three clustering algorithms- thek-means algorithm, the Expectation-maximization algorithm and the LocallyAdaptive Clustering algorithm were employed tocluster instances, where all instances wererepresented using some combined features.
Inthe end the Group-average agglomerativeclustering was used to cluster the consensusmatrix M, which was obtained from theName of Participant Team Result ReportNatural Language Processing Laboratory at Northeastern University (NEU) ?
?Beijing University of Posts and Telecommunications (BUPT) ?
?Beijing Institute of Technology (BIT) ?Shanghai Jiao Tong University (SJTU)Laboratory of Intelligent Information Processing and ApplicationInstitutional at Leshan Teachers?
College (LSTC)?
?Natural Language Processing Laboratory at Soochow University (SCU) ?
?Fudan University (FDU) ?
?Institute of Computational Linguistics at Peking University 1 (PKU1) ?
?Beijing University of Information Science and Technology (BUIST) ?Tsinghua University Research Institute of Information Technology,Speech and Language Technologies R&D Center (THU)Information Retrieval Laboratory at Dalian University of Technology(DLUT)?
?Institute of Computational Linguistics at Peking University 2 (PKU2) ?
?City University of HK (CTU)Institute of Software Chinese Academy of Sciences (ISCAS) ?
?Cognitive Science Department at Xiamen University (XMU) ?
?Harbin Institute of Technology Shenzhen Graduate School (HITSZGS)National Taipei University of Technology (NTUT)Table 2: The registered teams.
???
means that the team submitted the result or the report.adjacency matrices of the individual clustersgenerated by the three single clusteringalgorithms mentioned above.LSTC: This team extracted the five neighborwords and their POSs around the target word asfeatures.
Then the k-means algorithm was usedto cluster the instances of each target word.NEU: The ?Global collocation?
and the?local collocation?
were extracted as features.
Aconstraint hierarchical clustering algorithm wasused to cluster the instances of each targetword.XMU: The neighbor words of the targetword were extracted as features and TongYiCiCiLin1 was employed to measure the similaritybetween instances.
The word instances are????????????????????????????????????????
?????????????????????1?
http://www.ir?lab.org/?clustered using the improved hierarchicalclustering algorithm based on parts of speech.DLUT: This team used the information gainto determine the size of the feature window.TongYiCi CiLin was used to solve the datasparseness problem.
The word instances areclustered using an improvement k-meansalgorithm where k-initial centers were selectedbased on maximum distance.ISCAS: This team employed k-meansclustering algorithm to cluster the second orderco-occurrence vectors of contextual words.TongYiCi CiLin and singular valuedecomposition method were used to solve theproblem of data sparseness.
Please note that thissystem was submitted by the organizers.
Theorganizers have taken great care in order toguaranty all participants are under the sameconditions.PKU2: This team used local tokens, localbigram feature and topical feature to representwords as vectors.
Spectral clustering methodwas used to cluster the instances of each targetword.PKU1: This team extracted three types offeatures to represent instances as feature vectors.Then the clustering was done by using k-meansalgorithm.SCU: All words except stop words ininstances were extracted to produce the featurevectors, based on which the similarity matrixwere generated.
After that, the spectralclustering algorithm was applied to groupinstances into clusters.4.2 Official ResultsIn this section we present the official results ofthe participant systems (ISCAS* was submittedby organizers; BUIST** was submitted after thedeadline).
We also provide the result of abaseline -- 1c1w, which group all instances of atarget word into a single cluster.Table 3 shows the FScore of the mainsystems submitted by participant teams on thetest dataset.
Table 4 shows the FScore andV-Measure of all participant systems.
Systemswere ranked according to their FScore.Systems Rank FScoreBUPT_mainsys 1 0.7933PKU1_main_system 2 0.7812FDU 3 0.7788DLUT_main_system 4 0.7729PKU2 5 0.7598ISCAS* 6 0.7209SCU 7 0.7108NEU_WSI_1 8 0.6715XMU 9 0.6534BIT 10 0.63661c1w 11 0.6147BUIST** 12 0.5972LSTC 13 0.5789Table 3: FScore of main systems on the testdataset including one baseline -1c1w.Systems Rank FScore V-MeasureBUPT_mainsys 1 0.7933 0.4628BUPT_LAC 2 0.7895 0.4538BUPT_EM 3 0.7855 0.4356BUPT_kmeans 4 0.7849 0.4472PKU1_main_system 5 0.7812 0.4300FDU 6 0.7788 0.4196DLUT_main_system 7 0.7729 0.5032PKU1_agglo 8 0.7651 0.4096PKU2 9 0.7598 0.4078ISCAS* 10 0.7209 0.3174SCU 11 0.7108 0.3131NEU_WSI_1 12 0.6715 0.2331XMU 13 0.6534 0.1954NEU_WSI_0 14 0.6520 0.1947BIT 15 0.6366 0.17131c1w 16 0.6147 0.0DLUT_RUN2 17 0.6067 0.1192BUIST** 18 0.5972 0.1014DLUT_RUN3 19 0.5882 0.0906LSTC 20 0.5789 0.0535Table 4: FScore and V-Measure of all systems,including one baseline.From the results shown in Table 3 and 4, wecan see that:1)  As described in section 4.1, mostsystems use traditional clusteringmethods.
For example, the teams usingthe k-means algorithm contain BUPT,LSTC, PKU1, DLUT and ISCAS.
Theteams using the spectral clusteringalgorithm contain SCU and PKU2.
Theteam XMU and NEU use hierarchicalclustering algorithm.
The results showsthat if provided with the number oftarget word senses, traditional methodscan achieve a good performance.
But wealso notice that even the same methodcan have a different performance.
Thisseems to indicate that features which arepredictive of word senses are importantto the task of CWSI.2)  Most systems outperform the 1c1wbaseline, which indicates these systemsare able to induce correct senses oftarget words to some extent.3)  The rank of FScore is much the same asthat of V-Measure but with littledifference.
This may be because that thetwo evaluation measures both assessquality of a clustering solution byconsidering two different aspects, whereprecision corresponds to homogeneityand recall corresponds to completeness.But when assessing the quality of aclustering solution, the FScore onlyconsiders the contributions from theclasses which are most similar to theclusters while the V-Measure considersthe contributions from all classes.Systems Characters WordsBUPT_mainsys 0.6307 0.8392BUPT_LAC 0.6298 0.8346BUPT_EM 0.6191 0.8324BUPT_kmeans 0.6104 0.8341PKU1_main_system 0.6291 0.8240FDU 0.6964 0.8020DLUT_main_system 0.5178 0.8448PKU1_agglo 0.5946 0.8132PKU2 0.6157 0.8004ISCAS* 0.5639 0.7651SCU 0.5715 0.7501NEU_WSI_1 0.5786 0.6977XMU 0.5290 0.6885NEU_WSI_0 0.5439 0.6825BIT 0.5328 0.6659DLUT_RUN2 0.5196 0.6313BUIST** 0.5022 0.6240DLUT_RUN3 0.5066 0.6113LSTC 0.4648 0.61101c1w 0.4611 0.6581Table 5: FScore of all systems on the datasetonly containing either single characters orwords respectively.A Chinese word can be constituted by singleor multiple Chinese characters.
Senses ofChinese characters are usually determined bythe words containing the character.
In order tocompare the WSI performance on differentgranularity of words, we add 22 Chinesecharacters into the test corpus.
Table 5 showsthe results of the participant systemscorrespondingly on the corpus which onlycontains the 22 Chinese characters and thecorpus which only contains the 78 Chinesewords.From Table 5, we can see that:1) The FScore of systems on the corpusonly containing single characters issignificantly lower than that on thecorpus only containing words.
Webelieve this is because: 1) The SingleChinese characters usually containsmore senses than Chinese words; 2)Their senses are not determined directlyby their contexts but by the wordscontaining them.
Compared to thenumber of instances, the number ofwords containing the single character islarge.
So it is difficult to distinguishdifferent senses of single charactersbecause of the data sparseness.2) We noticed that all systems outperformthe 1c1w baseline on the corpus onlycontaining single characters but thereare some systems?
FScore are lowerthan the baseline on the corpus onlycontaining words.
It may be because thelarge number of characters?
senses andthe FScore favored the words whichhave small number of senses.5 ConclusionsIn this paper we describe the design and theresults of CLP2010 back-off task 4-Chineseword sense induction task.
17 teams registeredto this task and 12 teams submitted their results.In total there were 19 participant systems (Oneof them was submitted after the deadline).
And10 teams submitted their technical reports.
Allsystems are evaluated on a corpus containing100 target words and 5000 instances usingFScore measure and V-Measure.
Participantsare also provided with the number of senses andallowed to use resources withouthand-annotated.The evaluation results have shown that mostof the participant systems achieve a betterperformance than the 1c1w baseline.
We alsonotice that it is more difficult to distinguishsenses of Chinese characters than words.
Forfuture work, in order to test the performances ofChinese word sense induction systems underdifferent conditions, corpus from differentfields will be constructed and the number oftarget word senses will not be provided and willleave as an open task to the participant systems.AcknowledgmentsThis work has been partially funded by NationalNatural Science Foundation of China undergrant #60773027, #60736044 and #90920010and by ?863?
Key Projects #2006AA010108,?863?
Projects #2008AA01Z145.
We wouldlike to thank Dr. Han Xianpei and Zhang Weirufor their detailed comments.
We also want tothank the annotators for their hard work onpreparing the trial and test dataset.ReferencesAndrew Rosenberg and Julia Hirschberg.
2007.V-Measure: A conditional entropy-based externalcluster evaluation measure.
In Proceedings of the2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL),pages 410?420.Eneko Agirre, David Mart?
?nez, Oier L?opez deLacalle,and Aitor Soroa.
2006.
Two graph-basedalgorithms for state-of-the-art WSD.
InProceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages585?593, Sydney, Australia.Eneko Agirre and Aitor Soroa.
2007.
Semeval-2007task2: Evaluating word sense induction anddiscrimination systems.
In Proceedings ofSemEval-2007.
Association for ComputationalLlinguistics, pages 7-12, Prague.Ioannis P. Klapaftis and Suresh Manandhar, 2008.Word Sense Induction Using Graphs ofCollocations.
In Proceeding of the 2008conference on 18th European Conference onArtificial Intelligence, Pages: 298-302.Jean.
V?eronis.
2004.
Hyperlex: lexical cartographyfor information retrieval.
Computer Speech &Language,18(3):223.252.Samuel Brody and Mirella Lapata, 2009.
Bayesianword sense induction.
In Proceedings of the 12thConference of the European Chapter of theAssociation for Computational Linguistics, pages103-111, Athens, Greece.Ying Zhao and George Karypis.
2005.
Hierarchicalclustering algorithms for document datasets.
DataMining and Knowledge Discovery,10(2):141.168.Zhendong  Dong,http://www.keenage.com/zhiwang/e_zhiwang.html
