Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1191?1200, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLeft-to-Right Tree-to-String Decoding with PredictionYang Feng?
Yang Liu?
Qun Liu?
Trevor Cohn??
Department of Computer ScienceThe University of Sheffield, Sheffield, UK{y.feng, t.cohn}@sheffield.ac.uk?
State Key Laboratory on Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Sci.
and Tech., Tsinghua University, Beijing, Chinaliuyang2011@tsinghua.edu.cn?Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciences, Beijing, Chinaliuqun@ict.ac.cnAbstractDecoding algorithms for syntax based ma-chine translation suffer from high compu-tational complexity, a consequence of in-tersecting a language model with a con-text free grammar.
Left-to-right decoding,which generates the target string in order,can improve decoding efficiency by simpli-fying the language model evaluation.
Thispaper presents a novel left to right decod-ing algorithm for tree-to-string translation, us-ing a bottom-up parsing strategy and dynamicfuture cost estimation for each partial trans-lation.
Our method outperforms previouslypublished tree-to-string decoders, including acompeting left-to-right method.1 IntroductionIn recent years there has been rapid progress in thedevelopment of tree-to-string models for statisticalmachine translation.
These models use the syntac-tic parse tree of the source language to inform itstranslation, which allows the models to capture con-sistent syntactic transformations between the sourceand target languages, e.g., from subject-verb-objectto subject-object-verb word orderings.
Decoding al-gorithms for grammar-based translation seek to findthe best string in the intersection between a weightedcontext free grammar (the translation mode, given asource string/tree) and a weighted finite state accep-tor (an n-gram language model).
This intersectionis problematic, as it results in an intractably largegrammar, and makes exact search impossible.Most researchers have resorted to approximatesearch, typically beam search (Chiang, 2007).
Thedecoder parses the source sentence, recording thetarget translations for each span.1 As the partialtranslation hypothesis grows, its component ngramsare scored and the hypothesis score is updated.
Thisdecoding method though is inefficient as it requiresrecording the language model context (n?
1 words)on the left and right edges of each chart cell.
Thesecontexts allow for boundary ngrams to be evaluatedwhen the cell is used in another grammar produc-tion.
In contrast, if the target string is generatedin left-to-right order, then only one language modelcontext is required, and the problem of languagemodel evaluation is vastly simplified.In this paper, we develop a novel method of left-to-right decoding for tree-to-string translation usinga shift-reduce parsing strategy.
A central issue inany decoding algorithm is the technique used forpruning the search space.
Our left-to-right decod-ing algorithm groups hypotheses, which cover thesame number of source words, into a bin.
Pruningrequires the evaluation of different hypotheses in thesame bin, and elimating the least promising options.As each hypotheses may cover different sets of tree1The process is analogous for tree-to-string models, exceptthat only rules and spans matching those in the source trees areconsidered.
Typically nodes are visited according to a post-order traversal.1191nodes, it is necessary to consider the cost of uncov-ered nodes, i.e., the future cost.
We show that a goodfuture cost estimate is essential for accurate and effi-cient search, leading to high quality translation out-put.Other researchers have also considered the left-to-right decoding algorithm for tree-to-string mod-els.
Huang and Mi (2010) developed an Earley-style parsing algorithm (Earley, 1970).
In their ap-proach, hypotheses covering the same number oftree nodes were binned together.
Their method usesa top-down depth-first search, with a mechanism forearly elimation of some rules which lead to dead-ends in the search.
Huang and Mi (2010)?s methodwas shown to outperform the traditional post-order-traversal decoding algorithm, considering fewer hy-potheses and thus decoding much faster at the samelevel of performance.
However their algorithm useda very rough estimate of future cost, resulting inmore search errors than our approach.Our experiments show that compared with theEarley-style left-to-right decoding (Huang and Mi,2010) and the traditional post-order-traversal de-coding (Liu et al 2006) algorithms, our algorithmachieves a significant improvement on search capac-ity and better translation performance at the samelevel of speed.2 BackgroundA typical tree-to-string system (Liu et al 2006;Huang et al 2006) searches through a 1-best sourceparse tree for the best derivation.
It transduces thesource tree into a target-language string using a Syn-chronous Tree Substitution Grammar (STSG).
Thegrammar rules are extracted from bilingual wordalignments using the GHKM algorithm (Galley etal., 2004).We will briefly review the traditional decoding al-gorithm (Liu et al 2006) and the Earley-style top-down decoding algorithm (Huang and Mi, 2010) forthe tree-to-string model.2.1 Traditional DecodingThe traditional decoding algorithm processes sourcetree nodes one by one according to a post-ordertraversal.
For each node, it applies matched STSGrules by substituting each non-terminal with its cor-in theory beam searchtraditional O(nc|?V |4(g?1)) O(ncb2)top-down O(c(cr)d|V |g?1) O(ncb)bottom-up O((cr)d|V |g?1) O(nub)Table 1: Time complexity of different algorithms.
tra-ditional : Liu et al(2006), top-down : Huang and Mi(2010).
n is the source sentence length, b is the beamwidth, c is the number of rules used for each node, Vis the target word vocabulary, g is the order of the lan-guage model, d is the depth of the source parse tree, u isthe number of viable prefixes for each node and r is themaximum arity of each rule.responding translation.
For the derivation in Figure1 (b), the traditional algorithm applies r2 at nodeNN2r2 : NN2 (jieguo) ?
the result,to obtain ?the result?
as the translation of NN2.
Nextit applies r4 at node NP,r4 : NP ( NN1 (toupiao), x1 : NN2 )?
x1 of the voteand replaces NN2 with its translation ?the result?,then it gets the translation of NP as ?the result of thevote?.This algorithm needs to contain boundary wordsat both left and right extremities of the target stringfor the purpose of LM evaluation, which leads to ahigh time complexity.
The time complexity in the-ory and with beam search (Huang and Mi, 2010) isshown in Table 1.2.2 Earley-style Top-down DecodingThe Earley-style decoding algorithm performs a top-down depth-first parsing and generates the targettranslation left to right.
It applies Context-FreeGrammar (CFG) rules and employs three actions:predict, scan and complete (Section 3.1 describeshow to convert STSG rules into CFG rules).
We cansimulate its translation process using a stack with adot  indicating which symbol to process next.
Forthe derivation in Figure 1(b) and CFG rules in Fig-ure 1(c), Figure 2 illustrates the whole translationprocess.The time complexity is shown in Table 1 .11923 Bottom-Up Left-to-Right DecodingWe propose a novel method of left-to-right decodingfor tree-to-string translation using a bottom-up pars-ing strategy.
We use viable prefixes (Aho and John-son, 1974) to indicate all possible target strings thetranslations of each node should starts with.
There-fore, given a tree node to expand, our algorithmcan drop immediately to target terminals no matterwhether there is a gap or not.
We say that there is agap between two symbols in a derivation when thereare many rules separating them, e.g.
IP r6?
... r4?NN2.
For the derivation in Figure 1(b), our algo-rithm starts from the root node IP and applies r2first although there is a gap between IP and NN2.Then it applies r4, r5 and r6 in sequence to generatethe translation ?the result of the vote was releasedat night?.
Our algorithm takes the gap as a black-box and does not need to fix which partial deriva-tion should be used for the gap at the moment.
So itcan get target strings as soon as possible and therebyperform more accurate pruning.
A valid derivationis generated only when the source tree is completelymatched by rules.Our bottom-up decoding algorithm involves thefollowing steps:1.
Match STSG rules against the source tree.2.
Convert STSG rules to CFG rules.3.
Collect the viable prefix set for each node in apost-order transversal.4.
Search bottom-up for the best derivation.3.1 From STSG to CFGAfter rule matching, each tree node has its applica-ble STSG rule set.
Given a matched STSG rule, ourdecoding algorithm only needs to consider the treenode the rule can be applied to and the target side,so we follow Huang and Mi (2010) to convert STSGrules to CFG rules.
For example, an STSG ruleNP ( NN1 (toupiao), x1 : NN2 ) ?
x1 of the votecan be converted to a CFG ruleNP ?
NN2 of the voteThe target non-terminals are replaced with corre-sponding source non-terminals.
Figure 1 (c) showsall converted CFG rules for the toy example.
NoteIPNPNN1to?up?`aoNN2j?
?eguo?VPNTwa?nsha`ngVVgo?ngbu`(a) Source parse treer6: IPNP VP?
?r4: NPNN1to?up?`aoNN2r5: VPNTwa?nsha`ngVVgo?ngbu`?r2: NN2j?
?eguo?the result of the vote was released at night(b) A derivationr1: NN1 ?
the voter2: NN2 ?
the resultr3: NP ?
NN2 of NN1r4: NP ?
NN2 of the voter5: VP ?
was released at nightr6: IP ?
NP VPr7: IP ?
NN2 of the vote VPr8: IP ?
VP NP(c) Target-side CFG rule setFigure 1: A toy example.that different STSG rules might be converted to thesame CFG rule despite having different source treestructures.3.2 Viable PrefixDuring decoding, how do we decide which rulesshould be used next given a partial derivation, es-pecially when there is a gap?
A key observation isthat some rules should be excluded.
For example,any derivation for Figure 1(a) will never begin withr1 as there is no translation starting with ?the vote?.In order to know which rules can be excluded foreach node, we can recursively calculate the start-ing terminal strings for each node.
For example,1193NN1: {the vote} NN2: {the result}NT: ?
VV: ?NP: {the result}VP: {was released at night}IP: {the result, was released at night}Table 2: The Viable prefix sets for Figure 1 (c)according to r1, the starting terminal string of thetranslation for NN1 is ?the vote?.
According to r2,the starting terminal string for NN2 is ?the result?.According to r3, the starting terminal string of NPmust include that of NN2.
Table 2 lists the startingterminal strings of all nodes in Figure 1(a).
As thetranslations of node IP should begin with either ?theresult?
or ?was released at night?, the first rule mustbe either r2 or r5.
Therefore, r1 will never be usedas the first rule in any derivation.We refer to starting terminal strings of a node asa viable prefixes, a term borrowed from LR pars-ing (Aho and Johnson, 1974).
Viable prefixes areused to decide which rule should be used to ensureefficient left-to-right target generation.
Formally, as-sume that VN denotes the set of non-terminals (i.e.,source tree node labels), VT denotes the set of ter-minals (i.e., target words), v1, v2 ?
VN , w ?
VT ,pi ?
{VT ?
VN}?, we say that w is a viable prefix ofv1 if and only if:?
v1 ?
w, or?
v1 ?
wv2pi, or?
v1 ?
v2pi, and w is a viable prefix of v2.Note that we bundle all successive terminals in onesymbol.3.3 Shift-Reduce ParsingWe use a shift-reduce algorithm to search for thebest deviation.
The algorithm maintains a stack ofdotted rules (Earley, 1970).
Given the source tree inFigure 1(a), the stack is initialized with a dotted rulefor the root node IP:[ IP].Then, the algorithm selects one viable prefix of IPand appends it to the stack with the dot at the begin-ning (predict):[ IP] [ the result]2.Then, a scan action is performed to produce a partialtranslation ?the result?
:[ IP] [the result ].Next, the algorithm searches for the CFG rules start-ing with ?the result?
and gets r2.
Then, it pops therightmost dotted rule and append the left-hand side(LHS) of r2 to the stack (complete):[ IP] [NN2 ].Next, the algorithm chooses r4 whose right-handside ?NN2 of the vote?
matches the rightmost dot-ted rule in the stack3 and grows the rightmost dottedrule:[ IP] [NN2  of the vote].Figure 3 shows the whole process of derivationgeneration.Formally, we define four actions on the rightmostrule in the stack:?
Predict.
If the symbol after the dot in the right-most dotted rule is a non-terminal v, this actionchooses a viable prefix w of v and generates anew dotted rule for w with the dot at the begin-ning.
For example:[ IP] predict??
[ IP] [ the result]?
Scan.
If the symbol after the dot in the right-most dotted rule is a terminal string w, this ac-tion advances the dot to update the current par-tial translation.
For example:[ IP] [ the result] scan??
[ IP] [the result ]?
Complete.
If the rightmost dotted rule endswith a dot and it happens to be the right-handside of a rule, then this action removes theright-most dotted rule.
Besides, if the symbolafter the dot in the new rightmost rule corre-sponds to the same tree node as the LHS non-terminal of the rule, this action advance the dot.For example,[ IP] [NP  VP] [was released at night ]complete??
[ IP] [NP VP ]2There are another option: ?was released at night?3Here there is an alternative: r3 or r71194step action rule used stack hypothesis0 [ IP]1 p r6 [ IP] [ NP VP]2 p r4 [ IP] [ NP VP] [ NN2 of the vote]3 p r2 [ IP] [ NP VP] [ NN2 of the vote] [ the result]4 s [ IP] [ NP VP] [ NN2 of the vote] [the result ] the result5 c [ IP] [ NP VP] [NN2  of the vote] the result6 s [ IP] [ NP VP] [NN2 of the vote ] the result of the vote7 c [ IP] [NP  VP] the result of the vote8 p r5 [ IP] [NP  VP] [ was released at night] the result of the vote9 s [ IP] [NP  VP] [was released at night ] the ... vote was ... night10 c [ IP] [NP VP ] the ... vote was ... night11 c [IP ] the ... vote was ... nightFigure 2: Simulation of top-down translation process for the derivation in Figure 1(b).
Actions: p, predict; s, scan; c,complete.
?the ... vote?
and ?was ... released?
are the abbreviated form of ?the result of the vote?
and ?was released atnight?, respectively.step action rule used stack number hypothesis0 [ IP] 01 p [ IP] [ the result] 02 s [ IP] [the result ] 1 the result3 c r2 [ IP] [NN2 ] 1 the result4 g r4 or r7 [ IP] [NN2  of the vote] 1 the result5 s [ IP] [NN2 of the vote ] 2 the result of the vote6 c r4 [ IP] [NP ] 2 the result of the vote7 g r6 [ IP] [NP  VP] 2 the result of the vote8 p [ IP] [NP  VP] [ was released at night] 2 the result of the vote9 s [ IP] [NP  VP] [was released at night ] 4 the ... vote was ... night10 c r5 [ IP] [NP VP ] 4 the ... vote was ... night11 c r6 [IP ] 4 the ... vote was ... nightFigure 3: Simulation of bottom-up translation process for the derivation in Figure 1(b).
Actions: p, predict; s, scan; c,complete; g, grow.
The column of number gives the number of source words the hypothesis covers.If the string cannot rewrite on the frontier non-terminal, then we add the LHS to the stack withthe dot after it.
For example:[ IP] [the result ] complete??
[ IP] [NN2 ]?
Grow.
If the right-most dotted rule ends witha dot and it happens to be the starting part ofa CFG rule, this action appends one symbol ofthe remainder of that rule to the stack 4.
Forexample:4We bundle the successive terminals in one rule into a sym-bol[ IP] [NN2 ]grow??
[ IP] [NN2  of the vote]From the above definition, we can find that theremay be an ambiguity about whether to use a com-plete action or a grow action.
Similarly, predict ac-tions must select a viable prefix form the set for anode.
For example in step 5, although we selectto perform complete with r4 in the example, r7 isapplicable, too.
In our implementation, if both r4and r7 are applicable, we apply them both to gener-ate two seperate hypotheses.
To limit the exponen-tial explosion of hypotheses (Knight, 1999), we usebeam search over bins of similar partial hypotheses(Koehn, 2004).1195IPNPNN2 of NN1of the voteVPwas released at nightr7r4 r5r6r3Figure 4: The translation forest composed of applicableCFG rules for the partial derivation of step 3 in Figure 3.3.4 Future CostPartial derivations covering different tree nodes maybe grouped in the same bin for beam pruning5.
Inorder to performmore accurate pruning, we take intoconsideration future cost, the cost of the uncoveredpart.
The merit of a derivation is the covered cost(the cost of the covered part) plus the future cost.We borrow ideas from the Inside-Outside algorithm(Charniak and Johnson, 2005; Huang, 2008; Mi etal., 2008) to compute the merit.
In our algorithm,the merit of a derivation is just the Viterbi inside cost?
of the root node calculated with the derivationscontinuing from the current derivation.Given a partial derivation, we calculate its futurecost by searching through the translation forest de-fined by all applicable CFG rules.
Figure 4 showsthe translation forest for the derivation of step 3.
Wecalculate the future cost for each node as follows:given a node v, we define its cost function f(v) asf(v) =????
?1 v is completedlm(v) v is a terminal stringmaxr?Rv f(r)?pi?rhs(r) f(pi) otherwisewhere VN is the non-terminal set, VT is the terminalset, v, pi ?
VN ?
VT+, Rv is the set of currently ap-plicable rules for v, rhs(r) is the right-hand symbolset of r, lm is the local language model probability,f(r) is calculated using a linear model whose fea-tures are bidirectional translation probabilities andlexical probabilities of r. For the translation forestin Figure 4, if we calculate the future cost of NP with5Section 3.7 will describe the binning schemer4, thenf(NP ) = f(r4) ?
f(NN2) ?
lm(of the vote)= f(r4) ?
1 ?
lm(of the vote)Note that we calculate lm(of the vote) locally and donot take ?the result?
derived from NN2 as the con-text.
The lm probability of ?the result?
has been in-cluded in the covered cost.As a partial derivation grows, some CFG ruleswill conflict with the derivation (i.e.
inapplicable)and the translation forest will change accordingly.For example, when we reach step 5 from step 3 (seeFigure 4 for its translation forest), r3 is inapplica-ble and thereby should be ruled out.
Then the nodeson the path from the last covered node (it is ?of thevote?
in step 5) to the root node should update theirfuture cost, as they may employ r3 to produce thefuture cost.
In step 5, NP and IP should be updated.In this sense, we say that the future cost is dynamic.3.5 Comparison with Top-Down DecodingIn order to generate the translation ?the result?
basedon the derivation in Figure 1(b), Huang and Mi?stop-down algorithm needs to specify which rules toapply starting from the root node until it yields ?theresult?.
In this derivation, rule r6 is applied to IP, r4to NP, r2 to NN2.
That is to say, it needs to repre-sent the partial derivation from IP to NN2 explicitly.This can be a problem when combined with beampruning.
If the beam size is small, it may discard theintermediate hypotheses and thus never consider thestring.
In our example with a beam of 1, we mustselect a rule for IP among r6, r7 and r8 although wedo not get any information for NP and VP.Instead, our bottom-up algorithm allows top-down and bottom-up information to be used togetherwith the help of viable prefixes.
This allows us toencode more candidate derivations than the purelytop-down method.
In the above example, our al-gorithm does not specify the derivation for the gapfrom IP and ?the result?.
In fact, all derivationscomposed of currently applicable rules are allowed.When needed, our algorithm derives the derivationdynamically using applicable rules.
So when ouralgorithm performs pruning at the root node, it hasgot much more information and consequently intro-duces fewer pruning errors.11963.6 Time ComplexityAssume the depth of the source tree is d, the max-imum number of matched rules for each node is c,the maximum arity of each rule is r, the languagemodel order is g and the target-language vocabularyis V, then the time complexity of our algorithm isO((cr)d|V |g?1).
Analysis is as follows:Our algorithm expands partial paths with termi-nal strings to generate new hypotheses, so the timecomplexity depends on the number of partial pathsused.
We split a path which is from the root node to aleaf node with a node on it (called the end node) andget the segment from the root node to the end nodeas a partial path, so the length of the partial path isnot definite with a maximum of d. If the length isd?(d?
?
d), then the number of partial paths is (cr)d?
.Besides, we use the rightest g ?
1 words to signa-ture each partial path, so we can get (cr)d?
|V |g?1states.
For each state, the number of viable prefixesproduced by predict operation is cd?d?
, so the totaltime complexity is f = O((cr)d?
|V |g?1cd?d?)
=O(cdrd?
|V |g?1) = O((cr)d|V |g?1).3.7 Beam SearchTomake decoding tractable, we employ beam search(Koehn, 2004) and choose ?binning?
as follows: hy-potheses covering the same number of source wordsare grouped in a bin.
When expanding a hypothe-sis in a beam (bin), we take series of actions untilnew terminals are appended to the hypothesis, thenadd the new hypothesis to the corresponding beam.Figure 3 shows the number of source words each hy-pothesis covers.Among the actions, only the scan action changesthe number of source words each hypothesis cov-ers.
Although the complete action does not changesource word number, it changes the covered cost ofhypotheses.
So in our implementation, we take scanand complete as ?closure?
actions.
That is to say,once there are some complete actions after a scan ac-tion, we finish all the compete actions until the nextaction is grow.
The predict and grow actions decidewhich rules can be used to expand hypotheses next,so we update the applicable rule set during these twoactions.Given a source sentence with n words, we main-tain n beams, and let each beam hold b hypothesesat most.
Besides, we prune viable prefixes of eachnode up to u, so each hypothesis can expand to unew hypotheses at most, so the time complexity ofbeam search is O(nub).4 Related WorkWatanabe et al(2006) present a novel Earley-style top-down decoding algorithm for hierarchicalphrase-based model (Chiang, 2005).
Their frame-work extracts Greibach Normal Form rules only,which always has at least one terminal on the leftof each rule, and discards other rules.Dyer and Resnik (2010) describe a translationmodel that combines the merits of syntax-basedmodels and phrase-based models.
Their decoderworks in two passes: for first pass, the decoder col-lects a context-free forest and performs tree-basedsource reordering without a LM.
For the secondpass, the decoder adds a LM and performs bottom-up CKY decoding.Feng et al(2010) proposed a shift-reduce algo-rithm to add BTG constraints to phrase-based mod-els.
This algorithm constructs a BTG tree in areduce-eager manner while the algorithm in this pa-per searches for a best derivation which must be de-rived from the source tree.Galley and Manning (2008) use the shift-reducealgorithm to conduct hierarchical phrase reorderingso as to capture long-distance reordering.
This al-gorithm shows good performance on phrase-basedmodels, but can not be applied to syntax-based mod-els directly.5 ExperimentsIn the experiments, we use two baseline systems:our in-house tree-to-string decoder implemented ac-cording to Liu et al(2006) (denoted as traditional)and the Earley-style top-down decoder implementedaccording to Huang and Mi (2010) (denoted as top-down), respectively.
We compare our bottom-upleft-to-right decoder (denoted as bottom-up) withthe baseline in terms of performance, translationquality and decoding speed with different beamsizes, and search capacity.
Lastly, we show the in-fluence of future cost.
All systems are implementedin C++.11975.1 Data SetupWe used the FBIS corpus consisting of about 250KChinese-English sentence pairs as the training set.We aligned the sentence pairs using the GIZA++toolkit (Och and Ney, 2003) and extracted tree-to-string rules according to the GHKM algorithm (Gal-ley et al 2004).
We used the SRILM toolkit (Stol-cke, 2002) to train a 4-gram language model on theXinhua portion of the GIGAWORD corpus.We used the 2002 NIST MT Chinese-English testset (571 sentences) as the development set and the2005 NIST MT Chinese-English test set (1082 sen-tences) as the test set.
We evaluated translation qual-ity using BLEU-metric (Papineni et al 2002) withcase-insensitive n-gram matching up to n = 4.
Weused the standard minimum error rate training (Och,2003) to tune feature weights to maximize BLEUscore on the development set.5.2 Performance ComparisonOur bottom-up left-to-right decoder employs thesame features as the traditional decoder: rule proba-bility, lexical probability, language model probabil-ity, rule count and word count.
In order to comparethem fairly, we used the same beam size which is 20and employed cube pruning technique (Huang andChiang, 2005).We show the results in Table 3.
From the re-sults, we can see that the bottom-up decoder out-performs top-down decoder and traditional decoderby 1.1 and 0.8 BLEU points respectively and theimprovements are statistically significant using thesign-test of Collins et al(2005) (p < 0.01).
Theimprovement may result from dynamically search-ing for a whole derivation which leads to more ac-curate estimation of a partial derivation.
The addi-tional time consumption of the bottom-up decoderagainst the top-down decoder comes from dynamicfuture cost computation.Next we compare decoding speed versus transla-tion quality using various beam sizes.
The resultsare shown in Figure 5.
We can see that our bottom-up decoder can produce better BLEU score at thesame decoding speed.
At small beams (decodingtime around 0.5 second), the improvement of trans-lation quality is much bigger.System BLEU(%) Time (s)Traditional 29.8 0.84Top-down 29.5 0.41Bottom-up 30.6 0.81Table 3: Performance comparison.29.429.629.830.030.230.430.630.80.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8BLEUScoreAvg Decoding Time (secs per sentence)bottom-uptop-downtraditionalFigure 5: BLEU score against decoding time with variousbeam size.5.3 Search Capacity ComparisonWe also compare the search capacity of the bottom-up decoder and the traditional decoder.
We do thisin the following way: we let both decoders use thesame weights tuned on the traditional decoder, thenwe compare their translation scores of the same testsentence.From the results in Table 4, we can see that formany test sentences, the bottom-up decoder findstarget translations with higher score, which havebeen ruled out by the traditional decoder.
This mayresult from more accurate pruning method.
Yet forsome sentences, the traditional decoder can attainhigher translation score.
The reason may be that thetraditional decoder can hold more than two nonter-minals when cube pruning, while the bottom-up de-coder always performs dual-arity pruning.Next, we check whether higher translation scoresbring higher BLEU scores.
We compute the BLEUscore of both decoders on the test sentence set onwhich bottom-up decoder gets higher translationscores than the traditional decoder does.
We recordthe results in Figure 6.
The result shows that higherscore indeed bring higher BLEU score, but the im-provement of BLEU score is not large.
This is be-cause the features we use don?t reflect the real statis-119828.029.030.031.032.033.034.035.010  20  30  40BLEUScoreBeam Sizebottom-uptraditionalFigure 6: BLEU score with various beam sizes on the subtest set consisting of sentences on which the bottom-updecoder gets higher translation score than the traditionaldecoder does.b > = <10 728 67% 347 32% 7 1%20 657 61% 412 38% 13 1%30 615 57% 446 41% 21 2%40 526 49% 523 48% 33 3%50 315 29% 705 65% 62 6%Table 4: Search capacity comparison.
The first column isbeam size, the following three columns denote the num-ber of test sentences, on which the translation scores ofthe bottom-up decoder are greater, equal to, lower thanthat of the traditional decoder.System BLEU(%) Time (s)with 30.6 0.81without 28.8 0.39Table 5: Influence of future cost.
The results of thebottom-up decoder with and without future cost are givenin the second and three rows, respectively.tical distribution of hypotheses well.
In addition, theweights are tuned on the traditional decoder, not onthe bottom-up decoder.
The bottom-up decoder canperform better with weights tuned by itself.5.4 Influence of Future CostNext, we will show the impact of future cost via ex-periments.
We give the results of the bottom-up de-coder with and without future cost in Table 5.
Fromthe result, we can conclude that future cost plays asignificant role in decoding.
If the bottom-up de-coder does not employ future cost, its performancewill be influenced dramatically.
Furthermore, cal-culating dynamic future cost is time consuming.
Ifthe bottom-up decoder does not use future cost, itdecodes faster than the top-down decoder.
This isbecause the top-down decoder has |T | beams, whilethe bottom-up decoder has n beams, where T is thesource parse tree and n is the length of the sourcesentence.6 ConclusionsIn this paper, we describe a bottom-up left-to-rightdecoding algorithm for tree-to-string model.
Withthe help of viable prefixes, the algorithm generatesa translation by constructing a target-side CFG treeaccording to a post-order traversal.
In addition, ittakes into consideration a dynamic future cost to es-timate hypotheses.On the 2005 NIST Chinese-English MT transla-tion test set, our decoder outperforms the top-downdecoder and the traditional decoder by 1.1 and 0.8BLEU points respectively and shows more powerfulsearch ability.
Experiments also prove that futurecost is important for more accurate pruning.7 AcknowledgementsWe would like to thank Haitao Mi and DouweGelling for their feedback, and anonymous review-ers for their valuable comments and suggestions.This work was supported in part by EPSRC grantEP/I034750/1 and in part by High Technology R&DProgram Project No.
2011AA01A207.ReferencesA.
V. Aho and S. C. Johnson.
1974.
Lr parsing.
Com-puting Surveys, 6:99?124.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proc.
of ACL, pages 173?180.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL,pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33:201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL, pages 531?540.1199Chris Dyer and Philip Resnik.
2010.
Context-free re-ordering, finite-state translation.
In Proc.
of NAACL,pages 858?866, June.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13:94?102.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010.
Anefficient shift-reduce decoding algorithm for phrased-based machine translation.
In Proc.
of Coling, pages285?293.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proc.
of EMNLP, pages 848?856.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc ofNAACL, pages 273?280.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
of IWPT, pages 53?64.Liang Huang and Haitao Mi.
2010.
Efficient incremen-tal decoding for tree-to-string translation.
In Proc.
ofEMNLP, pages 273?283.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL,pages 586?594.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25:607?615.Philipp Koehn.
2004.
Pharaoh: A beam search decoderfor phrased-based statistical machine translation.
InProc.
of AMTA, pages 115?124.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL, pages 609?616, July.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL, pages 192?199.Frans J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29:19?51.Frans J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proc.
of ICSLP.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of COLING, pages777?784.1200
