Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 26?33,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsNAIST at 2013 CoNLL Grammatical Error Correction Shared TaskIppei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi,Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, Yuji MatsumotoNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara 630-0192, Japan{ippei-y,tomoya-kos,kensuke-mi,keisuke-sa,tomoya-m,yuta-h,komachi,matsu}@is.naist.jpAbstractThis paper describes the Nara Instituteof Science and Technology (NAIST) er-ror correction system in the CoNLL 2013Shared Task.
We constructed three sys-tems: a system based on the Treelet Lan-guage Model for verb form and subject-verb agreement errors; a classifier trainedon both learner and native corpora fornoun number errors; a statistical machinetranslation (SMT)-based model for prepo-sition and determiner errors.
As forsubject-verb agreement errors, we showthat the Treelet Language Model-basedapproach can correct errors in which thetarget verb is distant from its subject.
Oursystem ranked fourth on the official run.1 IntroductionGrammatical error correction is the task of auto-matically detecting and correcting grammatical er-rors in text, especially text written by second lan-guage learners.
Its purpose is to assist learners inwriting and helps them learn languages.Last year, HOO 2012 (Dale et al 2012) washeld as a shared task on grammatical error cor-rection, focusing on prepositions and determiners.The CoNLL-2013 shared task (Dahlmeier et al2013) includes these areas and also noun number,verb form, and subject-verb agreement errors.We divide the above 5 error types into threegroups: (1) subject-verb agreement (SVA) andverb form (Vform) errors, (2) noun number (Nn)errors, and (3) preposition (Prep) and determiner(ArtOrDet) errors.
For the subject-verb agreementand verb form errors, we used a syntactic languagemodel, the Treelet Language Model, because syn-tactic information is important for verb error cor-rection.
For the noun number errors, we used abinary classifier trained on both learner and nativecorpora.
For the preposition and determiner errors,we adopt a statistical machine translation (SMT)-based approach, aiming at correcting errors in con-ventional expressions.
After each subsystem cor-rects the errors of the corresponding error types,we merge the outputs of all the subsystems.The result shows our system achieved 21.85in F-score on the formal run before revision and28.14 after revision.The rest of this paper is organized as follows.Section 2 presents an overview of related work.Section 3 describes the system architecture of eachof the three subsystems.
Section 4 shows experi-mental settings and results.
Section 5 presents dis-cussion.
Section 6 concludes this paper.2 Related WorkLee and Seneff (2008) tried correcting Englishverb errors including SVA and Vform.
They pro-posed correction candidates with template match-ing on parse trees and filtered candidates by uti-lizing n-gram counts.
Our system suggests candi-dates based on the Part-Of-Speech (POS) tag of atarget word and filters them by using a syntacticlanguage model.For the noun number errors, we improved thesystem proposed by Izumi et al(2003).
InIzumi et al(2003), a noun number error detec-tion method is a part of an automatic error de-tection system for transcribed spoken English byJapanese learners.
They used a maximum entropymethod whose features are unigrams, bigrams andtrigrams of surface words, of POS tags and ofthe root forms.
They trained a classifier on onlya learner corpus.
The main difference betweentheirs and ours is a domain of the training corpusand features we used.
We trained a classifier onthe mixed corpus of the leaner corpus and the na-tive corpus.
We employ a treepath feature in oursystem.Our SMT system for correcting preposition and26determiner errors is based on Mizumoto et al(2012).
They constructed a translation model fromthe data of the language-exchange social networkservice Lang-81 and evaluated its performance for18 error types, including preposition and deter-miner errors in the Konan-JIEM Learner Corpus.On preposition error correction, they showed thattheir SMT system outperformed a system usinga maximum entropy model.
The main differencewith this work is that our new corpus collectionhere is about three times larger.3 System Architecture3.1 Subject-Verb Agreement and Verb FormFor SVA and Vform errors, we used the TreeletLanguage Model (Pauls and Klein, 2012) to cap-ture syntactic information and lexical informationsimultaneously.
We will first show examples ofSVA and Vform errors and then describe our modelused to correct them.
Finally, we explain the pro-cedure for error correction.3.1.1 ErrorsAccording to Lee and Seneff (2008), both SVA andVform errors are classified as syntactic errors.
Ex-amples are as follows:Subject-Verb Agreement (SVA) The verb is notcorrectly inflected in number and person withrespect to its subject.They *has been to Nara many times.In this example, a verb ?has?
is wrongly in-flected.
It should be ?have?
because its subject isthe pronoun ?they?.Verb Form (Vform) This type of error mainlyconsists of two subtypes,2 one of which includesauxiliary agreement errors.They have *be to Nara many times.Since the ?have?
in this sentence is an auxil-iary verb, the ?be?
is incorrectly inflected and itshould be ?been?.The other subtype includes complementation1http://lang-8.com2In the NUCLE (Dahlmeier et al 2013) corpus, most ofsemantic errors related to verbs are included in other errortypes such as verb tense errors, not Vform errors.errors like the following:They want *go to Nara this summer.Verbs can be a complement of another verband preposition.
The ?go?
in the above sentenceis incorrect.
It should be in the infinitive form, ?togo?.3.1.2 Treelet Language ModelWe used the Treelet Language Model (Pauls andKlein, 2012) for SVA and Vform error correction.Our model assigns probability to a productionrule of the form r = P ?
C1 ?
?
?Cd in a con-stituent tree T , conditioned on a context h consist-ing of previously generated treelets,3 where P isthe parent symbol of a rule r and Cd1 = C1 ?
?
?Cdare its children.p(r) = p(Cd1 |h)The probability of a constituent tree T is given bythe following equation:p(T ) =?r?Tp(r)The context h differs depending on whether Cd1 isa terminal symbol or a sequence of non-terminalsymbols.Terminal When Cd1 is a terminal symbol w,p(Cd1 |h) = p(w|P,R, r?, w?1, w?2)where P is the POS tag of w, R is the right siblingof P , r?
is the production rule which yields P andits siblings, and w?2 and w?1 are the two wordspreceding w.Non-Terminal When Cd1 is a sequence of non-terminal symbols,p(Cd1 |h) = p(Cd1 |P, P ?, r?
)where P is the parent symbol of Cd1 , P ?
is the par-ent symbol of P .In order to capture a richer context, we apply theannotation and transformation rules below to parsetrees in order.
We use almost the same annota-tion and transformation rules as those proposed by3The term treelet is used to refer to an arbitrary connectedsubgraph of a tree (Quirk et al 2005)27Original Candidatesam/VBP, are/VBP or is/VBZ {am/VBP, are/VBP, is/VBZ}was/VBD or were/VBD {was/VBD, were/VBD}being/VBG {be/VB, being/VBG}been/VBN {be/VB, been/VBN}be/VB {be/VB, being/VBG, been/VBN}Table 1: Examples of candidates in the case of ?be?ROOTSVPVPADVPNNStimesJJmanyPPNNPNaraTOtoVBNbeenVBPhaveNPPRPTheyROOTS@ROOT-haveVP@S-haveADVP-NNTSNNTStimesJJmanyPP-toNNPNaraTO-totoVBN-beenbeenVBP-havehavePRP-theyTheyFigure 1: The tree on the left is before annotations and transformations which convert it to the tree onthe right.Pauls and Klein (2012).
For instance, the commonCFG tree on the left side of Figure 1 is transformedto the one on the right side.Temporal NPs Pauls and Klein (2012) marked ev-ery noun which is the head of an NP-TMP at leastonce in the Penn Treebank.
For example, NN ?time is replaced with NNT ?
time and NNS ?times is replaced with NNTS ?
times.
This ruleseems to be useful for correcting verb tense er-rors.4Head Annotations We annotated every non-terminaland preterminal with its head word.5 If the headword is not a closed class word,6 we annotatednon-terminal symbols with the head POS tag in-stead of the head word.NP Flattening Pauls and Klein (2012) deleted NPsdominated by other NPs, unless the child NPs arein coordination or apposition.
These NPs typically4Verb tense (Vt) errors are not covered in this shared task.5We identified the head with almost the same rules usedin Collins (1999).6We took the following to be the closed class words: allinflections of the verbs do, be, and have; and any word taggedwith IN, WDT, PDT, WP, WP$, TO, WRB, RP, DT, SYM,EX, POS, PRP, AUX, MD or CC.occur when nouns are modified by PPs.
Our modeltherefore assigns probability to nouns conditionedon the head of modifying PPs with prepositionssuch as ?in?, ?at?
and so on by applying simul-taneously the NP Flattening and the Head Annota-tions.
However, our model cannot assign probabil-ity to prepositions conditioned on verbs or nounson which the prepositions depend.
For this reasonwe did not use our model to correct prepositionalerrors.Number Annotations Pauls and Klein (2012) di-vided numbers into five classes: CD-YR for num-bers that consist of four digits, which are usuallyyears; CD-NM for entirely numeric numbers; CD-DC for numbers that have a decimal; CD-MX fornumbers that mix letters and digits; and CD-ALfor numbers that are entirely alphabetic.SBAR Flattening They removed any S nodes whichare children of an SBAR.VP Flattening They removed any VPs immedi-ately dominated by a VP, unless it is con-joined with another VP.
The chains of verbsare represented as separated VPs for each verb,such as (VP (MD will) (VP (VB be) (VP (VBG28playing) .
.
.))).
This transformation turns theabove VPs into (VP (MD will) (VB be) (VBGplaying) .
.
.).
This has an effect on the cor-rection of auxiliary agreement errors becauseour model can assign probability to main verbsstrongly conditioned on their auxiliary verbs.Gapped Sentence Annotation They annotated all Sand SBAR nodes that have a VP before any NP.Parent Annotation They annotated all VPs and chil-dren of the ROOT node with their parent symbol.Unary Deletion All unary rules are deleted exceptthe root and the preterminal rules.
We kept onlythe bottom-most symbol of the unary rule chain.This brings many symbols into the context of aproduction rule.3.1.3 ProcedureOur system for SVA and Vform errors tries to cor-rect the words in a sentence from left to right.
Cor-rection proceeds in the following steps.1.
If the POS tag of the word is ?VB?, ?VBD?,?VBG?, ?VBN?, ?VBP?
or ?VBZ?, our sys-tem generates sentences which have the wordreplaced with candidates.
For example, if theoriginal word is an inflection of ?be?, the sys-tem generates candidates as shown in Table1.2.
The system parses those sentences and ob-tains the k-best parses for each sentence.3.
The system keeps only the one sentence towhich our language model assigned the high-est probability in the parses.4.
The system repeats Steps 1 to 3 with the sen-tence kept in Step 3 until the rightmost wordof that sentence.Note that the system uses the Berkeley Parser7 inStep 2.3.2 Noun Number3.2.1 ErrorsA noun number error is the mistake of using thesingular form for a plural noun, and vice versa, asin the following:7http://code.google.com/p/berkeleyparser/I saw many *student yesterday.In this example, the inflection of ?student?is mistaken.
It should be ?students?
because it ismodified by ?many?.To correct such errors, we use a binary classi-fication approach because the inflection of a nounis either ?singular?
or ?plural?.
If the binary clas-sifier detects an error with a sufficiently high con-fidence, the system changes the noun form.
Weadopt the adaptive regularization of weight vectorsalgorithm (AROW) (Crammer et al 2009).
AROWis a variant of a confidence weighted linear classi-fication algorithm which is suitable for the classi-fication of large scale data.3.2.2 Binary classifier approachThe binary classifier indicates ?singular?
or ?plu-ral?
for all nouns except proper and uncountablenouns.
First, if a noun is found in the training cor-pus, we extract an instance with features createdby the feature template in Table 2.8 Second, wetrain a classifier with extracted instances and la-bels from the training corpus.We use unigram, bigram, and trigram featuresaround the target word and the path features be-tween the target word and all the other nodes inthe NPs that dominate the target word as the right-most constituent.
The path feature is commonlyused in semantic role labeling tasks (Pradhan etal., 2004).
For the path features, we do not usethe right subtree of the NP as the path features be-cause we assume that right subtrees do not affectthe number of the target word.
We limit the maxi-mum depth of the subtree containing the NP to be3 because nodes over this limit may be noisy.
Toencode the relationship between the target wordand another node in the NP, we append a symbolwhich reflects the direction of tree traversal to thelabel: ?p?
for going up (parent) and ?c?
for goingdown (child).
For example, we show extracted fea-tures in Table 2 for the phrase ?some interestingand recent topics about politics and economics?.In the training corpus, since the proportions ofsingular and plural nouns are unequal, we set dif-ferent thresholds for classifying singular and plu-ral forms.
These thresholds limit the probabilitieswhich the binary classifier uses for error detection.We have used a development set to determine the8Target word refers to a noun whose POS tag is ?NN?
or?NNS?
in the Penn Treebank tagset.29Feature name Word, Pos used as features Examplesurface unigram word?1, word?2 and, recent, about, politicssurface bigram word?2 word?1 and recent, about politicssurface trigram word?3 word?2 word?1 interesting and recent, about politics andPOS unigram POS?1, POS?2 CC, JJ, IN, NNPOS bigram POS?1 POS?2 CC JJ, IN NNPOS trigram POS?3 POS?2 POS?1 JJ CC JJ, IN NN CONJlemma unigram lemma?2, lemma?1 and, recent, about, politicslemma bigram lemma?2 lemma?1 and recent, about politicslemma trigram lemma?3 lemma?2 lemma?1 interesting and recent, about politics andlemma target lemma of target word topicpath feature path between the target word p NP, pc JJ, pc recent, pp NP, ppc CC, ppc and,and the other nodes in NP ppc NP, ppcc DT, ppcc some, ppcc JJ, ppcc interestingTable 2: Features used for the detection of noun number errors and example features for the phrase ?someinteresting and recent topics about politics and economics?.best thresholds for singular and plural forms, re-spectively.For proper and uncountable nouns, we do notchange number because of the nature of thosenouns.
In order to determine whether to changenumber or not, we create a list which consists ofwords frequently used as singular forms in the na-tive corpus.3.3 Prepositions and DeterminersFor preposition and determiner errors, we con-struct a system using a phrase-based statisticalmachine translation (Koehn et al 2003) frame-work.
The SMT-based approach functions wellin corrections of conventional usage of determin-ers and prepositions such as ?the young?
and ?takecare of ?.
The characteristic of the SMT-based ap-proach is its ability to capture tendencies in learn-ers?
errors.
This approach translates erroneousphrases that learners often make to correct phrases.Hence, it can handle errors in conventional expres-sions without over-generalization.The phrase-based SMT framework which weused is based on the log-linear model (Och andNey, 2002), where the decision rule is expressedas follow:argmaxeP (e|f) = argmaxeM?m=1?mhm(e, f)Here, f is an input sentence, e are hypotheses,hm(e, f) feature functions and ?m their weights.The hypothesis that maximizes the weighted sumof the feature functions is chosen as an output sen-tence.The feature functions encode components ofthe phrase-based SMT, including the translationmodel and the language model.
The translationmodel suggests translation hypotheses and the lan-guage model filters out ill-formed hypotheses.For an error correction system based on SMT,the translation model is constructed from pairs oforiginal sentences and corrected sentences, and thelanguage model is built on a native corpus (Brock-ett et al 2006).Brockett et al(2006) trained the translationmodel on a corpus where the errors are restrictedto mass noun errors.
In our case, we trained ourmodel on a corpus with no restriction on errortypes.
Consequently, the system corrects all typesof errors.
To focus on preposition and determinererrors, we retain proposed edits that include 48prepositions and 25 determiners listed in Table 3.4 Experiments4.1 Experimental setting4.1.1 Subject-Verb Agreement and VerbFormWe describe here the training data and tools usedto train our model.
Our model was trained with theBerkeley LM9 version 1.1.3.
We constructed thetraining data by concatenating the WSJ sections ofthe Penn Treebank and the AFP sections of the En-glish Gigaword Corpus version 5.10 Our trainingdata consists of about 27 million sentences.
Al-though human-annotated parses for the WSJ areavailable, there is no gold standard for the AFP,so we parsed the AFP automatically by using theBerkeley Parser released on October 9, 2012.9http://code.google.com/p/berkeleylm/10LDC2011T0730Preposition about, across, after, against, along, among, around, as, at, before, behind, below,beside, besides, between, beyond, but, by, despite, down, during, for, from, in,inside, into, near, of, off, on, onto, opposite, outside, over, past, round, without,than, through, to, toward, towards, under, until, up, upon, with, withinDeterminer the, a, an, all, these, those, many, much, another, no, some, any, my,our, their, her, his, its, no, each, every, certain, its, this, thatTable 3: Preposition and determiner lists4.1.2 Noun NumberWe trained a binary classifier on a merged corpusof the English Gigaword and the NUCLE data.From the English Gigaword corpus, we used theNew York Times (NYT) as a training corpus.
Inorder to create the training corpus, we correctedall but noun number errors in the NUCLE data us-ing gold annotations.The AROW++ 11 0.1.2 was used for the binaryclassification.
For changing noun forms, we usedthe pattern.en toolkit.12The maximum depth of subtrees containing anNP is set to 3 when we extracted the path features.We built and used a list of nouns that appear insingular forms frequently in a native corpus.
Wecounted the frequency of nouns in entire EnglishGigaword.
If a noun appears in more than 99%13of occurrences in singular form, we included it inthe list.
The resulting list contains 836 nouns.4.1.3 Prepositions and DeterminersWe used Moses 2010-08-13 with default parame-ters for our decoder14 and GIZA++ 1.0.515 as thealignment tool.
The grow-diag-final heuristics wasapplied for phrase extraction.
As a language mod-eling tool we used IRSTLM version 5.8016 withWitten-Bell smoothing.The translation model was trained on the NU-CLE corpus and our Lang-8 corpus.17 From theLang-8 corpus, we filtered out noisy sentences.Out of 1,230,257 pairs of sentences, 1,217,124pairs of sentences were used for training.
As forthe NUCLE corpus we used 55,151 pairs of sen-tences from the official data provided as training11https://code.google.com/p/arowpp/12http://www.clips.ua.ac.be/pages/pattern-en13We tested many thresholds, and set 99% as threshold.14http://sourceforge.net/projects/mosesdecoder/15http://code.google.com/p/giza-pp/16http://sourceforge.net/projects/irstlm/17consisting of entries through 2012.data.
We used a 3-gram language model built onthe entire English Gigaword corpus.4.2 ResultTable 4 shows the overall results of our submit-ted systems and the results of an additional ex-periment.
In the additional experiment, we triedthe SMT-based approach described in Section 3.3for errors in SVA, Vform and Nn.
While the sys-tem based on the Treelet Language Model out-performed the SMT-based system on the SVA er-rors and the Vform errors, the binary classifier ap-proach did not perform as well as the SMT-basedsystem on the Nn errors.5 Discussion5.1 Subject-Verb Agreement and Verb FormWe provide here examples of our system?s output,beginning with a successful example.source: This is an age which most people *is re-tired and *has no sources of incomes.hypothesis: This is an age which most people areretired and have no sources of incomes.The source sentence of this pair includes two SVAerrors.
The first is that ?be?
should agree with itssubject ?people?
and must be ?are?.
Our system isable to correct errors where the misinflected pred-icate is adjacent to its subject.
The second erroris also an agreement error, in this case between?have?
and its subject ?people?.
Our model canassign probability to yields related to predicatesconditioned strongly on their subjects even if thedistance between the predicate and its subject islong.
The same can be said of Vform errors.One mistake made by our system is miscorrec-tion due to the negative effect of other errors.source/hypothesis: The rising life *expectancies*are like a two side sword to the modern world.31submitted system additional experimentsALL Verb Nn Prep ArtOrDet Verb NnPrecision 0.2707 0.1378 0.4452 0.2649 0.3118 0.2154 0.3687original Recall 0.1832 0.2520 0.1641 0.1286 0.2029 0.0569 0.2020F-score 0.2185 0.1782 0.2399 0.1732 0.2458 0.0900 0.2610Precision 0.3392 0.1814 0.5578 0.3245 0.4027 0.3846 0.4747revised Recall 0.2405 0.2867 0.1708 0.1494 0.2497 0.0880 0.2137F-score 0.2814 0.2222 0.2616 0.2046 0.3082 0.1433 0.2947Table 4: Results of the submitted system for each type of error and results of additional experimentswith the SMT-based system.
The score is evaluated on the m2scorer (Dahlmeier and Ng, 2012).
ALLis the official result of formal run, and each of the others shows the result of the corresponding errortype.
Since our system did not distinguish SVA and Vform, we report the combined result for them in thecolumn Verb.gold: The rising life expectancy is like a two sidesword to the modern world.Since the subject of ?are?
is ?expectancies?, thesentence looks correct at first.
However, this ex-ample includes not only an SVA error but also anNn error, and therefore the predicate ?are?
shouldbe corrected along with correcting its subject ?ex-pectancies?.An example of a Vform error is shown below.source/hypothesis: Besides, we can try to reducethe bad effect *cause by the new technology.gold: Besides, we can try to reduce the bad effectcaused by the new technology.The word ?cause?
is tagged as ?NN?
in this sen-tence.
This error is ignored because our systemmakes corrections on the basis of the original POStag.
For a similar example, our system does notmake modifications between the to-infinitive andthe other forms.5.2 Noun NumberWe provide here examples of our system?s output,beginning with a successful example.source: many of cell *phone are equipped withGPShypothesis/gold: many of cell phones areequipped with GPSIn the example, the noun ?phone?
should be in theplural form ?phones?.
This is because the phrase?many of?
modifies the noun.
In this case, the un-igrams ?many?
and ?are?, and the bigram ?manyof?
are features with strong weights for the pluralclass as expected.However, n-gram features sometimes work tothe contrary of our expectations.source/hypothesis: RFID is not only used totrack products for logistical and storage *purpose,it is also used to track peoplegold: RFID is not only used to track products forlogistical and storage purposes, it is also used totrack peopleThe ?purpose?
is in the PP which is modified by?products?.
Thus, ?purpose?
should not be af-fected by the following words.
However, the verb?is?, which is immediately after ?purpose?, has astrong influence to keep the word in singular form.Therefore, it may be better not to use a verb thatthe word is not immediately dependent on as a fea-ture.5.3 Prepositions and DeterminersWhile the SMT-based system can capture thestatistics of learners?
errors, it cannot correctphrases that are not found in the training corpus.
(1) source: *with economic situationgold: in economic situation(2) source: *with such situationgold: in such situationOur system was not able to correct the sourcephrase in (1), in spite of the fact that the similarphrase pair (2) was in the training data.
To correctsuch errors, we should construct a system that al-lows a gap in source and target phrases as in Galleyand Manning (2010).326 ConclusionThis paper described the architecture of our cor-rection system for errors in verb forms, subjectverb agreement, noun number, prepositions anddeterminers.
For verb form and subject verbagreement errors, we used the Treelet LanguageModel.
By taking advantage of rich syntactic in-formation, it corrects subject-verb agreement er-rors which need to be inflected according to a dis-tant subject.
For noun number errors, we used abinary classifier using both learner and native cor-pora.
For preposition and determiner errors, webuilt an SMT-based system trained on a larger cor-pus than those used in prior works.
We show thatour subsystems are effective to each error type.
Onthe other hand, our system cannot handle the er-rors strongly related to other errors well.
In futurework we will explore joint correction of multipleerror types, especially noun number and subject-verb agreement errors, which are closely relatedto each other.AcknowledgementsWe would like to thank Yangyang Xi for giving uspermission to use text from Lang-8 and the anony-mous reviewers for helpful comments.ReferencesChris Brockett, William B. Dolan, and Michael Ga-mon.
2006.
Correcting ESL errors using phrasalsmt techniques.
In Proceedings of COLING/ACL2006, pages 249?256.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Koby Crammer, Alex Kulesza, and Mark Dredze.2009.
Adaptive regularization of weight vectors.
InProceedings of NIPS 2009, pages 414?422.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In Pro-ceedings of NAACL 2012, pages 568?572.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerenglish: The NUS corpus of learner English.
In Pro-ceedings of BEA 2013, pages 313?330.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: a report on the preposition anddeterminer error correction shared task.
In Proceed-ings of BEA 2012, pages 54?62.Michel Galley and Christopher D. Manning.
2010.Accurate non-hierarchical phrase-based translation.In Processing of HLT/NAACL 2010, pages 966?974.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-chai Supnithi, and Hitoshi Isahara.
2003.
Auto-matic error detection in the Japanese learners?
En-glish spoken data.
In Proceedings of ACL 2003,pages 145?148.Philipp Koehn, Franz Josef Och, and Daniel C Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT/NAACL 2003, pages 48?54.John Lee and Stephanie Seneff.
2008.
Correcting mis-use of verb forms.
In Proceedings of ACL 2008,pages 174?182.Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-machi, Masaaki Nagata, and Yuji Matsumoto.
2012.The effect of learner corpus size in grammatical er-ror correction of ESL writings.
In Proceedings ofCOLING 2012, pages 863?872.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of ACL2002, pages 295?302.Adam Pauls and Dan Klein.
2012.
Large-scale syntac-tic language modeling with treelets.
In Proceedingsof ACL 2012, pages 959?968.Sameer S Pradhan, Wayne H Ward, Kadri Hacioglu,James H Martin, and Dan Jurafsky.
2004.
Shallowsemantic parsing using support vector machines.
InProceedings of HLT/NAACL 2004, pages 233?240.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proceedings of ACL 2005,pages 271?279.33
