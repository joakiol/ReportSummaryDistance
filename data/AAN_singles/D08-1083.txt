Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793?801,Honolulu, October 2008. c?2008 Association for Computational LinguisticsLearning with Compositional Semantics as Structural Inference forSubsentential Sentiment AnalysisYejin Choi and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853{ychoi,cardie}@cs.cornell.eduAbstractDetermining the polarity of a sentiment-bearing expression requires more than a sim-ple bag-of-words approach.
In particular,words or constituents within the expressioncan interact with each other to yield a particu-lar overall polarity.
In this paper, we view suchsubsentential interactions in light of composi-tional semantics, and present a novel learning-based approach that incorporates structural in-ference motivated by compositional seman-tics into the learning procedure.
Our exper-iments show that (1) simple heuristics basedon compositional semantics can perform bet-ter than learning-based methods that do not in-corporate compositional semantics (accuracyof 89.7% vs. 89.1%), but (2) a method thatintegrates compositional semantics into learn-ing performs better than all other alterna-tives (90.7%).
We also find that ?content-word negators?, not widely employed in pre-vious work, play an important role in de-termining expression-level polarity.
Finally,in contrast to conventional wisdom, we findthat expression-level classification accuracyuniformly decreases as additional, potentiallydisambiguating, context is considered.1 IntroductionDetermining the polarity of sentiment-bearing ex-pressions at or below the sentence level requiresmore than a simple bag-of-words approach.
One ofthe difficulties is that words or constituents withinthe expression can interact with each other to yielda particular overall polarity.
To facilitate our discus-sion, consider the following examples:1: [I did [not]?
have any [doubt]?
about it.
]+2: [The report [eliminated]?
my [doubt]?.
]+3: [They could [not]?
[eliminate]?
my [doubt]?.
]?In the first example, ?doubt?
in isolation carriesa negative sentiment, but the overall polarity of thesentence is positive because there is a negator ?not?,which flips the polarity.
In the second example, both?eliminated?
and ?doubt?
carry negative sentimentin isolation, but the overall polarity of the sentenceis positive because ?eliminated?
acts as a negator forits argument ?doubt?.
In the last example, there areeffectively two negators ?
?not?
and ?eliminated?
?which reverse the polarity of ?doubt?
twice, result-ing in the negative polarity for the overall sentence.These examples demonstrate that words or con-stituents interact with each other to yield theexpression-level polarity.
And a system that sim-ply takes the majority vote of the polarity of indi-vidual words will not work well on the above exam-ples.
Indeed, much of the previous learning-basedresearch on this topic tries to incorporate salient in-teractions by encoding them as features.
One ap-proach includes features based on contextual va-lence shifters1 (Polanyi and Zaenen, 2004), whichare words that affect the polarity or intensity of sen-timent over neighboring text spans (e.g., Kennedyand Inkpen (2005), Wilson et al (2005), Shaikh etal.
(2007)).
Another approach encodes frequent sub-sentential patterns (e.g., McDonald et al (2007)) asfeatures; these might indirectly capture some of thesubsentential interactions that affect polarity.
How-1For instance, ?never?, ?nowhere?, ?little?, ?most?, ?lack?,?scarcely?, ?deeply?.793ever, both types of approach are based on learningmodels with a flat bag-of-features: some structuralinformation can be encoded as higher order features,but the final representation of the input is still a flatfeature vector that is inherently too limited to ade-quately reflect the complex structural nature of theunderlying subsentential interactions.
(Liang et al,2008)Moilanen and Pulman (2007), on the other hand,handle the structural nature of the interactions moredirectly using the ideas from compositional seman-tics (e.g., Montague (1974), Dowty et al (1981)).
Inshort, the Principle of Compositionality states thatthe meaning of a compound expression is a func-tion of the meaning of its parts and of the syntac-tic rules by which they are combined (e.g., Mon-tague (1974), Dowty et al (1981)).
And Moilanenand Pulman (2007) develop a collection of compo-sition rules to assign a sentiment value to individualexpressions, clauses, or sentences.
Their approachcan be viewed as a type of structural inference, buttheir hand-written rules have not been empiricallycompared to learning-based alternatives, which onemight expect to be more effective in handling someaspects of the polarity classification task.In this paper, we begin to close the gap betweenlearning-based approaches to expression-level po-larity classification and those founded on composi-tional semantics: we present a novel learning-basedapproach that incorporates structural inference mo-tivated by compositional semantics into the learningprocedure.Adopting the view point of compositional seman-tics, our working assumption is that the polarity of asentiment-bearing expression can be determined in atwo-step process: (1) assess the polarities of the con-stituents of the expression, and then (2) apply a rela-tively simple set of inference rules to combine themrecursively.
Rather than a rigid application of hand-written compositional inference rules, however, wehypothesize that an ideal solution to the expression-level polarity classification task will be a methodthat can exploit ideas from compositional seman-tics while providing the flexibility needed to handlethe complexities of real-world natural language ?exceptions, unknown words, missing semantic fea-tures, and inaccurate or missing rules.
The learning-based approach proposed in this paper takes a firststep in this direction.In addition to the novel learning approach, thispaper presents new insights for content-word nega-tors, which we define as content words that cannegate the polarity of neighboring words or con-stituents.
(e.g., words such as ?eliminated?
in theexample sentences).
Unlike function-word nega-tors, such as ?not?
or ?never?, content-word nega-tors have been recognized and utilized less activelyin previous work.
(Notable exceptions include e.g.,Niu et al (2005), Wilson et al (2005), and Moilanenand Pulman (2007).2)In our experiments, we compare learning- andnon-learning-based approaches to expression-levelpolarity classification ?
with and without com-positional semantics ?
and find that (1) simpleheuristics based on compositional semantics outper-form (89.7% in accuracy) other reasonable heuris-tics that do not incorporate compositional seman-tics (87.7%); they can also perform better than sim-ple learning-based methods that do not incorporatecompositional semantics (89.1%), (2) combininglearning with the heuristic rules based on compo-sitional semantics further improves the performance(90.7%), (3) content-word negators play an impor-tant role in determining the expression-level polar-ity, and, somewhat surprisingly, we find that (4)expression-level classification accuracy uniformlydecreases as additional, potentially disambiguating,context is considered.In what follows, we first explore heuristic-basedapproaches in ?2, then we present learning-based ap-proaches in ?3.
Next we present experimental resultsin ?4, followed by related work in ?5.2 Heuristic-Based MethodsThis section describes a set of heuristic-based meth-ods for determining the polarity of a sentiment-bearing expression.
Each assesses the polarity of thewords or constituents using a polarity lexicon thatindicates whether a word has positive or negativepolarity, and finds negators in the given expressionusing a negator lexicon.
The methods then infer theexpression-level polarity using voting-based heuris-tics (?
2.1) or heuristics that incorporate composi-tional semantics (?2.2).
The lexicons are described2See ?5.
Related Work for detailed discussion.794VOTE NEG(1) NEG(N) NEGEX(1) NEGEX(N) COMPOtype of negators none function-word function-word & content-wordmaximum # of negations applied 0 1 n 1 n nscope of negators N/A over the entire expression compositionalTable 1: Heuristic methods.
(n refers to the number of negators found in a given expression.
)Rules Examples1 Polarity( not [arg1] ) = ?
Polarity( arg1 ) not [bad]arg1.2 Polarity( [VP] [NP] ) = Compose( [VP], [NP] ) [destroyed]VP [the terrorism]NP .3 Polarity( [VP1] to [VP2] ) = Compose( [VP1], [VP2] ) [refused]V P1 to [deceive]V P2 the man.4 Polarity( [adj] to [VP] ) = Compose( [adj], [VP] ) [unlikely]adj to [destroy]V P the planet.5 Polarity( [NP1] [IN] [NP2] ) = Compose( [NP1], [NP2] ) [lack]NP1 [of]IN [crime]NP2 in rural areas.6 Polarity( [NP] [VP] ) = Compose( [VP], [NP] ) [pollution]NP [has decreased]V P .7 Polarity( [NP] be [adj] ) = Compose( [adj], [NP] ) [harm]NP is [minimal]adj .Definition of Compose( arg1, arg2 )Compose( arg1, arg2 ) =For COMPOMC: if (arg1 is a negator) then ?
Polarity( arg2 )(COMPOsition with Majority Class) else if (Polarity( arg1 ) == Polarity( arg2 )) then Polarity( arg1 )else the majority polarity of dataCompose( arg1, arg2 ) =For COMPOPR: if (arg1 is a negator) then ?
Polarity( arg2 )(COMPOsition with PRiority) else Polarity( arg1 )Table 2: Compositional inference rules motivated by compositional semantics.in ?2.3.2.1 VotingWe first explore five simple heuristics based on vot-ing.
VOTE is defined as the majority polarity voteby words in a given expression.
That is, we countthe number of positive polarity words and negativepolarity words in a given expression, and assign themajority polarity to the expression.
In the case of atie, we default to the prevailing polarity of the data.For NEG(1), we first determine the majority polar-ity vote as above, and then if the expression containsany function-word negator, flip the polarity of themajority vote once.
NEG(N) is similar to NEG(1), ex-cept we flip the polarity of the majority vote n timesafter the majority vote, where n is the number offunction-word negators in a given expression.NEGEX(1) and NEGEX(N) are defined similarly asNEG(1) and NEG(N) above, except both function-word negators and content-word negators are con-sidered as negators when flipping the polarity of themajority vote.
See Table 1 for summary.
Note that aword can be both a negator and have a negative priorpolarity.
For the purpose of voting, if a word is de-fined as a negator per the voting scheme, then thatword does not participate in the majority vote.For brevity, we refer to NEG(1) and NEG(N) col-lectively as NEG, and NEGEX(1) and NEGEX(N) col-lectively as NEGEX.2.2 Compositional semanticsWhereas the heuristics above use voting-based in-ference, those below employ a set of hand-writtenrules motivated by compositional semantics.
Table 2shows the definition of the rules along with moti-vating examples.
In order to apply a rule, we firstdetect a syntactic pattern (e.g., [destroyed]V P [theterrorism]NP ), then apply the Compose function asdefined in Table 2 (e.g., Compose([destroyed], [theterrorism]) by rule #2).33Our implementation uses part-of-speech tags and function-words to coarsely determine the patterns.
An implementation795Compose first checks whether the first argument isa negator, and if so, flips the polarity of the secondargument.
Otherwise, Compose resolves the polar-ities of its two arguments.
Note that if the secondargument is a negator, we do not flip the polarity ofthe first argument, because the first argument in gen-eral is not in the semantic scope of the negation.4 In-stead, we treat the second argument as a constituentwith negative polarity.We experiment with two variations of the Com-pose function depending on how conflicting polari-ties are resolved: COMPOMC uses a Compose func-tion that defaults to the Majority Class of the po-larity of the data,5 while COMPOPR uses a Composefunction that selects the polarity of the argument thathas higher semantic PRiority.
For brevity, we referto COMPOPR and COMPOMC collectively as COMPO.2.3 LexiconsThe polarity lexicon is initialized with the lexiconof Wilson et al (2005) and then expanded using theGeneral Inquirer dictionary.6 In particular, a wordcontained in at least two of the following categoriesis considered as positive: POSITIV, PSTV, POSAFF,PLEASUR, VIRTUE, INCREAS, and a word containedin at least one of the following categories is consid-ered as negative: NEGATIV, NGTV, NEGAFF, PAIN,VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRAN-LOSS.For the (function- and content-word) negator lex-icon, we collect a handful of seed words as well asGeneral Inquirer words that appear in either NOTLWor DECREAS category.
Then we expand the list ofcontent-negators using the synonym information ofWordNet (Miller, 1995) to take a simple vote amongsenses.based on parse trees might further improve the performance.4Moilanen and Pulman (2007) provide more detailed dis-cussion on the semantic scope of negations and the semanticpriorities in resolving polarities.5The majority polarity of the data we use for our experi-ments is negative.6Available at http://www.wjh.harvard.edu/?inquirer/.When consulting the General Inquirer dictionary, senses withless than 5% frequency and senses specific to an idiom aredropped.3 Learning-Based MethodsWhile we expect that a set of hand-written heuristicrules motivated by compositional semantics can beeffective for determining the polarity of a sentiment-bearing expression, we do not expect them to be per-fect.
Interpreting natural language is such a com-plex task that writing a perfect set of rules wouldbe extremely challenging.
Therefore, a more idealsolution would be a learning-based method that canexploit ideas from compositional semantics whileproviding the flexibility to the rigid application ofthe heuristic rules.
To this end, we present a novellearning-based approach that incorporates inferencerules inspired by compositional semantics into thelearning procedure (?3.2).
To assess the effect ofcompositional semantics in the learning-basedmeth-ods, we also experiment with a simple classifica-tion approach that does not incorporate composi-tional semantics (?3.1).
The details of these twoapproaches are elaborated in the following subsec-tions.3.1 Simple Classification (SC)Given an expression x consisting of n words x1,..., xn, the task is to determine the polarity y ?
{positive, negative} of x.
In our simple binaryclassification approach, x is represented as a vec-tor of features f(x), and the prediction y is given byargmaxyw?f(x, y), wherew is a vector of parameterslearned from training data.
In our experiment, weuse an online SVM algorithm called MIRA (MarginInfused Relaxed Algorithm) (Crammer and Singer,2003)7 for training.For each x, we encode the following features:?
Lexical: We add every word xi in x, and alsoadd the lemma of xi produced by the CASSpartial parser toolkit (Abney, 1996).?
Dictionary: In order to mitigate the problem ofunseen words in the test data, we add featuresthat describeword categories based on theGen-eral Inquirer dictionary.
We add this feature foreach xi that is not a stop word.?
Vote: We experiment with two variations ofvoting-related features: for SC-VOTE, we add7We use the Java implementation of this algorithmavailable at http://www.seas.upenn.edu/?strctlrn/StructLearn/StructLearn.html.796Simple Classification Classification with Compositional Inferencey ?
argmaxy score(y) Find K best z and denote them as Z = {z(1), ..., z(K)}l?
loss flat(y?, y) s.t.
?
i < j, score(z(i)) > score(z(j))w?
update(w, l, y?, y) zbad ?
mink z(k) s.t.
loss compo(y?, z(k), x) > 0(if such zbad not found in Z, skip parameter update for this.
)If loss compo(y?, z?, x) > 0zgood ?
mink z(k) s.t.
loss compo(y?, z(k), x) = 0z?
?
zgood(if such zgood not found in Z, stick to the original z?.
)l ?
loss compo(y?, zbad, x)?
loss compo(y?, z?, x)w?
update(w, l, z?, zbad)Definitions of score functions and loss functionsscore(y) := w ?
f(x, y) score(z) :=?i score(zi) :=?i w ?
f(x, zi, i)loss flat(y?, y) := if (y?
= y) 0 else 1 loss compo(y?, z, x) := if (y?
= C(x, z)) 0 else 1Figure 1: Training procedures.
y?
?
{positive, negative} denotes the true label for a given expression x = x1, ..., xn.z?
denotes the pseudo gold standard for hidden variables z.a feature that indicates the dominant polarity ofwords in the given expression, without consid-ering the effect of negators.
For SC-NEGEX,we count the number of content-word nega-tors as well as function-word negators to de-termine whether the final polarity should beflipped.
Then we add a conjunctive feature thatindicates the dominant polarity together withwhether the final polarity should be flipped.
Forbrevity, we refer to SC-VOTE and SC-NEGEXcollectively as SC.Notice that in this simple binary classification set-ting, it is inherently difficult to capture the compo-sitional structure among words in x, because f(x, y)is merely a flat bag of features, and the predictionis governed simply by the dot product of f(x, y) andthe parameter vector w.3.2 Classification with CompositionalInference (CCI)Next, instead of determining y directly from x,we introduce hidden variables z = (z1, ..., zn)as intermediate decision variables, where zi ?
{positive, negative, negator, none}, so that zirepresents whether xi is a word with posi-tive/negative polarity, or a negator, or none of theabove.
For simplicity, we let each intermediate de-cision variable zi (a) be determined independentlyfrom other intermediate decision variables, and (b)For each token xi,if xi is a word in the negator lexiconthen z?i ?
negatorelse if xi is in the polarity lexicon as negativethen z?i ?
negativeelse if xi is in the polarity lexicon as positivethen z?i ?
positiveelsethen z?i ?
noneFigure 2: Constructing Soft Gold Standard z?depend only on the input x, so that zi = argmaxziw ?f(x, zi, i), where f(x, zi, i) is the feature vector en-coding around the ith word (described on the nextpage).
Once we determine the intermediate decisionvariables, we apply the heuristic rules motivated bycompositional semantics (from Table 2) in order toobtain the final polarity y of x.
That is, y = C(x, z),where C is the function that applies the composi-tional inference, either COMPOPR or COMPOMC.For training, there are two issues we need tohandle: the first issue is dealing with the hiddenvariables z.
Because the structure of composi-tional inference C does not allow dynamic program-ming, it is intractable to perform exact expectation-maximization style training that requires enumerat-ing all possible values of the hidden variables z. In-stead, we propose a simple and tractable training797rule based on the creation of a soft gold standard forz.
In particular, we exploit the fact that in our task,we can automatically construct a reasonably accu-rate gold standard for z, denoted as z?
: as shown inFigure 2, we simply rely on the negator and polar-ity lexicons.
Because z?
is not always correct, weallow the training procedure to replace z?
with po-tentially better assignments as learning proceeds: inthe event that the soft gold standard z?
leads to an in-correct prediction, we search for an assignment thatleads to a correct prediction to replace z?.
The exactprocedure is given in Figure 1, and will be discussedagain shortly.Figure 1 shows how we modify the parameter up-date rule of MIRA (Crammer and Singer, 2003) toreflect the aspect of compositional inference.
In theevent that the soft gold standard z?
leads to an incor-rect prediction, we search for zgood, the assignmentwith highest score that leads to a correct prediction,and replace z?
with zgood.
In the event of no suchzgood being found among the K-best assignments ofz, we stick with z?.The second issue is finding the assignment of zwith the highest score(z) = ?i w ?
f(x, zi, i) thatleads to an incorrect prediction y = C(x, z).
Be-cause the structure of compositional inference Cdoes not allow dynamic programming, finding suchan assignment is again intractable.
We resort to enu-merating only over K-best assignments instead.
Ifnone of the K-best assignments of z leads to an in-correct prediction y, then we skip the training in-stance for parameter update.Features.
For each xi in x, we encode the follow-ing features:?
Lexical: We include the current word xi as wellas the lemma of xi produced by CASS partialparser toolkit (Abney, 1996).
We also add aboolean feature to indicate whether the currentword is a stop word.?
Dictionary: In order to mitigate the problemwith unseen words in the test data, we add fea-tures that describe word categories based on theGeneral Inquirer dictionary.
We add this fea-ture for each xi that is not a stop word.
Wealso add a number of boolean features that pro-vide following properties of xi using the polar-ity lexicon and the negator lexicon:?
whether xi is a function-word negator?
whether xi is a content-word negator?
whether xi is a negator of any kind?
the polarity of xi according to Wilson etal.
(2005)?s polarity lexicon?
the polarity of xi according to the lexiconderived from the General Inquirer dictio-nary?
conjunction of the above two features?
Vote: We encode the same vote feature that weuse for SC-NEGEX described in ?
3.1.As in the heuristic-based compositional semanticsapproach (?
2.2), we experiment with two variationsof this learning-based approach: CCI-COMPOPRand CCI-COMPOMC, whose compositional infer-ence rules are COMPOPR and COMPOMC respec-tively.
For brevity, we refer to both variations col-lectively as CCI-COMPO.4 ExperimentsThe experiments below evaluate our heuristic- andlearning-based methods for subsentential sentimentanalysis (?
4.1).
In addition, we explore the roleof context by expanding the boundaries of thesentiment-bearing expressions (?
4.2).4.1 Evaluation with given boundariesFor evaluation, we use the Multi-Perspective Ques-tion Answering (MPQA) corpus (Wiebe et al,2005), which consists of 535 newswire documentsmanually annotated with phrase-level subjectivityinformation.
We evaluate on all strong (i.e., inten-sity of expression is ?medium?
or higher), sentiment-bearing (i.e., polarity is ?positive?
or ?negative?)
ex-pressions.8 As a result, we can assume the bound-aries of the expressions are given.
Performance isreported using 10-fold cross-validation on 400 doc-uments; a separate 135 documents were used as adevelopment set.
Based on pilot experiments on thedevelopment data, we set parameters for MIRA asfollows: slack variable to 0.5, and the number ofincorrect labels (constraints) for each parameter up-date to 1.
The number of iterations (epochs) fortraining is set to 1 for simple classification, and to 48We discard expressions with confidencemarked as ?uncer-tain?.798Heuristic-Based Learning-BasedVOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI(1) (N) EX EX MC PR VOTE NEG COMPO COMPO(1) (N) EX MC PR86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7Table 3: Performance (in accuracy) on MPQA dataset.Heuristic-Based Learning-BasedVOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCIData (1) (N) EX EX MC PR VOTE NEG COMPO COMPO(1) (N) EX MC PR[-0,+0] 86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7[-1,+1] 86.4 81.0 81.2 87.2 87.2 89.3 89.0 88.3 88.4 89.5 89.4[-5,+5] 85.9 79.0 79.4 85.7 85.6 88.2 88.0 86.4 87.1 88.7 88.7[-?,+?]
85.3 75.8 76.9 83.9 83.9 87.0 86.9 85.8 85.8 87.3 87.5Table 4: Performance (in accuracy) on MPQA data set with varying boundaries of expressions.for classification with compositional inference.
Weuse K = 20 for classification with compositionalinference.Results.
Performance is reported in Table 3.
In-terestingly, the heuristic-based methods NEG (?82.2%) that only consider function-word negatorsperform even worse than VOTE (86.5%), which doesnot consider negators.
On the other hand, theNEGEXmethods (87.7%) that do consider content-wordnegators as well as function-word negators performbetter than VOTE.
This confirms the importance ofcontent-word negators for determining the polari-ties of expressions.
The heuristic-based methodsmotivated by compositional semantics COMPO fur-ther improve the performance over NEGEX, achiev-ing up to 89.7% accuracy.
In fact, these heuris-tics perform even better than the SC learning-basedmethods (?
89.1%).
This shows that heuristics thattake into account the compositional structure of theexpression can perform better than learning-basedmethods that do not exploit such structure.Finally, the learning-based methods that in-corporate compositional inference CCI-COMPO (?90.7%) perform better than all of the previousmethods.
The difference between CCI-COMPOPR(90.7%) and SC-NEGEX (89.1%) is statistically sig-nificant at the .05 level by paired t-test.
The dif-ference between COMPO and any other heuristic thatis not based on computational semantics is alsostatistically significant.
In addition, the differencebetween CCICOMPOPR (learning-based) and COM-POMC (non-learning-based) is statistically signifi-cant, as is the difference between NEGEX and VOTE.4.2 Evaluation with noisy boundariesOne might wonder whether employing additionalcontext outside the annotated expression boundariescould further improve the performance.
Indeed, con-ventional wisdom would say that it is necessary toemploy such contextual information (e.g., Wilson etal.
(2005)).
In any case, it is important to determinewhether our results will apply to more real-worldsettings where human-annotated expression bound-aries are not available.To address these questions, we gradually relaxour previous assumption that the exact boundaries ofexpressions are given: for each annotation bound-ary, we expand the boundary by x words for eachdirection, up to sentence boundaries, where x ?
{1, 5,?}.
We stop expanding the boundary if itwill collide with the boundary of an expression witha different polarity, so that we can consistently re-cover the expression-level gold standard for evalua-tion.
This expansion is applied to both the trainingand test data, and the performance is reported in Ta-ble 4.
From this experiment, we make the followingobservations:?
Expanding the boundaries hurts the perfor-799mance for any method.
This shows that most ofrelevant context for judging the polarity is con-tained within the expression boundaries, andmotivates the task of finding the boundaries ofopinion expressions.?
The NEGEX methods perform better than VOTEonly when the expression boundaries are rea-sonably accurate.
When the expression bound-aries are expanded up to sentence boundaries,they perform worse than VOTE.
We conjecturethis is because the scope of negators tends to belimited to inside of expression boundaries.?
The COMPO methods always perform betterthan any other heuristic-based methods.
Andtheir performance does not decrease as steeplyas the NEGEX methods as the expressionboundaries expand.
We conjecture this is be-cause methods based on compositional seman-tics can handle the scope of negators more ade-quately.?
Among the learning-based methods, those thatinvolve compositional inference (CCI-COMPO)always perform better than those that do not(SC) for any boundaries.
And learning withcompositional inference tend to perform bet-ter than the rigid application of heuristic rules(COMPO), although the relative performancegain decreases once the boundaries are relaxed.5 Related WorkThe task focused on in this paper is similar to thatof Wilson et al (2005) in that the general goal of thetask is to determine the polarity in context at a sub-sentence level.
However, Wilson et al (2005) for-mulated the task differently by limiting their evalua-tion to individual words that appear in their polaritylexicon.
Also, their approach was based on a flat bagof features, and only a few examples of what we callcontent-word negators were employed.Our use of compositional semantics for the taskof polarity classification is preceded by Moilanenand Pulman (2007), but our work differs in thatwe integrate the key idea of compositional seman-tics into learning-based methods, and that we per-form empirical comparisons among reasonable al-ternative approaches.
For comparison, we evalu-ated our approaches on the polarity classificationtask from SemEval-07 (Strapparava and Mihalcea,2007).
We achieve 88.6% accuracy with COMPOPR,90.1% with SCNEGEX, and 87.6% with CCICOM-POMC.9 There are a number of possible reasons forour lower performance vs. Moilanen and Pulman(2007) on this data set.
First, SemEval-07 does notinclude a training data set for this task, so we use400 documents from the MPQA corpus instead.
Inaddition, the SemEval-07 data is very different fromthe MPQA data in that (1) the polarity annotationis given only at the sentence level, (2) the sentencesare shorter, with simpler structure, and not as manynegators as the MPQA sentences, and (3) there aremany more instances with positive polarity than inthe MPQA corpus.Nairn et al (2006) also employ a ?polarity?
prop-agation algorithm in their approach to the semanticinterpretation of implicatives.
However, their notionof polarity is quite different from that assumed hereand in the literature on sentiment analysis.
In partic-ular, it refers to the degree of ?commitment?
of theauthor to the truth or falsity of a complement clausefor a textual entailment task.McDonald et al (2007) use a structured modelto determine the sentence-level polarity and thedocument-level polarity simultaneously.
But deci-sions at each sentence level does not consider struc-tural inference within the sentence.Among the studies that examined content-wordnegators, Niu et al (2005) manually collected asmall set of such words (referred as ?words thatchange phases?
), but their lexicon was designedmainly for the medical domain and the type of nega-tors was rather limited.
Wilson et al (2005) alsomanually collected a handful of content-word nega-tors (referred as ?general polarity shifters?
), but notextensively.
Moilanen and Pulman (2007) collecteda more extensive set of negators semi-automaticallyusing WordNet 2.1, but the empirical effect of suchwords was not explicitly investigated.9For lack of space, we only report our performance on in-stances with strong intensities as defined in Moilanen and Pul-man (2007), which amounts to only 208 test instances.
Thecross-validation set of MPQA contains 4.9k instances.8006 ConclusionIn this paper, we consider the task of determiningthe polarity of a sentiment-bearing expression, con-sidering the effect of interactions among words orconstituents in light of compositional semantics.
Wepresented a novel learning-based approach that in-corporates structural inference motivated by compo-sitional semantics into the learning procedure.
Ourapproach can be considered as a small step towardbridging the gap between computational semanticsand machine learning methods.
Our experimen-tal results suggest that this direction of research ispromising.
Future research includes an approachthat learns the compositional inference rules fromdata.AcknowledgmentsThis work was supported in part by National ScienceFoundation Grants BCS-0624277 and IIS-0535099and by Department of Homeland Security GrantN0014-07-1-0152.
We also thank Eric Breck, Lil-lian Lee, Mats Rooth, the members of the CornellNLP reading seminar, and the EMNLP reviewers forinsightful comments on the submitted version of thepaper.ReferencesSteven Abney.
1996.
Partial parsing via finite-statecascades.
Journal of Natural Language Engineering,2(4):337344.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
JMLR3:951.David R. Dowty, Robert E. Wall and Stanley Peters.1981.
Introduction to Montague Semantics.Andrea Esuli and Fabrizio Sebastiani.
2006.
SentiWord-Net: A Publicly Available Lexical Resource for Opin-ion Mining.
In Proceedings of 5th Conference on Lan-guage Resources and Evaluation (LREC),.Percy Liang, Hal Daume?
III and Dan Klein.
2008.
Struc-ture Compilation: Trading Structure for Features.
InInternational Conference on Machine Learning.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings of theACM SIGKDD International Conference on Knowl-edge Discovery & Data Mining (KDD-2004).Alistair Kennedy and Diana Inkpen.
2005.
Senti-ment Classification of Movie and Product Reviews Us-ing Contextual Valence Shifters.
In Proceedings ofFINEXIN 2005, Workshop on the Analysis of Infor-mal and Formal Information Exchange during Nego-tiations.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In Proceedings of COLING.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells and Jeff Reynar.
2007.
Structured Models forFine-to-Coarse Sentiment Analysis.
In Proceedings ofAssociation for Computational Linguistics (ACL) .George A. Miller.
1995.
WordNet: a lexical database forEnglish.
In Communications of the ACM, 38(11):3941Richard Montague.
1974.
Formal Philosophy; Selectedpapers of Richard Montague.
Yale University Press.Karo Moilanen and Stephen Pulman.
2007.
SentimentComposition.
In Proceedings of Recent Advances inNatural Language Processing (RANLP 2007).Rowan Nairn, Cleo Condoravdi and Lauri Karttunen2006.
Computing relative polarity for textual infer-ence.
In Inference in Computational Semantics (ICoS-5).Yun Niu, Xiaodan Zhu, Jianhua Li and Graeme Hirst.2005.
Analysis of polarity information inmedical text.In Proceedings of the American Medical InformaticsAssociation 2005 Annual Symposium (AMIA).Livia Polanyi and Annie Zaenen.
2004.
Contextual lex-ical valence shifters.
In Exploring Attitude and Affectin Text: Theories and Applications: Papers from the2004 Spring Symposium, AAAI.Mostafa Shaikh, Helmut Prendinger and MitsuruIshizuka.
2007.
Assessing sentiment of text by se-mantic dependency and contextual valence analysis.In Proc 2nd Int?l Conf on Affective Computing and In-telligent Interaction (ACII?07).Carlo Strapparava and Rada Mihalcea.
2007.
Semeval-2007 task 14: Affective text.
In Proceedings of Se-mEval.Janyce Wiebe, Theresa Wilson and Claire Cardie.
2005.Annotating expressions of opinions and emotionsin language.
In Language Resources and Evalua-tion (formerly Computers and the Humanities), 39(2-3):165210.Theresa Wilson, Janyce Wiebe and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of HLT/EMNLP.801
