Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403?411,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExtracting Parallel Sentences from Comparable Corpora using DocumentLevel AlignmentJason R. Smith?Center for Lang.
and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218jsmith@cs.jhu.eduChris Quirk and Kristina ToutanovaMicrosoft ResearchOne Microsoft WayRedmond, WA 98052{chrisq,kristout}@microsoft.comAbstractThe quality of a statistical machine transla-tion (SMT) system is heavily dependent uponthe amount of parallel sentences used in train-ing.
In recent years, there have been severalapproaches developed for obtaining parallelsentences from non-parallel, or comparabledata, such as news articles published withinthe same time period (Munteanu and Marcu,2005), or web pages with a similar structure(Resnik and Smith, 2003).
One resource notyet thoroughly explored is Wikipedia, an on-line encyclopedia containing linked articlesin many languages.
We advance the stateof the art in parallel sentence extraction bymodeling the document level alignment, mo-tivated by the observation that parallel sen-tence pairs are often found in close proximity.We also include features which make use ofthe additional annotation given by Wikipedia,and features using an automatically inducedlexicon model.
Results for both accuracyin sentence extraction and downstream im-provement in an SMT system are presented.1 IntroductionFor any statistical machine translation system, thesize of the parallel corpus used for training is a ma-jor factor in its performance.
For some languagepairs, such as Chinese-English and Arabic-English,large amounts of parallel data are readily available,but for most language pairs this is not the case.
The?This research was conducted during the author?s intern-ship at Microsoft Research.domain of the parallel corpus also strongly influ-ences the quality of translations produced.
Manyparallel corpora are taken from the news domain, orfrom parliamentary proceedings.
Translation qual-ity suffers when a system is not trained on any datafrom the domain it is tested on.While parallel corpora may be scarce, compara-ble, or semi-parallel corpora are readily availablein several domains and language pairs.
These cor-pora consist of a set of documents in two languagescontaining similar information.
(See Section 2.1for a more detailed description of the types of non-parallel corpora.)
In most previous work on ex-traction of parallel sentences from comparable cor-pora, some coarse document-level similarity is usedto determine which document pairs contain paral-lel sentences.
For identifying similar web pages,Resnik and Smith (2003) compare the HTML struc-ture.
Munteanu and Marcu (2005) use publicationdate and vector-based similarity (after projectingwords through a bilingual dictionary) to identifysimilar news articles.Once promising document pairs are identified,the next step is to extract parallel sentences.
Usu-ally, some seed parallel data is assumed to be avail-able.
This data is used to train a word align-ment model, such as IBM Model 1 (Brown et al,1993) or HMM-based word alignment (Vogel et al,1996).
Statistics from this word alignment modelare used to train a classifier which identifies bilin-gual sentence pairs as parallel or not parallel.
Thisclassifier is applied to all sentence pairs in docu-ments which were found to be similar.
Typically,some pruning is done to reduce the number of sen-403tence pairs that need to be classified.While these methods have been applied to newscorpora and web pages, very little attention hasbeen given to Wikipedia as a source of parallel sen-tences.
This is surprising, given that Wikipediacontains annotated article alignments, and muchwork has been done on extracting bilingual lexi-cons on this dataset.
Adafre and de Rijke (2006)extracted similar sentences from Wikipedia articlepairs, but only evaluated precision on a small num-ber of extracted sentences.In this paper, we more thoroughly investigateWikipedia?s viability as a comparable corpus, anddescribe novel methods for parallel sentence ex-traction.
Section 2 describes the multilingual re-sources available in Wikipedia.
Section 3 gives fur-ther background on previous methods for parallelsentence extraction on comparable corpora, and de-scribes our approach, which finds a global sentencealignment between two documents.
In Section4, we compare our approach with previous meth-ods on datasets derived from Wikipedia for threelanguage pairs (Spanish-English, German-English,and Bulgarian-English), and show improvements indownstream SMT performance by adding the paral-lel data we extracted.2 Wikipedia as a Comparable CorpusWikipedia (Wikipedia, 2004) is an online collabo-rative encyclopedia available in a wide variety oflanguages.
While the English Wikipedia is thelargest, with over 3 million articles, there are 24language editions with at least 100,000 articles.Articles on the same topic in different languagesare also connected via ?interwiki?
links, which areannotated by users.
This is an extremely valuableresource when extracting parallel sentences, as thedocument alignment is already provided.
Table1 shows how many of these ?interwiki?
links arepresent between the English Wikipedia and the 16largest non-English Wikipedias.Wikipedia?s markup contains other useful indica-tors for parallel sentence extraction.
The many hy-perlinks found in articles have previously been usedas a valuable source of information.
(Adafre andde Rijke, 2006) use matching hyperlinks to iden-tify similar sentences.
Two links match if the arti-Figure 1: Captions for an image of a foil in English andSpanishcles they refer to are connected by an ?interwiki?link.
Also, images in Wikipedia are often storedin a central source across different languages; thisallows identification of captions which may be par-allel (see Figure 1).
Finally, there are other minorforms of markup which may be useful for findingsimilar content across languages, such as lists andsection headings.
In Section 3.3, we will explainhow features are derived from this markup.2.1 Types of Non-Parallel CorporaFung and Cheung (2004) give a more fine-graineddescription of the types of non-parallel corpora,which we will briefly summarize.
A noisy parallelcorpus has documents which contain many parallelsentences in roughly the same order.
Comparablecorpora contain topic aligned documents which arenot translations of each other.
The corpora Fungand Cheung (2004) examine are quasi-comparable:they contain bilingual documents which are notnecessarily on the same topic.Wikipedia is a special case, since the alignedarticle pairs may range from being almost com-pletely parallel (e.g., the Spanish and English en-tries for ?Antiparticle?)
to containing almost no par-allel sentences (the Spanish and English entries for?John Calvin?
), despite being topic-aligned.
It isbest characterized as a mix of noisy parallel andcomparable article pairs.
Some Wikipedia authorswill translate articles from another language; others404French German Polish Italian Dutch Portuguese Spanish Japanese496K 488K 384K 380K 357K 323K 311K 252KRussian Swedish Finnish Chinese Norwegian Volapu?k Catalan Czech232K 197K 146K 142K 141K 106K 103K 87KTable 1: Number of aligned bilingual articles in Wikipedia by language (paired with English).write the content themselves.
Furthermore, even ar-ticles created through translations may later divergedue to independent edits in either language.3 Models for Parallel Sentence ExtractionIn this section, we will focus on methods for ex-tracting parallel sentences from aligned, compara-ble documents.
The related problem of automaticdocument alignment in news and web corpora hasbeen explored by a number of researchers, includ-ing Resnik and Smith (2003), Munteanu and Marcu(2005), Tillmann and Xu (2009), and Tillmann(2009).
Since our corpus already contains docu-ment alignments, we sidestep this problem, and willnot discuss further details of this issue.
That said,we believe that our methods will be effective in cor-pora without document alignments when combinedwith one of the aforementioned algorithms.3.1 Binary Classifiers and RankersMuch of the previous work involves building abinary classifier for sentence pairs to determinewhether or not they are parallel (Munteanu andMarcu, 2005; Tillmann, 2009).
The training datausually comes from a standard parallel corpus.There is a substantial class imbalance (O(n) pos-itive examples, and O(n2) negative examples), andvarious heuristics are used to mitigate this prob-lem.
Munteanu and Marcu (2005) filter out neg-ative examples with high length difference or lowword overlap (based on a bilingual dictionary).We propose an alternative approach: we learna ranking model, which, for each sentence in thesource document, selects either a sentence in thetarget document that it is parallel to, or ?null?.
Thisformulation of the problem avoids the class imbal-ance issue of the binary classifier.In both the binary classifier approach and theranking approach, we use a Maximum Entropyclassifier, following Munteanu and Marcu (2005).3.2 Sequence ModelsIn Wikipedia article pairs, it is common for par-allel sentences to occur in clusters.
A global sen-tence alignment model is able to capture this phe-nomenon.
For both parallel and comparable cor-pora, global sentence alignments have been used,though the alignments were monotonic (Gale andChurch, 1991; Moore, 2002; Zhao and Vogel,2002).
Our model is a first order linear chain Condi-tional Random Field (CRF) (Lafferty et al, 2001).The set of source and target sentences are observed.For each source sentence, we have a hidden vari-able indicating the corresponding target sentenceto which it is aligned (or null).
The model is simi-lar to the discriminative CRF-based word alignmentmodel of (Blunsom and Cohn, 2006).3.3 FeaturesOur features can be grouped into four categories.Features derived from word alignmentsWe use a feature set inspired by (Munteanu andMarcu, 2005), who defined features primarily basedon IBM Model 1 alignments (Brown et al, 1993).We also use HMM word alignments (Vogel et al,1996) in both directions (source to target and targetto source), and extract the following features basedon these four alignments:11.
Log probability of the alignment2.
Number of aligned/unaligned words3.
Longest aligned/unaligned sequence of words4.
Number of words with fertility 1, 2, and 3+We also define two more features which are in-dependent of word alignment models.
One is asentence length feature taken from (Moore, 2002),1These are all derived from the one best alignment, andnormalized by sentence length.405which models the length ratio between the sourceand target sentences with a Poisson distribution.The other feature is the difference in relative doc-ument position of the two sentences, capturing theidea that the aligned articles have a similar topicprogression.The above features are all defined on sentencepairs, and are included in the binary classifier andranking model.Distortion featuresIn the sequence model, we use additional dis-tortion features, which only look at the differencebetween the position of the previous and currentaligned sentences.
One set of features bins thesedistances; another looks at the absolute differencebetween the expected position (one after the previ-ous aligned sentence) and the actual position.Features derived from Wikipedia markupThree features are derived from Wikipedia?smarkup.
The first is the number of matching linksin the sentence pair.
The links are weighted by theirinverse frequency in the document, so a link thatappears often does not contribute much to this fea-ture?s value.
The image feature fires whenever twosentences are captions of the same image, and thelist feature fires when two sentences are both itemsin a list.
These last two indicator features fire witha negative value when the feature matches on onesentence and not the other.None of the above features fire on a null align-ment, in either the ranker or CRF.
There is also abias feature for these two models, which fires on allnon-null alignments.Word-level induced lexicon featuresA common problem with approaches for paral-lel sentence classification, which rely heavily onalignment models trained from unrelated corpora,is low recall due to unknown words in the candi-date sentence-pairs.
One approach that begins toaddress this problem is the use of self-training, asin (Munteanu and Marcu, 2005).
However, a self-trained sentence pair extraction system is only ableto acquire new lexical items that occur in parallelsentences.
Within Wikipedia, many linked articlepairs do not contain any parallel sentences, yet con-tain many words and phrases that are good transla-tions of each other.In this paper we explore an alternative approachto lexicon acquisition for use in parallel sentenceextraction.
We build a lexicon model using an ap-proach similar to ones developed for unsupervisedlexicon induction from monolingual or compara-ble corpora (Rapp, 1999; Koehn and Knight, 2002;Haghighi et al, 2008).
We briefly describe the lex-icon model and its use in sentence-extraction.The lexicon model is based on a probabilisticmodelP (wt|ws, T, S) wherewt is a word in the tar-get language, ws is a word in the source language,and T and S are linked articles in the target andsource languages, respectively.We train this model similarly to the sentence-extraction ranking model, with the difference thatwe are aligning word pairs and not sentence pairs.The model is trained from a small set of annotatedWikipedia article pairs, where for some words inthe source language we have marked one or morewords as corresponding to the source word (in thecontext of the article pair), or have indicated that thesource word does not have a corresponding transla-tion in the target article.
The word-level annotatedarticles are disjoint from the sentence-aligned arti-cles described in Section 4.
The following featuresare used in the lexicon model:Translation probability.
This is the translationprobability p(wt|ws) from the HMM word align-ment model trained on the seed parallel data.
Wealso use the probability in the other direction, aswell as the log-probabilities in the two directions.Position difference.
This is the absolute value ofthe difference in relative position of words ws andwt in the articles S and T .Orthographic similarity.
This is a function of theedit distance between source and target words.
Theedit distance between words written in different al-phabets is computed by first performing a determin-istic phonetic translation of the words to a commonalphabet.
The translation is inexact and this is apromising area for improvement.
A similar sourceof information has been used to create seed lexiconsin (Koehn and Knight, 2002) and as part of the fea-ture space in (Haghighi et al, 2008).Context translation probability.
This featurelooks at all words occurring next to word ws in the406article S and next to wt in the article T in a localcontext window (we used one word to the left andone word to the right), and computes several scor-ing functions measuring the translation correspon-dence between the contexts (using the IBM Model1 trained from seed parallel data).
This feature issimilar to distributional similarity measures used inprevious work, with the difference that it is limitedto contexts of words within a linked article pair.Distributional similarity.
This feature corre-sponds more closely to context similarity measuresused in previous work on lexicon induction.
Foreach source headword ws, we collect a distribu-tion over context positions o ?
{?2,?1,+1,+2}and context words vs in those positions based on acount of times a context word occurred at that off-set from a headword: P (o, vs|ws) ?
weight(o) ?C(ws, o, vs).
Adjacent positions ?1 and +1 havea weight of 2; other positions have a weight of 1.Likewise we gather a distribution over target wordsand contexts for each target headword P (o, vt|wt).Using an IBM Model 1 word translation tableP (vt|vs) estimated on the seed parallel corpus,we estimate a cross-lingual context distribution asP (o, vt|ws) =?vs P (vt|vs) ?
P (o, vs|ws).
We de-fine the similarity of a words ws and wt as one mi-nus the Jensen-Shannon divergence of the distribu-tions over positions and target words.2Given this small set of feature functions, wetrain the weights of a log-linear ranking model forP (wt|ws, T, S), based on the word-level annotatedWikipedia article pairs.
After a model is trained,we generate a new translation table Plex(t|s) whichis defined as Plex(t|s) ?
?t?T,s?S P (t|s, T, S).The summation is over occurrences of the sourceand target word in linked Wikipedia articles.
Thisnew translation table is used to define anotherHMM word-alignment model (together with dis-tortion probabilities trained from parallel data) foruse in the sentence extraction models.
Two copiesof each feature using the HMM word alignmentmodel are generated: one using the seed data HMM2We restrict our attention to words with ten or more occur-rences, since rare words have poorly estimated distributions.Also we discard the contribution from any context position andword pair that relates to more than 1,000 distinct source or tar-get words, since it explodes the computational overhead andhas little impact on the final similarity score.model, and another using this new HMM model.The training data for Bulgarian consisted of twopartially annotated Wikipedia article pairs.
ForGerman and Spanish we used the feature weightsof the model trained on Bulgarian, because we didnot have word-level annotated Wikipedia articles.4 Experiments4.1 DataWe annotated twenty Wikipedia article pairs forthree language pairs: Spanish-English, Bulgarian-English, and German-English.
Each sentencein the source language was annotated with pos-sible parallel sentences in the target language(the target language was English in all experi-ments).
The pairs were annotated with a qualitylevel: 1 if the sentences contained some parallelfragments, 2 if the sentences were mostly paral-lel with some missing words, and 3 if the sen-tences appeared to be direct translations.
In allexperiments, sentence pairs with quality 2 or 3were taken as positive examples.
The resultingdatasets are available at http://research.microsoft.com/en-us/people/chrisq/wikidownload.aspx.For our seed parallel data, we used the Europarlcorpus (Koehn, 2005) for Spanish and German andthe JRC-Aquis corpus for Bulgarian, plus the articletitles for parallel Wikipedia documents, and trans-lations available from Wiktionary entries.34.2 Intrinsic EvaluationUsing 5-fold cross-validation on the 20 documentpairs for each language condition, we compared thebinary classifier, ranker, and CRF models for paral-lel sentence extraction.
To tune for precision/recall,we used minimum Bayes risk decoding.
We definethe loss L(?, ?)
of picking target sentence ?
whenthe correct target sentence is ?
as 0 if ?
= ?, ?if ?
= NULL and ?
6= NULL, and 1 otherwise.By modifying the null loss ?, the precision/recalltrade-off can be adjusted.
For the CRF model, weused posterior decoding to make the minimum riskdecision rule tractable.
As a summary measure ofthe performance of the models at different levels ofrecall we use average precision as defined in (Ido3Wiktionary is an online collaborative dictionary, similar toWikipedia.407Language Pair Binary Classifier Ranker CRFAvg Prec R@90 R@80 Avg Prec R@90 R@80 Avg Prec R@90 R@80English-Bulgarian 75.7 33.9 56.2 76.3 38.8 57.0 80.6 52.9 59.5English-Spanish 90.4 81.3 87.6 93.4 81.0 84.5 94.7 87.6 90.2English-German 61.8 9.4 27.5 66.4 25.7 42.4 78.9 52.2 54.7Table 2: Average precision, recall at 90% precision, and recall at 80% precision for each model in all three languagepairs.
In these experiments, the Wikipedia features and lexicon features are omitted.Setting Ranker CRFAvg Prec R@90 R@80 Avg Prec R@90 R@80English-BulgarianOne Direction 76.3 38.8 57.0 80.6 52.9 59.5Intersected 78.2 47.9 60.3 79.9 38.8 57.0Intersected +Wiki 80.8 39.7 68.6 82.1 53.7 62.8Intersected +Wiki +Lex 89.3 64.4 79.3 90.9 72.0 81.8English-SpanishOne Direction 93.4 81.0 84.5 94.7 87.6 90.2Intersected 94.3 82.4 89.0 95.4 88.5 91.8Intersected +Wiki 94.5 82.4 89.0 95.6 89.2 92.7Intersected +Wiki +Lex 95.8 87.4 91.1 96.4 90.4 93.7English-GermanOne Direction 66.4 25.7 42.4 78.9 52.2 54.7Intersected 71.9 36.2 43.8 80.9 54.0 67.0Intersected +Wiki 74.0 38.8 45.3 82.4 56.9 71.0Intersected +Wiki +Lex 78.7 46.4 59.1 83.9 58.7 68.8Table 3: Average precision, recall at 90% precision, and recall at 80% precision for the Ranker and CRF in all threelanguage pairs.
?+Wiki?
indicates that Wikipedia features were used, and ?+Lex?
means the lexicon features wereused.et al, 2006).
We also report recall at precision of90 and 80 percent.
Table 2 compares the differentmodels in all three language pairs.In our next set of experiments, we looked at theeffects of the Wikipedia specific features.
Since theranker and CRF are asymmetric models, we alsoexperimented with running the models in both di-rections and combining their outputs by intersec-tion.
These results are shown in Table 3.Identifying the agreement between two asym-metric models is a commonly exploited trick else-where in machine translation.
It is mostly effec-tive here as well, improving all cases except forthe Bulgarian-English CRF where the regression isslight.
More successful are the Wikipedia features,which provide an auxiliary signal of potential par-allelism.The gains from adding the lexicon-based featurescan be dramatic as in the case of Bulgarian (theCRF model average precision increased by nearly9 points).
The lower gains on Spanish and Germanmay be due in part to the lack of language-specifictraining data.
These results are very promising andmotivate further exploration.
We also note that thisis perhaps the first successful practical applicationof an automatically induced word translation lexi-con.4.3 SMT EvaluationWe also present results in the context of a full ma-chine translation system to evaluate the potentialutility of this data.
A standard phrasal SMT sys-tem (Koehn et al, 2003) serves as our testbed, us-ing a conventional set of models: phrasal mod-408els of source given target and target given source;lexical weighting models in both directions, lan-guage model, word count, phrase count, distortionpenalty, and a lexicalized reordering model.
Giventhat the extracted Wikipedia data takes the standardform of parallel sentences, it would be easy to ex-ploit this same data in a number of systems.For each language pair we explored two trainingconditions.
The ?Medium?
data condition used eas-ily downloadable corpora: Europarl for German-English and Spanish-English, and JRC/Acquis forBulgarian-English.
Additionally we included titlesof all linked Wikipedia articles as parallel sentencesin the medium data condition.
The ?Large?
datacondition includes all the medium data, and also in-cludes using a broad range of available sources suchas data scraped from the web (Resnik and Smith,2003), data from the United Nations, phrase books,software documentation, and more.In each condition, we explored the impact of in-cluding additional parallel sentences automaticallyextracted from Wikipedia in the system trainingdata.
For German-English and Spanish-English,we extracted data with the null loss adjusted toachieve an estimated precision of 95 percent, andfor English-Bulgarian a precision of 90 percent.
Ta-ble 4 summarizes the characteristics of these datasets.
We were pleasantly surprised at the amountof parallel sentences extracted from such a var-ied comparable corpus.
Apparently the averageWikipedia article contains at least a handful ofparallel sentences, suggesting this is a very fertileground for training MT systems.The extracted Wikipedia data is likely to makethe greatest impact on broad domain test sets ?
in-deed, initial experimentation showed little BLEUgain on in-domain test sets such as Europarl, whereout-of-domain training data is unlikely to provideappropriate phrasal translations.
Therefore, we ex-perimented with two broad domain test sets.First, Bing Translator provided a sample of trans-lation requests along with translations in German-English and Spanish-English, which acted our stan-dard development and test set.
Unfortunately nosuch tagged set was available in Bulgarian-English,so we held out a portion of the large system?s train-ing data to use for development and test.
In eachlanguage pair, the test set was split into a devel-opment portion (?Dev A?)
used for minimum errorrate training (Och, 2003) and a test set (?Test A?
)used for final evaluation.Second, we created new test sets in each ofthe three language pairs by sampling parallel sen-tences from held out Wikipedia articles.
Toensure that this test data was clean, we man-ually filtered the sentence pairs that were nottruly parallel and edited them as necessary toimprove adequacy.
We called this ?Wikitest?.This test set is available at http://research.microsoft.com/en-us/people/chrisq/wikidownload.aspx.
Characteristics of thesetest sets are summarized in Table 5.We evaluated the resulting systems using BLEU-4 (Papineni et al, 2002); the results are pre-sented in Table 6.
First we note that the extractedWikipedia data are very helpful in medium dataconditions, significantly improving translation per-formance in all conditions.
Furthermore we foundthat the extracted Wikipedia sentences substantiallyimproved translation quality on held-out Wikipediaarticles.
In every case, training on medium dataplus Wikipedia extracts led to equal or better trans-lation quality than the large system alone.
Further-more, adding the Wikipedia data to the large datacondition still made substantial improvements.5 ConclusionsOur first substantial contribution is to demonstratethat Wikipedia is a useful resource for mining par-allel data.
The sheer volume of extracted parallelsentences within Wikipedia is a somewhat surpris-ing result in the light of Wikipedia?s construction.We are also releasing several valuable resources tothe community to facilitate further research: man-ually aligned document pairs, and an edited testset.
Hopefully this will encourage research intoWikipedia as a resource for machine translation.Secondly, we improve on prior pairwise mod-els by introducing a ranking approach for sentencepair extraction.
This ranking approach sidesteps theproblematic class imbalance issue, resulting in im-proved average precision while retaining simplicityand clarity in the models.Also by modeling the sentence alignment of thearticles globally, we were able to show a substan-tial improvement in task accuracy.
Furthermore a409German English Spanish English Bulgarian Englishsentences 924,416 924,416 957,884 957,884 413,514 413,514Medium types 351,411 320,597 272,139 247,465 115,756 69,002tokens 11,556,988 11,751,138 18,229,085 17,184,070 10,207,565 10,422,415sentences 6,693,568 6,693,568 7,727,256 7,727,256 1,459,900 1,459,900Large types 1,050,832 875,041 1,024,793 952,161 239,076 137,227tokens 100,456,622 96,035,475 155,626,085 137,559,844 29,741,936 29,889,020sentences 1,694,595 1,694,595 1,914,978 1,914,978 146,465 146,465Wiki types 578,371 525,617 569,518 498,765 107,690 74,389tokens 21,991,377 23,290,765 29,859,332 28,270,223 1,455,458 1,516,231Table 4: Statistics of the training data size in all three language pairs.German English Spanish English Bulgarian EnglishDev A sentences 2,000 2,000 2,000 2,000 2,000 2,000tokens 16,367 16,903 24,571 21,493 39,796 40,503Test A sentences 5,000 5,000 5,000 5,000 2,473 2,473tokens 42,766 43,929 68,036 60,380 52,370 52,343Wikitest sentences 500 500 500 500 516 516tokens 8,235 9,176 10,446 9,701 7,300 7,701Table 5: Statistics of the test data sets.Language pair Training data Dev A Test A WikitestSpanish-English Medium 32.6 30.5 33.0Medium+Wiki 36.7 (+4.1) 33.8 (+3.3) 39.1 (+6.1)Large 39.2 37.4 38.9Large+Wiki 39.5 (+0.3) 37.3 (-0.1) 41.1 (+2.2)German-English Medium 28.7 26.6 13.0Medium+Wiki 31.5 (+2.8) 29.6 (+3.0) 18.2 (+5.2)Large 35.0 33.7 17.1Large+Wiki 34.8 (-0.2) 33.9 (+0.2) 20.2 (+3.1)Bulgarian-English Medium 36.9 26.0 27.8Medium+Wiki 37.9 (+1.0) 27.6 (+1.6) 37.9 (+10.1)Large 51.7 49.6 36.0Large+Wiki 51.7(+0.0) 49.4 (-0.2) 39.5(+3.5)Table 6: BLEU scores under various training and test conditions.
The first column is from minimum error rate training;the next two columns are on held-out test sets.
For training data conditions including extracted Wikipedia sentences,parenthesized values indicate absolute BLEU difference against the corresponding system without Wikipedia extracts.small sample of annotated articles is sufficient totrain these global level features, and the learnedclassifiers appear very portable across languages.
Itis difficult to say whether such improvement willcarry over to other comparable corpora with lessdocument structure and meta-data.
We plan to ad-dress this question in future work.Finally, initial investigations have shown thatsubstantial gains can be achieved by using an in-duced word-level lexicon in combination with sen-tence extraction.
This helps address modeling wordpairs that are out-of-vocabulary with respect to theseed parallel lexicon, while avoiding some of theissues in bootstrapping.410ReferencesS.
F Adafre and M. de Rijke.
2006.
Finding similarsentences across multiple languages in wikipedia.
InProceedings of EACL, pages 62?69.Phil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InProceedings of ACL.P.
F Brown, V. J Della Pietra, S. A Della Pietra, andR.
L Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational linguistics, 19(2):263?311.P.
Fung and P. Cheung.
2004.
Multi-level bootstrap-ping for extracting parallel sentences from a quasi-comparable corpus.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,page 1051.W.
A Gale and K. W Church.
1991.
Identifying wordcorrespondences in parallel texts.
In Proceedingsof the workshop on Speech and Natural Language,pages 152?157.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL,pages 771?779.Roy Bar-Haim Ido, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor.
2006.
The second pascal recognising tex-tual entailment challenge.P.
Koehn and K. Knight.
2002.
Learning a translationlexicon from monolingual corpora.
In Proceedings ofthe ACL Workshop on Unsupervised Lexical Acquisi-tion.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT/NAACL, pages 127?133, Edmonton,Canada, May.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In MT summit, volume 5.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceed-ings of the 18th International Conference on MachineLearning, pages 282?289.R.
C Moore.
2002.
Fast and accurate sentence align-ment of bilingual corpora.
Lecture Notes in ComputerScience, 2499:135?144.D.
S Munteanu and D. Marcu.
2005.
Improv-ing machine translation performance by exploitingnon-parallel corpora.
Computational Linguistics,31(4):477?504.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics, pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelpha, Pennsylva-nia, USA.R.
Rapp.
1999.
Automatic identification of word trans-lations from unrelated English and German corpora.In Proceedings of ACL.P.
Resnik and N. A Smith.
2003.
The web as a parallelcorpus.
Computational Linguistics, 29(3):349?380.C.
Tillmann and J. Xu.
2009.
A simple sentence-levelextraction algorithm for comparable data.
In Pro-ceedings of HLT/NAACL, pages 93?96.C.
Tillmann.
2009.
A Beam-Search extraction algo-rithm for comparable data.
In Proceedings of ACL,pages 225?228.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InProceedings of the 16th conference on Computationallinguistics-Volume 2, pages 836?841.Wikipedia.
2004.
Wikipedia, the free encyclopedia.
[Online; accessed 20-November-2009].B.
Zhao and S. Vogel.
2002.
Adaptive parallel sentencesmining from web bilingual news collection.
In Pro-ceedings of the 2002 IEEE International Conferenceon Data Mining, page 745.
IEEE Computer Society.411
