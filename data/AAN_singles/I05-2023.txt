Improved-Edit-Distance Kernel for Chinese Relation ExtractionWanxiang CheSchool of Computer Sci.
and Tech.Harbin Institute of TechnologyHarbin China, 150001tliu@ir.hit.edu.cnJianmin Jiang, Zhong Su, Yue PanIBM CRLBeijing China, 100085{jiangjm, suzhong,panyue}@cn.ibm.comTing LiuSchool of Computer Sci.
and Tech.Harbin Institute of TechnologyHarbin China, 150001tliu@ir.hit.edu.cnAbstractIn this paper, a novel kernel-basedmethod is presented for the problemof relation extraction between namedentities from Chinese texts.
The ker-nel is defined over the original Chi-nese string representations around par-ticular entities.
As a kernel func-tion, the Improved-Edit-Distance (IED)is used to calculate the similarity be-tween two Chinese strings.
By em-ploying the Voted Perceptron and Sup-port Vector Machine (SVM) kernel ma-chines with the IED kernel as the clas-sifiers, we tested the method by extract-ing person-affiliation relation from Chi-nese texts.
By comparing with tradi-tional feature-based learning methods,we conclude that our method needs lessmanual efforts in feature transformationand achieves a better performance.1 IntroductionRelation extraction (RE) is a basic and impor-tant problem in information extraction field.
Itextracts the relations among the named enti-ties.
Examples of relations are person-affiliation,organization-location, and so on.
For example, inthe Chinese sentence ?????IBM???????
(Gerstner is the chairman of IBM Corpora-tion.
), the named entities are???
(people) andIBM??
(organization).
The relation betweenthem is person-affiliation.Usually, we can regard RE as a classificationproblem.
All particular entity pairs are foundfrom a text and then decided whether they are arelation which we need or not.At the beginning, a number of manually en-gineered systems were developed for RE prob-lem (Aone and Ramos-Santacruz, 2000).
Theautomatic learning methods (Miller et al, 1998;Soderland, 1999) are not necessary to have some-one on hand with detailed knowledge of how theRE system works, or how to write rules for it.Usually, the machine learning method repre-sents the NLP objects as feature vectors in thefeature extraction step.
The methods are namedfeature-based learning methods.
But in manycases, data cannot be easily represented explicitlyvia feature vectors.
For example, in most NLPproblems, the feature-based representations pro-duce inherently local representations of objects,for it is computationally infeasible to generatefeatures involving long-range dependencies.
Onthe other hand, finding the suitable features of aparticular problem is a heuristic work.
Their ac-quisition may waste a lot of time.Different from the feature-based learningmethods, the kernel-based methods do not needto extract the features from the original text, butretain the original representation of objects anduse the objects in algorithms only via comput-ing a kernel (similarity) function between a pairof objects.
Then the kernel-based methods useexisting learning algorithms with dual form, e.g.the Voted Perceptron (Freund and Schapire, 1998)or SVM (Cristianini and Shawe-Taylor, 2000), askernel machine to do the classification task.132Haussler (1999) and Watkins (1999) proposeda new kernel method based on discrete structuresrespectively.
Lodhi et al (2002) used string ker-nels to solve the text classification problem.
Ze-lenko et al (2003) used the kernel methodsfor extracting relations from text.
They definedthe kernel function over shallow parse represen-tation of text.
And the kernel method is used inconjunction with the SVM and the Voted Percep-tron learning algorithms for the task of extractingperson-affiliation and organization-location rela-tions from text.As mentioned above, the discrete structure ker-nel methods are more suitable to RE problemsthan the feature-based methods.
But the string-based kernel methods only consider the wordforms without their semantics.
Shallow parserbased kernel methods need shallow parser sys-tems.
Because the performance of shallow parsersystems is not high enough until now, especiallyfor Chinese text, we cannot depend on it com-pletely.To cope with these problems, we propose theImproved-Edit-Distance (IED) algorithm to cal-culate the kernel (similarity) function.
We con-sider the semantic similarity between two wordsin two strings and some structure information ofstrings.The rest of the paper is organized as follows.
InSection 2, we introduce the kernel-based machinelearning algorithms and their application in nat-ural language processing problems.
In Section 3,we formalize the relation extraction problem asa machine learning problem.
In Section 4, wegive a novel kernel method, named the IED kernelmethod.
Section 5 describes the experiments andresults on a particular relation extraction problem.In Section 6, we discuss the reason why the IEDbased kernel method yields a better result thanother methods.
Finally, in Section 7, we give theconclusions and comments on the future work.2 Kernel-based Machine LearningMost machine learning methods represent an ob-ject as a feature vector.
They are well-knownfeature-based learning methods.Kernel methods (Cristianini and Shawe-Taylor,2000) are an attractive alternative to feature-basedmethods.
The kernel methods retain the originalrepresentation of objects and use the object onlyvia computing a kernel function between a pairof objects.
As we know, a kernel function is asimilarity function satisfying certain properties.There are a number of learning algorithms thatcan operate using only the dot product of exam-ples.
We call them kernel machines.
For in-stance, the Perceptron learning algorithm (Cris-tianini and Shawe-Taylor, 2000), Support VectorMachine (SVM) (Vapnik, 1998) and so on.3 Relation Extraction ProblemWe regard the RE problem as a classificationlearning problem.
We only consider the relationbetween two entities in a sentence and no rela-tions across sentences.
For example, the sen-tence ????????IBM?????????
(President Bush met Gerstner, the chair-man of IBM Corporation.)
contains three enti-ties,??
(people), ???
(people) and IBM??
(organization).
The three entities form twocandidate person-affiliation relation pairs: ??-IBM??
and ???-IBM??
.
The con-texts of the entities pairs produce the examplesfor the binary classification problem.
Then, fromthe context examples, a classifier can decide ???-IBM??
is a real person-affiliation relationbut ??-IBM??
is not.3.1 Feature-based MethodsThe feature-based methods have to transform thecontext into features.
Expert knowledge is re-quired for deciding which elements or their com-binations thereof are good features.
Usually thesefeatures?
values are binary (0 or 1).The feature-based methods will cost lots of la-bor to find suitable features for a particular appli-cation field.
Another problem is that we can eitherselect only the local features with a small win-dow or we will have to spend much more trainingand test time.
At the same time, the feature-basedmethods will not use the combination of these fea-tures.3.2 Kernel-based MethodsDifferent from the feature-based methods, kernel-based methods do not require much labor on ex-tracting the suitable features.
As explained in theintroduction to Section 2, we retain the original133string form of objects and consider the similarityfunction between two objects.
For the problem ofthe person-affiliation relation extraction, the ob-jects are the context around people and organiza-tion with a fixed window size w. It means thatwe get w words around each entity as the samplesin the classification problem.
Again consideringthe example ????????IBM????????
?, with w = 2, the object for the pair ???
(people) and IBM??
(organization) canbe written as ???
?
ORG ??
PEO ?
?Through the objects transformed from the origi-nal texts, we can calculate the similarity betweenany two objects by using the kernel (similarity)function.For the Chinese relation extraction problem,we must consider the semantic similarity betweenwords and the structure of strings while comput-ing similarity.
Therefore we must consider thekernel function which has a good similarity mea-sure.
The methods for computing the similaritybetween two strings are: the same-word basedmethod (Nirenburg et al, 1993), the thesaurusbased method (Qin et al, 2003), the Edit-Distancemethod (Ristad and Yianilos, 1998) and the statis-tical method (Chatterjee, 2001).
We know that thesame-word based method cannot solve the prob-lem of synonyms.
The thesaurus based methodcan overcome this difficulty but does not con-sider the structure of the text.
Although the Edit-Distance method uses the structure of the text, italso has the same problem of the replacement ofsynonyms.
As for the statistical method, it needslarge corpora of similarity text and thus is difficultto use for realistic applications.For the reasons described above, we propose anovel Improved-Edit-Distance (IED) method forcalculating the similarity between two Chinesestrings.4 IED Kernel MethodLike normal kernel methods, the new IED ker-nel method includes two components: the ker-nel function and the kernel machine.
We use theIED method to calculate the semantic similaritybetween two Chinese strings as the kernel func-tion.
As for the kernel machine, we tested theVoted Perceptron with dual form and SVM with acustomized kernel.
In the following subsections,(a) Edit-Distance (b) Improved-Edit-DistanceFigure 1: The comparison between the Edit-Distance and the Improved-Edit-Distancewe will introduce the kernel function, the IEDmethod, and kernel machines.4.1 Improved-Edit-DistanceBefore the introduction to IED, we will givea brief review of the classical Edit-Distancemethod (Ristad and Yianilos, 1998).The edit distance between two strings is de-fined as: The minimum number of edit operationsnecessary to transform one string into another.There are three edit operations, Insert, Delete, andReplace.
For example, in Figure 1(a), the edit dis-tance between ?????
(like apples)?
and ??????
(like bananas)?
is 4, as indicated by thefour dotted lines.As we see, the method of computing the editdistance between two Chinese strings cannot re-flect the actual situation.
First, the Edit-Distancemethod computes the similarity measured in Chi-nese character.
But in Chinese, most of the char-acters have no concrete meanings, such as ???,???
and so on.
The single character cannot ex-press the meanings of words.
Second, the costof the Replace operation is different for differentwords.
For example, the operation of ??
(love)?being replace by ???(like)?
should have a smallcost, because they are synonyms.
At last, if thereare a few words being inserted into a string, themeaning of it should not be changed too much.Such as ?????
(like apples)?
and ??????
(like sweet apples)?
are very similar.Based on the above idea, we provide the IEDmethod for computing the similarity between twoChinese strings.
It means that we will use Chinesewords as the basis of our measurement (instead ofcharacters).
By using a thesaurus, the similaritybetween two Chinese words can be computed.
Atthe same time, the cost of the Insert operation isreduced.Here, we use the CiLin (Mei et al, 1996) as134the thesaurus resource to compute the similaritybetween two Chinese words.
In CiLin, the se-mantics of words are divided into High, Middle,and Low classes to describe a semantic systemfrom general to special semantic.
For example:???(apple)?
is Bh07, ???(banana)?
is Bh07,????(tomato)?
is Bh06, and so on.The semantic distance between word A andword B can be defined as:Dist(A, B) = mina?A,b?Bdist(a, b)where A and B are the semantic sets of wordA and word B respectively.
The distance be-tween semantic a and b is: dist(a, b) = 2 ?
(3 ?
d), where d means that the semantic codeis different from the dth class.
If the seman-tic code is same, then the semantic distance is0.
Therefore, Dist(?????)
= 0 andDist(??????)
= 2.Table 1 defines the variations of the edit dis-tance on string ?AB?
after doing various edit op-erations.
Where, ???
denotes one to four words,?A?
and ?B?
are two words which user inputs.
X?denotes the synonyms of X.Table 1: The Variations of Edit-Distance with ABRank Pattern1 AB2 A?B3 AB?
; A?B4 A?B?
; A?
?B5 A?
; B?According to Table 1, we can define the cost ofvarious edit operations in IED.
See Table 2, where???
denotes the delete operation.Table 2: The Cost of Edit Operation in IEDEdit Operation CostA?A 0Insert 0.1A?A?
Dist(A, A?
)/10 + 0.5Others 1By the redefinition of the cost of edit opera-tions, the computation of IED between ??????
and ???????
is as shown Figure 1(b),where the Replace cost of ????????
is 0.5and ?????????
is 0.7.
Thus the cost of IEDis 1.2.
Compared with the cost of classical Edit-Distance, the cost of IED is much more appropri-ate in the actual situation.We use dynamic programming to compute theIED similar with the computing of edit distance.In order to compute the similarity between twostrings, we should convert the distance value intoa similarity.
Empirically, the maximal similarityis set to be 10.
The similarity is 10 minus theimproved edit distance of two Chinese strings.4.2 Kernel MachinesWe use the Voted Perceptron and SVM algorithmsas the kernel machines here.The Voted Perceptron algorithm was describedin (Freund and Schapire, 1998).
We usedSVMlight (Joachims, 1998) with custom kernel asthe implementation of the SVM method.
In ourexperiments, we just replaced the custom kernelwith the IED kernel function.5 Experiments and ResultsIn this section, we show how to extract theperson-affiliation relation from text and givesome experimental results.
It is relativelystraightforward to extend the IED kernel methodto other RE problems.The corpus for our experiments comes fromBejing Youth Daily1.
We annotated about 500news with named entities of PEO and ORG.
Weselected 4,200 sentences (examples) with bothPEO and ORG pairs as described in Section 3.There are about 1,200 positive examples and3,000 negative examples.
We took about 2,500random examples as training data and the rest ofabout 1,700 examples as test data.5.1 Infection of Window Size in KernelMethodsThe change of the performance of the IED kernelmethod varying while the window size w is shownin Table 3.
Here the Voted Perceptron is used asthe kernel machine.Our experimental results show that the IEDkernel method got the best performance with thehighest F -Score when the window size w =1http://www.bjyouth.com/1352.
As w grows, the Precision becomes higher.With smaller w?s, the Recall becomes higher.5.2 Comparison between Feature andKernel MethodsFor the feature-based methods implementation,we use the words which are around the PEO andthe ORG entities and their POS.
The window sizeis w (See Section 3).
All examples can be trans-formed into feature vectors.
We used the regular-ized winnow learning algorithm (Zhang, 2001) totrain on the training data and predict the test data.From the experimental results, we find that whenw = 2, the performance of feature-based methodis highest.The comparison of the performance betweenthe feature-based and the kernel-base methods isshown in Table 4.Figure 2 displays the change of F -Score fordifferent methods varying with the training datasize.Figure 2: The learning curve (of F -Score) for theperson-affiliation relation, comparing IED kernelwith feature-based algorithmsTable 3: The Performance Effected by ww Precision Recall F -Score1 66.67% 92.68% 77.55%2 93.55% 87.80% 90.85%3 94.23% 74.36% 83.12%Table 4: The Performance ComparisonPrecision Recall F -ScoreRegularized Winnow 75.90% 96.92% 85.14%Voted Perceptron 93.55% 87.80% 90.85%SVM 94.15% 88.38% 91.17%From Table 4 and Figure 2, we can seethat the IED kernel methods perform better forthe person-affiliation relation extraction problemthan for the feature-based methods.Figure 2 shows that the Voted Perceptronmethod gets close to, but not as good as, the per-formance to the SVM method on the RE problem.But when using the method, we can save signifi-cantly on computation time and programming ef-fort.6 DiscussionOur experimental results show that the kernel-based and the feature-based methods can get thebest performance with the highest F -Score whenthe window size w = 2.
This shows that for re-lation extraction problem, the two words aroundentities are the most significant ones.
On the otherhand, with w becoming bigger, the Precision be-comes higher.
And with w becoming smaller, theRecall becomes higher.From Table 4 and Figure 2, we can see thatthe IED kernel methods perform very well forthe person-affiliation relation extraction.
Further-more, it does not need an expensive feature selec-tion stage like feature-based methods.
Becausethe IED kernel method uses the semantic similar-ity between words, it can get a better extension.We can conclude that the IED kernel method re-quires much fewer examples than feature-basedmethods for achieving the same performance.For example, there is a test sentence ????
??
??
?
IBM??
??
?
(ChairmanHu Jintao met the CEO of IBM Corporation).
Thefeature-based method judges the ???-IBM??
as a person-affiliation relation, because thecontext around???
and IBM??
is similarwith the context of the person-affiliation relation.However, the IED kernel method does the correctjudgment based on the structure information.
Forthis case the IED kernel method gets a higher pre-cision.
At the same time, because the IED kernelmethod considers the extension of synonyms, itsrecall does not decrease very much.The speed is a practical problem in apply-ing kernel-based methods.
Kernel-based clas-sifiers are relatively slow compared to feature-based classifiers.
The main reason is that the com-puting of kernel (similarity) function takes much136time.
Therefore, it becomes a key problem to im-prove the efficiency of the computing of the ker-nel function.7 ConclusionsWe presented a new approach for using kernel-based machine learning methods for extracting re-lations between named entities from Chinese textsources.
We define kernels over the original rep-resentations of Chinese strings around the partic-ular entities and use the IED method for comput-ing the kernel function.
The kernel-based meth-ods need not transform the original expression ofobjects into feature vectors, so the methods needless manual efforts than the feature-based meth-ods.
We applied the Voted Perceptron and theSVM learning method with custom kernels to ex-tract the person-affiliation relations.
The methodcan be extended to extract other relations betweenentities, such as organization-location, etc.
Wealso compared the performance of kernel-basedmethods with that of feature-based methods, andthe experimental results show that kernel-basedmethods are better than feature-based methods.AcknowledgementsThis research has been supported by NationalNatural Science Foundation of China via grant60435020 and IBM-HIT 2005 joint project.ReferencesChinatsu Aone and Mila Ramos-Santacruz.
2000.Rees: A large-scale relation and event extractionsystem.
In Proceedings of the 6th Applied NaturalLanguage Processing Conference, pages 76?83.Niladri Chatterjee.
2001.
A statistical approachfor similarity measurement between sentences forEBMT.
In Proceedings of Symposium on Transla-tion Support Systems STRANS-2001, Indian Insti-tute of Technology, Kanpur.N.
Cristianini and J. Shawe-Taylor.
2000.
An Intro-duction to Support Vector Machines.
CambridgeUniversity Press, Cambirdge University.Yoav Freund and Robert E. Schapire.
1998.
Largemargin classification using the perceptron algo-rithm.
In Computational Learning Theory, pages209?217.David Haussler.
1999.
Convolution kernels on dis-crete structures.
Technical Report UCSC-CRL-99-10, 7,.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: learning with many rele-vant features.
In Proceedings of ECML-98, number1398, pages 137?142, Chemnitz, DE.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
J. Mach.
Learn.Res., 2:419?444.Jiaju Mei, Yiming Lan, Yunqi Gao, and HongxiangYin.
1996.
Chinese Thesaurus Tongyici Cilin (2ndEdtion).
Shanghai Thesaurus Press, Shanghai.Scott Miller, Michael Crystal, Heidi Fox, LanceRamshaw, Richard Schwartz, Rebecca Stone,Ralph Weischedel, and the Annotation Group.1998.
Algorithms that learn to extract information?BBN: Description of the SIFT system as used forMUC.
In Proceedings of the Seventh Message Un-derstanding Conference (MUC-7).S.
Nirenburg, C. Domashnev, and D.J.
Grannes.
1993.Two approaches to matching in example-based ma-chine translation.
In Proceedings of the Fifth In-ternational Conference on Theoretical and Method-ological Issues in Machine Translation, pages 47?57, Kyoto, Japan.Bing Qin, Ting Liu, Yang Wang, Shifu Zheng, andSheng Li.
2003.
Chinese question answering sys-tem based on frequently asked questions.
Jour-nal of Harbin Institute of Technology, 10(35):1179?1182.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string-edit distance.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 20(5):522?532.Stephen Soderland.
1999.
Learning information ex-traction rules for semi-structured and free text.
Ma-chine Learning, 34(1-3):233?272.Vladimir N. Vapnik.
1998.
Statistical Learning The-ory.
Wiley.Chris Watkins.
1999.
Dynamic alignment kernels.Technical Report CSD-TR-98-11, 1,.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relation ex-traction.
J. Mach.
Learn.
Res., 3:1083?1106.Tong Zhang.
2001.
Regularized winnow methods.In Advances in Neural Information Processing Sys-tems 13, pages 703?709.137
