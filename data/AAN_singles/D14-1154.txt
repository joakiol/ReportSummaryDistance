Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1465?1468,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsVerifiably Effective Arabic Dialect IdentificationKareem Darwish, Hassan Sajjad, Hamdy MubarakQatar Computing Research InstituteQatar Foundation{kdarwish,hsajjad,hmubarak}@qf.org.qaAbstractSeveral recent papers on Arabic dialect identi-fication have hinted that using a word unigrammodel is sufficient and effective for the task.However, most previous work was done on astandard fairly homogeneous dataset of dialec-tal user comments.
In this paper, we showthat training on the standard dataset does notgeneralize, because a unigram model may betuned to topics in the comments and does notcapture the distinguishing features of dialects.We show that effective dialect identificationrequires that we account for the distinguishinglexical, morphological, and phonological phe-nomena of dialects.
We show that accountingfor such can improve dialect detection accu-racy by nearly 10% absolute.1 IntroductionModern Standard Arabic (MSA) is the lingua francaof the so-called Arab world, which includes north-ern Africa, the Arabian Peninsula, and Mesopotamia.However, Arabic speakers generally use dramaticallydifferent languages (or dialects) in daily interactionsand in social media.
These dialects may differ in vocab-ulary, morphology, and spelling from MSA and mostdo not have standard spellings.
There is often largelexical overlap between dialects and MSA.
Performingproper Arabic dialect identification may positively im-pact many Natural Language Processing (NLP) appli-cation.
For example, transcribing dialectal speech orautomatically translating into a particular dialect wouldbe aided by the use of targeted language models that aretrained on texts in that dialect.This has led to recent interest in automatic identifi-cation of different Arabic dialects (Elfardy et al., 2013;Cotterell et al., 2014; Zaidan et al., 2014).
Though pre-vious work (Cotterell et al., 2014) have reported highaccuracies for dialect identification using word uni-gram model, which implies that this is a solved prob-lem, we argue that the problem is far from being solved.The reason for this assertion stems from the fact that theavailable dialectal data is drawn from singular sources,namely online news sites, for each dialect.
This is prob-lematic because comments on singular news site arelikely to have some homogeneity in topics and jargon.Such homogeneity has caused fairly simple classifica-tion techniques that use word unigrams and character n-grams to yield very high identification accuracies.
Per-haps, this can be attributed to topical similarity and notjust differences between dialects.
To showcase this, wetrained a classifier using the best reported methods, andwe tested the classifier on a new test set of 700 tweets,with dialectal Egyptian (ARZ) and MSA tweets, whichled to a low accuracy of 83.3%.
We also sorted wordsin the ARZ part from our training dataset by how muchthey discriminate between ARZ and MSA (using mu-tual information) and indeed many of the top wordswere in fact MSA words.There seems to be a necessity to identify lexical andlinguistic features that discriminate between MSA anddifferent dialects.
In this paper, we highlight somesuch features that help in separating between MSAand ARZ.
We identify common ARZ words that donot overlap with MSA and identify specific linguisticphenomena that exist in ARZ, and not MSA, such asmorphological patterns, word concatenations, and verbnegation constructs (Section 3).
We also devise meth-ods for capturing the linguistic phenomena, and we usethe appearance of such phenomena as features (Sec-tion 4).
Further, we show the positive impact of usingthe new features in identifying ARZ (Section 5).2 Previous WorkPrevious work on Arabic dialect identification uses n-gram based features at both word-level and character-level to identify dialectal sentences (Elfardy et al.,2013; Cotterell et al., 2014; Zaidan et al., 2011; Zaidanet al., 2014).
Zaidan et al.
(2011) created a dataset ofdialectal Arabic.
They performed cross-validation ex-periments for dialect identification using word n-grambased features.
Elfardy et al.
(2013) built a system todistinguish between ARZ and MSA.
They used wordn-gram features combined with core (token-based andperplexity-based features) and meta features for train-ing.
Their system showed a 5% improvement overthe system of Zaidan et al.
(2011).
Later, Zaidan etal.
(2014) used several word n-gram based and char-acter n-gram based features for dialect identification.The system trained on word unigram-based feature per-formed the best with character five-gram-based featurebeing second best.
A similar result is shown by Cot-terell et al.
(2014) where word unigram model performs1465the best.All of the previous work except Cotterell et al.
(2014)1evaluate their systems using cross-validation.These models heavily rely on the coverage of trainingdata to achieve better identification.
This limits the ro-bustness of identification to genres inline with the train-ing data.Language identification is a related area to dialectidentification.
It has raised some of the issues which wediscussed in this paper in the context of dialect identi-fication.
Lui et al.
(2011) showed that in-domain lan-guage identification performs better than cross domainlanguage identification.
Tiedemann et al.
(2012) arguedthat the linguistic understanding of the differences be-tween languages can lead to a better language identi-fication system.
kilgarriff (2001) discussed the differ-ences between datasets as a poor representation of dif-ferences between dialects of English.In this paper, we exploit the linguistic phenomenathat are specific to Arabic dialects to show that theyproduce significant improvements in accuracy.
Weshow that this also helps in achieving high qualitycross-domain dialect identification system.3 Dialectal Egyptian PhenomenaThere are several phenomena in ARZ that set it apartfrom MSA.
Some of them are as follows:Dialectal words: ARZ uses unique words that donot overlap with MSA and may not overlap with otherdialects.
Some of the common ARZ words are: ?zy?
(like), ?kdh?
(like this), and ?Azyk?
(how are you)2.These dialectal terms stem from the following:?
Using proper Arabic words that are rarely used inMSA such as ?$nTp?
(bag) and ?n$wf?
(we see).?
Fusing multiple words together by concatenating anddropping letters such as the word ?mEl$?
(no worry),which is a fusion of ?mA Elyh $y?
?.?
Using non-standard spelling of words such as?SAbE?
(finger) instead of ?<sbE?
in MSA.
Conse-quently, broken plurals may also be non-standard.?
using non-Arabic words such as ?<y$Arb?
(scarf),which is transliterated from the French ?echarpe.?
altering the forms of some pronouns such as the fem-inine second person pronoun from ?k?
to ?ky?, the sec-ond person plural pronoun ?tm?
to ?tw?, and the objectpronoun ?km?
to ?kw?.Morphological differences: ARZ makes use of par-ticular morphological patterns that do not exist in MSAand often alters some morphological constructs.
Someexamples include:?
Adding the letter ?b?
in front of verb in present tense.Ex.
MSA: ?ylEb?
(he plays)?
EG: ?bylEb?.?
Using the letters ?H?
or ?h?, instead of ?s?, to indi-cate future tense.
Ex.
MSA: ?sylEb?
(he will play)?EG: ?hylEb?
or ?HylEb?.1Zaidan et al.
(2014) applied their classifier to a differentgenre but did not evaluate it?s performance.2Buckwalter encoding is used throughout the paper.?
Adding the letters ?At?
to passive past tense verbs.Ex.
MSA: ?luEiba?
(was played)?
?AtlaEab?.?
Adding the letters ?m?
or ?mA?
before the verb and?$?
or ?$y?
after the verb to express negation.
Ex.MSA: ?lm ylEb?
(he did not play)?
?mlEb$?.?
the merging of verbs and prepositional phrases of theform (to-pronoun) that follow it.
Ex.
MSA: ?ylEb lh?
(he plays for/to him)?
?bylEblh?.?
Replacing a short vowel with a long vowel in im-perative verbs that are derived from hollow roots.
Ex.MSA: ?qul?
(say)?
?qwl?.Letter substitution: in ARZ the following lettersubstitutions are common:?
?v??
?t?.
Ex.
MSA: ?kvyr?
(a lot)?
EG: ?ktyr?.?
?}??
?y?.
Ex.
MSA: ?b}r?
(well)?
?byr?.?
Trailing ?y??
?Y?.
Ex.
MSA: ?Hqy?
(my right)??HqY?.?
?*??
?d?.
Ex.
MSA: ?xu*?
(take)?
?xud?.?
middle or trailing ?>?
?
?A?.
Ex.
MSA: ?f>r?(mouse)?
?fAr?.?
?D??
?Z?.
Ex.
MSA: ?DAbT?
(officer)?
?ZAbT?.?
?Z??
?D?.
Ex.
MSA: ?Zhr?
(back)?
?Dhr?.?
Middle ?|?
?
?yA?.
Ex.
MSA: ?ml|n?
(full) ??mlyAn?.?
Removal of trailing ?
?
?.
Ex.
MSA: ?AlsmA?
?
(thesky)?
?AlsmA?.Syntactic differences: some of the following phe-nomena are generally observed:?
Common use of masculine plural or singular nounforms instead dual and feminine plural.
Ex.
MSA ?jny-hyn?
(two pounds)?
EG: ?Atnyn jnyh?.?
Dropping some articles and preposition in some syn-tactic constructs.
For example, the preposition ?<lY?
(to) in ?>nA rAyH <lY Al$gl?
(I am going to work)is typically dropped.
Also, the particle ?>n?
(to) isdropped in the sentence ?>nA mHtAj >n >nAm?
(Ineed to sleep).?
Using only one form of noun and verb suffixes suchas ?yn?
instead of ?wn?
and ?wA?
instead of ?wn?
re-spectively.
Also, so-called ?five nouns?, are used inonly one form (ex.
?>bw?
(father of) instead of ?>bA?or ?>by?
).4 Detecting Dialectal PeculiaritiesARZ is different from MSA lexically, morphologically,phonetically, and syntactically.
Here, we present meth-ods to handle such peculiarities.
We chose not to han-dle syntactic differences, because they may be capturedusing word n-gram models.To capture lexical variations, we extracted and sortedby frequency all the unigrams from the Egyptian side ofthe LDC2012T09 corpus (Zbib et al., 2012), which has?
38k Egyptian-English parallel sentences.
A linguistwas tasked with manually reviewing the words from thetop until 1,300 dialectal words were found.
Some of thewords on the list included dialectal words, commonlyused foreign words, words that exhibit morphologicalvariations, and others with letter substitution.1466For morphological phenomenon, we employed threemethods, namely:?
Unsupervised Morphology Induction: We em-ployed the unsupervised morpheme segmentation tool,Morfessor (Virpioja et al., 2013).
It is a data driventool that automatically learns morphemes from data inan unsupervised fashion.
We used the trained model tosegment the training and test sets.?
Morphological Rules: In contrast to Morfessor, wedeveloped only 15 morphological rules (based on theanalysis proposed in Section 3) to segment ARZ text.These rules would separate prefixes and suffixes like alight stemmer.
Example rules would segment a leading?b?
and segment a combination of a leading ?m?
andtrailing ?$?.
?Morphological Generator: For morphological gen-eration, we enumerated a list of ?
200 morphologicalpatterns that derive dialectal verbs from Arabic roots.One such pattern is ytCCC that would generate the di-alectal verb-form ytktb (to be written) from the root?ktb?.
We used the root list that is distributed with Se-bawai (Darwish, 2002).
We also expanded the list byattaching negation affixes and pronouns.
We retainedgenerated word forms that: a) exist in a large corpus of63 million Arabic tweets from 2012 with more than 1billion tokens; and b) don?t appear in a large MSA cor-pus of 10 years worth of Aljazeera articles containing114 million tokens3.
The resulting list included 94kverb surface forms such as ?mbyEmlhA$?
(he does notdo it).For phonological differences, we used a morpholog-ical generator that makes use of the aforementionedroot list and an inventory of ?
605 morphological pat-terns (with diacritization) to generate possible Arabicstems.
The generated stems with their diacritics werechecked against a large diacritized Arabic corpus con-taining more than 200 million diacritized words4.
Ifgenerated words contained the letters ?v?, ?
}?, ?
*?, and?D?, we used the aforementioned letter substitutions.We retained words that exist in the large tweet corpusbut not in the Aljazeera corpus.
The list contained 8ksurface forms.5 Evaluation SetupDataset: We performed dialect identification exper-iment for ARZ and MSA.
For ARZ, we used theEgyptian side of the LDC2012T09 corpus (Zbib etal., 2012)5.
For MSA, we used the Arabic sideof the English/Arabic parallel corpus from the Inter-national Workshop on Arabic Language Translation6which consists of ?
150k sentences.
For testing, weconstructed an evaluation set that is markedly different3http://aljazeera.net4http://www.sh.rewayat2.com5We did not use the Arabic Online Commentary data(Zaidan et al., 2011) as annotations were often not reliable.6https://wit3.fbk.eu/mt.php?release=2013-01from the training set.
We crawled Arabic tweets fromTwitter during March 2014 and selected those whereuser location was set to Egypt or a geographic locationwithin Egypt, leading to 880k tweets.
We randomlyselected 2k tweets, and we manually annotated themas ARZ, MSA, or neither until we obtained 350 ARZand 350 MSA tweets.
We used these tweets for testing.We plan to release the tweet ID?s and our annotations.We preprocessed the training and test sets using themethod described by Darwish et al.
(2012), which in-cludes performing letter and word normalizations, andsegmented all data using an open-source MSA wordsegmentor (Darwish et al., 2012).
We also removedpunctuations, hashtags, and name mentions from thetest set.
We used a Random Forest (RF) ensemble clas-sifier that generates many decision trees, each of whichis trained on a subset of the features.7We used the RFimplementation in Weka (Breiman, 2001).5.1 Classification RunsBaseline BL: In our baseline experiments, we usedword unigram, bigram, and trigram models and charac-ter unigram to 5-gram models as features.
We first per-formed a cross-validation experiment using ARZ andMSA training sets.
The classifier achieved fairly highresults (+95%) which are much higher than the resultsmentioned in the literature.
This could be due in partto the fact that we were doing ARZ-MSA classificationinstead of multi-dialect classification and MSA data isfairly different in genre from ARZ data.
We did not fur-ther discuss these results.
This experiment was a sanitycheck to see how does in-domain dialect identificationperform.Later, we trained the RF classifier on the completetraining set using word n-gram features (WRD), char-acter n-gram features (CHAR), or both (BOTH) andtested it on the tweets test set.
We referred to this sys-tem as BL later on.Dialectal Egyptian Lexicon Slex: As mentioned ear-lier, we constructed three word lists containing 1,300manually reviewed ARZ words (MAN), 94k dialectalverbs (VERB), and 8k words with letter substitutions(SUBT).
Using the lists, we counted the number ofwords in a tweet that exist in the word lists and used itas a standalone feature for classifications.
LEX refersto concatenation of all three lists.Morphological Features: For Smrph, we trained Mor-fessor separately on the MSA and Egyptian trainingdata and applied to the same training data for segmen-tation.
For Srule, we segmented Egyptian part of thetraining data using the morphological rules mentionedin Section 4.
For both, word and character n-gram fea-tures were calculated from the segmented data and the7We tried also the multi-class Bayesian classifier andSVM classifier.
SVM classifier had comparable results withRandom Forest classifier.
However, it was very slow.
So, wedecided to go with Random Forest classifier for the rest of theexperiments.1467SYS WRD CHR BOTH BEST+LEXBL 53.0 74.0 83.3 84.7Smrph72.0 88.0 62.1 89.3Srule53.9 85.9 85.9 90.1Table 1: Dialect identification accuracy usingvarious classification settings: only word-based(WRD), character-based (CHAR), and both features.BEST+LEX is built on the best feature of that systemplus a feature built on the concatenation of all listsSYS MAN +VERB +SUBTSlex93.6 94.6 94.4Table 2: Accuracy of the dialect identification systemwith the addition of various types of lexiconclassifier was trained on them and tested on the tweettest set.5.2 ResultsTable 1 summarizes the results.
Unlike results in the lit-erature, character-based n-gram features outperformedword-based n-gram features, as they seemed to bettergeneralize to the new test set, where lexical overlap be-tween the training and test sets was low.
Except forSmrph, adding both character and word n-gram fea-tures led to improved results.
We observed that Mor-fessor over-segmented the text, which in turns createdsmall character segments and enabled the character-based language model to learn the phenomenon inheritin a word.
The baseline system achieved an accuracyof 84.7% when combined with the Slexfeature.
Com-bining Smrphand Srulefeatures with the Slexfeatureled to further improvement.
However, as shown in Ta-ble 2, using the Slexfeature alone with the MAN andVERB lists led to the best results (94.6%), outperform-ing using all other features either alone or in combina-tion.
This suggests that having a clean list of dialectalwords that cover common dialectal phenomena is moreeffective than using word and character n-grams.
It alsohighlights the shortcomings of using a homogeneoustraining set where word unigrams could be capturingtopical cues along with dialectal ones.6 ConclusionIn this paper, we identified lexical, morphological,phonological, and syntactic features that help distin-guish between dialectal Egyptian and MSA.
Given thesubstantial lexical overlap between dialectal Egyptianand MSA, targeting words that exhibit distinguishingtraits is essential to proper dialect identification.
Weused some of these features for dialect detection lead-ing to nearly 10% (absolute) improvement in classifi-cation accuracy.
We plan to extend our work to otherdialects.ReferencesLeo Breiman.
2001.
Random Forests.
Machine Learn-ing.
45(1):5-32.Ryan Cotterell, Chris Callison-Burch.
2014.
A Multi-Dialect, Multi-Genre Corpus of Informal WrittenArabic.
LREC-2014, pages 241?245.Kareem Darwish.
2002.
Building a shallow morpho-logical analyzer in one day.
In Proceedings of theACL-2002 Workshop on Computational Approachesto Semitic Languages.Kareem Darwish, Walid Magdy, Ahmed Mourad.2012.
Language Processing for Arabic MicroblogRetrieval.
CIKM-2012, pages 2427?2430.Kareem Darwish, Ahmed Abdelali, Hamdy Mubarak.2014.
Using Stem-Templates to improve Arabic POSand Gender/Number Tagging.
LREC-2014.Heba Elfardy, Mona Diab.
2013.
Sentence Level Di-alect Identification in Arabic.
ACL-2013, pages456?461.Sami Virpioja, Peter Smit, Stig-Arne Grnroos, andMikko Kurimo.
2013.
Morfessor 2.0: Python Im-plementation and Extensions for Morfessor Base-line.
Aalto University publication series SCI-ENCE + TECHNOLOGY, 25/2013.
Aalto Univer-sity, Helsinki, 2013.Omar F. Zaidan, Chris Callison-Burch.
2011.
The Ara-bic Online Commentary Dataset: An AnnotatedDataset of Informal Arabic with High Dialectal Con-tent.
ACL-11, pages 37?41.Omar F. Zaidan, Chris Callison-Burch.
2014.
ArabicDialect Identification.
CL-11, 52(1).Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz,John Makhoul, Omar F. Zaidan, Chris Callison-Burch.
2012.
Machine translation of Arabic dialects.NAACL-2012, pages 49?59.Marco Lui and Timothy Baldwin.
2011.
Cross-domain feature selection for language identification.IJCNLP-2011, page 553?561.J?org Tiedemann and Nikola Ljubesic.
2012.
Efficientdiscrimination between closely related languages.COLING-2012, 2619?2634.Adam Kilgarriff.
2001.
Comparing corpora.
CL-01,6(1).1468
