Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 87?93,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Hybrid Approach to Chinese Base Noun Phrase ChunkingFang Xu Chengqing Zong Jun ZhaoNational Laboratory of Pattern RecognitionInstitute of AutomationChinese Academy of Sciences, Beijing 100080,China{fxu, cqzong, jzhao}@nlpr.ia.ac.cnAbstractIn this paper, we propose a hybrid ap-proach to chunking Chinese base nounphrases (base NPs), which combinesSVM (Support Vector Machine) modeland CRF (Conditional Random Field)model.
In order to compare the resultrespectively from two chunkers, we usethe discriminative post-processingmethod, whose measure criterion is theconditional probability generated fromthe CRF chunker.
With respect to thespecial structures of Chinese base NPand complete analyses of the first tworesults, we also customize some appro-priate grammar rules to avoid ambigui-ties and prune errors.
According to ouroverall experiments, the methodachieves a higher accuracy in the finalresults.1 IntroductionChunking means extracting the non-overlappingsegments from a stream of data.
These segmentsare called chunks (Dirk and Satoshi, 2003).
Thedefinition of base noun phrase (base NP) is sim-ple and non-recursive noun phrase which doesnot contain other noun phrase descendants.
BaseNP chunking could be used as a precursor formany elaborate natural language processing tasks,such as information retrieval, name entity extrac-tion and text summarization and so on.
Manyother problems similar to text processing can alsobenefit from base NP chunking, for example,finding genes in DNA and phoneme informationextraction.The initial work on base NP chunking is fo-cused on the grammar-based method.
Ramshawand Marcus (1995) introduced a transformation-based learning method which considered chunk-ing as a kind of tagging problem.
Their work in-spired many others to study the applications oflearning methods to noun phrase chunking.
(Cardie and Pierce, 1998, 1999) applied a scoringmethod to select new rules and a naive heuristicfor matching rules to evaluate the results' accu-racy.CoNLL-2000 proposed a shared task (Tjongand Buchholz, 2000), which aimed at dividing atext in syntactically correlated parts of words.The eleven systems for the CoNLL-2000 sharedtask used a wide variety of machine learningmethods.
The best system in this workshop is onthe basis of Support Vector Machines used by(Kudo and Matsumoto, 2000).Recently, some new statistical techniques,such as CRF (Lafferty et al 2001) and structurallearning methods (Ando and Zhang, 2005) havebeen applied on the base NP chunking.
(Fei andFernando, 2003) considered chunking as a se-quence labeling task and achieved good perform-ance by an improved training methods of CRF.
(Ando and Zhang, 2005) presented a novel semi-supervised learning method on chunking andproduced performances higher than the previousbest results.The research on Chinese Base NP Chunking is,however, still at its developing stage.
Research-ers apply similar methods of English Base NPchunking to Chinese.
Zhao and Huang (1998)made a strict definition of Chinese base NP andput forward a quasi-dependency model to analy-sis the structure of Chinese base NPs.
There aresome other methods to deal with Chinese phrase(no only base NP) chunking, such as HMM(Heng Li et al, 2003), Maximum Entropy (ZhouYaqian et al, 2003), Memory-Based Learning(Zhang and Zhou, 2002) etc.87However, according to our experiments over30,000 Chinese words, the best results of Chi-nese base NP chunking are about 5% less thanthat of English chunking (Although we shouldadmit the chunking outcomes vary among differ-ent sizes of corpus and rely on the details of ex-periments).
The differences between ChineseNPs and English NPs are summarized as follow-ing points: First, the flexible structure of Chinesenoun phrase often results in the ambiguities dur-ing the recognition procedure.
For example,many English base NPs begin with the determi-native, while the margin of Chinese base NPs ismore uncertain.
Second, the base NPs beginswith more than two noun-modifiers, such as ??
(high)/JJ  ?
(new)/JJ ??
(technology)/NN?, thenoun-modifiers ?
?/JJ ?
can not be completelyrecognized.
Third, the usage of Chinese word isflexible, as a Chinese word may serve with multiPOS (Part-of-Speech) tags.
For example, a nounis used as a verbal or an adjective component inthe sentence.
In this way the chunker is puzzledby those multi-used words.
Finally, there are nostandard datasets and elevation systems for Chi-nese base NP chunking as the CoNLL-2000shared task, which makes it difficult to compareand evaluate different Chinese base NP chunkingsystems.In this paper, we propose a hybrid approach toextract the Chinese base NPs with the help of theconditional probabilities derived from the CRFalgorithm and some appropriate grammar rules.According to our preliminary experiments onSVM and CRF, our approach outperforms bothof them.The remainder of the paper is organized as fol-lows.
Section 2 gives a brief introduction of thedata representations and methods.
We explainour motivations of the hybrid approach in section3.
The experimental results and conclusions areintroduced in section 4 and section 5 respectively.2 Task Description2.1 Data RepresentationRamshaw and Marcus (1995) gave mainly twokinds of base NPs representation ?
theopen/close bracketing and IOB tagging.
For ex-ample, a bracketed Chinese sentence,[ ??
(foreign businessmen) ??(investment)]??
(become) [ ??
(Chinese) ??
(foreigntrade)] [ ??
(important) ???
(growth)] ?The IOB tags are used to indicate the bounda-ries for each base NP where letter ?B?
means thecurrent word starts a base NP, ?I?
for a word in-side a base NP and ?O?
for a word outside a NPchunk.
In this case the tokens for the former sen-tence would be labeled as follows:?
?/B ?
?/I ?
?/V ?
?/B ?
?/I ??/B??
?/O  ?/OCurrently, most of the work on base NP identi-fication employs the trainable, corpus-based al-gorithm, which makes full use of the tokens andcorresponding POS tags to recognize the chunksegmentation of the test data.
The SVM and CRFare two representative effective models widelyused.2.2 Chunking with SVMsSVM is a machine learning algorithm for a linearbinary classifier in order to maximize the marginof confidence of the classification on the trainingdata set.
According to the different requirements,distinctive kernel functions are employed totransfer non-linear problems into linear problemsby mapping it to a higher dimension space.By transforming the training data into the formwith IOB tags, we can view the base NP chunk-ing problem as a multi-class classification prob-lem.
As SVMs are binary classifiers, we use thepairwise method to convert the multi-class prob-lem into a set of binary class problem, thus theI/O/B classifier is reduced into 3 kinds of binaryclassifier ?
I/O classifier, O/B classifier, B/Iclassifier.In our experiments, we choose TinySVM 1  to-gether with YamCha 2  (Kudo and Matsumoto,2001) as the one of the baseline systems for ourchunker.
In order to construct the feature sets fortraining SVMs, all information available in thesurrounding contexts, including tokens, POS tagsand IOB tags.
The tool YamCha makes it possi-ble to add new features on your own.
Therefore,in the training stage, we also add two new fea-tures according to the words.
First, we give spe-cial tags to the noun words, especially the propernoun, as we find in the experiment the propernouns sometimes bring on errors, such as base1 http://chasen.org/~taku/software/TinySVM/2 http://chasen.org/~taku/software/yamcha88NP ???
(Sichuan)/NR ??
(basin)/NN?, con-taining the proper noun ???
/NR?, could bemistaken for a single base NP ???/NN?
; Sec-ond, some punctuations such as separating marks,contribute to the wrong chunking, because manyChinese compound noun phrases are connectedby separating mark, and the ingredients in thesentence are a mixture of simple nouns and nounphrases, for example,???
(National)/NN ???
( Statistics Of-fice)/NN???
(Chinese)/NR ??
(Social Sci-ences)/NN ???
(Academy)/NN ?
(and)/CC???
(Chinese Academy of Sciences)/NN-SHORT?The part of base NP ?
??
?/B ?
?/I ???/I?
can be recognized as three independent baseNPs --??
?/B ?
?/B ???/B?.
The kind oferrors comes from the conjunction ??(and)?
andthe successive sequences of nouns, which con-tribute little to the chunker.
More informationd analyses will be provided in Section 4. an2.3 Conditional Random FieldsLafferty et al( 2001) present the ConditionalRandom Fields for building probabilistic modelsto segment and label sequence data, which wasused effectively for base NP chunking (Sha &Pereira, 2003).
Lafferty et al (2001) point outthat each of the random variable label sequencesY conditioned on the random observation se-quence X.
The joint distribution over the labelsequence Y given X has the form111( | , ) exp( ( , ))( )( , ) ( , , , )j jjni iip y x F y xZ xF y x f y y x i?
??===?
?where 1( , , ,j i i )f y y x i?
is either a transition fea-ture function ( 1, , ,i is y y x i? )
or a state featurefunction 1( , , , )i it y y x i?
; 1,iy y?
i are labels, x isan input sequence, i  is an input position, ( )Z x isa normalization factor; k?
is the parameter to beestimated from training data.Then we use the maximum likelihood training,such as the log-likelihood to train CRF giventraining data ( ){ },k kT x y= ,1( ) log ( , )( ) k kk kL F y xZ x?
?
?= + ??
??
??
( )L ?
is minimized by finding unique zero ofthe gradient( | , )( ) [ ( , ) ( , )]kk k p Y x kkL F y x E F Y x???
= ??
( | , ) ( , )kp Y x kE F Y x?
can be computed using a vari-ant of the forward-backward algorithm.
We de-fine a transition matrix as following:' '( , | ) exp( ( , , , ))i j jjM y y x f y y x i?= ?Then,1111( | , ) ( , | )( )ni i iip y x M y y xZ x?
+ ?== ?and let * denote component-wise matrix product,( | , )1( , ) ( | , ) ( , )( )( )( ) 1kp Y x k k kyTi i i iiTnE F Y x p Y y x F y xf MZ xZ x a?
??
?
?= =?
== ??
?Where i i?
?,  as the forward and backwardstate-cost vectors defined by1 1 1 1,1 0 1Ti i T i ii iM i n M i ni i n?
??
??
+ +    0 < ?
?
?
<?=  =?
?
=              =?
?Sha & Pereira (2003) provided a thorough dis-cussion of CRF training methods including pre-conditioned Conjugate Gradient, limited-Memory Quasi-Newton and voted perceptron.They also present a novel approach to modelconstruction and feature selection in shallowparsing.We use the software CRF++3 as our Chinesebase NP chunker baseline software.
The resultsof CRF are better than that of SVM, which is thesame as the outcome of the English base NPchunking in (Sha & Pereira, 2003).
However, wefind CRF products some errors on identifyinglong-range base NP, while SVM performs wellin this aspect and the errors of SVM and CRF areof different types.
In this case, we develop acombination approach to improve the results.3 Our Approach?
(Tjong et al, 2000) pointed out that the perform-ance of machine learning can be improved bycombining the output of different systems, sothey combined the results of different classifiers3 http://www.chasen.org/~taku/software/CRF++/89and obtained good performance.
Their combina-tion system generated different classifiers by us-ing different data labels and applied respectivevoting weights accordingly.
(Kudo and Matsu-moto 2001) designed a voting arrangement byapplying cross validation and VC-bound andLeave-One-Out bound for the voting weights.The voting systems improve the accuracy, thechoices of weights and the balance between dif-ferent weights is based on experiences, whichdoes not concern the inside features of the classi-fication, without the guarantee of persuasivetheoretical supports.
Therefore, we developed ahybrid approach to combine the results of theSVM and CRF and utilize their advantages.
(Simon, 2003) pointed out that the SVM guaran-tees a high generalization using very rich featuresfrom the sentences, even with a large and high-dimension training data.
CRF can build efficientand robust structure model of the labels, whenone doesn?t have prior knowledge about data.Figure 1 shows the preliminary chunking andpos-processing procedure in our experimentsFirst of all, we use YamCha and CRF++ re-spectively to treat with the testing data.
We gottwo original results from those chunkers, whichuse the exactly same data format; in this case wecan compare the performance between CRF andSVM.
After comparisons, we can figure out thesame words with different IOB tags from the twoformer chunkers.
Afterward, there exist twoproblems: how to pick out the IOB tags identi-fied improperly and how to modify those wrongIOB tags.To solve the first question, we use the condi-tional probability from the CRF to help deter-mine the wrong IOB tags.
For each word of thetesting data, the CRF chunker works out a condi-tional probability for each IOB tag and choosesthe most probable tag for the output.
We bringout the differences between the SVM and CRF,such as ???
(Sichuan)?
in a base noun phraseis recognized as ?I?
and ?O?
respectively, andthe distance between P(I| ????)
and P(O| ????)
is tiny.
According to our experiment, about80% of the differences between SVM and CRFshare the same statistical characters, which indi-cate the correct answers are inundated by thenoisy features in the classifier.CRF SVMTesting dataComparisonError pruning withrules and P (Y|X)Final resultFigure 1 the Experiments?
ProcedureUsing the comparison between SVM and CRFwe can check most of those errors.
Then wecould build some simple grammar rules to figureout the correct tags for the ambiguous words cor-responding to the surrounding contexts.
Then Atthe error pruning step, judging from the sur-rounding texts and the grammar rules, the baseNP is corrected to the right form.
We give 5mainly representative grammar rules to explainhow they work in the experiments.The first simple sample of grammar rules isjust like ?BNP ?
NR NN?, used to solve theproper noun problems.
Take the ???
(Si-chuan)/NR/B ??
(basin)/NN/I?
for example,the comparison finds out the base NP recognizedas ???
(Sichuan)/NR/I ??
(basin)/NN/B?.Second, with respect to the base NP connectingwith separating mark and conjunction words, tworules ?BNP ?
BNP CC (BNP | Noun), BNP?BNP PU (BNP | Noun)?
is used to figure outthose errors; Third, with analyzing our experi-ment results, the CRF and SVM chunker recog-nize differently on the determinative, thereforethe rule ?BNP ?
JJ BNP?, our combinationmethods figure out new BNP tags from the pre-liminary results according to this rule.
Finally,the most complex situation is the determinationof the Base NPs composed of series of nouns,especially the proper nouns.
With figuring outthe maximum length of this kind of noun phrase,we highlight the proper nouns and then separatethe complex noun phrase to base noun phrases,and according to the our experiments, this90method could solve close to 75% of the ambigu-ity in the errors from complex noun phrases.
To-tally, the rules could solve about 63% of thefound errors.4 ExperimentsThe CoNLL 2000 provided the software4 to con-vert Penn English Treebank II into the IOB tagsform.
We use the Penn Chinese Treebank 5.05,which is improved and involved with more POStags, segmentation and syntactic bracketing.
Asthe sentences in the Treebank are longer and re-lated to more complicated structures, we modifythe software with robust heuristics to cope withthose new features of the Chinese Treebank andgenerate the training and testing data sets fromthe Treebank.
Afterward we also make somemanual adjustments to the final data.In our experiments, the SVM chunker uses apolynomial kernel with degree 2; the cost perunit violation of the margin, C=1; and toleranceof the termination criterion, 0.01?
= .In the base NPs chunking task, the evaluationmetrics for base NP chunking include precision P,recall R and the F?
.
Usually we refer to the F?as the creditable metric.22#100%##100%#( 1)( 1)of correct proposed baseNPPof proposed baseNPof correct proposed baseNPRof corect baseNPRFFR F??
?
?==+=      =+?
?All the experiments were performed on aLinux system with 3.2 GHz Pentium 4 and 2Gmemory.
The total size of the Penn ChineseTreebank words is 13 MB, including about500,000 Chinese words.
The quantity of trainingcorpus amounts to 300,000 Chinese words.
Eachword contains two Chinese characters in average.We mainly use five kinds of corpus, whose sizesinclude 30000, 40000, 50000, 60000 and 70,000words.
The corpus with an even larger size isimproper according to the training corpusamount.4 http://ilk.kub.nl/~sabine/chunklink/5 http://www.cis.upenn.edu/~chinese/From Figure 2, we can see that the resultsfrom CRF are better than that from SVM and theerror-pruning performs the best.
Our hybrid er-ror-pruning method achieves an obvious im-provement F-scores by combining the outcomefrom SVM and CRF classifiers.
The test F-scoresare decreasing when the sizes of corpus increase.The best performance with F-score of 89.27% isachieved by using a test corpus of 30k words.We get about 1.0% increase of F-score after us-ing the hybrid approach.
The F-score is higherthan F-score 87.75% of Chinese base NP chunk-ing systems using the Maximum Entropy methodin (Zhou et al, 2003),.
Which used the smaller 3MB Penn Chinese Treebank II as the corpus.The Chinese Base NP chunkers are not supe-rior to those for English.
Zhang and Ando (2005)produce the best English base NP accuracy withF-score of 94.39+ (0.79), which is superior to ourbest results.
The previous work mostly consid-ered base NP chunking as the classification prob-lem without special attention to the lexicalinformation and syntactic dependence of words.On the other hand, we add some grammar rulesto strength the syntactic dependence between thewords.
However, the syntactic structure derivedfrom Chinese is much more flexible and complexthan that from English.
First, some Chinesewords contain abundant meanings or play differ-ent syntactic roles.
For example, "??
(amongwhich)/NN ?
?
(Chongqing)/NR ?
?
(district)/NN" is recognized as a base NP.
Actu-91ally the Chinese word ??
?/NN (among)?
refersto the content in the previous sentence and ???(thereinto)?
sometimes used as an adverb.
Sec-ond, how to deal with the conjunctions is a majorproblem, especially the words ??
(and)?
canappear in the preposition structure ??
??
??
(relate to)?, which makes it difficult to judgethose types of differences.
Third, the chunkerscan not handle with compact sequence data ofchunks with name entities and new words (espe-cially the transliterated words) satisfactorily,such as???
( China ) /NR ????
( Red Cross )/NR??
( Honorary ) /NN ??
(Chairman )/NN ???
( Jiang Ze-min ) /NR?As it points above, the English name entitiessequences are connected with the conjunctionsuch as ?of, and, in?.
While in Chinese there areno such connection words for name entities se-quences.
Therefore when we use the statisticalmethods, those kinds of sequential chunks con-tribute slightly to the feature selection and classi-fier training, and are treated as the useless noisein the training data.
In the testing section, it isclose the separating margin and hardly deter-mined to be in the right category.
What?s more,some other factors such as Idiomatic and special-ized expressions also account for the errors.
Byhighlighting those kinds of words and usingsome rules which emphasize on those properwords, we use our error-pruning methods anduseful grammar rules to correct about 60% errors.5 ConclusionsThis paper presented a new hybrid approach foridentifying the Chinese base NPs.
Our hybridapproach uses the SVM and CRF algorithm todesign the preliminary classifiers for chunking.Furthermore with the direct comparison betweenthe results from the former chunkers, we figureout that those two statistical methods are myopicabout the compact chunking data of sequentialnoun.
With the intention of capturing the syntac-tic dependence within those sequential chunkingdata, we make use of the conditional probabili-ties of the chunking tags given the correspondingtokens derived from CRF and some simplegrammar rules to modify the preliminary results.The overall results achieve 89.27% precisionon the base NP chunking.
We attempt to explainsome existing semantic problems and solve acertain part of problems, which have been dis-covered and explained in the paper.
Future workwill concentrate on working out some adaptivemachine learning methods to make grammarrules automatically, select better features andemploy suitable classifiers for Chinese base NPchunking.
Finally, the particular Chinese basephrase grammars need a complete study, and theapproach provides a primary solution andframework to process an analyses and compari-sons between Chinese and English parallel baseNP chunkers.AcknowledgmentsThis work was partially supported by the NaturalScience Foundation of China under Grant No.60575043, and 60121302, the China-France PRAproject under Grant No.
PRA SI02-05, the Out-standing Overseas Chinese Scholars Fund of theChinese Academy of Sciences under GrantNo.2003-1-1, and Nokia (China) Co. Ltd, as well.ReferencesClaire Cardie and David Pierce.
1998.
Error-DrivenPruning of Treebank Grammars for Base NounPhrase Identification.
Proceedings of the 36th ACLand COLING-98, 218-224.Claire Cardie and David Pierce.
1999.
The role oflexicalization and pruning for base noun phrasegrammars.
Proceedings of the 16th AAAI, 423-430.Dirk Ludtke and Satoshi Sato.
2003.
Fast Base NPChunking with Decision Trees ??
Experiments onDifferent POS tag Settings.
CICLing 2003, 136-147.
LNC S2588, Springer-Verlag Berlin Heidel-berg.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the CoNLL-2000 Shared Task:Chunking.
Proceedings of CoNLL and LLL-2000,127-132.Erik F. Tjong Kim Sang, Walter Daelemans, Herv?D?ean, Rob Koeling, Yuval Krymolowski, VasinPunyakanok, and Dan Roth.
2000.
Applying systemcombination to base noun phrase identification.Proceedings of COLING 2000, 857-863.Fei Sha and Fernando Pereira.
2003.
Shallow Parsingwith Conditional Random Fields.
Proceedings ofHLT-NAACL 2003, 134-141.Heng Li, Jonathan J. Webster, Chunyu Kit, andTianshun Yao.
2003.
Transductive HMM basedChinese Text Chunking.
IEEE NLP-KE 2003, Bei-jing, 257-262.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text Chunking using Transformation-Based Learn-ing.
Proceedings of the Third ACL Workshop onVery Large Corpora, 82?94.92Lafferty A. McCallum and F. Pereira.
2001.
Condi-tional random Fields.
Proceedings of ICML 2001,282-289.Rie Kubota Ando and Tong Zhang.
2004.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
RC23462.
Technicalreport, IBM.Rie Kubota Ando and Tong Zhang.
2005.
A High-Performance Semi-Supervised Learning Methodfor Text Chunking.
Proceedings of the 43rd AnnualMeeting of ACL, 1-9.Simon Lacoste-Julien.
2003.
Combining SVM withgraphical models for supervised classification: anintroduction to Max-Margin Markov Network.CS281A Project Report, UC Berkeley.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machine.
Proceeding of theNAACL, 192-199.Zhang Yuqi and Zhou Qiang.
2002.
Chinese Base-Phrases Chunking.
First SigHAN Workshop onChinese Language Processing, COLING-02.Zhao Jun and Huang Changling.
1998.
A Quasi-Dependency Model for Structural Analysis of Chi-nese BaseNPs.
36th Annual Meeting of the ACLand 17th International Conference on Computa-tional Linguistics.Zhou Yaqian, Guo YiKun, Huang XuanLing and WuLide.
2003.
Chinese and English Base NP Recogni-tion on a Maximum Entropy Model.
Vol140, No13.Journal of Computer Research and Development.
(In Chinese)93
