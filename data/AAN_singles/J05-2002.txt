A General Technique to Train LanguageModels on Language ModelsMark-Jan Nederhof?University of GroningenWe show that under certain conditions, a language model can be trained on the basis of asecond language model.
The main instance of the technique trains a finite automaton on thebasis of a probabilistic context-free grammar, such that the Kullback-Leibler distance betweengrammar and trained automaton is provably minimal.
This is a substantial generalization ofan existing algorithm to train an n-gram model on the basis of a probabilistic context-freegrammar.1.
IntroductionIn this article, the term language model is used to refer to any description that assignsprobabilities to strings over a certain alphabet.
Language models have importantapplications in natural language processing, and in particular, in speech recognitionsystems (Manning and Schu?tze 1999).Language models often consist of a symbolic description of a language, such asa finite automaton (FA) or a context-free grammar (CFG), extended by a probabilityassignment to, for example, the transitions of the FA or the rules of the CFG, by whichwe obtain a probabilistic finite automaton (PFA) or probabilistic context-free grammar(PCFG), respectively.
For certain applications, one may first determine the symbolic partof the automaton or grammar and in a second phase try to find reliable probabilityestimates for the transitions or rules.
The current article is involved with the secondproblem, that of extending FAs or CFGs to become PFAs or PCFGs.
We refer to thisprocess as training.Training is often done on the basis of a corpus of actual language use in a certaindomain.
If each sentence in this corpus is annotated by a list of transitions of anFA recognizing the sentence or a parse tree for a CFG generating the sentence, thentraining may consist simply in relative frequency estimation.
This means that we estimateprobabilities of transitions or rules by counting their frequencies in the corpus, relativeto the frequencies of the start states of transitions or to the frequencies of the left-handside nonterminals of rules, respectively.
By this estimation, the likelihood of the corpusis maximized.The technique we introduce in this article is different in that training is done onthe basis not of a finite corpus, but of an input language model.
Our goal is to findestimations for the probabilities of transitions or rules of the input FA or CFG such that?
Faculty of Arts, Humanities Computing, P.O.
Box 716, NL-9700 AS Groningen, The Netherlands.E-mail: markjan@let.rug.nl.Submission received: 20th January 2004; Revised submission received: 5th August 2004; Accepted forpublication: 19th September 2004?
2005 Association for Computational LinguisticsComputational Linguistics Volume 31, Number 2the resulting PFA or PCFG approximates the input language model as well as possible,or more specifically, such that the Kullback-Leibler (KL) distance (or relative entropy)between the input model and the trained model is minimized.
The input FA or CFG tobe trained may be structurally unrelated to the input language model.This technique has several applications.
One is an extension with probabilitiesof existing work on approximation of CFGs by means of FAs (Nederhof 2000).
Themotivation for this work was that application of FAs is generally less costly thanapplication of CFGs, which is an important benefit when the input is very large, asis often the case in, for example, speech recognition systems.
The practical relevance ofthis work was limited, however, by the fact that in practice one is more interested inthe probabilities of sentences than in a purely Boolean distinction between grammaticaland ungrammatical sentences.Several approaches were discussed by Mohri and Nederhof (2001) to extend thiswork to approximation of PCFGs by means of PFAs.
A first approach is to directly maprules with attached probabilities to transitions with attached probabilities.
Althoughthis is computationally the easiest approach, the resulting PFA may be a very inaccurateapproximation of the probability distribution described by the input PCFG.
In particu-lar, there may be assignments of probabilities to the transitions of the same FA that leadto more accurate approximating language models.A second approach is to train the approximating FA by means of a corpus.
Ifthe input PCFG was itself obtained by training on a corpus, then we already possesstraining material.
However, this may not always be the case, and no training materialmay be available.
Furthermore, as a determinized approximating FA may be muchlarger than the input PCFG, the sparse-data problem may be more severe for theautomaton than it was for the grammar.1 Hence, even if sufficient material was availableto train the CFG, it may not be sufficient to accurately train the FA.A third approach is to construct a training corpus from the PCFG by means ofa (pseudo)random generator of sentences, such that sentences that are more likelyaccording to the PCFG are generated with greater likelihood.
This has been proposedby Jurafsky et al (1994), for the special case of bigrams, extending a nonprobabilistictechnique by Zue et al (1991).
It is not clear, however, whether this idea is feasiblefor training of finite-state models that are larger than bigrams.
The reason is thatvery large corpora would have to be generated in order to obtain accurate probabilityestimates for the PFA.
Note that the number of parameters of a bigram model isbounded by the square of the size of the lexicon; such a bound does not exist forgeneral PFAs.The current article discusses a fourth approach.
In the limit, it is equivalent to thethird approach above, as if an infinite corpus were constructed on which the PFA istrained, but we have found a way to avoid considering sentences individually.
The keyidea that allows us to handle an infinite set of strings generated by the PCFG is that weconstruct a new grammar that represents the intersection of the languages described bythe input PCFG and the FA.
Within this new grammar, we can compute the expectedfrequencies of transitions of the FA, using a fairly standard analysis of PCFGs.
Theseexpected frequencies then allow us to determine the assignment of probabilities totransitions of the FA that minimizes the KL distance between the PCFG and the resultingPFA.1 In Nederhof (2000), several methods of approximation were discussed that lead to determinizedapproximating FAs that can be much larger than the input CFGs.174Nederhof Training Models on ModelsThe only requirement is that the FA to be trained be unambiguous, by which wemean that each input string can be recognized by at most one computation of the FA.The special case of n-grams has already been formulated by Stolcke and Segal (1994),realizing an idea previously envisioned by Rimon and Herz (1991).
An n-gram model ishere seen as a (P)FA that contains exactly one state for each possible history of the n ?
1previously read symbols.
It is clear that such an FA is unambiguous (even deterministic)and that our technique therefore properly subsumes the technique by Stolcke and Segal(1994), although the way that the two techniques are formulated is rather different.
Alsonote that the FA underlying an n-gram model accepts any input string over the alphabet,which does not hold for general (unambiguous) FAs.Another application of our work involves determinization and minimization ofPFAs.
As shown by Mohri (1997), PFAs cannot always be determinized, and no practicalalgorithms are known to minimize arbitrary nondeterministic (P)FAs.
This can be aproblem when deterministic or small PFAs are required.
We can, however, alwayscompute a minimal deterministic FA equivalent to an input FA.
The new results in thisarticle offer a way to extend this determinized FA to a PFA such that it approximatesthe probability distribution described by the input PFA as well as possible, in terms ofthe KL distance.Although the proposed technique has some limitations, in particular, that the modelto be trained is unambiguous, it is by no means restricted to language models based onfinite automata or context-free grammars, as several other probabilistic grammaticalformalisms can be treated in a similar manner.The structure of this article is as follows.
We provide some preliminary definitionsin Section 2.
Section 3 discusses how the expected frequency of a rule in a PCFG can becomputed.
This is an auxiliary step in the algorithms to be discussed below.
Section 4defines a way to combine a PFA and a PCFG into a new PCFG that extends a well-knownrepresentation of the intersection of a regular and a context-free language.
Therebywe merge the input model and the model to be trained into a single structure.
Thisstructure is the foundation for a number of algorithms, presented in section 5, whichallow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1),training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of anunambiguous FA on the basis of a PFA (section 5.3).2.
PreliminariesMany of the definitions on probabilistic context-free grammars are based on Santos(1972) and Booth and Thompson (1973), and the definitions on probabilistic finiteautomata are based on Paz (1971) and Starke (1972).A context-free grammar G is a 4-tuple (?, N, S, R), where ?
and N are two finitedisjoint sets of terminals and nonterminals, respectively, S ?
N is the start symbol, andR is a finite set of rules, each of the form A ?
?, where A ?
N and ?
?
(?
?
N)?.
Aprobabilistic context-free grammar G is a 5-tuple (?, N, S, R, pG ), where ?, N, S and Rare as above, and pG is a function from rules in R to probabilities.In what follows, symbol a ranges over the set ?, symbols w, v range over theset ?
?, symbols A, B range over the set N, symbol X ranges over the set ?
?
N,symbols ?,?,?
range over the set (?
?
N)?, symbol ?
ranges over the set R, andsymbols d, e range over the set R?.
With slight abuse of notation, we treat a rule?
= (A ?
?)
?
R as an atomic symbol when it occurs within a string d?e ?
R?.The symbol  denotes the empty string.
String concatenation is represented byoperator ?
or by empty space.175Computational Linguistics Volume 31, Number 2For a fixed (P)CFG G, we define the relation ?
on triples consisting of two strings?,?
?
(?
?
N)?
and a rule ?
?
R by ?
??
?, if and only if ?
is of the form wA?
and ?is of the form w?
?, for some w ?
??
and ?
?
(?
?
N)?, and ?
= (A ?
?).
A leftmostderivation (in G) is a string d = ?1 ?
?
?
?m, m ?
0, such that ?0?1?
?1?2?
?
?
?
?m?
?m, forsome ?0, .
.
.
,?m ?
(?
?
N)?
; d =  is always a leftmost derivation.
In the remainderof this article, we let the term derivation refer to leftmost derivation, unless spec-ified otherwise.
If ?0?1?
?
?
?
?m?
?m for some ?0, .
.
.
,?m ?
(?
?
N)?, then we say thatd = ?1 ?
?
?
?m derives ?m from ?0, and we write ?0 d?
?m;  derives any ?0 ?
(?
?
N)?from itself.
A derivation d such that S d?
w, for some w ?
?
?, is called a completederivation.
We say that G is unambiguous if for each w ?
?
?, S d?
w for at mostone d ?
R?.Let G be a fixed PCFG (?, N, S, R, pG ).
For ?,?
?
(?
?
N)?
and d = ?1 ?
?
?
?m ?
R?,m ?
0, we define pG (?
d?
?)
=?mi=1 pG (?i) if ?d?
?, and pG (?d?
?)
= 0 otherwise.
Theprobability pG (w) of a string w ?
??
is defined to be?d pG (Sd?
w).PCFG G is said to be proper if??,?
pG (A??
?)
= 1 for all A ?
N, that is, if theprobabilities of all rules ?
= (A ?
?)
with left-hand side A sum to one.
PCFG G is said tobe consistent if?w pG (w) = 1.
Consistency implies that the PCFG defines a probabilitydistribution on the set of terminal strings.
There is a practical sufficient condition forconsistency that is decidable (Booth and Thompson 1973).A PCFG is said to be reduced if for each nonterminal A, there are d1, d2 ?
R?,w1, w2 ?
?
?, and ?
?
(?
?
N)?
such that pG (Sd1?
w1A?)
?
pG (w1A?d2?
w1w2) > 0.
Inwords, if a PCFG is reduced, then for each nonterminal A, there is at least one derivationd1d2 with nonzero probability that derives a string w1w2 from S and that includessome rule with left-hand side A.
A PCFG G that is not reduced can be turned intoone that is reduced and that describes the same probability distribution, provided that?w pG (w) > 0.
This reduction consists in removing from the grammar any nonterminalA for which the above conditions do not hold, together with any rule that containssuch a nonterminal; see Aho and Ullman (1972) for reduction of CFGs, which is verysimilar.A finite automaton M is a 5-tuple (?, Q, q0, qf , T), where ?
and Q are twofinite sets of terminals and states, respectively, q0, qf ?
Q are the initial and finalstates, respectively, and T is a finite set of transitions, each of the form r a?
s, wherer ?
Q ?
{qf}, s ?
Q, and a ?
?.2 A probabilistic finite automaton M is a 6-tuple (?, Q,q0, qf , T, pM), where?, Q, q0, qf , and T are as above, and pM is a function from transitionsin T to probabilities.In what follows, symbols q, r, s range over the set Q, symbol ?
ranges over the set T,and symbol c ranges over the set T?.For a fixed (P)FA M, we define a configuration to be an element of Q ?
?
?, and wedefine the relation  on triples consisting of two configurations and a transition ?
?
Tby (r, w)? (s, w?)
if and only if w is of the form aw?, for some a ?
?, and ?
= (r a?
s).A computation (in M) is a string c = ?1 ?
?
?
?m, m ?
0, such that (r0, w0)?1 (r1, w1)?2 ?
?
?
?m (rm, wm), for some (r0, w0), .
.
.
, (rm, wm) ?
Q ?
??
; c =  is always a compu-tation.
If (r0, w0)?1 ?
?
?
?m (rm, wm) for some (r0, w0), .
.
.
, (rm, wm) ?
Q ?
??
and c = ?1 ?
?
?
?m ?
T?, then we write (r0, w0)c (rm, wm).
We say that c recognizes w if (q0, w)c (qf ,).2 That we only allow one final state is not a serious restriction with regard to the set of strings we canprocess; only when the empty string is to be recognized could this lead to difficulties.
Lifting therestriction would encumber the presentation with treatment of additional cases without affecting,however, the validity of the main results.176Nederhof Training Models on ModelsLet M be a fixed FA (?, Q, q0, qf , T).
The language L(M) accepted by M isdefined to be {w ?
??
| ?c[(q?, w)c (qf ,)]}.
We say M is unambiguous if for eachw ?
?
?, (q0, w)c (qf ,) for at most one c ?
T?.
We say M is deterministic if for each(r, w) ?
Q ?
?
?, there is at most one combination of ?
?
T and (s, w?)
?
Q ?
??
suchthat (r, w)? (s, w?).
Turning a given FA into one that is deterministic and accepts thesame language is called determinization.
All FAs can be determinized.
Turning a given(deterministic) FA into the smallest (deterministic) FA that accepts the same languageis called minimization.
There are effective algorithms for minimization of deterministicFAs.Let M be a fixed PFA (?, Q, q0, qf , T, pM).
For (r, w), (s, v) ?
Q ?
??
andc = ?1 ?
?
?
?m ?
T?, we define pM((r, w)c (s, v)) =?mi=1 pM(?i) if (r, w)c (s, v), andpM((r, w)c (s, v)) = 0 otherwise.
The probability pM(w) of a string w ?
??
is definedto be?c pM((q0, w)c (qf ,)).PFA M is said to be proper if?
?,a,s: ?=(r a?s)?T pM(?)
= 1 for all r ?
Q ?
{qf}.3.
Expected Frequencies of RulesLet G be a PCFG (?, N, S, R, pG ).
We assume without loss of generality that S does notoccur in the right-hand side of any rule from R. For each rule ?, we defineE(?)
=?d,d?,wpG (Sd?d??
w) (1)If G is proper and consistent, (1) is the expected frequency of ?
in a complete derivation.Each complete derivation d?d?
can be written as d?d??d??
?, with d?
= d??d??
?, whereS d?
w?A?, A ??
?,?
d???
w??,?
d????
w???
(2)for some A, ?, ?, w?, w?
?, and w???.
ThereforeE(?)
= outer(A) ?
pG (?)
?
inner(?)
(3)where we defineouter(A) =?d,w?,?,d???,w??
?pG (Sd?
w?A?)
?
pG (?d????
w???)
(4)inner(?)
=?d??,w?
?pG (?d???
w??)
(5)for each A ?
N and ?
?
(?
?
N)?.
From the definition of inner, we can easily derive thefollowing equations:inner(a) = 1 (6)inner(A) =??,?:?=(A??
)pG (?)
?
inner(?)
(7)inner(X?)
= inner(X) ?
inner(?)
(8)177Computational Linguistics Volume 31, Number 2This can be taken as a recursive definition of inner, assuming ?
=  in (8).
Similarly, wecan derive a recursive definition of outer:outer(S) = 1 (9)outer(A) =??,B,?,?:?=(B??A?
)outer(B) ?
pG (?)
?
inner(?)
?
inner(?)
(10)for A = S.In general, there may be cyclic dependencies in the equations for inner and outer;that is, for certain nonterminals A, inner(A) and outer(A) may be defined in termsof themselves.
There may even be no closed-form expression for inner(A).
However,one may approximate the solutions to arbitrary precision by means of fixed-pointiteration.4.
Intersection of Context-Free and Regular LanguagesWe recall a construction from Bar-Hillel, Perles, and Shamir (1964) that computes theintersection of a context-free language and a regular language.
The input consists of aCFG G = (?, N, S, R) and an FA M= (?, Q, q0, qf , T); note that we assume, without lossof generality, that G and M share the same set of terminals ?.The output of the construction is CFG G?
= (?, N?, S?, R?
), where N?
= Q ?(?
?
N) ?
Q, S?
= (q0, S, qf ), and R?
consists of the set of rules that is obtained asfollows: For each rule ?
= (A ?
X1 ?
?
?Xm) ?
R, m ?
0, and each sequence of statesr0, .
.
.
, rm ?
Q, let the rule ??
= ((r0, A, rm) ?
(r0, X1, r1) ?
?
?
(rm?1, Xm, rm))be in R?
; for m = 0, R?
contains a rule ??
= ((r0, A, r0) ?
) for eachstate r0. For each transition ?
= (r a?
s) ?
T, let the rule ??
= ((r, a, s) ?
a) bein R?.Note that for each rule (r0, A, rm) ?
(r0, X1, r1) ?
?
?
(rm?1, Xm, rm) from R?, there is aunique rule A ?
X1 ?
?
?Xm from R from which it has been constructed by the above.Similarly, each rule (r, a, s) ?
a uniquely identifies a transition r a?
s. This means that ifwe take a derivation d?
in G?, we can extract a sequence h1(d?)
of rules from G and asequence h2(d?)
of transitions from M, where h1 and h2 are string homomorphisms thatwe define pointwise ash1(??)
= ?
if ??
= ((r0, A, rm) ?
(r0, X1, r1) ?
?
?
(rm?1, Xm, rm))and ?
= (A ?
X1 ?
?
?Xm)(11) if ??
= ((r, a, s) ?
a) (12)h2(??)
= ?
if ??
= ((r, a, s) ?
a) and ?
= (ra?
s) (13) if ??
= ((r0, A, rm) ?
(r0, X1, r1) ?
?
?
(rm?1, Xm, rm)) (14)178Nederhof Training Models on ModelsWe define h(d?)
= (h1(d?
), h2(d?)).
It can be easily shown that if h(d?)
= (d, c) andS?d??
w, then for the same w, we have S d?
w and (q0, w)c (qf ,).
Conversely, if for somew, d, and c we have S d?
w and (q0, w)c (qf ,), then there is precisely one derivation d?such that h(d?)
= (d, c) and S?d??
w.It was observed by Lang (1994) that G?
can be seen as a parse forest, that is, acompact representation of all parse trees according to G that derive strings recognizedby M. The construction can be generalized to, for example, tree-adjoining grammars(Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000;Bertsch and Nederhof 2001).
The construction for the latter also has implications forlinear context-free rewriting systems (Seki et al 1991).The construction has been extended by Nederhof and Satta (2003) to apply to aPCFG G = (?, N, S, R, pG ) and a PFA M = (?, Q, q0, qf , T, pM).
The output is aPCFG G?
= (?, N?, S?, R?, p?
), where N?, S?, and R?
are as before, and p?
isdefined byp?
((r0, A, rm) ?
(r0, X1, r1) ?
?
?
(rm?1, Xm, rm)) = pG (A ?
X1 ?
?
?Xm) (15)p?
((r, a, s) ?
a) = pM(ra?
s) (16)If d?, d, and c are such that h(d?)
= (d, c), then clearly p?(d?)
= pG (d) ?
pM(c).5.
Training Models on ModelsWe restrict ourselves to a few cases of the general technique of training a model on thebasis of another model.5.1 Training a PFA on a PCFGLet us assume we have a proper and consistent PCFG G = (?, N, S, R, pG ) and an FAM = (?, Q, q0, qf , T) that is unambiguous.
This FA may have resulted from (nonprob-abilistic) approximation of CFG (?, N, S, R), but it may also be totally unrelated to G.Note that an FA is guaranteed to be unambiguous if it is deterministic; any FA can bedeterminized.
Our goal is now to assign probabilities to the transitions from FA M toobtain a proper PFA that approximates the probability distribution described by G aswell as possible.Let us define 1 as the function that maps each transition from T to one.
This meansthat for each r, w, c and s, 1((r, w)c (s,)) = 1 if (r, w)c (s,), and 1((r, w)c (s,)) = 0otherwise.Of the set of strings generated by G, a subset is recognized by computations of M;note again that there can be at most one such computation for each string.
The expectedfrequency of a transition ?
in such computations is given byE(?)
=?w,c,c?pG (w) ?
1((q0, w)c?c? (qf ,)) (17)Now we construct the PCFG G?
as explained in section 4 from the PCFG G and thePFA (?, Q, q0, qf , T, 1).
Let ?
= (ra?
s) ?
T and ?
= ((r, a, s) ?
a).
On the basis of the179Computational Linguistics Volume 31, Number 2properties of function h, we can now rewrite E(?)
asE(?)
=?d,w,c,c?pG (Sd?
w) ?
1((q0, w)c?c? (qf ,))=?e,d,w,c,c?
:h(e)=(d,c?c?
)pG (Sd?
w) ?
1((q0, w)c?c? (qf ,))=?e,e?,wp?(S?e?e??
w)= E(?)
(18)Hereby we have expressed the expected frequency of a transition ?
= (r a?
s) interms of the expected frequency of rule ?
= ((r, a, s) ?
a) in derivations in PCFG G?.It was explained in section 3 how such a value can be computed.
Note that sinceby definition 1(?)
= 1, also p?(?)
= 1.
Furthermore, for the right-hand side a of ?,inner(a) = 1.
Therefore,E(?)
= outer((r, a, s)) ?
p?(?)
?
inner(a)= outer((r, a, s)) (19)To obtain the required PFA (?, Q, q0, qf , T, pM), we now define the probabilityfunction pM for each ?
= (ra?
s) ?
T aspM(?)
=outer((r, a, s))?a?,s?
:(r a??s?
)?Touter((r, a?, s?
))(20)That such a relative frequency estimator pM minimizes the KL distance between pG andpM on the domain L(M) is proven in the appendix.An example with finite languages is given in Figure 1.
We have, for example,pM(q0a?
q1) =outer((q0, a, q1))outer((q0, a, q1)) + outer((q0, c, q1))=1313 +23= 13 (21)5.2 Training a PCFG on a PFASimilarly to section 5.1, we now assume we have a proper PFA M = (?, Q, q0,qf , T, pM) and a CFG G = (?, N, S, R) that is unambiguous.
Our goal is to find afunction pG that lets proper and consistent PCFG (?, N, S, R, pG ) approximate M aswell as possible.
Although CFGs used for natural language processing are usuallyambiguous, there may be cases in other fields in which we may assume grammars areunambiguous.180Nederhof Training Models on ModelsFigure 1Example of input PCFG G, with rule probabilities between square brackets, input FA M, thereduced PCFG G?, and the resulting trained PFA.Let us define 1 as the function that maps each rule from R to one.
Of the set ofstrings recognized by M, a subset can be derived in G. The expected frequency of a rule?
in those derivations is given byE(?)
=?d,d?,wpM(w) ?
1(Sd?d??
w) (22)Now we construct the PCFG G?
from the PCFG G = (?, N, S, R, 1) and thePFA M as explained in section 4.
Analogously to section 5.1, we obtain for each?
= (A ?
X1 ?
?
?Xm)E(?)
=?r0,r1,...,rmE((r0, A, rm) ?
(r0, X1, r1) ?
?
?
(rm?1, Xm, rm))=?r0,r1,...,rmouter((r0, A, rm)) ?
inner((r0, X1, r1) ?
?
?
(rm?1, Xm, rm)) (23)To obtain the required PCFG (?, N, S, R, pG ), we now define the probability functionpG for each ?
= (A ?
?)
aspG (?)
=E(?)???=(A???
)?R E(??
)(24)The proof that this relative frequency estimator pG minimizes the KL distance betweenpM and pG on the domain L(G) is almost identical to the proof in the appendix for asimilar claim from section 5.1.5.3 Training a PFA on a PFAWe now assume we have a proper PFA M1 = (?, Q1, q0,1, qf,1, T1, p1) and an FAM2 = (?, Q2, q0,2, qf,2, T2) that is unambiguous.
Our goal is to find a function p2 so that181Computational Linguistics Volume 31, Number 2proper PFA (?, Q2, q0,2, qf,2, T2, p2) approximates M1 as well as possible, minimizingthe KL distance between p1 and p2 on the domain L(M2).One way to solve this problem is to map M2 to an equivalent right-linear CFG G andthen to apply the algorithm from section 5.2.
The obtained probability function pG canbe translated back to an appropriate function p2.
For this special case, the constructionfrom section 4 can be simplified to the ?cross-product?
construction of finite automata(see, e.g., Aho and Ullman 1972).
The simplified forms of the functions inner and outerfrom section 3 are commonly called forward and backward, respectively, and they aredefined by systems of linear equations.
As a result, we can compute exact solutions, asopposed to approximate solutions by iteration.AppendixWe now prove that the choice of pM in section 5.1 is such that it minimizes the Kullback-Leibler distance between pG and pM, restricted to the domain L(M).
Without thisrestriction, the KL distance is given byD(pG?pM) =?wpG (w) ?
logpG (w)pM(w)(25)This can be used for many applications mentioned in section 1.
For example, an FA Mapproximating a CFG G is guaranteed to be such that L(M) ?
L(G) in the case of mostpractical approximation algorithms.
However, if there are strings w such that w /?
L(M)and pG (w) > 0, then (25) is infinite, regardless of the choice of pM.
We therefore restrictpG to the domain L(M) and normalize it to obtainpG|M(w) =pG (w)Z , if w ?
L(M) (26)0, otherwise (27)where Z =?w:w?L(M) pG (w).
Note that pG|M = pG if L(M) ?
L(G).
Our goal is now toshow that our choice of pM minimizesD(pG|M?pM) =?w:w?L(M)pG|M(w) ?
logpG|M(w)pM(w)= log 1Z +1Z?w:w?L(M)pG (w) ?
logpG (w)pM(w)(28)As Z is independent of pM, it is sufficient to show that our choice of pM minimizes?w:w?L(M)pG (w) ?
logpG (w)pM(w)(29)Now consider the expression??pM(?)E(?)
(30)182Nederhof Training Models on ModelsBy the usual proof technique with Lagrange multipliers, it is easy to show that ourchoice of pM in section 5.1, given bypM(?)
=E(?)???,a?,s?:?
?=(r a??s?
)?TE(??
)(31)for each ?
= (r a?
s) ?
T, is such that it maximizes (30), under the constraint ofproperness.For ?
?
T and w ?
?
?, we define #?
(w) to be zero, if w /?
L(M), and otherwise to bethe number of occurrences of ?
in the (unique) computation that recognizes w.
Formally,#?
(w) =?c,c?
1((q0, w)c?c? (qf ,)).
We rewrite (30) as??pM(?)E(?)
=??pM(?
)?w pG (w)?#?(w)=?w??pM(?
)pG (w)?#?(w)=?w(??pM(?)#?
(w))pG (w)=?w:pM(w)>0pM(w)pG (w)=?w:pM(w)>02pG (w)?log pM(w)=?w:pM(w)>02pG (w)?log pM(w)?pG (w)?log pG (w)+pG (w)?log pG (w)=?w:pM(w)>02?pG (w)?logpG (w)pM (w)+pG (w)?log pG (w)= 2?
?w:pM (w)>0 pG (w)?logpG (w)pM (w) ?
2?w:pM (w)>0 pG (w)?log pG (w) (32)We have already seen that the choice of pM that maximizes (30) is given by (31), and(31) implies pM(w) > 0 for all w such that w ?
L(M) and pG (w) > 0.
Since pM(w) > 0 isimpossible for w /?
L(M), the value of2?w:pM (w)>0 pG (w)?log pG (w) (33)is determined solely by pG and by the condition that pM(w) > 0 for all w such thatw ?
L(M) and pG (w) > 0.
This implies that (30) is maximized by choosing pM suchthat2?
?w:pM (w)>0 pG (w)?logpG (w)pM (w) (34)183Computational Linguistics Volume 31, Number 2is maximized, or alternatively that?w:pM(w)>0pG (w) ?
logpG (w)pM(w)(35)is minimized, under the constraint that pM(w) > 0 for all w such that w ?
L(M) andpG (w) > 0.
For this choice of pM, (29) equals (35).Conversely, if a choice of pM minimizes (29), we may assume that pM(w) > 0 forall w such that w ?
L(M) and pG (w) > 0, since otherwise (29) is infinite.
Again, for thischoice of pM, (29) equals (35).
It follows that the choice of pM that minimizes (29) concurswith the choice of pM that maximizes (30), which concludes our proof.AcknowledgmentsComments by Khalil Sima?an, Giorgio Satta,Yuval Krymolowski, and anonymousreviewers are gratefully acknowledged.
Theauthor is supported by the PIONIER ProjectAlgorithms for Linguistic Processing, fundedby NWO (Dutch Organization for ScientificResearch).ReferencesAho, Alfred V. and Jeffrey D. Ullman.
1972.Parsing, volume 1 of The Theory of Parsing,Translation and Compiling.
Prentice Hall,Englewood Cliffs, NJ.Bar-Hillel, Yehoshua, M. Perles, andE.
Shamir.
1964.
On formal properties ofsimple phrase structure grammars.
InYehoshua Bar-Hillel, editor, Language andInformation: Selected Essays on Their Theoryand Application.
Addison-Wesley, Reading,MA, pages 116?150.Bertsch, Eberhard and Mark-Jan Nederhof.2001.
On the complexity of someextensions of RCG parsing.
In Proceedingsof the Seventh International Workshop onParsing Technologies, pages 66?77, Beijing,October.Booth, Taylor L. and Richard A. Thompson.1973.
Applying probabilistic measures toabstract languages.
IEEE Transactions onComputers, C-22(5):442?450.Boullier, Pierre.
2000.
Range concatenationgrammars.
In Proceedings of the SixthInternational Workshop on ParsingTechnologies, pages 53?64, Trento, Italy,February.Jurafsky, Daniel, Chuck Wooters, GaryTajchman, Jonathan Segal, AndreasStolcke, Eric Fosler, and Nelson Morgan.1994.
The Berkeley Restaurant Project.
InProceedings of the International Conference onSpoken Language Processing (ICSLP-94),pages 2139?2142, Yokohama, Japan.Lang, Bernard.
1994.
Recognition can beharder than parsing.
ComputationalIntelligence, 10(4):486?494.Manning, Christopher D. and HinrichSchu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press,Cambridge, MA.Mohri, Mehryar.
1997.
Finite-statetransducers in language and speechprocessing.
Computational Linguistics,23(2):269?311.Mohri, Mehryar and Mark-Jan Nederhof.2001.
Regular approximation ofcontext-free grammars throughtransformation.
In J.-C. Junqua and G. vanNoord, editors, Robustness in Language andSpeech Technology.
Kluwer Academic,pages 153?163.Nederhof, Mark-Jan. 2000.
Practicalexperiments with regular approximationof context-free languages.
ComputationalLinguistics, 26(1):17?44.Nederhof, Mark-Jan and Giorgio Satta.
2003.Probabilistic parsing as intersection.
InProceedings of the Eighth InternationalWorkshop on Parsing Technologies, pages137?148, Laboratoire Lorrain de rechercheen informatique et ses applications(LORIA), Nancy, France, April.Paz, Azaria.
1971.
Introduction to ProbabilisticAutomata.
Academic Press, New York.Rimon, Mori and J. Herz.
1991.
Therecognition capacity of local syntacticconstraints.
In Proceedings of the FifthConference of the European Chapter of theACL, pages 155?160, Berlin, April.Santos, Eugene S. 1972.
Probabilisticgrammars and automata.
Information andControl, 21:27?47.Seki, Hiroyuki, Takashi Matsumura,Mamoru Fujii, and Tadao Kasami.1991.
On multiple context-free grammars.Theoretical Computer Science,88:191?229.184Nederhof Training Models on ModelsStarke, Peter H. 1972.
Abstract Automata.North-Holland, Amsterdam.Stolcke, Andreas and Jonathan Segal.
1994.Precise N-gram probabilities fromstochastic context-free grammars.
InProceedings of the 32nd Annual Meetingof the ACL, pages 74?79, Las Cruces,NM, June.Vijay-Shanker, K. and David J. Weir.1993.
The use of shared forests intree adjoining grammar parsing.
InProceedings of the Sixth Conference of theEuropean Chapter of the ACL, pages 384?393,Utrecht, The Netherlands, April.Zue, Victor, James Glass, David Goodine,Hong Leung, Michael Phillips, JosephPolifroni, and Stephanie Seneff.
1991.Integration of speech recognition andnatural language processing in the MITVoyager system.
In Proceedings of theICASSP-91, Toronto, volume 1, pages713?716.185
