Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 81?87,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPThe Modulation of Cooperation and Emotion in Dialogue:The REC CorpusFederica CavicchioMind and Brain Center/ Corso Bettini 31,38068 Rovereto (Tn) Italyfederica.cavicchio@unitn.itAbstractIn this paper we describe the Rovereto EmotiveCorpus (REC) which we collected to investigatethe relationship between emotion and coopera-tion in dialogue tasks.
It is an area where stillmany unsolved questions are present.
One of themain open issues is the annotation of the so-called ?blended?
emotions and their recognition.Usually, there is a low agreement among ratersin annotating emotions and, surprisingly, emo-tion recognition is higher in a condition of mod-ality deprivation (i. e. only acoustic or only visu-al modality vs. bimodal display of emotion).
Be-cause of these previous results, we collected acorpus in which ?emotive?
tokens are pointedout during the recordings by psychophysiologi-cal indexes (ElectroCardioGram, and GalvanicSkin Conductance).
From the output values ofthese indexes a general recognition of each emo-tion arousal is allowed.
After this selection wewill annotate emotive interactions with our mul-timodal annotation scheme, performing a kappastatistic on annotation results to validate ourcoding scheme.
In the near future, a logistic re-gression on annotated data will be performed tofind out correlations between cooperation andnegative emotions.
A final step will be an fMRIexperiment on emotion recognition of blendedemotions from face displays.1 IntroductionIn the last years many multimodal corpora havebeen collected.
These corpora have been recordedin several languages and have being elicited withdifferent methodologies: acted (such as for emo-tion corpora, see for example Goeleven, 2008),task oriented corpora, multiparty dialogs, corporaelicited with scripts or storytelling and ecologicalcorpora.
Among the goals of collection and analy-sis of corpora there is shading light on crucial as-pects of speech production.
Some of the main re-search questions are how language and gesturecorrelate with each other (Kipp et al, 2006) andhow emotion expression modifies speech (MagnoCaldognetto et al, 2004) and gesture (Poggi,2007).
Moreover, great efforts have been done toanalyze multimodal aspects of irony, persuasionor motivation.Multimodal coding schemes are mainly focusedon dialogue acts, topic segmentation and the socalled ?emotional area?.
The collection of mul-timodal data has raised the question of codingscheme reliability.
The aim of testing codingscheme reliability is to assess whether a schemeis able to capture observable reality and allowssome generalizations.
From mid Nineties, thekappa statistic has begun to be applied to vali-date coding scheme reliability.
Basically, thekappa statistic is a statistical method to assessagreement among a group of observers.
Kappahas been used to validate some multimodal cod-ing schemes too.
However, up to now many mul-timodal coding schemes have a very low kappascore (Carletta, 2007, Douglas-Cowie et al,2005; Pianesi et al, 2005, Reidsma et al, 2008).This could be due to the nature of multimodaldata.
In fact, annotation of mental and emotionalstates of mind is a very demanding task.
The lowannotation agreement which affects multimodalcorpora validation could also be due to the natureof the kappa statistics.
In fact, the assumptionunderlining the use of kappa as reliability meas-ure is that coding scheme categories are mutuallyexclusive and equally distinct one another.
Thisis clearly difficult to be obtained in multimodalcorpora annotation, as communication channels(i.e.
voice, face movements, gestures and post-ure) are deeply interconnected one another.To overcome these limits we are collecting anew corpus, Rovereto Emotive Corpus (REC), atask oriented corpus with psychophysiologicaldata registered and aligned with audiovisual da-ta.
In our opinion this corpus will allow to clear-ly identify emotions and, as a result, having aclearer idea of facial expression of emotions indialogue.
In fact, REC is created to shade lighton the relationship between cooperation andemotions in dialogues.
This resource is the first81up to now with audiovisual and psychophysio-logical data recorded together.2 The REC CorpusREC (Rovereto Emotive Corpus) is an audiovi-sual and psychophysiological corpus of dialo-gues elicited with a modified Map Task.
TheMap Task is a cooperative task involving twoparticipants.
It was used for the first time by theHCRC group at Edinburg University (Andersonet al, 1991).
In this task two speakers sit oppo-site one another and each of them has a map.They cannot see each other?s map because thethey are separated by a short barrier.
One speak-er, designated the Instruction Giver, has a routemarked on her map; the other speaker, the In-struction Follower, has no route.
The speakersare told that their goal is to reproduce the In-struction Giver's route on the Instruction Follow-er's map.
To the speakers are told explicitly thatthe maps are not identical at the beginning of thedialogue session.
However, it is up to them todiscover how the two maps differ.Our map task is modified with respect to theoriginal one.
In our Map Task the two participantsare sitting one in front of the other and areseparated by a short barrier or a full screen.
Theyboth have a map with some objects.
Some of themare in the same position and with the same name,but most of them are in different positions or havenames that sound similar to each other (e. g. MasoMichelini vs. Maso Nichelini, see Fig.
1).
Oneparticipant (the giver) must drive the otherparticipant (the follower) from a starting point(the bus station) to the finish (the Castle).Figure 1: Maps used in the recording of REC corpusGiver and follower are both native Italian speak-ers.
In the instructions it was told them that theywill have no more than 20 minutes to accomplishthe task.
The interaction has two conditions:screen and no screen.
In screen condition a barrierwas present between the two speakers.
In noscreen condition a short barrier, as in the originalmap task, was placed allowing giver and followerto see each other?s face.
With these two condi-tions we want to test whether seeing the speakersface during interactions influences facial emotiondisplay and cooperation (see Kendon, 1967; Ar-gyle and Cook 1976; for the relationship betweengaze/no gaze and facial displays; for the influenceof gaze on cooperation and coordination seeBrennan et al, 2008).
A further condition, emo-tion elicitation, was added.
In ?emotion?
condi-tion the follower or the giver can alternatively bea confederate, with the aim of getting the otherparticipant angry.
In this condition the psycho-physiological state of the confederate is not rec-orded.
In fact, as it is an acted behavior, it is notinteresting for research purpose.
All the partici-pants had given informed consent and the experi-mental protocol has been approved by the HumanResearch Ethics Committee of Trento University.REC is by now made up of 17 dyadic interac-tions, 9 with confederate, for a total of 204 min-utes of audiovisual and psychophysiological re-cordings (electrocardiogram and derived heartrate value, and skin conductance).
Our goal isreaching 12 recordings in the confederate condi-tion.
During each dialogue, the psychophysiologi-cal state of non-confederate giver or follower isrecorded and synchronized with video and audiorecordings.
So far, REC corpus is the only multi-modal corpus which has psychophysiological datato assess emotive states.The psychophysiological state of each partici-pant has been recorded with a BIOPAC MP150system.
In particular, Electrocardiogram (ECG)was recorded by Ag AgC1 surface electrodesfixed on participant?s wrists, low pass filter 100Hz, at a 200 samples/second rate.
Heart Rate(HR) has been automatic calculated as number ofheart beats per minute.
Galvanic Skin Conduc-tance (SK) was recorded with Ag AgC1 elec-trodes attached to the palmar surface of thesecond and third fingers of the non dominanthand, and recorded at a rate of200samples/second.
Artefacts due to hand move-ments have been removed with proper algorithms.Audiovisual interactions are recorded with 2 Ca-non Digital Cameras and 2 free field Sennheiserhalf-cardioid microphones with permanently pola-rized condenser, placed in front of each speaker82The recording procedure of REC is the follow-ing.
Before starting the task, we record baselinecondition that is to say we record participants?psychophysiological outputs for 5 minutes with-out challenging them.
Then the task started andwe recorded the psychophysiological outputs dur-ing the interaction which we called task condition.Then the confederate started challenging thespeaker with the aim of getting him/her angry.
Todo so, the confederate at minutes 4, 9 and 13 ofthe interaction plays a script (negative emotionelicitation in giver; Anderson et al, 2005):?You driving me in the wrong direction, try to bemore accurate!?;?
?It?s still wrong, this can?t be your best, tryharder!
So, again, from where you stop?;?
?You?re obviously not good enough in givinginstruction?.In Fig.
2 we show the results of a 1x5 ANOVAexecuted in confederate condition.
Heart rate(HR) is confronted over the five times of interest(baseline, task, after 4 minutes, after 9 minutes,after 13 minutes).
The times of interest are base-line, task, and after 4, 9 and 13 minutes, that is tosay just after emotion elicitation with the script.We find that HR is significantly different in thefive conditions, which means that the procedureto elicit emotions is incremental and allowsrecognition of different psychophysiologicalstates, which in turns are linked to emotive states.Mean HR values are in line with the ones showedby Anderson et al (2005).
Moreover, from theinspection of skin conductance values (Fig.
3)there is a linear increase of the number of peaksof conductance over time.
This can be due to twofactors: emotion elicitation but also an increasingof task difficulty leading to higher stress andtherefore to an increasing number of skinconductance peaks.As Cacioppo et al (2000) pointed out, it is notpossible to assess the emotion typology frompsychophysiological data alone.
In fact, HR andskin conductance are signals of arousal which inturns can be due both to high arousal emotionssuch as happiness or anger.
Therefore, we askedparticipants after the conclusion of the task toreport on a 8 points rank scale the valence of theemotions felt towards the interlocutor during thetask (from extremely positive to extremelynegative).
On 10 participants, 50% of them ratedthe experience as quite negative, 30% rated theexperience as almost negative, 10% ofparticipants rated it as negative and 10% asneutral.Figure 2: 1x5 ANOVA on heart rate (HR) over time inemotion elicitation condition in 9 partecipantsParticipants who have reported a neutral orpositive experience were discarded from thecorpus.Figure 3: Number of skin conductance positive peaksover time in emotion elicitation condition in 9 parteci-pants3 Annotation Method and Coding SchemeThe emotion annotation coding scheme used toanalyze our map task is quite far from the emotionannotation schemes proposed in ComputationalLinguistic literature.
Craggs and Woods (2005)proposed to annotate emotions with a schemewhere emotions are expressed at different blend-ing levels (i. e. blending of different emotion andemotive levels).
In Craggs and Woods opinions?annotators must label the given emotion with amain emotive term (e. g. anger, sadness, joy etc.
)correcting the emotional state with a score rang-ing from 1 (low) to 5 (very high).
Martin et al(2006) used a three steps rank scale of emotionvalence (positive, neutral and negative) to anno-tate their corpus recorded from TV interviews.TimeMeasure: MEASURE_162,413 ,704 60,790 64,03675,644 ,840 73,707 77,58293,407 ,916 91,295 95,519103,169 1,147 100,525 105,813115,319 1,368 112,165 118,473Time12345Mean Std.
Error Lower Bound Upper Bound95% Confidence IntervalPeaks/Time83But both these methods had quite poor results interms of annotation agreement among coders.Several studies on emotions have shown howemotional words and their connected conceptsinfluence emotion judgments and their labeling(for a review, see Feldman Barrett et al, 2007).Thus, labeling an emotive display (e. g. a voice ora face) with a single emotive term could be notthe best solution to recognize an emotion.
Moreo-ver researchers on emotion recognition from facedisplays find that some emotions as anger or fearare discriminated only by mouth or eyes configu-rations.
Face seems to be evolved to transmit or-thogonal signals, with a lower correlation eachother.
Then, these signals are deconstructed by the?human filtering functions?, i. e. the brain, as op-timized inputs (Smith et al, 2005).
The FacialAction Units (FACS, Ekman and Friesen, 1978) isa good scheme to annotate face expressions start-ing from movement of muscular units, called ac-tion units.
Even if accurate, it is a little problemat-ic to annotate facial expression, especially themouth ones, when the subject to be annotated isspeaking, as the muscular movements for speechproduction overlaps with the emotional configura-tion.On the basis of such findings, an ongoing de-bate is whether the perception of a face and, spe-cifically, of a face displaying emotions, is basedon holistic perception or perception of parts.
Al-though many efforts are ongoing in neuroscienceto determine the basis of emotion perception anddecoding, little is still known on how brains andcomputer might learn part of an object such as aface.
Most of the research in this field is based onPCA-alike algorithms which learn holistic repre-sentations.
On the contrary other methods such asnon Negative Matrix Factorization are based ononly positive constrains leading to part based ad-ditive representations.
Keeping this in mind, wedecide not to label emotions directly but toattribute valence and activation to nonverbal sig-nals, ?deconstructing?
them in simpler elements.These elements have implicit emotive dimen-sions, as for example mouth shape.
Thus, in ourcoding scheme a smile would be annotate as ?
)?and a large smile as ?+)?.
The latter means ahigher valence and arousal than the previous sig-nal, as when the speaker is laughing.In the following, we describe the modalitiesand the annotation features of our multimodalannotation scheme.
As an example, the analysis ofemotive labial movements implemented in ourannotation scheme is based on a little amount ofsigns similar to emoticons.
We sign two levelsof activation using the plus and minus signs.
So,annotation values for mouth shape are:?o open lips when the mouth is open;?- closed lips when the mouth is closed;? )
corners up e.g.
when smiling; +) opensmile;?
( corners down;  +( corners very down?1cornerup for asymmetric smile;?O protruded, when the lips are rounded.Similar signals are used to annotate eyebrowsshape.3.1    Cooperation AnalysisThe approach we have used to analyze coopera-tion in dialogue task is mainly based on BethanDavies model (Bethan Davies, 2006).
The basiccoded unit is the ?move?, which means individuallinguistic choices to successfully fulfill Map Task.The idea of evaluating utterance choices in rela-tion to task success can be traced back to Ander-son and Boyle (1994) who linked utterance choic-es to the accuracy of the route performed on themap.
Bethan Davies extended the meaning of?move?
to the goal evaluation, from a narrow setof indicators to a sort of data-driven set.
In partic-ular, Bethan Davies stressed some useful pointsfor the computation of collaboration between twocommunicative partners:?social needs of dialogue: there is a mini-mum ?effort?
needed to keep the conversa-tion going.
It includes minimal answers like?yes?
or ?no?
and feedbacks.
These briefutterances are classified by Bethan Davies(following Traum, 1994) as low effort, asthey do not require much planning to theoverall dialogue and to the joint task;?responsibility of supplying the needs of thecommunication partner: to keep an utter-ance going, one of the speakers can providefollow-ups which take more considerationof the partner?s intentions and goals in thetask performance.
This involves longer ut-terances, and of course a larger effort;?responsibility of maintaining a knowntrack of communication or starting a newone: there is an effort in considering the ac-tions of a speaker within the context of aparticular goal: that is, they mainly dealwith situations where a speaker is reactingto the instruction or question offered by theother participant, rather than moving thediscourse on another goal.
In fact the latter84is perceived as a great effort as it involvesreasoning about the task as a whole, besideplanning and producing a particular utter-ance.Following Traum (1994), speakers tend to engagein lower effort behaviors than higher ones.
Thus,if you do not answer to a question, theconversation will end, but you can choosewhether or not to query an instruction or offer asuggestion about what to do next.
This is reflectedin a weighting system where behaviors accountfor the effort invested and provides a basis for theempirical testing of dialogue principles.
The useof this system provides a positive and negativescore for each dialogue move.
We slightlysimplified the Bethan Davies?
weighting systemand propose a system giving positive and negativeweights in an ordinal scale from +2 to -2.
We alsoattribute a weight of 0 for actions which are in thearea of ?minimum social needs?
of dialogue.
InTable 1 we report some of the dialogue moves,called cooperation type, and the correspondingcooperation weighting level.
There is also adescription of different type of moves in terms ofGrice?s conversational rules breaking orfollowing.
Due to the nature of the map task,where giver and a follower have differentdialogue roles, we have two slightly differentversions of the cooperation annotation scheme.For example ?giving instruction?
is present onlywhen annotating the giver cooperation.
On theother hand ?feedback?
is present in bothannotation schemes.
Other communicativecollaboration indexes we codify in our codingscheme are the presence or absence of eye contactthrough gaze direction (to the interlocutor, to themap, unfocused), even in full screen condition,where the two speakers can?t see each other.Dialogue turns management (turn giving, turnoffering, turn taking, turn yielding, turnconcluding, and feedback) has been annotated aswell.
Video clips have been orthographicallytranscribed.
To do so, we adopted a subset of theconventions applied to the transcription of thespeech corpus of the LUNA project corpusannotation (see Rodriguez et al, 2007).3.2 Coding Procedure and Kappa ScoresUp to now we have annotated 9 emotive tokens ofan average length of 100 seconds each.
They havebeen annotated with the coding scheme previous-ly described by 6 annotators.
Our coding schemehas been implemented into ANVIL software(Kipp, 2001).
A Fleiss?
kappa statistic (Fleiss,1971) has been performed on the annotations.
Wechoose Fleiss?
kappa as it is the suitable statisticswhen chance agreement is calculated on morethan two coders.
In this case the agreement is ex-pected on the basis of a single distribution reflect-ing the combined judgments of all coders.Table 1: Computing cooperation in our coding scheme(from Bethan Davies, 2006 adapted)Thus, expected agreement is measured as theoverall proportion of items assigned to a categoryk by all coders n.Cooperation annotation for giver has a Fleiss?kappa score of 0.835 (p<0.001), while for follow-er cooperation annotation is 0.829 (p<0.001).Turn management has a Fleiss kappa score of0.784 (p<0.001).
As regard gaze, Fleiss kappascore is 0.788 (p<0.001).
Mouth shape annotationhas a Fleiss kappa score of 0.816 (p<0.001) andeyebrows shape annotation has a Fleiss kappa of0.855 (p<0.001).
In the last years a large debateon the interpretation of kappa scores has wide-spread.
There is a general lack of consensus onhow to interpret those values.
Some authors (All-wood et al, 2006) consider as reliable for multi-modal annotation kappa values between 0.67 and0.8.
Other authors accept as reliable only scoringrates over 0.8 (Krippendorff, 2004) to allow somegeneralizations.
What is clear is that it seems in-appropriate to propose a general cut off point,especially for multimodal annotation where verylittle literature on kappa agreement has been re-ported.
In this field it seems more necessary thatresearches report clearly the method they apply(e. g. the number of coders, if they code indepen-dently or not, if their coding relies only manual-ly).CooperationlevelCooperation type-2 No response to answer: breaks the maxims of quality,quantity and relevance-2 No information add when required: breaks the maxims ofquality, quantity and manner-2 No turn giving, no check: breaks the maxims of quality,quantity and relevance-1 Inappropriate reply (no giving info): breaks the maxims ofquantity and relevance0 Giving instruction: cooperation baseline, task demands1 Question answering y/n: applies the maxims of quality andrelevance1 Repeating instruction: applies the maxims of quantity andmanner2 Question answering y/n + adding info: applies the maximsof quantity, quality and relevance2 Checking the other understands (ci sei?
Capito?
): appliesthe maxims of quantity, quality and manner2 Spontaneous info/description adding: applies the maxims ofquantity, quality and manner85Our kappa scores are very high if comparedwith other multimodal annotation results.
This isbecause we analyze cooperation and emotion withan unambiguous coding scheme.
In particular, wedo not refer to emotive terms directly.
In factevery annotator has his/her own representation ofa particular emotion, which could be pretty differ-ent from the one of another coder.
This represen-tation will represent a problem especially for an-notation of blended emotions, which are ambi-guous and mixed by nature.
As some authors haveargued (Colletta et al, 2008) annotation of mentaland emotional states is a very demanding task.The analysis of non verbal features requires a dif-ferent approach if compared with other linguisticstasks as multimodal communication is multichan-nel (e.g.
audiovisual) and has multiple semanticlevels (e.g.
a facial expression can deeply modifythe sense of a sentence, such as in humor or iro-ny).The final goal of this research is performing alogistic regression on cooperation and emotiondisplay.
We will also investigate speakers?
role(giver or follower) and screen/no screen condi-tions role with respect to cooperation.
Our pre-dictions are that in case of full screen condition(i. e. the two speakers can?t see each other) thecooperation will be lower with respect to shortscreen condition (i. e. the two speakers can seeeach other?s face) while emotion display will bewider and more intense for full screen conditionwith respect to short barrier condition.
No predic-tions are made on the speaker role.4       Conclusions and Future DirectionsCooperative behavior and its relationship withemotions is a topic of great interest in the field ofdialogue annotation.
Usually emotions achieve alow agreement among raters (see Douglas-Cowieet al, 2005) and surprisingly emotion recognitionis higher in a condition of modality deprivation(only acoustic or only visual vs. bimodal).Neuroscience research on emotion shows thatemotion recognition is a process performed firstlyby sight, but the awareness of the emotion ex-pressed is mediated by the prefrontal cortex.Moreover a predefined set of emotion labels caninfluence the perception of facial expression.Therefore we decide to deconstruct each signalwithout attributing directly an emotive label.
Weconsider promising the implementation in compu-tational coding schemes of neuroscience evi-dences on transmitting and decoding of emotions.Further researches will implement an experimenton coders?
brain activation of to understand ifemotion recognition from face is a whole or a partbased process.ReferencesAllwood J., Cerrato L., Jokinen K., Navarretta C., andPaggio P. 2006.
A Coding Scheme for the Annota-tion of Feedback, Turn Management and Sequenc-ing Phenomena.
In Martin, J.-C., K?hnlein, P.,Paggio, P., Stiefelhagen, R., Pianesi, F.
(Eds.)
Mul-timodal Corpora: From Multimodal Behavior Theo-ries to Usable Models: 38-42.Anderson A., Bader M., Bard E., Boyle E., Doherty G.M., Garrod S., Isard S., Kowtko J., McAllister J.,Miller J., Sotillo C., Thompson H. S. and WeinertR.
1991.
The HCRC Map Task Corpus.
Languageand Speech, 34:351-366Anderson A. H., and Boyle E. A.
1994.
Forms of in-troduction in dialogues: Their discourse contextsand communicative consequences.
Language andCognitive Process , 9(1):101 - 122Anderson J. C., Linden W., and Habra M. E. 2005.
Theimportance of examining blood pressure reactivityand recovery in anger provocation research.
Interna-tional Journal of Psychophysiology 57(3): 159-163Argyle M. and Cook M. 1976 Gaze and mutual gaze,Cambridge: Cambridge University PressBethan Davies L. 2006.
Testing Dialogue Principles inTask-Oriented Dialogues: An Exploration of Coop-eration, Collaboration, Effort and Risk.
In Universi-ty of Leeds papersBrennan S. E., Chen X., Dickinson C. A., Neider M.A.
and Zelinsky J. C. 2008.
Coordinating cognition:The costs and benefits of shared gaze during colla-borative search.
Cognition 106(3): 1465-1477Ekman P. and Friesen WV.
1978.
FACS Facial ActionCodind Scheme.
A technique for the measurement offacial action, Palo Alto, CA: Consulting PressCarletta, J.
2007.
Unleashing the killer corpus: expe-riences in creating the multi-everything AMI Meet-ing Corpus, Language Resources and Evaluation,41: 181-190Colletta, J.-M., Kunene, R., Venouil, and A. Tcherkas-sof, A.
2008.
Double Level Analysis of the Multi-modal Expressions of Emotions in Human-machineInteraction.
In Martin, J.-C., Patrizia, P., Kipp, M.,Heylen, D., (Eds.)
Multimodal Corpora: From Mod-els of Natural Interaction to Systems and Applica-tions, 5-11Craggs R., and Wood M. 2004.
A Categorical Annota-tion Scheme for Emotion in the Linguistic Contentof Dialogue.
In Affective Dialogue Systems, Elsevi-er, 89-10086Douglas-Cowie E., Devillers L., Martin J.-C., Cowi R.,Savvidou S., Abrilian S., and Cox C. 2005.
Multi-modal Databases of Everyday Emotion: Facing upto Complexity.
In 9th European Conference onSpeech Communication and Technology (Inters-peech'2005) Lisbon, Portugal, September 4-8, 813-816Feldman Barrett L., Lindquist K. A., and Gendron M.2007.
Language as Context for the Perception ofEmotion.
Trends in Cognitive Sciences, 11(8): 327-332.Fleiss J. L. 1971.
Measuring Nominal Scale Agree-ment among Multiple Coders Psychological Bulletin11(4): 23-34.Goeleven E., De Raedt R., Leyman L., and Ver-schuere, B.
2008.
The Karolinska Directed Emo-tional Faces: A validation study, Cognition andEmotion, 22:1094 -1118Kendon A.
1967.
Some Functions of Gaze Directionsin Social Interaction, Acta Psychologica 26(1):1-47Kipp M., Neff M., and Albrecht I.
2006.
An Annota-tion Scheme for Conversational Gestures: How toeconomically capture timing and form.
In Martin,J.-C., K?hnlein, P., Paggio, P., Stiefelhagen, R.,Pianesi, F.
(Eds.)
Multimodal Corpora: From Mul-timodal Behavior Theories to Usable Models, 24-28Kipp M. 2001.
ANVIL - A Generic Annotation Toolfor Multimodal Dialogue.
In Eurospeech 2001Scandinavia 7th European Conference on SpeechCommunication and TechnologyKrippendorff K. 2004.
Reliability in content analysis:Some common misconceptions and recommenda-tions.
Human Communication Research, 30:411-433Magno Caldognetto E., Poggi I., Cosi P., Cavicchio F.and Merola G. 2004.
Multimodal Score: an AnvilBased Annotation Scheme for Multimodal Audio-Video Analysis.
In Martin, J.-C., Os, E.D.,K?hnlein, P., Boves, L., Paggio, P., Catizone, R.(eds.)
Proceedings of Workshop Multimodal Corpo-ra: Models Of Human Behavior For The Specifica-tion And Evaluation Of Multimodal Input And Out-put Interfaces.
29-33Martin J.-C., Caridakis G., Devillers L., Karpouzis K.and Abrilian S. 2006.
Manual Annotation and Au-tomatic Image Processing of Multimodal EmotionalBehaviors: Validating the Annotation of TV Inter-views.
In Fifth international conference on Lan-guage Resources and Evaluation (LREC 2006), Ge-noa, ItalyPianesi F., Leonardi C., and Zancanaro M. 2006.
Mul-timodal Annotated Corpora of Consensus DecisionMaking Meetings.
In Martin, J.-C., K?hnlein, P.,Paggio, P., Stiefelhagen, R., Pianesi, F.
(Eds.)
Mul-timodal Corpora: From Multimodal Behavior Theo-ries to Usable Models,  6--9Poggi I., 2007.
Mind, hands, face and body.
A goal andbelief view of multimodal communication, Berlin:Weidler BuchverlagReidsma D. Heylen D., and Op den Akker R. 2008.
Onthe Contextual Analysis of Agreement Scores.
InMartin, J.-C., Patrizia, P., Kipp, M., Heylen, D.,(Eds.)
Multimodal Corpora: From Models of Natu-ral Interaction to Systems and Applications, 52--55Rodr?guez K., Stefan K. J., Dipper S., G?tze M., Poe-sio M., Riccardi G., and Raymond C., and Wis-niewska J., 2007.
Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus.
In Proceed-ings of the Linguistic Annotation Workshop at theACL'07 (LAW-07), Prague, Czech Republic.Smith M. L., Cottrell G. W., Gosselin F., and SchynsP.
G. 2005.
Transmitting and Decoding Facial Ex-pressions.
Psychological Science 16(3):184-189Tassinary L. G. and Cacioppo J. T. 2000.
The skeleto-motor system: Surface electromyography.
In LGTassinary, GG Berntson, JT Cacioppo (eds) Hand-book of psychophysiology, New York: CambridgeUniversity Press, 263-299Traum D. R. 1994.
A Computational Theory ofGrounding in Natural Language Conversation, PhDDissertation.
urresearch.rochester.edu87
