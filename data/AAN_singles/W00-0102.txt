Using Long Runs as Predictors of Semantic Coherence in aPartial Document Retrieval SystemHyopil ShinComputing Research Laboratory, NMSUPO Box 30001Las Cruces, NM, 88003hshin@crl.nmsu.eduJerrold F. StachComputer Science Telecommunications, UMKC5100 Rockhill RoadKansas City, MO, 64110stach @cstp.umkc.eduAbstractWe propose a method for dealing withsemantic complexities occurring ininformation retrieval systems on the basis oflinguistic observations.
Our method followsfrom an analysis indicating that long runs ofcontent words appear in a stopped ocumentcluster, and our observation that these longruns predominately originate from theprepositional phrase and subjectcomplement positions and as such, may beuseful predictors of semantic coherence.From this linguistic basis, we test threestatistical hypotheses over a small collectionof documents from different genre.
Bycoordinating thesaurus emantic ategories(SEMCATs) of the long run words to thesemantic categories of paragraphs, weconclude that for paragraphs containing bothlong runs and short runs, the SEMCATweight of long runs of content words is astrong predictor of the semantic oherenceof the paragraph.IntroductionOne of the fundamental deficiencies of currentinformation retrieval methods is that the wordssearchers use to construct terms often are not thesame as those by which the searched informationhas been indexed.
There are two components othis problem, synonymy and polysemy(Deerwester et.
al., 1990).
By definition ofpolysemy, a document containing the searchterms or indexed with the search terms is notnecessarily relevant.
Polysemy contributesheavily to poor precision.
Attempts to deal withthe synonymy problem have relied onintellectual or automatic term expansion, or theconstruction of a thesaurus.Also the ambiguity of natural language causessemantic complexities that result in poorprecision.
Since queries are mostly formulatedas words or phrases in a language, and theexpressions of a language are ambiguous inmany cases, the system must have ways todisambiguate the query.In order to resolve semantic omplexities ininformation retrieval systems, we designed amethod to incorporate semantic information intocurrent IR systems.
Our method (1) adoptswidely used Semantic Information orCategories, (2) calculates Semantic Weightbased on probability, and (3) (for the purpose ofverifying the method) performs partial textretrieval based upon Semantic Weight orCoherence to overcome cognitive overload ofthe human agent.
We make two basicassumptions: 1.
Matching search terms tosemantic categories hould improve retrievalprecision.
2.
Long runs of content words have alinguistic basis for Semantic Weight and canalso be verified statistically.1 A Brief Overview of Previous ApproachesThere have been several attempts to deal withcomplexity using semantic information.
Thesemethods are hampered by the lack ofdictionaries containing proper semanticcategories for classifying text.
Semanticmethods designed by Boyd et.
al.
(1994) andWendlandt et.
al.
(1991) demonstrate onlysimple examples and are restricted to smallnumbers of words.
In order to overcome this6deficiency, we propose to incorporate thestructural information of the thesaurus, emanticcategories (SEMCATs).
However, we must alsoincorporate semantic ategories into current IRsystems in a compatible manner.
The problemwe deal with is partial text retrieval when all theterms of the traditional vector equations are notknown.
This is the case when retrieval isassociated with a near real time filter, or whenthe size or number of documents in a corpus isunknown.
In such cases we can retrieve onlypartial text, a paragraph or page.
But since thereis no document wide or corpus wide statistics, itis difficult to judge whether or not the textfragment is relevant.
The method we employ inthis paper identifies semantic "hot spots" inpartial text.
These "hot spots" are loci ofsemantic oherence in a paragraph of text.
Suchparagraphs are likely to convey the central ideasof the document.We also deal with the computational spectsof partial text retrieval.
We use a simplestop/stem ethod to expose long runs of contextwords that are evaluated relative to the searchterms.
Our goal is not to retrieve a highlyrelevant sentence, but rather to retrieve aportionof text that is semantically coherent with respectto the search terms.
This locale can be returnedto the searcher for evaluation and if it isrelevant, the search terms can be refined.
Thisapproach is compatible with Latent SemanticIndexing (LSI) for partial text retrieval when theterms of the vector space are not known.
LSI isbased on a vector space information retrievalmethod that has demonstrated improvedperformance over the traditional vector spacetechniques.
So when incorporating semanticinformation, it is necessary to adopt existingmathematical methods including probabilisticmethods and statistical methods.2 Theoretical Background2.1 Long RunsPartial Information Retrieval has to withdetection of main ideas.
Main ideas are topicsentences that have central meaning to the text.Our method of detecting main idea paragraphsextends from Jang (1997) who observed thatafter stemming and stopping a document, longruns of cor/tent words cluster.
Content word runsare a sequence of content words with a functionword(s) prefix and suffix.
These runs can beweighted for density in a stopped ocument andvector processed.
We observed that these longcontent word runs generally originate from theprepositional phrase and subject complementpositions, providing a linguistic basis for a denseneighbourhood of long runs of content wordssignalling a semantic locus of the writing.
Wesuppose that these neighbourhoods may containmain ideas of the text.
In order to verify this, wedesigned a methodology to incorporate semanticfeatures into information retrieval and examinedlong runs of content words as a semanticpredictor.We examined all the long runs of the Jang(1997) collection and discovered most of themoriginate from the prepositional phrase andsubject complement positions.
According toHalliday (1985), a preposition is explained as aminor verb.
It functions as a minor Predicatorhaving a nominal group as its complement.
Thusthe internal structure of 'across the lake' is likethat of 'crossing the lake', with a non-finiteverb as Predicator (thus our choice of _> 3 wordsas a long run).
When we interpret thepreposition as a "minor Predicator" and "minorProcess", we are interpreting the prepositionalphrase as a kind of minor clause.
That is,prepositional phrases function as a clause andtheir role is predication.Traditionally, predication is what a statementsays about its subject.
A named predicationcorresponds to an externally defined function,namely what the speaker intends to say his orher subject, i.e.
their referent.
If long runslargely appear in predication positions, it wouldsuggest hat the speaker is saying somethingimportant and the longer runs of content wordswould signal a locus of the speaker's intention.Extending from the statistical analysis of Jang(1997) and our observations of those long runsin the collection, we give a basic assumption ofour study:Long runs of content words containsignificant semantic information that aspeaker wants to express and focus,and thus are semantic indicators or locior main ideas.7In this paper, we examine the SEMCATvalues of long and short runs, extracted from arandom document of the collection in Jang(1997), to determine if the SEMCAT weights oflong runs of content words are semanticpredictors.2.2 SEMCATsWe adopted Roget's Thesaurus for our basicsemantic ategories (SEMCATs).
We extractedthe semantic categories from the onlineThesaurus for convenience.
We employ the 39intermediate categories as basic semanticinformation, since the 6 main categories are toogeneral, and the many sub-categories are toonarrow to be taken into account.
We refer tothese 39 categories as SEMCATs.Table 1: Semantic Categories (SEMCATs)Abbreviation1 AFIG2 ANT3 CAU4 CHN5 COIV6 'CRTH7 D IM8 EXIS9 EXOTIo FORM11 GINV12 INOM13 MECO14 MFRE15 MIG16 MOAF17 MOCO18 MOT19 NOIC2o NUM21 OPIG22 ORD23 ORGM24 PEAFFull DescriptionAffection in GeneralAntagonismCausationChangeConditional Intersocial VolitionCreative ThoughtDimensionsExistenceExtension of ThoughtFormGeneral Inter social VolitionInorganic MatterMeans of CommunicationMaterials for ReasoningMatter in generalMoral AffectionsModes of CommunicationMotionNature of Ideas CommunicatedNumberOperations of IntelligenceIn GeneralOrderOrganic MatterPersonal Affections2526PORE Possessive RelationsPRCO27 PRVO28 QUAN29 REAF3o RELN31 REOR32 REPR33 ROVO34 SIG35 SIVO36 SYAF37 TIME38 VOAC39 VOIGPrecursory Conditions andOperationsProspective VolitionQuantityReligious AffectionsRelationReasoning OrganizationReasoning ProcessResult of Voluntary ActionSpace in GeneralSpecial Inter social VolitionSympathetic AffectionsTimeVoluntary ActionVolition in General2.3 Indexing Space and Stop ListsMany of the most frequently occurring words inEnglish, such as "the," "of, .... and," "to," etc.
arenon-discriminators with respect o informationfiltering.
Since many of these function wordsmake up a large fraction of the text of mostdocuments, their early elimination in theindexing process speeds processing, savessignificant amounts of index space and does notcompromise the filtering process.
In the BrownCorpus, the frequency of stop words is 551,057out of 1,013,644 total words.
Function wordstherefore account for about 54.5% of the tokensin a document.The Brown Corpus is useful in text retrievalbecause it is small and efficiently exposescontent word runs.
Furthermore, minimizing thedocument token size is very important in NLP-based methods, because NLP-based methodsusually need much larger indexing spaces thanstatistical-based methods due to processes fortagging and parsing.3 Experimental BasisIn order to verify that long runs contribute toresolve semantic omplexities and can be usedas predictors of semantic intent, we employed aprobabilistic, vector processing methodology.3.1 Revised Probability and Vector ProcessingIn order to understand the calculation ofSEMCATs, it is helpful to look at the structure8of a preprocessed ocument.
One document"Barbie" in the Jang (1997) collection has a totalof 1,468 words comprised of 755 content wordsand 713 function words.
The document has 17paragraphs.
Filtering out function words usingthe Brown Corpus exposed the runs of contentwords as shown in Figure 1.Figurel: Preprocessed Text DocumentBARBIE * * * * FAVORITE  COMPANIONDETRACTORS LOVE * * * PLASTICPERFECTION * * FASHION DOLL * *IMPOSSIBLE FIGURE * LONG * * * POPULARGIRL * MATTEL * WORLD * TOYMAKER *PRODUCTS RANGE * FISHER PRICE INFANT *SALES * * * TALL MANNEQUIN * BARBIE * *AGE * * * BEST  SELL ING GIRLS  BRAND * *POISED * STRUT * * * CHANGE * * MALEDOMINATED WORLD * MULTIMEDIASOFTWARE * VIDEO GAMESIn Figure 1, asterisks occupy positions wherefunction words were filtered out.
The bold typeindicates the location of the longest runs ofcontent words.
The run length distribution ofFigure 1 is shown below:Table 2: Distribution of Content Run Lengths ina sample DocumentRun Length1Frequency112 83 24 2The traditional vector processing modelrequires the following set of terms:?
(dO the number of documents in thecollection that each word occurs in?
(idf) the inverse document frequency of eachword determined by logl0(N/df) where N isthe total number of documents.
If a wordappears in a query but not in a document, itsidf is undefined.?
The category probability of each queryword.Wendlandt (1991) points out that it is useful toretrieve a set of documents based upon keywords only, and then considers only thosedocuments for semantic category and attributeanalysis.
Wendlandt (1991) appends the scategory weights to the t term weights of eachdocument vector Di and the Query vector Q.Since our basic query unit is a paragraph,document frequencY (df) and inverse documentfrequency (idf) have to be redefined.
As wepointed out in Section 1, all terms are not knownin partial text retrieval.
Further, our approach isbased on semantic weight rather than wordfrequency.
Therefore any frequency basedmeasures defined by Boyd et al (1994) andWendlandt (1991) need to be built from theprobabilities of individual semantic categories.Those modifications are described below.
As asimplifying assumption, we assume SEMCATshave a uniform probability distribution withregard to a word.3.2 Calculating SEMCATsOur first task in computing SEMCAT valueswas to create a SEMCAT dictionary for ourmethod.
We extracted SEMCATs for everyword from the World Wide Web version ofRoget's thesaurus.
SEMCATs give probabilitiesof a word corresponding to a semantic ategory.The content word run 'favorite companiondetractors love' is of length 4.
Each word of therun maps to at least one SEMCAT.
The word'favorite' maps to categories 'PEAF and SYAF'.
'companion' maps to categories 'ANT, MECO,NUM, ORD, ORGM, PEAF, PRVO, QUAN,and SYAF'.
'detractor' maps to 'MOAF'.
'love'maps to 'AFIG, ANT, MECO, MOAF, MOCO,ORGM, PEAF, PORE, PRVO, SYAF, andVOIG'.
We treat the long runs as a semanticcore from which to calculate SEMCAT values.SEMCAT weights are calculated based on thefollowing equations.Eq.1 Pjk(Probability) - The likelihood ofSEMCAT Sj occurring due to the K thtrigger.
For example, assuming auniform probability distribution, thecategory PEAF triggered by the wordfavorite above, has the followingprobability:PPEAF, favorite ---- 0.5(1/2)Eq.2 Swj (SEMCAT Weights in Long runs)is the sum of each SEMCAT(j) weightof long runs based on their probabilities.In the above example, the long run9'favorite companion detractors love,' iheSEMCAT 'MOAF' has SWMoAv :(detractor(l) + love(.09)) = 1.09.
Wecan write;Swj= ?
POi=1Eq.3 edwj (Expected data weights in aparagraph) - Given a set of N contentwords (data) in a paragraph, theexpected weight of the SEMCATs oflong runs in a paragraph is:Nedwj = E Poi=1Eq.4 idwj (Inverse data weights in aparagraph) - The inverse data weight ofSEMCATs of long runs for a set of Ncontent words in a paragraph isidwj= loglo((e~wj\])Eq.5 Weight(Wj) - The weight of SEMCATSj in.a paragraph isWj = SwjxidwjEq.6 Relevance Weights (SemanticCoherence)W=?
W 0i=1Our method performs the following steps:1. calculate the SEMCAT weight of each longcontent word run in every paragraph (Sw)2. calculate the expected ata weight of eachparagraph (edw)3. calculate the inverse xpected ata weight ofeach paragraph (idw)4. calculate the actual weight of eachparagraph (Swxidw)5. calculate coherence weights (total relevance)by summing the weights of (Swxidw).In every paragraph, extraction of SEMCATsfrom long runs is done first.
The next step isfinding the same SEMCATs of long runsthrough every word in a paragraph (expecteddata weight), then calculate idw, and finallySw?idw.
The final, total relevance weights arean accumulation of all weights of SEMCATs ofcontent words in a paragraph.
Total relevancetells how many SEMCATs of the Query's longruns appear in a paragraph.
Higher values implythat the paragraph is relevant to the long runs ofthe Query.The following is a program output forcalculating SEMCAT weights for an arbitrarylong run: "SEVEN INTERACTIVEPRODUCTS LED"SEMCAT: EXOT Sw : 1.00 edw : 1.99 idw :1.44 Swxidw : 1.44SEMCAT: GINV Sw : 0.33 edw : 1.62 idw :1.53 Swxidw : 0.51SEMCAT: MOT Sw : 0.20 edw : 0.71 idw :1.89 Swxidw : 0.38SEMCAT: NUM Sw : 0.20 edw : 1.76 idw :1.49 Swxidw : 0.30SEMCAT: ORGM Sw : 0.20 edw : 1.67 idw :1.52 Sw?idw : 0.30SEMCAT: PEAF Sw : 0.53 edw : 1.50 idw :1.56 Swxidw : 0.83SEMCAT: REAF Sw : 0.20 edw : 0.20 idw :2.44 Swxidw : 0.49SEMCAT: SYAF Sw : 0.33 edw : 1.19 idw :1.66 Swxidw : 0.55Total (Swxidw) : 4.794 Experimental ResultsThe goal of employing probability and vectorprocessing is to prove the linguistic basis thatlong runs of content words can be used aspredictors of semantic intent But we also want toexploit the computational advantage ofremoving the function words from thedocument, which reduces the number of tokensprocessed by about 50% and thus reduces vectorspace and probability computations.
If it is truethat long runs of content words are predictors ofsemantic oherence, we can further reduce thecomplexity of vector computations: (1) byeliminating those paragraphs without long runsfrom consideration, (2) within remainingparagraphs with long runs, computing andsumming the semantic oherence of the longestruns only, (3) ranking the eligible paragraphs forretrieval based upon their semantic weightsrelative to the query.Jang (1997) established that the distributionof long runs of content words and short runs ofcontent words in a collection of paragraphs aredrawn from different populations.
This implies10that either long runs or short runs are predictors,but since all paragraphs contain short runs, i.e.
asingle content word separated by functionwords, only long runs can be useful predictors.Furthermore, only long runs as we define themcan be used as predictors because short runs areinsufficient to construct the language constructsfor prepositional phrase and subject complementpositions.
If short runs were discriminators, thelinguistic assumption of this research would beviolated.
The statistical analysis of Jang (1997)does not indicate this to be the case.To proceed in establishing the viability ofour approach, we proposed the followingexperimental hypotheses:(HI) The SEMCAT weights for long runsof content words are statistically greaterthan weights for short runs of contentwords.
Since each content word can mapto multiple SEMCATs, we cannotassume that the semantic weight of along run is a function of its length.
Thesemantic oherence of long runs shouldbe a more granular discriminator.
(H2) For paragraphs containing long runsand short runs, the distribution of longrun SEMCAT weights is statisticallydifferent from the distribution of shortrun SEMCAT weights.
(H3) There is a positive correlationbetween the sum of long run SEMCATweights and the semantic oherence of aparagraph, the total paragraph SEMCATweight.A detailed description of these experimentsand their outcome are described in Shin (1997,1999).
The results of the experiments and theimplications of those results relative to themethod we propose are discussed below.
Table 3gives the SEMCAT weights for seventeenparagraphs randomly chosen from one documentin the collection of Jang (1997).Table 3: SEMCAT Weights of 17 Paragraphs ChosenRandomly FromParagrapha Collection23Short Runs I Long RunsWeight Weight29.84 18.6031.29 12.8123.29 14.254 23.94 11.63L 5 34.63 35.00I 6 22.85 03.327 21.74 00.00I 8 35.84 15.94i9 30.15 00.00!
-!
I 0 13.40 00.00i 11 23.01 07.8212 31.69 04.7913 36.54 00.0014 17.91 10.5515 19.70 05.8316 17.11 00.0017 31.86 00.00The data was evaluated using a standard two wayF test and analysis of variance table with ct = .05.The analysis of variance table for the paragraphsin Table 3 is shown in Table 4.Table 4: Analysis of Vari~Variation DegreesofBetweenTreatments 1V R = 2904.51BetweenBlocks 16Vc = 1502.83Residual orRandom 16VE= 677.77Total 33V = 5085.11iance for Table 2 DataFreedomMeanSquare F2904.5193.9242.3668.562.21At the .05 significance level, Fa _ .o5 = 4.49 for1,16 degrees of freedom.
Since 68.56 > 4.49 wereject the assertion that column means (runweights) are equal in Table 2.
Long run andshort run weights come from different?
populations.
We accept HI.For the between paragraph treatment, herow means (paragraph weights) have an F valueof 2.21.
At the .05 significance level, F,~ = 05 =2.28 for 16,16 degrees of freedom.
Since 2.21 <2.28 we cannot reject the assertion that there isno significant difference in SEMCAT weightsbetween paragraphs.
That is, paragraph weightsdo not appear to be taken from differentpopulations, as do the long run and short runweight distributions.
Thus, the semantic weight11of the content words in a paragraph cannot beused to predict the semantic weight of theparagraph.
We therefore proceed to examine H2.Notice that two paragraphs in Table 2 arewithout long runs.
We need to repeat theanalysis of variance for only those paragraphswith long runs to see if long runs arediscriminators.
Table 5 summarizes thoseparagraphs.Table 5: SEMCAT weights of 11 paragraphscontaining long runs and short runsParagraph Short Runs Long RunsWeight Weight1 29.84 18.602 31.29 12.813 23.29 4.254 23.94 11.635 34.63 35.006 22.85 03.328 35.84 15.9411 23.01 07.8212 31.69 04.7914 17.91 10.5515 19.70 05.83This data was evaluated using a standard two wayF test and analysis of variance with cx = .05.
Theanalysis of variance table for the paragraphs inTable 5 follows.Table 6: AnalVariationBetweenTreatmentsV R = 1430.98BetweenBlocksV c = 944 .08Residual orRandomVF= 49.19TotalV = 2424.26sis of Variance for Table 5 DataDegrees Meanof SquareFreedom1 1430.9810 94.4010 4.9121F291.4419.22At the .05 significance l vel, F== .05 = 4.10 for2,10 degrees of freedom.
4.10 < 291.44.
At the.05 significance level, F= = .05 = 2.98 for 10,10degrees of freedom.
2.98 < 19.22.
Forparagraphs in a collection containing both longand short runs, the SEMCAT weights of thelong runs and short runs are drawn fromdifferent distributions.
We accept H2.For paragraphs containing long runs andshort runs, the distributions of long runSEMCAT weights is different from thedistribution of short run SEMCAT weights.
Weknow from the linguistic basis for long runs thatshort runs cannot be used as predictors.
Wetherefore proceed to examine the Pearsoncorrelation between the long run SEMCATweights and paragraph SEMCAT weights forthose paragraphs with both long and shortcontent word runs.Table 7: Correlation of Long Run SEMCATWeights to Paragraph SEMCAT WeightParagraph Long RunsSemanticWeight18.6012.81ParagraphSemanticWeight48.4444.103 4.25 27.544 11.63 35.575 35.00 69.636 03.32 26.178 15.94 51.7811 07.82 30.8312 04.79 31.6914 10.55 28.4615 05.83 25.53The weights in Table have a positive PearsonProduct Correlation coefficient of .952.
Wetherefore accept H3.
There is a positivecorrelation between the sum of long runSEMCAT weights and the semantic oherenceof a paragraph, the total paragraph SEMCATweight.5.
ConclusionThis research tested three statistical hypothesesextending from two observations: (1) Jang(1997) observed the clustering of long runs ofcontent words and established the distribution oflong run lengths and short run lengths are drawnfrom different populations, (2) our observationthat these long runs of content words originatefrom the prepositional phrase and subjectcomplement positions.
According to Halliday(1985) those grammar structures function as12minor predication and as such are loci ofsemantic intent or coherence.
In order tofacilitate the use of long runs as predictors, wemodified the traditional measures of Boyd et al(1994), Wendlandt (1991) to accommodatesemantic categories and partial text retrieval.The revised metrics and the computationalmethod we propose were used in the statisticalexperiments presented above.
The main findingsof this work are1.
the distribution semantic coherence(SEMCAT weights) of long runs is notstatistically greater than that of shortruns,2.
for paragraphs containing both long runsand short runs, the SEMCAT weightdistributions are drawn from differentpopulations3.
there is a positive correlation betweenthe sum of long run SEMCAT weightsand the total SEMCAT weight of theparagraph (its semantic oherence).Significant additional work is required tovalidate these preliminary results.
The collectionemployed in Jang (1997) is not a standardCorpus so we have no way to test precision andrelevance of the proposed method.
The results ofthe proposed method are subject o the accuracyof the stop lists and filtering function.Nonetheless, we feel the approach proposedhas potential to improve performance throughreduced token processing and increasedrelevance through consideration of semanticcoherence of long runs.
Significantly, ourapproach does not require knowledge of thecollection.Halliday M.A.K.
(1985) An Introduction toFunctional Grammar.
Edward Arnold, London.Jang S. (1997) Extracting Context from UnstructuredText Documents by Content Word Density.
M.S.Thesis, University of Missouri-Kansas City.Moffat A., Davis R., Wilkinson, R., and Zobel J.
(1994) Retrieval of Partial Documents.
InProceedings of TREC-2.Shin H. (1997) Incorporating Semantic Categories(SEMCATs) into a Partial Information RetrievalSystem.
M.S.
Thesis, University of Missouri-Kansas City.Shin H., Stach J.
(1999) Incorporating ProbabilisticSemantic Categories (SEMCATs) Into VectorSpace Techniques for Partial Document Retrieval.Journal of Computer Science and InformationManagement, vol.
2, No.
4, December 1999, toappear.Wendlandt E. and Driscoll R. (1991) Incorporating asemantic analysis into a document retrievalstrategy.
CACM 31, pp.
54-48.ReferencesBoyd R., Driscoll J, and Syu I.
(1994) IncorporatingSemantics Within a Connectionist Model and aVector Processing Model.
In Proceedings of theTREC-2, NIST.Deerwester S., Furnas G., Landauer T., andHarshman R. (1990) Indexing by Latent SemanticAnaysis.
Journal of the American Society ofInformation Science 41-6.13
