Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 809?816Manchester, August 2008Acquiring Sense Tagged Examples using Relevance FeedbackMark Stevenson, Yikun Guo and Robert GaizauskasDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield, S1 4DPUnited Kingdominital.surname@dcs.shef.ac.ukAbstractSupervised approaches to Word Sense Dis-ambiguation (WSD) have been shown tooutperform other approaches but are ham-pered by reliance on labeled training ex-amples (the data acquisition bottleneck).This paper presents a novel approach to theautomatic acquisition of labeled examplesfor WSD which makes use of the Informa-tion Retrieval technique of relevance feed-back.
This semi-supervised method gener-ates additional labeled examples based onexisting annotated data.
Our approach isapplied to a set of ambiguous terms frombiomedical journal articles and found tosignificantly improve the performance of astate-of-the-art WSD system.1 IntroductionThe resolution of lexical ambiguities has long beenconsidered an important part of the process ofunderstanding natural language.
Supervised ap-proaches to Word Sense Disambiguation (WSD)have been shown to perform better than unsuper-vised ones (Agirre and Edmonds, 2007) but requireexamples of ambiguous words used in context an-notated with the appropriate sense (labeled exam-ples).
However these often prove difficult to obtainsince manual sense annotation of text is a complexand time consuming process.
In fact, Ng (1997)estimated that 16 person years of manual effortwould be required to create enough labeled exam-ples to train a wide-coverage WSD system.
Thisc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.limitation is commonly referred to as the data ac-quisition bottleneck.
It is particularly acute in spe-cific domains, such as biomedicine, where termsmay have technical usages which only domain ex-perts are likely to be aware of.
For example, pos-sible meanings of the term ?ganglion?
in UMLS(Humphreys et al, 1998) include ?neural structure?or ?benign mucinous tumour?, although only thefirst meaning is listed in WordNet.
These domain-specific semantic distinctions make manual senseannotation all the more difficult.One approach to the data acquisition bottleneckis to generate labeled training examples automat-ically.
Others, such as Leacock et al (1998) andAgirre and Mart?
?nez (2004b), used informationfrom WordNet to construct queries which wereused to retrieve training examples.
This paperpresents a novel approach to this problem.
Rele-vance feedback, a technique used in InformationRetrieval (IR) to improve search results, is adaptedto identify further examples for each sense of am-biguous terms.
These examples are then usedto train a semi-supervised WSD system either bycombining them with existing annotated data orusing them alone.
The approach is applied to a setof ambiguous terms in biomedical texts, a domainfor which existing resources containing labeled ex-amples, such as the NLM-WSD data set (Weeberet al, 2001), are limited.The next section outlines previous techniqueswhich have been used to avoid the data acquisi-tion bottleneck.
Section 3 describes our approachbased on relevance feedback.
The WSD system weuse is described in Section 4.
Section 5 describesexperiments carried out to determine the useful-ness of the automatically retrieved examples.
Thefinal section summarises conclusions which can bedrawn from this work and outlines future work.8092 Previous ApproachesA variety of approaches to the data acquisition bot-tleneck have been proposed.
One is to use un-supervised algorithms, which do not require la-beled training data.
Examples include Lesk (1986)who disambiguated ambiguous words by examin-ing their dictionary definitions and selecting thesense whose definition overlapped most with def-initions of words in the ambiguous word?s con-text.
Leroy and Rindflesch (2005) presented anunsupervised approach to WSD in the biomedi-cal domain using information derived from UMLS(Humphreys et al, 1998).However, results from SemEval (Agirre et al,2007) and its predecessors have shown that su-pervised approaches to WSD generally outperformunsupervised ones.
It has also been shown that re-sults obtained from supervised methods improvewith access to additional labeled data for training(Ng, 1997).
Consequently various techniques forautomatically generating training data have beendeveloped.One approach makes use of the fact that differ-ent senses of ambiguous words often have differenttranslations (e.g.
Ng et al (2003)).
Parallel text isused as training data with the alternative transla-tions serving as sense labels.
However, disadvan-tages of this approach are that the alternative trans-lations do not always correspond to the sense dis-tinctions in the original language and parallel textis not always available.Another approach, developed by Leacock etal.
(1998) and extended by Agirre and Mart?
?nez(2004b), is to examine a lexical resource, Word-Net in both cases, to identify unambiguous termswhich are closely related to each of the senses of anambiguous term.
These ?monosemous relatives?are used to as query terms for a search engine andthe examples returned used as additional trainingdata.In the biomedical domain, Humphrey et al(2006) use journal descriptors to train modelsbased on the terms which are likely to co-occurwith each sense.
Liu et al (2002) used informa-tion in UMLS to disambiguate automatically re-trieved examples which were then used as labeledtraining data.
The meanings of 35 ambiguous ab-breviations were identified by examining the close-ness of concepts in the same abstract in UMLS.Widdows et al (2003) employ a similar approach,although their method also makes use of parallelcorpora when available.All of these approaches rely on the existence ofan external resource (e.g.
parallel text or a domainontology).
In this paper we present a novel ap-proach, inspired by the relevance feedback tech-nique used in IR, which automatically identifes ad-ditional training examples using existing labeleddata.3 Generating Examples using RelevanceFeedbackThe aim of relevance feedback is to generate im-proved search queries based on manual analysis ofa set of retrieved documents which has been shownto improve search precision (Salton, 1971; Robert-son and Spark Jones, 1976).
Variations of rele-vance feedback have been developed for a range ofIR models including Vector Space and probabilis-tic models.
The formulation of relevance feedbackfor the Vector Space Model is most pertinent to ourapproach.Given a collection of documents, C, containinga set of terms, Cterms, a basic premise of the Vec-tor Space Model is that documents and queries canbe represented by vectors whose dimensions repre-sent the Cterms.
Relevance feedback assumes thata retrieval system returns a set of documents, D,for some query, q.
It is also assumed that a userhas examined D and identified some of the docu-ments as relevant to q and others as not relevant.Relevant documents are denoted by D+qand theirrelevant as D?q, where D+q?
D, D?q?
Dand D+q?D?q= ?.
This information is used tocreate a modified query, qm, which should be moreaccurate than q.
A standard approach to construct-ing qmwas described by Rocchio (1971):qm= ?q+?|D+q|?
?d?D+qd ??|D?q|?
?d?D?qd (1)where the parameters ?, ?
and ?
are set for partic-ular applications.
Rocchio (1971) set ?
to 1.Our scenario is similar to the relevance feedbackproblem since the sense tagged examples provideinformation about the documents in which a par-ticular meaning of an ambiguous term is likely tobe found.
By identifying the features which dis-tinguish the documents containing one sense fromthe others we can create queries which can then beused to retrieve further examples of the ambiguouswords used in the same sense.
However, unlike810score(t, s) = idf(t)????|D+s|?
?d?D+scount(t, d)??|D?s|?
?d?D?scount(t, d)??
(2)the relevance feedback scenario there is no origi-nal query to modify.
Consequently we start witha query containing just the ambiguous term anduse relevance feedback to generate queries whichaim to retrieve documents where that term is beingused in a particular sense.The remainder of this section describes how thisapproach is applied in more detail.3.1 Corpus AnalysisThe first stage of our process is to analyse the la-beled examples and identify good search terms.For each sense of an ambiguous term, s, the la-beled examples are divided into two sets: thoseannotated with the sense in question and the re-mainder (annotated with another sense).
In rele-vance feedback terminology the documents anno-tated with the sense in question are considered tobe relevant and the remainder irrelevant.
These ex-amples are denoted by D+sand D?srespectively.At its core relevance feedback, as outlinedabove, aims to discover how accurately each termin the collection discriminates between relevantand irrelevant documents.
This approach was usedto inspire a technique for identifying terms whichare likely to indicate the sense in which an am-biguous word is being used.
We compute a singlescore for each term, reflecting its indicativeness ofthat sense, using the formula in equation 2, wherecount(t, d) is the number of times term t occurs indocument d and idf(t) is the inverse document fre-quency term weighting function commonly used inIR.
We compute idf as follows:idf(t) = log|C|df(t)(3)where D is the set of all annotated examples (i.e.D = D+s?
D?s) and df(t) the number of docu-ments in C which contain t.1In our experiments the ?
and ?
parameters inequation 2 are set to 1.
Documents are lemma-tised and stopwords removed before computingrelevance scores.1Our computation of idf(t) is based on only informationfrom the labeled examples, i.e.
we assume C = D+s?D?s.Alternatively idf could be computed over a larger corpus oflabeled and unlabeled examples.Table 1 shows the ten terms with the highestrelevance score for two senses of the term ?cul-ture?
in UMLS: ?laboratory culture?
(?In periph-eral blood mononuclear cell culture streptococcalerythrogenic toxins are able to stimulate trypto-phan degradation in humans?)
and ?anthropolog-ical culture?
(?The aim of this paper is to de-scribe the origins, initial steps and strategy, cur-rent progress and main accomplishments of intro-ducing a quality management culture within thehealthcare system in Poland.?).
?anthropological culture?
?laboratory culture?cultural 26.17 suggest 6.32recommendation 14.82 protein 6.13force 14.80 presence 5.86ethnic 14.79 demonstrate 5.86practice 14.76 analysis 5.78man 14.76 gene 5.58problem 13.04 compare 5.47assessment 12.94 level 5.36experience 11.60 response 5.35consider 11.58 data 5.35Table 1: Relevant terms for two senses of ?culture?3.2 Query GenerationUnlike the traditional formulation of relevancefeedback there is no initial query.
To create aquery designed to retrieve examples of each sensewe simply combine the ambiguous term and then terms with the highest relevance scores.
Wefound that using the three highest ranked termsprovided good results.
So, for example, the queriesgenerated for the two senses of culture shownin Table 1 would be ?culture culturalrecommendation force?
and ?culturesuggest protein presence?.3.3 Example CollectionThe next stage is to collect a set of examples usingthe generated queries.
We use the Entrez retrievalsystem (http://www.ncbi.nlm.nih.gov/sites/gquery) which provides an online in-terface for carrying out boolean queries over thePubMed database of biomedical journal abstracts.Agirre and Mart?
?nez (2004b) showed that it isimportant to preserve the bias of the original cor-pus when automatically retrieving examples and811consequently the number retrieved for each senseis kept in proportion to the original corpus.
Forexample, if our existing labeled examples contain75 usages of ?culture?
in the ?laboratoy culture?sense and 25 meaning ?anthropological culture?
wewould ensure that 75% of the examples returnedwould refer to the first sense and 25% to the sec-ond.Unsurprisingly, we found that the most usefulabstracts for a particular sense are the ones whichcontain more of the relevant terms identified usingthe process in Section 3.1.
However, if too manyterms are included Entrez may not return any ab-stracts.
To ensure that a sufficient number of ab-stracts are returned we implemented a process ofquery relaxation which begins by querying Entrezwith the most specific query for set of terms.
If thatquery matches enough abstracts these are retrievedand the search for labeled examples for the rele-vant sense considered complete.
However, if thatquery does not match enough abstracts it is relaxedand Entrez queried again.
This process is repeateduntil enough examples can be retrieved for a par-ticular sense.The process of relaxing queries is carried out asfollows.
Assume we have an ambiguous term, a,and a set of terms T identified using the processin Section 3.1.
The first, most specific query,is formed from the conjunction of all terms ina ?
T , i.e.
?a and t1AND t2AND ... t|T |?.This is referred to as the level |T | query.
Ifthis query does not return enough abstracts themore relaxed level |T | ?
1 query is formed.This query returns documents which include theambiguous word and all but one of the terms in T :?a AND ((t1AND t2AND ... AND tn?1) OR(t1AND t2AND ... tn?2AND tn) OR ... OR(t2AND t3... AND tn))?.
Similarly, level|T | ?
2 queries return documents containing theambiguous term and all but two of the termsin T .
Level 1 queries, the most relaxed, returndocuments containing the ambiguous term andone of the terms in T .
We do not use just theambiguous term as the query since this does notcontain any information which could discriminatebetween the possible meanings.
Figure 1 showsthe queries which are formed for the ambigu-ous term ?culture?
and the three most salientterms identified for the ?anthropological culture?sense.
The ?matches?
column lists the numberof PubMed abstracts the query matches.
It canbe seen that there are no matches for the level 3query and 83 for the more relaxed level 2 query.For this sense, abstracts returned by the level 2query would be used if 83 or fewer examples wererequired, otherwise abstracts returned by the level1 query would be used.Note that the queries submitted to Entrez are re-stricted so the terms only match against the titleand abstract of the PubMed articles.
This avoidsspurious matches against other parts of the recordsincluding metadata and authors?
names.4 WSD SystemThe basis of our WSD system was developed byAgirre and Mart?
?nez (2004a) and participated inthe Senseval-3 challenge (Mihalcea et al, 2004)with a performance which was close to the bestsystem for the English and Basque lexical sampletasks.
The system has been adapted to the biomed-ical domain (Stevenson et al, 2008) and has thebest reported results over the NLM-WSD corpus(Weeber et al, 2001), a standard data set for eval-uation of WSD algorithms in this domain.The system uses a wide range of features whichare commonly employed for WSD:Local collocations: A total of 41 features whichextensively describe the context of the ambiguousword and fall into two main types: (1) bigramsand trigrams containing the ambiguous word con-structed from lemmas, word forms or PoS tags,and (2) preceding/following lemma/word-form ofthe content words (adjective, adverb, noun andverb) in the same sentence with the target word.Syntactic Dependencies: This feature mod-els longer-distance dependencies of the ambiguouswords than can be represented by the local colloca-tions.
Five relations are extracted: object, subject,noun-modifier, preposition and sibling.
These areidentified using heuristic patterns and regular ex-pressions applied to PoS tag sequences around theambiguous word (Agirre and Mart?
?nez, 2004a).Salient bigrams: Salient bigrams within the ab-stract with high log-likelihood scores, as describedby Pedersen (2001).Unigrams: Lemmas of all content words(nouns, verbs, adjectives, adverbs) in the targetword?s sentence and, as a separate feature, lem-mas of all content words within a 4-word windowaround the target word, excluding those in a listof corpus-specific stopwords (e.g.
?ABSTRACT?,?CONCLUSION?).
In addition, the lemmas of any812Level Matches Query3 0 culture AND (cultural AND recommendation AND force)2 83 culture AND ((cultural AND recommendation) OR (cultural AND force) OR(recommendation AND force))1 6,358 culture AND (cultural OR recommendation OR force)Figure 1: Examples of various query levelsunigrams which appear at least twice in the en-tire corpus which are found in the abstract are alsoincluded as features.
This feature was not usedby Agirre and Mart?
?nez (2004a), but Joshi et al(2005) found them to be useful for this task.Features are combined using the Vector SpaceModel, a memory-based learning algorithm (seeAgirre and Mart?
?nez (2004a)).
Each occurrenceof an ambiguous word is represented as a binaryvector in which each position indicates the oc-currence/absence of a feature.
A single centroidvector is generated for each sense during training.These centroids are compared with the vectors thatrepresent new examples using the cosine metric tocompute similarity.
The sense assigned to a newexample is that of the closest centroid.5 Experiments5.1 SetupThe NLM-WSD corpus Weeber et al (2001) wasused for evaluation.
It contains 100 examples of 50ambiguous terms which occur frequently in MED-LINE.
Each example consists of the abstract froma biomedical journal article which contains an in-stance of the ambiguous terms which has beenmanually annotated with a UMLS concept.The 50 ambiguous terms which form the NLM-WSD data set represent a range of challenges forWSD systems.
Various researchers (Liu et al,2004; Leroy and Rindflesch, 2005; Joshi et al,2005; McInnes et al, 2007) chose to exclude someof the terms (generally those with highly skewedsense distributions or low inter-annotator agree-ment) and evaluated their systems against a subsetof the terms.
The number of terms in these subsetsrange between 9 and 28.
The Most Frequent Sense(MFS) heuristic has become a standard baseline inWSD (McCarthy et al, 2004) and is simply the ac-curacy which would be obtained by assigning themost common meaning of a term to all of its in-stances in a corpus.
The MFS for the whole NLM-WSD corpus is 78% and ranges between 69.9%and 54.9% for the various subsets.
We report re-sults across the NLM-WSD corpus and four sub-sets from the literature for completeness.The approach described in Section 3 was ap-plied to the NLM-WSD data set.
10-fold crossvalidation is used for all experiments.
Conse-quently 10 instances of each ambiguous term wereheld back for testing during each fold and addi-tional examples generated by examining the 90 re-maining instances.
Three sets of labeled exampleswere generated for each fold, containing 90, 180and 270 examples for each ambiguous term.
TheNLM-WSD corpus represents the only reliably la-beled data to which we have access and is used toevaluate all approaches (that is, systems trained oncombinations of the NLM-WSD corpus and/or theautomatically generated examples).5.2 ResultsVarious WSD systems were created.
The ?basic?system was trained using only the NLM-WSD dataset and was used as a benchmark.
Three systems,?+90?, ?+180?
and ?+270?
were trained using thecombination of the NLM-WSD data set and, re-spectively, the 90, 180 and 270 automatically re-trieved examples for each term.
A further threesystems, ?90?, ?180?
and ?270?
were trained us-ing only the automatically retrieved examples.The performance of our system is shown in Ta-ble 2.
The part of the table labeled ?Subsets prop-erties?
lists the number of terms in each subset ofthe NLM-WSD corpus and the relevant MFS base-line.Adding the first 90 automatically retrieved ex-amples (?+90?
column) significantly improves per-formance of our system from 87.2%, over allwords, to 88.5% (Wilcoxon Signed Ranks Test,p < 0.01).
Improvements are observed overall subsets of the NLM-WSD corpus.
Althoughthe improvements may seem modest they shouldbe understood in the context of the WSD systemwe are using which has exceeded previously re-ported performance figures and therefore repre-sents a high baseline.Table 2 also shows that adding more auto-matically retrieved examples (?+180?
and ?+270?columns) causes a drop in performance and re-813Subset Properties Combined New onlySubset Terms MFSbasic+90 +180 +270 90 180 270All words 50 78.0 87.2 88.5 87.0 86.1 85.6 84.5 82.7Joshi et.
al.
28 66.9 82.3 83.8 81.6 80.9 79.8 78.0 76.3Liu et.
al.
22 69.9 77.8 79.6 76.9 76.1 74.9 72.0 70.9Leroy 15 55.3 84.3 85.9 84.4 83.6 81.2 80.0 78.0McInnes et.
al.
9 54.9 79.6 81.8 80.4 79.4 75.2 73.0 71.4Table 2: Performance of system using a variety of combinations of training examplessults using these examples are worse than using theNLM-WSD corpus alone.
The query relaxationprocess, outlined in Section 3.3, uses less discrim-inating queries when more examples are requiredand it is likely that this is leading to noise in thetraining examples.The rightmost portion of Table 2 shows perfor-mance when the system is trained using only theautomatically generated examples which is con-sistently worse than using the NLM-WSD corpusalone.
Performance also decreases as more exam-ples are added.
However, results obtained usingonly the automatically generated training exam-ples are consistently better than the relevant base-line.Table 3 shows the performance of the sys-tem trained on the NLM-WSD data set comparedagainst training using only the 90 automaticallygenerated examples for each ambiguous term inthe NLM-WSD corpus.
It can be seen that thereis a wide variation between the performance ofthe additional examples compared with the origi-nal corpus.
For 11 terms training using the addi-tional examples alone is more effective than usingthe NLM-WSD corpus.
However, there are severalwords for which the performance using the auto-matically acquired examples is considerably worsethan using the NLM-WSD corpus.Information about the performance of a systemtrained using only the 90 automatically acquiredexamples can be used to boost WSD performancefurther.
In this scenario, which we refer to as ex-ample filtering, the system has a choice whetherto make use of the additional training data or not.For each word, performance of the WSD systemtrained using only the 90 automatically acquiredexamples is compared against the one trained onthe NLM-WSD data set (i.e.
results shown in Ta-ble 3).
If the performance is as good, or better,then the additional examples are used, otherwiseonly examples in the NLM-WSD corpus are usedas training data.
Since the annotated examples inthe NLM-WSD corpus have already been exam-ined to generate the additional examples, examplefiltering does not require any more labeled data.Results obtained when example filtering is usedare shown in Table 4.
The columns ?+90(f)?,?+180(f)?
and ?+270(f)?
show performance whenthe relevant set of examples is filtered.
(Notethat all three sets of examples are filtered againstthe performance of the first 90 examples, i.e.
re-sults shown in Table 3.)
This table shows thatexample filtering improves performance when theWSD system is trained using the automatically re-trieved examples.
Performance using the first 90filtered examples (?+90(f)?
column) is 89%, overall words, compared with 88.5% when filtering isnot used.
While performance decreases as largersets of examples are used, results using each of thethree sets of filtered examples is signifcantly bet-ter than the basic system (Wilcoxon Signed RanksTest, p < 0.01 for ?+90(f)?
and ?+180(f)?, p <0.05 for ?+270(f)?
).6 Conclusion and Future WorkThis paper has presented a novel approach to thedata acquisition bottleneck for WSD.
Our tech-nique is inspired by the relevance feedback tech-nique from IR.
This is a semi-supervised approachwhich generates labeled examples using availablesense annotated data and, unlike previously pub-lished approaches, does not rely on external re-sources such as parallel text or an ontology.
Eval-uation was carried out on a WSD task from thebiomedical domain for which the number of la-beled examples available for each ambiguous termis limited.
The automatically acquired examplesimprove the performance of a WSD system whichhas already been shown to exceed previously pub-lished results.The approach presented in this paper could beextended in several ways.
Our experiments focus814basic +90(f) +180(f) +270(f)All words 87.2 89.0 88.2 87.9Joshi et.
al.
82.3 84.6 83.5 83.3Liu et.
al.
84.3 86.6 85.7 85.5Leroy 77.8 80.3 79.1 78.5McInnes et.
al.
79.6 82.4 81.6 80.8Table 4: Performance using example filteringon the biomedical domain.
The relevance feedbackapproach could be applied to other lexical ambi-guities found in biomedical texts, such as abbre-viations with multiple expansions (e.g.
Liu et al(2002)), or to WSD of general text, possibly usingthe SemEval data for evaluation.Future work will explore alternative methods forgenerating query terms including other types ofrelevance feedback and lexical association mea-sures (e.g.
Chi-squared and mutual information).Experiments described here rely on a boolean IRengine (Entrez).
It is possible that an IR sys-tem which takes term weights into account couldlead to the retrieval of more useful MEDLINE ab-stracts.
Finally, it would be interesting to explorethe relation between query relaxation and the use-fulness of the retrieved abstracts.AcknowledgmentsThe authors are grateful to David Martinez for theuse of his WSD system for these experiments andto feedback provided by three anonymous review-ers.
This work was funded by the UK Engineer-ing and Physical Sciences Research Council, grantnumber EP/E004350/1.ReferencesE.
Agirre and P. Edmonds, editors.
2007.
WordSense Disambiguation: Algorithms and Applica-tions.
Text, Speech and Language Technology.Springer.E.
Agirre and D.
Mart??nez.
2004a.
The Basque Coun-try University system: English and Basque tasks.
InRada Mihalcea and Phil Edmonds, editors, Senseval-3: Third International Workshop on the Evaluationof Systems for the Semantic Analysis of Text, pages44?48, Barcelona, Spain, July.E.
Agirre and D.
Mart??nez.
2004b.
Unsupervised WSDbased on automatically retrieved examples: The im-portance of bias.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-04), Barcelona, Spain.E.
Agirre, L. Marquez, and R. Wicentowski, editors.2007.
SemEval 2007: Proceedings of the 4thInternational Workshop on Semantic Evaluations,Prague, Czech Republic.S.
Humphrey, W. Rogers, H. Kilicoglu, D. Demner-Fushman, and T. Rindflesch.
2006.
Word SenseDisambiguation by selecting the best semantic typebased on Journal Descriptor Indexing: Preliminaryexperiment.
Journal of the American Society for In-formation Science and Technology, 57(5):96?113.L.
Humphreys, D. Lindberg, H. Schoolman, andG.
Barnett.
1998.
The Unified Medical LanguageSystem: An Informatics Research Collaboration.Journal of the American Medical Informatics Asso-ciation, 1(5):1?11.M.
Joshi, T. Pedersen, and R. Maclin.
2005.
A Com-parative Study of Support Vector Machines Appliedto the Word Sense Disambiguation Problem for theMedical Domain.
In Proceedings of the Second In-dian Conference on Artificial Intelligence (IICAI-05), pages 3449?3468, Pune, India.C.
Leacock, M. Chodorow, and G. Miller.
1998.Using corpus statistics and WordNet relations forsense identification.
Computational Linguistics,24(1):147?165.G.
Leroy and T. Rindflesch.
2005.
Effects of Infor-mation and Machine Learning algorithms on WordSense Disambiguation with small datasets.
Interna-tional Journal of Medical Informatics, 74(7-8):573?585.M.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proceedings ofACM SIGDOC Conference, pages 24?26, Toronto,Canada.H.
Liu, S. Johnson, and C. Friedman.
2002.
AutomaticResolution of Ambiguous Terms Based on MachineLearning and Conceptual Relations in the UMLS.Journal of the American Medical Informatics Asso-ciation, 9(6):621?636.H.
Liu, V. Teller, and C. Friedman.
2004.
A Multi-aspect Comparison Study of Supervised Word SenseDisambiguation.
Journal of the American MedicalInformatics Association, 11(4):320?331.815word basic 90 ?adjustment 71 70 -1association 100 100 0blood pressure 48 50 2cold 88 86 -2condition 89 90 1culture 96 91 -5degree 96 86 -10depression 88 85 -3determination 87 82 -5discharge 95 92 -3energy 98 99 1evaluation 76 75 -1extraction 85 82 -3failure 66 71 5fat 85 83 -2fit 87 85 -2fluid 100 100 0frequency 95 94 -1ganglion 97 95 -2glucose 91 92 1growth 70 67 -3immunosuppression 79 79 0implantation 90 88 -2inhibition 98 98 0japanese 73 75 2lead 91 90 -1man 87 82 -5mole 95 84 -11mosaic 87 83 -4nutrition 53 43 -10pathology 85 85 0pressure 94 96 2radiation 84 82 -2reduction 89 90 1repair 87 86 -1resistance 98 97 -1scale 86 79 -7secretion 99 99 0sensitivity 93 91 -2sex 87 84 -3single 99 99 0strains 92 92 0support 86 89 3surgery 97 98 1transient 99 99 0transport 93 93 0ultrasound 87 85 -2variation 94 89 -5weight 77 77 0white 73 74 1Average 87.2 85.6 -1.58Table 3: Comparison of performance using orig-inal training data and 90 automatically generatedexamplesD.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2004.
Finding Predominant Senses in UntaggedText.
In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Lingusitics (ACL-2004), pages 280?287, Barcelona, Spain.B.
McInnes, T. Pedersen, and J. Carlis.
2007.
Us-ing UMLS Concept Unique Identifiers (CUIs) forWord Sense Disambiguation in the Biomedical Do-main.
In Proceedings of the Annual Symposiumof the American Medical Informatics Association,pages 533?537, Chicago, IL.R.
Mihalcea, T. Chklovski, and A. Kilgarriff.
2004.The Senseval-3 English lexical sample task.
InProceedings of Senseval-3: The Third InternationalWorkshop on the Evaluation of Systems for the Se-mantic Analysis of Text, Barcelona, Spain.H.
Ng, B. Wang, and S. Chan.
2003.
Exploiting Paral-lel Texts for Word Sense Disambiguation: an Empir-ical Study.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics(ACL-03), pages 455?462, Sapporo, Japan.H.
Ng.
1997.
Getting serious about Word Sense Dis-ambiguation.
In Proceedings of the SIGLEX Work-shop ?Tagging Text with Lexical Semantics: What,why and how?
?, pages 1?7, Washington, DC.T.
Pedersen.
2001.
A Decision Tree of Bigrams is anAccurate Predictor of Word Sense.
In Proceedingsof the Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL-01), pages 79?86, Pittsburgh, PA., June.S.
Robertson and K. Spark Jones.
1976.
Relevanceweighting of search terms.
Journal of the Ameri-can Society for Information Science and Technology,27(3):129?146.J.
Rocchio.
1971.
Relevance feedback in InformationRetrieval.
In G. Salton, editor, The SMART RetrievalSystem ?
Experiments in Automatic Document Pro-cessing.
Prentice Hall, Englewood Cliffs, NJ.G.
Salton.
1971.
The SMART Retrieval System ?
Ex-periments in Automatic Document Processing.
Pren-tice Hall Inc., Englewood Cliffs, NJ.M.
Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.2008.
Knowledge Sources for Word Sense Disam-biguation of Biomedical Text.
In Proceedings of theWorkshop on Current Trends in Biomedical NaturalLanguage Processing at ACL 2008, pages 80?87.M.
Weeber, J. Mork, and A. Aronson.
2001.
Devel-oping a Test Collection for Biomedical Word SenseDisambiguation.
In Proceedings of AMAI Sympo-sium, pages 746?50, Washington, DC.D.
Widdows, S. Peters, S. Cedernerg, C. Chan, D. Stef-fen, and P. Buitelaar.
2003.
Unsupervised Mono-lingual and Bilingual Word-sense Disambiguation ofMedical Documents using UMLS.
In Workshop on?Natural Langauge Processing in Biomedicine?
atACL 2003, pages 9?16, Sapporo, Japan.816
