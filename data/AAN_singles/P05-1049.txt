Proceedings of the 43rd Annual Meeting of the ACL, pages 395?402,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsWord Sense Disambiguation Using Label Propagation BasedSemi-Supervised LearningZheng-Yu Niu, Dong-Hong JiInstitute for Infocomm Research21 Heng Mui Keng Terrace119613 Singapore{zniu, dhji}@i2r.a-star.edu.sgChew Lim TanDepartment of Computer ScienceNational University of Singapore3 Science Drive 2117543 Singaporetancl@comp.nus.edu.sgAbstractShortage of manually sense-tagged data isan obstacle to supervised word sense dis-ambiguation methods.
In this paper we in-vestigate a label propagation based semi-supervised learning algorithm for WSD,which combines labeled and unlabeleddata in learning process to fully realizea global consistency assumption: simi-lar examples should have similar labels.Our experimental results on benchmarkcorpora indicate that it consistently out-performs SVM when only very few la-beled examples are available, and its per-formance is also better than monolingualbootstrapping, and comparable to bilin-gual bootstrapping.1 IntroductionIn this paper, we address the problem of word sensedisambiguation (WSD), which is to assign an appro-priate sense to an occurrence of a word in a givencontext.
Many methods have been proposed to dealwith this problem, including supervised learning al-gorithms (Leacock et al, 1998), semi-supervisedlearning algorithms (Yarowsky, 1995), and unsuper-vised learning algorithms (Schu?tze, 1998).Supervised sense disambiguation has been verysuccessful, but it requires a lot of manually sense-tagged data and can not utilize raw unannotated datathat can be cheaply acquired.
Fully unsupervisedmethods do not need the definition of senses andmanually sense-tagged data, but their sense cluster-ing results can not be directly used in many NLPtasks since there is no sense tag for each instance inclusters.
Considering both the availability of a largeamount of unlabelled data and direct use of wordsenses, semi-supervised learning methods have re-ceived great attention recently.Semi-supervised methods for WSD are character-ized in terms of exploiting unlabeled data in learningprocedure with the requirement of predefined senseinventory for target words.
They roughly fall intothree categories according to what is used for su-pervision in learning process: (1) using external re-sources, e.g., thesaurus or lexicons, to disambiguateword senses or automatically generate sense-taggedcorpus, (Lesk, 1986; Lin, 1997; McCarthy et al,2004; Seo et al, 2004; Yarowsky, 1992), (2) exploit-ing the differences between mapping of words tosenses in different languages by the use of bilingualcorpora (e.g.
parallel corpora or untagged monolin-gual corpora in two languages) (Brown et al, 1991;Dagan and Itai, 1994; Diab and Resnik, 2002; Li andLi, 2004; Ng et al, 2003), (3) bootstrapping sense-tagged seed examples to overcome the bottleneck ofacquisition of large sense-tagged data (Hearst, 1991;Karov and Edelman, 1998; Mihalcea, 2004; Park etal., 2000; Yarowsky, 1995).As a commonly used semi-supervised learningmethod for WSD, bootstrapping algorithm worksby iteratively classifying unlabeled examples andadding confidently classified examples into labeleddataset using a model learned from augmented la-beled dataset in previous iteration.
It can be foundthat the affinity information among unlabeled ex-amples is not fully explored in this bootstrappingprocess.
Bootstrapping is based on a local consis-tency assumption: examples close to labeled exam-ples within same class will have same labels, whichis also the assumption underlying many supervisedlearning algorithms, such as kNN.Recently a promising family of semi-supervisedlearning algorithms are introduced, which can ef-fectively combine unlabeled data with labeled data395in learning process by exploiting cluster structurein data (Belkin and Niyogi, 2002; Blum et al,2004; Chapelle et al, 1991; Szummer and Jaakkola,2001; Zhu and Ghahramani, 2002; Zhu et al, 2003).Here we investigate a label propagation based semi-supervised learning algorithm (LP algorithm) (Zhuand Ghahramani, 2002) for WSD, which works byrepresenting labeled and unlabeled examples as ver-tices in a connected graph, then iteratively propagat-ing label information from any vertex to nearby ver-tices through weighted edges, finally inferring thelabels of unlabeled examples after this propagationprocess converges.Compared with bootstrapping, LP algorithm isbased on a global consistency assumption.
Intu-itively, if there is at least one labeled example in eachcluster that consists of similar examples, then unla-beled examples will have the same labels as labeledexamples in the same cluster by propagating the la-bel information of any example to nearby examplesaccording to their proximity.This paper is organized as follows.
First, we willformulate WSD problem in the context of semi-supervised learning in section 2.
Then in section3 we will describe LP algorithm and discuss thedifference between a supervised learning algorithm(SVM), bootstrapping algorithm and LP algorithm.Section 4 will provide experimental results of LP al-gorithm on widely used benchmark corpora.
Finallywe will conclude our work and suggest possible im-provement in section 5.2 Problem SetupLet X = {xi}ni=1 be a set of contexts of occur-rences of an ambiguous word w, where xi repre-sents the context of the i-th occurrence, and n isthe total number of this word?s occurrences.
LetS = {sj}cj=1 denote the sense tag set of w. The firstl examples xg(1 ?
g ?
l) are labeled as yg (yg ?
S)and other u (l+u = n) examples xh(l+1 ?
h ?
n)are unlabeled.
The goal is to predict the sense of win context xh by the use of label information of xgand similarity information among examples in X .The cluster structure in X can be represented as aconnected graph, where each vertex corresponds toan example, and the edge between any two examplesxi and xj is weighted so that the closer the verticesin some distance measure, the larger the weight as-sociated with this edge.
The weights are defined asfollows: Wij = exp(?d2ij?2 ) if i 6= j and Wii = 0(1 ?
i, j ?
n), where dij is the distance (ex.
Euclid-ean distance) between xi and xj , and ?
is used tocontrol the weight Wij .3 Semi-supervised Learning Method3.1 Label Propagation AlgorithmIn LP algorithm (Zhu and Ghahramani, 2002), labelinformation of any vertex in a graph is propagatedto nearby vertices through weighted edges until aglobal stable stage is achieved.
Larger edge weightsallow labels to travel through easier.
Thus the closerthe examples, more likely they have similar labels(the global consistency assumption).In label propagation process, the soft label of eachinitial labeled example is clamped in each iterationto replenish label sources from these labeled data.Thus the labeled data act like sources to push out la-bels through unlabeled data.
With this push from la-beled examples, the class boundaries will be pushedthrough edges with large weights and settle in gapsalong edges with small weights.
If the data structurefits the classification goal, then LP algorithm can usethese unlabeled data to help learning classificationplane.Let Y 0 ?
Nn?c represent initial soft labels at-tached to vertices, where Y 0ij = 1 if yi is sj and 0otherwise.
Let Y 0L be the top l rows of Y 0 and Y 0Ube the remaining u rows.
Y 0L is consistent with thelabeling in labeled data, and the initialization of Y 0Ucan be arbitrary.Optimally we expect that the value of Wij acrossdifferent classes is as small as possible and the valueof Wij within same class is as large as possible.This will make label propagation to stay within sameclass.
In later experiments, we set ?
as the aver-age distance between labeled examples from differ-ent classes.Define n ?
n probability transition matrix Tij =P (j ?
i) = Wij?nk=1 Wkj, where Tij is the probabilityto jump from example xj to example xi.Compute the row-normalized matrix T by T ij =Tij/?nk=1 Tik.
This normalization is to maintainthe class probability interpretation of Y .396?2 ?1 0 1 2 3 4?2?1012?2 ?1 0 1 2 3?2?1012?2 ?1 0 1 2 3?2?1012?2 ?1 0 1 2 3?2?1012labeled +1unlabeledlabeled ?1(a) Dataset with Two?Moon Pattern (b) SVM(c) Bootstrapping (d) Ideal ClassificationA 8A 9B8B9A10B10A0B0Figure 1: Classification result on two-moon pattern dataset.
(a) Two-moon pattern dataset with two labeled points, (b) clas-sification result by SVM, (c) labeling procedure of bootstrap-ping algorithm, (d) ideal classification.Then LP algorithm is defined as follows:1.
Initially set t=0, where t is iteration index;2.
Propagate the label by Y t+1 = TY t;3.
Clamp labeled data by replacing the top l rowof Y t+1 with Y 0L .
Repeat from step 2 until Y t con-verges;4.
Assign xh(l + 1 ?
h ?
n) with a label sj?
,where j?
= argmaxjYhj .This algorithm has been shown to converge toa unique solution, which is Y?U = limt??
Y tU =(I ?
T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).We can see that this solution can be obtained with-out iteration and the initialization of Y 0U is not im-portant, since Y 0U does not affect the estimation ofY?U .
I is u ?
u identity matrix.
T uu and T ul areacquired by splitting matrix T after the l-th row andthe l-th column into 4 sub-matrices.3.2 Comparison between SVM, Bootstrappingand LPFor WSD, SVM is one of the state of the art super-vised learning algorithms (Mihalcea et al, 2004),while bootstrapping is one of the state of the artsemi-supervised learning algorithms (Li and Li,2004; Yarowsky, 1995).
For comparing LP withSVM and bootstrapping, let us consider a datasetwith two-moon pattern shown in Figure 1(a).
Theupper moon consists of 9 points, while the lowermoon consists of 13 points.
There is only one la-beled point in each moon, and other 20 points are un-?2 ?1 0 1 2 3?2?1012?2 ?1 0 1 2 3?2?1012?2 ?1 0 1 2 3?2?1012?2 ?1 0 1 2 3?2?1012?2 ?1 0 1 2 3?2?1012?2 ?1 0 1 2 3?2?1012(a) Minimum Spanning Tree (b) t=1(c) t=7 (d) t=10(e) t=12 (f) t=100BACFigure 2: Classification result of LP on two-moon patterndataset.
(a) Minimum spanning tree of this dataset.
The conver-gence process of LP algorithm with t varying from 1 to 100 isshown from (b) to (f).labeled.
The distance metric is Euclidian distance.We can see that the points in one moon should bemore similar to each other than the points across themoons.Figure 1(b) shows the classification result ofSVM.
Vertical line denotes classification hyper-plane, which has the maximum separating marginwith respect to the labeled points in two classes.
Wecan see that SVM does not work well when labeleddata can not reveal the structure (two moon pattern)in each class.
The reason is that the classificationhyperplane was learned only from labeled data.
Inother words, the coherent structure (two-moon pat-tern) in unlabeled data was not explored when infer-ring class boundary.Figure 1(c) shows bootstrapping procedure usingkNN (k=1) as base classifier with user-specified pa-rameter b = 1 (the number of added examples fromunlabeled data into classified data for each class ineach iteration).
Termination condition is that the dis-tance between labeled and unlabeled points is morethan inter-class distance (the distance between A0and B0).
Each arrow in Figure 1(c) representsone classification operation in each iteration for eachclass.
After eight iterations, A1 ?
A8 were tagged397as +1, and B1 ?
B8 were tagged as ?1, whileA9 ?
A10 and B9 ?
B10 were still untagged.
Thenat the ninth iteration, A9 was tagged as +1 since thelabel of A9 was determined only by labeled points inkNN model: A9 is closer to any point in {A0 ?
A8}than to any point in {B0 ?
B8}, regardless of theintrinsic structure in data: A9 ?
A10 and B9 ?
B10are closer to points in lower moon than to points inupper moon.
In other words, bootstrapping methoduses the unlabeled data under a local consistencybased strategy.
This is the reason that two points A9and A10 are misclassified (shown in Figure 1(c)).From above analysis we see that both SVM andbootstrapping are based on a local consistency as-sumption.Finally we ran LP on a connected graph-minimumspanning tree generated for this dataset, shown inFigure 2(a).
A, B, C represent three points, andthe edge A ?
B connects the two moons.
Figure2(b)- 2(f) shows the convergence process of LP witht increasing from 1 to 100.
When t = 1, label in-formation of labeled data was pushed to only nearbypoints.
After seven iteration steps (t = 7), point Bin upper moon was misclassified as ?1 since it firstreceived label information from point A through theedge connecting two moons.
After another three it-eration steps (t=10), this misclassified point was re-tagged as +1.
The reason of this self-correcting be-havior is that with the push of label information fromnearby points, the value of YB,+1 became higherthan YB,?1.
In other words, the weight of edgeB ?
C is larger than that of edge B ?
A, whichmakes it easier for +1 label of point C to travel topoint B.
Finally, when t ?
12 LP converged to afixed point, which achieved the ideal classificationresult.4 Experiments and Results4.1 Experiment DesignFor empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmarkcorpora - ?interest?, ?line?
1 and the data in Englishlexical sample task of SENSEVAL-3 (including all57 English words ) 2.1Available at http://www.d.umn.edu/?tpederse/data.html2Available at http://www.senseval.org/senseval3Table 1: The upper two tables summarize accuracies (aver-aged over 20 trials) and paired t-test results of SVM and LP onSENSEVAL-3 corpus with percentage of training set increasingfrom 1% to 100%.
The lower table lists the official result ofbaseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3.Percentage SVM LPcosine LPJS1% 24.9?2.7% 27.5?1.1% 28.1?1.1%10% 53.4?1.1% 54.4?1.2% 54.9?1.1%25% 62.3?0.7% 62.3?0.7% 63.3?0.9%50% 66.6?0.5% 65.7?0.5% 66.9?0.6%75% 68.7?0.4% 67.3?0.4% 68.7?0.3%100% 69.7% 68.4% 70.3%Percentage SVM vs. LPcosine SVM vs. LPJSp-value Sign.
p-value Sign.1% 8.7e-004 ?
8.5e-005 ?10% 1.9e-006 ?
1.0e-008 ?25% 9.2e-001 ?
3.0e-006 ?50% 1.9e-006 ?
6.2e-002 ?75% 7.4e-013 ?
7.1e-001 ?100% - - - -Systems Baseline htsa3 IRST-Kernels nuselsAccuracy 55.2% 72.9% 72.6% 72.4%We used three types of features to capture con-textual information: part-of-speech of neighboringwords with position information, unordered sin-gle words in topical context, and local collocations(as same as the feature set used in (Lee and Ng,2002) except that we did not use syntactic relations).For SVM, we did not perform feature selection onSENSEVAL-3 data since feature selection deterio-rates its performance (Lee and Ng, 2002).
Whenrunning LP on the three datasets, we removed thefeatures with occurrence frequency (counted in bothtraining set and test set) less than 3 times.We investigated two distance measures for LP: co-sine similarity and Jensen-Shannon (JS) divergence(Lin, 1991).For the three datasets, we constructed connectedgraphs following (Zhu et al, 2003): two instancesu, v will be connected by an edge if u is among v?sk nearest neighbors, or if v is among u?s k nearestneighbors as measured by cosine or JS distance mea-sure.
For ?interest?
and ?line?
corpora, k is 10 (fol-lowing (Zhu et al, 2003)), while for SENSEVAL-3data, k is 5 since the size of dataset for each wordin SENSEVAL-3 is much less than that of ?interest?and ?line?
datasets.3984.2 Experiment 1: LP vs. SVMIn this experiment, we evaluated LP and SVM3 on the data of English lexical sample task inSENSEVAL-3.
We used l examples from trainingset as labeled data, and the remaining training ex-amples and all the test examples as unlabeled data.For each labeled set size l, we performed 20 trials.In each trial, we randomly sampled l labeled exam-ples for each word from training set.
If any sensewas absent from the sampled labeled set, we redidthe sampling.
We conducted experiments with dif-ferent values of l, including 1%?Nw,train, 10%?Nw,train, 25%?Nw,train, 50%?Nw,train, 75%?Nw,train, 100%?Nw,train (Nw,train is the numberof examples in training set of word w).
SVM and LPwere evaluated using accuracy 4 (fine-grained score)on test set of SENSEVAL-3.We conducted paired t-test on the accuracy fig-ures for each value of l. Paired t-test is not run whenpercentage= 100%, since there is only one pairedaccuracy figure.
Paired t-test is usually used to esti-mate the difference in means between normal pop-ulations based on a set of random paired observa-tions.
{?, ?
}, {<, >}, and ?
correspond to p-value ?
0.01, (0.01, 0.05], and > 0.05 respectively.?
(or ?)
means that the performance of LP is sig-nificantly better (or significantly worse) than SVM.< (or >) means that the performance of LP is better(or worse) than SVM.
?means that the performanceof LP is almost as same as SVM.Table 1 reports the average accuracies and pairedt-test results of SVM and LP with different sizesof labled data.
It also lists the official results ofbaseline method and top 3 systems in ELS task ofSENSEVAL-3.From Table 1, we see that with small labeleddataset (percentage of labeled data ?
10%), LP per-forms significantly better than SVM.
When the per-centage of labeled data increases from 50% to 75%,the performance of LPJS and SVM become almostsame, while LPcosine performs significantly worsethan SVM.3we used linear SVM light, available athttp://svmlight.joachims.org/.4If there are multiple sense tags for an instance in trainingset or test set, then only the first tag is considered as correctanswer.
Furthermore, if the answer of the instance in test set is?U?, then this instance will be removed from test set.Table 2: Accuracies from (Li and Li, 2004) and average ac-curacies of LP with c ?
b labeled examples on ?interest?
and?line?
corpora.
Major is a baseline method in which they al-ways choose the most frequent sense.
MB-D denotes monolin-gual bootstrapping with decision list as base classifier, MB-Brepresents monolingual bootstrapping with ensemble of NaiveBayes as base classifier, and BB is bilingual bootstrapping withensemble of Naive Bayes as base classifier.Ambiguous Accuracies from (Li and Li, 2004)words Major MB-D MB-B BBinterest 54.6% 54.7% 69.3% 75.5%line 53.5% 55.6% 54.1% 62.7%Ambiguous Our resultswords #labeled examples LPcosine LPJSinterest 4?15=60 80.2?2.0% 79.8?2.0%line 6?15=90 60.3?4.5% 59.4?3.9%4.3 Experiment 2: LP vs. BootstrappingLi and Li (2004) used ?interest?
and ?line?
corporaas test data.
For the word ?interest?, they used itsfour major senses.
For comparison with their re-sults, we took reduced ?interest?
corpus (constructedby retaining four major senses) and complete ?line?corpus as evaluation data.
In their algorithm, c isthe number of senses of ambiguous word, and b(b = 15) is the number of examples added into clas-sified data for each class in each iteration of boot-strapping.
c ?
b can be considered as the size ofinitial labeled data in their bootstrapping algorithm.We ran LP with 20 trials on reduced ?interest?
cor-pus and complete ?line?
corpus.
In each trial, werandomly sampled b labeled examples for each senseof ?interest?
or ?line?
as labeled data.
The restserved as both unlabeled data and test data.Table 2 summarizes the average accuracies of LPon the two corpora.
It also lists the accuracies ofmonolingual bootstrapping algorithm (MB), bilin-gual bootstrapping algorithm (BB) on ?interest?
and?line?
corpora.
We can see that LP performs muchbetter than MB-D and MB-B on both ?interest?
and?line?
corpora, while the performance of LP is com-parable to BB on these two corpora.4.4 An Example: Word ?use?For investigating the reason for LP to outperformSVM and monolingual bootstrapping, we used thedata of word ?use?
in English lexical sample task ofSENSEVAL-3 as an example (totally 26 examplesin training set and 14 examples in test set).
For data399?0.4 ?0.2 0 0.2 0.4 0.6?0.500.5?0.4 ?0.2 0 0.2 0.4 0.6?0.500.5?0.4 ?0.2 0 0.2 0.4 0.6?0.500.5?0.4 ?0.2 0 0.2 0.4 0.6?0.500.5?0.4 ?0.2 0 0.2 0.4 0.6?0.500.5?0.4 ?0.2 0 0.2 0.4 0.6?0.500.5(a) Initial Setting (b) Ground?truth(c) SVM (d) Bootstrapping(e) Bootstrapping (f) LPB ACFigure 3: Comparison of sense disambiguation results be-tween SVM, monolingual bootstrapping and LP on word ?use?.
(a) only one labeled example for each sense of word ?use?as training data before sense disambiguation (?
and ?
denotethe unlabeled examples in SENSEVAL-3 training set and testset respectively, and other five symbols (+, ?, ?, ?, and ?
)represent the labeled examples with different sense tags sam-pled from SENSEVAL-3 training set.
), (b) ground-truth re-sult, (c) classification result on SENSEVAL-3 test set by SVM(accuracy= 314 = 21.4%), (d) classified data after bootstrap-ping, (e) classification result on SENSEVAL-3 training set andtest set by 1NN (accuracy= 614 = 42.9% ), (f) classifica-tion result on SENSEVAL-3 training set and test set by LP(accuracy= 1014 = 71.4% ).visualization, we conducted unsupervised nonlineardimensionality reduction5 on these 40 feature vec-tors with 210 dimensions.
Figure 3 (a) shows thedimensionality reduced vectors in two-dimensionalspace.
We randomly sampled only one labeled ex-ample for each sense of word ?use?
as labeled data.The remaining data in training set and test set servedas unlabeled data for bootstrapping and LP.
All ofthese three algorithms are evaluated using accuracyon test set.From Figure 3 (c) we can see that SVM misclassi-5We used Isomap to perform dimensionality reduction bycomputing two-dimensional, 39-nearest-neighbor-preservingembedding of 210-dimensional input.
Isomap is available athttp://isomap.stanford.edu/.fied many examples from class + into class ?
sinceusing only features occurring in training set can notreveal the intrinsic structure in full dataset.For comparison, we implemented monolingualbootstrapping with kNN (k=1) as base classifier.The parameter b is set as 1.
Only b unlabeled ex-amples nearest to labeled examples and with thedistance less than dinter?class (the minimum dis-tance between labeled examples with different sensetags) will be added into classified data in each itera-tion till no such unlabeled examples can be found.Firstly we ran this monolingual bootstrapping onthis dataset to augment initial labeled data.
The re-sulting classified data is shown in Figure 3 (d).
Thena 1NN model was learned on this classified data andwe used this model to perform classification on theremaining unlabeled data.
Figure 3 (e) reports thefinal classification result by this 1NN model.
We cansee that bootstrapping does not perform well since itis susceptible to small noise in dataset.
For example,in Figure 3 (d), the unlabeled example B 6 happenedto be closest to labeled example A, then 1NN modeltagged example B with label ?.
But the correct labelof B should be + as shown in Figure 3 (b).
Thiserror caused misclassification of other unlabeled ex-amples that should have label +.In LP, the label information of example C cantravel to B through unlabeled data.
Then example Awill compete with C and other unlabeled examplesaround B when determining the label of B.
In otherwords, the labels of unlabeled examples are deter-mined not only by nearby labeled examples, but alsoby nearby unlabeled examples.
Using this classifi-cation strategy achieves better performance than thelocal consistency based strategy adopted by SVMand bootstrapping.4.5 Experiment 3: LPcosine vs. LPJSTable 3 summarizes the performance comparisonbetween LPcosine and LPJS on three datasets.
Wecan see that on SENSEVAL-3 corpus, LPJS per-6In the two-dimensional space, example B is not the closestexample to A.
The reason is that: (1) A is not close to mostof nearby examples around B, and B is not close to most ofnearby examples around A; (2) we used Isomap to maximallypreserve the neighborhood information between any exampleand all other examples, which caused the loss of neighborhoodinformation between a few example pairs for obtaining a glob-ally optimal solution.400Table 3: Performance comparison between LPcosine andLPJS and the results of three model selection criteria are re-ported in following two tables.
In the lower table, < (or >)means that the average value of function H(Qcosine) is lower(or higher) than H(QJS), and it will result in selecting cosine(or JS) as distance measure.
Qcosine (or QJS) represents a ma-trix using cosine similarity (or JS divergence).
?
and ?
denotecorrect and wrong prediction results respectively, while ?meansthat any prediction is acceptable.LPcosine vs. LPJSData p-value SignificanceSENSEVAL-3 (1%) 1.1e-003 ?SENSEVAL-3 (10%) 8.9e-005 ?SENSEVAL-3 (25%) 9.0e-009 ?SENSEVAL-3 (50%) 3.2e-010 ?SENSEVAL-3 (75%) 7.7e-013 ?SENSEVAL-3 (100%) - -interest 3.3e-002 >line 8.1e-002 ?H(D) H(W ) H(YU )Data cos. vs. JS cos. vs. JS cos. vs. JSSENSEVAL-3 (1%) > (?)
> (?)
< (?
)SENSEVAL-3 (10%) < (?)
> (?)
< (?
)SENSEVAL-3 (25%) < (?)
> (?)
< (?
)SENSEVAL-3 (50%) > (?)
> (?)
> (?
)SENSEVAL-3 (75%) > (?)
> (?)
> (?
)SENSEVAL-3 (100%) < (?)
> (?)
< (?
)interest < (?)
> (?)
< (?
)line > (?)
> (?)
> (?
)forms significantly better than LPcosine, but theirperformance is almost comparable on ?interest?
and?line?
corpora.
This observation motivates us to au-tomatically select a distance measure that will boostthe performance of LP on a given dataset.Cross-validation on labeled data is not feasi-ble due to the setting of semi-supervised learning(l ?
u).
In (Zhu and Ghahramani, 2002; Zhu etal., 2003), they suggested a label entropy criterionH(YU ) for model selection, where Y is the labelmatrix learned by their semi-supervised algorithms.The intuition behind their method is that good para-meters should result in confident labeling.
Entropyon matrix W (H(W )) is a commonly used measurefor unsupervised feature selection (Dash and Liu,2000), which can be considered here.
Another pos-sible criterion for model selection is to measure theentropy of c ?
c inter-class distance matrix D cal-culated on labeled data (denoted as H(D)), whereDi,j represents the average distance between the i-th class and the j-th class.
We will investigate threecriteria, H(D), H(W ) and H(YU ), for model se-lection.
The distance measure can be automaticallyselected by minimizing the average value of functionH(D), H(W ) or H(YU ) over 20 trials.Let Q be the M ?N matrix.
Function H(Q) canmeasure the entropy of matrix Q, which is definedas (Dash and Liu, 2000):Si,j = exp (??
?Qi,j), (1)H(Q) = ?M?i=1N?j=1(Si,j logSi,j + (1?
Si,j) log (1?
Si,j)),(2)where ?
is positive constant.
The possible value of ?is?
ln 0.5I?
, where I?
=1MN?i,j Qi,j .
S is introducedfor normalization of matrix Q.
For SENSEVAL-3 data, we calculated an overall average score ofH(Q) by ?wNw,test?w Nw,testH(Qw).
Nw,test is thenumber of examples in test set of word w. H(D),H(W ) and H(YU ) can be obtained by replacing Qwith D, W and YU respectively.Table 3 reports the automatic prediction resultsof these three criteria.From Table 3, we can see that using H(W )can consistently select the optimal distance measurewhen the performance gap between LPcosine andLPJS is very large (denoted by?
or?).
But H(D)and H(YU ) fail to find the optimal distance measurewhen only very few labeled examples are available(percentage of labeled data ?
10%).H(W ) measures the separability of matrix W .Higher value of H(W ) means that distance mea-sure decreases the separability of examples in fulldataset.
Then the boundary between clusters is ob-scured, which makes it difficult for LP to locate thisboundary.
Therefore higher value of H(W ) resultsin worse performance of LP.When labeled dataset is small, the distances be-tween classes can not be reliably estimated, whichresults in unreliable indication of the separabilityof examples in full dataset.
This is the reason thatH(D) performs poorly on SENSEVAL-3 corpuswhen the percentage of labeled data is less than 25%.For H(YU ), small labeled dataset can not revealintrinsic structure in data, which may bias the esti-mation of YU .
Then labeling confidence (H(YU ))can not properly indicate the performance of LP.This may interpret the poor performance of H(YU )on SENSEVAL-3 data when percentage ?
25%.4015 ConclusionIn this paper we have investigated a label propaga-tion based semi-supervised learning algorithm forWSD, which fully realizes a global consistency as-sumption: similar examples should have similar la-bels.
In learning process, the labels of unlabeled ex-amples are determined not only by nearby labeledexamples, but also by nearby unlabeled examples.Compared with semi-supervised WSD methods inthe first and second categories, our corpus basedmethod does not need external resources, includ-ing WordNet, bilingual lexicon, aligned parallel cor-pora.
Our analysis and experimental results demon-strate the potential of this cluster assumption basedalgorithm.
It achieves better performance than SVMwhen only very few labeled examples are avail-able, and its performance is also better than mono-lingual bootstrapping and comparable to bilingualbootstrapping.
Finally we suggest an entropy basedmethod to automatically identify a distance measurethat can boost the performance of LP algorithm on agiven dataset.It has been shown that one sense per discourseproperty can improve the performance of bootstrap-ping algorithm (Li and Li, 2004; Yarowsky, 1995).This heuristics can be integrated into LP algorithmby setting weight Wi,j = 1 if the i-th and j-th in-stances are in the same discourse.In the future we may extend the evaluation of LPalgorithm and related cluster assumption based al-gorithms using more benchmark data for WSD.
An-other direction is to use feature clustering techniqueto deal with data sparseness and noisy feature prob-lem.Acknowledgements We would like to thankanonymous reviewers for their helpful comments.Z.Y.
Niu is supported by A*STAR Graduate Schol-arship.ReferencesBelkin, M., & Niyogi, P.. 2002.
Using Manifold Structure for Partially LabeledClassification.
NIPS 15.Blum, A., Lafferty, J., Rwebangira, R., & Reddy, R.. 2004.
Semi-SupervisedLearning Using Randomized Mincuts.
ICML-2004.Brown P., Stephen, D.P., Vincent, D.P., & Robert, Mercer.. 1991.
Word SenseDisambiguation Using Statistical Methods.
ACL-1991.Chapelle, O., Weston, J., & Scho?lkopf, B.
2002.
Cluster Kernels for Semi-supervised Learning.
NIPS 15.Dagan, I.
& Itai A.. 1994.
Word Sense Disambiguation Using A Second Lan-guage Monolingual Corpus.
Computational Linguistics, Vol.
20(4), pp.
563-596.Dash, M., & Liu, H.. 2000.
Feature Selection for Clustering.
PAKDD(pp.
110?121).Diab, M., & Resnik.
P.. 2002.
An Unsupervised Method for Word Sense TaggingUsing Parallel Corpora.
ACL-2002(pp.
255?262).Hearst, M.. 1991.
Noun Homograph Disambiguation using Local Context inLarge Text Corpora.
Proceedings of the 7th Annual Conference of the UWCentre for the New OED and Text Research: Using Corpora, 24:1, 1?41.Karov, Y.
& Edelman, S.. 1998.
Similarity-Based Word Sense Disambiguation.Computational Linguistics, 24(1): 41-59.Leacock, C., Miller, G.A.
& Chodorow, M.. 1998.
Using Corpus Statistics andWordNet Relations for Sense Identification.
Computational Linguistics, 24:1,147?165.Lee, Y.K.
& Ng, H.T.. 2002.
An Empirical Evaluation of Knowledge Sources andLearning Algorithms for Word Sense Disambiguation.
EMNLP-2002, (pp.41-48).Lesk M.. 1986.
Automated Word Sense Disambiguation Using Machine Read-able Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.
Pro-ceedings of the ACM SIGDOC Conference.Li, H. & Li, C.. 2004.
Word Translation Disambiguation Using Bilingual Boot-strapping.
Computational Linguistics, 30(1), 1-22.Lin, D.K.. 1997.
Using Syntactic Dependency as Local Context to Resolve WordSense Ambiguity.
ACL-1997.Lin, J.
1991.
Divergence Measures Based on the Shannon Entropy.
IEEE Trans-actions on Information Theory, 37:1, 145?150.McCarthy, D., Koeling, R., Weeds, J., & Carroll, J.. 2004.
Finding PredominantWord Senses in Untagged Text.
ACL-2004.Mihalcea R.. 2004.
Co-training and Self-training for Word Sense Disambigua-tion.
CoNLL-2004.Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004.
The SENSEVAL-3 EnglishLexical Sample Task.
SENSEVAL-2004.Ng, H.T., Wang, B., & Chan, Y.S.. 2003.
Exploiting Parallel Texts for WordSense Disambiguation: An Empirical Study.
ACL-2003, pp.
455-462.Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000.
Word Sense Disambiguation byLearning from Unlabeled Data.
ACL-2000.Schu?tze, H.. 1998.
Automatic Word Sense Discrimination.
Computational Lin-guistics, 24:1, 97?123.Seo, H.C., Chung, H.J., Rim, H.C., Myaeng.
S.H., & Kim, S.H.. 2004.
Unsu-pervised Word Sense Disambiguation Using WordNet Relatives.
Computer,Speech and Language, 18:3, 253?273.Szummer, M., & Jaakkola, T.. 2001.
Partially Labeled Classification with MarkovRandom Walks.
NIPS 14.Yarowsky, D.. 1995.
Unsupervised Word Sense Disambiguation Rivaling Super-vised Methods.
ACL-1995, pp.
189-196.Yarowsky, D.. 1992.
Word Sense Disambiguation Using Statistical Models ofRoget?s Categories Trained on Large Corpora.
COLING-1992, pp.
454-460.Zhu, X.
& Ghahramani, Z.. 2002.
Learning from Labeled and Unlabeled Datawith Label Propagation.
CMU CALD tech report CMU-CALD-02-107.Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003.
Semi-Supervised Learning UsingGaussian Fields and Harmonic Functions.
ICML-2003.402
