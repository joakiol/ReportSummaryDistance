Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 72?79,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsState-Transition Interpolation and MAP Adaptation for HMM-basedDysarthric Speech RecognitionHarsh Vardhan SharmaBeckman Institute405 North Mathews AvenueUrbana, IL 61801, USAhsharma@illinois.eduMark Hasegawa-JohnsonBeckman Institute405 North Mathews AvenueUrbana, IL 61801, USAjhasegaw@illinois.eduAbstractThis paper describes the results of our experi-ments in building speaker-adaptive recogniz-ers for talkers with spastic dysarthria.
Westudy two modifications ?
(a) MAP adapta-tion of speaker-independent systems trainedon normal speech and, (b) using a transitionprobability matrix that is a linear interpolationbetween fully ergodic and (exclusively) left-to-right structures, for both speaker-dependentand speaker-adapted systems.
The experi-ments indicate that (1) for speaker-dependentsystems, left-to-right HMMs have lower worderror rate than transition-interpolated HMMs,(2) adapting all parameters other than transi-tion probabilities results in the highest recog-nition accuracy compared to adapting anysubset of these parameters or adapting allparameters including transition probabilities,(3) performing both transition-interpolationand adaptation gives higher word error ratethan performing adaptation alone and, (4)dysarthria severity is not a sufficient indica-tor of the relative performance of speaker-dependent and speaker-adapted systems.1 IntroductionAfter more than two decades of research, speechrecognition is a well-established and reliablehuman-computer interaction technology.
The accu-racy of the newest generation of large vocabularyspeech recognizers, after adaptation to a user with-out speech pathology, is high enough to provide auseful human-computer interface especially for peo-ple who find it difficult to type with a keyboard.Automatic speech recognition (ASR) systemsgenerally assume that the speech signal is a realisa-tion of some message encoded as a sequence of oneor more symbols.
To effect the reverse operation ofrecognising the underlying symbol sequence given aspoken utterance, the continuous speech waveformis first converted to a sequence of equally spaceddiscrete parameter vectors.
The role of the recog-niser is to effect a mapping between sequences ofspeech vectors and the wanted underlying symbolsequences.
Most speech recognizers today are basedon the hidden Markov model (HMM) paradigm: it isassumed that the sequence of observed speech vec-tors is generated by a Markov model as shown inFig.
1.
A Markov model is a finite state machinewhich changes state once every time unit and eachtime t that a state j is entered, a speech vector ot isgenerated from the probability density bj(ot) whicha22 a33 a44a23 a34 a45a12a35a24a13HiddenMarkovModelObservationSequence1 2 3 4 5 6 7b2( 1) b2( 2) b3( 3) b3( 4)b4( 5) b4( 6) b4( 7)Figure 1: The Markov generation model.72is a mixture-Gaussian density for most standard sys-tems.
The transition from state i to state j is alsoprobabilistic and is governed by the discrete prob-ability aij .
Fig.
1 shows an example of this pro-cess where the five state model moves through thestate sequence X = 1, 2, 2, 3, 3, 4, 4, 4, 5 in order togenerate the sequence o1 to o7.
The entry and exitstates (1, 5) are non-emitting.
This is to facilitatethe construction of composite models: most systemsuse HMMs to perform modeling at the phone-levelrather than word-level; as such, word-level mod-els are constructed by stringing together phone-levelHMMs for the constituent phones.Fig.
2 shows how HMMs can be used for isolatedword recognition.
Firstly, an HMM is trained foreach vocabulary word using a number of examplesof that word ?
given a set of training examples cor-responding to a particular model, the parameters ofthat model ({aij} and {bj(ot)}) are determined by arobust and efficient re-estimation procedure.
In thisexample, the vocabulary consists of just three words:?one?, ?two?
and ?three?.
Secondly, to recognisesome unknown word, the likelihood (probability) ofeach model generating that word is calculated andthe most likely model identifies the word.EstimateModelsP( | 2)(a) TRAININGUnknown =Training Examples?one?123?two?
?three?1 2 3(b) RECOGNITIONP( | 1) P( | 3)Choose MaxFigure 2: Using HMMs for isolated word recognition.For creating a speech recognizer for a particularspeaker, there are two approaches: one is to createa speaker-dependent (SD) system by utlizing speechof that speaker alone to train the HMMs; the otheris to create a speaker-adapted (SA) system by firsttraining the HMMs in a speaker-independent fashionby utlizing speech of several speakers, and then cus-tomising the HMMs to the characteristics of the par-ticular speaker by using training examples of theirspeech to modify the HMM parameters.
The param-eter values do not get overwritten; they are adjustedusing a regularized or constrained machine learningalgorithm.
Regularization (e.g., using Maximum APosteriori learning) or constraints (e.g., using lin-ear transformations) allow the SA model to use farmore trainable parameters per minute of trainingdata without over-training the system.Despite the advances in speech technology, theirbenefits have not been available to people with grossmotor impairments mainly because these impair-ments include a component of dysarthria ?
a groupof motor speech disorders resulting from disturbedmuscular control of the speech mechanism due todamage of the peripheral or central nervous system.Dysarthria is often a symptom of a gross motor dis-order, whose other symptoms usually make it hardto use a keyboard and mouse.
Published case stud-ies have shown that some dysarthric users may findit easier to use an ASR system instead of a key-board (Carlson and Bernstein, 1987; Coleman andMeyers, 1991; Deller et al, 1988; Deller et al,1991; Fried-Oken, 1985).
Polur and Miller stud-ied the development of HMM-based small vocabu-lary (eight repetitions each of ten digits and fifteen?command?
words in English) SD systems for threemale subjects subjectively classified by a trainedclinician as moderately dysarthric (Polur and Miller,2005a; Polur and Miller, 2005b).
They found thatan ergodic HMM with a slight left-to-right character(called a transition-interpolated HMM from hereon)provides higher word recognition accuracy (WRA)than a standard left-to-right HMM, apparently be-cause the transition-interpolated HMM is able tocapture outlier events as a backward or nonlinearprogress through the intended word.
The benefitof using ergodic modeling over left-to-right mod-eling in distorted speech applications with disrup-tion events, pause events, and limited training datahas also been noted earlier by Deller, Hsu and Fer-rier (Deller et al, 1991).
Section 2.1.2 explains73in more detail the difference between these HMMtopologies.Speaking for long periods of time is tiring, espe-cially for a person with dysarthria, therefore it is dif-ficult for a person with dysarthria to train a speaker-dependent ASR.
Speaker adaptation then seems auseful method to overcome this obstacle in devel-oping dysarthric speech recognizers.
Raghavendraet al (Raghavendra et al, 2001) have comparedrecognition accuracies of an SA system and an SDsystem.
They found that the SA system adaptedwell to the speech of talkers with mild or moder-ate dysarthria, but the recognition scores were lowerthan for an unimpaired speaker.
The subject withsevere dysarthria was able to achieve better perfor-mance with the SD system than with the SA sys-tem.
These findings were also supported by Rudz-icz (Rudzicz, 2007) who compared the performanceof SD and ?SA?
systems on the Nemours database(Menendez-Pidal et al, 1996) by varying indepen-dently the amount of data for training and the num-ber of Gaussian components used for modeling theoutput probability distributions.
The ?SA?
techniqueimplemented is not speaker-adaptation in the con-ventional sense: it uses the parameter values for thespeaker-independent system as the starting point totrain HMMs for a particular dysarthric speaker.
Ina training algorithm without regularization or con-straint terms, it is possible for a system of this typeto over-train, resulting in loss of accuracy on testdata from the same speaker, and Rudzicz?s resultssuggest that such over-training may have occurredin some cases.
He further concluded that there wasnot enough data in the database to represent intra-speaker variation.The study described in this paper investigated thedevelopment of medium vocabulary HMM recog-nizers for dysarthric speech of various degrees ofseverity with the following aims: (1) to test the per-formance of SA systems relative to SD systems, forvarious degrees of dysarthria severity, (2) to test theperformance of an SD system employing transition-interpolated HMMs relative to an SD system usingstrictly left-to-right HMMs, (3) to test the perfor-mance of an SA system with transition-interpolatedHMMs relative to an SD system having strictly left-to-right HMMs and, (4) to see if the results in theabove three cases are essentially a function of thetalker?s dysarthria severity.2 Experimental Setup2.1 Modifications investigatedThe following modifications to the HMM structurewere studied in our experiments:2.1.1 AdaptationAll SA systems were developed by adapting aspeaker-independent system in a Maximum A Pos-teriori (MAP) manner, as outlined by Gauvain andLee (Gauvain and Lee, 1991; Gauvain and Lee,1992).
MAP adaptation involves the use of priorknowledge about the model parameter distribution.Hence, if we know what the parameters of the modelare likely to be (before observing any adaptationdata) using the prior knowledge, we might well beable to make good use of the limited adaptation data,to obtain a decent MAP estimate.
For MAP adapta-tion purposes, the informative priors that are gener-ally used are the speaker independent model param-eters (empirical Bayes approach).
In (Gauvain andLee, 1991), they derive expressions of MAP esti-mates for all HMM parameters except the transitionprobabilities (Gaussian mixture-component means,diagonal Gaussian mixture-component covariancematrices and, mixture-component weights) and alsoprovide an initialization scheme for the prior den-sity of these parameters.
In (Gauvain and Lee,1992), they derive expressions for MAP estimates oftransition probabilities in addition to those for full-covariance Gaussian mixture-component parame-ters, and provide a MAP variant of the Expectation-Maximization (EM) re-estimation algorithm.
Allsystems developed in our study modeled the obser-vations as mixture of Gaussians with diagonal co-variance matrices.2.1.2 Transition-InterpolationFig.
3 illustrates the topologies of strictly left-to-right (LR) and transition-interpolated (TI) HMMswith 3 emitting states.
If A = {aij} be the N ?N transition probability matrix for an N-state HMM,then we have for an LR HMM: for each state i,0 < aii , ai,i+1 < 1; aii + ai,i+1 = 1 and aij =0 for j 6= i, i + 1.
In other words, each emittingstate has only two possible state-transitions: giventhe current state, the HMM either remains in the74same state or moves into the succeeding state; it willnot jump over states or go to a preceding state.StrictlyLeft-to-RightHMMa22 a33 a44a23 a34 a45a12Transition-InterpolatedHMMa22 a33 a44a23 a34 a45a12?32 ?43?24?42Figure 3: Difference between strictly left-to-right andtransition-interpolated HMM topologies.The TI model is an LR model which has non-zerotransition probabilties for jumps and transitions topreceding states from a particular state (for emit-ting states).
These probabilties are however smallcompared to self-transition and next-state?transitionprobabilties.
A TI HMM is initialized as follows: foreach emitting state i, aij =  for j 6= i, i+1 where0 <  << 1; aii , ai,i+1 >>  and?Nj=1 aij =1.
After this initialization, the transition probabil-ity matrix is re-estimated for speaker-dependent sys-tems using the standard Maximum Likelihood EMalgorithm, and for speaker-adapted systems usingthe MAP variant of the EM algorithm.2.2 Data usedThe experiments described in this paper utilizedspeech of 7 speakers from the UA-Speech database(Kim et al, 2008).
This corpus was constructed withthe aim of developing large-vocabulary dysarthricASR systems which would allow users to enter un-limited text into a computer.
All speakers exhib-ited symptoms of spastic dysarthria, according to aninformal evaluation by a certified speech-languagepathologist.
Each speaker recorded 765 isolatedwords in 3 blocks of 255 words each; (a) commonto all blocks: 10 digits (D), 19 computer commands(C), 26 radio alphabet letters (L), and 100 commonwords (CW) selected from the Brown corpus of writ-ten English; and (b) unique to each block: 100 un-common words (UW) selected from children?s nov-els digitized by Project Gutenberg.
Vocabularies Dand CW were primarily composed of monosylla-bles, C and L of bisyllables, and UW of polysyl-labic words.
The speakers?
speech was affected bydysarthria associated with cerebral palsy.
Data ac-quisition and intelligibility assessment is describedin more detail in (Kim et al, 2008).
Two hundreddistinct words were selected from the recording ofthe second block: 10 digits, 25 radio alphabet letters,19 computer commands and, 73 words randomly se-lected from each of the CW and UW categories.
Fivenaive listeners were recruited for each speaker andwere instructed to provide orthographic transcrip-tions of each word that they thought the speakersaid.
The percentage of correct responses was thenaveraged across five listeners to obtain each speak-ers intelligibility.
Table 1 lists the speakers whosespeech materials from the UA-Speech database wereused, along with their human listener intelligibilityratings.
The first letter of the speaker code (?M?
or?F?)
indicates their gender.Speaker Age Speech Intelligbility (%)M09 18 high (86%)M05 21 mid (58%)M06 18 low (39%)F02 30 low (29%)M07 58 low (28%)F03 51 very low (6%)M04 >18 very low (2%)Table 1: Summary of Speaker Information (in decreasingorder of human listener intelligibility rating).For building the ?MAP prior?
speaker-independent system, the unadapted HMMs weretrained on speech from the TIMIT corpus (Garofoloet al, 1993).2.3 System ConfigurationsTable 2 lists the characteristics of the various sys-tem configurations that were studied: SD standsfor speaker-dependent, SA for speaker-adapted;LR implies use of strictly left-to-right HMMs, TIfor transition-interpolated HMMs; ?m?,?v?,?w?,?t?respectively denote means, variances, mixture-75component weights and transition probabilities.These systems were developed for each of the sevenSystem (Type) HMM Parameters adaptedC00 (SD) LR ?C01 (SD) TI ?C11 (SA) LR mC12 (SA) LR m,vC13 (SA) LR m,v,wC14 (SA) LR m,v,w,tC15 (SA) TI m,v,w,tTable 2: Summary of ASR System Configurationsspeakers listed in Table 1, and employed word-internal, context-dependent triphone HMMs, withthree hidden states and observations modeled asmixture-of-Gaussians.
Configuration C00 was de-veloped by Sharma and Hasegawa-Johnson (2009)and is the baseline configuration for the present ex-periments.
For configurations C11 through C15,the speaker-independent systems trained on TIMITemployed left-to-right HMMs.
For systems C15,the transition-interpolation was performed after ob-taining the speaker-independent TIMIT-trained left-to-right HMMs and before adaptation to the UA-Speech speaker?s data: the original non-zero entriesin the transition probability matrices were scaleddown so that the sum of each row was unity afterchanging the zero-entries to .
For each speaker, allof blocks 1 and 3 were used as training data (sys-tems C00, C01) or adaptation data (systems C11-C15) and all of block 2 was used for testing.
Thespeaker-independent system was trained on all ofTIMIT?s training data and was tested on speech of32 randomly chosen speakers from its test data.The features extracted from the speech waveformcomprised of 12 Perceptual Linear Prediction co-efficients (Hermansky, 1990) for 25 ms Hamming-windowed segments obtained every 10 ms, plus theenergy of the windowed segment.
?Velocity?
and?Acceleration?
components were also calculated forthis 13-dimensional feature, which finally resultedin a 39-dimensional acoustic feature vector.The measure used for assessing the performanceof the developed recognizers is the fraction of task?vocabulary words correctly recognized (in percent),defined in Equation 1.PWC =# words correctly recognized# words attempted?
100(1)For each configuration, the number of Gaussiancomponents in the state-specific observation proba-bility densities was increased (in an iterative man-ner) in powers of 2, from 1 to 32 components (forC00 and C01) or 64 components (for C11-C15):standard methods for choosing this number (usingdevelopment test data) could not be employed onaccount of insufficient data.
The results reportedin the next section should therefore be interpretedas development test results.
In order to avoid over-tuning, the number of Gaussian components wasconstrained to be the same across all speakers.
Forthe speaker-dependent systems (C00 and C01), re-sults are for HMMs with 2 Gaussian components perprobability density.
For the speaker-adapted systems(C11-C15), results are for HMMs with 32 Gaussiancomponents per probability density: while train-ing the speaker-independent TIMIT system, it wasfound that the phone recognition accuracy increasedmonotonically when going from 1 to 32 Gaussiancomponents but decreased when going from 32 to64 components.3 ResultsTables 3, 4 list the PWC scores for the various sys-tem configurations developed.
The speakers arelisted in decreasing order of intelligibility rating.The scores for systems C00 are restated here fromSharma and Hasegawa-Johnson (2009) (Table 6, un-der the column ?T10?
).We see that speaker-dependent systems with left-to-right HMMs (C00) have higher recognition ac-curacy than the speaker-dependent systems withtransition-interpolated HMMs (C01), for all speak-ers except M06.
System C11 for a particularspeaker, with adaptation of Gaussian means aloneperforms either better or worse than both sys-tems C00 and C01 for that speaker.
System C12with adaptation of Gaussian means and variances,has better recognition accuracy than both speaker-dependent systems, for all speakers except F02 andM07 (worse than both speaker-dependent systems).System C13 with adaptation of all parameters ex-76System ConfigurationSpeaker C00 C01 C11 C12M09 52.04 47.3 57.1 62.1M05 35.52 33.7 31 39.4M06 34.01 36.1 38.6 38.5F02 35.06 32.8 20.8 26.9M07 43.87 40.7 32 35.9F03 12.61 11.3 17.4 22.2M04 2.82 1.7 3.7 4.2Table 3: PWC scores for each speaker?s configurationsC00-C12.System ConfigurationSpeaker C00 C13 C14 C15M09 52.04 66.4 65.8 64.2M05 35.52 45.2 44 38.1M06 34.01 40.7 40.1 39.2F02 35.06 30.4 29.7 26.6M07 43.87 43 41.8 35.9F03 12.61 27.7 26.2 25.7M04 2.82 4.2 3.8 3.1Table 4: PWC scores for each speaker?s configurationsC00,C13-C15.cept transition-probabilities has the highest recogni-tion accuracy for all subjects except F02 and M07(highest among speaker-adapted systems only).
Sys-tem C14 which adapts all parameters including tran-sition probabilities, always performs worse than thecorresponding system C13, for all speakers.
How-ever, like system C13, it has better recognition ac-curacy than both speaker-dependent systems for allspeakers except F02 and M07.
Finally, perform-ing transition-interpolation and adaptation of all pa-rameters (system C15) worsens the performance tobelow that of the corresponding system C14; addi-tionally, C15 has better recognition accuracy thanboth speaker-dependent systems whenever the cor-responding C13 (and C14) system also performs bet-ter than them.These results are plotted in Fig.
4 along withthe human listeners?
intelligibility ratings of thesespeakers (the black circles).
For speakers M09 andM05, system C13 with the best overall PWC scoreis still far from doing as well as human listeners.
ForM09 M05 M06 F02 M07 F03 M040102030405060708090speaker (from left: decreasing order of intelligibility)# Words Correct (%)C00C01C11C12C13C14C15Figure 4: PWC scores for various system configurations(the black circles indicate speakers?
human listener intel-ligibility ratings).the remaining subjects, it has however been able todo as well or better than human listeners even whenit performed worse than the corresponding speaker-dependent systems (C00,C01): in fact, for speakerM06, it does better than human listeners when thespeaker-dependent systems don?t.Fig.
5 plots, for all speakers, the percentage dif-ference PWC(x)/PWC(C00)-1 between the PWC ofsystem x (x ?
{C01?
C15}) and the PWC of sys-tem C00.M09 M05 M06 F02 M07 F03 M04?40?20020406080100120speaker (from left: decreasing order of intelligibility)Changein RecognitionAccuracy (%) C01C11C12C13C14C15Figure 5: Percentage change in PWC scores for vari-ous system configurations relative to configuration C00?sPWC score.For speakers who have an intelligibility ratingabove 35% or below 25%, the speaker-adaptedsystems generally do better than their speaker-dependent counterparts.
System C01, with tran-sition interpolation, performs worse than system77C00 for all speakers except M06.
The surpris-ing result though is that for speakers with highlysevere dysarthria (F03 and M04), speaker-adaptedsystems have substantially better recognition ac-curacies than their speaker-dependent counterparts,when previous studies have indicated that for suchsubjects, speaker-dependent systems perform betterthan speaker-adapted systems.4 ConclusionsThis study investigated adaptation and state-transition interpolation techniques for medium vo-cabulary HMM-based speech recognition of talkerswith spastic dysarthria.
It was found that performingtransition-interpolation generally worsens recogni-tion performance when compared to left-to-rightHMMs.
Performing both adaptation and transition-interpolation results in higher recognition accuracycompared to the speaker-dependent systemwith left-to-right HMMs but adaptation-only systems havestill better performance.
This implies that state-transitions not accounted for in left-to-right HMMsdo not capture (or capture rather poorly) the out-lier events that differentiate dysarthric speech fromunimpaired speech at the sub-phone level.The most interesting outcome of our experimentsis that for subjects that have very severe dysarthria,speaker-adaptation was able to achieve substantialimprovement in recognition accuracy, compared tothe speaker-dependent systems.
This finding is sig-nificant in that it is contrary to the conclusions ofpreviously published studies.
The results reportedin this paper therefore suggest that the severity ofdysarthria as quantified by the subject?s intelligibil-ity rating is not a sufficient indicator of the rela-tive performance of speaker-dependent and speaker-adapted systems.ReferencesGloria S. Carlson and Jared Bernstein.
1987.
SpeechRecognition of Impaired Speech.
Proceedings ofRESNA 10th Annual Conference on RehabilitationTechnology, 165?167.Colette L. Coleman and Lawrence S. Meyers.
1991.Computer Recognition of the Speech of Adults withCerebral Palsy and Dysarthria.
AAC: Augmentativeand Alternative Communication, 7(1):34?42.John R. Deller, D. Frank Hsu and Linda J. Ferrier.
1988.Encouraging Results in the Automated Recognitionof Cerebral Palsy Speech.
IEEE Transactions onBiomedical Engineering, 35(3):218?220.John R. Deller, D. Frank Hsu and Linda J. Ferrier.
1991.On the use of Hidden Markov modelling for Recog-nition of Dysarthric Speech.
Computer Methods andPrograms in Biomedicine, 35(2):125?139.Melanie Fried-Oken.
1985.
Voice Recognition Device asa Computer Interface for Motor and Speech ImpairedPeople.
Archives of Physical Medicine and Rehabili-tation, 66:678?681.Jean-luc Gauvain and Chin-hui Lee.
1991.
BayesianLearning of Gaussian Mixture Densities for HiddenMarkov Models.
Proceedings of DARPA Speech andNatural Language Workshop, 272?277.Jean-luc Gauvain and Chin-hui Lee.
1992.
MAP Estima-tion of Continuous Density HMM: Theory and Appli-cations.
Proceedings of DARPA Speech and NaturalLanguage Workshop, 185?190.John S. Garofolo, Lori F. Lamel, William M.Fisher, Jonathan G. Fiscus, David S. Pallett,Nancy L. Dahlgren and Victor Zue.
1993.TIMIT Acoustic-Phonetic Continuous Speech Corpus.http://www.ldc.upenn.edu/Catalog/LDC93S1.html.Hynek Hermansky.
1990.
Perceptual Linear Predictive(PLP) Analysis of Speech.
Journal of the AcousticalSociety of America, 87(4):1738?1752.Heejin Kim, Mark Hasegawa-Johnson, Adrienne Perl-man, Jon Gunderson, Thomas Huang, Kenneth Watkinand Simone Frame.
2008.
Dysarthric SpeechDatabase for Universal Access Research.
Proceedingsof Interspeech, Brisbane, Australia, 22?26.Xavier Menendez-Pidal, James B. Polikoff, Shirley M.Peters, Jennie E. Leonzio, H. T. Bunnell.
1996.
TheNemours Database of Dysarthric Speech.
Proceedingsof the Fourth International Conference on Spoken Lan-guage Processing, Philadelphia, PA, USA.Prasad D. Polur and Gerald E. Miller.
2005a.
Effectof High-Frequency Spectral Components in ComputerRecognition of Dysarthric Speech based on a Mel-Cepstral Stochastic Model.
Journal of RehabilitationResearch & Development, 42(3):363?372.Prasad D. Polur and Gerald E. Miller.
2005b.
Experi-ments with Fast Fourier Transform, Linear Predictiveand Cepstral Coefficients in Dysarthric Speech Recog-nition Algorithms using Hidden Markov Model.
IEEETransactions on Neural Systems and RehabilitationEngineering, 13(4):558?561.Parimala Raghavendra, Elisabet Rosengren and SheriHunnicutt.
2001.
An Investigation of DifferentDegrees of Dysarthric Speech as Input to Speaker-Adaptive and Speaker-Dependent Recognition Sys-78tems.
AAC: Augmentative and Alternative Communi-cation, 17(4):265?275.Frank Rudzicz.
2007.
Comparing Speaker-Dependentand Speaker-Adaptive Acoustic Models for Recogniz-ing Dysarthric Speech.
Proceedings of ASSETS?07,Tempe, AZ, USA.Harsh Vardhan Sharma and Mark Hasegawa-Johnson.2009.
Universal Access: Speech Recognition for Talk-ers with Spastic Dysarthria.
Proceedings of Inter-speech, Brighton, UK, 1451?1454.79
