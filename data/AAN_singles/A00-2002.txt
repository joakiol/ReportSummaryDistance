The Automat ic  Translation of Discourse StructuresDanie l  MarcuInformation Sciences Institute andDepartment of Computer ScienceUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292marcu@isi.eduLynn Car l sonU.S.
Department of DefenseFt.
Meade, MD 20755Imcarls@afterlife.
ncsc.
milMaki  WatanabeDepartment of LinguisticsUniversity of Southern CaliforniaLos Angeles, CA 90089mwatanab@usc.eduAbst rac tWe empirically show that there are significant differ-ences between the discourse structure of Japanesetexts and the discourse structure of their corre-sponding English translations.
To improve trans-lation quality, we propose a computational modelfor rewriting discourse structures.
When we trainour model on a parallel corpus of manually builtJapanese and English discourse structure trees, welearn to rewrite Japanese trees as trees that arecloser to the natural English rendering than the orig-inal ones.1 Mot ivat ionAlmost all current MT systems process text one sen-tence at a time.
Because of this limited focus, MTsystems cannot re-group and re-order the clausesand sentences of an input text to achieve the mostnatural rendering in a target language.
Yet, evenbetween languages as close as English and French,there is a 10% mismatch in number of sentences- -  what is said in two sentences in one languageis said in only one, or in three, in the other (Galeand Church, 1993).
For distant language pairs, suchas Japanese and English, the differences are moresignificant.Consider, for example, Japanese sentence (1), aword-by-word "gloss" of it (2), and a two-sentencetranslation of it that was produced by a professionaltranslator (3).
(1)\[The Ministry of Health and Welfare last yearrevealed I \] \[population of future estimate ac-cording to 2\] \[in future 1.499 persons as thelowest s\] \[that after *SAB* rising to turn that 4\]\[*they* estimated but s \] \[already the estimatemisses a point ~\] \[prediction became.
7\](2)\[In its future population estimates'\] \[made (3)public last year, 2\] \[the Ministry of Health andWelfare predicted that the SAB would drop toa new low of 1.499 in the future, s) \[but wouldmake a comeback after that, 4\] \[increasing onceagain, s\] \[However, it looks as if that predictionwill be quickly shattered.
6\]The labeled spans of text represent elementarydiscourse units (edus), i.e., minimal text spans thathave an unambiguous discourse function (Mann andThompson, 1988).
If we analyze the text frag-ments closely, we will notice that in translating sen-tence (1), a professional translator chose to realizethe information in Japanese unit 2 first (unit 2 intext (1) corresponds roughly to unit 1 in text (3));to realize then some of the information in Japaneseunit 1 (part of unit 1 in text (1) corresponds to unit2 in text (3)); to fuse then information given in units1, 3, and 5 in text (1) and realize it in English asunit 3; and so on.
Also, the translator chose to re-package the information in the original Japanese sen-tence into two English sentences.At the elementary unit level, the correspondencebetween Japanese sentence (1) and its English trans-lation (3) can be represented as in (4}, where j C edenotes the fact that the semantic ontent of unitj is realized fully in unit e; j D e denotes the factthat the semantic ontent of unit e is realized fullyin unit j; j = e denotes the fact that units j and eare semantically equivalent; and j ~ e denotes thefact that there is a semantic overlap between units jand e, but neither proper inclusion nor proper equiv-alence..!1 D e2;jt  -~ e3;.12 ---- el;33 C e3;.14 ~ e4;j4 ~ es;.15 ~ e3;.16 C e6;.17 C e6(4)9C :.(~(.
(:S'.~0r I tJe 3bota'lc.r- o31.~ c; - 3ttlio~te- e.~- - ' - -~ I:~) (~)l~ - IP .~ra~2,~ ,~,Ooll l t , \] iLtt ~:I..,?
(The Ministry dL '(popu ~t ,m -ofHgMth .~nd ~f Ntu le  esti~n.~tereve~ie,:~)............... -,,.,~__~ ._~--- ~ ...-.<_-7_"" ___~__--,~(5) (6) (7)~.. .~ ---....~ hev\] estin'at~d ~ ~alr~acy the redicti0n6?,) (~) but) estlnate miss - a becar.e)e 1 499 persons \[SAI~ rising - \[oa: the lowest) turn th~1)' i :  .................................................................................................................... ==:~::::?2' ~2~?:::-~.
:.,~a ...............Is).
.
.
.
.
.
.
_~_ .b~ ~, ,~d i Ho,e~v~,looks as if, ~ , !
r .~  ,~.
- pNxil ~i~',~ will(1) 12) O) .
,,- quicklyin its future made public the MiniMry ~i~.~ , .~-  dn iota| ihatN~ CNI.popukltion kin yem', o~ I'~slth and ~ '  "~?-~imm?~ Welfare (4) (5)the SAB make II onto again.wotdd drop toafte~ that,a r, ew I~ ol1.499 In thefulute.Figure 1: The discourse structures of texts (1) and (3).Hence.
the mappings in (4) provide all explicitrepresentation of the way information is re-orderedand re-packaged when translated from Japanese intoEnglish.
However, when translat ing text, it is alsothe case that t he rhetorical rendering changes.
Whatis realized ill Japanese using an CONTRAST relationcan be realized in English using, for example, a COXl-PARISON or  a CONCESSION relation.Figure I presents in the style of Mann and Thomp-son (1988) the discourse structures of text frag-ments (1) and (3), Each discourse structure is atree whose leaves correspond to the edus and whoseinternal nodes correspond to contiguous text spans.Each node is characterized by a s tatus  (NUCLEUS orSATELLITE) and a rhetor ica l  re lat ion,  which is a re-lation that holds between two non-overlapping textspans.
The distinction between nuclei and satellitescomes from the empirical observation that the nu-cleus expresses what is more essential to the writer'sintention than the satellite: and that the nucleus ofa rhetorical relation is comprehensible independentof tile satellite, but not vice versa.
When spans areequally important ,  the relation is nmltinuclear: forexample,  the CONTRAST relation that holds betweenunit \[3\] and span \[4.5\] in the rhetorical structure ofthe English text in figure 1 is nmhinuclear.
Rhetor-ical relations that end in the suffix "'-e'" denote re-lations that correspond to embedded syntactic con-stituents.
For example, the ELABORATION-OBJECT-ATTRIBUTE-E relation that holds between units 2and 1 in the English discourse structure correspondsto a restrictive relative.If one knows the mappings at the edu level,one can determine the mappings at the span (dis-course constituent) level as well.
For example, us-ing the elementary mappings in (4), one call deter-mine that Japanese span \[1,2\] corresponds to Englishspan \[I,2\], Japanese unit \[4\] to English span \[4,5\],Japanese span \[6.7\] to English unit \[6\], Japanesespan \[1.5\] to English span \[1.5\], and so on.
As Fig-ure 1 shows, the CONCESSION relation that holds be-tween spans \[1,5\] and \[6,7\] in the Japanese tree corre-sponds to a similar relation that.
holds between span\[1,5\] and unit \[6\] in the English tree (modulo the factthat,  in Japanese, the relation holds between sen-t ence fragments, while in English it holds betweenfull sentences).
However, the TEMPORAL-AFTER re-lation that holds between units \[:3\] and \[4\] ill theJapanese tree is realized as a CONTRAST relationbetween unit \[3\] and span \[4.5\] in the English tree.And because Japanese units \[6\] and \[7\] are fusedinto unit \[6\] in English, the relation ELABORATION-OBJECT-ATTRIBUTE-E is 11o longer made explicit inthe English text.Some of the differences between the two discoursetrees in Figure 1 have been tradit ionally addressed10Corpus k~ (#) k, (#) k,~ (#) k~ (#)Japanese 0.856 (80) 0.785 (3377) 0.724 (3377) 0.650 (3377)English 0.925 (60) 0.866 (1826) 0.839 (1826) 0.748 (1826)Table 1: Tagging reliabilityin MT systems at the syntactic level.
For exam-ple, the re-ordering of units 1 and 2, can be dealtwith using only syntactic models.
However, as wewill see in Section 2, there are significant differencesbetween Japanese and English with respect to theway information is packaged and organized rhetori-cally not only at the sentence level, but also, at theparagraph and text levels.
More specifically, as hu-mans translate Japanese into English, they re-orderthe clauses, sentences, and paragraphs of Japanesetexts, they re-package the information into clauses,sentences, and paragraphs that are not a one-to-onemapping of the original Japanese units, and theyrhetorically re-organize the structure of the trans-lated text so as to reflect rhetorical constraints pe-cific to English.
If a translation system is to producetext that is not only grammatical but also coherent,it will have to ensure that the discourse structure ofthe target text reflects the natural renderings of thetarget language, and not that of the source language.In Section 2, we empirically show that there aresignificant differences between the rhetorical struc-ture of Japanese texts and their corresponding En-glish translations.
These differences justify our in-vestigation into developing computational modelsfor discourse structure rewriting.
In Section 3, wepresent such a rewriting model, which re-orders theedus of the original text, determines English-specificclause, sentence, and paragraph boundaries, and re-builds the Japanese discourse structure of a text us-ing English-specific rhetorical renderings.
In Sec-tion 4, we evaluate the performance of an imple-mentation of this model.
We end with a discussion.2 ExperimentIn order to assess the role of discourse structure inMT, we built manually a corpus of discourse treesfor 40 Japanese texts and their corresponding trans-lations.
The texts were selected randomly from theARPA corpus (White and O'Connell, 1994).
On av-erage, each text had about 460 words.
The Japanesetexts had a total of 335 paragraphs and 773 sen-tences.
The English texts had a total of 337 para-graphs and 827 sentences.We developed a discourse annotation protocol forJapanese and English along the lines followed byMarcu et al (1999).
We used Marcu's discourse an-notation tool (1999) in order to manually constructthe discourse structure of all Japanese and Englishtexts in the corpus.
10% of the Japanese and En-glish texts were rhetorically labeled by two of us.The tool and the annotation protocol are availableat http://www.isi.edu/~marcu/software/.
The an-notation procedure yielded over the entire corpus2641 Japanese edus and 2363 English edus.We computed the reliability of the annotation us-ing Marcu et al (1999)'s method for computingkappa statistics (Siegel and Castellan, 1988) over hi-erarchical structures.
Table 1 displays average kappastatistics that reflect the reliability of the annota-tion of elementary discourse units, k~,, hierarchicaldiscourse spans, ks, hierarchical nuclearity assign-ments, k,~, and hierarchical rhetorical relation as-signments, k~.
Kappa figures higher than 0.8 corre-spond to good agreement; kappa figures higher than0.6 correspond to acceptable agreement.
All kappastatistics were statistically significant at levels higherthan a = 0.01.
In addition to the kappa statis-tics, table 1 also displays in parentheses the averagenumber of data points per document, over which thekappa statistics were computed.For each pair of Japanese-English discourse struc-tures, we also built manually an alignment file,which specified in the notation discussed on page 1the correspondence b tween the edus of the Japanesetext and the edus of its English translation.We computed the similarity between English andJapanese discourse trees using labeled recall and pre-cision figures that reflected the resemblance of tileJapanese and English discourse structures with re-spect to their assignment of edu boundaries, hierar-chical spans, nuclearity, and rhetorical relations.Because the trees we compared differ from onelanguage to the other in the number of elementaryunits, the order of these units, and the way the unitsare grouped recursively into discourse spans, wecomputed two types of recall and precision figures.In computing Position-Dependent (P-D) recall andprecision figures, a Japanese span was considered tomatch an English span when the Japanese span con-tained all the Japanese dus that corresponded to tileedus in the English span, and when the Japanese andEnglish spans appeared in tile same position withrespect to the overall structure.
For example, theEnglish tree in figure 1 is characterized by 10 sub-sentential spans: \[1\], \[2\], \[3\], \[4\], \[5\], \[6\], \[1,2\], \[4,5\],\[3,5\], and \[1,5\].
(Span \[1,6\] subsumes 2 sentences,so it is not sub-sentential.)
The Japanese discoursetree has only 4 spans that could be matched in thesame positions with English spans, namely spans\[1,2\], \[4\], \[5\], and \[1,5\].
Hence the similarity betweenthe Japanese tree and the English tree with respect11 11Level Units Spans Status/Nuclearity RelationsP-D P P-D R P-D P P -DR P -DP  P -DR P-I) P P -DRSentence 29.1 25.0 27.2 22.7 21.3 17.7 14.9 12.4Paragraph 53.9 53.4 46.8 47.3 38.6 39.0 31.9 32.3Text 41.3 42.6 31.5 32.6 28.8 29.9 26.1 27.1Weighted Average 36.0 32.5 31.8 28.4 26.0 23.1 20.1 17.9All 8.2 7.4 5.9 5.3 4.4 3.9 3.3 3.0P-I R P-I P P-I R P-I P P-I R P-I P P-I R P-I PSentence 71.0 61.0 56.0 46.6 44.3 36.9 30.5 25.4Paragraph 62.1 61.6 53.2 53.8 43.3 43.8 35.1 35.5Text 74.1 76.5 54.4 56.5 48.5 50.4 41.1 42.7Weighted Average 55.2 49.2 44.8 39.9 33.1 29.526.8 24.3 All69.6 63.074.5 66.8 50.6 45.8 39.4 35.7Table 2: Similarity of the Japaneseto their discourse structure below the sentence levelhas a recall of 4/10 and a precision of 4/11 (in Fig-ure 1, there are 11 sub-sentential Japanese spans).In computing Position-Independent (P-I) recalland precision figures, even when a Japanese span"floated" during the translation to a position in theEnglish tree that was different from the position inthe initial tree, the P-I recall and precision figureswere not affected.
The Position-Independent figuresreflect the intuition that if two trees tl and t2 bothhave a subtree t, tl and t2 are more similar thanif they were if they didn't share any tree.
At thesentence level, we hence assume that if, for exam-ple, the syntactic structure of a relative clause istranslated appropriately (even though it is not ap-propriately attached), this is better than translatingwrongly that clause.
The Position-Independent fig-ures offer a more optimistic metric for comparingdiscourse trees.
They span a wider range of valuesthan the Position-Dependent figures, which enablea finer grained comparison, which in turn enablesa better characterization of the differences betweenJapanese and English discourse structures.
Whenone takes an optimistic stance, for the spans at thesub-sentential level in the trees in Table 1 the recallis 6/10 and the precision is 6/11 because in additionto spans \[1,2\], \[4\], \[5\], and \[1,5\], one can also matchJapanese span \[1\] to English span \[2\] and Japanesespan \[2\] to Japanese span \[1\].In order to provide a better estimate of how closetwo discourse trees were, we computed Position-Dependent and -Independent recall and precision fig-ures for the sentential level (where units are given byedus and spans are given by sets of edus or single sen-tences); paragraph level (where units are given bysentences and spans are given by sets of sentencesor single paragraphs); and text level (where unitsare given by paragraphs and spans are given by setsof paragraphs).
These figures offer a detailed pic-ture of how discourse structures and relations aremapped from one language to the other across alland English discourse structuresdiscourse levels, from sentence to text.
The differ-ences at the sentence level can be explained by differ-ences between the syntactic structures of Japaneseand English.
The differences at the paragraph andtext levels have a purely rhetorical explanation.As expected, when we computed the recall andprecision figures with respect to the nuclearity andrelation assignments, we also factored in the statusesand the rhetorical relations that labeled each pair ofspans.Table 2 smnmarizes the results (P-D and P-I (R)ecall and (P)recision figures) for each level(Sentence, Paragraph, and Text).
The numbersin the "Weighted Average" line report averages ofthe Sentence-, Paragraph-, and Text-specific figures,weighted according to the number of units at eachlevel.
The numbers in the "All" line reflect recall andprecision figures computed across the entire trees,with no attention paid to sentence and paragraphboundaries.Given the significantly different syntactic struc-tures of Japanese and English, we were not surprisedby the low recall and precision results that reflectthe similarity between discourse trees built belowthe sentence level.
However, as Table 2 shows, thereare significant differences between discourse trees atthe paragraph and text levels as well.
For exam-pie, the Position-Independent figures show that onlyabout 62% of the sentences and only about 53% ofthe hierarchical spans built across sentences couldbe matched between the two corpora.
When onelooks at the status and rhetorical relations associ-ated with the spans built across sentences at theparagraph level, the P-I recall and precision figuresdrop to about 43% and 35% respectively.The differences in recall and precision are ex-plained both by differences in the way information ispackaged into paragraphs in the two languages andthe way it is structured rhetorically both within andabove the paragraph level.These results strongly suggest hat if one attempts12to translate Japanese into English on a sentence-by-sentence basis, it is likely that the resulting text willbe unnatural from a discourse perspective.
For ex-ample, if some information rendered using a CON-TRAST relation in Japanese is rendered using anELABORATION relation in English, it would be in-appropriate to use a discourse marker like "but" inthe English translation, although that would be con-sistent with the Japanese discourse structure.An inspection of the rhetorical mappings betweenJapanese and English revealed that some Japaneserhetorical renderings are consistently mapped intoone or a few preferred renderings in English.
For ex-ample, 34 of 115 CONTRAST relations in the Japanesetexts are mapped into CONTRAST relations in En-glish; 27 become nuclei of relations uch as ANTITHE-SIS and CONCESSION, 14 are translated as COMPAR-ISON relations, 6 as satellites of CONCESSION rela-tions, 5 as LIST relations, etc.
Our goal is to learnthese systematic discourse mapping rules and exploitthem in a machine translation context.3 Towards  a d i scourse -basedmach ine  t rans la t ion  sys tem3.1 Overa l l  a rch i tec tureWe are currently working towards building the mod-ules of a Discourse-Based Machine Translation sys-tem that works along the following lines.1.
A discourse parser, such as those described bySumita et al (1992), Kurohashi (1994), andMarcH (1999), initially derives the discoursestructure of the text given as input.2.
A discourse-structure transfer modulerewrites the discourse structure of the inputtext so as to reflect a discourse renderingthat is natural to the target language.3.
A statistical module maps the input textinto the target language using translation andlanguage models that incorporate discourse-specific features, which are extracted from theoutputs of the discourse parser and discoursetransfer modules.In this paper, we focus only on the discourse-structure transfer module.
That is, we investigatethe feasibility of building such a module.3.2 The  d i scourse -based  t rans fer  mode lIn order to learn to rewrite discourse structure trees,we first address a related problem, which we definebelow:Def in i t ion  3.1 Given two trees Ts and Tt and acorrespondence Table C defined between Ts and Ttat the leaf level in terms of-----, C, D, and ~ relations,find a sequence of actions that rewrites the tree T~into Tt.If for any tuple (Ts, Tt, C> such a sequence of actionscan be derived, it is then possible to use a corpusof (Ts, Tt, C) tuples in order to automatically learnto derive from an unseen tree Ts,, which has thesame structural properties as the trees Ts, a treeTtj, which has structural properties imilar to thoseof the trees Tt.In order to solve the problem in definition 3.1, weextend the shift-reduce parsing paradigm applied byMagerman (1995), Hermjakob and Mooney (1997),and MarcH (1999).
In this extended paradigm, thetransfer process starts with an empty Stack and anInput List that contains a sequence of elementarydiscourse trees edts, one edt for each edu in the treeTs given as input.
The status and rhetorical rela-tion associated with each edt is undefined.
At eachstep, the transfer module applies an operation that isaimed at building from the units in T, the discoursetree Tt.
In the context of our discourse-transfer mod-ule, we need 7 types of operations:?
SHIFT operations transfer the first edt fromthe input list into the stack;?
REDUCE operations pop the two discoursetrees located at the top of the stack; combinethem into a new tree updating the statusesand rhetorical relation names of the trees in-volved in the operation; and push the newtree on the top of the stack.
These opera-tions are used to build the structure of thediscourse tree in the target language.?
BREAK operations are used in order to breakthe edt at the beginning of the input list intoa predetermined number of units.
These op-erations are used to ensure that the result-ing tree has the same number of edts as Tt.A BREAK operation is necessary whenever aJapanese edu is mapped into nmltiple Englishunits.?
CREATE-NEXT operations are used in orderto create English discourse constituents thathave no correspondent in the Japanese tree.?
FUSE operations are used in order to fuse theedt at the top of the stack into the tree thatimmediately precedes it.
These operationsare used whenever multiple Japanese edus aremapped into one English edu.?
SWAP operations wap the edt at the begin-ning of the input list with an edt found oneor more positions to the right.
These oper-ations are necessary for re-ordering discourseconstituents.?
ASSIGNTYPE operations assign one or more ofthe following types to the tree at the top ofthe stack: Unit, MultiUnit, Sentence, Para-graph, MultiParagraph, and Text.
These op-13erations are necessary in order to ensure sen-tence and paragraph boundaries that are spe-cific to the target language.For example, the first sentence of the English tree inFigure 1 can be obtained from the original Japanesesequence by following the sequence of actions (5),whose effects are shown in Figure 2.
For the purposeof compactness, the figure does not illustrate the ef-fect of ASSIGNTYPE actions.
For the same purpose,some lines correspond to more than one action,BREAK 2; SWAP 2; SHIFT; ASSIGNTYPEUNIT; SHIFT; REDUCE-NS-ELABORATION-OBJECT-ATTRIBUTE-E; ASSIGNTYPEMULTIUNIT; SHIFT; ASSIGNTYPE UNIT;SHIFT; ASSIGNTYPE UNIT; FUSE;ASSIGNTYPE UNIT; SWAP 2; SHIFT;ASSIGNTYPE UNIT; FUSE; BREAK 2; (5)SHIFT; ASSIGNTYPE UNIT; SHIFT;ASSIGNTYPE UNIT; REDUCE-NS-ELABORATION-ADDITIONAL; ASSIGNTYPEMULTIUNIT; REDUCE-NS-CONTRAST;ASSIGNTYPE MULTIUNIT; REDUCE-SN-BACKGROUND; ASSIGNTYPE SENTENCE.For our corpus, in order to enable a discourse-based transfer module to derive any English dis-course tree starting from any Japanese discoursetree, it is sufficient o implement:* one SHIFT operation;?
3 x 2 ?
85 REDUCE operations; (For eachof the three possible pairs of nuclear-ity assignments NUCLEUS-SATELLITE (NS),SATELLITE-NUCLEUS (SN), AND NUCLEUS-NUCLEUS (NN), there are two possible waysto reduce two adjacent rees (one resultsin a binary tree, the other in a non-binarytree (Marcu, 1999)), and 85 relation names.)?
three types of BREAK operations; (In our cor-pus, a Japanese unit is broken into two, three,or at most four units.)?
one type of CREATE-NEXT operation;?
one type of FUSE operation;?
eleven types of SWAP operations; (In ourcorpus, Japanese units are at most l l posi-tions away from their location in an English-specific rendering.)?
seven types of ASSIGN~\]~YPE operations: Unit,MultiUnit, Sentence, MultiSentence, Para-graph, MultiParagraph, and Text.These actions are sufficient for rewriting any treeTs into any tree Tt, where Tt may have a differentnumber of edus, where the edus of Tt may have adifferent ordering than the edus of Ts, and wherethe hierarchical structures of the two trees may bedifferent as well.3.3 Learn ing  the  parameters  o f  thed i scourse - t rans fer  mode lWe associate with each configuration of our trans-fer model a learning case.
The cases were gener-ated by a program that automatically derived thesequence of actions that mapped the Japanese treesin our corpus into the sibling English trees, using thecorrespondences at the elementary unit level thatwere constructed manually.
Overall, the 40 pairs ofJapanese and English discourse trees yielded 14108cases.To each learning example, we associated a set offeatures from the following classes:Operat iona l  and  d iscourse  features  reflect thenumber of trees in the stack, the input list,and the types of the last five operations.They encode information pertaining to thetypes of the partial trees built up to a certaint ime and the rhetorical relations that hold be-tween these trees.Cor respondence-based  features  reflect the nu-clearity, rhetorical relations, and types ofthe Japanese trees that correspond to theEnglish-like partial trees derived up to a giventime.Lex ica l features  specify whether the Japanesespans that correspond to the structures de-rived up to a given time use potential dis-course markers, such as dakara (because) andno ni (although).The discourse transfer module uses the C4.5 pro-gram (Quinlan, 1993) in order to learn decision treesand rules that specify how Japanese discourse treesshould be mapped into English-like trees.
A ten-foldcross-validation evaluation of the classifier yielded anaccuracy of 70.2% (+ 0.21).In order to better understand the strengths andweaknesses of the classifier, we also attempted tobreak the problem into smaller components.
Hence,instead of learning all actions at once, we attemptedto learn first whether the rewriting procedure shouldchoose a SHIFT, REDUCE, BREAK, FUSE, SWAP, orASSIGNTYPE operation (the "Main Action Type"classifier in table 3), and only then to refine thisdecision by determining what type of reduce opera-tion to perform, how many units to break a Japaneseunits into, how big the distance to the SWAP-ed unitshould be, and what type of ASSIGNTYPE operationone should perform.
Table 3 shows the sizes of each14STACK22 1"\[~A BORATION_(IB~TI-2 l "}~1 ABORATION_O~E_E - -2 I "  1' 3. .
.
.
.
.
.
.
.
.
.
.
.
.
.2 1'* 1",3EI.ABOP, AT ION~(~E_E2 1" I', 3,5IiLA B( )RA T ION_(~T 'E_ I~2 1" I', 3, 5 4' 4'"HI ABOl,b%TIONIOBIE~:'r A'rrRIBUTE\],3,~ .LAB(~T ON-ADDIT2 1"" 4" 4"2 1" 1 ", 3, 5 ~ A ~  \[:BACKGROUND2 1"" .
- FJ.ABOIIATION-ADD\[1 r. 3 ,y -~4" 4"INPUT LIST1 2 3 4 5 6 71" 1'" 2 3 4 5 6 72 I"  1' 3 4 5I"  I' 3 4 5 61" 3 4 5 6 71' 3 4 5 6 74 5 6 74 5 6 74 6 76 7c)N~6"IONAL6)N~J.677BREAK 2SWAP 2SHIFTSHIFTREDUCE-NS-ELABORATION-OBJECT-ATTRIBUTE-ESHIFT; SHIFTFUSESWAP 2; SHIFT; FUSEBREAK 2; SHIFT; SHIFTREDUCE-NS-ELABORATION-ADDITIONALREDUCE-NN-CONTRASTREDUCE-SN-BACKGROUNDASSIGNTYPE SENTENCEFigure 2: Example of incremental tree reconstruction.data set and the performance of each of these classi-tiers, as determined using a ten-fold cross-validationprocedure.
For the purpose of comparison, each clas-sifier is paired with a majority baseline.The results in Table 3 show that the most diffi-cult subtasks to learn are that of determining thenumber of units a Japanese unit should be brokeninto and that of determining the distance to the unitthat is to be swapped.
The features we used arenot able to refine the baseline classifiers for theseaction types.
The confusion matrix for the "MainAction Type" classifier (see Table 5) shows that thesystem has trouble mostly identifying BREAK andCREATE-NEXT actions.
The system has difficultylearning what type of nuclearity ordering to pre-fer (the "Nuclearity-Reduce" classifier) and what re-lation to choose for the English-like structure (the"Relation-Reduce" classifier).Figure 3 shows a typical learning curve, the onethat corresponds to the "Reduce Relation" classifier.Our learning curves suggest hat more training datamay improve performance.
However, they also sug-gest that better features may be needed in order toimprove performance significantly.Table 4 displays ome learned rules.
The first ruleaccounts for rhetorical mappings in which the or-der of the nucleus and satellite of an ATTRIBUTIONrelation is changed when translated from Japaneseinto English.
The second rule was learned in orderto map EXAMPLE Japanese satellites into EVIDENCEEnglish satellites.1R 15Classifier @ casesGeneral(Learns all classes at once)Main Action TypeAssignTypeBreakNuclearity-ReduceRelation-ReduceSwap1410814108641639423882388842Accuracy (10-fold cross validation)70.20% (+0.21)82.53% (?0.25)90.46% (?0.39)82.91% (?1.40)67.43% (?1.03)48.20% (?i.01)62.98% (?1.62)Majority baseline accuracy22.05% (on ASSIGNTYPE UNIT)45.47% (on ASSIGNTYPE)57.30% (on ASSIGNTYPE Unit)82.91% (on BREAK 2)50.92% (on KS)17.18% (on ELABORATION-OBJECT-ATTRIBUTE-E)62.98% (on SWAP 1)Table 3: Performance of the classifiers~oo440o~oo38 oo~oaI I I IRtlauoaRcaa?
etC~ xlO 3Figure 3: Learning curve for the Relation-Reduceclassifier.if rhetRelOfStack-llnJapTree = ATTRIBUTIONthen rhetRelOffFopStacklnEngTree ~ ATTRIBUTIONif rhetRelOffFopStacklnJapTree ---- EXAMPLE AisSentenceTheLastUnitlnJapTreeOfropStack = f lsethen rhetRelOfI'opStackInEngTree ~ EVIDENCETable 4: Rule examples for the Relation-Reduceclassifier.4 Eva luat ion  o f  the  d i scourse -basedt rans fer  modu leBy applying the General classifier or the other sixclassifiers successively, one can map any Japanesediscourse tree into a tree whose structure comescloser to the natural rendering of English.
To evalu-ate the discourse-based transfer module, we carriedout a ten-fold cross-validation experiment.
That is,we trained the classifiers on 36 pairs of manuallybuilt and aligned discourse structures, and we thenused the learned classifiers in order to map 4 un-seen Japanese discourse trees into English-like trees.We measured the similarity of the derived trees withthe English trees built manually, using the metricsdiscussed in Section 2.
We repeated the procedureten times, each time training and testing on differentsubsets of tree pairs.Act ion  (a) (b) (c) (d) (e) (f) (g)ASSIGNTYPE (a) 660BREAK (b) 1 2 28 1CREATE-NEXT (C) I SFUSE (d) 69 8 3REDUCE (e) 4 18 193 30 3SHIFT (f) 1 4 15 44 243 25.SWAP (g) 3 4 14 43 25Table 5: Confusion matrix for the Main Action Typeclassifier.We take the results reported in Table 2 as a base-line for our model.
The baseline corresponds to ap-plying no knowledge of discourse.
Table 6 displaysthe absolute improvement (in percentage points) inrecall and precision figures obtained when the Gen-eral classifier was used to map Japanese trees intoEnglish-looking trees.
The General classifier yieldedthe best results.
The results in Table 6 are averagedover a ten-fold cross-validation experiment.The results in Table 6 show that our modeloutperforms the baseline with respect to buildingEnglish-like discourse structures for sentences, butit under-performs the baseline with respect o build-ing English-like structures at the paragraph and textlevels.
The main shortcoming of our model seems tocome from its low performance in assigning para-graph boundaries.
Because our classifier does notlearn correctly which spans to consider paragraphsand which spans not, the recall and precision resultsat the paragraph and text levels are negatively af-fected.
The poorer esults at the paragraph and textlevels can be also explained by errors whose effect cu-mulates during the step-by-step tree-reconstructionprocedure; and by the fact that, for these levels,there is less data to learn from.However, if one ignores the sentence and para-graph boundaries and evaluates the discourse struc-tures overall, one can see that our model outper-forms the baseline on all accounts according tothe Position-Dependent evaluation; outperforms thebaseline with respect to the assignment of elemen-tary units, hierarchical spans, and nuclearity sta-tuses according to the Position-Independent evalu-ation and under-performs the baseline only slightly16 16Level UnitsP-D R P-D PSpansP-D R P-D PStatus/NuclearityP-D R P-D PRelationsP-D R P-D PSentence +9.1 +25.5 +2.0 +19.9 +0.4 +13.4 -0.01 +8.4Paragraph -14.7 +1.4 -12.5 -1.7 -11.0 -2.4 -9.9 -3.3Text -9.6 -13.5 -7.1 -11.1 -6.3 -10.0 -5.2 -8.8Weighted Average +1.5 +14.1 -2.1 +9.9 -3.1 +6.4 -3.0 +3.9All -1.2 +2.5 -0.1 +2.9 +0.6 +3.5 +0.7 +2.6P-I R P-I P P-I R P-I P P-I R P-I P P-I R P-I PSentence +13.4 +30.4 +3.1 +36.1 -6.3 +18.6 -10.1 +3.9Paragraph -15.6 +0.6 -13.5 -0.8 -11.7 -1.8 -10.3 -2.8Text -15.4 -23.3 -13.0 -20.4 -13.2 -19.5 -11.5 -17.0Weighted Average +3.6 +15.5 -2.7 +17.1 -8.5 +7.3 -10.5 -0.4All +12.7 +29.6 +2.0 +28.8 -5.1 +13.0 -7.9 +2.2Table 6: Relative evaluation of the discourse-based transfer module with respect o the figures in Table 2.with respect o the rhetorical relation assignmentaccording to the Position-Independent evaluation.More sophisticated discourse features, such as thosediscussed by Maynard (1998), for example, and atighter integration with the lexicogrammar of thetwo languages may yield better cues for learningdiscourse-based translation models.5 Conc lus ionWe presented a systematic empirical study of therole of discourse structure in MT.
Our study stronglysupports the need for enriching MT systems witha discourse module, capable of re-ordering and re-packaging the information i  a source text in a waythat is consistent with the discourse rendering of atarget language.
We presented an extended shift-reduce parsing model that can be used to map dis-course trees specific to a source language into dis-course trees specific to a target language.
Our modeloutperforms a baseline with respect o its ability topredict the discourse structure of sentences.
Ourmodel also outperforms the baseline with respectto its ability to derive discourse structures that arecloser to the natural, rhetorical rendering in a tar-get language than the original discourse structuresin the source language.
Our model is still unable todetermine correctly how to re-package sentences intoparagraphs; a better understanding of the notion of"paragraph" is required in order to improve this.Re ferencesWilliam A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual cor-pora.
Computational Linguistics, 19(1):75-102.Ulf Hermjakob and Raymond J. Mooney.
1997.Learning parse and translation decisions from ex-amples with rich context.
In Proc.
of ACL'97,pages 482-489, Madrid, Spain..Sadao Kurohashi and Makoto Nagao.
1994.
Auto-matic detection of discourse structure by check-ing surface information in sentences.
In Proc.
ofCOLING'94, volume 2, pages 1123-1127, Kyoto,Japan.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proc.
of A CL '95, pages276-283, Cambridge, Massachusetts.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functionaltheory of text organization.
Text, 8(3):243-281.Daniel Marcu.
1999.
A decision-based approach torhetorical parsing.
In Proc.
of A CL'99, pages 365-372, Maryland.Daniel Marcu, Estibaliz Amorrortu, and MagdalenaRomera.
1999.
Experiments inconstructing a cor-pus of discourse trees.
In Proc.
of the A CL'99Workshop on Standards and Tools for DiscourseTagging, pages 48-57, Maryland.Senko K. Maynard.
1998.
Principles of JapaneseDiscourse: A Handbook.
Cambridge Univ.
Press.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann Publishers.Sidney Siegel and N.J. Castellan.
1988.
Non-parametric Statistics for the Behavioral Sciences.McGraw-Hill, Second edition.Kazuo Sumita, Kenji Ono, T. Chino, TeruhikoUkita, and Shin'ya Amano.
1992.
A discoursestructure analyzer for Japanese text.
In Proceed-ings of the International Conference on Fifth Gen-eration Computer Systems, v 2, pages 1133-1140.J.
White and T. O'Connell.
1994.
Evaluation inthe ARPA machine-translation program: 1993methodology.
In Proceedings ofthe ARPA HumanLanguage Technology Workshop, pages 135-140,Washington, D.C.17 17
