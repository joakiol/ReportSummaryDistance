Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 10?19,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsUndirected Machine Translation withDiscriminative Reinforcement LearningAndrea GesmundoGoogle Inc.andrea.gesmundo@gmail.comJames HendersonXerox Research Centre Europejames.henderson@xrce.xerox.comAbstractWe present a novel Undirected MachineTranslation model of Hierarchical MT thatis not constrained to the standard bottom-up inference order.
Removing the order-ing constraint makes it possible to condi-tion on top-down structure and surround-ing context.
This allows the introduc-tion of a new class of contextual featuresthat are not constrained to condition onlyon the bottom-up context.
The modelbuilds translation-derivations efficiently ina greedy fashion.
It is trained to learnto choose jointly the best action and thebest inference order.
Experiments showthat the decoding time is halved and forest-rescoring is 6 times faster, while reachingaccuracy not significantly different fromstate of the art.1 IntroductionMachine Translation (MT) can be addressed as astructured prediction task (Brown et al., 1993; Ya-mada and Knight, 2001; Koehn et al., 2003).
MT?sgoal is to learn a mapping function, f , from an in-put sentence, x, into y = (t, h), where t is thesentence translated into the target language, andh is the hidden correspondence structure (Lianget al., 2006).
In Hierarchical MT (HMT) (Chi-ang, 2005) the hidden correspondence structure isthe synchronous-tree composed by instantiationsof synchronous rules from the input grammar, G.Statistical models usually define f as: f(x) =argmaxy?YScore(x, y), where Score(x, y) is afunction whose parameters can be learned with aspecialized learning algorithm.
In MT applica-tions, it is not possible to enumerate all y ?
Y .HMT decoding applies pruning (e.g.
Cube Prun-ing (Huang and Chiang, 2005)), but even thenHMT has higher complexity than Phrase BasedMT (PbMT) (Koehn et al., 2003).
On the otherhand, HMT improves over PbMT by introducingthe possibility of exploiting a more sophisticatedreordering model not bounded by a window size,and producing translations with higher syntactic-semantic quality.
In this paper, we present theUndirected Machine Translation (UMT) frame-work, which retains the advantages of HMT andallows the use of a greedy decoder whose com-plexity is lower than standard quadratic beam-search PbMT.UMT?s fast decoding is made possible througheven stronger pruning: the decoder chooses a sin-gle action at each step, never retracts that action,and prunes all incompatible alternatives to that ac-tion.
If this extreme level of pruning was ap-plied to the CKY-like beam-decoding used in stan-dard HMT, translation quality would be severelydegraded.
This is because the bottom-up infer-ence order imposed by CKY-like beam-decodingmeans that all pruning decisions must be based ona bottom-up approximation of contextual features,which leads to search errors that affect the qual-ity of reordering and lexical-choice (Gesmundoand Henderson, 2011).
UMT solves this problemby removing the bottom-up inference order con-straint, allowing many different inference ordersfor the same tree structure, and learning the in-ference order where the decoder can be the mostconfident in its pruning decisions.Removing the bottom-up inference order con-straint makes it possible to condition on top-downstructure and surrounding context.
This undirectedapproach allows us to integrate contextual featuressuch as the Language Model (LM) in a more flex-10ible way.
It also allows us to introduce a new classof undirected features.
In particular, we introducethe Context-Free Factor (CFF) features.
CFF fea-tures compute exactly and efficiently a bound onthe context-free cost of a partial derivation?s miss-ing branches, thereby estimating the future cost ofpartial derivations.
The new class of undirectedfeatures is fundamental for the success of a greedyapproach to HMT, because the additional non-bottom-up context is sometimes crucial to have thenecessary information to make greedy decisions.Because UMT prunes all but the single cho-sen action at each step, both choosing a good in-ference order and choosing a correct action re-duce to a single choice of what action to takenext.
To learn this decoding policy, we proposea novel Discriminative Reinforcement Learning(DRL) framework.
DRL is used to train mod-els that construct incrementally structured out-put using a local discriminative function, withthe goal of optimizing a global loss function.We apply DRL to learn the UMT scoring func-tion?s parameters, using the BLEU score as theglobal loss function.
DRL learns a weight vectorfor a linear classifier that discriminates betweendecisions based on which one leads to a com-plete translation-derivation with a better BLEUscore.
Promotions/demotions of translations areperformed by applying a Perceptron-style updateon the sequence of decisions that produced thetranslation, thereby training local decisions to op-timize the global BLEU score of the final trans-lation, while keeping the efficiency and simplic-ity of the Perceptron Algorithm (Rosenblatt, 1958;Collins, 2002).Our experiments show that UMT with DRL re-duces decoding time by over half, and the time torescore translations with the Language Model by6 times, while reaching accuracy non-significantlydifferent from the state of the art.2 Undirected Machine TranslationIn this section, we present the UMT frame-work.
For ease of presentation, and followingsynchronous-grammar based MT practice, we willhenceforth restrict our focus to binary grammars(Zhang et al., 2006; Wang et al., 2007).A UMT decoder can be formulated as a func-tion, f , that maps a source sentence, x ?
X , intoa structure defined by y = (t, h) ?
Y , where tis the translation in the target language, and his the synchronous tree structure generating theinput sentence on the source side and its trans-lation on the target side.
Synchronous-trees arecomposed of instantiations of synchronous-rules,r, from a grammar, G. A UMT decoder buildssynchronous-trees, h, by recursively expandingpartial synchronous-trees, ?
.
?
includes a partialtranslation.
Each ?
is required to be a connectedsub-graph of some synchronous-tree h. Thus, ?is composed of a subset of the rules from any hthat generates x on the source side, such that thereis a connected path between any two rules in ?
.Differently from the partial structures built by abottom-up decoder, ?
does not have to cover acontiguous span on x.
Formally, ?
is defined by:1) The set of synchronous-rule instantiations in ?
:I ?
{r1, r2, ?
?
?
, rk|ri?
G, 1 ?
i ?
k};2) The set of connections among the synchronous-rule instantiations, C .Let ci= (ri, rji) be the notation to represent theconnection between the i-th rule and the rule rji.The set of connections can be expressed as:C ?
{(r1, rj1), (r2, rj2), ?
?
?
, (rk?1, rjk?1)}3) The postcondition set, P , which specifiesthe non-terminals in ?
that are available forcreating new connections.
Each postcondition,pi= (rx,X y )i, indicates that the rule rx has thenon-terminal X y available for connections.
Theindex y identifies the non-terminal in the rule.
Ina binary grammar y can take only 3 values: 1 forthe first non-terminal (the left child of the sourceside), 2 for the second non-terminal, and h for thehead.
The postcondition set can be expressed as:P?
{(rx1,Xy1)1, ?
?
?
, (rxm,Xym)m}4) The set of carries, K .
We define a differentcarry, ?i, for each non-terminal available forconnections.
Each carry stores the extra infor-mation required to correctly score the non-localinteractions between ?
and the rule that will beconnected at that non-terminal.
Thus |K| = |P |.Let ?ibe the carry associated with the postcon-dition pi.
The set of carries can be expressed as:K ?
{?1, ?2, ?
?
?
, ?m}Partial synchronous-trees, ?
, are expanded byperforming connection-actions.
Given a ?
we canconnect to it a new rule, r?, using one available non-terminal represented by postcondition, pi?
P ,and obtain a new partial synchronous-tree ??
.
For-mally: ??
?
?
?
?
a?
?, where, a?
= [r?, pi],represents the connection-action.11Algorithm 1 UMT Decoding1: function Decoder (x; w, G) : (t,h)2: ?.
{I, C, P,K} ?
{?, ?, ?, ?}
;3: Q?
LeafRules(G);4: while |Q| > 0 do5: [r?, pi]?
PopBestAction (Q,w);6: ?
?
CreateConnection(?, r?
, pi);7: UpdateQueue(Q, r?, pi);8: end while9: Return(?
);10: procedure CreateConnection(?
, r?, pi) : ?
?11: ??
.I ?
?.I + r?
;12: ??
.C ?
?.C + (r?, rpi);13: ??
.P ?
?.P ?
pi;14: ??
.K ?
?.K ?
?i;15: ??
.K .UpdateCarries(r?, pi);16: ??
.P .AddAvailableConnectionsFrom(r?
, pi);17: ??
.K .AddCarriesForNewConnections(r?
, pi);18: Return(??
);19: procedure UpdateQueue( Q, r?, pi) :20: Q.RemoveActionsWith(pi);21: Q.AddNewActions(r?, pi);2.1 Decoding AlgorithmAlgorithm 1 gives details of the UMT decodingalgorithm.
The decoder takes as input the sourcesentence, x, the parameters of the scoring func-tion, w, and the synchronous-grammar, G. Atline 2 the partial synchronous-tree ?
is initializedby setting I , C , P and K to empty sets ?.
Atline 3 the queue of candidate connection-actionsis initialized as Q ?
{ [rleaf, null] | rleafis aleaf rule}, where null means that there is no post-condition specified, since the first rule does notneed to connect to anything.
A leaf rule rleafisany synchronous rule with only terminals on theright-hand sides.
At line 4 the main loop starts.Each iteration of the main loop will expand ?
us-ing one connection-action.
The loop ends whenQ is empty, implying that ?
covers the full sen-tence and has no more missing branches or par-ents.
The best scoring action according to theparameter vector w is popped from the queue atline 5.
The scoring of connection-actions is dis-cussed in details in Section 3.2.
At line 6 the se-lected connection-action is used to expand ?
.
Atline 7 the queue of candidates is updated accord-ingly (see lines 19-21).
At line 8 the decoder it-erates the main loop, until ?
is complete and isreturned at line 9.Lines 10-18 describe the CreateConnection(?
)procedure, that connects the partial synchronous-tree ?
to the selected rule r?
via the postcondi-tion pispecified by the candidate-action selectedin line 5.
This procedure returns the resulting par-tial synchronous-tree: ??
?
?
?
?
[r?, pi] ?.
Atline 11, r?
is added to the rule set I .
At line 12 theconnection between r?
and rpi(the rule specifiedin the postcondition) is added to the set of connec-tions C .
At line 13, piis removed from P .
Atline 14 the carry kimatching with piis removedfrom K .
At line 15 the set of carries K is updated,in order to update those carries that need to pro-vide information about the new action.
At line 16new postconditions representing the non-terminalsin r?
that are available for subsequent connectionsare added in P .
At line 17 the carries associatedwith these new postconditions are computed andadded to K .
Finally at line 18 the updated partialsynchronous-tree is returned.In the very first iteration, theCreateConnection(?)
procedure has nothingto compute for some lines.
Line 11 is not exe-cuted since the first leaf rule needs no connectionand has nothing to connect to.
lines 12-13 arenot executed since P and K are ?
and piis notspecified for the first action.
Line 15 is notexecuted since there are no carries to be updated.Lines 16-17 only add the postcondition and carryrelative to the leaf rule head link.The procedure used to update Q is reported inlines 19-21.
At line 20 all the connection-actionsinvolving the expansion of piare removed fromQ.
These actions are the incompatible alternativesto the selected action.
In the very first iteration,all actions in Q are removed because they are allincompatible with the connected-graph constraint.At line 21 new connection-actions are added toQ.
These are the candidate actions proposing aconnection to the available non-terminals of theselected action?s new rule r?.
The rules used forthese new candidate-actions must not be in con-flict with the current structure of ?
(e.g.
the rulecannot generate a source side terminal that is al-ready covered by ?
).123 Discriminative ReinforcementLearningTraining a UMT model simply means training theparameter vector w that is used to choose the bestscoring action during decoding.
We propose anovel method to apply a kind of minimum errorrate training (MERT) to w. Because each ac-tion choice must be evaluated in the context ofthe complete translation-derivation, we formalizethis method in terms of Reinforcement Learning.We propose Discriminative Reinforcement Learn-ing as an appropriate way to train a UMT model tomaximize the BLEU score of the complete deriva-tion.
First we define DRL as a novel generic train-ing framework.3.1 Generic Framework of DRLRL can be applied to any task, T , that can be for-malized in terms of:1) The set of states S1;2) A set of actions Asfor each state s ?
S;3) The transition function T : S ?
As?
S, thatspecifies the next state given a source state andperformed action2;4) The reward function, R : S ?As?
R;5) The discount factor, ?
?
[0, 1].A policy is defined as any map ?
: S ?
A. Itsvalue function is given by:Vpi(s0) =?
?i=0?iR(si, ?
(si)) (1)where path(s0|?)?
?s0, s1, ?
?
?
, s?|??
is the se-quence of states determined by following policy ?starting at state s0.
The Q-function is the total fu-ture reward of performing action a0in state s0andthen following policy ?
:Qpi(s0, a0) = R(s0, a0) + ?Vpi(s1) (2)Standard RL algorithms search for a policy thatmaximizes the given reward.Because we are taking a discriminative ap-proach to learn w, we formalize our optimizationtask similarly to an inverse reinforcement learningproblem (Ng and Russell, 2000): we are given in-formation about the optimal action sequence andwe want to learn a discriminative reward func-tion.
As in other discriminative approaches, this1S can be either finite or infinite.2For simplicity we describe a deterministic process.
Togeneralize to the stochastic process, replace the transitionfunction with the transition probability: Psa(s?
), s??
S.Algorithm 2 Discriminative RL1: function Trainer (?,T ,D ) : w2: repeat3: s?SampleState(S);4: a??
?w(s);5: a?
?SampleAction(As);6: if Qpiw(s, a?)
< Qpiw(s, a?)
in D then7: w?
w + ?w(s, a?)?
?w(s, a?
);8: end if9: until convergence10: Return(w);approach simplifies the task of learning the re-ward function in two respects: the learned rewardfunction only needs to be monotonically relatedto the true reward function, and this property onlyneeds to hold for the best competing alternatives.This is all we need in order to use the discrimina-tive reward function in an optimal classifier, andthis simplification makes learning easier in caseswhere the true reward function is too complicatedto model directly.In RL, an optimal policy ??
is one which, ateach state s, chooses the action which maximizesthe future reward Qpi?
(s, a).
We assume that thefuture discriminative reward can be approximatedwith a linear function ?Qpi(s, a) in some feature-vector representation ?
: S ?As?
Rd that mapsa state-action pair to a d-dimensional features vec-tor:?Qpi(s, a) = w ?
(s, a) (3)where w ?
Rd.
This gives us the following policy:?w(s) = argmaxa?Asw ?
(s, a) (4)The set of parameters of this policy is the vec-tor w. With this formalization, all we need tolearn is a vector w such that the resulting deci-sions are compatible with the given informationabout the optimal action sequence.
We propose aPerceptron-like algorithm to learn these parame-ters.Algorithm 2 describes the DRL meta-algorithm.The Trainer takes as input ?, the task T , and ageneric set of data D describing the behaviors wewant to learn.
The output is the weight vector wof the learned policy that fits the data D. The al-gorithm consists in a single training loop that isrepeated until convergence (lines 2-9).
At line 3a state, s, is sampled from S. At line 4, a?
is set to13be the action that would be preferred by the cur-rent w-policy.
At line 5 an action, a?, is sampledfrom Assuch that a?
6= a?.
At line 6 the algo-rithm checks if preferring path(T (s, a?
), ?w) overpath(T (s, a?
), ?w) is a correct choice accordingto the behaviors data D that the algorithm aims tolearn.
If the current w-policy contradicts D, line 7is executed to update the weight vector to promote?w(s, a?)
and penalize ?w(s, a?
), where ?w(s, a)is the summation of the features vectors of the en-tire derivation path starting at (s, a) and followingpolicy ?w.
This way of updating w has the ef-fect of increasing the ?Q(?)
value associated withall the actions in the sequence that generated thepromoted structure, and reducing the ?Q(?)
valueof the actions in the sequence that generated thepenalized structure3 .We have described the DRL meta-algorithm tobe as general as possible.
When applied to a spe-cific problem, more details can be specified: 1) itis possible to choose specific sampling techniquesto implement lines 3 and 5; 2) the test at line 6needs to be detailed according to the nature of Tand D; 3) the update statement at line 7 can be re-placed with a more sophisticated update approach.We address these issues and describe a range ofalternatives as we apply DRL to UMT in Section3.2.3.2 Application of DRL to UMTTo apply DRL we formalize the task of translatingx with UMT as T ?
{S, {As}, T,R, ?
}:1) The set of states S is the space of all possibleUMT partial synchronous-trees, ?
;2) The set A?,xis the set of connection-actionsthat can expand ?
connecting new synchronous-rule instantiations matching the input sentence xon the source side;3) The transition function T is the connectionfunction ??
?
?
?
?
a ?
formalized in Section 2and detailed by the procedure CreateConnection(?
)in Algorithm 1;4) The true reward function R is the BLEU score.BLEU is a loss function that quantifies the differ-ence between the reference translation and the out-put translation t. The BLEU score can be com-puted only when a terminal state is reached and afull translation is available.
Thus, the rewards areall zero except at terminal states, called a Pure De-3Preliminary experiments with updating only the featuresfor a?
and a?
produced substantially worse results.layed Reward function;5) Considering the nature of the problem and re-ward function, we choose an undiscounted setting:?
= 1.Next we specify the details of the DRL algo-rithm.
The data D consists of a set of pairs ofsentences, D ?
{(x, t?
)}, where x is the sourcesentence and t?
is the reference translation.
Thefeature-vector representation function ?
maps apair (?, a) to a real valued vector having any num-ber of dimensions.
Each dimension correspondsto a distinct feature function that maps: {?}
?A?,x?
R. Details of the features functions im-plemented for our model are given in Section 4.Each loop of the DRL algorithm analyzes a singlesample (x, t?)
?
D. The state s is sampled from auniform distribution over ?s0, s1, ?
?
?
, s?|??.
Theaction a?
is sampled from a Zipfian distributionover {A?,x?
a?}
sorted with the ?Qpiw(s, a) func-tion.
In this way actions with higher score havehigher probability to be drawn, while actions at thebottom of the rank still have a small probability tobe selected.
The if at line 6 tests if the translationproduced by path(T (s, a?
), ?w) has higher BLEUscore than the one produced by path(T (s, a?
), ?w).For the update statement at line 7 we usethe Averaged Perceptron technique (Freund andSchapire, 1999).
Algorithm 2 can be eas-ily adapted to implement the efficient AveragedPerceptron updates (e.g.
see Section 2.1.1 of(Daume?
III, 2006)).
In preliminary experiments,we found that other more aggressive update tech-nique, such as Passive-Aggressive (Crammer etal., 2006), Aggressive (Shen et al., 2007), orMIRA (Crammer and Singer, 2003), lead to worstaccuracy.
To see why this might be, consider thata MT decoder needs to learn to construct struc-tures (t, h), while the training data specifies thegold translation t?
but gives no information on thehidden-correspondence structure h. As discussedin (Liang et al., 2006), there are output structuresthat match the reference translation using a wronginternal structure (e.g.
assuming wrong internalalignment).
While in other cases the output trans-lation can be a valid alternative translation but getsa low BLEU score because it differs from t?.
Ag-gressively promoting/penalizing structures whosecorrectness can be only partially verified can beexpected to harm generalization ability.144 Undirected FeaturesIn this section we show how the features designedfor bottom-up HMT can be adapted to the undi-rected approach, and we introduce a new featurefrom the class of undirected features that are madepossible by the undirected approach.Local features depend only on the action rule r.These features can be used in the undirected ap-proach without adaptation, since they are indepen-dent of the surrounding structure.
For our experi-ments we use a standard set of local features: theprobability of the source phrase given the targetphrase; the lexical translation probabilities of thesource words given the target words; the lexicaltranslation probabilities of the target words giventhe source words; and the Word Penalty feature.Contextual features are dependent on the inter-action between the action rule r and the avail-able context.
In UMT all the needed informationabout the available context is stored in the carry?i.
Therefore, the computation of contextual fea-tures whose carry?s size is bounded (like the LM)requires constant time.The undirected adaptation of the LM featurecomputes the scores of the new n-grams formedby adding the terminals of the action rule r to thecurrent partial translation ?
.
In the case that theaction rule r is connected to ?
via a child non-terminal, the carry is expressed as ?i?
([WL?WR]).
Where WLand WRare respectively the leftand right boundary target words of the span cov-ered by ?
.
This notation is analogous to the stan-dard star notation used for the bottom-up decoder(e.g.
(Chiang, 2007) Section 5.3.2).
In the casethat r is connected to ?
via the head non-terminal,the carry is expressed as ?i?
(WR]-[WL).
WhereWLand WRare respectively the left and rightboundary target words of the surrounding contextprovided by ?
.
The boundary words stored in thecarry and the terminals of the action rule are all theinformation needed to compute and score the newn-grams generated by the connection-action.In addition, we introduce the Context-Free Fac-tor (CFF) features.
An action rule r is connectedto ?
via one of r?s non-terminals, Xr,?.
Thus, thescore of the interaction between r and the contextstructure attached to Xr,?can be computed ex-actly, while the score of the structures attached toother r nonterminals (i.e.
those in postconditions)cannot be computed since these branches are miss-ing.
Each of these postcondition nonterminalshas an associated CFF feature, which is an upperbound on the score of its missing branch.
Moreprecisely, it is an upper bound on the context-freecomponent of this score.
This upper bound can beexactly and efficiently computed using the ForestRescoring Framework (Huang and Chiang, 2007;Huang, 2008).
This framework separates the MTdecoding in two steps.
In the first step only thecontext-free factors are considered.
The output ofthe first step is a hypergraph called the context-free-forest, which compactly represents an expo-nential number of synchronous-trees.
The secondstep introduces contextual features by applying aprocess of state-splitting to the context-free-forest,rescoring with non-context-free factors, and effi-ciently pruning the search space.To efficiently compute CFF features we runthe Inside-Outside algorithm with the (max,+)semiring (Goodman, 1999) over the context-free-forest.
The result is a map that gives the maxi-mum Inside and Outside scores for each node inthe context-free forest.
This map is used to get thevalue of the CFF features in constant time whilerunning the forest rescoring step.5 ExperimentsWe implement our model on top of Cdec (Dyer etal., 2010).
Cdec provides a standard implemen-tation of the HMT decoder (Chiang, 2007) andMERT training (Och, 2003) that we use as base-line.We experiment on the NIST Chinese-Englishparallel corpus.
The training corpus contains239k sentence pairs with 6.9M Chinese words and8.9M English words.
The test set contains 919sentence pairs.
The hierarchical translation gram-mar was extracted using the Joshua toolkit (Li etal., 2009) implementation of the suffix array ruleextractor algorithm (Callison-Burch et al., 2005;Lopez, 2007).Table 1 reports the decoding time measures.HMT with beam1 is the fastest possible configu-ration for HMT, but it is 71.59% slower than UMT.This is because HMT b1 constructs O(n2) sub-trees, many of which end up not being used inthe final result, whereas UMT only constructs therule instantiations that are required.
HMT withbeam30 is the fastest configuration that reachesstate of the art accuracy, but increases the aver-age time per sentence by an additional 131.36%when compared with UMT.
The rescoring time is15Model sent.
t. sent.
t. var.
resc.
t. resc.
t. var.UMT 135.2ms - 38.9 ms -HMT b1 232.0ms +71.59% 141.3 ms +263.23%HMT b30 312.8ms +131.36% 226.9 ms +483.29%Table 1: Decoding speed comparison.Model sent.
t. sent.
t. var.UMT with DRL 267.4 ms -HMT b1 765.2 ms +186.16%HMT b30 1153.5 ms +331.37%Table 2: Training speed comparison.Model BLEU relative loss p-valueUMT with DRL 30.14 6.33% 0.18HMT b1 30.87 4.07% 0.21HMT b30 32.18 - -Table 3: Accuracy comparison.the average time spent on the forest rescoring step,which is the only step where the decoders actu-ally differ.
This is the step that involves the inte-gration of the Language Model and other contex-tual features.
For HMT b30, rescoring takes twothirds of the total decoding time.
Thus rescoringis the most time consuming step in the pipeline.The rescoring time comparison shows even biggergains for UMT.
HMT b30 is almost 6 times slowerthan UMT.Table 2 reports the training time measures.These results show HMT b30 training is morethan 4 times slower than UMT training with DRL.Comparing with Table 1, we notice that the rela-tive gain on average training time is higher thanthe gain measured at decoding time.
This is be-cause MERT has an higher complexity than DRL.Both of the training algorithms requires 10 train-ing epochs to reach convergence.Table 3 reports the accuracy measures.
As ex-pected, accuracy degrades the more aggressivelythe search space is pruned.
UMT trained withDRL loses 2.0 BLEU points compared to HMTb30.
This corresponds to a relative-loss of 6.33%.Although not inconsequential, this variation isnot considered big (e.g.
at the WMT-11 Ma-chine Translation shared task (Callison-Burch etal., 2011)).
To measure the significance of thevariation, we compute the sign test and measurethe one-tail p-value for the presented models incomparison to HMT b30.
From the values re-ported in the fourth column, we can observe thatthe BLEU score variations would not normally beconsidered significant.
For example, at WMT-11two systems were considered equivalent if p >0.1, as in these cases.
The accuracy cannot becompared in terms of search score since the mod-els we are comparing are trained with distinct al-gorithms and thus the search scores are not com-parable.To test the impact of the CFF features, wetrained and tested UMT with DRL with and with-out these features.
This resulted in an accuracy de-crease of 2.3 BLEU points.
Thus these features areimportant for the success of the greedy approach.They provide an estimate of the score of the miss-ing branches, thus helping to avoid some actionsthat have a good local score but lead to final trans-lations with low global score.To validate the results, additional experimentswere executed on the French to Italian portionof the Europarl corpus v6.
This portion contains190k pairs of sentences.
The first 186k sentenceswere used to extract the grammar and train the twomodels.
The final tests were performed on the re-maining 4k sentence pairs.
With this corpus wemeasured a similar speed gain.
HMT b30 is 2.3times slower at decoding compared to UMT, and6.1 times slower at rescoring, while UMT loses1.1 BLEU points in accuracy.
But again the ac-curacy differences are not considered significant.We measured a p-value of 0.25, which is not sig-nificant at the 0.1 level.6 Related WorkModels sharing similar intuitions have been pre-viously applied to other structure prediction tasks.For example, Nivre et al.
(2006) presents a lineartime syntactic dependency parser, which is con-strained in a left-to-right decoding order.
Thismodel offers a different accuracy/complexity bal-ance than the quadratic time graph-based parser ofMcdonald et al.
(2005).Other approaches learning a model specificallyfor greedy decoding have been applied with suc-16cess to other less complex tasks.
Shen et al.
(2007)present the Guided Learning (GL) framework forbidirectional sequence classification.
GL success-fully combines the tasks of learning the order ofinference and training the local classifier in a sin-gle Perceptron-like algorithm, reaching state of theart accuracy with complexity lower than the ex-haustive counterpart (Collins, 2002).Goldberg and Elhadad (2010) present a simi-lar training approach for a Dependency Parser thatbuilds the tree-structure by recursively creatingthe easiest arc in a non-directional manner.
Thismodel also integrates the tasks of learning the or-der of inference and training the parser in a singlePerceptron.
By ?non-directional?
they mean theremoval of the constraint of scanning the sentencefrom left to right, which is typical of shift-reducemodels.
However this algorithm still builds thetree structures in a bottom-up fashion.
This modelhas a O(n log n) decoding complexity and accu-racy performance close to the O(n2) graph-basedparsers (Mcdonald et al., 2005).Similarities can be found between DRL and pre-vious work that applies discriminative training tostructured prediction: Collins and Roark (2004)present an Incremental Parser trained with the Per-ceptron algorithm.
Their approach is specific todependency parsing and requires a function to testexact match of tree structures to trigger parameterupdates.
On the other hand, DRL can be applied toany structured prediction task and can handle anykind of reward function.
LASO (Daume?
III andMarcu, 2005; Daume?
III et al., 2005) and SEARN(Daume?
III et al., 2009; Daume?
III et al., 2006)are generic frameworks for discriminative trainingfor structured prediction: LASO requires a func-tion that tests correctness of partial structures totrigger early updates, while SEARN requires anoptimal policy to initialize the learning algorithm.Such a test function or optimal policy cannot becomputed for tasks such as MT where the hiddencorrespondence structure h is not provided in thetraining data.7 Discussion and Future WorkIn general, we believe that greedy-discriminativesolutions are promising for tasks like MT, wherethere is not a single correct solution: normallythere are many correct ways to translate the samesentence, and for each correct translation thereare many different derivation-trees generating thattranslation, and each correct derivation tree can bebuilt greedily following different inference orders.Therefore, the set of correct decoding paths is areasonable portion of UMT?s search space, givinga well-designed greedy algorithm a chance to finda good translation even without beam search.In order to directly evaluate the impact of ourproposed decoding strategy, in this paper the onlynovel features that we consider are the CFF fea-tures.
But to take full advantage of the powerof discriminative training and the lower decodingcomplexity, it would be possible to vastly increasethe number of features.
The UMT?s undirected na-ture allows the integration of non-bottom-up con-textual features, which cannot be used by stan-dard HMT and PbMT.
And the use of a history-based model allows features from an arbitrarilywide context, since the model does not need to befactorized.
Exploring the impact of this advantageis left for future work.8 ConclusionThe main contribution of this work is the pro-posal of a new MT model that offers an accu-racy/complexity balance that was previously un-available among the choices of hierarchical mod-els.We have presented the first Undirected frame-work for MT.
This model combines advantagesgiven by the use of hierarchical synchronous-grammars with a more efficient decoding algo-rithm.
UMT?s nature allows us to design novelundirected features that better approximate con-textual features (such as the LM), and to introducea new class of undirected features that cannot beused by standard bottom-up decoders.
Further-more, we generalize the training algorithm intoa generic Discriminative Reinforcement Learningmeta-algorithm that can be applied to any struc-tured prediction task.ReferencesPeter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19:263?311.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longer17phrases.
In ACL ?05: Proceedings of the 43rd Con-ference of the Association for Computational Lin-guistics, Ann Arbor, MI, USA.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In WMT ?11:Proceedings of the 6th Workshop on Statistical Ma-chine Translation, Edinburgh, Scotland.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In ACL?05: Proceedings of the 43rd Conference of the As-sociation for Computational Linguistics, Ann Arbor,MI, USA.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In ACL ?04:Proceedings of the 42rd Conference of the Associa-tion for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP ?02:Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing, Philadel-phia, PA, USA.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, andYoram Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research,7:551?585.Hal Daume?
III and Daniel Marcu.
2005.
Learningas search optimization: approximate large marginmethods for structured prediction.
In ICML ?05:Proceedings of the 22nd International Conferenceon Machine Learning, Bonn, Germany.Hal Daume?
III, John Langford, and Daniel Marcu.2005.
Search-based structured prediction as clas-sification.
In ASLTSP ?05: Proceedings of theNIPS Workshop on Advances in Structured Learn-ing for Text and Speech Processing, Whistler, BritishColumbia, Canada.Hal Daume?
III, John Langford, and Daniel Marcu.2006.
Searn in practice.
Technical report.Hal Daume?
III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
Submit-ted to Machine Learning Journal.Hal Daume?
III.
2006.
Practical structured learningtechniques for natural language processing.
Ph.D.thesis, University of Southern California.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-delman, Phil Blunsom, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In ACL ?10: Proceedings of the ACL 2010 SystemDemonstrations, Uppsala, Sweden.Yoav Freund and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algorithm.Machine Learning, 37(3):277?296.Andrea Gesmundo and James Henderson.
2011.Heuristic Search for Non-Bottom-Up Tree StructurePrediction.
In EMNLP ?11: Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, Edinburgh, Scotland, UK.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In NAACL ?10: Proceedings of the11th Conference of the North American Chapter ofthe Association for Computational Linguistics, LosAngeles, CA, USA.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25:573?605.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In IWPT ?05: Proceedings of the 9th Inter-national Workshop on Parsing Technology, Vancou-ver, British Columbia, Canada.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In ACL ?07: Proceedings of the 45th Confer-ence of the Association for Computational Linguis-tics, Prague, Czech Republic.Liang Huang.
2008.
Forest-based algorithms in natu-ral language processing.
Ph.D. thesis, University ofPennsylvania.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 4th Conference ofthe North American Chapter of the Association forComputational Linguistics, Edmonton, Canada.Zhifei Li, Chris Callison-Burch, Chris Dyer, San-jeev Khudanpur, Lane Schwartz, Wren Thornton,Jonathan Weese, and Omar Zaidan.
2009.
Joshua:An open source toolkit for parsing-based machinetranslation.
In WMT ?09: Proceedings of the4th Workshop on Statistical Machine Translation,Athens, Greece.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrimina-tive approach to machine translation.
In COLING-ACL ?06: Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thConference of the Association for ComputationalLinguistics, Sydney, Australia.18Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In EMNLP-CoNLL ?07:Proceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, Prague,Czech Republic.Ryan Mcdonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In ACL ?05: Proceedings of the43rd Conference of the Association for Computa-tional Linguistics, Ann Arbor, MI, USA.Andrew Y. Ng and Stuart Russell.
2000.
Algorithmsfor inverse reinforcement learning.
In ICML ?00:Proceedings of the 17th International Conference onMachine Learning, Stanford University, CA, USA.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In LREC ?06: Proceedings ofthe 5th International Conference on Language Re-sources and Evaluation, Genoa, Italy.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Pro-ceedings of the 41st Conference of the Associationfor Computational Linguistics, Sapporo, Japan.Frank Rosenblatt.
1958.
The Perceptron: A proba-bilistic model for information storage and organiza-tion in the brain.
Psychological Review, 65(6):386?408.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classifi-cation.
In ACL ?07: Proceedings of the 45th Confer-ence of the Association for Computational Linguis-tics, Prague, Czech Republic.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing syntax trees to improve syntax-based ma-chine translation accuracy.
In EMNLP-CoNLL ?07:Proceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, Prague,Czech Republic.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In ACL ?01: Pro-ceedings of the 39th Conference of the Associationfor Computational Linguistics, Toulouse, France.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In NAACL ?06: Proceedings of the7th Conference of the North American Chapter ofthe Association for Computational Linguistics, NewYork, New York.19
