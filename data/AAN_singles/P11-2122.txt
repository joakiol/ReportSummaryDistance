Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 693?698,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUsing Derivation Trees for Treebank Error DetectionSeth Kulick and Ann Bies and Justin MottLinguistic Data ConsortiumUniversity of Pennsylvania3600 Market Street, Suite 810Philadelphia, PA 19104{skulick,bies,jmott}@ldc.upenn.eduAbstractThis work introduces a new approach tochecking treebank consistency.
Derivationtrees based on a variant of Tree AdjoiningGrammar are used to compare the annotationof word sequences based on their structuralsimilarity.
This overcomes the problems ofearlier approaches based on using strings ofwords rather than tree structure to identify theappropriate contexts for comparison.
We re-port on the result of applying this approachto the Penn Arabic Treebank and how this ap-proach leads to high precision of error detec-tion.1 IntroductionThe internal consistency of the annotation in a tree-bank is crucial in order to provide reliable trainingand testing data for parsers and linguistic research.Treebank annotation, consisting of syntactic struc-ture with words as the terminals, is by its naturemore complex and thus more prone to error thanother annotation tasks, such as part-of-speech tag-ging.
Recent work has therefore focused on the im-portance of detecting errors in the treebank (Greenand Manning, 2010), and methods for finding sucherrors automatically, e.g.
(Dickinson and Meur-ers, 2003b; Boyd et al, 2007; Kato and Matsubara,2010).We present here a new approach to this problemthat builds upon Dickinson and Meurers (2003b), byintegrating the perspective on treebank consistencychecking and search in Kulick and Bies (2010).
Theapproach in Dickinson andMeurers (2003b) has cer-tain limitations and complications that are inher-ent in examining only strings of words.
To over-come these problems, we recast the search as one ofsearching for inconsistently-used elementary trees ina Tree Adjoining Grammar-based form of the tree-bank.
This allows consistency checking to be basedon structural locality instead of n-grams, resulting inimproved precision of finding inconsistent treebankannotation, allowing for the correction of such in-consistencies in future work.2 Background and Motivation2.1 Previous Work - DECCAThe basic idea behind the work in (Dickinson andMeurers, 2003a; Dickinson and Meurers, 2003b) isthat strings occurring more than once in a corpusmay occur with different ?labels?
(taken to be con-stituent node labels), and such differences in labelsmight be the manifestation of an annotation error.Adopting their terminology, a ?variation nucleus?
isthe string of words with a difference in the annota-tion (label), while a ?variation n-gram?
is a largerstring containing the variation nucleus.
(1) a.
(NP the (ADJP mostimportant) points)b.
(NP the most important points)For example, suppose the pair of phrases in (1)are taken from two different sentences in a cor-pus.
The ?variation nucleus?
is the string mostimportant, and the larger surrounding n-gramis the string the most important points.This is an example of error in the corpus, since thesecond annotation is incorrect, and this differencemanifests itself by the nucleus having in (a) the labelADJP but in (b) the default label NIL (meaning fortheir system that the nucleus has no covering node).Dickinson and Meurers (2003b) propose a ?non-693fringe heuristic?, which considers two variation nu-clei to have a comparable context if they are prop-erly contained within the same variation n-gram -i.e., there is at least one word of the n-gram on bothsides of the nucleus.
For the the pair in (1), the twoinstances of the variation nucleus satisfy the non-fringe heuristic because they are properly containedwithin the identical variation n-gram (with the andpoints on either side).
See Dickinson and Meur-ers (2003b) for details.
This work forms the basisfor the DECCA system.12.2 Motivation for Our Approach(2) a. NPqmpsummitNP$rmSharmNPAl$yxthe Sheikhb.
NPqmpsummitNP$rmSharmAl$yxthe Sheikhc.
NPqmpsummitNPNP$rmSharmAl$yxthe SheikhNP( mSrEgypt)We motivate our approach by illustrating the lim-itations of the DECCA approach.
Consider the trees(2a) and (2b), taken from two instances of the three-word sequence qmp $rm Al$yx in the ArabicTreebank.2 There is no need to look at any surround-ing annotation to conclude that there is an incon-sistency in the annotation of this sequence.3 How-ever, based on (2ab), the DECCA system would noteven identify the three-word sequence qmp $rmAl$yx as a nucleus to compare, because both in-stances have a NP covering node, and so are consid-ered to have the same label.
(The same is true forthe two-word subsequence $rm Al$yx.
)Instead of doing the natural comparison of the1http://www.decca.osu.edu/.2In Section 4 we give the details of the corpus.
We use theBuckwalter Arabic transliteration scheme (Buckwalter, 2004).3While the nature of the inconsistency is not the issue here,(b) is the correct annotation.inconsistent structures for the identical word se-quences as in (2ab), the DECCA approach wouldinstead focus on the single word Al$yx, which hasa NP label in (2a), while it has the default labelNIL in (2b).
However, whether it is reported as avariation depends on the irrelevant fact of whetherthe word to the right of Al$yx is the same in bothinstances, thus allowing it to pass the non-fringeheuristic (since it already has the same word, $rm,on the left).Consider now the two trees (2bc).
There is anadditional NP level in (2c) because of the adjunct( mSr ), causing qmp $rm Al$yx to have nocovering node, and so have the default label NIL,and therefore categorized as a variation compared to(2b).
However, this is a spurious difference, sincethe label difference is caused only by the irrelevantpresence of an adjunct, and it is clear, without look-ing at any further structure, that the annotation ofqmp $rm Al$yx is identical in (2bc).
In this casethe ?non-fringe heuristic?
serves to avoid report-ing such spurious differences, since if qmp $rmAl$yx did not have an open parenthesis on the rightin (b), and qmp did not have the same word to itsimmediate left in both (b) and (c), the two instanceswould not be surrounded by the same larger varia-tion n-gram, and so would not pass the non-fringeheuristic.This reliance on irrelevant material arises from us-ing on a single node label to characterize a struc-tural annotation and the surrounding word contextto overcome the resulting complications.
Our ap-proach instead directly compares the annotations ofinterest.3 Using Derivation Tree FragmentsWe utilize ideas from the long line of Tree AdjoiningGrammar-based research (Joshi and Schabes, 1997),based on working with small ?elementary trees?
(ab-breviated ?etrees?
in the rest of this paper) that arethe ?building blocks?
of the full trees of a treebank.This decomposition of the full tree into etrees alsoresults in a ?derivation tree?
that records how the el-ementary trees relate to each other.We illustrate the basics of TAG-based deriva-tion we are using with examples based on thetrees in (2).
Our grammar is a TAG variant with694qmpsummit#c1S:1.2NPNP^#c2M:1,right#c4 NPmSrEgyptqmpsummit#a1S:1.2NPNP^$rmSharm#a2S:1.2NPNP^Al$yxThe SheikhNP#a3qmpsummit#b1S:1.2NPNP^$rmSharm#b2NPAl$yxThe SheikhFor (2a)                    For (2b)                For (2c)A:1.1,left#b3NPAl$yxThe SheikhA:1.1,left#c3$rmSharmFigure 1: Etrees and Derivation Trees for (2abc).tree-substitution, sister-adjunction, and Chomsky-adjunction (Chiang, 2003).
Sister adjunction at-taches a tree (or single node) as a sister to anothernode, and Chomsky-adjunction forms a recursivestructure as well, duplicating a node.
As typicallydone, we use head rules to decompose a full tree andextract the etrees.
The three derivation trees, corre-sponding to (2abc), are shown in Figure 1.Consider first the derivation tree for (2a).
It hasthree etrees, numbered a1, a2, a3, which are thenodes in the derivation tree which show how thethree etrees connect to each other.
This derivationtree consists of just tree substitutions.
The ?
sym-bol at node NP?
in a1 indicates that it is a sub-stitution node, and the S:1.2 above a2 indicatesthat it substitutes into node at Gorn address 1.2 intree a1 (i.e., the substitution node), and likewisefor a3 substituting into a2.
The derivation tree for(2b) also has three etrees, although the structureis different.
Because the lower NP is flat in (2b),the rightmost noun, Al$yx, is taken as the headof the etree b2, with the degenerate tree for $rmsister-adjoining to the left of Al$yx, as indicatedby the A:1.1,left.
The derivation tree for (2c)is identical to that of (2b), except that it has theadditional tree c4 for the adjunct mSr, which rightChomsky-adjoins to the root of c2, as indicated bythe M:1,right.44We leave out the irrelevant (here) details of the parenthesesThis tree decomposition and resulting derivationtree provide us with the tool for comparing nucleiwithout the interfering effects from words not in thenucleus.
We are interested not in the derivation treefor an entire sentence, but rather only that slice ofit having etrees with words that are in the nucleusbeing examined, which we call the derivation treefragment.
That is, for a given nucleus being exam-ined, we partition its instances based on the coveringnode in the full tree, and within each set of instanceswe compare the derivation tree fragments for eachinstance.
These derivation tree fragments are therelevant structures to compare for inconsistent an-notation, and are computed separately for each in-stance of each nucleus from the full derivation treethat each instance is part of.5For example, for comparing our three instancesof qmp $rm Al$yx, the three derivation tree frag-ments would be the structures consisting of (a1, a2,a3), (b1, b2, b3) and (c1, c2, c3), along with theirconnecting Gorn addresses and attachment types.This indicates that the instances (2ab) have differ-ent internal structures (without the need to look at asurrounding context), while the instances (2bc) haveidentical internal structures (allowing us to abstractaway from the interfering effects of adjunction).Space prevents full discussion here, but the etreesand derivation trees as just described require refine-ment to be truly appropriate for comparing nuclei.The reason is that etrees might encode more infor-mation than is relevant for many comparisons of nu-clei.
For example, a verb might appear in a corpuswith different labels for its objects, such as NP orSBAR, etc., and this would lead to its having dif-ferent etrees, differing in their node label for thesubstitution node.
If the nucleus under compari-son includes the verb but not any words from thecomplement, the inclusion of the different substi-tution nodes would cause irrelevant differences forthat particular nucleus comparison.We solve these problems by mapping down thein the derivation tree.5A related approach is taken by Kato and Matsubara (2010),who compare partial parse trees for different instances of thesame sequence of words in a corpus, resulting in rules based ona synchronous Tree Substitution Grammar (Eisner, 2003).
Wesuspect that there are some major differences between our ap-proaches regarding such issues as the representation of adjuncts,but we leave such a comparison for future work.695System nuclei n-grams instancesDECCA 24,319 1,158,342 2,966,274Us 54,496 not used 605,906Table 1: Data examined by the two systems for the ATBSystem nuclei non-duplicate types offound nuclei found inconsistencyDECCA 4,140 unknown unknownUs-internal 9,984 4,272 1,911Table 2: Annotation inconsistencies reported for the ATBrepresentation of the etrees in a derivation tree frag-ment to form a ?reduced?
derivation tree fragment.These reductions are (automatically) done for eachnucleus comparison in a way that is appropriate forthat particular nucleus comparison.
A particularetree may be reduced in one way for one nucleus,and then a different way for a different nucleus.
Thisis done for each etree in a derivation tree fragment.4 Results on Test CorpusGreen and Manning (2010) discuss annotation con-sistency in the Penn Arabic Treebank (ATB), and forour test corpus we follow their discussion and usethe same data set, the training section of three partsof the ATB (Maamouri et al, 2008a; Maamouri etal., 2009; Maamouri et al, 2008b).
Their work isideal for us, since they used the DECCA algorithmfor the consistency evaluation.
They did not use the?non-fringe?
heuristic, but instead manually exam-ined a sample of 100 nuclei to determine whetherthey were annotation errors.4.1 Inconsistencies ReportedThe corpus consists of 598,000 tokens.
Table 1 com-pares token manipulation by the two systems.
TheDECCA system6 identified 24,319 distinct variationnuclei, while our system had 54,496.
DECCA ex-amined 1,158,342 n-grams, consisting of 2,966,2746We worked at first with version 0.2 of the software.
How-ever this software does not implement the non-fringe heuristicand does not make available the actual instances of the nucleithat were found.
We therefore re-implemented the algorithmto make these features available, being careful to exactly matchour output against the released DECCA system as far as the nu-clei and n-grams found.instances (i.e., different corpus positions of the n-grams), while our system examined 605,906 in-stances of the 54,496 nuclei.
For our system, thenumber of nuclei increases and the variation n-grams are eliminated.
This is because all nuclei withmore than one instance are evaluated, in order tosearch for constituents that have the same root butdifferent internal structure.The number of reported inconsistencies is shownin Table 2.
DECCA identified 4,140 nuclei as likelyerrors - i.e., contained in larger n-grams, satisfyingthe non-fringe heuristic.
Our system identified 9,984nuclei as having inconsistent annotation - i.e., withat least two instances with different derivation treefragments.4.2 Eliminating Duplicate NucleiSome of these 9,984 nuclei are however redundant,due to nuclei contained within larger nuclei, such as$rm Al$yx inside qmp $rm Al$yx in (2abc).Eliminating such duplicates is not just a simple mat-ter of string inclusion, since the larger nucleus cansometimes reveal different annotation inconsisten-cies than just those in the smaller substring nucleus,and also a single nucleus string can be included indifferent larger nuclei.
We cannot discuss here thefull details of our solution, but it basically consistsof two steps.First, as a result of the analysis described so far,for each nucleus we have a mapping of each instanceof that nucleus to a derivation tree fragment.
Sec-ond, we test for each possible redundancy (meaningstring inclusion) whether there is a true structural re-dundancy by testing for an isomorphism between themappings for two nuclei.
For this test corpus, elimi-nating such duplicates leaves 4,272 nuclei as havinginconsistent annotation.
It is unknown how manyof the DECCA nuclei are duplicates, although manycertainly are.
For example, qmp $rm Al$yx and$rm Al$yx are reported as separate results.4.3 Grouping Inconsistencies by StructureAcross all variation nuclei, there are only a finitenumber of derivation tree fragments and thus waysin which such fragments indicate an annotation in-consistency.
We categorize each annotation incon-sistency by the inconsistency type, which is simplya set of numbers representing the different derivation696tree fragments.
We can then present the results notby listing each nucleus string, but instead by the in-consistency types, with each type having some num-ber of nuclei associated with it.For example, instances of $rm Al$yx mighthave just the derivation tree fragments (a2, a3) and(b2, b3) in Figure 1, and the numbers representingthis pair is the ?inconsistency type?
for this (nucleus,internal context) inconsistency.
There are nine othernuclei reported as having an inconsistency based onthe exact same derivation tree fragments (abstractingonly away from the particular lexical items), and soall these nuclei are grouped together as having thesame ?inconsistency type?.
This grouping results inthe 4,272 non-duplicate nuclei found being groupedinto 1,911 inconsistency types.4.4 Precision and RecallThe grouping of internal checking results by incon-sistency types is a qualitative improvement in con-sistency reporting, with a high precision.7 By view-ing inconsistencies by structural annotation types,we can examine large numbers of nuclei at a time.Of the first 10 different types of derivation tree in-consistencies, which include 266 different nuclei, all10 appear to real cases of annotation inconsistency,and the same seems to hold for each of the nuclei inthose 10 types, although we have not checked everysingle nucleus.
For comparison, we chose a sampleof 100 nuclei output by DECCA on this same data,and by our judgment the DECCA precision is about74%, including 15 duplicates.Measuring recall is tricky, even using the errorsidentified in Green and Manning (2010) as ?gold?errors.
One factor is that a system might report avariation nucleus, but still not report all the relevantinstances of that nucleus.
For example, while bothsystems report $rm Al$yx as a sequence with in-consistent annotation, DECCA only reports the twoinstances that pass the ?non-fringe heuristic?, whileour system lists 132 instances of $rm Al$yx, parti-tioning them into the two derivation tree fragments.We will be carrying out a careful accounting of therecall evaluation in future work.7?Precision?
here means the percentage of reported varia-tions that are actually annotation errors.5 Future WorkWhile we continue the evaluation work, our pri-mary concern now is to use the reported inconsistentderivation tree fragments to correct the annotationinconsistencies in the actual data, and then evaluatethe effect of the corpus corrections on parsing.
Oursystem groups all instances of a nucleus into differ-ent derivation tree fragments, and it would be easyenough for an annotator to specify which is correct(or perhaps instead derive this automatically basedon frequencies).However, because the derivation trees and etreesare somewhat abstracted from the actual trees in thetreebank, it can be challenging to automatically cor-rect the structure in every location to reflect the cor-rect derivation tree fragment.
This is because of de-tails concerning the surrounding structure and theinteraction with annotation style guidelines such ashaving only one level of recursive modification ordifferences in constituent bracketing depending onwhether a constituent is a ?single-word?
or not.
Weare focusing on accounting for these issues in cur-rent work to allow such automatic correction.AcknowledgmentsWe thank the computational linguistics group at theUniversity of Pennsylvania for helpful feedback ona presentation of an earlier version of this work.We also thank Spence Green and Chris Manningfor supplying the data used in their analysis of thePenn Arabic Treebank.
This work was supportedin part by the Defense Advanced Research ProjectsAgency, GALE Program Grant No.
HR0011-06-1-0003 (all authors) and by the GALE program,DARPA/CMO Contract No.
HR0011-06-C-0022(first author).
The content of this paper does notnecessarily reflect the position or the policy of theGovernment, and no official endorsement should beinferred.ReferencesAdriane Boyd, Markus Dickinson, and Detmar Meurers.2007.
Increasing the recall of corpus annotation er-ror detection.
In Proceedings of the Sixth Workshopon Treebanks and Linguistic Theories (TLT 2007),Bergen, Norway.697Tim Buckwalter.
2004.
Buckwalter Arabic morphologi-cal analyzer version 2.0.
Linguistic Data ConsortiumLDC2004L02.David Chiang.
2003.
Statistical parsing with an auto-matically extracted tree adjoining grammar.
In DataOriented Parsing.
CSLI.Markus Dickinson and Detmar Meurers.
2003a.
Detect-ing errors in part-of-speech annotation.
In Proceed-ings of the 10th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL-03), pages 107?114, Budapest, Hungary.Markus Dickinson and Detmar Meurers.
2003b.
Detect-ing inconsistencies in treebanks.
In Proceedings of theSecond Workshop on Treebanks and Linguistic The-ories (TLT 2003), Sweden.
Treebanks and LinguisticTheories.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In The Companion Vol-ume to the Proceedings of 41st Annual Meeting ofthe Association for Computational Linguistics, pages205?208, Sapporo, Japan, July.
Association for Com-putational Linguistics.Spence Green and Christopher D. Manning.
2010.
Bet-ter Arabic parsing: Baselines, evaluations, and anal-ysis.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (Coling 2010),pages 394?402, Beijing, China, August.
Coling 2010Organizing Committee.A.K.
Joshi and Y. Schabes.
1997.
Tree-adjoining gram-mars.
In G. Rozenberg and A. Salomaa, editors,Handbook of Formal Languages, Volume 3: BeyondWords, pages 69?124.
Springer, New York.Yoshihide Kato and Shigeki Matsubara.
2010.
Correct-ing errors in a treebank based on synchronous tree sub-stitution grammar.
In Proceedings of the ACL 2010Conference Short Papers, pages 74?79, Uppsala, Swe-den, July.
Association for Computational Linguistics.Seth Kulick and Ann Bies.
2010.
A TAG-deriveddatabase for treebank search and parser analysis.
InTAG+10: The 10th International Conference on TreeAdjoining Grammars and Related Formalisms, Yale.Mohamed Maamouri, Ann Bies, Seth Kulick, FatmaGaddeche, Wigdan Mekki, Sondos Krouna, andBasma Bouziri.
2008a.
Arabic treebank part 1 - v4.0.Linguistic Data Consortium LDC2008E61, December4.Mohamed Maamouri, Ann Bies, Seth Kulick, FatmaGaddeche, Wigdan Mekki, Sondos Krouna, andBasma Bouziri.
2008b.
Arabic treebank part 3 - v3.0.Linguistic Data Consortium LDC2008E22, August 20.Mohamed Maamouri, Ann Bies, Seth Kulick, FatmaGaddeche, Wigdan Mekki, Sondos Krouna, andBasma Bouziri.
2009.
Arabic treebank part 2- v3.0.Linguistic Data Consortium LDC2008E62, January20.698
