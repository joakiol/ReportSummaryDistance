Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161?1171,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLexical Event Ordering with an Edge-Factored ModelOmri Abend, Shay B. Cohen and Mark SteedmanSchool of Informatics, University of EdinburghEdinburgh EH8 9AB, United Kingdom{oabend,scohen,steedman}@inf.ed.ac.ukAbstractExtensive lexical knowledge is necessary fortemporal analysis and planning tasks.
We ad-dress in this paper a lexical setting that al-lows for the straightforward incorporation ofrich features and structural constraints.
We ex-plore a lexical event ordering task, namely de-termining the likely temporal order of eventsbased solely on the identity of their predi-cates and arguments.
We propose an ?edge-factored?
model for the task that decomposesover the edges of the event graph.
We learnit using the structured perceptron.
As lexi-cal tasks require large amounts of text, we donot attempt manual annotation and instead usethe textual order of events in a domain wherethis order is aligned with their temporal order,namely cooking recipes.1 IntroductionTemporal relations between events are often im-plicit, and inferring them relies on lexical and worldknowledge about the likely order of events.
For in-stance, to execute the instruction ?fry the onion,?
thehearer should probably obtain oil beforehand, evenif not instructed so explicitly.
Lexical knowledgeabout the likely order of events is therefore neces-sary for any semantic task that requires temporal rea-soning or planning, such as classifying temporal re-lations (Mani et al, 2006; Lapata and Lascarides,2006; Yoshikawa et al, 2009; D?Souza and Ng,2013; Mirza and Tonelli, 2014, inter alia), textualentailment (Dagan et al, 2013) or temporal infor-mation extraction (Ling and Weld, 2010).
Lexicaltemporal knowledge is further important for model-ing grammatical phenomena such as tense and as-pect (Steedman, 2002).In this paper we address the task of lexical eventordering, namely predicting the ordering of eventsbased only on the identity of the words compris-ing their predicates and arguments.
Concretely, thetask is to predict the order of an unordered set ofpredicate-argument structures.
Predicting the likelyorder of event types is a step towards more in-tricate planning and reasoning scenarios (see ?3),and is useful in itself for tasks such as concept-to-text generation (Reiter et al, 2000), or in val-idating the correctness of instruction sets.
A re-lated idea can be found in modeling sentence coher-ence (Lapata, 2003; Barzilay and Lapata, 2008, in-ter alia), although here we focus on lexical relationsbetween events, rather than coherence relations be-tween complete sentences.Compiling a resource of temporal tendencies be-tween events can hardly be done manually, given thenumber and wealth of phenomena that have to beaccounted for.
Temporally annotated corpora, oftenannotated according to TimeML principles (Puste-jovsky et al, 2003), are a useful resource for study-ing temporal relations.
However, due to incurredcosts, annotated corpora are too small for most lexi-cal tasks.
For instance, the TimeML annotated dataused in the latest TempEval shared task containsonly 100K words or so (UzZaman et al, 2013).Previous work that does not rely on manuallyannotated data has had some success in discover-ing temporal lexical relations between predicates(Chklovski and Pantel, 2004; Chambers and Juraf-sky, 2008b; Talukdar et al, 2012).
However, de-spite their appeal, these methods have mostly fo-1161cused on inducing simple event types, consisting ofsingle words (e.g., ?buy-own?)
or fixed expressions,and are hard to extend to include rich features (e.g.,order-based and pattern-based features).
Further-more, measuring recall without annotated data is no-toriously difficult, and evaluation is often precision-based or extrinsic.We take a graph-based structured prediction ap-proach to the task, motivated by the flexibility it al-lows in incorporating various feature sets and con-straints.
We use an edge-factored model, which de-composes over the edges in the graph of events com-prising the recipe (?4).
We estimate the model us-ing the structured perceptron algorithm.
We com-pare the structured perceptron approach to an ap-proximate greedy baseline and to a locally normal-ized model reminiscent of common approaches fororder learning, obtaining superior results (?8).
Thelearning algorithm is of potential use in other or-dering tasks such as machine translation reordering(Tromble and Eisner, 2009).We focus on domains in which the order of eventsin the text is aligned with their temporal order.
Bydoing so we avoid the costly and error-prone manualannotation of temporal relations by using the textualorder of recipes to approximate their temporal or-der.1Specifically, we address the cooking recipesdomain, which we motivate in ?2.In summary, the contribution of this paper isthree-fold: (1) we explore the task of lexical eventordering and means for its evaluation; (2) we presentan edge-factored model for the task, and show it canbe used to predict the order of events well (77.7%according to standard measures for ordering evalua-tion); (3) we present a method for extracting eventsand create a dataset of ordered events using recipesextracted from the web.2 Related WorkTemporal semantics is receiving increasing attentionin recent years.
Lexical features are in frequent useand rely in most part on external resources whichare either manually compiled or automatically in-duced.
The line of work most closely related toours focuses on inducing lexical relations between1See Cassidy et al (2014) for a discussion of inter-annotatoragreement in TimeML-based schemes.event types.
Most work has been unsupervised, of-ten using pattern-based approaches relying on man-ually crafted (Chklovski and Pantel, 2004) or in-duced patterns (Davidov et al, 2007), that corre-late with temporal relations (e.g., temporal discourseconnectives).
Talukdar et al (2012) uses the textualorder of events in Wikipedia biographical articlesto induce lexical information.
We use both textualorder and discourse connectives to define our fea-ture set, and explore a setting which allows for thestraightforward incorporation of additional features.Chambers and Jurafsky (2008b; 2009) addressedthe unsupervised induction of partially ordered eventchains (or schema) in the news domain, centeredaround a common protagonist.
One of their evalu-ation scenarios tackles a binary classification relatedto event ordering, and seeks to distinguish orderedsets of events from randomly permuted ones, yield-ing an accuracy of 75%.
Manshadi et al (2008) usedlanguage models to learn event sequences and con-ducted a similar evaluation on weblogs with about65% accuracy.
The classification task we explorehere is considerably more complex (see ?8).The task of script knowledge induction has beenfrequently addressed in recent years.
Balasubrama-nian et al (2013) and Pichotta and Mooney (2014)extended Chambers and Jurafsky?s model to includeevents that have multiple arguments.
Jans et al(2012) use skip-grams to capture event-event rela-tions between not necessarily consecutive events.Regneri et al (2010) constructed a temporal lexi-cal knowledge base through crowd-sourcing.
Theirapproach is appealing as it greatly reduces the costsincurred by manual annotation and can potentiallybe used in conjunction with lexical information ob-tained from raw text.
Modi and Titov (2014) jointlylearns the stereotypical order of events and their dis-tributional representation, in order to capture para-phrased instances of the same event type.
Frermannet al (2014) models the joint task of inducing eventparaphrases and their order using a Bayesian frame-work.
All latter three works evaluated their inducedtemporal ordering knowledge on a binary predic-tion of whether a temporal relation between a pairof (not necessarily related) events holds, and not onthe prediction of a complete permutation given anunordered event set as in this work.
Their evalua-tion was conducted on 30 event pairs manually an-1162notated through crowd-sourcing, where Modi andTitov (2014) further included an evaluation on alarge set of pairs automatically extracted from theGigaword corpus.The appeal of the cooking domain for studyingvarious semantic phenomena has been recognizedby several studies in NLP and AI (Tasse and Smith,2008; Bollini et al, 2013; Cimiano et al, 2013; Reg-neri et al, 2013; Malmaud et al, 2014).
The domainis here motivated by several considerations.
First,recipes mostly describe concrete actions, rather thanabstract relations, which are less relevant to tem-poral ordering.
Second, from a practical point ofview, many recipes are available online in computer-readable format.
Third, the restrictiveness of thecooking domain can also be seen as an advantage, asit can reveal major conceptual challenges raised bythe task, without introducing additional confounds.3 Temporally Ordering Lexical EventsWe formalize our task as follows.
Let U be a setof event types, namely actions or states (representedas predicates) and objects which these actions oper-ate on (represented as arguments to the predicates;mostly ingredients or kitchenware).
Formally, eache ?
U is a tuple ?a, c1, .
.
.
, cn?
where a is the mainverb or predicate describing the event (such as ?stir?or ?mix?)
and c1, .
.
.
, cnis a list of arguments thatthe predicate takes (e.g., ?salt?
or ?spoon?).
Twoadditional marked events, s and f , correspond to?start?
and ?finish?
events.
A recipe is a sequenceof events in U , starting at s and ending at f .Given a recipe R = ?e1, ..., em?, we wish to pre-dict the order of the events just from the (multi)set{ei}mi=1.
In this work we use the textual order ofevents to approximate their temporal order (see, e.g.,Talukdar et al (2012) for a similar assumption).The validity of this assumption for cooking recipesis supported in ?6.Figure 1 gives an example of a set of events ex-tracted from our dataset for the dish ?Apple CrispAla [sic] Brigitte.?
Lexical information places quitea few limitations on the order of this recipe.
Forinstance, in most cases serving is carried out at theend while putting the ingredients in is done prior tobaking them.
However, lexical knowledge in itselfis unlikely to predict the exact ordering of the eventsas given in the recipe (e.g., spreading butter mightbe done before or after baking).One of the major obstacles in tackling planningproblems in AI is the knowledge bottleneck.
Lexi-cal event ordering is therefore a step towards moreambitious goals in planning.
For instance, temporalrelations may be used to induce planning operators(Mour?ao et al, 2012), which can in turn be used togenerate a plan (recipe) given a specified goal andan initial set of ingredients.4 Model, Inference and LearningIn this section we describe the main learning compo-nents that compose our approach to event ordering.4.1 Edge-Factored Model for Event OrderingWe hereby detail the linear model we use for or-dering events.
Let S = {e1, .
.
.
, em} ?
U bea set of events as mentioned in ?3.
Let G(S) =(S ?
{s, f}, E(S)) be an almost-complete directedgraph withE(S) = (S?{s})?(S?
{f}) ?
U?U .Every Hamiltonian path2inG(S) that starts in s andends in f defines an ordering of the events in S.The edge (ei, ej) in such a path denotes that eiisthe event that comes before ej.The modeling problem is to score Hamiltonianpaths in a given directed graph G(S).
Here, we usean edge-factored model.
Let ?
: (U ?
U) ?
Rdbea feature function for pairs of events, represented asdirected edges.
In addition, let ?
?
Rdbe a weightvector.
We define the score of a Hamiltonian pathh = (h1, .
.
.
, hm+1) (hi?
E(S)) as:score(h|S) =m+1?i=1?>?
(hi) (1)Given a weight vector ?
and a set of events S, in-ference is carried out by computing the highest scor-ing Hamiltonian path in G(S):h?= arg maxh?H(S)score(h|S) (2)where H(S) is the set of Hamiltonian paths inG(S)that start with s and end with f .
The path h?is thebest temporal ordering of the set of events S accord-ing to the model in Eq.
1 with weight vector ?.2A path in a graph that visits all nodes exactly once.1163(a)e1= ?butter, dish?e2= ?put, apples,water, ...flour, cinnamon, it?e3= ?mix,with spoon, ?e4= ?spread, butter, salt, ...over mix?e5= ?bake,F?e6= ?serve, cream, cream?
(b) Butter a deep baking dish, putapples, water, flour, sugar and cin-namon in it.
Mix with spoon andspread butter and salt over the ap-ple mix.
Bake at 350 degrees F untilthe apples are tender and the crustbrown, about 30 minutes.
Servewith cream or whipped cream.
(c)(a)e1 = hbutter, dishie2 = hput, apples,water, ...flour, cinnamon, itie3 = hmix, spoon, ie4 = hspread, butter, salt,mixie5 = hbake,Fie6 = hserve, cream, creami (b) Butter a deep baking dish,put apples, water, flour, sugarand cinnamon in it.
Mix withspoon and spread butter and saltover the apple mix.
Bake at350 degrees F until the applesare tender and the crust brown,about 30 minutes.
Serve withcream or whipped cream.s e1 e2e3 e4e5e6 f (c)(a)e1 = hmix, ?, tarragon, vinegarie2 = hblend, ?,mustardie3 = hmix, ?, salt, pepperie4 = hblend, ?,mayonnaise, sour creamie5 = hcover, ?ie6 = hchill, ?i (b) you mix the tarragonand vinegar together andblend in the mustard.you mix in the salt andpepper, blending well.you blend in the mayon-naise and then the sourcream.
you cover andchill.
s e1 e2e3 e4e5e6 eFigure 1: (a) Example of events describing a recipe for the dish ?.?
(b) The actual recipe for this dish.
(c) A completegraph over the set of events with start and end states.
Each internal node in the graph is one of the events ei fori 2 {1, .
.
.
, 5}.
The path in bold denotes the correct Hamiltonian path describing the set of actions that need to betaken to follow the recipe.3 Model, Inference and LearningIn this section we describe the main learning compo-nents that compose our approach to event ordering.3.1 Edge-Factored Model for Event OrderingWe now turn to explain the linear model we use forordering events in time.
Let S = {v1, .
.
.
, vm} ?
Ube a set of events as mentioned in section 2.
LetG(S) = (S [ {s, e}, E(S)) be an almost-completedirected graph withE(S) = (S[{s})?
(S[{e}) ?
(U ?
U).
Every Hamiltonian path5 in G(S) thatstarts in s and ends in e can be thought of as an or-dering of the events in S. The edge (vi, vj) in sucha path denotes that vi is the event that comes beforevj .The modeling problem, therefore, is to scoreHamiltonian paths in a given directed graph G(S)such as the above.
Here, we use an edge-factoredmodel.
Let   : (U ?
U) !
Rd be a featurevector for pairs of events, represented as directededges.
In addition, let ?
2 Rd be a weight vec-tor.
Then, we define the score of an Hamiltonianpath h = (h1, .
.
.
, hm+1) (where hi 2 E(S) fori 2 {1, .
.
.
,m + 1}) as:5An Hamiltonian path in a graph is a path that visits allnodes exactly once.score(h|S) = m+1Xi=1?> (hi) (1)Given a weight vector w and a set of events S, in-ference is carried out by computing the highest scor-ing Hamiltonian path in G(S):h?
= arg maxh2H(S)score(h|S) (2)where H(S) is the set of Hamiltonian paths in G(S)that start with s and end with e. h?
is the best tem-poral ordering of the set of events S according to thestructured model in Equation 1 with weight vectorw.3.2 InferenceAs mentioned above, inference with the edge-factored model we presented would have to solve themaximization problem in Eq.
2.
This correspondsto finding an Hamiltonian path in a complete graph,which is generally an NP-hard problem6.
In the gen-eral case there is no reasonable approximation algo-rithm to solve the maximization algorithm, although6The NP complete problem of finding a Hamiltonian cyclein an undirected graph can be trivially reduced to finding themaximal Hamiltonian cycle in a directed graph.Figure 1: (a) Example of events describing a recipe for the dish ?Apple Crisp Ala [sic] Brigitte.?
For brevity, argumentsare represented as headwords and their syntactic type is omitted.
(b) The actual recipe for this dish.
(c) A completegraph over the set of events with start and end states.
Each internal node in the graph is one of the events ei fori 2 {1, .
.
.
, 6}.
The path in blue bold denotes the correct Hamiltonian path describing the set of actions as ordered inthe recipe.
Red edges denote edges from the start state a d to the end state.
The edges, in practice, are weighted.ILP formulation yields superior performance to theother evaluated systems (?8).
ILP has been proven tobe a practical and flexible tool in various structuredprediction tasks in NLP (Roth and tau Yih, 2007;Talukdar et al, 2012; Scaria et al, 2013).
Our ILPformulation is given in Appendix A.We experiment with an additional greedy infer-ence algorithm, similar to the one described by La-pata (2003) for sentence ordering.
The algorithm it-eratively selects an outgoing edge (starting from thenode s) that has the largest weight to a node that hasnot been visited so far, until all vertices are covered,at which point the path terminates by travelling to f .4.3 LearningThe learning problem takes as input a dataset con-sisting of unordered sets of events, paired with a tar-get ordering.
We consider two types of learning al-gorithms for the edge-factored model in the previoussection.
The first learns in a global training settingusing the averaged structured perceptron (Collins,2002), with the decoding algorithm being either theone based on ILP (henceforth, GLOBAL-PRC), orthe greedy one (GREEDY-PRC).The second learning algorithm we try is basedon factored training.
This algorithm maximizes thelikelihood of a conditional log-linear model p:p(e2|e1, ?, S) = exp  ?> (e1, e2) Z(?, S, e1)Z(?, S, e1) = Xe : (e1,e)2E(S) exp  ?> (e1, e) where e1, e2 2 S [ {s, f}.
This is a locally normal-ized log-linear model that gives the probability oftransitioning to node e2 from node e1.
Maximizingthe score in Eq.
1 has an interpretation of finding thehighest scoring path according to an edge-factoredMarkovian model, such that:p(h|?, S) = m+1Yi=2 p(ei|ei 1, ?, S),where h = (h1, .
.
.
, hm+1) is a Hamiltonian pathwith hi = (ei 1, ei) being a directed edge inthe path.
Initial experimentation suggested thatgreedy inference (henceforth, GREEDY-LOGLIN)works better in practice than the ILP formulationfor the locally-normalized model.
We therefore donot report results on global inference with this log-linear model.
GREEDY-LOGLIN closely resemblesthe learning model of Lapata (2003), except that it isa discriminative log-linear model, rather of a gener-ative Markovian model.5 The Feature SetTable 1 presents all the complete set of features usedfor defining the feature function  .
We considerthree sets of features: Lexical encodes the writ-ten forms of the event pair predicates and objects;Figure 1: (a) The sequence of events representing the recipe for the dish ?Apple Crisp Ala [sic] Brigitte.?
(b) The actual recipefor this dish.
(c) A complete graph over the set of events with start and finish states.
Each internal node in the graph is one of theevents eifor i ?
{1, .
.
.
, 6}.
The path in blue bold denotes the correct Hamiltonian path describing the set of actions as ordered inthe recipe.
Red edges denot edges from the start state and to the end state.
The edges, in practice, are weighted.4.2 InferenceAs mentioned above, inference with the edge-factored model requires solving he maximizationproblem in Eq.
2.
This corresponds to finding aHamiltonian path in a complete graph, which is gen-erally an NP-hard problem.
Reasonable approxima-tions for this problem are also NP-hard.
Still tec -niques are developed for specialized cases, due tothe problem?s importance in discrete optimization.Despite its the retical NP-hardness, this maxi-mization problem can be repr sented as an IntegerLinear Program (ILP), and then solved using generictechniques for ILP optimizatio .
Due to the rela-tively short length of recipes (13.8 vents on averagein our corpus), th probl m can be effectively solvedin most cases.The proposed algorithmic setting is appealing forits flexibility.
The linear score formulation allowsus to use rich features, while using ILP allows toeasily incorporate structural constraints.
Indeed, ILPhas been proven valuable in various NLP tasks (Rothand Yih, 2007; Talukdar et al, 2012; Scaria al.,2013).
See Appendix A for our ILP formulation.As a baseline, we experiment with an additionalgreedy inference algorithm, similar to the one de-scribed by Lapata (2003) for sentence ordering.
healgorithm iteratively selects an outgoing edge (start-ing from the node s) that has the largest weight toa node that has not been visited so far, until all ver-tices are covered, at which point the path terminatesby traveling to f .4.3 LearningThe learning problem takes as input a dataset con-sisting of unordered sets of events, paired with a tar-get ordering.
We c nsider two types of learning al-gorithms for the edge-factored model in the previoussection.
The first learns in a global training settingusing the averaged structured perceptron (Collins,200 ), with the decoding algorithm being either theone based on ILP (henceforth, GLOBAL-PRC), orthe greedy one (GREEDY-PRC).
Given a training in-stance S and its correct label hc, the structured per-ceptron calls the inference procedure as a subroutineand updates the weight vector ?
according to the dif-ference between the value of the feature function onthe predicted path (?h??
(hi)) and on the correctpath (?hc?
(hi)).The second learning algorithm we try is basedon factored training.
This algorithm maximizes thelikelihood of a conditional log-linear model p:p(e2|e1, ?, S) =exp(?>?
(e1, e2))Z(?, S, e1)Z(?, S, e1) =?e : (e1,e)?E(S)exp(?>?
(e1, e))where e1, e2?
S ?
{s, f}.
This is a locally normal-ized log-linear model that gives the probability oftransitioning to node e2from node e1.
Maximizingthe score in Eq.
1 has an interpretation of finding thehighest scoring path according to an edge-factoredMarkovian model, such that:1164p(h|?, S) =m+1?i=1p(ei|ei?1, ?, S),where h = (h1, .
.
.
, hm+1) is a Hamiltonian pathwith hi= (ei?1, ei) being a directed edge inthe path.
Initial experimentation suggested thatgreedy inference (henceforth, GREEDY-LOGLIN)works better in practice than the ILP formulation forthe locally-normalized model.
We therefore do notreport results on global inference with this log-linearmodel.
We suspect that greedy inference works bet-ter with the log-linear model because it is trainedlocally, while the perceptron algorithm includes aglobal inference step in its training, and thereforebetter matches global decoding.GREEDY-LOGLIN closely resembles the learn-ing model of Lapata (2003), as both are first-order Markovian and use the same (greedy) in-ference procedure.
Lapata?s model differs fromGREEDY-LOGLIN in being a generative model,where each event is a tuple of features, and the tran-sition probability between events is defined as theproduct of transition probabilities between featurepairs.
GREEDY-LOGLIN is discriminative, so to bemaximally comparable to the presented model.5 The Feature SetTable 1 presents the complete set of features.
Weconsider three sets of features: Lexical encodes thewritten forms of the event pair predicates and ob-jects; Brown uses Brown clusters (Brown et al,1992) to encode similar information, but allows gen-eralization between distributionally similar words;and Frequency encodes the empirical distribution oftemporally-related phenomena.The feature definitions make use of several func-tions.
For brevity, we sometimes say that an evente is (a, c1) if e?s predicate is a and its first argu-ment is c1, disregarding its other arguments.
Let Cbe a reference corpus of recipes for collecting statis-tics.
The function B(w) gives the Brown cluster ofa word w, as determined by clustering C into 50clusters {1, .
.
.
, 50}.
The function ORD(a, c) re-turns the mean ordinal number of an (a, c) event inC.
The ordinal number of the event eiin a recipe(e1, ..., em) is defined as i?m2.Template ExampleLexical(a1, a2) (fry, add)(a1, c21) (fry, oil)(a2, c11) (onions, add)(c11, c21) (onions, oil)Brown(B(a1),B(a2)) (1,3)?
(k, k?)
?
K. (5,4) : 2|{(c1i, c2j) : B(c1i) = k,B(c2j) = k?
}|?k ?
K. |{c2i: B(c2i) = k}| 12 : 1B(a2) 5Frequency?` ?
L. log(+ P`[(a2, c21)|(a1, c11)]) -2.3?` ?
L. PMI`((a1, c11), (a2, c21)) 3.1?` ?
L. PMI`((a2, c21), (a1, c11)) -2.0ORD(a2, c21) 3.2Table 1: Feature templates used for computing ?.
Thetemplates operate on two events ?a1, c11, .
.
.
, c1m1?
and?a2, c21, .
.
.
, c2m2?.
B(w) maps a word w to its Brown clus-ter in K = {1, .
.
.
, 50}.
ORD(e) returns the mean ordinalvalue of e. Feature templates which start with ?
stand for mul-tiple features, one for each element in the set quantified over.Non-numerical feature templates correspond to binary features.E.g., (fry, add) as an instance of (a1, a2) is a binary feature in-dicating the appearance of an event with a1= fry on one endof the edge and an event with a2= add on the other end of it. = 10?3in our experiments.
See text for elaboration.We further encode the tendency of two events toappear with temporal discourse connectives, such as?before?
or ?until.?
We define a linkage betweentwo events as a triplet (e1, e2, `) ?
(U ?
U ?
L),where L is the set of linkage types, defined accord-ing to their marker?s written form.
?6 details theextraction process of linkages from recipes.
We fur-ther include a special linkage type linear based onthe order of events in the text, and consider everypair of events e1and e2that follow one another in arecipe as linked under this linkage type.For each linkage type ` ?
L, we define an empir-ical probability distribution P`((a, c1), (a?, c?1)) =P ((a, c1), (a?, c?1)|`), based on simple counting.
Thefunction PMI gives the point-wise mutual informa-tion of two events and is defined as:PMI`((a, c), (a?, c?))
= log(P`((a, c1), (a?, c?1))P`(a, c1) ?
P`(a?, c?1))Frequency-based features encode the empiricalestimate of the probabilities that various pairs of fea-tures would occur one after the other or linked with adiscourse marker.
They are equivalent to using prob-abilities extracted from maximum likelihood estima-1165tion according to a bigram model in the discrimi-native learning.
While some of this information isimplicitly found in the lexical features, collectingfrequency counts from a large training set is muchquicker than running costly structured optimization.Rather the discriminative training can weigh the dif-ferent empirical probabilities according to their dis-criminative power.
Indeed we find that these featuresare important in practice and can result in high ac-curacy even after training on a small training set.6 The Recipe DatasetData and Preprocessing.
The data is extractedfrom a recipe repository found on the web.3Therecipes are given as free text.
To extract event typeswe run the Stanford CoreNLP4pipeline of a to-kenizer, POS tagger, a lexical constituency parser(the englishPCFG parsing model) and extract typedStanford dependencies (de Marneffe and Manning,2008).
As is common with web extractions, therecipes contain occasional spelling, grammaticaland formatting errors.
The corpus consists of 139files, 73484 recipes, 1.02M events (13.8 events perrecipe on average) and 11.05M words.5Event Extraction.
We focus on verbal events anddo not extract nominal and adjectival argumentstructures, which are not as well supported by cur-rent parsing technology.
Any verb is taken to definean event, aside from modal verbs, auxiliaries andsecondary verbs.
A secondary verb (e.g., ?let,?
?be-gin?)
does not describe an action in its own right,but rather modifies an event introduced by anotherverb.
We identify these verbs heuristically using alist given in Dixon (2005, p. 490?491) and a fewsimple rules defined over parse trees.
E.g., from thesentence ?you should begin to chop the onion,?
weextract a single event with a predicate ?chop.?
Ar-guments are taken to be the immediate dependentsof the predicate that have an argument dependencytype (such as direct or indirect objects) according tothe extracted Stanford dependencies.
For preposi-tional phrases, we include the preposition as part of3http://www.ffts.com/recipes.htm4http://nlp.stanford.edu/software/corenlp.shtml5Links to the original recipes, the preprocessed recipes andall extracted events can be found in http://homepages.inf.ed.ac.uk/oabend/event_order.html.the argument.
Argument indices are determined bytheir order in the text.
The order of events is takento be the order of their verbs in the text.Linkage Extraction.
We focus on a subset of link-age relations, which are relevant for temporal rela-tions.
We use Pitler and Nenkova?s (2009) explicitdiscourse connectives classifier to identify temporaldiscourse linkers, discarding all other discourse link-ers.
Once a discourse linker has been detected, weheuristically extract its arguments (namely the pairof verbs it links) according to a deterministic ex-traction rule defined over the parse tree.
We find28 distinct connectives in our training set, wherethe 5 most common linkers ?until,?
?then,?
?before,??when?
and ?as?
cover over 95% of the instances.We extract 36756 such linkages from the corpus, 0.5linkages per recipe on average.Temporal and Textual Ordering.
In order toconfirm that temporal and textual order of recipesare generally in agreement, we manually exam-ine the first 20 recipes in our development set.One recipe was excluded as noise6, resulting in 19recipes and 353 events.
We identify the sources ofmisalignment between the linear order and the tem-poral order of the events.713 events (3.7%) did nothave any clear temporal orderings.
These consistedof mostly negations and modalities (e.g., ?do notoverbrown!?
), sub-section headings (e.g., ?Prepara-tion?)
or other general statements that do not consti-tute actions or states.
For the remaining 340 events,we compare their linear and the temporal orderings.We estimate the frequency of sub-sequences thatcontradict the temporal order and confirm that theyoccur only infrequently.
We find that most disagree-ments fall into these two categories: (1) disjunctionsbetween several events, only one of which will actu-ally take place (e.g., ?roll Springerle pin over dough,or press mold into top?
); (2) a pair, or less com-monly a triplet, of events are expressed in reverseorder.
For instance, ?place on greased and flouredcookie sheet,?
where greasing and flouring shouldoccur before the placing action.
We note that assum-ing the alignment of the temporal and textual order6This did not result from an extraction problem, but ratherfrom the recipe text itself being too noisy to interpret.7Events are parsed manually so to avoid confounding theresults with the parser?s performance.1166of recipes does not suggest that the textual order isthe only order of events that would yield the sameoutcome.We compute the Kendall?s Tau correlation, astandard measure for information ordering (Lapata,2006), between the temporal and linear orderingsfor each recipe.
In cases of several events thathappen simultaneously (including disjunctions), wetake their ordinals to be equal.
For instance, for threeevents where the last two happen at the same time,we take their ordering to be (1,2,2) in our analysis.We find that indeed temporal and textual orderingsare in very high agreement, with 6 recipes of the19 perfectly aligned.
The average Kendall?s Tau be-tween the temporal ordering and the linear one is0.924.7 Experimental SetupEvaluation.
We compute the accuracy of our algo-rithms by comparing the predicted order to the onein which the events are written.
We first computethe number of exact matches, denoted with EXACT,namely the percentage of recipes in which the pre-dicted and the textual orders are the same.For a more detailed analysis of imperfect pre-dictions, we compute the agreement between sub-sequences of the orderings.
We borrow the notion ofa ?concordant pair?
from the definition of Kendall?sTau and generalize it to capture agreement of longersub-sequences.
Two k-tuples of integers (x1, ..., xk)and (y1, ..., yk) are said to ?agree in order?
if for ev-ery 1 ?
i < j ?
k, xi< xjiff yi< yj.
Given twoorderings of the same recipe O1= (e?
(1), ..., e?
(m))and O2= (e?
(1), ..., e?
(m)) (where ?
and ?
are per-mutations over [m] = {1, .
.
.
,m}) and given a se-quence of k monotonically increasing indices t =(i1, ..., ik), t is said to be a ?concordant k-tuple?
ofO1andO2if (?
(i1), ..., ?
(ik)) and (?
(i1), ..., ?
(ik))agree in order, as defined above.Denote the unordered recipes of the test data as{Ri}Ni=1, where Ri= {ei1, ..., eimi} ?
U for all i,and their target orderings ?
= {?i}Ni=1, where ?iisa permutation over [mi].
Assume we wish to eval-uate a set of predicted orderings for this test dataT = {?i}Ni=1, where again ?iis a permutation over[mi].
Denote the number of concordant k-tuples of?iand ?ias conc(?i, ?i).
The total number of ofmonotonically increasing k-tuples of indices is(mik).The k-wise (micro-averaged) accuracy of T with re-spect to ?
is:acck(?,T) =?Ni=1conc(?i, ?i)?Ni=1(mik)Any k-tuples containing the start node s or theend node f are excluded, as their ordering is triv-ial.
Recipes of length less than k are discarded whencomputing acck.
A micro-averaged accuracy mea-sure is used so as not to disproportionately weighshort recipes.
However, in order to allow com-parison to mean Kendall?s Tau, commonly used inworks on order learning, we further report a macro-averaged acc2by computing acc2for each recipeseparately, and taking the average of resulting ac-curacy levels.
Average Kendall?s Tau can now becomputed by 2acc2?1 for the macro-averaged acc2score.Data.
We randomly partition the text into train-ing, test and development sets, taking an 80-10-10percent split.
We do not partition the individualfiles so as to avoid statistical artifacts introduced byrecipe duplications or near-duplications.
The train-ing, development and test sets contain 58038, 7667and 7779 recipes respectively.
The total number offeature template instantiations in the training data is8.94M.Baselines and Algorithms.
We compare threelearning algorithms.
GLOBAL-PRC is the struc-tured perceptron algorithm that uses ILP inference.GREEDY-PRC is a structured perceptron in which in-ference is done greedily.
GREEDY-LOGLIN is thelocally normalized log-linear model with greedy in-ference.
RANDOM randomly (uniformly) selects apermutation of the recipe?s events.Experimental Settings.
The structured percep-tron algorithms, GLOBAL-PRC and GREEDY-PRC,are run with a learning rate of 0.1 for 3 iterations.To avoid exceedingly long runs, we set a time limitin seconds ?
on the running time of each ILP in-ference stage used in GLOBAL-PRC.
We considertwo training scenarios: 4K, which trains on thefirst 4K recipes of the training set, and 58K, whichtrains on the full training data of 58K recipes.
InGLOBAL-PRC we set ?
to be 30 seconds for the 4K1167Alg.
acc2acc3acc4EXACTMI MA4KGLOBAL-PRC 71.2 77.7 44.7 27.9 35.1GREEDY-PRC 60.8 68.0 30.6 15.0 20.4GREEDY-LOGLIN 65.6 71.5 35.8 18.7 21.058KGLOBAL-PRC 68.9 76.4 41.3 24.8 34.4GREEDY-PRC 60.7 67.8 30.6 15.2 20.5GREEDY-LOGLIN 66.3 72.4 36.6 19.4 21.3RANDOM 50.0 51.2 16.7 4.2 0.5Table 2: Accuracy of the different models on the test data inpercents.
Columns correspond to evaluation measures, namelyaccuracy of sub-sequences of lengths 2 (micro and macro aver-ages), 3 and 4, and exact match.
The upper (lower) part presentsresults for a training set of 4K (58K) samples.
GLOBAL-PRC isrun with ?
= 30 for 4K and with ?
= 5 for 58K.
In 58K,all models are run with the Full feature set.
In 4K, follow-ing prior experimentation on the development set, we select thebest performing feature sets (Full for GREEDY-LOGLIN andGREEDY-PRC; Fr + Lex for GLOBAL-PRC).scenario, and 5 seconds in the 58K scenario.
Thenumber of threads was limited to 3.
Where the timelimit is reached before an optimal solution is found,the highest scoring Hamiltonian path found up tothat point is returned by the ILP solver.
In the in-frequent samples where no feasible solution is foundduring training, the sample is skipped over, while attest time, we perform greedy inference instead.We define the following feature sets.
Fr includesonly features of class Frequency, while Fr + Lexincludes features from both the Frequency andLexical categories.
Full includes all feature sets.All above feature sets take C, the reference corpusfor computing FREQUENCY features, to be the en-tire 58K training samples in both scenarios.
In the4K scenario, we also experiment with FrLim, whichincludes all features, but takes C to contain only the4K samples of the training data.We use the Gurobi package for ILP.8Brown clus-ters are extracted from the 58K samples of the train-ing data using Liang?s implementation.9The convexlog-likelihood function of GREEDY-LOGLIN is op-timized using LBFGS.
All features are selected andall parameters are tuned using the development set.8 ResultsTable 2 presents the results of the three major al-gorithms in the two main scenarios 58K and 4K.8http://www.gurobi.com9https://github.com/percyliang/brown-cluster?
Set acc2acc3acc4EXACTMI MA30 FrLim 55.9 61.5 21.6 6.9 8.2Fr 68.7 75.9 40.6 23.9 31.7Fr + Le 68.9 76.2 40.7 23.8 32.1Full 68.4 76.0 39.9 23.1 31.85 FrLim 55.1 60.9 20.9 6.5 8.2Fr 65.9 74.2 36.0 19.3 30.4Fr + Le 66.2 74.3 36.8 20.4 30.7Full 66.3 74.5 36.9 20.4 30.4Table 3: The performance of GLOBAL-PRC on the develop-ment set in various settings (4K scenario).
Columns correspondto evaluation measures, namely accuracy of sub-sequences oflengths 2 (micro and macro averages), 3 and 4, and exact match.The upper (lower) part of the table presents results for a timelimit of ?=30 (5).
Fr includes the Frequency features esti-mated on 58K recipes.
Fr + Le further includes Lexical fea-tures.
Full includes all features.
FrLim includes all features,where Frequency features are estimated only on 4K recipes.We find that the structured perceptron algorithm,GLOBAL-PRC, obtains the best results in both casesand under all evaluation measures.
The importanceof global optimization was also stressed in otherworks on event ordering (Chambers and Jurafsky,2008a; Talukdar et al, 2012).In order to assess the contribution of the differentcomponents of the model of the best scoring model,GLOBAL-PRC, we compare the performance of thedifferent feature sets and settings of ?
on the de-velopment set in 4K (Table 3).
Results reveal thestrong impact of the Frequency feature set on theresults.
Using this category set alne (Fr) yieldsslightly lower results than using the full feature set,while estimating the Frequency features on a smallcorpus (FrLim) lowers results dramatically.
AddingLexical and Brown features yields a small improve-ment over using Frequency alone.While Table 3 demonstrates the importance of ?in the performance of GLOBAL-PRC, it also showsthat on a limited time budget, a small training set andfew features (4K, Fr) and a reasonably small ?
(5)can yield competitive results.
Increasing ?
from 5 to30 generally improves results by 2 to 3 percent ab-solute.
The importance of ?
is further demonstratedin Table 2, where performance with 4K training in-stances and ?
= 30 is better than with 58K traininginstances and ?
= 5.
Preliminary experiments con-ducted on the development data with higher valuesof ?
of 60 and 120 suggest that further increasing ?1168yields no further improvement.Previous studies evaluated their models on the re-lated problem of distinguishing randomly permutedand correctly ordered chains of events (?2).
In thispaper we generalize this task to complete event or-dering.
In order to demonstrate the relative difficultyof the tasks, we apply our highest scoring model(4K, Fr + Le) to the binary task (without re-trainingit).
We do so by computing the percentage of casesin which the correct ordering obtains a higher scorethan an average ordering.
The high resulting accu-racy of 93%, as opposed to considerably lower accu-racies obtained under ordering evaluation measures,reflects the relative difficulty of the tasks.The proposed edge-factored model can easily cap-ture pair-wise ordering relations between events, butis more limited in accounting for relations betweenlarger sets of events.
A simple way of doing so isby adding the feature?eP (ei|e)P (e|ej) betweenevents eiand ej(in addition to the regular transi-tion probabilities P (ei|ej)).
However, preliminaryexperimentation with this technique did not yieldimproved performance.
Future work will addresshigher-order models that straightforwardly accountfor such long-distance dependencies.To qualitatively assess what generalizations arelearned by the model, we apply GLOBAL-PRC tothe development data and look at what event pairsobtained either particularly high or particularly lowresults.
For each pair of predicates and their firstarguments (a1,c11), (a2,c21), we compute the averageweight of an edge connecting events of these types,discarding pairs of frequency less than 20.The 20 highest scoring edges contain pairs suchas (?add,?
?mixing after addition?
), (?beat whites,?
?fold into mixture?)
and (?cover for minutes,??cook?
), in addition to a few noisy pairs resultingfrom parser errors.
The 20 lowest scoring edges con-tain event pairs that are likely to appear in the oppo-site order.
11 of the cases include as a first argu-ment the predicates ?serve,?
?cool?
or ?chill,?
whichare likely to occur at the end of a recipe.
3 otheredges linked duplications (e.g., (?reduce heat,?
?re-duce heat?
)), which are indeed unlikely to immedi-ately follow one another.
These findings suggest theimportance of detecting both lexical pairs that areunlikely to follow one another, in addition to thosethat are likely to.9 ConclusionWe addressed the problem of lexical event ordering,and developed an edge-factored model for tacklingit.
We rely on temporally aligned texts, using a newdataset of cooking recipes as a test case, therebyavoiding the need for costly and error-prone man-ual annotation.
We present results of a pair-wiseaccuracy of over 70% using a basic set of features,and show the utility of the structured perceptron al-gorithm over simpler greedy and local approaches.The setup we explore, which uses a discriminativemodel and an ILP formulation, is easy to extend bothin terms of features and in terms of more complexformal constraints and edge dependencies, as wasdone in graph-based dependency parsing (McDon-ald et al, 2005).
Future work will address the ex-tension of the feature set and model, and the appli-cation of this model to temporal semantics and plan-ning tasks.
We will further address the applicationof semi-supervised variants of the proposed tech-niques (e.g., self-training) to other domains, whereno sizable corpora of temporally aligned data can befound.AcknowledgmentsWe would like to thank Nathan Schneider, RoySchwartz, Bonnie Webber and the members of theProbmodels group at the University of Edinburghfor helpful comments.
This work was supported byERC Advanced Fellowship 249520 GRAMPLUS.Appendix A: Maximal Hamiltonian PathLet G(S) = (S ?
{s, f}, E(S)) be an almost-complete directed graph with E = E(S) = (S ?
{s}) ?
(S ?
{f}).
Let cij?
R be weights for itsedges ((i, j) ?
E).
A Hamiltonian path betweens, f ?
V can be found by solving the following pro-gram, returning P = {(i, j)|xij= 1}.maxxij?
{0,1} : (i,j)?Eui?Z : i?Vn?i6=jcijxijs.t.n?i=0,i6=jxij= 1 ?j 6= s;n?j=0,j 6=ixij= 1 ?i 6= e;ui?
uj+ |V |xij?
|V | ?
1 ?
(i, j) ?
E1169ReferencesNiranjan Balasubramanian, Stephen Soderland, Mausam,and Oren Etzioni.
2013.
Generating coherent eventschemas at scale.
In EMNLP ?13, pages 1721?1731.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Mario Bollini, Stefanie Tellex, Tyler Thompson,Nicholas Roy, and Daniela Rus.
2013.
Interpretingand executing recipes with a cooking robot.
In Exper-imental Robotics, pages 481?495.
Springer.Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Taylor Cassidy, Bill McDowell, Nathanael Chambers,and Steven Bethard.
2014.
An annotation frameworkfor dense event ordering.
In ACL ?14: Short Papers,pages 501?506.Nathanael Chambers and Dan Jurafsky.
2008a.
Jointlycombining implicit constraints improves temporal or-dering.
In EMNLP ?08, pages 698?706.Nathanael Chambers and Dan Jurafsky.
2008b.
Unsuper-vised learning of narrative event chains.
In ACL-HLT?08, pages 789?797.Nathanael Chambers and Dan Jurafsky.
2009.
Unsuper-vised learning of narrative schemas and their partici-pants.
In ACL-IJCNLP ?09, pages 602?610, Suntec,Singapore, August.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In EMNLP ?04, pages 33?40.Philipp Cimiano, Janna L?uker, David Nagel, andChristina Unger.
2013.
Exploiting ontology lexicafor generating natural language texts from RDF data.In European Workshop on Natural Language Genera-tion, pages 10?19.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In EMNLP ?02, pages 1?8.Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-simo Zanzotto.
2013.
Recognizing textual entailment:Models and applications.
Synthesis Lectures on Hu-man Language Technologies, 6(4):1?220.Dmitry Davidov, Ari Rappoport, and Moshe Koppel.2007.
Fully unsupervised discovery of concept-specific relationships by web mining.
In ACL ?07,pages 232?239.Marie-Catherine de Marneffe and Christopher D Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In the COLING ?08 workshop on Cross-Framework and Cross-Domain Parser Evaluation,pages 1?8.Robert M.W.
Dixon.
2005.
A Semantic Approach ToEnglish Grammar.
Oxford University Press.Jennifer D?Souza and Vincent Ng.
2013.
Classifyingtemporal relations with rich linguistic knowledge.
InNAACL-HLT ?13, pages 918?927.Lea Frermann, Ivan Titov, and Manfred Pinkal.
2014.
Ahierarchical bayesian model for unsupervised induc-tion of script knowledge.
In EACL ?14, pages 49?57.Bram Jans, Steven Bethard, Ivan Vuli?c, and Marie-Francine Moens.
2012.
Skip n-grams and rankingfunctions for predicting script events.
In EACL ?12,pages 336?344.Maria Lapata and Alex Lascarides.
2006.
Learningsentence-internal temporal relations.
Journal of Artifi-cial Intelligence Research (JAIR), 27:85?117.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In ACL ?03, pages545?552.Mirella Lapata.
2006.
Automatic evaluation of informa-tion ordering: Kendall?s tau.
Computational Linguis-tics, 32(4):471?484.Xiao Ling and Daniel S Weld.
2010.
Temporal informa-tion extraction.
In AAAI ?10, pages 1385 ?
1390.Jonathan Malmaud, Earl Wagner, Nancy Chang, andKevin Murphy.
2014.
Cooking with semantics.
Inthe ACL 2014 Workshop on Semantic Parsing, pages33?38.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine learn-ing of temporal relations.
In ACL-COLING ?06, pages753?760.Mehdi Manshadi, Reid Swanson, and Andrew S. Gor-don.
2008.
Learning a probabilistic model of eventsequences from internet weblog stories.
In FLAIRS?08, pages 159?164.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL ?05, pages 91?98.Paramita Mirza and Sara Tonelli.
2014.
Classifying tem-poral relations with simple features.
In EACL ?14,pages 308?317, Gothenburg, Sweden, April.Ashutosh Modi and Ivan Titov.
2014.
Inducing neuralmodels of script knowledge.
In CoNLL ?14, pages 49?57.Kira Mour?ao, Luke Zettlemoyer, Ronald Petrick, andMark Steedman.
2012.
Learning STRIPS operatorsfrom noisy and incomplete observations.
In UAI ?12.Karl Pichotta and Raymond Mooney.
2014.
Statisticalscript learning with multi-argument events.
In EACL?14, pages 220?229.Emily Pitler and Ani Nenkova.
2009.
Using syntax todisambiguate explicit discourse connectives in text.
InACL-IJCNLP ?09: Short Papers, pages 13?16.1170James Pustejovsky, Jos?e M Castano, Robert Ingria,Robert J Gaizauskas, Andrea Setzer, Graham Katz,and Dragomir R Radev.
2003.
TimeML: Robustspecification of event and temporal expressions in text.New directions in question answering, 3:28?34.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning script knowledge with webexperiments.
In ACL ?10, pages 979?988.Michaela Regneri, Marcus Rohrbach, Dominikus Wet-zel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.2013.
Grounding action descriptions in videos.
Trans-actions of the Association for Computational Linguis-tics (TACL), 1:25?36.Ehud Reiter, Robert Dale, and Zhiwei Feng.
2000.Building natural language generation systems, vol-ume 33.
Cambridge: Cambridge university press.Dan Roth and Wen-tau Yih.
2007.
Global inference forentity and relation identification via a linear program-ming formulation.
In Lise Getoor and Ben Taskar, ed-itors, Introduction to Statistical Relational Learning.MIT Press.Aju Thalappillil Scaria, Jonathan Berant, Mengqiu Wang,Christopher D Manning, Justin Lewis, Brittany Hard-ing, and Peter Clark.
2013.
Learning biological pro-cesses with global constraints.
In EMNLP ?13.Mark Steedman.
2002.
Plans, affordances, and com-binatory grammar.
Linguistics and Philosophy, 25(5-6):723?753.Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.2012.
Acquiring temporal constraints between rela-tions.
In CIKM ?12, pages 992?1001.Dan Tasse and Noah A Smith.
2008.
SOUR CREAM:Toward semantic processing of recipes.
Technicalreport, Technical Report CMU-LTI-08-005, CarnegieMellon University, Pittsburgh, PA.Roy Tromble and Jason Eisner.
2009.
Learning linearordering problems for better translation.
In EMNLP?09, pages 1007?1016.Naushad UzZaman, Hector Llorens, Leon Derczynski,James Allen, Marc Verhagen, and James Pustejovsky.2013.
Semeval-2013 task 1: Tempeval-3: Evaluatingtime expressions, events, and temporal relations.
In*SEM-SemEval ?13, pages 1?9.Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-hara, and Yuji Matsumoto.
2009.
Jointly identifyingtemporal relations with markov logic.
In ACL-IJCNLP?09, pages 405?413.1171
