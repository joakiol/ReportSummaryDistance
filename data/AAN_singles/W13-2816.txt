Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 109?116,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsTwo Approaches to Correcting Homophone Confusionsin a Hybrid Machine Translation SystemPierrette Bouillon1, Johanna Gerlach1, Ulrich Germann2, Barry Haddow2, Manny Rayner1(1) FTI/TIM, University of Geneva, Switzerland{Pierrette.Bouillon,Johanna.Gerlach,Emmanuel.Rayner}@unige.ch(2) School of Informatics, University of Edinburgh, Scotland{ugermann,bhaddow}@inf.ed.ac.ukAbstractIn the context of a hybrid French-to-English SMT system for translating on-line forum posts, we present two meth-ods for addressing the common problemof homophone confusions in colloquialwritten language.
The first is based onhand-coded rules; the second on weightedgraphs derived from a large-scale pro-nunciation resource, with weights trainedfrom a small bicorpus of domain language.With automatic evaluation, the weightedgraph method yields an improvement ofabout +0.63 BLEU points, while the rule-based method scores about the same as thebaseline.
On contrastive manual evalua-tion, both methods give highly significantimprovements (p < 0.0001) and scoreabout equally when compared against eachother.1 Introduction and motivationThe data used to train Statistical Machine Transla-tion (SMT) systems is most often taken from theproceedings of large multilingual organisations,the generic example being the Europarl corpus(Koehn, 2005); for academic evaluation exercises,the test data may well also be taken from the samesource.
Texts of this kind are carefully cleaned-upformal language.
However, real MT systems of-ten need to handle text from very different genres,which as usual causes problems.This paper addresses a problem common in do-mains containing informally written text: spellingerrors based on homophone confusions.
Con-cretely, the work reported was carried out in thecontext of the ACCEPT project, which deals withthe increasingly important topic of translating on-line forum posts; the experiments we describewere performed using French data taken from theSymantec forum, the concrete task being to trans-late it into English.
The language in these posts isvery far from that which appears in Hansard.
Peo-ple write quickly and carelessly, and no attempt ismade to clean up the results.
In particular, spellingis often uncertain.One of the particular challenges in the taskconsidered here is that French has a high fre-quency of homophones, which often cause confu-sion in written language.
Everyone who speaksEnglish is familiar with the fact that careless writ-ers may confuse its (?of or belonging to it?)
andit?s (contraction of ?it is?
or ?it has?).
French hasthe same problem, but to a much greater degree.Even when someone is working in an environmentwhere an online spell-checker is available, it iseasy to write ou (?or?)
instead of ou` (?where?
),la (?the-feminine?)
instead of la` (?there?)
or ce(?this?)
instead of se (?him/herself?).
Even worse,there is systematic homophony in verb-form end-ings: for example, utiliser (?to use?)
utilisez (?youuse?)
and utilise?
(?used?)
are all homophones.In French posts from the Symantec forum, wefind that between 10% and 15% of all sentencescontain at least one homophone error, dependingon exactly how the term is defined1.
Substitutinga word with an incorrect homophone will often re-sult in a translation error.
Figure 1 shows typicalexamples of homophone errors and their effect ontranslation.The core translation engine in our applicationis a normal SMT system, bracketed between pre-and post-editing phases.
In what follows, we con-trast two different approaches to handling homo-phone errors, which involve pre-editing in dif-ferent ways.
The first approach is based onknowledge-intensive construction of regular ex-pression rules, which use the surrounding contextto correct the most frequent types of homophone1Unclear cases include hyphenation, elison and some ex-amples of missing or incorrect accents.109source automatic translationoriginal La sa ne pose pas de proble`me ...
The its is not the issue ...corrected La` c?a ne pose pas de proble`me ...
Here it is not a problemoriginal ... (du moins on ne recoit pas l?alerte).
... (at least we do not recoit alert).corrected ... (du moins on ne rec?oit pas l?alerte).
.. (at least it does not receive the alert).Figure 1: Examples of homophone errors in French forum data, contrasting English translations producedby the SMT engine from plain and corrected versions.confusions.The second is an engineering method: we use acommercial pronunciation-generation tool to gen-erate a homophone dictionary, then use this dictio-nary to turn the input into a weighted graph whereeach word is replaced by a weighted disjunction ofhomophones.
Related, though less elaborate, workhas been reported by Bertoldi et al(2010), whoaddress spelling errors using a character-level con-fusion network based on common character con-fusions in typed English and test them on artifi-cially created noisy data.
Formiga and Fonollosa(2012) also used character-based models to correctspelling on informally written English data.The two approaches in the present paper ex-ploit fundamentally different knowledge sourcesin trying to identify and correct homophone er-rors.
The rule-based method relies exclusivelyon source-side information, encoding patterns in-dicative of common French homophone confu-sions.
The weighted graph method shifts the bal-ance to the target side; the choice between poten-tial homophone alternatives is made primarily bythe target language model, though the source lan-guage weights and the translation model are alsoinvolved.The rest of the paper is organised as follows.Section 2 describes the basic framework in moredetail, and Section 3 the experiments.
Section 4summarises and concludes.2 Basic frameworkThe goal of the ACCEPT project is to provideeasy cross-lingual access to posts in online fo-rums.
Given the large variety of possible techni-cal topics and the limited supply of online gurus,it frequently happens that users, searching forumposts online, find that the answer they need is in alanguage they do not know.Currently available tools, for example GoogleTranslate, are of course a great deal better thannothing, but still leave much to be desired.
Whenone considers that advice given in an online fo-rum may not be easy to follow even for native lan-guage speakers, it is unsurprising that a Google-translated version often fails to be useful.
There isconsequently strong motivation to develop an in-frastructure explicitly designed to produce high-quality translations.
ACCEPT intends to achievethis by a combination of three technologies: pre-editing of the source; domain-tuned SMT; andpost-editing of the target.
The pre- and post-editing stages are performed partly using auto-matic tools, and partly by manual intervention onthe part of the user communities which typicallygrow up around online forums.
We now brieflydescribe the automatic parts of the system.2.1 SMT engine and corpus dataThe SMT engine used is a phrase-based systemtrained with the standard Moses pipeline (Koehnet al 2007), using GIZA++ (Och and Ney,2000) for word alignment and SRILM (Stolcke,2002) for the estimation of 5-gram Kneser-Neysmoothed (Kneser and Ney, 1995) language mod-els.For training the translation and lexicalised re-ordering models we used the releases of europarland news-commentary provided for the WMT12shared task (Callison-Burch et al 2012), togetherwith a dataset from the ACCEPT project consist-ing mainly of technical product manuals and mar-keting materials.For language modelling we used the target sidesof all the parallel data, together with approx-imately 900 000 words of monolingual Englishdata extracted from web forums of the type thatwe wish to translate.
Separate language modelswere trained on each of the data sets, then thesewere linearly interpolated using SRILM to min-imise perplexity on a heldout portion of the forumdata.110For tuning and testing, we extracted 1022 sen-tences randomly from a collection of monolin-gual French Symantec forum data (distinct fromthe monolingual English forum data), translatedthese using Google Translate, then post-editedto create references.
The post-editing was per-formed by a native English speaker, who is alsofluent in French.
This 1022-sentence parallel textwas then split into two equal halves (devtest aand devtest b) for minimum error rate tuning(MERT) and testing, respectively.2.2 Rule-based pre-editing engineRule-based processing is carried out using theAcrolinx engine (Bredenkamp et al 2000), whichsupports spelling, grammar, style and terminologychecking.
These methods of pre-editing were orig-inally designed to be applied by authors during thetechnical documentation authoring process.
Theauthor gets error markings and improvement sug-gestions, and decides about reformulations.
It isalso possible to apply the provided suggestionsautomatically as direct reformulations.
Rules arewritten in a regular-expression-based formalismwhich can access tagger-generated part-of-speechinformation.
The rule-writer can specify both pos-itive evidence (patterns that will trigger applica-tion of the rule) and negative evidence (patternsthat will block application).3 ExperimentsWe compared the rule-based and weighted graphapproaches, evaluating each of them on the 511sentence devtest b corpus.
The baseline SMTsystem, with no pre-editing, achieves an averageBLEU score of 42.47 on this set.3.1 The rule-based approachUnder the ACCEPT project, a set of lightweightpre-editing rules have been developed specificallyfor the Symantec Forum translation task.
Someof the rules are automatic (direct reformulations);others present the user with a set of suggestions.The evaluations described in Gerlach et al(2013)demonstrate that pre-editing with the rules has asignificant positive effect on the quality of SMT-based translation.The implemented rules address four main phe-nomena: differences between informal and for-mal language (Rayner et al 2012), differencesbetween local French and English word-order, el-lision/punctuation, and word confusions.
Rulesfor resolving homophone confusions belong to thefourth group.
They are shown in Table 1, togetherwith approximate frequencies of occurrence in thedevelopment corpus.Table 1: Hand-coded rules for homophone confu-sions and per-sentence frequency of applicabilityin the development corpus.
Some of the rules alsocover non-homophone errors, so the frequency fig-ures are slight overestimates as far as homophonesare concerned.Rule Freq.a/as/a` 4.17%noun phrase agreement 3.20%incorrect verb ending (er/e?/ez) 2.90%missing hyphenation 2.08%subject verb agreement 1.90%missing elision 1.26%du/du?
0.35%la/la` 0.32%ou/ou` 0.28%ce/se 0.27%Verb/noun 0.23%tous/tout 0.22%indicative/imperative 0.19%future/conditional tense 0.14%sur/su?r 0.10%quel que/quelque 0.08%ma/m?a 0.06%quelle/qu?elle/quel/quels 0.05%c?a/sa 0.04%des/de`s 0.04%et/est 0.02%ci/si 0.01%m?y/mi/mis 0.01%other 0.17%Total 18.09%The set of Acrolinx pre-editing rules potentiallyrelevant to resolution of homophone errors wasapplied to the devtest b set test corpus (Sec-tion 2.1).
In order to be able to make a fair com-parison with the weighted-graph method, we onlyused rules with a unique suggestion, which couldbe run automatically.
Applying these rules pro-duced 430 changed words in the test corpus, butdid not change the average BLEU score signifi-cantly (42.38).Corrections made with a human in the loop,used as ?oracle?
input for the SMT system, by the111way, achieve an average BLEU score2 of 43.11 ?roughly on par with the weighted-graph approachdescribed below.3.2 The weighted graph approachIn our second approach, the basic idea is to trans-form the input sentence into a confusion network(Bertoldi et al 2008) which presents the trans-lation system with a weighted list of homophonealternatives for each input word.
The system isfree to choose a path through a network of wordsthat optimizes the internal hypothesis score; theweighting scheme for the alternatives can be usedto guide the decoder.
The conjecture is that thecombination of the confusion network weights, thetranslation model and the target language modelcan resolve homophone confusions.3.2.1 Defining sets of confusable wordsTo compile lists of homophones, we used the com-mercial Nuance Toolkit pronounce utility asour source of French pronunciation information.We began by extracting a list of all the lexicalitems which occurred in the training portion ofthe French Symantec forum data, giving us 30 565words.
We then ran pronounce over this list.The Nuance utility does not simply perform tablelookups, but is capable of creating pronunciationson the fly; it could in particular assign plausiblepronunciations to most of the misspellings that oc-curred in the corpus.
In general, a word is givenmore than one possible pronunciation.
This can befor several reasons; in particular, some sounds inFrench can systematically be pronounced in morethan one way, and pronunciation is often also de-pendent on whether the word is followed by a con-sonant or vowel.
Table 2 shows examples.Using the data taken from pronounce, wegrouped words together into clusters which havea common pronunciation; since words typicallyhave more than one pronunciation, they will typi-cally also belong to more than one cluster.
We thencontructed sets of possible alternatives for wordsby including, for each word W , all the words W ?such that W and W ?
occurred in the same cluster;since careless French writing is also characterisedby mistakes in placing accents, we added all wordsW ?
such that W and W ?
are identical up to drop-ping accents.
Table 3 shows typical results.2With parameter sets from tuning the system on raw in-put and input preprocessed with the fully automatic rules; cf.Sec.
3.3.Word Pronunciationans A?A?zpre?vu p r E v yp r e v yque?bec k e b E kroule r u lr u l *Table 2: Examples of French pronunciations gen-erated by pronounce.
The format used is theNuance version of ARPABET.Intuitively, it is in general unlikely that, on see-ing a word which occurs frequently in the corpus,we will want to hypothesize that it may be a mis-spelling of one which occurs very infrequently.We consequently filtered the sets of alternativesto remove all words on the right whose frequencywas less than 0.05 times that of the word on theleft.Table 3: Examples of sets of possible alternativesfor words, generated by considering both homo-phone and accent confusions.Word Alternativesaux au aux hautcre?er cre?er cre?ez cre?e?
cre?e?e cre?e?es cre?e?sco?te cote cote?
co?te co?te?
quot quoteho?te haut haute ho?te ho?tesil e elle elles il ils l le yme`ne main mene?
me`nenom nom noms nonou ou ou`saine sain saine saines sce`ne seinetraits trait traits tray tre tres tre`s3.2.2 Setting confusion network weightsIn a small series of preliminary experiments wefirst tested three na?
?ve weighting schemes for theconfusion networks.?
using a uniform distribution that assignsequal weight to all spelling alternatives;?
setting weights proportional to the unigramprobability of the word in question;?
computing the weights as state probabilitiesin a trellis with the forward-backward algo-rithm (Rabiner, 1989), an algorithm widely112Table 4: Decoder performance with different con-fusion network weighting schemes.weighting scheme av.
BLEUa std.none (baseline system) 42.47 ?
.22uniform 41.50 ?
.37unigram 41.58 ?
.26fwd-bwd (bigram) 41.81 ?
.16bigram context(interpolated)43.10 ?
.32aBased on muliple tuning runs with random parameter ini-tializations.used in speech recognition.
Suppose thateach word w?i in the observed translation in-put sentence is produced while the writer hasa particular ?true?
word wi ?
Ci in mind,where Ci is the set of words confusable withw?i.
For the sake of simplicity, we assume thatwithin a confusion set, all ?true word?
op-tions are equally likely, i.e., p(w?i |wi = x) =1|Ci| for x ?
Ci.
The writer chooses the nextword wi+1 according to the conditional wordbigram probability p(wi+1 |wi).The forward probability fwd i(x) is the prob-ability of arriving in state wi = x at timei, regardless of the sequence of states visiteden-route; the backward probability bwd i(x)is the probability of arriving at the end of thesentence coming from state wi = x, regard-less of the path taken.
These probabilities canbe computed efficiently with dynamic pro-gramming.The weight assigned to a particular ho-mophone alternative x at position i in theconfusion network is the joint forward andbackward probability:weight i(x) = fwd i(x) ?
bwd i(x).In practice, it turns out that these three na?
?veweighting schemes do more harm than good, asthe results in Table 4 show.
Clearly, they rely toomuch on overall language statistics (unigram andbigram probabilities) and pay too little attention tothe actual input.We therefore designed a fourth weightingscheme (?bigram context interpolated?)
thatgives more weight to the observed input and com-putes the weights as the average of two score com-ponents.
The first is a binary feature functionthat assigns 1 to each word actually observed inthe input, and 0 to its homophone alternatives.The second component is the bigram-based in-context probability of each candidate.
Unlike theforward-backward weighting scheme, which con-siders all possible context words for each candi-date (as specified in the respective confusion sets),the new scheme only considers the words in theactual input as context words.It would have be desirable to keep the two scorecomponents separate and tune their weights to-gether with all the other parameters of the SMTsystem.
Unfortunately, the current implementa-tion of confusion network-based decoding in theMoses decoder allows only one single weight inthe specification of confusion networks, so that wehad to combine the two components into one scorebefore feeding the confusion network into the de-coder.With the improved weighting scheme, the con-fusion network approach does outperform thebaseline system, giving an average BLEU of 43.10(+0.63).3.3 Automatic evaluation (BLEU)Due to the relatively small size of the evalua-tion set and instability inherent in minimum errorrate training (Foster and Kuhn, 2009; Clark et al2011), results of individual tuning and evaluationruns can be unreliable.
We therefore preformedmultiple tuning and evaluation runs for each sys-tem (baseline, rule-based and weighted graph).
Toillustrate the precision of the BLEU score on ourdata sets, we plot in Fig.
2 for each individual tun-ing run the BLEU score achieved on the tuningset (x-axis) against the performance on the evalua-tion set (y-axis).
The variance along the x-axis foreach system is due to search errors in parameteroptimization.
Since the search space is not con-vex, the tuning process can get stuck in local max-ima.
The apparent poor local correlation betweenperformance on the tuning set and performance onthe evaluation set for each system shows the effectof the sampling error.With larger tuning and evaluation sets, wewould expect the correlation between the twoto improve.
The scatter plot suggests that theweighted-graph system does on average producesignificantly better translations (with respect toBLEU) than both the baseline and the rule-basedsystem, whereas the difference between the base-line and the rule-based system is within the range11341.641.84242.242.442.642.84343.243.443.643.846.4 46.6 46.8 47 47.2 47.4 47.6 47.8BLEUscoreonevaluationsetBLEU score on tuning setBLEU scores on evaluation set?
?
.95 conf.
int.?
baseline 42.47 .22 42.04?42.89?
rule-based 42.38 .23 41.94?42.83+ weighted graph 43.10 .32 42.48?43.72Figure 2: BLEU scores (in points) for the baseline, rule-based and weighted graph-based systems.of statistical error.To study the effect of tuning condition (tun-ing on raw vs. input pre-processed by rules), wealso translated both the raw and the pre-processedevaluation corpus with all parameter setting thatwe had obtained during the various experiments.Figure 3 plots (with solid markers) performanceon raw input (x-axis) against translation of pre-processed input (y-axis).
We observe that whilepreprocessing harms performance for certain pa-rameter settings, most of the time proprocessingdoes lead to improvements in BLEU score.
Theslight deterioration we observed when comparingsystem tuned on exactly the type of input that theywere to translate later (i.e., raw or preprocessed)seems to be a imprecision in the measurementcaused by training instability and sampling errorrather than the result of systematic input deterio-ration due to preprocessing.
Overall, the improve-ments are small and not statistically significant,but there appears to be a positive trend.To gauge the benefits of more extensive pre-processing and input error correction we producedand translated ?oracle?
input by also applying rulesfrom the Acrolinx engine that currently require ahuman in the loop who decides whether or not therule in question should be applied.
The boost inperformance is shown by the hollow markers inFig.
3.
Here, translation of pre-processed inputconsistently fares better than translation of the rawinput.3.4 Human evaluationAlthough BLEU suggests that the weighted-graphmethod significantly outscores both the baselineand the rule-based method (p < 0.05 over 25 tun-ing runs), the absolute differences are small, andwe decided that it would be prudent to carry out ahuman evaluation as well.
Following the method-ology of Rayner et al(2012), we performed con-trastive judging on the Amazon Mechanical Turk(AMT) to compare different versions of the sys-tem.
Subjects were recruited from Canada, a bilin-gual French/English country, requesting Englishnative speakers with good written French; we alsolimited the call to AMT workers who had alreadycompleted at least 50 assignments, at least 80%of which had been accepted.
Judging assignmentswere split into groups of 20 triplets, where eachtriplet consisted of a source sentence and two dif-ferent target sentences; the judge was asked tosay which translation was better, using a five-pointscale {better, slightly-better, about-equal, slightly-worse, worse}.
The order of the two targets was11441.54242.54343.541.6  41.8  42  42.2  42.4  42.6  42.8  43BLEUscoreoninput preprocessedbyrules/ oracleinputBLEU score on raw baseline inputbaseline vs. oracle input; system tuned on baseline inputbaseline vs. oracle input; system tuned on preprocessed inputbaseline vs. rule-processed input; system tuned on baseline inputbaseline vs. rule-processed input; system tuned on preprocessed inputthreshold for improvement (above this line) vs. deterioration (below)Figure 3: BLEU scores (in points) the two input conditions ?baseline?
and ?rule-based?
(solid markers).The hollow markers show the BLEU score on human-corrected ?oracle?
input using a more extensive setof rules / suggestions from the Acrolinx engine that require a human in the loop.randomised.
Judges were paid $1 for each groupof 20 triplets.
Each triplet was judged three times.Using the above method, we posted AMT tasksTable 5: Comparison between baseline, rule-basedand weighted-graph versions, evaluated on the511-utterance devtest b corpus and judged bythree AMT-recruited judges.
Figures are presentedboth for majority voting and for unanimous deci-sions only.Majority Unanimousbaseline vs rule-basedbaseline better 83 16.2% 48 9.4%r-based better 204 40.0% 161 31.5%Unclear 36 7.0% 93 18.1%Equal 188 36.8% 209 40.9%baseline vs weighted-graphbaseline better 115 22.5% 52 10.1%w-graph better 193 37.8% 119 23.3%Unclear 46 9.0% 99 19.4%Equal 157 30.7% 241 47.2%rule-based vs weighted-graphr-based better 141 27.6% 68 13.3%w-graph better 123 24.1% 70 13.7%Unclear 25 4.9% 142 27.8%Equal 222 43.4% 231 45.2%to compare a) the baseline system against therule-based system, b) the baseline system againstthe best weighted-graph system (interpolated-bigram) from Section 3.2.2 and c) the rule-based system and the weighted-graph systemagainst each other.
The results are shown inTable 5; in the second and third columns, dis-agreements are resolved by majority voting, andin the fourth and fifth we only count caseswhere the judges are unanimous, the others be-ing scored as unclear.
In both cases, we re-duce the original five-point scale to a three-pointscale {better, equal/unclear, worse}3.
Irrespec-tive of the method used to resolve disagreements,the differences ?rule-based system/baseline?
and?weighted-graph system/baseline?
are highly sig-nificant (p < 0.0001) according to the McNe-mar sign test, while the difference ?rule-basedsystem/weighted-graph system?
is not significant.We were somewhat puzzled that BLEU makesthe weighted-graph system clearly better than therule-based one, while manual evaluation ratesthem as approximately equal.
The explanationseems to be to do with the fact that manual evalu-ation operates at the sentence level, giving equalimportance to all sentences, while BLEU oper-3For reasons we do not fully understand, we get betterinter-judge agreement this way than we do when we origi-nally ask for judgements on a three-point scale.115ates at the word level and consequently countslonger sentences as more important.
If we calcu-late BLEU on a per-sentence basis and then av-erage the scores, we find that the results for thetwo systems are nearly the same; per-sentenceBLEU differences also correlate reasonably wellwith majority judgements (Pearson correlation co-efficient of 0.39).
It is unclear to us, however,whether the difference between per-sentence andper-word BLEU evaluation points to anything par-ticularly interesting.4 ConclusionsWe have presented two methods for addressingthe common problem of homophone confusions incolloquial written language in the context of anSMT system.
The weighted-graph method pro-duced a small but significant increase in BLEU,while the rule-based one was about the same asthe baseline.
Both methods, however, gave clearlysignificant improvements on contrastive manualevaluation carried out through AMT, with no sig-nificant difference in performance when the twowere compared directly.The small but consistent improvements inBLEU score that we observed with the human-in-the-loop oracle input over the fully automaticrule-based setup invite further investigation.
Howmany of the decisions currently left to the hu-man can be automated?
Is there a fair way ofcomparing and evaluating fully automatic againstsemi-automatic setups?
Work on these topics is inpreparation and will be reported elsewhere.AcknowledgementsThe work described in this paper was performedas part of the Seventh Framework Programme AC-CEPT project, under grant agreement 288769.ReferencesBertoldi, Nicola, Mauro Cettolo, and MarcelloFederico.
2010.
?Statistical machine translationof texts with misspelled words.?
NAACL.
LosAngeles, CA, USA.Bertoldi, Nicola, Richard Zens, Marcello Fed-erico, and Wade Shen.
2008.
?Efficient speechtranslation through confusion network decod-ing.?
IEEE Transactions on Audio, Speech &Language Processing, 16(8):1696?1705.Bredenkamp, Andrew, Berthold Crysmann, andMirela Petrea.
2000.
?Looking for errors : Adeclarative formalism for resource-adaptive lan-guage checking.?
LREC.
Athens, Greece.Callison-Burch, Chris, Philipp Koehn, ChristofMonz, et al(eds.).
2012.
Seventh Workshopon Statistical Machine Translation (WMT).Montre?al, Canada.Clark, Jonathan H., Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
?Better hypothesis test-ing for statistical machine translation: Control-ling for optimizer instability.?
ACL-HLT.
Port-land, OR, USA.Formiga, Lluis and Jose?
A. R. Fonollosa.
2012.?Dealing with input noise in statistical machinetranslation.?
COLING.
Mumbai, India.Foster, George and Roland Kuhn.
2009.
?Sta-bilizing minimum error rate training.?
WMT.Athens, Greece.Gerlach, Johanna, Victoria Porro, Pierrette Bouil-lon, and Sabine Lehmann.
2013.
?La pre?-e?dition avec des re`gles peu cou?teuses, utile pourla TA statistique??
TALN-RECITAL.
Sablesd?Olonne, France.Kneser, Reinhard and Hermann Ney.
1995.
?Im-proved backing-off for m-gram language mod-eling.?
ICASSP.
Detroit, MI, USA.Koehn, Philipp.
2005.
?Europarl: A parallel cor-pus for statistical machine translation.?
MTSummit X. Phuket, Thailand.Koehn, Philipp, Hieu Hoang, Alexandra Birch,et al2007.
?Moses: Open source toolkit forstatistical machine translation.?
ACL Demon-stration Session.
Prague, Czech Republic.Och, Franz Josef and Hermann Ney.
2000.
?Im-proved statistical alignment models.?
ACL.Hong Kong.Rabiner, Lawrence R. 1989.
?A tutorial on hid-den markov models and selected applications inspeech recognition.?
Proceedings of the IEEE,257?286.Rayner, Manny, Pierrette Bouillon, and BarryHaddow.
2012.
?Using source-language trans-formations to address register mismatches inSMT.?
AMTA.
San Diego, CA, USA.Stolcke, Andreas.
2002.
?SRILM - an extensi-ble language modeling toolkit.?
ICSLP.
Denver,CO, USA.116
