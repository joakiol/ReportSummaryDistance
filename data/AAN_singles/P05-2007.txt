Proceedings of the ACL Student Research Workshop, pages 37?42,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsAmerican Sign Language Generation:Multimodal NLG with Multiple Linguistic ChannelsMatt HuenerfauthComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104 USAmatt@huenerfauth.comAbstractSoftware to translate English text intoAmerican Sign Language (ASL) animationcan improve information accessibility forthe majority of deaf adults with limitedEnglish literacy.
ASL natural languagegeneration (NLG) is a special form of mul-timodal NLG that uses multiple linguisticoutput channels.
ASL NLG technology hasapplications for the generation of gestureanimation and other communication signalsthat are not easily encoded as text strings.1 Introduction and MotivationsAmerican Sign Language (ASL) is a full naturallanguage ?
with a linguistic structure distinct fromEnglish ?
used as the primary means of communi-cation for approximately one half million deafpeople in the United States (Neidle et al, 2000,Liddell, 2003; Mitchell, 2004).
Without aural ex-posure to English during childhood, a majority ofdeaf U.S. high school graduates (age 18) have onlya fourth-grade (age 10) English reading level (Holt,1991).
Technology for the deaf rarely addressesthis literacy issue; so, many deaf people find it dif-ficult to read text on electronic devices.
Softwarefor translating English text into animations of acomputer-generated character performing ASL canmake a variety of English text sources accessible tothe deaf, including: TV closed captioning, teletypetelephones, and computer user-interfaces  (Huener-fauth, 2005).
Machine translation (MT) can alsobe used in educational software for deaf children tohelp them improve their English literacy skills.This paper describes the design of our English-to-ASL MT system (Huenerfauth, 2004a, 2004b,2005), focusing on ASL generation.
This overviewillustrates important correspondences between theproblem of ASL natural language generation(NLG) and related research in Multimodal NLG.1.1 ASL Linguistic IssuesIn ASL, several parts of the body convey meaningin parallel: hands (location, orientation, shape), eyegaze, mouth shape, facial expression, head-tilt, andshoulder-tilt.
Signers may also interleave lexicalsigning (LS) with classifier predicates (CP) duringa performance.
During LS, a signer builds ASLsentences by syntactically combining ASL lexicalitems (arranging individual signs into sentences).The signer may also associate entities under dis-cussion with locations in space around their body;these locations are used in pronominal reference(pointing to a location) or verb agreement (aimingthe motion path of a verb sign to/from a location).During CPs, signers?
hands draw a 3D scene inthe space in front of their torso.
One could imag-ine invisible placeholders floating in front of asigner representing real-world objects in a scene.To represent each object, the signer places his/herhand in a special handshape (used specifically forobjects of that semantic type: moving vehicles,seated animals, upright humans, etc.).
The hand ismoved to show a 3D location, movement path, orsurface contour of the object being described.
Forexample, to convey the English sentence ?the carparked next to the house,?
signers would indicate alocation in space to represent the house using aspecial handshape for ?bulky objects.?
Next, theywould use a ?moving vehicle?
handshape to trace a3D path for the car which stops next to the house.371.2 Previous ASL MT SystemsThere have been some previous English-to-ASLMT projects ?
see survey in (Huenerfauth, 2003).Amid other limitations, none of these systems ad-dress how to produce the 3D locations and motionpaths needed for CPs.
A fluent, useful English-to-ASL MT system cannot ignore CPs.
ASL sign-frequency studies show that signers produce a CPfrom 1 to 17 times per minute, depending on genre(Morford and MacFarlane, 2003).
Further, it isthose English sentences whose ASL translationuses a CP that a deaf user with low English literacywould need an MT system to translate.
These Eng-lish sentences look structurally different than theirASL CP counterpart ?
often making the Englishsentence difficult to read for a deaf user.2 ASL NLG: A Form of Multimodal NLGNLG researchers think of communication signalsin a variety of ways: some as a written text, otheras speech audio (with prosody, timing, volume,and intonation), and those working in MultimodalNLG as text/speech with coordinated graphics(maps, charts, diagrams, etc).
Some MultimodalNLG focuses on ?embodied conversational agents?
(ECAs), computer-generated animated charactersthat communicate with users using speech, eyegaze, facial expression, body posture, and gestures(Cassell et al, 2000; Kopp et al, 2004).The output of any NLG system could be repre-sented as a stream of values (or features) thatchange over time during a communication signal;some NLG systems specify more values than oth-ers.
Because the English writing system does notrecord a speaker?s prosody, facial expression orgesture1, a text-based NLG system specifies fewercommunication stream values in its output thandoes a speech-based or ECA system.
A text-basedNLG system requires literate users, to whom it cantransfer some of the processing burden; they mustmentally reconstruct more of the language per-formance than do users of speech or ECA systems.Since most writing systems are based on strings,text-based NLG systems can easily encode theiroutput as a single stream, namely a sequence of1Some punctuation marks loosely correspond to intonation orpauses, but most prosodic information is lost.
Facial expres-sion and gesture is generally not conveyed in writing, exceptperhaps for the occasional use of ?emoticons.?
;-)words/characters.
To generate more complex sig-nals, multimodal systems decompose their outputinto several sub-streams ?
we?ll refer to these as?channels.?
Dividing a communication signal intochannels can make it easier to represent the variouschoices the generator must make; generally, a dif-ferent processing component of the system willgovern the output of each channel.
The trade-off isthat these channels must be coordinated over time.Instead of thinking of channels as dividing acommunication signal, we can think of them asgroupings of individual values in the data streamthat are related in some way.
The channels of amultimodal NLG system generally correspond tonatural perceptual/conceptual groupings called?modalities.?
Coarsely, audio and visual parts ofthe output are thought of as separate modalities.When parts of the output appear on different por-tions of the display, then they are also generallyconsidered separate modalities.
For instance, amultimodal NLG system for automobile drivingdirections may have separate processing channelsfor text, maps, other graphics, and sound effects.An ECA system may have separate channels foreye gaze, facial expression, manual gestures, andspeech audio of the animated character.When a language has no commonly-known writ-ing system ?
as is the case for ASL ?
then it?s notpossible to build a text-based NLG system.
Wemust produce an animation of a character (like anECA) performing ASL; so, we must specify howthe hands, eye gaze, mouth shape, facial expres-sion, head-tilt, and shoulder-tilt are coordinatedover time.
With no conventional string-encodingof ASL (that would compress the signal into a sin-gle stream), an ASL signal is spread over multiplechannels of the output ?
a departure from mostMultimodal NLG systems, which have a singlelinguistic channel/modality that is coordinated withother non-linguistic resources (Figure 1).Figure 1: Linguistic Channels in Multimodal SystemsEnglish TextDriving MapsOther GraphicsPrototypical Driving-Direction SystemSound EffectsLeft HandHead-TiltEye-GazeFacial ExpressionRight HandPrototypical ASL SystemLinguisticChannels38Of course, we could invent a string-based nota-tion for ASL so that we could use traditional text-based NLG technology.
(Since ASL has no writ-ing system, we would have to invent an artificialnotation.)
Unfortunately, since the users of thesystem wouldn?t be trained in this new writing sys-tem, it could not be used as output; we would stillneed to generate a multimodal animation output.An artificial writing system could only be used forinternal representation and processing, However,flattening a naturally multichannel signal into asingle-channel string (prior to generating a mul-tichannel output) can introduce its own complica-tions to the ASL system?s design.
For this reason,this project has been exploring ways to representthe hierarchical linguistic structure of informationon multiple channels of ASL performance (andhow these structures are coordinated or uncoordi-nated across channels over time).Some multimodal systems have explored usinglinguistic structures to control (to some degree) theoutput of multiple channels.
Research on generat-ing animations of a speaking ECA character thatperforms meaningful gestures (Kopp et al, 2004)has similarities to this ASL project.
First of all, thechannels in the signal are basically the same; ananimated human-like character is shown onscreenwith information about eye, face, and arm move-ments being generated.
However, an ASL systemhas no audio speech channel but potentially morefine-grained channels of detailed body movement.The less superficial similarity is that (Kopp et.al, 2004) have attempted to represent the semanticmeaning of some of the character?s gestures and tosynchronize them with the speech output.
Thismeans that, like an ASL NLG system, severalchannels of the signal are being governed by thelinguistic mechanisms of a natural language.Unlike ASL, the gesture system uses the speechaudio channel to convey nearly all of the meaningto the user; the other channels are generally used toconvey additional/redundant information.
Further,the internal structure of the gestures is not gener-ally encoded in the system; they are typicallyatomic/lexical gesture events which are synchro-nized to co-occur with portions of speech output.A final difference is that gestures which co-occurwith English speech (although meaningful) can besomewhat vague and are certainly less systematicand conventional than ASL body movements.
So,while both systems may have multiple linguisticchannels, the gesture system still has one primarylinguistic channel (audio speech) and a few chan-nels controlled in only a partially linguistic way.3 This English-to-ASL MT DesignThe linguistic and multimodal issues discussedabove have had important consequences on thedesign of our English-to-ASL MT system.
Thereare several unique features of this system causedby: (1) ASL having multiple linguistic channelsthat must be coordinated during generation, (2)ASL having both an LS and a CP form of signing,(3) CP signing visually conveying 3D spatial rela-tionships in front of the signer?s torso, and (4) ASLlacking a conventional written form.
While ASL-particular factors influenced this design, section 5will discuss how this design has implications forNLG of traditional written/spoken languages.3.1 Coordinating Linguistic ChannelsSection 2 mentioned that this project is developingmultichannel (non-string) encodings of ASL ani-mation; these encodings must coordinate multiplechannels of the signal as they are generated by thelinguistic structures and rules of ASL.
Kopp et al(2004) have explored how to coordinate meaning-ful gestures with speech signal during generation;however, their domain is somewhat simpler.
Theirgestures are atomic events without internal hierar-chical structure.
Our project is currently develop-ing grammar-like coordination formalisms thatallow complex linguistic signals on multiple chan-nels to be conveniently represented.23.2 ASL Computational Linguistic ModelsThis project uses representations of discourse, se-mantics, syntax, and (sign) phonology tailored toASL generation (Huenerfauth, 2004b).
In particu-lar, since this MT system will generate animationsof classifier predicates (CPs), the system consults a3D model of real-world scenes under discussion.Further, since multimodal NLG requires a form ofscheduling (events on multiple channels are coor-dinated over a performance timeline), all of thelinguistic models consulted and modified duringASL generation are time-indexed according to atimeline of the ASL performance being produced.2Details of this work will be described in future publication.39Previous ASL phonological models were de-signed to represent non-CP ASL, but CPs use areduced set of handshapes, standard eye-gaze andhead-tilt patterns, and more complex orientationsand motion paths.
The phonological model devel-oped for this system makes it easier to specify CPs.Because ASL signers can use the space in frontof their body to visually convey information, it ispossible during CPs to show the exact 3D layout ofobjects being discussed.
(The use of channels rep-resenting the hands means that we can now indi-cate 3D visual information ?
not possible withspeech or text.)
To represent this 3D detailed formof meaning, this system has an unusual semanticmodel for generating CPs.
We populate the vol-ume of space around the signer?s torso with invisi-ble 3D objects representing entities discussed byCPs being generated (Huenerfauth, 2004b).
Thesemantic model is the set of placeholders aroundthe signer (augmented with the CP handshape usedfor each).
Thus, the semantics of the ?car parkednext to the house?
example (section 1.1) is that a?bulky?
object occupies a particular 3D locationand a ?vehicle?
object moves toward it and stops.Of course, the system will also need more tradi-tional semantic representations of the informationto be conveyed during generation, but this 3Dmodel helps the system select the proper 3D mo-tion paths for the signers?
hands when ?drawing?the 3D scenes during CPs.
The work of (Kopp etal., 2004) studies gestures to convey spatial infor-mation during an English speech performance, butunlike this system, they use a logical-predicate-based semantics to represent information aboutobjects referred to by gesture.
Because ASL CPsindicate 3D layout in a linguistically conventionaland detailed way, we use an actual 3D model ofthe objects being discussed.
Such a 3D model mayalso be useful for ECA systems that wish to gener-ate more detailed 3D spatial gesture animations.The discourse model in this ASL system recordsfeatures not found in other NLG systems.
It trackswhether a 3D location has been assigned to eachdiscourse entity, where that location is around thesigner, and whether the latest location of the entityhas been indicated by a CP.
The discourse modelis not only relevant during CP performance; sinceASL LS performance also assigns 3D locations toobjects under discussion (for pronouns and verbalagreement), this model is also used for LS.3.3 Generating 3D Classifier PredicatesAn essential step in producing an animation of anASL CP is the selection of 3D motion paths for thecomputer-generated signer?s hands, eye gaze, andhead tilt.
The motion paths of objects in the 3Dmodel described above are used to select corre-sponding motion paths for these parts of thesigner?s body during CPs.
To build the 3D place-holder model, this system uses preexisting scene-visualization software to analyze an English textdescribing the motion of real-world objects andbuild a 3D model of how the objects mentioned intext are arranged and move (Huenerfauth, 2004b).This model is ?overlaid?
onto the volume in frontof the ASL signer (Figure 2).
For each object inthe scene, a corresponding invisible placeholder ispositioned in front of the signer; the layout ofplaceholders mimics the layout of objects in the 3Dscene.
In the ?car parked next to the house?
exam-ple, a miniature invisible object representing a?house?
is positioned in front of the signer?s torso,and another object (with a motion path terminatingnext to the ?house?)
is added to represent the ?car.
?The locations and orientations of the placehold-ers are later used by the system to select the loca-tions and orientations for the signer?s hands whileperforming CPs about them.
So, the motion pathcalculated for the car will be the basis for the 3Dmotion path of the signer?s hand during the classi-fier predicate describing the car?s motion.
Giventhe information in the discourse/semantic models,the system generates the hand motions, head-tilt,and eye-gaze for a CP.
It stores a library contain-ing templates representing a prototypical form ofeach CP the system can produce.
The templatesTEXT:THE CARPARKED NEXTTO THE HOUSE.VisualizationSoftware3D MODEL:Overlay infront of ASLsignerConvert to 3Dplaceholderlocations/pathsFigure 2: Converting English Text to 3D Placeholder40are planning operators (with logical pre-conditions,monitored termination conditions, and effects),allowing the system to ?trigger?
other elements ofASL signing performance that may be requiredduring a CP.
A planning-based NLG approach,described in (Huenerfauth, 2004b), is used to selecta template, fill in its missing parameters, and builda schedule of the animation events on multiplechannels needed to produce a sequence of CPs.3.4 A Multi-Path ArchitectureA multimodal NLG system may have several pres-entation styles it could use to convey informationto its user; these styles may take advantage of thevarious output channels to different degrees.
InASL, there are multiple channels in the linguisticportion of the signal, and not surprisingly, the lan-guage has multiple sub-systems of signing thattake advantage of the visual modality in differentways.
ASL signers can select whether to conveyinformation using lexical signing (LS) or classifierpredicates (CPs) during an ASL performance (sec-tion 1.1).
These two sub-systems use the spacearound the signer differently; during CPs, locationsin space associated with objects under discussionmust be laid out in a 3D manner corresponding tothe topological layout of the real-world scene un-der discussion.
Locations associated with objectsduring LS (used for pronouns and verb agreement)have no topological requirement.
The layout of the3D locations during LS may be arbitrary.The CP generation approach in section 3.3 iscomputationally expensive; so, we would only liketo use this processing pathway when necessary.English input sentences not producing classifierpredicates would not need to be processed by thevisualization software; in fact, most of these sen-tences could be handled using the more traditionalMT technologies of previous systems.
For thisreason, our English-to-ASL MT system has multi-ple processing pathways (Huenerfauth, 2004a).The pathway for handling English input sentencesthat produce CPs includes the scene visualizationsoftware, while other input sentences undergo lesssophisticated processing using a traditional MTapproach (that is easier to implement).
In this way,our CP generation component can actually be lay-ered on top of a pre-existing English-to-ASL MTsystem to give it the ability to produce CPs.
Thismulti-path design is equally applicable to the archi-tecture of written-language MT systems.
The de-sign allows an MT system to combine a resource-intensive deep-processing MT method for difficult(or important) inputs and a resource-light broad-coverage MT method for other inputs.3.5 Evaluation of Multichannel NLGThe lack of an ASL writing system and the mul-tichannel nature of ASL can make NLG or MTsystems which produce ASL animation output dif-ficult to evaluate using traditional automatic tech-niques.
Many such approaches compare a stringproduced by a system to some human-produced?gold-standard?
string.
While we could invent anartificial ASL writing system for the system toproduce as output, it?s not clear that human ASLsigners could accurately or consistently producewritten forms of ASL sentences to serve as ?goldstandards?
for such an evaluation.
And of course,real users of the system would never be shown arti-ficial ?written ASL?
; they would see full anima-tions instead.
User-based studies (where ASLsigners evaluate animation output directly) may bea more meaningful measure of an ASL system.We are planning such an evaluation of a proto-type CP-generation module of the system duringthe summer/fall of 2005.
Members of the deafcommunity who are native ASL signers will viewanimations of classifier predicates produced by thesystem.
As a control, they will also be shown an-imations of CPs produced using 3D motion capturetechnology to digitally record the performance ofCPs by other native ASL signers.
Their evaluationof animations from both sources will be comparedto measure the system?s performance.
The mul-tichannel nature of the signal also makes other in-teresting experiments possible.
To study thesystem?s ability to animate the signer?s hands only,motion-captured ASL could be used to animate thehead/body of the animated character, and the NLGsystem can be used to control only the hands of thecharacter.
Thus, channels of the NLG system canbe isolated for evaluation ?
an experimental designonly available to a multichannel NLG system.4 Unique Design Features for ASL NLGThe design portion of this English-to-ASL projectis nearly complete, and the implementation of thesystem is ongoing.
Evaluations of the system will41be available after the user-based study discussedabove; however, the design itself has highlightedinteresting issues about the requirements of NLGsoftware for sign languages like ASL.The multichannel nature of ASL has led thisproject to study mechanisms for coordinating thevalues of the linguistic models used during genera-tion (including the output animation specificationitself).
The need to handle both the LS and CPsubsystems of the language has motivated: a multi-path MT architecture, a discourse model that storesdata relevant to both subsystems, a model of thespace around the signer capable of storing both LSand CP placeholders, and a phonological modelwhose values can be specified by either subsystem.Since this English-to-ASL MT system is the firstto address ASL classifier predicates, designing anNLG process capable of producing the 3D loca-tions and paths in a CP animation has been a majordesign focus for this project.
These issues havebeen addressed by the system?s use of a 3D modelof placeholders produced by scene-visualizationsoftware and a planning-based NLG process oper-ating on templates of prototypical CP performance.5 Applications Beyond Sign LanguageSign language NLG requires 3D spatial representa-tions and multichannel coordinated output, but it?snot unique in this requirement.
In fact, generationof a communication signal for any language mayrequire these capabilities (even for spoken lan-guages like English).
We have mentionedthroughout this paper how gesture/speech ECAresearchers may be interested in NLG technologiesfor ASL ?
especially if they wish to produce ges-tures that are more linguistically conventional, in-ternally complex, or 3D-topologically precise.Many other computational linguistic applica-tions could benefit from an NLG design with mul-tiple linguistic channels (and indirectly benefitfrom ASL NLG technology).
For instance, NLGsystems producing speech output could encodeprosody, timing, volume, intonation, or other vocaldata as multiple linguistically-determined channelsof the output (in addition to a channel for the stringof words being generated).
And so, ASL NLGresearch not only has exciting accessibility benefitsfor deaf users, but it also serves as a research vehi-cle for NLG technology to produce a variety ofricher-than-text linguistic communication signals.AcknowledgmentsI would like to thank my advisors Mitch Marcusand Martha Palmer for their guidance, discussion,and revisions during the preparation of this work.ReferencesCassell, J., Sullivan, J., Prevost, S., and Churchill, E.(Eds.).
2000.
Embodied Conversational Agents.Cambridge, MA: MIT Press.Holt, J.
1991.
Demographic, Stanford Achievement Test- 8th Edition for Deaf and Hard of Hearing Students:Reading Comprehension Subgroup Results.Huenerfauth, M. 2003.
Survey and Critique of ASLNatural Language Generation and Machine Transla-tion Systems.
Technical Report MS-CIS-03-32,Computer and Information Science, University ofPennsylvania.Huenerfauth, M. 2004a.
A Multi-Path Architecture forMachine Translation of English Text into AmericanSign Language Animation.
In Proceedings of theStudent Workshop of the Human Language Tech-nologies conference / North American chapter of theAssociation for Computational Linguistics annualmeeting: HLT/NAACL 2004, Boston, MA, USA.Huenerfauth, M. 2004b.
Spatial and Planning Models ofASL Classifier Predicates for Machine Translation.10th International Conference on Theoretical andMethodological Issues in Machine Translation: TMI2004, Baltimore, MD.Huenerfauth, M. 2005.
American Sign Language SpatialRepresentations for an Accessible User-Interface.
In3rd International Conference on Universal Access inHuman-Computer Interaction.
Las Vegas, NV, USA.Kopp, S., Tepper, P., and Cassell, J.
2004.
TowardsIntegrated Microplanning of Language and IconicGesture for Multimodal Output.
Int?l Conference onMultimodal Interfaces, State College, PA, USA.Liddell, S. 2003.
Grammar, Gesture, and Meaning inAmerican Sign Language.
UK: Cambridge U. Press.Mitchell, R. 2004.
How many deaf people are there inthe United States.
Gallaudet Research Institute, GradSchool & Prof. Progs.
Gallaudet U.  June 28, 2004.http://gri.gallaudet.edu/Demographics/deaf-US.phpMorford, J., and MacFarlane, J.
2003.
Frequency Char-acteristics of ASL.
Sign Language Studies, 3:2.Neidle, C., Kegl, J., MacLaughlin, D., Bahan, B., andLee R.G.
2000.
The Syntax of American Sign Lan-guage: Functional Categories and Hierarchical Struc-ture.
Cambridge, MA: The MIT Press.42
