Proceedings of the 6th Workshop on Statistical Machine Translation, pages 159?165,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsExpected BLEU Training for Graphs: BBN System Description forWMT11 System Combination TaskAntti-Veikko I. Rosti?
and Bing Zhang and Spyros Matsoukas and Richard SchwartzRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA{arosti,bzhang,smatsouk,schwartz}@bbn.comAbstractBBN submitted system combination outputsfor Czech-English, German-English, Spanish-English, and French-English language pairs.All combinations were based on confusionnetwork decoding.
The confusion networkswere built using incremental hypothesis align-ment algorithm with flexible matching.
Anovel bi-gram count feature, which can penal-ize bi-grams not present in the input hypothe-ses corresponding to a source sentence, wasintroduced in addition to the usual decoderfeatures.
The system combination weightswere tuned using a graph based expectedBLEU as the objective function while incre-mentally expanding the networks to bi-gramand 5-gram contexts.
The expected BLEUtuning described in this paper naturally gen-eralizes to hypergraphs and can be used tooptimize thousands of weights.
The com-bination gained about 0.5-4.0 BLEU pointsover the best individual systems on the officialWMT11 language pairs.
A 39 system multi-source combination achieved an 11.1 BLEUpoint gain.1 IntroductionThe confusion networks for the BBN submissionsto the WMT11 system combination task were builtusing incremental hypothesis alignment algorithm?This work was supported by DARPA/I2O Contract No.HR0011-06-C-0022 under the GALE program (Approved forPublic Release, Distribution Unlimited).
The views, opinions,and/or findings contained in this article/presentation are those ofthe author/presenter and should not be interpreted as represent-ing the official views or policies, either expressed or implied,of the Defense Advanced Research Projects Agency or the De-partment of Defense.with flexible matching (Rosti et al, 2009).
A novelbi-gram count feature was used in addition to thestandard decoder features.
The N-best list based ex-pected BLEU tuning (Rosti et al, 2010), similar tothe one proposed by Smith and Eisner (2006), wasextended to operate on word lattices.
This method isclosely related to the consensus BLEU (CoBLEU)proposed by Pauls et al (2009).
The minimum oper-ation used to compute the clipped counts (matches)in the BLEU score (Papineni et al, 2002) was re-placed by a differentiable function, so there wasno need to use sub-gradient ascent as in CoBLEU.The expected BLEU (xBLEU) naturally generalizesto hypergraphs by simply replacing the forward-backward algorithm with inside-outside algorithmwhen computing the expected n-gram counts andsufficient statistics for the gradient.The gradient ascent optimization of the xBLEUappears to be more stable than the gradient-free di-rect 1-best BLEU tuning or N -best list based min-imum error rate training (Och, 2003), especiallywhen tuning a large number of weights.
On the of-ficial WMT11 language pairs with up to 30 weights,there was no significant benefit from maximizingxBLEU.
However, on a 39 system multi-sourcecombination (43 weights total), it yielded a signif-icant gain over gradient-free BLEU tuning and N -best list based expected BLEU tuning.2 Hypothesis Alignment and FeaturesThe incremental hypothesis alignment with flexiblematching (Rosti et al, 2009) produces a confusionnetwork for each system output acting as a skele-ton hypothesis for the ith source sentence.
A con-fusion network is a graph where all paths visit all159vertices.
Consecutive vertices are connected by oneor more edges representing alternatives.
Each edgel is associated with a token and a set of scores.
A to-ken may be a word, punctuation symbol, or specialNULL token indicating a deletion in the alignment.The set of scores includes a vector ofNs system spe-cific confidences, siln, indicating whether the tokenwas aligned from the output of the system n.1 Otherscores may include a language model (LM) scoreas well as non-NULL and NULL token indicators(Rosti et al, 2007).
As Rosti et al (2010) described,the networks for all skeletons are connected to a startand end vertex with NULL tokens in order to forma joint lattice with multiple parallel networks.
Theedges connecting the start vertex to the initial ver-tices in each network have a heuristic prior estimatedfrom the alignment statistics at the confidence cor-responding to the skeleton system.
The edges con-necting the final vertices of each network to the endvertex have all system confidences set to one, so thefinal edge does not change the score of any path.A single word confidence is produced from theconfidence vector by taking an inner product withthe system weights ?n which are constrained to sumto one,2?n ?n = 1.
The total edge score is pro-duced by a log-linear interpolation of the word con-fidence with other features film:sil = log( Ns?n=1?nsiln)+?m?mfilm (1)The usual features film include the LM score as wellas non-NULL and NULL token indicators.
Basedon an analysis of the system combination outputs, alarge number of bi-grams not present in any inputhypothesis are often produced, some of which areclearly ungrammatical despite the LM.
These novelbi-grams are due to errors in hypothesis alignmentand the confusion network structure where any wordfrom the incoming edges of a vertex can be followedby any word from the outgoing edges.
After expand-ing and re-scoring the joint lattice with a bi-gram, anew feature indicating the presence of a novel bi-gram may be added on the edges.
A negative weight1The confidences are binary when aligning 1-best outputs.More elaborate confidences may be estimated fromN -best lists;see for example Rosti et al (2007).2See (Rosti et al, 2010) for a differentiable constraint.for this feature discourages novel bi-grams in theoutput during decoding.3 Weight OptimizationThe most common objective function used in ma-chine translation is the BLEU-N score (Papineni etal., 2002) defined as follows:3BLEU =N?n=1(?i mni?i hni) 1N?(1?
?i ri?i h1i)(2)where N is the maximum n-gram order (typicallyN = 4), mni is the number of n-gram matches(clipped counts) between the hypothesis ei and ref-erence e?i for segment i, hni is the number of n-gramsin the hypothesis, ri is the reference length,4 and?
(x) = min(1.0, ex) is the brevity penalty.
Usinggn to represent an arbitrary n-gram, cign to repre-sent the count of gn in hypothesis ei, and c?ign torepresent the count of gn in reference e?i, the BLEUstatistics can be defined as follows:mni =?gnmin(cign , c?ign) (3)hni =?gncign (4)The unigram count h1i is simply the hypothesislength and higher order n-gram counts can be ob-tained by hni = hn?1i ?
1.
The reference n-gramcounts for each sentence can be stored in an n-gramtrie for efficient scoring.5The BLEU score is not differentiable due to theminimum operations on the matches mni and brevitypenalty ?(x).
Therefore gradient-free optimizationalgorithms, such as Powell?s method or downhillsimplex (Press et al, 2007), are often employed inweight tuning (Och, 2003).
System combinationweights tuned using the downhill simplex methodto directly optimize 1-best BLEU score of the de-coder outputs served as the first baseline in the ex-periments.
The distributed optimization approachused here was first described in (Rosti et al, 2010).3Superscripts indicate the n-gram order in all variables inthis paper.
They are used as exponents only for the constant e.4If multiple references are available, ri is the referencelength closest to the hypothesis length, h1i .5If multiple references are available, the maximum n-gramcounts are stored.160A set of system combination weights was first tunedfor unpruned lattices re-scored with a bi-gram LM.Another set of re-scoring weights was tuned for 300-best lists re-scored with a 5-gram LM.3.1 Graph expected BLEUGradient-free optimization algorithms work wellwith a relatively small number of weights.
Weightoptimization for a 44 system combination in Rostiet al (2010) was shown to be unstable with down-hill simplex algorithm.
Instead, an N-best list basedexpected BLEU tuning with gradient ascent yieldedbetter results.
This served as the second baseline inthe experiments.
The objective function is definedby replacing the n-gram statistics with expected n-gram counts and matches as in (Smith and Eisner,2006), and brevity penalty with a differentiable ap-proximation:?
(x) =ex ?
11 + e1000x+ 1 (5)An N-best list represents a subset of the search spaceand multiple decoding iterations with N-best listmerging is required to improve convergence.
In thiswork, expected BLEU tuning is extended for lat-tices by replacing the minimum operation in n-grammatches with another differentiable approximation.The expected n-gram statistics for path j, which cor-respond to the standard statistics in Equations 3 and4, are defined as follows:m?ni =?gn?
( ?j?JiPijcijgn , c?ign)(6)h?ni =?gn?j?JiPijcijgn (7)where Ji is the set of all paths in a lattice or allderivations in a hypergraph for the ith source sen-tence, Pij is the posterior of path j, and cijgn isthe count of n-grams gn in hypothesis eij on pathj.
The path posterior and approximate minimum aredefined by:Pij =?l?j e?sil?j??Ji?l?j?
e?sil(8)?
(x, c) =x?
c1 + e1000(x?c)+ c (9)where sil is the total score on edge l defined in Equa-tion 1 and ?
is an edge score scaling factor.
Thescaling factor affects the shape of the edge posteriordistribution; ?
> 1.0 makes the edge posteriors onthe 1-best path higher than edge posteriors on otherpaths and ?
< 1.0 makes the posteriors on all pathsmore uniform.The graph expected BLEU can be factored asxBLEU = ePB where:P =1NN?n=1(log?im?ni ?
log?ih?ni)(10)B = ?(1?
?i ri?i h?1i)(11)and ri is the reference length.6 This objective func-tion is closely related to CoBLEU (Pauls et al,2009).
Unlike CoBLEU, xBLEU is differentiableand standard gradient ascent algorithms can be usedto find weights that maximize the objective.Note, the expected counts can be expressed interms of edge posteriors as:?j?JiPijcijgn =?l?Lipil?
(cnil, gn) (12)where Li is the set of all edges for the ith sentence,pil is the edge posterior, ?
(x, c) is the Kroneckerdelta function which is 1 if x = c and 0 if x 6= c, andcnil is the n-gram context of edge l. The edge posteri-ors can be computed via standard forward-backwardalgorithm for lattices or inside-outside algorithm forhypergraphs.
As with the BLEU statistics, only ex-pected unigram counts h?1i need to be accumulatedfor the hypothesis n-gram counts in Equation 7 ash?ni = h?n?1i ?
1 for n > 1.
Also, the expectedn-gram counts for each graph can be stored in ann-gram trie for efficient gradient computation.3.2 Gradient of graph expected BLEUThe gradient of the xBLEU with respect to weight ?can be factored as:?xBLEU??=?i?l?Li?sil???j?Ji?xBLEU?
logPij?
logPij?sil(13)where the gradient of the log-path-posterior with re-spect to the edge score is given by:?
logPij?sil= ?(?
(l ?
j)?
pil)(14)6If multiple reference are available, ri is the reference lengthclosest to the expected hypothesis length h?1i .161?xBLEU?
?= ?eP(BNN?n=1?i(m?nik ?mnikmn?h?nik ?
hnikhn))+ C??(1?
C)?ih?1ik ?
h1ikh1(15)and ?
(l ?
j) is one if edge l is on path j, and zerootherwise.
Using the factorization xBLEU = ePB,Equation 13 can be expressed using sufficient statis-tics as shown in Equation 15, where ??
(x) is thederivative of ?
(x) with respect to x, mn =?i m?ni ,hn =?i h?ni , C =?ri/?i h?1i , and the remainingsufficient statistics are given by:?
?ign = ??
( ?j?JiPijcijgn , c?ign)mnik =(?l?Lipil?sil??)(?j?JiPij?gn?
?igncijgn)m?nik =?l?Li?sil???j:l?JiPij?gn?
?igncijgnhnik =(?l?Lipil?sil??
)(?j?JiPij?gncijgn)h?nik =?l?Li?sil??
?j:l?JiPij?gncijgnwhere ??
(x, c) is the derivative of ?
(x, c) with re-spect to x, and the parentheses in the equations formnik and hnik signify that the second terms do not de-pend on the edge l.3.3 Forward-backward algorithm underexpectation semiringThe sufficient statistics for graph expected BLEUcan be computed using expectation semirings (Liand Eisner, 2009).
Instead of computing singleforward/backward or inside/outside scores, addi-tional n-gram elements are tracked for matches andcounts.
For example in a bi-gram graph, the ele-ments for edge l are represented by a 5-tuple7 sl =?pl, r1lh, r2lh, r1lm, r2lm?
where pl = e?sil and:rnlh =?gn?
(cnil, gn)e?sil (16)rnlm =?gn?
?igne?sil (17)Assuming the lattice is topologically sorted, the for-ward algorithm8 under expectation semiring for a 3-7The sentence index i is dropped for brevity.8For inside-outside algorithm, see (Li and Eisner, 2009).tuple9 sl = ?pl, r1lh, r1lm?
is defined by:?0 = ?1, 0, 0?
(18)?v =?l?Iv?u(l) ?
sl (19)where Iv is the set of all edges with target vertexv and u(l) is the source vertex for edge l, and theoperations are defined by:s1 ?
s2 = ?p1 + p2, r11h + r12h, r11m + r12m?s1 ?
s2 = ?p1p2, p1r12h + p2r11h, p1r12m + p2r11m?The backward algorithm for ?u can be implementedvia the forward algorithm in reverse through thegraph.
The sufficient statistics for the gradient canbe accumulated during the backward pass notingthat:?j?JiPij?gn?
?igncijgn =rnm(?0)p(?0)(20)?j?JiPij?gncijgn =rnh(?0)p(?0)(21)where rnm(?)
and rnh(?)
extract the nth order r ele-ments from the tuple for matches and counts, respec-tively, and p(?)
extracts the p element.
The statisticsfor the paths traveling via edge l can be computedby:?j:l?JiPij?gn?
?igncijgn =rnm(?u ?
sl ?
?v)p(?0)(22)?j:l?JiPij?gncijgn =rnh(?u ?
sl ?
?v)p(?0)(23)where the u and v subscripts in ?u and ?v are thestart and end vertices for edge l. To avoid under-flow, all the computations can be carried out in logdomain.9A 3-tuple for uni-gram counts is used as an example in or-der to save space.
In a 5-tuple for bi-gram counts, all r elementsare computed independently of other r elements with the sameoperations.
Similarly, tri-gram counts require 7-tuples and four-gram counts require 9-tuples.162tune cz-en de-en es-en fr-enSystem TER BLEU TER BLEU TER BLEU TER BLEUworst 66.03 18.09 69.03 16.28 60.56 21.02 62.75 21.83best 53.75 28.36 58.39 24.28 50.26 30.55 50.48 30.87latBLEU 53.99 29.25 56.70 26.49 48.34 34.55 48.90 33.90nbExpBLEU 54.43 29.04 56.36 27.33 48.44 34.73 48.58 34.23latExpBLEU 53.89 29.37 56.24 27.36 48.27 34.93 48.53 34.24test cz-en de-en es-en fr-enSystem TER BLEU TER BLEU TER BLEU TER BLEUworst 65.35 17.69 69.03 15.83 61.22 19.79 62.36 21.36best 52.21 29.54 58.00 24.16 50.15 30.14 50.15 30.32latBLEU 52.80 29.89 55.87 26.22 48.29 33.91 48.51 32.93nbExpBLEU 52.97 29.93 55.77 26.52 48.39 33.86 48.25 32.94latExpBLEU 52.68 29.99 55.74 26.62 48.30 34.10 48.17 32.91Table 1: Case insensitive TER and BLEU scores on newssyscombtune (tune) and newssyscombtest (test)for combinations of outputs from four source languages.
Three tuning methods were used: lattice BLEU (latBLEU),N-best list based expected BLEU (nbExpBLEU), and lattice expected BLEU (latExpBLEU).3.4 Entropy on a graphExpanding the joint lattice to n-gram orders aboven = 2 is often impractical without pruning.
If theedge posteriors are not reliable, which is usuallythe case for unoptimized weights, pruning might re-move good quality paths from the graph.
As a com-promise, an incremental expansion strategy may beadopted by first expanding and re-scoring the latticewith a bi-gram, optimizing weights for xBLEU-2,and then expanding and re-scoring the lattice witha 5-gram.
Pruning should be more reliable with theedge posteriors computed using the tuned bi-gramweights.
A second set of weights may be tuned withthe 5-gram graph to maximize xBLEU-4.When the bi-gram weights are tuned, it may bebeneficial to increase the edge score scaling factorto focus the edge posteriors to the 1-best path.
Onthe other hand, a lower scaling factor may be bene-ficial when tuning the 5-gram weights.
Rosti et al(2010) determined the scaling factor automaticallyby fixing the perplexity of the merged N -best listsused in tuning.
Similar strategy may be adopted inincremental n-gram expansion of the lattices.Entropy on a graph can also be computed usingthe expectation semiring formalism (Li and Eisner,2009) by defining sl = ?pl, rl?
where pl = e?sil andrl = log pl.
The entropy is given by:Hi = log p(?0)?r(?0)p(?0)(24)where p(?0) and r(?0) extract the p and r elementsfrom the 2-tuple ?0, respectively.
The average targetentropy over all sentences was set manually to 3.0in the experiments based on the tuning convergenceand size of the pruned 5-gram lattices.4 Experimental EvaluationSystem outputs for all language pairs with En-glish as the target were combined (cz-en,de-en, es-en, and fr-en).
Unpruned Englishbi-gram and 5-gram language model compo-nents were trained using the WMT11 corpora:EuroParl, GigaFrEn, UNDoc Es, UNDoc Fr,NewsCommentary, News2007, News2008,News2009, News2010, and News2011.Additional six Gigaword v4 components in-cluded: AFP, APW, XIN+CNA, LTW, NYT, andHeadlines+Datelines.
The total numberof words used to train the LMs was about 6.4billion.
Interpolation weights for the sixteencomponents were tuned to minimize perplexity onthe newstest2010-ref.en development set.The modified Kneser-Ney smoothing (Chen and163Goodman, 1998) was used in training.
Experimentsusing a LM trained on the system outputs and inter-polated with the general LM were also conducted.The interpolation weights between 0.1 and 0.9 weretried, and the weight yielding the highest BLEUscore on the tuning set was selected.
A tri-gram truecasing model was trained on all the LM trainingdata.
This model was used to restore the case of thelower-case system combination output.All twelve 1-best system outputs on cz-en, 26outputs on de-en, 16 outputs on es-en, and 24outputs on fr-en were combined.
Three differentweight optimization methods were tried.
First, lat-tice based 1-best BLEU optimization of the bi-gramdecoding weights followed by N-best list basedBLEU optimization of 5-gram re-scoring weightsusing 300-best lists, both using downhill simplex.Second, N-best list based expected BLEU optimiza-tion of the bi-gram and 5-gram weights using 300-best lists with merging between bi-gram decodingiterations.
Third, lattice based expected BLEU opti-mization of bi-gram and 5-gram decoding weights.The L-BFGS (Liu and Nocedal, 1989) algorithmwas used in gradient ascent.
Results for all four sin-gle source experiments are shown in Table 1, includ-ing case insensitive TER (Snover et al, 2006) andBLEU scores for the worst and best systems, andthe system combination outputs for the three tuningmethods.
The gains on tuning and test sets were con-sistent, though relatively smaller on cz-en due toa single system (online-B) dominating the othersystems by about 5-6 BLEU points.
The tuningmethod had very little influence on the test set scoresapart from de-en where the lattice BLEU opti-mization yields slightly lower BLEU scores.
Thisseems to suggest that the gradient free optimizationis not as stable with a larger number of weights.10The novel bi-gram feature did not have significantinfluence on the TER or BLEU scores, but the num-ber of novel bi-grams was reduced by up to 100%.Finally, experiments combining 39 system out-puts by taking the top half of the outputs from eachlanguage pair were performed.
The selection wasbased on case insensitive BLEU scores on the tun-ing set.
Table 2 shows the scores for seven combi-10A total number of 30 weights, 26 system and 4 featureweights, were tuned for de-en.xx-en tune testSystem TER BLEU TER BLEUworst 62.81 21.19 62.92 20.29best 51.11 30.87 50.80 30.32latBLEU 40.95 40.75 41.06 39.81+biasLM 41.18 40.90 41.16 39.90nbExpBLEU 40.81 41.36 41.05 40.15+biasLM 40.72 41.99 40.65 40.89latExpBLEU 40.57 41.68 40.62 40.60+biasLM 40.42 42.23 40.52 41.38-nBgF 40.85 41.41 40.88 40.55Table 2: Case insensitive TER and BLEU scores onnewssyscombtune (tune) and newssyscombtest(test) for xx-en combination.
Combinations using lat-tice BLEU tuning (latBLEU), N-best list based expectedBLEU tuning (nbExpBLEU), and lattice expected BLEUtuning (latExpBLEU) with and without the system out-put biased LM (biasLM) are shown.
Final row, markednBgF, corresponds to the above tuning without the novelbi-gram feature.nations using the three tuning methods with or with-out the system output biased LM, and finally withoutthe novel bi-gram count feature.
There is a clear ad-vantage from the expected BLEU tuning on the tun-ing set, and lattice tuning yields better scores thanN-best list based tuning.
The difference betweenlatBLEU and nbExpBLEU without biasLM isnot quite as large on the test set but latExpBLEUyields significant gains over both.
The biasLM alsoyields significant gains on all but latBLEU tuning.Finally, removing the novel bi-gram count featureresults in a significant loss, probably due to the largenumber of input hypotheses.
The number of novelbi-grams in the test set output was reduced to zerowhen using this feature.5 ConclusionsThe BBN submissions for WMT11 system combi-nation task were described in this paper togetherwith a differentiable objective function, graph ex-pected BLEU, which scales well for a large numberof weights and can be generalized to hypergraphs.System output biased language model and a novelbi-gram count feature also gave significant gains ona 39 system multi-source combination.164ReferencesStanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical Report TR-10-98, Computer ScienceGroup Harvard University.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 40?51.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45(3):503?528.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318.Adam Pauls, John DeNero, and Dan Klein.
2009.
Con-sensus training for consensus decoding in machinetranslation.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 1418?1427.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2007.
Numerical recipes:the art of scientific computing.
Cambridge UniversityPress, 3rd edition.Antti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved word-level system combi-nation for machine translation.
In Proceedings of the45th Annual Meeting of the Association of Computa-tional Linguistics, pages 312?319.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2009.
Incremental hypothe-sis alignment with flexible matching for building con-fusion networks: BBN system description for WMT09system combination task.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages61?65.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2010.
BBN system descriptionfor WMT10 system combination task.
In Proceedingsof the Fifth Workshop on Statistical Machine Transla-tion, pages 321?326.David A. Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 787?794.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciula, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Conference of the Associa-tion for Machine Translation in the Americas, pages223?231.165
