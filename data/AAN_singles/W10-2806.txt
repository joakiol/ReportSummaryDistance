Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38?44,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsSemantic Composition with Quotient AlgebrasDaoud ClarkeUniversity of HertfordshireHatfield, UKdaoud@metrica.netRudi LutzUniversity of SussexBrighton, UKrudil@sussex.ac.ukDavid WeirUniversity of SussexBrighton, UKdavidw@sussex.ac.ukAbstractWe describe an algebraic approach forcomputing with vector based semantics.The tensor product has been proposed asa method of composition, but has the un-desirable property that strings of differentlength are incomparable.
We consider howa quotient algebra of the tensor algebra canallow such comparisons to be made, offer-ing the possibility of data-driven models ofsemantic composition.1 IntroductionVector based techniques have been exploited in awide array of natural language processing appli-cations (Schu?tze, 1998; McCarthy et al, 2004;Grefenstette, 1994; Lin, 1998; Bellegarda, 2000;Choi et al, 2001).
Techniques such as latent se-mantic analysis and distributional similarity anal-yse contexts in which terms occur, building up avector of features which incorporate aspects of themeaning of the term.
This idea has its origins inthe distributional hypothesis of Harris (1968), thatwords with similar meanings will occur in similarcontexts, and vice-versa.However, there has been limited attention paidto extending this idea beyond individual words,so that the distributional meaning of phrases andwhole sentences can be represented as vectors.While these techniques work well at the wordlevel, for longer strings, data becomes extremelysparse.
This has led to various proposals explor-ing methods for composing vectors, rather than de-riving them directly from the data (Landauer andDumais, 1997; Foltz et al, 1998; Kintsch, 2001;Widdows, 2008; Clark et al, 2008; Mitchell andLapata, 2008; Erk and Pado, 2009; Preller andSadrzadeh, 2009).
Many of these approaches usea pre-defined composition operation such as ad-dition (Landauer and Dumais, 1997; Foltz et al,1998) or the tensor product (Smolensky, 1990;Clark and Pulman, 2007; Widdows, 2008) whichcontrasts with the data-driven definition of com-position developed here.2 Tensor AlgebrasFollowing the context-theoretic semantics ofClarke (2007), we take the meaning of strings asbeing described by a multiplication on a vectorspace that is bilinear with respect to the additionof the vector space, i.e.x(y + z) = xy + xz (x+ y)z = xz + yzIt is assumed that the multiplication is associative,but not commutative.
The resulting structure is anassociative algebra over a field ?
or simply analgebra when there is no ambiguity.One commonly used bilinear multiplication op-erator on vector spaces is the tensor product (de-noted ?
), whose use as a method of combiningmeaning was first proposed by Smolensky (1990),and has been considered more recently by Clarkand Pulman (2007) and Widdows (2008), who alsolooked at the direct sum (which Widdows callsthe direct product, denoted ?
).We give a very brief account of the tensor prod-uct and direct sum in the finite-dimensional case;see (Halmos, 1974) for formal and complete defi-nitions.
Roughly speaking, if u1, u2, .
.
.
un forman orthonormal basis for a vector space U andv1, v2, .
.
.
vm form an orthonormal basis for vectorspace V , then the space U ?V has dimensionalitynm with an orthonormal basis formed by the setof all ordered pairs (ui, vj), denoted by ui ?
vj ,of the individual basis elements.
For arbitrary el-ements u =?ni=1 ?iui and v =?mj=1 ?jvj thetensor product of u and v is then given byu?
v =n?im?j?i?j ui ?
vj38For two finite dimensional vector spaces U andV (over a field F ) of dimensionality n and m re-spectively, the direct sum U ?
V is defined as thecartesian product U ?
V together with the oper-ations (u1, v1) + (u2, v2) = (u1 + u2, v1 + v2),and a(u1, v1) = (au1, av1), for u1, u2 ?
U ,v1, v2 ?
V and a ?
F .
In this case the vectorsu1, u2, .
.
.
un, v1, v2, .
.
.
vm form an orthonormalset of basis vectors in U ?
V , which is thus ofdimensionality n + m. In this case one normallyidentifies U with the set of vectors in U ?V of theform (u, 0), and V with the set of vectors of theform (0, v).
This construction makes U ?
V iso-morphic to V ?U , and thus the direct sum is oftentreated as commutative, as we do in this paper.The motivation behind using the tensor productto combine meanings is that it is very fine-grained.So, if, for example, red is represented by a vectoru consisting of a feature for each noun that is mod-ified by red, and apple is represented by a vectorv consisting of a feature for each verb that occurswith apple as a direct object, then red apple willbe represented by u ?
v with a non-zero compo-nent for every pair of non-zero features (one fromu and one from v).
So, there is a non-zero ele-ment for each composite feature, something thathas been described as red, and something that hasbeen done with an apple, for example, sky and eat.Both ?
and ?
are intuitively appealing as se-mantic composition operators, since u and v arereconstructible from each of u?
v and u?
v, andthus no information is lost in composing u and v.Conversely, this is not possible with ordinary vec-tor addition, which also suffers from the fact that itis strictly commutative (not simply up to isomor-phism like?
), whereas natural language composi-tion is in general manifestly non-commutative.We make use of a construction called the tensoralgebra on a vector space V (where V is a spaceof context features), defined as:T (V ) = R?
V ?
(V ?
V )?
(V ?
V ?
V )?
?
?
?Any element of T (V ) can be described as a sum ofcomponents with each in a different tensor powerof V .
Multiplication is defined as the tensor prod-uct on these components, and extended linearly tothe whole of T (V ).
We define the degree of a vec-tor u in T (V ) to be the tensor power of its high-est dimensional non-zero component, and denoteit deg(u); so for example, both v?v and u?
(v?v)have degree two, for 0 6= u, v ?
V .
We restrictT (V ) to only contain vectors of finite degree.A standard way to compare elements of a vectorspace is to make use of an inner product, whichprovides a measure of semantic distance on thatspace.
Assuming we have an inner product ?
?, ??
onV , T (V ) can be given an inner product by defining?
?, ??
= ??
for ?, ?
?
R, and?x1 ?
y1, x2 ?
y2?
= ?x1, x2?
?y1, y2?for x1, y1, x2, y2 ?
V , and then extending this in-ductively (and by linearity) to the whole of T (V ).We assume that words are associated with vec-tors in V , and that the higher tensor powers repre-sent strings of words.
The problem with the tensorproduct as a method of composition, given the in-ner product as we have defined it, is that stringsof different lengths will have orthogonal vectors,clearly a serious problem, since strings of differentlengths can have similar meanings.
In our previousexample, the vector corresponding to the conceptred apple lives in the vector space U ?
V , and sowe have no way to compare it to the space V ofnouns, even though red apple should clearly be re-lated to apple.Previous work has not made full use of the ten-sor product space; only tensor products are used,not sums of tensor products, giving us the equiva-lent of the product states of quantum mechanics.Our approach imposes relations on the vectors ofthe tensor product space that causes some productstates to become equivalent to entangled states,containing sums of tensor products of different de-grees.
This allows strings of different lengths toshare components.
We achieve this by construct-ing a quotient algebra.3 Quotient AlgebrasAn ideal I of an algebra A is a sub-vector spaceof A such that xa ?
I and ax ?
I for all a ?
Aand all x ?
I .
An ideal introduces a congruence?
on A defined by x ?
y if and only if x?
y ?
I .For any set of elements ?
?
A there is a uniqueminimal ideal I?
containing all elements of ?
; thisis called the ideal generated by ?.
The quotientalgebra A/I is the set of all equivalence classesdefined by this congruence.
Multiplication is de-fined on A/I by the multiplication on A, since ?is a congruence.By adding an element x ?
y to the generatingset ?
of an ideal, we are saying that we want toset x ?
y to zero in the quotient algebra, whichhas the effect of setting x equal to y.
Thus, if we39have a set of pairs of vectors that we wish to makeequal in the quotient algebra, we put their differ-ences in the generating set of the ideal.
Note thatputting a single vector v in the generating set canhave knock-on effects, since all products of v withelements of A will also end up in the ideal.Although we have an inner product defined onT (V ), we are not aware of any satisfactory methodfor defining an inner product on T (V )/I , a con-sequence of the fact that both T (V ) and I arenot complete.
Instead, we define an inner prod-uct on a space which contains the quotient algebra,T (V )/I .
Rather than considering all elements ofthe ideal when computing the quotient, we con-sider a sub-vector space of the ideal, limiting our-selves to the space Gk generated from ?
by onlyallowing multiplication by elements up to a certaindegree, k.Let us denote the vector subspace generated bylinearity alone (no multiplications) from a sub-set ?
of T (V ) by G(?).
Also suppose B ={e1, .
.
.
, eN} is a basis for V .
We then definethe spaces Gk as follows.
Define sets ?k (k =0, 1, 2, .
.
.)
inductively as follows:?0 = ?
?k = ?k?1 ?
{(ei ?
?k?1)|ei ?
B}?
{(?k?1 ?
ei)|ei ?
B}DefineGk = G(?k)We note thatG0 ?
G1 ?
.
.
.
Gk ?
.
.
.
?
I ?
T (V )form an increasing sequence of linear vector sub-spaces of T (V ), and thatI =?
?k=0GkThis means that for any x ?
I there exists a small-est k such that for all k?
?
k we have that x ?
Gk?
.Lemma.
Let x ?
I, x 6= 0 and let deg(x) = d.Then for all k ?
d ?
mindeg(?)
we have thatx ?
Gk, where mindeg(?)
is defined to be theminimum degree of the non-zero components oc-curring in the elements of ?.Proof.
We first note that for x ?
I it mustbe the case that deg(x) ?
mindeg(?)
since Iis generated from ?.
Therefore we know d ?mindeg(?)
?
0.
We only need to show thatx ?
Gd?mindeg(?).
Let k?
be the smallest in-teger such that x ?
Gk?
.
Since x 6?
Gk?
?1 itmust be the case that the highest degree term ofx comes from V ?
Gk?
?1 ?
Gk?
?1 ?
V .
There-fore k?
+ mindeg(?)
?
d ?
k?
+ maxdeg(?
).From this it follows that the smallest k?
for whichx ?
Gk?
satisfies k?
?
d ?
mindeg(?
), and weknow x ?
Gk for all k ?
k?.
In particular x ?
Gkfor k ?
d?mindeg(?
).We show that T (V )/Gk (for an appropriatechoice of k) captures the essential features ofT (V )/I in terms of equivalence:Proposition.
Let deg(a ?
b) = d and let k ?d ?
mindeg(?).
Then a ?
b in T (V )/Gk if andonly if a ?
b in T (V )/I .Proof.
Since Gk ?
I , the equivalence class of anelement a in T (V )/I is a superset of the equiva-lence class of a in T (V )/Gk, which gives the for-ward implication.
The reverse follows from thelemma above.In order to define an inner product onT (V )/Gk, we make use of the result of Berbe-rian (1961) that if M is a finite-dimensionallinear subspace of a pre-Hilbert space P , thenP = M ?
M?, where M?
is the orthogonalcomplement of M in P .
In our case this impliesT (V ) = Gk ?
G?k and that every elementx ?
T (V ) has a unique decomposition asx = y + x?k where y ?
Gk and x?k ?
G?k .
Thisimplies that T (V )/Gk is isomorphic to G?k , andthat for each equivalence class [x]k in T (V )/Gkthere is a unique corresponding element x?k ?
G?ksuch that x?k ?
[x]k. This element x?k can bethought of as the canonical representation of allelements of [x]k in T (V )/Gk, and can be foundby projecting any element in an equivalence classonto G?k .
This enables us to define an innerproduct on T (V )/Gk by ?
[x]k, [y]k?k = ?x?k, y?k?.The idea behind working in the quotient algebraT (V )/I rather than in T (V ) is that the elementsof the ideal capture differences that we wish to ig-nore, or alternatively, equivalences that we wish toimpose.
The equivalence classes in T (V )/I repre-sent this imposition, and the canonical representa-tives in I?
are elements which ignore the distinc-tions between elements of the equivalence classes.40However, by using Gk, for some k, instead ofthe full ideal I , we do not capture some of theequivalences implied by I .
We would, therefore,like to choose k so that no equivalences of impor-tance to the sentences we are considering are ig-nored.
While we have not precisely established aminimal value for k that achieves this, in the dis-cussion that follows, we set k heuristically ask = l ?mindeg(?
)where l is the maximum length of the sentencescurrently under consideration, and ?
is the gen-erating set for the ideal I .
The intuition behindthis is that we wish all vectors occurring in ?
tohave some component in common with the vec-tor representation of our sentences.
Since com-ponents in the ideal are generated by multipli-cation (and linearity), in order to allow the ele-ments of ?
containing the lowest degree compo-nents to potentially interact with our sentences,we will have to allow multiplication of those el-ements (and all others) by components of degreeup to l ?mindeg(?
).Given a finite set ?
?
T (V ) of elements gen-erating the ideal I , to compute canonical repre-sentations, we first compute a generating set ?kfor Gk following the inductive definition givenearlier, and removing any elements that are notlinearly independent using a standard algorithm.Using the Gram-Schmidt process (Trefethen andBau, 1997), we then calculate an orthonormal ba-sis ??
for Gk, and, by a simple extension of Gram-Schmidt, compute the projection of a vector u ontoG?k using the basis ?
?.We now show how ?, the set of vectors gener-ating the ideal, can be constructed on the basis ofa tree-bank, ensuring that the vectors for any twostrings of the same grammatical type are compa-rable.4 Data-driven CompositionSuppose we have a tree-bank, its associated tree-bank grammar G, and a way of associating a con-text vector with every occurrence of a subtree inthe tree-bank (where the vectors indicate the pres-ence of features occurring in that particular con-text).
The context vector associated with a spe-cific occurrence of a subtree in the tree-bank is anindividual context vector.We assume that for every rule, there is a distin-guished non-terminal on the right hand side whichwe call the head.
We also assume that for everyproduction pi there is a linear function ?pi from thespace generated by the individual context vectorsof the head to the space generated by the individ-ual context vectors of the left hand side.
Whenthere is no ambiguity, we simply denote this func-tion ?.Let X?
be the sum over all individual vectors ofsubtrees rooted withX in the tree-bank.
Similarly,for each Xj in the right-hand-side of the rule pii :X ?
X1 .
.
.
Xr(pii), where r(pi) is the rank of pi,let i?,j be the sum over the individual vectors ofthose subtrees rooted with Xj where the subtreeoccurs as the jth daughter of a local tree involvingthe production pii in the tree-bank.For each rule pi : X ?
X1 .
.
.
Xr with headXhwe add vectors?pi,i = ?(ei)?X?1?.
.
.?X?h?1?ei?X?h+1?.
.
.
?X?rfor each basis element ei of VXh to the generatingset.
The reasoning behind this is to ensure that themeaning corresponding to a vector associated withthe head of a rule is maintained as it is mapped tothe vector space associated with the left hand sideof the rule.It is often natural to assume that the individualcontext vector of a non-terminal is the same as theindividual context vector of its head.
In this case,we can take ?
to be the identity map.
In particular,for a rule of the form pi : X ?
X1, then ?pi,i iszero.It is important to note at this point that we havepresented only one of many ways in which a gram-mar could be used to generate an ideal.
In partic-ular, it is possible to add more vectors to the ideal,allowing more fine-grained distinctions, for exam-ple through the use of a lexicalised grammar.For each sentence w, we compute the tensorproduct w?
= a?1 ?
a?2 ?
?
?
?
?
a?n where the stringof words a1 .
.
.
an form w, and each a?i is a vectorin V .
For a sentence w we find an element w?O ofthe orthogonal complement of Gk in T (V ) suchthat w?O ?
[w?
], where [w?]
denotes the equivalenceclass of w?
given the subspace Gk.5 ExampleWe show how our formalism applies in a simpleexample.
Assume we have a corpus whichconsists of the following sentences:41applebigappleredapplecitybigcityredcitybookbigbookredbookapple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33city 1.0 0.26 0.24 0.0 0.0 0.0big city 1.0 0.33 0.0 0.0 0.0red city 1.0 0.0 0.0 0.0book 1.0 0.26 0.24big book 1.0 0.33red book 1.0Figure 1: Similarities between phrasessee red apple see big citybuy apple visit big appleread big book modernise citythrow old small red book see modern citybuy large new booktogether with the following productions.1.
N?
?
Adj N?2.
N?
?
Nwhere N and Adj are terminals representing nounsand adjectives, along with rules for the terminals.We consider the space of adjective/noun phrases,generated by N?, and define the individual contextof a noun to be the verb it occurs with, and the in-dividual context of an adjective to be the noun itmodifies.
For each rule, we take ?
to be the iden-tity map, so the vector spaces associated with Nand N?, and the vector space generated by indi-vidual contexts of the nouns are all the same.
Inthis case, the only non-zero vectors which we addto the ideal are those for the second rule (ignoringthe first rule, since we do not consider verbs in thisexample except as contexts), which has the set ofvectors?i = ei ?
A?dj?
eiwhere i ranges over the basis vectors for contextsof nouns: see, buy , visit , read ,modernise , andA?dj = 2eapple + 2ebook + ecityIn order to compute canonical representationsof vectors, we take k = 1.5.1 DiscussionFigure 1 shows the similarity between the nounphrases in our sample corpus.
Note that the vec-tors we have put in the generating set describe onlycompositionality of meaning ?
thus for examplethe similarity of the non-compositional phrase bigapple to city is purely due to the distributionalsimilarity between apple and city and compositionwith the adjective big.Our preliminary investigations indicate that thecosine similarity values are very sensitive to theparticular corpus and features chosen; we are cur-rently investigating other ways of measuring andcomputing similarity.One interesting feature in the results is how ad-jectives alter the similarity between nouns.
For ex-ample, red apple and red city have the same sim-ilarity as apple and city, which is what we wouldexpect from a pure tensor product.
This also ex-plains why all phrases containing book are disjointto those containing city, since the original vectorfor book is disjoint to city.The contribution that the quotient algebra givesis in comparing the vectors for nouns with thosefor noun-adjective phrases.
For example, red ap-ple has components in common with apple, as wewould expect, which would not be the case withjust the tensor product.6 Conclusion and Further WorkWe have presented the outline of a novel approachto semantic composition that uses quotient alge-bras to compare vector representations of stringsof different lengths.42The dimensionality of the construction we useincreases exponentially in the length of the sen-tence; this is a result of our use of the tensor prod-uct.
This causes a problem for computation us-ing longer phrases; we hope to address this in fu-ture work by looking at the representations we use.For example, product states can be represented inmuch lower dimensions by representing them asproducts of lower dimensional vectors.The example we have given would seem to in-dicate that we intend putting abstract (syntactic)information about meaning into the set of generat-ing elements of the ideal.
However, there is no rea-son that more fine-grained aspects of meaning can-not be incorporated, even to the extent of puttingin vectors for every pair of words.
This wouldautomatically incorporate information about non-compositionality of meaning.
For example, by in-cluding the vector ?big apple ?
b?ig ?
a?pple , wewould expect to capture the fact that the term bigapple is non-compositional, and more similar tocity than we would otherwise expect.Future work will also include establishing theimplications of varying the constant k and explor-ing different methods for choosing the set ?
thatgenerates the ideal.
We are currently preparingan experimental evaluation of our approach, usingvectors obtained from large corpora.7 AcknowledgmentsWe are grateful to Peter Hines, Stephen Clark, Pe-ter Lane and Paul Hender for useful discussions.The first author also wishes to thank Metrica forsupporting this research.ReferencesJerome R. Bellegarda.
2000.
Exploiting latent se-mantic information in statistical language modeling.Proceedings of the IEEE, 88(8):1279?1296.Sterling K. Berberian.
1961.
Introduction to HilbertSpace.
Oxford University Press.Freddy Choi, Peter Wiemer-Hastings, and JohannaMoore.
2001.
Latent Semantic Analysis for textsegmentation.
In Proceedings of the 2001 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 109?117.Stephen Clark and Stephen Pulman.
2007.
Combin-ing symbolic and distributional models of meaning.In Proceedings of the AAAI Spring Symposium onQuantum Interaction, pages 52?55, Stanford, CA.Stephen Clark, Bob Coecke, and MehrnooshSadrzadeh.
2008.
A compositional distribu-tional model of meaning.
In Proceedings of theSecond Quantum Interaction Symposium (QI-2008),pages 133?140, Oxford, UK.Daoud Clarke.
2007.
Context-theoretic Semanticsfor Natural Language: an Algebraic Framework.Ph.D.
thesis, Department of Informatics, Universityof Sussex.Katrin Erk and Sebastian Pado.
2009.
Paraphrase as-sessment in structured vector space: Exploring pa-rameters and datasets.
In Proceedings of the EACLWorkshop on Geometrical Methods for Natural Lan-guage Semantics (GEMS).P.
W. Foltz, W. Kintsch, and T. K. Landauer.
1998.The measurement of textual coherence with latentsemantic analysis.
Discourse Process, 15:285?307.Gregory Grefenstette.
1994.
Explorations in auto-matic thesaurus discovery.
Kluwer Academic Pub-lishers, Dordrecht, NL.Paul Halmos.
1974.
Finite dimensional vector spaces.Springer.Zellig Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York.W.
Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.T.
K. Landauer and S. T. Dumais.
1997.
A solu-tion to Plato?s problem: the latent semantic analysistheory of acquisition, induction and representationof knowledge.
Psychological Review, 104(2):211?240.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th An-nual Meeting of the Association for ComputationalLinguistics and the 17th International Conferenceon Computational Linguistics (COLING-ACL ?98),pages 768?774, Montreal.43Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In ACL ?04: Proceedings of the 42ndAnnual Meeting on Association for ComputationalLinguistics, page 279, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof ACL-08: HLT, pages 236?244, Columbus, Ohio,June.
Association for Computational Linguistics.Anne Preller and Mehrnoosh Sadrzadeh.
2009.
Bellstates and negation in natural languages.
In Pro-ceedings of Quantum Physics and Logic.Heinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Paul Smolensky.
1990.
Tensor product variable bind-ing and the representation of symbolic structures inconnectionist systems.
Artificial Intelligence, 46(1-2):159?216, November.Lloyd N. Trefethen and David Bau.
1997.
NumericalLinear Algebra.
SIAM.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Proceedings of theSecond Symposium on Quantum Interaction, Ox-ford, UK.44
