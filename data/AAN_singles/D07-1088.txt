Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
837?846, Prague, June 2007. c?2007 Association for Computational LinguisticsExtracting Data Records from Unstructured Biomedical Full TextDonghui Feng       Gully Burns       Eduard HovyInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA, 90292{donghui, burns, hovy}@isi.eduAbstractIn this paper, we address the problem ofextracting data records and their attributesfrom unstructured biomedical full text.There has been little effort reported on thisin the research community.
We argue thatsemantics is important for record extractionor finer-grained language processing tasks.We derive a data record template includingsemantic language models from unstruc-tured text and represent them with a dis-course level Conditional Random Fields(CRF) model.
We evaluate the approachfrom the perspective of Information Extrac-tion and achieve significant improvementson system performance compared withother baseline systems.1 IntroductionThe discovery and extraction of specific types ofinformation, and its (re)structuring and storage intodatabases, are critical tasks for data mining,knowledge acquisition, and information integrationfrom large corpora or heterogeneous resources(e.g., Muslea et al, 2001; Arasu and Garcia-Molina, 2003).
For example, webpages of productson Amazon may contain a list of data records suchas books, watches, and electronics.
Automaticextraction of individual records will facilitate theaccess and management of data resources.Most current approaches address this problemfor structured or semi-structured text, for instance,from XML format files or lists and/or tabular datarecords on webpages (e.g., Liu et al, 2003; Zhu etal., 2006).
The techniques applied rely strongly onthe analysis of document structure derived fromthe webpage?s html tags (e.g., the DOM treemodel).Regarding unstructured text, most InformationExtraction (IE) work has focused on named entities(people, organizations, places, etc.).
Such IE treatseach extracted element as a separate record.
Muchless work has focused on the case where severalrelated pieces of information have to be extractedto jointly comprise a single data record.
In thiswork, it is usually assumed that there is only onerecord for each document (e.g., Kristjannson et al,2004).
Almost no work tries to extract multipledata records from a single document.
Multiple datarecords can be scattered across the narrative in freetext.
The problem becomes much harder as thereare no explicit boundaries between data recordsand no heavily indicative format features (like htmltags) to utilize.With the exponential increase of unstructuredtext resources (e.g., digitalized publications, papersand/or technical reports), knowledge needs havemade it a necessity to explore this problem.
Forexample, biomedical papers contain numerous ex-periments and findings.
But the large volume andrate of publication have made it infeasible to readthrough the articles and manually identify data re-cords and attributes.We present a study to extract data records andattributes from the biomedical research literature.This is part of an effort to develop a KnowledgeBase Management System to benefit neuroscienceresearch.
Specifically we are interested in knowl-edge of various aspects (attributes) of Tract-tracingExperiments (TTE) (data records) in neuroscience.The goal of TTE experiments is to chart the inter-connectivity of the brain by injecting tracer chemi-cals into a region of the brain and identifying cor-responding labeled regions where the tracer is837Figure 1.
An example of data records and attributes in a research article.taken up and transported to (Burns et al, 2007).To extract data records from the research litera-ture, we need to solve two sub-problems: discover-ing individual attributes of records and groupingthem into one or more individual records, each re-cord representing one TTE experiment.
Each at-tribute may contain a list of words or phrases andeach record may contain a list of attributes.Listing each sentence from top to bottom, wecall the first problem the Horizontal Problem (HP)and the second the Vertical Problem (VP).
Figure1 provides an example of a TTE research articlewith colored fragments representing attributes anddashed frames representing data records.
For in-stance, the third dashed frame represents one ex-periment record having three attributes with corre-sponding biological interpretations: ?no labeledcells?, ?the DCN?, and ?the contralateral AVCN?.We view the HP and VP problems as two se-quential labeling problems and describe our ap-proach using two-level Conditional Random Fields(CRF) (Lafferty et al, 2001) models to extract datarecords and their attributes.The HP problem (finding individual attributevalues) is solved using a sentence-level CRF label-ing model that integrates a rich set of linguisticfeatures.
For the VP problem, we apply a dis-course-level CRF model to identify individual ex-periments (data records).
This model utilizes deepsemantic knowledge from the HP results (attributelabels within sentences) together with semanticlanguage models and achieves significant im-provements over baseline systems.This paper mainly focuses on the VP problem,since linguistic features for the HP problem is thegeneral IE topic of much past research (e.g., Pengand McCallum, 2004).
We apply various featurecombinations to learn the most suitable and indica-tive linguistic features.The remainder of this paper is organized as fol-lows: in the next section we discuss related work.Following that, we present the approach to extractdata records in Section 3.
We give extensive ex-perimental evaluations in Section 4 and concludein Section 5.2 Related WorkAs mentioned, data record extraction has beenextensively studied for structured and semi-structured resources (e.g., Muslea et al, 2001;Arasu and Garcia-Molina, 2003; Liu et al, 2003;Zhu et al, 2006).
Most of those approaches rely onthe analysis of document structure (reflected in, forexample, html tags), from which record templatesare derived.
However, this approach does not applyto unstructured text.
The reason lies in thedifficulty of representing a data record template infree text without formatting tags and integrating it838into a learning system.
We show how to addressthis problem by deriving data record templatesthrough language analysis and representing themwith a discourse level CRF model.Given the problem of identifying one or morerecords in free text, it is natural to turn toward textsegmentation.
The Natural Language Processing(NLP) community has come up with varioussolutions towards topic-based text segmentation(e.g., Hearst, 1994; Choi, 2000; Malioutov andBarzilay, 2006).
Most unsupervised textsegmentation approaches work under optimizationcriteria to maximize the intra-segment similarityand minimize the inter-segment similarity based onword distribution statistics.
However, thisapproach cannot be applied directly to data recordextraction.
A careful study of our corpus showsthat data records share many words and phrasesand are not distinguishable based on wordsimilairties.
In other words, different experiments(records) always belong to the same topic and thereis no way to segment them using standard topicsegmentation techniques (even if one views theproblem as a finer-level segmentation thantraditional text segmentation).
In addition, mosttext segmentation approaches require aprespecified number of segments, which in ourdomain cannot be provided.
(Wick et al, 2006) report extracting database re-cords by learning record field compatibility.
How-ever, in our case, the field compatibility is hard todistinguish even by a human expert.
Cluster-basedor pairwise field similarity measures do not applyto our corpora without complex knowledge reason-ing.
Most of Wick et al?s data (faculty and stu-dent?s homepages) contains one record.In addition, as explained below, we have foundthat surface word statistics alone are not sufficientto derive data record templates for extraction.Some (limited) form of semantic understanding oftext is necessary.
We therefore first perform somesentence level extraction (following the HPproblem) and then integrate semantic labels andsemantic language model features into a discourselevel CRF model to represent the template forextracting data records in the future.Recently an increasing number of research ef-forts on text mining and IE have used CRF models(e.g., Peng and McCallum, 2004).
The CRF modelprovides a compact way to integrate different typesof features when sequential labeling is important.Recent work includes improved model variants(e.g., Jiao et al, 2006; Okanohara et al, 2006) andapplications such as web data extraction (Pinto etal., 2003), scientific citation extraction (Peng andMcCallum, 2004), and word alignment (Blunsomand Cohn, 2006).
But none of them have usedCRFs for discourse level data record extraction.We use a CRF model to represent a data recordtemplate and integrate various knowledge as CRFfeatures.
Instead of traditional work on the sen-tence level, our focus here is on the discourse level.As this has not been carefully explored, we ex-periment with various selected features.For the biomedical domain, our work will facili-tate biomedical research by supporting the con-struction of Knowledge Base Management Sys-tems (e.g., Stephan et al, 2001; Hahn et al, 2002;Burns and Cheng, 2006).
Unlike the well-studiedproblem of relation extraction from biomedicaltext, our work focuses on grouping extracted at-tributes across sentences into meaningful data re-cords.
TTE experiment is only one of many ex-perimental types in biology.
Our work can be gen-eralized to many different types of data records tofacilitate biology research.In the next section, we present our approach toextracting data records.3 Extracting Data RecordsInspired by the idea of Noun Phrase (NP) chunkingin a single sentence, we view the data recordsextraction problem as discourse chunking from asequence of sentences using a sequential labelingCRF model.3.1 Sequential Labeling Model: CRFThe CRF model addresses the problem of labelingsequential tokens while relaxing the strongindependence assumptions of Hidden MarkovModels (HMMs) and avoiding the presence oflabel bias from having few successor states.
Foreach current state, we obtain the conditionalprobability of its output states given previouslyassigned values of input states.
For most languageprocessing tasks, this model is simply a linear-chain Markov Random Fields model.In typical labeling processes using CRFs eachtoken is viewed as a labeling unit.
For our prob-lem, we process each input document),...,,( 21 nsssD =  as a sequence of individual sen-839tences, with a corresponding labeling sequence oflabels, ),...,,( 21 nlllL = , so that each sentence corre-sponds to only one label.
In our problem, each datarecord corresponds to a distinct TTE experiment.Similar to NP chunking, we define three labels forsentences, ?B_REC?
(beginning of record),?I_REC?
(inside record), and ?O?
(other).
The de-fault label ?O?
indicates that this sentence is be-yond our concern.The CRF model is trained to maximize theprobability of )|( DLP , that is, given an inputdocument D, we find the most probable labelingsequence L. The decision rule for this procedure is:)|(maxarg?
DLPLL=                                        (1)A CRF model of the two sequences is character-ized by a set of feature functions kf and their corre-sponding weights k?
.
As in Markov fields, theconditional probability )|( DLP  can be computedusing Equation 2.???????
?== ?Tt kttkkStDllfZDLP11 ),,,(*exp1)|( ?
(2)where ),,,( 1 tDllf ttk ?
is a feature function, represent-ing either the state transition feature ),,( 1 Dllf ttk ?
orthe feature of output state ),( Dlf tk given the inputsequence.
All these feature functions are user-defined boolean functions.CRF works under the framework of supervisedlearning, which requires a pre-labeled training setto learn and optimize system parameters to maxi-mize the probability or its log format.
Equippedwith this model, we investigate how to apply it andprepare features accordingly.3.2 Feature PreparationThe CRF model provides a compact, unifiedframework to integrate features.
However, unlikesentence-level processing, where features are veryintuitive and circumscribed, it is not obvious whatfeatures are most indicative for our problem.
Wetherefore explore three categories of features fordiscourse level chunking.3.2.1 Semantic Attribute LabelsMost text segmentation approaches computesurface word similarity scores in given corporawithout semantic analysis.
However, in our case,data records have very similar characteristics andshare most of the words.
They are notdistinguishable just from an analysis of surfaceword statistics.
We have to understand thesemantics before we can make decisions about datarecord extraction.In our case, we care about the four types of at-tributes of each data record (one TTE experiment).Table 1 gives the definitions of the four attributesfor each data record.Name DescriptioninjectionLocation the named brain region where the injection was made.tracerChemical the tracer chemical used.labelingLocation the region/location where the labeling was found.labelingDescriptiona description of labeling, in-cluding label density or labeltype.Table 1.
Attributes of data records (a TTE experiment).To obtain this semantic attributes information ofindividual sentences (the HP problem), we firstapply another sentence-level CRF model to labeleach sentence.
We consider five categories of fea-tures based on language analysis.
Table 2 showsthe features for each category.Name Feature DescriptionTOPOGRAPHY Is word topog-raphic?BRAIN_REGION Is word a regionname?TRACER Is word a tracerchemical?DENSITY Is word a den-sity term?LexiconKnowledgeLABELING_TYPE Does word de-note a labelingtype?SurfaceWordWord Current wordContextWindowCONT-INJ If current wordis within a win-dow of injectioncontextPrev-word Previous word WindowWords Next-word Next wordRoot-form Root form ofthe word if dif-ferentGov-verb The governingverbSubject The sentencesubjectDependencyFeaturesObject The sentenceobjectTable 2.
The features for labeling words.840a.
Lexicon knowledge.
We used names of brainstructures taken from brain atlases (Swanson,2004), standard terms to denote neuro-anatomical topographical relationships (e.g.,?rostral?
), the name or abbreviation of thetracer chemical used (e.g., ?PHAL?
), andcommonsense descriptions for descriptions ofthe labeling (e.g., ?dense?, ?light?).b.
Surface and window word.
The currentword and the words around are important in-dicators of the most probable label.c.
Context window.
The TTE is a description ofthe inject-label-findings process.
Whenever aword having a root form of ?injection?
or?deposit?
appears, we generate a contextwindow and all the words falling into thiswindow are assigned a feature of ?CONT-INJ?.d.
Dependency features.
We apply a depend-ency parser MiniPar (Lin, 1998) to parse eachsentence, and then derive four types of fea-tures from the parsing result.
These featuresare (a) root form of every word, (b) the sub-ject within the sentence, (c) the object withinthe sentence, and (d) the governing verbs.The labeling system assigns a label for every to-ken in each sentence.
We achieved the best per-formance with an F-score of 0.79 (based on a pre-cision of 0.80 and a recall of 0.78).
This is not thefocus of this paper.
Please refer to our previouswork (Burns et al, 2007) for details.Figure 2.
An example of semantic attribute labels.With the sentence-level understanding of eachsentence, we obtain the semantic attribute labelsfor the data records.
Figure 2 gives an examplesentence with semantic attribute labels.
Here<tracerChemical>, <labelingLocation>, and <la-belingDescription> are recognized by the system,and the attribute names will be used as features forthis sentence.3.2.2 Semantic Language ModelSince text narratives might adhere to logical waysof expressing facts, language models for each sen-tence will also provide good features to extractdata records.
However, in biomedical research arti-cles many of the technical words/phrases used inthe narrative are repeated across experiments, mak-ing the surface word language model of little use inderiving generalized data record templates.
Con-sidering this, we replace in each sentence the la-beled fragments with their attribute labels and thenderive semantic language models from that format.By ?semantic language model?
we therefore meana combination of semantic labels and surfacewords.For example, in the sentence shown in Figure 2,we have the semantic language model trigramslocation-of-<tracerChemical>, sites-in-<injectionLocation>, and <labelingDescription>-followed-the.
In addition, we also query WordNetfor the root form of each word to generalize thesemantic language models.
This for example pro-duces the semantic language model trigrams site-in-<injectionLocation> and <labelingDescription>-follow-the.We believe the collected semantic languagemodels represent an inherent structure of unstruc-tured data records.
By integrating them as featureswith a CRF model, we expect to represent data re-cord templates and use the learned model to extractnew data records.However, it is not clear what semantic languagemodels are most indicative and useful.
A bag-of-words (language models) approach may bringmuch noise in.
We show below a comparison ofregular language models and semantic languagemodels in evaluations.3.2.3 Layout and Word HeuristicsThe previous two categories of features come fromthe discovery of semantic components of sentencesand their narrative form word analysis.
When in-terviewing the neuroscience expert annotator, welearned that some layout and word level heuristicsmay also help to delineate individual data records.Table 3 gives the two types of heuristic features.When a sentence contains heuristic words, itwill be assigned to a word heuristic feature.
If thesentence is at the boundary of a paragraph, it willbe assigned a layout heuristic feature, namely thefirst or the last sentence in the paragraph.<SENT FILE="1995-360-213-ns.xml" INDEX= "63">Regardless of the precise location of <tracerChemical>PHAL </tracerChemical> injection sites in <injectionLo-cation> the MEA </injectionLocation> , <labelingDe-scription> labeled axons </labelingDescription> followedthe same basic routes .</SENT>841Name Feature Descrip-tionEXP_B_WORDINJECTCASEEXPERIMENTAPPLICATIONDEPOSITPLACEMENTINTRODUCTIONHeuristicwords forbeginningof an ex-perimentdescrip-tionPOS_IN_PARA FIRST_IN_PARALAST_IN_PARAPosition ofthe sen-tence inthe para-graphTable 3.
The heuristic features.4 Empirical EvaluationTo evaluate the effectiveness and performance ofour technique, we conducted extensive experi-ments to measure the data record extraction ap-proach.4.1 Experimental SetupWe used the machine learning package MALLET(McCallum, 2002) to conduct the CRF modeltraining and labeling.We have obtained the digital publications of9474 Journal of Comparative Neurology (JCN)1articles from 1982 to 2005.
We have converted thePDF format into plain text, maintaining paragraphbreaks (some errors still occur though).
A simpleheuristic based approach identifies semantic sec-tions of the paper (e.g, Introduction, Results, Dis-cussion).
As most experimental descriptions appearin the Results section, we only process the Resultssection.
A neuroscience expert manually annotatedthe data records in the Results section of 58 re-search articles.
The total number of sentences inthe Results section of the 58 files is 6630 (averag-ing 114.3 sentences per article).Training Set Testing SetDocs 39 19Data Records 249 133Table 4.
Experiment configuration.We randomly divided this material into trainingand testing sets under a 2:1 ratio, giving 39 docu-ments in the training set and 19 in the testing set.1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248Table 4 gives the numbers of documents and datarecords in the training and the testing set.4.2 Evaluation MetricsTo evaluate data record extraction, we notice it isnot fair to strictly evaluate the boundaries of datarecords because this does not penalize the near-miss and false positive of data records in a reason-able way; sentences near a boundary that containno relevant record information can be included oromitted without affecting the results.
Hence thestandard Pk (Beeferman et al, 1997) and WinDiff(Pevzner and Hearst, 2002) measures for text seg-mentation are not so suitable for our task.As we are concerned with the usefulness ofknowledge in extracted data records, we insteadevaluate from the perspective of IE.
We measuresystem performance on the quality of the extracteddata records.
For each extracted data record, it willbe aligned to one of the data records in the goldstandard using the ?dominance rule?
(if the datarecord can be aligned to multiple records in thegold standard, it will be aligned to the one withhighest overlap).
Then we evaluate the precision,recall, and F1 scores of extracted units of the datarecord.
The units are the attributes in data records.system by the units extracted  theof #unitscorrect   # ofprecision =   (3)standard gold in the units  theof #unitscorrect   # ofrecall =                (4)ecallrprecisionrecall*precisionF +=*21                                    (5)These measures provide an indication of thecompleteness and correctness of each extractedrecord (experiment).
We also measure the numberof distinct records extracted, compared with thegold standard as appearing in the document.4.3 Experiment ResultsTo fully compare the effectiveness of our semanticanalysis functionality, we evaluated system per-formance for all the following systems:TextTiling (TT): To compare with text segmen-tation techniques, we use TextTiling (Hearst, 1994)with default parameters as the first baseline sys-tem.Random Guess (RG): In order to demonstratethe data balance of all the possible labels in thetesting set, we also use another baseline systemwith random decisions for each sentence.842Domain Heuristics (DH): In a regular TTE ex-periment, only one tracer chemical will typicallybe used.
Given this heuristic, we assume each datarecord contains one tracer chemical.
In this system,we first locate sentences with identified tracechemicals, and then we greedily expand backwardand forward until another new tracer chemical ap-pears or no other attribute is included.Surface Text (ST): To measure the effective-ness of the semantic analysis (attribute labels andsemantic language models), the ST system utilizesonly standard surface word language models andheuristic features.Semantic Analysis (SEM): The SEM systemuses all the semantic features available (includingidentified attributes and semantic language models)and two heuristic features.Table 5 shows the final performance of thesedifferent systems.
The second column provides thenumbers of extracted data records.
In this task, alarger number does not necessarily mean a bettersystem, as a system might produce too many falsepositives.
The remaining three columns representthe precision, recall, and F1 scores, averaged overall data records.
With our approach, the systemperformance is significantly improved comparedwith other systems.
System TT fails in this task asit only outputs the full document as one single re-cord.# ofRecordsPrec.
Rec.
F1TT 19 0.3861 1.0 0.5571RG 758 0.6331 0.0913 0.1595DH 162 0.6703 0.4902 0.5663ST 82 0.8182 0.8339 0.8260SEM 72 0.8505 0.9258 0.8865Table 5.
System performance.To investigate how plain text language modelsand semantic language models affect system per-formance, we also experimented with all the lan-guage models.
Table 6 shows comparisons of threetypes of language models.
Systems with semanticanalysis always work better than those with onlysurface text analysis.
Without semantic analysis,unigram features work better than bigram and tri-gram features.
This matches our intuition: withoutgeneralizing to semantic language models, higherorder language models will be relatively sparse andcontain much noise.
However, when taking intoaccount the semantic features, we found that bi-gram and trigram semantic language model fea-tures outperformed unigrams.
They are especiallyimportant in boosting the recall scores as they cap-ture more generalized information when derived.Unigram (%) Bigram (%) Trigram (%)Prec/Rec/F1 Prec/Rec/F1 Prec/Rec/F1ST 81.8/83.4/82.6 69.1/88.4/77.6 57.9/88.8/70.1SEM 85.1/86.6/85.6 85.1/92.6/88.7 82.2/92.7/87.1Table 6.
Language model comparisons.As an example, Table 7 gives a list of high qual-ity bigram semantic language models ranked bytheir information gains based on the training data.through_<labelingLocation> rat_no<labelingDescription>_be of_<tracerChemical><labelingLocation>_( <tracerChemical>_be<tracerChemical>_injection be_injectinto_<injectionLocation> be_center<labelingDescription>_from inject_with<tracerChemical>_in injection_ofin_<labelingLocation> in_experimentTable 7.
An example list of top-ranked bigrams.The main difficulty for data record extractionfrom unstructured text lies in deriving and repre-senting a template for future extraction.
We actu-ally take advantage of CRF and represent the tem-plate with a CRF model.Each data record is measured with precision, re-call, and F1 scores.
Figure 3 depicts the distribu-tion of extracted data records according to thesemeasures in the best system.Distribution0510152025303540455055600 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Performance#ofextractedrecordsPrecRecF1Figure 3.
Data records performance distribution.The results are encouraging, especially given thecomplexity and flexibility of data record descrip-tions in the unstructured text.
In Figure 3, Axis X843represents the value interval for precision, recall,and F1, and Axis Y represents the number of ex-tracted records with their corresponding values.For example, 57 records have recall scores fallinginto [0.9, 1.0].Figure 4 gives an example alignment betweensystem result and the gold standard.
Each record isrepresented by a range of sentences.
The numbersfollowing each record in the system result are indi-vidual data record?s precision and recall scores.System                                   GoldFigure 4.
An example of record extraction in one doc.This is a real example from the testing set.
Forrecords R1, R3, and R6, the system can extract theexact sentences contained.
For record R2 and R5,although they do not exactly match at the sentencelevel, the extracted record contains the entire re-quired set of attributes as in the gold standard.4.4 Error Analysis and DiscussionWhen we investigated the errors, we found thatsometimes the extracted data records combinedtwo or more smaller gold standard records, or viceversa.
As shown in Figure 4, extracted records R4and R7 are both combinations of records in thegold standard.
This is partially due to the granular-ity definition problem.
Authors may mention sev-eral approaches/symptoms to one type of experi-ment for a single purpose.
In this case, it is almostinfeasible to have annotators strictly agree ongranularity and thus to teach the system to acquirethis knowledge.
For example, in the gold standard,the annotator annotated three successive sentencesas three separate records but the system outputthose as only one data record.
In this extreme case,it is too hard to expect the system to perform well.In our approach, the semantic attribute labelsand semantic language models require the result ofthe initial sentence-level labeling, which has an F-score of 0.79.
The error may propagate into thedata record extraction procedure and lower overallsystem performance.In our current experiments, we also assume allthe attributes within one segment belong to onerecord.
However, the situation of embedded datarecords will make this problem harder.
For exam-ple, authors sometimes compare the current ex-periment with other approaches in referenced pa-pers.
In this case, those attributes should be ex-cluded from the records.
We need to invent rules orconstraints to filter them out.
When such referenceoccurs at experiment boundaries, it brings higherrisk for correct results.It is a very hard problem to extract from unstruc-tured text neat structured records.
The annotatorssometimes employ background knowledge or rea-soning when performing manual extraction; suchknowledge cannot today be easily modeled andintegrated into learning systems.In our study, we also compared some feature se-lection approaches.
Similar to (Yang and Pedersen,1997), we tried Feature Instance Frequency, Mu-tual Information, Information Gain, and CHI-square test.
But we eventually found that the sys-tem including all the features worked best, andwith all the other configurations unchanged, fea-ture instance frequency worked at almost the samelevel as other complex measures such as mutualinformation and information gain.5 Conclusion and Future WorkIn this paper, we explored the problem of extract-ing data records from unstructured text.
The lackof structure makes it difficult to derive meaningfulobjects and their values without resorting to deeperlanguage analysis techniques.
We derived indica-tive linguistic features to represent data recordtemplates in free text, using a two-pass approach inwhich the second pass used the IE labels derivedfrom the first to compose attributes into coherentdata records.
We evaluated the results from an IEperspective and reported potential problems of er-ror generation.
?R1:S12~S29 (1.0/1.0)?R2: S31~S41 (1.0/1.0)R3: S42~S52 (1.0/1.0)?R4: S56~S73(0.517/1.0)?R5: S75~S88 (1.0/1.0)?R6: S91~S106(1.0/1.0)?R7: S108~S118(0.523/1.0)?
?R1': S12~S29?R2': S31~S40?R3': S42~S52?R4': S56~S63?R5': S65~S73R6': S74~S88..R7': S91~S106?R8': S108~S114R9': S115~S118?844For the future, we plan to explore additional fea-ture types and feature selection strategies to deter-mine what is ?good?
for unstructured record tem-plates to improve our results.
More effort will alsobe put into the sentence-level analysis to reduceerror propagations.
In addition, ontology basedknowledge inference strategies might be useful tovalidate attributes in single record and in turn helpdata record extraction.
The last thing under ourdirection is to explore new models if applicable.We hope this thought-provoking problem willattract more attention from the community.
In thefuture, we plan to make our corpus available to thecommunity.
The solution to this problem willhighly affect the access of knowledge in large scaleunstructured text corpora.AcknowledgementsThe work was supported in part by an ISI seedfunding, and in part by a grant from the NationalLibrary of Medicine (RO1 LM07061).
The authorswant to thank Feng Pan for his helpful suggestionswith the manuscript.
We would also like to thankthe anonymous reviewers for their valuable com-ments.ReferencesArasu, A., and Garcia-Molina, H. 2003.
Extractingstructured data from web pages.
In Proc.
of SIMOD-2003.Beeferman, D., Berger, A., and Lafferty, J.
1997.
Textsegmentation using exponential models.
In Proc.
ofEMNLP-1997.Blunsom, P. and Cohn, T. 2006.
Discriminative wordalignment with conditional random fields.
In Proc.
ofACL-2006.Brazma, A., et al, 2001.
Minimum information about amicroarray experiment (MIAME)-toward standardsfor microarray data.
Nat Genet, 29(4): p. 365-71.Burns, G.A.
and Cheng, W.-C. 2006.
Tools for knowl-edge acquisition within the NeuroScholar system andtheir application to anatomical tract-tracing data.
InJournal of Biomedical Discovery and Collaboration.Burns, G., Feng, D., and Hovy, E.H. 2007.
IntelligentApproaches to Mining the Primary Research Litera-ture: Techniques, Systems, and Examples.
BookChapter in Computational Intelligence in Bioinfor-matics, Springer-Verlag, Germany.Choi, F. Y. Y.
2000.
Advances in domain independentlinear text segmentation.
In Proc.
of NAACL-2000.Hahn, U., Romacher, M., and Schulz, S. 2002.
Creatingknowledge repositories from biomedical reports theMEDSYNDIKATE text mining system.
In Proc.
ofPSB-2002.Hearst, M. 1994.
Multi-paragraph segmentation of ex-pository text.
In Proc.
of ACL-1994.Jiao, F., Wang, S., Lee, C., Greiner, R., andSchuurmans, D. 2006.
Semi-supervised conditionalrandom fields for improved sequence segmentationand labeling.
In Proc.
of ACL-2006.Kristjannson, T., Culotta, A. Viola, P., and McCallum,2004.
A. Interactive information extraction with con-strained conditional random fields.
In Proc.
of AAAI-2004.Lafferty, J., McCallum, A. and Pereira, F. 2001 Condi-tional Random Fields: probabilistic models for seg-menting and labeling Sequence Data.
In Proc.
ofICML-2001.Lin, D. 1998.
Dependency-based evaluation of MINI-PAR.
In Proc.
of Workshop on the Evaluation ofParsing Systems.Liu, B., Grossman, R., and Zhai, Y.
2003.
Mining datarecords in web pages.
In Proc.
of SIGKDD-2003.Malioutov, I. and Barzilay, R. 2006.
Minimum cutmodel for spoken lecture segmentation.
In Proc.
ofACL-2006.McCallum, A.K.
2002.
MALLET: A Machine Learningfor Language Toolkit.
http://mallet.cs.umass.edu.Muslea, I., Minton, S., and Knoblock, C.A.
2001.Hierarchical wrapper induction for semistructuredinformation sources.
Autonomous Agents and Multi-Agent Systems 4:93-114.Okanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J.2006.
Improving the scalability of semi-markov con-ditional random fields for named entity recognition.In Proc.
of ACL-2006.Peng, F. and McCallum, A.
2004.
Accurate informationextraction from research papers using conditionalrandom fields.
In Proc.
of HLT-NAACL-2004.Pevzner, L., and Hearst, M. 2002.
A Critique and Im-provement of an Evaluation Metric for Text Segmen-tation.
Computational Linguistics.Pinto, D., A. McCallum, X. Wei, and W.B.
Croft.
2003.Table Extraction Using Conditional Random Fields.In Proc.
of SIGIR-2003.845Stephan, K.E.
et al, 2001.
Advanced database method-ology for the Collation of Connectivity data on theMacaque brain (CoCoMac).
Philos Trans R Soc LondB Biol Sci, 356(1412).Swanson, L.W.
2004.
Brain Maps: Structure of the RatBrain.
3rd edition, Elsevier Academic Press.Wick, M., Culotta, A., and McCallum, A.
2006.
Learn-ing field compatibilities to extract database recordsfrom unstructured text.
In Proc.
of EMNLP-2006.Yang, Y., and Pedersen, J.
1997.
A comparative studyon feature selection in text categorization.
In Proc.
ofICML-1997, pp.
412-420.Zhu, J., Nie, Z., Wen, J., Zhang, B., and Ma, W. 2006.Simultaneous record detection and attribute labelingin web data extraction.
In Proc.
of KDD-2006.846
