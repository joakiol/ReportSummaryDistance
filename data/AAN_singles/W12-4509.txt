Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 88?94,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsUBIU for Multilingual Coreference Resolution in OntoNotesDesislava Zhekova Sandra Ku?bler Joshua Bonner Marwa Ragheb Yu-Yin HsuIndiana UniversityBloomington, IN, USA{dzhekova, skuebler, jebonner, mragheb, hsuy}@indiana.eduAbstractThe current work presents the participa-tion of UBIU (Zhekova and Ku?bler, 2010)in the CoNLL-2012 Shared Task: Model-ing Multilingual Unrestricted Coreference inOntoNotes (Pradhan et al, 2012).
Our systemdeals with all three languages: Arabic, Chi-nese and English.
The system results showthat UBIU works reliably across all three lan-guages, reaching an average score of 40.57 forArabic, 46.12 for Chinese, and 48.70 for En-glish.
For Arabic and Chinese, the system pro-duces high precision, while for English, preci-sion and recall are balanced, which leads tothe highest results across languages.1 IntroductionMultilingual coreference resolution has been gain-ing considerable interest among researchers in re-cent years.
Yet, only a very small number of sys-tems target coreference resolution (CR) for morethan one language (Mitkov, 1999; Harabagiu andMaiorano, 2000; Luo and Zitouni, 2005).
A firstattempt at gaining insight into the comparability ofsystems on different languages was accomplished inthe SemEval-2010 Task 1: Coreference Resolutionin Multiple Languages (Recasens et al, 2010).
Sixsystems participated in that task, UBIU (Zhekovaand Ku?bler, 2010) among them.
However, since sys-tems participated across the various languages ratherirregularly, Recasens et al (2010) reported that thedata points were too few to allow for a proper com-parison between different approaches.
Further sig-nificant issues concerned system portability acrossthe various languages and the respective languagetuning, the influence of the quantity and quality ofdiverse linguistic annotations as well as the perfor-mance and behavior of various evaluation metrics.The CoNLL-2011 Shared Task: Modeling Unre-stricted Coreference in OntoNotes (Pradhan et al,2011) targeted unrestricted CR, which aims at iden-tifying nominal coreference but also event corefer-ence, within an English data set from the OntoNotescorpus.
Not surprisingly, attempting to include suchevent mentions had a detrimental effect on over-all accuracy, and the best performing systems (e.g.,(Lee et al, 2011)) did not attempt event anaphora.The current shared task extends the task definition tothree different languages (Arabic, Chinese and En-glish), which can prove challenging for rule-basedapproaches such as the best performing system from2011 (Lee et al, 2011).In the current paper, we present UBIU, a memory-based coreference resolution system, and its re-sults in the CoNLL-2012 Shared Task.
We give anoverview of UBIU in Section 2.
In Section 3, wepresent the system results, after which Section 4 laysout some conclusive remarks.2 UBIUUBIU (Zhekova and Ku?bler, 2010) is a corefer-ence resolution system designed specifically for amultilingual setting.
As shown by Recasens et al(2010), multilingual coreference resolution can beapproached by various machine learning methodssince machine learning provides a possibility for ro-bust abstraction over the variation of language phe-nomena and specificity.
Therefore, UBIU employs88a machine learning approach, memory-based learn-ing (MBL) since it has proven to be a good so-lution to various natural language processing tasks(Daelemans and van den Bosch, 2005).
We em-ploy TiMBL (Daelemans et al, 2010), which usesk nearest neighbour classification to assign class la-bels to the targeted instances.
The classifier set-tings we used were determined by a non-exhaustivesearch over the development data and are as follows:the IB1 algorithm, similarity is computed based onweighted overlap, gain ratio is used for the relevanceweights and the number of nearest neighbors is set tok=3 (cf.
(Daelemans et al, 2010) for an explanationof the system parameters).In UBIU, we use a pairwise mention model (Soonet al, 2001; Broscheit et al, 2010) since this modelhas proven more robust towards multiple languages(Wunsch, 2009) than more elaborate ones.
We con-centrate on nominal coreference resolution, i.e.
weignore the more unrestricted cases of event corefer-ence.
Below, we describe the modules used in UBIUin more detail.2.1 PreprocessingThe preprocessing module oversees the proper for-matting of the data for all modules applied in laterstages during coreference resolution.
During pre-processing, we use the speaker information, if pro-vided, and replace all 1st person singular pronounsfrom the token position with the information pro-vided in the speaker column and adjust the POS tagcorrespondingly.2.2 Mention DetectionMention detection is the process of detecting thephrases that are potentially coreferent and are thusconsidered candidates for the coreference process.Mention detection in UBIU is based on the parse andnamed entity information provided by the sharedtask.
This step is crucial for the overall system per-formance, and we aim for high recall at this stage.Singleton mentions that are added in this step canbe filtered out in later stages.
However, if we failto detect a mention in this stage, it cannot be addedlater.
We predict a mention for each noun phrase andnamed entity provided in the data.
Additionally, weextract mentions for possessive pronouns in Englishas only those did not correspond to a noun phrase inMDR P F1Arabic 97.13 19.06 31.87Chinese 98.33 31.64 47.88English 96.73 30.75 46.67Table 1: Mention detection (development set).the syntactic structure provided by the task.
In Ara-bic and Chinese, possessives are already marked asnoun phrases.The system results on mention detection on thedevelopment set are listed in Table 1.
The resultsshow that we reach very high recall but low preci-sion, as intended.
The majority of the errors are dueto discrepancies between noun phrases and namedentities on the one hand and mentions on the other.Furthermore, since we do not target event corefer-ence, we do not add mentions for the verbs in thedata, which leads to a reduction of recall.In all further system modules, we represent amention by its head, which is extracted via heuris-tic methods.
For Arabic, we select the first noun orpronoun while for Chinese and English, we extractthe the pronoun or the last noun of a mention unlessit is a common title.
Additionally, we filter out men-tions that correspond to types of named entities thatin a majority of the cases in the training data are notcoreferent (i.e.
cardinals, ordinals, etc.
).One problem with representing mentions mostlyby their head is that it is difficult to decide betweenthe different mention spans of a head.
Since auto-matic mentions are considered correct only if theymatch the exact span of a gold mention, we includeall identified mention spans for every extracted headfor classification, which can lead to losses in evalu-ation.
For example, consider the instance from thedevelopment set in (1): the noun phrase the Avenueof Stars is coreferent and thus marked as a gold men-tion (key 7).
UBIU extracts two different spans forthe same head Avenue: the Avenue (MD 3) and theAvenue of Stars (MD 5).
(1)token POS parse key MD outputthe DT (NP(NP* (7 (3|(5 (9Avenue NNP *) - 3) 9)of IN (PP* - - -Stars NNPS (NP*))) 7) (4)|5) -Both mention spans are passed to the coreferenceresolver, together with additional features (i.e.
men-89MD MUC B3 CEAFE AverageF1 F1 F1 F1 F1long 100.0 100.0 100.0 100.0 100.0short 50.00 0 66.66 66.66 44.44Table 2: The scores for the short example in (1).tion length, head modification, etc.)
that will allowthe resolver to distinguish between the spans.
Theclassifier decides that the shorter mention is coref-erent and that the longer mention is a singleton.
Inorder to show the effect of this decision, we assumethat there is one coreferent mention to key 7.
Weconsider the two possible spans and show the re-spective scores in Table 2.
The evaluation in Table 2shows that providing the correct coreference link butthe wrong, short mention span, the Avenue, has con-siderable effects to the overall performance.
First,as defined by the task, the mention is ignored by allevaluation metrics leading to a decrease in mentiondetection and coreference performance.
Moreover,the fact that this mention is ignored means that thesecond mention becomes a singleton and is not con-sidered by MUC either, leading to an F1 score of 0.This example shows the importance of selecting thecorrect mention span.2.3 Singleton ClassificationA singleton is a mention which corefers with noother mention, either because it does not refer to anyentity or because it refers to an entity with no othermentions in the discourse.
Because singletons com-prise the majority of mentions in a discourse, theirpresence can have a substantial effect on the perfor-mance of machine learning approaches to CR, bothbecause they complicate the learning task and be-cause they heavily skew the proportion in the train-ing data towards negative instances, which can biasthe learner towards assuming no coreference relationbetween pairs of mentions.
For this reason, informa-tion concerning singletons needs to be incorporatedinto the CR process so that such mentions can beeliminated from consideration.Boyd et al (2005), Ng and Cardie (2002), andEvans (2001) experimented with machine learningapproaches to detect and/or eliminate singletons,finding that such a module provides an improve-ment in CR performance provided that the classifier# Feature Description1 the depth of the mention in the syntax tree2 the length of the mention3 the head token of the mention4 the POS tag of the head5 the NE of the head6 the NE of the mention7 PR if the head is premodified, PO if it is not; UN otherwise8 D if the head is in a definite mention; I otherwise9 the predicate argument corresponding to the mention10 left context token on position token -311 left context token on position token -212 left context token on position token -113 left context POS tag of token on position token -314 left context POS tag of token on position token -215 left context POS tag of token on position token -110 right context token on position token +111 right context token on position token +212 right context token on position token +313 right context POS tag of token on position token +114 right context POS tag of token on position token +215 right context POS tag of token on position token +316 the syntactic label of the mother node17 the syntactic label of the grandmother node18 a concatenation of the labels of the preceding nodes19 C if the mention is in a PP; else ITable 3: The features used by the singleton classifier.does not eliminate non-singletons too frequently.
Ng(2004) additionally compared various feature- andconstraint-based approaches to incorporating single-ton information into the CR pipeline.
Feature-basedapproaches integrate information from the single-ton classifier as features while constraint-based ap-proaches filter singletons from the mention set.
Fol-lowing these works, we include a k nearest neigh-bor classifier for singleton mentions in UBIU with19 commonly-used features described below.
How-ever, unlike Ng (2004), we use a combination of thefeature- and constraint-based approaches to incorpo-rate the classifier?s results.Each training/testing instance represents a nounphrase or a named entity from the data together withfeatures describing this phrase in its discourse.
Thelist of features is shown in Table 3.
The instancesthat are classified by the learner as singletons witha distance to their nearest neighbor below a thresh-old (i.e., half the average distance observed in thetraining data) are filtered from the mention set, andare thus not considered in the pairwise coreferenceclassification.
For the remainder of the mentions, theclass that the singletons classifier has assigned to theinstance is used as a feature in the coreference clas-sifier.
Experiments on the development set showed90MD MUC B3 CEAFE AverageF1 F1 F1 F1 F1Arabic+SC 58.36 34.75 58.26 37.39 43.47-SC 56.12 34.96 58.52 36.05 43.18Chinese+SC 52.30 42.70 61.11 32.86 45.56-SC 50.40 41.19 60.96 32.47 44.87English+SC 67.38 53.20 59.23 34.90 49.11-SC 65.55 51.57 59.18 34.38 48.38Table 4: Evaluation of using (+SC) or not (-SC) the sin-gleton classifier in UBIU on the development set.that the most important features across all languagesare the POS tag of the head word, definiteness, andthe mother node in the syntactic representation.
In-formation about head modification is helpful for En-glish and Arabic, but not for Chinese.The results of using the singleton classifier inUBIU on the development set are shown in Table 4.They show a moderate improvement for all evalu-ation metrics and all languages, with the exceptionof MUC and B3 for Arabic.
The most noticeableimprovement can be observed in mention detection,which gains approx.
2% in all languages.
A man-ual inspection of the development data shows thatthe version using the singleton classifier extracts aslightly higher number of coreferent mentions thanthe version without.
However, the reduction of men-tions that are never coreferent, which was the maingoal of the singleton classifier, is also present in theversion without the classifier, so that the results ofthe classifier only have a minimal influence on thefinal results.2.4 Coreference ClassificationCoreference classification is the process in whichall identified mentions are paired up and featuresare extracted to build feature vectors that representthe mention pairs in their context.
Each mentionis represented in the feature vector by its syntactichead.
The vectors for the pairs are then used by thememory-based learner TiMBL.As anaphoric mentions, we consider all definitephrases; we then create a pair for each anaphor witheach mention preceding it within a window of 10(English, Chinese) or 7 (Arabic) sentences.
We con-sider a shorter window of sentences for Arabic be-cause of its NP-rich syntactic structure and its longersentences, which leads to an increased number ofpossible mention pairs.
The set of features that weuse, listed in Table 5, is an extension of the set byRahman and Ng (2009).
Before classification, weapply a morphological filter, which excludes vectorsthat disagree in number or gender (applied only ifthe respective information is provided or can be de-duced from the data).Both the anaphor and the antecedent carry a la-bel assigned to them by the singletons classifier.Yet, we consider as anaphoric only the heads ofdefinite mentions.
Including a feature representingthe class assigned by the singletons classifier foreach anaphor triggers a conservative learner behav-ior, i.e., fewer positive classes are assigned.
Thus, toaccount for this behavior, we ignore those labels forthe anaphor and include only one feature (no.
25 inTable 5) in the vector for the antecedent.2.5 PostprocessingIn postprocessing, we create the equivalence classesof mentions that were classified as coreferent and# Feature Description1 mj - the antecedent2 mk - the mention (further m.) to be resolved3 C if mj is a pronoun; else I4 C if mk is a pronoun; else I5 the concatenated values of feature 3 and feature 46 C if the m. are the same string; else I7 C if one m. is a substring of the other; else I8 C if both m. are pronominal and are the same string; else I9 C if both are non-pronominal and are the same string; else I10 C if both are pronouns; I if neither is a pronoun; else U11 C if both are proper nouns; I if neither is; else U12 C if both m. have the same speaker; I if they do not13 C if both m. are the same named entity; I if they are not andU if they are not assigned a NE14 token distance between mj and mk15 sentence distance between mj and mk16 normalised levenstein distance for both m.17 PR if mj is premodified, PO if it is not; UN otherwise18 PR if mk is premodified, PO if it is not; UN otherwise19 the concatenated values for feature 17 and 1820 D if mj is in a definite m.; I otherwise21 C if mj is within the subject; I-within an object; U otherwise22 C if mk is within the subject; I-within an object; U otherwise23 C if neither is embedded in a PP; I otherwise24 C if neither is embedded in a NP; I otherwise25 C if mj has been classified as singleton; I otherwise26 C if both are within ARG0-ARG4; I-within ARGM; else U27 C if mj is within ARG0-ARG4; I-within ARGM; else U28 C if mk is within ARG0-ARG4; I-within ARGM; else U29 concatenated values for features 27 and 2830 the predicate argument label for mj31 the predicate argument label for mk32 C if both m. agree in number; else I33 C if both m. agree in gender; else ITable 5: The features used by the coreference classifier.91MD MUC B3 CEAFE AverageR P F1 R P F1 R P F1 R P F1 F1Automatic Mention DetectionautoArabic 27.54 80.34 41.02 19.64 62.13 29.85 41.91 90.72 57.33 56.79 24.81 34.53 40.57Chinese 35.12 72.52 47.32 31.19 57.97 40.56 49.49 77.65 60.45 45.92 25.24 32.58 44.53English 65.78 68.49 67.11 54.28 52.79 53.52 62.26 54.90 58.35 33.52 34.96 34.22 48.70goldArabic 28.00 82.21 41.78 15.47 45.92 23.15 39.22 84.86 53.65 55.10 24.22 33.65 36.82Chinese 37.84 74.84 50.27 33.95 60.29 43.44 50.95 77.28 61.41 46.68 26.13 33.50 46.12English 66.05 69.62 67.79 54.45 53.59 54.02 61.66 55.62 58.48 33.82 34.65 34.23 48.91Gold Mention BoundariesautoArabic 27.48 75.53 40.29 18.75 56.47 28.16 42.67 89.25 57.74 55.53 25.36 34.82 40.24Chinese 36.97 73.98 49.30 32.09 58.30 41.39 49.43 77.38 60.32 46.35 25.71 33.07 44.93English 66.45 70.91 68.61 54.96 54.67 54.82 61.85 55.60 58.56 34.38 34.67 34.53 49.30goldArabic 28.06 82.39 41.87 15.56 46.18 23.28 39.23 84.95 53.67 55.10 24.20 33.63 36.86Chinese 37.89 74.79 50.30 33.93 60.19 43.39 50.87 77.27 61.35 46.62 26.13 33.49 46.08English 65.82 71.72 68.65 54.68 55.51 55.09 61.22 56.59 58.82 34.85 34.04 34.44 49.45Gold MentionsautoArabic 100 100 100 42.48 80.36 55.58 50.87 89.69 64.92 71.96 34.52 46.66 55.72Chinese 100 100 100 42.02 79.57 55.00 50.22 80.81 61.94 60.27 27.08 37.37 51.44English 100 100 100 68.38 78.11 72.92 63.04 58.60 60.74 52.64 37.10 43.53 59.06goldArabic 100 100 100 45.58 73.27 56.20 52.27 82.35 63.95 70.17 37.54 48.91 56.35Chinese 100 100 100 44.12 80.89 57.10 51.79 80.53 63.04 60.37 27.69 37.96 52.70English 100 100 100 68.54 78.10 73.01 63.14 58.63 60.80 52.84 37.44 43.83 59.21Table 6: UBIU system performance in the shared task.insert the appropriate class/entity IDs in the data,removing mentions that constitute a class on theirown ?
singletons.
We bind all pronouns (except theones that were labeled as singletons by the singletonclassifier) that were not assigned an antecedent tothe last seen subject and if such is not present to thelast seen mention.
We consider all positively classi-fied instances in the clustering process.3 EvaluationThe results of the final system evaluation are pre-sented in Table 6.
Comparing the results for mentiondetection (MD) on the development set (see Table 1,which shows MD before the resolution step) and thefinal test set (Table 6, showing MD after resolutionand the deletion of singletons), we encounter a rever-sal of precision and recall tendencies (even thoughthe results are not fully comparable since they arebased on different data sets).
This is due to the factthat during mention detection, we aim for high re-call, and after coreference resolution, all mentionsidentified as singletons by the system are excludedfrom the answer set.
Thus mentions that are coref-erent in the key set but wrongly classified in the an-swer set are removed, leading to a decrease in re-call.
With regard to MD precision, a considerableincrease is recorded, showing that the majority ofthe mentions that the system indicates as coreferenthave the correct mention spans.
Additionally, theproblem of selecting the correct span (as describedin Section 2) is another factor that has a considerableeffect on precision at that stage ?
mentions that wereaccurately attached to the correct coreference chainare not considered if their span is not identical to thespan of their counterparts in the key set.Automatic Mention Detection In the first part inTable 6, we show the system scores for UBIU?s per-formance when no mention information is providedin the data.
We report both gold (using gold linguis-tic annotations) and auto (using automatically an-notated data) settings.
A comparison of the resultsshows that there are only minor differences betweenthem with gold outperforming auto apart from Ara-bic for which there is a drop of 3.75 points in thegold setting.
However, the small difference betweenall results shows that the quality of the automatic an-notation is good enough for a CR system and thatfurther improvements in the quality of the linguisticinformation will not necessarily improve CR.If we compare results across languages, we seethat Arabic has the lowest results.
One of the rea-sons for this decreased performance can be found inthe NP-rich syntactic structure of Arabic.
This leadsto a high number of identified mentions and in com-bination with the longer sentence length to a higher92number of training/test instances.
Another reasonfor the drop in performance for Arabic can be foundin the lack of annotations expected by our system(named entities and predicted arguments) that werenot provided by the task due to time constraints andthe accuracy of the annotations.
Further, Arabic isa morphologically rich language for which only thesimplified standard POS tags were provided and notthe gold standard ones that contain much richer andthus more helpful morphology information.The results for Chinese and English are relativelyclose.
We can also see that the CEAFE results areextremely close, with a difference of less than 1%.MUC, in contrast, shows the largest differences withmore than 30% between Arabic and English in thegold setting.
It is also noteworthy that the results forEnglish show a balance between precision and recallwhile both Arabic and Chinese favor precision overrecall in terms of mention detection, MUC, and B3.The reasons for this difference between languagesneed to be investigated further.Gold Mention Boundaries The results for this setof experiments is based on a version of the test setthat contains the gold boundaries of all mentions, in-cluding singletons.
Thus, we use these gold men-tion boundaries instead of the ones generated by oursystem.
These experiments give us an insight onhow well UBIU performs on selecting the correctboundaries.
Since we do not expect the system?sselection to be perfect, we would expect to see im-proved system performance given the correct bound-aries.
The results are shown in the second part ofTable 6.
As for using automatically generated men-tions the tendencies in scores between gold and autolinguistic annotations are kept.
A further compari-son of the overall results between the two settingsalso shows only minor changes.
The only exceptionis the auto setting for Arabic, for which we see dropin MD precision of approximately 5%.
This also re-sults in lower MUC and B3 precision and CEAFErecall.
The reasons for this drop in performanceneed to be investigated further.
The fact that mostresults for both auto and gold settings change onlysightly shows that having information about the cor-rect mention boundaries is not very helpful.
Thus,the system seems to have reached its optimal per-formance on selecting mention boundaries given theinformation that it has.Gold Mentions The last set of experiments isbased on a version of the test set that contains thegold mentions, i.e., all mentions that are coreferent,but without any information about the identity of thecoreference chains.
The results of this set of exper-iments gives us information about the quality of thecoreference classifier.
The results are shown in thethird part of Table 6.
Using gold parses leads toonly minor improvement of the overall system per-formance, yet, in that case all languages, includingArabic, show consistent increase of results.
Alto-gether, there is a major improvement of the scores inMD, MUC, and CEAFE .
The B3 scores only showminor improvements, resulting from a slight drop inprecision across languages.
The results also showconsiderably higher precision than recall for MUCand B3, and higher recall for CEAFE .
This meansthat the coreference decisions that the system makesare highly reliable but that it still has a preferencefor treating coreferent mentions as singletons.A comparison across languages shows that pro-viding gold mentions has a considerable positive ef-fect on the system performance for Arabic since forthat setting Chinese leads to lower overall scores.We assume that this is again due to the NP-rich syn-tactic structure of Arabic and the fact that provid-ing the mentions decreases drastically the number ofmentions the system works with and has to choosefrom during the resolution process.4 Conclusion and Future WorkWe presented the UBIU system for coreference res-olution in a multilingual setting.
The system per-formed reliably across all three languages of theCoNLL 2012 shared task.
For the future, we areplanning an in-depth investigation of the perfor-mance of the mention detection module and the sin-gleton classifier, as well as in investigation into morecomplex models for coreference classification thanthe mention pair model.AcknowledgmentsThis work is based on research supported by the USOffice of Naval Research (ONR) Grant #N00014-10-1-0140.
We would also like to thank Kiran Ku-mar for his help with tuning the system.93ReferencesAdriane Boyd, Whitney Gegg-Harrison, and Donna By-ron.
2005.
Identifying non-referential it: A machinelearning approach incorporating linguistically moti-vated patterns.
In Proceedings of the ACL Workshopon Feature Engineering for Machine Learning in Nat-ural Language Processing, FeatureEng ?05, pages 40?47, Ann Arbor, MI.Samuel Broscheit, Massimo Poesio, Simone PaoloPonzetto, Kepa Joseba Rodriguez, Lorenza Romano,Olga Uryupina, Yannick Versley, and Roberto Zanoli.2010.
BART: A Multilingual Anaphora ResolutionSystem.
In Proceedings of the 5th International Work-shop on Semantic Evaluation (SemEval), pages 104?107, Uppsala, Sweden.Walter Daelemans and Antal van den Bosch.
2005.Memory-Based Language Processing.
Studies inNatural Language Processing.
Cambridge UniversityPress, Cambridge, UK.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2010.
TiMBL: Tilburg MemoryBased Learner, version 6.3,reference guide.
Techni-cal Report ILK 10-01, Induction of Linguistic Knowl-edge, Computational Linguistics, Tilburg University.Richard Evans.
2001.
Applying machine learning to-ward an automatic classification of it.
Literary andLinguistic Computing, 16(1):45 ?
57.Sanda M. Harabagiu and Steven J. Maiorano.
2000.Multilingual coreference resolution.
In Proceedingsof ANLP 2000, Seattle, WA.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 shared task.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 28?34, Port-land, OR.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-Lingual Coreference Resolution with Syntactic Fea-tures.
In Proceedings of HLT/EMNLP 2005, Vancou-ver, Canada.Ruslan Mitkov.
1999.
Multilingual anaphora resolution.Machine Translation, 14(3-4):281?299.Vincent Ng and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to improvecoreference resolution.
In Proceedings COLING ?02,pages 1?7, Taipei, Taiwan.Vincent Ng.
2004.
Learning noun phrase anaphoricity toimprove coreference resolution: Issues in representa-tion and optimization.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics, ACL ?04, Barcelona, Spain.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 shared task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofCoNLL 2011, Portland, OR.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of theSixteenth Conference on Computational Natural Lan-guage Learning (CoNLL 2012), Jeju, Korea.Altaf Rahman and Vincent Ng.
2009.
Supervised modelsfor coreference resolution.
In Proceedings of EMNLP,pages 968?977, Singapore.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.
Anto`nia Mart?
?, Mariona Taule?, Ve?roniqueHoste, Massimo Poesio, and Yannick Versley.
2010.SemEval-2010 task 1: Coreference resolution inmultiple languages.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, pages 1?8,Uppsala, Sweden.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Holger Wunsch.
2009.
Rule-Based and Memory-BasedPronoun Resolution for German: A Comparison andAssessment of Data Sources.
Ph.D. thesis, Universita?tTu?bingen.Desislava Zhekova and Sandra Ku?bler.
2010.
UBIU: Alanguage-independent system for coreference resolu-tion.
In Proceedings of the 5th International Work-shop on Semantic Evaluation, pages 96?99, Uppsala,Sweden.94
