Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1395?1404,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLanguage-independent Compound Splitting with Morphological OperationsKlaus Macherey1 Andrew M. Dai2 David Talbot1 Ashok C. Popat1 Franz Och11Google Inc.1600 Amphitheatre Pkwy.Mountain View, CA 94043, USA{kmach,talbot,popat,och}@google.com2University of Edinburgh10 Crichton StreetEdinburgh, UK EH8 9ABa.dai@ed.ac.ukAbstractTranslating compounds is an important prob-lem in machine translation.
Since many com-pounds have not been observed during train-ing, they pose a challenge for translation sys-tems.
Previous decompounding methods haveoften been restricted to a small set of lan-guages as they cannot deal with more complexcompound forming processes.
We present anovel and unsupervised method to learn thecompound parts and morphological operationsneeded to split compounds into their com-pound parts.
The method uses a bilingualcorpus to learn the morphological operationsrequired to split a compound into its parts.Furthermore, monolingual corpora are used tolearn and filter the set of compound part can-didates.
We evaluate our method within a ma-chine translation task and show significant im-provements for various languages to show theversatility of the approach.1 IntroductionA compound is a lexeme that consists of more thanone stem.
Informally, a compound is a combina-tion of two or more words that function as a singleunit of meaning.
Some compounds are written asspace-separated words, which are called open com-pounds (e.g.
hard drive), while others are writtenas single words, which are called closed compounds(e.g.
wallpaper).
In this paper, we shall focus onlyon closed compounds because open compounds donot require further splitting.The objective of compound splitting is to split acompound into its corresponding sequence of con-stituents.
If we look at how compounds are createdfrom lexemes in the first place, we find that for somelanguages, compounds are formed by concatenatingexisting words, while in other languages compound-ing additionally involves certain morphological op-erations.
These morphological operations can be-come very complex as we illustrate in the followingcase studies.1.1 Case StudiesBelow, we look at splitting compounds from 3 differ-ent languages.
The examples introduce in part thenotation used for the decision rule outlined in Sec-tion 3.1.1.1.1 English Compound SplittingThe word flowerpot can appear as a closed or opencompound in English texts.
To automatically splitthe closed form we have to try out every split pointand choose the split with minimal costs according toa cost function.
Let's assume that we already knowthat flowerpot must be split into two parts.
Then wehave to position two split points that mark the end ofeach part (one is always reserved for the last charac-ter position).
The number of split points is denotedby K (i.e.
K = 2), while the position of split pointsis denoted by n1 and n2.
Since flowerpot consists of9 characters, we have 8 possibilities to position splitpoint n1 within the characters c1, .
.
.
, c8.
The finalsplit point corresponds with the last character, that is,n2 = 9.
Trying out all possible single splits resultsin the following candidates:flowerpot ?
f+ lowerpotflowerpot ?
fl+ owerpot...flowerpot ?
flower+ pot...flowerpot ?
flowerpo+ t1395If we associate each compound part candidate witha cost that reflects how frequent this part occurs in alarge collection of English texts, we expect that thecorrect split flower + pot will have the lowest cost.1.1.2 German Compound SplittingThe previous example covered a casewhere the com-pound is constructed by directly concatenating thecompound parts.
While this works well for En-glish, other languages require additional morpholog-ical operations.
To demonstrate, we look at the Ger-man compound Verkehrszeichen (traffic sign) whichconsists of the two nouns Verkehr (traffic) and Zei-chen (sign).
Let's assume that we want to split thisword into 3 parts, that is, K = 3.
Then, we get thefollowing candidates.Verkehrszeichen ?
V+ e+ rkehrszeichenVerkehrszeichen ?
V+ er+ kehrszeichen...Verkehrszeichen ?
Verkehr+ s+ zeichen...Verkehrszeichen ?
Verkehrszeich+ e+ nUsing the same procedure as described before, wecan lookup the compound parts in a dictionary or de-termine their frequency from large text collections.This yields the optimal split points n1 = 7, n2 =8, n3 = 15.
The interesting part here is the addi-tional s morpheme, which is called a linking mor-pheme, because it combines the two compound partsto form the compound Verkehrszeichen.
If we havea list of all possible linking morphemes, we canhypothesize them between two ordinary compoundparts.1.1.3 Greek Compound SplittingThe previous example required the insertion of alinking morpheme between two compound parts.We shall now look at a more complicated mor-phological operation.
The Greek compound??????????
(cardboard box) consists of the twoparts ?????
(paper) and ?????
(box).
Here, theproblem is that the parts ?????
and ?????
are notvalid words in Greek.
To lookup the correct words,we must substitute the suffix of the compound partcandidates with some other morphemes.
If we allowthe compound part candidates to be transformed bysome morphological operation, we can lookup thetransformed compound parts in a dictionary or de-termine their frequencies in some large collection ofGreek texts.
Let's assume that we need only one splitpoint.
Then this yields the following compound partcandidates:??????????
?
?
+ ???????????????????
?
?
+ ?????????
g2 : ?
/ ???????????
?
?
+ ?????????
g2 : ?
/ ?...??????????
?
?????
+ ?????
g1 : ?
/ ?
,g2 : ?
/ ?...??????????
?
?????????
+ ?
g1 : ?
/ ???????????
?
?????????
+ ?
g2 : ?
/ ?Here, gk : s/t denotes the kth compound part whichis obtained by replacing string s with string t in theoriginal string, resulting in the transformed part gk.1.2 Problems and ObjectivesOur goal is to design a language-independent com-pound splitter that is useful for machine translation.The previous examples addressed the importance ofa cost function that favors valid compound parts ver-sus invalid ones.
In addition, the examples haveshown that, depending on the language, the morpho-logical operations can become very complex.
Formost Germanic languages like Danish, German, orSwedish, the list of possible linking morphemes israther small and can be provided manually.
How-ever, in general, these lists can become very large,and language experts who could provide such listsmight not be at our disposal.
Because it seems in-feasible to list the morphological operations explic-itly, we want to find and extract those operationsautomatically in an unsupervised way and providethem as an additional knowledge source to the de-compounding algorithm.Another problem is how to evaluate the qualityof the compound splitter.
One way is to compilefor every language a large collection of compoundstogether with their valid splits and to measure theproportion of correctly split compounds.
Unfortu-nately, such lists do not exist for many languages.1396While the training algorithm for our compound split-ter shall be unsupervised, the evaluation data needsto be verified by human experts.
Since we are in-terested in improving machine translation and to cir-cumvent the problem of explicitly annotating com-pounds, we evaluate the compound splitter within amachine translation task.
By decompounding train-ing and test data of a machine translation system, weexpect an increase in the number of matching phrasetable entries, resulting in better translation qualitymeasured in BLEU score (Papineni et al, 2002).If BLEU score is sensitive enough to measure thequality improvements obtained from decompound-ing, there is no need to generate a separate gold stan-dard for compounds.Finally, we do not want to split non-compoundsand named entities because we expect them to betranslated non-compositionally.
For example, theGerman wordDeutschland (Germany) could be splitinto two parts Deutsch (German) + Land (coun-try).
Although this is a valid split, named entitiesshould be kept as single units.
An example for anon-compound is the German participle vereinbart(agreed) which could be wrongly split into the partsVerein (club) + Bart (beard).
To avoid overly eagersplitting, we will compile a list of non-compounds inan unsupervised way that serves as an exception listfor the compound splitter.
To summarize, we aim tosolve the following problems:?
Define a cost function that favors valid com-pound parts and rejects invalid ones.?
Learn morphological operations, which is im-portant for languages that have complex com-pound forming processes.?
Apply compound splitting to machine transla-tion to aid in translation of compounds that havenot been seen in the bilingual training data.?
Avoid splitting non-compounds and named en-tities as this may result in wrong translations.2 Related workPrevious work concerning decompounding can bedivided into two categories: monolingual and bilin-gual approaches.Brown (2002) describes a corpus-driven approachfor splitting compounds in a German-English trans-lation task derived from a medical domain.
A largeproportion of the tokens in both texts are cognateswith a Latin or Greek etymological origin.
While theEnglish text keeps the cognates as separate tokens,they are combined into compounds in the Germantext.
To split these compounds, the author comparesboth the German and the English cognates on a char-acter level to find reasonable split points.
The algo-rithm described by the author consists of a sequenceof if-then-else conditions that are applied on the twocognates to find the split points.
Furthermore, sincethe method relies on finding similar character se-quences between both the source and the target to-kens, the approach is restricted to cognates and can-not be applied to split more complex compounds.Koehn and Knight (2003) present a frequency-based approach to compound splitting for German.The compound parts and their frequencies are es-timated from a monolingual corpus.
As an exten-sion to the frequency approach, the authors describea bilingual approach where they use a dictionary ex-tracted from parallel data to find better split options.The authors allow only two linking morphemes be-tween compound parts and a few letters that can bedropped.
In contrast to our approach, those opera-tions are not learned automatically, but must be pro-vided explicitly.Garera and Yarowsky (2008) propose an approachto translate compounds without the need for bilin-gual training texts.
The compound splitting pro-cedure mainly follows the approach from (Brown,2002) and (Koehn and Knight, 2003), so the em-phasis is put on finding correct translations for com-pounds.
To accomplish this, the authors use cross-language compound evidence obtained from bilin-gual dictionaries.
In addition, the authors describe asimple way to learn glue characters by allowing thedeletion of up to two middle and two end charac-ters.1 More complex morphological operations arenot taken into account.Alfonseca et al (2008b) describe a state-of-the-art German compound splitter that is particularly ro-bust with respect to noise and spelling errors.
Thecompound splitter is trained on monolingual data.Besides applying frequency and probability-basedmethods, the authors also take the mutual informa-tion of compound parts into account.
In addition, the1However, the glue characters found by this procedure seemto be biased for at least German and Albanian.
A very frequentglue morpheme like -es- is not listed, while glue morphemeslike -k- and -h- rank very high, although they are invalid gluemorphemes for German.
Albanian shows similar problems.1397authors look for compound parts that occur in dif-ferent anchor texts pointing to the same document.All these signals are combined and the weights aretrained using a support vector machine classifier.
Al-fonseca et al (2008a) apply this compound splitteron various other Germanic languages.Dyer (2009) applies a maximum entropy modelof compound splitting to generate segmentation lat-tices that serve as input to a translation system.To train the model, reference segmentations are re-quired.
Here, we produce only single best segmen-tations, but otherwise do not rely on reference seg-mentations.3 Compound Splitting AlgorithmIn this section, we describe the underlying optimiza-tion problem and the algorithm used to split a tokeninto its compound parts.
Starting from Bayes' de-cision rule, we develop the Bellman equation andformulate a dynamic programming-based algorithmthat takes a word as input and outputs the constituentcompound parts.
We discuss the procedure used toextract compound parts from monolingual texts andto learn themorphological operations using bilingualcorpora.3.1 Decision Rule for Compound SplittingGiven a token w = c1, .
.
.
, cN = cN1 consisting of asequence of N characters ci, the objective functionis to find the optimal number K?
and sequence of splitpoints n?K?0 such that the subwords are the constituentsof the token, where2 n0 := 0 and nK := N :w = cN1 ?
(K?, n?K?0 ) == argmaxK,nK0{Pr(cN1 ,K, nK0 )}(1)= argmaxK,nK0{Pr(K) ?
Pr(cN1 , nK0 |K)}u argmaxK,nK0{p(K) ?K?k=1p(cnknk?1+1, nk?1|K)?
?p(nk|nk?1,K)} (2)with p(n0) = p(nK |?)
?
1.
Equation 2 requires thattoken w can be fully decomposed into a sequence2For algorithmic reasons, we use the start position 0 to rep-resent a fictitious start symbol before the first character of theword.of lexemes, the compound parts.
Thus, determin-ing the optimal segmentation is sufficient for findingthe constituents.
While this may work for some lan-guages, the subwords are not valid words in generalas discussed in Section 1.1.3.
Therefore, we allowthe lexemes to be the result of a transformation pro-cess, where the transformed lexemes are denoted bygK1 .
This leads to the following refined decision rule:w = cN1 ?
(K?, n?K?0 , g?K?1 ) == argmaxK,nK0 ,gK1{Pr(cN1 ,K, nK0 , gK1 )}(3)= argmaxK,nK0 ,gK1{Pr(K) ?
Pr(cN1 , nK0 , gK1 |K)}(4)u argmaxK,nK0 ,gK1{p(K) ?K?k=1p(cnknk?1+1, nk?1, gk|K)?
??
?compound part probability??
p(nk|nk?1,K)}(5)The compound part probability is a zero-ordermodel.
If we penalize each split with a constant splitpenalty ?, and make the probability independent ofthe number of splits K, we arrive at the followingdecision rule:w = cN1 ?
(K?, n?K?1 , g?K?1 )= argmaxK,nK0 ,gK1{?K ?K?k=1p(cnknk?1+1, nk?1, gk)}(6)3.2 Dynamic ProgrammingWe use dynamic programming to find the optimalsplit sequence.
Each split infers certain costs thatare determined by a cost function.
The total costs ofa decomposed word can be computed from the in-dividual costs of the component parts.
For the dy-namic programming approach, we define the follow-ing auxiliary function Q with nk = j:Q(cj1) = maxnk0 ,gk1{?k ?k??=1p(cn?n?
?1+1, n?
?1, g?
)}that is, Q(cj1) is equal to the minimal costs (maxi-mum probability) that we assign to the prefix stringcj1 where we have used k split points at positions nk1 .This yields the following recursive equation:Q(cj1) = maxnk,gk{?
?
Q(cnk?11 )??
p(cnknk?1+1, nk?1, gk)}(7)1398Algorithm 1 Compound splittingInput: input word w = cN1Output: compound partsQ(0) = 0Q(1) = ?
?
?
= Q(N) = ?for i = 0, .
.
.
, N ?
1 dofor j = i + 1, .
.
.
, N dosplit-costs = Q(i) + cost(cji+1, i, gj) +split-penaltyif split-costs < Q(j) thenQ(j) = split-costsB(j) = (i, gj)end ifend forend forwith backpointerB(j) = argmaxnk,gk{?
?
Q(cnk?11 )??
p(cnknk?1+1, nk?1, gk)}(8)Using logarithms in Equations 7 and 8, we can inter-pret the quantities as additive costs rather than proba-bilities.
This yields Algorithm 1, which is quadraticin the length of the input string.
By enforcing thateach compound part does not exceed a predefinedconstant length `, we can change the second for loopas follows:for j = i + 1, .
.
.
,min(i + `,N) doWith this change, Algorithm 1 becomes linear in thelength of the input word, O(|w|).4 Cost Function and Knowledge SourcesThe performance of Algorithm 1 depends onthe cost function cost(?
), that is, the probabilityp(cnknk?1+1, nk?1, gk).
This cost function incorpo-rates knowledge about morpheme transformations,morpheme positionswithin a compound part, and thecompound parts themselves.4.1 Learning Morphological Operations usingPhrase TablesLet s and t be strings of the (source) language al-phabet A.
A morphological operation s/t is a pairof strings s, t ?
A?, where s is replaced by t. Withthe usual definition of the Kleene operator ?, s andt can be empty, denoted by ?.
An example for sucha pair is ?/es, which models the linking morphemees in the German compound Bundesagentur (federalagency):Bundesagentur ?
Bund+ es+ Agentur .Note that by replacing either s or t with ?, we canmodel insertions or deletions of morphemes.
Theexplicit dependence on position nk?1 in Equation 6allows us to determine if we are at the beginning,in the middle, or at the end of a token.
Thus, wecan distinguish between start, middle, or end mor-phemes and hypothesize them during search.3 Al-though not explicitly listed in Algorithm 1, we dis-allow sequences of linking morphemes.
This canbe achieved by setting the costs to infinity for thosemorpheme hypotheses, which directly succeed an-other morpheme hypothesis.To learn the morphological operations involvedin compounding, we determine the differences be-tween a compound and its compound parts.
This canbe done by computing the Levenshtein distance be-tween the compound and its compound parts, withthe allowable edit operations being insertion, dele-tion, or substitution of one or more characters.
If westore the current and previous characters, edit opera-tion and the location (prefix, infix or suffix) at eachposition during calculation of the Levenshtein dis-tance then we can obtain the morphological opera-tions required for compounding.
Applying the in-verse operations, that is, replacing twith s yields theoperation required for decompounding.4.1.1 Finding Compounds and their PartsTo learn the morphological operations, we needcompounds together with their compound parts.
Thebasic idea of finding compound candidates and theircompound parts in a bilingual setting are related tothe ideas presented in (Garera and Yarowsky, 2008).Here, we use phrase tables rather than dictionaries.Although phrase tablesmight containmore noise, webelieve that overall phrase tables cover more phe-nomena of translations thanwhat can be found in dic-tionaries.
The procedure is as follows.
We are givena phrase table that provides translations for phrasesfrom a source language l into English and from En-glish into l. Under the assumption that English doesnot contain many closed compounds, we can search3We jointly optimize over K and the split points nk, so weknow that cnKnK?1 is a suffix of w.1399the phrase table for those single-token source wordsf in language l, which translate into multi-token En-glish phrases e1, .
.
.
, en for n > 1.
This resultsin a list of (f ; e1, .
.
.
, en) pairs, which are poten-tial compound candidates together with their Englishtranslations.
If for each pair, we take each token eifrom the English (multi-token) phrase and lookupthe corresponding translation for language l to getgi, we should find entries that have at least somepartial match with the original source word f , if fis a true compound.
Because the translation phrasetable was generated automatically during the train-ing of a multi-language translation system, there isno guarantee that the original translations are cor-rect.
Thus, the bilingual extraction procedure issubject to introduce a certain amount of noise.
Tomitigate this, thresholds such as minimum edit dis-tance between the potential compound and its parts,minimum co-occurrence frequencies for the selectedbilingual phrase pairs and minimum source and tar-get word lengths are used to reduce the noise at theexpense of finding fewer compounds.
Those entriesthat obey these constraints are output as triples ofform:(f ; e1, .
.
.
, en; g1, .
.
.
, gn) (9)where?
f is likely to be a compound,?
e1, .
.
.
, en is the English translation, and?
g1, .
.
.
, gn are the compound parts of f .The following example for German illustrates theprocess.
Suppose that the most probable translationfor?berweisungsbetrag is transfer amount using thephrase table.
We then look up the translation back toGerman for each translated token: transfer translatesto?berweisung and amount translates toBetrag.
Wethen calculate the distance between all permutationsof the parts and the original compound and choosethe one with the lowest distance and highest transla-tion probability: ?berweisung Betrag.4.2 Monolingual Extraction of CompoundPartsThe most important knowledge source required forAlgorithm 1 is a word-frequency list of compoundparts that is used to compute the split costs.
Theprocedure described in Section 4.1.1 is useful forlearning morphological operations, but it is not suffi-cient to extract an exhaustive list of compound parts.Such lists can be extracted frommonolingual data forwhich we use language model (LM) word frequencylists in combination with some filter steps.
The ex-traction process is subdivided into 2 passes, one overa high-quality news LM to extract the parts and theother over a web LM to filter the parts.4.2.1 Phase 1: Bootstrapping passIn the first pass, we generate word frequency lists de-rived from news articles for multiple languages.
Themotivation for using news articles rather than arbi-trary web texts is that news articles are in generalless noisy and contain fewer spelling mistakes.
Thelanguage-dependent word frequency lists are filteredaccording to a sequence of filter steps.
These filtersteps include discarding all words that contain digitsor punctuations other than hyphen, minimum occur-rence frequency, and a minimum length which weset to 4.
The output is a table that contains prelim-inary compound parts together with their respectivecounts for each language.4.2.2 Phase 2: Filtering passIn the second pass, the compound part vocabularyis further reduced and filtered.
We generate a LMvocabulary based on arbitrary web texts for each lan-guage and build a compound splitter based on the vo-cabulary list that was generated in phase 1.
We nowtry to split every word of the web LM vocabularybased on the compound splitter model from phase1.
For the compound parts that occur in the com-pound splitter output, we determine how often eachcompound part was used and output only those com-pound parts whose frequency exceed a predefinedthreshold n.4.3 ExampleSuppose we have the following word frequenciesoutput from pass 1:floor 10k poll 4kflow 9k pot 5kflower 15k potter 20kIn pass 2, we observe the word flowerpot.
With theabove list, the only compound parts used are flowerand pot.
If we did not split any other words andthreshold at n = 1, our final list would consist offlower and pot.
This filtering pass has the advantageof outputting only those compound part candidates1400which were actually used to split words from webtexts.
The thresholding also further reduces the riskof introducing noise.
Another advantage is that sincethe set of parts output in the first pass may contain ahigh number of compounds, the filter is able to re-move a large number of these compounds by exam-ining relative frequencies.
In our experiments, wehave assumed that compound part frequencies arehigher than the compound frequency and so removewords from the part list that can themselves be splitand have a relatively high frequency.
Finally, afterremoving the low frequency compound parts, we ob-tain the final compound splitter vocabulary.4.4 Generating Exception ListsTo avoid eager splitting of non-compounds andnamed entities, we use a variant of the procedure de-scribed in Section 4.1.1.
By emitting all those sourcewords that translate with high probability into single-token English words, we obtain a list of words thatshould not be split.44.5 Final Cost FunctionThe final cost function is defined by the followingcomponents which are combined log-linearly.?
The split penalty ?
penalizes each compoundpart to avoid eager splitting.?
The cost for each compound part gk is com-puted as ?
logC(gk), where C(gk) is the un-igram count for gk obtained from the news LMword frequency list.
Since we use a zero-ordermodel, we can ignore the normalization andwork with unigram counts rather than unigramprobabilities.?
Because Algorithm 1 iterates over the charac-ters of the input token w, we can infer from theboundaries (i, j) if we are at the start, in themiddle, or at the end of the token.
Applyinga morphological operation adds costs 1 to theoverall costs.Although the cost function is language dependent,we use the same split penalty weight ?
= 20 for alllanguages except for German, where the split penaltyweight is set to 13.5.5 ResultsTo show the language independence of the approachwithin a machine translation task, we translate fromlanguages belonging to different language familiesinto English.
The publicly available Europarl corpusis not suitable for demonstrating the utility of com-pound splitting because there are few unseen com-pounds in the test section of the Europarl corpus.The WMT shared translation task has a broader do-main compared to Europarl but covers only a fewlanguages.
Hence, we present results for German-English using the WMT-07 data and cover other lan-guages using non-public corporawhich contain newsas well as open-domain web texts.
Table 1 lists thevarious corpus statistics.
The source languages aregrouped according to their language family.For learning the morphological operations, we al-lowed the substitution of at most 2 consecutive char-acters.
Furthermore, we only allowed at most onemorphological substitution to avoid introducing toomuch noise.
The found morphological operationswere sorted according to their frequencies.
Thosewhich occurred less than 100 times were discarded.Examples of extracted morphological operations aregiven in Table 2.
Because the extraction proceduredescribed in Section 4.1 is not purely restricted to thecase of decompounding, we found that many mor-phological operations emitted by this procedure re-flect morphological variations that are not directlylinked to compounding, but caused by inflections.To generate the language-dependent lists of com-pound parts, we used language model vocabularylists5 generated from news texts for different lan-guages as seeds for the first pass.
These lists werefiltered by discarding all entries that either con-tained digits, punctuations other than hyphens, or se-quences of the same characters.
In addition, the in-frequent entries were discarded as well to further re-duce noise.
For the second pass, we used the listsgenerated in the first pass together with the learnedmorphological operations to construct a preliminarycompound splitter.
We then generated vocabularylists for monolingual web texts and applied the pre-liminary compound splitter onto this list.
The used4Because we will translate only into English, this is not anissue for the introductory example flowerpot.5The vocabulary lists also contain the word frequencies.
Weuse the term vocabulary list synonymously for a word frequencylist.1401Family Src Language #Tokens Train src/trg #Tokens Dev src/trg #Tokens Tst src/trgGermanic Danish 196M 201M 43, 475 44, 479 72, 275 74, 504German 43M 45M 23, 151 22, 646 45, 077 43, 777Norwegian 251M 255M 42, 096 43, 824 70, 257 73, 556Swedish 201M 213M 42, 365 44, 559 70, 666 74, 547Hellenic Greek 153M 148M 47, 576 44, 658 79, 501 74, 776Uralic Estonian 199M 244M 34, 987 44, 658 57, 916 74, 765Finnish 205M 246M 32, 119 44, 658 53, 365 74, 771Table 1: Corpus statistics for various language pairs.
The target language is always English.
The source languages aregrouped according to their language family.Language morpholog.
operationsDanish -/?, s/?German -/?, s/?, es/?, n/?, e/?, en/?Norwegian -/?, s/?, e/?Swedish -/?, s/?Greek ?/?, ?/?, ?/?, ?/?, ?/?, ?/?Estonian -/?, e/?, se/?Finnish ?/n, n/?, ?/enTable 2: Examples of morphological operations that wereextracted from bilingual corpora.compound parts were collected and sorted accordingto their frequencies.
Those which were used at least2 times were kept in the final compound parts lists.Table 3 reports the number of compound parts keptafter each pass.
For example, the Finnish news vo-cabulary list initially contained 1.7M entries.
Afterremoving non-alpha and infrequent words in the firstfilter step, we obtained 190K entries.
Using the pre-liminary compound splitter in the second filter stepresulted in 73K compound part entries.The finally obtained compound splitter was in-tegrated into the preprocessing pipeline of a state-of-the-art statistical phrase-based machine transla-tion system that works similar to the Moses de-coder (Koehn et al, 2007).
By applying the com-pound splitter during both training and decoding weensured that source language tokens were split inthe same way.
Table 4 presents results for vari-ous language-pairs with and without decompound-ing.
Both the Germanic and the Uralic languagesshow significant BLEU score improvements of 1.3BLEU points on average.
The confidence inter-vals were computed using the bootstrap resamplingnormal approximation method described in (Noreen,1989).
While the compounding process for Ger-manic languages is rather simple and requires only afew linking morphemes, compounds used in Uraliclanguages have a richer morphology.
In contrast tothe Germanic and Uralic languages, we did not ob-serve improvements for Greek.
To investigate thislack of performance, we turned off transliterationand kept unknown source words in their originalscript.
We analyzed the number of remaining sourcecharacters in the baseline system and the system us-ing compound splitting by counting the number ofGreek characters in the translation output.
The num-ber of remaining Greek characters in the translationoutput was reduced from 6, 715 in the baseline sys-tem to 3, 624 in the systemwhich used decompound-ing.
In addition, a few other metrics like the numberof source words that consisted of more than 15 char-acters decreased as well.
Because we do not knowhow many compounds are actually contained in theGreek source sentences6 and because the frequencyof using compounds might vary across languages,we cannot expect the same performance gains acrosslanguages belonging to different language families.An interesting observation is, however, that if onelanguage from a language family shows performancegains, then there are performance gains for all thelanguages in that family.
We also investigated the ef-fect of not using any morphological operations.
Dis-allowing all morphological operations accounts fora loss of 0.1 - 0.2 BLEU points across translationsystems and increases the compound parts vocabu-lary lists by up to 20%, which means that most of thegains can be achieved with simple concatenation.The exception lists were generated according tothe procedure described in Section 4.4.
Since weaimed for precision rather than recall when con-structing these lists, we inserted only those source6Quite a few of the remaining Greek characters belong torare named entities.1402Language initial vocab size #parts after 1st pass #parts after 2nd passDanish 918, 708 132, 247 49, 592German 7, 908, 927 247, 606 45, 059Norwegian 1, 417, 129 237, 099 62, 107Swedish 1, 907, 632 284, 660 82, 120Greek 877, 313 136, 436 33, 130Estonian 742, 185 81, 132 36, 629Finnish 1, 704, 415 190, 507 73, 568Table 3: Number of remaining compound parts for various languages after the first and second filter step.System BLEU[%] w/o splitting BLEU[%] w splitting ?
CI 95%Danish 42.55 44.39 1.84 (?
0.65)German WMT-07 25.76 26.60 0.84 (?
0.70)Norwegian 42.77 44.58 1.81 (?
0.64)Swedish 36.28 38.04 1.76 (?
0.62)Greek 31.85 31.91 0.06 (?
0.61)Estonian 20.52 21.20 0.68 (?
0.50)Finnish 25.24 26.64 1.40 (?
0.57)Table 4: BLEU score results for various languages translated into English with and without compound splitting.Language Split source translationGerman no Die EU ist nicht einfach ein Freundschaftsclub.
The EU is not just a Freundschaftsclub.yes Die EU ist nicht einfach ein Freundschaft Club The EU is not simply a friendship club.Greek no ??
?????
???????????
??????????
; What ???????????
configuration?yes ??
?????
?????
??????
??????????
; What is pulse code modulation?Finnish no Lis?vuodevaatteet ja pyyheliinat ovat kaapissa.
Lis?vuodevaatteet and towels are in the closet.yes Lis?
Vuode Vaatteet ja pyyheliinat ovat kaapissa.
Extra bed linen and towels are in the closet.Table 5: Examples of translations into English with and without compound splitting.words whose co-occurrence count with a unigramtranslation was at least 1, 000 and whose translationprobability was larger than 0.1.
Furthermore, we re-quired that at least 70%of all target phrase entries fora given source word had to be unigrams.
All decom-pounding results reported in Table 4 were generatedusing these exception lists, which prevented wrongsplits caused by otherwise overly eager splitting.6 Conclusion and OutlookWe have presented a language-independent methodfor decompounding that improves translations forcompounds that otherwise rarely occur in the bilin-gual training data.
We learned a set of morpholog-ical operations from a translation phrase table anddetermined suitable compound part candidates frommonolingual data in a two pass process.
This al-lowed us to learn morphemes and operations for lan-guages where these lists are not available.
In addi-tion, we have used the bilingual information storedin the phrase table to avoid splitting non-compoundsas well as frequent named entities.
All knowledgesources were combined in a cost function that wasapplied in a compound splitter based on dynamicprogramming.
Finally, we have shown this improvestranslation performance on languages from differentlanguage families.The weights were not optimized in a systematicway but set manually to their respective values.
Inthe future, the weights of the cost function should belearned automatically by optimizing an appropriateerror function.
Instead of using gold data, the devel-opment data for optimizing the error function couldbe collected without supervision using the methodsproposed in this paper.1403ReferencesEnrique Alfonseca, Slaven Bilac, and Stefan Paries.2008a.
Decompounding query keywords from com-pounding languages.
In Proc.
of the 46th Annual Meet-ing of the Association for Computational Linguistics(ACL): Human Language Technologies (HLT), pages253--256, Columbus, Ohio, USA, June.Enrique Alfonseca, Slaven Bilac, and Stefan Paries.2008b.
German decompounding in a difficult corpus.In A. Gelbukh, editor, Lecture Notes in Computer Sci-ence (LNCS): Proc.
of the 9th Int.
Conf.
on IntelligentText Processing and Computational Linguistics (CI-CLING), volume 4919, pages 128--139.
Springer Ver-lag, February.Ralf D. Brown.
2002.
Corpus-Driven Splitting of Com-poundWords.
In Proc.
of the 9th Int.
Conf.
on Theoret-ical andMethodological Issues inMachine Translation(TMI), pages 12--21, Keihanna, Japan, March.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for mt.
In Proc.
ofthe Human Language Technologies (HLT): The An-nual Conf.
of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL), pages406--414, Boulder, Colorado, June.Nikesh Garera and David Yarowsky.
2008.
TranslatingCompounds by Learning Component Gloss Transla-tion Models via Multiple Languages.
In Proc.
of the3rd Internation Conference on Natural Language Pro-cessing (IJCNLP), pages 403--410, Hyderabad, India,January.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In Proc.
of the 10thConf.
of the European Chapter of the Association forComputational Linguistics (EACL), volume 1, pages187--193, Budapest, Hungary, April.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proc.
of the 44thAnnual Meeting of the Association for ComputationalLinguistics (ACL), volume 1, pages 177--180, Prague,Czech Republic, June.Eric W. Noreen.
1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley & Sons, Canada.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proc.
of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 311--318, Philadel-phia, Pennsylvania, July.1404
