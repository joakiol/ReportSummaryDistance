Coling 2010: Poster Volume, pages 748?756,Beijing, August 2010Head-modifier Relation based Non-lexical Reordering Modelfor Phrase-Based TranslationShui Liu1, Sheng Li1, Tiejun Zhao1, Min Zhang2, Pengyuan Liu31School of Computer Science and Technology, Habin Institute of Technology{liushui,lisheng,tjzhao}@mtlab.hit.edu.cn2Institute for Infocomm Researchmzhang@i2r.a-star.edu.sg3Institute of Computational Linguistics, Peking Universityliupengyuan@pku.edu.cnAbstractPhrase-based statistical MT (SMT) is amilestone in MT.
However, the transla-tion model in the phrase based SMT isstructure free which greatly limits itsreordering capacity.
To address this is-sue, we propose a non-lexical head-modifier based reordering model onword level by utilizing constituent basedparse tree in source side.
Our experi-mental results on the NIST Chinese-English benchmarking data show that,with a very small size model, our me-thod significantly outperforms the base-line by 1.48% bleu score.1 IntroductionSyntax has been successfully applied to SMT toimprove translation performance.
Research inapplying syntax information to SMT has beencarried out in two aspects.
On the one hand, thesyntax knowledge is employed by directly inte-grating the syntactic structure into the transla-tion rules i.e.
syntactic translation rules.
On thisperspective, the word order of the target transla-tion is modeled by the syntax structure explicit-ly.
Chiang (2005), Wu (1997) and Xiong (2006)learn the syntax rules using the formal gram-mars.
While more research is conducted to learnsyntax rules with the help of linguistic analysis(Yamada and Knight, 2001; Graehl and Knight,2004).
However, there are some challenges tothese models.
Firstly, the linguistic analysis isfar from perfect.
Most of these methods requirean off-the-shelf parser to generate syntacticstructure, which makes the translation resultssensitive to the parsing errors to some extent.To tackle this problem, n-best parse trees andparsing forest (Mi and Huang, 2008; Zhang,2009) are proposed to relieve the error propaga-tion brought by linguistic analysis.
Secondly,some phrases which violate the boundary oflinguistic analysis are also useful in these mod-els ( DeNeefe et al, 2007; Cowan et al 2006).Thus, a tradeoff needs to be found between lin-guistic sense and formal sense.On the other hand, instead of using syntactictranslation rules, some previous work attemptsto learn the syntax knowledge separately andthen integrated those knowledge to the originalconstraint.
Marton and Resnik (2008) utilize thelanguage linguistic analysis that is derived fromparse tree to constrain the translation in a softway.
By doing so, this approach addresses thechallenges brought by linguistic analysisthrough the log-linear model in a soft way.Starting from the state-of-the-art phrase basedmodel Moses ( Koehn e.t.
al, 2007), we proposea head-modifier relation based reordering modeland use the proposed model as  a soft syntaxconstraint in the phrase-based translationframework.
Compared with most of previoussoft constraint models, we study the way to util-ize the constituent based parse tree structure bymapping the parse tree to sets of head-modifierfor phrase reordering.
In this way, we build aword level reordering model instead of phras-al/constituent level model.
In our model, withthe help of the alignment and the head-modifierdependency based relationship in the sourceside, the reordering type of each target wordwith alignment in source side is identified asone of pre-defined reordering types.
With thesereordering types, the reordering of phrase intranslation is estimated on word level.748Fig 1.
An Constituent based Parse Tree2 BaselineMoses, a state-of-the-art phrase based SMT sys-tem is used as our baseline system.
In Moses,given the source language f and target languagee, the decoder is to find:ebest = argmaxe p ( e | f ) pLM ( e ) ?length(e)        (1)where p(e|f) can be computed using phrasetranslation model, distortion model and lexicalreordering model.
pLM(e) can be computed us-ing the language model.
?length(e) is word penaltymodel.Among the above models, there are threereordering-related components: language model,lexical reordering model and distortion model.The language model can reorder the local targetwords within a fixed window in an implied way.The lexical reordering model and distortionreordering model tackle the reordering problembetween adjacent phrase on lexical level andalignment level.
Besides these reordering model,the decoder induces distortion pruning con-straints to encourage the decoder translate theleftmost uncovered word in the source sidefirstly and to limit the reordering within a cer-tain range.3 ModelIn this paper, we utilize the constituent parsetree of source language to enhance the  reorder-ing capacity of the translation model.
Instead ofdirectly employing the parse tree fragments(Bod, 1992; Johnson, 1998) in reordering rules(Huang and Knight, 2006; Liu 2006; Zhang andJiang 2008), we make a mapping from trees tosets of head-modifier dependency relations(Collins 1996 ) which  can be obtained  from theconstituent based parse tree with the help ofhead rules ( Bikel, 2004 ).3.1 Head-modifier RelationAccording to Klein and Manning (2003) andCollins (1999), there are two shortcomings in n-ary Treebank grammar.
Firstly, the grammar istoo coarse for parsing.
The rules in differentcontext always have different distributions.
Se-condly, the rules learned from training corpuscannot cover the rules in testing set.Currently, the state-of-the-art parsing algo-rithms (Klein and Manning, 2003; Collins 1999)decompose the n-ary Treebank grammar intosets of head-modifier relationships.
The parsingrules in these algorithms are constructed in theform of finer-grained binary head-modifier de-pendency relationships.
Fig.2 presents an exam-ple of head-modifier based dependency treemapped from the constituent parse tree in Fig.1.749Fig.
2.
Head-modifier Relationships with Aligned TranslationMoreover, there are several reasons for whichwe adopt the head-modifier structured tree asthe main frame of our reordering model.
Firstly,the dependency relationships can reflect someunderlying binary long distance dependencyrelations in the source side.
Thus, binary depen-dency structure will suffer less from the longdistance reordering constraint.
Secondly, inhead-modifier relation, we not only can utilizethe context of dependency relation in reorderingmodel, but also can utilize some well-knownand proved helpful context (Johnson, 1998) ofconstituent base parse tree in reordering model.Finally, head-modifier relationship is matureand widely adopted method in full parsing.3.2 Head-modifier Relation Based Reor-dering ModelBefore elaborating the model, we define somenotions further easy understanding.
S=<f1, f2?fn> is the source sentence; T=<e1,e2,?,em> isthe target sentence; AS={as(i) | 1?
as(i) ?
n }where as(i) represents that the ith word in sourcesentence  aligned to the as(i)th word in targetsentence; AT={aT(i) | 1?
aT (i) ?
n } where aT(i)represents that the ith word in target sentencealigned to the aT(i)th word in source sentence;D= {( d(i), r(i) )| 0?
d(i) ?n} is the head-modifier relation set of  the words in S whered(i) represents that the ith word in source sen-tence is the modifier of d(i)th  word in sourcesentence under relationship r(i); O= < o1, o2,?,om > is the sequence of the reordering type ofevery word in target language.
The reorderingmodel probability is P(O| S, T, D, A).Relationship: in this paper, we not only use thelabel of the constituent label as Collins (1996),but also use some well-known context in pars-ing to define the head-modifier relationship r(.
),including the POS of the modifier m,  the POSof the head h, the dependency direction d, theparent label of the dependency label l, thegrandfather label of the dependency relation p,the POS of adjacent siblings of the modifier s.Thus, the head-modifier relationship can berepresented as a 6-tuple <m, h, d, l, p, s>.r(.)
relationshipr(1) <VV, - , -, -, -, - >r(2) <NN, NN, right, NP, IP, - >r(3) <NN,VV, right, IP, CP, - >r(4) <VV, DEC, right, CP, NP, - >r(5) <NN,VV, left, VP, CP, - >r(6) <DEC, NP, right, NP, VP, - >r(7) <NN, VV, left, VP,  TOP, - >Table 1.
Relations Extracted from Fig 2.In Table 1, there are 7 relationships extractedfrom the source head-modifier based dependen-cy tree as shown in Fig.2.
Please notice that, inthis paper, each source word has a correspond-ing relation.Reordering type: there are 4 reordering typesfor target words with linked word in the sourceside in our model: R= {rm1, rm2, rm3 , rm4}.
Thereordering type of target word as(i) is defined  asfollows:?
rm1: if the position number of the ithword?s head is less than i ( d(i) < i ) insource language, while the position num-ber of the word aligned to i is less than750as(d(i)) (as(i)  < as(d(i)) ) in target lan-guage;?
rm2: if the position number of the ithword?s head is less than i ( d(i) < i ) insource language, while the position num-ber of the word aligned to i is larger thanas(d(i)) (as(i) > as(d(i)) ) in target lan-guage.?
rm3: if the position number of the ithword?s head is larger than i ( d(i) > i ) insource language, while the position num-ber of the word aligned to i is larger thanas(d(i)) (as(i) > as(d(i))) in target language.?
rm4: if the position number of the ithword?s head is larger than i ( d(i) > i) insource language, while the position num-ber of the word aligned to i is less thanas(d(i)) (as(i) < as(d(i)) ) in target lan-guage.Fig.
3.
An example of the reordering types inFig.
2.Fig.
3 shows examples of all the reorderingtypes.
In Fig.
3, the reordering type is labeled atthe target word aligned to the modifier: for ex-ample, the reordering type of rm1 belongs to thetarget word ?scale?.
Please note that, in general,these four types of reordering can be dividedinto 2 categories: the target words order of rm2and rm4 is identical with source word order,while rm1 and rm3 is the swapped order ofsource.
In practice, there are some special casesthat can?t be classified into any of the definedreordering types: the head and modifier insource link to the same word in target.
In suchcases, rather than define new reordering types,we classify these special cases into these fourdefined reordering types: if the head is right tothe modifier in source, we classify the reorder-ing type into rm2; otherwise, we classify thereordering type into rm4.Probability estimation: we adopt maximumlikelihood (ML) based estimation in this paper.In ML estimation, in order to avoid the datasparse problem brought by lexicalization, wediscard the lexical information in source andtarget language:??
?m1iTi (i)))r(a-,-, |P(oA) D, T, S, |P(O(2)where oi?
{rm1,rm2,rm3,rm4} is the reorder-ing type of ith word in  target language.To get a non-zero probability, additive smoothing( Chen and Goodman, 1998) is used:???????????????
?||),,,,,(),,,,,,(||)))((()))((,() )))(((-,-,|P(o)()()()()()()()()()()()(iOspldhmFspldhmoFOiarFiaroFiarFiaiaiaiaiaiaRoiaiaiaiaiaiaitRoTitTTTTTTiTTTTTTi(3)where F(. )
is the frequency of the statistic eventin training corpus.
For a given set of dependen-cy relationships mapping from constituent tree,the reordering type of ith word is confined totwo types: it is whether one of rm1 and rm2 orrm3 and rm4.
Therefore, |O|=2 instead of |O|=4in (2).
The parameter ?
is an additive factor toprevent zero probability.
It is computed as:),,,,,(1)()()()()()( iaiaiaiaiaiaRoTTTTTTispldhmFC ?????
(4)where c is a constant parameter(c=5 in this pa-per).In above, the additive parameter ?
is an adap-tive parameter decreasing with the size of thestatistic space.
By doing this, the data sparseproblem can be relieved.4 Apply the Model to DecoderOur decoding algorithm is exactly the same as(Kohn, 2004).
In the translation procedure, thedecoder keeps on extending new phrases with-out overlapping, until all source words are trans-lated.
In the procedure, the order of the target751words in decoding procedure is fixed.
That is,once a hypothesis is generated, the order of tar-get words cannot be changed in the future.
Tak-ing advantage of this feature, instead of compu-ting a totally new reordering score for a newlygenerated hypothesis, we merely calculate thereordering score of newly extended part of thehypothesis in decoding.
Thus, in decoding, tocompute the reordering score, the reorderingtypes of each target word in the newly extendedphrase need to be identified.The method to identify the reordering typesin decoding is proposed in Fig.4.
According tothe definition of reordering, the reordering typeof the target word is identified by the directionof head-modifier dependency on the source side,the alignment between the source side and tar-get side, and the relative translated order ofword pair under the head-modifier relationship.The direction of dependency and the alignmentcan be obtained in input sentence and phrasetable.
While the relative translation order needsto record during decoding.
A word index is em-ployed to record the order.
The index is con-structed in the form of true/false array: the indexof the source word is set with true when theword has been translated.
With the help of thisindex, reordering type of every word in thephrase can be identified.1: Input: alignment array AT; the Start is thestart position of the phrase in the source side;head-modifier relation d(.
); source word in-dex C, where C[i]=true  indicates that theith word in source has been translated.2: Output: reordering type array O which re-serves the reordering types of each word inthe target phrase3: for i = 1, |AT| do4:    P  ?
aT(i) + Start5:    if (d (P)<P) then6:      if C [d(p)] = false then7:         O[i] ?
rm18:      else9:         O[i] ?
rm210:        end if11:  else12:     if  C[d(p)] = true then13:        O[i] ?
rm314:       else15:          O[i] ?
rm416:       end if17:    end if18: C[p] ?true //update word index19: end forFig.
4.
Identify the Reordering Types of  NewlyExtended PhraseAfter all the reordering types in the newly ex-tended phrase are identified, the reorderingscores of the phrase can be computed by usingequation (3).5 Preprocess the AlignmentIn Fig.
4, the word index is to identify the reor-dering type of the target translated words.
Ac-tually, in order to use the word index withoutambiguity, the alignment in the proposed algo-rithm needs to satisfy some constraints.Firstly, every word in the source must havealignment word in the target side.
Because, inthe decoding procedure, if the head word is notcovered by the word index, the algorithm cannotdistinguish between the head word will not betranslated in the future and the head word is nottranslated yet.
Furthermore, in decoding, asshown in Fig.4, the index of source would be setwith true only when there is word in targetlinked to it.
Thus, the index of the source wordwithout alignment in target is never set with true.Fig.
5.
A complicated Example of Alignment inHead-modifier based Reordering ModelSecondly, if the head word has more than onealignment words in target, different alignmentpossibly result in different reordering type.
Forexample, in Fig.
5, the reordering type of e2 isdifferent when f2 select to link word e1 and e3in the source side.To solve this problem, we modify the align-ment to satisfy following conditions: a) eachword in source just has only one alignmentword in target, and b) each word in target has atmost one word aligned in source as its anchorword which decides the reordering type of thetarget word.To make the alignment satisfy above con-straints, we modify the alignment in corpus.
In752order to explain the alignment preprocessing,the following notions are defined: if there is alink between the source word f j  and target wordei, let  l(ei ,fj) = 1 , otherwise l(ei ,fj) = 0; thesource word fj?F1-to-N , iff  ?i l(ei,fj) >1, suchas the source word f2 in Fig.
5; the source wordfj?FNULL, iff ?i l(ei,fj) = 0, such as the sourceword f4 in Fig.
5; the target word ei?E1-to-N  , iff?j l(ei,fj) > 1, such as the target word e1 in Fig.5.In preprocessing, there are 3 types of opera-tion, including DiscardLink(fj) , BorrowLink( f j )and FindAnchor(ei ) :DiscardLink( fj ) : if the word fj in source withmore than one words aligned in target, i.e.
fj?F1-to-N ; We set the target word en with l(en, fj) =1, where en= argmaxi p(ei | fj) and   p(ei | fj) isestimated by ( Koehn e.t.
al, 2003), while setrest of words linked to fj with l (en, fj) = 0.BorrowLink( fj ): if the word fj in source with-out a alignment word in target, i.e.
fj?FNULL ;let l(ei,fj)=1 where ei  aligned to the word fj ,which is the nearest word to  fj  in the sourceside; when there are two words nearest to fj withalignment words in the target side at the sametime, we select the alignment of  the left wordfirstly .FindAnchor( ): for the word ei  in target withmore than one words aligned in source , i.e.
ei?E1-to-N ; we select the word  fm  aligned to ei asits anchor word to decide the reordering type ofei  ,  where fm= argmaxj p(ei | fj) and  p(fj | ei) isestimated by ( Koehn et al 2003); For the restof words aligned to  ei , we would set their wordindexes with true in the update procedure ofdecoding  in the 18th line of Fig.4.With these operations, the required alignmentcan be obtained by preprocessing the originalignment as shown in Fig.
6.1: Input: set of alignment A between target lan-guage e and source language f2: Output: the 1-to-1 alignment required by themodel3:  foreach fi?F1-to-N do4:    DiscardLink( fi )5:  end for6:  foreach fi  ?FNULL  do7:    BorrowLink( fi )8:  end for9:  foreach  ei?E1-to-N do10:   FindAnchor(ei )11:endforFig.
6.
Alignment Pre-Processing algorithmFig.
7.
An Example of Alignment Preprocessing.An example of  the preprocess the alignmentin Fig.
5 is shown in Fig.
7 : firstly, Discar-dLink(f2) operation discards the link between f2and e1  in (a); then the link between f4 and e3 isestablished by operation BorrowLink(f4 )  in (b);at last, FindAnchor(e3) select f2 as the anchorword of e3  in source in (c).
After the prepro-cessing, the reordering type of e3   can be identi-fied.
Furthermore, in decoding, when the de-coder scans over e2, the word index sets theword index of f3 and f4 with true.
In this way,the never-true word indexes in decoding areavoided.6 Training the Reordering ModelBefore training, we get the required alignmentby alignment preprocessing as indicated above.Then we train the reordering model with thisalignment: from the first word to the last wordin the target side, the reordering type of eachword is identified.
In this procedure, we skip thewords without alignment in source.
Finally, allthe statistic events required in equation (3) areadded to the model.In our model, there are 20,338 kinds of rela-tions with reordering probabilities which aremuch smaller than most phrase level reorderingmodels on the training corpus FBIS.Table 1 is the distribution of different reor-dering types in training model.753Type of Reordering   Percentage   %rm1rm2rm33.6927.6120.94rm4 47.75Table 1: Percentage of different reorderingtypes in modelFrom Table 1, we can conclude that the reor-dering type rm2 and rm4 are preferable in reor-dering which take over nearly 3/4 of total num-ber of reordering type and are identical withword order of the source.
The statistic data indi-cate that most of the words order doesn?t changein our head-modifier reordering view.
Thismaybe can explain why the models (Wu, 1997;Xiong, 2006; Koehn, et., 2003) with limitedcapacity of reordering can reach certain perfor-mance.7 Experiment and Discussion7.1 Experiment SettingsWe perform Chinese-to-English translation taskon NIST MT-05 test set, and use NIST MT-02as our tuning set.
FBIS corpus is selected as ourtraining corpus, which contains 7.06M Chinesewords and 9.15M English words.
We use GI-ZA++(Och and Ney, 2000) to make the corpusaligned.
A 4-gram language model is trainedusing Xinhua portion of the English Gigawordcorpus (181M words).
All models are tuned onBLEU, and evaluated on both BLEU and NISTscore.To map from the constituent trees to sets ofhead-modifier relationships, firstly we use theStanford parser (Klein, 2003) to parse thesource of corpus FBIS, then we use the head-finding rules in (Bikel, 2004) to get the head-modifier dependency sets.In our system, there are 7 groups of features.They are:1.
Language model score (1 feature)2. word penalty score (1 feature)3. phrase model scores (5 features)4. distortion score (1 feature)5. lexical RM scores (6 features)6.
Number of each reordering type (4 fea-tures)7.
Scores of each reordering type (4 fea-tures, computed by equation (3))In these feature groups, the top 5 groups offeatures are the baseline model, the left twogroup scores are related with our model.In decoding, we drop all the OOV words anduse default setting in Moses: set the distortionlimitation with 6, beam-width with 1/100000,stack size with 200 and max number of phrasesfor each span with 50.7.2 Results and DiscussionWe take the replicated Moses system as ourbaseline.
Table 2 shows the results of our model.In the table, Baseline model is the model includ-ing feature group 1, 2, 3 and 4.
Baselinerm mod-el is the Baseline model with feature group 5.
H-M model is the Baseline model with featuregroup 6 and 7.
H-Mrm model is the Baselinermmodel with feature group 6 and 7.Model BLEU% NISTBaseline 27.06 7.7898Baselinerm  27.58     7.8477H-M  28.47     8.1491H-Mrm 29.06 8.0875Table 2: Performance of  the Systems on NIST-05(bleu4 case-insensitive).From table 2, we can conclude that our reor-dering model is very effective.
After addingfeature group 6 and 7, the performance is im-proved by 1.41% and 1.48% in bleu score sepa-rately.
Our reordering model is more effectivethan the lexical reordering model in Moses:1.41% in bleu score is improved by adding ourreordering model to Baseline model, while 0.48is improved by adding the lexical reordering toBaseline model.threshold KOR BLEU NIST?1 20,338  29.06  8.0875?2      13,447 28.83   8.3658?3      10,885 28.64 8.0350?4        9,518 28.94 8.1002?5        8,577       29.18   8.1213Table 3: Performance on NIST-05 with Differ-ent Relation Frequency Threshold (bleu4 case-insensitive).Although our model is lexical free, the datasparse problem affects the performance of themodel.
In the reordering model, nearly halfnumbers of the relations in our model occur lessthan three times.
To investigate this, we statistic754the frequency of the relationships in our model,and expertise our H-M full model with differentfrequency threshold.In Table 3, when the frequency of relation isnot less than the threshold, the relation is addedinto the reordering model; KOR is the numberof relation type in the reordering model.Table 3 shows that, in our model, many rela-tions occur only once.
However, these low-frequency relations can improve the perfor-mance of the model according to the experimen-tal results.
Although low frequency statisticevents always do harm to the parameter estima-tion in ML, the model can estimate more eventsin the test corpus with the help of low frequencyevent.
These two factors affect the experimentresults on opposite directions: we consider thatis the reason the result don?t increase or de-crease with the increasing of frequency thre-shold in the model.
According to the results, themodel without frequency threshold achieves thehighest bleu score.
Then, the performance dropsquickly, when the frequency threshold is setwith 2.
It is because there are many events can?tbe estimated by the smaller model.
Although, inthe model without frequency threshold, thereare some probabilities overestimated by theseevents which occur only once, the size of themodel affects the performance to a larger extent.When the frequency threshold increases above 3,the size of model reduces slowly which makesthe overestimating problem become the impor-tant factor affecting performance.
From theseresults, we can see the potential ability of ourmodel: if our model suffer less from data sparsproblem, the performance should be further im-proved, which is to be verified in the future.8 Related Work and MotivationThere are several researches on adding linguis-tic analysis to MT in a ?soft constraint?
way.Most of them are based on constituents in parsetree.
Chiang(2005), Marton and Resnik(2008)explored the constituent match/violation in hie-ro; Xiong (2009 a) added constituent parse treebased linguistic analysis into BTG model;Xiong (2009 b) added source dependency struc-ture to BTG; Zhang(2009) added tree-kernel toBTG model.
All these studies show promisingresults.
Making soft constrain is an easy andefficient way in adding linguistic analysis intoformal sense SMT model.In modeling the reordering, most of previousstudies are on phrase level.
In Moses, the lexicalreordering is modeled on adjacent phrases.
In(Wu, 1996; Xiong, 2006), the reordering is alsomodeled on adjacent translated phrases.
In hiero,the reordering is modeled on the segments ofthe unmotivated translation rules.
The tree-to-string models (Yamada et al 2001; Liu etal.2006) are model on phrases with syntax re-presentations.
All these studies show excellentperformance, while there are few studies onword level model in recent years.
It is because,we consider, the alignment in word level modelis complex which limits the reordering capacityof word level models.However, our work exploits a new directionin reordering that, by utilizing the decomposeddependency relations mapped from parse tree asa soft constraint, we proposed a novel head-modifier relation based word level reorderingmodel.
The word level reordering model isbased on a phrase based SMT framework.
Thus,the task to find the proper position of translatedwords converts to score the reordering of thetranslated words, which relax the tension be-tween complex alignment and word level reor-dering in MT.9 Conclusion and Future WorkExperimental results show our head-modifierrelationship base model is effective to the base-line (enhance by 1.48% bleu score), even withlimited size of model and simple parameter es-timation.
In the future, we will try more compli-cated smooth methods or use maximum entropybased reordering model.
We will study the per-formance with larger distortion constraint, suchas the performances of   the distortion constraintover 15, or even the performance without distor-tion model.10 AcknowledgementThe work of this paper is funded by NationalNatural Science Foundation of China (grant no.60736014), National High Technology Re-search and Development Program of China (863Program) (grant no.
2006AA010108), and Mi-crosoft Research Asia IFP (grant no.
FY09-RES-THEME-158).755ReferencesDekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpo-ra.
Computational Lingustics,23(3):377-403.David Chiang.
2005.
A hierarchical phrase-basedmodel for SMT.
ACL-05.263-270.David Chiang.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2):201-228.Kenji Yamada and K. Knight.
2001.
A syntax-basedstatistical translation model.
ACL-01.523-530.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic Constraints for Hierarchical Phrased-basedTranslation.
ACL-08.
1003-1011.Libin  shen, Jinxi Xu and Ralph Weischedel.
2008.
ANew String-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
ACL-08.
577-585.J.
Graehl and K. Knight.2004.Train ing Tree trans-ducers.
In proceedings of the 2004 Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association for Computation-al Linguistics.Dekai Wu.
1996.
A Polynomial-Time Algorithm forStatistical Machine Translation.
In proceedings ofACL-1996Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Ma x-imum Entropy Based Phrase Reordering Modelfor Statistical Machine Translation.
In  proceed-ings of COLING-ACL 2006Deyi Xiong, Min Zhang, Aiti AW and Haizhou Li.2009a.
A Syntax-Driven Bracket Model forPhrase-Based Translation.
ACL-09.315-323.Deyi Xiong, Min Zhang, Aiti AW and Haizhou Li.2009b.
A Source Dependency Model for StatisticMachine translation.
MT-Summit 2009.Och, F.J. and Ney, H. 2000.
Improved statisticalalignment models.
In Proceedings of ACL 38.Philipp Koehn, et al Moses: Open Source Toolkitfor Statistical Machine Translation, ACL 2007.Philipp Koehn, Franz Joseph Och, and Daniel Mar-cu.2003.
Statistical Phrase-based Translation.
InProceedings of HLT-NAACL.Philipp Koehn.
2004.
A Beam Search Decoder forPhrase-Based Translation model.
In : Proceedingof AMTA-2004,WashingtonRens Bod.
1992.
Data oriented Parsing( DOP ).
InProceedings of COLING-92.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics,24:613-632.Liang Huang, Kevin Knight, and Aravind Joshi.
Sta-tistical Syntax-Directed Translation with Ex-tended Domain of Locality.
2006.
In Proceedingsof the 7th AMTA.Yang Liu, Qun Liu, and Shouxun Lin.
Tree-to-StringAlignment Template for Statistical MachineTranslation.
2006.In Proceedings of the ACL 2006.Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li,Chew Lim Tan and Sheng Li.
2008.
A Tree Se-quence Alignment-based Tree-to-Tree TranslationModel.
ACL-HLT-08.
559-567.Dan Klein, Christopher D. Manning.
Accurate Un-lexicalized Parsing.
2003.
In Proceedings ofACL-03.
423-430.M.
Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proceedings ofACL-96.
184-191.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Univ.
ofPennsylvania.Andreas Zollmann.
2005.
A Consistent and EfficientEstimator for the Data-Oriented Parsing Model.Journal of Automata, Languages  and Combinator-ics.
2005(10):367-388Mark Johnson.
2002.
The DOP estimation method isbiased and inconsistent.
Computational Linguis-tics 28, 71-76.Daniel M. Bikel.
2004.
On the Parameter Space ofGenerative Lexicalized Statistical Parsing Models.Ph.D.
thesis.
Univ.
of Pennsylvania.S.
F. Chen, J. Goodman.
An Empirical Study ofSmoothing Techniques for Language Modeling.In Proceedings of the 34th annual meeting on As-sociation for Computational Linguistics,1996.310-318.Haitao Mi and Liang Huang.
2008.
Forest-basedtranslation Rule Extract ion.
ENMLP-08.
2006-214.Hui Zhang, Min Zhang , Haizhou Li, A iti Aw andChew Lim Tan.
Forest-based Tree Sequence toString Translation Model.
ACL-09: 172-180S DeNeefe, K. Knight, W. Wang, and D. Marcu.2007.
What can syntax-based MT learn fromphrase-based MT ?
In Proc.
EMNLP-CoNULL.Brooke Cowan, Ivona Kucerova, and MichaelCollins.2006.
A discriminative model for tree-to-tree translation.
In Proc.
EMNLP.756
