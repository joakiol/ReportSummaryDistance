Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 514?523,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEntity Retrieval via Entity Factoid Hierarchy?Chunliang Lu, Wai Lam, Yi LiaoKey Laboratory of High Confidence Software TechnologiesMinistry of Education (CUHK Sub-Lab)Department of Systems Engineering and Engineering ManagementThe Chinese University of Hong Kong{cllu,wlam,yliao}@se.cuhk.edu.hkAbstractWe propose that entity queries are gener-ated via a two-step process: users first se-lect entity facts that can distinguish tar-get entities from the others; and thenchoose words to describe each selectedfact.
Based on this query generationparadigm, we propose a new entity rep-resentation model named as entity fac-toid hierarchy.
An entity factoid hierar-chy is a tree structure composed of fac-toid nodes.
A factoid node describes oneor more facts about the entity in differentinformation granularities.
The entity fac-toid hierarchy is constructed via a factorgraph model, and the inference on the fac-tor graph is achieved by a modified variantof Multiple-try Metropolis algorithm.
En-tity retrieval is performed by decompos-ing entity queries and computing the querylikelihood on the entity factoid hierarchy.Using an array of benchmark datasets, wedemonstrate that our proposed frameworksignificantly improves the retrieval perfor-mance over existing models.1 IntroductionEntity retrieval, which aims at returning specificentities to directly answer a user?s query, hasdrawn much attention these years.
Various entityretrieval tasks have been proposed, such as TRECEntity (Balog et al, 2012; Wang et al, 2011) andINEX-LD (Wang et al, 2012; Wang and Kang,2012).
Many existing entity retrieval models fol-low the document retrieval assumption: when is-suing queries, users choose the words that may?The work described in this paper is substantially sup-ported by grants from the Research Grant Council of theHong Kong Special Administrative Region, China (ProjectCodes: 413510 and 14203414) and the Direct Grant of theFaculty of Engineering, CUHK (Project Code: 4055034).appear in the ?entity pseudo-document?.
Basedon the assumption, these models construct in-ternal entity representations by combining vari-ous entity descriptions, and use these representa-tions to compute the rank of the candidate enti-ties for a given entity query.
These models in-clude fielded versions of BM25 and Mixture ofLanguage Models (Neumayer et al, 2012), EntityLanguage Model (Raghavan et al, 2004), Hierar-chical Expert Model (Petkova and Croft, 2006),Structured Positional Entity Language Model (Luet al, 2013).However, a closer examination of entity queriesreveals that most of them are not simple uniformword samples from the ?entity pseudo-document?.Instead, they can be decomposed into multipleparts, where each part describes a fact about targetentities.
For example, the query ?National capitalssituated on islands?
describes two facts regardinga target entity: it is a national capital; it is lo-cated on an island.
Compared to the assumptionin document retrieval models, where query termsare assumed to be generated from a single docu-ment, these query terms can be regarded to be in-dependently generated from two underlying docu-ments.
According to this observation, we proposethat an entity query is generated via a two-stepprocess: users first select facts that can distinguishtarget entities from the others; and then choosewords that describe the selected facts.
Based onthe proposed query generation paradigm, we de-sign a new entity retrieval framework.
On onehand, an entity is modeled to have multiple in-ternal representations, each regarding one or moreclosely related facts.
On the other hand, an entityquery is decomposed into one or more subqueries,each describing a fact about target entities.
In thisway, entity retrieval can be performed by combin-ing the probabilities of subqueries being satisfiedfor each candidate entity.One of the central components of our proposed514he was born in 1961 he was born in August 1961he was born in Honolulu Hawaiiborn?2, 1961?2, august?1born?3, 1961?2, august?1, hawaii?1, honolulu?1A BCDEborn in Honolulu Hawaiiborn in 1961 born in August 1961(a)born in Honolulu Hawaiiborn in 1961 born in August 1961born 1961august(b)born in Honolulu Hawaiiborn in 1961 born in August 1961born 1961august(c)Figure 1: An example of entity factoid hierarchy containing two factoids about Barack Obamaretrieval framework is a novel entity representa-tion known as entity factoid hierarchy.
An entityfactoid hierarchy is a tree structure composed offactoid nodes, which is automatically constructedfrom a collection of entity descriptions.
We abusethe term ?factoid?
to denote a single piece of infor-mation regarding an entity.
factoid node in thehierarchy describes one or more factoids.
Factoidnodes in different levels capture the information ofdifferent levels of detail (referred to as informationgranularities hereafter), where lower level nodescontain more detailed information and higher levelnodes abstract the details away.
The entity factoidhierarchy is constructed via a factor graph model,and the inference on the factor graph is achievedby a modified variant of Multiple-try Metropolisalgorithm.
Each factoid node is indexed sepa-rately as a pseudo-document.
During retrieval, thequery likelihood for a candidate entity are com-puted by transversing the factoid hierarchy.
Com-pared to exiting entity retrieval models, our pro-posed framework exhibits two advantages:?
By organizing entity descriptions in a hier-archical structure, detailed entity informationis preserved and we can return finer confi-dence value.
Suppose that the entity ?BarackObama?
is only described by one sentence:?born in 1961?.
Traditional entity models,which model an entity as a pseudo-document,would return high confidence value for thequery ?who is born in 1961?.
However, aswe add more and more sentences to describe?Barack Obama?, the confidence value re-turned for the query decreases due to thelonger entity pseudo-document.
This result isnot desirable for entity retrieval, since addingmore descriptions about other facts shouldnot affect the confidence of existing facts.Our factoid hierarchy avoids this problem bypreserving all the entity descriptions in a hi-erarchical structure.
When performing re-trieval, entity factoid hierarchy can be tra-versed to locate the best supporting descrip-tion for the query.?
By separating entity facts in different factoidn des, our model prevent ambiguity causedby mixing terms describing different facts.Suppose ?Barack Obama?
is described bytwo sentences: ?Barack Obama is a presi-dent of United States?
and ?Barack Obamais a graduate of Harvard Law School?, andour query is ?Who is a president of Har-vard Law School?.
A traditional docu-ment retrieval model with a bag-of-word en-tity pseudo-document would return ?BarackObama?
with high confidence, since all thequery terms appear in the entity descriptions.But obviously, this result is not correct.
Inour factoid hierarchy, these two facts are sep-arated in lower level factoid nodes.
Whilehigher level nodes are still mixed with termsfrom child nodes, they are penalized to avoidgiving high confidence value.2 Factoid Hierarchy2.1 Hierarchy RepresentationAs mentioned in the previous section, all the infor-mation regarding an entity is organized in a partic-ular factoid hierarchy.
We denote the term ?fac-toid?
as a single piece of information regarding anentity, such as the birth date of Barack Obama.A factoid node in the hierarchy describes one ormore factoids.
Each factoid node is associatedwith a bag-of-words vector to represent the fac-toid description.
Factoid nodes in different depthencode information in different granularities.An example of an entity factoid hierarchy, re-garding two factoids (birth date and birth place)about Barack Obama, is given in Figure 1.
The515example hierarchy is constructed from three sen-tences about Barack Obama: he was born in 1961;he was born in August 1961; he was born in Hon-olulu Hawaii.
These three sentences correspondto the leaf nodes A, B, and C respectively in Fig-ure 1.
In general, a leaf node in the factoid hi-erarchy comes directly from a sentence or a RDFtriple describing the entity.
Since it is extractedeither from human written texts or from manu-ally crafted structured databases, a leaf node repre-sents the most exact representation regarding oneor more factoids.
During the construction of thehierarchy, intermediate nodes are formed as par-ents for nodes that contain closely related factoids.The factoid description for an intermediate node isthe sum of bag-of-words vectors of its child nodes.In this way, intermediate nodes capture the wordsthat are used more frequently with higher weightsto describe the underlying factoids in a more gen-eral form.
As we merge more nodes and moveup in the hierarchy, intermediate nodes becomeblended with more different factoids.
Node D inFigure 1 is an intermediate factoid node, as a par-ent node for nodes A and B both describing thebirth date.
The root node in an entity factoid hi-erarchy summarizes all the descriptions regardingan entity, which is similar to the ?entity pseudo-document?
used in some existing entity retrievalmodels.
Each entity factoid hierarchy has only oneroot node.
For example, node E in Figure 1 is theroot node, and it contains words from all the threesentences.Note that the depth of a leaf node varies withthe number of descriptions associated with the fac-toids.
Some factoids may be associated with lotsof detailed information and are expressed in manysentences, while others are only expressed in oneor two sentences.
For example, the factoid thatObama is elected president in 2008 may be de-scribed in many sentences and in different con-texts; while the factoid that Obama is born inKapiolani Maternity & Gynecological Hospital isonly mentioned in a few sentences.
In this case,factoid nodes associated with more details mayhave deeper hierarchical structure.2.2 Factor Graph ModelTo construct the entity factoid hierarchy, we makeuse of a hierarchical discriminative factor graphmodel.
A similar factor graph model has been pro-posed to solve the coreference resolution in (Singhet al, 2011; Wick et al, 2012).
Here we design afactor graph model corresponding to the entity fac-toid hierarchy, together with new factor types andinference mechanism.Generally speaking, a factor graph is composedof two parts: a set of random variables and a set offactors that model the dependencies between ran-dom variables.
An example of the factor graphconstruction corresponding to the factoid hierar-chy involved in Figure 1 is given in Figure 2.
Inour factor graph approach, each factoid is repre-sented as a random variable fi, corresponding to arounded square node in Figure 2.
The pairwise bi-nary decision variable yij, denotes whether a fac-toid fiis a child of another factoid fjcorrespond-ing to a circle node in Figure 2.
The set of fac-toids F plus the set of decision variables y are therandom variables in our factor graph model.
Tomodel the dependency between factoids, we con-sider two types of factors.
?pis the set of factorsthat consider the compatibility between two fac-toid nodes, i.e., to indicate whether two nodes haveparent-child relationship.
?uis the set of factorsthat measure the compatibility of the factoid nodeitself.
Such factor is used to check whether a newintermediate node should be created.
Factors arerepresented as square nodes in Figure 2.
Given afactor graph model m, our target is to find the bestassignments for the decision variable y that maxi-mizes the objective function in Equation (1).P (y, F |m) =?f?F?p(f, fp)?u(f) (1)2.3 Factors DesignThe pairwise factors ?pand unit-wise factors ?ucompute the compatibility scores among factoidnodes.
Each factor type is associated with a weightw to indicate the importance of the factor duringinference.
For the notation, the bag-of-words rep-resentation for a factoid node is denoted as d. Weuse superscripts p and c to denote the variables ofparent nodes and child nodes.
To capture the in-terrelations between factoid nodes, the followingfactors are used in our factor graph model.Bag-of-words similarity To check whether twofactoid nodes refer to the same fact, we comparethe similarity between their bag-of-words descrip-tions.
We choose Kullback-Leibler divergence(KL divergence) as the similarity measure.
Bydefinition, the KL divergence of Q from P, de-noted DKL(P ||Q), is a measure of the informa-516born in 1961 born in August 1961born in Honolulu Hawaiiborn*2, 1961*2, august*1born*3, 1961*2, august*1, hawaii*1, honolulu*1A BCDEborn in Honolulu Hawaiiborn in 1961 born in August 1961(a)born in Honolulu Hawaiiborn in 1961 born in August 1961born 1961august(b)born in Honolulu Hawaiiborn in 1961 born in August 1961born 1961august(c)Figure 2: Generation of an factoid hierarchy via factor graph inference.
Factoid nodes are initialized assingletons in (a).
During one step of sampling in (b), two factoid nodes are selected and one proposal isto add a common parent.
If we accept the proposal, we end up with the factoid hierarchy in (c).tion lost when Q is used to approximate P. It isa non-symmetric measure and fits in our problemnicely, i.e., measuring whether a parent node is amore abstract representation of its child node.
Thecompatibility score is computed as:?
w1?DKL(dp||dq)=?
w1?m?i=1dpi?
log(dpidci), (2)where dpiis the smoothed term frequency of thefactoid description for the parent node; dciis forthe child node; w1is a global weighting parame-ter among different factors.
In fact, we have alsoexplored other popular text similarity metrics sum-marized in (Huang, 2008), and find that KL diver-gence performs the best.Entropy penalty We penalize the entropy of thefactoid description to encourage a smaller vocab-ulary of words describing the underlying factoids:?w2?H(d)log ||d||0, (3)where H(d) denotes the Shannon entropy forthe bag-of-words representation of the factoid de-scription d; ||d||0is the number of unique terms inthe factoid description.Structure penalty The depth of a factoid nodeindicates the level of information granularity.However, we also need to control the depth of thefactoid hierarchy.
A factoid node should not havetoo many levels.
We define the depth penalty as:?w3?
|nd?||d||0s|, (4)where ndis the depth of a factoid node and s isthe parameter that controls the average depth offactoid nodes per term.
In this way, we can con-trol the average depth of factoid nodes in the entityfactoid hierarchy.2.4 InferenceExact inference is impossible for our factor graphmodel due to the large state space.
Here we adopt amodified variant of Multiple-try Metropolis algo-rithm to conduct maximum probability estimationfor inference, following the work in (Wick et al,2013).
At each sampling step, multiple changesto the current setting are proposed.
The accep-tance probability for a given proposal is equal tothe likelihood ratio of the proposed hypothesis tothe current hypothesis.
In our case, we initializethe MCMC procedure to the singleton configura-tion, where each entity description, such as a sen-tence or a RDF triple, forms its own factoid hierar-chy initially.
At each sampling step, we randomlyselect two nodes and propose several alternativelocal modifications.
If fiand fjare not connected,i.e., sharing no common child nodes, the followingchanges are proposed:?
Add factoid fias the parent of fj, if fjhasno parent node;?
Remove fjfrom its current parent, if fjhas aparent;?
Create a new common parent for fiand fj, ifboth fiand fjhave no parent.Otherwise, if fiand fjare in the same cluster, thefollowing changes are proposed:?
Remove fjfrom its current parent;?
Move fj?s children to fj?s parent and deletefj, if fjis an intermediate node.A sampling step of the inference process is il-lustrated in Figure 2.
Initially, all the decision vari-ables y are set to zero.
That is, each factoid nodeis regarded as forming its own factoid hierarchy,as illustrated in Figure 2(a).
During the inference,local modifications are proposed to the current fac-tor graph hypothesis.
For example, in Figure 2(b),517the two factoid nodes at the bottom are selectedand proposed to add a new intermediate factoid astheir common parent.
If we accept the proposal,we get an intermediate factoid hierarchy as illus-trated in Figure 2(c).The sampling process is iterated until no pro-posal has been accepted in a certain number ofsuccessive steps, or a maximum number of stepshas been reached.
Each entity factoid hierarchy isinferred separately, allowing us to parallelize theinference across multiple machines.3 Entity Retrieval3.1 Retrieval ModelAfter we preprocess available information sourcesand construct the entity factoid hierarchy, we areready to answer entity queries.
Our retrievalmodel is based on the query likelihood model.
Us-ing Bayes?
rule, the probability that an entity e isa target entity for a query q can be written as:p(e|q) =p(q|e)p(e)p(q).
(5)The probability of the query p(q) is the same forall entities and can be ignored.
Furthermore, weassume that the prior probability of an entity beinga target entity is uniform.
Thus, p(e) can also beignored.
The task is to rank an entity e in responseto a query q by estimating the query generationprobability p(q|e).To compute p(q|e), recall that our two-stepquery generation process assumes that users gen-erate queries by first selecting facts and thenchoosing query words for each fact.
Based on thequery generation process, we first decompose thequery q into m subqueries qi(discussed in Sec-tion 3.2).
Then the probability p(q|e) can be com-puted as:p(q|e) =m?i=1p(qi|e) (6)=m?i=1n?k=1p(qi|fk)p(fk|e) (7)'m?i=1maxkp(qi|fk).
(8)Equation (6) decomposes the query into sub-queries, assuming that all the subqueries are inde-pendent.
Equation (7) iterates through all the fac-toid nodes fkin the factoid hierarchy of an entitye.
Equation (8) simplifies the computation by as-suming that the underlying factoid generating sub-query qiis the factoid fkwith the highest querygeneration probability.To compute p(qi|fk), the probability of the fac-toid fkgenerating the subquery qi, we use themultinomial unigram language model:p(qi|fk) = e(fk)?jp(tji|fk), (9)where tjiis the term j in the subquery qi.
e(fk)is the penalty term for factoids containing manychildren:e(fk) = w ?1c(fk), (10)where c(fk) is the number of child nodes forfk.
To understand why we add this penalty term,consider a query ?who is born in 2008?.
Sup-pose ?Barack Obama?
is described by two sen-tences: ?born in 1961?
and ?elected president in2008?.
When computing p(qi|fk) for the rootnode, although it contains both the terms ?born?and ?2008?, it should be penalized since the termscome from two different child nodes.3.2 Query analysisAs mentioned earlier, we decompose the originalquery q into multiple factoid subqueries qi.
Forlong queries issued in a verbose sentence, suchas ?which presidents were born in 1945?, depen-dency parsing is performed (Klein and Manning,2003) and the resulting dependency tree is used tosplit the original query.
For short queries issuedin keywords, such as ?vietnam war movies?, wedecompose it based on possible key concepts ex-pressed in the query.
Usually a short query onlycontains a single entity, which is used to segmentthe original query into subqueries.Furthermore, stop structures in verbose queriesis removed, following the method proposed in(Huston and Croft, 2010).
Here a stop structureis defined as a phrase which provides no informa-tion regarding the information needs, such as ?tellme the?.
We also inject target entity type informa-tion by replacing the leading ?who ?
as ?person?,and ?where?
as ?place?
for all the queries.3.3 Retrieval ProcessFor the purpose of retrieval, each node in theentity factoid hierarchy is regarded as a pseudo-document describing one or more factoids about518the entity, and is indexed as a bag-of-words doc-ument during the preprocessing.
The retrieval isperformed in a two-step process.
First, for each in-dividual subquery, we retrieve top 1000 candidateentities by performing retrieval on all root nodes.This gives us an initial pool of candidate entitiesby merging the returned entities for subqueries.After that, for each candidate entity, we traverseits factoid hierarchy and compute the query gen-eration probability p(q|e) using Equations (8) and(9).
Top ranked entities are returned as retrievalresults.4 Experiments4.1 DatasetWe perform entity retrieval experiments using theDBpedia-Entity dataset used in (Balog and Neu-mayer, 2013).
The dataset is a mixture of multipleentity retrieval datasets, covering entity queries ofvarious styles such as keyword queries like ?viet-nam war movies?
and verbose queries like ?Whatis the capital of Canada?.
Some query statistics areshown in Table 2.Query set #query avg(|q|) avg(#rel)INEX-XER 55 5.5 29.7TREC Entity 17 6.7 12.9SemSearch ES 130 2.7 8.6SemSearch LS 43 5.4 12.5QALD-2 140 7.9 41.2INEX-LD 100 4.8 36.8Total 485 5.3 26.7Table 2: DBpedia-Entity dataset statisticsThe data corpus we use are DBpedia 3.9 andthe corresponding English Wikipedia data dumpon April 4, 2013.
It should be noted that the origi-nal DBpedia-Entity benchmark only uses DBpediafor entity modeling (Balog and Neumayer, 2013).In our experiments, we also conducted another setof experiments which include full-text Wikipediaarticles as additional entity descriptions, to eval-uate the capacity of different models on handlingfree texts as information sources.4.2 Comparison models and variants of ourmodelFor comparison, we have implemented the follow-ing two existing models:?
BM25.
BM25 is a popular document re-trieval method and also used to perform en-tity retrieval (Balog and Neumayer, 2013).All the descriptions about an entity are ag-gregated into an entity pseudo-document.
Weuse k1= 1.2, b = 0.8 for the model parame-ter, similar to the original papers.?
MLM-tc.
The Mixture of Language Modelrepresents an entity as a document withmultiple fields, where each field is givena different weight for generating the queryterms.
MLM is often adopted to do entityretrieval (Neumayer et al, 2012).
Here weadopt the MLM-tc model used in (Balog andNeumayer, 2013), where two fields are con-sidered: title and content fields (described inSection 4.3).
The parameters used are 0.8 forthe title field and 0.2 for the content field.Note that both MLM-tc and BM25 are alsocompared in (Balog and Neumayer, 2013), andhave shown the best MAP performances among allthe compared models.For our models, the following two variants areimplemented and compared.?
Factoid Retrieval Model with Hierarchy(FRMwH).
Our full model uses entity fac-toid graph as entity representation.
Each fac-toid node is indexed as a bag-of-words docu-ment.
The retrieval model described in Sec-tion 3 is employed.?
Factoid Retrieval Model (FRM).
Thismodel does not use entity factoid hierarchyas entity representation.
Instead, K-Meansclustering algorithm is used to cluster the sen-tences into text clusters.
Each text cluster isthen indexed as a document.
Compared to theFRMwH model, an entity only has a flat clus-ter of factoid descriptions.
The same retrievalmodel is used.All the four models use the same query prepro-cessing techniques.4.3 SetupThe entity descriptions come from texts inWikipedia articles and structured informationfrom DBpedia.
For DBpedia information, we con-sider top 1000 most frequent predicates as fields.We convert RDF predicates to free text by break-ing the camelcase predicate name to terms, for ex-ample ?birthPlace?
is converted to ?birth place?.For Wikipedia texts, we first remove all markuptext such as images, categories.
Infoboxes are also519ModelINEX-XER TREC Entity SemSearch ES SemSearch LS QALD-2 INEX-LD TotalMAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10Experiments with only DBpedia informationBM25 .1890 .2706 .1257 .1571 .2732 .2426 .2050 .2286 .2211 .1976 .1104 .2158 .1806 .1901MLM-tc .1439 .2176 .1138 .1143 .2962 .2641 .1755 .1976 .1789 .1598 .1093 .2144 .1720 .1792FRM:::.2186:::.2186 .1548::::.1548 .2430 .2430 .2088 .2088:::.2462::::.2462 .1178 .1178:::.1854:::.1965FRMwH:::.2260:::.2260:::.1742:::.1742 .2270 .2270 .1642 .1642:::.2286::::.2286:::.1358 .1358:::.1905:::.2004Experiments with both DBpedia and Wikipedia informationBM25 .1313 .1887 .1374 .1667 .2916 .2526 .1867 .1833 .1552 .1253 .1698 .2680 .1848 .1821MLM-tc .0777 .0981 .0942 .0875 .2794 .2398 .1071 .1071 .1024 .0771 .1501 .2370 .1515 .1452FRM:::.1922:::.1922:::.1601::::.1601 .2279 .2279:::.1729::::.1729:::.1965:::.1965:::.1793::::.1793:::.1934:::.1998FRMwH:::.2634:::.2634:::.1770::::.1770 .2267 .2267:::.1910::::.1910:::.2491:::.2491 .1554 .1554:::.2092:::.2130Table 1: Retrieval performance for various modelsremoved since the information is already well cap-tured in DBpedia.
Each Wikipedia article is thensegmented to a list of sentences, which are consid-ered as factoid descriptions regarding the entity.For the BM25 model, all the descriptions aboutan entity are aggregated into an entity pseudo-document.
For the MLMtc model, the title fieldis constructed by combining DBpedia propertieswhose property names are ending with ?title?,?name?
or ?label?, such as ?fullName?
(Neumayeret al, 2012), and the content field is the sameas the entity pseudo-document used in the BM25model.The inference algorithm for the entity factoidhierarchy is implemented based on the factoriepackage (McCallum et al, 2009).
The parame-ters used in the inference are manually tuned ona small set of entities.
The retrieval algorithms,including BM25 and Language Modeling, are im-plemented based on Apache Lucene1.
For lan-guage models, Bayesian smoothing with Dirich-let priors is used, with parameter ?
= 2000.
ForFRM, to cluster the entity descriptions, we usethe K-Means clustering algorithm implemented inCarrot22.4.4 ResultsWe report two standard retrieval measures: meanaverage precision (MAP) and precision at 10(P@10).
Top 100 ranked entities are evaluated foreach query.
Two set of experiments are conducted:experiments with only DBpedia information; ex-periments with both DBpedia and Wikipedia in-formation.
The experiment result is shown in Ta-ble 1.
To conduct the statistical significance anal-ysis, we use two-tailed paired t-test at the 0.05level.
The symbols underline and:::::wave:::::::::underline1Apache Lucene: http://lucene.apache.org/2Carrot2: http://www.carrot2.org/are used to indicate significant improvement ofour model compared with the BM25 and MLM-tc models respectively.The first set of rows in Table 1 show the per-formance of four models using only DBpedia in-formation.
Both of our models have better overallperformance.
On datasets with verbose queries,such as INEX-XER and TREC Entity, both ourmodels outperform the baseline models.
One rea-son is that our retrieval model relies on the as-sumption that verbose queries can be decomposedinto multiple subqueries.
The second set of rowsshow the performance of four models using bothDBpedia and Wikipedia information.
After addingthe additional information from Wikipedia arti-cles, MLM-tc attains much worse performance,while BM25 performs roughly the same.
Onepossible reason is that Wikipedia articles con-tain much irrelevant information regarding enti-ties, and these two existing models cannot eas-ily make use of additional information.
In con-trast, with Wikipedia full-text available, both ofour proposed models achieve obviously better per-formances.Our full model, FRMwH, has shown consis-tently better overall performance compared withthe FRM model.
It demonstrates that it is worth-while to employ our proposed entity hierarchicalstructure for entity representation.4.5 AnalysisFor the retrieval performance, we also perform atopic-level analysis between our model FRMwHand the baseline model BM25, shown in Fig-ure 3.
The X-axis represents individual querytopics, ordered by average precision difference(shown on the Y-axis).
Positive Y value indi-cates that FRMwH performs better than the BM25model for the query.
From the figure, most of520?0.8?0.40.40.8(a)?0.8?0.40.40.8(b)?0.8?0.40.40.8(c)?0.8?0.40.40.8(d)?0.8?0.40.40.8(e)?0.8?0.40.40.8(f)Figure 3: Topic-level differences between FRMwH and BM25.
Positive values mean FRMwH is better.
(a) INEX-XER; (b) TREC Entity; (c) SemSearch ES; (d) SemSearch LS; (e) QALD-2; (f) INEX-LD.queries are affected by using FRMwH model.
Onthe datasets with verbose queries, such as INEX-XER and TREC Entity, we can see most of thequery are improved.
FRMwH performs slightlyworse for datasets like SemSearch ES which ismostly composed of keyword queries.
For thequeries that show little or no performance differ-ences, manual inspection shows that both modelsfail to find any relevant results, due to the lack ofsupporting descriptions in Wikipedia and DBpe-dia.5 Related WorkBesides the entity retrieval models reviewed inSection 1, there are models that do not maintainan explicit entity representation.
Instead, theycompute the entity relevance score based on theco-occurance between entities and query termsin the documents directly.
Most of these mod-els are originally proposed for expertise retrieval,where the appearance of a person name indicatesthe association with the expertise mentioned inthe same document.
Typical models include vot-ing model (Macdonald and Ounis, 2006), graphmodel (Serdyukov et al, 2008), etc.
However, itis not easy to generalize these models for open do-main entity retrieval.Entity models are also used in other fields be-sides entity retrieval.
For example, entity topicmodels are used to perform entity prediction, clas-sification of entity pairs, construction of entity-entity network (Newman et al, 2006), as well asentity linking (Han and Sun, 2012).
These modelsare not suitable for our retrieval framework.The decomposing of entity queries into fac-toid queries is related to query segmentation.Query segmentation has been used by search en-gines to support inverse lookup of words andphrases (Risvik et al, 2003; Bergsma and Wang,2007).
Our use of query decomposition is quitedifferent compared to query segmentation.
Be-sides query segmentation, query decompositionhas also been used to facilitate the acquisition andoptimization of high-order contextual term associ-ations (Song et al, 2012).Our work is also related to the information ex-traction and knowledge representation field sinceour framework involves extraction and aggrega-tion of knowledge from free texts.
However, mostexisting approaches takes two extreme ways: ei-ther extract relations based on pre-defined ontol-ogy, such as DBpedia (Lehmann et al, 2014); orcluster relation without referring to some ontol-ogy, such as OpenIE (Etzioni et al, 2011).
Thoughour main goal is not on constructing a completeknowledge base, we do leverage both existingknowledge bases as well as free text data.Semantic search also targets on returning an-swers directly (Pound et al, 2010; Blanco et al,2011; Tonon et al, 2012; Kahng and Lee, 2012).However, they are mainly based on structuredlinked data, as well as structured query languagelike SPARQL.
While this is an effective approachif we have a powerful thorough knowledge base,in practice many facts cannot be effectively repre-sented as linked data.
Only a small set of relations(thousands in DBpedia) have been defined in theontology, such as ?birthPlace?.
Furthermore, evenif we can define a formal representation of humanknowledge, retrieve them effectively is still a prob-lem due to the difficulty of transforming the hu-man query into a structured query on a knowledgebase.6 ConclusionsWe propose that an entity query is generated in atwo-step process: users first select the facts thatcan distinguish target entities from the others; thenchoose words to express those facts.
Followingthis motivation, we propose a retrieval frameworkby decomposing the original query into factoidqueries.
We also propose to construct an entity521factoid hierarchy as the entity model for the pur-pose of entity retrieval.
Our entity factoid hier-archy can integrate information of different gran-ularities from both free text and structured data.Extensive experiments demonstrate the effective-ness of our framework.ReferencesKrisztian Balog and Robert Neumayer.
2013.
A testcollection for entity search in DBpedia.
In Proceed-ings of the 36th International ACM SIGIR confer-ence on Research and Development in InformationRetrieval, pages 737?740.K.
Balog, P. Serdyukov, and A. P. de Vries.
2012.Overview of the TREC 2011 entity track.
In Pro-ceedings of the Twentieth Text REtrieval Conference.Shane Bergsma and Qin Iris Wang.
2007.
Learningnoun phrase query segmentation.
In Proc.
EMNLP-CoNLL, pages 819?826.Roi Blanco, Harry Halpin, Daniel M. Herzig, Pe-ter Mika, Jeffrey Pound, and Henry S. Thompson.2011.
Entity search evaluation over structured webdata.
In Proceedings of the 1st International Work-shop on Entity-Oriented Search, EOS ?11.Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam Mausam.
2011.Open information extraction: The second genera-tion.
In Proceedings of the Twenty-Second Inter-national Joint Conference on Artificial Intelligence,pages 3?10.Xianpei Han and Le Sun.
2012.
An entity-topic modelfor entity linking.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, EMNLP-CoNLL ?12, pages 105?115.Anna Huang.
2008.
Similarity measures for text doc-ument clustering.
In Proceedings of the Sixth NewZealand Computer Science Research Student Con-ference, pages 49?56.Samuel Huston and W. Bruce Croft.
2010.
Evaluatingverbose query processing techniques.
In Proceed-ings of the 33rd International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 291?298.Minsuk Kahng and Sang-goo Lee.
2012.
Exploitingpaths for entity search in rdf graphs.
In Proceedingsof the 35th international ACM SIGIR conference onResearch and development in information retrieval,SIGIR ?12, pages 1027?1028.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430.Jens Lehmann, Robert Isele, Max Jakob, AnjaJentzsch, Dimitris Kontokostas, Pablo N. Mendes,Sebastian Hellmann, Mohamed Morsey, Patrick vanKleef, S?oren Auer, and Christian Bizer.
2014.
DB-pedia - a large-scale, multilingual knowledge baseextracted from wikipedia.
Semantic Web Journal,6(2):167?195.Chunliang Lu, Lidong Bing, and Wai Lam.
2013.Structured positional entity language model for en-terprise entity retrieval.
In Proceedings of the 22NdACM International Conference on Conference on In-formation & Knowledge Management, CIKM ?13,pages 129?138.Craig Macdonald and Iadh Ounis.
2006.
Votingfor candidates: adapting data fusion techniques foran expert search task.
In Proceedings of the 15thACM International Conference on Information andKnowledge Management, pages 387?396.Andrew McCallum, Karl Schultz, and Sameer Singh.2009.
FACTORIE: Probabilistic programming viaimperatively defined factor graphs.
In Neural In-formation Processing Systems (NIPS), pages 1249?1257.Robert Neumayer, Krisztian Balog, and Kjetil Nrvg.2012.
When simple is (more than) good enough:Effective semantic search with (almost) no seman-tics.
In Advances in Information Retrieval, pages540?543.David Newman, Chaitanya Chemudugunta, andPadhraic Smyth.
2006.
Statistical entity-topic mod-els.
In Proceedings of the 12th ACM SIGKDD Inter-national Conference on Knowledge Discovery andData Mining, pages 680?686.D.
Petkova and W.B.
Croft.
2006.
Hierarchical lan-guage models for expert finding in enterprise cor-pora.
In Tools with Artificial Intelligence, 2006.ICTAI ?06.
18th IEEE International Conference on,pages 599?608, Nov.Jeffrey Pound, Peter Mika, and Hugo Zaragoza.
2010.Ad-hoc object retrieval in the web of data.
In Pro-ceedings of the 19th international conference onWorld wide web, WWW ?10, pages 771?780.Hema Raghavan, James Allan, and Andrew Mccallum.2004.
An exploration of entity models, collectiveclassification and relation description.
In Proceed-ings of KDD Workshop on Link Analysis and GroupDetection, pages 1?10.K.
M. Risvik, T. Mikolajewski, and P. Boros.
2003.Query segmentation for web search.
In Proceedingsof the Twelfth International World Wide Web Con-ference (Poster session).522Pavel Serdyukov, Henning Rode, and Djoerd Hiemstra.2008.
Modeling multi-step relevance propagationfor expert finding.
In Proceeding of the 17th ACMConference on Information and Knowledge Mining,pages 1133?1142.Sameer Singh, Amarnag Subramanya, FernandoPereira, and Andrew McCallum.
2011.
Large-scalecross-document coreference using distributed infer-ence and hierarchical models.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 793?803.Dawei Song, Qiang Huang, Peter Bruza, and Ray-mond Lau.
2012.
An aspect query language modelbased on query decomposition and high-order con-textual term associations.
Comput.
Intell., 28(1):1?23, February.Alberto Tonon, Gianluca Demartini, and PhilippeCudr?e-Mauroux.
2012.
Combining inverted indicesand structured search for ad-hoc object retrieval.
InProceedings of the 35th international ACM SIGIRconference on Research and development in infor-mation retrieval, SIGIR ?12, pages 125?134.Qiuyue Wang and Jinglin Kang.
2012.
Inte-grated retrieval over structured and unstructureddata.
In Pamela Forner, Jussi Karlgren, and ChristaWomser-Hacker, editors, CLEF (Online WorkingNotes/Labs/Workshop), pages 42?44.Zhanyi Wang, Wenlong Lv, Heng Li, Wenyuan Zhou,Li Zhang, Xiao Mo, Liaoming Zhou, Weiran Xu,Guang Chen, and Jun Guo.
2011.
PRIS at TREC2011 entity track: Related entity finding and entitylist completion.
In TREC.Qiuyue Wang, Jaap Kamps, Georgina Ram?
?rez Camps,Maarten Marx, Anne Schuth, Martin Theobald,Sairam Gurajada, and Arunav Mishra.
2012.Overview of the inex 2012 linked data track.
InCLEF (Online Working Notes/Labs/Workshop).Michael Wick, Sameer Singh, and Andrew McCallum.2012.
A discriminative hierarchical model for fastcoreference at large scale.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics, pages 379?388.Michael Wick, Sameer Singh, Harshal Pandya, and An-drew McCallum.
2013.
A joint model for discover-ing and linking entities.
In CIKM 2013 Workshopon Automated Knowledge Base Construction, pages67?72.523
