Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1130?1139,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUnsupervised Entity Linking with Abstract Meaning RepresentationXiaoman Pan1, Taylor Cassidy2, Ulf Hermjakob3, Heng Ji1, Kevin Knight31Computer Science Department, Rensselaer Polytechnic Institute{panx2,jih}@rpi.edu2IBM Research & Army Research Laboratorytaylor.cassidy.civ@mail.mil3Information Sciences Institute, University of Southern California{ulf,knight}@isi.eduAbstractMost successful Entity Linking (EL) meth-ods aim to link mentions to their referent en-tities in a structured Knowledge Base (KB)by comparing their respective contexts, of-ten using similarity measures.
While the KBstructure is given, current methods have suf-fered from impoverished information repre-sentations on the mention side.
In this paper,we demonstrate the effectiveness of AbstractMeaning Representation (AMR) (Banarescuet al, 2013) to select high quality sets of en-tity ?collaborators?
to feed a simple similar-ity measure (Jaccard) to link entity mentions.Experimental results show that AMR capturescontextual properties discriminative enough tomake linking decisions, without the need forEL training data, and that system with AMRparsing output outperforms hand labeled tradi-tional semantic roles as context representationfor EL.
Finally, we show promising prelimi-nary results for using AMR to select sets of?coherent?
entity mentions for collective en-tity linking1.1 IntroductionThe Entity Linking (EL) task (Ji et al, 2010; Jiet al, 2011; Ji et al, 2014) aims at automati-cally linking each named entity mention appearingin a source text document to its unique referent ina target knowledge base (KB).
For example, con-sider the following sentence posted to a discussionforum during the 2012 U.S. presidential election:1The web service of this EL system is at:blender02.cs.rpi.edu:3300 and some related AMR toolsare at: github.com/panx27/amr-reader?Where would McCain be without Sarah??.
AnEntity Linker should link the entity mentions ?Mc-Cain?
and ?Sarah?
to the entities John McCainand Sarah Palin, respectively, which serve asunique identifiers for the real people.A typical EL system works as follows.
Given amention m (a string in a source document), the topN most likely entity referents from the KB are enu-merated based on prior knowledge about which en-tities are most likely referred to using m. The can-didate entities are re-ranked to ultimately link eachmention to the top entity in its candidate list.
Re-ranking consists of two key elements: context rep-resentation and context comparison.
For a givenmention, candidate entities are re-ranked based ona comparison of information obtained from the con-text of m with known structured and/or unstructuredinformation associated with the top N KB entities,which can be considered the ?context?
of the KB en-tity2.
The basic intuition is that the entity referentsof m and related mentions should be similarly con-nected in the KB.However, there might be many entity mentions inthe context of a target entity mention that could po-tentially be leveraged for disambiguation.
In this pa-per, we show that a deeper semantic knowledge rep-resentation - including the Abstract Meaning Rep-resentation (AMR) (Banarescu et al, 2013) - cancapture contextual properties that are discriminativeenough to disambiguate entity mentions that currentstate-of-the-art systems cannot handle, without theneed for EL training data.
Specifically, for a given2Most work uses Wikipedia and related resources to de-rive the KB, prior link likelihood, and entity information (e.g.,Wikipedia article text and infoboxes).1130entity mention, using AMR provides a rich contextrepresentation, facilitating the selection of an opti-mal set of collaborator entity mentions, i.e., thoseco-occurring mentions most useful for disambigua-tion.
In previous approaches, collaborator sets havetended to be too narrow or too broad, introducingnoise.
We then use unsupervised graph inferencefor context comparison, achieving results compa-rable with state-of-the-art supervised methods andsubstantially outperforming context representationbased on traditional Semantic Role Labeling.In addition, most state-of-the-art EL approachesnow rely on collective inference, where a set of co-herent mentions are linked simultaneously by choos-ing an ?optimal?
or maximally ?coherent?
set ofnamed entity targets - one target entity for each men-tion in the coherent set.
We show preliminary resultssuggesting that AMR is effective for the partitioningof all mentions in a document into coherent sets forcollective linking.We evaluate our approach using both human andautomatic AMR annotation, limiting target namedentity types to person (PER), organization (ORG),and geo-political entities (GPE)3.2 Related WorkIn most recent collective inference methods for EL(e.g., (Kulkarni et al, 2009; Pennacchiotti and Pan-tel, 2009; Fernandez et al, 2010; Radford et al,2010; Cucerzan, 2011; Guo et al, 2011; Han andSun, 2011; Ratinov et al, 2011; Chen and Ji, 2011;Kozareva et al, 2011; Dalton and Dietz, 2013)),the target entity mention?s ?collaborators?
may sim-ply include all mentions which co-occur in the samediscourse (sentence, paragraph or document) (Rati-nov et al, 2011; Nguyen et al, 2012).
But this ap-proach usually introduces many irrelevant mentions,and it?s very difficult to automatically determine thescope of discourse.
In contrast, some recent workexploited more restricted measures by only choos-ing those mentions which are topically related (Cas-sidy et al, 2012; Xu et al, 2012), bear a relationfrom a fixed set (Cheng and Roth, 2013), coreferen-tial (Nguyen et al, 2012; Huang et al, 2014), so-cially related (Cassidy et al, 2012; Huang et al,3The mapping from AMR entity types to these three maintypes is at: amr.isi.edu/lib/ne-type-sc.txt2014), dependent (Ling et al, 2014), or a combi-nation of these through meta-paths (Huang et al,2014).
These measures can collect more precisecollaborators but suffer from low coverage of pre-defined information templates and the unsatisfyingquality of state-of-the-art coreference resolution, re-lation and event extraction.In this paper, we demonstrate that AMR is an ap-propriate and elegant way to acquire, select, repre-sent and organize deeper knowledge in text.
To-gether with our novel utilization of the rich struc-tures in merged KBs, the whole framework carriesrich enough evidence for effective EL, without theneed for any labeled data, collective inference, orsophisticated similarity.3 Knowledge Network Construction fromSourceAbstract Meaning Representation (AMR) (Ba-narescu et al, 2013) is a sembanking language thatcaptures whole sentence meanings in a rooted, di-rected, labeled, and (predominantly) acyclic graphstructure.
AMR utilizes multi-layer linguistic anal-ysis such as PropBank frames, non-core semanticroles, coreference, named entity annotation, modal-ity and negation to represent the semantic structureof a sentence.
AMR strives for a more logical, lesssyntactic representation.
Compared to traditionaldependency parsing and semantic role labeling, thenodes in AMR are entities instead of words, and theedge types are much more fine-grained4.
AMR thuscaptures deeper meaning compared with other rep-resentations more commonly used to represent men-tion context in EL.We use AMR to represent semantic informationabout entity mentions expressed in their textual con-text.
Specifically, given an entity mention m, weuse a rule based method to construct a KnowledgeNetwork, which is a star-shaped graph with m at thehub, with leaf nodes obtained from entity mentionsreachable by AMR graph traversal from m, as wellas AMR node attributes such as entity type.
A sub-set of the leaf nodes are selected as m?s collabora-tors using rules presented in the following subsec-4AMR distinguishes between entities and concepts, the for-mer being instances of the latter.
We consider AMR concepts asentity mentions, and use AMR entity annotation for coreferenceresolution.1131tions.
Note that while we only evaluate linking ofPER, ORG, and GPE entities, collaborators may beof any type.
We also outline preliminary efforts touse AMR to create sets of coherent entity mentions.In each of the following subsections we describeelements of AMR useful for context representationin EL.
For each element we explain how our cur-rent system makes use of it (primarily, by using it toadd entity mentions to a particular entity mention?sset of collaborators).
In doing so, we mainly referto several examples from political discussion forumsabout ?Mitt Romney?, ?Ron Paul?
and ?Gary John-son?.
Their AMR graphs are depicted in Figure 1.3.1 Entity NodesEach AMR node represents an entity mention, andcontains its canonical name as inferred from senten-tial context.
This property is called name expan-sion.
Consider the following sentence: ?Indonesialies in a zone where the Eurasian, Philippine andPacific plates meet and occasionally shift, causingearthquakes and sometimes generating tsunamis.
?.Here, the nodes representing the three plates willbe labeled as ?Eurasian Plate?, ?Philippine Plate?and ?Pacific Plate?
respectively, even though thesestrings do not occur in the sentence.
Note thatthese labels may be recovered primarily by ap-pealing to syntactic reasoning, without consultinga KB.
In our implementation we consider these ex-panded names as mentions (these strings supersederaw mentions as input to the salience based candi-date enumeration (Section 5.2)).
Because the ini-tial enumeration of entity candidates depends heav-ily on the mention?s surface form, independent ofcontext, name expansion will help us link ?Philip-pine?
to ?Philippine Sea Plate?
as opposedto the country.An AMR node also contains an entity type.
AMRdefines 8 main entity types (Person, Organization,Location, Facility, Event, Product, Publication, Nat-ural object, Other) and over one hundred fine-grained subtypes.
For example, company, govern-ment organization, military, criminal organization,political party, school, university, research institute,team and league are subtypes of organization.
Thefine-grained entity types defined in AMR help usrestrict KB entity candidates for a given mentionby encouraging entity type matching.
For exam-anticipate-01instance :ARG0 :ARG1iinstance :time :ARG1nominate-01instancepolarity :ARG0date-entityinstance2012:yearMitt RomneypersonGOPpolitical-party(a) I am cautiously anticipating the GOP nominee in2012 not to be Mitt Romney.governorinstanceMassachusettsstate:ARG2 :ARG1have-org-role-91instance:ARG0-ofRomneyperson(b) Romney was the Governor of Massachusetts...great:modgrandsoninstancepioneerinstance :modMormonreligious-group:ARG2 :ARG1have-rel-role-91instance :ARG0Romneyperson(c) Romney is the great-great-grandson of a Mormonpioneer...candidateinstance :mod :exampleRepublicanpolitical-party :op2 :op1 :op3andinstancePaulpersonRomneypersonJohnsonperson(d) Republican candidates like Romney, Paul, andJohnson...Figure 1: AMR for the Walk-through Exampleple, in ?The Yuri dolgoruky is the first in a seriesof new nuclear submarines to be commissioned this1132year but the bulava nuclear-armed missile devel-oped to equip the submarine has failed tests and thedeployment prospects are uncertain.
?, AMR labels?Yuri dolgoruky?
as a product instead of a person.We manually mapped AMR entity types to equiva-lent DBpedia types to inform type matching restric-tions5.
However, to make our context comparisonalgorithm less dependent on the quality of this map-ping, and on automatic AMR name type assignment,we add a mention?s type to its collaborators6.
Infuture work we plan to investigate the effects of dif-ferent type matching techniques, varying degrees ofstrictness.3.2 Semantic RolesAMR defines core roles based on theOntoNotes (Hovy et al, 2006) semantic rolelayer.
Each predicate is associated with a senseand frame description.
If a target entity mentionm and a context entity mention n are both playingcore roles for the same predicate, we consider n asa collaborator of m. Consider the following post:?Did Palin apologize to Giffords?
He needs toconduct a beer summit between Palin and NBC.
?.We add ?Giffords?
and ?NBC?
as collaborators of?Palin?, because they play core roles in both the?apologize-01?
and ?meet-03?
events.AMR defines new core semantic roles which didnot exist in PropBank (Palmer et al, 2005), Nom-Bank (Meyers et al, 2004), or Ontonotes (Hovy etal., 2006).
Intuitively, the following special rolesshould provide discriminative collaborators:?
The ARG2 role of the have-org-role-91 frame in-dicates the title held by an entity (ARG0), such asPresident and Governor, within a particular orga-nization (ARG1).?
ARG2 and ARG3 of have-rel-role-91 are used todescribe two related entities of the same type, suchas family members.AMR defines a rich set of general semantic rela-tions through non-core semantic roles.
We choosethe following subset of non-core roles to providecollaborators for entity mentions: domain, mod,5The mapping from three main types and AMR entity typesto Dbpedia types is at: nlp.cs.rpi.edu/amrel/dbtype.txt6A more strict approach might disallow type mismatches be-tween entity mentions and their target KB entities outright.cause, concession, condition, consist-of, extent,part, purpose, degree, manner, medium, instrument,ord, poss, quant, subevent, subset, topic.3.3 Background Time and LocationAMR provides rich temporal and spatial informa-tion about entities and events.
Types instantiatedin AMR include time, year, month, day, source,destination, path and location.
We exploit time andlocation entities as collaborators for entity mentionswhen they each play a role in the same predicate.For example, in the following post, the time roleof the ?die-01?
event is ?2008?
: ?I just Read ofClark?s death in 2008?.
We can link ?Clark?
toArthur C Clark in the KB, which containsthe triple: ?Arthur C Clark, date-of-death,2008-03-19?
(see Section 4).
Similarly, it?s verychallenging to link the abbreviation ?BMKG?, inthe following sentence, to the correct target entityIndonesian Agency for Meteorology,Climatology and Geophysics, whoseheadquarters are listed as Jakarta in the KB:?It keeps on shaking.
Jakarta BMKG spokesmanMujuhidin said?.
Here, ?Jakarta?
is added as acollaborator of ?BMKG?
since AMR labels it as thelocation of the organization, which facilitates thecorrect link because in DBpedia Jakarta is listedas its headquarter.Authors often assume that readers will infer im-plicit temporal information about events.
In fact,half of the events extracted by information extrac-tion (IE) systems lack time arguments (Ji et al,2009).
Therefore if an AMR parse includes no timeinformation, we use the document creation time asan additional collaborator for mention in question.For example, knowing the document creation time?2005-06-05?
can help us link ?Hsiung Feng?
inthe following sentence ?The BBC reported that Tai-wan has successfully test fired the Hsiung Feng,its first cruise missile.?
to Hsiung Feng IIE,which was deployed in 2005.
Similarly, we includedocument creation location as a global collaborator.3.4 CoreferenceFor linking purposes, we treat a coreferential chainof mentions as a single ?mention?.
In doing so, thecollaborator set for the entire chain is computed asthe union over all of the chain?s mentions?
collabo-1133rator sets.
From here on we refer to a coreferentialchain of mentions as simply a ?mention?.AMR currently only represents sentence-levelcoreference resolution.
In order to construct aknowledge network across sentences, we use the fol-lowing heuristic rules.
If two names have a substringmatch (on a token-wise basis with stop words re-moved), or one name consists of the initials of an-other in all capital letters, then we mark them ascoreferential.
We replace all names in a corefer-ential chain with their canonical name, which mayhave been derived via name expansion (Section 3.1):full names for people and abbreviations for organi-zations.3.5 Knowledge Networks for CoherentMentionsAMR defines a rich set of conjunction relations:?and?, ?or?, ?contrast-01?, ?either?, ?comparedto?, ?prep along with?, ?neither?, ?slash?, ?be-tween?
and ?both?.
These relations are often ex-pressed between entities that have other relations incommon.
We therefore group mentions connectedby conjunction relations into sets of coherent men-tions.
This representation is used only in preliminaryexperiments on collective entity linking.Figure 2 shows the expanded knowledge net-work that includes results from individual networksfor each of the coherent mentions from the walk-through example.
For each coherent set, we mergethe knowledge networks of all of its mentions7.Johnson RomneyPaulgrandsoncoreferencenominate-01 : polarity -governorRepublicanmodify modifymodifyconjunction: andconjunction: and conjunction: andMassachusettsGOPMitt RomneyMormonpioneerFigure 2: Knowledge Network for Mentions in Source7recall that by mention, we mean a coreferential chain ofmentions that may extend across sentences4 Knowledge Network Construction fromKBWe combine Wikipedia with derivative resources tocreate the KB.
The KB is a single knowledge net-work in which nodes are entities (Wikipedia arti-cles) or constant values (e.g.
a dollar amount ordate), and the edges represent relations.
We usethis structure for context representation for entities,which together with context representation for men-tions (Section 3) feeds re-ranking based on contextcomparison.The KB is formally represented by triples:?
Entity, EdgeLabel,Node ?where Entity is the entity?s unique identifier, Edge-Label is relation type, and Node is the correspondingrelation value - either another Entity or a constant.These triples are derived from typed relations ex-pressed within Wikipedia infoboxes, Templates, andCategories, untyped hyperlinks within Wikipediaarticle text, typed relations within DBpedia (db-pedia.org) and Freebase (www.freebase.com), andGoogle?s ?people also searched for?
list8.
Figure 3shows a portion of the KB pertaining to the examplein Figure 1.In order to merge nodes from multiple KBs, weuse the Wikipedia title as a primary key, and then useDBpedia wikiPageID and Freebase Key relations.5 Linking Knowledge Networks5.1 OverviewIn this section we present our detailed algorithm tolink each mention to a KB entity using a simple simi-larity measure over knowledge networks.
Recall thata rule-based method has already been employed toconstruct star-shaped knowledge networks for indi-vidual mentions and entities (see sections 3 and 4;A KB knowledge network is the subnetwork of theentire KB centered at a candidate entity).For each mention to be linked, an initial list ofcandidate entities are enumerated based on entitysalience with respect to the mention, independent ofmention context (Section 5.2)9.
Context collabora-tor re-ranking proceeds in an unsupervised fashion8In response to a query entity Google provides a list of en-tities that ?people also search for?
- we add them to the entity?snetwork.9Here, ?mention?
means coreferential chain of mentions.1134Mitt Romney Gary JohnsonRon PaulUnited Statespresidentialcandidates, 2012category categorycategoryLivingPeople RepublicanPartyinlink/outlinkSalt LakeCity, UtahoutlinkoutlinkLyndon B. JohnsonAuthoroutlinkoutlinkFreebase object typeFreebase object typePaul the ApostleMormonPersondataoutlinkinfobox: religionWiki TemplatetypePaul McCartneyAndrew JohnsonGeorge W. RomneyGooglepeoplealsosearchforoutlinkoutlinkcategorycategorycategoryMedicareinfobox: political partyinfobox: other partyoutlinkMitt Romney presidentialcampaign, 2012DbpediawikiPageID426208Governor ofMassa-chusettsinlinkFigure 3: Knowledge Network for Entities in KBagnostic to knowledge network edge labels using theJaccard similarity measure computed between themention and each entity, by taking their collabora-tor sets as inputs (Section 5.3).We also describe Context Coherence re-ranking interms of KB knowledge networks only, which con-stitutes preliminary steps toward unsupervised col-lective entity linking in section 5.4 based on the no-tion of coherence described in section 3.5.
We leavea combination of the two re-ranking approaches tofuture work.5.2 SalienceWe use commonness (Medelyan and Legg, 2008)as a measure of context independent salience foreach mention m, to generate an initial rankedlist of candidate entities E ?
pe1, ..., eNq whereN is the cutoff for number of candidates.
In allexperiments, we used N = 15 which can give us anoracle accuracy score 97.58%.Commonnesspm, eq ?countpm, eq?e1countpm, e1qHere, countpm, eq is the number of hyperlinkswith anchor text m and entity e within all ofWikipedia.
As illustrated in Figure 3, using thissalience measure ?Romney?
is successfully linked toMitt Romney.
For the mention ?Paul?, the politi-cian Ron Paul is ranked at top 2 (less popular thanthe musician Paul McCartney).
For the men-tion ?Johnson?, the correct entity Gary Johnsonis ranked at top 9, after more popular entities such asLyndon B. Johnson and Andrew Johnson.5.3 Context Collaborator Based Re-rankingContext collaborator based re-ranking is driven bythe similarity between mention and entity knowl-edge networks.
We construct knowledge networkgpmq for each mention m, and knowledge networkgpeiq for each entity candidate eiin m?s entitycandidate list E. We re-rank E according to JaccardSimilarity, which computes the similarity betweengpmq and gpeiq:Jpgpmq, gpeiqq ?|gpmq X gpeiq||gpmq Y gpeiq|Note that the edge labels (e.g., nominate-01 fora mention, or infobox: religion for an entity) areignored, as the similarity metric operates over setsof collaborators (leaf nodes in the knowledge net-works).
For set intersection and union computa-tion, elements are treated as lists of lower-cased to-kens with stop words removed, and two elementsare considered equal if and only if they have one or1135more token in common.
Due to the support fromtheir neighbor Republican in the KB (Figure 3)which matches the neighbor ?Republican?
of men-tions ?Paul?
and ?Johnson?
(Figure 2), Ron Pauland Gary Johnson are promoted to top 1 andtop 3 respectively.
Gary Johnson is still behindtwo former U.S. presidents Andrew Johnsonand Lyndon B. Johnson who also shares theneighbor Republican in the KB.5.4 Context Coherence Based Re-rankingContext coherence based re-ranking is driven by thesimilarity among KB entities.
Let Rmbe a set ofcoherent entity mentions, and REbe the set of corre-sponding entity candidate lists, which are generatedaccording to salience.
Given RE, we generate everycombination of possible top candidate lists for thementions in Rm, and denote the set of these combi-nations Cm.
Formally, Cmis the Cartesian productof all candidate lists E P RE.
In the walk-throughexample, Rmcontains [?Romney?, ?Paul?, ?John-son?
], and Cmcontains [Mitt Romney, RonPaul, Gary Johnson], [Mitt Romney,Paul McCartney, Lyndon Johnson], etc.We compute coherence for each combinationc P Cmas Jaccard Similarity, by applying a formof Equation 5.3 generalized to take any number ofarguments to the set of knowledge networks for allentities in c, i.e., tgpeq|e P cu.
The highest similar-ity combination is selected, yielding a top candidatefor each m P Rm.
For example, compared toAndrew Johnson and Lyndon Johnson,Gary Johnson is more coherently connectedwith Mitt Romney and Ron Paul, therefore itis promoted to top 1 with the coherence measure.6 Experiments6.1 Data And Scoring MetricFor our experiments we use a publicly availableAMR R3 corpus (LDC2013E117) that includesmanual EL annotations for all entity mentions(LDC2014E15)10.For evaluation we used all the discussion forumposts (DF), and news documents (News) that were10EL annotations are available to KBP shared task regis-trants (nlp.cs.rpi.edu/kbp/2014) via Linguistic Data Consortium(www.ldc.upenn.edu).sorted according to alphabetic order of documentIDs and taken as a tenth.
The detailed data statis-tics are presented in Table 111.PER ORG GPE AllNews 159 187 679 1,025DF 235 129 224 588All 394 316 903 1,613Table 1: Total # of Entity Mentions in Test SetFor each mention, we check whether the KB en-tity returned by an approach is correct or not.
Wecompute accuracy for an approach as the proportionof mentions correctly linked.6.2 Experiment ResultsWe focus primarily on context collaborator based re-ranking results.
We compare our results with severalbaseline and state-of-the-art approaches in Table 2.In Table 3 we present preliminary results for collec-tive linking.Our Unsupervised Context Collaborator Ap-proach substantially outperforms the popularitybased methods.
More importantly, we see that AMRprovides the best context representation for collabo-rator selection.
Even system AMR outperformed notonly baseline co-occurrence based collaborator se-lection methods, but also outperforms the collabora-tor selection method based on human annotated coresemantic roles.
Figure 4 depicts accuracy increasesas more AMR annotation is used in selecting collab-orators.
From the commonness baseline, additionalknowledge about individual names leads to substan-tial gains followed by additional gains after incorpo-rating links denoting semantic roles.
Note that coref-erence here includes cross-sentence co-reference notbased on AMR (Section 3.4).
Furthermore, the re-sults using human annotated AMR outperform thestate-of-the-art supervised methods trained from alarge scale EL training corpus, which rely on collec-tive inference12.
These results all verify the impor-tance of incorporating a wider range of deep knowl-edge.
Finally, Table 2 presents results in which our11The list of document IDs in the test set is at:nlp.cs.rpi.edu/amrel/testdoc.txt12Note that the ground-truth EL annotation for the test setwas created by correcting the output from supervised methods,so it may even favor these methods.1136Approach Definition News DF TotalPopularityCommonness based on the popularity measure as described in section 5.2.
89.76 68.99 82.20GoogleSearchuse the top Wikipedia page returned by Google search using themention as a key word.88.10 77.17 84.12Supervised State-of-the-artsupervised re-ranking using multi-level linguistic features forcollaborators and collective inference, trained from 20,000 en-tity mentions from TAC-KBP2009-2014.
We combined twosystems (Chen and Ji, 2011; Cheng and Roth, 2013) using rulesto highlight their strengths.93.07 87.41 91.01UnsupervisedContextCollaboratorApproachSen.
LevelCooccurrencesentence-level co-occurrence based collaborator selection 93.17 73.25 85.92(collaborators limited to human AMR-labeled named entities) 90.77 70.31 83.31Doc.
LevelCooccurrencedocument-level co-occurrence based collaborator selection 90.05 69.86 82.69(collaborators limited to human AMR-labeled named entities) 87.51 69.37 80.90Human AMR using human annotated AMR nodes and edges.
93.56 86.88 91.13System AMR using AMR nodes and edges automatically generated by anAMR parser (Flanigan et al, 2014).90.15 85.69 88.52Human SRL using human annotated core semantic roles defined in Prop-Bank (Palmer et al, 2005) and NomBank (Meyers et al, 2004):ARG0, ARG1, ARG2, ARG4 and ARG5.93.27 71.21 85.24UnsupervisedCombinedApproachHuman AMR coherence approach used where possible (215 mentions), col-laborator approach elsewhere (remaining 1398 mentions), usinghuman annotated AMR nodes and edges.94.34 88.25 92.12Table 2: Accuracy (%) on Test Set (1613 mentions)context coherence method is used where possible(i.e., those 215 mentions that are members of co-herent sets according to our criteria as described inSection 3.5), and the context collaborator approachbased on human AMR annotation is applied else-where.Figure 4: AMR Annotation Layers Effects on AccuracyTable 3 focuses on the 215 mentions that met ournarrow criteria for forming a coherent set of men-tions.
We applied the context coherence based re-ranking method (Section 5.4) to collectively linkthose mentions.
This approach substantially outper-forms the co-occurrence baseline, and even outper-forms the context collaborator approach applied tothose 215 mentions, especially for discussion forumdata.Approach Description News DF AllCoherence: coherence set built fromwithin-sentence collaborators limited tohuman AMR-labeled Named Entities.72.64 76.85 75.47Coherence: coherence set built from hu-man AMR conjunctions (Sec.
3.5)96.73 95.16 96.28Collaborator: used coherent set based onhuman AMR as collaborators.91.50 82.26 88.84Table 3: Context Coherence Accuracy (%) on 215 Men-tions which Can Form Coherent Sets6.3 Remaining Error Analysis and DiscussionA challenging source of errors pertains to the knowl-edge gap between the source text and KB.
Newsand social media are source text genres that tend tofocus on new information, trending topics, break-ing events, or even mundane details about the en-tity.
In contrast, the KB usually provides a snap-shot summarizing only the entity?s most represen-tative and important facts.
A source-KB similaritydriven approach alone will not suffice when a men-tion?s context differs substantially from anything onthe KB side.
AMR annotation?s synthesis of wordsand phrases from the surface texts into concepts onlyprovides a first step toward bridging the knowledgegap.
Successful linking may require (1) reasoningusing general knowledge, or (2) retrieval of othersources that contain additional useful linking infor-mation.
Table 4 illustrates two relevant examples1137Type Source Knowledge BaseGeneralKnowledge[Christies]mdenial of marriage privledges togays will alienate independents and his ?I wantedto have the people vote on it?
will ring hollow.
[Chris Christie]ehas said that he favoured New Jer-sey?s law allowing same-sex couples to form civilunions, but would veto any bill legalizing same-sexmarriage in New Jersey.ExternalKnowledgeTranslation out of hype-speak: some kook madethreatening noises at [Brownback]mand go ar-rested.
[Samuel Dale ?Sam?
Brownback]e(born September12, 1956) is an American politician, the 46th and cur-rent Governor of Kansas.Table 4: Examples of Knowledge Gapthat our system does not correctly link.
In the firstexample, if we don?t already know that Christie isthe topic of discussion, as humans we might useour general knowledge that ?governors veto bills?to pick the correct entity.
Using this type of knowl-edge presents interesting challenges (e.g., governorsdon?t always veto bills, nor are they the only oneswho can do so).
In the second example, the rumorabout this politician is not important enough to be re-ported in his Wikipedia page.
We might first figureout, using cross-document coreference techniques,that a news article with the headline ?Man AccusedOf Making Threatening Phone Call To Kansas Gov.Sam Brownback May Face Felony Charge...?
istalking about the same rumor.
Then we might usebiographical facts (e.g., Brownback is the governorof Kansas) from the article to enrich Brownback?sknowledge network on the source side.Sometimes helpful neighbor concepts areomitted because the current collaborator se-lection criteria are too restricted.
For exam-ple, ?armed?
and ?conflicts?
are informativewords for linking ?The Stockholm Institute?to Stockholm International PeaceResearch Institute in the following sen-tence ?The Stockholm Institute stated that 23 of25 major armed conflicts in the world in 2000occurred in impoverished nations.
?, but they werenot selected as context collaborators.
In addi-tion, our cross-sentence coreference resolution iscurrently limited to proper names.
Expanding itto include nominals could further enrich contextcollaborators to overcome some remaining errors.For example, in the sentence, ?The first woman toserve on SCOTUS?, if we know ?The first woman?is coreferential with ?Sandra Day O?Connor?
inthe previous sentence, we can link ?SCOTUS?
toSupreme Court of the United Statesinstead of Scotus College.7 Conclusions and Future WorkEL requires a representation of the relations amongentities in text.
We showed that the Abstract Mean-ing Representation (AMR) can better capture andrepresent the contexts of entity mentions for EL thanprevious approaches.
We plan to improve AMRrepresentation as well as automatic annotation.
Weshowed that AMR enables EL performance compa-rable to the supervised state of the art using an unsu-pervised, non-collective approach.
We plan to com-bine collaborator and coherence methods into a uni-fied approach, and to use edge labels in knowledgenetworks for context comparison (note that the lastof these is quite challenging due to normalization,polysemy, and semantic distance issues).
We haveonly applied a subset of AMR representations to theEL task, but we aim to explore how more AMRknowledge can be used for other more challengingInformation Extraction and Knowledge Base Popu-lation tasks.AcknowledgmentsThis work was supported by the U.S. DARPA DEFTProgram No.
FA8750-13-2-0041 and FA8750-13-2-0045, DARPA BOLT Program No.
HR0011-12-C-0014, ARL NS-CTA No.
W911NF-09-2-0053,NSF Awards IIS-0953149 and IIS-1523198, AFRLDREAM project, DHS CCICADA, gift awards fromIBM, Google, Disney and Bosch.
The views andconclusions contained in this document are those ofthe authors and should not be interpreted as rep-resenting the official policies, either expressed orimplied, of the U.S. Government.
The U.S. Gov-ernment is authorized to reproduce and distributereprints for Government purposes notwithstandingany copyright notation here on.1138ReferencesL.
Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Grif-fitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer,and N. Schneider.
2013.
Abstract meaning representa-tion for sembanking.
In Proc.
ACL 2013 Workshop onLinguistic Annotation and Interoperability with Dis-course.T.
Cassidy, H. Ji, L. Ratinov, A. Zubiaga, and H. Huang.2012.
Analysis and enhancement of wikification formicroblogs with context expansion.
In Proc.
In-ternational Conference on Computational Linguistics(COLING 2012).Z.
Chen and H. Ji.
2011.
Collaborative ranking: A casestudy on entity linking.
In Proc.
Empirical Methods inNatural Language Processing (EMNLP 2011).X.
Cheng and D. Roth.
2013.
Relational inference forwikification.
In Proc.
Empirical Methods in NaturalLanguage Processing (EMNLP 2013).S.
Cucerzan.
2011.
Tac entity linking by performing full-document entity extraction and disambiguation.
InProc.
Text Analysis Conference (TAC 2011).J.
Dalton and L. Dietz.
2013.
A neighborhood relevancemodel for entity linking.
In Proc.
Open Research Ar-eas in Information Retrieval (OAIR 2013).N.
Fernandez, J.
A. Fisteus, L. Sanchez, and E. Martin.2010.
Webtlab: A cooccurence-based approach to kbp2010 entity-linking task.
In Proc.
Text Analysis Con-ference (TAC 2010).J.
Flanigan, S. Thomson, J. Carbonell, C. Dyer, and N. A.Smith.
2014.
A discriminative graph-based parser forthe abstract meaning representation.
In Proc.
Associa-tion for Computational Linguistics (ACL 2014).Y.
Guo, W. Che, T. Liu, and S. Li.
2011.
A graph-basedmethod for entity linking.
In Proc.
International JointConference on Natural Language Processing (IJCNLP2011).X.
Han and L. Sun.
2011.
A generative entity-mentionmodel for linking entities with knowledge base.
InProc.
Association for Computational Linguistics: Hu-man Language Technologies (ACL-HLT 2011).E.d Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90% solution.In Proc.
Human Language Technology conference -North American chapter of the Association for Com-putational Linguistics (HLT-NAACL 2006).H.
Huang, Y. Cao, X. Huang, H. Ji, and C. Lin.2014.
Collective tweet wikification based on semi-supervised graph regularization.
In Proc.
Associationfor Computational Linguistics (ACL 2014).H.
Ji, R. Grishman, Z. Chen, and P. Gupta.
2009.
Cross-document event extraction and tracking: Task, eval-uation, techniques and challenges.
In Proc.
RecentAdvances in Natural Language Processing (RANLP2009).H.
Ji, R. Grishman, H. T. Dang, K. Griffitt, and J. Ellis.2010.
Overview of the tac 2010 knowledge base pop-ulation track.
In Proc.
Text Analysis Conference (TAC2010).H.
Ji, R. Grishman, and H. T. Dang.
2011.
Overviewof the tac 2011 knowledge base population track.
InProc.
Text Analysis Conference (TAC 2011).H.
Ji, J. Nothman, and H. Ben.
2014.
Overview of tac-kbp2014 entity discovery and linking tasks.
In Proc.Text Analysis Conference (TAC 2014).Z.
Kozareva, K. Voevodski, and S. Teng.
2011.
Classlabel enhancement via related instances.
In Proc.Empirical Methods in Natural Language Processing(EMNLP 2011).S.
Kulkarni, A. Singh, G. Ramakrishnan, andS.
Chakrabarti.
2009.
Collective annotation ofwikipedia entities in web text.
In Proc.
KnowledgeDiscovery and Data Mining (KDD 2009).X.
Ling, S. Singh, and D. S. Weld.
2014.
Context rep-resentation for named entity linking.
In Proc.
PacificNorthwest Regional NLP Workshop (NW-NLP 2014).O.
Medelyan and C. Legg.
2008.
Integrating cycand wikipedia: Folksonomy meets rigorously definedcommon-sense.
In Proc.
AAAI 2008 Workshop onWikipedia and Artificial Intelligence.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The nombankproject: An interim report.
In Proc.
HLT-NAACL 2004Workshop on Frontiers in Corpus Annotation.H.
Nguyen, H. Minha, T. Cao, and T. Nguyenb.
2012.Jvn-tdt entity linking systems at tac-kbp2012.
In Proc.Text Analysis Conference (TAC 2012).M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.M.
Pennacchiotti and P. Pantel.
2009.
Entity extractionvia ensemble semantics.
In Proc.
Empirical Methodsin Natural Language Processing (EMNLP 2009).W.
Radford, B. Hachey, J. Nothman, M. Honnibal, andJ.
R. Curran.
2010.
Cmcrc at tac10: Document-levelentity linking with graph-based re-ranking.
In Proc.Text Analysis Conference (TAC 2010).L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambigua-tion to wikipedia.
In Proc.
Association for Compu-tational Linguistics: Human Language Technologies(ACL-HLT 2011).J.
Xu, Q. Lu, J. Liu, and R. Xu.
2012.
Nlpcomp in tac2012 entity linking and slot-filling.
In Proc.
Text Anal-ysis Conference (TAC 2012).1139
