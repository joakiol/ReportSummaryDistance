Proceedings of NAACL HLT 2009: Short Papers, pages 233?236,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEstimating and Exploiting the Entropy of Sense DistributionsPeng JinInstitute of Computational LinguisticsPeking UniversityBeijing Chinajandp@pku.edu.cnDiana McCarthy, Rob Koeling and John CarrollUniversity of SussexFalmer, East SussexBN1 9QJ, UK{dianam,robk,johnca}@sussex.ac.ukAbstractWord sense distributions are usually skewed.Predicting the extent of the skew can help aword sense disambiguation (WSD) system de-termine whether to consider evidence from thelocal context or apply the simple yet effec-tive heuristic of using the first (most frequent)sense.
In this paper, we propose a method toestimate the entropy of a sense distribution toboost the precision of a first sense heuristic byrestricting its application to words with lowerentropy.
We show on two standard datasetsthat automatic prediction of entropy can in-crease the performance of an automatic firstsense heuristic.1 IntroductionWord sense distributions are typically skewed andWSD systems do best when they exploit this ten-dency.
This is usually done by estimating the mostfrequent sense (MFS) for each word from a trainingcorpus and using that sense as a back-off strategy fora word when there is no convincing evidence fromthe context.
This is known as the MFS heuristic 1and is very powerful since sense distributions areusually skewed.
The heuristic becomes particularlyhard to beat for words with highly skewed sense dis-tributions (Yarowsky and Florian, 2002).
Althoughthe MFS can be estimated from tagged corpora, thereare always cases where there is insufficient data, orwhere the data is inappropriate, for example because1It is also referred to as the first sense heuristic in the WSDliterature and in this paper.it comes from a very different domain.
This has mo-tivated some recent work attempting to estimate thedistributions automatically (McCarthy et al, 2004;Lapata and Keller, 2007).
This paper examines thecase for determining the skew of a word sense distri-bution by estimating entropy and then using this toincrease the precision of an unsupervised first senseheuristic by restricting application to those wordswhere the system can automatically detect that it hasthe most chance.
We use a method based on thatproposed by McCarthy et al (2004) as this approachdoes not require hand-labelled corpora.
The methodcould easily be adapted to other methods for predic-ing predominant sense.2 MethodGiven a listing of senses from an inventory, themethod proposed by McCarthy et al (2004) pro-vides a prevalence ranking score to produce a MFSheuristic.
We make a slight modification to Mc-Carthy et al?s prevalence score and use it to es-timate the probability distribution over the sensesof a word.
We use the same resources as Mc-Carthy et al (2004): a distributional similarity the-saurus and a WordNet semantic similarity measure.The thesaurus was produced using the metric de-scribed by Lin (1998) with input from the gram-matical relation data extracted using the 90 mil-lion words of written English from the British Na-tional Corpus (BNC) (Leech, 1992) using the RASPparser (Briscoe and Carroll, 2002).
The thesaurusconsists of entries for each word (w) with the top50 ?nearest neighbours?
to w, where the neighboursare words ranked by the distributional similarity that233they share with w. The WordNet similarity scoreis obtained with the jcn measure (Jiang and Con-rath, 1997) using the WordNet Similarity Package0.05 (Patwardhan and Pedersen, 2003) and WordNetversion 1.6.
The jcn measure needs word frequencyinformation, which we obtained from the BNC.2.1 Estimates of Predominance, Probabilityand EntropyFollowing McCarthy et al (2004), we calculateprevalence of each sense of the word (w) using aweighted sum of the distributional similarity scoresof the top 50 neighbours of w. The sense of w thathas the highest value is the automatically detectedMFS (predominant sense).
The weights are deter-mined by the WordNet similarity between the sensein question and the neighbour.
We make a modi-fication to the original method by multiplying theweight by the inverse rank of the neighbour fromthe list of 50 neighbours.
This modification magni-fies the contribution to each sense depending on therank of the neighbour while still allowing a neigh-bour to contribute to all senses that it relates too.We verified the effect of this change compared to theoriginal ranking score by measuring cross-entropy.
2Let Nw = n1,n2 .
.
.nk denote the ordered set of thetop k = 50 neighbours of w according to the distri-butional similarity thesaurus, senses(w) is the set ofsenses of w and dss(w,n j) is the distributional sim-ilarity score of a word w and its jth neighbour.
Letwsi be a sense of w then wnss(wsi,n j) is the maxi-mum WordNet similarity score between wsi and theWordNet sense of the neighbour (n j) that maximisesthis score.
The prevalence score is calculated as fol-lows with 1rankn j being our modification to McCarthyet alPrevalence Score(wsi) = ?n j?Nw dss(w,n j)?wnss(wsi,n j)?wsi??senses(w)wnss(wsi?
,n j)?
1rankn j(1)To turn this score into a probability estimate we sumthe scores over all senses of a word and the proba-bility for a sense is the original score divided by thissum:2Our modified version of the score gave a lower cross-entropy with SemCor compared to that in McCarthy et al Theresult was highly significant with p < 0.01 on the t-test.p?
(wsi) = prevalence score(wsi)?ws j?w prevalence score(ws j)(2)To smooth the data, we evenly distribute 1/10 of thesmallest prevalence score to all senses with a unde-fined prevalence score values.
Entropy is measuredas:H(senses(w)) =?
?wsi?senses(w)p(wsi)log(p(wsi))using our estimate (p?)
for the probability distribu-tion p over the senses of w.3 ExperimentsWe conducted two experiments to evaluate the ben-efit of using our estimate of entropy to restrict appli-cation of the MFS heuristic.
The two experimentsare conducted on the polysemous nouns in SemCorand the nouns in the SENSEVAL-2 English all wordstask (we will refer to this as SE2-EAW).3.1 SemCorFor this experiment we used all the polysemousnouns in Semcor 1.6 (excluding multiwords andproper nouns).
We depart slightly from (McCarthyet al, 2004) in including all polysemous nounswhereas they limited the experiment to those witha frequency in SemCor of 3 or more and where thereis one sense with a higher frequency than the others.Table 1 shows the precision of finding the predomi-nant sense using equation 1 with respect to differententropy thresholds.
At each threshold, the MFS inSemcor provides the upper-bound (UB).
The randombaseline (RBL) is computed by selecting one of thesenses of the target word randomly as the predomi-nant sense.
As we hypothesized, precision is higherwhen the entropy of the sense distribution is lower,which is an encouraging result given that the entropyis automatically estimated.
The performance of therandom baseline is higher at lower entropy whichshows that the task is easier and involves a lower de-gree of polysemy of the target words.
However, thegains over the random baseline are greater at lowerentropy levels indicating that the merits of detect-ing the skew of the distribution cannot all be due tolower polysemy levels.234H precision #(?)
eq 1 RBL UB tokens0.5 - - - 00.9 80.3 50.0 84.8 4660.95 85.1 50.0 90.9 13601 68.5 50.0 87.4 98741.5 67.6 42.6 86.9 112872 58.0 36.7 79.5 259972.5 55.7 34.4 77.6 315993.0 50.2 30.6 73.4 414014.0 47.6 28.5 70.8 469875.0 (all) 47.3 27.3 70.5 47539Table 1: First sense heuristic on SemCorFreq ?
P #tokens1 45.9 11325 50.1 576510 50.7 10736100 49.4 395431000(all) 47.3 47539#senses ?
P #tokens2 67.2 107365 55.4 311818 50.1 4139312 47.8 4604130(all) 47.3 47539Table 2: Precision (P) of equation 1 on SemCor with re-spect to frequency and polysemyWe also conducted a frequency and polysemyanalysis shown in Table 2 to demonstrate that theincrease in precision is not all due to frequency orpolysemy.
This is important, since both frequencyand polysemy level (assuming a predefined sense in-ventory) could be obtained without the need for au-tomatic estimation.
As we can see, while precisionis higher for lower polysemy, the automatic estimateof entropy can provide a greater increase in preci-sion than polysemy, and frequency does not seem tobe strongly correlated with precision.3.2 SENSEVAL-2 English All Words DatasetThe SE2-EAW task provides a hand-tagged test suiteof 5,000 words of running text from three articlesfrom the Penn Treebank II (Palmer et al, 2001).Again, we examine whether precision of the MFSH precision #(?)
eq 1 RBL SC UB tokens0.5 - - - - 00.9 1 50.0 1 1 70.95 94.7 50.0 94.7 1 191 69.6 50.0 81.3 94.6 1121.5 68.0 49.0 81.3 93.8 1282 69.6 34.7 68.2 87.7 4212.5 65.0 33.0 65.0 86.5 4883.0 56.6 27.5 60.8 80.1 6874.0 52.6 25.6 58.8 79.2 7665.0 (all) 51.5 25.6 58.5 79.3 769Table 3: First sense heuristic on SE2-EAWheuristic can be increased by restricting applicationdepending on entropy.
We use the same resources asfor the SemCor experiment.
3 Table 3 gives the re-sults.
The most frequent sense (MFS) from SE2-EAWitself provides the upper-bound (UB).
We also com-pare performance with the Semcor MFS (SC).
Per-formance is close to the Semcor MFS while not re-lying on any manual tagging.
As before, precisionincreases significantly for words with low estimatedentropy, and the gains over the random baseline arehigher compared to the gains including all words.4 Related WorkThere is promising related work on determining thepredominant sense for a MFS heuristic (Lapata andKeller, 2007; Mohammad and Hirst, 2006) but ourwork is the first to use the ranking score to estimateentropy and apply it to determine the confidence inthe MFS heuristic.
It is likely that these methodswould also have increased precision if the rankingscores were used to estimate entropy.
We leave suchinvestigations for further work.Chan and Ng (2005) estimate word sense distri-butions and demonstrate that sense distribution esti-mation improves a supervised WSD classifier.
Theyuse three sense distribution methods, including thatof McCarthy et al (2004).
While the other twomethods outperform the McCarthy et al method,3We also used a tool for mapping from WordNet 1.7 toWordNet 1.6 (Daude?
et al, 2000) to map the SE2-EAW noundata (originally distributed with 1.7 sense numbers) to 1.6 sensenumbers.235they rely on parallel training data and are not appli-cable on 9.6% of the test data for which there areno training examples.
Our method does not requireparallel training data.Agirre and Mart?
?nez (2004) show that sense dis-tribution estimation is very important for both super-vised and unsupervised WSD.
They acquire taggedexamples on a large scale by querying Google withmonosemous synonyms of the word senses in ques-tion.
They show that the method of McCarthy etal.
(2004) can be used to produce a better samplingtechnique than relying on the bias from web dataor randomly selecting the same number of exam-ples for each sense.
Our work similarly shows thatthe automatic MFS is an unsupervised alternative toSemCor but our work does not focus on samplingbut on an estimation of confidence in an automaticMFS heuristic.5 ConclusionsWe demonstrate that our variation of the McCarthyet al (2004) method for finding a MFS heuristic canbe used for estimating the entropy of a sense dis-tribution which can be exploited to boost precision.Words which are estimated as having lower entropyin general get higher precision.
This suggests thatautomatic estimation of entropy is a good criterionfor getting higher precision.
This is in agreementwith Kilgarriff and Rosenzweig (2000) who demon-strate that entropy is a good measure of the difficultyof WSD tasks, though their measure of entropy wastaken from the gold-standard distribution itself.As future work, we want to compare this approachof estimating entropy with other methods for es-timating sense distributions which do not requirehand-labelled data or parallel texts.
Currently, wedisregard local context.
We wish to couple the con-fidence in the MFS with contextual evidence and in-vestigate application on coarse-grained datasets.AcknowledgementsThis work was funded by the China Scholarship Council,the National Grant Fundamental Research 973 Programof China: Grant No.
2004CB318102, the UK EPSRCproject EP/C537262 ?Ranking Word Senses for Disam-biguation?, and a UK Royal Society Dorothy HodgkinFellowship to the second author.ReferencesE.
Agirre and D.
Mart??nez.
2004.
Unsupervised wsdbased on automatically retrieved examples: The im-portance of bias.
In Proceedings of EMNLP-2004,pages 25?32, Barcelona, Spain.E.
Briscoe and J. Carroll.
2002.
Robust accurate sta-tistical annotation of general text.
In Proceedings ofLREC-2002, pages 1499?1504, Las Palmas, CanaryIslands, Spain.Y.S.
Chan and H.T.
Ng.
2005.
Word sense disambigua-tion with distribution estimation.
In Proceedings ofIJCAI 2005, pages 1010?1015, Edinburgh, Scotland.J.
Daude?, L.
Padro?, and G. Rigau.
2000.
Mapping word-nets using structural information.
In Proceedings ofthe 38th Annual Meeting of the Association for Com-putational Linguistics, Hong Kong.J.
Jiang and D. Conrath.
1997.
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Interna-tional Conference on Research in Computational Lin-guistics, Taiwan.A.
Kilgarriff and J. Rosenzweig.
2000.
Framework andresults for english SENSEVAL.
Computers and theHumanities.
Senseval Special Issue, 34(1?2):15?48.M.
Lapata and F. Keller.
2007.
An information retrievalapproach to sense ranking.
In Proceedings of NAACL-2007, pages 348?355, Rochester.G.
Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of COLING-ACL 98, Mon-treal, Canada.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004.Finding predominant senses in untagged text.
In Pro-ceedings of ACL-2004, pages 280?287, Barcelona,Spain.S.
Mohammad and G. Hirst.
2006.
Determining wordsense dominance using a thesauru s. In Proceedings ofEACL-2006, pages 121?128, Trento, Italy.M.
Palmer, C. Fellbaum, S. Cotton, L. Delfs, andH.
Trang Dang.
2001.
English tasks: All-words andverb lexical sample.
In Proceedings of the SENSEVAL-2 workshop, pages 21?24.S.
Patwardhan and T. Pedersen.
2003.
Thewordnet::similarity package.
http://wn-similarity.sourceforge.net/.D.
Yarowsky and R. Florian.
2002.
Evaluating sensedisambiguation performance across diverse parame-ter spaces.
Natural Language Engineering, 8(4):293?310.236
