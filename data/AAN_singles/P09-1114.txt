Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1012?1020,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPMulti-Task Transfer Learning for Weakly-Supervised Relation ExtractionJing JiangSchool of Information SystemsSingapore Management University80 Stamford Road, Singapore 178902jingjiang@smu.edu.sgAbstractCreating labeled training data for rela-tion extraction is expensive.
In this pa-per, we study relation extraction in a spe-cial weakly-supervised setting when wehave only a few seed instances of the tar-get relation type we want to extract butwe also have a large amount of labeledinstances of other relation types.
Ob-serving that different relation types canshare certain common structures, we pro-pose to use a multi-task learning methodcoupled with human guidance to addressthis weakly-supervised relation extractionproblem.
The proposed framework mod-els the commonality among different re-lation types through a shared weight vec-tor, enables knowledge learned from theauxiliary relation types to be transferredto the target relation type, and allows easycontrol of the tradeoff between precisionand recall.
Empirical evaluation on theACE 2004 data set shows that the pro-posed method substantially improves overtwo baseline methods.1 IntroductionRelation extraction is the task of detecting andcharacterizing semantic relations between entitiesfrom free text.
Recent work on relation extractionhas shown that supervised machine learning cou-pled with intelligent feature engineering or ker-nel design provides state-of-the-art solutions to theproblem (Culotta and Sorensen, 2004; Zhou et al,2005; Bunescu and Mooney, 2005; Qian et al,2008).
However, supervised learning heavily re-lies on a sufficient amount of labeled data for train-ing, which is not always available in practice dueto the labor-intensive nature of human annotation.This problem is especially serious for relation ex-traction because the types of relations to be ex-tracted are highly dependent on the application do-main.
For example, when working in the financialdomain we may be interested in the employmentrelation, but when moving to the terrorism domainwe now may be interested in the ethnic and ide-ology affiliation relation, and thus have to createtraining data for the new relation type.However, is the old training data really useless?Inspired by recent work on transfer learning anddomain adaptation, in this paper, we study howwe can leverage labeled data of some old relationtypes to help the extraction of a new relation typein a weakly-supervised setting, where only a fewseed instances of the new relation type are avail-able.
While transfer learning was proposed morethan a decade ago (Thrun, 1996; Caruana, 1997),its application in natural language processing isstill a relatively new territory (Blitzer et al, 2006;Daume III, 2007; Jiang and Zhai, 2007a; Arnold etal., 2008; Dredze and Crammer, 2008), and its ap-plication in relation extraction is still unexplored.Our idea of performing transfer learning is mo-tivated by the observation that different relationtypes share certain common syntactic structures,which can possibly be transferred from the oldtypes to the new type.
We therefore propose to usea general multi-task learning framework in whichclassification models for a number of related tasksare forced to share a common model componentand trained together.
By treating classificationof different relation types as related tasks, thelearning framework can naturally model the com-mon syntactic structures among different relationtypes in a principled manner.
It also allows usto introduce human guidance in separating thecommon model component from the type-specificcomponents.
The framework naturally transfersthe knowledge learned from the old relation typesto the new relation type and helps improve the re-call of the relation extractor.
We also exploit ad-1012ditional human knowledge about the entity typeconstraints on the relation arguments, which canusually be derived from the definition of a relationtype.
Imposing these constraints further improvesthe precision of the final relation extractor.
Em-pirical evaluation on the ACE 2004 data set showsthat our proposed method largely outperforms twobaseline methods, improving the average F1 mea-sure from 0.1532 to 0.4132 when only 10 seed in-stances of the new relation type are used.2 Related workRecent work on relation extraction has been dom-inated by feature-based and kernel-based super-vised learning methods.
Zhou et al (2005) andZhao and Grishman (2005) studied various fea-tures and feature combinations for relation extrac-tion.
We systematically explored the feature spacefor relation extraction (Jiang and Zhai, 2007b) .Kernel methods allow a large set of features to beused without being explicitly extracted.
A num-ber of relation extraction kernels have been pro-posed, including dependency tree kernels (Culottaand Sorensen, 2004), shortest dependency pathkernels (Bunescu and Mooney, 2005) and more re-cently convolution tree kernels (Zhang et al, 2006;Qian et al, 2008).
However, in both feature-basedand kernel-based studies, availability of sufficientlabeled training data is always assumed.Chen et al (2006) explored semi-supervisedlearning for relation extraction using label prop-agation, which makes use of unlabeled data.Zhou et al (2008) proposed a hierarchical learningstrategy to address the data sparseness problem inrelation extraction.
They also considered the com-monality among different relation types, but com-pared with our work, they had a different problemsetting and a different way of modeling the com-monality.
Banko and Etzioni (2008) studied opendomain relation extraction, for which they man-ually identified several common relation patterns.In contrast, our method obtains common patternsthrough statistical learning.
Xu et al (2008) stud-ied the problem of adapting a rule-based relationextraction system to new domains, but the typesof relations to be extracted remain the same.Transfer learning aims at transferring knowl-edge learned from one or a number of old tasksto a new task.
Domain adaptation is a spe-cial case of transfer learning where the learn-ing task remains the same but the distributionof data changes.
There has been an increasingamount of work on transfer learning and domainadaptation in natural language processing recently.Blitzer et al (2006) proposed a structural cor-respondence learning method for domain adap-tation and applied it to part-of-speech tagging.Daume III (2007) proposed a simple feature aug-mentation method to achieve domain adaptation.Arnold et al (2008) used a hierarchical prior struc-ture to help transfer learning and domain adap-tation for named entity recognition.
Dredze andCrammer (2008) proposed an online method formulti-domain learning and adaptation.Multi-task learning is another learningparadigm in which multiple related tasks arelearned simultaneously in order to achieve betterperformance for each individual task (Caruana,1997; Evgeniou and Pontil, 2004).
Although itwas not originally proposed to transfer knowledgeto a particular new task, it can be naturally used toachieve this goal because it models the common-ality among tasks, which is the knowledge thatshould be transferred to a new task.
In our work,transfer learning is done through a multi-tasklearning framework similar to Evgeniou andPontil (2004).3 Task definitionOur study is conducted using data from the Au-tomatic Content Extraction (ACE) program1.
Wefocus on extracting binary relation instances be-tween two relation arguments occurring in thesame sentence.
Some example relation instancesand their corresponding relation types as definedby ACE can be found in Table 1.We consider the following weakly-supervisedproblem setting.
We are interested in extractinginstances of a target relation type T , but this re-lation type is only specified by a small set of seedinstances.
We may possibly have some additionalknowledge about the target type not in the form oflabeled instances.
For example, we may be giventhe entity type restrictions on the two relation ar-guments.
In addition to such limited informationabout the target relation type, we also have a largeamount of labeled instances for K auxiliary rela-tion types A1, .
.
.
,AK .
Our goal is to learn a re-lation extractor for T , leveraging all the data andinformation we have.1http://projects.ldc.upenn.edu/ace/1013Syntactic Pattern Relation Instance Relation Type (Subtype)arg-2 arg-1 Arab leaders OTHER-AFF (Ethnic)his father PER-SOC (Family)South Jakarta Prosecution Office GPE-AFF (Based-In)arg-1 of arg-2 leader of a minority government EMP-ORG (Employ-Executive)the youngest son of ex-director Suharto PER-SOC (Family)the Socialist People?s Party of Montenegro GPE-AFF (Based-In)arg-1 [verb] arg-2 Yemen [sent] planes to Baghdad ART (User-or-Owner)his wife [had] three young children PER-SOC (Family)Jody Scheckter [paced] Ferrari to both victories EMP-ORG (Employ-Staff)Table 1: Examples of similar syntactic structures across different relation types.
The head words of thefirst and the second arguments are shown in italic and bold, respectively.Before introducing our transfer learning solu-tion, let us first briefly explain our basic classifi-cation approach and the features we use, as wellas two baseline solutions.3.1 Feature configurationWe treat relation extraction as a classificationproblem.
Each pair of entities within a single sen-tence is considered a candidate relation instance,and the task becomes predicting whether or noteach candidate is a true instance of T .
We usefeature-based logistic regression classifiers.
Fol-lowing our previous work (Jiang and Zhai, 2007b),we extract features from a sequence representa-tion and a parse tree representation of each rela-tion instance.
Each node in the sequence or theparse tree is augmented by an argument tag thatindicates whether the node subsumes arg-1, arg-2, both or neither.
Nodes that represent the argu-ments are also labeled with the entity type, subtypeand mention type as defined by ACE.
Based on thefindings of Qian et al (2008), we trim the parsetree of a relation instance so that it contains onlythe most essential components.
We extract uni-gram features (consisting of a single node) and bi-gram features (consisting of two connected nodes)from the graphic representations.
An example ofthe graphic representation of a relation instanceis shown in Figure 1 and some features extractedfrom this instance are shown in Table 2.
Thisfeature configuration gives state-of-the-art perfor-mance (F1 = 0.7223) on the ACE 2004 data set ina standard setting with sufficient data for training.3.2 Baseline solutionsWe consider two baseline solutions to the weakly-supervised relation extraction problem.
In the firstNPNPB3PP1leaderNNPERofINgovernmentNNORGNPB1 0222Figure 1: The combined sequence and parse treerepresentation of the relation instance ?leader ofa minority government.?
The non-essential nodesfor ?a?
and for ?minority?
are removed based onthe algorithm from Qian et al (2008).Feature ExplanationORG2 arg-2 is an ORG entity.of0 government2 arg-2 is ?government?
andfollows the word ?of.
?NP3 ?
PP2 There is a noun phrasecontaining both arguments,with arg-2 contained in aprepositional phrase insidethe noun phrase.Table 2: Examples of unigram and bigram featuresextracted from Figure 1.baseline, we use only the few seed instances of thetarget relation type together with labeled negativerelation instances (i.e.
pairs of entities within thesame sentence but having no relation) to train abinary classifier.
In the second baseline, we takethe union of the positive instances of both the tar-get relation type and the auxiliary relation types asour positive training set, and together with the neg-ative instances we train a binary classifier.
Notethat the second baseline method essentially learns1014a classifier for any relation type.Another existing solution to weakly-supervisedlearning problems is semi-supervised learning,e.g.
bootstrapping.
However, because our pro-posed transfer learning method can be combinedwith semi-supervised learning, here we do not in-clude semi-supervised learning as a baseline.4 A multi-task transfer learning solutionWe now present a multi-task transfer learning so-lution to the weakly-supervised relation extractionproblem, which makes use of the labeled data fromthe auxiliary relation types.4.1 Syntactic similarity between relationtypesTo see why the auxiliary relation types may helpthe identification of the target relation type, let usfirst look at how different relation types may be re-lated and even similar to each other.
Based on ourinspection of a sample of the ACE data, we findthat instances of different relation types can sharecertain common syntactic structures.
For example,the syntactic pattern ?arg-1 of arg-2?
strongly in-dicates that there exists some relation between thetwo arguments, although the nature of the relationmay be well dependent on the semantic meaningsof the two arguments.
More examples are shownin Table 1.
This observation suggests that someof the syntactic patterns learned from the auxiliaryrelation types may be transferable to the target re-lation type, making it easier to learn the target rela-tion type and thus alleviating the insufficient train-ing data problem with the target type.How can we incorporate this desired knowledgetransfer process into our learning method?
Whileone can make explicit use of these general syntac-tic patterns in a rule-based relation extraction sys-tem, here we restrict our attention to feature-basedlinear classifiers.
We note that in feature-based lin-ear classifiers, a useful syntactic pattern is trans-lated into large weights for features related to thesyntactic pattern.
For example, if ?arg-1 of arg-2?is a useful pattern, in the learned linear classifierwe should have relatively large weights for fea-tures such as ?the word of occurs before arg-2?
or?a preposition occurs before arg-2,?
or even morecomplex features such as ?there is a prepositionalphrase containing arg-2 attached to arg-1.?
It isthe weights of these generally useful features thatare transferable from the auxiliary relation typesto the target relation type.4.2 Statistical learning modelAs we have discussed, we want to force the linearclassifiers for different relation types to share theirmodel weights for those features that are relatedto the common syntactic patterns.
Formally, weconsider the following statistical learning model.Let ?k denote the weight vector of the linearclassifier that separates positive instances of aux-iliary type Ak from negative instances, and let ?Tdenote a similar weight vector for the target typeT .
If different relation types are totally unrelated,these weight vectors should also be independent ofeach other.
But because we observe similar syn-tactic structures across different relation types, wenow assume that these weight vectors are relatedthrough a common component ?
:?T = ?T + ?,?k = ?k + ?
for k = 1, .
.
.
,K.If we assume that only weights of certain gen-eral features can be shared between different rela-tion types, we can force certain dimensions of ?
tobe 0.
We express this constraint by introducing amatrix F and setting F?
= 0.
Here F is a squarematrix with all entries set to 0 except that Fi,i = 1if we want to force ?i = 0.Now we can learn these weight vectors in amulti-task learning framework.
Let x representthe feature vector of a candidate relation instance,and y ?
{+1,?1} represent a class label.
LetDT = {(xTi , yTi )}NTi=1 denote the set of labeledinstances for the target type T .
(Note that thenumber of positive instances in DT is very small.
)And let Dk = {(xki , yki )}Nki=1 denote the labeledinstances for the auxiliary type Ak.We learn the optimal weight vectors {??k}Kk=1,?
?T and ??
by optimizing the following objectivefunction:({?
?k}Kk=1, ?
?T , ??
)= argmin{?k},?T ,?,F?=0[L(DT , ?T + ?
)+K?k=1L(Dk, ?k + ?)+?T?
?
?T ?2 +K?k=1?k??
?k?2 + ?????2].
(1)1015The objective function follows standard empir-ical risk minimization with regularization.
HereL(D, ?)
is the aggregated loss of labeling x withy for all (x, y) in D, using weight vector ?.
Inlogistic regression models, the loss function is thenegative log likelihood, that is,L(D, ?)
= ??
(x,y)?Dlog p(y|x, ?
),p(y|x, ?)
= exp(?y ?
x)?y??
{+1,?1} exp(?y?
?
x).?T?
, ?k?
and ??
are regularization parameters.By adjusting their values, we can control the de-gree of weight sharing among the relation types.The larger the ratio ?T?
/??
(or ?k?/??)
is, the morewe believe that the model for T (or Ak) shouldconform to the common model, and the smallerthe type-specific weight vector ?T (or ?k) will be.The model presented above is based on our pre-vious work (Jiang and Zhai, 2007c), which bearsthe same spirit of some other recent work on multi-task learning (Ando and Zhang, 2005; Evgeniouand Pontil, 2004; Daume III, 2007).
It is generalfor any transfer learning problem with auxiliary la-beled data from similar tasks.
Here we are mostlyinterested in the model?s applicability and effec-tiveness on the relation extraction problem.4.3 Feature separationRecall that we impose a constraint F?
= 0 whenoptimizing the objective function.
This constraintgives us the freedom to force only the weights of asubset of the features to be shared among differentrelation types.
A remaining question is how to setthis matrix F , that is, how to determine the set ofgeneral features to use.
We propose two ways ofsetting this matrix F .Automatically setting FOne way is to fix the number of non-zero entriesin ?
to be a pre-defined number H of general fea-tures, and allow F to change during the optimiza-tion process.
This can be done by repeating thefollowing two steps until F converges:1.
Fix F , and optimize the objective function asin Equation (1).2.
Fix (?T + ?)
and (?k + ?
), and search for?T , {?k} and ?
that minimizes (?T?
?
?T ?2 +?Kk=1 ?k??
?k?2 + ????
?2), subject to theconstraint that at most H entries of ?
are non-zero.Human guidanceAnother way to select the general features is to fol-low some guidance from human knowledge.
Re-call that in Section 4.1 we find that the common-ality among different relation types usually liesin the syntactic structures between the two ar-guments.
This observation gives some intuitionabout how to separate general features from type-specific features.
In particular, here we considertwo hypotheses regarding the generality of differ-ent kinds of features.Argument word features: We hypothesize thatthe head words of the relation arguments are morelikely to be strong indicators of specific relationtypes rather than any relation type.
For example, ifan argument has the head word ?sister,?
it stronglyindicates a family relation.
We refer to the set offeatures that contain any head word of an argu-ment as ?arg-word?
features.Entity type features: We hypothesize that theentity types and subtypes of the relation argumentsare also more likely to be associated with specificrelation types.
For example, arguments that arelocation entities may be strongly correlated withphysical proximity relations.
We refer to the set offeatures that contain the entity type or subtype ofan argument as ?arg-NE?
features.We hypothesize that the arg-word and arg-NEfeatures are type-specific and therefore should beexcluded from the set of general features.
Wecan force the weights of these hypothesized type-specific features to be 0 in the shared weight vec-tor ?, i.e.
we can set the matrix F to achieve thisfeature separation.Combined methodWe can also combine the automatic way of settingF with human guidance.
Specifically, we still fol-low the first automatic procedure to choose gen-eral features, but we then filter out any hypothe-sized type-specific feature from the set of generalfeatures chosen by the automatic procedure.4.4 Imposing entity type constraintsFinally, we consider how we can exploit additionalhuman knowledge about the target relation type Tto further improve the classifier.
We note that usu-ally when a relation type is defined, we often havestrong preferences or even hard constraints on thetypes of entities that can possibly be the two rela-tion arguments.
These type constraints can help us1016Target Type T BL BL-A TL-auto TL-guide TL-comb TL-NEP 0.0000 0.1692 0.2920 0.2934 0.3325 0.5056Physical R 0.0000 0.0848 0.1696 0.1722 0.2383 0.2316F 0.0000 0.1130 0.2146 0.2170 0.2777 0.3176Personal P 1.0000 0.0804 0.1005 0.3069 0.3214 0.6412/Social R 0.0386 0.1708 0.1598 0.7245 0.7686 0.7631F 0.0743 0.1093 0.1234 0.4311 0.4533 0.6969Employment P 0.9231 0.3561 0.5230 0.5428 0.5973 0.7145/Membership R 0.0075 0.1850 0.2617 0.2648 0.3632 0.3601/Subsidiary F 0.0148 0.2435 0.3488 0.3559 0.4518 0.4789Agent- P 0.8750 0.0603 0.1813 0.1825 0.1835 0.1967Artifact R 0.0343 0.2353 0.6471 0.6225 0.6422 0.6373F 0.0660 0.0960 0.2833 0.2822 0.2854 0.3006PER/ORG P 0.8889 0.0838 0.1510 0.1592 0.1667 0.1844Affiliation R 0.0567 0.4965 0.6950 0.8369 0.8794 0.8723F 0.1067 0.1434 0.2481 0.2676 0.2802 0.3045GPE P 1.0000 0.2530 0.3904 0.3604 0.3560 0.5824Affiliation R 0.0077 0.4509 0.6416 0.5992 0.6166 0.6127F 0.0153 0.3241 0.4854 0.4501 0.4513 0.5972P 1.0000 0.0298 0.0503 0.0471 0.1370 0.1370Discourse R 0.0036 0.0789 0.1075 0.1147 0.3477 0.3477F 0.0071 0.0433 0.0685 0.0668 0.1966 0.1966P 0.8124 0.1475 0.2412 0.2703 0.2992 0.4231Average R 0.0212 0.2432 0.3832 0.4764 0.5509 0.5464F 0.0406 0.1532 0.2532 0.2958 0.3423 0.4132Table 3: Comparison of different methods on ACE 2004 data set.
P, R and F stand for precision, recalland F1, respectively.remove some false positive instances.
We there-fore manually identify the entity type constraintsfor each target relation type based on the defini-tion of the relation type given in the ACE annota-tion guidelines, and impose these type constraintsas a final refinement step on top of the predictedpositive instances.5 Experiments5.1 Data set and experiment setupWe used the ACE 2004 data set to evaluate ourproposed methods.
There are seven relation typesdefined in ACE 2004.
After data cleaning, we ob-tained 4290 positive instances among 48614 can-didate relation instances.
We took each relationtype as the target type and used the remainingtypes as auxiliary types.
This gave us seven setsof experiments.
In each set of experiments for asingle target relation type, we randomly dividedall the data into five subsets, and used each subsetfor testing while using the other four subsets fortraining, i.e.
each experiment was repeated fivetimes with different training and test sets.
Eachtime, we removed most of the positive instancesof the target type from the training set except onlya small number S of seed instances.
This gaveus the weakly-supervised setting.
We kept all thepositive instances of the target type in the test set.In order to concentrate on the classification accu-racy for the target relation type, we removed thepositive instances of the auxiliary relation typesfrom the test set, although in practice we needto extract these auxiliary relation instances usinglearned classifiers for these relation types.5.2 Comparison of different methodsWe first show the comparison of our proposedmulti-task transfer learning methods with the twobaseline methods described in Section 3.2.
Theperformance on each target relation type and theaverage performance across seven types are shownin Table 3.
BL refers to the first baseline and BL-A refers to the second baseline which uses auxil-1017?T?
100 1000 10000P 0.6265 0.3162 0.2992R 0.1170 0.3959 0.5509F 0.1847 0.2983 0.3423Table 4: The average performance of TL-combwith different ?T?
.
(?k?
= 104 and ??
= 1.
)iary relation instances.
The four TL methods areall based on the multi-task transfer learning frame-work.
TL-auto sets F automatically within theoptimization problem itself.
TL-guide chooses allfeatures except arg-word and arg-NE features asgeneral features and sets F accordingly.
TL-combcombines TL-auto and TL-guide, as described inSection 4.3.
Finally, TL-NE builds on top of TL-comb and uses the entity type constraints to re-fine the predictions.
In this set of experiments,the number of seed instances for each target re-lation type was set to 10.
The parameters wereset to their optimal values (?T?
= 104, ?k?
= 104,??
= 1, and H = 500).As we can see from the table, first of all, BLgenerally has high precision but very low recall.BL-A performs better than BL in terms of F1 be-cause it gives better recall.
However, BL-A stillcannot achieve as high recall as the TL methods.This is probably because the model learned by BL-A still focuses more on type-specific features foreach relation type rather than on the commonlyuseful general features, and therefore does nothelp much in classifying the target relation type.The four TL methods all outperform the twobaseline methods.
TL-comb performs better thanboth TL-auto and TL-guide, which shows thatwhile we can either choose general features au-tomatically by the learning algorithm or manu-ally with human knowledge, it is more effectiveto combine human knowledge with the multi-tasklearning framework.
Not surprisingly, TL-NE im-proves the precision over TL-comb without hurt-ing the recall much.
Ideally, TL-NE should notdecrease recall if the type constraints are strictlyobserved in the data.
We find that it is not alwaysthe case with the ACE data, leading to the smalldecrease of recall from TL-comb to TL-NE.5.3 The effect of ?T?Let us now take a look at the effect of using dif-ferent ?T?
.
As we can see from Table 4, smaller?T?
gives higher precision while larger ?T?
gives0.10.150.20.250.30.350.40.450.5100  1000  10000avgF1HTL-combTL-autoBL-AFigure 2: Performance of TL-comb and TL-autoas H changes.higher recall.
These results make sense becausethe larger ?T?
is, the more we penalize largeweights of ?T .
As a result, the model for the tar-get type is forced to conform to the shared model?
and prevented from overfitting the few seed tar-get instances.
?T?
is therefore a useful parameterto help us control the tradeoff between precisionand recall for the target type.While varying ?k?
also gives similar effect fortypeAk, we found that setting ?k?
to smaller valueswould not help T because in this case the auxiliaryrelation instances would be used more for train-ing the type-specific component ?k rather than thecommon component ?.5.4 Sensitivity of HAnother parameter in the multi-task transfer learn-ing framework is the number of general featuresH , i.e.
the number of non-zero entries in theshared weight vector ?.
To see how the perfor-mance may vary as H changes, we plot the per-formance of TL-comb and TL-auto in terms of theaverage F1 across the seven target relation types,with H ranging from 100 to 50000.
As we can seein Figure 2, the performance is relatively stable,and always above BL-A.
This suggests that theperformance of TL-comb and TL-auto is not verysensitive to the value of H .5.5 Hypothesized type-specific featuresIn Section 4.3, we showed two sets of hypoth-esized type-specific features, namely, arg-wordfeatures and arg-NE features.
We also experi-mented with each set separately to see whetherboth sets are useful.
The comparison is shown inTable 5.
As we can see, using either set of type-specific features in either TL-guide or TL-combcan improve the performance over BL-A, but the1018arg-word arg-NE unionTL-guide 0.2095 0.2983 0.2958TL-comb 0.2215 0.3331 0.3423BL-A 0.1532Table 5: Average F1 using different hypothesizedtype-specific features.00.10.20.30.40.50.610  100  1000avgF1STL-NE (104)TL-NE (102)BLBL-AFigure 3: Performance of TL-NE, BL and BL-Aas the number of seed instances S of the target typeincreases.
(H = 500.
?T?
was set to 104 and 102).arg-NE features are probably more type-specificthan arg-word features because they give betterperformance.
Using the union of the two sets isstill the best for TL-comb.5.6 Changing the number of seed instancesFinally, we compare TL-NE with BL and BL-Awhen the number of seed instances increases.
Weset S from 5 up to 1000.
When S is large, theproblem becomes more like traditional supervisedlearning, and our setting of ?T?
= 104 is no longeroptimal because we are now not afraid of overfit-ting the large set of seed target instances.
There-fore we also included another TL-NE experimentwith ?T?
set to 102.
The comparison of the perfor-mance is shown in Figure 3.
We see that as S in-creases, both BL and BL-A catch up, and BL over-takes BL-A when S is sufficiently large becauseBL uses positive training examples only from thetarget type.
Overall, TL-NE still outperforms thetwo baselines in most of the cases over the widerange of values of S, but the optimal value for ?T?decreases as S increases, as we have suspected.The results show that if ?T?
is set appropriately,our multi-task transfer learning method is robustand advantageous over the baselines under boththe weakly-supervised setting and the traditionalsupervised setting.6 Conclusions and future workIn this paper, we applied multi-task transfer learn-ing to solve a weakly-supervised relation extrac-tion problem, leveraging both labeled instances ofauxiliary relation types and human knowledge in-cluding hypotheses on feature generality and en-tity type constraints.
In the multi-task learningframework that we introduced, different relationtypes are treated as different but related tasks thatare learned together, with the common structuresamong the relation types modeled by a sharedweight vector.
The shared weight vector corre-sponds to the general features across different re-lation types.
We proposed to choose the generalfeatures either automatically inside the learning al-gorithm or guided by human knowledge.
We alsoleveraged additional human knowledge about thetarget relation type in the form of entity type con-straints.
Experiment results on the ACE 2004 datashow that the multi-task transfer learning methodachieves the best performance when we combinehuman guidance with automatic general featureselection, followed by imposing the entity typeconstraints.
The final method substantially outper-forms two baseline methods, improving the aver-age F1 measure from 0.1532 to 0.4132 when only10 seed target instances are used.Our work is the first to explore transfer learningfor relation extraction, and we have achieved verypromising results.
Because of the practical impor-tance of transfer learning and adaptation for rela-tion extraction due to lack of training data in newdomains, we hope our study and findings will leadto further investigation into this problem.
Thereare still many issues that remain unsolved.
For ex-ample, we have not looked at the degrees of re-latedness between different pairs of relation types.Presumably, when adapting to a specific target re-lation type, we want to choose the most similarauxiliary relation types to use.
Our current studyis based on ACE relation types.
It would also beinteresting to study similar problems in other do-mains, for example, the protein-protein interactionextraction problem in biomedical text mining.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
Journal of MachineLearning Research, 6:1817?1853, November.1019Andrew Arnold, Ramesh Nallapati, and William W.Cohen.
2008.
Exploiting feature hierarchy fortransfer learning in named entity recognition.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics, pages 245?253.Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 28?36.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 120?128.Razvan Bunescu and Raymond Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages724?731.Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28:41?75.Jinxiu Chen, Donghong Ji, Chew Lim Tan, andZhengyu Niu.
2006.
Relation extraction using la-bel propagation based semi-supervised learning.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 129?136.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof the 42nd Meeting of the Association for Compu-tational Linguistics, pages 423?429.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics, pages 256?263.Mark Dredze and Koby Crammer.
2008.
Onlinemethods for multi-domain learning and adaptation.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages689?697.Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi-task learning.
In Proceedings ofthe 10th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 109?117.Jing Jiang and ChengXiang Zhai.
2007a.
Instanceweighting for domain adaptation in nlp.
In Proceed-ings of the 45th Annual Meeting of the Associationfor Computational Linguistics, pages 264?271.Jing Jiang and ChengXiang Zhai.
2007b.
A systematicexploration of the feature space for relation extrac-tion.
In Proceedings of the Human Language Tech-nologies Conference, pages 113?120.Jing Jiang and ChengXiang Zhai.
2007c.
A two-stageapproach to domain adaptation for statistical classi-fiers.
In Proceedings of the 16th ACM Conferenceon Information and Knowledge Management, pages401?410.Longhua Qian, Guodong Zhou, Fang Kong, Qiaom-ing Zhu, and Peide Qian.
2008.
Exploiting con-stituent dependencies for tree kernel-based semanticrelation extraction.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics, pages 697?704.Sebastian Thrun.
1996.
Is learning the n-th thing anyeasier than learning the first?
In Advances in NeuralInformation Processing Systems 8, pages 640?646.Feiyu Xu, Hans Uszkoreit, Hong Li, and Niko Felger.2008.
Adaptation of relation extraction rules to newdomains.
In Proceedings of the 6th InternationalConference on Language Resources and Evaluation,pages 2446?2450.Min Zhang, Jie Zhang, and Jian Su.
2006.
Exploringsyntactic features for relation extraction using a con-volution tree kernel.
In Proceedings of the HumanLanguage Technology Conference, pages 288?295.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, pages 419?426.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, pages 427?434.GuoDong Zhou, Min Zhang, DongHong Ji, andQiaoMing Zhu.
2008.
Hierarchical learning strat-egy in semantic relation extraction.
InformationProcessing and Management, 44(3):1008?1021.1020
