NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 100?104,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHierarchical clustering of word class distributionsGrzegorz Chrupa?agchrupala@lsv.uni-saarland.deSpoken Language SystemsSaarland UniversityAbstractWe propose an unsupervised approach toPOS tagging where first we associate eachword type with a probability distribution overword classes using Latent Dirichlet Alloca-tion.
Then we create a hierarchical cluster-ing of the word types: we use an agglomer-ative clustering algorithm where the distancebetween clusters is defined as the Jensen-Shannon divergence between the probabilitydistributions over classes associated with eachword-type.
When assigning POS tags, we findthe tree leaf most similar to the current wordand use the prefix of the path leading to thisleaf as the tag.
This simple labeler outper-forms a baseline based on Brown clusters on9 out of 10 datasets.1 IntroductionUnsupervised induction of word categories has beenapproached from three broad perspectives.
First, it isof interest to cognitive scientists who model syntac-tic category acquisition by children (Redington et al1998, Mintz 2003, Parisien et al 2008, Chrupa?a andAlishahi 2010), where the primary concern is match-ing human performance patterns and satisfying cog-nitively motivated constraints such as incrementallearning.Second, learning categories has been cast asunsupervised part-of-speech tagging task (recentwork includes Ravi and Knight (2009), Lee et al(2010), Lamar et al (2010), Christodoulopouloset al (2011)), and primarily motivated as useful fortagging under-resourced languages.Finally, learning categories has also been re-searched from the point of view of feature learning,where the induced categories provide an interme-diate level of representation, abstracting away andgeneralizing over word form features in an NLP ap-plication (Brown et al 1992, Miller et al 2004, Linand Wu 2009, Turian et al 2010, Chrupala 2011,Ta?ckstro?m et al 2012).
The main difference fromthe part-of-speech setting is that the focus is on eval-uating the performance of the learned categories inreal tasks rather than on measuring how closely theymatch gold part-of-speech tags.
Some researchershave used both approaches to evaluation.This difference in evaluation methodology alsonaturally leads to differing constraints on the natureof the induced representations.
For part-of-speechtagging what is needed is a mapping from word to-kens to a small set of discrete, atomic labels.
Forfeature learning, there are is no such limitation, andother types of representations have been used, suchas low-dimensional continuous vectors learned byneural network language models as in Bengio et al(2006), Mnih and Hinton (2009), or distributionsover word classes learned using Latent Dirichlet Al-location as in Chrupala (2011).In this paper we propose a simple method of map-ping distributions over word classes to a set of dis-crete labels by hierarchically clustering word classdistributions using Jensen-Shannon divergence as adistance metric.
This allows us to effectively usethe algorithm of Chrupala (2011) and similar ones insettings where using distributions directly is not pos-sible or desirable.
Equivalently, our approach canbe seen as a generic method to convert a soft clus-tering to hard clustering while conserving much ofthe information encoded in the original soft clusterassignments.
We evaluate this method on the unsu-pervised part-of-speech tagging task on ten datasets100in nine languages as part of the shared task at theNAACL-HLT 2012 Workshop on Inducing Linguis-tic Structure.2 ArchitectureOur system consists of the following components (i)a soft word-class induction model (ii) a hierarchi-cal clustering algorithm which builds a tree of wordclass distributions (iii) a labeler which for each wordtype finds the leaf in the tree with the most similarword-class distribution and outputs a prefix of thepath leading to that leaf.2.1 Soft word-class modelWe use the probabilistic soft word-class model pro-posed by Chrupala (2011), which is based on LatentDirichlet Allocation (LDA).
LDA was introduced byBlei et al (2003) and applied to modeling the topicstructure in document collections.
It is a generative,probabilistic hierarchical Bayesian model which in-duces a set of latent variables, which correspond tothe topics.
The topics themselves are multinomialdistributions over words.The generative structure of the LDA model is thefollowing:?k ?
Dirichlet(?
), k ?
[1,K]?d ?
Dirichlet(?
), d ?
[1, D]znd ?
Categorical(?d), nd ?
[1, Nd]wnd ?
Categorical(?znd ), nd ?
[1, Nd](1)Chrupala (2011) interprets the LDA model interms of word classes as follows: K is the numberof classes, D is the number of unique word types,Nd is the number of context features (such as rightor left neighbor) associated with word type d, zndis the class of word type d in the nthd context, andwnd is the nthd context feature of word type d. Hy-perparameters ?
and ?
control the sparseness of thevectors ?d and ?k.Inference in LDA in general can be performed us-ing either variational EM or Gibbs sampling.
Herewe use a collapsed Gibbs sampler to estimate twosets of parameters: the ?d parameters correspondto word class probability distributions given a wordtype while the ?k correspond to feature distributionsgiven a word class.
In the current paper we focuson ?d which we use to represent a word type d as adistribution over word classes.Soft word classes are more expressive than hardcategories.
They make it easy and efficient to ex-press shared ambiguities: Chrupala (2011) gives anexample of words used as either first names or sur-names, where this shared ambiguity is reflected inthe similarity of their word class distributions.Another important property of soft word classesis that they make it easy to express graded similar-ity between words types.
With hard classes, a pairof words either belong to the same class or to differ-ent classes, i.e.
similarity is a binary indicator.
Withsoft word classes, we can use standard measures ofsimilarity between probability distributions to judgehow similar words are to each other.
We take advan-tage of this feature to build a hierarchical clusteringof word types.2.2 Hierarchical clustering of word typesIn some settings, e.g.
in the unsupervised part-of-speech tagging scenario, words should be labeledwith a small set of discrete labels.
The question thenarises how to map a probability distribution overword classes corresponding to each word type in thesoft word class setting to a discrete label.
The mostobvious method would be to simply output the high-est scoring word class, but this has the disadvantageof discarding much of the information present in thesoft labeling.What we do instead is to create a hierarchicalclustering of word types using the Jensen-Shannon(JS) divergence between the word-class distribu-tions as a distance function.
JS divergence is aninformation-theoretic measure of dissimilarity be-tween two probability distributions (Lin 1991).
Itis defined as follows:JS (P,Q) =12(DKL (P,M) +DKL (Q,M)) (2)where M is the mean distribution P+Q2 and DKL isthe Kullback-Leibler (KL) divergence:DKL(P,Q) =?iP (i) log2P (i)Q(i)(3)Unlike KL divergence, JS divergence is symmetricand is defined for any pair of discrete probability dis-tributions over the same domain.101We use a simple agglomerative clustering algo-rithm to build a tree hierarchy over the word classdistributions corresponding to word types (see Al-gorithm 1).
We start with a set of leaf nodes, one foreach of D word types, containing the unnormalizedword-class probabilities for the corresponding wordtype: i.e.
the co-occurrence counts of word-type andword-class, n(z, d), output by the Gibbs sampler.We then merge that pair of nodes (P,Q) whose JSdivergence is the smallest, remove these two nodesfrom the set, and add the new merged node with twobranches.
We proceed in this fashion until we obtaina single root node.When merging two nodes we sum their co-occurrence count tables: thus the nodes always con-tain unnormalized probabilities which are normal-ized only when computing JS scores.Algorithm 1 Bottom-up clustering of word typesS = {n(?, d) | d ?
[1, D]}while |S| > 1 do(P,Q) = argmin(P,Q)?S?S JS (P,Q)S ?
S \ {P,Q} ?
{merge(P,Q)}The algorithm is simple but not very efficient: ifimplemented carefully it can be at best quadratic inthe number of word types.
However, in practice itis unnecessary to run it on more than a few hun-dred word types which can be done very quickly.
Inthe experiments reported on below we build the treebased only on the 1000 most frequent words.Figure 1 shows two small fragments of a hierar-chy built from 200 most frequent words of the En-glish CHILDES dataset using 10 LDA word classes.2.3 Tree paths as labelsOnce the tree is built, it can be used to assign a labelto any word which has an associated word class dis-tribution.
In principle, it could be used to performeither type-level or token-level tagging: token-leveldistributions could be composed from the distribu-tions associated with current word type (?)
and thedistributions associated with the current context fea-tures (?).
Since preliminary experiments with token-level tagging were not successful, here we focus ex-clusively on type-level tagging.Given the tree and a word-type paired with a classdistribution, we generate a path to a leaf in the treeDaddyMommyPaulFraseritthatthesethose?llgoinggoin(g)couldcanFigure 1: Two fragments of a hierarchy over word classdistributionsas follows.
If the word is one of the ones used toconstruct the tree, we simply record the path fromthe root to the leaf containing this word.
If the wordis not at any of the leaves (i.e.
it is not one of the1000 most frequent words), we traverse the tree, ateach node comparing the JS divergence between theword and the left and right branches, and then de-scend along the branch for which JS is smaller.
Werecord the path until we reach a leaf node.We can control the granularity of the labeling byvarying the length of the prefix of the path from theroot to the leaf.3 ExperimentsWe evaluate our method on the unsupervised part-of-speech tagging task on ten dataset in nine lan-guages as part of the shared task.For each dataset we run LDA word class induc-tion1 on the union of the unlabeled sentences in thetrain, development and test sets, setting the num-ber of classes K ?
{10, 20, 40, 80}, and build ahierarchy on top of the learned word-class proba-bility distributions as explained above.
We then la-bel the development set using path prefixes of lengthL ?
{8, 9, .
.
.
, 20} for each of the trees, and record1We ran 200 Gibbs sampling passes, and set the LDA hyper-parameters to ?
= 10K and ?
= 0.1.102Dataset K L Brown HCDArabic 40 13 39.6 51.4Basque 40 16 39.5 48.3Czech 80 8 42.1 42.4Danish 40 19 50.2 56.8Dutch 40 10 43.3 54.8English CH 10 12 64.1 67.8English PTB 40 8 61.6 60.2Portuguese 80 10 51.7 52.4Slovene 80 19 44.5 46.6Swedish 20 17 51.8 56.1Table 1: Evaluation of coarse-grained POS tagging ontest dataDataset K L Brown HCDArabic 40 13 42.2 52.9Basque 40 16 38.5 54.4Czech 40 19 45.3 46.8Danish 40 20 49.2 63.6Dutch 20 12 49.4 53.4English CH 10 12 66.0 78.2English PTB 80 14 62.0 61.3Portuguese 80 11 52.9 54.7Slovene 80 20 45.8 51.9Swedish 20 17 51.8 56.1Table 2: Evaluation of coarse-grained POS tagging ontest datathe V-measure (Rosenberg and Hirschberg 2007)against gold part-of-speech tags.
We choose thebest-performing pair ofK and L and use this settingto label the test set.
We tune separately for coarse-grained and fine-grained POS tags.
Other than usingthe development set labels to tune these two param-eters our system is unsupervised and uses no dataother than the sentences in the provided data files.Table 1 and Table 2 show the best settings forthe coarse- and fine-grained POS tagging for all thedatasets, and the V-measure scores on the test setachieved by our labeler (HCD for Hierarchy overClass Distributions).
Also included are the scores ofthe official baseline, i.e.
labeling with Brown clus-ters (Brown et al 1992), with the number of clustersset to match the number of POS tags in each dataset.The best K stays the same when increasing thegranularity in the majority of cases (7 out of 10).On the CHILDES dataset of child-directed speech,llllllllll0 20 40 60 80 100 1200.000.050.100.150.20Vocabulary size in thousandsError Reductionareuczdanlen?chen?ptbptslsvFigure 2: Error reduction as a function of vocabulary sizewhich has the smallest vocabulary of all, the optimalnumber of LDA classes is also the smallest (10).
Asexpected, the best path prefix length L is typicallylarger for the fine-grained labeling.Our labels outperform the baseline on 9 out of 10datasets, for both levels of granularity.
The only ex-ception is the English Penn Treebank dataset, wherethe HCD V-measure scores are slightly lower thanBrown cluster scores.
This may be taken as an il-lustration of the danger arising if NLP systems areexclusively evaluated on a single dataset: such adataset may well prove to not be very representative.Part of the story seems to be that our methodtends to outperform the baseline by larger marginson datasets with smaller vocabularies2.
The scatter-plot in Figure 2 illustrates this tendency for coarse-grained POS tagging: Pearson?s correlation is ?0.6.4 ConclusionWe have proposed a simple method of convert-ing a set of soft class assignments to a set of dis-crete labels by building a hierarchical clustering overword-class distributions associated with word types.This allows to use the efficient and effective LDA-based word-class induction method in cases where ahard clustering is required.
We have evaluated this2We suspect performance on datasets with large vocabular-ies could be improved by increasing the number of frequentwords used to build the word-type hierarchy; due to time con-straints we had to postpone verifying it.103method on the POS tagging task on which our ap-proach outperforms a baseline based on Brown clus-ters in 9 out of 10 cases, often by a substantial mar-gin.In future it would be interesting to investigatewhether the hierarchy over word-class distributionswould also be useful as a source of features in asemi-supervised learning scenario, instead, or in ad-dition to using word-class probabilities as featuresdirectly.
We would also like to revisit and further in-vestigate the challenging problem of token-level la-beling.ReferencesBengio, Y., Schwenk, H., Sene?cal, J., Morin, F.,and Gauvain, J.
(2006).
Neural Probabilistic Lan-guage Models.
Innovations in Machine Learning,pages 137?186.Blei, D., Ng, A., and Jordan, M. (2003).
La-tent dirichlet alocation.
The Journal of MachineLearning Research, 3:993?1022.Brown, P. F., Mercer, R. L., Della Pietra, V. J., andLai, J. C. (1992).
Class-based n-gram modelsof natural language.
Computational Linguistics,18(4):467?479.Christodoulopoulos, C., Goldwater, S., and Steed-man, M. (2011).
A bayesian mixture model forpart-of-speech induction using multiple features.In EMNLP.Chrupala, G. (2011).
Efficient induction of proba-bilistic word classes with LDA.
In IJCNLP.Chrupa?a, G. and Alishahi, A.
(2010).
OnlineEntropy-based Model of Lexical Category Acqui-sition.
In CoNLL.Lamar, M., Maron, Y., Johnson, M., and Bienen-stock, E. (2010).
Svd and clustering for unsuper-vised pos tagging.
In ACL.Lee, Y., Haghighi, A., and Barzilay, R. (2010).Simple type-level unsupervised pos tagging.
InEMNLP.Lin, D. and Wu, X.
(2009).
Phrase clustering fordiscriminative learning.
In ACL/IJCNLP.Lin, J.
(1991).
Divergence measures based onthe shannon entropy.
Information Theory, IEEETransactions on, 37(1):145?151.Miller, S., Guinness, J., and Zamanian, A.
(2004).Name tagging with word clusters and discrimina-tive training.
In HLT/NAACL.Mintz, T. (2003).
Frequent frames as a cue for gram-matical categories in child directed speech.
Cog-nition, 90(1):91?117.Mnih, A. and Hinton, G. (2009).
A scalable hierar-chical distributed language model.
In NIPS.Parisien, C., Fazly, A., and Stevenson, S. (2008).
Anincremental bayesian model for learning syntacticcategories.
In CoNLL.Ravi, S. and Knight, K. (2009).
Minimized mod-els for unsupervised part-of-speech tagging.
InACL/IJCNLP.Redington, M., Crater, N., and Finch, S. (1998).
Dis-tributional information: A powerful cue for ac-quiring syntactic categories.
Cognitive Science:A Multidisciplinary Journal, 22(4):425?469.Rosenberg, A. and Hirschberg, J.
(2007).
V-measure: A conditional entropy-based externalcluster evaluation measure.
In EMNLP/CoNLL.Turian, J., Ratinov, L., and Bengio, Y.
(2010).
Wordrepresentations: A simple and general method forsemi-supervised learning.
In ACL.Ta?ckstro?m, O., McDonald, R., and Uszkoreit, J.(2012).
Cross-lingual word clusters for directtransfer of linguistic structure.
In NAACL.104
