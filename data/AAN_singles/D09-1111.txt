Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1066?1075,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPDiscriminative Substring Decoding for TransliterationColin Cherry and Hisami SuzukiMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052{colinc,hisamis}@microsoft.comAbstractWe present a discriminative substring de-coder for transliteration.
This decoderextends recent approaches for discrimi-native character transduction by allow-ing for a list of known target-languagewords, an important resource for translit-eration.
Our approach improves uponSherif and Kondrak?s (2007b) state-of-the-art decoder, creating a 28.5% relative im-provement in transliteration accuracy ona Japanese katakana-to-English task.
Wealso conduct a controlled comparison oftwo feature paradigms for discriminativetraining: indicators and hybrid generativefeatures.
Surprisingly, the generative hy-brid outperforms its purely discriminativecounterpart, despite losing access to richsource-context features.
Finally, we showthat machine transliterations have a posi-tive impact on machine translation quality,improving human judgments by 0.5 on a4-point scale.1 IntroductionTransliteration occurs when a word is borrowedinto a language with a different character set.The word is transcribed into the new characterset in such a way as to maintain rough phoneticcorrespondence; for example, the English wordhip-hop becomes 2#7;#7 [hippuhoppu],when transliterated into Japanese.
A task fre-quently of interest to the NLP community is back-transliteration, where one seeks the original word,given the borrowed form.We investigate machine transliteration as amethod to handle out-of-vocabulary items in aJapanese-to-English translation system.
Moreoften than not, this will correspond to back-transliteration.
Our goal is to prevent the copy-ing or deletion of Japanese words when they aremissing from our statistical machine translation(SMT) system?s translation tables.
This can havea substantial impact on the quality of SMT output,transforming translations of questionable useful-ness, such as:Avoid using a5JAK account.1into the far more informative:Avoid using a Freemail account.Though the techniques we present here arelanguage-independent, we focus this study onthe task of Japanese katakana-to-English back-transliteration.
Katakana is one of the four char-acter types used in the Japanese writing system(along with hiragana, kanji and Roman alpha-bet), consisting of about 50 syllabic characters.It is used primarily to spell foreign loanwords(e.g., !GL( [chokoreeto] ?
chocolate),and names (e.g., JS(S [kurinton] ?
Clin-ton).
Therefore, katakana is a strong indicatorthat a Japanese word can be back-transliterated.However, katakana can also be used to spell sci-entific names of animals and plants (e.g., B[kamo] ?
duck), onomatopoeic expressions (e.g.,0C0C [bashabasha] ?
splash) and for-eign origin words that are not transliterations (e.g.,;! [hochikisu] ?
stapler).
These un-transliterable cases constitute about 10% of thekatakana words in our data.We employ a discriminative substring decoderfor machine transliteration.
Following Sherif andKondrak (2007b), the decoder operates on shortsource substrings, with each operation producingone or more target characters, as shown in Fig-ure 1.
However, where previous approaches em-ploy generative modeling, we use structured per-ceptron training to discriminatively tune parame-ters according to 0-1 transliteration accuracy.
This15JAK is romanized as [furiimeeru]1066?
?
?
?tho m sonFigure 1: Example substring derivationallows us to test novel methods for the use of tar-get lexicons in discriminative character transduc-tion, allowing our decoder to benefit from a list ofknown target words.
Perhaps more significantly,our framework allows us to test two competingstyles of features:?
sparse indicators, designed to capture thesame channel and language modeling datacollected by previous generative models, and?
components of existing generative models,used as real-valued features in a discrimina-tively weighted, generative hybrid.Note that generative hybrids are the norm inSMT, where translation scores are provided bya discriminative combination of generative mod-els (Och, 2003).
Substring-based transliterationwith a generative hybrid model is very similar toexisting solutions for phrasal SMT (Koehn et al,2003), operating on characters rather than words.Unlike out-of-the-box phrasal SMT solutions, ourgenerative hybrid benefits from a target a lexicon.As we will show, this is the difference between aweak baseline and a strong competitor.We demonstrate that despite recent successes indiscriminative character transduction using indi-cator features (Jiampojamarn et al, 2008; Dreyeret al, 2008), our generative hybrid performs sur-prisingly well, producing our highest translitera-tion accuracies.
Researchers frequently compareagainst a phrasal SMT baseline when evaluating anew transduction technique (Freitag and Khadivi,2007; Dreyer et al, 2008); however, we are carefulto vary only the features in our comparison.
Con-founding variables, such as alignment, decoderand training method, are held constant.We also include a human evaluation oftransliteration-augmented SMT output.
Thoughhuman evaluations are too expensive to allow acomparison between transliteration systems, weare able to show that adding our transliterationsto a production-level SMT engine results in a sub-stantial improvement in translation quality.2 BackgroundThis work draws inspiration from previous workin transliteration, which we divide into similarityand transduction-based approaches.
We also dis-cuss recent successes in discriminative charactertransduction that have influenced this work.2.1 Similarity-based transliterationIn similarity-based transliteration, a character-based, cross-lingual similarity metric is calculated(or bootstrapped) from known transliteration pairs.Given a source word s, its transliteration is the tar-get word t most similar to s, where t is drawn fromsome pool of candidates.
This approach may alsobe referred to as transliteration discovery.Brill et al (2001) describe a katakana-to-English approach with an EM-learned edit dis-tance, which bootstraps from a small number ofexamples to learn transliteration pairs from querylogs.
Bilac and Tanaka (2005) harvest translitera-tion candidates from comparable bilingual corpora(conference abstracts in English and Japanese),and use distributional as well as phonetic simi-larity to choose among them.
Sherif and Kon-drak (2007a) also bootstrap a learned edit dis-tance for Arabic named entities, with candidatepairs drawn from sentence or document-alignedparallel text.
Klementiev and Roth (2006) boot-strap an SVM classifier trained to detect truetransliteration-pairs.
They draw candidates fromcomparable news text, using date information toprovide further clues as to aligned named entities.Bergsma and Kondrak (2007) extend the classifi-cation approach with features derived from a char-acter alignment.
They train from bilingual dic-tionaries and word-aligned parallel text, selectingnegative examples to target false-friends.The work of Hermjakob et al (2008) is par-ticularly relevant to this paper, as they incorpo-rate a similarity-based transliteration system intoan Arabic-to-English SMT engine.
They employa hand-crafted cross-lingual similarity metric, anduse capitalized n-grams from the Google n-gramcorpus as candidates.
With such a huge candidatelist, a cross-lingual indexing scheme is designedfor fast candidate look-up.
Their work also ad-dresses the question of when to transliterate (asopposed to translate), a realistic concern when de-ploying a transliteration component in SMT.
This,however, is not of so much concern for katakana,as it is used primarily for loanwords.10672.2 Transduction-based transliterationThe approach presented in this paper is an instanceof transduction-based transliteration, where thesource word is transformed into a target word us-ing a sequence of character-level operations.
Theparameters of the transduction process are learnedfrom a collection of transliteration pairs.
Thesesystems do not require a list of candidates, butmany incorporate a target lexicon, favoring targetwords that occur in the lexicon.
This approach isalso known as transliteration generation.The majority of transliteration generation ap-proaches are based on the noisy channel model,where a target t is generated according toP (t|s) ?
P (s|t)P (t).
This approach is typi-fied by finite-state transliteration, where the var-ious stages of the channel model are representedby finite state transducers and automata.
Earlysystems employed a complex channel, passingthrough multiple phonetic representations (Knightand Graehl, 1998; Bilac and Tanaka, 2004), butlater versions replaced characters directly (Al-Onaizan and Knight, 2002).
Sherif and Kondrak(2007b) extend this approach with substring oper-ations in the style of phrasal SMT, and show thatdoing so improves both accuracy as well as spaceand time efficiency.
Note that it is possible to in-corporate a target lexicon by making P (t) a wordunigram model with a character-based back-off.Li et al (2004) present an alternative to thenoisy channel with their joint n-gram model,which calculates P (s, t).
This formulation allowsoperations to be conditioned on both source andtarget context.
However, the inclusion of a candi-date list is more difficult in this setting, as P (t) isnot given its own model.Zelenko and Aone (2006) investigate a purelydiscriminative, alignment-free approach totransliteration generation.
The target word isconstructed one character at a time, with eachnew character triggering a suite of features,including indicators for near-by source and targetcharacters, as well a generative target languagemodel.
Freitag and Khadivi (2007) propose a dis-criminative, latent edit distance for transliteration.In this case, training data need not be aligned inadvance, but a latent alignment is produced duringdecoding.
Again, the target word is constructedone character at a time, using edit operationsthat are scored according to source and targetcontext features.
Both approaches train using astructured perceptron, as we do here.
However,these models represent a dramatic departure fromthe existing literature, while ours has clear analogsto the well-known noisy-channel paradigm, whichallows for useful comparisons and insights intothe advantages of discriminative training.2.3 Discriminative character transductionWhile our chosen application is transliteration,our decoder is influenced by recent successes ingeneral-purpose discriminative transduction.
Ji-ampojamarn et al (2008) describe a discrimina-tive letter-to-phoneme substring transducer, whileDreyer et al (2008) describe a discriminative char-acter transducer with a latent derivation structurefor morphological transformations.
Both modelsare extremely effective, but both rely exclusivelyon indicator features; they do not explore the useof knowledge-rich generative models.
Our indica-tor system uses an extended version of the Jiampo-jamarn et al (2008) feature set.3 MethodsWe adopt a discriminative substring decoder forour transliteration task.
A structured percep-tron (Collins, 2002) learns weights for our translit-eration features, which are drawn from two broadclasses: indicator and hybrid generative features.3.1 Structured perceptronThe decoder?s discriminative parameters arelearned with structured perceptron training.
Leta derivation d describe a substring operation se-quence that transliterates a source word into a tar-get word.
Given an input training corpus of suchderivations D = d1.
.
.
dn, a vector feature func-tion on derivations~F (d), and an initial weight vec-tor ~w, the perceptron performs two steps for eachtraining example di?
D:?
Decode:?d = argmaxd?D(src(di))(~w ?~F (d))?
Update: ~w = ~w +~F (di)?~F (?d)where D(src(d)) enumerates all possible deriva-tions with the same source side as d. To improvegeneralization, the final feature vector is the aver-age of all vectors found during learning (Collins,2002).
Accuracy on the development set is usedto select the number of times we pass through alldi?
D.Given the above framework, we require trainingderivations D, feature vectors~F , and a decoder to1068carry out the argmax over all d reachable from aparticular source word.
We describe each of thesecomponents in turn below.3.2 Training derivationsNote that the above framework describes a max-derivation decoder trained on a corpus of gold-standard derivations, as opposed to a max-transliteration decoder trained directly on source-target pairs.
By building the entire system on thederivation level, we side-step issues that can oc-cur when perceptron training with hidden deriva-tions (Liang et al, 2006), but we also introduce theneed to transform our training source-target pairsinto training derivations.Training derivations can be learned unsu-pervised from source-target pairs using char-acter alignment techniques.
Previously, thishas been done using an EM-learned edit dis-tance (Ristad and Yianilos, 1998), or generaliza-tions thereof (Brill and Moore, 2000; Jiampoja-marn et al, 2007).
We opt for an alternative align-ment technique, similar to the word-aligner de-scribed by Zhang et al (2008).
This approachemploys variational EM with sparse priors, alongwith hard length limits, to reduce the length ofsubstrings operated upon.
By doing so, we hope tolearn only non-compositional transliteration units.Our aligner produces only monotonic align-ments, and does not allow either the source or tar-get side of an operation to be empty.
The samerestrictions are imposed during decoding.
In thisway, each alignment found by variational EM isalso an unambiguous derivation.
We align ourtraining corpus with a maximum substring lengthof three characters.
The same derivations are usedto train all of the transliteration systems tested inthis paper.3.3 FeaturesWe employ two main types of features: indicatorsand hybrid generative models.
Indicators detectbinary events in a derivation, such as the presenceof a particular operation.
Hybrid generative fea-tures assign a real-valued probability to a deriva-tion, based on statistics collected from trainingderivations.
There are few generative features andeach carries a substantial amount of information,while indicators are sparse and knowledge-poor.We treat these two classes of features as distinct.We do so because researchers often use either oneapproach or the other.2Furthermore, it is notclear how to optimally employ training derivationswhen combining generative models and sparse in-dicators: generative models need large amounts ofdata to collect statistics and relatively little for per-ceptron training,3while sparse indicators requireonly a large perceptron training set.We can further divide feature space accordingto the information required to calculate each fea-ture.
Both feature sets can be partitioned into thefollowing subtypes:?
Emission: How accurate are the operationsused by this derivation??
Transition: Does the target string producedby this derivation look like a well-formed tar-get character sequence??
Lexicon: Does the target string containknown words from a target lexicon?Indicator FeaturesPrevious approaches to discriminative charactertransduction tend to employ only sparse indica-tors (Jiampojamarn et al, 2008; Dreyer et al,2008).
This is because sparsity is not a major con-cern in character-based domains, and sparse indi-cators are extremely flexible.Our emission and transition indicator featuresfollow Jiampojamarn et al (2008).
Emission indi-cators are centered around an operation, such as[( ?
tho].
Minimally, an indicator exists foreach operation.
Many more source context fea-tures can be generated by conjoining an operationwith source n-grams found within a fixed win-dow of C characters to either side of the operation.These source context features have minimal com-putational cost, and they allow each operator to ac-count for large, overlapping portions of the source,even when the substrings being operated upon aresmall.
Meanwhile, transition indicators stand infor a character-based target language model.
Indi-cators are built for each possible target n-gram, forn = 1 .
.
.K, allowing the perceptron to constructa discriminative back-off model.
Development ex-periments lead us to select C = 3 and K = 5.2Generative hybrids are often accompanied by a smallnumber of unsparse indicators, such as operation count.3Perceptron training on the same data used for modelconstruction can lead to overconfidence in model quality.One can address this problem by using a large number ofmodeling-training folds (Collins et al, 2005), but we do notdo so here.1069Indicator lexicon features are novel to this work.Given access to a target lexicon with type fre-quencies, we opt to create features that indicatethe frequencies of generated target words accord-ing to coarse bins.
Experiments on our develop-ment set lead to the selection of 5 frequency bins:[< 2,000], [< 200], [< 20], [< 2], [< 1].
To keepthe model linear, these features are cumulative;thus, generating a word with frequency 126 willresult in both the [< 2, 000] and [< 200] featuresfiring.
Note that a single transliteration can po-tentially generate multiple target words, and doingso can have a major impact on how often the lex-icon features fire.
Thus, we employ another fea-ture that indicates the introduction of a new word.We expect these frequency indicators to be supe-rior to a word-level unigram model, as they allowthe designer to select notable frequencies.
In par-ticular, the bins we have selected do not give anyadvantage to extremely common words, as theseare generally less likely to be transliterated.Hybrid Generative FeaturesWe begin with the three components of the gener-ative noisy channel employed by Sherif and Kon-drak (2007b).
Their transliteration probability is:P (t|s) ?
PE(s|t) ?max [PT(t), PL(t)] (1)Inspired by the linear models used in SMT (Och,2003), we can discriminatively weight the compo-nents of this generative model, producing:wElogPE(s|t) + wTlogPT(t) + wLlogPL(t)with weights w learned by perceptron training.These three models conveniently align with ourthree feature subtypes.
Emission information isprovided by PE(s|t), which is estimated by maxi-mum likelihood on the operations observed in ourtraining derivations.
Including source context isdifficult in such a model.
To compensate for this,all systems using PE(s|t) also use composed op-erations, which are constructed from operation se-quences observed in the training set.
This removesthe length limit on substring operations.4PT(t)provides transition information through a charac-ter language model, estimated on the target side4Derivations built by our character aligner use opera-tions on substrings of maximum length 3.
To enable per-ceptron training with composed operations, once PE(s|t)has been estimated by counting composed operations in theinitial alignments, we re-align our training examples withthose composed operations to maximize PE(s|t), creatingnew training derivations.of the training derivations.
In our implementation,we employ a KN-smoothed 7-gram model (Kneserand Ney, 1995).
Finally, PL(t) is a unigram tar-get word model, estimated from the same type fre-quencies used to build our lexicon indicators.Since we have adopted a linear model, we areno longer constrained by the original generativestory.
Therefore, we are free to incorporate otherSMT-inspired features: PE?
(t|s), target charactercount, and operation count.5Feature summaryThe indicator and hybrid-generative feature setseach provide a discriminative version of the noisychannel model.
In the case of transition and lexi-con features, both systems have access to the ex-act same information, but encode that informationdifferently.
The lexicon encoding is the most dra-matic difference, with the indicators using a smallnumber of frequency bins, and the generative uni-gram model providing a single, real-valued featurethat is proportional to frequency.In the case of their emission features, thetwo systems actually encode different information.Both have access to the same training derivations,but the indicator system provides source contextthrough n-gram indicators, while the generativesystem does so using composed operations.3.4 DecoderOur decoder builds upon machine translation?smonotone phrasal decoding (Zens and Ney, 2004),or equivalently, the sequence tagging algorithmused in semi-Markov CRFs (Sarawagi and Co-hen, 2004).
This dynamic programming (DP) de-coder extends the Viterbi algorithm for HMMsby operating on one or more source characters (asubstring) at each step.
A DP block stores thebest scoring solution for a particular prefix.
Eachblock is subdivided into cells, which maintain thecontext necessary to calculate target-side features.We employ a beam, keeping only the 40 highest-scoring cells for each block, which speeds up in-ference at the expense of optimality.
We foundthat the beam had no major effect on perceptrontraining, nor on the system?s final accuracy.Previously, target lexicons have been usedprimarily in finite-state transliteration, as theyare easily encoded as finite-state-acceptors (Al-Onaizan and Knight, 2002; Sherif and Kondrak,5Character and operation counts also fit in the indicatorsystem, but did not improve performance in development.10702007b).
It is possible to extend the DP decoder toalso use a target lexicon.
By encoding the lexiconas a trie, and adding the trie index to the contexttracked by the DP cells, we can provide access tofrequency estimates for words and word prefixes.This has the side-effect of creating a new cell foreach target prefix; however, in the character do-main, this remains computationally tractable.4 Data4.1 Wikipedia training and test dataOur katakana-to-English training data is de-rived from bilingually-linked Wikipedia titles.Any Japanese Wikipedia article with an entirelykatakana title and a linked English article resultsin training pair.
This results in 60K transliterationpairs; we removed 2K pairs for development, and2K for held-out testing.The remaining 56K training pairs are quitenoisy.
As mentioned earlier, roughly 10% of ourexamples are simply not transliterable, but ap-proximate Wikipedia title translations are an evenmore substantial source of noise.
For example,S4E@ [konpyuutageemu] ?
com-puter game is aligned with the English articleComputer and video games.
We found it ben-eficial, in terms of both speed and accuracy, todo some coarse alignment-based pruning.
Afteralignment, the operations used by all derivationsare counted.
Any operation that is used fewer thanthree times is eliminated, along with any deriva-tion using that operation.
The goal is to eliminateloose transliteration pairs from our data, where aword or initial is included in one language butnot the other.
This results in 40K training pairs.Despite the noise in the Wikipedia data, there areclear advantages in using it for training transliter-ation models: it is available for any language pair,it reflects recent trends and events, and the amountof data increases daily.
As we will see below, themodel trained on this data performs well on a testset from a very different domain.All systems use development set accuracy toselect their meta-parameters, such as the numberof perceptron iterations, the size of the source-context window, and the n-gram length used incharacter language modeling.
The hybrid gener-ative system further splits the training set, using38K derivations for the calculation of its emissionand transition models, and 2K derivations for per-ceptron training its model weights.4.2 Machine translation test dataIn order to see how effective our transliteratoris on out-of-domain test data, we also createdtest data from a log of translation requests toa web-based, Japanese-to-English translation ser-vice.6Out of 5,000 randomly selected transla-tion requests, there are 312 cases where katakanasource words are out-of-vocabulary for the MTsystem, and therefore remain untranslated.
Wecreated a reference translation (not necessarily atransliteration) for these katakana words by man-ually selecting the corresponding English word(s)in the sentence-level reference translation, whichwas produced independently from this experiment.This test set is quite divergent from the Wikipediatitles: only 17 (5.5%) of its katakana words arefound in the Wikipedia training data, and six ofthese did not agree on the English translation.4.3 English lexiconOur English lexicon is derived from two over-lapping data sources: the English gigaword cor-pus (LDC2003T05; GW) and the language modeltraining data for our SMT system, which containsselections from Europarl, gigaword, and web-harvested text.
Both are lowercased.
We com-bine the unigram frequency counts from the twosources by taking the max when they overlap.
Theresulting lexicon has 5M types, 2.5M of whichhave frequency 1.5 ExperimentsIn this section, we summarize development exper-iments, and then conduct a comparison on our twotransliteration test sets.
We report 0-1 accuracy: atransliteration is only correct if it exactly matchesthe reference.
For the comparison experiments,we also report 10-best accuracy, where a systemis correct if it includes the correct transliterationsomewhere in its 10-best list.5.1 BaselinesWe compare our systems against a re-implementation of Sherif and Kondrak?s (2007b)noisy-channel substring decoder.
This uses thesame PE, PTand PLmodels as our hybrid gen-erative system, but employs a two-pass decodingscheme to find the max transliteration accordingto Equation 1.
It represents a purely generativesolution using otherwise identical architecture.6http://www.microsofttranslator.com1071Since our hybrid generative system implementsa model that is very similar to those used in phrasalSMT, we also compare against a state-of-the-artphrasal SMT system (Moore and Quirk, 2007).This system is trained by applying the standardSMT pipeline to our Wikipedia title pairs, treat-ing characters as words, using a 7-gram character-level language model, and disabling re-ordering.Unfortunately, the decoder?s architecture does notallow the use of a word-level unigram model, re-ducing the usefulness of this baseline.
Instead, weinclude the target lexicon as a second character-level language model.
This baseline indicates thelevel of performance one can expect by applyingphrasal SMT straight out of the box.Comparing the two baselines qualitatively, bothuse a combination of generative models inspiredby the noisy channel.
Sherif and Kondrak em-ploy a word-level unigram model without discrim-inatively weighting the models, while the PhrasalSMT approach uses weights derived from max-BLEU training without word-level unigrams.
Theobvious question of what happens when one doesboth will be answered by our hybrid generativesystem.5.2 Development experimentsTable 1 shows development set accuracy for anumber of systems and feature types, along withthe model size of the corresponding systems,where size is measured in terms of the number ofnon-zero discriminatively-trained parameters.
Theaccuracy of the Sherif and Kondrak baseline isshown as SK07.
Despite its lack of discrimina-tive training, word-level unigrams allow the SK07baseline to outperform Phrasal SMT .
In future ex-periments, we compare only against SK07.The indicator system was tested using only op-eration indicators, with source context, transitionand lexicon indicators added incrementally.
Allfeature types have a substantial impact, with thelexicon providing the boost needed to surpass thebaseline.
Note that the inclusion of the five fre-quency bins is sufficient to decrease the overallfeature count of the system by 600K, as muchfewer mistakes are made during training.Development of the hybrid generative systemused the SK07 baseline as a starting point.
The re-sult of combining its three components into a flatlinear model, with all weights set to 1, is shownin Table 1 as Linear SK07.
This violation ofTable 1: Development accuracy and model sizeSystem Acc.
SizeBaseline Phrasal SMT 30.7 8SK07 33.5 ?Indicator Operations only 3.6 6.8K+ source context 23.9 2.8M+ transition 28.6 3.1M+ lexicon 44.2 2.5M+ gen. lexicon 44.1 3.0MGenerative Linear SK07 31.7 ?+ perceptron 42.4 3+ SMT features 44.1 6+ ind.
lexicon 44.3 12conditional independence assumptions results in adrop in accuracy.
However, the + perceptron lineshows that setting the three weights with percep-tron training results in a huge boost in accuracy,nearly matching our indicator system.
Adding fea-tures inspired by SMT, such as PE?
(t|s), elimi-nates the gap between the two.5.3 Development discussionConsidering their differences, the two systems?proximity in score is quite surprising.
Given thecharacter domain?s lack of sparsity, and the largeamount of available training data, we had expectedthe hybrid generative system to behave only asa strong baseline; instead, it matched the perfor-mance of the indicator system.
However, thisis not unprecedented: discriminatively weightedgenerative models have been shown to outperformpurely discriminative competitors in various NLPclassification tasks (Raina et al, 2004; Toutanova,2006), and remain the standard approach in statis-tical translation modeling (Och, 2003).Examining the development results on anexample-by-example basis, we see that the twosystems make mostly the same mistakes: for 87%of examples, either both systems are right, or bothare wrong.
The remainder represents a (relativelysmall) opportunity to improve through system orfeature combination: an oracle that perfectly se-lects between the two scores 50.6.One opportunity for straight-forward combina-tion is the target lexicon.
Because lexicon frequen-cies are drawn from an independent word list, andnot the transliteration training derivations, there isno reason why both systems cannot use both lex-icon representations.
Unfortunately, doing so has1072Table 2: Test set comparisonsWikipedia MTSystem Acc.
Top 10 Acc.
Top 10SK07 33.5 57.9 38.8 57.0Generative 43.0 65.6 42.9 58.3Indicator 42.5 63.5 43.6 57.7little impact, as is shown in each system?s final rowin Table 1.
Adding the word unigram model tothe indicator system results in slightly lower per-formance, and a much larger model.
Adding thefrequency bins to the generative system does im-prove performance slightly, but attempts to com-pletely replace the generative system?s word uni-gram model with frequency bins resulted in a sub-stantial drop in accuracy.75.4 Test set comparisonsTable 2 shows the accuracies of the systems se-lected during development on our testing data.
Onthe held-out Wikipedia examples, the trends ob-served during development remain the same, withthe generative system expanding its lead.
Mov-ing to 10-best accuracies changes little, except forslightly narrowing the gap between SK07 and thediscriminative systems.The second column of Table 2 compares thesystems on our MT test set.
As discussed ear-lier, this data is quite different from the Wikipediatraining set, and as a result, the systems?
differ-ences are less pronounced.
1-best accuracy stillshows the discriminative systems having a definiteadvantage, but at the 10-best level, those distinc-tions are muted.Compared with the previous work on katakana-to-English transliteration, these accuracies do notlook particularly high: both Knight and Graehl(1998) and Bilac and Tanaka (2004) report accu-racies above 60% for 1-best transliteration.
Weshould emphasize that this is due to the difficultyof our test data, and that we have tested against abaseline that has been shown to outperform Knightand Graehl (1998).
The test data was not filteredfor noise, leaving untransliterable cases and loosetranslations intact.
The accuracies reported aboveare under-estimates of real performance: manytransliterations not matching the reference maystill be useful to a human reader, such as differ-7Lexicon replacement experiment is not shown in Table 1.ences in inflection (e.g.,L!.) [rechinoido]?
retinoids, transliterated as retinoid), and spac-ing (e.g.
IL- [shierareone]?
SierraLeone, transliterated as sierraleone).6 Integration with machine translationWe used the transliterations from our indicatorsystem to augment a Japanese-to-English MT sys-tem.8This treelet-based SMT system (Quirk etal., 2005) is trained on about 4.6M parallel sen-tence pairs from diverse sources including bilin-gual books, dictionaries and web publications.Our goal is to measure the impact of machinetransliterations on end-to-end translation quality.6.1 Evaluation methodWe use the MT-log translation pairs describedin Section 4.2 as a sentence-level translation testset.
For each katakana word left untranslated bythe baseline SMT engine, we generated 10-besttransliteration candidates and added the katakana-English pairs to the SMT system?s translation ta-ble.
Perceptron scores were exponentiated, thennormalized, to create probabilities, which weregiven to the SMT system as P (source|target);9all other translation features were set to log 1.We translated the test set with and without theaugmented translation table.
120 sentences wererandomly selected from the cases where the trans-lations output by the two SMT systems differed,and were submitted for two types of human evalu-ation.
In the absolute evaluation, each SMT out-put was assigned a score between 1 and 4 (1 =completely useless; 4 = perfect translation); in therelative evaluation, the evaluators were presentedwith a pair of SMT outputs, with and without thetransliteration table, and were asked to judge ifthey preferred one translation over the other.
Inboth evaluation settings, the machine-translatedsentences were evaluated by two native speakersof English who have no knowledge of Japanese,with access to a reference translation.6.2 ResultsThe evaluation results show that our translitera-tor does improve the quality of SMT.
The BLEU8The human evaluation was carried out before we discov-ered the effectiveness of the hybrid generative system, butrecall that the performance of the two is similar.9The perceptron scores are more naturally interpreted asP (target |source), but the opposite direction is generally thehighest-weighted feature in the SMT system?s linear model.1073Table 3: Relative translation evaluationevaluator 1 preferenceeval2pref +translit equal baseline sum+translit 95 0 2 97equal 19 1 2 22baseline 1 0 0 1sum 115 1 4 120score on the entire test set improved only slightly,from 21.8 to 22.0.
However, in the absolute hu-man evaluation, the transliteration table increasedthe average human judgement from 1.5 to 2 out ofa maximum score of 4.
Table 3 shows the resultsof the relative evaluation along with the judges?sentence-level agreement.
In 95 out of 120 cases,both annotators agreed that the augmented tableproduced a better translation than the baseline.One might expect that any replacement ofkatakana would improve the perception of MTquality.
This is not necessarily the case: itcan be more confusing to have a drasticallyincorrect transliteration, such as transliterating#7M [appurooda] ?
uploader incor-rectly as applaud.
Fortunately, Table 3 shows thatwe make very few of these sorts of mistakes: thebaseline is preferred only rarely.
Also note that,according the MT 10-best accuracies in Table 2,we would have expected to improve at most 60%of cases, however, the human judgements indicatethat our actual rate of improvement is closer to80%, which demonstrates that even an imperfecttransliteration is often useful.7 ConclusionWe have presented a discriminative substring de-coder for transliteration.
Our decoder is basedon recent approaches for discriminative charac-ter transduction, extended to provide access to atarget lexicon.
We have presented a comparisonof indicator and hybrid generative features in acontrolled setting, demonstrating that generativemodels perform surprisingly well when discrim-inatively weighted.
We have also shown our dis-criminative models to be superior to a state-of-the-art generative system.
Finally, we have demon-strated that machine transliteration is immediatelyuseful to end-to-end SMT.As mentioned earlier, by focusing on katakana,we bypass the problem of deciding when totransliterate rather than translate; next, we plan tocombine our models with a classifier that makessuch a decision, allowing us to integrate transliter-ation into SMT for other language pairs.ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in Arabic text.
In ACLWorkshop on Comp.
Approaches to Semitic Lan-guages.Shane Bergsma and Grzegorz Kondrak.
2007.Alignment-based discriminative string similarity.
InACL, pages 656?663, Prague, Czech Republic, June.Slaven Bilac and Hozumi Tanaka.
2004.
A hybridback-transliteration system for Japanese.
In COL-ING, pages 597?603, Geneva, Switzerland.Slaven Bilac and Hozumi Tanaka.
2005.
Extractingtransliteration pairs from comparable corpora.
InProceedings of the Annual Meeting of the NaturalLanguage Processing Society, Japan.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InACL, pages 286?293, Morristown, NJ.Eric Brill, Gary Kacmarcik, and Chris Brockett.2001.
Automatically harvesting katakana-englishterm pairs from search engine query logs.
In AsiaFederation of Natural Language Processing.Michael Collins, Brian Roark, and Murat Sarac?lar.2005.
Discriminative syntactic language modelingfor speech recognition.
In ACL, pages 507?514,Ann Arbor, USA, June.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP.Markus Dreyer, Jason Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductionswith finite-state methods.
In EMNLP, pages 1080?1089, Honolulu, Hawaii, October.Dayne Freitag and Shahram Khadivi.
2007.
A se-quence alignment model based on the averaged per-ceptron.
In EMNLP, pages 238?247, Prague, CzechRepublic, June.Ulf Hermjakob, Kevin Knight, and Hal Daum?e III.2008.
Name translation in statistical machine trans-lation - learning when to transliterate.
In ACL, pages389?397, Columbus, Ohio, June.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many align-ments and hidden markov models to letter-to-phoneme conversion.
In HLT-NAACL, pages 372?379, Rochester, New York, April.1074Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discriminativetraining for letter-to-phoneme conversion.
In ACL,pages 905?913, Columbus, Ohio, June.Alexandre Klementiev and Dan Roth.
2006.
Namedentity transliteration and discovery from multilin-gual comparable corpora.
In HLT-NAACL, pages82?88, New York City, USA, June.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In In-ternational Conference on Acoustics, Speech, andSignal Processing (ICASSP-95), pages 181?184.Kevin Knight and Jonathan Graehl.
1998.
Ma-chine transliteration.
Computational Linguistics,24(4):599?612.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In HLT-NAACL.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.In ACL, pages 159?166, Barcelona, Spain, July.Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrimina-tive approach to machine translation.
In COLING-ACL, pages 761?768, Sydney, Australia, July.Robert Moore and Chris Quirk.
2007.
Fasterbeam-search decoding for phrasal statistical ma-chine translation.
In MT Summit XI.Franz J. Och.
2003.
Minimum error rate training forstatistical machine translation.
In ACL, pages 160?167.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In ACL, pages 271?279, AnnArbor, USA, June.Rajat Raina, Yirong Shen, Andrew Y. Ng, and AndrewMcCallum.
2004.
Classification with hybrid gener-ative/discriminative models.
In Advances in NeuralInformation Processing Systems 16.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string-edit distance.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 20(5):522?532.Sunita Sarawagi and William Cohen.
2004.
Semi-markov conditional random fields for informationextraction.
In ICML.Tarek Sherif and Grzegorz Kondrak.
2007a.
Boot-strapping a stochastic transducer for Arabic-Englishtransliteration extraction.
In ACL, pages 864?871,Prague, Czech Republic, June.Tarek Sherif and Grzegorz Kondrak.
2007b.Substring-based transliteration.
In ACL, pages 944?951, Prague, Czech Republic, June.Kristina Toutanova.
2006.
Competitive generativemodels with structure learning for nlp classificationtasks.
In EMNLP, pages 576?584, Sydney, Aus-tralia, July.Dmitry Zelenko and Chinatsu Aone.
2006.
Discrimi-native methods for transliteration.
In EMNLP, pages612?617, Sydney, Australia, July.Richard Zens and Hermann Ney.
2004.
Improvementsin phrase-based statistical machine translation.
InHLT-NAACL, pages 257?264, Boston, USA, May.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InACL, pages 97?105, Columbus, Ohio, June.1075
