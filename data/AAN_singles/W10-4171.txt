Soochow University: Description and Analysis of the ChineseWord Sense Induction System for CLP2010Hua Xu   Bing Liu   Longhua Qian?
Guodong ZhouNatural Language Processing LabSchool of Computer Science and TechnologySoochow University, Suzhou, China 215006Email:{20094227034,20084227065055,qianlonghua,gdzhou}@suda.edu.cn?
Corresponding authorAbstractRecent studies on word sense induction(WSI) mainly concentrate on Europeanlanguages, Chinese word sense inductionis becoming popular as it presents a newchallenge to WSI.
In this paper, wepropose a feature-based approach usingthe spectral clustering algorithm to thisproblem.
We also compare variousclustering algorithms and similaritymetrics.
Experimental results show thatour system achieves promisingperformance in F-score.1 IntroductionWord sense induction (WSI) is an open problemof natural language processing (NLP), whichgoverns the process of automatic discovery ofthe possible senses of a word.
WSI is similar toword sense disambiguation (WSD) both inmethods employed and in problem encountered.In the procedure of WSD, the senses are as-sumed to be known and the task focuses onchoosing the correct one for an ambiguous wordin a context.
The main difference between themis that the task of WSD generally requires large-scale manually annotated lexical resources whileWSI does not.
As WSI doesn?t rely on themanually annotated corpus, it has become one ofthe most important topics in current NLP re-search (Pantel and Lin, 2002; Neill, 2002; Rapp,2003).
Typically, the input to a WSI algorithm isa target word to be disambiguated.
The task ofWSI is to distinguish which target words sharethe same meaning when they appear in differentcontexts.
Such result can be at the very leastused as empirically grounded suggestions forlexicographers or as input for WSD algorithm.Other possible uses include automatic thesaurusor ontology construction, machine translation orinformation retrieval.
Compared with Europeanlanguages, the study of WSI in Chinese is scarce.Furthermore, as Chinese has its special writingstyle and Chinese word senses have their owncharacteristics, the methods that work well inEnglish may not perform effectively in Chineseand the usefulness of WSI in real-world applica-tions has yet to be tested and proved.The core idea behind word sense induction isthat contextual information provides importantcues regarding a word?s meaning.
The idea datesback to (at least) Firth (1957) (?You shall knowa word by the company it keeps?
), and under-lies most WSD and lexicon acquisition work todate.
For example, when the adverb phrase oc-curring prior to the ambiguous word???
?,then the target word is more likely to be a verband the meaning of which is ?to hold something?
;Otherwise, if an adjective phrase locates in thesame position, then it probably means ?confi-dence?
in English.
Thus, the words surroundsthe target word are main contributor to senseinduction.The bake off task 4 on WSI in the first CIPS-SIGHAN Joint Conference on Chinese Lan-guage Processing (CLP2010) is intended topromote the exchange of ideas among partici-pants and improve the performance of ChineseWSI systems.
Generally, our WSI system alsoadopts a clustering algorithm to group the con-texts of a target word.
Differently, after generat-ing feature vectors of words, we compute a simi-larity matrix with each cell denoting the similar-ity between two contexts.
Furthermore, the set ofsimilarity values of a context with other contextsis viewed as another kind of feature vector,which we refer to as similarity vector.
Both fea-ture vectors and similarity vectors can be sepa-rately used as the input to clustering algorithms.Experimental results show our system achievesgood performances on the development datasetas well as on the final test dataset provided bythe CLP2010.2 System DescriptionThis section sequentially describes the architec-ture of our WSI system and its main components.2.1 System ArchitectureFigure 1 shows the architecture of our WSIsystem.
The first step is to preprocess the rawdataset for feature extraction.
After that, weextract ?bag of words?
from the sentencecontaining a target word (feature extraction) andtransform them into high-dimension vectors(feature vector generation).
Then, similarities ofevery two vectors could be computed based onthe feature vectors (similarity measurement).
thesimilarities of an instance can be viewed asanother vector?similarity vector.
Both featurevectors and similarity vectors can be served asthe input for clustering algorithms.
Finally, weperform three clustering algorithms, namely, k-means, HAC and spectral clustering.DatasetPreprocessFeatureExtractionVectorGenerationSimilarityMeasurementSimilarityAs VectorClusteringWSIResultsFigure 1  Architecture of our ChineseWSI system2.2 Feature EngineeringIn the task of WSI, the target words with theirtopical context are first transformed into multi-dimensional vectors with various features, andthen applying clustering algorithm to detect therelevance of each other.Corpus PreprocessingFor each raw file, we first extract each sentenceembedded in the tag <instance>, includingthe <head> and </head> tags which are usedto identify the ambiguous word.
Then, we put allthe sentences related to one target word into afile, ordered by their instance IDs.
The next stepis word segmentation, which segments each sen-tence into a sequence of Chinese words and isunique for Chinese WSI.
Here, we use the soft-ware from Hylanda1 since it is ready to use andconsidered an efficient word segmentation tool.Finally, since we retain the <head> tag in thesentence, the <head> and </head> tags areusually separated after word segmentation, thuswe have to restore them in order to correctly lo-cate the target word during the process of featureextraction.Feature ExtractionAfter word segmentation, for a context of a par-ticular word, we extract all the words around itin the sentence and build a feature vector basedon a ?bag-of-words?
Boolean model.
?Bag-of-words?
means that we don?t consider the orderof words.
Meanwhile, in the Boolean model,each word in the context is used to generate afeature.
This feature will be set to 1 if the wordappears in the context or 0 if it does not.
Finally,we get a number of feature vectors, each of themcorresponds to an instance of the target word.One problem with this feature-based method isthat, since the size of word set may be huge, thedimension is also very high, which might lead todata sparsity problem.Similarity measurementOne commonly used metric for similarity meas-urement is cosine similarity, which measures theangle between two feature vectors in a high-dimensional space.
Formally, the cosine similar-ity can be computed as follows:cos ,ine similarity ?< > = ?x yx yx ywhere ,x y are two vectors in the vector spaceand x , y are the lengths of  ,x y  respectively.1 http://www.hylanda.com/Some clustering algorithms takes feature vec-tors as the input and use cosine similarity as thesimilarity measurement between two vectors.This may lead to performance degradation dueto data sparsity in feature vectors.
To avoid thisproblem, we compute the similarities of everytwo vectors and generate an  similaritymatrix, where  is the number of all the in-stances containing the ambiguous word.
Gener-ally, is usually much smaller than the dimen-sion size and may alleviate the data sparsityproblem.
Moreover, we view every row of thismatrix (i.e., an ordered set of similarities of aninstance with other instances) as another kind offeature vector.
In other words, each instance it-self is regarded as a feature, and the similaritywith this instance reflects the weight of the fea-ture.
We call this vector similarity vector, whichwe believe will more properly represent the in-stance and achieve promising performance.
*N NNN2.3 Clustering AlgorithmClustering is a very popular technique whichaims to partition a dataset into such subgroupsthat samples in the same group share more simi-larities than those from different groups.
Oursystem explores various cluster algorithms forChinese WSI, including K-means, hierarchicalagglomerative clustering (HAC), and spectralclustering (SC).K-means (KM)K-means is a very popular method for generalclustering used to automatically partition a dataset into k groups.
K-means works by assigningmultidimensional vectors to one of K clusters,where is given as a priori.
The aim of the al-gorithm is to minimize the variance of the vec-tors assigned to each cluster.KK-means proceeds by selecting k  initial clus-ter centers and then iteratively refining them asfollows:(1) Choose cluster centers to coincide withk randomly-chosen patterns or k  ran-domly defined points.k(2) Assign each pattern to the closest clustercenter.
(3) Recompute the cluster centers using thecurrent cluster memberships.
(4) If a convergence criterion is not met, goto step 2.Hierarchical Agglomerative Clustering (HAC)Different from K-means, hierarchical clusteringcreates a hierarchy of clusters which can berepresented in a tree structure called adendrogram.
The root of the tree consists of asingle cluster containing all objects, and theleaves correspond to individual object.Typically, hierarchical agglomerativeclustering (HAC) starts at the leaves andsuccessively merges two clusters together aslong as they have the shortest distance among allthe pair-wise distances between any two clusters.Given a specified number of clusters, the keyproblem is to determine where to cut the hierar-chical tree into clusters.
In this paper, we gener-ate the final flat cluster structures greedily bymaximizing the equal distribution of instancesamong different clusters.Spectral Clustering (SC)Spectral clustering refers to a class of techniqueswhich rely on the eigen-structure of a similaritymatrix to partition points into disjoint clusterswith points in the same cluster having high simi-larity and points in different clusters having lowsimilarity.Compared to the ?traditional algorithms?
suchas K-means or single linkage, spectral clusteringhas many fundamental advantages.
Results ob-tained by spectral clustering often outperformthe traditional approaches, spectral clustering isvery simple to implement and can be solved ef-ficiently by standard linear algebra methods.3 System EvaluationThis section reports the evaluation dataset andsystem performance for our feature-based Chi-nese WSI system.3.1  Dataset and Evaluation MetricsWe use the CLP2010 bake off task 4 sampledataset as our development dataset.
There are2500 examples containing 50 target words andeach word has 50 sentences with different mean-ings.
The exact meanings of the target words areblind, only the number of the meanings is pro-vided in the data.
We compute the system per-formance with the sample dataset because it con-tains the answers of each candidate meaning.The test dataset provided by the CLP2010 issimilar to the sample dataset.
It contains 100target words and 5000 instances in total.
How-ever, it doesn?t provide the answers.The F-score measurement is the same as Zhaoand Karypis (2005).
Given a particularclass rL of size and a particular cluster  ofsize , suppose  in the cluster  belong torn iSin irn iS rL ,then the value of this class and cluster is de-fined to beF2 ( , ) ( ,( , )( , ) ( , )r i r ir ir i r i)R L S P L SF L SR L S P L S?
?= +( , ) /r i ir rR L S n n=( , ) /r i ir iP L S n n=where ( , )r iR L S is the recall value andis the precision value.
The F-score of class( , )r iP L SrL isthe maximum value and F-score value follow: F( ) max ( , )ir S r iF score L F L S?
=1( )crrrnF score F score Ln=?
= ?
?where  is the total number of classes and n  isthe total size.c3.2 Experiment ResultsTable 1 reports the F-score of our feature-basedChinese WSI for different feature sets withvarious window sizes using K-means clustering.Since there are different results for each run ofK-means clustering algorithm, we perform 20trials and compute their average as the finalresults.
The columns denote different windowsize n, that is, the n words before and after thetarget word are extracted as features.
Particularly,the size of infinity (?)
means that all the wordsin the sentence except the target word areconsidered.
The rows represent variouscombinations of feature sets and similaritymeasurements, currently, four of which areconsidered as follows:F-All: all the words are considered as featuresand from them feature vectors are constructed.F-Stop: the top 150 most frequently occurringwords in the total ?word bags?
of the corpus areregarded as stop words and thus removed fromthe feature set.
Feature vectors are then formedfrom these words.S-All: the feature set and the feature vectorare the same as those of F-All, but instead thesimilarity vector is used for clustering (c.f.
Sec-tion 2.2).S-Stop: the feature set and the feature vectorare the same as those of F-Stop, but instead thesimilarity vector is used for clustering.Table 1 Experimental results for differ-ent feature sets with different window sizes us-ing K-means clusteringThis table shows that S-Stop achieves the bestperformance of 0.7320 in F-score.
This suggeststhat for K-means clustering, Chinese WSI canbenefit much from removing stop words andadopting similarity vector.
It also shows that:Feature/Similarity 3 7 10 ?F-All 0.5949 0.6199 0.6320 0.6575F-Stop 0.6384 0.6500 0.6493 0.6428S-All 0.5856 0.6044 0.6186 0.6843S-Stop 0.6532 0.6696 0.6804 0.7320z As the window size increases, the perform-ance is almost consistently enhanced.
Thisindicates that all the words in the sentencemore or less help disambiguate the targetword.z Removing stop words consistently improvesthe F-score for both similarity metrics.
Thismeans some high frequent words do not helpdiscriminate the meaning of the target words,and further work on feature selection is thusencouraged.z Similarity vector consistently outperformsfeature vector for stop-removed features, butnot so for all-words features.
This may bedue to the fact that, when the window size islimited, the influence of frequently occur-ring stop words is relatively high, thus thesimilarity vector misrepresent the context ofthe target word.
On the contrary, when stopwords are removed or the context is wide,the similarity vector can better reflect thetarget word?s context, leading to better per-formance.In order to intuitively explain why the simi-larity vector is more discriminative than the fea-ture vector, we take two sentences containingthe Chinese word ????
(hold, grasp) as an ex-ample (Figure 2).
These two sentences have fewcommon words, so clustering via feature vectorsputs them into different classes.
However, sincethe similarities of these two feature vectors withother feature vectors are much similar, cluster-ing via similarity vectors group them into thesame class.Figure 2  An example from the datasetAccording to the conclusion of the above ex-periments, it is better to include all the wordsexcept stop words in the sentence as the featuresin the subsequent experiment.
Table 2 lists theresults using various clustering algorithms withthis same experimental setting.
It shows that thespectral clustering algorithm achieves the bestperformance of 0.7692 in F-score for ChineseWSI using the S-All setup.
Additionally, thereare some interesting findings:mi-ofhaningthisntolus-llyer-re.ex-derers the density information, therefore S-Allwill not significantly improve the perform-ance.Feature/SimilarityKM HAC SCF-All 0.6428 0.6280 0.7686S-All 0.7320 0.6332 0.7692Table 2 Experiments results using dif-ferent clustering algorithms<lexelt item="??"
snum="4"><instance id="0012">????????????????????????????????????<head>??</head>????????????
?</instance><instance id="0015">??????????????????????????????????<head>??</head>?????????????????????????????????
?</instance></lexelt>3.3 Final System PerformanceFor the CLP2010 task 4 test dataset which con-tains 100 target words and 5000 instances in to-tal, we first extract all the words except stopwords in a sentence containing the target word,then produce the feature vector for each contextand generate the similarity matrix, finally weperform the spectral cluster algorithm.
Probablybecause the distribution of the target word in thetest dataset is different from that in the develop-ment dataset, the F-score of our system on thetest dataset is 0.7108, about 0.05 units lowerthan that we got on the sample dataset.4 Conclusions and Future WorkIn our Chinese WSI system, we extract all thewords except stop words in the sentence, con-struct feature vectors and similarity vectors, andapply the spectral clustering algorithm to thisproblem.
Experimental results show that oursimple and efficient system achieve a promisingresult.
Moreover, we also compare various clus-tering algorithms and similarity metrics.
We findthat although the spectral clustering algorithmoutperforms other clustering algorithms, the K-means clustering with similarity vectors can alsoachieve comparable results.For future work, we will incorporate morelinguistic features, such as base chunking, parsetree feature as well as dependency informationinto our system to further improve the perform-ance.AcknowledgementThis research is supported by Project 60873150,60970056 and 90920004 under the NationalNatural Science Foundation of China.
We would z Although SC performs best, KM with silarity vectors achieves comparable results0.7320 units in F-score, slightly lower tthat of SC.z HAC performs worst among all clusteralgorithms.
An observation reveals thatalgorithm always groups the instances ihighly skewed clusters, i.e., one or two cters are extremely large while others usuahave only one instance in each cluster.z It is surprising that S-All slightly outpforms F-All by only 0.0006 units in F-scoThe truth is that, as discussed in the firstperiment, KM using F-All doesn?t consiinstance density while S-All does.
On thecontrary, SC identifies the eign-structure inthe instance space and thus already consid-also like to thank other contributors in the NLPlab at Soochow University.ReferencesJain A, Murty M. 1999.Flynn P. Data clustering : AReview [J].
ACM Computing Surveys,1999,31(3) :2642323F.
Bach and M. Jordan.2004.
Learning spectral clus-tering.
In Proc.
of NIPS-16.
MIT Press, 2004.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In Proceedings of the 12thConference of the European Chapter of the ACL(EACL 2009), pages 103?111.Neill, D. B.
2002.
Fully Automatic Word Sense In-duction by Semantic Clustering.
Cambridge Uni-versity, Master?s Thesis, M.Phil.
in ComputerSpeech.Agirre, E. and Soroa, A.
2007.
Semeval-2007 task 02:Evaluating word sense induction and discrimina-tion systems.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations:7-12Ioannis P. Klapaftis and Suresh Manandhar.
2008.Word sense induction using graphs of collocations.In Proceedings of the 18th European ConferenceOn Artificial Intelligence (ECAI-2008), Patras,Greece, July.
IOS Press.Kannan, R., Vempala, S and Vetta, A.
2004.
On clus-terings: Good, bad and spectral.
J. ACM, 51(3),497?515.Reinhard Rapp.2004.
A practical solution to theproblem of automatic word sense induction.
Pro-ceedings of the ACL 2004 on Interactive posterand demonstration sessions, p.26-es, July 21-26,2004, Barcelona, SpainBordag, S. 2006.
Word sense induction: Triplet-basedclustering and automatic evaluation.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguis-tics (EACL, Trento, Italy).
137--144.Ying Zhao, and George Karypis.2005.
HierarchicalClustering Algorithms for Document Datasets.
Da-ta Mining and Knowledge Discovery, 10, 141?168.
