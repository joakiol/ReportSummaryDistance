Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 327?332,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPolynomial Time Joint Structural Inference for Sentence CompressionXian Qian and Yang LiuThe University of Texas at Dallas800 W. Campbell Rd., Richardson, TX, USA{qx,yangl}@hlt.utdallas.eduAbstractWe propose two polynomial time infer-ence algorithms to compress sentences un-der bigram and dependency-factored ob-jectives.
The first algorithm is exact andrequires O(n6) running time.
It extend-s Eisner?s cubic time parsing algorithmby using virtual dependency arcs to linkdeleted words.
Two signatures are addedto each span, indicating the number ofdeleted words and the rightmost kept wordwithin the span.
The second algorithm isa fast approximation of the first one.
It re-laxes the compression ratio constraint us-ing Lagrangian relaxation, and thereby re-quires O(n4) running time.
Experimentalresults on the popular sentence compres-sion corpus demonstrate the effectivenessand efficiency of our proposed approach.1 IntroductionSentence compression aims to shorten a sentenceby removing uninformative words to reduce read-ing time.
It has been widely used in compres-sive summarization (Liu and Liu, 2009; Li et al,2013; Martins and Smith, 2009; Chali and Hasan,2012; Qian and Liu, 2013).
To make the com-pressed sentence readable, some techniques con-sider the n-gram language models of the com-pressed sentence (Clarke and Lapata, 2008; Mc-Donald, 2006).
Recent studies used a subtree dele-tion model for compression (Berg-Kirkpatrick etal., 2011; Morita et al, 2013; Qian and Liu, 2013),which deletes a word only if its modifier in theparse tree is deleted.
Despite its empirical suc-cess, such a model fails to generate compressionsthat are not subject to the subtree constraint (seeFigure 1).
In fact, we parsed the Edinburgh sen-tence compression corpus using the MSTparser1,1http://sourceforge.net/projects/mstparser/Warrensays the economy continues the steady improvementROOTWarren says steadythe economy continues the improvementROOTFigure 1: The compressed sentence is not a sub-tree of the original sentence.
Words in gray areremoved.and found that 2561 of 5379 sentences (47.6%) donot satisfy the subtree deletion model.Methods beyond the subtree model are also ex-plored.
Trevor et al proposed synchronous treesubstitution grammar (Cohn and Lapata, 2009),which allows local distortion of the tree topolo-gy and can thus naturally capture structural mis-matches.
(Genest and Lapalme, 2012; Thadaniand McKeown, 2013) proposed the joint compres-sion model, which simultaneously considers the n-grammodel and dependency parse tree of the com-pressed sentence.
However, the time complexitygreatly increases since the parse tree dynamical-ly depends on the compression.
They used IntegerLinear Programming (ILP) for inference which re-quires exponential running time in the worst case.In this paper, we propose a new exact decod-ing algorithm for the joint model using dynam-ic programming.
Our method extends Eisner?scubic time parsing algorithm by adding signa-tures to each span, which indicate the number ofdeleted words and the rightmost kept word with-in the span, resulting in O(n6) time complexityandO(n4) space complexity.
We further propose afaster approximate algorithm based on Lagrangianrelaxation, which has TO(n4) running time andO(n3) space complexity (T is the iteration num-ber in the subgradient decent algorithm).
Experi-ments on the popular Edinburgh dataset show that327x x ... x x ... x ...0 (root) 2 i i+1 jx x1 nw0idepw i2depw ijdepw i i+1depw2ibgr w i i+1bgr w i+1 jbgrFigure 2: Graph illustration for the objective func-tion.
In this example, words x2, xi, xi+1, xjarekept, others are deleted.
The value of the ob-jective function is wtok2+ wtoki+ wtoki+1+ wtokj+wdep0i+wdepi2+wdepii+1+wdepij+wbgr2i+wbgrii+1+wbgri+1j.the proposed approach is 10 times faster than ahigh-performance commercial ILP solver.2 Task DefinitionWe define the sentence compression task as: givena sentence composed of n words, x = x1, .
.
.
, xn,and a length L ?
n, we need to remove (n ?
L)words from x, so that the sum of the weights ofthe dependency tree and word bigrams of the re-maining part is maximized.
Formally, we solvethe following optimization problem:maxz,y?iwtokizi+?i,jwdepijzizjyij(1)+?i<jwbgrijzizj?i<k<j(1?
zk)s.t.
z is binary ,?izi= Ly is a projective parse tree over thesubgraph: {xi|zi= 1}where z is a binary vector, ziindicates xiis kep-t or not.
y is a square matrix denoting the pro-jective dependency parse tree over the remainingwords, yijindicates if xiis the head of xj(notethat each word has exactly one head).
wtokiis theinformativeness of xi, wbgrijis the score of bigramxixjin an n-gram model, wdepis the score of de-pendency arc xi?
xjin an arc-factored depen-dency parsing model.
Hence, the first part of theobjective function is the total score of the kep-t words, the second and third parts are the scoresof the parse tree and bigrams of the compressedsentence, zizj?i<k<j(1?
zk) = 1 indicates bothxiand xjare kept, and are adjacent after compres-sion.
A graph illustration of the objective functionis shown in Figure 2.Warren says steadythe economy continues the improvementROOTFigure 3: Connect deleted words using virtual arc-s.3 Proposed Method3.1 Eisner?s Cubic Time Parsing AlgorithmThroughout the paper, we assume that all the parsetrees are projective.
Our method is a generaliza-tion of Eisner?s dynamic programming algorithm(Eisner, 1996), where two types of structures areused in each iteration, incomplete spans and com-plete spans.
A span is a subtree over a number ofconsecutive words, with the leftmost or the right-most word as its root.
An incomplete span denotedas Iijis a subtree inside a single arc xi?
xj, withroot xi.
A complete span is denoted as Cij, wherexiis the root of the subtree, and xjis the furthestdescendant of xi.Eisner?s algorithm searches the optimal tree ina bottom up order.
In each step, it merges twoadjacent spans into a larger one.
There are tworules for merging spans: one merges two completespans into an incomplete span, the other merges anincomplete span and a complete span into a largecomplete span.3.2 Exact O(n6) Time AlgorithmFirst we consider an easy case, where the bigramscores wbgrijin the objective function are ignored.The scores of unigrams wtokican be transferedto the dependency arcs, so that we can remove al-l linear terms wtokizifrom the objective function.That is:?iwtokizi+?i,jwdepijzizjyij=?i,j(wdepij+ wtokj)zizjyijThis can be easily verifed.
If zj= 0, then in bothequations, all terms having zjare zero; If zj= 1,i.e., xjis kept, since it has exactly one head wordxkin the compressed sentence, the sum of theterms having zjis wtokj+ wdepkjfor both equations.Therefore, we only need to consider the scoresof arcs.
For any compressed sentence, we couldaugment its dependency tree by adding a virtual328i i+1i i+1+ =i jr+1 j+ =i ri ji+1 j+ =i i+1... ...i jr j+ =i rCase 1Case 2Case 3Case 4Figure 4: Merging rules for dependency-factoredsentence compression.
Incomplete spans andcomplete spans are represented by trapezoids andtriangles respectively.arc i?
1 ?
i for each deleted word xi.
If the firstword x1is deleted, we connect it to the root of theparse tree x0, as shown in Figure 3.
In this way,we derive a full parse tree of the original sentence.This is a one-to-one mapping.
We can reverselyget the the compressed parse tree by removing allvirtual arcs from the full parse tree.
We restrictthe score of all the virtual arcs to be zero, so thatscores of the two parse trees are equivalent.Now the problem is to search the optimal fullparse tree with n?
L virtual arcs.We modify Eisner?s algorithm by adding a sig-nature to each span indicating the number of vir-tual arcs within the span.
Let Iij(k) and Cij(k)denote the incomplete and complete spans with kvirtual arcs respectively.
When merging two span-s, there are 4 cases, as shown in Figure 4.?
Case 1 Link two complete spans by a virtualarc : Iii+1(1) = Cii(0) + Ci+1i+1(0).The two complete spans must be single word-s, as the length of the virtual arc is 1.?
Case 2 Link two complete spans by a non-virtual arc: Iij(k) = Cir(k?)+Cjr+1(k??
), k?+k?
?= k.?
Case 3 Merge an incomplete span and a com-plete span.
The incomplete span is coveredby a virtual arc: Iij(j ?
i) = Iii+1(1) +Ci+1j(j ?
i ?
1).
The number of the virtu-al arcs within Ci+1jmust be j ?
i ?
1, sincethe descendants of the modifier of a virtualarc xjmust be removed.?
Case 4 Merge an incomplete span and a com-plete span.
The incomplete span is coveredby a non-virtual arc: Cij(k) = Iir(k?)
+Crj(k??
), k?+ k?
?= k.The score of the new span is the sum of the twospans.
For case 2, the weight of the dependencyarc i ?
j, wdepijis also added to the final score.The root node is allowed to have two modifiers:one is the modifier in the compressed sentence, theother is the first word if it is removed.For each combination, the algorithm enumer-ates the number of virtual arcs in the left and rightspans, and the split position (e.g., k?, k?
?, r in case2), thus it takes O(n3) running time.
The overalltime complexity is O(n5) and the space complex-ity is O(n3).Next, we consider the bigram scores.
The fol-lowing proposition is obvious.Proposition 1.
For any right-headed span IijorCij, i > j, words xi, xjmust be kept.Proof.
Suppose xjis removed, there must be a vir-tual arc j?
1 ?
j which is a conflict with the factthat xjis the leftmost word.
As xjis a descendantof xi, ximust be kept.When merging two spans, a new bigram is cre-ated, which connects the rightmost kept words inthe left span and the leftmost kept word in the rightspan.
According to the proposition above, if theright span is right-headed, its leftmost word is kep-t.
If the right span is left-headed, there are twocases: its leftmost word is kept, or no word in thespan is kept.
In any case, we only need to considerthe leftmost word in the right span.Let Iij(k, p) and Cij(k, p) denote the single andcomplete span with k virtual arcs and the right-most kept word xp.
According to the propositionabove, we have, for any right-headed span p = i.We slightly modify the two merging rulesabove, and obtain:?
Case 2?
Link two complete spans by anon-virtual arc: Iij(k, j) = Cir(k?, p) +Cjr+1(k?
?, j), k?+ k?
?= k. The score of thenew span is the sum of the two spans pluswdepij+ wbgrp,r+1.329?
Case 4?
Merge an incomplete span and acomplete span.
The incomplete span is cov-ered by a non-virtual arc.
For left-headedspans, the rule is Cij(k, q) = Iir(k?, p) +Crj(k?
?, q), k?+ k?
?= k, and the score ofthe new span is the sum of the two span-s plus wbgrpr; for right-headed spans, the ruleis Cij(k, i) = Iir(k?, i) + Crj(k?
?, r), and thescore of the new span is the sum of the twospans.The modified algorithm requires O(n6) runningtime and O(n4) space complexity.3.3 Approximate O(n4) Time AlgorithmIn this section, we propose an approximate algo-rithm where the length constraint?izi= L is re-laxed by Lagrangian Relaxation.
The relaxed ver-sion of Problem (1) ismin?maxz,y?iwtokizi+?i,jwdepijzizjyij(2)+?i<jwbgrijzizj?i<k<j(1?
zk)+?(?izi?
L)s.t.
z is binaryy is a projective parse tree over thesubgraph: {xi|zi= 1}Fixing ?, the optimal z,y can be found using asimpler version of the algorithm above.
We dropthe signature of the virtual arc number from eachspan, and thus obtain an O(n4) time algorithm.
S-pace complexity is O(n3).
Fixing z,y, the dualvariable is updated by?
= ?
+ ?(L?
?izi)where ?
> 0 is the learning rate.
In this paper, ourchoice of ?
is the same as (Rush et al, 2010).4 Experiments4.1 Data and SettingsWe evaluate our method on the data set from(Clarke and Lapata, 2008).
It includes 82newswire articles with manually produced com-pression for each sentence.
We use the same par-titions as (Martins and Smith, 2009), i.e., 1,188sentences for training and 441 for testing.Our model is discriminative ?
the scores ofthe unigrams, bigrams and dependency arcs arethe linear functions of features, that is, wtoki=vTf(xi), where f is the feature vector of xi, and vis the weight vector of features.
The learning taskis to estimate the feature weight vector based onthe manually compressed sentences.We run a second order dependency parsertrained on the English Penn Treebank corpus togenerate the parse trees of the compressed sen-tences.
Then we augment these parse trees byadding virtual arcs and get the full parse treesof their corresponding original sentences.
In thisway, the annoation is transformed into a set ofsentences with their augmented parse trees.
Thelearning task is similar to training a parser.
We runa CRF based POS tagger to generate POS relatedfeatures.We adopt the compression evaluation metric asused in (Martins and Smith, 2009) that measuresthe macro F-measure for the retained unigrams(Fugr), and the one used in (Clarke and Lapata,2008) that calculates the F1 score of the grammat-ical relations labeled by RASP (Briscoe and Car-roll, 2002).We compare our method with other 4 state-of-the-art systems.
The first is linear chain CRFs,where the compression task is casted as a bina-ry sequence labeling problem.
It usually achieveshigh unigram F1 score but low grammatical rela-tion F1 score since it only considers the local inter-dependence between adjacent words.
The secondis the subtree deletion model (Berg-Kirkpatrick etal., 2011) which is solved by integer linear pro-gramming (ILP)2.
The third one is the bigrammodel proposed by McDonald (McDonald, 2006)which adopts dynamic programming for efficientinference.
The last one jointly infers tree struc-tures alongside bigrams using ILP (Thadani andMcKeown, 2013).
For fair comparison, system-s were restricted to produce compressions thatmatched their average gold compression rate ifpossible.4.2 FeaturesThree types of features are used to learn our mod-el: unigram features, bigram features and depen-dency features, as shown in Table 1.
We also usethe in-between features proposed by (McDonald et2We use Gurobi as the ILP solver in the paper.http://www.gurobi.com/330Features for unigram xiwi?2, wi?1, wi, wi+1, wi+2ti?2, ti?1, ti, ti+1, ti+2witiwi?1wi, wiwi+1ti?2ti?1, ti?1ti, titi+1, ti+1ti+2ti?2ti?1ti, ti?1titi+1, titi+1ti+2whether wiis a stopwordFeatures for selected bigram xixjdistance between the two words: j ?
iwiwj, wi?1wj, wi+1wj, wiwj?1, wiwj+1titj, ti?1tj, ti+1tj, titj?1, titj+1Concatenation of the templates above{titktj|i < k < j}Dependency Features for arc xh?
xmdistance between the head and modifier h?mdependency typedirection of the dependency arc (left/right)whwm, wh?1wm, wh+1wm, whwm?1, whwm+1thtm, th?1tm, th+1tm, thtm?1, thtm+1th?1thtm?1tm, thth+1tm?1tmth?1thtmtm+1, thth+1tmtm+1Concatenation of the templates above{thtktm|xklies between xhand xm}Table 1: Feature templates.
widenotes the wordform of token xiand tidenotes the POS tag of xi.al., 2005), which were shown to be very effectivefor dependency parsing.4.3 ResultsWe show the comparison results in Table 2.
Asexpected, the joint models (ours and TM13) con-sistently outperform the subtree deletion model, s-ince the joint models do not suffer from the sub-tree restriction.
They also outperform McDon-ald?s, demonstrating the effectiveness of consid-ering the grammar structure for compression.
Itis not surprising that CRFs achieve high unigramF scores but low syntactic F scores as they do notSystem C Rate FuniRASP Sec.Ours(Approx) 0.68 0.802 0.598 0.056Ours(Exact) 0.68 0.805 0.599 0.610Subtree 0.68 0.761 0.575 0.022TM13 0.68 0.804 0.599 0.592McDonald06 0.71 0.776 0.561 0.010CRFs 0.73 0.790 0.501 0.002Table 2: Comparison results under various qualitymetrics, including unigram F1 score (Funi), syn-tactic F1 score (RASP), and compression speed(seconds per sentence).
C Rate is the compressionratio of the system generated output.
For fair com-parison, systems were restricted to produce com-pressions that matched their average gold com-pression rate if possible.consider the fluency of the compressed sentence.Compared with TM13?s system, our model withexact decoding is not significantly faster due to thehigh order of the time complexity.
On the oth-er hand, our approximate approach is much moreefficient, about 10 times faster than TM13?
sys-tem, and achieves competitive accuracy with theexact approach.
Note that it is worth pointingout that the exact approach can output compressedsentences of all lengths, whereas the approximatemethod can only output one sentence at a specificcompression rate.5 ConclusionIn this paper, we proposed two polynomial timedecoding algorithms using joint inference for sen-tence compression.
The first one is an exac-t dynamic programming algorithm, and requiresO(n6) running time.
This one does not showsignificant advantage in speed over ILP.
The sec-ond one is an approximation of the first algorith-m.
It adopts Lagrangian relaxation to eliminate thecompression ratio constraint, yielding lower timecomplexity TO(n4).
In practice it achieves nearlythe same accuracy as the exact one, but is muchfaster.3The main assumption of our method is that thedependency parse tree is projective, which is nottrue for some other languages.
In that case, ourmethod is invalid, but (Thadani and McKeown,2013) still works.
In the future, we will study thenon-projective cases based on the recent parsingtechniques for 1-endpoint-crossing trees (Pitler etal., 2013).AcknowledgmentsWe thank three anonymous reviewers for theirvaluable comments.
This work is partly support-ed by NSF award IIS-0845484 and DARPA underContract No.
FA8750-13-2-0041.
Any opinion-s expressed in this material are those of the au-thors and do not necessarily reflect the views ofthe funding agencies.ReferencesTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL-HLT, pages 481?490, June.3Our code is available at http://code.google.com/p/sent-compress/331T.
Briscoe and J. Carroll.
2002.
Robust accurate statis-tical annotation of general text.Yllias Chali and Sadid A. Hasan.
2012.
On the effec-tiveness of using sentence compression models forquery-focused multi-document summarization.
InProceedings of COLING, pages 457?474.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: An integer linearprogramming approach.
J. Artif.
Intell.
Res.
(JAIR),31:399?429.Trevor Cohn and Mirella Lapata.
2009.
Sentencecompression as tree transduction.
J. Artif.
Int.
Res.,34(1):637?674, April.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: an exploration.
In Pro-ceedings of COLING.Pierre-Etienne Genest and Guy Lapalme.
2012.
Fullyabstractive approach to guided summarization.
InProceedings of the ACL, pages 354?358.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013.Document summarization via guided sentence com-pression.
In Proceedings of EMNLP, October.Fei Liu and Yang Liu.
2009.
From extractive to ab-stractive meeting summaries: Can it be done bysentence compression?
In Proceedings of ACL-IJCNLP 2009, pages 261?264, August.Andr?e F. T. Martins and Noah A. Smith.
2009.
Sum-marization with a joint model for sentence extractionand compression.
In Proceedings of the Workshopon Integer Linear Programming for Natural Lan-gauge Processing, pages 1?9.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL.Ryan McDonald.
2006.
Discriminative SentenceCompression with Soft Syntactic Constraints.
InProceedings of EACL, April.Hajime Morita, Ryohei Sasano, Hiroya Takamura, andManabu Okumura.
2013.
Subtree extractive sum-marization via submodular maximization.
In Pro-ceedings of ACL, pages 1023?1032, August.Emily Pitler, Sampath Kannan, and Mitchell Marcus.2013.
Finding optimal 1-endpoint-crossing trees.
InTransactions of the Association for ComputationalLinguistics, 2013 Volume 1.Xian Qian and Yang Liu.
2013.
Fast joint compressionand summarization via graph cuts.
In Proceedingsof EMNLP, pages 1492?1502, October.Alexander M Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decomposi-tion and linear programming relaxations for naturallanguage processing.
In Proceedings of EMNLP.Kapil Thadani and Kathleen McKeown.
2013.
Sen-tence compression with joint structural inference.
InProceedings of the CoNLL, August.332
