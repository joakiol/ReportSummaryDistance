Interpreting and Generating IndirectAnswersNancy Green"University of North Carolina atGreensboroSandra Carberry tUniversity of DelawareThis paper presents an implemented computational model for interpreting and generating indirectanswers to yes-no questions in English.
Interpretation and generation are treated, respectively,as recognition of and construction of a responder's discourse plan for a full answer.
An indirectanswer is the result of the responder providing only part of the planned response, but intending forhis discourse plan to be recognized by the questioner.
Discourse plan construction and recognitionmake use of shared knowledge of discourse strategies, represented in the model by discourse planoperators.
In the operators, coherence relations are used to characterize types of informationthat may accompany each type of answer.
Recognizing a mutually plausible coherence relationobtaining between the actual response and a possible direct answer plays an important role inrecognizing the responder's discourse plan.
During generation, stimulus conditions model aspeaker's motivation for selecting a satellite.
Also during generation, the speaker uses his owninterpretation capability to determine what parts of the plan are inferable by the hearer and thus donot need to be explicitly given.
The model provides wider coverage than previous computationalmodels for generating and interpreting indirect answers and extends the plan-based theory ofimplicature in several ways.1.
IntroductionIn the following example, 1 Q asks a question in (1)i and R provides the requestedinformation in (1)iii, although not explicitly giving (1)ii.
(In this paper, we use squarebrackets as in (1)ii to indicate information which, in our judgment, the speaker in-tended to convey but did not explicitly state.
For consistency, we refer to the ques-tioner and responder as Q and R, respectively.
For readability, we have standardizedpunctuation and capitalization and have omitted prosodic information from sourcessince it is not used in our model.
)(1) i. Q: Actually you'l l  probably get a car won' t  you as soon as you getthere?ii.
R: \[No.\]iii.
I can't drive.Interpreting such responses, which we refer to as indirect answers, requires the hearerto derive a conversational implicature (Grice 1975).
For example, the inference that RDepartment ofMathematical Sciences, Greensboro, NC 27412-5001t Department ofComputer and Information Sciences, Newark, DE 197161 Based on an example on page 220 in Stenstr6m (1984).
The reader may assume that any unattributedexamples in the paper are constructed.
(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 3will not get a car on arrival, although licensed by R's use of (1)iii in some discoursecontexts, is not a semantic onsequence of the proposition that R cannot drive.According to one study of spoken English (Stenstr6m 1984) (described in Sec-tion 2), 13% of responses to certain yes-no questions were indirect answers.
Thus, arobust dialogue system should be able to interpret indirect answers.
Furthermore, thereare good reasons for generating an indirect answer instead of just a yes or no answer.First, an indirect answer may be considered more polite than a direct answer (Brownand Levinson 1978).
For example, in (1)i, Q has indicated (by the manner in whichQ expressed the question) that Q believes it likely that R will get a car.
By avoidingexplicit disagreement with this belief, the response in (1)iii would be considered morepolite than a direct answer of (1)ii.
Second, an indirect answer may be more efficientthan a direct answer.
For example, even if (1)ii is given, including (1)iii in R's responsecontributes toefficiency by forestalling and answering a possible follow-up of well, whynot?
from Q, which can be anticipated since the form of Q's question suggests thatQ may be surprised by a negative answer.
Third, an indirect answer may be used toavoid misleading Q (Hirschberg 1985), as illustrated in (2).
2(2) i. Q: Have you gotten the letters yet?ii.
R: I've gotten the letter from X.This example illustrates a case in which, provided that R had gotten some but not allof the letters in question, just yes would be untruthful and just no would be misleading(since Q might conclude from the latter that R had gotten none of them).We have developed a computational model, implemented in Common LISP, forinterpreting and generating indirect answers to yes-no questions in English (Green1994).
By a yes-no question we mean one or more utterances used as a request by Qthat R convey R's evaluation of the truth of a proposition p. Consisting of one or moreutterances, an indirect answer is used to convey, yet does not semantically entail, R'sevaluation of the truth of p, i.e., that p is true, that p is false, that p might be true, thatp might be false, or that p is partially true.
In contrast, a direct answer entails R's eval-uation of the truth of p. The model presupposes that Q and R mutually believe thatQ's question has been understood by R as intended by Q, that Q's question is appro-priate, and that R can provide one of the above answers.
Furthermore, it is assumedthat Q and R are engaged in a cooperative and polite task-oriented dialogue.
3 Themodel is based upon examples of uses of direct and indirect answers found in tran-scripts of two-person telephone conversations between travel agents and their clients(SRI 1992), examples given in previous tudies (Brown and Levinson 1978; Hirschberg1985; Kiefer 1980; Levinson 1983; Stenstr6m 1984) and constructed examples reflectingour judgments.To give an overview of the model, generation and interpretation are treated, re-spectively, as construction of and recognition of the responder's discourse plan spec-ification for a full answer.
In general, a discourse plan specification (for the sake ofbrevity, hereafter referred to as discourse plan) explicitly relates a speaker's beliefsand discourse goals to his program of communicative actions (Pollack 1990).
Dis-course plan construction and recognition make use of the beliefs that are presumed2 (2) is Hirschberg's example (59).3 We assume that it is worthwhile to model politeness-motivated language behavior for both generationand interpretation.
For example in generation, it would seem to be a desirable trait for a software agentthat interacts with humans.
In interpretation, it would contribute to the robustness of the interpreter.390Green and Carberry Indirect Answersto be shared by the participants, as well as shared knowledge of discourse strategies,represented in the model by a set of discourse plan operators encoding generic pro-grams of communicative actions for conveying full answers.
A full answer consistsof a direct answer, which we refer to as the nucleus, and "extra" appropriate infor-mation, which we refer to as the satellite(s).
4 In the operators, coherence relationsare used to characterize types of satellites that may accompany each type of answer.Stimulus conditions are used to characterize the speaker's motivation for including asatellite.
An indirect answer is the result of the speaker (R) expressing only part of theplanned response, i.e., omitting the direct answer (and possibly more), but intendingfor his discourse plan to be recognized by the hearer (Q).
Furthermore, we argue thatbecause of the role of interpretation i generation, Q's belief that R intended for Q torecognize the answer is warranted by Q's recognition of the plan.The inputs to the interpretation component of the model (a model of Q's inter-pretation of an indirect answer) are the semantic representation of the questionedproposition, the semantic representation f the utterances given by R during R's turn,shared pragmatic knowledge, and Q's beliefs, including those presumed by Q to beshared with R. (Beliefs presumed by an agent to be shared by another agent are here-after referred to as shared beliefs, and those that are not presumed to be shared asnonshared beliefs).
5 The output is a set of alternative discourse plans that might beascribed to R by Q, ranked by plausibility.
R's inferred discourse plan provides theintended answer and possibly other information about R's beliefs and intentions.
Theinputs to the generation component (a model of R's construction of a response) are thesemantic representation f the questioned proposition, shared pragmatic knowledge,and R's beliefs (both shared and nonshared).
The output of generation is R's discourseplan for a full answer, including a specification of which parts of the plan do not needto be explicitly given by R, i.e., which parts should be inferable by Q from the rest ofthe answer.
6This paper describes the knowledge and processes provided in our model forinterpreting and generating indirect answers.
(The model is not intended as a cogni-tive model, i.e., we are not claiming that it reflects the participants' cognitive statesduring the time course of comprehension and generation.
Rather, its purpose is tocompute the end products of comprehension and generation, and to contribute to acomputational theory of conversational implicature.)
As background, Section 2 de-scribes some relevant generalizations about questions and answers in English.
Sec-tion 3 describes the reversible knowledge in our model, i.e., knowledge used both ininterpretation and generation of indirect answers.
Sections 4 and 5 describe the inter-pretation and generation components, respectively.
Section 5 includes a description ofadditional pragmatic knowledge required for generation.
Section 6 provides an eval-uation of the work.
Finally, the last section discusses future research and provides asummary.4 This terminology was adopted from Rhetorical Structure Theory (Mann and Thompson 1983, 1988),discussed inSection 2.5 Our notion of shared belief is similar to the notion of one-sided mutual belief (Clark and Marshall1981).
However, following Thomason (1990), ashared belief is merely represented in the conversationalrecord as i f  it were mutually believed, although each participant eed not actually believe it.6 However, our model does not address the interesting question of under what conditions a directanswer should be given explicitly even when it is inferable from other parts of the response.
For somerelated work on the function of redundant information, see Walker (1993).391Computational Linguistics Volume 25, Number 32.
BackgroundThis section begins with some results of a corpus-based study of questions and re-sponses in English that provide the motivation for the notion of a full answer inour model.
Next, we describe informally how coherence relations (similar to subject-matter elations of Rhetorical Structure Theory \[Mann and Thompson 1983, 1988\]) areused to characterize the possible types of indirect answers handled in our model.2.1 Descriptive Study of Questions and ResponsesStenstr6m (1984) describes characteristics of questions and responses in English, basedon her study of a corpus of 25 conversations (face-to-face and telephone).
She foundthat 13% of responses to polar questions (typically expressed as subject-auxilliary in-verted questions) were indirect answers, and that 7% of responses to requests for con-firmation (expressed as tag-questions and declaratives) were indirect.
7 Furthermore,she points out the similarity in function of indirect answers to the extra information,referred to as qualify acts in her classification scheme, often accompanying direct an-swers (Stenstr/)m 1984).
8Stenstr6m notes that both are used?
to answer an implicit wh-question, as in (3), 9(3) i. Q: Isn't your  country seat there somewhere?ii.
R: \[Yes/No\].iii.
Stoke d'Abernon.?
for social reasons, as in (4),(4) i. Q: Did you go to his lectures?ii.
R: \[Yes.\]iii.
Oh he had a really caustic sense of humour  actually.?
to provide an explanation, as in (5),(5) i. Q: And also did you find my blue and green striped tie?ii.
R: \[No.\]iii.
I haven't  looked for it.
(6)or to provide clarification, as in (6).i.
Q: I don' t  think you've been upstairs yet.ii.
R: \[Yes, I have been upstairs.\]iii.
Um only just to the loo.In the above examples, coherence would not be affected by making the associateddirect answer explicit.
She suggests that the main distinction between qualify acts andindirect answers is the absence or presence of a direct answer.7 Both of these types of requests are classified as yes-no questions in our model.
Also, in Stenstr6m'sscheme, an utterance may be classified as performing more than one function.
For example, anutterance may be classified as both a polar question and a request for identification (i.e., an implicitwh-question).8 Other types of acts noted by StenstrOm as possibly accompanying direct answers, amplify and expand,are not relevant to the problem of modeling indirect answers,9 (3), (4), (5), and (6) are based on StenstrOm's (65), (67), (68), and (142), respectively.
In (3) either a yes orno could be conveyed, epending upon how there is interpreted and shared background knowledgeabout he location of Stoke d'Abernon.392Green and Carberry Indirect AnswersThus, in our model, the notion of a full answer is used to model both indirectanswers and direct answers accompanied by qualify acts.
A full answer consists of adirect answer, which we refer to as the nucleus, and possibly extra information of var-ious types, which we refer to as satellites} ?
Then, an indirect answer can be modeledas the result of R giving one or more satellites of the full answer, without giving thenucleus explicitly, but intending for the full answer to be recognized.
A benefit of thisapproach is that it also can be used to model  the generation of qualify acts accom-panying direct answers.
(That is, a qualify act would be a result of R providing thesatellite(s) along with an explicit nucleus.)
In the next section, we informally describehow different ypes of satellites of full answers (i.e., types of indirect answers) can becharacterized.2.2 Characterizing Types of Indirect AnswersConsider the constructed responses hown in (1) through (5) of Table 1, which arerepresentative of the types of full answers handled in our model, u The (a) sentencesare yes-no questions and each (b) sentence xpresses a possible type of direct answer.
12Each of the sentences labeled (c) through (e) could accompany the preceding (b) sen-tence in a full answer, ~3 or could be used without (b), i.e., as an indirect answer usedto convey the answer given in (b).
Also, to the right of each of the (c)-(e) sentences ia name intended to suggest he type of relation holding between that sentence and theassociated (b) sentence.
For example, (lc) provides a condition for the truth of (lb),(ld) elaborates upon (lb), and (le) provides the agent's motivation for (lb).
Many ofthese relations are similar to the subject-matter relations of Rhetorical Structure The-ory (RST) (Mann and Thompson 1983, 1988), a general theory of discourse coherence.Thus, we refer to these as coherence relations.
Other sentences providing the sametype of information, i.e., satisfying the same coherence relation, could be substitutedfor each (c)-(e) sentence without destroying coherence.
For example, another plau-sible condition could be substituted for (lc).
Thus, as this table illustrates, a smallset of coherence relations characterizes a wide range of possible indirect answers} 4Furthermore, as it illustrates, certain coherence relations are characteristic of only oneor two types of answer, e.g., giving a cause instead of yes, or an obstacle insteadof no.To give a brief overview of Rhetorical Structure Theory as it relates to our model,one of the goals of RST is to provide a set of relations for describing the organization ofcoherent ext.
An RST relation is defined as a relation between two text spans, calledthe nucleus and satellite.
The nucleus is the span which is "more essential to thewriter 's purpose \[than the satellite is\]" (Mann and Thompson 1988, 266).
A relationdefinition provides a set of constraints on the nucleus and satellite, and an effectfield.
According to RST, implicit relational proposit ions are conveyed in discourse.10 As noted earlier, this terminology isborrowed from Rhetorical Structure Theory, described below.11 Constructed examples are used here to provide a concise means of demonstrating the classes ofsatellites.12 Specifically, the possible types of direct answers handled in the model are: (lb) that p is true, (2b) that pis false, (3b) that there is some truth to p, (4b) that p may be true, or (5b) that p may be false, where p isthe questioned proposition.13 When more than one of the (c)-(e) sentences is used in the same response, coherence may be improvedby use of discourse connectives.14 However, we are not claiming that this set is exhaustive, i.e., that it characterizes allpossible indirectanswers.393Computational Linguistics Volume 25, Number 3Table 1Examples of coherence relations in full answers.1.2.3.4.5.a.
Are you going shopping tonight?b.
Yes.c.
If I finish my homework.d.
I 'm going to the mall.e.
I need new running shoes.a.
Aren't you going shopping tonight?b.
No.c.
I wouldn't have enough time to study.d.
My car's not running.e.
I 'm going tomorrow night.a.
Is dinner ready?b.
To some extent.c.
The pizza is ready.a.
Is Lynn here?b.
I think so.c.
Her books are here.d.
She's usually here all day.e.
I think she has a meeting here at 5.a.
Is Lynn here?b.
I don't think so.c.
Her books are gone.d.
She's not usually here this late.e.
I think she has a dentist appointmentthis afternoon.ConditionElaborationCauseOtherwiseObstacleContrastContrastResultUsuallyPossible CauseResultUsuallyPossible ObstacleFor example, (7) conveys, in addition to the proposit ional content of (7)i and (7)ii, therelational proposit ion that the 1899 Duryea is in the writer 's  collection of classic cars.
15(7) i. I love to collect classic automobiles.ii.
My favorite car is my 1899 Duryea.Such relational proposit ions are described in RST in a relation definition's effect field.The organization of (7) would be described in RST by the relation of Elaboration,where (7)i is the nucleus and (7)ii a satellite.
To see the usefulness of RST for theanalysis of full answers to yes-no questions, consider (8).
(8) i. Q: Do you collect classic automobiles?ii.
R: Yes.iii.
I recently purchased an Austin-Healey 3000.Although (8)ii is not semantically entailed by (8)iii, R could use (8)iii alone in responseto (8)i to conversationally implicate (8)ii.
Further, just as (7)ii provides an elaboration15 This example is from Mann and Thompson (1983), page 81.394Green and Carberry Indirect AnswersTable 2Similar RST relations.Coherence RelationCauseConditionContrastElaborationObstacleOtherwisePossible-causePossible-obstacleResultUsuallySimilar RST Relation Name(s)Non-Volitional Cause, Purpose, Volitional CauseConditionContrastElaborationOtherwiseNon-Volitional Result, Volitional Resultof (7)i, (8)iii provides an elaboration of (8)ii, whether (8)ii is given explicitly as ananswer or not.
16 Also, in giving just (8)iii as a response, R intends Q to recognize notonly (8)ii but also this relation, i.e., that the car is part of R's collection.Table 2 lists, for each of the coherence relations defined in our model (shown inthe left-hand column), similar RST relations (shown in the right-hand column), if any.Although other RST relations can be used to describe other parts of a response (e.g.,Restatement), only relations that contribute to the interpretation of indirect answers areincluded in our model.
The formal representation f the coherence relations providedin our model is discussed in Section 3.3.
Reversible KnowledgeAs shown informally in the previous section, coherence relations can be used to char-acterize various types of satellites of full answers.
Coherence rules, described in Sec-tion 3.1, provide sufficient conditions for the mutual  plausibil ity of a coherence rela-tion.
During generation, plausibil ity of a coherence relation is evaluated with respectto the beliefs that R presumes to be shared with Q.
During interpretation, the samerules are evaluated with respect o the beliefs Q presumes to be shared with R. Thus,during generation R assumes that a coherence relation that is plausible with respectto his shared beliefs would be plausible to Q as well.
That is, Q ought to be able torecognize the implicit relation between the nucleus and satellite.However, the generation and interpretation of indirect answers requires additionalknowledge.
For example, for R's contribution to be recognized as an answer, theremust  be a discourse expectation (Levinson 1983; Reichman 1985) of an answer.
Also,during interpretation, for a particular answer to be licensed by R, the attribution ofR's intention to convey that answer must  be consistent with Q's beliefs about R'sintentions.
For example, a putative implicature that p holds would not be licensedif R provides a disclaimer that it is not R's intention to convey that p holds.
Thisand other types of knowledge about full answers is represented as discourse planoperators, described in Section 3.2.
In our model, a discourse plan operator capturesshared, domain- independent knowledge that is used, along with coherence rules, by16 This may seem to conflict with the idea in RST that the nucleus, being more essential to the writer'spurpose than a satellite, cannot be omitted.
However, at least in the case of the coherence r lationsplaying a role in our model, it appears that the nucleus need not be given explicitly when it is inferablein the discourse context.395Computational Linguistics Volume 25, Number 3It is mutually plausible to the agent that (cr-obstacle q p) holds,where q is the proposition that a state Sq does not hold during time period tq,and p is the proposition that an event e v does not occur during time period t v,if the agent believes it to be mutually believed that Sq is a preconditionof a typical plan for doing ev,and that tq is before or includes tv,unless it is mutually believed that sq does hold during tq,or that ep does occur during tp.It is mutually plausible to the agent that (cr-obstacle q p) holds,where q is the proposition that a state sq holds during time period tq,and p is the proposition that a state sv does not hold during time period tv,if the agent believes it to be nmtually believed that 8q typically prevents p,and that tq is before or includes tv,unless it is mutually believed that Sq does not hold during lq,or that s v does hold during t v.Figure 1Glosses of two coherence rules for cr-obstacle.the generation component to construct a discourse plan for a full answer.
Interpreta-tion is modeled as inference of R's discourse plan from R's response using the sameset of discourse plan operators and coherence rules.
Inference of R's discourse plancan account for how Q derives an implicated answer, since a discourse plan explicitlyrepresents the relationship of R's communicative acts to R's beliefs and intentions.Together, the coherence rules and discourse plan operators described in this sectionmake up the reversible pragmatic knowledge, i.e., pragmatic knowledge used by boththe generation and interpretation components, of the model.
Other pragmatic knowl-edge, used only by the generation process to constrain content planning, is presentedin Section 5.3.1 Coherence RulesCoherence rules specify sufficient conditions for the plausibility to an agent with re-spect to the agent's hared beliefs (which we hereafter refer to as the mutual  plausi-bility) of a relational proposition (CR q p), where CR is a coherence relation and q andp are propositions.
(Thus, if the relational proposition is plausible to R with respect othe beliefs that R presumes to be shared with Q, R assumes that it would be plausibleto Q, too.)
To give some examples, glosses of some rules for the coherence relation,which we refer to as cr-obstacle are given in Figure 1.17 The first rule characterizes asubclass of cr-obstacle, illustrated in (9), relating the nonoccurrence of an agent's voli-tional action (reported in (9)ii) to the failure of a precondition (reported in (9)iii) of apotential plan for doing the action.
(9) i. Q: Are you going to campus tonight?ii.
R: No.iii.
My car's not running.17 For readability, we have omitted the prefix cr- in Tables 1 and 2.396Green and Carberry Indirect AnswersIn other words, it is mutually plausible to an agent that the propositions conveyed in(9)iii and (9)ii are related by cr-obstacle, provided that the agent has a shared belief thata typical plan for R to go to campus has a precondition that R's car is running.
Thesecond rule in Figure 1 characterizes another subclass of cr-obstacle, illustrated in (10),relating the failure of one condition (reported in (10)i) to the satisfaction of anothercondition (reported in (10)ii).
(10) i. R: My car's not running.ii.
The timing belt is broken.In other words, it is mutually plausible to an agent that the propositions conveyed in(10)ii and (10)i are related by cr-obstacle, provided that the agent has a shared beliefthat having a broken timing belt typically prevents a car from running.Coherence rules are evaluated with respect o an agent's hared beliefs.
Coherencerules and the agent's beliefs are encoded as Horn clauses in the implementation ofour model.
The sources of an agent's hared beliefs include:terminological knowledge: e.g., that driving a car is a type of action,domain knowledge, including- -  domain planning knowledge: e.g., that a subaction of a typicalplan to go to campus is to drive to campus, and that a typicalplan for driving a car has a precondition that the car is running,- -  other domain knowledge: e.g., that a broken timing belttypically prevents a car from running, and?
the discourse context: e.g., that R has asserted that R's car is not running.3.2 Discourse Plan OperatorsThe discourse plan operators provided in the model encode generic programs forexpressing full answers (and subcomponents of full answers).
TM For example, the dis-course plan operators for constructing full yes (Answer-yes) and full no (Answer-no)answers are shown in Figure 2.19The first line of a discourse plan operator, its header, e.g., (Answer-yes s h ?p), givesthe type of discourse action, the participants (s denotes the speaker and h the hearer),and a propositional variable.
(Propositional variables are denoted by symbols prefixedwith "?".)
In top-level operators uch as Answer-yes and Answer-no, the header vari-able would be instantiated with the questioned proposition.
Applicabil ity condit ions,when instantiated, specify necessary conditions for appropriate use of a discourse planoperator.
2?For example, the first applicability condition of Answer-yes and Answer-norequires the speaker and hearer to share the discourse xpectation that the speaker willinform the hearer of the speaker's evaluation of the truth of the questioned propositionp.
Present in each of the five top-level answer operators, this particular applicabilitycondition restricts the use of these operators to contexts where an answer is expected,18 The particular formalism we adopted to encode the operators was chosen to provide a concise andperspicuous organization ofthe knowledge required for our interpretation a d generation components.We make no further claims about he formalism itself.19 There are three other "top-level" operators in the model for expressing the remaining types of fullanswers illustrated in Table 1.20 In general, an applicability condition is a condition that must hold for a plan operator to be invoked,but that a planner will not attempt to bring about (Carberry 1990).397Computational Linguistics Volume 25, Number 3(Answer-yes s h ?p):Applicability conditions:(discourse-expectation(informif s h ?p))(bel s ?p)Nucleus:(inform s h ?p)Satellites:(Use-condition s h ?p)(Use-elaboration s h ?p)(Use-cause s h ?p)Primary goals:(BMB h s ?p)Figure 2(Answer-no s h ?p)Applicabil ity conditions:(discourse-expectation(informif s h ?p)(bel s (not ?p))Nucleus:(inform s h (not ?p))Satellites:(Use-otherwise s h (not ?p))(Use-obstacle s h (not ?p))(Use-contrast s h (not ?p))Primary goals:(BMB h s (not ?p))Discourse plan operators for yes and no answers.and  is needed to account  for the hearer ' s  a t tempt  to in terpret  a response  as an an-swer, even when it is not  a direct  answer,  m The second app l icab i l i ty  cond i t ion  of thetop- level  operators  requires  the speaker  to ho ld  the eva luat ion  of p to be conveyed;e.g., in Answer-no it requires  that the speaker  be l ieve that  p is false.
The pr imary  goalsof a d iscourse  p lan  speci fy  the d iscourse  goals  that  the speaker  in tends  for the hearerto recognize,  n For  example ,  the pr imary  goal  of Answer-yes can be g lossed as the goalthat  Q wi l l  accept  the yes answer,  at least for the purposes  of the conversat ion.
23The nuc leus  and  satel l i tes of a d iscourse  p lan  descr ibe pr imi t ive  or nonpr imi t iveacts to be per fo rmed to achieve the pr imary  goals  of the plan.
24 Inform is a p r imi t iveact that  can be rea l ized directly.
The nonpr imi t ive  acts are def ined by  d iscourse  p lanoperators  themselves.
(Thus, a d iscourse  p lan  may have  a h ierarchica l  structure.)
Aful l  answer  may conta in  zero, one, or more  instances of each type  of satel l i te, and  thedefau l t  (but not  requi red)  order  of nuc leus  and  satel l i tes in a ful l  answer  is the orderg iven in the cor respond ing  operator .Cons ider  the Use-elaboration and Use-obstacle discourse  p lan  operators ,  shown inF igure 3, descr ib ing  poss ib le  satel l i tes of Answer-yes and Answer-no,  respectively.
A l lsatel l i te operators  inc lude a second propos i t iona l  var iab le  referred to as the ex is tent ia l21 Without recourse to the notion of discourse xpectation, it is difficult o account for the interpretationin (9)iii of My car's not running as The speaker isnot going to campus tonight, while blocking interpretationssuch as The speaker will rent a car.
Note that the latter interpretation may be licensed when the discourseexpectation is that R will provide an answer to Are you going to rent a car?
In general, discourseexpectations provide a contextual constraint on what inferences are licensed by the speaker.
(Similarly,it has been argued that scalar implicatures depend on the existence of a salient partially ordered set inthe discourse context; see Section 4.3.)
For a discussion of the overall role of discourse xpectations inour model, see Section 4.2.
One might argue that this type of applicability condition limits thegenerality of the operators and thus, could lead to a proliferation of context-specific operators, whichwould result in inefficient processing.
First, we are not claiming that all discourse operators require thistype of applicability condition, only those operators characterizing discourse-expectation-motivatedunits of discourse.
Second, with an indexing scheme sensitive to discourse xpectations, this would notnecessarily ead to efficiency problems.22 We refer to these as primary to distinguish them from other discourse goals the speaker may have butthat he does not necessarily intend for the hearer to recognize.23 During interpretation (see Section 4.1), in order for the implicature to be licensed, the applicabilityconditions and primary goals of any plan ascribed to R must be consistent with Q's beliefs about R'sbeliefs and goals.
Thus, applicability conditions and primary goals play an important role in cancelingspurious putative implicatures.24 The discourse plan operators in our model are not intended to describe all acts that may accompany adirect answer.
For example, the model does not address the generation ofparts of the response, such asrepetition or restatement, which entail the answer.398Green and Carberry Indirect Answers(Use-elaboration s h ?p):Existential variable: ?qApplicability conditions:(bel s (cr-elaboration ?q ?p))(Plausible (cr-elaboration ?q ?p))Nucleus:(inform s h ?q)Satellites:(Use-cause s h ?q)(Use-elaboration s h ?q)Primary goals:(BMB h s (cr-elaboration ?q ?p))Figure 3Two satellite discourse plan operators.
(Use-obstacle s h ?p):Existential variable: ?qApplicability conditions:(bel s (cr-obstacle ?q ?p))(Plausible (cr-obstacle ?q ?p))Nucleus:(inform s h ?q)Satellites:(Use-obstacle s h ?q)(Use-elaboration s h ?q)Primary goals:(BMB h s (cr-obstacle ?q ?p))variable.
For example, (9)ii-(9)iii could be described by a plan constructed from anAnswer-no discourse plan operatorwhose header variable is instantiated with the proposition p that R isgoing to campus tonight, andwhich has a satellite constructed from a Use-obstacle discourse planoperator whose header variable is instantiated with (not p), theproposition that R is not going to campus tonight, and whose existentialvariable q is instantiated with the proposition that R's car is not running.In general, each satellite operator in our model has applicability conditions andprimary goals analogous to those shown in Figure 3.
(Each satellite operator has aname of the form, Use-CR, where CR is the name of a coherence r lation.)
The first ap-plicability condition of a satellite operator, Use-CR, requires that the speaker believesthat the relational proposition (CR q p) holds for propositions q and p instantiating theexistential variable and header variable, respectively.
The second applicability condi-tion requires that, given the beliefs that the speaker presumes to be shared with thehearer, this relational proposition is plausible.
(Mutual plausibility is evaluated usingthe coherence rules described in Section 3.1.)
The primary goal of a satellite operatorcan be glossed as the goal that the hearer will accept he relational proposition.4.
InterpretationThis section describes the interpretation process.
In our model, implicated answersare derived by an answer recognizer.
Algorithms for the answer ecognizer are de-scribed in Section 4.1.
Of course, dialogue consists of more than questions and answers.Section 4.2 describes the role of the answer ecognizer in a discourse-processing ar-chitecture.
Finally, Section 4.3 discusses how this model relates to previous models ofconversational implicature.4.1 Answer Recognizer4.1.1 Main Algorithm.
The structure of the answer ecognizer is shown in Figure 4.The inputs to the answer ecognizer include:?
the set of discourse plan operators and coherence rules described inSection 3,399Computational Linguistics Volume 25, Number 3I DiscoursePlanlOperatorsI Q's Shared ~_~BeliefsQ's questionDiscourse ExpectationR's turn_l Top-down Planr RecognitionI ITheorem t -\[ HypothesisProver GenerationCoherence Rules IFigure 4Structure of the answer ecognizer.~~I Ranked Set of--~ Plan Ranking CandidateDiscourse Plans?
Q's beliefs (including the discourse xpectation that R will provide ananswer to the questioned proposition p),?
the semantic representation f p, and?
for each utterance performed by R during R's turn, the type ofcommunicative act signaled by its form (e.g., to inform), and thesemantic representation f its content.
25Answer recognition isperformed in two phases.
The goal of the first phase is to derivea set of candidate discourse plans plausibly underlying R's response.
The first phasemakes use of two subcomponents: one that we refer to as the hypothesis generationcomponent, and a theorem prover.
The output of the first phase of answer ecognitionis a set of candidate discourse plans since there may be alternate interpretations of R'sresponse.
The goal of the second phase of answer ecognition is to evaluate the relativeplausibility of each candidate discourse plan.
The final output of answer ecognitionconsists of a partially ordered set of the candidates ranked by plausibility.Plan recognition is primarily top-down, i.e., expectation-driven.
More specifically,Q26 attempts to interpret he response as having been generated from a discourseplan constructed from the discourse plan operators for full answers.
The problem ofreconstructing R's discourse plan has several aspects (to be described in more detailshortly):?
Instantiating discourse plan operators with the questioned propositionand appropriate propositions from R's response.25 The turn in question eed not be the turn immediately following Q's asking of the question, asdiscussed in Section 4.2.
Also, we make the simplifying assumption that R's answer is given within asingle turn.26 For convenience, we refer to the answer ecognizer component as Q, and to the answer generator as R.400Green and Carberry Indirect Answers?
Consistency checking: determining whether the beliefs and goals thatwould be attributed to R by virtue of ascribing a particular discourseplan to R are consistent with Q's beliefs about R's beliefs and goals.?
Coherence evaluation: determining whether a putative satellite of acandidate plan is plausibly coherent, i.e., given a candidate plan's (orsubplan's) nucleus proposition p, putative satellite proposition q, and theputative satellite's coherence relation CR, determining whether Qbelieves that (CR q p) is mutual ly plausible.
Coherence valuation makesuse of the coherence rules described in Section 3.1.?
Hypothesis generation: hypothesizing any "missing parts" of theresponse that are required in order to assimilate acts in R's response intoa coherent candidate plan.
Hypothesis generation also makes use of thecoherence rules.Initially, the header variable of each "top-level" answer discourse plan operator 27is instantiated with the questioned proposition p, i.e., all occurrences of the headervariable are replaced with p. Next, consistency checking is performed to eliminateany candidates whose applicability conditions or pr imary goals are not consistentwith Q's beliefs about R's beliefs and goals.
For all remaining candidates, the answerrecognizer next attempts to recognize an act from R's turn as the nucleus of the plan,i.e., to check whether R gave a direct answer.
If no acts in R's turn match the nucleus,then the nucleus is marked as hypothesized.
For all remaining acts in R's turn, theanswer recognizer attempts to recognize all possible satellites, as specified in eachremaining candidate plan.
In the model the discourse plan operators do not specify arequired ordering of satellites.
2sThe subprocedure of satellite recognition is describedin more detail in Section 4.1.2.4.1.2 Satellite Recognition.
Satellite recognition is the (recursive) process of recogniz-ing an instance of a satellite of a candidate plan.
The inputs consist of:?
sat-op, a discourse plan operator for a possible satellite,?
the proposition p conveyed by the nucleus of the higher-level plan (i.e.,the plan whose satellites are currently being recognized),?
act-list, a list of acts in R's turn that have not yet been assimilated intothe candidate plan,?
cur-act, the current act (inform s h q) in act-list, where s is the speaker, h isthe hearer, and q is the propositional content of the act.The output is a set (possibly empty) of candidate instances of sat-op.
To give a sim-plified, preliminary version of the algorithm, first, the header variable and existentialvariable of sat-op are instantiated with p and q, respectively.
Then, coherence valu-ation and consistency checking are performed.
If successful, cur-act is recognized to27 Five of these are defined in our model, corresponding to the five types of answers illustrated in Table 1.28 The operators do specify a preferred order, however, which is used in generation.
Also, our processmodel includes astructural constraint on satellite ordering.
During interpretation, only instancessatisfying this constraint are considered.
That is, the constraint eliminates interpretations which, in ourjudgment, are not plausible due to incoherence.
For a description of the constraint, see Green (1994).We expect that other such constraints may be incorporated into the process model.401Computational Linguistics Volume 25, Number 3Answer-no\[ii\] Use-obstacle\[iii\] Use-obstacle1vFigure 5Candidate discourse plan with hypotheses.be the nucleus of sat-op, and for each remaining act in act-list, satellite recognition isperformed for each satellite of sat-op.However, the satellite recognition algorithm as described so far would not be ableto handle R's response in (11), since there is no plausible coherence relation in themodel directly relating (11)iv to (11)ii (or to any other direct answer that could berecognized in the model).
(11) i. Q: Are you driving to campus tonight?ii.
R: \[No.\]iii.
\[My car's not running.\]iv.
My car has a broken timing belt.Whenever the answer ecognizer is unable to recognize cur-act as the nucleus of sat-op,a subprocedure we refer to as hypothesis generation is invoked.
Hypothesis genera-tion will be described in detail in the following section.
It returns a set of alternativehypothesized propositions, each of which represents the content of a possible implicitinform act to be inserted at the current point of expanding the candidate plan.
29 Inthis example, the proposition conveyed in (11)iii would be returned as a hypothesizedproposition, which is used to instantiate the existential variable of a Use-obstacle satel-lite, thereby enabling satellite recognition to proceed.
Then, (11)iv can be recognized(without hypothesis generation being required) as a satellite of (11)iii.
Ultimately, theplan shown in Figure 5 would be inferred.
(Only the hierarchical structure and com-municative acts are shown.
By convention, the left-most child of a node is the nucleusand its siblings are the satellites.
Labels of sentences in (11) that could realize a leafnode are used to label the node.
Hypothesized nodes are indicated by square brack-ets.)
The complete satellite recognition algorithm, employing hypothesis generation,is given in Figure 6.29 Thus, hypothesis generation may provide additional inferences, i.e., more than just the implicatedanswer.
Hinkelman (1989) refers to such implicatures, licensed by attributing a plan to an agent, asplan-based implicatures.402Green and Carberry Indirect AnswersINPUTOUTPUTp: proposition from nucleus of higher-level plancur-act: current act, (inform s h q), to be recognizedact-list: list of remaining acts in R's turnop: discourse plan operator (Use-CR s h ?p)sat-cand-set: set of candidate instances of opunderlying part of R's response1.
Instantiate header variable ?p of op with p.2.
Instantiate existential variable ?q of op with q of cur-act.a.
Prove that it is plausible that q and p are related by CR.If not, go to step 2c.b.
Check consistency.
If not consistent, hen go to step 2c;else go to step 3a.c.
Try substituting each q returned by hypothesis generation for ?q:Check consistency and coherence as in steps 2a and 2b.For each q passing both checks, proceed with step 3b.If none pass, then fail.3.
a.
Mark cur-act as used.
Go to step 4.b.
Mark nucleus as hypothesized.4.
For each unused act in act-list, attempt to recognize ach satelliteof op.Figure 6Satellite recognition algorithm.4.1.3 Hypothesis Generation.
Based upon the assumption that the response is co-herent, the goal of hypothesis generation is to fill in missing parts of a candidateplan in such a way that an utterance in R's turn can be recognized as part of theplan.
The use of hypothesis generation broadens the coverage of our model to caseswhere more is missing from a full answer than just the nucleus of a top-level op-erator.
(From the point of view of generation, it enables the construction of a moreconcise, though no less informative, response.)
The hypothesis generation algorithmconstructs chains of mutually plausible propositions, each beginning with the propo-sition (e.g., the proposition conveyed in (11)iv) to be related to a goal proposition ina candidate plan (e.g., the proposition conveyed in (11)ii), and ending with the goalproposition, where each pair of adjacent propositions in the chain is linked by a plau-sible coherence relation.
The algorithm returns the proposition (e.g., the propositionconveyed in (11)iii) immediately preceding the goal proposition in each chain.
Thus,when top-down recognition has reached an impasse, hypothesis generation (atype ofbottom-up data-driven reasoning) provides ahypothesis that enables top-down recog-nition to continue another level of growth.
An example of hypothesis generation isgiven in Section 4.1.5.The algorithm for hypothesis generation, which is given in Figure 7, performs abreadth-first earch subject o a processing constraint on the maximum depth of thesearch tree.
Note that a chain may have a length greater than three, e.g., the chainmay consist of propositions (p0, pl, P2, P3), where P0 is the proposition to be relatedto the candidate plan, p3 is the goal, and P2 would be returned as a hypothesizedproposition.
In such a case, after p2 has been assimilated into the candidate plan, if plis not present in R's turn, then hypothesis generation is invoked again and pl would403Computational Linguistics Volume 25, Number 3INPUTSOUTPUTP0: initial propositionpg: goal propositionGCR: goal coherence relation, i.e., coherence relationthat must hold between hypothesized proposition and PaS: set of coherence relationsN: maximum search depthhypoth-list: list of alternative hypothesized propositions1.
Initialize root of search tree with p0.2.
Expand nodes of tree ill breadth-first order until eitherno more expansion is possible or maximum tree depth of N is reached,whichever happens first.
To expand a node pi:a.
Find all nodes pi+l such that for some relation CR in S,(Plausible (CR Pi pi+l)) is provable.b.
Make each such Pi+l a child of Pi, linked by CR.c.
A goal state is reached whenever Pi+l is pg andCR is identical to GCR.d.
Whenever a goal state is reached, add the parent of pgto hypoth-list.Figure 7Hypothesis generation algorithm.be hypothesized also.
3?
Finally, the search for a proposition Pi+l in step 2a is performedin our implementation using a theorem prover.4.1.4 Ranking Candidate Plans.
Two heuristics are used to rank the relative plausi-bility of the set of candidate plans output by the first phase of answer recognition.First, plausibility decreases as the number of hypotheses in a candidate increases.
(Assuming that all else is equal, it is safer to favor interpretations requiring fewerhypotheses.)
Second, plausibility increases as the number of utterances in R's turnthat are accounted for by the plan increases.
(The more of R's turn accounted for, themore coherent the turn is likely to be, although not all of the utterances in R's turn arenecessarily part of the full answer.)
To give an example, consider the two candidateplans shown in Figure 8, corresponding to alternative interpretations of R's responsein (12).
31(12) i. Q: Are you going to campus tonight?ii.
R: \[No/Yes\]iii.
\[My car's not running.\]iv.
My car has a broken timing belt,v.
\[so\] I 'm going to take the bus.vi.
Do you know how much the fare is?30 The implementation saves the chains to avoid the expense of recomputing intermediate hypotheses.31 This constructed example was designed to illustrate multiple aspects of the model.
In our judgement,normally it would sound more coherent to give (12)v before (12)iv if (12)vi were not included.However, when (12)vi is included, (12)v not only elaborates upon the yes, but also serves asbackground for (12)vi.
Another possible motivation for giving (12)v after (12)iv might be to delaygiving dispreferred information (Levinson 1983), e.g., if the speaker believed that a yes was anunexpected or unwanted answer to (12)i.404Green and Carberry Indirect AnswersAnswer-no Answer-yes\[no\] Use-obstacle \[yes\] Use-elaboration\[iii\] Use-obstacle v Use-causeiv \[iiil Use-causeIivFigure 8Ranking candidate plans.By these heuristics, a yes answer would be the preferred interpretation, since the candi-date Answer-yes plan uses the same number of hypotheses a  the candidate Answer-noplan, and accounts for more of R's response.
((12)vi is not recognized as part of ei-ther answer.)
The preference heuristics are intended to capture local coherence only.Since global information may play a role in selecting the correct interpretation, thehigher-level discourse processor (described in section 4.2) must decide which plan toattribute to the speaker.4.1.5 Answer  Recogn i t ion  Example.
In this section, we illustrate the interpretation findirect answers in the model by describing how the two candidate plans shown inFigure 8 would be derived from R's response of (12)iv through (12)vi.
First, each of thefive top-level answer discourse plan operators would be instantiated with the ques-tioned proposition p,the proposition that R is going to campus tonight.
Assuming thatQ has no beliefs about R's beliefs and goals that are inconsistent with the applicabilityconditions and primary goals of these candidates, none of the candidates would beeliminated yet.
Second, for each candidate the recognizer would check whether thecommunicative act specified in the nucleus was present in R's turn.
In this example,since a direct answer was not explicitly provided by R, the recognizer would mark thenucleus of each candidate as hypothesized.
The hypothesized nucleus of the candidateAnswer-no and Answer-yes plans would be (inform s h (not p)) and (inform s h p), respec-tively.
Next, the recognizer would try to recognize the acts expressed as (12)iv through(12)vi as satellites of each candidate plan.
Assume that these acts are represented as(inform s h piv), (inform s h pv), and (inform s h pvi), respectively.To recognize an instance of a satellite, first, a satellite discourse plan operatorwould be instantiated.
The header variable would be instantiated by unifying thesatellite plan header with the corresponding act in the higher-level plan.
For example,the header variable of a Use-obstacle satellite of an Answer-no candidate would be in-stantiated with (not p) in this example.
The existential variable would be instantiatedwith the proposition conveyed in some utterance to be recognized as a satellite, e.g.,Ply.
However, before a candidate satellite may be attached to the higher-level candi-date plan, the answer ecognizer must verify that the candidate satellite passes thefollowing two tests: First, the candidate satellite's applicability conditions and pri-mary goals must be consistent with Q's beliefs about R's beliefs and goals.
Second,the specified coherence relation must be plausible with respect o the beliefs that Qpresumes to be shared with R, i.e., the satellite's instantiated applicability condition405Computational Linguistics Volume 25, Number 3of the form (Plausible (CR q p)) must be provable using the coherence rules describedin Section 3.
For example, given the beliefs that Q presumes to be shared with R andthe coherence rules provided in the model, the act underlying (12)iv could not be thenucleus of a candidate Use-obstacle satellite of the Answer-no candidate, because therecognizer would not be able to prove that cr-obstacle is a plausible coherence relationholding between Piv and (not p).
On the other hand, the act underlying (12)v wouldbe interpreted as the nucleus of a candidate Use-elaboration satellite of the Answer-yescandidate, since the above tests are satisified, e.g., the recognizer could prove thatcr-elaboration is a plausible coherence relation holding between pv and p.To return to consideration of the recognition of the Answer-no candidate, uponfinding that the act underlying (12)iv cannot serve as a satellite, hypothesis genera-tion would be attempted.
Recall that the goal of hypothesis generation is to supplya hypothesized missing act of the plan so that top-down recognition can continue.Hypothesis generation would search for a chain of plausibly related propositions, be-ginning with the proposition (i.e., Piv) to be related to the candidate Answer-no plan,and ending with the goal proposition (i.e., (not p)).
As mentioned in Section 4.1.3, eachpair of adjacent propositions in the chain must be linked by a plausible coherence re-lation.
In this example, hypothesis generation would construct the chain (piv, piii, (notp)), where both pairs of adjacent propositions would be related by cr-obstacle and Piiiis the hypothesis that R's car is not running.
Hypothesis generation would return theproposition immediately preceding the goal proposition in this chain, i.e., pill.
Thus,piii would be used to instantiate the existential variable of a Use-obstacle satellite of theAnswer-no candidate plan, and satellite recognition would proceed.
(The nucleus of thissatellite would be marked as hypothesized.)
Then, the recognizer would recognize piv(without requiring hypothesis generation) as a Use-obstacle satellite of this Use-obstaclesatellite.
No remaining utterances in R's turn can be related to the candidate Answer-noplan, resulting in the candidate shown on the left in Figure 8.Finally, to finish consideration of the recognition of the Answer-yes candidate, sinceneither the act underlying (12)iv nor the act underlying (12)vi can serve as a satellite ofthe Answer-yes candidate or its Use-elaboration satellite, hypothesis generation wouldagain be invoked.
Hypothesis generation would provide Piii, the hypothesis that R's caris not rtmning, as a plausible xplanation for why R is going to take the bus.
Thus, piiiwould be used to instantiate the existential variable of a Use-cause satellite of the Use-elaboration satellite of the Answer-yes candidate plan, and satellite recognition wouldproceed.
(The nucleus of the Use-cause satellite would be marked as hypothesized.
)Then, the recognizer would recognize piv (without requiring hypothesis generation) asa Use-cause satellite of this Use-cause satellite.
No remaining utterances in R's turn canbe related to the candidate Answer-yes plan, resulting in the candidate shown on theright in Figure 8.Given the shared beliefs and the coherence rules provided in the model, none ofthe utterances in R's turn would be recognized as satellites of the other three top-level candidate answer plans.
Candidates that do not account for any actual partsof the response are eliminated at the end of phase one.
Thus the output of phaseone of interpretation would be just the two candidates hown in Figure 8.
Phasetwo would evaluate the Answer-yes candidate as more preferred than the Answer-nocandidate, since the former interpretation requires the same number of hypothesesand also accounts for more of R's response.4.2 Role of the Answer Recognizer in Discourse ProcessingAs discourse researchers have pointed out (e.g., Reichman 1985; Levinson 1983)) theasking of a yes-no question creates the expectation that R will provide the answer406Green and Carberry Indirect Answers(directly or indirectly), if possible.
Other acceptable, though less preferred, responsesinclude I don't know and replies that provide other helpful information.
Furthermore,an answer need not be given in the turn immediately following the turn in which thequestion was asked.
For example, in (13) the yes-no question in (13)i is not answereduntil (13)v, separated by a request for clarification in (13)ii and its answer in (13)iii.
(13) i. Q: Is Dr. Smith teaching CS360 next semester?ii.
R: Do you mean Dr. Smithson?iii.
Q: Yes.iv.
R: \[No.\]v. He will be on sabbatical next semester.In Carberry's discourse-processing model for ellipsis interpretation (Carberry 1990),a mechanism is provided for updating the shared discourse xpectations of dialogueparticipants throughout a conversation.
Our answer ecognizer would have the follow-ing role in such an architecture: The answer recognizer would be invoked wheneverthe current discourse xpectation is that R will provide an answer.
(If answer recog-nition were unsuccessful, then the discourse processor would invoke other types ofrecognizers for other types of responses.)
The answer recognizer eturns a partiallyordered set (possibly empty) of answer discourse plans that it is plausible to ascribeto R as underlying (part or all of) the turn.
The final choice of which discourse planto ascribe to R should be made by the higher-level discourse processor, since it mustselect an interpretation consistent with the rest of the discourse.4.3 Comparison to Previous Approaches to Conversational ImplicatureGrice (1975) has proposed a theory of conversational implicature to account for certaintypes of conversational inferences.
According to Grice, a speaker may convey morethan the conventional meaning of an utterance by making use of the hearer's expec-tation that the speaker is adhering to general principles of cooperative conversation.Two necessary (but not sufficient) properties of conversational implicatures involvecancelability and speaker intention (Grice 1975; Hirschberg 1985).
First, potential con-versational implicatures may be canceled explicitly, i.e., disavowed by the speaker inthe preceding or subsequent discourse context, or even canceled implicitly given aparticular set of shared beliefs.
In fact, potential implicatures may undergo a changein status from cancelable to noncancelable in the subsequent discourse (Gunji 1981).Second, conversational implicatures are part of the intended meaning of an utter-ance.
Grice proposes everal maxims of cooperative conversation that a hearer uses asjustification for inferring conversational implicatures.
However, Grice's theory is inad-equate as the basis for a computational model of how conversational implicatures arederived.
As frequently noted, Grice's maxims may support spurious or contradictoryinferences.
To date, few computational models have addressed the interpretation ofconversational implicatures.Hirschberg's model (Hirschberg 1985) addresses a class of conversational impli-catures, scalar implicatures, which overlaps with the class of implicated answers ad-dressed in our model.
(That is, scalar implicatures arise in question-answer xchangesas well as in other contexts, and, not all types of implicated answers are scalar im-plicatures.)
According to Hirschberg, a scalar implicature depends upon the existenceof a partially ordered set of values that is salient in the discourse context.
Her modelprovides licensing rules that specify, given such a set, which scalar implicatures are407Computational Linguistics Volume 25, Number 3It is mutually plausible to the agent hat (cr-contrast q p*) holds,where q is a propositionand p* is the proposition that p is partly true,if the agent believes it to be mutually believed that q is less than p in a salient partial order,unless it is mutually believed that p is true or that q is not true.It is mutually plausible to the agent hat (cr-contrast q (not p)) holds,where q and p are propositions,if the agent believes it to be mutually believed that q is an alternate to p in a salient partial order,unless it is mutually believed that p is true or that q is not true.Figure 9Glosses of two coherence rules for cr-contrast.licensed in terms of values in the set that are lower than, alternate to, or higher thanthe value referred to in an utterance.
For example, given a salient partially orderedset such that the value for the letter from X is lower than the value for all of the letters inquestion, in saying (2)ii (repeated below in (14)ii) R licenses the implicature that R hasnot gotten all of the letters in question.
(14) i. Q: Have you gotten the letters yet?ii.
R: I 've gotten the letter from X.In our model, the response in (14)ii would be analyzed as generated from an Answer-hedge discourse plan whose nucleus has not been explicitly given and which has asingle Use-contrast satellite whose nucleus is expressed in (14)ii.
3a The coherence rulesfor cr-contrast, which are based upon the notions elucidated by Hirschberg, are glossedin Figure 9.
33 However, the discourse plan operators in our model also characterize avariety of indirect answers that are not scalar implicatures, i.e., indirect answers basedon the other coherence relations hown in Table 2.A model such as Hirschberg's, which does not take the full response into account,faces certain problems in handling cancellation by the subsequent discourse context("backwards" cancellation).
For example, given a salient partially ordered set such thatgoing to campus is ranked as an alternate to going shopping, Hirschberg's model wouldpredict, correctly in the case of (15) and incorrectly in the case of (16), that R intendedto convey a no.
(15) i. Q: Are you going shopping?ii.
R: \[no\]iii.
I 'm going to campus.iv.
I have a night class.
(16) i. Q: Are you going shopping?ii.
R: \[yes\]32 The nucleus of such a plan conveys that the questioned proposition ispartly but not completely true.33 The uppermost rule in the figure is the one applying to this example.
The other ule applies toUse-contrast satellites of Answer-no plans.408Green and Carberry Indirect Answersiii.
I 'm going to campus.iv.
The bookstore is having a sale.In our model, (16) would be interpreted by recognizing an Answer-yes plan (with aUse-elaboration and a Use-cause satellite underlying (16)iii and (16)iv, respectively) asmore plausible than an Answer-no plan, rather than by use of backwards cancellation.
34In other words, in our model subsequent context can provide evidence for or againsta particular interpretation, since a discourse plan may be expressed by multiple utter-ances.Also, a model such as Hirschberg's provides no explanation for why potential im-plicatures may become noncancelable.
Our model predicts that a potential implicatureof an utterance becomes noncancelable after the point in the conversation when thefull discourse plan accounting for that utterance has been attributed to the speaker.For example, imagine a situation in which Q and R mutually intend to discuss two jobcandidates, A and B.
Also, suppose that they mutually believe that they should notdiscuss any candidate until two letters of recommendation have been received for thecandidate, and further, that both letters for B have been received.
Our model predictsthat the scalar implicature potentially licensed in (17)ii (i.e., that R has not gotten bothletters for A yet) is no longer cancelable after R's turn in (17)iv, since by that point, theparticipants apparently would share the belief that Q had succeeded in recognizingR's discourse plan underlying (17)iiY(17) i. Q: Have you gotten the letters for A yet?ii.
R: I've gotten the letter from X.iii.
Q: Then let's discuss B now.iv.
R: O.K.
I think we should interview B, don't you?Inference of coherence relations has been used in modeling temporal (Lascaridesand Asher 1991; Lascarides, Asher, and Oberlander 1992) and other defeasible dis-course inferences (Hobbs 1978; Dahlgren 1989).
Inference of plausible coherence re-lations is necessary but not sufficient for interpreting indirect answers.
For example,Q also must believe that there is a shared discourse expectation of an answer to aparticular question.
In other words, in our model, discourse plans provide additionalconstraints on the beliefs and intentions of the speaker that a hearer uses in interpret-ing a response.
Another limitation of the above approaches i that they provide noexplanation for the phenomenon of loss of cancelability described above.Plan recognition has been used to model the interpretation of indirect speech acts(Perrault and Allen 1980; Hinkelman 1989) and ellipsis (Carberry 1990; Litman 1986),discourse phenomena that share with conversational implicature the two necessaryconditions described above, cancelablity and speaker intention.
However, these modelsare inadequate for interpreting indirect answers, i.e., for deriving an implicated answerp from an indirect answer q.
In these models, for p to be derivable from q, it is necessary34 of course, in the case where R provides only I'm going to campus, both yes and no interpretations wouldbe inferred as equally plausible in our model.
Although prosodic information isnot used in our model,it is an interesting question for future research whether it can help in recognizing the speaker'sintentions in such cases.35 In other words, it would sound as if R had changed his mind or was contradicting himself if he said Infact I've gotten both letters for A after saying (17)iv.409Computational Linguistics Volume 25, Number 3for the hearer to infer that the speaker is performing or at least constructing a domainplan relating p and q.
However, q need not play such a role in the speaker's inferred oractual domain plans, as shown in (18).
36 That is, it is not necessary to infer that R hasa domain plan involving the renting of a car by X in order to recognize R's intentionto convey no.
)(18) i. Q: X will be renting a car, won't he?ii.
R: \[No.\]iii.
He can't drive.In other words, these models lack requisite knowledge ncoded in our model in termsof possible satellites (based on coherence relations) of top-level discourse plan opera-tors.
Also, the above plan-based models face the same problems as Hirschberg's sincethey do not address multiutterance r sponses.Philosophers (Thomason 1990; McCafferty 1987) have argued for a plan-basedtheory of implicature as an alternative to Grice's theory.
Thomason proposes that im-plicatures are comprehended by a process of accommodation of the conversationalrecord to fit the inferred plans of the speaker.
According to McCafferty, "implicaturesare things that the speaker plans that the hearer believe (and that the hearer can realizethat the speaker plans that the hearer believe)" (p. 18).
He claims that a theory basedupon inferring the speaker's plan avoids the problem of predicting spurious impli-catures, since the spurious implicature would not be part of the speaker's plan.
Ourmodel is consistent with this view of conversational implicature.
McCafferty sketchesa possible plan-based model to account for the implicated yes answer in (19).
37(19) i. Q: Has Smith been dating anyone?ii.
R: \[Yes.\]iii.
He's been flying to New York every weekend.Although it was not McCafferty's intention to provide a computational model, butrather to show the plausibility of a plan-based theory of conversational implicature,some limitations of his suggestions for developing a computational model should benoted.
First, his proposed rules cannot be used to derive an alternate, plausible inter-pretation of (19)iii, in which R scalar implicates a no.
38 Our model can account for bothinterpretations.
The first interpretation would be accounted for by an inferred Answer-yes plan with a Use-elaboration satellite underlying (19)iii, while the latter would beaccounted for by an inferred Answer-no plan with a Use-contrast satellite underlying(19)iii.
More generally, his proposed rules cannot account for types of indirect an-swers described in our model by coherence relations whose definitions do not involveplanning knowledge.
Second, even if rules could be added to McCafferty's model toaccount for a speaker's plan to convey a no by use of (19)iii, his model does not pro-vide a way of using information from other parts of the response, e.g., (20)iv, to helprecognize the intended answer.
As noted earlier, in our model such information can36 (18) is based upon (1), modified for expository purposes.37 (19) is from McCafferty (1987), page 67, and is similar to an example of Grice's.
In a Gricean account,this implicature would be justified in terms of the Maxim of Relevance.38 That is, an interpretation in which flying to New York is mutually believed to be an alternate odatingsomeone ina salient partially ordered set.410Green and Carberry Indirect Answersbe used to provide evidence favoring one candidate discourse plan over another.
(Forexample, (20)iv would be accounted for by the addition of a Use-obstacle satellite tothe Answer-no candidate described above.
)(20) i. Q: Has Smith been dating anyone?ii.
R: \[No.\]iii.
He's been flying to New York every weekend.iv.
Besides, he's married.4.4 SummaryThis section closes with a summary of the argument for the adequacy of our modelas a model of conversational implicature.
As discussed earlier, two necessary condi-tions for conversational implicature are cancelability and speaker intention.
We havedemonstrated that our model can handle forward and backward cancellation, and pro-vides an explanation for the "loss of cancelability" phenomenon.
Regarding speakerintention, in our model a conversationally implicated answer is an answer that Rplanned that Q recognize (and that Q recognizes that R planned that Q recognize).
39We have demonstrated how Q's recognition of R's discourse plan (in particular, thegoal to provide an answer to the question) can be performed using the knowledge andalgorithms in our model.
Furthermore, we argue that Q's recognition of R's intentionthat Q recognize R's plan follows from the role of interpretation in generation, amely,Q and R mutually believe that R will not say what he does unless R believes that Qwill be able to interpret he response as intended.
In our model, during generation(to be described in Section 5), R constructs a model of Q's beliefs (using R's sharedbeliefs), and then simulates Q's interpretation f a trial pruned response.
R's decisionto use the pruned response depends upon whether R believes that Q would still beable to recognize the answer after the plan has been pruned.
During interpretation,given the shared discourse xpectation that R will provide an answer to Q's yes-noquestion, Q's use of (Q's) shared beliefs to interpret the response, and Q's belief that Rexpects that Q will be able to recognize the answer, Q's recognition of a discourse planfor an answer warrants Q's belief that R intended for Q to recognize this intention.5.
GenerationThis section describes our approach to the generation of indirect answers.
Genera-tion is modeled as a two-phase process of discourse plan construction.
First, in thecontent planning phase, a discourse plan for a full answer is constructed.
Second,the plan pruning phase uses the model's own interpretation capability to determinewhat information in the full response does not need to be stated explicitly.
In ap-propriate discourse contexts, i.e., in contexts where the direct answer can be inferredby Q from other parts of the full answer, a plan for an indirect answer is therebygenerated.
When the direct answer must be given explicitly, the result is a plan for adirect answer accompanied by appropriate extra information.
(According to the studymentioned in Section 2 \[Stenstrbm 1984\], 85% of direct answers are accompanied bysuch information.
Thus, it is important to model this type of response as well.
)While the pragmatic knowledge described in Section 3 is sufficient for interpreta-tion, it is not sufficient for the problem of content planning during generation.
Applica-39 Applying McCafferty's description f conversational implicature to indirect answers.411Computational Linguistics Volume 25, Number 3(Use-elaboration s h ?p):Existential variable: ?qApplicability conditions:(bel s (cr-elaboration ?q ?p))(Plausible (cr-elaboration ?q ?p))Stimulus conditions:(answer-ref-indicated s h ?p ?q)(clarify-concept-indicated s h ?p ?q)Nucleus:(inform s h ?q)Satellites:(Use-cause s h ?q)(Use-elaboration s h ?q)Primary goals:(BMB h s (or-elaboration ?q ?p))Figure 10(Use-obstacle s h ?p):Existential variable: ?qApplicability conditions:-(bel s (cr-obstacle ?q ?p))(Plausible (cr-obstacle ?q ?p))Stimulus conditions:(explanation-indicated s h ?p ?q)(excuse-indicated s h ?p ?q)Nucleus:(inform s h ?q)Satellites:(Use-obstacle s h ?q)(Use-elaboration s h ?q)Primary goals:(BMB h s (or-obstacle ?q ?p))Two satellite discourse plan operators with stimulus conditions.bility conditions prevent inappropriate use of a discourse plan.
However, they do notmodel a speaker's motivation for choosing to provide extra information.
Consider (21).
(21) i. Q: I need a ride to the mall.ii.
Are you going shopping tonight?iii.
R: \[No.\]iv.
My car's not running.v.
The timing belt is broken.R's reason for providing the information i (21)iv might have been to give an excuse fornot being able to offer Q a ride, and R's reason for providing the information i  (21)vmight have been to provide an explanation for news in (21)iv that may surprise Q.Furthermore, a full answer might be too verbose if every satellite whose applicabilityconditions held were included in the full answer.
On the other hand, at the time whenhe is asked a question, R may not hold the primary goals of a potential satellite.
(In ourmodel the only goal R is assumed to have initially is the goal to provide the answer.
)Thus, an approach to selecting satellites driven only by these satellite goals would fail.To overcome these problems, we have augmented the satellite discourse plan oper-ators, as described in Section 3, with one or more stimulus conditions.
Two examplesare shown in Figure 10.
Stimulus conditions describe general types of situations inwhich a speaker is motivated to include a satellite during plan construction.
They canbe thought of as situational triggers, which give rise to new speaker goals (i.e., theprimary goals of the satellite operator), and which are the compiled result of deeperplanning based upon principles of cooperativity (Grice 1975) or politeness (Brownand Levinson 1978).
4o In order for a satellite to be included, all of its applicabilityconditions and at least one of its stimulus conditions must be true.40 It was beyond the scope of our research to model  recognition of st imulus conditions.
We argue inSection 5.3, however, that this does not compromise our approach as a model  of conversationalimplicature.412Green and Carberry Indirect AnswersOur methodology for identifying stimulus conditions was to survey linguisticstudies, described in Section 5.1, as well as to analyze the possible motivation ofthe speaker in the examples in our corpus.
The rules used in our model to evaluatestimulus conditions are given in Section 5.2.
Section 5.3 presents our implementedgeneration algorithm, and Section 5.4 illustrates the algorithm with an example.5.1 Linguistic StudiesIn linguistic studies, the reasons given for including extra information 41in a responseto a yes-no question can be categorized as:?
to provide implicitly requested information,?
to provide an explanation for an unexpected answer,?
to qualify a direct answer, or?
politeness-related.5.1.1 Implicitly Requested Information.
As mentioned in Section 2, Stenstr6m claimsthat the typical reason for providing extra information is to answer an implicit wh-question.
Kiefer (1980) observes that several types of yes-no questions, when usedto perform indirect speech acts, have the property that one or both of the "binary"answers (i.e., yes or no) used alone is an inappropriate response to them.
For example,in response to (22)i, 42 when interpreted as (22)ii, an answer of (22)iii or (22)v 43 wouldbe appropriate, but not (22)iv alone.
(22) i. Q: Is John leaving for Stockholm TOMORROW?ii.
Q: When is John leaving for Stockholm?iii.
R: Yes.iv.
R: No.v.
R: No, John is going to leave the day after tomorrow.Kiefer also provides examples of cases where the other binary answer alone is inap-propriate, or where either alone is inappropriate.Clark (1979) studied how different factors may influence the responder's con-fidence that the literal meaning of a question was intended and confidence that aparticular indirect meaning was intended.
In one experiment, in which subjects re-sponded to the question, Do you accept credit cards?, about half of the subjects providedinformation answering an indirect request of What credit cards do you accept?
Clark spec-ulates that the half who included information addressing the indirect request in theirresponse had some, but not necessarily total, confidence that it was intended.According to Levinson (1983), a yes-no question often may be interpreted as aprerequest for another equest, i.e., it may be used in the first position of the sequence41 We are reporting only cases where the extra information may be used as an indirect answer.42 In this example, Kiefer's (11b), we follow Kiefer's use of capitalization toindicate that tomorrow ouldbe stressed in spoken English.43 Kiefer's (21b).413Computational Linguistics Volume 25, Number 3T1-T4, where the occurrence of T3 and T4 are conditional upon R's answer in T2:?
TI: Q makes a prerequest to determine if a precondition of an action tobe requested by Q in T3 holds.?
T2: R gives an answer indicating whether the precondition holds?
T3: Q makes the request?
T4: R responds to the request in T3.Levinson claims that prerequests are used to check whether the planned request (in T3)is likely to succeed so that a dispreferred response to it can be avoided by Q. Anotherreason is that, since receiving an offer is preferred to making a request (Schegloff 1979),by making a prerequest, Q gives R the opportunity to offer whatever Q would requestin T3, i.e., the sequence would then consist of just T1 and T4.
In analyses based onspeech act theory, in a sequence consisting of just T1 and T4, the prerequest would bereferred to as an indirect speech act.5.1.2 Explanation for Unexpected Answer.
Stenstr6m notes that a reason for pro-viding extra information is to provide an explanation justifying a negative answer.
44According to Levinson (1983), the presence of an explanation is a distinguishing fea-ture of dispreferred responses to questions and other second parts of adjacency pairs(Schegloff 1972).
In an adjacency pair, each member of the pair is produced by a differ-ent speaker, and the occurrence of the first part creates the expectation that the secondpart will appear, although not necessarily immediately following the first member.Levinson claims that dispreferred responses to first parts of adjacency pairs can beidentified by structural features uch as:?
use of pauses or displacement,?
prefacing with markers (e.g.
uh or well), appreciations, apologies, orrefusals,?
providing explanations, and?
declinations given in an indirect or mitigated manner.For example in (23), 45 the marker well is used and an explanation is given.
(23) i. Q: What about coming here on the way or doesn't hat give youenough time?ii.
R: Well no I'm supervising hereAlthough Levinson defines preference in terms of structural features, he notes thatthere is a correlation between preference and content.
For example, unexpected an-swers to questions, refusals of requests and offers, and admissions of blame are typi-cally marked with features from the above list.44 She found that 61% of negative direct answers but only 24% of positive direct answers wereaccompanied by qualify acts.45 Levinson's example (55).414Green and Carberry Indirect Answers5.1.3 Avoid Misunderstanding.
Stenstr6m notes that extra information may be givento qualify an answer.
Hirschberg (1985) claims that speakers may give indirect answersto block potential unintended scalar implicatures of a yes or no alone.
For example in(2), repeated below as (24), R's response ispreferable to just no, since that would licensethe incorrect scalar implicature that R had not received any of the letters in question.However, by use of (24)ii in an appropriate discourse context, R is able to conveyexplicitly which letter has been received as well as to conversationally implicate thatR has not gotten the other letters in question.
(24) i. Q: Have you gotten the letters yet?ii.
R: I've gotten the letter from X.5.1.4 Politeness.
StrenstrOm claims that extra information may be given for social rea-sons.
Kiefer notes that extra information may be given as an excuse when the answerindicates that the speaker has failed to fulfill a social obligation.
Brown and Levinson(1978) claim that politeness strategies, which may at times conflict with Gricean max-ims, account for many uses of language.
According to Brown and Levinson, certaincommunicative acts are intrinsically face-threatening acts (FTAs).
That is, doing anFTA is likely to injure some conversational participant's face, or public self-image.For example, orders and requests threaten the recipient's negative face, "the want ...that his actions be unimpeded by others" (p. 67).
On the other hand, disagreement orbearing "bad news" threatens the speaker's positive face, the want to be looked uponfavorably by others.
Further, they claim that politeness strategies can be ranked, andthat the greater the threat associated with a face-threatening act, the more motivateda speaker is to use a higher-numbered strategy.Brown and Levinson propose the following ranked set of strategies (listed in orderfrom lower to higher ank):1..perform the FTA.
(Brown and Levinson claim that this amounts tofollowing Gricean maxims.
)perform the FTA with redressive action, i.e., in a manner that indicatesthat no face threat is intended, using positive politeness trategies(strategies that increase the hearer's positive face).
Such strategiesinclude:..Strategy 1: attending to the hearer's interests or needsStrategy 6: avoiding disagreement, e.g., by displacing an answerperform the FTA with redressive action, using negative politenessstrategies ( trategies for increasing negative face).
These include:Strategy 6: giving an excuse or an apologyperform the FTA off-record, i.e., by use of conversational implicature.In the next section, we provide several stimulus conditions that reflect positive po-liteness trategy 1 and negative politeness trategy 6.
However, although politenessconsiderations may motivate a speaker to convey an answer indirectly, it is beyondthe scope of our generation model to choose between a direct and an indirect answeron this basis.415Computational Linguistics Volume 25, Number 3Table 3Stimulus conditions of discourse plan operators.OperatorUse-causeUse-conditionUse-contrastUse-elaborationUse-obstacleUse-otherwiseUse-possible-causeUse-possible-obstacleUse-resultUse-usuallyStimulus Conditionsexplanation-indicatedexcuse-indicatedclarify-condition-indicatedappeasement-indicatedanswer-ref-indicatedclarify-extent-indicatedsubstitute-indicatedanswer-ref-indicatedclarify-concept-indicatedexplanation-indicatedexcuse-indicatedexplanation-indicatedexcuse-indicatedexplanation-indicatedexcuse-indicatedexplanation-indicatedexcuse-indicatedexplanation-indicatedexplanation-indicated5.2 Stimulus Condit ionsIn this section we provide glosses of rules giving sufficient conditions for the stim-ulus conditions used in our model.
(The rules are encoded as Horn clauses in ourimplementation f the model.)
Table 3 summarizes which stimulus conditions appearin which discourse plan operators.
As mentioned above, for an instance of a satelliteoperator to be selected uring generation, all of its applicability conditions and at leastone of its stimulus conditions must hold.5.2.1 Explanation-indicated.
This stimulus condition appears in all of the operatorsfor providing causal explanations.
For example in (1), repeated below as (25), R givesan explanation of why R won't get a car.
(25) i. Q: Actually you'll probably get a car won't you as soon as you getthere?ii.
R: \[No.\]iii.
I can't drive.R's response may contribute to greater dialogue fficiency by anticipating a follow-uprequest for an explanation.
The rule for this stimulus condition may be glossed as: (aspeaker) s is motivated to give (a hearer) h an explanation for (not p), if s suspects thath suspects that (a proposition) p is true, unless it is the case that s has reason to believethat h will accept (not p) on s's authority, s may acquire the suspicion that h suspectsthat p is true by means of syntactic lues from the yes-no question, e.g., from the formof the question in (25)i.5.2.2 Excuse-indicated.
Although this stimulus condition appears in some of the samecausal operators as explanation-indicated, it represents a different kind of motivation.416Green and Carberry Indirect AnswersA yes-no question may be interpreted as a prerequest.
Thus, a negative answer to ayes-no question used as a prerequest may be interpreted as a refusal.
To soften therefusal, i.e., in accordance with negative politeness trategy 6, the speaker may givean explanation of the negative answer, as illustrated in (21), repeated below in (26).
(26) i. Q: I need a ride to the mall.ii.
Are you going shopping tonight?iii.
R: \[No.\]iv.
My car's not running.v.
The timing belt is broken.The rule for this stimulus condition may be glossed as: s is motivated to give h anexcuse for (not p), if s suspects that h's request, (informifs h p), is a prerequest.
Techniquesfor interpreting indirect speech acts (Perrault and Allen 1980; Hinkelman 1989) can beused to determine whether the rule's antecedent holds.5.2.3 Answer-ref-indicated.
This condition appears in Use-elaboration, illustrated by(27), 46 and in Use-contrast, illustrated by (28).
47(27) i. Q: Did you have a hotel in mind?ii.
\[What hotel did you have in mind?\]iii.
R: \[Yes.\]iv.
There's a Holiday Inn right near where I 'm working.
(28) i. Q: You're on that?ii.
\[Who's on that?\]iii.
R: No no no.iv.
Dave is.In (27), R has interpreted the question in (27)i as a prerequest for the wh-questionshown in (27)ii.
Thus, (27)iv not only answers the question in (27)i but also the an-ticipated wh-question in (27)ii.
Similarly in (28), R may interpret he question in (28)ias a prerequest for the wh-question in (28)ii, and so gives (28)iv to provide an answerto both (28)i and (28)ii.
The rule for this stimulus condition may be glossed as: s ismotivated to provide h with q, if s suspects that h wants to know the referent of aterm t in q.
As in excuse-indicated, techniques for interpreting indirect speech acts canbe used to determine if the rule's antecedent holds.
4846 From SRI Tapes (1992), tape 1.47 Stenstr6m's (102).
A no answer may be conversationally implicated by use of (28)iv alone.48 However, following Clark (1979), the rule does not require that R be certain that Q was making anindirect request.417Computational Linguistics Volume 25, Number 35.2.4 Substitute-indicated.
This condition appears in Use-contrast, illustrated by (29).
(29) i. Q: Do you have Verdi's Otello or Aida?ii.
R: \[No.\]iii.
We have Rigoletto.Although Q may not have intended to use (29)i as a prerequest for the question WhatVerdi operas do you have?, R suspects that the answer to this wh-question might behelpful to Q, and so provides it (in accordance with positive politeness trategy 1).The rule for this stimulus condition may be glossed as: s is motivated to provide hwith q, if s suspects that it would be helpful for h to know the referent of a term t inq.
The rule's antecedent would hold whenever obstacle detection techniques (Allenand Perrault 1980) determine that h's not knowing the referent of t is an obstacle toan inferred plan of h's.
However, not all helpful responses, in the sense described inAllen and Perrault (1980), can be used as indirect answers.
For example, even if theclerk (R) at the music store believes that Q's not knowing the closing time could bean obstacle to Q's buying a recording, a response of (30) alone would not convey nosince it cannot be coherently related to an Answer-no plan.
(30) i. R: We close at 5:30 tonight.5.2.5 Clarify-concept-indicated.
This stimulus condition appears in Use-elaboration, asillustrated in (31).
49(31) i. Q: Do you have a pet?ii.
R: We have a turtle.In (31), R was motivated to elaborate on the type of pet R has since turtles are notprototypical pets.
The rule for this stimulus condition may be glossed as: s is motivatedto clarify p to h with q, if p contains a concept c, and q provides an atypical instanceof c. Stereotypical knowledge would be used to evaluate the rule's antecedent.5.2.6 Clarify-condition-indicated.
This stimulus condition appears in the operatorUse-condition, as illustrated by (32).
5o(32) i. Q: Um let me can I make the reservation and change it by tomorrow?ii.
R: \[Yes.\]iii.
If it's still available.In (32), a truthful yes answer depends on the truth of (32)iii.
The rules for this stimuluscondition may be glossed as: s is motivated to clarify a condition q for p to h if 1) sdoesn't know if q holds, or 2) s suspects that q does not hold.49 Example (177) from Hirschberg (1985).50 From SRI Tapes (1992), tape 10ab.418Green and Carberry Indirect Answers5.2.7 Clarify-extent-indicated.
This stimulus condition appears in Use-contrast, as il-lustrated by (2), repeated below as (33).
(33) i. Q: Have you gotten the letters yet?ii.
R: I've gotten the letter from X.On the strict interpretation of (33)i, Q is asking whether R has gotten all of the letters,but on a looser interpretation, Q is asking if R has gotten any of the letters.
Then, ifR has gotten some but not all of the letters, just yes would be untruthful.
However, ifQ is speaking loosely, then just no might lead Q to erroneously conclude that R hasnot gotten any of the letters.
R's answer circumvents this problem, by conveying theextent o which the questioned proposition (on the strict interpretation) is true.The rules for this stimulus condition may be glossed as: s is motivated to clarify toh the extent q to which p is true, or the alternative q to p which is true, if s suspects thath does not know if q holds, and s believes that q is the highest expression alternative top that does hold.
According to Hirschberg (1985) (following Gazdar), sentences pi andpj (representing the propositional content of two utterances) are expression alternativesif they are the same except for having comparable components ei and ej, respectively.As mentioned earlier, Hirschberg claims that in a discourse context here may be apartial ordering of values that the discourse participants mutually believe to be salient.She claims that the ranking of ei and ej in this ordering can be used to describe theranking of pi and pj.
In the above example, (33)ii is a realization of the highest rueexpression alternative to the questioned proposition, p, i.e., the proposition that R hasgotten all the letters.
515.2.8 Appeasement-indicated.
This stimulus condition appears in Use-contrast, as il-lustrated by (34).
52(34) i. Q: Did you manage to read that section I gave you?ii.
R: I've read the first couple of pages.In (34), R conveys that there is some (though not much) truth to the questioned propo-sition in an effort to soften his answer (in accordance with positive politeness trategy1).
More than one stimulus condition may motivate R to include the same satellite.For example, in (34), R may have been motivated also by clarify-extent-indicated, whichwas described above.
However, it is possible to provide a context for (35) whereappeasement-indicated holds but clarify-extent-indicated does not, or a context for (34)where the converse is true.
(35) i. Q: Did you wash the dishes?ii.
R: I brought you some flowers.The rules for this stimulus condition may be glossed as: s is motivated to appease hwith q for p not holding or only being partly true, if s suspects that (not p) is undesirable51 Recall that additional constraints on p and q arise from the applicability conditions ofoperatorscontaining this stimulus condition, amely Use-contrast inthis case.
Thus, another constraint is that it isplausible that cr-contrast holds.
The coherence rule for cr-contrast was described inSection 4.3.52 Example (56) from Hirchberg (1985).419Computational Linguistics Volume 25, Number 3Table 4General principles underlying stimulus conditions.1.
Efficiency: explanation-indicated, answer-ref-indicated2.
Accuracy: clarify-concept-indicated, clarify-extent-indicated,clarify-condition-indicated3.
Politeness: excuse-indicated, appeasement-indicated, substitute-indicatedto h but that q is desirable to h. The antecedents o this rule would be evaluated usingheuristic rules and stereotypical nd specific knowledge about h's desires.
For example,two heuristics of rational agency that might lead to beliefs about h's desires are 1) ifan agent wants you to perform an action A, then your failure to perform A may beundesirable to the agent, and 2) if an agent wants you to do A, then it is desirable tothe agent that you perform a part of A.5.2.9 Summary.
In summary, the stimulus conditions in our model can be classifiedaccording to three general principles, as shown in Table 4.
The first category, effi-ciency, includes the motivation to provide implicitly requested information as wellas to provide an explanation for unexpected information.
In other words, giving thistype of extra information contributes to the efficiency of the conversation by elimi-nating the need for follow-up wh-questions or follow-up why?
or why not?
questions,respectively.
In the category of accuracy, in addition to the reason cited by Hirschberg(which is represented in our model as clarify-extent-indicated), we have identified twoother reasons for giving extra information, which contribute to accuracy.
The categoryof politeness includes reasons for redressing face-threatening acts using positive andnegative politeness trategies.5.3 Generation AlgorithmThe inputs to generation consist of:?
the set of discourse plan operators (described in Section 3) augmentedwith stimulus conditions,?
the set of coherence rules (also described in Section 3),?
the set of stimulus condition rules (described in Section 5.2),?
R's beliefs (including the discourse xpectation that R will provide ananswer to some questioned proposition p), and?
the semantic representation f p.The model presupposes that when answer generation begins, the speaker's (R's) onlygoal is to satisfy the above discourse expectation.
R's nonshared beliefs (includingbeliefs whose strength is not necessarily certainty) about Q's beliefs, intentions, andpreferences are used in generation to evaluate whether a stimulus condition holds.The output of the generation algorithm is a discourse plan that can be realized by atactical generation component (McKeown 1985).
5353 The plan that is output specifies an ordering of discourse acts based upon the ordering of coherencerelations pecified in the discourse plan operators.
However, reordering may be required, e.g., to modela speaker who has multiple goals.420Green and Carberry Indirect AnswersThe answer generation algorithm has two phases.
In the first phase, content plan-ning, the generator creates a discourse plan for a full answer, i.e., a direct answerand extra appropriate information.
In the second phase, plan pruning, the generatordetermines which propositions of the planned full answer do not need to be explic-itly stated.
For example, given an appropriate model of R's beliefs, the system wouldgenerate a plan for asserting only the proposition conveyed in (36)v and (36)vi as ananswer to (36)i.
54(36) i. Q: Is Mark here \[at he office\]?ii.
R: \[No.\]iii.
\[He's at home.\]iv.
\[He is caring for his daughter.\]v. His daughter has the measles,vi.
but he's logged on.An advantage of this approach is that, even when it is not possible to omit the directanswer, a full answer is generated.5.3.1 Content Planning.
Content planning is performed by top-down expansion of ananswer discourse plan operator.
First, each top-level answer discourse plan operatoris instantiated with the questioned proposition until one is found such that its applica-bility conditions hold.
s5 Next, the satellites of this operator are expanded (recursively).The algorithm for expanding a satellite adds each instance of a satellite such that allof its applicability conditions and at least one of its stimulus conditions hold.
Thus,different instantiations of the same type of satellite may be included in a plan fordifferent reasons.
For example, (36)iii and (36)vi both realize Use-contrast atellites, theformer included due to the answer-ref-indicated stimulus condition, and the latter dueto the substitute-indicated stimulus condition.For each stimulus condition of a satellite, our implementation f the algorithmuses a theorem prover to search the set of R's beliefs (encoded as Horn clauses)for a proposition satisfying a formula consisting of a conjunction of the applicabilityconditions and that stimulus condition.
A proposition satisfying each such formulais used to instantiate the existential variable of the satellite operator.
For example,to generate the response in (36), the following formula would be constructed fromthe Use-contrast operator's applicability conditions and one of its stimulus conditions,(answer-ref-indicated), where p is the proposition that Mark is at the office, and ?q is theexistential variable to be instantiated.
((and (bel s (cr-contrast ?q (not p)))(Plausible (cr-contrast ?q (not p)))(answer-ref-indicated s h ?q)))The result of the search is to instantiate ?q with the proposition that Mark is at home,due to the speaker's belief that the hearer might have been using (36)i as a prerequest54 Parts (36)i-(36)v were overheard by one of the authors ina naturally occurring dialogue, and (36)viwas added for expository purposes.55 It is assumed that exactly one is appropriate.421Computational Linguistics Volume 25, Number 3for the question, Where is Mark?
For a more complete description of how R's responsein (36) is generated, see Section 5.4.We have employed a simple approach to planning because the focus of our re-search was on the use of the response as an indirect answer, i.e., on aspects of responsegeneration that play a role in interpreting the implicature.
In a more sophisticated dis-course planning formalism, such as argued for in Young, Moore, and Pollack (1994), itwould be possible to represent and reason about other intended effects of the response.
(In our model, the effects or primary goals are used in interpretation but their onlyrole in generation is in simulated interpretation.
However, their role in interpretation isimportant; they constrain what discourse plans can be ascribed to the speaker.)
Whilewe believe that use of more sophisticated planning formalisms is well motivated fordiscourse generation in general, we leave the problem of generating indirect answersin such formalisms for future research.
The use of stimulus conditions to motivate theselection of optional satellite operators is sufficient for our current goals.5.3.2 Plan Pruning.
The output of the content planning phase, an expanded iscourseplan representing a full answer, is the input to the plan pruning phase of generation.The expanded plan is represented as a tree of discourse acts.
The goal of this phase isto make the response more concise, i.e., to determine which of the planned acts can beomitted while still allowing Q to infer the full discourse plan.
56 To do this, the generatorconsiders each of the acts in the frontier of the tree from right to left.
(This ensures thata satellite is considered before its related nucleus.)
The generator creates a trial planconsisting of the original plan minus the nodes pruned so far and minus the currentnode.
Then, using the answer ecognizer, the generator simulates Q's interpretation ofa response containing the information that would be given explicitly according to thetrial plan.
In the simulation, R's shared beliefs are used to model Q's shared beliefs.If Q could infer the full plan (as the most preferred interpretation), then the currentnode can be pruned.
Otherwise, it is left in the plan and the next node is considered.For example, consider Figure 11 as we illustrate the possible effect of pruningon a full discourse plan.
The leaf nodes, representing discourse acts, are numbered 1through 8.
Arcs labeled N and S lead to a nucleus or satellite, respectively.
Node 8corresponds to the direct answer.
Plan pruning would process the nodes in order from1 to 8.
The maximal set of nodes that could be pruned in Figure 11 is the set containing2, 3, 4, 7, and 8.
That is, nodes 2 through 4 might be inferable from 1, node 7 from 5or 6, and node 8 from 4 or 7, but nodes 1, 5, and 6 cannot be pruned since they arenot inferable from other nodesY In the event that it is determined that no node canbe pruned, the full plan would be output.
The interpretation algorithm (described inSection 4) would use hypothesis generation to recognize missing propositions otherthan the direct answer, i.e., the propositions at nodes 2, 3, 4, and 7.To comment on the role of interpretation i  generation, it is a key component of ourclaim to have provided an adequate model of conversational implicature.
Given theshared discourse xpectation that R will provide an answer to Q's yes-no question, Q'suse of (Q's) shared beliefs to interpret the response, and Q's belief that R expects that Qwill be able to recognize the answer, Q's recognition of a discourse plan for an answerwarrants Q's belief that R intended for Q to recognize this intention.
In particular, Rwould not have pruned the direct answer unless, given the beliefs that R presumes56 Conciseness i  not the only possible motive for omitting the direct answer.
As mentioned earlier, anindirect answer may be used to avoid performing a face-threatening act.However, it is beyond thescope of our model to determine whether to omit the direct answer on grounds of politeness.57 In fact, leaves that have no satellites oftheir own cannot be pruned.422Green and Carberry Indirect Answers7 6 5 4NFigure 11Example of full discourse plan before pruning.Q:R:a.b.C.d.Figure 12Is Mark here?\[No.\]\[He's at home.\]\[He is caring for his daughter.\]His daughter has the measles.but he is logged on.Generation example: exchange.i2 1to be shared with Q, R believes that Q will be able to recognize a chain of mutuallyplausible coherence relations from the actual response to the intended answer, andthus be able to recognize R's plan.
Note that although stimulus conditions are notrecognized uring interpretation i  our approach, the model does account for therecognition of those parts of the plan concerning the answer.
For example, althoughQ may not know whether R was motivated by excuse-indicated or explanation-indicatedto provide (21)iv in response to (21)ii, Q can recognize R's intention to convey a noby Q's recognition of (21)iv as the nucleus of a Use-Obstacle satellite of Answer-No.Thus, Q can thereby attribute to R the primary goal of the Answer-No plan to conveya no.5.4 Generation ExampleThis example models R's generation of the response in the exchange shown in Fig-ure 12, which repeats (36).
The discourse plan constructed by the algorithm is depictedin Figure 13, where (a) through (d) refer to communicative acts that could be performedby saying the sentences with corresponding labels in Figure 12.
Square brackets in theplan indicate acts that have been pruned, i.e., that are not explicitly included in theresponse.First, each top-level answer operator is instantiated with the questioned propo-sition, p, the proposition that Mark is at the office.
An (Answer-no s h p) plan wouldbe selected for expansion since its applicability conditions can be proven.
To expandthis plan, the algorithm attempts to expand each of its satellites as described in Sec-tion 5.3.1.
The generation algorithm searches for (at most) one instance of a satellite foreach possible motivation of a satellite.
In this example, two satellites of (Use-contrasth (not p)) are added to the plan.
In one, motivated by the Answer-ref stimulus condi-423Computational Linguistics Volume 25, Number 3\[no\](Answer-no sh p)(Use-contrast  h (not p))j~xexplanation-indicated\[a\] (Use-cause s h pa ) ~ ation-indicated\[b\] (Use-cause s h Pb )(Use-contrast  h (not p))Figure 13Final plan.tion, the existential proposition is instantiated with pa, the proposition that Mark is athome.
In other words, proposition, pa was found to satisfy ?q in formula 1 below.1.
((and (bel s (cr-contrast ?q (not p)))(Plausible (cr-contrast ?q (not p)))(answer-ref-indicated s h ?q)))In the other (Use-contrast  h (not p)) satellite, motivated by the substitute-indicated stim-ulus condition, the existential proposition is instantiated with pd, the proposition thatMark is logged on.
That is, Pa was found to satisfy ?q in formula 2 below.2.
((and (bel s (cr-contrast ?q (not p)))(Plausible (cr-contrast ?q (not p)))(substitute-indicated s h ?q)))The former (Use-contrast  h (not p)) satellite (i.e., the one constructed using pa) canbe expanded by adding a (Use-cause s h pa) satellite to it.
This satellite's existentialvariable is instantiated with Pb, the proposition that Mark is caring for his daughter,which was found to satisfy ?q in formula 3 below.3.
((and (bel s (cr-cause ?q pa))(Plausible (cr-cause ?q pa))(explanation-indicated s h pa)))Finally, this satellite is expanded using pc, the proposition that Mark's daughter hasthe measles, which was found to satisfy ?q in formula 4 below.4.
((and (bel s (cr-cause ?q pb))(Plausible (cr-cause ?q Pb))(explanation-indicated s h Pb)))The output of phase one is a discourse plan for a full answer, as shown in Figure 14.The second phase of generation, plan pruning, will walk the tree bottom-up.
The root424Green and Carberry Indirect Answers5(Answer-no s h p)no (Use-contrast s h (not p)) (Use-contrast s h (not p))a (Use-cause s h p ) db (Use-cause s h Pb )CFigure 14Plan for full answer (before pruning).of each subtree has been annotated with a sequence number to show the order inwhich a subtree is visited in the bottom-up traversal of the tree, i.e., 1 through 5.Since subtree 1 has no satellites (only a nucleus), the traversal moves to subtree 2.For the same reason, the traversal moves to subtree 3.
Next, the nucleus of subtree3 is tentatively pruned, i.e., a trial response consisting of the direct answer plus (a),(c), and (d) is created.
Simulated interpretation of this trial response results in theinference of a discourse plan identical to the full plan as the most preferred (in fact,the only) interpretation of the trial response.
Thus, (b) can be pruned, and subtree 4 isconsidered next.
By a similar process, (a) is also pruned.
Last, the tree with root labeled5 is examined, and it is determined that the direct answer (no) can also be pruned.The final result of the traversal is that the direct answer, (a), and (b) are marked aspruned, and a response consisting of just acts (c) and (d) is returned by the generator.5.5 Related Work in GenerationThis work differs from most previous work in cooperative response generation in thatthe information given in an indirect answer conversationally implicates the direct an-swer.
Hirschberg (1985) implemented a system that determines whether a yes or noalone licenses any unwanted scalar implicatures, and if so, proposes alternative truescalar responses that do not.
In our model, that type of response is generated by con-structing a response from an Answer-no r Answer-hedge operator having a single Use-contrast satellite, motivated by clarify-extent-indicated, as illustrated in Section 5.2.7.
58However, Hirschberg's model does not account for other types of indirect answers,which can be constructed using the other operators (or other combinations of theabove operators) in our model, nor for other motives for selecting Use-contrast suchas answer-ref-indicated and appeasement-indicated.Rhetorical or coherence relations (Grimes 1975; Hall iday 1976; Mann and Thomp-son 1988) have been used in several text-generation systems to aid in ordering parts ofa text (e.g., Hovy 1988) as well as in content planning (e.g., McKeown 1985; Moore andParis 1993).
The discourse plan operators based on coherence relations in our model58 As mentioned earlier, the coherence rules for cr-contrast as well as the rules for clarify-extent-indicatedmake use of notions elucidated by Hirschberg (1985).425Computational Linguistics Volume 25, Number 3(i.e., the operators used as satellites of top-level operators) play a similar role in con-tent planning.
However, none of the above approaches model the speaker's motivationfor selecting optional satellites.
Stimulus conditions provide principled discourse-levelknowledge (based upon principles of efficiency, accuracy, and politeness) for choice ofan appropriate discourse strategy.
Also, stimulus conditions enable content selectionto be sensitive not only to the current discourse context, but also to the anticipatedeffect of a part of the planned response.
Finally, none of the above systems incorpo-rate a model of discourse plan recognition into the generation process, which enablesindirect answers to be generated in our model.Moore and Pollack (1992) show the need to distinguish the intentional and in-formational structure of discourse, where the latter is characterized by the sort ofrelations classified as subject-matter relations in RST.
In our model, the operators usedas satellites of top-level answer discourse plan operators are based on relations imi-lar to RST's subject-matter relations.
The primary goals of these operators are similarto the effect fields of the corresponding RST relation definitions.
However, our modeldoes distinguish the two types of knowledge.
In our model stimulus conditions reflect,though they do not directly encode, communicative subgoals leading to the adoptionof informational subgoals.
For example, the explanation-indicated stimulus conditionmay be triggered in situations when the responder's communicative subgoal wouldlead R to select a Use-cause satellite of Answer-yes, rather than a Use-elaboration satellite.Moore and Paris (1993) argue that it is necessary for generation systems to repre-sent not only the speaker's top-level goal, but also the communicative subgoals thata speaker hoped to achieve by use of an informational relation so that, if that subgoalis not achieved, then an alternative rhetorical means can be tried.
Although stimulusconditions do reflect he speaker's motivation for including satellites in a plan, it wasbeyond the scope of our work to address the problem of failure to achieve a subgoalof the original response.
Therefore, our system does not record which stimulus condi-tion motivated a satellite; if the stimulus condition was recorded in the final plan thenour system would have access to information about the speaker's motivation for thesatellite.
In our current approach, if a follow-up question is asked then a response tothe follow-up question is planned independently of the previous response.
However,if R's beliefs have changed since the original question was asked by Q (e.g., as a resultof information about Q's beliefs obtained from Q's follow-up question), then it is pos-sible in our approach for R's response to contain different information.
Furthermore,in our approach the original response may provide the information that a questionerwould have to elicit by follow-up questions in a system that can provide only directanswers.Finally, our use of interpretation during plan pruning has precursors in previouswork.
In Horacek's,approach to generating concise explanations (Horacek 1991), a setof propositions representing the full explanation is pruned by eliminating propositionsthat can be derived from the remaining ones by a set of contextual rules.
Jamesonand Wahlster (1982) use an anticipation feedback loop algorithm to generate llipticalutterances.6.
Implementation and EvaluationWe have implemented a prototype of the model in Common LISP.
The implementedsystem can interpret and generate the types of examples discussed in Sections 4 and5 and the specific examples tested in the experiments described below.
The overallcoverage of the implemented system can be defined as all (direct and indirect) re-sponses that can be composed from the 5 top-level operators and 10 satellite operators426Green and Carberry Indirect Answersblue block_____~ \] \[ green block purple cone.
~/" live mousered blockF igure  15Blocks world picture used in Experiment 1.yellow ballO(for 8 stimulus conditions and 24 coherence relation rules) provided in the model.The performance of the system running on a UNIX workstation depends mainly onthe amount of hypothesis generation performed (which can be controlled by setting aparameter limiting the depth of the breadth-first earch during hypothesis generation).We have evaluated the system with two experiments.
The purpose of the firstexperiment was to determine whether users' interpretations of indirect answers wouldagree with the system's interpretations.
The purpose of the second experiment wasto see how users would evaluate the unrequested information selected for an indirectanswer by the system.
The system was run to verify that it could actually interpretor generate the responses that were evaluated in the first and second experiments,respectively.
Each experiment was conducted by means of a questionnaire given to10 adult subjects who were not familiar with this research work.
At the beginningof each questionnaire, subjects were given a brief textual and pictorial description ofthe setting in which the questions and responses supposedly had occurred.
(A black-and-white version of the picture shown to subjects for the first experiment is given inFigure 15.
Since the picture used in the experiment was in color, we have annotatedthe objects in the figure to indicate their color.
A similar picture was used for thesecond experiment.)
The fictional setting was described as a laboratory inhabited bya talking robot and a mouse; outside of the laboratory is a manager who cannot seeinside the laboratory, but the robot and manager can communicate with each other.In both experiments, the questionnaire consisted of questions upposedly posed bythe manager to the robot, and the robot's possible or actual responses.
This settingwas selected because it could easily be presented to subjects with a minimum ofdescription about the beliefs that might motivate the robot's responses.
Also, a newdomain was used so that our experience with the other domains would not influencethe evaluation.427Computational Linguistics Volume 25, Number 36.1 Experiment 16.1.1 Experiment.
The first experiment addressed whether the subjects' interpretationsof indirect answers would agree with the system's interpretations.
The subjects weregiven 19 yes-no question-response exchanges.
Each response consisted of from 1 to 3sentences without an explicit yes or no, e.g., as in (37) (item 3 in the questionnaire forExperiment 1).
(37) i. Q: Is the yellow ball on the table?ii.
R: The yellow ball is on the floor.
(For more examples, see the appendix.)
Fourteen of the responses were indirect, i.e.,our system would interpret hem as generated from Answer-No or Answer-Yes.
59(Forexample, (37)ii was interpreted as a no generated by Answer-No.)
These 14 responsesmade use of all of the possible satellites of Answer-No and Answer-Yes in the model.Several responses made use of multiple satellites.
For example, the response in item19 of the questionnaire was similar to the response shown on the right-hand side ofFigure 8.
The other 5 responses we characterize as bogus, i.e., would not be interpretedas answers by our system, e.g., (38) (item 2 in the questionnaire).
(38) i. Q: Can you pick up the ball?ii.
R: A red block is on the table.The purpose of the so-called bogus responses was to make certain that the subjectswere not just interpreting every response as saying yes or no.
For each response, thesubjects were asked to select one of the following interpretations:1.
Yes (glossed as I would interpret this as yes)2.
Yes-?
(glossed as I could interpret this as yes but I am uncertain),3.
No (glossed as I would interpret this as no),4.
No-?
(glossed as I could interpret this as no but I am uncertain), or5.
Other (glossed as I would not interpret this as yes or no).6.1.2 Results.
The results are shown in Table 5.
The rows of the table present the resultsfor each question-response pair.
The second column gives the system's interpretationof the response for cases where the system would interpret the response as an indirectyes or no, or indicates bogus for a bogus response.
The third column gives the numberof subjects who selected yes or no in agreement with the system's interpretation.
Thefourth column gives the number of subjects who selected yes or no in agreement withthe system's interpretation but with some uncertainty (i.e., a Yes-?
or No-?).
The lastcolumn gives the number of subjects who judged the response as Other.In the overwhelming majority of cases the subjects' interpretations agreed withthe systems' interpretation as a yes or no, though occasionally with some uncertainty.None of the subjects elected the opposite interpretation, i.e., a Yes~Yes-?
for a no, ora No~No-?
for a yes.
Of the 14 questions interpreted as a yes or no by the system,59 We have not yet evaluated indirect answers that would be generated by the other three top-leveloperators inour model.428Green and Carberry Indirect AnswersTable 5Results of experiment on interpretation f indirect answers.Example Indirect Indirect IndirectNumber Answer Interpretation Interpretation (?)
Other1 Yes 6 3 12 (bogus) 0 1 93 No 9 0 14 No 5 5 05 No 8 2 06 Yes 7 3 07 (bogus) 0 1 98 No 9 1 09 Yes 9 1 010 No 8 2 011 Yes 3 7 012 (bogus) 0 0 1013 Yes 8 2 014 No 8 2 015 No 7 3 016 Yes 4 6 017 (bogus) 0 0 1018 (bogus) 0 0 1019 Yes 10 0 0only 2 items were interpreted by subjects as Other (each by a different subject).
In28% of the instances where the subjects interpreted a response as saying yes or no,they noted some degree of uncertainty.
During debriefing, the subjects who tendedto express uncertainty said that while they might interpret he response as yes or no,one generally had some uncertainty when the direct answer was omitted.
Only onesubject interpreted a bogus question as answering yes or no.To test the statistical significance of the pattern of responses hown in Table 5, wetook a very conservative approach.
We grouped Indirect Interpretation (?)
with Other inTable 5 for question-response instances where the system interpreted the response asan indirect answer, and we grouped Indirect Interpretation (?)
with Indirect Interpretationfor instances where the system did not interpret he response as an indirect answer.Thus Indirect Interpretation (?)
responses by the subjects were treated as disagreeingwith the system's interpretation of the example.
We then applied Cochran's Q test(Cochran 1950) to the resulting two columns of data.
The result shows that the patternof responses i statistically significant (not the result of random chance) at better thanthe level p < .005.
To determine whether the subjects differentiated between responsesthat the system interpreted as indirect answers and those that it did not, we appliedthe Mann Whitney U statistic (Siegel 1956), which showed no score overlap at thelevel p < .005.6.2 Experiment 26.2.1 Experiment.
Although the linguistic studies discussed in Section 5 show that inhuman-human dialogue people often include additional unrequested information intheir responses to yes-no questions, we conducted a second experiment to determinehow users would evaluate responses consisting of the kinds of extra unrequestedinformation produced by our system.
The subjects were given 11 yes-no questions(some preceded by 1 or 2 sentences to establish some additional context), each with a429Computational Linguistics Volume 25, Number 3set of 4 possible responses.
The subjects were told to suppose that all of the responsesin a set were true, and were asked to select the best response in each set.
For eachquestion, the response choices included:?
a direct response of Yes or No (depending on the correct answer to thequestion),?
the direct response with further emphasis (such as No, I can't),?
2 extended responses, containing the direct answer with extraunrequested information,e.g., (39)iii-(39)vi, respectively.
((39) is item 1 in the questionnaire for Experiment 2.
)(39) i. M: I am looking for the blue ball.ii.
Is it on the table?iii.
R: No.iv.
R: No.
It's not.v.
R: No.
It's on the floor.vi.
R: No.
It cost $5.In 9 of the 11 sets, 1 of the extended responses was motivated by our stimulus condi-tions (e.g., (39)v), and 1 was not (e.g., (39)vi).
In the other 2 sets (questionnaire items3 and 7), neither of the extended responses was motivated by any stimulus condition.The purpose of these 2 so-called bogus examples was to make certain that the subjectswere not inclined to always select responses with extra information.6.2.2 Results.
The results are shown in Table 6.
The rows of the table present theresults for each question.
The second column lists the stimulus condition, if any, thatour system used to trigger one of the extended responses to the question.
Items 3 and7 contained bogus responses, i.e., none of the responses was motivated by a stimuluscondition.
The next three columns indicate respectively the number of subjects whoselected the response motivated by the listed stimulus condition, the number whoselected the direct answer alone or the direct answer with emphasis but no additionalinformation, and the number who selected an extended response not motivated bya stimulus condition.
Note that none of the subjects selected a response with extrainformation for the two bogus questions, indicating that they were not merely inclinedto select responses with extra information.Items 8 and 10 warrant some discussion.
Question 8 was problematic.
The origi-nal question given to the first four subjects asked whether the robot could tell the labmanager the time.
The response "No.
There is no clock in here."
was motivated by thestimulus condition excuse-indicated.
However, two of the four subjects elected just Noas the best response, and explained uring debriefing that if the robot could tell time,then certainly he had an internal clock that he could use (since all computers haveinternal clocks) and thus the absence of a clock in the room was not relevant.
Sincethe prior beliefs of these subjects conflicted with the beliefs that were intended as thecontext for interpreting the robot's response, we altered the question for the remainderof the study to circumvent this problem.
In item 10, the extra information in the sys-tem's response was motivated by the appeasement-indicated stimulus condition.
In that430Green and Carberry Indirect AnswersTable 6Results of experiment on including extra information.Direct OtherExample Answer ExtendedNumber Stimulus Condition SC Response SC Only Response1 answer-re f-indicated 10 0 02 substitute-indicated 10 0 03 none 0 10 04 explanation-indicated 9 1 05 clarify-extent-indicated 10 0 06 clarify-condition-indicated 10 0 07 none 0 10 08 excuse-indicated 8 2 09 clarify-concept-indicated 10 0 010 appeasement-indicated 4 5 111 explanation-indicated 9 1 0response, the robot answers No (that he has not yet done the requested task) and thenattempts to appease the questioner by describing another task that he has completed.Since only 4 of the 10 subjects elected this response, it is possible that the subjectsdid not view appeasement as an appropriate stimulus condition in human-machinedialogue, despite the fact that it does occur in human-human dialogue.
Alternatively,the subjects did not have enough information to recognize the response as attemptedappeasement.To test the statistical significance of the pattern of responses in Table 6, we againtook a conservative approach and grouped Other Extended Response (which was selectedonly once by a subject) with Direct Answer Only so that it was treated as disagreeingwith the system response.
Once again we applied Cochran's Q test (Cochran 1950) andthe Mann Whitney U statistic (Siegel 1956).
Cochran's Q test showed that the patternof responses in Table 6 is statistically significant at the level p < .005, and the MannWhitney U statistic showed that there is no score overlap at the level p ~ .0253.6.3 SummaryThe first experiment suggests that our system's interpretations of indirect answersagree with the judgments of human interpreters.
The second experiment suggests thatour stimulus conditions result in the construction of responses containing extra infor-mation that users will view favorably.
However, we have not addressed the questionof when other stylistic onsiderations might limit the amount of extra information thatis included, nor the question of choosing between a direct and an indirect responsewhen both are possible.
These issues will be addressed in future research.7.
Conc lus ionsIn summary, we have proposed and implemented a computational model for inter-preting and generating indirect answers to yes-no questions in English.
This paperdescribes the knowledge and processes provided by the model.
Generation and inter-pretation are treated, respectively, as construction of and recognition of a responder'sdiscourse plan for a full answer.
A discourse plan explicitly relates a speaker's beliefsand discourse goals to his program of communicative actions.
An indirect answer isthe result of the responder providing only part of the planned response, but intending431Computational Linguistics Volume 25, Number 3for his discourse plan to be recognized by the questioner.
Discourse plan constructionand recognition make use of the beliefs that are presumed to be shared by the partic-ipants, as well as shared knowledge of discourse strategies, represented in the modelby shared discourse plan operators.
In the operators, coherence relations are used tocharacterize types of satellites that may accompany each type of answer.
Recognizinga mutually plausible coherence relation obtaining between the actual response anda possible direct answer plays an important role in recognizing the responder's dis-course plan.
The use of hypothesis generation in interpretation broadens the coverageof the model to cases where more is missing from a full answer than just the nucleus ofa top-level operator.
(From the point of view of generation, it enables the constructionof a more concise, though no less informative, response.)
Stimulus conditions model aspeaker's motivation for selecting a satellite.
During generation, the speaker uses hisown interpretation capability to determine what parts of the plan are inferable by thehearer in the current discourse context and thus do not need to be explicitly given.We argue that because of the role of interpretation i  generation, Q's belief that Rintended for Q to recognize the answer is warranted by Q's successful recognition ofthe plan.Although it was not our goal to develop a cognitive model of how implicatures areproduced and comprehended, certain aspects of the model might be incorporated intoa cognitive model.
To a large extent he model is recognitional rather than inferential.
Ofcourse, we make no claims about the cognitive plausibility of the particular coherencerelations and discourse plan operators used in our model, which were encoded solelyon the basis of their descriptive and computational utility.
We await further cognitivestudies on coherence relations as begun in Sanders, Spooren, and Noordman (1992),Knott and Dale (1994), and Knott (1995).
Since the work reported in this paper wasperformed, the first author has investigated the automatic ompilation of discourseplan operators in a computational cognitive architecture (SOAR) (Green and Lehman1998).In conclusion, our model provides wider coverage than previous computationalmodels for generating and interpreting answers.
Specifically, it covers both direct andindirect answers, multiple-sentence r sponses, a variety of types of indirect answer(i.e., characterized in terms of multiple coherence relations), and multiple types ofspeaker motivation for deciding to provide extra information (i.e.
characterized interms of different stimulus conditions).
In addition, it appears that this approach couldbe extended to other discourse-expectation-based types of conversational implicature.As a computational model of conversational implicature, it extends current plan-basedtheories of implicature in several ways.
First, it demonstrates the role of shared dis-course expectations and pragmatic knowledge.
Second, it makes predictions aboutcancelability in terms of intentional structure of discourse.
Lastly, it treats generationas a process drawing upon the speaker's own interpretation mechanism.Appendix: Questionnaire for Experiment 11.
Q: Can you make a stack of 3 blocks?R: I can put the green block on the blue block.2.
Q: Can you pick up the ball?R: A red block is on the table.3.
Q: Is the yellow ball on the table?R: The yellow ball is on the floor.432Green and Carberry Indirect Answers......10.11.Q: Can you pick up the green block?R: The green block is glued to the table.I glued it yesterday.Q: Can you build a stack with the green block on top of the cone?R: The cone does not have a flat top.Q: Are you going to move the green block?R: The green block is very heavy.I 'm going to use the forklift.Q: Can the mouse climb up the red block?R: The yel low ball is on the floor.Q: Can you build a stack with the green block on top of the cone?R: The green block would fall off the cone.Q: Can you pick up the red block?R: If I move the blue block off of it.Q: Can you pick up the blue block?R: My arms won't  move.The human forgot to oil me.Q: Is there a yellow ball on the floor?R: The mouse pushed the yellow ball off the table.12.
Q: Are there any blocks on the table?R: The table is about three feet tall.13.
Q: Is something on the red block?R: I put the blue block on the red block.14.
Q: Can you put the mouse on the green block?R: He runs too fast.15.16.17.18.Q: Is the blue block on the table surface?R: I put the blue block on the red block.Q: Did you move the cone off the green block?R: I wanted to pick up the green block.Q: Is the blue block on the red block?R: The mouse squeaks a lot.Q: Did you pick up the cone?R: There are three blocks, one cone, a yellow ball, and a mouse.19.
Q:R:Are you going to pick up the blue block?The blue block is sticky.The mouse poured honey on it.I am going to use a pair of tongs.433Computational Linguistics Volume 25, Number 3AcknowledgmentsThis describes the first author's dissertationresearch at the University of Delaware.
Wewould like to thank Dan Chester of theUniversity of Delaware for providing uswith an implementation f a Horn clausetheorem prover (Chester 1980) to use in thiswork, and Fred Masterson also of theUniversity of Delaware for help with thestatistical analysis of the experiments.
Also,we wish to thank the journal referees fortheir helpful comments.ReferencesAllen, James E and C. Raymond Perrault.1980.
Analyzing intention in utterances.Artificial Intelligence 15:143-178.Brown, Penelope and Stephen C. Levinson.1978.
Universals in language usage:Politeness phenomena.
In Esther N.Goody, editor, Questions and Politeness:Strategies in Social Interaction.
CambridgeUniversity Press, pages 56-289.Carberry, Sandra.
1990.
Plan Recognition iNatural Language Dialogue.
MIT Press,Cambridge, MA.Chester, Daniel.
1980.
HCPRVR: Aninterpreter for logic programs.
InProceedings ofthe First Annual NationalConference on Artificial Intelligence,pages 93-95.Clark, Herbert H. 1979.
Responding toindirect speech acts.
Cognitive Psychology11:430-477.Clark, Herbert and C. Marshall.
1981.Definite reference and mutual knowledge.In Aravind K. Joshi, Bonnie Webber, andIvan Sag, editors, Elements of DiscourseUnderstanding.
Cambridge UniversityPress.Cochran, W. G. 1950.
The comparison ofpercentages in matched samples.Biometrika 37:256-266.Dahlgren, Kathleen.
1989.
Coherencerelation assignment.
In Proceedings oftheEleventh Annual Meeting of the CognitiveScience Society, pages 588-596.Green, Nancy L. 1994.
A ComputationalModel for Generating and Interpreting IndirectAnswers.
Ph.D. thesis, University ofDelaware.Green, Nancy and Jill F. Lehman.
1998.
Anapplication of explanation-based l arningto discourse generation andinterpretation.
In Papers from the 1998AAAI Spring Symposium on ApplyingMachine Learning to Discourse Processing,pages 33-39.Grice, H. Paul.
1975.
Logic andconversation.
In Peter Cole and Jerry L.Morgan, editors, Syntax and Semantics III:Speech Acts.
Academic Press, pages 41-58.Grimes, Joseph E. 1975.
The Thread ofDiscourse.
Mouton, The Hague.Gunji, Takao.
1981.
Toward a ComputationalTheory of Pragmatics--Discourse,Presupposition, and Implicature.
Ph.D.thesis, Ohio State University.Halliday, M. A. K. and Ruqaiya Hasan.
1976.Cohesion in English.
Longman, New York.Hinkelman, Elizabeth A.
1989.
Linguistic andPragmatic Constraints on UtteranceInterpretation.
Ph.D. thesis, University ofRochester.Hirschberg, Julia B.
1985.
A Theory of ScalarImplicature.
Ph.D. thesis, University ofPennsylvania.Hobbs, Jerry R. 1978.
Resolving pronounreferences.
Lingua, 44:311-338.Horacek, Helmut.
1991.
Exploitingconversational implicature for generatingconcise explanations.
In Proceedings oftheEuropean Association for ComputationalLinguistics, pages 191-193.Hovy, Eduard H. 1988.
Planning coherentmultisentential text.
In Proceedings ofthe26th Annual Meeting, pages 163-169.Association for ComputationalLinguistics.Jameson, Anthony and Wolfgang Wahlster.1982.
User modelling in anaphorageneration: Ellipsis and definitedescription.
In Proceedings ofthe EuropeanConference on Artificial Intelligence.Kiefer, Ferenc.
1980.
Yes-no questions aswh-questions.
In John Searle, FerencKiefer, and Manfred Bierwisch, editors,Speech Act Theory and Pragmatics.
Reidel,Dordrecht, Holland, pages 48-68.Knott, Alistair.
1995.
A Data-DrivenMethodology for Motivating aSet of CoherenceRelations.
Ph.D. thesis, University ofEdinburgh.Knott, Alistair and Robert Dale.
1994.
Usinglinguistic phenomena to motivate a set ofcoherence relations.
Discourse Processes,35-62.Lascarides, Alex and Nicholas Asher.
1991.Discourse relations and defeasibleknowledge.
In Proceedings ofthe 29thAnnual Meeting, pages 55-62.
Associationfor Computational Linguistics.Lascarides, Alex, Nicholas Asher, and JonOberlander.
1992.
Inferring discourserelations in context.
In Proceedings ofthe30th Annual Meeting, pages 1-8.Association for ComputationalLinguistics.434Green and Carberry Indirect AnswersLevinson, Stephen C. 1983.
Pragmatics.Cambridge University Press, Cambridge.Litman, Diane J.
1986.
Understanding planellipsis.
In Proceedings ofthe Fifth NationalConference on Artificial Intelligence,pages 619-624.Mann, William C. and Sandra A.Thompson.
1983.
Relational propositionsin discourse.
Technical ReportISI/RR-83-115, Information SciencesInstitute, University of SouthernCalifornia, Marina del Rey, CA.Mann, William C. and Sandra A.Thompson.
1988.
Rhetorical StructureTheory: Toward a functional theory oftext organization.
Text 8(3):167-182.McCafferty, Andrew S. 1987.
Reasoning aboutImplicature: A Plan-Based Approach.
Ph.D.thesis, University of Pittsburgh,Pittsburgh.McKeown, Kathleen R. 1985.
Text Generation.Cambridge University Press, Cambridge.Moore, Johanna D. and Cecile Paris.
1993.Planning text for advisory dialogues:Capturing intentional and rhetoricalinformation.
Computational Linguistics19(4):651-694.Moore, Johanna D. and Martha E. Pollack.1992.
A problem for RST: The need formulti-level discourse analysis.Computational Linguistics 18(4):537-544.Perrault, Raymond and James Allen.
1980.A plan-based analysis of indirect speechacts.
American Journal of ComputationalLinguistics 6(3-4):167-182.Pollack, Martha.
1990.
Plans as complexmental attitudes.
In Philip R. Cohen, JerryMorgan, and Martha Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA.Reichman, Rachel.
1985.
Getting ComputersTo Talk Like You And Me.
MIT Press,Cambridge, MA.Sanders, Ted J., Wilbert P. Spooren, and LeoG.
Noordman.
1992.
Toward a taxonomyof coherence r lations.
Discourse Processes15:1-35.Schegloff, Emanuel A.
1972.
Sequencing inconversational openings.
In J. J. Gumperzand Dell H. Hymes, editors, Directions inSociolinguistics.
Holt, Rinehart andWinston, New York, pages 346-80.Schegloff, Emanuel A.
1979.
Identificationand recognition i  telephone conversationopenings.
In G. Psathas, editor, EverydayLanguage: Studies in Ethnomethodology.Irvington, New York, pages 23-78.Siegel, Sidney.
1956.
Nonparametric Statisticsfor the Behavioral Sciences.
McGraw-Hill,New York.SRI Tapes.
1992.
Transcripts of audiotapeconversations.
Prepared by JacquelineKowto under the Direction of Patti Priceat SRI International, Menlo Park, CA.StenstrOm, Anna-Brita.
1984.
Questions andresponses in English conversation.
InClaes Schaar and Jan Svartvik, editors,Lund Studies in English 68.
CWK Gleerup,Malm6, Sweden.Thomason, Richmond H. 1990.Accommodation, meaning, andimplicature: Interdisciplinary foundationsfor pragmatics.
In Philip R. Cohen, JerryMorgan, and Martha Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 325-363.Walker, Marilyn A.
1993.
InformationalRedundancy and Resource Bounds inDialogue.
Ph.D. thesis, University ofPennsylvania.Young, R. Michael, Johanna D. Moore, andMartha E. Pollack.
1994.
Towards aprincipled representation f discourseplans.
In Proceedings ofthe Sixteenth AnnualMeeting of the Cognitive Science Society,pages 946-951.435
