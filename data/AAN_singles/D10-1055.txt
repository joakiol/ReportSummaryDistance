Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 564?574,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsFurther Meta-Evaluation of Broad-Coverage Surface RealizationDominic Espinosa and Rajakrishnan Rajkumar and Michael White and Shoshana BerleantDepartment of LinguisticsThe Ohio State UniversityColumbus, Ohio, USA{espinosa,raja,mwhite,berleant}@ling.ohio-state.eduAbstractWe present the first evaluation of the utility ofautomatic evaluation metrics on surface real-izations of Penn Treebank data.
Using outputsof the OpenCCG and XLE realizers, alongwith ranked WordNet synonym substitutions,we collected a corpus of generated surface re-alizations.
These outputs were then rated andpost-edited by human annotators.
We eval-uated the realizations using seven automaticmetrics, and analyzed correlations obtainedbetween the human judgments and the auto-matic scores.
In contrast to previous NLGmeta-evaluations, we find that several of themetrics correlate moderately well with humanjudgments of both adequacy and fluency, withthe TER family performing best overall.
Wealso find that all of the metrics correctly pre-dict more than half of the significant system-level differences, though none are correct inall cases.
We conclude with a discussion of theimplications for the utility of such metrics inevaluating generation in the presence of varia-tion.
A further result of our research is a cor-pus of post-edited realizations, which will bemade available to the research community.1 Introduction and BackgroundIn building surface-realization systems for naturallanguage generation, there is a need for reliableautomated metrics to evaluate the output.
Unlikein parsing, where there is usually a single gold-standard parse for a sentence, in surface realizationthere are usually many grammatically-acceptableways to express the same concept.
This parallelsthe task of evaluating machine-translation (MT) sys-tems: for a given segment in the source language,there are usually several acceptable translations intothe target language.
As human evaluation of trans-lation quality is time-consuming and expensive, anumber of automated metrics have been developedto evaluate the quality of MT outputs.
In this study,we investigate whether the metrics developed forMT evaluation tasks can be used to reliably evaluatethe outputs of surface realizers, and which of thesemetrics are best suited to this task.A number of surface realizers have been devel-oped using the Penn Treebank (PTB), and BLEUscores are often reported in the evaluations of thesesystems.
But how useful is BLEU in this con-text?
The original BLEU study (Papineni et al,2001) scored MT outputs, which are of generallylower quality than grammar-based surface realiza-tions.
Furthermore, even for MT systems, theusefulness of BLEU has been called into question(Callison-Burch et al, 2006).
BLEU is designed towork with multiple reference sentences, but in tree-bank realization, there is only a single reference sen-tence available for comparison.A few other studies have investigated the use ofsuch metrics in evaluating the output of NLG sys-tems, notably (Reiter and Belz, 2009) and (Stent etal., 2005).
The former examined the performance ofBLEU and ROUGE with computer-generated weatherreports, finding a moderate correlation with humanfluency judgments.
The latter study applied sev-eral MT metrics to paraphrase data from Barzilayand Lee?s corpus-based system (Barzilay and Lee,2003), and found moderate correlations with humanadequacy judgments, but little correlation with flu-ency judgments.
Cahill (2009) examined the perfor-mance of six MT metrics (including BLEU) in evalu-ating the output of a LFG-based surface realizer for564German, also finding only weak correlations withthe human judgments.To study the usefulness of evaluation metrics suchas BLEU on the output of grammar-based surfacerealizers used with the PTB, we assembled a cor-pus of surface realizations from three different re-alizers operating on Section 00 of the PTB.
Twohuman judges evaluated the adequacy and fluencyof each of the realizations with respect to the ref-erence sentence.
The realizations were then scoredwith a number of automated evaluation metrics de-veloped for machine translation.
In order to investi-gate the correlation of targeted metrics with humanevaluations, and gather other acceptable realizationsfor future evaluations, the judges manually repairedeach unacceptable realization during the rating task.In contrast to previous NLG meta-evaluations, wefound that several of the metrics correlate moder-ately well with human judgments of both adequacyand fluency, with the TER family performing best.However, when looking at statistically significantsystem-level differences in human judgments, wefound that some of the metrics get some of the rank-ings correct, but none get them all correct, with dif-ferent metrics making different ranking errors.
Thissuggests that multiple metrics should be routinelyconsulted when comparing realizer systems.Overall, our methodology is similar to that ofprevious MT meta-evaluations, in that we collectedhuman judgments of system outputs, and com-pared these scores with those assigned by auto-matic metrics.
A recent alternative approach to para-phrase evaluation is ParaMetric (Callison-Burch etal., 2008); however, it requires a corpus of annotated(aligned) paraphrases (which does not yet exist forPTB data), and is arguably focused more on para-phrase analysis than paraphrase generation.The plan of the paper is as follows: Section 2 dis-cusses the preparation of the corpus of surface real-izations.
Section 3 describes the human evaluationtask and the automated metrics applied.
Sections 4and 5 present and discuss the results of these evalua-tions.
We conclude with some general observationsabout automatic evaluation of surface realizers, andsome directions for further research.2 Data PreparationWe collected realizations of the sentences in Sec-tion 00 of the WSJ corpus from the following threesources:1.
OpenCCG, a CCG-based chart realizer (White,2006)2.
The XLE Generator, a LFG-based system de-veloped by Xerox PARC (Crouch et al, 2008)3.
WordNet synonym substitutions, to investigatehow differences in lexical choice compare togrammar-based variation.1Although all three systems used Section 00 ofthe PTB, they were applied with various parame-ters (e.g., language models, multiple-output versussingle-output) and on different input structures.
Ac-cordingly, our study does not compare OpenCCG toXLE, or either of these to the WordNet system.2.1 OpenCCG realizationsOpenCCG is an open source parsing/realizationlibrary with multimodal extensions to CCG(Baldridge, 2002).
The OpenCCG chart realizertakes logical forms as input and produces stringsby combining signs for lexical items.
Alternativerealizations are scored using integrated n-gramand perceptron models.
For robustness, fragmentsare greedily assembled when necessary.
Realiza-tions were generated from 1,895 gold standardlogical forms, created by constrained parsing ofdevelopment-section derivations.
The followingOpenCCG models (which differ essentially in theway the output is ranked) were used:1.
Baseline 1: Output ranked by a trigram wordmodel2.
Baseline 2: Output ranked using three languagemodels (3-gram words + 3-gram words withnamed entity class replacement + factored lan-guage model of words, POS tags and CCG su-pertags)1Not strictly surface realizations, since they do not involvean abstract input specification, but for simplicity we refer tothem as realizations throughout.5653.
Baseline 3: Perceptron with syntax features andthe three LMs mentioned above4.
Perceptron full-model: n-best realizationsranked using perceptron with syntax featuresand the three n-gram models, as well as dis-criminative n-gramsThe perceptron model was trained on sections 02-21 of the CCGbank, while a grammar extracted fromsection 00-21 was used for realization.
In addition,oracle supertags were inserted into the chart duringrealization.
The purpose of such a non-blind test-ing strategy was to evaluate the quality of the outputproduced by the statistical ranking models in isola-tion, rather than focusing on grammar coverage, andavoid the problems associated with lexical smooth-ing, i.e.
lexical categories in the development sec-tion not being present in the training section.To enrich the variation in the generated realiza-tions, dative-alternation was enforced during real-ization by ensuring alternate lexical categories of theverb in question, as in the following example:(1) the executives gave [the chefs] [a stand-ing ovation](2) the executives gave [a standing ovation][to the chefs]2.2 XLE realizationsThe corpus of realizations generated by the XLEsystem contained 42,527 surface realizations of ap-proximately 1,421 section 00 sentences (an aver-age of 30 per sentence), initially unranked.
TheLFG f-structures used as input to the XLE genera-tor were derived from automatic parses, as describedin (Riezler et al, 2002).
The realizations werefirst tokenized using Penn Treebank conventions,then ranked using perplexities calculated from thesame trigram word model used with OpenCCG.
Foreach sentence, the top 4 realizations were selected.The XLE generator provides an interesting pointof comparison to OpenCCG as it uses a manually-developed grammar with inputs that are less abstractbut potentially noisier, as they are derived from au-tomatic parses rather than gold-standard ones.2.3 WordNet synonymizerTo produce an additional source of variation, thenouns and verbs of the sentences in section 00 ofthe PTB were replaced with all of their WordNetsynonyms.
Verb forms were generated using verbstems, part-of-speech tags, and the morphg tool.2These substituted outputs were then filtered usingthe n-gram data which Google Inc. has made avail-able.3 Those without any 5-gram matches centeredon the substituted word (or 3-gram matches, in thecase of short sentences) were eliminated.3 EvaluationFrom the data sources described in the previous sec-tion, a corpus of realizations to be evaluated by thehuman judges was constructed by randomly choos-ing 305 sentences from section 00, then selectingsurface realizations of these sentences using the fol-lowing algorithm:1.
Add OpenCCG?s best-scored realization.2.
Add other OpenCCG realizations until all fourmodels are represented, to a maximum of 4.3.
Add up to 4 realizations from either the XLEsystem or the WordNet pool, chosen randomly.The intent was to give reasonable coverage of allrealizer systems discussed in Section 2 without over-loading the human judges.
?System?
here meansany instantiation that emits surface realizations, in-cluding various configurations of OpenCCG (usingdifferent language models or ranking systems), andthese can be multiple-output, such as an n-best list,or single-output (best-only, worst-only, etc.).
Ac-cordingly, more realizations were selected from theOpenCCG realizer because 5 different systems werebeing represented.
Realizations were chosen ran-domly, rather than according to sentence types orother criteria, in order to produce a representativesample of the corpus.
In total, 2,114 realizationswere selected for evaluation.2http://www.informatics.sussex.ac.uk/research/groups/nlp/carroll/morph.html3http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13/readme.txt5663.1 Human judgmentsTwo human judges evaluated each surface realiza-tion on two criteria: adequacy, which represents theextent to which the output conveys all and only themeaning of the reference sentence; and fluency, theextent to which it is grammatically acceptable.
Therealizations were presented to the judges in sets con-taining a reference sentence and the 1-8 outputs se-lected for that sentence.
To aid in the evaluation ofadequacy, one sentence each of leading and trailingcontext were displayed.
Judges used the guidelinesgiven in Figure 1, based on the scales developedby the NIST Machine Translation Evaluation Work-shop.In addition to rating each realization on the twofive-point scales, each judge also repaired each out-put which he or she did not judge to be fully ade-quate and fluent.
An example is shown in Figure 2.These repairs resulted in new reference sentences fora substantial number of sentences.
These repairedrealizations were later used to calculate targeted ver-sions of the evaluation metrics, i.e., using the re-paired sentence as the reference sentence.
Althoughtargeted metrics are not fully automatic, they are ofinterest because they allow the evaluation algorithmto focus on what is actually wrong with the input,rather than all textual differences.
Notably, targetedTER (HTER) has been shown to be more consistentwith human judgments than human annotators arewith one another (Snover et al, 2006).3.2 Automatic evaluationThe realizations were also evaluated using seven au-tomatic metrics:?
IBM?s BLEU, which scores a hypothesis bycounting n-gram matches with the referencesentence (Papineni et al, 2001), with smooth-ing as described in (Lin and Och, 2004)?
The NIST n-gram evaluation metric, similar toBLEU, but rewarding rarer n-gram matches, andusing a different length penalty?
METEOR, which measures the harmonic meanof unigram precision and recall, with a higherweight for recall (Banerjee and Lavie, 2005)?
TER (Translation Edit Rate), a measure of thenumber of edits required to transform a hy-pothesis sentence into the reference sentence(Snover et al, 2006)?
TERP, an augmented version of TER whichperforms phrasal substitutions, stemming, andchecks for synonyms, among other improve-ments (Snover et al, 2009)?
TERPA, an instantiation of TERP with editweights optimized for correlation with ade-quacy in MT evaluations?
GTM (General Text Matcher), a generaliza-tion of the F-measure that rewards contiguousmatching spans (Turian et al, 2003)Additionally, targeted versions of BLEU, ME-TEOR, TER, and GTM were computed by using thehuman-repaired outputs as the reference set.
Thehuman repair was different from the reference sen-tence in 193 cases (about 9% of the total), and weexpected this to result in better scores and correla-tions with the human judgments overall.4 Results4.1 Human judgmentsTable 1 summarizes the dataset, as well as the meanadequacy and fluency scores garnered from the hu-man evaluation.
Overall adequacy and fluency judg-ments were high (4.16, 3.63) for the realizer sys-tems on average, and the best-rated realizer systemsachieved mean fluency scores above 4.4.2 Inter-annotator agreementInter-annotator agreement was measured using the?-coefficient, which is commonly used to measurethe extent to which annotators agree in categoryjudgment tasks.
?
is defined as P (A)?P (E)1?P (E) , whereP (A) is the observed agreement between annota-tors and P (E) is the probability of agreement dueto chance (Carletta, 1996).
Chance agreement forthis data is calculated by the method discussed inCarletta?s squib.
However, in previous work inMT meta-evaluation, Callison-Burch et al (2007),assume the less strict criterion of uniform chanceagreement, i.e.
15 for a five-point scale.
They also567Score Adequacy Fluency5 All the meaning of the reference Perfectly grammatical4 Most of the meaning Awkward or non-native; punctuation errors3 Much of the meaning Agreement errors or minor syntactic problems2 Meaning substantially different Major syntactic problems, such as missing words1 Meaning completely different Completely ungrammaticalFigure 1: Rating scale and guidelinesRef.
It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf spurns them againRealiz.
It weren?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns themRepair It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns themFigure 2: Example of repairintroduce the notion of ?relative?
?, which measureshow often two or more judges agreed that A > B,A = B, or A < B for two outputs A and B, irre-spective of the specific values given on the five-pointscale; here, uniform chance agreement is taken to be13 .
We report both absolute and relative ?
in Table 2,using actual chance agreement rather than uniformchance agreement.The ?
scores of 0.60 for adequacy and 0.63 for flu-ency across the entire dataset represent ?substantial?agreement, according to the guidelines discussed in(Landis and Koch, 1977), better than is typicallyreported for machine translation evaluation tasks;for example, Callison-Burch et al (2007) reported?fair?
agreement, with ?
= 0.281 for fluency and?
= 0.307 for adequacy (relative).
Assuming theuniform chance agreement that the previously citedwork adopts, our inter-annotator agreements (bothabsolute and relative) are still higher.
This is likelydue to the generally high quality of the realizationsevaluated, leading to easier judgments.4.3 Correlation with automatic evaluationTo determine how well the automatic evaluationmethods described in Section 3 correlate with thehuman judgments, we averaged the human judg-ments for adequacy and fluency, respectively, foreach of the rated realizations, and then computedboth Pearson?s correlation coefficient and Spear-man?s rank correlation coefficient between thesescores and each of the metrics.
Spearman?s corre-lation makes fewer assumptions about the distribu-tion of the data, but may not reflect a linear rela-tionship that is actually present.
Both are frequentlyreported in the literature.
Due to space constraints,we show only Spearman?s correlation, although theTER family scored slightly better on Pearson?s coef-ficient, relatively.The results for Spearman?s correlation are givenin Table 3.
Additionally, the average scores for ad-equacy and fluency were themselves averaged intoa single score, following (Snover et al, 2009), andthe Spearman?s correlation of each of the automaticmetrics with these scores are given in Table 4.
Allreported correlations are significant at p < 0.001.4.4 Bootstrap sampling of correlationsFor each of the sub-corpora shown in Table 1, wecomputed confidence intervals for the correlationsbetween adequacy and fluency human scores withselected automatic metrics (BLEU, HBLEU, TER,TERP, and HTER) as described in (Koenh, 2004).
Wesampled each sub-corpus 1000 times with replace-ment, and calculated correlations between the rank-ings induced by the human scores and those inducedby the metrics for each reference sentence.
We thenused these coefficients to estimate the confidence in-terval, after excluding the top 25 and bottom 25 co-efficients, following (Lin and Och, 2004).
The re-sults of this for the BLEU metric are shown in Table5.
We determined which correlations lay within the95% confidence interval of the best performing met-ric in each row of Table Table 3; these figures areitalicized.5685 Discussion5.1 Human judgments of systemsThe results for the four OpenCCG perceptron mod-els mostly confirm those reported in (White and Ra-jkumar, 2009), with one exception: the B-3 modelwas below B-2, though the P-B (perceptron-best)model still scored highest.
This may have been dueto differences in the testing scenario.
None of thedifferences in adequacy scores among the individ-ual systems are significant, with the exception of theWordNet system.
In this case, the lack of word-sense disambiguation for the substituted words re-sults in a poor overall adequacy score (e.g., wagefloor ?
wage story).
Conversely, it scores highestfor fluency, as substituting a noun or verb with a syn-onym does not usually introduce ungrammaticality.5.2 Correlations of human judgments with MTmetricsOf the non-human-targeted metrics evaluated, BLEUand TER/TERP demonstrate the highest correla-tions with the human judgments of fluency (r =0.62, 0.64).
The TER family of evaluation metricshave been observed to perform very well in MT-evaluation tasks, and although the data evaluatedhere differs from typical MT data in some impor-tant ways, the correlation of TERP with the humanjudgments is substantial.
In contrast with previousMT evaluations where TERP performs considerablybetter than TER, these scored close to equal on ourdata, possibly because TERP?s stem, synonym, andparaphrase matching are less useful when most ofthe variation is syntactic.The correlations with BLEU and METEOR arelower than those reported in (Callison-Burch et al,2007); in that study, BLEU achieved adequacy andfluency correlations of 0.690 and 0.722, respec-tively, and METEOR achieved 0.701 and 0.719.
Thecorrelations for these metrics might be expected tobe lower for our data, since overall quality is higher,making the metrics?
task more difficult as the out-puts involve subtler differences between acceptableand unacceptable variation.The human-targeted metrics (represented by theprefixed H in the data tables) correlated even morestrongly with the human judgments, compared to thenon-targeted versions.
HTER demonstrated the bestcorrelation with realizer fluency (r = 0.75).For several kinds of acceptable variation involv-ing the rearrangement of constituents (such as da-tive shift), TERP gives a more reasonable score thanBLEU, due to its ability to directly evaluate phrasalshifts.
The following realization was rated 4.5 forfluency, and was more correctly ranked by TERPthan BLEU:(3) Ref: The deal also gave Mitsui access toa high-tech medical product.
(4) Realiz.
: The deal also gave access to ahigh-tech medical product to Mitsui.For each reference sentence, we compared theranking of its realizations induced from the humanscores to the ranking induced from the TERP score,and counted the rank errors by the latter, infor-mally categorizing them by error type (see Table7).
In the 50 sentences with the highest numbers ofrank errors, 17 were affected by punctuation differ-ences, typically involving variation in comma place-ment.
Human fluency judgments of outputs withonly punctuation problems were generally high, andmany realizations with commas inserted or removedwere rated fully fluent by the annotators.
However,TERP penalizes such insertions or deletions.
Agree-ment errors are another frequent source of rank-ing errors for TERP.
The human judges tended toharshly penalize sentences with number-agreementor tense errors, whereas TERP applies only a singlesubstitution penalty for each such error.
We expectthat with suitable optimization of edit weights toavoid over-penalizing punctuation shifts and under-penalizing agreement errors, TERP would exhibit aneven stronger correlation with human fluency judg-ments.None of the evaluation metrics can distinguish anacceptable movement of a word or constituent froman unacceptable movement, with only one referencesentence.
A substantial source of error for bothTERP and BLEU is variation in adverbial placement,as shown in (7).Similar errors are seen with prepositional phrasesand some commonly-occurring temporal adverbs,which typically admit a number of variations inplacement.
Another important example of accept-able variation which these metrics do not generallyrank correctly is dative alternation:569(7)Ref.
We need to clarify what exactly is wrong with it.Realiz.
Flu.
TERP BLEUWe need to clarify exactly what is wrong with it.
5 0.1 0.5555We need to clarify exactly what ?s wrong with it.
5 0.2 0.4046We need to clarify what , exactly , is wrong with it.
5 0.2 0.5452We need to clarify what is wrong with it exactly.
4.5 0.1 0.6756We need to clarify what exactly , is wrong with it.
4 0.1 0.7017We need to clarify what , exactly is wrong with it.
4 0.1 0.7017We needs to clarify exactly what is wrong with it.
3 0.103 0.346(5) Ref.
When test booklets were passedout 48 hours ahead of time, she says shecopied questions in the social studies sec-tion and gave the answers to students.
(6) Realiz.
When test booklets were passedout 48 hours ahead of time , she says shecopied questions in the social studies sec-tion and gave students the answers.The correlations of each of the metrics with thehuman judgments of fluency for the realizer systemsindicate at least a moderate relationship, in contrastwith the results reported in (Stent et al, 2005) forparaphrase data, which found an inverse correlationfor fluency, and (Cahill, 2009) for the output of a sur-face realizer for German, which found only a weakcorrelation.
However, the former study employeda corpus-based paraphrase generation system ratherthan grammar-driven surface realizers, and the re-sulting paraphrases exhibited much broader varia-tion.
In Cahill?s study, the outputs of the realizerwere almost always grammatically correct, and theautomated evaluation metrics were ranking marked-ness instead of grammatical acceptability.5.3 System-level comparisonsIn order to investigate the efficacy of the metricsin ranking different realizer systems, or competingrealizations from the same system generated usingdifferent ranking models, we considered seven dif-ferent ?systems?
from the whole dataset of realiza-tions.
These consisted of five OpenCCG-based re-alizations (the best realization from three baselinemodels, and the best and the worst realization fromthe full perceptron model), and two XLE-based sys-tems (the best and the worst realization, after rank-ing the outputs of the XLE realizer with an n-grammodel).
The mean of the combined adequacy andfluency scores of each of these seven systems wascompared with that of every other system, result-ing in 21 pairwise comparisons.
Then Tukey?s HSDtest was performed to determine the systems whichdiffered significantly in terms of the average ade-quacy and fluency rating they received.4 The testrevealed five pairwise comparisons where the scoreswere significantly different.Subsequently, for each of these systems, an over-all system-level score for each of the MT metricswas calculated.
For the five pairwise comparisonswhere the adequacy-fluency group means differedsignificantly, we checked whether the metric rankedthe systems correctly.
Table 8 shows the results ofa pairwise comparison between the ranking inducedby each evaluation metric, and the ranking inducedby the human judgments.
Five of the seven non-targeted metrics correctly rank more than half of thesystems.
NIST, METEOR, and GTM get the mostcomparisons right, but neither NIST nor GTM cor-rectly rank the OpenCCG-baseline model 1 with re-spect to the XLE-best model.
TER and TERP get twoof the five comparisons correct, and they incorrectlyrank two of the five OpenCCG model comparisons,as well as the comparison between the XLE-worstand OpenCCG-best systems.For the targeted metrics, HNIST is correct for allfive comparisons, while neither HBLEU nor HME-TEOR correctly rank all the OpenCCG models.
Onthe other hand, HTER and HGTM incorrectly rank theXLE-best system versus OpenCCG-based models.In summary, some of the metrics get some of therankings correct, but none of the non-targeted met-rics get al of them correct.
Moreover, different met-rics make different ranking errors.
This argues for4This particular test was chosen since it corrects for multiplepost-hoc analyses conducted on the same data-set.570the use of multiple metrics in comparing realizersystems.6 ConclusionOur study suggests that although the task of evalu-ating the output from realizer systems differs fromthe task of evaluating machine translations, the au-tomatic metrics used to evaluate MT outputs delivermoderate correlations with combined human fluencyand adequacy scores when used on surface realiza-tions.
We also found that the MT-evaluation met-rics are useful in evaluating different versions of thesame realizer system (e.g., the various OpenCCG re-alization ranking models), and finding cases wherea system is performing poorly.
As in MT-evaluationtasks, human-targeted metrics have the highest cor-relations with human judgments overall.
These re-sults suggest that the MT-evaluation metrics are use-ful for developing surface realizers.
However, thecorrelations are lower than those reported for MTdata, suggesting that they should be used with cau-tion, especially for cross-system evaluation, whereconsulting multiple metrics may yield more reliablecomparisons.
In our study, the targeted version ofTERP correlated most strongly with human judg-ments of fluency.In future work, the performance of the TER familyof metrics on this data might be improved by opti-mizing the edit weights used in computing its scores,so as to avoid over-penalizing punctuation move-ments or under-penalizing agreement errors, bothof which were significant sources of ranking errors.Multiple reference sentences may also help mitigatethese problems, and the corpus of human-repairedrealizations that has resulted from our study is a stepin this direction, as it provides multiple referencesfor some cases.
We expect the corpus to also proveuseful for feature engineering and error analysis indeveloping better realization models.5AcknowledgementsWe thank Aoife Cahill and Tracy King for providingus with the output of the XLE generator.
We alsothank Chris Callison-Burch and the anonymous re-viewers for their helpful comments and suggestions.5The corpus can be downloaded from http://www.ling.ohio-state.edu/?mwhite/data/emnlp10/.This material is based upon work supported bythe National Science Foundation under Grant No.0812297.ReferencesJason Baldridge.
2002.
Lexically Specified DerivationalControl in Combinatory Categorial Grammar.
Ph.D.thesis, University of Edinburgh.S.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved corre-lation with human judgments.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion, pages 65?72.R.
Barzilay and L. Lee.
2003.
Learning to paraphrase:An unsupervised approach using multiple-sequencealignment.
In proceedings of HLT-NAACL, volume2003, pages 16?23.Aoife Cahill.
2009.
Correlating human and automaticevaluation of a german surface realiser.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Pa-pers, pages 97?100, Suntec, Singapore, August.
Asso-ciation for Computational Linguistics.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.
Re-evaluating the role of BLEU in machine translation re-search.
In Proceedings of EACL, volume 2006, pages249?256.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(meta-)evaluation of machine translation.
In StatMT ?07: Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 136?158, Morristown, NJ,USA.
Association for Computational Linguistics.C.
Callison-Burch, T. Cohn, and M. Lapata.
2008.
Para-metric: An automatic evaluation metric for paraphras-ing.
In Proceedings of the 22nd International Con-ference on Computational Linguistics-Volume 1, pages97?104.
Association for Computational Linguistics.J.
Carletta.
1996.
Assessing agreement on classificationtasks: the kappa statistic.
Computational linguistics,22(2):249?254.Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,John Maxwell, and Paula Newman.
2008.
Xle docu-mentation.
Technical report, Palo Alto Research Cen-ter.Philip Koenh.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing.J.R.
Landis and G.G.
Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174.571Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metrics formachine translation.
In COLING ?04: Proceedingsof the 20th international conference on ComputationalLinguistics, page 501, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical report, IBM Research.E.
Reiter and A. Belz.
2009.
An investigation into thevalidity of some metrics for automatically evaluatingnatural language generation systems.
ComputationalLinguistics, 35(4):529?558.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. III Maxwell, and Mark John-son.
2002.
Parsing the wall street journal usinga lexical-functional grammar and discriminative esti-mation techniques.
In Proceedings of 40th AnnualMeeting of the Association for Computational Lin-guistics, pages 271?278, Philadelphia, Pennsylvania,USA, July.
Association for Computational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In InProceedings of Association for Machine Translation inthe Americas, pages 223?231.M.
Snover, N. Madnani, B.J.
Dorr, and R. Schwartz.2009.
Fluency, adequacy, or HTER?
: exploring dif-ferent human judgments with a tunable MT metric.In Proceedings of the Fourth Workshop on StatisticalMachine Translation, pages 259?268.
Association forComputational Linguistics.Amanda Stent, Matthew Marge, and Mohit Singhai.2005.
Evaluating evaluation methods for generation inthe presence of variation.
In Proceedings of CICLing.J.P.
Turian, L. Shen, and I.D.
Melamed.
2003.
Evalua-tion of machine translation and its evaluation.
recall(C?
R), 100:2.Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 410?419, Singapore,August.
Association for Computational Linguistics.Michael White.
2006.
Efficient Realization of Coordi-nate Structures in Combinatory Categorial Grammar.Research on Language and Computation, 4(1):39?75.572Type System #Refs #Paraphrases Average Paraphrases/Ref #Exact Matches Adq FluSingle output OpenCCG Baseline 1 296 296 1.0 72 4.17 3.65OpenCCG Baseline 2 296 296 1.0 82 4.34 3.94OpenCCG Baseline 3 296 296 1.0 76 4.31 3.86OpenCCG Perceptron Best 296 1.0 1.0 112 4.37 4.09OpenCCG Perceptron Worst 117 117 1.0 5 4.34 3.36XLE Best 154 154 1.0 24 4.41 4.07XLE Worst 157 157 1.0 13 4.08 3.73Multiple output OpenCCG-Perceptron All 296 767 2.6 158 4.45 3.91OpenCCG All 296 1131 3.8 162 4.20 3.61XLE All 174 557 3.2 54 4.17 3.81Wordnet Subsitutions 162 486 3.0 0 3.66 4.71Realizer All 296 1628 5.0 169 4.16 3.63All 296 2114 7.1 169 4.05 3.88Table 1: Descriptive statisticsSystem Adq Flup(A) p(E) ?
p(A) p(E) ?OpenCCG-Abs 0.73 0.47 0.48 0.70 0.24 0.61OpenCCG-Rel 0.76 0.47 0.54 0.76 0.34 0.64XLE-Abs 0.68 0.42 0.44 0.69 0.27 0.58XLE-Rel 0.73 0.45 0.50 0.69 0.37 0.50Wordnet-Abs 0.57 0.25 0.43 0.77 0.66 0.33Wordnet-Rel 0.74 0.34 0.61 0.73 0.60 0.33Realizer-Abs 0.70 0.44 0.47 0.69 0.24 0.59Realizer-Rel 0.74 0.41 0.56 0.73 0.33 0.60All-Abs 0.67 0.38 0.47 0.71 0.29 0.59All-Rel 0.74 0.36 0.60 0.75 0.34 0.63Table 2: Corpora-wise inter-annotator agreement (absolute and relative ?
values shown)Sys N B M G TP TA T HT HN HB HM HGOpenCCG-Adq 0.27 0.39 0.35 0.18 0.39 0.34 0.4 0.43 0.3 0.43 0.43 0.23OpenCCG-Flu 0.49 0.55 0.4 0.42 0.6 0.46 0.6 0.72 0.58 0.69 0.57 0.53XLE-Adq 0.52 0.51 0.55 0.31 0.5 0.5 0.5 0.52 0.47 0.51 0.61 0.4XLE-Flu 0.56 0.56 0.48 0.37 0.55 0.5 0.55 0.61 0.54 0.61 0.51 0.51Wordnet-Adq 0.17 0.14 0.24 0.15 0.37 0.26 0.22 0.64 0.52 0.56 0.32 0.6Wordnet-Flu 0.26 0.21 0.24 0.24 0.22 0.27 0.26 0.34 0.32 0.34 0.3 0.34Realizer-Adq 0.47 0.6 0.57 0.42 0.59 0.57 0.6 0.62 0.49 0.62 0.65 0.48Realizer-Flu 0.51 0.62 0.52 0.5 0.63 0.53 0.64 0.75 0.59 0.73 0.65 0.63All-Adq 0.37 0.37 0.33 0.32 0.42 0.31 0.43 0.53 0.44 0.48 0.44 0.45All-Flu 0.21 0.62 0.51 0.32 0.61 0.55 0.6 0.7 0.33 0.71 0.62 0.48Table 3: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (-Adq: adequacy and -Flu: Fluency); Scoreswhich fall within the 95 %CI of the best are italicized.Sys N B M G TP TA T HT HN HB HM HGOpenCCG 0.49 0.57 0.42 0.4 0.61 0.46 0.62 0.73 0.58 0.7 0.59 0.51XLE 0.63 0.64 0.59 0.39 0.62 0.58 0.63 0.69 0.6 0.68 0.63 0.54Wordnet 0.21 0.14 0.21 0.19 0.38 0.25 0.23 0.65 0.56 0.57 0.31 0.63Realizer 0.55 0.68 0.57 0.5 0.68 0.58 0.69 0.78 0.61 0.77 0.7 0.63All 0.34 0.58 0.47 0.38 0.61 0.48 0.61 0.75 0.48 0.73 0.61 0.58Table 4: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (combined adequacy and fluency scores)573System Adq FluSp 95%L 95%U Sp 95%L 95%URealizer 0.60 0.58 0.63 0.62 0.59 0.65XLE 0.51 0.47 0.56 0.56 0.51 0.61OpenCCG 0.39 0.35 0.42 0.55 0.52 0.59All 0.37 0.34 0.4 0.62 0.6 0.64Wordnet 0.14 0.06 0.21 0.21 0.13 0.28Table 5: Spearman?s correlation analysis (bootstrap sampling) of the BLEU scores of various systems with humanadequacy and fluency scoresSys HJ N B M G TP TA T HT HN HB HM HG HJ1-HJ2OpenCCG HJ-1 0.44 0.52 0.39 0.36 0.56 0.43 0.58 0.75 0.58 0.72 0.62 0.52 0.76HJ-2 0.5 0.58 0.43 0.4 0.62 0.46 0.63 0.7 0.55 0.68 0.56 0.49XLE HJ-1 0.6 0.6 0.55 0.37 0.57 0.55 0.58 0.69 0.63 0.68 0.64 0.54 0.75HJ-2 0.6 0.6 0.56 0.39 0.6 0.55 0.61 0.64 0.54 0.61 0.57 0.51Wordnet HJ-1 0.2 0.18 0.26 0.16 0.37 0.28 0.24 0.7 0.59 0.64 0.35 0.65 0.72HJ-2 0.25 0.16 0.23 0.19 0.37 0.25 0.25 0.59 0.52 0.51 0.32 0.56Realizer HJ-1 0.51 0.65 0.56 0.49 0.64 0.56 0.66 0.8 0.62 0.78 0.72 0.64 0.82HJ-2 0.55 0.68 0.56 0.5 0.67 0.57 0.68 0.74 0.58 0.73 0.66 0.6All HJ-1 0.32 0.53 0.45 0.37 0.57 0.44 0.57 0.77 0.5 0.74 0.62 0.59 0.79HJ-2 0.35 0.58 0.46 0.37 0.61 0.47 0.6 0.71 0.44 0.69 0.57 0.54Table 6: Spearman?s correlations of NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA), humanvariants (HT, HN, HB, HM, HG), and individual human judgments (combined adq.
and flu.
scores)Factor CountPunctuation 17Adverbial shift 16Agreement 14Other shifts 8Conjunct rearrangement 8Complementizer ins/del 5PP shift 4Table 7: Factors influencing TERP ranking errors for 50 worst-ranked realization groupsMetric Score Errorsnist 4 C1-XBbleu 3 XB-PW C1-XBmeteor 4 XW-PBter 2 PW-PB XW-PB C1-PBterp 2 PW-PB XW-PB C1-PBterpa 3 XW-PB C1-PBgtm 4 C1-XBhnist 5hbleu 3 PW-PB XW-PBhmeteor 2 PW-PB XW-PB C1-PBhter 3 XB-PW C1-XBhgtm 3 XB-PW C1-XBTable 8: Metric-wise ranking performance in terms of agreement with a ranking induced by combined adequacy andfluency scores; each metric gets a score out of 5 (i.e.
number of system-level comparisons that emerged significant asper the Tukey?s HSD test)Legend: Perceptron Best (PB); Perceptron Worst (PW); XLE Best (XB); XLE Worst (XW); OpenCCG baseline mod-els 1 to 3 (C1 ... C3)574
