Proceedings of NAACL HLT 2007, Companion Volume, pages 201?204,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Three-step Deterministic Parser for Chinese Dependency ParsingKun Yu Sadao Kurohashi Hao LiuGraduate School of Informatics Graduate School of Informatics Graduate School of InformationScience and TechnologyKyoto University Kyoto University The University of Tokyokunyu@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp liuhao@kc.t.u-tokyo.ac.jpAbstractThis paper presents a three-step dependencyparser to parse Chinese deterministically.
By divid-ing a sentence into several parts and parsing themseparately, it aims to reduce the error propagationcoming from the greedy characteristic of determi-nistic parsing.
Experimental results showed thatcompared with the deterministic parser whichparsed a sentence in sequence, the proposed parserachieved extremely significant improvement ondependency accuracy.1 IntroductionRecently, as an attractive alternative to probabilisticparsing, deterministic parsing (Yamada and Matsumoto,2003; Nivre and Scholz, 2004) has drawn great attentionwith its high efficiency, simplicity and good accuracycomparable to the state-of-the-art generative probabilis-tic models.
The basic idea of deterministic parsing isusing a greedy parsing algorithm that approximates aglobally optimal solution by making a sequence of lo-cally optimal choices (Hall et al, 2006).
This greedyidea guarantees the simplicity and efficiency, but at thesame time it also suffers from the error propagationfrom the previous parsing choices to the left decisions.For example, given a Chinese sentence, which meansPaternity test is a test that gets personal identitythrough DNA analysis, and it brings proof for findinglost children, the correct dependency tree is shown bysolid line  (see Figure 1).
But, if word ??
(through) isincorrectly parsed as depending on word ?
(is) (shownby dotted line), this error will result in the incorrectparse of word??
(a test) as depending on word ??
(brings) (shown by dotted line).This problem exists not only in Chinese, but also inother languages.
Some efforts have been done to solvethis problem.
Cheng et al (2005) used a root finder todivide one sentence into two parts by the root word andparsed them separately.
But the two-part division is notenough when a sentence is composed of several coordi-nating sub-sentences.
Chang et al (2006) applied apipeline framework in their dependency parser to makethe local predictions more robust.
While it did not showgreat help for stopping the error propagation betweendifferent parsing stages.Figure 1.
Dependency tree of a sentence  (word sequence is top-down)This paper focuses on resolving this issue for Chi-nese.
After analyzing the dependency structure of sen-tences in Penn Chinese Treebank 5.1 (Xue et al, 2002),we found an interesting phenomenon: if we define amain-root as the head of a sentence, and define a sub-sentence as a sequence of words separated by punctua-tions, and the head1 of these words is the child of main-root or main-root itself, then the punctuations that de-pend on main-root can be a separator of sub-sentences.For example, in the example sentence there are threepunctuations marked as PU_A, PU_B and PU_C, inwhich PU_B and PU_C depends on main-root butPU_A depends on word ??(gets).
According to ourobservation, PU_B and PU_C can be used for segment-ing this sentence into two sub-sentences A and B (cir-cled by dotted line in Figure 2), where the sub-root of Ais main-root and the sub-root of B depends on main-root.This phenomenon gives us a useful clue: if we dividea sentence by the punctuations whose head is main-root,then the divided sub-sentences are basically independ-ent of each other, which means we can parse them sepa-rately.
The shortening of sentence length and the recog-nition of sentence structure guarantee the robustness ofdeterministic parsing.
The independent parsing of eachsub-sentence also prevents the error-propagation.
In1The head of sub-sentence is defined as a sub-root.201addition, because the sub-root depends on main-root oris main-root itself, it is easy to combine the dependencystructure of each sub-sentence to create the final de-pendency tree.Figure 2.
A segmentation of the sentence in Figure 1Based on above analyses, this paper proposes a three-step deterministic dependency parser for Chinese, whichworks as:Step1(Sentence Segmentation): Segmenting a sen-tence into sub-sentences by punctuations (sub-sentencesdo not contain the punctuations for segmentation);Step2(Sub-sentence Parsing): Parsing each sub-sentence deterministically;Step3(Parsing Combination): Finding main-rootamong all the sub-roots, then combining the dependencystructure of sub-sentences by making main-root as thehead of both the left sub-roots and the punctuations forsentence segmentation.2 Sentence SegmentationAs mentioned in section 1, the punctuations dependingon main-root can be used to segment a sentence intoseveral sub-sentences, whose sub-root depends on main-root or is main-root.
But by analysis, we found onlyseveral punctuations were used as separator commonly.To ensure the accuracy of sentence segmentation, wefirst define the punctuations which are possible for seg-mentation as valid punctuation, which includes comma,period, colon, semicolon, question mark, exclamatorymark and ellipsis.
Then the task in step 1 is to findpunctuations which are able to segment a sentence fromall the valid punctuations in a sentence, and use them todivide the sentence into two or more sub-sentences.We define a classifier (called as sentence seg-menter) to classify the valid punctuations in a sentenceto be good or bad for sentence segmentation.
SVM (Se-bastiani, 2002) is selected as classification model for itsrobustness to over-fitting and high performance.Table 1 shows the binary features defined for sen-tence segmentation.
We use a lexicon consisting of allthe words in Penn Chinese Treebank 5.1 to lexicalizeword features.
For example, if word ?
(for) is the27150th word in the lexicon, then feature Word1 ofPU_B (see Figure 2) is ?27150:1?.
The pos-tag featuresare got in the same way by a pos-tag list containing 33pos-tags, which follow the definition in Penn ChineseTreebank.
Such method is also used to get word andpos-tag features in other modules.Table 1.
Features for sentence segmenterFeature DescriptionWordn/Posn word/pos-tag in different position, n=-2,-1,0,1,2Word_left/Pos_leftword/pos-tag between the first left valid punctua-tion and current punctuationWord_right/Pos_rightword/pos-tag between current punctuation andthe first right valid punctuation#Word_left/#Word_rightif the number of words between the first left/rightvalid punctuation and current punctuation ishigher than 2, set as 1; otherwise set as 0V_left/V_rightif there is a verb between the first left/right validpunctuation and current punctuation, set as 1;otherwise set as 0N_leftFirst/N_rightFirstif the left/right neighbor word is a noun, set as 1;otherwise set as 0P_rightFirst/CS_rightFirstif the right neighbor word is a preposi-tion/subordinating conjunction, set as 1; other-wise set as 03 Sub-sentence Parsing3.1 Parsing AlgorithmThe parsing algorithm in step 2 is a shift-reduce parserbased on (Yamada and Matsumoto, 2003).
We call it assub-sentence parser.Two stacks P and U are defined, where stack P keepsthe words under consideration and stack U remains allthe unparsed words.
All the dependency relations cre-ated by the parser are stored in queue A.At start, stack P and queue A are empty and stack Ucontains all the words.
Then word on the top of stack Uis pushed into stack P, and a trained classifier findsprobable action for word pair <p,u> on the top of thetwo stacks.
After that, according to different actions,dependency relations are created and pushed into queueA, and the elements in the two stacks move at the sametime.
Parser stops when stack U is empty and the de-pendency tree can be drawn according to the relationsstored in queue A.Four actions are defined for word pair <p, u>:LEFT: if word p modifies word u, then push puinto A and push u into P.RIGHT: if word u modifies word p, then push upinto A and pop p.REDUCE: if there is no word u?
(u?
?U and u?
?u)which modifies p, and word next to p in stack P is p?shead, then pop p.SHIFT: if there is no dependency relation between pand u, and word next to p in stack P is not p?s head, thenpush u into stack P.202We construct a classifier for each action separately,and classify each word pair by all the classifiers.
Thenthe action with the highest classification score is se-lected.
SVM is used as the classifier, and One vs. Allstrategy (Berger, 1999) is applied for its good efficiencyto extend binary classifier to multi-class classifier.3.2 FeaturesFeatures are crucial to this step.
First, we define somefeatures based on local context (see Flocal in Table 2),which are often used in other deterministic parsers(Yamada and Matsumoto, 2003; Nivre et al, 2006).Then, to get top-down information, we add some globalfeatures (see Fglobal in Table 2).
All the features are bi-nary features, except that Distance is normalized be-tween 0-1 by the length of sub-sentence.Before parsing, we use a root finder (i.e.
the sub-sentence root finder introduced in Section 4) to getRootn feature, and develop a baseNP chunker to getBaseNPn feature.
In the baseNP chunker, IOB represen-tation is applied for each word, where B means the wordis the beginning of a baseNP, I means the word is insideof a baseNP, and O means the word is outside of abaseNP.
Tagging is performed by SVM with One vs. Allstrategy.
Features used in baseNP chunking are currentword, surrounding words and their corresponding pos-tags.
Window size is 5.Table 2.
Features for sub-sentence parserFeature DescriptionWordn/Posnword/pos-tag in different position,n= P0, P1, P2, U0, U1, U2 (Pi/Ui meanthe ith position from top in stack P/U)Word_childn/Pos_childnthe word/pos-tag of the children ofWordn, n= P0, P1, P2, U0, U1, U2LocalFeature(Flocal)Distance distance between p and u in sentenceRootnif Wordn is the sub-root of this sub-sentence, set as 1; otherwise set as 0GlobalFeature(Fglobal) BaseNPn baseNP tag of WordnTable 3.
Features for sentence/sub-sentence root finderFeature DescriptionWordn/Posn words in different position, n=-2,-1,0,1,2Word_left/Pos_left wordn/posn where n<-2Word_right/Pos_right wordn/posn where n>2#Word_left/#Word_rightif the number of words between thestart/end of sentence and current word ishigher than 2, set as 1; otherwise set as 0V_left/V_rightif there is a verb between the start/end ofsentence and current word, set as 1; oth-erwise set as 0Nounn/Verbn/Adjnif the word in different position is anoun/verb/adjective, set as 1; otherwiseset as 0. n=-2,-1,1,2Dec_right if the word next to current word in rightside is ?
(of), set as 1; otherwise set as 0CC_leftif there is a coordinating conjunctionbetween the start of sentence and currentword, set as 1; otherwise set as 0BaseNPn baseNP tag of Wordn4 Parsing CombinationA root finder is developed to find main-root for parsingcombination.
We call it as sentence root finder.
Wealso retrain the same module to find the sub-root in step2, and call it as sub-sentence root finder.We define the root finding problem as a classificationproblem.
A classifier, where we still select SVM, istrained to classify each word to be root or not.
Then theword with the highest classification score is chosen asroot.
All the binary features for root finding are listed inTable 3.
Here the baseNP chunker introduced in section3.2 is used to get the BaseNPn feature.5 Experimental Results5.1 Data Set and Experimental SettingWe use Penn Chinese Treebank 5.1 as data set.
Totransfer the phrase structure into dependency structure,head rules are defined based on Xia?s head percolationtable (Xia and Palmer, 2001).
16,984 sentences and1,292 sentences are used for training and testing.
Thesame training data is also used to train the sentencesegmenter, the baseNP chunker, the sub-sentence rootfinder, and the sentence root finder.
During both train-ing and testing, the gold-standard word segmentationand pos-tag are applied.TinySVM is selected as a SVM toolkit.
We use apolynomial kernel and set the degree as 2 in all the ex-periments.5.2 Three-step Parsing vs. One-step ParsingFirst, we evaluated the dependency accuracy and rootaccuracy of both three-step parsing and one-step parsing.Three-step parsing is the proposed parser and one-stepparsing means parsing a sentence in sequence (i.e.
onlyusing step 2).
Local and global features are used in bothof them.Results (see Table 4) showed that because of theshortening of sentence length and the prevention of er-ror propagation three-step parsing got 2.14% increaseon dependency accuracy compared with one-step pars-ing.
Based on McNemar?s test (Gillick and Cox, 1989),this improvement was considered extremely statisticallysignificant (p<0.0001).
In addition, the proposed parsergot 1.01% increase on root accuracy.Table 4.
Parsing result of three-step and one-step parsingParsing Strategy Dep.Accu.
(%)Root Accu.(%)Avg.
ParsingTime (sec.
)One-step Parsing 82.12 74.92 22.13Three-step Parsing 84.26 (+2.14)75.93(+1.01)24.27(+2.14)Then we tested the average parsing time for each sen-tence to verify the efficiency of proposed parser.
Theaverage sentence length is 21.68 words.
Results (seeTable 4) showed that compared with one-step parsing,the proposed parser only used 2.14 more seconds aver-203agely when parsing one sentence, which did not affectefficiency greatly.To verify the effectiveness of proposed parser oncomplex sentences, which contain two or more sub-sentences according to our definition, we selected 665such sentences from testing data set and did evaluationagain.
Results (see Table 5) proved that our parseroutperformed one-step parsing successfully.Table 5.
Parsing result of complex sentenceParsing Strategy Dep.Accu.
(%) Root Accu.
(%)One-step Parsing 82.56 78.95Three-step Parsing 84.94 (+2.38) 79.25 (+0.30)5.3 Comparison with Others?
WorkAt last, we compare the proposed parser with Nivre?sparser (Hall et al, 2006).
We use the same head rulesfor dependency transformation as what were used inNivre?s work.
We also used the same training (section1-9) and testing (section 0) data and retrained all themodules.
Results showed that the proposed parserachieved 84.50% dependency accuracy, which was0.20% higher than Nivre?s parser (84.30%).6 DiscussionIn the proposed parser, we used five modules: sentencesegmenter (step1); sub-sentence root finder (step2);baseNP chunker (step2&3); sub-sentence parser (step2);and sentence root finder (step3).The robustness of the modules will affect parsing ac-curacy.
Thus we evaluated each module separately.
Re-sults (see Table 6) showed that all the modules got rea-sonable accuracy except for the sentence root finder.Considering about this, in step 3 we found main-rootonly from the sub-roots created by step 2.
Because thesub-sentence parser used in step 2 had good accuracy, itcould provide relatively correct candidates for main-rootfinding.
Therefore it helped decrease the influence ofthe poor sentence root finding to the proposed parser.Table 6.
Evaluation result of each moduleModule F-score(%) Dep.Accu(%)Sentence Segmenter (M1) 88.04 ---Sub-sentence Root Finder (M2) 88.73 ---BaseNP Chunker (M3) 89.25 ---Sub-sentence Parser (M4) --- 85.56Sentence Root Finder (M5) 78.01 ---Then we evaluated the proposed parser assuming us-ing gold-standard modules (except for sub-sentenceparser) to check the contribution of each module toparsing.
Results (see Table 7) showed that (1) the accu-racy of current sentence segmenter was acceptable be-cause only small increase on dependency accuracy androot accuracy was got by using gold-standard sentencesegmentation; (2) the correct recognition of baseNPcould help improve dependency accuracy but gave alittle contribution to root accuracy; (3) the accuracy ofboth sub-sentence root finder and sentence root finderwas most crucial to parsing.
Therefore improving thetwo root finders is an important task in our future work.Table 7.
Parsing result with gold-standard modulesGold-standard Module Dep.Accu(%) Root.Accu(%)w/o 84.26 75.93M1 84.51 76.24M1+M2 86.57 80.34M1+M2+M3 88.63 80.57M1+M2+M3+M5 91.25 91.027 Conclusion and Future WorkWe propose a three-step deterministic dependencyparser for parsing Chinese.
It aims to solve the errorpropagation problem by dividing a sentence into inde-pendent parts and parsing them separately.
Resultsbased on Penn Chinese Treebank 5.1 showed that com-pared with the deterministic parser which parsed a sen-tence in sequence, the proposed parser achieved ex-tremely significant increase on dependency accuracy.Currently, the proposed parser is designed only forChinese.
But we believe it can be easily adapted to otherlanguages because no language-limited information isused.
We will try this work in the future.
In addition,improving sub-sentence root finder and sentence rootfinder will also be considered in the future.AcknowledgementWe would like to thank Dr. Daisuke Kawahara and Dr. Eiji Aramakifor their helpful discussions.
We also thank the three anonymousreviewers for their valuable comments.ReferenceA.Berger.
Error-correcting output coding for text classification.
1999.In Proceedings of the IJCAI-99 Workshop on Machine Learningfor Information Filtering.M.Chang, Q.Do and D.Roth.
2006.
A Pipeline Framework for De-pendency Parsing.
In Proceedings of Coling-ACL 2006.Y.Cheng, M.Asahara and Y.Matsumoto.
2005.
Chinese DeterministicDependency Analyzer: Examining Effects of Global Features andRoot Node Finder.
In Proceedings of IJCNLP 2005.L.Gillick and S.J.Cox.
1989.
Some Statistical Issues in the Compari-son of Speech Recognition Algorithms.
In Proceedings of ICASSP.J.Hall, J.Nivre and J.Nilsson.
2006.
Discriminative Classifiers forDeterministic Dependency Parsing.
In Proceedings of Coling-ACL2006.
pp.
316-323.J.Nivre and M.Scholz.
2004.
Deterministic Dependency Parsing ofEnglish Text.
In Proceedings of Coling 2004. pp.
64-70.F.Sebastiani.
2002.
Machine learning in automated text categorization.ACM Computing Surveys, 34(1): 1-47.F.Xia and M.Palmer.
2001.
Converting Dependency Structures toPhrase Structures.
In HLT-2001.N.Xue, F.Chiou and M.Palmer.
2002.
Building a Large-Scale Anno-tated Chinese Corpus.
In Proceedings of COLING 2002.H.Yamada and Y.Matsumoto.
2003.
Statistical Dependency Analysiswith Support Vector Machines.
In Proceedings of IWPT.
2003.204
