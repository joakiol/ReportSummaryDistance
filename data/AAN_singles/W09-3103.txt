Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 11?18,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPAn Extensible Crosslinguistic Readability FrameworkJesse Saba KirchnerDepartment of LinguisticsUC Santa Cruz1156 High StreetSanta Cruz, CA 95064kirchner@ucsc.eduJustin NugerDepartment of LinguisticsUC Santa Cruz1156 High StreetSanta Cruz, CA 95064jnuger@ucsc.eduYi ZhangBaskin School of EngineeringUC Santa Cruz1156 High Street, SOE 3Santa Cruz, CA 95064yiz@soe.ucsc.eduAbstractAutomatic assessment of the readabilitylevel (i.e., the relative linguistic complex-ity) of documents in a large number oflanguages is an important problem thatcan be applied to many real-world appli-cations, such as retrieving age-appropriatesearch engine results for kids, construct-ing automatic tutoring systems, and so on.Unfortunately, existing readability label-ing techniques have only been applied toa very small number of languages.
In thispaper, we present an extensible crosslin-guistic readability framework based on theuse of parallel corpora to quickly createreadability software for thousands of lan-guages, including languages for which nolinguists are available to define readabilityrules or for which documents with read-ability labels are lacking to train readabil-ity models.
To demonstrate our idea, wedeveloped a system based on the proposedframework.
This paper discusses the theo-retical and practical issues involved in de-signing such a system and presents the re-sults of an experiment conducted with thesystem.1 IntroductionAutomatically labeling the reading difficulty of anarbitrary document is an important problem in sev-eral human language technology applications.
Itcan, for example, be used in the next generation ofpersonalized information retrieval systems to finddocuments tailored to children at different gradelevels.
In a tutoring system, it can be used to findonline reading materials of the appropriate diffi-culty level for students (Heilman et al, 2006).Of the world?s more than 6,000 languages(Grimes, 2005), readability classification softwareexists for a striking few, and it is limited in cover-age to languages spoken in countries with promi-nent standing in global economics and politics.A substantial number of the remaining languagesnevertheless have a sufficient corpus of digitaldocuments ?
a number which may already bein the hundreds and soon in the thousands (Pao-lillo et al, 2005).
A natural idea is to createsoftware to automatically predict readability lev-els (henceforth ?RLs?)
for these documents.
Suchsoftware has significant potential for applicationsin different areas of research, such as creating websearch engines for kids speaking languages notcovered by existing readability software, as de-scribed above.There is much research on assessing the read-ing difficulties of texts in a particular language,and the existing work can be roughly classified asfalling under two approaches.
The first approachuses manually or semi-automatically crafted rulesdesigned by computational linguists who are fa-miliar with the language in question (Anderson,1981).
The second approach learns readabilitymodels for a particular language based on labeleddata (Collins-Thompson and Callan, 2004).Unfortunately, existing approaches cannot beeasily extended to handle thousands of differentlanguages.
The first approach, using rules de-vised by computational linguists familiar with thelanguages, is impractical because for many lan-guages, especially minority or understudied lan-guages, there are relatively few linguists suffi-ciently familiar with the language to design suchsoftware.
Even if these linguists exist, it is un-likely that a search engine company that wanted toserve the whole world would have the resources to11hire all of them.
The second approach, using ma-chine learning techniques on labeled data, is veryexpensive because it requires the support of edu-cated speakers of each language to provide read-ability labels for documents in the language.
Theavailability of such speakers cannot always be as-sumed.
Again, recruiting annotators for thousandsof different languages is not economically feasibleor practical for a company.
An alternative strategythat can scale to thousands of different languagesis needed.In this paper, we propose a general frameworkto solve this problem based on a parallel corpuscrawled from the web.
To illustrate the idea, wedeveloped an Extensible Crosslinguistic Readabil-ity system (henceforth ?ECR system?
), which usesa Cross-Lingual Information Retrieval (henceforth?CLIR?)
system that we call EXCLAIM.
The ECRsystem functions to create RL classification soft-ware in any language with sufficient coverage inthe CLIR system.
We also report the promising ?though very preliminary ?
results of an experi-ment that tests a real-world application of this sys-tem.
Investigation of the basic assumptions andgeneralization of parameters and evaluation met-rics are left for future work.The rest of this paper is organized as follows.The problem setting is described in Section 2.
Thearchitecture of our ECR system is explained inSection 3.
Our experimental design is laid out inSection 4, followed by experimental result analy-sis in Section 5.
Section 6 gives an overview ofrelated work, and section 7 concludes.2 Problem and Proposed Methodology2.1 Existing Approaches to ReadabilityClassificationIn traditional approaches to computational read-ability classification, there is a variety of language-specific system requirements needed in order toperform the RL classification task.
For somelanguages, this task is relatively well-studied.For example, the simple and widely-used Laes-barhedsindex (henceforth ?LIX?)
calculates RLsfor texts written in Western European languages1with the following LIX formula:RLD = wordssentences +100 ?
wordschar>6words1In practice, LIX may be substituted with other metrics,such as Flesch-Kincaid.where D is a document written in an unfamiliarlanguage, and RLD is the readability score of thedocument D.The above formula relies on specific parame-ters which have been tuned to a certain set of lan-guages.
These include the total number of wordsin D (words), the total number of sentences in D(sentences), and the total number of words in Dwith more than six characters (wordschar>6 ).Although this formula may be successful inRL classification for languages like English andFrench (Bjo?rnsson and Ha?rd af Segerstad(1979),Anderson (1981)), it remains essentially parochialin the context of other languages because the pa-rameters overfit the data from the Western Euor-pean languages for which it was designed.
Sincethe LIX formula depends on measuring the num-ber of characters in a word to find words greaterthan 6, it is ineffective in determining the readabil-ity of documents written in languages with differ-ent writing systems, such as Chinese.
This is dueto the fact that some languages, like Chinese, arewritten with characters based on semantic mean-ing rather than phonemes, as in English, and alarge number of Chinese words consist of just oneor two characters, regardless of semantic complex-ity (Li and Thompson, 1981).
In a similar vein,many languages of the world (even some that usephonemically-based writing systems) do not ad-here to the implicit assumption of the LIX formulathat semantically ?complex?
words are longer thansimpler words (Greenberg, 1954).
In these lan-guages, then, the same metric cannot be used as avalid measure of RL difficulty of documents, sinceword length does not correlate with semantic com-plexity.One recent alternative approach has been devel-oped for readability labeling that uses multiple sta-tistical language models (Collins-Thompson andCallan, 2004).
The idea is to train statistical lan-guage models for each grade level automaticallyfrom manually labeled training documents.
How-ever, even an approach like this is not scalable tohandle thousands of languages, since it is hard torecruit annotators of all of these languages to man-ually label the training data.2.2 Proposed SolutionWe propose a scalable solution to the problem oflabeling the readability of documents in many lan-guages.
The general idea is to combine CLIR12technology with off-the-shelf readability softwarefor at least one well-studied language, such asEnglish.
First, off-the-shelf readability softwareis used to assign RLs to a set of documents inthe source language, e.g.
English, which serve astraining data.
Second, a set of key terms is se-lected from each group of documents correspond-ing to a particular RL to construct a readabilitymodel for that RL.
Third, for each of these setsof terms, the cross-lingual query-expansion com-ponent of the CLIR system returns a semanticallyrelevant set of terms in the target language.
Fi-nally, these target-language term sets are used tobuild the target-language RL models, which canbe used to assign RLs to documents in the tar-get language, even if language-specific readabilityclassification software does not exist for that lan-guage.
This solution plausibly extends to any ofthe languages covered by the CLIR system.
It ispossible to create a CLIR system by crawling theinternet for parallel corpora, which exist for manylanguage pairs.
As a result, the proposed solutionalready has the potential to cover many differentlanguages.The success of this method relies on the as-sumption that readability levels remain fairly con-stant across syntactically and semantically paral-lel documents in the two languages in question,or simply across documents typified by equivalentkey terms.
This does not seem unreasonable: if thesame information is represented in two differentlanguages in semantically and structurally compa-rable ways, it is likely that the reading difficulty ofthe two texts should not differ much, if at all.
Ifthis assumption is true, generation of readabilitysoftware really depends only on the availability ofa solid CLIR system, and the problem of requir-ing trained computational linguists and native lan-guage speakers to design the system is mitigated.Figure 1 shows a simple process model of a sys-tem for generating RL classifiers for various lan-guages.
A set of training documents from a sourcelanguage (i.e., the ?L1?
in Figure 1) is assignedRLs by the off-the-shelf RL classification soft-ware R(L1).
Using the source langauge files andthe RLs produced by R(L1), the ECR system pro-duces a source language (L1) readability model.Through the system interface, the CLIR system(EXCLAIM) uses the L1 readability model to pro-duce a target language (L2) readability model.
Thesystem uses the L2 readability model to produce aFigure 1: ECR Domainnew RL classifier R(L2) for the target language.The newly developed classifier R(L2) can then beused to classify documents in the L2.3 System ArchitectureTo address any theoretical or empirical concernsand questions about the proposed solution, includ-ing those relating to the assumption that key termequivalence correlates with RL equivalence, wehave developed an ECR system compatible withan existing CLIR system and have proposed eval-uation metrics for this system.
We developedthe ECR system to meet the needs of two differ-ent kinds of users.
First, higher-level intermedi-ate users can build RL classification software fora given target language.
Second, end users canuse the software to classify documents in that lan-guage.
In this section, we give a developer?s-eyeview of the system architecture (shown in Figure2), making specific reference to the points at whichintermediate and end users may interact with thesystem.
For presentational clarity, we periodicallyadopt the arbitrary assumption that the source lan-guage is English, as this is the source language ofour experiment described in the following section.The ECR system has three primary tasks.
Thefirst task is to enable intermediate users to developRL classification model for the source language.The second task is to provide the intermediate userwith a toolkit to construct language-specific soft-ware that automatically tags documents in the tar-get language with the appropriate RLs.
The finaltask is to provide an interface module for the end13Figure 2: ECR System designuser to utilize this software.In order to approach the first task, one needs aset of documents in a source language for whichoff-the-shelf readability software is available.
Thisset of documents functions as a training data set;if a user is trying to assign RLs to documentsin a particular domain ?
e.g., forestry, medical,leisure, etc.
?
then (s)he can already help shapethe results of the system by providing domain-relevant source langauge data at this stage.
Toaid the intermediate user in obtaining RLs for thisset of data, the ECR system has a number of pa-rameters that may be selected, based on differentmodels of RL-tagging ?
for example, we selectedEnglish as the source language and the aforemen-tioned LIX formula due to its simplicity.
The doc-uments are then organized according to the gener-ated RLs and separated into different RL groups.At this point, the K most salient words areextracted from each source language RL groups(RLS) based on the following tf*idf term weight-ing:2wi ,j =(0.5 + 0.5 freqi,jmaxl freql,j)?
logNni2In principle, this choice is arbitrary and any other appro-priate term-weighting formula could also be used.The selected words RLS = {f1 , f2 , ...fK }form the basis for constructing an RL classifica-tion model for an unknown target language.In order to construct a target language RL clas-sification model, the cross-lingual query expan-sion component of a CLIR system is necessaryto select semantically comparable and semanti-cally related words in the target language.
TheCLIR system we developed is called EXCLAIM,or the EXtensible Cross-Linguistic AutomaticInformation Machine.
We constructed EXCLAIMfrom a semantically (though not structurally) par-allel corpus crawled from Wikipedia (Wikime-dia Foundation, 1999).
All Wikipedia articles withboth source and target language versions collec-tively function as data to construct the CLIR com-ponent.
Due to Wikipedia?s coverage of a largeamount of languages (English being the languagewith the largest collection of articles at the timeof writing), CLIR components for English pairedwith a wide number of target languages was cre-ated for EXCLAIM.For each RLS, the query-expansion componentof EXCLAIM determines a set of correspondingwords for the target language RLT.
Initially, eachword in RLS is matched with the source languagedocument in EXCLAIM for which it has the highesttf*idf term weight.
The M most salient terms inthe corresponding target language document (cal-culated once again using the tf*idf formula) arethen added to RLT.
Therefore, RLT contains nomore than K ?
M terms.
The total set of RLTsform the base of the target language readabilityclassification model.Using this model, the system generates targetlanguage readability classification software on thefly, which plugs into the system?s existing inter-face module for end users.
Through the module,the end user can use the newly generated softwareto determine RLs for a set of target language doc-uments without requiring any specialized knowl-edge of the languages or the software developmentprocess.4 Experimental DesignWe conducted an experiment to demonstrate thisidea and to test our ECR system.
Without lossof generality, we chose English as our source lan-guage and Chinese as our target language.
WhileChinese is a major language for which it wouldbe relatively easy to find linguistic experts to write14readability rules and native speakers to label doc-ument readability for training, our goal is not todemonstrate that the proposed solution is the bestsolution to build readability software for Chinese.Instead, we chose these languages for the follow-ing reasons.
First, we are capable of reading bothlanguages and are thus able to judge the quality ofthe ECR system.
Second, publicly available En-glish readability labeling software exists, and weare not aware of such software for Chinese.
Third,we had access to a parallel set of documents thatcould be used for the evaluation of our experiment.Fourth, the many differences between English andChinese might demonstrate the applicability of oursystem for a diverse set of languages.
However,the features that made Chinese a desirable targetlanguage for us are not essential for the proposedsolution, and do not affect the extensibility of theapproach.We created a test set using a collection ofChinese-English parallel documents from themedical domain (Chinese Community Health Re-source Center, 2004).
The set comprised 65 docu-ments in English and their human-translated Chi-nese translations.
Although a typical user doesnot need to have access to sets of bilingual doc-uments for the system to run successfully, we cir-cumvented both the lack of off-the-shelf Chinesereadability labeling software and the lack of la-beled Chinese documents for the evaluation of theresults of our system by using a high quality trans-lated parallel document set.
Since RLs are roughmeasures of semantic and structural complexity,we assume they should be approximately if notexactly the same for a given document and itstranslation in a different language, an extension ofthe ideas in Collins-Thompson and Callan (2004).Based on this assumption, we can accurately com-pare the RLs of the translated CCHRC Chinesemedical documents to the RLs of the original En-glish documents, which we call the ?true RLs?
ofthe testing documents.LIX-based RLs can be roughly mapped to gradelevels, e.g., a text that is classified with an RL of8 is appropriate for the average 8th grade reader.Since we can assign RLs to the English versionsof the 65 CCHRC documents, these RLs can serveas targets to match when generating RLs for thecorresponding Chinese versions of the same docu-ments.An advantage of our system arises from a com-plete vertical integration which allows a user withknowledge of the eventual goal to help shape thedevelopment of the target language RL classifica-tion model and software.
In our case, the targetlanguage (Chinese) test set was from the medicaldomain, so we selected the OHSU87 medical ab-stract corpus as an English data set.
We automati-cally classified the OHSU87 documents using theLIX mapping schema assigned by the UNIX Dic-tion and Style tools,3 given in the following Table.LIX Index RL LIX Index RLUnder 34.0 4 48.0-50.9 934.0-37.9 5 51.0-53.9 1038.0-40.9 6 54.0-56.9 1141.0-43.9 7 57.0 and over 1244.0-47.9 8Table 1: Mapping of LIX Index scores to RLs asassigned by DictionThen, we concatenated the English OHSU87 doc-uments in each RL group.
The tf*idf formula wasused to select the K English words most represen-tative of each RL group.Next, we automatically selected a set of Chi-nese words for each RL class to create a corre-sponding Chinese readability model by passingeach English word through the CLIR system, EX-CLAIM, to retrieve the most relevant English doc-ument in the Wikipedia corpus, where relevanceis measured using the tf*idf vector space model.The top M Chinese words from the correspondingChinese document in the parallel Wikipedia cor-pus were added to RLT.
By repeating this pro-cess for each word of each RL class, the Chinesereadability model was constructed.
In our exper-iment, we set K = 50 and M = 10 arbitrarily.The ECR system then automatically generated thesubsequent RL classification software for Chinese.Finally, we assigned a RL to each document inthe test set.
At this point the procedure is essen-tially similar to document retrieval task.
Each RLgroup?s set of words RLT was treated as a docu-ment (dj ), and each test document to be labeledwas treated as a query (q).
RLs were ranked basedon the cosine similarity between RLT and q. Fi-nally, the top-ranked RL was assigned to each testdocument.3Available online at http://www.gnu.org/software/diction/diction.html.155 Empirical ResultsThe results are presented below in Table 2.
TheRL assigned to each Chinese document is com-pared to the ?true RL?
of the English document, onthe assumption that translation does not affect thereadability level.
Although only 7.8% of the RLswere predicted accurately (i.e., the highest rankedRL for the Chinese document corresponded iden-tically to the RL of the translated English docu-ment), over 50% were either perfectly accurate oroff by only one RL.Correctly predicted RL 7.8%RL off by 1 grade level 43.1%RL off by 2 grade levels 18.4%RL off by 3 grade levels 18.4%RL off by 4 grade levels 6.1%RL off by 5 grade levels 3.1%RL off by 6 grade levels 0%RL off by 7 grade levels 3.1%RL off by 8 grade levels 0%Table 2: Distribution of RLs as predicted by ourECR systemThis table motivates us to represent the resultsin a more comprehensive fashion.
Intuitively, thesystem tends to succeed at assigning RLs near thecorrect level, though not necessarily at the exactlevel.
To quantify this intuition, we used RootMean Squared Error (RMSE) to evaluate the ex-perimental results.
We compared our results totwo kinds of baseline RL assignments.
The firstmethod was to randomly assign RLs 1000 timesand take the average of the RMSE obtained ineach assignment; this yielded an average RMSEof 3.05.
The second method used a fixed equaldistribution of the nine RLs, applying each RL toeach document an equal number of times, and tak-ing the average of these results.
This baseline re-turned an average RMSE of 3.65.
The averageRMSE of our ECR system?s performance on theCCHRC Chinese documents is 2.48.
This numbercompares favorably against both of the baseline al-gorithms.Recall that the actual RL-tagging procedure hasbeen treated as a document retrieval task, usingVector Space Cosine similarity.
As such, RLs arenot simply ?picked out?
for each document: eachdocument receives a cosine similarity score foreach RL, calculated on the basis of its similarity tothe language model word set constructed for eachRL.
For the results above, only the top ranked RLwas considered, as this would be the RL yielded ifthe user wanted a discrete numeric value to assignto the text.
If we allow for enough flexibility to se-lect the better of the two top-ranked RLs assignedto each document by our ECR system, the resultsare as given in Table 3.Correctly predicted RL 10.8%RL off by 1 grade level 49.2%RL off by 2 grade levels 27.7%RL off by 3 grade levels 7.7%RL off by 4 grade levels 1.5%RL off by 5 grade levels 0%RL off by 6 grade levels 3.1%RL off by 7 grade levels 0%RL off by 8 grade levels 0%Table 3: RL Distribution (Best of Two Top-Ranked RLs)While this extra selection is certain to improvethe RMSE, what is surprising is the extent towhich the RMSE improves.
Once again, RMSEcan be calculated in the following way.
The twotop-ranked RLs for each document are taken intoconsideration, and of these two RLs, the RL near-est to the true RL is selected.
Selecting the best ofthe two top-ranked RLs causes the RMSE to dropto 1.91.6 Related WorkThe method described above builds on recent workthat has exploited the web and parallel corpora todevelop language technologies for minority lan-guages (Trosterud (2002), inter alia).Yarowsky et al (2001) describe a system anda set of algorithms for automatically deriving au-tonomous monolingual POS-taggers, base noun-phrase bracketers, named-entity taggers, and mor-phological analyzers for an arbitrary target lan-guage.
Bilingual text corpora are treated withexisting text analysis tools for English, and theiroutput is projected onto the target language viastatistically derived word alignments.
Their ap-proach is especially interesting insofar as the sys-tem does not require hand-annotation of target-language training data or virtually any target-language-specific knowledge or resources.Martin et al (2003) present an English-Inuktitutaligned parallel corpus, demonstrating superior16sentence alignment via Pointwise Mutual Informa-tion (PMI).
Their approach provides broad cov-erage of cross-linguistic morphology, which hasimplications for dictionary expansion tasks; prob-lems encountered in dealing with the agglutina-tive morphology of Inuktitut are suggestive of themyriad issues arising from cross-language com-parisons.Rogati et al (2003) present an unsupervisedlearning approach to building an Arabic stemmer,modeled on statistical machine translation.
Theauthors use an English stemmer and a small par-allel corpus as training resources, with no paralleltext necessary after the training phase.
Additionalmonolingual texts can be incorporated to improvethe stemmer by allowing it to adapt to a specificdomain.While Yarowsky et al (2001), Martin etal.
(2003) and Rogati et al (2003) all focuson aligned parallel corpora, our approach dif-fers in that we use comparable documents fromWikipedia are linked thematically on the basisof semantic content alone: there is no presumedstructural or lexical alignment between paralleldocuments.
We have adapted the methods usedin conjunction with aligned parallel corpora foruse with non-aligned parallel corpora to handlethe task pursued by Collins-Thompson and Callan(2004), which presents a new approach to predict-ing the RLs of a document by evaluating readabil-ity in terms of statistical language modeling.
Theirapproach employs multiple language models to es-timate the most likely RL for each document.This approach contrasts with other previousmonolingual methods of calculating readability,such as Chall and Dale (1995), which assessesthe readability of texts by calculating the percent-age of terms that do not appear on a 3,000 wordlist that 80% of tested fourth-grade students wereable to read.
Similarly, Stenner et al (1988) usethe word frequency information from a 5-million-word corpus.While our work has drawn from several tech-niques employed in prior research, we have mainlyhybridized the technique of using parallel cor-pus employed by Yarowsky (2001) and the lan-guage modeling approach employed by Collins-Thompson and Callan (2004).
Our approach relieson parallel corpora to build a readability classi-fier for one language based on readability softwarefor another language.
Rather than focusing onlanguage-specific readability classification basedon training data drawn from the same languageas the testing data (Collins-Thompson and Callan,2004), we have constructed a radically extensibletool that can easily create readability classifiersfor an arbitrary target language using training datafrom a source language such as English.
The resultis a system capable of allowing a user to constructreadability software for languages like Indonesian,for example, even if that user does not speak In-donesian ?
this is possible due to the large paral-lel English-Indonesian corpus on Wikipedia.7 ConclusionWe have proposed a general framework to quicklyconstruct a standalone readability classifier foran arbitrary (and possibly unfamiliar) languageusing statistical language models based both onmonolingual and non-aligned parallel corpora.
Todemonstrate the proposed idea, we developed anExtensible Crosslingual Readability system.
Weevaluated the system on the task of predictingreadability level of a set of Chinese medical docu-ments.
The experimental results show that the pre-dicted RLs were correct or nearly correct for over50% of the documents.
This research is importantbecause it is the only technique we are aware ofthat is capable of straightforwardly creating read-ability labels for hundreds, or theoretically eventhousands, of different languages.Although the general framework and architec-ture of the proposed system are straightforward,the details of implementation of the system mod-ules could be further improved to achieve bet-ter performance.
For example, all target lan-guage words are selected from a single ?best-matching document?
using EXCLAIM in this pa-per.
Further experimentation might discover abetter word selection module.
Future work mayalso reveal delineation points for over- and under-specialized sets of training data.
The OHSU87data set was selected on the basis of its medicaldomain coverage, however it may not have pro-vided broad enough coverage of the appropriatedomain-independent vocabulary in the CCHRCdocuments.
And finally, we conducted the ex-periment using our own CLIR system, EXCLAIM,while other CLIR systems might yield better re-sults.17AcknowledgementsThe research reported here was partly supportedby NSF Grant #BCS-0846979 and the Institute ofEducation Sciences, US Department of Education,through Grant R305A00596 to the University ofCalifornia, Santa Cruz.
Any opinions, findings,conclusions or recommendations expressed in thispaper are the authors?, and do not necessarily re-flect those of the sponsors.ReferencesJonathan Anderson.
1981.
Analysing the readabilityof English and non-English texts in the classroomwith Lix.
Paper presented at the Annual Meeting ofthe Australian Reading Association.C.
H. Bjo?rnsson and Birgit Ha?rd af Segerstad.
1979.Lix pa?
Franska och tio andra spra?k.
Pedagogisktcentrum, Stockholms skolfo?rvaltning.Jeanne S. Chall and Edgar Dale.
1995.
Readability Re-visited: The New Dale-Chall Readability Formula.Brookline, Cambridge, Mass.Chinese Community Health Resource Center.
2004.CCHRC Medical Documents.
Retrieved Decem-ber 9, 2006, fromhttp://www.cchphmo.com/cchrchealth/index E.html.Kevyn Collins-Thompson and Jamie Callan.
2004.A language modeling approach to predicting read-ing difficulty.
In Proceedings of HLT/NAACL 2004.ACL.Joseph H. Greenberg.
1954.
A quantitative approachto the morphological typology of language.
InMethod and Perspective in Anthropology: Papersin Honor of Wilson D. Wallis, pages 192?220, Min-neapolis.
University of Minnesota Press.Barbara Grimes.
2005.
Ethnologue: Languages of theWorld, 15th ed.
Summer Institute of Linguistics.Michael Heilman, Kevyn Collins-Thompson, JamieCallan, and Maxine Eskenazi.
2006.
Classroomsuccess of an intelligent tutoring system for lexicalpractice and reading comprehension.
In Proceed-ings of the Ninth International Conference on Spo-ken Language Processing.Charles N. Li and Sandra Thompson.
1981.
MandarinChinese: A Functional Reference Grammar.
Uni-versity of California Press.Joel Martin, Howard Johnson, Benoit Farley, and AnnaMaclachlan.
2003.
Aligning and using an English-Inuktitut parallel corpus.
In Proceedings of the HLT-NAACL 2003 workshop on building and using par-allel texts: Data driven machine translation and be-yond.
ACL.John Paolillo, Daniel Pimienta, and Daniel Prado.2005.
Measuring Linguistic Diversity on the Inter-net.
UNESCO, France.Monica Rogati, Scott McCarley, and Yiming Yang.2003.
Unsupervised learning of arabic stemming us-ing a parallel corpus.
In Proceedings of the 41st an-nual meeting of the Association for ComputationalLinguistics.
ACL.A.J.
Stenner, I. Horabin, D.R.
Smith, and M. Smith.1988.
The Lexile Framework.
Metametrics,Durham, NC.Trond Trosterud.
2002.
Parallel corpora as tools forinvestigating and developing minority languages.
InParallel corpora, parallel worlds, pages 111?122.Rodopi.Wikimedia Foundation.
1999.
Wikipedia, thefree encyclopedia.
Retrieved May 8, 2006, fromhttp://en.wikipedia.org/.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection across aligned corpora.In Proceedings of the First International Conferenceon Human Language Technology Research, pages161?168.18
