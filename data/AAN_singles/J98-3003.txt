A Generative Perspective on VerbAlternationsManfred Stede*Technische Universit/it BerlinVerb alternations have been researched xtensively in linguistics, but they have not yet receiveda systematic treatment in natural anguage generation systems; consequently, generators cannotmake informed choices among alternatives.
As a step towards overcoming this discrepancy, wereview some linguistic work on several prominent alternations, revise and extend it, and suggesta set of rules that allow the series of alternated forms to be produced from a single base form of theverb, the lexical entry.
The framework has been implemented in the Moose sentence generator,which can thus choose aparticular verb alternation in order to accomplish generation goals suchas placing emphasis on the most important element of the sentence.1.
IntroductionIn this paper, we approach the problem of verb alternations from the perspective ofknowledge-based natural language generation (NLG), which aims at producing aset ofverbalizations from a common underlying representation.
This viewpoint places omespecific requirements on the nature of lexical representations, which will be explainedin Section 2.
Thereafter, we will investigate the problem of systematically generatinga number of verb alternations--a problem that so far has received little attention inthe NLG community.Why should a language generator have knowledge about producing alternations?Because a sophisticated discourse production may call for using a verb in one or an-other configuration, depending on the current situation of utterance.
Alternations canplace the emphasis on different elements of the sentence, and distribution of empha-sis is influenced by the development of the discourse--and thus related to a wholerange of other generation decisions.
To illustrate, consider the following alternativebeginnings of a little story:(1) (a) Tom was in a hurry, but he had to change the oil before hitting theroad.
He crawled under the car and unscrewed the drain bolt.
Theengine drained in 20 seconds .
.
.
.
(b) Time was short, but the oil had to be changed before Tom could hitthe road.
Within 20 seconds, he drained the engine.
Then ...(c) Tom was in a hurry, but he had to change the oil before hitting theroad.
Quickly, he crawled under the car and unscrewed the drain bolt.For 20 long seconds, the oil drained from the engine .
.
.
.
* TU Berlin, FB Informafik, Sekr.
FR 6--10, Franklinstr.
28/29, 10587 Berlin, Germany(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 3(d) Tom was in a hurry, but he had to change the oil before hitting theroad.
Crawling under the car, he drained the old oil from the engine, andthen.
.
.Depending on how the story develops, which might hinge on stylistic parameters, adifferent configuration of to drain, with different subjects, objects, and prepositionalphrases, should be produced.
Therefore, if an NLG system is expected to be able tocope with such differences, it needs knowledge of what alternations are possible fora given verb, and how the different syntactic onfigurations relate to differences inmeaning.In linguistics, the central goal of research on alternations i to uncover the relation-ships between syntax and semantics (linking rules), and to form classifications of verbsaccording to their alternation behavior (Levin 1993).
To accomplish these goals, theneed for fine-grained lexical-semantic representations is pointed out, although there isno strong consensus yet on exactly what such representations should look like (see thediscussion in Levin and Rappaport Hovav \[1995, chapter 1\]).
NLG, in any case, needsrepresentations to work with; and in order to account for verb alternations, we needto devise rather fine-grained ones.
In particular, a generator has to relate the (possible)changes in meaning to the changes in form, so that--from a given representation--thecorrect set of "alternated verb forms" can be produced.
In other words, the generatorneeds to know the conditions under which some input representation licenses the useof a specific alternation.As a step in this direction, we consider a number of alternations that affect theaspectual category (or Aktionsart) of the verb--a group that Levin (1993, 12) chosenot to focus on.
We look at the differences in meaning that coincide with such alter-nations, and propose suitable representations for input specifications (the underlyingontology), for verb meaning, and for the alternation rules.
These rules are able tosequentially derive the various alternated forms from a single base form, which isstated in the lexical entry.
The approach as been implemented in a bilingual genera-tion system, which can produce the "alternated paraphrases" in English and German.To demonstrate its capabilities, we will show how a salience parameter associatedwith the input can give rise to selecting one or another of the alternatives; pecifically,the production of the alternative drain sentences (1a-d), from a common underlyingrepresentation, will be demonstrated.The paper is organized as follows: Section 2 discusses the specific requirements oflexical information in NLG, focusing on verbs, and suggests a format for dictionaryentries.
Section 3 develops our approach to verb alternations and proposes rules thataccount for several such alternations.
Section 4 describes the implementation of thealternation framework in the Moose generator, and Section 5 concludes and comparesour approach to related research.
While Sections 2 and 3 are concerned with linguisticrepresentations and make only little reference to NLG, Section 4 presupposes someknowledge of the generation concepts and systems that Moose is built upon.2.
Representing Verb Meaning for Generation2.1 Lexical InformationThe central characteristic ofknowledge-based NLG is the existence of a domain model,which anchors the representations serving as input to the generator.
The domain modelprovides the basis for drawing inferences on the representations.
One class of suchinferences is the subsumption check, which our approach exploits for the task oflanguage generation (see Section 4).
The NLG system is thus in charge of mapping a402Stede Verb Alternationsnetwork of instantiated omain concepts to linguistic utterances, and the specific roleof lexemes within this process is to "carry over" the meaning from the underlyingdomain model to utterances in natural anguage.
To accomplish this step, the lexicon ofthe generator requires two basic components: it has to explain what words mean withrespect o the domain model and it has to explain how words can be combined intowell-formed sentences.
At this point, the study of lexical semantics becomes relevant--it should systematically relate these two tasks.In NLG, however, lexical semantics has for a long time been relatively neglected;words and concepts were often conveniently put into a simple one-to-one correspon-dence.
The key to incorporating lexical semantics into NLG is breaking up this tightcorrespondence and arranging for more flexible mappings.
As soon as entities in theknowledge representation scheme do not correspond to words in a direct manner, therelationship between word meaning and entities in the knowledge base (KB) needsto be specified in some more elaborate way.
Now, lexical semantics has to supply theinterface between knowledge and words: It has to specify what words can be used toexpress what parts of what KB entities, and, possible under what circumstances.
Tothis end, the relevant work in linguistics needs to be identified, extended, and adaptedfor generation purposes.
This adaptation is not straightforward, however, because thestarting position of linguistics is different from that of NLG: For a linguist, the syntax-semantics interface is of central concern, whereas in NLG, there is the additional levelof instantiated knowledge, which needs to be systematically related to linguistic levels.Our approach is to employ a level of sentence-semantic representation that mediatesbetween the knowledge-level input and syntactic realization.
A well-motivated se-mantic level allows us to encapsulate all syntactic decisions in a front-end generationmodule; we use the Penman system (Penman Group 1989) for this purpose.
For theother mapping, from KB to sentence-semantics, we use the lexicon as the primarysource of information.In this approach, lexical entries therefore have two main components: a denota-tion that defines the applicability conditions of the lexeme with respect o the domainmodel (i.e., it can be matched against he generator's input), and a partial semanticspecification (PSemSpec), which specifies the contribution that the lexeme makes tosentence meaning.
1 The task of lexicalization in our generator thus consists of first find-ing lexemes that can convey parts of the input, and then determining the preferredcombination of candidate lexemes, yielding a sentence-semantic specification (Sem-Spec), which is then processed by the surface generator.
The generation architectureis presented in Section 4.The next two sections explain the denotation and the partial semantic specificationassociated with verb lexicon entries, and thereby also the two levels of representationused in the generation system.2.2 Event Ontology and AktionsartThe development of the domain model and the underlying ontology for our systemfocused on the treatment of events so that they can be appropriately verbalized indifferent languages.
The hierarchy of situations, shown in Figure 1, is organized alonga variant of the ontological categories proposed by Vendler (1967) and developedfurther by Bach (1986), inter alia.
We briefly discuss the three types of situation inturn.1 The lexical entries in our system have several other components, which are listed in Section 2.4.403Computational Linguistics Volume 24, Number 3SITUATIONSTATE ACTIVITY / \PROTRACTED- MOMENT.-ACTIVITY ACTIVITYFigure 1EVENTCULMINATION TRANSITIONPROTRACTED- MOMENT.-CULMINATION CULMINATIONSituation types in the ontology of MOOSE.States are seen much in the same way as Bach sees them: Something is attributed toan object for some period of time, and the object is not perceived as "doing" anything.The bottle is empty is true for the bottle without it doing anything about it.
We do notmake further distinctions among states here.Activities were called "processes" by Bach, but we need this term on a differentlevel of description (see below).
They are quite similar to states, but there is alwayssomething "going on," as in The water wasflowing toward the sea.
We distinguish two sub-types here: protracted activities take place over an extended period of time, whereasmomentaneous  activities occur in an instant; a "point adverbial" such as at noon servesas a linguistic test.Events are occurrences that have a structure to them; in particular, their result, ortheir coming to an end is included in them: to destroy abuilding, to write a book.
As theircentral feature we take them to always involve some change of state: the building losesits integrity, the book comes into existence, or gets finished.
While Bach (1986) did notinvestigate the internal structure of events, others suggested that this needs to be done(e.g., Moens and Steedman 1988; Parsons 1990).
Pustejovsky (1991) treated Vendlerianaccomplishments and achievements a transitions from a state Q(y) to NOT-Q(y), andsuggested that accomplishments in addition have an intrinsic agent performing anactivity that brings about the change of state.We follow this line, but modify it in some ways.
Basically, we see any event asinvolving a change of state; an activity responsible for the change can optionally bepresent.
A plain transition is necessarily momentaneous (The room lit up), whereasa transition-with-activity inherits its protracted/momentaneous feature from the em-bedded activity.
We call these tripartite vents culminations.
2 They are composed of apre-state (holding before the event commences), a post-state (holding when the eventis over), and an optional activity that brings the transition about.
Generalizing fromPustejovsky's proposal, we take state transitions to be more than merely oppositionsof Q(y) and NOT-Q(y); they can also amount o a gradual change on some scale, orinvolve other values.
Also in contrast o Pustejovsky, we do not regard the presenceof a volitional agent as responsible for any of the category distinctions; rather, theagentivity feature cuts across the categories discussed.
Other aspects of our ontologyare designed following proposals by Jackendoff (1990), in particular his analysis ofmovement events.2 Moens and Steedman (1988) also use this term, but they restrict i to momentaneous events.Unfortunately, the terminology used in the literature for these kinds of categories varies o much that astandardization seems out of reach.404Stede Verb Alternationsevent- 1fillcon~f - - - -~  > 'not-full-state-1 ~ ' ' ~~ f ~  > pa~>b~tination ~fill-state-2 .
.
.
.
.
.
;~.~~ water-1\] value\[ > 'fullFigure 2SitSpec representing a fill-event.Subsumed by the general ontological system, adomain model is defined that holdsthe concepts relevant for representing situations and that specifies the exact conditionsfor their well-formedness.
We use the term SitSpec for a network of instances ofdomain model concepts, which will be the input to our generator.
The root node ofany SitSpec is of the type situation.
As an example, the event of a person namedJill filling a tank with water is shown in Figure 2 in  a graphical KL-ONE notation(Brachman and Schmolze 1985), with relation names appearing in boxes.
The eventcombines the activity of Jill pouring water into the tank with the fill-state of the tankchanging to full.
A verbalization of this event can emphasize ither of these aspects.Since we decompose event structure in such a way, it follows that the denotationsof verbs for verbalizing events need to be fairly complex.
The type of event denotedrelates to the Aktionsart of the verb: the inherent features characterizing (primarily) thetemporal distribution of the event denoted.
3 A generator needs to know these featureswhen verbalizing different kinds of events, so that it can produce (for example) thecorrect emporal modifier to express the duration of either an activity or a culmination.The variety of phenomena in Aktionsart are far from clear-cut, and there is no generallyaccepted and well-defined set of features.
In the following, we use the terms given byBussmann (1983) and discuss only those Aktionsart features that are directly relevantfor us because they relate types of situations to denotations of verbs.
Thus, withinthe context of our system, we define Aktionsart features in terms of patterns of verbdenotations.
Table 1 lists the correspondences.Simple cases are stative verbs like to own or to know.
Durative verbs characterizecontinuous occurrences that do not have internal structure, like to sleep, to sit.
In the classof nondurative verbs we find the semelfactive ones, which denote asingle occurrence,thus in our system a momentaneous activity, as, for example, to knock.
Interestingly,an iterative reading can be enforced on a semelfactive verb by a durative adverbial:She poked me for an hour.
Transformative rbs involve a change of some state, withouta clearly recognizable vent that would be responsible for it: The room lit up.
Thedenotation of such verbs thus involves a pre-state and a post-state.
In our ontology,these are transitions.
Resultative verbs, on the other hand, characterize situations in3 This is often treated on a par with aspect, but we prefer to make a terminological distinction betweenthe grammaticalized categories such as progressive versus nonprogressive in English (aspeCt),.
and thestatic verb-inherent features.405Computational Linguistics Volume 24, Number 3Table 1Correspondences between Aktionsart anddenotations.Aktionsart Denotation patternstative (state X)durative (protracted-activity X)semelfactive (moment aneous-act ?vit y X)transformative (event (PRE-STATE X)(POST-STATE not-X) )resultative (event (ACTIVITY X)(POST-STATE Y)causative (activity (CAUSER X))which something is going on and then comes to an end, thereby resulting in somenew state (culminations in our ontology).
Their denotation includes an activity anda post-state.
In the literature, such verbs are often also called inchoative.
4 The finalverb-inherent feature we use is the well-known causative, which reflects the presenceof a causer in the denotation (as in Figure 2).Verb alternations, as we will discuss them shortly, can involve a shift in Aktionsartand thus a systematic hange of the denotation.
But first we have to introduce thesecond major component of verb semantics--sentence m aning.2.3 Sentence MeaningA SitSpec representing a possibly complex event structure can be verbalized by a va-riety of sentences, which can differ in terms of their argument structure, aspectualcomposition, etc.
From the viewpoint of NLG, we wish to select he most appropriatesentence on the grounds of target parameters, uch as the salience assignment men-tioned in the beginning of the paper.
In order to produce a sentence that accomplishessemantic goals of this kind, it is impractical to map the very abstract SitSpec directlyto a syntactic structure.
Instead, we use a sentence-semantic level of description thatallows us, on the one hand, to control those generation decisions that affect the mean-ing of the sentence, and, on the other hand, to encapsulate the syntactic realizationdecisions in the front-end generation grammar.2.3.1 Halliday's Ideational Structure.
To describe sentence meaning, we use the "ide-ational structure" introduced by Hall iday (1985).
It resembles other approaches basedon semantic ase roles, but an important feature of Hall iday's work is his thoroughclassification of process types and of the semantic relationships holding between theverb and the other elements in a clause.
5This extensive analysis renders the approachparticularly useful for sentence generation.
Hall iday's process classification has beenfurther developed for NLG purposes by C. Matthiessen, J. Bateman and others (see, forinstance, Matthiessen and Bateman \[1991\]).
The resulting "upper model" (UM) is partof the Penman generator and used in our system as well.
The UM is a taxonomy of4 The term inchoative isused to cover a rather broad range of phenomena, including the beginning of anevent (e.g., to inflame) or its coming to an end.
We think the term is overloaded and prefer to useresultative for the latter group.5 Halliday proposes two additional levels of sentence description ("metafunctions'), which operate inparallel to ideational structure: the interpersonal and the textual.
For our present purposes, we canneglect them; for a broader scope of sentence generation, they are very important.406Stede Verb Alternations~11 poured water into the tank until it was filled.
(xl / anterior-extremal:domain (x2 / directed-action :lex pour:actor (x3 / person :name jill):actee (x4 / substance :lex water):destination (x5 / three-d-location :lex tank)):range (x6 / nondirected-action :lex fill:actee x5))Figure 3SemSpec and a corresponding sentence.linguistic categories that directs the grammar in verbalizing objects (in the generator'sinput) in terms of these categories.
Hence, the UM can be characterized as mirroringthe distinctions made in surface linguistic realizations: Typically, any two distinct UMtypes correspond to some difference in English sentences.The largest part of the UM is devoted to processes, which are characterized bytheir verbalization patterns.
For our purposes here, we need only a small fragment ofthe process hierarchy, namely the subtree of material processes.
They can be charac-terized by the fact that English verbalizations of them in present ense typically usethe progressive form, as in the house is collapsing (unmarked) as opposed to the housecollapses (marked).
They typically involve the participant roles "actor" and "actee "6but differ in terms of constraints on the types of the role fillers, and with respect otheir realization in language.
Material processes have two subgroups, one of which arenondirected-actions.
They do not involve external agency and are mostly intransitive.With such processes, the actee is not a genuine participant, but rather an elaborationof the process.
Verbs falling into this category are those of movement, of expressingskills, as well as support verbs like to take as in take a shower.
The other subgroup,directed-actions, are always transitive, and they involve an external agent of the pro-cess.The upper model thus reflects the semantic distinctions made by the language,and the systemic-functional grammar takes care of the syntactic realization of thesedistinctions.
Accordingly the lexicalization component we are proposing here is incharge of producing a sentence-semantic specification along the lines of ideationalstructure (using the upper model categories), such that the relevant decisions affectingsentence meaning can be controlled during lexical choice.
As an example, Figure 3shows one of the SemSpecs and an English sentence that can be derived (as explainedin Section 4) from the SitSpec given in Figure 2.
Besides actor and actee, the role"destination" is used in the SemSpec; later we will also encounter "source.
"The UM is a good starting point, but in some respects the process classification isnot quite fine-grained enough.
A deficiency that is directly relevant for our treatment ofalternations concerns the valency patterns of verbs, where some additional distinctionsare needed.2.3.2 Valency.
As introduced by Tesni~re (1959), valency refers to the distinction be-tween actants and circumstantials (central participants associated with the verb versustemporal, locational, and other circumstances).
This separation is in principle widelyaccepted, but views differ on where to draw the line and how to motivate it.
Thenotion of valency was further developed predominantly in German linguistics, witha culmination point being the valency dictionary of German verbs by Helbig and6 Actee is the upper model role that conflates what more often is called patient, theme, and goal.407Computational Linguistics Volume 24, Number 3Schenkel (1973).
They made an additional distinction between obligatory and optionalactants; Somers (1987, chapter 1) proceeded to propose six different levels of valencybinding.
He also pointed out that there are different opinions on the type of entitiesthat are subject o a verb's valency requirements: ome authors describe them by syn-tactic class, some by semantic deep cases, and some by their function (subject, object,etc.
).Halliday (1985), in his classification, essentially adopts the basic Tesni~rian distinc-tion and suggests ome semantic and syntactic riteria for deciding between actants,which he calls participants, and circumstances.
Spatio-temporal information, for in-stance, is generally treated as a circumstance.
As a syntactic indicator, for Halliday,participants are typically realized as nominal groups (with some obvious exceptions,as in say that x), and circumstances as prepositional phrases or as adverbs.
But nei-ther this syntactic division corresponding to participants and circumstances (direct orindirect object versus adverbs or prepositional phrases), nor the semantic postulatethat spatio-temporal spects are circumstances, holds in general.
Regarding spatial re-lationships, we find verbs that specifically require path expressions, which cannot betreated on a par with circumstances: Consider, for example, to put, which requires adirect object and a destination.
Causative to pour requires a direct object as well asa path with either a source, or a destination, or both: pour the water from the can intothe bucket.
7 Some verbs, as is well-known, can occur with either a path (Tom walkedinto the garden) or a place (Tom walked in the garden), and only in the garden can here betreated as a circumstance.
And to disconnect requires a direct object (the entity that isdisconnected) and a source (the entity that something is disconnected from), whichcan be omitted if it is obvious from the context: Disconnect the wire!As a step toward a more fine-grained istinction between participants and cir-cumstances, we adopt the three categories proposed by Helbig and Schenkel (1973)and thus distinguish between obligatory and optional participants on the one hand,and circumstances on the other.
Moreover, we differentiate between requirements ofprocess types (as encoded in the process taxonomy) and requirements of individualverbs, which are to be encoded in the lexical entries.
In a nutshell valency (as a lexicalproperty) supplements he participant/circumstance requirements hat can be statedfor types of processes.To encode the valency information, we introduce the partial semantic specification(PSemSpec) as one central component of lexical entries.
The participant roles statedin the PSemSpec are either obligatory or optional; in the latter case they are markedwith angle brackets:to d isconnectPSS: (x / d i rected-act ion:actor A :actee B < :source C >)With obligatory participants, the verb is only applicable if the elements denoted bythese participants are present in the input structure to be verbalized (the SitSpec).Optional participants need not be included in the verbalization: If they are presentin the SitSpec, they may be omitted if there is some good reason (e.g., a stylisticpreference); if they are not present in the SitSpec, the verb can be used anyway.
Thedisconnect example illustrates that, in contrast o Halliday, we allow for verbs selectingpath expressions, here as an optional complement.
We can thus distinguish between7 Given a suitable context, though, the sentence She poured the wine is perfectly acceptable.
But this usageseems to be restricted toa small class of digestible iquids.408Stede Verb Alternationscases like the following:?
Tom disconnected the wire {from the plug}.
To disconnect requires a source,but it can be omitted in a suitable specific context.?
Sally ate.
While to eat usually requires a direct object, it can also be usedintransitively due to the strong semantic expectation it creates on thenature of the object--independent of he context.?
Tom put the book on the table.
To put requires a destination, and it cannot beomitted, no matter how specific the context.?
The water drained from the tank {in the garage}.
Locative circumstances likein the garage are not restricted to particular verbs and can occur inaddition to paths required by the verb.The criterion of optionality, as indicated above, singles out the obligatory comple-ments from the other two categories.
But how, exactly, can we motivate the distinctionbetween optional participants and circumstances in our framework?
By relating thePSemSpec to the SitSpec, via the denotation.
In the disconnect ase, for instance, the twoitems connector and connectee are both integral elements of the situation.
The situationwould not be well-formed with either of them absent, and the domain model encodesthis restriction.
Therefore, both elements also occur in the denotation of to disconnect,and a coindexed variable provides the link to the PSemSpec.
Only when building thesentence SemSpec is it relevant to know that the connectee can be omitted.
The con-nectee in the denotation therefore must have its counterpart in the PSemSpec--that isthe source, but there it is marked as optional (see Figure 4 below).With circumstances, the situation is different: A SitSpec is complete and well-formed without the information on, for instance, the location of an event.
Hence, averb's denotation cannot contain that information, and it follows that it is not presentin the PSemSpec, either.2.4 Lexical EntriesWe have introduced the two central components of lexical entries and now give acomplete list of the components used in our system.
The connotations are not directlyrelevant for our mechanism of handling verb alternations, therefore they will not bedealt with here.
Salience assignment will be discussed in Section 4.Denotation: A partial SitSpec that defines the applicability condition ofthe lexeme: If its denotation subsumes some part of the input SitSpec,then (and only then) it is a candidate l xical option for the verbalization.Covering: The subset of the denotation nodes that are actually expressedby the lexeme.
One of the constraints for sentence production is thatevery node be covered by some lexeme.Partial SemSpec (PSemSpec): The contribution that the lexeme canmake to a sentence SemSpec.
By means of shared variables, the partialSemSpec is linked to the denotation.Connotations: Stylistic features pertaining to formality, floridity, etc.Salience assignment (for verbs only): A specification of the differentdegrees of prominence that the verb assigns to the participants.409Computational Linguistics Volume 24, Number 3DISCONNECTCAUSER ~- :actorCONNECTOR ~- :acteeCONNECTEE ~ <:source>POURPATH-SOURCEOBJECT*PATH-DESTINATION*CAUSER--~ :actor> <:actee>substance-sourcedurative-causativeDRAINOBJECTPATH-SOURCE*PATH-DESTINATION*CAUSER> :actor:~ <:SOUrCe>durative-causativelocative/clear-intransitiveresultative-causativeMOVE/WALKOBJECT ~ :actor*PATH*CAUSERdurafive-causativeOBJECT --*CAUSEROPEN> :actorresultative-causativeSPRAYCAUSER ~ :actorOBJECT > :acteePATH-DESTINATION --->- :destinationspray-loadF ILLCONTENT > :actorCONTAINER > :acteeVALUE > <:destination(default)>*CAUSERstative-resultativeresultative-causativeLEAKPATH-SOURCE ~ :actorOBJECT <:actee>*PATI-I-DESTINATIONsubstance-sourceFigure 4Excerpts from sample lexical entries for verbs.Alternation rules: (for verbs only): Pointers to lexical rules that representalternations the verb can undergo (see Section 3).Morphosyntactic features: Standard features needed by the surfacegenerator to produce correct utterances.Figure 4 gives excerpts from sample lexical entries, which demonstrate the linkingbetween entities from the denotation and the PSemSpec.
Notice that the linking isshown for the base form of the verb, which can be quite simple, as in open or move.Items appearing with an asterisk in front of them are optional in the SitSpec: for example,a SitSpec underlying an open-event is well-formed without a causer being present.These items get verbalized with the help of rules such as the alternation rules to whichwe turn in the next section.
In the lexical entries, the names of applicable alternationrules are listed below the line.3.
AlternationsHaving explained denotations and PSemSpecs, specifically for verbs, we can now turnto the task of accounting for the different alternations a verb can undergo.
Under thisheading, we will look both at the so-called transitivity alternations, which are charac-terized by a change in the number of participants (e.g., the causative), and at diatheses,410Stede Verb Alternationswhich only affect he mapping between the participants and syntactic realization (e.g.,the passive).
Thus, a variant such as topicalization does not qualify as an alternation,since the syntactic realization of the participants remains unchanged; they are merelyreordered.
The most comprehensive source of information on alternations is the com-pilation by Levin (1993); we will now look at some of the more prominent alternationslisted there and characterize them in terms of changes in denotation and valency ofthe verbs.3.1 Alternations as Meaning ExtensionsA simple way of treating alternations i  to use a separate l xical entry for every con-figuration, but that would clearly miss the linguistic generalizations.
Instead, we wishto represent the common "kernel" of the different configurations only once, and usea set of lexical rules to derive the alternation possibilities.
Jackendoff (1990) is con-cerned with this problem for a number of alternations; pecifically, in his frameworkof lexical-conceptual structure (LCS) he seeks to explain the relationships between sta-five, inchoative, and causative readings of a verb.
In Jackendoff's analysis, the formsare derived sequentially by embedding in the primitives INCH and CAUSE, respec-tively:?
stative: BE(\[Thi,x \](A), \[INa \[Thing \]A \])?
inchoative: INCH \[BE(\[Thing \](A), \[INd \[Thing \]A \])\]?
causative: CAUSE(\[Thi,g \]A, INCH \[BE(\[Thing \](A), \[INd \[Thing \]A \])\]For our NLG purposes, the idea of deriving complex verb configurations frommore basic ones is attractive, but it is necessary that we relate verb meaning to ourexplicit reatment ofevent structure, instead of masking that structure with a primitivesuch as INCH.
When verbalizing a SitSpec, we first have to determine candidatelexemes, i.e., match the SitSpec against lexicon entries; having only one lexical entryfor a verb reduces the search space considerably.
Moreover, since the verb entry willbe the most basic form, its denotation is relatively simple and therefore the matching isinexpensive.
Finding more complex verb configurations will then require some furthermatching, but only locally and to those verbs that have already been determined asverbalization options.In general the idea is to see verb alternations ot just as relations between differentverb forms, but to add directionality o the concept of alternation and treat them asfunctions that map one into another.
From this viewpoint, there are two groups ofalternations: (1) Alternations that do not affect he denotation of the verb.
Examplesare the passive or the substance-source alternation (The tank leaked oil; Oil leaked fromthe tank): The truth conditions do not change.
(2) Alternations that do change thedenotation of the verb.The second group is the critical one, because if we derive verb configurations fromothers and rewrite the denotation i this process, it has to be ensured that the processis monotonic.
8 Therefore we define directionality for group (2) to the effect that analternation always adds meaning: The newly derived form communicates more thanthe old form the denotation gets extended.
This notion is different from the standard,nondirectional way in which alternations are seen in linguistics; to label the difference,we call alternations ofgroup (2) extensions.
In this section, we will introduce a number8 Monotonicity might be too strong arequirement foralternations i  general, though.
See Section 5.3.411Computational Linguistics Volume 24, Number 3of extension rules for which we can give a clear definition in terms of Aktionsartfeatures, as they were introduced in Section 2.2.
These rules extend the denotationof a verb and rewrite its PSemSpec in parallel to reflect he change in valency; theresult is a new verbalization option, which can differ from the previous one in termsof coverage or attribution of salience (see Section 4).
The rules will be convenientlysimple to state, thanks to the upper model, which provides the right level of abstractionfrom syntax.To illustrate the goal we return to the example of Tom removing the oil from anengine.
If a SitSpec encodes this situation, then to drain is a candidate l xeme.
Whileit can appear in a number of different configurations, we wish to match only one ofits forms against he SitSpec, though.
This is the most basic one, denoting an activity:The oil drained from the engine.
Here, the case frame of the verb has to encode thatfrom the engine is an optional constituent.
Now, an extension rule has to systematicallyderive the causative form: Tom drained the oil from the engine.
And also from the firstconfiguration, another rule derives the resultative r ading, which adds the informationthat the engine nded up empty: The engine drained of the oil.
Here, of the oil is an optionalconstituent.
To this last form, a causative xtension can apply and yield Tom drainedthe engine of the oil.To compute these configurations automatically, such that valency and meaning arechanged in parallel, we define an alternation or extension rule as a 5-tuple with thefollowing components:NAM: a unique name;DXT: extension of denotation;C0V: additions to the covering-list;ROC: role changes in PSemSpec;NR0: additional PSemSpec roles and fillers.The DXT contains the denotation subgraph that the new verbalization has in additionto the old one.
The syntax is, of course, the same as that of the denotation of a lexicalentry.
Specifically, it can contain variables; these can co-occur in the C0V list: the itemsthat the new verbalization covers appear in addition to those of the old one.
R0C is alist of pairs that exchange participant role names or the UM type in the PSemSpec; thisreplacement can also change optionality.
For example, (< :actee > :actor) means"replace the term :actee in the PSemSpec of the old verbalization, where it wasoptional, with :actor, which is not optional."
Finally, NR0 contains new roles andfillers that are to be added to the new PSemSpec; these will also contain variablesfrom the denotation extension.Applying such a rule to a verbalization option vo works as follows: Add thecontents of DXT to the denotation of vo, and match the new part against he SitSpec.
Ifit matches, make a copy vo t of vo and assign it a new name as well as the denotationjust formed.
Add the C0V list, which has been instantiated by the matching, to thecovering-list of vo'.
Exchange the role names in the PSemSpec of vo ~ as prescribed byROC, and, importantly, in the order they appear there.
Finally, add NR0 to the PSemSpec.In the following, we first give an example for a rule that changes only the PSem-Spec without affecting the denotation.
Afterwards, we describe those alternations thatchange the Aktionsart of the verb and thus the form of the SitSpec expressed (asdiscussed in Section 2.2): the stative-resultative, causative, and locative alternations.412Stede Verb AlternationsBefore introducing these rules, it should be emphasized that we do not provide appli-cability conditions for the alternation and extension rules, which would inspect someverb denotation and on that basis decide whether an alternation can apply; instead,the rules are triggered irectly from the lexical entry of a verb.
Whether general ap-plicability conditions can be specified, so that the rules need not be attached to eachindividual verb, is a central open research question that linguistic alternation researchis concerned with.3.2 Encoding Alternation RulesThe best-known alternation that affects only the valency of the verb is the passive,which we do not investigate here.
Instead, we show one alternation that is particularlyrelevant for verbs in the domain of substances and containers.Substance-Source Alternation.
Example: The tank leaked water / Water leaked from the tank.This is an alternation discussed by Levin (1993); to make use of it here, we have toadd directionality and declare one of the two configurations as more basic.
Levin listsverbs of "substance mission" as undergoing it, for example drip, radiate, sweat, andleak.
9 To decide on the more basic form, we use the fact that in The tank leaked waterthe water is an optional constituent, and hence the minimal configuration of the verbis The tank leaked.
With the from configuration, o deletion is possible.As a representative of the verb class, we show the denotation and PSemSpec of toleak:DEN: (leak (OBJECT A)(PATH (SOURCE B)))PSS: (x / nondirected-action:lex leak :actor B < :actee A >)The following alternation rule applies to all these substance mission verbs andderives the from configuration:NAM : substance-sourceDXT: ()C0V: ()R0C: ((:actor :source)(< :actee > :actor))NRO: ()Let us now consider several alternations that change denotation, and hence are exten-sions.Stative-Resultative.
Example: Water filled the tank / The tank filled with water.
In discussingverbs that denote a state, Jackendoff (1990) points out that fill, cover, surround, andsaturate can describe ither a state or an inchoative vent, and encodes the differencewith the primitive INCH we have shown in the introduction to this section.
Our goalis to do without the primitive, and to define the change in terms of the Aktionsart ofthe verb; to this end, we use resultative in place of inchoative (see Section 2.2).9 Unnoticed by Levin, to leak can also be a verb of substance "intrusion," as in The camera leaked light.
Thisreading, which we do not handle here, reverses the directionality ofthe path involved.413Computational Linguistics Volume 24, Number 3On a similar matter, Levin (1993) describes the "locatum subject" alternation;which for instance holds between I filled the pail with water and Water filled the pail.It thus relates a causative and a noncausative form.
Levin states that the alternationapplies to a class of "fill verbs," of which there are many more than the four given byJackendoff.
Her alternation is not exactly the one we need here, since it also involvesa causative form; deriving the causative is a separate step in our framework.What we need here is a mixture of Jackendoff's and Levin's insights: Several ofLevin's fill verbs can be both transitive and intransitive, and some of the intransitivereadings denote 'to become Xed'.
Among these verbs are fill, flood, soak, encrust, orsaturate: The kitchen flooded with water means the same as The kitchen became flooded withwater.
For this subgroup of the fill verbs, we define an extension rule that derives aresultative reading from a state reading.
1?
Notice that this is different from Levin'slocatum subject alternation, since it does not involve a causer.NAM : stative-resultativeDXT: (event (Y (ACTIVITY X)))C0V: (X Y)R0C: ((:actor :inclusive)(:actee :actor)(directed-action nondirected-action)NRO: ()To illustrate the rule with an example, consider the denotation and PSemSpec ofthe state reading of f  ill:DEN: (fill-state (CONTAINER A)(C0NTENT B)(VALUE C) )PSS: (x / directed-action :lex fill:actor B :actee A < :destination C >)When matching it against a SitSpec with a tank and water, this yields the verbalizationThe water filled the tank, covering only the post-state of the SitSpec.
Now, the alternationrule extends the denotation to also cover the event and the activity that brings thefilling about.
Applying the changes to the PSemSpec results in(X / nondirected-action :lex fill:inclusive B :actor A < :destination C >)which corresponds to the sentence The tank filled with the water.A few stative verbs cannot be resultative without being also causative.
Considerto cover in these examples from Jackendoff:Snow covered the ground.,The ground covered with snow.Bill covered the ground with snow.10 The two roles "inclusive" and "of-matter" (used later) are the roles used by Penman to realize thedesired structure, but they are not very good descriptions ofthese semantic relationships.
For a moresystematic treatment, for instance along the lines of Somers (1987), the upper model needs to beextended.
See Section 4.1.414Stede Verb AlternationsFor these, a stative-culmination extension derives the resultative + causative form di-rectly from the stative one.
The rule is similar to the one given above, so we do notshow it here.Causative Extensions.
Example: The napkin soaked !
Tom soaked the napkin.
Levin discussesa causative/inchoative alternation that applies to a large number of verbs.
The classformed by them is somewhat heterogeneous with respect o Aktionsart, though; itcontains, for example, to turn as well as to open.
The former is in its basic form durative(The wheels turned), and the latter transformative (The door opened).
Accordingly, we splitthe alternation in two, which only differ in the DXT component, reflecting the differencein Aktionsart.
The durative-causative extension adds a causer to the denotation andmakes the former :actor the new :actee.
It equally applies to semelfactive verbsdenoting amomentaneous activity: The bell rang / The visitor ang the bell.
The resultative-causative xtension also covers the activity, because Tom opened the door expresses thatTom did something to achieve the change of state.
The causer itself is not coveredthough, because it still has to be verbalized separately.NAM : durative-causativeDXT: (activity (CAUSER X))COY: ()ROC: ((:actor :actee))NRO: (:actor X)NAM: resultative-causativeDXT: (event (ACTIVITY (X (CAUSER Y))))COY: ()ROC: ((:actor :actee))NRO: (:actor Y)The first rule derive~for examplG Tom walked the dog from The dog walke~ andthesecond Tom closed thedoor ~omThe doorclosed.Locative Extensions.
Example: (a) Sally sprayed the wall with paint.
/ (b ) Sally sprayed paintonto the wall.
The locative alternation has been studied by lexical-semanticists exten-sively.
Its characteristic is that configuration (a) of the verb conveys that something isperformed in a "complete" or "holistic" manner, whereas configuration (b) lacks thisfacet of meaning.
Levin points out that this alternation has received much attentionin linguistics research and notes that, in spite of the efforts, a satisfactory definition ofthe holistic facet has not been found.
Jackendoff, in his treatment of the alternation,suggests encoding the holistic feature in a primitive: The function ONd is a deriva-tive of ON and means that something "distributively" covers a surface, e.g., the paintcovers all of the wall.
Introducing a primitive, though, amounts to conceding thatno explanation in terms that are already known can be given.
We cannot solve thequestion of "holisticness," either, but we want to point to the fact that the two verbconfigurations correlate with a change in Aktionsart: Sally sprayed paint onto the wall isdurative (she can do it for two hours), whereas Sally sprayed the wall with paint is trans-formative (she can do it in two hours).
That observation leads us to propose that theexample is best analyzed as involving a mere activity in the with configuration, and anadditional transition in the onto configuration.
Support for this analysis comes fromPinker (1989), who postulates a change in meaning when moving from one configura-tion to the other: In (b) above, Sally causes the paint to move onto the wall  whereasin (a), Sally causes the wall to change its state by means of moving the paint onto it.415Computational Linguistics Volume 24, Number 3Sally sprayed paint onto the wall.
(spray-i (CAUSER sally-l)(OBJECT paint-l)(PATH (path-i (DESTINATION wall-l))))Sally sprayed the wall with paint.
(event-I (PRE-STATE (covered-state-I (OBJECT wall-l)(VALUE (not 'covered))))(ACTIVITY (spray-i (CAUSER sally-l)(OBJECT paint-l)(PATH (path-i (DESTINATION wall-l)))))(POST-STATE (covered-state-I (OBJECT wall-l)(VALUE 'covered))))Figure 5SitSpecsfor configura~ons of ~ spray.Pinker sees (a) as derived from (b) and suggests, as a constraint on the applicability ofthe alternation, that the motion (here: spray) causes an effect on the surface/container.While we decided not to discuss applicability conditions here, we support he ideathat the difference between (a) and (b) can be expressed with an additional change ofstate.
In our framework, we thus assign two different SitSpecs to the sentences, oneactivity and one event, as shown in Figure 5.The crucial point now is that the first SitSpec is fully embedded in the second;this is in correspondence with the truth conditions: If Sally has sprayed the wall withpaint, then she also has sprayed paint onto the wall.
To generalize the correspon-dence to an extension rule, we need to assume in the domain model a concept likecompletion-state, which is to subsume all those states in the domain model that have"extreme" values: an empty bucket, a fully loaded truck a completely covered sur-face, and so forth.
The exact interpretation f completion-state is the open questionthat Levin (1993) referred to, and that Jackendoff treated with his d subscript.
We dothink, though, that an abstract state in the domain model, which subsumes a rangeof the concrete states, is preferable to introducing a primitive on the linguistic level(unless the primitive is relevant for other linguistic phenomena as well).The following alternation rule applies to durative verb readings that denote ac-tivities of something being moved to somewhere, and extends them to also coverthe post-state, which must be subsumed by completion-state.
In this wan it derivesreading (a) from (b) in the spray example, and analogously for the other verbs un-dergoing the alternation, e.g.
: Tom loaded hay onto the wagon !
Torn loaded the wagon withhay; Jill stuffed the feathers into the cushion !
Jill stuffed the cushion with the feathers.
ThePSemSpec is modified as follows: The former :dest inat ion (wall) becomes the new:actee, whereas the former :actee (paint) now fills the role < : inclusive >, and isoptional there, because Jill stuffed the cushion is also well formed.NAM : locative-transitiveDXT : (event(MOVE (OBJECT X)(PATH (DESTINATION Y)))(POST-STATE (Z completion-state (OB3ECT Y))))COY : (Z)ROC: ((:actee < :inclusive >)(:destination :actee))NRO: ()416Stede Verb AlternationsLevin distinguishes two kinds of locative alternation: the spray/load alternationjust discussed and the clear (transitive) alternation.
The latter applies only to the verbsclear, clean, drain, empty and can be seen as the "semantic inverse" of the spray/loadalternation, because one group of verbs denotes activities of placing something some-where, and the other describes activities of removing something from somewhere; butboth have the same holistic effect in one of the verb configurations.
Thus, the rule forthe clear-alternation is very similar to the one just shown.
It derives, for example, Tomdrained the container of the water from Tom drained the water from the container, nNAM: clear-transitiveDXT : (event(MOVE (OBJECT X)(PATH (SOURCE Y)))(POST-STATE (Z completion-state (OBJECT Y))))C0V: (Z)ROC: ((:actee < :of-matter >)(:source :actee))NR0: ()The clear verbs, except for to clean, can in addition be intransitive, and Levin statesa separate alternation for them.
For to drain, the first configuration is The water drainedfrom the tank, and the second is either The tank drained or ?The tank drained of the water.According to Levin, "the intransitive form may be best in the absence of the of-phrase"(Levin 1993, 55).
The SitSpec denoted by the first configuration is:The water drained from the tank.
(move-i (OBJECT water-l)(PATH (path-I (SOURCE tank-l))))Note that our durative-causative extension rule given above applies in this case andextends the coverage of the SitSpec to one corresponding to Tom drained the water fromthe tank.
A rule that is parallel to that for the transitive case is given below; it derives: of-matter > is optional, we can also produce ?ThetankdrainedofthewateG sincethe <the preferred The ~nk drained.NAM: locative/clear-intransitiveDXT: (event(MOVE (OBJECT X)(PATH (SOURCE Y)))(POST-STATE (Z completion-state (OBJECT Y))))C0V: (z)ROC: ((:actor < :of-matter >)(:source :actor))NRO: ()3.3 Deriving Alternations SuccessivelyThe extension rules, as we have introduced them above, constitute a framework forsystematically deriving more complex verb configurations from simpler ones; the out-put produced by one rule serves as input to another.
Figure 6 provides asynopsis: The11 We ignore the role of the definite determiner here, which in fact has critical influence on the holisticinterpretation of mass nouns.
See, for example, White (1994).417Computational Linguistics Volume 24, Number 3?
"-(?-tat'~ x )  l (~cti'(event(PRE-STATEX)--~ r 7.......... !~t?g: .s .7 :~?!
:x!
!
.
.
t /(event(ACTWITY X) I..... .._(.P.O..S.T:S.T..A.~.X)) .......... \]RESULTATIVE Jc uSl~t'l~aefi?n~ resultative-(event(PRE-STATE X) \[} (ACTIVITY(CAUSER Y)) \]\[ RESULTATIV E+CAUSATIVE \]~y.X)..\] ,TIVE \]durative- causativeDURATIVE+CAUSATIVE \]causative spray/~Figure 6Dependency ofextension rules.boxes contain the denotation patterns that correspond to the Aktionsart feature, andthe rules transform a configuration with one Aktionsart into another.
In this graph,every verb base form has an entry point corresponding to the Aktionsart of its mostbasic configuration.
Examples: to fill is stative, to drain is durative, to open is transfor-mative, to remove is resultative + causative.
The "double box" in the middle is the entrypoint for both transformative and resultative verbs, but the incoming arrows produceresultative forms.
From the entry point of a verb, arcs can be followed and rules ap-plied if the respective alternation is specified in the lexical entry.
The six categoriesaccount for the Aktionsart features listed in Section 2.2, and the rules take care ofpossible shifts between them.
Thus, the full range of SitSpecs that our ontology allowsis being covered.To illustrate the functionality, we return to the example of to drain.
Figure 7 showshow the extension rules successively derive the various configurations.
Apart from thepassive, this is the complete "alternation space" of to drain according to Levin's (1993)catalogue.
Notice that the examples given also cover the four different drain clausesneeded to produce the alternative sentences given in (1) in the introduction.4.
Implementation: Two-Step Sentence Generation with MOOSEThe MOOSE sentence generator grew out of experiences with building the TECHDOCsystem (RSsner and Stede 1994), which produces instructional text in multiple lan-guages from a common representation.
Specifically, MOOSE accounts for the fact thatevents can receive different verbalizations even in closely related languages uch asEnglish and German.
It is designed as a sentence generation module that pays at-tention to language-specific lexical idiosyncrasies, and that can be incorporated into418JStede Verb AlternationsDenotation: (activity (OBJECT A)(PATH (SOURCE B)))PSemSpec: (xl / nondirected-act ion :lex drain:actor A :source B)(0) The oil drainedfromthe engine.Locative/clear-intransitive of (0):Denotation: (event (ACTIVITY (OBJECT A)(PATH (SOURCE B)))(POST-STATE (C (OBJECT B))))PSemSpec: (xl / nondirected-act ion :lex drain:of-matter A :actor B)(1) The engine drained ofthe oil.Durative-causative of (0):Denotation: (activity (OBJECT A)(PATH (SOURCE B))(CAUSER C))PSemSpec: (xl / directed-action :lex drain:actee A :source B :actor C)(2) Tom drained the oil from the engine.Resultative-causative of (1):Denotation: (event (ACTIVITY (OBJECT A)(PATH (SOURCE B))(CAUSER C) )(POST-STATE (C (OBJECT B))))PSemSpec: (xl / directed-action :lex drain:of-matter A :actee B :actor C)(3) Tom drained the engine of the oil.Figure 7Derivation of drain configurations.a larger-scale text generator} 2 In the following, we first describe the overall systemarchitecture, then discuss the process of lexicalization in some detail, and finally turnto the selection of a verb alternation on the basis of salience parameters.4.1 System ArchitectureFigure 8 provides an overview of the architecture of MOOSE.
The generator assumes alanguage-neutral level of event representation, the situation specification SitSpec (seethe example in Figure 2).
The SitSpec instantiates concepts from a domain model,which is implemented in the KL-ONE language LOOM (MacGregor and Bates 1987).Using the denotations of the lexicon entries of the target language, the lexical optionsfor verbalizing the SitSpec are determined.
In the next step, for verbs, the applicablealternations and extensions are computed and added to the set of options.
Then alanguage-specific semantic specification SemSpec (see the example in Figure 3) is con-structed in accordance with generation parameters pertaining to brevity, salience, andstylistic features.
The SemSpec is then handed over to a surface generator: Penman(Penman group 1989) for English, and a variant developed at FAW Ulm for German.The SemSpec language is a subset of the input representation language that wasdeveloped for Penman, the sentence plan language (SPL) (Kasper 1989).
An SPL ex-pression consists of variables, types, and case roles; an example was given in Figure 3.Penman and SPL are based on the upper model (UM) (Bateman et al 1990) introduced12 Fr6hlich and van de Riet (1997) describe how MOOSE is employed in the generation component of aninformation system.419Computational Linguistics Volume 24, Number 3DomainModel?o& ' - .
.
.
.
: ,  .
.i .
?/ i~:SitSpecLexiconMorphosyntax Morphosyntax MowhosyntaxAlternations Alternations AlternationsPartial SemSpec Partial SemSpec Partial SemSpecConnotation Connotation ConnotationDenotation Denotation DenotationAlternation/Extensionrules -- Ve lzatlon@ ................ LLocal yor eredlverbalization / /~N Upper Modeloptions F_JG(4) I Unijicati?n of ~ .............................Well-formed, .
'complete, ..'preferred /SemSpecf Su,~a--~enera"on 1(5) L Penman EZG J/ \ ,English Germansentence sentenceFigure 8MOOSE system architecture.in Section 2.3.1.
For any type appearing in an SPL, Penman eeds to know by whichUM type it is subsumed, so that appropriate generation decisions can be made.The way we use Penman and the UM in the Moose architecture is somewhatdifferent from the original Penman conception.
In Penman, the domain model wassupposed to be subsumed by the UM, which indeed simplifies generation from inputthat uses domain model concepts.
However, the range of alternative verbalizations420Stede Verb Alternationsthat can be produce d from the same input is seriously limited under this approach(see Stede and Grote \[1995\]), and therefore Moose opts for a complete separation ofDM and UM; they are distinct axonomies.
Consequently, asopposed to a general SPLterm used in Penman, a SemSpec used in Moose must contain only upper modelconcepts and no domain model concepts.Furthermore, since our system takes lexicalization as the decisive task in mappinga SitSpec to a SemSpec, the UM concepts referred to in a SemSpec must be annotatedwith :lex expressions.
Thus, a SemSpec is a lexicalized structure, and accordingly,Moose interprets the upper model as a taxonomy of lexical classes.
This contradicts hePenman philosophy of viewing the UM as abstract semantics and clearly distinct fromthe generation grammar, which in accordance with systemic-functional linguistics is anintegrated lexicogrammar, with "lexis as most delicate grammar" (Hasan 1987).
Thisidea, however, has been a theoretical rather than a practical one, and lexical mattersthus have not been a strong point of Penman.
For instance, the distinction betweenobligatory and optional participants ofa verb was quite blurred.
Also, Penman allowedonly for very simple lexical choice mechanisms, asit assumed a straightforward one-to-one mapping between concepts and words.
MOOSE overcomes these problems byassigning acentral role to the lexicon, placing a lot of information i it, and taking it asthe crucial device for the SitSpec-SemSpec mapping.
SemSpec, then, is an intermediatelevel of representation that reflects sentence semantics and that mediates betweenthe language-neutral conceptual representation a d linguistic realization.
The simpleform of our alternation rules shown in the last section, which abstract over syntacticrealization, demonstrates the utility of SemSpec as a level of description.In practice, our aim to upgrade SPL from a convenient input notation of a front-end NLG module to a systematic and well-motivated level of description i volves notonly building Moose "around" Penman, but also making some changes to the uppermodel and the generation grammar.
But for the purposes of this paper, which focuseson the semantics of verb alternations and their role in NLG, we avoid dealing withPenman's internals and rather treat it as a "black box.
"4.2 Lexical izationIn order for serious lexical choices to be possible, the first step of lexicalization iMoose consists of determining the set of verbalization options: all the lexemes whosedenotations can potentially cover some part of the input SitSpec.
Since we represent theinternal structure of events, the denotation of a lexeme need not be a single concept;instead, it can be a complete configuration of concepts and roles.
The consequencesare a higher computational cost in finding lexical options, but also a greater flexibilityin finding different verbalizations of the same event.
As an example, consider thedenotation of the causative reading of tofilh(event (PRE-STATE (fill-state (VALUE (not 'full))(CONTAINER A)))(ACTIVITY (CAUSER B))(P0ST-STATE (fill-state (VALUE < D 'full >)(CONTAINER A)(C0NTENT C) ) ) )Given some input SitSpec involving filling, the variables of the denotation are boundto instances or atomic values of the SitSpec when it is matched against he denotation.The filler of the value role in the post-state appears in angle brackets because it is adefault value.
The accompanying partial SemSpec of to fill contains the same variables:421Computational Linguistics Volume 24, Number 3(x / directed-action :lex fill:actor B :actee A :inclusive C <:destination D>)When the denotation is matched against a SitSpec, the variable bindings are prop-agated to the partial SemSpec; and when it is later unified with the partial SemSpecscorresponding to the other elements, a complete SemSpec results, from which Penmanproduces a sentence like Jill filled the tank with oil.
(If the value is different from ' fu l l ,it also gets verbalized, such as in Jill fflled the tank to the second mark.
)Importantly, the matching between denotations and SitSpec does not test for iden-tity, but for subsumption--it exploits the functionality provided by LOOM.
In thisway, the selectional restrictions of verbs are checked when the lexical options are de-termined.
Moreover, the matcher finds not only the most specific word, but also theapplicable more general candidates, which is helpful, for instance, in achieving stylisticeffects, and in avoiding undue repetitions of the same specific term.Since we are using relatively fine-grained representations forSitSpecs and denota-tions, the generation ofvariants in incorporation is enabled by the covering mechanismin conjunction with the subsumption check.
In the example go by plane/fly, the generalverb to go covers only the move concept, and the role instrument-plane is left to beexpressed by a prepositional phrase; whereas the specific verb toffy covers the wholeconfiguration.
In this fashion, quite different coverings of the input SitSpec are possi-ble; for instance, MOOSE produces Tom poured water into the tank until it was full and Tomfilled the tank with water (amongst others) as paraphrases of the same event.After the initial matching between denotations and SitSpec, the various alterna-tions are computed for those verbs whose base form has been found as a candidate(step 2 in Figure 8).
Their lexical entry specifies which alternation/extension rulesapply, and they are executed sequentially, as outlined in the previous section, anddemonstrated for to drain in Figure 7.
For any extension rule that adds new items tothe denotation, the new material is matched against he SitSpec to ensure that thealternation is applicable, and to compute the additional covering.
In this way, all theapplicable alternated forms of a verb are added to the pool of verbalization options.The set of all lexemes that successfully matched some part of the SitSpec, togetherwith the alternated verb forms, constitute the search space for constructing an appro-priate SemSpec.
The options are first brought into an order of preference (step 3 inFigure 8) according to various parameters such as the desired salience assignment,which is explained in the next section.
Considering the options in this order, a com-plete and well-formed SemSpec is built from the partial SemSpecs that are associatedwith some of the lexical options--those that collectively cover the entire SitSpec andthus will take part in the sentence.
This is done by a unification process driven by thecandidate verb options; recall that their PSemSpec onsists of an upper model processand the mappings from situation elements to process participants, which is achievedby co-indexing with positions in the denotation.
By means of sharing this informationbetween denotation and PSemSpec, the lexicon entries serve as a "bridge" betweenthe SitSpec to be verbalized and the intermediate r presentation SemSpec.For more details on the kinds of mono- and multilingual variation produced byMOOSE, and on the lexicalization algorithm, see Stede (1996b).4.3 Producing Salience Variation with AlternationsHaving explained the basic machinery of MOOSE, we now demonstrate how the gen-erator can make an informed choice among the set of possible verb alternations onthe basis of a salience parameter, Since a full-fledged treatment of the role of salience422Stede Verb Alternationsis well beyond the scope of this paper, 13 we will merely sketch a possible division oflabor between text planning and sentence planning, and then describe the role of verbalternations as one means of realizing generation goals related to salience.
Specifically,we show under what circumstances the various drain sentences given in the examplesin (1) in Section 1 (and in Figure 7) are produced by MoosE.When text planning has been completed and linearization as well as the "chunk-ing" of the material into sentence-size pieces is accomplished, Moose takes over toperform the necessary sentence planning, which includes lexicalization.
Thus, textplanning has produced a sequence of SitSpecs, which may be enriched with infor-mation pertaining to the relative salience of the elements.
This information can resultfrom constraints on theme development, from rhetorical strategies, or from other con-siderations at the discourse level.
For instance, in the sample texts given in (1) at thebeginning of Section 1, different heme developments are responsible for the differentusages of to drain.For sentence planning, we assume that individual nodes of a SitSpec can have aforeground, background, or optional abel attached to them (but they need not).
Then,a realization is to be found that signals the differences in prominence on the linguisticsurface.
In general, there is no one-to-one correspondence b tween the configurationof salience labels and linguistic realization, though.
Instead, we view salience goals asgoals that the generator tries to fulfill if possible, similar to certain stylistic goals (seeStede \[1996a\]).
Thus, generation becomes a matter of constraints (say the right thing)and preferences (try to say it in a particular way), similar to Hovy's (1988) distinctionbetween "prescriptive" and "restrictive" planning.What, then, is the role of verb alternations in assigning different degrees of salience?Talmy (1988) listed a number of morphological and syntactic means to distributesalience across the elements of a clause.
For instance, he suggested the hierarchy sub-ject > direct object > indirect object > oblique, ranging from the most salient to theleast salient.
From a slightly different perspective, Kunze (1991) was concerned withdifferences in salience between similar verbs.
He advanced the view that they share acommon underlying base form and differ, inter alia, in distributing salience via theircase roles.
For our purposes here, we can adapt these insights (with some simplifica-tion) and state that an element is placed in the foreground if it is mapped to the roleactor (best) or to actee (second best).
Correspondingly, it is placed in the backgroundif it corresponds to a circumstance, i.e., a role that is not part of the verb's case frame.Now, consider again the sentences in Figure 7.
On the one hand, (0) and (1) omitthe fact of Tom's causing the event, and hence are preferred only if the respectiveSitSpec node is labeled as optional.
On the other hand, (0, 2) and (1, 3) differ in thatthe former render the oil prominent, while the latter emphasize the engine.
Figure 9shows the common SitSpec underlying the four sentences, and a set of salience labelsattached to three nodes, where the numbers correspond to the target sentences.
Forexample, when sentence (1) is the preferred output, the SitSpec would have an optlabel at node tom-1 and an fg label at node engine-1.For any verbalization option, base forms and alternations alike, the number offulfilled salience goals can be computed straightforwardly: Since variables in denota-tions and PSemSpecs are co-indexed, we can determine for every salience label in theSitSpec how the corresponding element participates in the SemSpec.
Using the criteriagiven above, preference values result for the various options, and they are factoredinto the overall preference ranking of the verbalization options.
All other things being13 Pattabhiraman (1992) devoted a dissertation to the topic of salience in NLG.423Computational Linguistics Volume 24, Number 3> ' full/ ~ > oil-Ievent- 1 drain- 1 ~ /engine-1(l,3):fgfill-state-2~ " > 'emptyFigure 9SitSpec representing a drain event.equal, the verb alternation that accomplishes the best realization of the salience labelsoutranks the other options and thus gets selected for building the SemSpec.
Again,notice that other syntactic and morphological means (e.g., expressing an element witha separate word versus incorporating it into another word) for assigning salience canbe integrated into this scheme.5.
Summary and Related WorkWe have proposed an NLG framework, together with suitable representation schemes,that can systematically produce a range of verb alternations from a common underly-ing input representation a d select he most appropriate form on the basis of salienceparameters.
Productive rules derive the more complex forms from a basic one, whichis the only one that needs to be stated in the lexical entry of the verb.
We have focusedon those alternations that affect the Aktionsart of the verb: They imply a type changein aspectual classifications such as those of Bach (1986).For generation, our approach uses two distinct ontologies: a language-neutral do-main model for event categorization, and a language-specific taxonomy, the uppermodel developed by Bateman et al (1991) on the basis of Halliday's (1985) work.
Thelexicon acts as the mediator between these two realms and serves to map a concep-tual input representation to a semantic sentence specification, which can be furtherprocessed by a front-end realization component.
Within this framework, multilingualgeneration is possible once language-specific upper models and front-ends are used(but multilinguality was not addressed in this paper).
The approach as been imple-mented in the MOOSE system, which uses the Penman generator (Penman Group 1989);MOOSE can serve as a plug-in sentence production module to a larger text generator.The examples discussed in this paper (including the alternated forms) were gener-ated from a domain model that encodes knowledge about automobile ngines, tanks,and related liquids.
It consists of 150 concepts and relations; the associated Englishlexicon has 200 words, including 50 verbs.
Given the nature of the work, which de-pends on quite fine-grained representations in both domain model and lexicon, it isdifficult to make statements on how well the approach "scales up."
Large-scale (au-tomatic) acquisition of dictionary entries typically does not result in representationsof the kind needed here, and furthermore, the domain model needs to be developedin tandem with the lexicon.
The precise shape of such models, on the other hand,also depends on the specific application the generator is used for; even though somesteps towards standardization i ontologies are being taken, this is still a bottleneckin knowledge-based NLG.424Stede Verb AlternationsIn the following, we compare our approach to some related work on verb alter-nations and on lexicalization in NLG.
Finally, we draw some conclusions as to theoverall scope of the work and its utility for NLG.5.1 AlternationsStarting from the aspectual categories proposed by Bach (1986), the verb classificationsof Levin (1993), and the lexical representations given by Jackendoff (1990), we havedeveloped a new synthesized approach for dealing with verb alternations that affectthe Aktionsart of verbs.
Our ontology for input representations and the specificationsfor lexical meaning have benefited from the earlier work just mentioned but essentiallyconstitute a new framework in which the specific alternation/extension rules could beformulated.The utility of these rules demonstrates the importance of defining a place for fine-grained lexical-semanfic representations in language generation.
To our knowledge,no other generator can systematically derive the various forms for the alternationsdiscussed in this paper.
Recentl.~ Dorr and Olsen (1996) suggested using verb rep-resentations based on Jackendoff's (1990) LCSs for NLG; specific kinds of LCSs areproposed to represent different classes of verbs on the basis of telicity.
Rules are pro-posed that relate telic and atelic versions of the same verb.
The central difference ofour approach is our distinction between SitSpec and SemSpec, and thus between de-notation and PSemSpec in the lexical entries.
Dorr and Olsen map directly betweenLCSs and syntax, so there is no systematic link to background knowledge yet (which,as we have pointed out, would be useful for generation).
Besides, as we mentioned inSection 3, LCS representations u e primitives (BECOME, INCH, d), where we opt fora more fine-grained ecomposition of the underlying event.For the alternations investigated, we have chosen the approach of defining a singlebase form from which alternated forms are derived.
For other alternations, this mightnot be feasible or practical--in such cases, different lexical entries are to be used.
Thereis, on the other hand, a line of research that questions the utility of distinguishing abase form from a more complex one in an alternation.
For example, Saint-Dizier (1996)states that his approach to alternations deliberately avoids three difficulties: the needto define a basic form from which alternations are produced; the need to explain therelation between the basic form and the alternated one; and the need to account forchanges in meaning produced by the alternation.
It seems that the work presentedin this paper aims precisely at those questions that Saint-Dizier's approach proposesto better leave aside.
For generation, however, we believe that a system must knowabout he fine-grained changes in meaning that a verb alternation implies--a generatorhas to relate some semantic input representation to verb meaning, after all, and thatincludes alternations.
And if the semantic hange induced by an alternation can bedescribed by a general rule that covers a whole class of verbs, a useful abstraction isgained.The final point to consider is the question of admitting lexical rules into one'sframework.
For example, Sanfilippo (1994) argues against this instrument on thegrounds that there is no general control regime on lexical rules that would determin-istically restrict any polysemic expansion.
Instead, he advocates coding the alternativelexical forms in a hierarchy of typed feature structures, where the underspecified formssubsume the specific ones.
His criticism applies to the notion of rules that are trig-gered automatically and proceed to derive new forms without principled limitations.Our "defensive" approach of listing applicable rules in the lexical entries avoids thisproblem but at the same time raises the question of why rules should be preferable toa simple enumeration of forms.
We return to this point in section 5.3.425Computational Linguistics Volume 24, Number 35.2 Lexicalization in NLGIn MOOSE, the lexicon is the central device for mapping between input representationsand intermediate s ntence-semantic representations.
The idea of using the lexicon earlyin the generation process is not new; it has been realized in several other generators, forexample in the frame-oriented system DIOGENES (Nirenburg and Nirenburg 1988).
Incontrast to earlier systems, however, MOOSE strengthens the role of lexical semanticsin the generation process by distinguishing between the SitSpec and SemSpec levelsand clearly specifying the relationships between the two (as done with the alterna-tion rules).
Furthermore, we have emphasized that lexical choice should be seen as aconstraint satisfaction process, similar to Reiter (1991), who focused his attention onnouns, while we have concentrated on verbs.There are several other generators using Penman as a front-end.
For example, theDRAFTER system (Paris et al 1995) builds SPLs and hands them over to Penman;contrary to MoosE, however, the domain model in DRAFTER is subsumed by theupper model, which significantly limits the range of lexical variation, as pointed outabove.Working in the framework of systemic-functional grammar (SFG), both Wanner(1992) and Teich and Bateman (1994) employ SPL as an intermediate description, butthey emphasize the integration of the SPL construction process into SFG.
Wanner usessystem networks to make fine-grained lexical choices in line with the three systemicmetafunctions.
Teich and Bateman develop system networks describing enre andregister variation to drive the generation process, and they query an external domainmodel when building the SPL.
In related work, Teich, Firzlaff, and Bateman (1994)present an implementation f Kunze's theory of semantic emphasis (cf.
Section 4.3).From a "basic semantic scheme" annotated with emphasis labels, an SPL with ap-propriate roles and upper model concepts is constructed.
The SPL can also containan emphatic/nonemphatic feature, which might lead, for instance, to a dative shift.Hence, this work shares our interest in salience and indeed goes a step further thanour present account in that the generation grammar can employ additional means forsalience variation.
However, in these three approaches, all lexical matters are takento be part of the (huge) grammar processing the SPL.
Thus, the central difference tothe MOOSE approach is our step of promoting the lexicon to the crucial device formapping between conceptual nd sentence-semantic representations.
Wehave arguedthat this step of keeping the lexicon separate and accessing it early has a number ofadvantages.Essentially the same difference holds between MOOSE and GOSSIP (Iordanskaja,Kittredge, and Polgu~re 1991), which also emphasizes the importance of lexical choiceand paraphrasing abilities.
Here, a powerful exicalization mechanism is embeddedin a meaning-text generation model following the theory of Mel'cuk, where lexicalfunctions play a central role in mapping between different levels of representation.These are semantic and syntactic levels, though, whereas MOOSE focuses on the in-terrace between conceptual nd semantic representations, and employs the lexicon atthat point.Representations more similar to ours have been used by Dorr and Voss (1996),who employ Jackendoff's (1990) LCSs as an interlingua in machine translation, and byDi Eugenio (1993), who also represents LCS in a KL-ONE language but for purposesof analysis rather than generation.
More specifically for NLG, structure mappingsbetween fine-grained representations have been suggested for instance by Horacek(1990), Nogier and Zock (1992), and Nicolov, Mellish, and Ritchie (1996).
In all theseapproaches, the input structure is directly mapped to a syntactic structure, though,while we have argued that an intermediate s ntence-semantic level is advantageous426Stede Verb Alternationsin order to explore generalizations ( uch as the alternation rules) as well as for multi-lingual purposes.5.3 ConclusionsIn a computational pproach to the lexicon, word sense enumeration should not bethe rule but be reserved for the exceptions (Pustejovsky 1995).
In line with this view,our approach seeks to exploit generalizations by accounting for different forms of averb with explicit alternation and extension rules that relate the changes in meaning tothe changes in form.
Ultimately, such an account establishes correspondences not onlybetween different forms of the same verb but also between different verbs; for example,applying the causative xtension to to rise yields (one form of) to raise.
Interconnectionsof this kind have not yet been integrated into the system presented here, though.Three assumptions have guided the development of our account of verb alterna-tions: (1) There is a single base form from which other forms can be derived.
(2) Alter-nation rules leave the denotation unchanged, and extension rules always add facetsof meaning to the simpler denotation.
(3) The changes in denotation correspond tochanges in form, which can be characterized on a case-role level of description.
Indealing with the telicity-related alternations discussed in Section 3, these assumptionshave proven useful.
For generalizing the approach to other alternations, assumption (2)could turn out to be too strong; in fact, even the causative xtension might not alwaysbe monotonic when temporal adverbials are part of the sentence.
In our framework,monotonicity is not a problem as long as the order of rule application is fixed anyway(cf.
Figure 6).
As soon as nonmonotonic rules are allowed, and the applicability ofrules is no longer defined in the lexicon entries but triggered directly by the input,circularity is to be avoided: It needs to be ensured that rules reducing meaning reduceonly parts that are not added by a different rule.Our selection of alternations was guided by their relationship to Aktionsart, in par-ticular to causation and telicity.
Since the notion of Aktionsart is not a well-demarcatedone in linguistics, and since the most comprehensive catalogue of alternations, the oneby Levin (1993), has largely excluded Aktionsart-related problems, it is rather difficultto evaluate our approach in terms of "how many alternations" it covers.
(Besides,we have argued in Section 3 that some of Levin's categorizations eed refinement.
)Clearly, there are other alternations involving telicity that we have not discussed here.Dorr and Olsen (1996) state that 27 of Levin's alternations add the telicity feature to averb's meaning; many of these are rather specific and apply only to very few verbs.Among the more prominent ones are the unspecified object alternation (Tom ate~Tomate a pizza) and the conative alternation (John cut at the bread~John cut the bread).
Bothlend themselves to extension rules as in our framework, because one form entails theother and adds information: When it holds that Tom ate a pizza, then it holds thatTom ate.
Other alternations involve specific prepositions, uch as Levin's through~withalternation: Alison pierced the needle through the cloth~Alison pierced the cloth with a needle.This does not pose problems for representing the changes in denotation, but renders areliance on case roles--assumption (3)above---questionable; if suitable generalizationsto similar prepositions cannot be found, the change in form ought to be stated directlyon the syntactic level.Finally, we look at the question of evaluating our approach from the perspec-tive of natural anguage generation.
From a descriptive viewpoint, as argued above,general exical rules are to be preferred over enumerating word senses.
Whether thispreference also carries over to the design of practical NLG systems, however, meritssome additional discussion.
For the lexicalization step, we can either successively ap-ply alternation rules to a successfully matched base form, or compile out the various427Computational Linguistics Volume 24, Number 3alternated forms, which must then all be considered in matching against he inputrepresentation.
While the first option obviously ields a much smaller lexicon, it is notself-evident whether it is faster or slower in a running system.As long as all alternated forms individually enter the matching phase, the compile-out option is hardly useful.
Rather, compilation can be advantageous if only the mostpreferred form of the verb is considered first, and the other ones only upon request ifthe first did not work out.
In this case, we are spared the effort of applying the rulesto reach the desired form at run-time.
Overall, the compilation decision hinges on thekind of criteria that the generator employs for its lexical choices.
If the desired saliencedistribution is the central factor, then storing precompiled options and their salienceinformation will be most effective.
If considerations of lexical style lead to preferringone verb over a set of others irrespective of the specific alternation, then applyingalternation rules only to the preferred verb will be more effective (in turn dependingon how many similar verbs are ruled out and thus spared from the matching process).Thus, there appears to be no general answer; the size of the lexicon, including theranges of nearly synonymous verbs, and the choice criteria used by the generatorhave to be taken into account.AcknowledgmentsThe research reported in this paperoriginated at the University of Toronto(Canada) and at the Research Center forApplied Knowledge Processing (FAW) inUlm (Germany).
In the respective places,thanks to Graeme Hirst and Dietmar R6snerfor discussions and advice, and to theNatural Sciences and Engineering ResearchCouncil of Canada (NSERC) and to FAW forfinancial support.
I am grateful to theanonymous reviewers for their valuablesuggestions for improving an earlier versionof this paper.ReferencesBach, Ernmon.
1986.
The algebra of events.Linguistics and Philosophy 9.Bateman, John, Robert Kasper, JohannaMoore, and Richard Whitney.
1990.
Ageneral organization of knowledge fornatural language processing: The penmanupper model.
Technical Report, USC/ISI,Marina del Rex CA.Brachrnan, Ronald and James G. Schmolze.1985.
An overview of the KL-ONEknowledge representation system.Cognitive Science 9(2).Bussmann, Hadumod.
1983.
Lexikon derSprachwissenschaft.
Kr6ner, Stuttgart.Di Eugenio, Barbara.
1993.
UnderstandingNatural Language Instructions: AComputational Approach to Purpose Clauses.Ph.D.
thesis, Department of Computerand Information Science, University ofPennsylvania.
Technical ReportMS-CIS-93-91.Dorr, Bonnie and Mari Broman Olsen.
1996.Multilingual generation: The role oftelicity in lexical choice and syntacticrealization.
Machine Translation (11):37--47.Dorr, Bonnie and Claire Voss.
1996.
Amulti-level approach to interlingual MT:Defining the interface betweenrepresentational languages.
InternationalJournal of Expert Systems 9(1).Fr6hlich, Marcel and R. van de Riet.
1997.Conceptual models as knowledgeresources to text generation.
In Proceedingsof the Third International Workshop on theApplication of Natural Language toInformation Systems, Vancouver.Halliday, Michael.
1985.
Introduction toFunctional Grammar.
Edward Arnold,London.Hasan, Ruqaiya.
1987.
The grammarian'sdream: Lexis as most delicate grammar.In Michael Halliday and Robin Fawcett,editors, New Developments in SystemicLinguistics.
Volume 1.
Pinter, London.Helbig, Gerhard and Wolfgang Schenkel.1973.
W6rterbuch zur Valenz undDistribution deutscher Verben.
VEBVerlag Enzyklop~idie, Leipzig.Horacek, Helmut.
1990.
The architecture ofa generation component in a completenatural language dialogue system.
InRobert Dale, Chris Mellish, and MichaelZock, editors, Current Research inNaturalLanguage Generation.
Academic Press,London.Hov~ Eduard H. 1988.
Generating NaturalLanguage Under Pragmatic Constraints.Lawrence Erlbaum, Hillsdale, New Jersey.Iordanskaja, Lidia, Richard Kittredge, andAlain Polgu~re.
1991.
Lexical selectionand paraphrase in a meaning-text428Stede Verb Alternationsgeneration model.
In C4cile Paris, WilliamSwartout, and William Mann, editors,Natural Language Generation i ArtificialIntelligence and Computational Linguistics.Kluwer, Dordrecht.Jackendoff, Ray.
1990.
Semantic Structures.MIT Press, Cambridge, MA.Kasper, Robert.
1989.
A flexible interface forlinking applications to Penman's entencegenerator.
In Proceedings ofthe DARPAWorkshop on Speech and Natural LanguageProcessing, University of Pennsylvania.Kunze, J6rgen.
1991.
Kasusrelationen u dsemantische Emphase (studia grammaticaXXXI).
Akademie Verlag, Berfin.Levin, Beth.
1993.
English Verb Classes andAlternations.
University of Chicago Press,Chicago.Levin, Beth and Malka Rappaport Hovav.1995.
Unaccusativity.
MIT Press,Cambridge, MA.MacGregor, Robert and Robert Bates.
1987.The Loom Knowledge RepresentationLanguage.
Techrfical ReportISI/RS-87-188, USC/ISI, Marina del Re)~CA.Matthiessen, Christian and John Bateman.1991.
Text Generation and SystemicFunctional Linguistics: Experiences fromEnglish and Japanese.
Pinter, London.Moens, Marc and Mark Steedman.
1988.Temporal ontology and temporalreference.
Computational Linguistics 14(2).Nicolov, Nicolas, Chris Mellish, and GraerneRitchie.
1996.
Approximate generationfrom non-hierarchical representations.
InProceedings ofthe Eighth InternationalWorkshop on Natural Language Generation,Herstmonceux Castle, England.Nirenburg, Sergei and Irene Nirenburg.1988.
A framework for lexical selection innatural anguage generation.
InProceedings ofthe 12 th InternationalConference on Computational Linguistics(COLING-88), Budapest.Nogier, Jean-Francois and Michael Zock.1992.
Lexical choice by pattern matching.Knowledge Based Systems 5(3).Paris, C4cile, Keith Vander Linden, MarkusFischer, Anthony Hartle)~ Lyn Pemberton,Richard Power, and Donia Scott.
1995.
Asupport ool for writing multilingualinstructions.
In Proceedings oftheInternational Joint Conference on ArtoqcialIntelligence (IJCAI-95), Montreal.Parsons, Terence.
1990.
Events in theSemantics ofEnglish: A Study in SubatomicSemantics.
MIT Press, Cambridge, MA.Pattabhiraman, T. 1992.
Aspects of Salience inNatural Language Generation.
Ph.D. thesis,School of Computing Science, SimonFraser University.The Penman group.
1989.
UnpublishedDocumentation f the Penman SentenceGeneration System.
USC/ISI, Marina delRey, CA.Pinker, Steve.
1989.
Learnability andCognition: The Acquisition of ArgumentStructure.
MIT Press, Cambridge, MA.Pustejovsky, James.
1991.
The syntax ofevent structure.
Cognition 41:47-81.Pustejovsk)~ James.
1995.
The GenerativeLexicon.
MIT Press, Cambridge, MA.Reiter, Ehud.
1991.
A new model of lexicalchoice for nouns.
Computational Intelligence7:240-251.R6sner, Dietmar and Manfred Stede.
1994.Generating multilingual documents froma knowledge base: The TECHDOCproject.
In Proceedings ofthe InternationalConference on Computational Linguistics(COLING-94), Kyoto.Saint-Dizier, Patrick.
1996.
Verb semanticclasses based on 'alternations' and onWordNet-like semantic riteria: Apowerful convergence.
In Proceedings oftheWorkshop on Predicative Forms in LexicalSemantics and Lexical Knowledge Bases,Toulouse.Sanfilippo, Antonio.
1994.
Word knowledgeacquisition, lexicon construction, anddictionary compilation.
In Proceedings ofthe International Conference on ComputationalLinguistics (COLING-94), Kyoto.Somers, Harold.
1987.
Valency and Case inComputational Linguistics.
EdinburghUniversity Press, Edinburgh.Stede, Manfred.
1996a.
Lexical Semantics andKnowledge Representation n MultilingualSentence Generation.
Ph.D. thesis, Dept.
ofComputer Science, University of Toronto.Technical Report CSRI-347.Stede, Manfred.
1996b.
Lexical paraphrasesin multilingual sentence generation.Machine Translation 11:75-107.Stede, Manfred and Brigitte Grote.
1995.
Thelexicon: Bridge between language-neutraland language-specific representations.
InWorking notes of the IJCAI Workshop onMultilingual Text Generation, Montreal.Talmy, Leonard.
1988.
The relation ofgrammar to cognition.
InB.
Rudzka-Ostyn, editor, Topics in CognitiveLinguistics.
John Benjamins, Amsterdam.Teich, Elke and John Bateman.
1994.Towards the application of textgeneration i an integrated publicationsystem.
In Proceedings ofthe SeventhInternational Workshop on Natural LanguageGeneration, Kennebunkport, ME.Teich, Elke, Beate Firzlaff, and JohnBateman.
1994.
Emphatic generation:429Computational Linguistics Volume 24, Number 3Employing the theory of semanticemphasis for text generation.
Paperpresented at the International Conferenceon Computational Linguistics(COLING-94), Kyoto.Tesni~re, L. 1959. t~le'ments de syntaxestructurale.
Klincksieck, Paris.Vendler, Zeno.
1967.
Linguistics andPhilosophy.
Cornell University Press,Ithaca, New York.Wanner, Leo.
1992.
Lexical choice and theorganization of lexical resources in textgeneration.
In Proceedings ofthe EuropeanConference on Artificial Intelligence (ECAI92),Vienna.White, Michael.
1994.
A ComputationalApproach to Aspectual Composition.
Ph.D.thesis, Dept.
of Computer andInformation Science, University ofPennsylvania.430
