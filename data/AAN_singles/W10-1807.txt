Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 56?63,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsInfluence of Pre-annotation on POS-tagged Corpus DevelopmentKare?n FortINIST CNRS / LIPNNancy / Paris, France.karen.fort@inist.frBeno?
?t SagotINRIA Paris-Rocquencourt / Paris 7Paris, France.benoit.sagot@inria.frAbstractThis article details a series of carefully de-signed experiments aiming at evaluatingthe influence of automatic pre-annotationon the manual part-of-speech annotationof a corpus, both from the quality and thetime points of view, with a specific atten-tion drawn to biases.
For this purpose, wemanually annotated parts of the Penn Tree-bank corpus (Marcus et al, 1993) undervarious experimental setups, either fromscratch or using various pre-annotations.These experiments confirm and detail thegain in quality observed before (Marcus etal., 1993; Dandapat et al, 2009; Rehbeinet al, 2009), while showing that biases doappear and should be taken into account.They finally demonstrate that even a notso accurate tagger can help improving an-notation speed.1 IntroductionTraining a machine-learning based part-of-speech(POS) tagger implies manually tagging a signifi-cant amount of text.
The cost of this, in terms ofhuman effort, slows down the development of tag-gers for under-resourced languages.One usual way to improve this situation is toautomatically pre-annotate the corpus, so that thework of the annotators is limited to the validationof this pre-annotation.
This method proved quiteefficient in a number of POS-annotated corpus de-velopment projects (Marcus et al, 1993; Danda-pat et al, 2009), allowing for a significant gainnot only in annotation time but also in consistency.However, the influence of the pre-tagging qualityon the error rate in the resulting annotated corpusand the bias introduced by the pre-annotation hasbeen little examined.
This is what we propose todo here, using different parts of the Penn Treebankto train various instances of a POS tagger and ex-periment on pre-annotation.
Our goal is to assessthe impact of the quality (i.e., accuracy) of thePOS tagger used for pre-annotating and to com-pare the use of pre-annotation with purely manualtagging, while minimizing all kinds of biases.
Wequantify the results in terms of error rate in the re-sulting annotated corpus, manual annotation timeand inter-annotator agreement.This article is organized as follows.
In Sec-tion 2, we mention some related work, while Sec-tion 3 describes the experimental setup, followedby a discussion on the obtained results (Section 4)and a conclusion.2 Related Work2.1 Pre-annotation for POS TaggingVery few manual annotation projects give detailsabout the campaign itself.
One major exception isthe Penn Treebank project (Marcus et al, 1993),that provided detailed information about the man-ual annotation methodology, evaluation and cost.Marcus et al (1993) thus showed that manual tag-ging took twice as long as correcting pre-taggedtext and resulted in twice the inter-annotator dis-agreement rate, as well as an error rate (using agold-standard annotation) about 50% higher.
Thepre-annotation was done using a tagger trained onthe Brown Corpus, which, due to errors introducedby an automatic mapping of tags from the Browntagset to the Penn Treebank tagset, had an errorrate of 7?9%.
However, they report neither the in-fluence of the training of the annotators on the po-tential biases in correction, nor that of the qualityof the tagger on the correction time and the ob-tained quality.Dandapat et al (2009) went further and showedthat, for complex POS-tagging (for Hindi andBangla), pre-annotation of the corpus allows fora gain in time, but not necessarily in consis-56tency, which depends largely on the pre-taggingquality.
They also noticed that untrained annota-tors were more influenced by pre-annotation thanthe trained ones, who showed ?consistent perfor-mance?.
However, this very complete and inter-esting experiment lacked a reference allowing foran evaluation of the quality of the annotations.
Be-sides, it only took into account two types of pre-tagging quality, high accuracy and low accuracy.2.2 Pre-annotation in Other AnnotationTasksAlex et al (2008) led some experiments in thebiomedical domain, within the framework of a?curation?
task of protein-protein interaction.
Cu-ration consists in reading through electronic ver-sion of papers and entering retrieved informationinto a template.
They showed that perfectly pre-annotating the corpus leads to a reduction of morethan 1/3 in curation time, as well as a better recallfrom the annotators.
Less perfect pre-annotationstill leads to a gain in time, but less so (a little lessthan 1/4th).
They also tested the effect of higherrecall or precision of pre-annotation on one anno-tator (curator), who rated recall more positivelythan precision.
However, as they notice, this resultcan be explained by the curation style and shouldbe tested on more annotators.Rehbein et al (2009) led quite thorough ex-periments on the subject, in the field of semanticframe assignment annotation.
They asked 6 an-notators to annotate or correct frame assignmentusing a task-specific annotation tool.
Here again,pre-annotation was done using only two types ofpre-tagging quality, state-of-the-art and enhanced.The results of the experiments are a bit disappoint-ing as they could not find a direct improvement ofannotation time using pre-annotation.
The authorsreckon this might be at least partly due to ?an inter-action between time savings from pre-annotationand time savings due to a training effect.?
Forthe same reason, they had to exclude some of theannotation results for quality evaluation in orderto show that, in line with (Marcus et al, 1993),quality pre-annotation helps increasing annotationquality.
They also found that noisy and low qual-ity pre-annotation does not overall corrupt humanjudgment.On the other hand, Fort et al (2009) claim thatpre-annotation introduces a bias in named entityannotation, due to the preference given by anno-tators to what is already annotated, thus prevent-ing them from noticing entities that were not pre-annotated.
This particular type of bias should notappear in POS-tagging, as all the elements are tobe annotated, but a pre-tagging could influencethe annotators, preventing them from asking them-selves questions about a specific pre-annotation.In a completely different field, Barque etal.
(2010) used a series of NLP tools, calledMACAON, to automatically identify the centralcomponent and optional peripheral components ofdictionary definitions.
This pre-processing gavedisappointing results as compared to entirely man-ual annotation, as it did not allow for a significantgain in time.
The authors consider that the badresults are due to the quality of the tool that theywish to improve as they believe that ?an automaticsegmentation of better quality would surely yieldsome gains.
?Yet, the question remains: is there a qualitythreshold for pre-annotation to be useful?
and ifso, how can we evaluate it?
We tried to answerat least part of these questions for a quite simpletask for which data is available: POS-tagging inEnglish.3 Experimental SetupThe idea underlying our experiments is the follow-ing.
We split the Penn Treebank corpus (Marcus etal., 1993) in a usual manner, namely we use Sec-tions 2 to 21 to train various instances of a POStagger, and Section 23 to perform the actual ex-periments.
In order to measure the impact of thePOS tagger?s quality, we trained it on subcorporaof increasing sizes, and pre-annotated Section 23with these various POS taggers.
Then, we man-ually annotated parts of Section 23 under variousexperimental setups, either from scratch or usingvarious pre-annotations, as explained below.3.1 Creating the TaggersWe used the MElt POS tagger (Denis and Sagot,2009), a maximum-entropy based system that isable to take into account both information ex-tracted from a training corpus and information ex-tracted from an external morphological lexicon.1It has been shown to lead to a state-of-the-art POStagger for French.
Trained on Sections 2 to 211MElt is freely available under LGPL license, on the webpage of its hosting project (http://gforge.inria.fr/projects/lingwb/) .57of the Penn Treebank (MEltALLen ), and evaluatedon Section 23, MElt exhibits a 96.4% accuracy,which is reasonably close to the state-of-the-art(Spoustova?
et al (2009) report 97.4%).
Since it istrained without any external lexicon, MEltALLen isvery close to the original maximum-entropy basedtagger (Ratnaparkhi, 1996), which has indeed asimilar 96.6% accuracy.We trained MElt on increasingly larger parts ofthe POS-tagged Penn Treebank,2 thus creating dif-ferent taggers with growing degrees of accuracy(see table 1).
We then POS-tagged the Section 23with each of these taggers, thus obtaining for eachsentence in Section 23 a set of pre-annotations,one from each tagger.Tagger Nb train.
sent.
Nb tokens Acc.
(%)MElt10en 10 189 66.5MElt50en 50 1,254 81.6MElt100en 100 2,774 86.7MElt500en 500 12,630 92.1MElt1000en 1,000 25,994 93.6MElt5000en 5,000 126,376 95.8MElt10000en 10,000 252,416 96.2MEltALLen 37,990 944,859 96.4Table 1: Accuracy of the created taggers evaluatedon Section 23 of the Penn Treebank3.2 ExperimentsWe designed different experimental setups toevaluate the impact of pre-annotation and pre-annotation accuracy on the quality of the resultingcorpus.
The subparts of Section 23 that we usedfor these experiments are identified by sentenceids (e.g., 1?100 denotes the 100 first sentences inSection 23).Two annotators were involved in the experi-ments.
They both have a good knowledge of lin-guistics, without being linguists themselves andhad only little prior knowledge of the Penn Tree-bank POS tagset.
One of them had previous exper-tise in POS tagging (Annotator1).
It should alsobe noticed that, though they speak fluent English,they are not native speakers of the language.
Theywere asked to keep track of their annotation time,noting the time it took them to annotate or correcteach series of 10 sentences.
They were also askedto use only a basic text editor, with no macro orspecific feature that could help them, apart from2More precisely, MEltien is trained on the i first sentencesof the overall training corpus, i.e.
Sections 2 to 21.the usual ones, like Find, Replace, etc.
The setof 36 tags used in the Penn Treebank and quitea number of particular cases is a lot to keep inmind.
This implies a heavy cognitive load in short-term memory, especially as no specific interfacewas used to help annotating or correcting the pre-annotations.It was demonstrated that training improvesthe quality of manual annotation in a significantway as well as allows for a significant gain intime (Marcus et al, 1993; Dandapat et al, 2009;Mikulova?
and S?te?pa?nek, 2009).
In particular, Mar-cus et al (1993) observed that it took the PennTreebank annotators 1 month to get fully efficienton the POS-tagging correction task, reaching aspeed of 20 minutes per 1,000 words.
The speed ofannotation in our experiments cannot be comparedto this, as our annotators only annotated and cor-rected small samples of the Penn Treebank.
How-ever, the annotators?
speed and correctness didimprove with practice.
As explained below, wetook this learning curve into account, as previouswork (Rehbein et al, 2009) showed it has an sig-nificant impact on the results.Also, during each experiment, sentences wereannotated sequentially.
Moreover, the experimentswere conducted in the order we describe them be-low.
For example, both annotators started theirfirst annotation task (sentences 1?100) with sen-tence 1.We conducted the following experiments:1.
Impact of the pre-annotation accuracy onprecision and inter-annotator agreement:In this experiment, we used sentences 1?400 with random pre-annotation: for eachsentence, one pre-annotation is randomlyselected among its possible pre-annotations(one for each tagger instance).
The aim ofthis is to eliminate the bias caused by the an-notators?
learning curve.
Annotation time foreach series of 10 consecutive sentences wasgathered, as well as precision w.r.t.
the refer-ence and inter-annotator agreement (both an-notators annotated sentences 1?100 and 301?400, while only one annotated 101?200 andthe other 201?300).2.
Impact of the pre-annotation accuracy onannotation time: This experiment is basedon sentences 601?760, with pre-annotation.We divided them in series of 10 sentences.58For each series, one pre-annotation is se-lected (i.e., the pre-annotation produced byone of the 8 taggers), in such a way that eachpre-annotation is used for 2 series.
We mea-sured the manual annotation time for each se-ries and each annotator.3.
Bias induced by pre-annotation: In thisexperiment, both annotators annotated sen-tences 451?500 fully manually.3 Later,they annotated sentences 451?475 with thepre-annotation from MEltALLen (the best tag-ger) and sentences 476?500 with the pre-annotation from MElt50en (the second-worsttagger).
We then compared the fully man-ual annotations with those based on pre-annotations to check if and how they divergefrom the Penn Treebank ?gold-standard?
; wealso compared annotation times, in order toget a confirmation of the gain in time ob-served in previous experiments.4 Results and Discussion4.1 Impact of the Pre-annotation Accuracyon Precision and Inter-annotatorAgreementThe quality of the annotations created during ex-periment 1 was evaluated using two methods.First, we considered the original Penn Treebankannotations as reference and calculated a simpleprecision as compared to this reference.
Figure 1gives an overview of the obtained results (note thatthe scale is not regular).However, this is not sufficient to evaluate thequality of the annotation as, actually, the referenceannotation is not perfect (see below).
We thereforeevaluated the reliability of the annotation, calcu-lating the inter-annotator agreement between An-notator1 and Annotator2 on the 100-sentence se-ries they both annotated.
We calculated this agree-ment on some of the subcorpora using pi, aka Car-letta?s Kappa (Carletta, 1996)4.
The results of thisare shown in table 2.3During this manual annotation step (with no pre-annotation), we noticed that the annotators used theFind/Replace all feature of the text editor to fastenthe tagging of some obvious tokens like the or Corp., whichpartly explains that the first groups of 10 sentences tooklonger to annotate.
Also, as no specific interface was use tohelp annotating, a (very) few typographic errors were made,such as DET instead of DT.4For more information on the terminology issue, refer tothe introduction of (Artstein and Poesio, 2008).Subcorpus pi1-100 0.955301-400 0.963Table 2: Inter-annotator agreement on subcorporaThe results show a very good agreement accord-ing to all scales (Krippendorff, 1980; Neuendorf,2002; Krippendorff, 2004) as pi is always superiorto 0.9.
Besides, it improves with training (from0.955 at the beginning to 0.963 at the end).We also calculated pi on the corpus we used toevaluate the pre-annotation bias (Experiment 3).The results of this are shown in table 3.Subcorpus Nb sent.
piNo pre-annotation 50 0.947MElt50en 25 0.944MEltALLen 25 0.983Table 3: Inter-annotator agreement on subcorporaused to evaluate biasHere again, the results are very good, though alittle bit less so than at the beginning of the mixedannotation session.
They are almost perfect withMEltALLen .Finally, we calculated pi throughout Experi-ment 2.
The results are given in Figure 2 and,apart from a bizarre peak at MElt50en, they show asteady progression of the accuracy and the inter-annotator agreement, which are correlated.
As forthe MElt50en peak, it does not appear in Figure 1, wetherefore interpret it as an artifact.4.2 Impact of the Pre-annotation Accuracyon Annotation TimeBefore discussing the results of Experiment 2, an-notation time measurements during Experiment 3confirm that using a good quality pre-annotation(say, MEltALLen ) strongly reduces the annotationtime as compared with fully manual annotation.For example, Annotator1 needed an average timeof approximately 7.5 minutes to annotate 10 sen-tences without pre-annotation (Experiment 3),whereas Experiment 2 shows that it goes down toapproximately 2.5 minutes when using MEltALLenpre-annotation.
For Annotator2, the correspond-ing figures are respectively 11.5 and 2.5 minutes.Figure 3 shows the impact on the pre-annotationtype on annotation times.
Surprisingly, only theworst tagger (MElt10en) produces pre-annotationsthat lead to a significantly slower annotation.
In59Figure 1: Accuracy of annotationother words, a 96.4% accurate pre-annotation doesnot significantly speed up the annotation processwith respect to a 81.6% accurate pre-annotation.This is very interesting, since it could mean thatthe development of a POS-annotated corpus for anew language with no POS tagger could be drasti-cally sped up.
Annotating approximately 50 sen-tences could be sufficient to train a POS taggersuch as MElt and use it as a pre-annotator, eventhough its quality is not yet satisfying.One interpretation of this could be the follow-ing.
Annotation based on pre-annotations involvestwo different tasks: reading the pre-annotated sen-tence and replacing incorrect tags.
The readingtask takes a time that does not really depends onthe pre-annotation quality.
But the correction tasktakes a time that is, say, linear w.r.t.
the num-ber of pre-annotation errors.
Therefore, when thenumber of pre-annotation errors is below a cer-tain level, the correction task takes significantlyless time than the reading task.
Therefore, be-low this level, variations in the pre-annotation er-ror rate do not lead to significant overall annota-tion time.
Apparently, this threshold is between66.5% and 81.6% pre-annotation accuracy, whichcan be reached with a surprisingly small trainingcorpus.4.3 Bias Induced by Pre-annotationWe evaluated both the bias induced by a pre-annotation with the best tagger, MEltALLen , and theone induced by one of the least accurate taggers,MElt50en.
The results are given in table 4 and 5, re-spectively.They show a very different bias according tothe annotator.
Annotator2?s accuracy raises from94.6% to 95.2% with a 81.6% accuracy tagger(MElt50en) and from 94.1% to 97.1% with a 96.4%accuracy tagger (MEltALLen ).
Therefore, Annota-tor2, whose accuracy is less than that of Annota-tor1 under all circumstances (see figure 1), seemsto be positively influenced by pre-annotation,whether it be good or bad.
The gain is howevermuch more salient with the best pre-annotation(plus 3 points).As for Annotator1, who is the most accurate an-notator (see figure 1), the results are more surpris-ing as they show a significant degradation of ac-curacy, from 98.1 without pre-annotation to 95.8with pre-annotation using MElt50en, the less accu-rate tagger.
Examining the actual results allowedus to see that, first, Annotator1 non pre-annotatedversion is better than the reference, and second,the errors made in the pre-annotated version withMElt50en are so obvious that they can only be due toa lapse in concentration.The results, however, remain stable with pre-annotation using the best tagger (from 98.4 to98.2), which is consistent with the results obtainedby Dandapat et al (2009), who showed that bet-ter trained annotators are less influenced by pre-annotation and show stable performance.When asked about it, both annotators saythey felt they concentrated more without pre-60Figure 2: Annotation accuracy and pi depending on the type of pre-annotationAnnotator No pre-annotation with MEltALLenAnnotator1 98.4 98.2Annotator2 94.1 97.1Table 4: Accuracy with or without pre-annotationwith MEltALLen (sentences 451-475)Annotator No pre-annotation with MElt50enAnnotator1 98.1 95.8Annotator2 94.6 95.2Table 5: Accuracy with or without pre-annotationwith MElt50en (sentences 476-500)annotation.
It seems that the rather good resultsof the taggers cause the attention of the annotatorsto be reduced, even more so as the task is repeti-tive and tedious.
However, annotators also had thefeeling that fully manual annotation could be moresubject to oversights.These impressions are confirmed by the com-parison of the contingency tables, as can be seenfrom Tables 6, 7 and 8 (in these tables, lines cor-respond to tags from the annotation and columnsto reference tags; only lines containing at leastone cell with 2 errors or more are shown, withall corresponding columns).
For example, Anno-tator1 makes more random errors when no pre-annotation is available and more systematic er-rors when MEltALLen pre-annotations are used (typ-ically, JJ instead of VBN, i.e., adjective instead ofpast participle, which corresponds to a systematictrend in MEltALLen ?s results).JJ VBNJJ 36 4(Annotator 1)JJ NN NNP NNPS VB VBNJJ 36 4NN 1 68 2NNP 24 2(Annotator 2)Table 6: Excerpts of the contingency tables forsentences 451?457 (512 tokens) with MEltALLenpre-annotationIN JJ NN NNP NNS RB VBD VBNJJ 30 2 2NNS 1 2 40RB 2 16VBD 1 17 2WDT 2(Annotator 1)JJ NN RB VBNJJ 28 3NN 2 75 1RB 2 16VBN 2 10(Annotator 2)Table 7: Excerpts of the contingency tables forsentences 476?500 (523 tokens) with MElt50en pre-annotation61Figure 3: Annotation time depending on the type of pre-annotationCD DT JJ NN NNP NNSCD 30 2JJ 2 72NN 2 148NNS 3 68(Annotator 1)CD DT IN JJ JJR NN NNP NNS RB VBNIN 104 2JJ 2 61 2 1 9NN 1 4 145NNPS 2NNS 1 2 68RBR 2(Annotator 2)Table 8: Excerpts of the contingency tables forsentences 450?500 (1,035 tokens) without pre-annotation5 Conclusion and Further WorkThe series of experiments we detailed in this arti-cle confirms that pre-annotation allows for a gainin quality, both in terms of accuracy w.r.t.
a ref-erence and in terms of inter-annotator agreement,i.e., reliability.
We also demonstrated that thiscomes with biases that should be identified andnotified to the annotators, so that they can be extracareful during correction.
Finally, we discoveredthat a surprisingly small training corpus could besufficient to build a pre-annotation tool that wouldhelp drastically speeding up the annotation.This should help developing taggers for under-resourced languages.
In order to check that, weintend to use this method in a near future to de-velop a POS tagger for Sorani Kurdish.We also want to experiment on other, moreprecision-driven, annotation tasks, like complexrelations annotation or definition segmentation,that are more intrinsically complex and for whichthere exist no automatic tool as accurate as forPOS tagging.AcknowledgmentsThis work was partly realized as part of theQuaero Programme5, funded by OSEO, FrenchState agency for innovation.ReferencesBeatrice Alex, Claire Grover, Barry Haddow, MijailKabadjov, Ewan Klein, Michael Matthews, Stu-art Roebuck, Richard Tobin, and Xinglong Wang.2008.
Assisted Curation: Does Text Mining ReallyHelp?
In Pacific Symposium on Biocomputing.Ron Artstein and Massimo Poesio.
2008.
Inter-coderAgreement for Computational Linguistics.
Compu-tational Linguistics, 34(4):555?596.Lucie Barque, Alexis Nasr, and Alain Polgue`re.
2010.From the Definitions of the Tre?sor de la LangueFranc?aise to a Semantic Database of the French Lan-guage.
In Proceedings of the 14th EURALEX Inter-national Congress, Leeuwarden.Jean Carletta.
1996.
Assessing Agreement on Classi-fication Tasks: the Kappa Statistic.
ComputationalLinguistics, 22:249?254.5http://quaero.org/62Sandipan Dandapat, Priyanka Biswas, Monojit Choud-hury, and Kalika Bali.
2009.
Complex LinguisticAnnotation - No Easy Way Out!
a Case from Banglaand Hindi POS Labeling Tasks.
In Proceedings ofthe third ACL Linguistic Annotation Workshop.Pascal Denis and Beno?
?t Sagot.
2009.
Coupling an An-notated Corpus and a Morphosyntactic Lexicon forState-of-the-art POS Tagging with Less Human Ef-fort.
In Proceedings of PACLIC 2009, Hong-Kong,China.Kare?n Fort, Maud Ehrmann, and Adeline Nazarenko.2009.
Vers une me?thodologie d?annotation desentite?s nomme?es en corpus ?
In Actes de la16e`me Confe?rence sur le Traitement Automatiquedes Langues Naturelles 2009 Traitement Automa-tique des Langues Naturelles 2009, Senlis, France.Klaus Krippendorff, 1980.
Content Analysis: An Intro-duction to Its Methodology, chapter 12.
Sage, Bev-erly Hills, CA.Klaus Krippendorff, 2004.
Content Analysis: An In-troduction to Its Methodology, second edition, chap-ter 11.
Sage, Thousand Oaks, CA.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Marie Mikulova?
and Jan S?te?pa?nek.
2009.
AnnotationQuality Checking and its Implications for Designof Treebank (in Building the Prague Czech-EnglishDependency Treebank).
In Proceedings of the EightInternational Workshop on Treebanks and LinguisticTheories, volume 4-5, Milan, Italy, December.Kimberly Neuendorf.
2002.
The Content AnalysisGuidebook.
Sage, Thousand Oaks CA.Adwait Ratnaparkhi.
1996.
A Maximum EntropyModel for Part-Of-Speech Tagging.
In Proceedingsof International Conference on Empirical Methodsin Natural Language Processing, pages 133?142.Ines Rehbein, Josef Ruppenhofer, and CarolineSporleder.
2009.
Assessing the Benefits of PartialAutomatic Pre-labeling for Frame-semantic Anno-tation.
In Proceedings of the Third Linguistic Anno-tation Workshop, pages 19?26, Suntec, Singapore,August.
Association for Computational Linguistics.Drahom?
?ra ?Johanka?
Spoustova?, Jan Hajic?, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervisedTraining for the Averaged Perceptron POS Tagger.In EACL ?09: Proceedings of the 12th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 763?771, Morristown,NJ, USA.63
