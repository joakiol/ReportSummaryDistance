Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 49?56Manchester, August 2008Enhancing Multilingual Latent Semantic Analysiswith Term Alignment InformationBrett W. BaderComputer Science &Informatics DepartmentSandia National LaboratoriesP.
O.
Box 5800, MS 1318Albuquerque, NM 87185-1318, USAbwbader@sandia.govPeter A. ChewCognitive Systems Research &Applications DepartmentSandia National LaboratoriesP.
O.
Box 5800, MS 1011Albuquerque, NM 87185-1011, USApchew@sandia.govAbstractLatent Semantic Analysis (LSA) isbased on the Singular Value Decompo-sition (SVD) of a term-by-documentmatrix for identifying relationshipsamong terms and documents from co-occurrence patterns.
Among the multi-ple ways of computing the SVD of arectangular matrix X, one approach is tocompute the eigenvalue decomposition(EVD) of a square 2 ?
2 composite ma-trix consisting of four blocks with X andXT in the off-diagonal blocks and zeromatrices in the diagonal blocks.
Wepoint out that significant value can beadded to LSA by filling in some of thevalues in the diagonal blocks (corre-sponding to explicit term-to-term ordocument-to-document associations)and computing a term-by-concept ma-trix from the EVD.
For the case of mul-tilingual LSA, we incorporateinformation on cross-language termalignments of the same sort used in Sta-tistical Machine Translation (SMT).Since all elements of the proposedEVD-based approach can rely entirelyon lexical statistics, hardly any price ispaid for the improved empirical results.In particular, the approach, like LSA orSMT, can still be generalized to virtu-ally any language(s); computation of theEVD takes similar resources to that ofthe SVD since all the blocks are sparse;?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.and the results of EVD are just as eco-nomical as those of SVD.1 IntroductionIt is close to two decades now since Deerwesteret al (1990) first proposed the application of theSingular Value Decomposition (SVD) to term-by-document arrays as a statistics-based way ofrepresenting how terms and documents fit to-gether within a semantic space.
Since the ap-proach was supposed to ?get beyond?
the termsthemselves to their underlying semantics, theapproach became known as Latent SemanticAnalysis (LSA).Soon after this application of SVD was widelypublicized, it was suggested by Berry et al(1994) that, with a parallel corpus, the approachcould be extended to pairs of languages to allowcross-language information retrieval (IR).
It hassince been confirmed that LSA can be appliednot just to pairs of languages, but also simultane-ously to groups of languages, again given theexistence of a multi-parallel corpus (Chew andAbdelali 2007).In this paper, we return to the basics of LSAby examining its relationship with SVD, and, inturn, the mathematical relationship of SVD to theeigenvalue decomposition (EVD).
These detailsare discussed in section 2.
It has previously beensuggested (for example, in Hendrickson 2007)that IR results could be improved by filling ininformation beyond that available directly in theterm-by-document matrix, and replacing SVDwith the more general EVD.
To our knowledge,however, these suggestions have not been publi-cized outside the mathematics community, norhave they been empirically tested in IR applica-tions.
With multilingual information retrieval asa use case, we consider alternatives in section 3for implementation of this idea.
One of these re-49lies on no extraneous information beyond what isalready available in the multi-parallel corpus, andis based entirely on the statistics of cross-language term alignments.
?Regular?
LSA hasbeen shown to work best when a weightingscheme such as log-entropy is applied to theelements in the term-by-document array (Dumais1991), and in section 3 we also consider variouspossibilities for how the term alignments shouldbest be weighted.
Section 4 recapitulates on aframework that allows EVD with term align-ments to be compared with a number of relatedapproaches (including LSA without term align-ments).
This is a recapitulation, because the sametesting framework has been used previously (forother linear-algebra based approaches) by Chewand Abdelali (2007) and Chew et al (2007).
Theresults of our comparison are presented and dis-cussed in section 5, and we conclude upon theseresults and suggest further avenues for researchin section 6.2 The relationship of SVD to EVD, andits application to information retrievalIn the standard LSA framework (Deerwester etal.
1990) the (sparse) term-by-document matrixX is factorized by the singular value decomposi-tion (SVD),X = USV T (1)where U is an orthonormal matrix of left singularvectors, S is a diagonal matrix of singular values,and V is an orthonormal matrix of right singularvectors (Golub and van Loan 1996).Typically for LSA, a truncated SVD is com-puted such that equality in (1) no longer holdsand that the best rank-R least-squares approxima-tion to matrix X is formed by keeping the R larg-est singular values in S and discarding the rest.This also means that the first R vectors of U andV are retained, where R indicates the number ofconcept dimensions in LSA.
Each column vectorin U maps the terms to a single arbitrary concept,such that terms which are semantically related(as determined by patterns of co-occurrence) willtend to be grouped together with large values incolumns of U.There are many ways to compute the SVD of asparse matrix.
One expedient way is to computethe eigenvalue decomposition (EVD) of eitherXTX or XXT, depending on the largest di-mension of X, to obtain U or V, respectively.With U or V, one may compute the rest of theSVD by a simple matrix-matrix multiplicationand renormalization.Another way to compute the SVD is to com-pute the eigenvalue decomposition of the 2-by-2block matrixB = 0 XXT 0	.The eigenvalues of B are the singular values ofX, replicated as both positive and negative, plusa number of zeroes if X is not square.
The leftand right singular vectors are contained withinthe eigenvectors of this composite matrix B. As-sume that X is of size m ?
n and that mn, withleft singular vectors U = Un Umn[ ], where Uncorresponds to the n positive singular values andUm-n corresponds to the remaining m-n zero sin-gular values.
Let Q denote the orthogonal matrixof eigenvectors corresponding to the nonnegativeeigenvalues of B, then the matrices of left andright singular vectors are stacked on top of eachother, U on top of V, as follows:Q = 12Un 2 ?UmnV 0	.Hence, one may compute the truncated SVD ofX by computing only the eigenvectors corre-sponding to the largest R eigenvalues and thenextracting and rescaling the U and V matricesfrom Q.Figure 1.
Eigenvalue decomposition in multilin-gual information retrievalIn the context of multilingual LSA using aparallel corpus, the block matrix B is depicted inFigure 1, where the terms are shaded accordingto each language.
Each language may have a dif-ferent number of terms, so the language blocksare not expected to be the same size as one an-other.
The eigenvectors and eigenvalues of B arealso shown.We may obtain a pair of U and S matrices foreach language by extracting the correspondingpartition of U from the eigenvectors.
We desireeach language-specific U matrix to have columnsof unit length, which we accomplish by comput-ing the length of each of its columns and then50rescaling the columns of U by the inverse lengthand multiplying the eigenvalues by these lengthsfor our S matrix.
We call this approach ?Tucker1?because the result is identical to creating a U andS matrix for each language from the generalTucker1 model found by three-way analysis ofthe terms-by-documents-by-language array(Tucker 1966).For applications in information retrieval, weusually want to compute a measure of similaritybetween documents.
Once we have U and S, wecan estimate similarities by computing the cosineof the angle between the document vectors in thesmaller ?semantic space?
of the R concepts foundby LSA.
New documents in different languagescan be projected into this common semanticspace by multiplying their document vectors(formed in exactly the same way as the columnsfor X) by the product US-1, to yield a document-by-concept vector.3 From SVD to term-alignment-basedEVDIf we compute just the SVD of a term-documentmatrix X, then the technique we use to accom-plish this (whether computing the EVD of theblock matrix B or otherwise) is immaterial froma computational linguist?s point of view: there isno advantage in one technique over another.However, the technique of EVD allows one toaugment the LSA framework with additional in-formation beyond just the term-document matrix.In Figure 1, the two diagonal blocks contain onlyzeroes, but we envision augmenting B with termalignment information such that the upper diago-nal block captures any term-to-term similarities.Additional term-term alignment informationserves to enhance the term-by-concept vectors inU by providing explicit, external knowledge sothat LSA can learn more refined concepts.
Whilenot explored in this paper, we also envision in-corporating any document-to-document similari-ties into the lower diagonal block.Let D1 and D2 denote symmetric matrices.
Weaugment the block matrix B and redefine it as amore general symmetric matrix,B = D1 XXT D2	.If both D1 and D2 are equal to the identity matrix,then the eigenvalues of B are shifted by one, butthe eigenvectors are not affected.Since our use case here is multilingual infor-mation retrieval, imagine for the moment that anoracle provides dictionary information thatmatches up words in each of our language pairs(Arabic-English, Arabic-French, etc.)
by mean-ing.
Thus, for example, we might have a pairingbetween English house and French maison.
Thisinformation may be encoded in the diagonalblock D1 by replacing zeroes in the cells for(house, maison) and its symmetric entry withsome nonzero value indicating the strength ofassociation for the two terms.
Completing allrelevant entries in D1 in this fashion serves tostrengthen the co-occurrence information in theparallel corpus that LSA normally finds via theSVD.In the simplest approach, if the oracle indi-cates a match between two terms i and j, then aone could be inserted in D1 at positions (i,j) and(j,i).
If D1 were filled with such term alignmentinformation, the matrix B would still be sparse.Without any document-document information,then D2 could be either the identity matrix or thezero matrix.
Our experience has shown that D2 =0 works slightly better in practice.
Figure 2shows a block matrix augmented with termalignments in this fashion.Figure 2.
Augmented block matrix with termalignmentsThe eigenvalue decomposition of B now in-corporates this extra term information providedin D1, and the eigenvectors show stronger corre-spondence between those terms indicated.
How-ever, with each term aligned with one or moreother terms, the row and column norms of D1 areunequal, which means that some terms may bebiased to appear more heavily in the eigenvec-tors.
In addition, the magnitude or ?weight?
of D1relative to X needs to be considered, otherwisethe explicit alignments in D1 and the co-51occurrence information in X may be out of bal-ance with one another.
Properly normalizing andscaling D1 may mitigate both of these risks.There are several possibilities for normalizingthe matrix D1.
Sinkhorn balancing (Sinkhorn1964) is a popular technique for creating a dou-bly stochastic matrix (rows and columns all sumto 1) from a square matrix of nonnegative ele-ments.
Sinkhorn balancing is an iterative algo-rithm in which, at each step, the row and columnsums are computed and then subsequently usedto rescale the matrix.
For balancing the matrix A,each iteration consists of two updatesA  WRAA  AWCwhere WR is a diagonal matrix containing theinverse of row sums of A, and WC is a diagonalmatrix containing the inverse of column sums ofA.
This algorithm exhibits linear convergence, somany iterations may be needed.
The algorithmmay be adapted for normalizing the row and col-umn vectors according to any norm.
Our experi-ence has shown that normalizing D1 with respectto the Euclidean norm works well in practice.In terms of scaling D1 relative to X, we simplymultiply D1 by a positive scalar value, which wedenote with the variable .
The optimal value of appears to be problem dependent.Let us return for the moment to the question ofhow we populate D1 in the first place, and whateach entry in that block represents.
In the simplecase described above, the existence of a 1 at po-sition (i,j) indicates that an alignment exists be-tween terms i and j, and a zero indicates that noalignment exists.
But in reality, a binary encod-ing like this may be too simplistic.
In this re-spect, it is instructive to consider how wepopulate D1 in the light of the weighting schemeused for X, since the latter is discussed in Du-mais (1991) and is by now quite well understood.In the simplest case, an entry of 1 in X at posi-tion (i,j) can denote that term i occurs in docu-ment j, just as in our simple case with D1.
Aslightly more refined alternative is to replace 1with fi,j, where fi,j denotes the raw frequency ofterm i within document j.
But, as Dumais (1991)shows, it is significantly better in practice to usea ?log-entropy?
weighting scheme.
This adjustsfi,j first by ?dampening?
high-frequency terms(using the log of the frequency), and secondly bygiving a lower weight to terms which occur inmany documents.2 The former adjustment is re-2 One can also raise the global weight in the log-entropyscheme to a power (which we denote with the variable ).lated to an insight from Zipf?s law, which is thatthe dampened term frequency will be in propor-tion to the log of the term?s rank in frequency.The latter adjustment is based on informationtheory; a term which is scattered across manydocuments (such as ?and?
in English) has a highentropy, and therefore lower intrinsic informa-tion content.Suppose, therefore, that our ?dictionary?
oraclecould not only indicate the existence of analignment, but also provide some numericalvalue for the strength of association between twoaligned terms.
(In practice, this is probably morethan one could hope for even from the best pub-lished bilingual dictionaries.)
This informationcould then replace the ones in D1 prior to Sink-horn balancing and matrix weighting.While one cannot expect to obtain this infor-mation from published dictionaries, there is infact a statistical approach to gathering the neces-sary information, which we borrow from SMT(Brown et al 1994).
All that is required is theexistence of a parallel corpus, which we alreadyhave in place for multilingual LSA.Here, an entry fi,j in D1 is based on the mutualinformation of term I and term J, or I(I;J) (capi-tals are used to indicate that the terms are treatedhere as random variables).
It is an axiom that:I(I;J) = H(I) + H(J) ?
H(I,J) (2)where H(I) and H(J) are the marginal entropiesof I and J respectively, and H(I,J) is the joint en-tropy of I and J.
Properties of H(I,J) include thefollowing:H(I,J)H(I)0H(I,J)H(J)0H(I,J)  H(I) + H(J) (3)Considering (2) and (3) together, it should beclear that I(I;J) will range between 0 and themaximum value for H(I) or H(J).For the purposes of populating D1, we com-pute the entropy of a term i by considering thenumber of documents where i occurs, and thenumber of documents where i does not occur,and express these as probabilities.
For the jointentropy H(I,J), we need to compute four prob-abilities based on all the possibilities: documentswhere both terms occur, those where I occurswithout J, those where J occurs without I, andSelecting   1 can, in practice, yield better results in theapplications we have tested.52those where neither occur.
The result of this isthat a numerical value is attached to each align-ment: higher values indicate that terms arestrongly correlated, and lower values indicatethat one term predicts little about the other.
Foreach pair of words (i,j) which co-occur in anytext chunk in the parallel corpus, we can say thatan alignment exists if, among all the possibilities,mutual information for i is maximized by select-ing j, and vice versa.
(Since the maximization ofmutual information is not necessarily reciprocal,the effect of this is to be conservative in postulat-ing alignments.)
The weight of this alignment isits mutual information (equivalent to the ?globalweight?
of log-entropy) multiplied by the log ofone plus the number of text chunks in which thatalignment appears (equivalent to the ?localweight?
of log-entropy).Some examples of English-French pairs at ei-ther end of this spectrum (where mutual informa-tion is non-zero) are given in Table 1.I(I;J) AlignmentweightI J0.000176 0.000176 hearing ?coutait0.000217 0.000217 misery mis?rable?0.270212 2.884297 house maison0.321754 3.506663 king roi0.415702 6.025456 and et0.472925 5.798080 I jeTable 1.
Term alignment and mutual informationWe believe that this approach, which weightsalignments based on mutual information, fitsvery well with the log-entropy scheme used forX, since both are solidly based on the samefoundation of information theory.All together, we call this particular processLSATA, which stands for LSA with term align-ments.4 Testing frameworkSince the inception of the Cross-LanguageEvaluation Forum (CLEF) in 2000, there hasbeen growing interest in cross-language IR, and anumber of parallel corpora have become avail-able (for example through the Linguistic DataConsortium).
Widely used examples include theCanadian Hansard parliament proceedings (inFrench and English).
Harder to obtain are multi-parallel corpora ?
those where the same text istranslated into more than two parallel languages.One such corpus which has not yet gainedwide acceptance, perhaps owing to the percep-tion that it has less relevance to real-world appli-cations than other parallel corpora, is the Bible.Yet the range of languages covered is unarguablyunmatched elsewhere, and one might contendthat its relevance is in some ways greater than,say, Hansard?s, as its impact on Western culturehas been broader than that of Canadian govern-ment debates.
Similarly, the Quran, while nottranslated into as many languages as the Bible,has had a significant impact on another largesegment of the world?s population.But the relevance or otherwise of the Bibleand/or Quran, and the extent to which they havebeen accepted by the computational linguisticscommunity at large as parallel corpora, are some-what beside the point for us here.
Our interest isin developing theory and applications whichhave universal applicability to as many lan-guages as possible, regardless of the subject mat-ter or whether the languages are ancient ormodern.
One might compare this approach toChomsky?s quest for Universal Grammar(Chomsky 1965), except that the theory in ourcase is based on lexical statistics and linear alge-bra rather than rule-based generative grammar.The Bible and Quran have in fact previouslybeen used for experiments similar to ours (e.g.,Chew et al 2007).
By using these texts as paral-lel corpora, therefore, we facilitate direct com-parison of our results with previous ones.
Butbesides this, the Bible has some especially attrac-tive properties for our current purposes.
First, thecarefulness of the translations means that we arerelatively unlikely to encounter situations wherecross-language term alignments are impossiblebecause some text is missing in one of the trans-lations.
Secondly, the relatively small size of theparallel text chunks (by and large, each chunk isa verse, most of which are about a sentence inlength) greatly facilitates the process of statisticalterm alignment.
(This is based on the combina-torics: the number of possible term-to-termalignments increases approximately quadraticallywith the number of terms per text chunk.
)Thus, our framework is as follows.
In ourterm-by-document matrix X, the documents areverses, and the terms are distinct wordforms inany of the five languages used in the test data inChew et al (2007): Arabic (AR), English (EN),French (FR), Russian (RU) and Spanish (ES).
Asin Chew et al (2007), too, our test data consistsof the text of the Quran in the same 5 languages.In this case, the ?documents?
are the 114 parallelsuras (or chapters) of the Quran.
We obtained alltranslations of the Bible and Quran from openly-53available websites such as that of Biola Univer-sity (2005-2006) and http://www.kuran.gen.tr.As already mentioned, SVD of a term-by-document matrix is equivalent to EVD of a blockmatrix in which two of the blocks (the non-diagonal ones) are X and XT.
As described insection 3, we fill in some of the values of D1 withnonzeroes (from term alignments derived fromthe Bible).
In all cases (both SVD and EVD), weperformed a truncated decomposition in either60, 240, or 300 dimensions.Term alignmentsettingsSVD/EVDdimensionsType ofdecompositionInclude termalignments?
/weighting type Sinkhornbalanced?Globalweight*AverageP1AverageMP5SVD 0.7116 0.5702Tucker1 0.7170 0.5770PARAFAC2N/A 1.80.7420 0.6580no N/A 0.7000 0.56914.0 1.8 0.7611 0.64741.0 0.7716 0.5972 yes (binary) yes4.0 1.6 0.7979 0.6467no N/A 0.6481 0.38041.0 0.7393 0.597212.01.80.8088 0.69721.0 0.7488 0.578960LSATAyes (log-MI) yes12.0 1.6 0.7933 0.6586SVD 0.8761 0.6554 240 PARAFAC2 N/A 1.8 0.8975 0.7853SVD N/A 1.8 0.8796 0.6575yes (binary) 4.0 1.6 0.9421 0.76951.8 0.8982 0.8000 300 LSATA yes (log-MI) yes 12.0 1.6 0.9182 0.8067*See footnote 2.Table 2.
Results with various linear algebraic decomposition methods and weighting schemesTo evaluate the different methods against oneanother, we use similar measures of precision aswere used with the same dataset by Chew et al(2007): precision at 1 document (P1) (the aver-age proportion of cases where the translation of adocument ranked highest among all retrieveddocuments of the same language) and multilin-gual precision at 5 documents (MP5) (the aver-age proportion of the top 5 ranked documentswhich were translations of the query documentinto any of the 5 languages, among all retrieveddocuments of any language).
By definition, MP5is always less than or equal to P1; MP5 measuressuccess in multilingual clustering, while P1measures success in retrieving documents whenthe source and target languages are pre-specified.5 Results and DiscussionTable 2 above presents a summary of our results.The main point to note is that the addition of in-formation on term alignments is clearly benefi-cial.
An approach based on the Tucker1decomposition algorithm, without any informa-tion on term alignments, achieves P1 of 0.7170and MP5 of 0.5770.
With scaled term alignmentinformation, the results improve to 0.7611 and0.6474, respectively.
Using a chi-squared test,we tested the significance of the increase in P1and found it to be highly significant (p  1.7 ?10-7).The results also show, however, that one needsto be careful about how the word-alignment in-formation is added.
Without some form of bal-ancing and scaling of D1, there is littleimprovement (and often significant deterioration)in the results when alignment information is in-cluded.In addition to comparing a block EVD ap-proach with term alignments to one without, wealso compared against another decompositionmethod, PARAFAC2, which has been found tobe more effective than SVD in cross-language IR(Chew et al 2007).
Here, the results are moreequivocal.
P1 is slightly higher under theLSATA approach (with binary values in D1) than54under PARAFAC2, while the reverse is true forMP5.
The difference for P1 is significant at p <0.05 but not at p < 0.01.
In any case, there arerisks in making a comparison betweenPARAFAC2 and LSATA.
For one thing,PARAFAC2, as implemented here, includes nomechanism for incorporating term-alignmentinformation.
It is not clear to us yet whether sucha mechanism could (mathematically or practi-cally) be incorporated into PARAFAC2.
Sec-ondly, we are not yet confident that we havefound the optimal weighting scheme for the D1block under the LSATA model.
Our experimentswith different weighting and normalizationschemes for the D1 block are still in relativelyinitial stages, though it can also be seen fromTable 2 that by selecting certain settings underLSATA (replacing binary weighting in D1 withmutual-information-based weighting, and apply-ing scaling with beta = 12.0), we were able toimprove upon PARAFAC2 under both measures.Although we have not tested all settings, Table2 also shows our best results to date with thisdataset, which have come from applying EVD tothe block matrix that includes D1.
The preciseoptimal settings for EVD appear to depend onwhether the objective is to maximize P1 or MP5.For P1, our best results (0.9421) were obtainedwith binary weighting, global term  = 1.6, and = 4.0.
For MP5, the best results (0.8067) wereobtained with mutual-information based weight-ing,  = 1.8, and  = 12.0.
It appears in all casesthat D1 needs to be balanced if it contains termalignment information.The evidence, then, appears to be strongly infavor of incorporating information beyond term-to-document associations within an IR approachbased on linear algebra.
It happens that LSATAoffers an obvious way to do this, while othermethods such as PARAFAC2 may or may not.Here, we have examined just one form of infor-mation besides term-to-document statistics: term-to-term statistics.
However, there is no reason tosuppose that the results might not be improvedstill further by incorporating information ondocument-to-document associations, or for thatmatter associations between terms or documentsand other linguistic, grammatical, or contextualobjects.6 ConclusionIn this paper, we have discussed the mathe-matical relationship between SVD and EVD, andspecifically the fact that SVD is a special case ofEVD.
For information retrieval, the significanceof this is that SVD allows for explicit encodingof associations between terms and documents,but not between terms and terms, or betweendocuments and documents.By moving from the special case of SVD tothe general case of EVD, however, we open upthe possibility that additional information can beencoded prior to decomposition.
We have exam-ined a particular use case for SVD: multilingualinformation retrieval.
This use case presents aninteresting example of additional informationwhich could be encoded on the term-by-termdiagonal block: cross-language pairings ofequivalent terms (such as house/maison).
Suchpairs can be obtained from bilingual dictionaries,but we can save ourselves the trouble of obtain-ing and using these.
Multilingual LSA requiresthat a parallel corpus have already been obtained,and well-understood statistical term alignmentprocedures can be applied to obtain cross-language term-to-term associations.
Moreover, ifthe corpus is multi-parallel, we can ensure thatthe statistical basis for alignment is the sameacross all language pairs.Our results show that by including term-to-term alignment information, then performingEVD, we can improve the results of cross-language IR quite significantly.It should be pointed out that while we havesuccessfully used statistics-based information inthe term-by-term diagonal block, there is no rea-son to suppose that similar or better results mightnot be achieved by manually filling in nonzeroesin either diagonal block.
The additional informa-tion encoded by these nonzeroes could includeassociations known a priori between documents(e.g., they were written by the same author) orterms (e.g., they occur together in a thesaurus),or both.
While in these examples the additionalinformation required might not be available fromthe training corpus, and its encoding could in-volve moving away from an entirely statistics-based model, the additional effort could be justi-fied depending upon the intended application.In future work, we would like to examine inparticular whether still further statistically-derivable (or readily available) data could be in-corporated into the model.
For example, one canconceive of a block EVD involving ?levels?
be-yond the ?term level?
and the ?document level?.In a 3?3 block EVD, for example, one might in-clude n-grams, terms, and documents; this ap-proach should also be extensible to essentially alllanguages.
Might the addition of further informa-55tion lead to even higher precision?
Avenues forresearch such as this raise their own questions,such as the type of weighting scheme whichwould have to be applied in a 3?3 block matrix.In summary, however, our results give ussome confidence that there can be significantbenefit in making more linguistic and/or statisti-cal information available to linear algebraic IRapproaches such as EVD.
Cross-language termalignments are just one example of the type ofadditional information which could be included;we believe that future research will uncovermany more similar examples.AcknowledgementSandia is a multiprogram laboratory operatedby Sandia Corporation, a Lockheed Martin Com-pany, for the United States Department of En-ergy?s National Nuclear Security Administrationunder contract DE-AC04-94AL85000.ReferencesMichael W. Berry, Susan T.
Dumais., and G. W.O?Brien.
1994.
Using Linear Algebra for Intelli-gent Information Retrieval.
SIAM: Review 37, 573-595.Biola University.
2005-2006.
The Unbound Bible.Accessed at http://www.unboundbible.org/ on Jan.29, 2008.Peter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1994.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics19(2), 263-311.Peter A. Chew and Ahmed Abdelali.
2007.
Benefitsof the ?Massively Parallel Rosetta Stone?
: Cross-Language Information Retrieval with over 30 Lan-guages .
Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics,ACL 2007.
Prague, Czech Republic, June 23?30,2007.
pp.
872-879.Noam Chomsky.
1965.
Aspects of the Theory of Syn-tax.
Cambridge, MA: MIT Press.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K.Landauer and R. Harshman.
1990.
Indexing by La-tent Semantic Analysis.
Journal of the AmericanSociety for Information Science 41:6, 391-407.Susan Dumais.
1991.
Improving the Retrieval of In-formation from External Sources.
Behavior Re-search Methods, Instruments, and Computers23(2):229-236.Gene H. Golub and Charles F. van Loan.
1996.
Ma-trix Computations, 3rd edition.
The Johns HopkinsUniversity Press: London.R.
A. Harshman.
1972.
PARAFAC2: Mathematicaland Technical Notes.
UCLA Working Papers inPhonetics 22, 30-47.Bruce Hendrickson.
2007.
Latent Semantic Analysisand Fiedler Retrieval.
Linear Algebra and its Ap-plications 421 (2-3), 345-355.P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
Proceedings of the JointConference on Human Language Technologies andthe Annual Meeting of the North American Chapterof the Association of Computational Linguistics(HLT/NAACL), 48-54.P.
Koehn.
2002.
Europarl: a Multilingual Corpus forEvaluation of Machine Translation.
Unpublished,accessed on Jan. 29, 2008 at http://www.iccs.inf.ed.ac.uk/~pkoehn/publications/europarl.pdf.Philip Resnik, Mari Broman Olsen, and Mona Diab.1999.
The Bible as a Parallel Corpus: Annotatingthe "Book of 2000 Tongues".
Computers and theHumanities, 33: 129-153.R.
Sinkhorn.
1964.
A Relation between ArbitraryPositive Matrices and Doubly Stochastic Matrices.Annals of Mathematical Statistics 35(2), 876-879.Ledyard R. Tucker.
1966.
Some Mathematical Noteson Three-mode Factor Analysis, Psychometrika 31,279-311.Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha,and C. Lee Giles.
2007.
Co-Ranking Authors andDocuments in a Heterogeneous Network.
SeventhIEEE InternationalConference on Data Mining,739-744.56
