What is the Minimal Set of Fragments that AchievesMaximal Parse Accuracy?Rens BodSchool of ComputingUniversity of Leeds, Leeds LS2 9JT, &Institute for Logic, Language and ComputationUniversity of Amsterdam, Spuistraat 134, 1012 VB Amsterdamrens@comp.leeds.ac.ukAbstractWe aim at finding the minimal set offragments which achieves maximal parseaccuracy in Data Oriented Parsing.
Expe-riments with the Penn Wall StreetJournal treebank show that counts ofalmost arbitrary fragments within parsetrees are important, leading to improvedparse accuracy over previous modelstested on this treebank (a precis -ion of 90.8% and a recall of 90.6%).
Weisolate some dependency relations whichprevious models neglect but whichcontribute to higher parse accuracy.1 IntroductionOne of the goals in statistical natural languageparsing is to find the minimal set of statisticaldependencies (between words and syntacticstructures) that achieves maximal parse accuracy.Many stochastic parsing models use linguisticintuitions to find this minimal set, for example byrestricting the statistical dependencies to thelocality of headwords of constituents (Collins1997, 1999; Eisner 1997), leaving it as an openquestion whether there exist important statisticaldependencies that go beyond linguisticallymotivated dependencies.
The Data OrientedParsing (DOP) model, on the other hand, takes arather extreme view on this issue: given anannotated corpus, all fragments (i.e.
subtrees)seen in that corpus, regardless of size andlexicalization, are in principle taken to form agrammar (see Bod 1993, 1998; Goodman 1998;Sima'an 1999).
The set of subtrees that is used isthus very large and extremely redundant.
Bothfrom a theoretical and from a computationalperspective we may wonder whether it ispossible to impose constraints on the subtreesthat are used, in such a way that the accuracy ofthe model does not deteriorate or perhaps evenimproves.
That is the main question addressed inthis paper.
We report on experiments carried outwith the Penn Wall Street Journal (WSJ)treebank to investigate several strategies forconstraining the set of subtrees.
We found thatthe only constraints that do not decrease the parseaccuracy consist in an upper bound of thenumber of words in the subtree frontiers and anupper bound on the depth of unlexicalizedsubtrees.
We also found that counts of subtreeswith several nonheadwords are important,resulting in improved parse accuracy overprevious parsers tested on the WSJ.2 The DOP1 ModelTo-date, the Data Oriented Parsing model hasmainly been applied to corpora of trees whoselabels consist of primitive symbols (but see Bod& Kaplan 1998; Bod 2000c, 2001).
Let us illus-trate the original DOP model presented in Bod(1993), called DOP1, with a simple example.Assume a corpus consisting of only two trees:NP VPSNPMaryVlikesJohnNP VPSNPVPeterhates SusanFigure 1.
A corpus of two treesNew sentences may be derived by combiningfragments, i.e.
subtrees, from this corpus, bymeans of a node-substitution operation indicatedas ?.
Node-substitution identifies the leftmostnonterminal frontier node of one subtree with theroot node of a second subtree (i.e., the secondsubtree is substituted  on the leftmost nonterminalfrontier node of the first subtree).
Thus a newsentence such as Mary likes Susan  can be derivedby combining subtrees from this corpus:NP VPSNPVlikesNPMaryNPSusan NP VPSNPMary Vlikes Susan=?
?Figure 2.
A derivation for Mary likes SusanOther derivations may yield the same tree, e.g.
:NP VPSNPVNPMary NP VPSNPMary Vlikes Susan=SusanVlikes?
?Figure 3.
Another derivation yielding same treeDOP1 computes the probability of a subtree t asthe probability of selecting t among all corpussubtrees that can be substituted on the same nodeas t. This probability is equal to the number ofoccurrences of t , | t |, divided by the total numberof occurrences of all subtrees t' with the sameroot label as t. Let r(t) return the root label of t.Then we may write:P(t)  =   | t |?
t': r(t')= r(t)  | t' |In most applications of DOP1, the subtreeprobabilities are smoothed by the techniquedescribed in Bod (1996) which is based onGood-Turing.
(The subtree probabilities are notsmoothed by backing off to smaller subtrees,since these are taken into account by the parsetree probability, as we will see.
)The probability of a derivation t1?...
?tn  iscomputed by the product of the probabilities ofits subtrees ti:P(t1?...
?tn)  =  ?i  P(ti)As we have seen, there may be several distinctderivations that generate the same parse tree.
Theprobability of a parse tree T is thus the sum of theprobabilities of its distinct derivations.
Let tid bethe i-th subtree in the derivation d that producestree T, then the probability of T is given byP(T)  =  ?d?i P(tid)Thus the DOP1 model considers counts ofsubtrees of a wide range of sizes in computingthe probability of a tree: everything from countsof single-level rules to counts of entire trees.
Thismeans that the model is sensitive to the frequencyof large subtrees while taking into account thesmoothing effects of counts of small subtrees.Note that the subtree probabilities in DOP1are directly estimated from their relative frequen-cies.
A number of alternative subtree estimatorshave  been proposed for DOP1 (cf.
Bonnema etal 1999), including maximum likelihoodestimation (Bod 2000b).
But since the relativefrequency estimator has so far not been outper -formed by any other estimator for DOP1, wewill stick to this estimator in the current paper.3 Computational IssuesBod (1993) showed how standard chart parsingtechniques can be applied to DOP1.
Each corpus-subtree t is converted into a context-free rule rwhere the lefthand side of r corresponds to theroot label of t  and the righthand side of rcorresponds to the frontier labels of t. Indices linkthe rules to the original subtrees so as to maintainthe subtree's internal structure and probability.These rules are used to create a derivation forestfor a sentence (using a CKY parser), and themost probable parse is computed by sampling asufficiently large number of random derivationsfrom the forest ("Monte Carlo disambiguation",see Bod 1998).
While this technique has beensuccessfully applied to parsing the ATIS portionin the Penn Treebank (Marcus et al 1993), it isextremely time consuming.
This is mainlybecause the number of random derivations thatshould be sampled to reliably estimate the mostprobable parse increases exponentially with thesentence length (see Goodman 1998).
It istherefore questionable whether Bod's samplingtechnique can be scaled to larger domains such asthe WSJ portion in the Penn Treebank.Goodman (1996, 1998) showed how DOP1can be reduced to a compact stochastic context-free grammar (SCFG) which contains exactlyeight SCFG rules for each node in the training settrees.
Although Goodman's method does still notallow for an efficient computation of the mostprobable parse (in fact, the problem of computingthe most probable parse in DOP1 is NP-hard --see Sima'an 1999), his method does allow for anefficient computation of the "maximum constit-uents parse", i.e.
the parse tree that is most likelyto have the largest number of correct constituents.Goodman has shown on the ATIS corpus thatthe maximum constituents parse performs atleast as well as the most probable parse if allsubtrees are used.
Unfortunately, Goodman'sreduction method is only beneficial if indeed allsubtrees are used.
Sima'an (1999: 108) arguesthat there may still be an isomorphic SCFG forDOP1 if the corpus-subtrees are restricted in sizeor lexicalization, but that the number of the rulesexplodes in that case.In this paper we will use Bod's subtree-to-rule conversion method for studying the impactof various subtree restrictions on the WSJcorpus.
However, we will not use Bod's MonteCarlo sampling technique from completederivation forests, as this turned out to beprohibitive for WSJ sentences.
Instead, weemploy a Viterbi n-best search using a CKYalgorithm and estimate the most probable parsefrom the 1,000 most probable derivations,summing up the probabilities of derivations thatgenerate the same tree.
Although this heuristicdoes not guarantee that the most probable parse isactually found, it is shown in Bod (2000a) toperform at least as well as the estimation of themost probable parse with Monte Carlotechniques.
However, in computing the 1,000most probable derivations by means of Viterbi itis prohibitive to keep track of all subderivations ateach edge in the chart (at least for such a largecorpus as the WSJ).
As in most other statisticalparsing systems we therefore use the pruningtechnique described in Goodman (1997) andCollins (1999: 263-264) which assigns a score toeach item in the chart equal to the product of theinside probability of the item and its priorprobability.
Any item with a score less than 10?5times of that of the best item is pruned from thechart.4 What is the Minimal Subtree Set thatAchieves Maximal Parse Accuracy?4.1 The base lineFor our base line parse accuracy, we used thenow standard division of the WSJ (see Collins1997, 1999; Charniak 1997, 2000; Ratnaparkhi1999) with sections 2 through 21 for training(approx.
40,000 sentences) and section 23 fortesting (2416 sentences ?
100 words); section 22was used as development set.
All trees werestripped off their semantic tags, co-referenceinformation and quotation marks.
We used alltraining set subtrees of depth 1, but due tomemory limitations we used a subset of thesubtrees larger than depth 1, by taking for eachdepth a random sample of 400,000 subtrees.These random subtree samples were not selectedby first exhaustively computing the complete setof subtrees (this was computationally prohibit -ive).
Instead, for each particular depth > 1 wesampled subtrees by randomly selecting a nodein a random tree from the training set, after whichwe selected random expansions from that nodeuntil a subtree of the particular depth wasobtained.
We repeated this procedure 400,000times for each depth > 1 and ?
14.
Thus nosubtrees of depth > 14 were used.
This resultedin a base line subtree set of 5,217,529 subtreeswhich were smoothed by the technique describedin Bod (1996) based on Good-Turing.
Since oursubtrees are allowed to be lexicalized (at theirfrontiers), we did not use a separate part-of-speech tagger: the test sentences were directlyparsed by the training set subtrees.
For wordsthat were unknown in our subtree set, weguessed their categories by means of the methoddescribed in Weischedel et al (1993) which usesstatistics on word-endings, hyphenation andcapitalization.
The guessed category for eachunknown word was converted into a depth-1subtree and assigned a probability by means ofsimple Good-Turing estimation (see Bod 1998).The most probable parse for each test sentencewas estimated from the 1,000 most probablederivations of that sentence, as described insection 3.We used "evalb"1 to compute the standardPARSEVAL scores for our parse results.
Wefocus on the Labeled Precision (LP) and LabeledRecall (LR) scores only in this paper, as these arecommonly used to rank parsing systems.Table 1 shows the LP and LR scoresobtained with our base line subtree set, andcompares these scores with those of previousstochastic parsers tested on the WSJ (respectivelyCharniak 1997, Collins 1999, Ratnaparkhi 1999,and Charniak 2000).The table shows that by using the base linesubtree set, our parser outperforms mostprevious parsers but it performs worse than theparser in Charniak (2000).
We will use ourscores of 89.5% LP and 89.3% LR (for testsentences ?
40 words) as the base line resultagainst which the effect of various subtreerestrictions is investigated.
While most subtreerestrictions diminish the accuracy scores, we willsee that there are restrictions that improve ourscores, even beyond those of Charniak (2000).1http://www.cs.nyu.edu/cs/projects/proteus/evalb/We will initially study our subtree restrictionsonly for test sentences ?
40 words (2245sentences), after which we will give in 4.6 ourresults for all test sentences ?
100 words (2416sentences).
While we have tested all subtreerestrictions initially on the development set(section 22 in the WSJ), we believe that it isinteresting and instructive to report these subtreerestrictions on the test set (section 23) rather thanreporting our best result only.Parser LP LR?
40 wordsChar97 87.4 87.5Coll99 88.7 88.5Char00 90.1 90.1Bod00 89.5 89.3?
100 wordsChar97 86.6 86.7Coll99 88.3 88.1Ratna99 87.5 86.3Char00 89.5 89.6Bod00 88.6 88.3Table 1.
Parsing results with the base line subtreeset compared to previous parsers4.2 The impact of subtree sizeOur first subtree restriction is concerned withsubtree size.
We therefore performed experi-ments with versions of DOP1 where the baseline subtree set is restricted to subtrees with acertain maximum depth.
Table 2 shows theresults of these experiments.depth ofsubtrees  LP  LR1 76.0 71.8?2 80.1 76.5?3 82.8 80.9?4 84.7 84.1?5 85.5 84.9?6 86.2 86.0?8 87.9 87.1?10 88.6 88.0?12 89.1 88.8?14 89.5 89.3Table 2.
Parsing results for different subtreedepths (for test sentences ?
40 words)Our scores for subtree-depth 1 are comparable toCharniak's treebank grammar if tested on wordstrings (see Charniak 1997).
Our scores areslightly better, which may be due to the use of adifferent unknown word model.
Note that thescores consistently improve if larger subtrees aretaken into account.
The highest scores areobtained if the full base line subtree set is used,but they remain behind the results of Charniak(2000).
One might expect that our results furtherincrease if even larger subtrees are used; but dueto memory limitations we did not performexperiments with subtrees larger than depth 14.4.3 The impact of lexical contextThe more words a subtree contains in its frontier,the more lexical dependencies can be taken intoaccount.
To test the impact of the lexical contexton the accuracy, we performed experiments withdifferent versions of the model where the baseline subtree set is restricted to subtrees whosefrontiers contain a certain maximum number ofwords; the subtree depth in the base line subtreeset was not constrained (though no subtreesdeeper than 14 were in this base line set).
Table 3shows the results of our experiments.# wordsin subtrees  LP  LR?1 84.4 84.0?2 85.2 84.9?3 86.6 86.3?4 87.6 87.4?6 88.0 87.9?8 89.2 89.1?10 90.2 90.1?11 90.8 90.4?12 90.8 90.5?13 90.4 90.3?14 90.3 90.3?16 89.9 89.8unrestricted 89.5 89.3Table 3.
Parsing results for different subtreelexicalizations (for test sentences ?
40 words)We see that the accuracy initially increases whenthe lexical context is enlarged, but that theaccuracy decreases if the number of words in thesubtree frontiers exceeds 12 words.
Our highestscores of 90.8% LP and 90.5% LR outperformthe scores of the best previously published parserby Charniak (2000) who obtains 90.1% for bothLP and LR.
Moreover, our scores also outper-form the reranking technique of Collins (2000)who reranks the output of the parser of Collins(1999) using a boosting method based onSchapire & Singer (1998), obtaining 90.4% LPand 90.1% LR.
We have thus found a subtreerestriction which does not decrease the parseaccuracy but even improves it.
This restrictionconsists of an upper bound of 12 words in thesubtree frontiers, for subtrees ?
depth 14.
(Wehave also tested this lexical restriction incombination with subtrees smaller than depth 14,but this led to a decrease in accuracy.
)4.4 The impact of structural contextInstead of investigating the impact of lexicalcontext, we may also be interested in studying theimportance of structural context.
We may raisethe question as to whether we need all unlexica-lized subtrees, since such subtrees do not containany lexical information, although they may beuseful to smooth lexicalized subtrees.
We accom-plished a set of experiments where unlexicalizedsubtrees of a certain minimal depth are deletedfrom the base line subtree set, while alllexicalized subtrees up to 12 words are retained.depth of deletedunlexicalizedsubtrees  LP  LR?1 79.9 77.7?2 86.4 86.1?3 89.9 89.5?4 90.6 90.2?5 90.7 90.6?6 90.8 90.6?7 90.8 90.5?8 90.8 90.5?10 90.8 90.5?12 90.8 90.5Table 4.
Parsing results for different structuralcontext (for test sentences ?
40 words)Table 4 shows that the accuracy increases ifunlexicalized subtrees are retained, but thatunlexicalized subtrees larger than depth 6 do notcontribute to any further increase in accuracy.
Onthe contrary, these larger subtrees even slightlydecrease the accuracy.
The highest scoresobtained are: 90.8% labeled precision and 90.6%labeled recall.
We thus conclude that purestructural context without any lexical informationcontributes to higher parse accuracy (even if thereexists an upper bound for the size of structuralcontext).
The importance of structural context isconsonant with Johnson (1998) who showed thatstructural context from higher nodes in the tree(i.e.
grandparent nodes) contributes to higherparse accuracy.
This mirrors our result of theimportance of unlexicalized subtrees of depth 2.But our results show that larger structural context(up to depth 6) also contributes to the accuracy.4.5 The impact of nonheadword dependenciesWe may also raise the question as to whether weneed almost arbitrarily large lexicalized  subtrees(up to 12 words) to obtain our best results.
Itcould be the case that DOP's gain in parseaccuracy with increasing subtree depth is due tothe model becoming sensitive to the influence oflexical heads higher in the tree, and that this gaincould also be achieved by a more compact modelwhich associates each nonterminal with itsheadword, such as a head-lexicalized SCFG.Head-lexicalized stochastic grammars haverecently become increasingly popular (see Collins1997, 1999; Charniak 1997, 2000).
Thesegrammars are based on Magerman's head-percolation scheme to determine the headword ofeach nonterminal (Magerman 1995).
Unfortunat-ely this means that head-lexicalized stochasticgrammars are not able to capture dependencyrelations between words that according toMagerman's head-percolation scheme are"nonheadwords" -- e.g.
between more and thanin the WSJ construction carry more people thancargo  where neither more  nor than are head-words of the NP constituent more people thancargo .
A frontier-lexicalized DOP model, on theother hand, captures these dependencies since itincludes subtrees in which more and than are theonly frontier words.
One may object that thisexample is somewhat far-fetched, but Chiang(2000) notes that head-lexicalized stochasticgrammars fall short in encoding even simpledependency relations such as between left  andJohn in the sentence John should have left .
Thisis because Magerman's head-percolation schememakes should  and have  the heads of theirrespective VPs so that there is no dependencyrelation between the verb left  and its subject John.Chiang observes that almost a quarter of allnonempty subjects in the WSJ appear in such aconfiguration.In order to isolate the contribution ofnonheadword dependencies to the parse accuracy,we eliminated all subtrees containing a certainmaximum number of nonheadwords, where anonheadword of a subtree is a word whichaccording to Magerman's scheme is not aheadword of the subtree's root nonterminal(although such a nonheadword may of course bea headword of one of the subtree's internalnodes).
In the following experiments we used thesubtree set for which maximum accuracy wasobtained in our previous experiments, i.e.containing all lexicalized subtrees with maximally12 frontier words and all unlexicalized subtreesup to depth 6.# nonheadwordsin subtrees  LP  LR0 89.6 89.6?1 90.2 90.1?2 90.4 90.2?3 90.3 90.2?4 90.6 90.4?5 90.6 90.6?6 90.6 90.5?7 90.7 90.7?8 90.8 90.6unrestricted 90.8 90.6Table 5.
Parsing results for different number ofnonheadwords (for test sentences ?
40 words)Table 5 shows that nonheadwords contribute tohigher parse accuracy: the difference betweenusing no and all nonheadwords is 1.2% in LPand 1.0% in LR.
Although this difference isrelatively small, it does indicate that nonhead-word dependencies should preferably not bediscarded in the WSJ.
We should note, however,that most other stochastic parsers do includecounts of single  nonheadwords: they appear inthe backed-off statistics of these parsers (seeCollins 1997, 1999; Charniak 1997; Goodman1998).
But our parser is the first parser that alsoincludes counts between two or more non-headwords, to the best of our knowledge, andthese counts lead to improved performance, ascan be seen in table 5.4.6 Results for all sentencesWe have seen that for test sentences ?
40 words,maximal parse accuracy was obtained by asubtree set which is restricted to subtrees with notmore than 12 words and which does not containunlexicalized subtrees deeper than 6.2 We used2It may be noteworthy that for the developmentset (section 22 of WSJ), maximal parse accuracywas obtained with exactly the same subtreerestrictions.
As explained in 4.1, we initially testedall restrictions on the development set, but wepreferred to report the effects of these restrictionsfor the test set.these restrictions to test our model on allsentences ?
100 words from the WSJ test set.This resulted in an LP of 89.7% and an LR of89.7%.
These scores slightly outperform the bestpreviously published parser by Charniak (2000),who obtained 89.5% LP and 89.6% LR for testsentences ?
100 words.
Only the rerankingtechnique proposed by Collins (2000) slightlyoutperforms our precision score, but not ourrecall score: 89.9% LP and 89.6% LR.5   Discussion: Converging ApproachesThe main goal of this paper was to find theminimal set of fragments which achievesmaximal parse accuracy in Data OrientedParsing.
We have found that this minimal set offragments is very large and extremely redundant:highest parse accuracy is obtained by employingonly two constraints on the fragment set: arestriction of the number of words in thefragment frontiers to 12 and a restriction of thedepth of unlexicalized fragments to 6.
No otherconstraints were warranted.There is an important question whymaximal parse accuracy occurs with exactly theseconstraints.
Although we do not know theanswer to this question, we surmise that theseconstraints differ from corpus to corpus and arerelated to general data sparseness effects.
Inprevious experiments with DOP1 on smaller andmore restricted domains we found that the parseaccuracy decreases also after a certain maximumsubtree depth (see Bod 1998; Sima'an 1999).
Weexpect that also for the WSJ the parse accuracywill decrease after a certain depth, although wehave not been able to find this depth so far.A major difference between our approachand most other models tested on the WSJ is thatthe DOP model uses frontier lexicalization whilemost other models use constituent lexicalization(in that they associate each constituent non -terminal with its lexical head -- see Collins 1996,1999; Charniak 1997; Eisner 1997).
The resultsin this paper indicate that frontier lexicalization isa promising alternative to constituent lexicaliza-tion.
Our results also show that the linguisticallymotivated constraint which limits the statisticaldependencies to the locality of headwords ofconstituents is too narrow.
Not only are counts ofsubtrees with nonheadwords important, alsocounts of unlexicalized subtrees up to depth 6increase the parse accuracy.The only other model that uses frontierlexicalization and that was tested on the standardWSJ split is Chiang (2000) who extracts astochastic tree-insertion grammar or STIG(Schabes & Waters 1996) from the WSJ,obtaining 86.6% LP and 86.9% LR for sentences?
40 words.
However, Chiang's approach islimited in at least two respects.
First, eachelementary tree in his STIG is lexicalized withexactly one lexical item, while our results showthat there is an increase in parse accuracy if morelexical items and also if unlexicalized trees areincluded (in his conclusion Chiang acknowledgesthat "multiply anchored trees" may be important).Second, Chiang computes the probability of atree by taking into account only one derivation,while in STIG, like in DOP1, there can be severalderivations that generate the same tree.Another difference between our approachand most other models is that the underlyinggrammar of DOP is based on a treebankgrammar (cf.
Charniak 1996, 1997), while mostcurrent stochastic parsing models use a "markovgrammar" (e.g.
Collins 1999; Charniak 2000).While a treebank grammar only assignsprobabilities to rules or subtrees that are seen in atreebank, a markov grammar assigns proba-bilities to any possible rule, resulting in a morerobust model.
We expect that the application ofthe markov grammar approach to DOP willfurther improve our results.
Research in thisdirection is already ongoing, though it has beentested for rather limited subtree depths only (seeSima'an 2000).Although we believe that our main result isto have shown that almost arbitrary fragmentswithin parse trees are important, it is surprisingthat a relatively simple model like DOP1outperforms most other stochastic parsers on theWSJ.
Yet, to the best of our knowledge, DOP isthe only model which does not a priori  restrictthe fragments that are used to compute the mostprobable parse.
Instead, it starts out by taking intoaccount all fragments seen in a treebank and theninvestigates fragment restrictions to discover theset of relevant fragments.
From this perspective,the DOP approach can be seen as striving for thesame goal as other approaches but from a dif-ferent direction.
While other approaches usuallylimit the statistical dependencies beforehand (forexample to headword dependencies) and then tryto improve parse accuracy by gradually letting inmore dependencies, the DOP approach starts outby taking into account as many dependencies aspossible and then tries to constrain them withoutlosing parse accuracy.
It is not unlikely that thesetwo opposite directions will finally converge tothe same, true set of statistical dependencies fornatural language parsing.As it happens, quite some convergence hasalready taken place.
The history of stochasticparsing models shows a consistent increase in thescope of statistical dependencies that are capturedby these models.
Figure 4 gives a (very)schematic overview of this increase (see Carroll& Weir 2000, for a more detailed account of asubsumption lattice where SCFG is at the bottomand DOP at the top).context-free rulesCharniak (1996)Collins (1996),Eisner (1996)context-free rules,headwordsCharniak (1997) context-free rules,headwords,grandparent nodesCollins (2000) context-free rules,headwords,grandparent nodes/rules,bigrams, two-level rules,two-level bigrams,nonheadwordsBod (1992) all fragments withinparse treesScope of StatisticalDependencies ModelFigure 4.
Schematic overview of the increase ofstatistical dependencies by stochastic parsersThus there seems to be a convergence towards amaximalist model which "takes all fragments [...]and lets the statistics decide" (Bod 1998: 5).While early head-lexicalized grammars restrictedthe fragments to the locality of headwords (e.g.Collins 1996; Eisner 1996), later models showedthe importance of including context from highernodes in the tree (Charniak 1997; Johnson 1998).This mirrors our result of the utility of(unlexicalized) fragments of depth 2 and larger.The importance of including single nonhead-words is now also uncontroversial (e.g.
Collins1997, 1999; Charniak 2000), and the currentpaper has shown the importance of including twoand more nonheadwords.
Recently, Collins(2000) observed that "In an ideal situation wewould be able to encode arbitrary features hs,thereby keeping track of counts of arbitraryfragments within parse trees".
This is in perfectcorrespondence with the DOP philosophy.ReferencesR.
Bod, 1992.
Data Oriented Parsing, ProceedingsCOLING'92, Nantes, France.R.
Bod, 1993.
Using an Annotated LanguageCorpus as a Virtual Stochastic Grammar,Proceedings AAAI'93, Washington D.C.R.
Bod, 1996.
Two Questions about Data-OrientedParsing, Proceedings 4th Workshop on VeryLarge Corpora, COLING'96, Copenhagen,Denmark.R.
Bod, 1998.
Beyond Grammar: An Experience-Based Theory of Language, Stanford, CSLIPublications, distributed by Cambridge Uni-versity Press.R.
Bod, 2000a.
Parsing with the ShortestDerivation, Proceedings COLING'2000,Saarbr?cken, Germany.R.
Bod, 2000b.
Combining Semantic and SyntacticStructure for Language Modeling, Proceed-ings ICSLP-2000, Beijing, China.R.
Bod, 2000c.
An Improved Parser for Data-Oriented Lexical-Functional Analysis, Proc-eedings ACL-2000, Hong Kong, China.R.
Bod, 2001.
Using Natural Language ProcessingTechniques for Musical Parsing, Proceed-ings ACH/ALLC'2001, New York, NY.R.
Bod and R. Kaplan, 1998.
A ProbabilisticCorpus-Driven Model for Lexical-FunctionalAnalysis, Proceedings COLING-ACL'98,Montreal, Canada.R.
Bonnema, P. Buying and R. Scha, 1999.
A NewProbability Model for Data-Oriented Parsing,Proceedings of the Amsterdam Colloqui-um'99, Amsterdam, Holland.J.
Carroll and D. Weir, 2000.
Encoding FrequencyInformation in Lexicalized Grammars, in H.Bunt and A. Nijholt (eds.
), Advances inProbabilistic and Other Parsing Technolo-gies, Kluwer Academic Publishers.E.
Charniak, 1996.
Tree-bank Grammars, Procee-dings AAAI'96, Menlo Park, Ca.E.
Charniak, 1997.
Statistical Parsing with aContext-Free Grammar and Word Statistics,Proceedings AAAI-97, Menlo Park, Ca.E.
Charniak, 2000.
A Maximum-Entropy-InspiredParser.
Proceedings ANLP-NAACL'2000,Seattle, Washington.D.
Chiang, 2000.
Statistical parsing with anautomatically extracted tree adjoininggrammar, Proceedings ACL'2000, HongKong, China.M.
Collins  1996.
A new statistical parser based onbigram lexical dependencies, ProceedingsACL'96, Santa Cruz, Ca.M.
Collins, 1997.
Three generative lexicalisedmodels for statistical parsing, ProceedingsACL'97, Madrid, Spain.M.
Collins, 1999.
Head-Driven Statistical Modelsfor Natural Language Parsing, PhD thesis,University of Pennsylvania, PA.M.
Collins, 2000.
Discriminative Reranking forNatural Language Parsing, ProceedingsICML-2000, Stanford, Ca.J.
Eisner, 1996.
Three new probabilistic models fordependency parsing: an exploration, Proc-eedings COLING-96, Copenhagen, Denmark.J.
Eisner, 1997.
Bilexical Grammars and a Cubic-Time Probabilistic Parser, Proceedings FifthInternational Workshop on Parsing Techno-logies, Boston, Mass.J.
Goodman, 1996.
Efficient Algorithms for Parsingthe DOP Model, Proceedings EmpiricalMethods in Natural Language Processing,Philadelphia, PA.J.
Goodman, 1997.
Global Thresholding andMultiple-Pass Parsing, Proceedings EMNLP-2, Boston, Mass.J.
Goodman, 1998.
Parsing Inside-Out, Ph.D. thesis,Harvard University, Mass.M.
Johnson, 1998.
PCFG Models of LinguisticTree Representations, Computational Ling-uistics 24(4), 613-632.D.
Magerman, 1995.
Statistical Decision-TreeModels for Parsing, Proceedings ACL'95,Cambridge, Mass.M.
Marcus, B. Santorini and M. Marcinkiewicz,1993.
Building a Large Annotated Corpus ofEnglish: the Penn Treebank, ComputationalLinguistics 19(2).A.
Ratnaparkhi, 1999.
Learning to Parse NaturalLanguage with Maximum Entropy Models,Machine Learning 34, 151-176.Y.
Schabes and R. Waters, 1996.
StochasticLexicalized Tree-Insertion Grammar.
In H.Bunt and M. Tomita (eds.)
Recent Advancesin Parsing Technology.
Kluwer AcademicPublishers.R.
Schapire and Y.
Singer, 1998.
ImprovedBoosting Algorithms Using Confedence-Rated Predictions, Proceedings 11th AnnualConference on Computational LearningTheory.
Morgan Kaufmann, San Francisco.K.
Sima'an, 1999.
Learning Efficient Disambig-uation.
PhD thesis, University of Amster-dam, The Netherlands.K.
Sima'an, 2000.
Tree-gram Parsing: LexicalDependencies and Structural Relations,Proceedings ACL'2000, Hong Kong, China.R.
Weischedel, M. Meteer, R, Schwarz, L.Ramshaw and J. Palmucci, 1993.
Copingwith Ambiguity and Unknown Words throughProbabilistic Models, C o m p u t a t i o n a lLinguistics, 19(2).
