Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 257?260,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPTransfer Learning, Feature Selection and Word Sense DisambguationParamveer S. Dhillon and Lyle H. UngarComputer and Information ScienceUniversity of Pennsylvania, Philadelphia, PA, U.S.A{pasingh,ungar}@seas.upenn.eduAbstractWe propose a novel approach for improv-ing Feature Selection for Word Sense Dis-ambiguation by incorporating a featurerelevance prior for each word indicatingwhich features are more likely to be se-lected.
We use transfer of knowledge fromsimilar words to learn this prior over thefeatures, which permits us to learn higheraccuracy models, particularly for the rarerword senses.
Results on the ONTONOTESverb data show significant improvementover the baseline feature selection algo-rithm and results that are comparable to orbetter than other state-of-the-art methods.1 IntroductionThe task of WSD has been mostly studied ina supervised learning setting e.g.
(Florian andYarowsky, 2002) and feature selection has alwaysbeen an important component of high accuracyword sense disambiguation, as one often has thou-sands of features but only hundreds of observa-tions of the words (Florian and Yarowsky, 2002).The main problem that arises with supervisedWSD techniques, including ones that do featureselection, is the paucity of labeled data.
For ex-ample, the training set of SENSEVAL-2 Englishlexical sample task has only 10 labeled examplesper sense (Florian and Yarowsky, 2002), whichmakes it difficult to build high accuracy modelsusing only supervised learning techniques.
It isthus an attractive alternative to use transfer learn-ing (Ando and Zhang, 2005), which improves per-formance by generalizing from solutions to ?sim-ilar?
learning problems.
(Ando, 2006) (abbrevi-ated as Ando[CoNLL?06]) have successfully ap-plied the ASO (Alternating Structure Optimiza-tion) technique proposed by (Ando and Zhang,2005), in its transfer learning configuration, to theproblem of WSD by doing joint empirical riskminimization of a set of related problems (wordsin this case).
In this paper, we show how a novelform of transfer learning that learns a feature rel-evance prior from similar word senses, aids in theprocess of feature selection and hence benefits thetask of WSD.Feature selection algorithms usually put a uni-form prior over the features.
I.e., they considereach feature to have the same probability of beingselected.
In this paper we relax this overly sim-plistic assumption by transferring a prior for fea-ture relevance of a given word sense from ?simi-lar?
word senses.
Learning this prior for featurerelevance of a test word sense makes those fea-tures that have been selected in the models of other?similar?
word senses become more likely to beselected.We learn the feature relevance prior only fromdistributionally similar word senses, rather than?all?
senses of each word, as it is difficult to findwords which are similar in ?all?
the senses.
Wecan, however, often find words which have one ora few similar senses.
For example, one sense of?fire?
(as in ?fire someone?)
should share featureswith one sense of ?dismiss?
(as in ?dismiss some-one?
), but other senses of ?fire?
(as in ?fire thegun?)
do not.
Similarly, other meanings of ?dis-miss?
(as in ?dismiss an idea?)
should not sharefeatures with ?fire?.As just mentioned, knowledge can only befruitfully transfered between the shared senses ofdifferent words, even though the models beinglearned are for disambiguating different senses ofa single word.
To address this problem, we clustersimilar word senses of different words, and thenuse the models learned for all but one of the wordsenses in the cluster (called the ?training wordsenses?)
to put a feature relevance prior on whichfeatures will be more predictive for the held outtest word sense.
We hold out each word sense inthe cluster once and learn a prior from the remain-ing word senses in that cluster.
For example, wecan use the models for discriminating the sensesof the words ?kill?
and the senses of ?capture?, to257put a prior on what features should be included ina model to disambiguate corresponding senses ofthe distributionally similar word ?arrest?.The remainder of the paper is organized as fol-lows.
In Section 2 we describe our ?baseline?
in-formation theoretic feature selection method, andextend it to our ?TRANSFEAT?
method.
Section 3contains experimental results comparing TRANS-FEAT with the baseline and Ando[CoNLL?06] onONTONOTES data.
We conclude with a brief sum-mary in Section 4.2 Feature Selection for WSDWe use an information theoretic approach to fea-ture selection based on the Minimum DescriptionLength (MDL) (Rissanen, 1999) principle, whichmakes it easy to incorporate information aboutfeature relevance priors.
These information theo-retic models have a ?dual?
Bayesian interpretation,which provides a clean setting for feature selec-tion.2.1 Information Theoretic Feature SelectionThe state-of-the-art feature selection methods inWSD use either an ?0or an ?1penalty on the coef-ficients.
?1penalty methods such as Lasso, beingconvex, can be solved by optimization and giveguaranteed optimal solutions.
On the other hand,?0penalty methods, like stepwise feature selec-tion, give approximate solutions but produce mod-els that are much sparser than the models given by?1methods, which is quite crucial in WSD (Flo-rian and Yarowsky, 2002).
?0models are also moreamenable to theoretical analysis for setting thresh-olds, and hence for incorporating priors.Penalized likelihood methods which are widelyused for feature selection minimize a score:Score = ?2log(likelihood) + Fq (1)where F is a function designed to penalize modelcomplexity, and q represents the number of fea-tures currently included in the model at a givenpoint.
The first term in the above equation repre-sents a measure of the in-sample error given themodel, while the second term is a model complex-ity penalty.As is obvious from Eq.
1, the description lengthof the MDL (Minimum Description Length) mes-sage is composed of two parts: SE, the num-ber of bits for encoding the residual errors giventhe models and SM, the number of bits for en-coding the model.
Hence the description lengthcan be written as: S = SE+ SM.
Now, whenwe evaluate a feature for possible addition to ourmodel, we want to maximize the reduction of ?de-scription length?
incurred by adding this featureto the model.
This change in description lengthis: ?S = ?SE?
?SM; where ?SE?
0 is thenumber of bits saved in describing residual errordue to increase in the likelihood of the data giventhe new feature and ?SM> 0 is the extra bitsused for coding this new feature.In our baseline feature selection model, we usethe following coding schemes:Coding Scheme for SE:The term SErepresents the cost of coding theresidual errors given the models and can be writtenas:SE= ?
log(P (y|w, x))?SErepresents the increase in likelihood (inbits) of the data by adding this new feature to themodel.
We assume a Gaussian model, giving:P (y|w, x) ?
exp(?(?ni=1(yi?
w ?
xi)22?2))where y is the response (word senses in our case),x?s are the features, w?s are the regression weightsand ?2 is the variance of the Gaussian noise.Coding Scheme for ?SM: For describing SM,the number of bits for encoding the model, weneed the bits to code the index of the feature (i.e.,which feature from amongst the total m candidatefeatures) and the bits to code the coefficient of thisfeature.The total cost can be represented as:SM= lf+ l?where lfis the cost to code the index of the featureand l?is the number of bits required to code thecoefficient of the selected feature.In our baseline feature selection algorithm, wecode lfby using log(m) bits (where m is thetotal number of candidate features), which isequivalent to the standard RIC (or the Bonferronipenalty) (Foster and George, 1994) commonlyused in information theory.
The above codingscheme1 corresponds to putting a uniform priorover all the features; I.e., each feature is equallylikely to get selected.For coding the coefficients of the selected fea-ture we use 2 bits, which is quite similar to the AIC1There is a duality between Information Theory andBayesian terminology: If there is 1kprobability of a fact beingtrue, then we need ?log( 1k) = log(k) bits to code it.258(Akaike Information Criterion) (Rissanen, 1999).Our final equation for SMis therefore:SM= log(m) + 2 (2)2.2 Extension to TRANSFEATWe now extend the baseline feature selection al-gorithm to include the feature relevance prior.
Wedefine a binary random variable fi?
{0,1} thatdenotes the event of the ith feature being in or notbeing in the model for the test word sense.
We canparameterize the distribution as p(fi= 1|?i) = ?i.I.e., we have a Bernoulli Distribution over the fea-tures.Given the data for the ith feature for all thetraining word senses, we can write: Di={fi1, ..., fiv, ..., fit}.
We then construct the like-lihood functions from the data (under the i.i.d as-sumption) as:p(Dfi|?i) =t?v=1p(fiv|?i) =t?v=1?fiv(1?
?i)1?fivThe posteriors can be calculated by putting a priorover the parameters ?iand using Bayes rule as fol-lows:p(?i|Dfi) = p(Dfi|?i) ?
p(?i|a, b)where a and b are the hyperparameters of the BetaPrior (conjugate of Bernoulli).
The predictive dis-tribution of ?iis:p(fi= 1|Dfi) =?10?ip(?i|Dfi)d?i= E[?i|Dfi]=k + ak + l + a + b(3)where k is the number of times that the ith featureis selected and l is the complement of k, i.e.
thenumber of times the ith feature is not selected inthe training data.In light of above, the coding scheme, which in-corporates the prior information about the predic-tive quality of the various features obtained fromsimilar word senses, can be formulated as follows:SM= ?
log (p(fi= 1|Dfi)) + 2In the above equation, the first term repre-sents the cost of coding the features, and the sec-ond term codes the coefficients.
The negativesigns appear due to the duality between Bayesianand Information-Theoretic representation, as ex-plained earlier.3 Experimental ResultsIn this section we present the experimental resultsof TRANSFEAT on ONTONOTES data.3.1 Similarity DeterminationTo determine which verbs to transfer from, wecluster verb senses into groups based on theTF/IDF similarity of the vector of features se-lected for that verb sense in the baseline (non-transfer learning) model.
We use only thosefeatures that are positively correlated with thegiven sense; they are the features most closelyassociated with the given sense.
We clustersenses using a ?foreground-background?
cluster-ing algorithm (Kandylas et al, 2007) rather thanthe more common k-means clustering becausemany word senses are not sufficiently similar toany other word sense to warrant putting into acluster.
Foreground-background clustering giveshighly cohesive clusters of word senses (the ?fore-ground?)
and puts all the remaining word sensesin the ?background?.
The parameters that it takesas input are the % of data points to put in ?back-ground?
(i.e., what would be the singleton clus-ters) and a similarity threshold which impactsthe number of ?foreground?
clusters.
We exper-imented with putting 20% and 33% data points inbackground and adjusted the similarity thresholdto give us 50 ?
100 ?foreground?
clusters.
Theresults reported below have 20% background and50 ?
100 ?foreground?
clusters.3.2 Description of Data and ResultsWe performed our experiments on ONTONOTESdata of 172 verbs (Hovy et al, 2006).
The dataconsists of a rich set of linguistic features whichhave proven to be beneficial for WSD.A sample feature vector for the word ?add?,given below, shows typical features.word_added pos_vbd morph_normalsubj_use subjsyn_16993 dobj_moneydobjsyn_16993 pos+1+2+3_rp+to+cdtp_account tp_accumulate tp_actualThe 172 verbs each had between 1,000 and 10,000nonzero features.
The number of senses variedfrom 2 (For example, ?add?)
to 15 (For example,?turn?
).We tested our transfer learning algorithm inthree slightly varied settings to tease apart the con-tributions of different features to the overall per-formance.
In our main setting, we cluster the word259senses based on the ?semantic + syntactic?
fea-tures.
In Setting 2, we do clustering based only on?semantic?
features (topic features) and in Setting3 we cluster based on only ?syntactic?
(pos, dobjetc.)
features.Table 1: 10-fold CV (microaveraged) accuraciesof various methods for various Transfer Learningsettings.
Note: These are true cross-validation ac-curacies; No parameters have been tuned on them.Method Setting 1 Setting 2 Setting 3TRANSFEAT 85.75 85.11 85.37Baseline Feat.
Sel.
83.50 83.09 83.34SVM (Poly.
Kernel) 83.77 83.44 83.57Ando[CoNLL?06] 85.94 85.00 85.51Most Freq.
Sense 76.59 77.14 77.24We compare TRANSFEAT against Baseline Fea-ture Selection, Ando[CoNLL?06], SVM (libSVMpackage) with a cross-validated polynomial kerneland a simple most frequent sense baseline.
Wetuned the ?d?
parameter of the polynomial kernelusing a separate cross validation.The results for the different settings are shownin Table 1 and are significantly better at the 5%significance level (Paired t-test) than the base-line feature selection algorithm and the SVM.
Itis comparable in accuracy to Ando[CoNLL?06].Settings 2 and 3, in which we cluster based ononly ?semantic?
or ?syntactic?
features, respec-tively, also gave significant (5% level in a Pairedt-Test) improvement in accuracy over the baselineand SVM model.
But these settings performedslightly worse than Setting 1, which suggests thatit is a good idea to have clusters in which the wordsenses have ?semantic?
as well as ?syntactic?
dis-tributional similarity.Some examples will help to emphasize the pointthat we made earlier that transfer helps the most incases in which the target word sense has much lessdata than the word senses from which knowledgeis being transferred.
?kill?
had roughly 6 timesmore data than all other word senses in its cluster(i.e., ?arrest?, ?capture?, ?strengthen?, etc.)
In thiscase, TRANSFEAT gave 3.19 ?
8.67% higher ac-curacies than competing methods2 on these threewords.
Also, for the case of word ?do,?
whichhad roughly 10 times more data than the otherword senses in its cluster (E.g., ?die?
and ?save?
),TRANSFEAT gave 4.09?6.21% higher accuracies2TRANSFEAT does better than Ando[CoNLL?06] on thesewords even though on average over all 172 verbs, the differ-ence is slender.than other methods.
Transfer makes the biggestdifference when the target words have much lessdata than the word senses they are generalizingfrom, but even in cases where the words have sim-ilar amounts of data we still get a 1.5 ?
2.5% in-crease in accuracy.4 SummaryThis paper presented a Transfer Learning formula-tion which learns a prior suggesting which featuresare most useful for disambiguating ambiguouswords.
Successful transfer requires finding similarword senses.
We used ?foreground/background?clustering to find cohesive clusters for variousword senses in the ONTONOTES data, consider-ing both ?semantic?
and ?syntactic?
similarity be-tween the word senses.
Learning priors on featureswas found to give significant accuracy boosts,with both syntactic and semantic features con-tributing to successful transfer.
Both feature setsgave substantial benefits over the baseline meth-ods that did not use any transfer and gave compa-rable accuracy to recent Transfer Learning meth-ods like Ando[CoNLL?06].
The performance im-provement of our Transfer Learning becomes evenmore pronounced when the word senses that weare generalizing from have more observations thanthe ones that are being learned.ReferencesR.
Ando and T. Zhang.
2005.
A framework for learn-ing predictive structures from multiple tasks and un-labeled data.
JMLR, 6:1817?1853.R.
Ando.
2006.
Applying alternating structureoptimization to word sense disambiguation.
In(CoNLL).R.
Florian and D. Yarowsky.
2002.
Modeling consen-sus: classifier combination for word sense disam-biguation.
In EMNLP ?02, pages 25?32.D.
P. Foster and E. I. George.
1994.
The risk infla-tion criterion for multiple regression.
The Annals ofStatistics, 22(4):1947?1975.E.
H. Hovy, M. P. Marcus, M. Palmer, L. A. Ramshaw,and R. M. Weischedel.
2006.
Ontonotes: The 90%solution.
In HLT-NAACL.V.
Kandylas, S. P. Upham, and L. H. Ungar.
2007.Finding cohesive clusters for analyzing knowledgecommunities.
In ICDM, pages 203?212.J.
Rissanen.
1999.
Hypothesis selection and testing bythe mdl principle.
The Computer Journal, 42:260?269.260
