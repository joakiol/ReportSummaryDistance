Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149?154,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Improving Phrase-Based Statistical Translation by modifyingphrase extraction and including several featuresMarta Ruiz Costa-jussa` and Jose?
A. R. FonollosaTALP Research CenterUniversitat Polite`cnica de Catalunya{mruiz,adrian}@gps.tsc.upc.eduAbstractNowadays, most of the statistical translation sys-tems are based on phrases (i.e.
groups of words).In this paper we study different improvements tothe standard phrase-based translation system.
Wedescribe a modified method for the phrase extrac-tion which deals with larger phrases while keepinga reasonable number of phrases.
We also proposeadditional features which lead to a clear improve-ment in the performance of the translation.
Wepresent results with the EuroParl task in the direc-tion Spanish to English and results from the evalu-ation of the shared task ?Exploiting Parallel Textsfor Statistical Machine Translation?
(ACL Work-shop on Parallel Texts 2005).1 IntroductionStatistical Machine Translation (SMT) is based onthe assumption that every sentence e in the targetlanguage is a possible translation of a given sen-tence f in the source language.
The main differencebetween two possible translations of a given sen-tence is a probability assigned to each, which hasto be learned from a bilingual text corpus.
Thus,the translation of a source sentence f can be for-mulated as the search of the target sentence e thatmaximizes the translation probability P (e|f),e?
= argmaxeP (e|f) (1)0This work has been supported by the European Unionunder grant FP6-506738 (TC-STAR project).If we use Bayes rule to reformulate the transla-tion probability, we obtain,e?
= argmaxeP (f |e)P (e) (2)This translation model is known as the source-channel approach [1] and it consists on a lan-guage model P (e) and a separate translation modelP (f |e) [5].In the last few years, new systems tend to usesequences of words, commonly called phrases [8],aiming at introducing word context in the transla-tion model.
As alternative to the source-channelapproach the decision rule can be modeled througha log-linear maximum entropy framework.e?
= argmaxe{ M?m=1?mhm(e, f)}(3)The features functions, hm, are the system mod-els (translation model, language model and others)and weigths, ?i, are typically optimized to max-imize a scoring function.
It is derived from theMaximum Entropy approach suggested by [13] [14]for a natural language understanding task.
It hasthe advantatge that additional features functionscan be easily integrated in the overall system.This paper addresses a modification of thephrase-extraction algorythm in [11].
It also com-bines several interesting features and it reports animportant improvement from the baseline.
It is or-ganized as follows.
Section 2 introduces the base-line; the following section explains the modificationin the phrase extraction; section 4 shows the differ-ent features which have been taken into account;section 5 presents the evaluation framework; and149the final section shows some conclusions on the ex-periments in the paper and on the results in theshared task.2 BaselineThe baseline is based on the source-channel ap-proach, and it is composed of the following modelswhich later will be combined in the decoder.The Translation Model.
It is based on bilin-gual phrases, where a bilingual phrase (BP ) issimply two monolingual phrases (MP ) in whicheach one is supposed to be the translation of eachother.
A monolingual phrase is a sequence of words.Therefore, the basic idea of phrase-based transla-tion is to segment the given source sentence intophrases, then translate each phrase and finally com-pose the target sentence from these phrase transla-tions [17].During training, the system has to learn a dictio-nary of phrases.
We begin by aligning the trainingcorpus using GIZA++ [6], which is done in bothtranslation directions.
We take the union of bothalignments to obtain a symmetrized word align-ment matrix.
This alignment matrix is the startingpoint for the phrase based extraction.Next, we define the criterion to extract the set ofBP of the sentence pair (f j2j1 ; ei2i1) and the alignmentmatrix A ?
J?I , which is identical to the alignmentcriterion described in [11].BP (fJ1 , eI1, A) = {(f j2j1 , ei2i1) :?
(j, i)?A : j1 ?
j ?
j2 ?
i1 ?
i ?
i2??
(j, i)?A : j1 ?
j ?
j2 ?
i1 ?
i ?
i2}The set of BP is consistent with the alignmentand consists of all BP pairs where all words withinthe foreign language phrase are only aligned to thewords of the English language phrase and viceversa.At least one word in the foreign language phrase hasto be aligned with at least one word of the Englishlanguage.
Finally, the algorithm takes into accountpossibly unaligned words at the boundaries of theforeign or English language phrases.The target language model.
It is combinedwith the translation probability as showed in equa-tion (2).
It gives coherence to the target text ob-tained by the concatenated phrases.3 Phrase ExtractionMotivation.
The length of a MP is defined asits number of words.
The length of a BP is thegreatest of the lengths of its MP .As we are working with a huge amount of data(see corpus statistics), it is unfeasible to build adictionary with all the phrases longer than length4.
Moreover, the huge increase in computationaland storage cost of including longer phrases doesnot provide a significant improve in quality [8].X-length In our system we considered two lengthlimits.
We first extract all the phrases of length 3or less.
Then, we also add phrases up to length5 if they cannot be generated by smaller phrases.Empirically, we chose 5, as the probability of reap-pearence of larger phrases decreases.Basically, we select additional phrases withsource words that otherwise would be missed be-cause of cross or long alignments.
For example,from the following sentence,Cuando el Parlamento Europeo , que tan fre-cuentemente insiste en los derechos de los traba-jadores y en la debida proteccio?n social , (...)NULL ( ) When ( 1 ) the ( 2 ) European ( 4) Parliament ( 3 4 ) , ( 5 ) that ( 6 ) so ( 7 )frequently ( 8 ) insists ( 9 ) on ( 10 ) workers ( 1115 ) ?
( 14 ) rights ( 12 ) and ( 16 ) proper ( 19 )social ( 21 ) protection ( 20 ) , ( 22 ) (...)where the number inside the clauses is thealigned word(s).
And the phrase that we are look-ing for is the following one.los derechos de los trabajadores # workers ?rightswhich only could appear in the case the maximumlength was 5.1504 Phrase ranking4.1 Conditional probability P (f |e)Given the collected phrase pairs, we estimated thephrase translation probability distribution by rela-tive frecuency.P (f |e) = N(f, e)N(e) (4)where N(f,e) means the number of times the phrasef is translated by e. If a phrase e has N > 1possible translations, then each one contributes as1/N [17].Note that no smoothing is performed, which maycause an overestimation of the probability of rarephrases.
This is specially harmful given a BPwhere the source part has a big frecuency of ap-pearence but the target part appears rarely.
Forexample, from our database we can extract the fol-lowing BP : ?you # la que no?, where the Englishis the source language and the Spanish, the tar-get language.
Clearly, ?la que no?
is not a goodtranslation of ?you?, so this phrase should have alow probability.
However, from our aligned trainingdatabase we obtain,P (f |e) = P (you|la que no) = 0.23This BP is clearly overestimated due to sparse-ness.
On the other, note that ?la que no?
can-not be considered an unusual trigram in Spanish.Hence, the language model does not penalise thistarget sequence either.
So, the total probability(P (f |e)P (e)) would be higher than desired.In order to somehow compensate these unreili-able probabilities we have studied the inclusion ofthe posterior [12] and lexical probabilities [1] [10]as additional features.4.2 Feature P (e|f)In order to estimate the posterior phrase probabil-ity, we compute again the relative frequency but re-placing the count of the target phrase by the countof the source phrase.P (e|f) = N?
(f, e)N(f) (5)where N?
(f,e) means the number of times thephrase e is translated by f. If a phrase f has N > 1possible translations, then each one contributes as1/N.Adding this feature function we reduce the num-ber of cases in which the overall probability is over-estimated.
This results in an important improve-ment in translation quality.4.3 IBM Model 1We used IBM Model 1 to estimate the probabilityof a BP .
As IBM Model 1 is a word translation andit gives the sum of all possible alignment probabil-ities, a lexical co-ocurrence effect is expected.
Thiscaptures a sort of semantic coherence in transla-tions.Therefore, the probability of a sentence pair isgiven by the following equation.P (f |e; M1) = 1(I + 1)JJ?j=1I?i=0p(fj |ei) (6)The p(fj |ei) are the source-target IBM Model 1word probabilities trained by GIZA++.
Becausethe phrases are formed from the union of source-to-target and target-to-source alignments, there canbe words that are not in the P (fj |ei) table.
In thiscase, the probability was taken to be 10?40.In addition, we have calculated the IBM?1 Model1.P (e|f ; M1) = 1(J + 1)II?I=1J?j=0p(ei|fj) (7)4.4 Language ModelThe English language model plays an importantrole in the source channel model, see equation (2),and also in its modification, see equation (3).
TheEnglish language model should give an idea of thesentence quality that is generated.As default language model feature, we use a stan-dard word-based trigram language model generatedwith smoothing Kneser-Ney and interpolation (byusing SRILM [16]).4.5 Word and Phrase PenaltyTo compensate the preference of the target lan-guage model for shorter sentences, we added two151Spanish EnglishTrain Sentences 1223443 1223443Words 34794006 33379333Vocabulary 168685 104975Dev Sentences 504 504Words 15353 15335OOV 25 16Test Sentences 504 504Words 10305 10667OOV 36 19Table 1: Statistics of training and test corpussimple features which are widely used [17] [7].
Theword penalty provides means to ensure that thetranslations do not get too long or too short.
Neg-ative values for the word penalty favor longer out-put, positive values favor shorter output [7].The phrase penalty is a constant cost per pro-duced phrase.
Here, a negative weight, whichmeans reducing the costs per phrase, results in apreference for adding phrases.
Alternatively, by us-ing a positive scaling factors, the system will favorless phrases.5 Evaluation framework5.1 Corpus StatisticsExperiments were performed to study the effectof our modifications in the phrases.
The trainingmaterial covers the transcriptions from April 1996to September 2004.
This material has been dis-tributed by the European Parlament.
In our ex-periments, we have used the distribution of RWTHof Aachen under the project of TC-STAR 1.
Thetest material was used in the first evaluation of theproject in March 2005.
In our case, we have usedthe development divided in two sets.
This mate-rial corresponds to the transcriptions of the sessionsfrom October the 21st to October the 28th.
It hasbeen distributed by ELDA2.
Results are reportedfor Spanish-to-English translations.1http://www.tcstar.org/2http://www.elda.org/5.2 ExperimentsThe decoder used for the presented translation sys-tem is reported in [2].
This decoder is calledMARIE and it takes into account simultaneouslyall the 7 features functions described above.
It im-plements a beam-search strategy.As evaluation criteria we use: the Word ErrorRate (WER), the BLEU score [15] and the NISTscore [3].As follows we report the results for several ex-periments that show the performance of: the base-line, adding the posterior probability, IBM Model1 and IBM1?1, and, finally, the modification of thephrases extraction.Optimisation.
Significant improvements can beobtained by tuning the parameters of the featuresadequately.
In the complet system we have 7 pa-rameters to tune: the relatives frecuencies P (f |e)and P (e|f), IBM Model 1 and its inverse, the wordpenalty, the phrase penalty and the weight of thelanguage model.
We applied the widely used algo-rithm SIMPLEX to optimise [9].
In Table 2 (line5th), we see the final results.Baseline.
We report the results of the baseline.We use the union alignment and we extract theBP of length 3.
As default language model fea-ture, we use the standard trigram with smoothingKneser-Ney and interpolation.
Also we tune theparameters (only two parameters) with the SIM-PLEX algorithm (see Table 2).Posterior probability.
Table 2 shows the effectof using the posterior probability: P (e|f).
We useall the features but the P (e|f) and we optimise theparameters.
We see the results without this featuredecrease around 1.1 points both in BLEU and WER(see line 2rd and 5th in Table 2).IBM Model 1.
We do the same as in the para-graph above, we do not consider the IBM Model1 and the IBM1?1.
Under these conditions, thetranslation?s quality decreases around 1.3 pointsboth in BLEU and WER (see line 3th and 5th inTable 2).152Modification of the Phrase Extraction.
Fi-nally, we made an experiment without modificationof the phrases?
length.
We can see the comparisonbetween: (1) the phrases of fixed maximum lengthof 3; and (2) including phrases with a maximumlength of 5 which can not be generated by smallerphrases.
We can see it in Table 2 (lines 4th and5th).
We observe that there is no much differencebetween the number of phrases, so this approachdoes not require more resources.
However, we getslightly better scores.5.3 Shared TaskThis section explains the participation of ?Exploit-ing Parallel Texts for Statistical Machine Transla-tion?.
We used the EuroParl data provided for thisshared task [4].
A word-to-word alignment was per-formed in both directions as explained in section2.
The phrase-based translation system which hasbeen considered implements a total of 7 features(already explained in section 4).
Notice that thelanguage model has been trained with the trainingprovided in the shared task.
However, the opti-mization in the parameters has not been repeated,and we used the parameters obtained in the sub-section above.
We have obtained the results in theTable 3.6 ConclusionsWe reported a new method to extract longerphrases without increasing the quantity of phrases(less than 0.5%).We also reported several features as P (e|f)which in combination with the functions of thesource-channel model provides significant improve-ment.
Also, the feature IBM1 in combination withIBM1?1 provides improved scores, too.Finally, we have optimized the parameters, andwe provided the final results which have been pre-sented in the Shared Task: Exploiting ParallelTexts for Statistical Machine Translation (June 30,2005) in conjunction with ACL 2005 in Ann Arbor,Michigan.7 AcknowledgementsThe authors want to thank Jose?
B. Marin?o, Adria`de Gispert, Josep M. Crego, Patrik Lambert andRafael E. Banchs (members of the TALP ResearchCenter) for their contribution to this work.References[1] P.F.
Brown, J. Cocke, S.A. Della Pietra,V.J.
Della Pietra, F. Jelinek, J.D.
Lafferty,R.L.
Mercer, and P.S.
Rossin.
A statistical ap-proach to machine translation.
ComputationalLinguistics, 16(2):79?85, June 1990.
[2] Josep M. Crego, Jose?
B. Marin?o, and Adria`de Gispert.
An Ngram-based Statistical Ma-chine Translation Decoder.
In Draft, 2005.
[3] G. Doddington.
Automatic evaluation ma-chine translation quality using n-gram co-ocurrence statistics.
In Proc.
ARPA Workshopon Human Language Technology, 2002.
[4] EuroParl: European Parliament Proceed-ings Parallel Corpus.
Available on-line at:http://people.csail.mit.edu/koehn/publica-tions/europarl/.
1996-2003.
[5] I.
Garc??a-Varea.
Traduccio?n Automa?tica es-tad?
?stica: Modelos de Traduccio?n basados enMa?xima Entrop?
?a y Algoritmos de Bu?squeda .UPV, Diciembre 2003.
[6] Giza++.
http://www-i6.informatik.rwth-aachen.de/?och/software/giza++.html/,1999.
[7] P. Koehn.
A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.2003.
[8] P. Koehn, F. J. Och, and D. Marcu.
Statisti-cal phrase-based translation.
In Proceedings ofthe Human Language Technology Conference(HLT-NAACL), pages 127?133, May 2003.
[9] J.A.
Nelder and R. Mead.
A simplex methodfor function minimization.
The ComputerJournal, 7:308?313, 1965.153Phr Length ?LM ?p(f |e) ?p(e|f) ?IBM1 ?IBM1?1 ?PP ?WP WER BLEU NIST # frases3 0.788 0.906 0 0 0 0 0 33.98 57.44 10.11 67.7M3+5length 0.788 0.941 0 0.771 0.200 3.227 0.448 28.97 64.71 11.07 68M3+5length 0.788 0.824 0.820 0 0 3.430 -0.083 29.17 64.59 10.99 68M3 0.746 0.515 0.979 0.514 0.390 1.537 -1.264 27.94 65.70 11.18 67.7M3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 27.88 65.82 11.23 68MTable 2: Results for the different experiments with optimized parameters in the direction SPA->ENGPhr Length ?LM ?p(f |e) ?p(e|f) ?IBM1 ?IBM1?1 ?PP ?WP BLEU # frases3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 29.84 34.8MTable 3: Results for the ACL training and ACL test (SPA->ENG)[10] F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,K.
Yamada, A. Fraser, S. Kumar, L. Shen,D.
Smith, K. Eng, V. Jain, Z. Jin, andD.
Radev.
A Smorgasbord of Features for Sta-tistical Machine Translation.
In Proceedings ofthe Human Language Technology Conference(HLT-NAACL), May 2004.
[11] F. J. Och and H. Ney.
The Alignment Tem-plate Approach to Statistical Machine Trans-lation.
Computational linguistics, 30:417?449,December 2004.
[12] Franz Josef Och and Hermann Ney.
Discrimi-native Training and Maximum Entropy Mod-els for Statistical Machine Translation.
InACL, pages pages 295?302, July 2002.
[13] Papineni, S.Roukos, and R.T. Ward.
Feature-based language understanding.
In EuropeanConf.
on Speech Communication and Technol-ogy, pages 1435?1438, September 1997.
[14] Papineni, S.Roukos, and R.T. Ward.
Maxi-mum likelihood and discriminative training ofdirect translation models.
In Proc.
Int.
Conf.on Acoustics, Speech, and Signal Proceedings,pages 189?192, May 1998.
[15] K.A.
Papineni, S. Roukos, T. Ward, and W.J.Zhu.
Bleu: a method for automatic evaluationof machine translation.
In Technical ReportRC22176 (W0109-022), IBM Research Divi-sion, 2001.
[16] A. Stolcke.
SRILM - An Extensible LanguageModeling Toolkit.
In Proceedings Intl.
Confer-ence Spoken Language Processing, September2002.
[17] R. Zens and H. Ney.
Improvements in Phrase-Based Statistical Machine Translation.
InProceedings of the Human Language Technol-ogy Conference (HLT-NAACL), pages 257?264, May 2004.154
