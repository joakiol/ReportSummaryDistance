Efficient Generation in Primitive Optimality TheoryJason EisnerDept.
of Computer and Information ScienceUniversity of Pennsylvania200 S. 33rd St., Philadelphia, PA 19104-6389, USAj eisner@linc, cis.
upenn, eduAbst ractThis paper introduces primitive Optimal-ity Theory (OTP), a linguistically moti-vated formalization of OT.
OTP specifiesthe class of autosegmental representations,the universal generator Gen, and the twosimple families of permissible constraints.In contrast to less restricted theories us-ing Generalized Alignment, OTP's opti-mal surface forms can be generated withfinite-state methods adapted from (Ellison,1994).
Unfortunately these methods taketime exponential on the size of the gram-mar.
Indeed the generation problem isshown NP-complete in this sense.
How-ever, techniques are discussed for makingEllison's approach fast in the typical case,including a simple trick that alone providesa 100-fold speedup on a grammar fragmentof moderate size.
One avenue for futureimprovements is a new finite-state notion,"factored automata," where regular lan-guages are represented compactly via for-mal intersections N~=IAi of FSAs.1 Why fo rmal i ze  OT?Phonology has recently undergone a paradigm shift.Since the seminal work of (Prince & Smolensky,1993), phonologists have published literally hun-dreds of analyses in the new constraint-based frame-work of Optimality Th.eory, or OT.
Old-style deriva-tional analyses have all but vanished from the lin-guistics conferences.The price of this creative ferment has been a cer-tain lack of rigor.
The claim for O.T as UniversalGrammar is not substantive or falsifiable withoutformal definitions of the putative Universal Gram-mar objects Repns,  Con, and Gen (see below).Formalizing OT is necessary not only to flesh it outas a linguistic theory, but also for the sake of compu-tational phonology.
Without knowing what classesof constraints may appear in grammars, we can sayonly so much about the properties of the system,or about algorithms for generation, comprehension,and learning.The central claim of OT is that the phonology ofany language can be naturally described as succes-sive filtering.
In OT, a phonological grammar fora language consists of ~ vector C1, C2, ?
.. C, of softconstra ints  drawn from a universal fixed set Con.Each constraint in the vector is a function that scorespossible output representations (surface forms):(1) Ci : Repns --* {0, 1, 2,...} (Ci E Con)If C~(R) = 0, the output representation R is said tosatisfy the ith constraint of the language.
Other-wise it is said to violate that constraint, where thevalue of C~(R) specifies the degree of violation.
Eachconstraint yields a filter that permits only minimalviolation of the constraint:(2) Filteri(Set)= {R E Set : Ci(R) is minimal}Given an underlying phonological input, its set oflegal surface forms under the grammar--typically ofsize 1--is just(3) Filter, (...Filter,.
(Filter 1 (Gen(input))))where the function Gen is fixed across languagesand Gen(input) C_ Repns is a potentially infiniteset of candidate surface forms.In practice, each surface form in Gen(input) mustcontain a silent copy of input, so the constraintscan score it on how closely its pronounced materialmatches input.
The constraints also score other cri-teria, such as how easy the material is to pronounce.If C1 in a given language is violated by just the formswith coda consonants, then Filterl(Gen(input)) in-cludes only coda-free candidates--regardless of theirother demerits, such as discrepancies from inputor unusual syllable structure.
The remaining con-straints are satisfied only as well as they can be giventhis set of survivors.
Thus, when it is impossibleto satisfy all constraints at once, successive filteringmeans early constraints take priority.Questions under the new paradigm include these:?
Generation.
How to implement he input-output mapping in (3)?
A brute-force approach313fails to terminate if Gen produces infinitelymany candidates.
Speakers must solve thisproblem.
So must linguists, if they are to knowwhat their proposed grammars predict.?
Comprehension.
How to invert the input-output mapping in (3)?
Hearers must solve this.?
Learn,ng.
How to induce a lexicon and aphonology like (1) for a particular language.given the kind of evidence available to child lan-guage learners?None of these questions is well-posed without restric-tions on Gen and Con.In the absence of such restrictions, computationallinguists have assumed convenient ones.
gllison(1994) solves the generation problem where Genproduces a regular set of strings and Con admitsall finite state transducers that can map a string toa number in unary notation.
(Thus Ci(R) = 4 if theCi transducer outputs the string l l l l  on input R.)Tesar (1995.
1996) extends this result to the casewhere Gen(mput) is the set of parse trees for inputunder some context-free grammar (CFG)3 Tesar'sconstraints are functions on parse trees such tha~Ci(\[A \[B1.. \] \[B~.-- .\]\]) can be computed from A, B:,B2, Ci(B1), and Ci(B~.).
The optimal tree can thenbe found with a standard dynamic-programmingchart parser for weighted CFGs.It is an important question whether these for-malisms are useful in practice.
On the one hand, arethey expressive nough to describe real languages?On the other, are they restrictive nough to admitgood comprehension and unsupervised-learning al-gorithms?The present paper sketches pr imi t ive  Opt ima l -i ty  Theory  (OTP)- -a  new formalization of OTthat is explicitly proposed as a linguistic hypothe-sis.
Representations are autosegmental, Gen is triv-ial, and only certain simple and phonologically ocalconstraints are allowed.
I then show the following:i.
Good news: Generation in OTP can be solvedattractively with finke-state methods.
The so-lution is given in some detail.2.
Good news: OTP usefully restricts the space ofgrammars to be learned.
(In particular.
Gener-alized Alignment is outside the scope of finite-state or indeed context-free methods.}3.
Bad news: While OTP generation is close to lin-ear on the size of the input form.
it is NP-hardon the size of the grammar, which for humanlanguages i  likely to be quite large.4.
Good yews: Ellison's algorithm can be improvedso that its exponential blowup is often avoided.
*This extension is useful for OT syntax but may havelittle application to phonology, since the context-freecase reduces to the regular case (i.e., Ellison) unless theCFG contains recursive productions.2 P r imi t ive  Opt ima l i ty  TheoryPrimitive Optimality Theory.
or OTP.
is a formal-ization of OT featuring ahomogeneous output repre-sentation, extremely' local constraints, and a simple,unrestricted Gen. Linguistic arguments t'or OTP'sconstraints and representations are given in !Eisner.1997).
whereas the present description focuses ,an itsformal properties and suitability for computationalwork.
An axiomatic treatment is omitted for rea-sons of space.
Despite its simplicity.
OTP appearscapable of capturing virtually all analyses found inthe (phonological) OT literature.2.1 Repns: Representat ions  in OTPTo represent imP\], OTP uses not the autosegmentairepresentation i  (4a) IGoldsmith.
1976: Goldsmith.1990) but rather the simplified autosegmental rep-resentation in (4b), which has no association lines.Similarly (Sa) is replaced by (Sb).
The central rep-resentational notion is that of a const i tuent  ime-line: an infinitely divisible line along on which con-stituents are laid out.
Every constituent has widthand edges.
(4) a. voi b. ,~o,\[ ?
J t ,o~haS/ n~Jt \]ha,1/ c!
cIc \]c!C C lab \ [  j lablabFor phonetic interpretation: \]~o, says to end voic-ing (laryngeal vibration).
At the same instant,\],,~, says to end nasality (raise velum}.
(5) a.O" O"/1 \  / ICVCVb.
~\[C\ [  ~ " : C ,-"~a\]?-.
j -V i .V.k timeline can carry tl~e full panoply of phonolog-ical and morphological ,:onstituents--an.vthing thatphonological constraints might have to refer to.Thus, a timetine bears not only autosegmental fe,.
'>tures like nasal gestures inasi and prosodic ,:on-stituents uch as syllables \[o'\].
but also stress marks\[x\], feature dpmains such as \[ATRdom\] (Cole L:Kisseberth, 1994) and morphemes such as \[Stem i.All these constituents are formally identicah eachmarks off an interval on the timeline.
Let Tiers de-note the fixed finite set of constituent ypes.
{has.~.
x, ATRdom.
S*.em .
.
.
.
}.It is always possible to recover the old representa-tion (4a) from the new one (4b), under the conven-tion that two constituents on the timeline are linkedif their interiors overlap (Bird & Ellison, 1994).
Theinter ior  of a constituent is the open interval that314excludes its edges: Thus, lab is linked to both con-sonants C in (4b), but the two consonants are notlinked to each other, because their interiors do notoverlap.By eliminating explicit association lines, OTPeliminates the need for faithfulness constraints onthem, or for well-formedness constraints against gap-ping or crossing of associations.
In addition, OTPcan refer naturally to the edges of syllables (or mor-phemes).
Such edges are tricky to define in (5a), be-cause a syllable's features are scattered across multi-ple tiers and perhaps hared with adjacent syllables.In diagrams of timelines, such as (4b) and (5b),the intent is that only horizontal order matters.Horizontal spacing and vertical order are irrelevant.Thus, a timeline may be represented as a finite col-lection S of labeled edge brackets, equipped with or-dering relations -~ and " that indicate which brack-ets precede ach other or fall in the same place.Valid timelines (those in Repns)  also require thatedge brackets come in matching pairs, that con-stituents have positive width, and that constituentsof the same type do not overlap (i.e., two con-stituents on the same tier may not be linked).2.2 Gem Input  and output  in OTPOT's principle of Containment (Prince & Smolen-sky, 1993) says that each of the potential outputs inRepns  includes a silent copy of the input, so thatconstraints evaluating it can consider the goodnessof match between input and output.
Accordingly,OTP represents both input and output constituentson the constituent imeline, but on different tiers.Thus surface nasal autosegments are bracketed with,~as\[ and \],,a~, while underlying nasal autosegmentsare bracketed with ,as\[ and \] .
.
.
.
The underliningis a notational convention to denote input material.No connection is required between \[nas\] and \[nas!except as enforced by constraints that prefer \[nas\]and \[nas\] or their edges to overlap in some way.
(6)shows a candidate in which underlying \[nas\] has sur-faced "in place" but with rightward spreading.
(6) ~o,\[ \]~o~.o,\[ \].o,Here the left edges and interiors overlap, but theright edges fail to.
Such overlap of interiors may beregarded as featural Input-Output Correspondencein the sense of (McCarthy & Prince, 1995).The lexicon and morphology supply to Gen anunderspec i f ied  t ime l ine - -a  partially ordered col-lection of input edges.
The use of a partial orderingallows the lexicon and morphology to supply float-ing tones, floating morphemes and templatic mor-phemes.Given such an underspecified timeline as lexicalinput, Gen  outputs the set of all fully specified time-lines that are consistent with it.
No new input con-stituents may be added.
In essence, Gen generatesevery way of refining the partial order of input con-stituents into a total order and decorating it freelywith output constituents.
Conditions such as theprosodic hierarchy (Selkirk, 1980) are enforced byuniversally high-ranked constraints, not by Gen. -~2.3 Con: The  pr imi t ive  const ra in tsHaving described the representations u ed, it is nowpossible to describe the constraints that evaluatethem.
OTP claims that Con is restricted to thefollowing two families of p r imi t ive  constra ints :(7) a --* /3 ("implication"):"Each ~ temporally overlaps ome ~.
"Scoring: Constraint(R) = number of a 's  in Rthat do not overlap any 8.
(8 )  a 3- /3 ("clash"):"Each cr temporally overlaps no/3.
"Scoring: Constraint(R) = number of (a, ';3)pairs in R such that the a overlaps the/3.That is, a --~ /3 says that a's attract /3's, whilea 3_ /3 says that c~'s repel/3's.
These are simple andarguably natural constraints; no others are used.In each primitive constraint, cr and /3 each spec-ify a phonological event.
An event is defined to beeither a type of labeled edge, written e.g.
~\[, orthe interior (excluding edges) of a type of labeledconstituent, written e .g .a .
To express some con-straints that appear in real phonologies, it is alsonecessary to allow, a and /3 to be non-empty con-junctions and disjunctions of events.
However, itappears possible to limit these cases to the forms in(9)-(10).
Note that other forms, such as those in(11), can be decomposed into a sequence of two or~The formalism is complicated slightly by the pos-sibility of deleting segments (syncope) or inserting seg-ments (epenthesis), as illustrated by the candidates be-low.
(i) Syncope (CVC ~ CC): the _V is crushed to zerowidth so the C's can be adjacent.c\[ Ic \]c~\[ 1~_ \]~v lv(ii) Epenthesis (CC ~ CVC): the C__'s are pushedapart.c\[ \]~ ~\[ \]~~_\[ \]~_ ~\[ \]~In order to Mlow adjacency of the surface consonants in(i), as expected by assimilation processes (and encour-aged by a high-ranked constraint), note that the underly-ing vowel must be allowed to have zero width--an optionavailable to to input but not output constituents.
Theinput representation must specify only v\[ "< Iv, notv\[ ~ \]v. Similarly, to allow (ii), the input representa-tion must specify only \]c, __.
c_~\[, not \]o, ~ c2\[.315more constraints.
3(9) ( c~1 and a~ and .. .  )
---* (/31 or/32 or .
.
.
)Scoring: Constraint(R) = number of sets ofevents {A1, A2,.
.
.}
of types (~l, a ,  .
.
.
.
respec-tively that all overlap on the timeline andwhose intersection does not overlap any eventof type/31,/3.,, ?
...(10) (a l  anda2 and .
.
. )
.L (/31 and/3~ and .
.
.
)Scoring: Constraint(R) = number of setsof events {A1,A~., .
.
.
,  B1,B~ .
.
.
.  }
of typesoq,a~ .
.
.
.
,/31,/32,... respectively that alloverlap on the timeline.
(Could a/so be notated:al ?
a2 ? ""
?
Zl ?
/~2 ?
"".
)(11)  ?X ~ ( fll and /32 ) \[cf.
o~ ~ /31 >> c~ - -~  /32\]( cq or ~.~ ) --* ,3 \[cf.
~1 ---* /3 >> a.~ --~ /3\]The unifying theme is that each primitive con-straint counts the number of times a candidate getsinto some bad local configuration.
This is an inter-val on the timeline throughout which certain events(one or more specified edges or interiors) are allpresent and certain other events (zero or more spec-ified edges or interiors) are all absent.Several examples of phonologically plausible con-straints, with monikers and descriptions, are givenbelow.
(Eisner, 1997) shows how to rewrite hun-dreds of constraints from the literature in the primi-tive constraint notation, and discusses the problem-atic case of reduplication.
(Eisner, in press) givesa detailed stress typology using only primitive con-straints; in particular, non-local constraints suchas FTBIN, FOOTFORM, and Generalized Alignment(McCarthy & Prince, 1993) are eliminated.
(12) a. ONSET: a \ [ -  C\["Every syllable starts with a consonant."b.
NONFINALITY: \]Wo,-d _1_ \]F"The end of a word may not be footed.
"c o\[ ,l"eet start and end on syllable boundaries."d.
PACKFEET:  \]F ""+ F\["Each foot is followed immediately by an-other foot; i.e., minimize the number of gapsbetween feet.
Note that the final foot, if any,will always violate this constraint.
"e, NOCLASH: \]X A_ x\["Two stress marks may not be adjacent."f.
PROGRESSIVEVOICING: \]voi _1_ C\["If the segment preceding a consonant isvoiced, voicing may not stop prior to the3Such a sequence does alter the meaning slightly.
Toget the exact original meaning, we would have to de-compose into so-cMled "unranked" constraints, wherebyCi (R) is defined as C,, (R)+Ci~ (R).
But such ties under-mine OT's idea of strict ranking: they confer the powerto minimize linear functions such as (C1 + C1 + C1 +C2 + C3 + C3)(R) = 3C1 (R) + C2(R) + 2C3(R).
For thisreason, OTP currently disallows unranked constraints; Iknow of no linguistic data that crucially require them.consonant but must be spread onto it.
"g, NASVOI: nas - -  voi"Every nasal gesture must be at least partlyvoiced."h.
FULLNASVOI: has _\[_ voi \ [ ,  has I \]voi"A nasal gesture may not be only partlyvoiced."i.
MAX(VOi) or PARSE(voi): vo._i ~ voi"Underlying voicing features urface."j.
DEP(voi) or FILL(voi): voi ---, voi"Voicing features appear on the surface onlyif they are a/so underlying."k.
NoSPREADRIGHT(voi): voi _1_ \]vo__i_"Underlying voicing may not spread right-ward as in (6).
"h NONDEGENERATE: F - -~\ ["Every foot must cross at least  one  mornboundary ,\[."m.
TAUTOMORPHEMICFOOT: F _\]_ .~Iorph\["No foot may cross a morpheme boundary.
"3 F in i te -s ta te  generat ion  in  OTP3.1 A s imple  generat ion  a lgor i thmRecall that the generation problem is to find theoutput set S,~, where(13) a.
So = Gen(inpu~) C_ Repnsb.
Si+l = Filteri+l(Si) C SiSince in OTP, the input is a partial order of edgebrackets, and Sn is a set of one or more total orders(timelines), a natural approach is to successively re-fine a partial order.
This has merit.
However, notevery Si can be represented as a single partial order,so the approach is quickly complicated by the needto encode disjunction.A simpler approach is to represent Si (as wellas inpu~ and Repns)  as a finite-state automaton(FSA), denoting a regular set of strings that encodetimelines.
The idea is essentially due to (Ellison,1994), and can be boiled down to two lines:(14) E l l i son 's  a lgor i thm (variant).So = input N Repns= all conceivable outputs containing inputSi+l = BestP~tths(Si N Ci+l)Each constraint Ci must be formulated as an edge-weighted FSA that scores candidates: Ci accepts anystring R, on a singl e path of weight Ci(R).
4 Best-Paths is Dijkstra's "single-source shortest paths"algorithm, a dynamic-programming al orithm thatprunes away all but the minimum-weight paths inan automaton, leaving an unweighted automaton.OTP is simple enough that it can be described inthis way.
The next section gives a nice encoding.4Weighted versions of the state-labeled finite au-tomata of (Bird & EUison, 1994) could be used instead.3163.2  OTP  w i th  automataWe may encode each timeline as a string over anenormous alphabet E. If \ [T iersl  = k, then eachsymbol in E is a k-tuple, whose components describewhat is happening on the various tiers at a givenmoment.
The components are drawn from a smallera lphabet A = { \[, \] ,  l, +, -}.
Thus at any time, theith tier may be beginning or ending a constituent ( \[,\] ) or both at once ( I ), or it may be in a steady statein the interior or exterior of a constituent (+, -) .At a minimum, the string must record all momentswhere there is an edge on some tier.
If all tiers are ina steady state, the string need not use any symbolsto say so.
Thus the string encoding is not unique.
(15) gives an expression for all strings that cor-rectly describe the single tier shown.
(16) describesa two-tier t imeline consistent with (15).
Note thatthe brackets on the two tiers are ordered with re-spect to each other.
Timelines like these could beassembled morphological ly from one or more lexicalentries (Bird & Ellison, 1994), or produced in thecourse of algor ithm (14).
(15) \]= -*\[+*1+'3-*(16)(-,->*<\[:,-)<,,->*<+, r><+, )*(I, +)<+, +)*(+, \])(+,-)*(*, \[)(*, +)*C\], 1)We store timeline expressions like (16) as deter-ministic FSAs.
To reduce the size of these automata,it is convenient to label arcs not with individual el-ements of El (which is huge) but with subsets of E,denoted by predicates.
We use conjunctive predi-cates where each conjunct lists the allowed symbolson a given tier:(17) +F, 3cr, \[l+-voi (arc label w/ 3 conjuncts)The arc label in (17) is said to ment ion  the tiersF, o', voi E T iers .
Such a predicate allows any sym-bol from A on the tiers it does not mention.The input FSA constrains only the input tiers.
In(14) we intersect it with Repns ,  which constrainsonly the output  tiers.
Repns  is defined as the inter-section of many automata  exactly like (18), calledt ie r  ru les ,  which ensure that brackets are properlypaired on a given tier such as F (foot).
(18) -F ,+FLike the tier rules, the constraint automata  Ci aresmall and determinist ic and can be built automat-ically.
Every edge has weight O or 1.
With somecare it is possible to draw each Ci with two or fewerstates, and with a number of arcs proport ional tothe number of tiers mentioned by the constraint.Keeping the constraints mall  is important  for ef-ficiency, since real languages have many constraintsthat must be intersected.Let us do the hardest case first.
An impl icat ionconstraint has the general form (9).
Suppose that allthe c~i are interiors, not edges.
Then the constrainttargets intervals of the form a = c~1 f'l c~2 fq ?
?
.. Eacht ime such an interval ends without any 3j havingoccurred during it, one violation is counted:(19) Weight-1 arcs are shown in bold; others areweight-0.
(other)(other)b during a ~ / ~" -1 /  I Ia endsA candidate that does see a #j during an c~ can goand rest in the r ight-hand state for the durat ion ofthe a.Let us fill in the details of (19).
How do we detectthe end of an a?
Because one or more of the ai  end(\],  I), while all the al  either end or continue (+), sothat we know we are leaving an a.
5 Thus:(20) (in all ai)- (some bj)in all aiAn unusually complex example is shown in (21).Note that to preserve the form of the predicatesin (17) and keep the automaton deterministic,  weneed to split some of the arcs above into multi-ple arcs.
Each flj gets its own arc, and we mustalso expand set differences into mult iple arcs, usingthe scheme W - z A y A z = W V ~(x A y A z) =(W A ~x) V (W A z A-~y) V (W A x A y A -~:).sit is important to take \], not +, as our indication thatwe have been inside ?
constituent.
This means that thetimeline ( \[, -)(+, -)*(+, \[)(% +)*('\], +)(-, +)*(-, \]) cannotavoid violating a clash constraint simply by instantiat-ing the (+, +)* part as e. Furthermore, the \] conventionmeans that a zero-width input constituent (more pre-cisely, a sequence of zero-width constituents, representedas a single 1 symbol) will often act as if it has an interior.Thus if V syncopates as in footnote 2, it still violates theparse constraint _V - -  V. This is an explicit property ofOTP: otherwise, nothing that failed to parse would everviolate PARSE, because it would be gone!On the other hand, "l does not have this special roleon the right hand side of ---+ , which does not quantifyuniversally over an interval.
The consequence for zero-width consituents i that even if a zero-width 1/_" overlaps(at the edge, say) with a surface V, the latter cannotclaim on this basis alone to satisfy FILL: V - -  V__.
Thistoo seems like the right move linguistically, although fur-ther study is needed.317(21) (pandq)  --* (borc \ [ )+p +q \[\]l-b \]+-cHow about other cases?
If the antecedent ofan implication is not.
an interval, then the con-straint needs only one state, to penalize mo-ments when the antecedent holds and the con-sequent does not.
Finally, a clash constraintcq I a2 _1_ ... is identical to the implicationconstraint (or1 and a.~ and .
.
. )
--* FALSE.
ClashFSAs are therefore just degenerate versions of im-plication FSAs, where the arcs looking for/3j do notexist because they would accept no symbol.
(22)shows the constraints (p and \]q ) --+ b and p 3_ q.
(22) +p +q4 Computat iona l  requ i rements4.1 Genera l i zed  A l ignment  is not  f ln i te-stateEllison's method can succeed only on a restrictedformalism such as OTP, which does not admit suchconstraints as the popular Generalized Alignment(GA) family of (McCarthy & Prince, 1993).
A typ-ical GA constraint is ALIGN(F, L, Word, L), whichsums the number of syllables between each left footedge F\[ and the left edge of the prosodic word.
Min-imizing this sum achieves a kind of left-to-right it-erative footing.
OTP argues that such non-local,arithmetic onstraints can generally be eliminatedin favor of simpler mechanisms (Eisner, in press).Ellison's method cannot directly express the aboveGA constraint, even outside OTP, because it cannotcompute a quadratic function 0 + 2 + 4 + -.. on astring like \[~cr\]F \[~a\]r \ [~\ ] r  '" '.
Path weights in anFSA cannot be more than linear on string length.Perhaps the filtering operation of any GA con-straint can be simulated with a system of finite-state constraints?
No: GA is simply too powerful.The proof is suppressed here for reasons of space,but it relies on a form of the pumping lemma forweighted FSAs.
The key insight is that among can-didates with a fixed number of syllables and a single(floating) tone, ALIGN(a, L, H, L) prefers candidateswhere the tone docks at the center.
A similar argu-ment for weighted CFGs (using two tones) shows thisconstraint o be too hard even for (Tesar, 1996).4.2 Generat ion  is NP-complete  even in OTPWhen algorithm (14) is implemented literally andwith moderate care, using an optimizing C compileron a 167MHz UltraSPARC, it takes fully 3.5 minutes(real time) to discover a stress pattern for the syl-lable sequence ~.6  The automatabecome impractically huge due to intersections.Much of the explosion in this case is introducedat the start and can be avoided.
Because Repnshas 21Tiersl = 512 states, So, $1, and $2 eachhave about 5000 states and 500,000 to 775,000 arcs.Thereafter the S~ automata become smaller, thanksto the pruning performed at each step by BestPaths.This repeated pruning is already an improvementover Ellison's original algorithm (which saves prun-ing till the end, and so continues to grow exponen-tially with every new constraint).
If we modify (14)further, so that each tier rule from Repns  is inter-sected with the candidate set only when its tier isfirst mentioned by a constraint, then the automataare pruned back as quickly as they grow.
They haveabout 10 times fewer states and 100 times fewer arcs.and the generation time drops to 2.2 seconds.This is a key practical trick.
But neither it norany other trick can help for all grammars, for in theworst case, the OTP generation problem is NP-hardon the number of tiers used by the grammar.
Thelocality of constraints does not save us here.
ManyNP-complete problems, such as graph coloring orbin packing, attempt o minimize some global countsubject o numerous local restrictions.
In the case ofOTP generation, the global count to minimize is thedegree of violation of Ci, and the local restrictionsare imposed by C1, C2,... Ci-1.Proof  of  NP-hardness  (by polytime reductionfrom Hamilton Path).
Given G = (V(G), E(G)),an n-vertex directed graph.
Put T iers  = V(G)tO{Stem, S}.
Consider the following vector of O(n -~)primitive constraints (ordered as shown):(23) a. VveV(a) :  ~\ [ -~s \ [b. Vv E V(G): \]~ - -  \]sc.
Vv e V(G): St-era -~  vd.
Stem .1_ Se.
Vu, ve  V(G) s.t.
uv ~ E(G): \]u .L o\[f. Is -SThe grammar is taken from the OTP stress typol-ogy proposed by (Eisner, in press).
It has tier rules for 9tiers, and then spends 26 constraints on obvious univer-sal properties of morns and syllables, followed by 6 con-straints for universal properties of feet and stress marksand finally 6 substantive constraints that can be freelyreranked to yield different stress ystems, such as left-to-right iambs with iambic lengthening.318Suppose the input is simply \[Stem\].
FilteringGen(input) through constraints (23a-d), we are leftwith just those candidates where Stem bears n (dis-joint) constituents of type S, each coextensive witha constituent bearing a different label v E V(G).
(These candidates atisfy (23a-c) but violate (23d)n times.)
(23e) says that a chain of abutting con-stituents \ [u Iv Iw\ ] .
?
?
i s  allowed only if it correspondsto a path in G. Finally, (23f) forces the grammar tominimize the number of such chains.
If the minimumis 1 (i.e., an arbitrarily selected output candidate vi-olates (23f) only once), then G has a Hamilton path.When confronted with this pathological case, thefinite:state methods respond essentially by enumer-ating all possible permutations of V(G) (thoughwith sharing of prefixes).
The machine state stores,among other things, the subset of V(G) that has al-ready been seen; so there are at least 2 ITiersl states.It must be emphasized that if the grammar isfixed in advance, algorithm (14) is close to linearin the size of the input form: it is dominated bya constant number of calls to Dijkstra's BestPathsmethod, each taking time O(\[input arcs\[ log \[inputstatesl).
There are nonetheless three reasons whythe above result is important.
(a) It raises the prac-tical specter of huge constant factors (> 2 4?)
for realgrammars.
Even if a fixed grammar can somehow becompiled into a fast form for use with many inputs,the compilation itself will have to deal with this con-stant factor.
(b) The result has the interesting im-plication that candidate sets can arise that cannotbe concisely represented with FSAs.
For if all Siwere polynomial-sized in (14), the algorithm wouldrun in polynomial time.
(c) Finally, the grammaris not fixed in all circumstances: both linguists andchildren crucially experiment with different heories.4.3 Work  in progress:  Factored  automataThe previous ection gave a useful trick for speedingup Ellison's algorithm in the typical case.
We arecurrently experimenting with additional improve-ments along the same lines, which attempt o de-fer intersection by keeping tiers separate as long aspossible.The idea is to represent the candidate set S/not asa large unweighted FSA, but rather as a collection Aof preferably small unweighted FSAs, called factors,each of which mentions as few tiers as possible.
Thiscollection, called a factored automaton ,  serves asa compact representation of hA.
It usually has farfewer states than 71.,4 would if the intersection werecarried out.For instance, the natural factors of So are inputand all the tier rules (see 18).
This requires onlyO(\[Tiers\[ + \[input\[) states, not O(21Tiersl.
\[input\[).Using factored automata helps Ellison's algorithm(14) in several ways:?
The candidate sets Si tend to be representedmore compactly.?
In (14), the constraint Ci+l needs to be inter-sected with only certain factors of Si.?
Sometimes Ci+l does not need to be intersectedwith the input, because they do not mentionany of the same tiers.
Then step i + 1 can beperformed in time independent of input length.Example: input = , which isa 43-state automaton, and C1 is F - -  x, which saysthat every foot bears a stress mark.
Then to find$1 = BestPaths(S0 71 C1), we need only considerS0's tier rules for F and x, which require well-formedfeet and well-formed stress marks, and combine themwith C1 to get a new factor that requires stressedfeet.
No other factors need be involved.The key operation in (14) is to find Bestpaths(A 71C), where .4 is an unweighted factored automatonand C is an ordinary weighted FSA (a constraint).This is the best  in tersect ion  problem.
For con-creteness let us suppose that C encodes F ---* x, atwo-state constraint.A naive idea is simply to add F ---* x to ..4 asa new factor.
However, this ignores the BestPathsstep: we wish to keep just the best paths in r\[  ~ x\[that are compatible with A.
Such paths might belong and include cycles in F\[ ---* x\[.
For example,a weight-1 path would describe a chain of optimalstressed feet interrupted by a single unstressed onewhere A happens to block stress.A corrected variant is to put I -- 71.A and runBestPaths on I 71 C. Let the pruned result be B.We could add B directly back to to ,4 as a newfactor, but it is large.
We would rather add a smallerfactor B' that has the same effect, in that 1 71 B' =1 71 B.
(B' will look something like the original C,but with some paths missing, some states split, andsome cycles unrolled.)
Observe that each state of Bhas the form i x c for some i E I and c E C. Weform B' from B by "re-merging" states i x c andi' x c where possible, using an approach similar toDFA minimization.Of course, this variant is not very efficient, becauseit requires us to find and use I = N.4.
What wereally want is to follow the above idea but use asmaller I, one that considers just the relevant factorsin .,4.
We need not consider factors that will notaffect the choice of paths in C above.Various approaches are possible for choosing suchan I.
The following technique is completely general,though it may or may not be practical.Observe that for BestPaths to do the correctthing, I needs to reflect the sum total of .A's con-straints on F and x, the tiers that C mentions.
Moreformally, we want I to be the projection of the can-didate set N.A onto just the F and x tiers.
Unfortu-nately, these constraints are not just reflected in thefactors mentioning F or x, since the allowed con-figurations of F and x may be mediated through319additional factors.
As an example, there may be afactor mentioning F and ?, some of whose paths areincompatible with the input factor, because the lat-ter allows ?
only in certain places or because onlyallows paths of length 14.1.
Number the tiers such that F and x are num-bered 0, and all other tiers have distinct positivenumbers.2.
Partition the factors of .4 into lists L0, L1,L2, .
.
.
Lk, according to the highest-numberedtier they mention.
(Any factor that mentionsno tiers at all goes onto L0.)3.
If k -- 0, then return MLk as our desired I.4.
Otherwise, MLk exhausts tier k's ability to me-diate relations among the factors.
Modify thearc labels of ML} so that they no longer restrict(mention) k. Then add a determinized, mini-mized version of the result to to Lj, where j isthe highest-numbered tier it now mentions.5.
Decrement k and return to step 3.If n.4 has k factors, this technique must per-form k - 1 intersections, just as if we had putI = n.4.
However, it intersperses the intersectionswith determinization and minimization operations,so that the automata being intersected tend notto be large.
In the best case, we will have k -1 intersection-determinization-minimizations thatcost O(1) apiece, rather than k -1  intersections thatcost up to 0(2 k) apiece.5 Conc lus ionsPrimitive Optimality Theory, or OTP, is an attemptto produce a a simple, rigorous, constraint-basedmodel of phonology that is closely fitted to the needsof working linguists.
I believe it is worth study bothas a hypothesis about Universal Grammar and as aformal object.The present paper introduces the OTP formal-ization to the computational linguistics community.We have seen two formal results of interest, bothhaving to do with generation of surface forms:?
OTP's  generative power is low: finite-stateoptimization.
In particular it is more con-strained than theories using Generalized Align-ment.
This is good news for comprehension a dlearning.?
OTP's  computational complexity, for genera-tion, is nonetheless high: NP-complete on thesize of the grammar.
This is mildly unfortunatefor OTP and for the OT approach in general.It remains true that for a fixed grammar, thetime to do generation is close to linear on thesize of the input (Ellison, 1994), which is heart-ening if we intend to optimize long utteranceswith respect o a fixed phonology.Finally, we have considered the prospect of buildinga practical tool to generate optimal outputs fromOT theories.
We saw above to set up the represen-tations and constraints efficiently using determinis-tic finite-state automata, and how to remedy somehidden inefficiencies in the seminal work of (Elli-son, 1994), achieving at least a 100-fold observedspeedup.
Delayed intersection and aggressive prun-ing prove to be important.
Aggressive minimizationand a more compact.
"factored" representation ofautomata may also turn out to help.ReferencesBird, Steven, &: T. Mark Ellison.
One Level Phonol-ogy: Autosegmental representations and rules asfinite automata.
Comp.
Linguistics 20:55-90.Cole, Jennifer, ~z Charles Kisseberth.
1994.
An op-timal domains theory of harmony.
Studies in theLinguistic Sciences 24: 2.Eisner, Jason.
In press.
Decomposing FootForm:Primitive constraints in OT.
Proceedings of SCIL8, NYU.
Published by MIT Working Papers.
(Available at http://ruccs.rutgers.edu/roa.html.
)Eisner, Jason.
What constraints hould OT allow?Handout for talk at LSA, Chicago.
(Available athttp://ruccs.rutgers.edu/roa.html.
)Ellison, T. Mark.
Phonological derivation in opti-mality theory.
COLING '94, 100%1013.Goldsmith, John.
1976.
Autosegmental phonology.Cambridge, Mass: MIT PhD.
dissertation.
Pub-lished 1979 by New York: Garland Press.Goldsmith, John.
i990.
Autosegmental nd metricalphonology.
Oxford: Blackwell Publishers.McCarthy, John, & Alan Prince.
1993.
General-ized alignment.
Yearbook of Morphology, ed.
GeertBooij & 3aap van Marle, pp.
79-153.
Kluwer.McCarthy, John and Alan Prince.
1995.
Faithful-ness and reduplicative identity.
In Jill Beckmanet al, eds., Papers in Optimality Theory.
UMass.Amherst: GLSA.
259-384.Prince, Alan, & Paul Smolensky.
1993.
Optimalitytheory: constrainl interaction in generative gram-mar.
Technical Reports of the Rutgers UniversityCenter for Cognitive Science.Selkirk, Elizabeth.
1980.
Prosodic domains inphonology: Sanskrit revisited.
In Mark Aranoffand Mary-Louise Kean, eds., Juncture, pp.
107-129.
Anna Libri, Saratoga, CA.Tesar, Bruce.
1995.
Computational Optimality The-ory.
Ph.D. dissertation, U. of Colorado, Boulder.Tesar, Bruce.
1996.
Computing optimal descriptionsfor Optimality Theory: Grammars with context-free position structures.
Proceedings of the 34thAnnual Meeting of the ACL.320
