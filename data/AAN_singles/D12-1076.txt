Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 832?842, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExplore Person Specific Evidence in Web Person Name DisambiguationLiwei Chen, Yansong Feng, Lei Zou, Dongyan ZhaoInstitute of Computer Science and TechnologyPeking UniversityBeijing{clwclw88,fengyansong,zoulei,zhaodongyan}@pku.edu.cnAbstractIn this paper, we investigate different usagesof feature representations in the web personname disambiguation task which has been suf-fering from the mismatch of vocabulary andlack of clues in web environments.
In litera-ture, the latter receives less attention and re-mains more challenging.
We explore the fea-ture space in this task and argue that collectingperson specific evidences from a corpus levelcan provide a more reasonable and robust es-timation for evaluating a feature?s importancein a given web page.
This can alleviate thelack of clues where discriminative features canbe reasonably weighted by taking their corpuslevel importance into account, not just relyingon the current local context.
We therefore pro-pose a topic-based model to exploit the personspecific global importance and embed it intothe person name similarity.
The experimen-tal results show that the corpus level topic in-formation provides more stable evidences fordiscriminative features and our method out-performs the state-of-the-art systems on threeWePS datasets.1 IntroductionResolving ambiguity associated with person namesfound on the Web is a key challenge in many Internetapplications, such as information retrieval, questionanswering, open information extraction, automaticknowledge acquisition(Wu and Weld, 2008) and soon.
For example, if you want to know more about aguy named George Foster and feed Yahoo!
with hisname, the results are not satisfactory where you getmore than 40 different persons named George Fos-ter scattering in the top 100 returned pages.
Noneof the dominant search engines currently helps usersgroup those returned pages into clusters accordingto whether they refer to the same person.
Users thushave to either read those pages carefully or adjusttheir queries by adding extra modifiers.
This moti-vates an intensive study in automatically resolvingperson name ambiguity in various web applications.However, resolving web person name ambiguityis not a trivial task.
Due to the difficulties in fig-uring out or predicting the number of namesakesin the returned pages, the task has been investi-gated in an unsupervised learning fashion in the lit-erature, which is apparently different from the tra-ditional word sense disambiguation or entity link-ing/disambiguation tasks, where the inventories ofcandidate word senses or entities are usually knowngiven the target word or entity mention.A general framework for this task can be formu-lated as first extracting various features from the webpages, and then grouping these pages into severalclusters each of which is assumed to represent onespecific person.
Despite of the inevitably noisy na-ture of web data, a key challenge is how to handlethe data sparsity problem which we mean as: mis-match of vocabulary and lack of clues.
The for-mer refers to the case that two web pages may de-scribe the same person but use different words thusthe word overlap between them are small.
Vari-ous features, including entities, biographical infor-mation, URL, etc., have been introduced to bridgethe gap(Mann and Yarowsky, 2003; Kalashnikovet al2008a; Ikeda et al2009; Jiang et al2009),832and external knowledge resources are also employedto capture the semantic relationship between enti-ties(Han and Zhao, 2009, 2010).
However, a morechallenging scenario is that there are few clues avail-able in the web pages.
For example, there is a pagementioning a nutritionist Emily Bender in WePS2dataset(Javier et al2009).
Throughout the wholepage we can find only one word, nutrition, related toher identification, while other pages about the nu-tritionist in the dataset contain substantial materi-als about her profession and job.
In this case, cur-rent efforts, focusing on either feature engineeringor background knowledge, are incapable to exploit-ing these limited clues from the current page to thewhole Emily Bender document set, where nutrition,as an important feature for recognizing a nutritionist,should be paid more attention.As far as we know, there is less work focusingon exploring person specific information to relievethe lack of clues problem.
Traditional vector spacemodel (VSM) is most widely used to accommodatevarious features, but it ignores any relations betweenthem(Mann and Yarowsky, 2003; Ikeda et al2009).Beyond bag-of-features, two kinds of features areexplored, co-occurrences of entities and Wikipediabased semantic relationship between entities, bothof which provide a reasonable relatedness for en-tity pairs.
More recent works adopt one of theserelationships(Jiang et al2009; Kalashnikov et al2008a; Han and Zhao, 2009).
Han and Zhao tryto model both aspects, but their co-occurrence es-timation, estimated from held-out resources, fails tocapture the person specific importance for a feature,which is crucial to enhance limited clues in a cor-pus level, e.g., the significance of nutrition for EmilyBender in WePS1 dataset.In this paper, we explore different usages of fea-tures and propose an approach which mines crossdocument information to capture the person specificimportance for a feature.
Specifically, we construct asemantic graph from Wikipedia concepts appearingin all documents that contain the target name (whichwe refer to name observation set), then group theminto several topics and further weight each feature byconsidering both the relatedness of the feature to itscorresponding topic and the importance of this topicin the current name observation set.
By incorporat-ing both the Wikipedia and topic information intoour person name similarity, our model exploits bothWikipedia based background knowledge and per-son specific importance.
We argue that the corpuslevel importance provides more stable evidences fordiscriminative features in various scenarios, espe-cially the tough case.
We compared our model withthe state of the arts on three WePS datasets (fromthe First and Second Web People Search Cluster-ing Task), and our experiments show that our modelconsistently outperforms other competitive modelson all three datasets.In the rest of this paper, we first review relatedwork, and in Section 3, show how we exploit theperson specific importance in our disambiguationmodel.
Experiment results are discussed in Sec-tion 4.
We conclude this paper in Section 5.2 Related WorkWeb person name ambiguity resolution can be for-mally defined as follows: Given a set of webpages {d1, d2, ..., dn}, where each page di (i =1, ..., n) contains an ambiguous name N which maycorrespond to several persons holding this nameamong these pages.
The disambiguation systemshould group these name observations into j cluster{c1, c2, ..., cj} each of which is expected to containweb pages about the same person.As mentioned before, the task is usually formu-lated in a unsupervised fashion, including two steps:feature extraction and person clustering.
Most re-search efforts so far have been made to the for-mer, exploring various features according to spe-cific applications, while the second step is currentlydominated by hierarchical agglomerative cluster-ing (HAC).
According to the reliance of extraknowledge resources, existing works can be catego-rized into non-resource methods and resource-basedmethods.
Non-resource methods extract various lo-cal features from the context of ambiguous names,and compute the similarity between feature vectors.These features include plain words(Bagga and Bald-win, 1998), biographical information(Mann andYarowsky, 2003; Niu et al2004), named enti-ties, compound key phrases, hyperlinks(Ikeda et al2009), etc.
The similarity between namesakes areusually measured by the cosine similarity(Baggaand Baldwin, 1998), or other graph based met-833rics(Iria et al2007; Kalashnikov et al2008a;Jiang et al2009).
Those methods pay more at-tention to extracting informative features and theirco-occurrences, but they usually treat the features lo-cally, and ignore the semantic relatedness of featuresbeyond the current document.Resource-based approaches, on the other hand,can leverage external resources to benefit from richbackground knowledge, which is crucial to rem-edy the data sparsity problem.
The employed re-sources include raw texts available on the web andonline encyclopedias.
Kalashnikov et alnd Yim-ing et alse extra web corpora to obtain co-occurrences between named entities.
Rao et alseGoogle Snippets to provide more contexts.
By em-ploying Wikipedia, the largest online encyclopedia,rich background knowledge about the semantic re-latedness between entities can be leveraged to im-prove the disambiguation performance, and relievethe coverage problem, to some extent.
Bunescuand Pasca and Cucerzan utilize Wikipedia?s cate-gory hierarchy to disambiguate entities, while Pilzuses Wikipedia?s link information.
Han and Zhaoadopt Wikipedia semantic relatedness to computethe similarity between name observations.
They alsocombine multiple knowledge sources and captureexplicit semantic relatedness between concepts andimplicit semantic relationship embedded in a seman-tic graph simultaneously(Han and Zhao, 2010).Most approaches discussed above explore vari-ous features in the current page or rely on exter-nal knowledge resources to bridge the vocabularygap, but pay less attention to the lack of clues sincethey ignore the person specific evidence in the cur-rent corpus level.
Our model focuses on solving thedata sparsity problem by utilizing other web pagesin the same name observation set to provide a robustbut person specific weighting for discriminative fea-tures beyond the current document alone.
In termsof extra resources, the Wikipedia based model (WS)by Han and Zhao (2009) is close to our model.
TheWS model uses Wikipedia to capture the relation-ship between entities in the local context to bridgethe vocabulary gap, but it is incapable to evaluatethe importance of a feature with regarding to the tar-get name, hence is unable to make use of limitedclues in the current web page.
Our method capturesperson specific evidences by generating topics fromall concepts in the current name observation set andweighting a feature accordingly.
In this case, dis-criminative features that are sparse in the currentpage can be globally weighted so as to provide amore accurate and stable person name similarity.3 The ModelOur model consists of three steps: feature extrac-tion, topic generation and name disambiguation.
Foran ambiguous name, we first extract three types offeatures and construct a semantic graph from allWikipedia concepts extracted from the current nameobservation set.
We then collect global person spe-cific evidences by clustering these concepts on thegraph into different topics, which in turn are usedto weight each concept by considering the impor-tance of its corresponding topic in the current nameobservation set and its highly related neighbors inboth the topic and its local context.
At last, we in-corporate the proposed topic representation into theperson name similarity functionand adopt the hierar-chical agglomerative clustering (HAC) algorithm togroup these web pages.3.1 Feature ExtractionWe extract features from the contexts of ambiguousnames, including Wikipedia concepts, named enti-ties and biographical information, such as email ad-dresses, phone numbers and birth years.Wikipedia Concept Extraction Each concept inWikipedia is described by an article containing hy-perlinks to other concepts which are supposed torelated to the current one.
All the linking rela-tions in Wikipedia construct a huge semantic graph,where we can mine rich semantic relationship be-tween concepts(David and Ian, 2008).
We col-lect Wikipedia concepts from all web pages in thedataset by comparing all n-grams (up to 8) fromthe dataset to Wikipedia anchor text dictionary andchecking whether it is a Wikipedia concept surfaceform.
We further prune the extracted concepts ac-cording to their keyphraseness(Mihalcea and Cso-mai, 2007).
Initially, each concept is weighted ac-cording to its average semantic relateness(David andIan, 2008) with other concepts in the current page.Named Entity and Biographical Information Ex-traction Although Wikipedia concepts can pro-834vide rich background knowledge, they suffer fromthe limited coverage.
It is common that somediscriminative features are not likely to be foundin Wikipedia, such as names of infamous peopleor organizations, email addresses, phone numbers,etc.
We therefore extract two extra kinds of fea-tures, named entities that do not appear in theWikipedia anchor text dictionary, and biographicalinformation.
We use Stanford Named Entity Rec-ognizer(Finkel et al2005) to collect named entitieswhich are not in the Wikipedia list.
We use regularexpressions to extract email address, phone numbersand birth years.
For convenience, we will also callconcept features for Wikipedia concept features andnon-concept features for the other two in the rest ofthis paper.3.2 Topic Generation and Weighting SchemeNow we proceed to describe the key step of ourmodel, topic generation and weighting strategy.
Thepurpose of introducing topics into our model is toexploit the corpus level importance of a feature fora given name so that we will not miss any discrim-inative features which are few in the current nameobservation but have shown significant importanceover the whole name observation set.Graph Construction In our model, we capturethe topic structure through a semantic graph.
Specif-ically, for each name observation set, we connectall Wikipedia concepts appearing in the current ob-servation set by their pairwise semantic relatedness-David and Ian (2008)to form a semantic graph.The constructed graph is usually very dense sinceany pair of unrelated concepts would be connectedby a small semantic relatedness resulting in manylight-weighted or even meaningless edges.
Wetherefore propose to prune some light-weightededges to make the graph stable and easier to harvestreasonable topics.
We use the following strategies toprune the graph:?
If an edge?s weight is lower than a predefinedthreshold, it will be pruned.?
If two vertices of an edge do not co-occur inany web page of the current observation set,then this edge will be pruned.HomeRunMajorLeagueBaseballStolenBaseCincinnatiRedsShortstopSportsLeagueCornerbackTackleNationalFootballLeague ProFootballWeekly0.38620.42280.37990.29760.32960.26970.24450.34670.36280.41450.4008 0.32050.27380.35670.32010.3136 0.2245Figure 1: An abridged example of the semantic graphfor George Foster.
The green node Sports League is ahub node, and the yellow node Pro Football Weekly is anoutlier.The second rule is set to be strict and is proposedto handle the following circumstance.
Some gen-eral concepts, such as swimming, football, basket-ball and golf, will be measured highly related witheach other by Wikipedia semantic relatedness andthus are very likely to be grouped into one topic,however, they are discriminative on their own whendisambiguating different persons.
For example, theconcept swimming is discriminative enough to dis-tinguish Russian swimmer Popov from basketballplayer Popov.
So it is not a good idea to groupthese concepts into one topic.
The proposed co-occurrence rule is based on the above observationthat it is rare that such kind of general concepts,e.g., swim and basketball, often co-occur with eachother when talking about one specific person.
Af-ter the pruning step, for each ambiguous name, weget a semantic graph from all Wikipedia conceptsextracted in this name observation set.
Figure 1 il-lustrates an abridged version of a semantic graph forGeorge Foster.835Graph Clustering Considering the graph con-struction strategy we use, it is more suitable for usto group the concepts on the graph into several top-ics using a density-based clustering model.We choose SCAN algorithm Xu et al2007) toperform the clustering step.
The SCAN algorithmutilizes a neighborhood structure to measure thesimilarity between two vertices.
If a vertex has aminimal of ?
neighbors with a similarity larger than?, it is called a core.
The algorithm1 starts from arandom vertex in a graph, examining whether it is acore or not.
If yes, the algorithm will expand a clus-ter from this vertex recursively, otherwise the vertexwill be assigned either a hub node or an outlier de-pending on the number of its neighboring clusters.A hub node connects to more than one cluster, whilean outlier connects to one or no cluster.
Take thesemantic graph in Figure 1 for example, the nodeSports League is a hub node, while the node ProFootball Weekly is an outlier.
Finally, all conceptsin the graph are grouped into K + 2 parts (K is thenumber of the clusters, and is determined automat-ically), including K clusters, the set of hub nodesand the set of outliers.One problem of applying SCAN in our work isthat it is originally designed for unweighted graphs.We have to adapt it to our weighted graph by mod-ifying the similarity function between two nodes asfollows:sim(c1, c2) = ?
?simnb(c1, c2)1 + ?+sr(c1, c2)1 + ?
(1)and simnb(c1, c2) is defined as:simnb(c1, c2) =?c?N(c1)?N(c2)sr(c1,c)+sr(c2,c)2|N(c1) ?N(c2)|where N(c) is the neighbor set of concept c. Thisnew similarity function contains two parts: theneighborhood similarity and the semantic related-ness between two concepts.
We combine them us-ing a linear combination, where ?
is a weight tunedduring training.Topic Generation Now we will map the cluster-ing results into different topics.
Intuitively, each1We omit the details of SCAN for brevity, and refer inter-ested reader to Xu et al2007) for more details.cluster will be treated as a topic.
However, we foundthat hub nodes usually correspond to general con-cepts which may be related to many topics, but witha loose relatedness.
We thus distribute each generalconcept into its every related topic, but with a lowerweight to distinguish from ordinary concepts in thistopic.Outliers may be concepts which are far away frommain themes of the corpus, or noise concepts.
Wecalculate the average semantic relatedness of an out-lier with its neighbor concepts that belong to onetopic.
If the result is lower than a threshold, thisoutlier will be discarded, otherwise it will be treatedas a non-concept feature.Now we are able to map the clustering resultsinto different topics.
Intuitively, each cluster willbe treated as a topic.
However, we found that hubnodes usually correspond to general concepts, e.g.,education or public, which may be related to manytopics, but with a loose relatedness.
We thus dis-tribute each general concept into its every relatedtopic, but with a lower weight to distinguish fromordinary concepts in this topic.
Outliers are foundto contain concepts which are far away from maintopics of the document set and look like noise con-cepts.
We therefore calculate the average semanticrelatedness of an outlier node with its neighboringconcepts which belong to some topics.
If the aver-age relatedness is lower than a threshold, this nodewill be discarded, otherwise it will be treated as anon-concept feature.Weighting Topics After generating all topics, weshould weight each topic according to its importancein the current name observation set as well as thequality of the topic (cluster).
Intuitively, if most con-cepts in the topic are considered to be discriminativein the current name set and they are closely relatedto each other, this topic should be weighted as im-portant.
By properly weighting the generated topics,we can capture the importance of a concept reliablyin the corpus level (in the current name observationset) rather than in the current page solely.Before we weight a topic, we first explain howwe re-weight a hub concept in a topic since our ini-tial feature weighting scheme(Han and Zhao, 2009)works on individual web page, lacks cross documentinformation and is likely to over-estimate the impor-836tance of a hub node (general concept) by by assign-ing a higher weight.
Suppose a hub node h connectsto a topic t with n neighbors, namely c1, c2, ?
?
?
, cn.The similarity between this hub node and the topicis computed by averaging the semantic relatednessbetween this hub node and these n neighbors:sim(h, t) =1nn?i=1sr(h, ci).
(2)We then update the weight of this hub node byconsidering its similarity with this topic: wt(h) =w(h) ?
sim(h, t) from which we can see that thehub node receives a lower weight than before indi-cating that it is not as important as ordinary conceptsin a topic.Now we proceed to weight the topic t by takinginto account the frequencies of its concepts and thecoherence between the concepts and their neighbor-hood in topic t:w(t) =n?i=1f(ci)n?n?i=1n coh(ci, t)n(3)where topic t contains n concepts {c1, c2, ..., cn},f(c) is the frequency of concept c over current nameobservation set, specially, when c is a hub node con-cept, we will distribute its frequency according toequation (2), having ft(c) = f(c)sim(c, t).
Andn coh(c, t) is the neighborhood coherence of con-cept c with topic t, defined as:n coh(c, t) =?q?N(c)?tsr(q, c)|N(c) ?
t|(4)where N(c) is the neighboring node set of conceptc.By incorporating corpus level concept frequen-cies into topic weighting, discriminative conceptsthat are sparse in one document and suppressed byconventional models can benefit from their corpuslevel importance as well as their coherence in relatedtopics.3.3 Clustering Person Name ObservationsNow the remaining key step is to compute the sim-ilarity between two name observations.
The simi-larity proposed in GRAPE(Jiang et al2009) mea-sures two documents by bridge tags (common fea-tures) shared by two document graphs.
Specifically,Jiang et altilize cohesion to weight a bridge tag ina document.
The more bridge tags two documentsshare, the stronger the cohesion of each bridge tagis, and in turn the more similar the two documentsare.However, this similarity bears a shortcoming thatthe bridge tags shared by the two documents re-quire an exact match of features, which does nottake any semantic relatedness into consideration.
Iftwo web pages mentioning the same person but havefew features in common, the GRAPE similarity maynot work properly.
We, therefore, propose a newsimilarity measure combining topic similarity, topicbased connectivity strength and GRAPE?s connec-tivity strength.Matching Topics to Person Name ObservationsWe first describe how to match the generated top-ics to different name observations.
In order to avoidunreliable estimation, we only match a topic to aname observation when they share at least one con-cept.
To measure the relatedness between a topicand a name observation, we formulate this similar-ity as the weighted average of semantic relatednessbetween each concept from one side and its closelyrelated counterpart from the other side,defined as:sim(A?
B) =?a?AwA(a)?
wB(ba)?
sr(a, ba)?a?AwA(a)?
wB(ba)(5)sim(A,B) = (sim(A?
B) + sim(B ?
A))/2,where A can be a topic and B a name observation orvice versa, ba is a concept inB that is most related toconcept a, wA(a) represents the weight of concept aestimated by the averaged relatedness between a andother concepts in A.Person Name Similarity Now we describe thefirst component in our proposed measure: topic sim-ilarity, which is calculated through the common top-ics shared by the two name observations, o1 and o2:TSm(o1, o2) =?t?T (o1,o2)sim(o1, t)?
sim(o2, t) (6)?sim(o1 ?
t, o2 ?
t)?
w(t)where T (o1, o2) contains all common topics of o1and o2, w(t) is the weight of topic t estimated using837equation (3), both sim(oi, t) and sim(o1 ?
t, o2 ?
t)measure the similarity between two concept sets andcan be estimated using equation (5).
The underly-ing idea of the equation is, if two name observationsshare more and closer common topics, and also thesetopics receive higher weights according to the cur-rent name observation set, then the two observationsshould be more related to each other.Specifically, the factor sim(o1 ?
t, o2 ?
t) is de-signed to measure the fine relatedness between o1and o2 given the topic t. Sometimes, both o1 ando2 are mapped to t and both close to this topic, butin fact they depict different aspects of t since someof our topics are more general thus include severalaspects.
The comparison of their intersections willprovide a more detailed view for their similarity.Inspired by the use of bridge tags inGRAPE(Jiang et al2009), we propose to capturethe connection strength between concept sets by themeans of our topics.
We consider common topicsas the bridge tags and define our topic based con-nectivity strength between two name observationsas:TCS(o1, o2) =12?t?T (o1,o2)sim(o1 ?
t, o2 ?
t)?
(Cohs(o1, t) + Cohs(o2, t)) (7)Note that we still need sim(o1 ?
t, o2 ?
t) to capturethe fine differences inside a topic.
Cohs(o, t) is acohesion measure to capture the relatedness betweennon-concept features in o and concept features in t,defined as:Cohs(o, t) =?c?o?tw(t)?
?q?EB(o)occ(c, q)fo(c)fo(q)(8)where EB(o) contains all non-concept features ino (e.g., non-Wikipedia entities and biographical in-formation), occ(c, q) is the co-occurring number ofconcept c and feature q, fo(q) is the relative fre-quency of q in observation o.
It is easy to find thata higher cohesion can be achieved by larger overlapbetween o and t, higher topic weight and more co-occurrences of concept features in t and other fea-tures in o.The third part is the original connectivity strengthdefined in GRAPE(Jiang et al2009): CS(o1, o2),calculated using plain features without topics (weomit the details for brevity).
Finally, we linearlycombine equation (6), (7) and CS(o1, o2) into theperson name similarity function as:S(o1, o2)= ?1 ?
TSm(o1, o2) + ?2 ?
TCS(o1, o2)+(1?
?1 ?
?2)?
CS(o1, o2) (9)where ?1 and ?2 are optimized during training.This final similarity function will then be embed-ded into a normal HAC algorithm to group the webpages into different namesakes where we computethe centroid-based distance between clusters(Mannand Yarowsky, 2003).4 ExperimentsWe compare our model with competitive baselineson three WePS datasets.
In the following, we firstdescribe the experimental setup, and then discuss thetheir performances.4.1 DataWikipedia Data Wikipedia offers free copies ofall available data to interested users in their website.We used the one released in March 6th, 2009 in ourexperiments.
We identified over 4,000,000 highlyconnected concepts in this dump; each concept linksto 10 other concepts in average.WePS Datasets We used three datasets in ourexperiments, WePS1 Training and Testing (Artileset al2007), WePS2 Testing (Javier et al2009).These datasets collected names from three differ-ent resources including Wikipedia names, programcommittee of a computer science conference and UScensus.
Each name were queried in Yahoo!
Searchand top N result pages (100 pages in WePS1 and150 pages in WePS2) were obtained and manuallylabeled.4.2 BaselinesWe compare our model TM with four baseline meth-ods: (1)VSM: traditional vector space model withcosine similarity.
We use features extracted in Sec-tion 3.1 and weight them using TFIDF.
The docu-ments are grouped using standard HAC algorithm.
(2)GRAPE(Jiang et al2009): we re-implement thestate-of-the-art system which outperforms any mod-els that do not use extra knowledge resources re-ported in WePS1 and WePS2.
(3)WS: the Wikipedia838Semantic method(Han and Zhao, 2009).
This sys-tem uses Wikipedia to enhance the results of namedisambiguation.
(4)SSR: the Structural Semantic re-latedness model(Han and Zhao, 2010) creates a se-mantic graph to re-calculate the semantic related-ness between features, and captures both explicitsemantic relations and implicit structural semanticknowledge.
We also build two variants of TM: TM-nTW which removes topic weighting to examinewhat effect the topic weighting strategy can makeand whether it can provide a person specific evi-dence and TM-nCP which does not use co-occurringinformation to prune the semantic graph to examinewhether the pruning is effective.4.3 ParametersThere are several parameters to be tuned in ourmodel.
In the SCAN algorithm, we use default pa-rameters according to (Xu et al2007) with an ex-ception: the weight ?
is tuned exhaustively to be 0.2.Note that the number of topics are automatically de-cided by SCAN.
The semantic graph pruning thresh-old is set to 0.27 tuned on a held out set.
Thesmoothing parameters in equation (9) are: ?1 = 0.3,?2 = 0.2 which are tuned using cross validation.Optimization of some parameters will be addressedin detail in the following subsection.
In HAC, alloptimal merging thresholds are selected by applyingleave-one-out cross validation.4.4 Results and DiscussionWe adopt the same evaluation process as (Han andZhao, 2009), and evaluating these models using Pu-rity, Inverse Purity and the F-measure (also used inWePS Task Artiles et al2007)).
The overall perfor-mance is shown in Table 1, and the best scores arein boldface.Let us first look at our model and its variants,TM-nTW and TM-nCP.
By introducing the corpuslevel topic weighting scheme, our model improvesin average 1.6% consistently over all datasets.
Re-call that our topic weightings are obtained over thewhole name observation set beyond local context,this improvement indicates that this corpus level per-son specific evidences render the person similaritymore reasonably than that of single document.
Onthe other hand, by pruning the semantic graph, ourmodel improves averagely 1.3% over TM-nCP.
ThisTable 1: Web person name disambiguation results on allthree WePS datasetsWePS1 TrainingMethod P IP FMeasureVSM 0.86 0.86 0.85GRAPE 0.93 0.90 0.91WS 0.88 0.89 0.87SSR 0.82 0.92 0.85TM-nTW 0.91 0.89 0.90TM-nCP 0.92 0.90 0.91TM 0.93 0.91 0.91WePS1 TestingMethod P IP FMeasureVSM 0.79 0.85 0.81GRAPE 0.93 0.83 0.87WS 0.88 0.90 0.88SSR 0.85 0.83 0.84TM-nTW 0.93 0.85 0.88TM-nCP 0.92 0.86 0.88TM 0.94 0.86 0.90WePS2 TestingMethod P IP FMeasureVSM 0.82 0.87 0.83GRAPE 0.88 0.90 0.89WS 0.85 0.89 0.86SSR 0.89 0.84 0.86TM-nTW 0.92 0.87 0.89TM-nCP 0.93 0.88 0.90TM 0.93 0.89 0.91shows that our co-occurrence based pruning strategycan help render the semantic graph with less noisyedges, thus generate more reasonable topics.Generally, our proposed model works best con-sistently over all three datasets.
Our method gains9.3% improvement on average in three datasets com-pared with VSM, 1.7% improvement compared toGRAPE, 3.8% over WS and 6.7% over SSR.
We alsoperformed significance testing on F-measures: thedifferences between our model and other models aresignificant.
We notice there are many noisy or shortweb pages which lead to inaccurate concept extrac-tion, but this cross document evidences, to some ex-tent, can remedy this.
In the Emily Bender exam-ple, our system correctly groups the odd page, whichcontains limited clues, into the nutritionist cluster,839while the rest, excluding WS and SSR, failed.
Sur-prisingly, SSR combines both kinds of relations andimplicit structural knowledge, but performs in thesame bulk with VSM in WePS1 training set.
Wethink the reason may be that some name observationsets are too small to estimate non-concept related-ness via random walk.
In WePS1 training set, manynames in this dataset contains several namesakes,each of which corresponds to a few web pages.
Inthis case, our corpus level weighting scheme andWS show no advantage over GRAPE which consid-ers word co-occurrences solely.
From the results,we can also find that there is no clear winner be-tween GRAPE and WS.
The former does not useWikipedia relatedness but only includes local rela-tionship, and performs even slightly better than WSin WePS2, which indicates that non-Wikipedia con-cepts are important disambiguation features as well.4.5 Parameter OptimizationIn this subsection, we discuss the optimization ofseveral parameters in the proposed method.
In totalwe need to set four parameters.
The first one is theedge pruning threshold during graph construction;the second one is the weight ?
in SCAN algorithm;the third one and the forth one are the combinationparameters in the final similarity function.
We willaddress the first two in the following.
The last twocombination parameters are tuned by exhaustivelysearching the space and omitted here for brevityFirst, we configure the pruning threshold.
Intu-itively, larger threshold can prune more unimpor-tant edges and improve the disambiguation perfor-mance.
However, if the threshold is too large, wemay prune important edges and harm the results.The F-measure of our method with respect to thepruning threshold is plotted in Figure 2.From Figure 2, we can know that in all threedata sets, a pruning threshold of 0.27 will lead tothe best performance.
Both increasing and decreas-ing of this pruning threshold will cause a decline ofthe F-Measure, because they will either leave morenoisy light-weighted edges or prune some importantedges.Secondly, we configure the neighborhood similar-ity weight.
The larger this weight is, the more neigh-borhood information can influence the similarity be-tween two nodes in the semantic graph.
We plot the0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.380.860.870.880.890.90.910.92Edge Pruning ThresholdFMeasureWePS1 TrainingWePS1 TestingWePS2 TestingFigure 2: The F-Measure v.s.
the edge pruning thresholdon three data sets.0 0.2 0.4 0.6 0.8 10.880.8850.890.8950.90.9050.910.9150.920.9250.93Neighborhood WeightFMeasureWePS1 TrainingWePS1 TestingWePS2 TestingFigure 3: The F-Measure v.s.
the neighborhood similarityweight on three data sets.performance of our method regarding to the neigh-borhood similarity weight in Figure 3.From Figure 3, we know that for the WePS 1 Test-ing and WePS2 Testing data sets, a neighborhoodsimilarity weight of 0.2 can result in the best perfor-mance, but for WePS 1 Training set, the weight forthe best performance is 0.6.
In fact, when the neigh-borhood similarity weight varies from 0 to 1, the dif-ference between the best and worst performance areless than 0.01, which indicates that neighborhoodsimilarity is as considerable as semantic relatedness.8405 Conclusion and Future WorkIn this paper, we explore the feature space in theweb person name disambiguation task and proposea topic-based model which exploits corpus levelperson specific evidences to handle the data spar-sity challenges, especially the case that limited ev-idences can be collected from the local context.
Inparticular, we harvest topics from wikipedia con-cepts appearing in the name observation set, andweight a concept based on both the relatedness ofthe concept to its corresponding topic and the im-portance of this topic in the current name observa-tion set, so that some discriminative but sparse fea-tures can obtain more reliable weights.
Experimen-tal results show that our weighting strategy does itsjob and the proposed model outperforms the-state-of-the-art systems.
Our current work utilizes thetopic information shared in one name observationset but is incapable to handle sparse name set, whichneeds more accurate relation extraction inside thename observations.
Jointly modeling entity link-ing and person (entity) disambiguation tasks willbe an interesting direction where the two tasks areclosely related and usually need to be considered atthe same time.
Investigating the person name dis-ambiguation task in different web applications willalso be of great importance, e.g., disambiguating aname in streaming data or during knowledge baseconstruction.
In addition, graphical model, whichhas been studied in academic author disambiguation,may be a good choice to cope with the noises andnon-standard forms in web data.AcknowledgmentsWe would like to thank Yidong Chen, Wei Wangand Tinghua Wang for their useful discussions andthe anonymous reviewers for their helpful commentswhich greatly improved the work and the presen-tation.
This work was supported by the NationalHigh Technology Research and Development Pro-gram of China (Grant No.
2012AA011101), Na-tional Natural Science Foundation of China (GrantNo.61003009) and Research Fund for the Doc-toral Program of Higher Education of China (GrantNo.20100001120029).ReferencesArtiles, J., Gonzalo, J., and Sekine, S. (2007).
Thesemeval-2007 weps evaluation: establishing abenchmark for the web people search task.
InSemEval, SemEval ?07, pages 64?69, Strouds-burg, PA, USA.
Association for ComputationalLinguistics.Bagga, A. and Baldwin, B.
(1998).
Entity-basedcross-document coreferencing using the vectorspace model.
In ACL, pages 79?85, Strouds-burg, PA, USA.
Association for ComputationalLinguistics.Bunescu, R. C. and Pasca, M. (2006).
Using ency-clopedic knowledge for named entity disambigua-tion.
In EACL.
The Association for ComputerLinguistics.Cucerzan, S. (2007).
Large-scale named entity dis-ambiguation based on wikipedia data.
In EMNLP-CoNLL, pages 708?716.
ACL.David, M. and Ian, H. (2008).
An effective, low-costmeasure of semantic relatedness obtained fromwikipedia links.
In AAAI, AAAI ?08.Finkel, J. R., Grenager, T., and Manning, C. (2005).Incorporating non-local information into informa-tion extraction systems by gibbs sampling.
InACL, pages 363?370, Ann Arbor, Michigan.
As-sociation for Computational Linguistics.Han, X. and Zhao, J.
(2009).
Named entity dis-ambiguation by leveraging wikipedia semanticknowledge.
In CIKM, CIKM ?09, pages 215?224,New York, NY, USA.
ACM.Han, X. and Zhao, J.
(2010).
Structural semanticrelatedness: a knowledge-based method to namedentity disambiguation.
In ACL, ACL ?10, pages50?59, Stroudsburg, PA, USA.
Association forComputational Linguistics.Ikeda, M., Ono, S., Sato, I., Yoshida, M., and Naka-gawa, H. (2009).
Person name disambiguation onthe web by two-stage clustering.
In WWW.Iria, J., Xia, L., and Zhang, Z.
(2007).
Wit: web peo-ple search disambiguation using random walks.
InSemEval, SemEval ?07, pages 480?483, Strouds-burg, PA, USA.
Association for ComputationalLinguistics.841Javier, A., Julio, G., and Satoshi, S. (2009).
Weps 2evaluation campaign: Overview of the web peo-ple search clustering task.
In WWW 2009.Jiang, L., Wang, J., An, N., Wang, S., Zhan, J.,and Li, L. (2009).
Grape: A graph-based frame-work for disambiguating people appearances inweb search.
In ICDM, ICDM ?09, pages 199?208,Washington, DC, USA.
IEEE Computer Society.Kalashnikov, D. V., Chen, Z., Mehrotra, S., andNuray-Turan, R. (2008a).
Web people search viaconnection analysis.
IEEE Trans.
on Knowl.
andData Eng., 20:1550?1565.Kalashnikov, D. V., Nuray-Turan, R., and Mehrotra,S.
(2008b).
Towards breaking the quality curse.
: aweb-querying approach to web people search.
InSIGIR, SIGIR ?08, pages 27?34, New York, NY,USA.
ACM.Mann, G. S. and Yarowsky, D. (2003).
Unsuper-vised personal name disambiguation.
In CONLL,CONLL ?03, pages 33?40, Stroudsburg, PA,USA.
Association for Computational Linguistics.Mihalcea, R. and Csomai, A.
(2007).
Wikify!
: link-ing documents to encyclopedic knowledge.
InProceedings of CIKM?07, pages 233?242.Niu, C., Li, W., and Srihari, R. K. (2004).
Weaklysupervised learning for cross-document personname disambiguation supported by informationextraction.
In ACL, ACL ?04, Stroudsburg, PA,USA.
Association for Computational Linguistics.Pilz, A.
(2010).
Entity disambiguation using linkbased relations extracted from wikipedia.
InICML.Rao, D., Garera, N., and Yarowsky, D. (2007).
Jhu1:an unsupervised approach to person name dis-ambiguation using web snippets.
In SemEval,SemEval ?07, pages 199?202, Stroudsburg, PA,USA.
Association for Computational Linguistics.Wu, F. and Weld, D. S. (2008).
Automatically re-fining the wikipedia infobox ontology.
In WWW,WWW ?08, pages 635?644, New York, NY, USA.ACM.Xu, X., Yuruk, N., Feng, Z., and Schweiger, T. A.
J.(2007).
Scan: a structural clustering algorithmfor networks.
In Proceedings of KDD, KDD ?07,pages 824?833, New York, NY, USA.
ACM.Yiming, L., Zaiqing, N., Taoyuan, C., Ying, G., andJi-Rong, W. (2007).
Name disambiguation usingweb connection.
In AAAI.842
