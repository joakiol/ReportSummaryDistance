Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127?132,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLessons from NRC?s Portage System at WMT 2010Samuel Larkin, Boxing Chen, George Foster, Ulrich Germann, Eric Joanis,Howard Johnson, and Roland KuhnNational Research Council of Canada (NRC)Gatineau, Qu?bec, Canada.Firstname.Lastname@cnrc-nrc.gc.caAbstractNRC?s Portage system participated in the Eng-lish-French (E-F) and French-English (F-E)translation tasks of the ACL WMT 2010 eval-uation.
The most notable improvement overearlier versions of Portage is an efficient im-plementation of lattice MERT.
While Portagehas typically performed well in Chinese toEnglish MT evaluations, most recently in theNIST09 evaluation, our participation in WMT2010 revealed some interesting differences be-tween Chinese-English and E-F/F-E transla-tion, and alerted us to certain weak spots inour system.
Most of this paper discusses theproblems we found in our system and ways offixing them.
We learned several lessons thatwe think will be of general interest.1 IntroductionPortage, the statistical machine translation sys-tem of the National Research Council of Canada(NRC), is a two-pass phrase-based system.
Thetranslation tasks to which it is most often appliedare Chinese to English, English to French (hen-ceforth ?E-F?
), and French to English (hence-forth ?F-E?
): in recent years we worked on Chi-nese-English translation for the GALE projectand for NIST evaluations, and English andFrench are Canada?s two official languages.
InWMT 2010, Portage scored 28.5 BLEU (un-cased) for F-E, but only 27.0 BLEU (uncased)for E-F. For both language pairs, Portage tru-ecasing caused a loss of 1.4 BLEU; other WMTsystems typically lost around 1.0 BLEU aftertruecasing.
In Canada, about 80% of translationsbetween English and French are from English toFrench, so we would have preferred better resultsfor that direction.
This paper first describes theversion of Portage that participated in WMT2010.
It then analyzes problems with the systemand describes the solutions we found for some ofthem.2 Portage system description2.1 Core engine and training dataThe NRC system uses a standard two-passphrase-based approach.
Major features in thefirst-pass loglinear model include phrase tablesderived from symmetrized IBM2 alignments andsymmetrized HMM alignments, a distance-baseddistortion model, a lexicalized distortion model,and language models (LMs) that can be eitherstatic or else dynamic mixtures.
Each phrase ta-ble used was a merged one, created by separatelytraining an IBM2-based and an HMM-basedjoint count table on the same data and then add-ing the counts.
Each includes relative frequencyestimates and lexical estimates (based on Zensand Ney, 2004) of forward and backward condi-tional probabilities.
The lexicalized distortionprobabilities are also obtained by adding IBM2and HMM counts.
They involve 6 features (mo-notone, swap and discontinuous features for fol-lowing and preceding phrase) and are condi-tioned on phrase pairs in a model similar to thatof Moses (Koehn et al, 2005); a MAP-basedbackoff smoothing scheme is used to combatdata sparseness when estimating these probabili-ties.
Dynamic mixture LMs are linear mixturesof ngram models trained on parallel sub-corporawith weights set to minimize perplexity of thecurrent source text as described in (Foster andKuhn, 2007); henceforth, we?ll call them ?dy-namic LMs?.Decoding uses the cube-pruning algorithm of(Huang and Chiang, 2007) with a 7-word distor-tion limit.
Contrary to the usual implementationof distortion limits, we allow a new phrase to end127more than 7 words past the first non-coveredword, as long as the new phrase starts within 7words from the first non-covered word.
Notwith-standing the distortion limit, contiguous phrasescan always be swapped.
Out-of-vocabulary(OOV) source words are passed through un-changed to the target.
Loglinear weights aretuned with Och's max-BLEU algorithm over lat-tices (Macherey et al, 2008); more details aboutlattice MERT are given in the next section.
Thesecond pass rescores 1000-best lists produced bythe first pass, with additional features includingvarious LM and IBM-model probabilities; ngram,length, and reordering posterior probabilities andfrequencies; and quote and parenthesis mismatchindicators.
To improve the quality of the maximafound by MERT when using large sets of partial-ly-overlapping rescoring features, we use greedyfeature selection, first expanding from a baselineset, then pruning.We restricted our training data to data that wasdirectly available through the workshop's web-site; we didn?t use the LDC resources mentionedon the website (e.g., French Gigaword, EnglishGigaword).
Below, ?mono?
refers to all mono-lingual data (Europarl, news-commentary, andshuffle); ?mono?
English is roughly three timesbigger than ?mono?
French (50.6 M lines in?mono?
English, 17.7 M lines in ?mono?
French).?Domain?
refers to all WMT parallel trainingdata except GigaFrEn (i.e., Europarl, news-commentary, and UN).2.2 Preprocessing and postprocessingWe used our own English and French pre- andpost-processing tools, rather than those availablefrom the WMT web site.
For training, all Englishand French text is tokenized with a language-specific tokenizer and then mapped to lowercase.Truecasing uses an HMM approach, with lexicalprobabilities derived from ?mono?
and transitionprobabilities from a 3-gram LM trained on tru-ecase ?mono?.
A subsequent rule-based pass ca-pitalizes sentence-initial words.
A final detokeni-zation step undoes the tokenization.2.3 System configurations for WMT 2010In the weeks preceding the evaluation, we triedseveral ways of arranging the resources availableto us.
We picked the configurations that gave thehighest BLEU scores on WMT2009 Newstest.We found that tuning with lattice MERT ratherthan N-best MERT allowed us to employ moreparameters and obtain better results.E-F system components:1.
Phrase table trained on ?domain?;2.
Phrase table trained on GigaFrEn;3.
Lexicalized distortion model trained on?domain?;4.
Distance-based distortion model;5.
5-gram French LM trained on ?mono?;6.
4-gram LM trained on French half ofGigaFrEn;7.
Dynamic LM composed of 4 LMs, eachtrained on the French half of a parallelcorpus (5-gram LM trained on ?domain?,4-gram LM on GigaFrEn, 5-gram LM onnews-commentary and 5-gram LM onUN).The F-E system is a mirror image of the E-F sys-tem.3 Details of lattice MERT (LMERT)Our system?s implementation of LMERT (Ma-cherey et al, 2008) is the most notable recentchange in our system.
As more and more featuresare included in the loglinear model, especially ifthey are correlated, N-best MERT (Och, 2003)shows more and more instability, because ofconvergence to local optima (Foster and Kuhn,2009).
We had been looking for methods thatpromise more stability and better convergence.LMERT seemed to fit the bill.
It optimizes overthe complete lattice of candidate translations af-ter a decoding run.
This avoids some of the prob-lems of N-best lists, which lack variety, leadingto poor local optima and the need for many de-coder runs.Though the algorithm is straightforward and ishighly parallelizable, attention must be paid tospace and time resource issues during implemen-tation.
Lattices output by our decoder were largeand needed to be shrunk dramatically for the al-gorithm to function well.
Fortunately, this couldbe achieved via the finite state equivalence algo-rithm for minimizing deterministic finite statemachines.
The second helpful idea was to sepa-rate out the features that were a function of thephrase associated with an arc (e.g., translationlength and translation model probability fea-tures).
These features could then be stored in asmaller phrase-feature table.
Features associatedwith language or distortion models could be han-dled in a larger transition-feature table.The above ideas, plus careful coding of datastructures, brought the memory footprint downsufficiently to allow us to use complete latticesfrom the decoder and optimize over the complete128development set for NIST09 Chinese-English.However, combining lattices between decoderruns again resulted in excessive memory re-quirements.
We achieved acceptable perfor-mance by searching only the lattice from the lat-est decoder run; perhaps information from earlierruns, though critical for convergence in N-bestMERT, isn?t as important for LMERT.Until a reviewer suggested it, we had notthought of pruning lattices to a specified graphdensity as a solution for our memory problems.This is referred to in a single sentence in (Ma-cherey et al, 2008), which does not specify itsimplementation or its impact on performance,and is an option of OpenFst (we didn?t useOpenFst).
We will certainly experiment with lat-tice pruning in future.Powell's algorithm (PA), which is at the coreof MERT, has good convergence when featuresare mostly independent and do not depart muchfrom a simple coordinate search; it can run intoproblems when there are many correlated fea-tures (as with multiple translation and languagemodels).
Figure 1 shows the kind of case wherePA works well.
The contours of the function be-ing optimized are relatively smooth, facilitatinglearning of new search directions from gradients.Figure 2 shows a more difficult case: there isa single optimum, but noise dominates and PAhas difficulty finding new directions.
Search of-ten iterates over the original co-ordinates, miss-ing optima that are nearby but in directions notdiscoverable from local gradients.
Probes in ran-dom directions can do better than iteration overthe same directions (this is similar to the methodproposed for N-best MERT by Cer et al, 2008).Each 1-dimensional MERT optimization is exact,so if our probe stabs a region with better scores,it will be discovered.
Figures 1 and 2 only hintat the problem: in reality, 2-dimensional searchisn?t a problem.
The difficulties occur as the di-mension grows: in high dimensions, it is moreimportant to get good directions and they areharder to find.For WMT 2010, we crafted a compromisewith the best properties of PA, yet alowing for amore aggressive search in more directions.
Westart with PA. As long as PA is adding new di-rection vectors, it is continued.
When PA stopsadding new directions, random rotation (ortho-gonal transformation) of the coordinates is per-formed and PA is restarted in the new space.
PAalmost always fails to introduce new directionswithin the new coordinates, then fails again, soanother set of random coordinates is chosen.
Thisprocess repeats until convergence.
In futurework, we will look at incorporating random res-tarts into the algorithm as additional insuranceagainst premature convergence.Our LMERT implementation has room forimprovement: it may still run into over-fittingproblems with many correlated features.
Howev-er, during preparation for the evaluation, we no-ticed that LMERT converged better than N-bestMERT, allowing models with more features andhigher BLEU to be chosen.After the WMT submission, we discoveredthat our LMERT implementation had a bug; oursubmission was tuned with this buggy LMERT.Comparison between our E-F submission tunedwith N-best MERT and the same system tunedwith bug-fixed LMERT shows BLEU gains of+1.5-3.5 for LMERT (on dev, WMT2009, andWMT2010, with no rescoring).
However, N-bestMERT performed very poorly in this particularcase; we usually obtain a gain due to LMERT of+0.2-1.0 (e.g., for the submitted F-E system).Figure 1: Convergence for PA (Smooth FeatureSpace)Figure 2: Convergence for PA with Random Rotation(Rough Feature Space)1294 Problems and Solutions4.1 Fixing LMERTJust after the evaluation, we noticed a discrepan-cy for E-F between BLEU scores computed dur-ing LMERT optimization and scores from the 1-best list immediately after decoding.
OurLMERT code had a bug that garbled any ac-cented word in the version of the French refer-ence in memory; previous LMERT experimentshad English as target language, so the bug hadn?tshowed up.
The bug didn?t affect characters inthe 7-bit ASCII set, such as English ones, onlyaccented characters.
Words in candidate transla-tions were not garbled, so correct translationswith accents received a lower BLEU score thanthey should have.
As Table 1 shows, this bugcost us about 0.5 BLEU for WMT 2010 E-F afterrescoring (according to NRC?s internal versionof BLEU, which differs slightly from WMT?sBLEU).
Despite this bug, the system tuned withbuggy LMERT (and submitted) was still betterthan the best system we obtained with N-bestMERT.
The bug didn?t affect F-E scores.Dev WMT2009 WMT2010LMERT (bug) 25.26 26.85 27.55LMERT(no bug)25.43 26.89 28.07Table 1: LMERT bug fix (E-F BLEU after rescoring)4.2 Fixing odd translationsAfter the evaluation, we carefully studied thesystem outputs on the WMT 2010 test data, par-ticularly for E-F. Apart from truecasing errors,we noticed two kinds of bad behaviour: transla-tions of proper names and apparent passthroughof English words to the French side.Examples of E-F translations of proper namesfrom our WMT 2010 submission (each from adifferent sentence):Mr. Onderka ?
M. Roman, Luk??
Marvan ?
G.Luk?
?, Janey ?
The, Janette Tozer ?
Janette,Aysel Tugluk ?
joints tugluk, Tawa Hallae ?Ottawa, Oleson ?
production,  Alcobendas ?
;When the LMERT bug was fixed, some butnot all of these bad translations were corrected(e.g., 3 of the 8 examples above were corrected).Our system passes OOV words through un-changed.
Thus, the names above aren?t OOVs,but words that occur rarely in the training data,and for which bad alignments have a dispropor-tionate effect.
We realized that when a sourceword begins with a capital, that may be a signalthat it should be passed through.
We thus de-signed a passthrough feature function that appliesto all capitalized forms not at the start of a sen-tence (and also to forms at the sentence start ifthey?re capitalized elsewhere).
Sequences of oneor more capitalized forms are grouped into aphrase suggestion (e.g., Barack Obama ?
bar-rack obama) which competes with phrase tableentries and is assigned a weight by MERT.The passthrough feature function yields a tinyimprovement over the E-F system with the bug-fixed LMERT on the dev corpus (WMT2008):+0.06 BLEU (without rescoring).
It yields a larg-er improvement on our test corpus: +0.27 BLEU(without rescoring).
Furthermore, it corrects allthe examples from the WMT 2010 test shownabove (after the LMERT bug fix 5 of the 8 ex-amples above still had problems, but when thepassthrough function is incorporated all of themgo away).
Though the BLEU gain is small, weare happy to have almost eradicated this type oferror, which human beings find very annoying.The opposite type of error is apparent pass-through.
For instance, ?we?re?
appeared 12 timesin the WMT 2010 test data, and was translated 6times into French as ?we?re?
- even though bettertranslations had higher forward probabilities.
Thesource of the problem is the backward probabili-ty P(E=?we?re?|F=?we?re?
), which is 1.0; thebackward probabilities for valid French transla-tions of ?we?re?
are lower.
Because of the highprobability P(E=?we?re?|F=?we?re?)
within theloglinear combination, the decoder often chooses?we?re?
as the French translation of ?we?re?.The (E=?we?re?, F=?we?re?)
pair in WMT2010 phrase tables arose from two sentence pairswhere the ?French?
translation of an English sen-tence is a copy of that English sentence.
In both,the original English sentence contains ?we?re?.Naturally, the English words on the ?French?side are word-aligned with their identical twinson the English side.
Generally, if the trainingdata has sentence pairs where the ?French?
sen-tence contains words from the English sentence,those words will get high backward probabilitiesof being translated as themselves.
This problemmay not show up as an apparent passthrough;instead, it may cause MERT to lower the weightof the backward probability component, thushurting performance.We estimated English contamination of theFrench side of the parallel training data by ma-130nually inspecting a random sample of ?French?sentences containing common English functionwords.
Manual inspection is needed for accurateestimation: a legitimate French sentence mightcontain mostly English words if, e.g., it is shortand cites the title of an English work (thiswouldn?t count as contamination).
The degree ofcontamination is roughly 0.05% for Europarl,0.5% for news-commentary, 0.5% for UN, and1% for GigaFrEn (in these corpora the French isalso contaminated by other languages, particular-ly German).
Foreign contamination of Englishfor these corpora appears to be much less fre-quent.Contamination can take strange forms.
We ex-pected to see English sentences copied over in-tact to the French side, and we did, but we didnot expect to see so many ?French?
sentencesthat interleaved short English word sequenceswith short French word sequences, apparentlybecause text with an English and a French col-umn had been copied by taking lines from alter-nate columns.
We found many of these inter-leaved ?French?
sentences, and found some ofthem in exactly this form on the Web (i.e., thecorruption didn?t occur during WMT data collec-tion).
The details may not matter: whenever the?French?
training sentence contains words fromits English twin, there can be serious damage viabackward probabilities.To test this hypothesis, we filtered all paralleland monolingual training data for the E-F systemwith a language guessing tool called text_cat(Cavnar and Trenkle, 1994).
From parallel data,we filtered out sentence pairs whose French sidehad a high probability of not being French; fromLM training data, sentences with a high non-French probability.
We set the filtering level byinspecting the guesser?s assessment of news-commentary sentences, choosing a rather aggres-sive level that eliminated 0.7% of news-commentary sentence pairs.
We used the samelevel to filter Europarl (0.8% of sentence pairsremoved), UN (3.4%), GigaFrEn (4.7%), and?mono?
(4.3% of sentences).Dev WMT2009 WMT2010Baseline 25.23 26.47 27.72Filtered 25.45 26.66 27.98Table 2: Data filtering (E-F BLEU, no rescoring)Table 2 shows the results: a small but consis-tent gain (about +0.2 BLEU without rescoring).We have not yet confirmed the hypothesis thatcopies of source-language words in the pairedtarget sentence within training data can damagesystem performance via backward probabilities.4.3 Fixing problems with LM trainingPost-evaluation, we realized that our arrange-ment of the training data for the LMs for bothlanguage directions was flawed.
The groupingtogether of disparate corpora in ?mono?
and?domain?
didn?t allow higher-quality, truly in-domain corpora to be weighted more heavily(e.g., the news corpora should have higherweights than Europarl, but they are lumped to-gether in ?mono?).
There are also potentiallyharmful overlaps between LMs (e.g., GigaFrEnis used both inside and outside the dynamic LM).We trained a new set of French LMs for the E-F system, which replaced all the French LMs(#5-7) described in section 2.3 in the E-F system:1.
5-gram LM trained on news-commentaryand shuffle;2.
Dynamic LM based on 4 5-gram LMstrained on French side of parallel data(LM trained on GigaFrEn, LM on UN,LM on Europarl, and LM on news-commentary).We did not apply the passthrough function orlanguage filtering (section 4.2) to any of thetraining data for any component (LMs, TMs, dis-tortion models) of this system; we did use thebug-fixed version of LMERT (section 4.1).The experiments with these new French LMsfor the E-F system yielded a small decrease ofNRC BLEU on dev (-0.15) and small increaseson WMT Newstest 2009 and Newstest 2010(+0.2 and +0.4 respectively without rescoring).We didn?t do F-E experiments of this type.4.4 Pooling improvementsThe improvements above were (individual un-cased E-F BLEU gains without rescoring inbrackets): LMERT bug fix (about +0.5); pass-through feature function (+0.1-0.3); languagefiltering for French (+0.2).
There was also asmall gain on test data by rearranging E-F LMtraining data, though the loss on ?dev?
suggeststhis may be a statistical fluctuation.
We builtthese four improvements into the evaluation E-Fsystem, along with quote normalization: in alltraining and test data, diverse single quotes weremapped onto the ascii single quote, and diversedouble quotes were mapped onto the ascii doublequote.
The average result on WMT2009 andWMT2010 was +1.7 BLEU points compared tothe original system, so there may be synergy be-131tween the improvements.
The original systemhad gained +0.3 from rescoring, while the finalimproved system only gained +0.1 from rescor-ing: a post-evaluation rescored gain of +1.5.An experiment in which we dropped lexica-lized distortion from the improved systemshowed that this component yields about +0.2BLEU.
Much earlier, when we were still trainingsystems with N-best MERT, incorporation of the6-feature lexicalized distortion often causedscores to go down (by as much as 2.8 BLEU).This illustrates how LMERT can make incorpo-ration of many more features worthwhile.4.5 Fixing truecasingOur truecaser doesn?t work as well as truecasersof other WMT groups: we lost 1.4 BLEU by tru-ecasing in both language directions, while otherslost 1.0 or less.
To improve our truecaser, wetried: 1.
Training it on all relevant data and 2.Collecting 3-gram case-pattern statistics insteadof unigrams.
Neither of these helped significant-ly.
One way of improving the truecaser would beto let case information from source words influ-ence the case of the corresponding target words.Alternatively, one of the reviewers stated thatseveral labs involved in WMT have no separatetruecaser and simply train on truecase text.
Wehad previously tried this approach for NIST Chi-nese-English and discarded it because of its poorperformance.
We are currently re-trying it onWMT data; if it works better than having a sepa-rate truecaser, this was yet another area wherelessons from Chinese-English were misleading.5 LessonsLMERT is an improvement over N-best MERT.The submitted system was one for which N-bestMERT happened to work very badly, so we gotridiculously large gains of +1.5-3.5 BLEU fornon-buggy LMERT over N-best MERT.
Theseresults are outliers: in experiments with similarconfigurations, we typically get +0.2-1.0 forLMERT over N-best MERT.
Post-evaluation,four minor improvements ?
a case-based pass-through function, language filtering, LM rear-rangement, and quote normalization ?
collective-ly gave a nice improvement.
Nothing we triedhelped truecaser performance significantly,though we have some ideas on how to proceed.We learned some lessons from WMT 2010.Always test your system on the relevant lan-guage pair.
Our original version of LMERT wasdeveloped on Chinese-English and worked wellthere, but had a bug that surfaced only when thetarget language had accents.European language pairs are more porous toinformation than Chinese-English.
Our WMTsystem reflected design decisions for Chinese-English, and thus didn?t exploit case informationin the source: it passed through OOVs to the tar-get, but didn?t pass through upper-case wordsthat are likely to be proper nouns.It is beneficial to remove foreign-languagecontamination from the training data.When entering an evaluation one hasn?t parti-cipated in for several years, always read systempapers from the previous year.
Some of theWMT 2008 system papers mention passthroughof some non-OOVs, filtering out of noisy train-ing data, and using the case of a source word topredict the case of the corresponding target word.ReferencesWilliam Cavnar and John Trenkle.
1994.
N-Gram-Based Text Categorization.
Proc.
Symposium onDocument Analysis and Information Retrieval,UNLV Publications/Reprographics, pp.
161-175.Daniel Cer, Daniel Jurafsky, and Christopher D.Manning.
2008.
Regularization and search for min-imum error rate training.
Proc.
Workshop onSMT, pp.
26-34.George Foster and Roland Kuhn.
2009.
StabilizingMinimum Error Rate Training.
Proc.
Workshopon SMT, pp.
242-249.George Foster and Roland Kuhn.
2007.
Mixture-Model Adaptation for SMT.
Proc.
Workshop onSMT, pp.
128-135.Liang Huang and David Chiang.
2007.
Forest Rescor-ing: Faster Decoding with Integrated LanguageModels.
Proc.
ACL, pp.
144-151.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Transcription Evalua-tion.
MT Eval.
Workshop.Wolfgang Macherey, Franz Josef Och, Ignacio Thay-er, and Jakob Uszkoreit.
2008.
Lattice-based Min-imum Error Rate Training for Statistical Machine-Translation.
Conf.
EMNLP, pp.
725-734.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
Proc.
ACL,pp.
160-167.Richard Zens and Hermann Ney.
2004.
Improvementsin Phrase-Based Statistical Machine Translation.Proc.
HLT/NAACL, pp.
257-264.132
