Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106?111,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 10:Linking Events and Their Participants in DiscourseJosef Ruppenhofer and Caroline SporlederComputational LinguisticsSaarland University{josefr,csporled}@coli.uni-sb.deRoser MoranteCNTSUniversity of AntwerpRoser.Morante@ua.ac.beCollin BakerICSIBerkeley, CA 94704collin@icsi.berkeley.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at Bouldermartha.palmer@colorado.eduAbstractIn this paper, we describe the SemEval-2010shared task on ?Linking Events and Their Par-ticipants in Discourse?.
This task is a variantof the classical semantic role labelling task.The novel aspect is that we focus on linkinglocal semantic argument structures across sen-tence boundaries.
Specifically, the task aims atlinking locally uninstantiated roles to their co-referents in the wider discourse context (if suchco-referents exist).
This task is potentially ben-eficial for a number of NLP applications andwe hope that it will not only attract researchersfrom the semantic role labelling communitybut also from co-reference resolution and infor-mation extraction.1 IntroductionSemantic role labelling (SRL) has been defined asa sentence-level natural-language processing task inwhich semantic roles are assigned to the syntacticarguments of a predicate (Gildea and Jurafsky, 2002).Semantic roles describe the function of the partici-pants in an event.
Identifying the semantic roles ofthe predicates in a text allows knowing who did whatto whom when where how, etc.SRL has attracted much attention in recent years,as witnessed by several shared tasks in Sense-val/SemEval (Ma`rquez et al, 2007; Litkowski, 2004;Baker et al, 2007; Diab et al, 2007), and CoNLL(Carreras and Ma`rquez, 2004; Carreras and Ma`rquez,2005; Surdeanu et al, 2008).
The state-of-the-artin semantic role labelling has now advanced somuch that a number of studies have shown that au-tomatically inferred semantic argument structurescan lead to tangible performance gains in NLP ap-plications such as information extraction (Surdeanuet al, 2003), question answering (Shen and Lapata,2007) or recognising textual entailment (Burchardtand Frank, 2006).However, semantic role labelling as it is currentlydefined also misses a lot of information that wouldbe beneficial for NLP applications that deal withtext understanding (in the broadest sense), such asinformation extraction, summarisation, or questionanswering.
The reason for this is that SRL has tra-ditionally been viewed as a sentence-internal task.Hence, relations between different local semantic ar-gument structures are disregarded and this leads to aloss of important semantic information.This view of SRL as a sentence-internal task ispartly due to the fact that large-scale manual anno-tation projects such as FrameNet1 and PropBank2typically present their annotations lexicographicallyby lemma rather than by source text.
Furthermore,in the case of FrameNet, the annotation effort didnot start out with the goal of exhaustive corpus an-notation but instead focused on isolated instances ofthe target words sampled from a very large corpus,which did not allow for a view of the data as ?full-textannotation?.It is clear that there is an interplay between localargument structure and the surrounding discourse(Fillmore, 1977).
In early work, Palmer et al (1986)discussed filling null complements from context byusing knowledge about individual predicates and ten-1http://framenet.icsi.berkeley.edu/2http://verbs.colorado.edu/?mpalmer/projects/ace.html106dencies of referential chaining across sentences.
Butso far there have been few attempts to find linksbetween argument structures across clause and sen-tence boundaries explicitly on the basis of semanticrelations between the predicates involved.
Two no-table exceptions are Fillmore and Baker (2001) andBurchardt et al (2005).
Fillmore and Baker (2001)analyse a short newspaper article and discuss howframe semantics could benefit discourse processingbut without making concrete suggestions of how tomodel this.
Burchardt et al (2005) provide a detailedanalysis of the links between the local semantic argu-ment structures in a short text; however their systemis not fully implemented either.In the shared task, we intend to make a first steptowards taking SRL beyond the domain of individualsentences by linking local semantic argument struc-tures to the wider discourse context.
In particular, weaddress the problem of finding fillers for roles whichare neither instantiated as direct dependents of ourtarget predicates nor displaced through long-distancedependency or coinstantatiation constructions.
Of-ten a referent for an uninstantiated role can be foundin the wider context, i.e.
in preceding or followingsentences.
An example is given in (1), where theCHARGES role (ARG2 in PropBank) of cleared is leftempty but can be linked to murder in the previoussentence.
(1) In a lengthy court case the defendant wastried for murder.
In the end, he was cleared.Another very rich example is provided by (2),where, for instance, the experiencer and the object ofjealousy are not overtly expressed as syntactic depen-dents of the noun jealousy but can be inferred to beWatson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anythingof art but that is mere jealousy because ourviews upon the subject differ.NIs are also very frequent in clinical reports.For example, in (3) the EXPERIENCER role of?cough?, ?tachypnea?, and ?breathing?
can be linkedto ?twenty-two month old?.
Text mining systems inthe biomedical domain focus on extracting relationsbetween biomedical entities and information aboutpatients.
It is important that these systems extractinformation as accurately as possible.
Thus, findingco-referents for NIs is also very relevant for improv-ing results on mining relations in biomedical texts.
(3) Twenty-two month old with history of recur-rent right middle lobe infiltrate.
Increasedcough, tachypnea, and work of breathing.In the following sections we describe the task inmore detail.
We start by providing some backgroundon null instantiations (Section 2).
Section 3 gives anoverview of the task, followed by a description ofhow we intend to create the data (Section 4).
Sec-tion 5 provides a short description of how null in-stantiations could be resolved automatically giventhe provided data.
Finally, Section 6 discusses theevaluation measures and we wrap up in Section 7.2 Background on Null InstantiationThe theory of null complementation used here is theone adopted by FrameNet, which derives from thework of Fillmore (1986).3 Briefly, omissions of corearguments of predicates are categorised along twodimensions, the licensor and the interpretation theyreceive.
The idea of a licensor refers to the fact thateither a particular lexical item or a particular gram-matical construction must be present for the omissionof a frame element (FE) to occur.
For instance, theomission of the agent in (4) is licensed by the passiveconstruction.
(4) No doubt, mistakes were made 0Protagonist.The omission is a constructional omission becauseit can apply to any predicate with an appropriatesemantics that allows it to combine with the passiveconstruction.
On the other hand, the omission in (5)is lexically specific: the verb arrive allows the Goalto be unspecified but the verb reach, also a memberof the Arriving frame, does not.
(5) We arrived 0Goal at 8pm.The above two examples also illustrate the secondmajor dimension of variation.
Whereas, in (4) theprotagonist making the mistake is only existentiallybound within the discourse (instance of indefinite null3Palmer et al?s (1986) treatment of uninstantiated ?essentialroles?
is very similar (see also Palmer (1990)).107instantiation, INI), the Goal location in (5) is an entitythat must be accessible to speaker and hearer fromthe discourse or its context (definite null instantiation,DNI).
Finally note that the licensing construction orlexical item fully and reliably determines the interpre-tation.
Missing by-phrases always have an indefiniteinterpretation and whenever arrive omits the Goallexically, the Goal has to be interpreted as definite,as it is in (5).The import of this classification to the task hereis that we will concentrate on cases of DNI whetherthey are licensed lexically or constructionally.3 Task DescriptionWe plan to run the task in the following two modes:Full Task For the full task we supply a test set inwhich the target words are marked and labelled withthe correct sense (i.e.
frame).4 The participants thenhave to:1. find the overt semantic arguments of the target(role recognition)2. label them with the correct role (role labelling)3. recognize definite null instantiations and findlinks to antecedents in the wider context (NIlinking)NIs only In the second mode, participants will besupplied with a test set which is annotated with goldstandard local semantic argument structure.5 Thetask is then restricted to recognizing that a core roleis missing, ascertaining that it must have a definiteinterpretation and finding a filler for it (i.e., sub-task3 from the full task).The full task and the null instantiation linking taskwill be evaluated separately.
By setting up a SRLtask, we expect to attract participants from the es-tablished SRL community.
Furthermore, by allow-ing participants to only address the second task, we4We supply the correct sense to ensure that all systems usethe same role inventory for each target (i.e., the role inventoryassociated with the gold standard sense).
This makes it easierto evaluate the systems consistently with respect to role assign-ments and null instantiation linking, which is our main focus.5The training set is identical for both set-ups and will containthe full annotation, i.e., frames, semantic roles and their fillers,and referents of null instantiations in the wider context (seeSection 4 for details).hope to also attract researchers from areas such as co-reference resolution or information extraction who donot want to implement a complete SRL system.
Wealso plan to provide the data with both FrameNet andPropBank style annotations to encourage researchersfrom both areas to take part.4 DataThe data will come from one of Arthur ConanDoyle?s fiction works.
We chose fiction rather thannews because we believe that fiction texts witha linear narrative generally contain more context-resolvable null instantiations.
They also tend to belonger and have a simpler structure than news textswhich typically revisit the same facts repeatedly atdifferent levels of detail (in the so-called ?invertedpyramid?
structure) and which mix event reports withcommentary and evaluation, thus sequencing mate-rial that is understood as running in parallel.
Fictiontexts should lend themselves more readily to a first at-tempt at integrating discourse structure into semanticrole labeling.
We chose Conan Doyle?s work becausemost of his books are not subject to copyright restric-tions anymore, which allows us to freely release theannotated data.We plan to make the data sets available with bothFrameNet and PropBank semantic argument anno-tation, so that participants can choose which frame-work they want to work in.
The annotations willoriginally be made using FrameNet-style and willlater be mapped semi-automatically to PropBank an-notations.
The data set for the FrameNet version ofthe task will be built at Saarland University, in closeco-operation with the FrameNet team in Berkeley.We aim for the same density of annotation as is ex-hibited by FrameNet?s existing full-text annotation6and are currently investigating whether the semanticargument annotation can be done semi-automatically,e.g., by starting the annotation with a run of the Shal-maneser role labeller (Erk and Pado?, 2006), whoseoutput is then corrected and expanded manually.
Toensure a high annotation quality, at least part of thedata will be annotated by two annotators and thenmanually adjudicated.
We also provide detailed an-notation guidelines (largely following the FrameNet6http://framenet.icsi.berkeley.edu/index.php?option=com_wrapper&Itemid=84108guidelines) and any open questions are discussed ina weekly annotation meeting.For the annotation of null instantiations and theirlinks to the surrounding discourse we have to createnew guidelines as this is a novel annotation task.
Wewill adopt ideas from the annotation of co-referenceinformation, linking locally unrealised roles to allmentions of the referents in the surrounding dis-course, where available.
We will mark only identityrelations but not part-whole or bridging relations be-tween referents.
The set of unrealised roles underconsideration includes only the core arguments butnot adjuncts (peripheral or extra-thematic roles inFrameNet?s terminology).
Possible antecedents arenot restricted to noun phrases but include all con-stituents that can be (local) role fillers for some pred-icate plus complete sentences (which can sometimesfill roles such as MESSAGE).The data-set for PropBank will be created by map-ping the FrameNet annotations onto PropBank andNomBank labels.
For verbal targets, we use the Sem-link7 mappings.
For nominal targets, there is noexisting hand-checked mapping between FrameNetand NomBank but we will explore a way of build-ing a FrameNet - NomBank mapping at least foreventive nouns indirectly with the help of Semlink.This would take advantage of the fact that PropBankverbs and eventive NomBank nouns both have a map-ping to VerbNet classes, which are referenced also bySemlink.
Time permitting, non-eventive nouns couldbe mapped manually.
For FrameNet targets of otherparts of speech, in particular adjectives and prepo-sitions, no equivalent PropBank-style counterpartswill be available.
The result of the automatic map-pings will be partly hand-checked.
The annotationsresolving null instantiations need no adjustment.We intend to annotate at least two data sets ofaround 4,000 words.
One set for testing and one fortraining.
Because we realise that the training set willnot be large enough to train a semantic role labellingsystem on it, we permit the participants to boost thetraining data for the SRL task by making use of theexisting FrameNet and PropBank corpora.87http://verbs.colorado.edu/semlink/8This may require some genre adaption but we believe this isfeasible.5 Resolving Null InstantiationsWe conceive of null instantiation resolution as a threestep problem.
First, one needs to determine whether acore role is missing.
This involves looking up whichcore roles are overtly expressed and which are not.In the second step, one needs to determine whatlicenses an omission and what its interpretation is.To do this, one can use rules and heuristics based onvarious syntactic and lexical facts of English.
As anexample of a relevant syntactic fact, consider that sub-jects in English can only be omitted when licensed bya construction.
One such construction is the impera-tive (e.g.
Please, sit down).
Since this constructionalso specifies that the missing referent must be theaddressee of the speaker of the imperative, it is clearwhat referent one has to try to find.As for using lexical knowledge, consider omis-sions of the Goods FE of the verb steal in the Theftframe.
FrameNet annotation shows that wheneverthe Goods FE of steal is missing it is interpreted in-definitely, suggesting that a new instance of the FEbeing missing should have the same interpretation.More evidence to the same effect can be derived us-ing Ruppenhofer?s (2004) observation that the inter-pretation of a lexically licensed omission is definiteif the overt instances of the FE have mostly definiteform (i.e.
have definite determiners such as that, the ,this), and indefinite if they are mostly indefinite (i.e.have bare or indefinite determiners such as a(n) orsome).
The morphology of overt instances of an FEcould be inspected in the FrameNet data, or if thepredicate has only one sense or a very dominant one,then the frequencies could even be estimated fromunannotated corpora.The third step is linking definite omissions to ref-erents in the context.
This linking problem could bemodelled as a co-reference resolution task.
Whilethe work of Palmer et al (1986) relied on speciallexicons, one might instead want to learn informationabout the semantic content of different role fillersand then assess for each of the potential referents inthe discourse context whether their semantic contentis close enough to the expected content of the nullinstantiated role.Information about the likely fillers of a role canbe obtained from annotated data sets (e.g., FrameNetor PropBank).
For instance, typical fillers of the109CHARGES role of clear might be murder, accusa-tions, allegations, fraud etc.
The semantic content ofthe role could then be represented in a vector spacemodel, using additional unannotated data to buildmeaning vectors for the attested role fillers.
Meaningvectors for potential role fillers in the context of thenull instantiation could be built in a similar fashion.The likelihood of a potential filler filling the targetrole can then be modelled as the distance between themeaning vector of the filler and the role in the vec-tor space model (see Pado?
et al (2008) for a similarapproach for semi-automatic SRL).We envisage that the manually annotated null in-stantiated data can be used to learn additionallyheuristics for the filler resolution task, such as in-formation about the average distance between a nullinstantiation and its most recent co-referent.6 EvaluationAs mentioned above we allow participants to addresseither the full role recognition and labelling task plusthe linking of null instantiations or to make use ofthe gold standard semantic argument structure andlook only at the null instantiations.
We also permitsystems to perform either FrameNet or PropBankstyle SRL.
Hence, systems can be entered for foursubtasks which will be evaluated separately:?
full task, FrameNet?
null instantiations, FrameNet?
full task, PropBank?
null instantiations, PropBankThe focus for the proposed task is on the null in-stantiation linking, however, for completeness, wealso evaluate the standard SRL task.
For role recogni-tion and labelling we use a standard evaluation set-up,i.e., for role recognition we will evaluate the accuracywith respect to the manually created gold standard,for role labelling we will evaluate precision, recall,and F-Score.The null instantiation linkings are evaluatedslightly differently.
In the gold standard, we will iden-tify referents for null instantiations in the discoursecontext.
In some cases, more than one referent mightbe appropriate, e.g., because the omitted argumentrefers to an entity that is mentioned multiple timesin the context.
In this case, a system should be givencredit if the null instantiation is linked to any of theseexpressions.
To achieve this we create equivalencesets for the referents of null instantiations.
If the nullinstantiation is linked to any item in the equivalenceset, the link is counted as a true positive.
We can thendefine NI linking precision as the number of all truepositive links divided by the number of links made bya system, and NI linking recall as the number of truepositive links divided by the number of links betweena null instantiation and its equivalence set in the goldstandard.
NI linking F-Score is then the harmonicmean between NI linking precision and recall.Since it may sometimes be difficult to determinethe correct extend of the filler of an NI, we scorean automatic annotation as correct if it includes thehead of the gold standard filler in the predicted filler.However, in order to not favour systems which linkNIs to excessively large spans of text to maximise thelikelihood of linking to a correct referent, we intro-duce a second evaluation measure, which computesthe overlap (Dice coefficient) between the words inthe predicted filler (P) of a null instantiation and thewords in the gold standard one (G):NI linking overlap = 2|P ?G||P |+ |G| (6)Example (7) illustrates this point.
The verb won inthe second sentence evokes the Finish competitionframe whose COMPETITION role is null instantiated.From the context it is clear that the competition roleis semantically filled by their first TV debate (head:debate) and last night?s debate (head: debate) inthe previous sentences.
These two expressions makeup the equivalence set for the COMPETITION role inthe last sentence.
Any system that would predict alinkage to a filler that covers the head of either ofthese two expressions would score a true positive forthis NI.
However, a system that linked to last night?sdebate would have an NI linking overlap of 1 (i.e.,2*3/(3+3)) while a system linking the whole secondsentence Last night?s debate was eagerly anticipatedto the NI would have an NI linking overlap of 0.67(i.e., 2*3/(6+3))(7) US presidential rivals Republican JohnMcCain and Democrat Barack Obama haveyesterday evening attacked each other over110foreign policy and the economy, in [theirfirst TV debate]Competition.
[Last night?sdebate]Competition was eagerly anticipated.Two national flash polls suggest that[Obama]Competitor wonFinish competition0Competition.7 ConclusionIn this paper, we described the SemEval-2010 sharedtask on ?Linking Events and Their Participants inDiscourse?.
With this task, we intend to take a firststep towards viewing semantic role labelling not as asentence internal problem but as a task which shouldreally take the discourse context into account.
Specif-ically, we focus on finding referents for roles whichare null instantiated in the local context.
This is po-tentially useful for various NLP applications.
Webelieve that the task is timely and interesting for anumber of researchers not only from the semanticrole labelling community but also from fields such asco-reference resolution or information extraction.While our task focuses specifically on finding linksbetween null instantiated roles and the discourse con-text, we hope that in setting it up, we can stimulate re-search on the interaction between discourse structureand semantic argument structure in general.
Possiblefuture editions of the task could then focus on addi-tional connections between local semantic argumentstructures (e.g., linking argument structures that referto the same event).8 AcknowledgementsJosef Ruppenhofer and Caroline Sporleder are supportedby the German Research Foundation DFG (under grantPI 154/9-3 and the Cluster of Excellence MultimodalComputing and Interaction (MMCI), respectively).
RoserMorante?s research is funded by the GOA project BIO-GRAPH of the University of Antwerp.ReferencesC.
Baker, M. Ellsworth, and K. Erk.
2007.
SemEval-2007 Task 19: Frame semantic structure extraction.
InProceedings of SemEval-07.A.
Burchardt and A. Frank.
2006.
Approximating textualentailment with LFG and framenet frames.
In Pro-ceedings of the Second Recognising Textual EntailmentWorkshop.A.
Burchardt, A. Frank, and M. Pinkal.
2005.
Buildingtext meaning representations from contextually relatedframes ?
A case study.
In Proceedings of IWCS-6.X.
Carreras and Ll.
Ma`rquez.
2004.
Introduction to theCoNLL-2004 shared task: Semantic role labeling.
InProceedings of CoNLL-04, pages 89?97.X.
Carreras and Ll.
Ma`rquez.
2005.
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
InProceedings of CoNLL-05, pages 152?164.M.
Diab, M. Alkhalifa, S. ElKateb, C. Fellbaum, A. Man-souri, and M. Palmer.
2007.
SemEval-2007 Task 18:Arabic semantic labeling.
In Proc.
of SemEval-07.K.
Erk and S. Pado?.
2006.
Shalmaneser - a flexibletoolbox for semantic role assignment.
In Proceedingsof LREC-06.C.J.
Fillmore and C.F.
Baker.
2001.
Frame semantics fortext understanding.
In Proc.
of the NAACL-01 Work-shop on WordNet and Other Lexical Resources.C.J.
Fillmore.
1977.
Scenes-and-frames semantics, lin-guistic structures processing.
In Antonio Zampolli,editor, Fundamental Studies in Computer Science, No.59, pages 55?88.
North Holland Publishing.C.J.
Fillmore.
1986.
Pragmatically controlled zeroanaphora.
In Proceedings of the Twelfth Annual Meet-ing of the Berkeley Liguistics Society.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3):245?288.K.
Litkowski.
2004.
SENSEVAL-3 Task: Automaticlabeling of semantic roles.
In Proc.
of SENSEVAL-3.L.
Ma`rquez, L. Villarejo, M. A.
Mart?`, and M. Taule`.
2007.SemEval-2007 Task 09: Multilevel semantic annotationof Catalan and Spanish.
In Proceedings of SemEval-07.S.
Pado?, M. Pennacchiotti, and C. Sporleder.
2008.
Se-mantic role assignment for event nominalisations byleveraging verbal data.
In Proceedings of Coling-2008.M.
Palmer, D. Dahl, R. Passonneau, L. Hirschman,M.
Linebarger, and J. Dowding.
1986.
Recoveringimplicit information.
In Proceedings of ACL-1986.M.
Palmer.
1990.
Semantic Processing for Finite Do-mains.
CUP, Cambridge, England.J.
Ruppenhofer.
2004.
The interaction of valence andinformation structure.
Ph.d., University of California,Berkeley, CA.D.
Shen and M. Lapata.
2007.
Using semantic roles toimprove question answering.
In Proc.
of EMNLP-07.M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.2003.
Using predicate arguments structures for infor-mation extraction.
In Proceedings of ACL-2003.M.
Surdeanu, R. Johansson, A. Meyers, Ll.
Ma`rquez, andJ.
Nivre.
2008.
The CoNLL-2008 shared task on jointparsing of syntactic and semantic dependencies.
InProceedings of CoNLL-2008, pages 159?177.111
