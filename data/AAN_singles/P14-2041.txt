Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 247?252,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsContent Importance Models for Scoring Writing From SourcesBeata Beigman Klebanov Nitin Madnani Jill Burstein Swapna SomasundaranEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541{bbeigmanklebanov,nmadnani,jburstein,ssomasundaran}@ets.orgAbstractSelection of information from externalsources is an important skill assessed ineducational measurement.
We address anintegrative summarization task used in anassessment of English proficiency for non-native speakers applying to higher educa-tion institutions in the USA.
We evaluate avariety of content importance models thathelp predict which parts of the source ma-terial should be selected by the test-takerin order to succeed on this task.1 IntroductionSelection and integration of information from ex-ternal sources is an important academic and lifeskill, mentioned as a critical competency in theCommon Core State Standards for English Lan-guage Arts/Literacy: College-ready students willbe able to ?gather relevant information from mul-tiple print and digital sources, assess the credibi-lity and accuracy of each source, and integrate theinformation while avoiding plagiarism.
?1Accordingly, large-scale assessments of writingincorporate tasks that test this skill.
One such testrequires test-takers to read a passage, then to lis-ten to a lecture discussing the same topic froma different point of view, and to summarize thepoints made in the lecture, explaining how theycast doubt on points made in the reading.
The qua-lity of the information selected from the lecture isemphasized in excerpts from the scoring rubric forthis test (below); essays are scored on a 1-5 scale:Score 5 successfully selects the important infor-mation from the lecture and coherently andaccurately presents this information in rela-tion to the relevant information presented inthe reading.1http://www.corestandards.org/ELA-Literacy/CCRA/W.Score 4 is generally good in selecting the impor-tant information from the lecture ..., but itmay have a minor omission.Score 3 contains some important informationfrom the lecture ..., but it may omit one majorkey point.Score 2 contains some relevant information fromthe lecture ...
The response significantlyomits or misrepresents important points.Score 1 provides little or no meaningful or rele-vant coherent content from the lecture.The ultimate goal of our project is to improveautomated scoring of such essays by taking intoaccount the extent to which a response integratesimportant information from the lecture.
This pa-per reports on the first step aimed at automaticallyassigning importance scores to parts of the lecture.The next step ?
developing an essay scoring sys-tem using content importance models along withother features of writing quality, will be addressedin future work.
A simple essay scoring mechanismwill be used for evaluation purposes in this paper,as described in the next section.2 Design of ExperimentIn evaluations of summarization algorithms, it iscommon practice to derive the gold standard con-tent importance scores from human summaries, asdone, for example, in the pyramid method, wherethe importance of a content element correspondsto the number of reference human summaries thatmake use of it (Nenkova and Passonneau, 2004).Selection of the appropriate content plays a cru-cial role in attaining a high score for the essayswe consider here, as suggested by the quotes fromthe scoring rubric in ?1, as well as by a corpusstudy by Plakans and Gebril (2013).
We thereforeobserve that high-scoring essays can be thought247of as high-quality human summaries of the lec-ture, albeit containing, in addition, references tothe reading material and language that contraststhe different viewpoints, making them a somewhatnoisy gold standard.
On the other hand, since low-scoring essays contain deficient summaries of thelecture, our setup allows for a richer evaluationthan typical in studies using gold standard humandata only, in that a good model should not onlyagree with the gold standard human summariesbut should also disagree with sub-standard humansummaries.
We therefore use correlation with es-say score to evaluate content importance models.The evaluation will proceed as follows.
Everyessay E is responding to a test prompt that con-tains a lecture L and a reading R. We identify theessay?s overlap with the lecture:O(E,L) = {x|x ?
L, x ?
E} (1)where the exact definition of x, that is, what istaken to be a single unit of information, will beone of the parameters to be studied.
The essay isthen assigned the following score by the contentimportance model M :SM(E) =?x?O(E,L)wM(x)?
C(x,E)nE(2)where wM(x) is the importance weight as-signed by model M to item x in the lecture,C(x,E) is the count of tokens in E that realizethe information unit x, and nEis the number oftokens in the essay.
In this paper, the distinctionbetween x and C is that between type and tokencount of instances of that type.2This simple sco-ring mechanism quantifies the rate of usage of im-portant information per token in the essay.
Finally,we calculate the correlation of scores assigned toessays by model M with scores assigned to thesame essays by human graders.This design ensures that once x is fixed, all thecontent importance models are evaluated withinthe same scoring scheme, so any differences in thecorrelations can be attributed to the differences inthe weights assigned by the importance models.2In the future, we intend to explore more complex rea-lization functions, allowing paraphrase, skip n-grams (as inROUGE (Lin, 2004)), and other approximate matches, suchas misspellings and inflectional variants.3 Content Importance ModelsOur setting can be thought of as a special kindof summarization task.
Test-takers are requiredto summarize the lecture while referencing thereading, making this a hybrid of single- and multi-document summarization, where one source istreated as primary and the other as secondary.We therefore consider models of content impor-tance that had been found useful in the summariza-tion literature, as well as additional models thatutilize a special feature of our scenario: We havehundreds of essays of varying quality respondingto any given prompt, as opposed to a typical newssummarization scenario where a small number ofhigh quality human summaries are available for agiven article.
A sample of these essays can be usedwhen developing a content importance model.We define the following importance models.For all definitions, x is a unit of informationin the lecture; C(x, t) is the number of tokens intext t that realize x; nLand nRare the number oftokens in the lecture and the reading, respectively.3Na?
?ve: w(x) = 1.
This is a simple overlap model.Prob: w(x) =C(x,L)nL, an MLE estimate ofthe probability that x appears in the lecture.Those x that appear more are more important.Position: w(x) =FP (x)nL, where FP (x) is theoffset of the first occurrence of x in the lec-ture.
The offset corresponds to the token?sserial number in the text, 1 through nL.LectVsRead: w(x) =C(x,L)nL?C(x,R)nR, that is, thedifference in the probabilities of occurrenceof x in the lecture and in the reading passagethat accompanies the lecture.
This model at-tempts to capture the contrastive aspect ofimportance ?
the content that is unique tothe lecture is more important than the contentthat is shared by the lecture and the reading.The following two models capitalize on evi-dence of use of information in better and worse es-says.
For estimating these models, we sample, foreach prompt, a development set of 750 essays re-sponding to the prompt (that is, addressing a givenpair of lecture and reading stimuli).
Out of these,we take, for each prompt, all essays at score points3Prob, Position, and LectVsRead models normalize bynRand nLto enable comparison of essays responding to dif-ferent lecture + reading stimuli (prompts).2484 and 5 (EGood) and all essays at score points 1and 2 (EBad).
These data do not overlap with theexperimental data described in section 4.
In bothdefinitions below, e is an essay.Good: w(x) =|{e?EGood|x?e}||EGood|.
An x is more im-portant if more good essays use it.
Hong andNenkova (2014) showed that a variant of thismeasure used on pairs of articles and their ab-stracts from the New York Times effectivelyidentified words that typically go into sum-maries, across topics.
In contrast, our mea-surements are prompt-specific.GoodVsBad: w(x) =|{e?EGood|x?e}||EGood|?|{e?EBad|x?e}||EBad|.
An x is more important ifgood essays use it more than bad essays.To our knowledge, this measure has notbeen used in the summarization literature,probably because a large sample of humansummaries of varying quality is typically notavailable.4 DataWe use 116 prompts drawn from an assessment ofEnglish proficiency for non-native speakers.
Eachprompt contains a lecture and a reading passage.For each prompt, we sample about 750 essays.Each essay has an operational score provided bya human grader.
Table 1 shows the distribution ofessay scores; mean score is 3.
Text transcripts ofthe lectures were used.Score 1 2 3 4 5Proportion 0.13 0.18 0.35 0.25 0.09Table 1: Distribution of essay scores.5 ResultsIndependently from the content importancemodels, we address the effect of the granularity ofthe unit of information.
Intuitively, since all thematerials for a given prompt deal with the sametopic, we expect large unigram overlaps betweenlecture and reading, and between good and badessays, whereas n-grams with larger n can bemore distinctive.
On the other hand, larger n leadto misses, where an information unit would failto be identified in an essay due to a paraphrase,thus impairing the ability of the scoring functionto use the content importance model effectively.We therefore evaluate each content importancemodel for different granularities of the contentunit x: n-grams for n = 1, 2, 3, 4.
Table 2 showsthe correlations with essay scores.Content Pearson?s rImportanceModel n=1 n=2 n=3 n=4Na?
?ve 0.24 0.27* 0.24 0.20Prob 0.04 0.14 0.17 0.14Position 0.22 0.30* 0.26* 0.20LectVsRead 0.09 0.25* 0.31* 0.26*Good 0.07 0.15 0.10 0.07GoodVsBad 0.54* 0.42* 0.32* 0.21Table 2: Correlations with essay scores attained bycontent models, for various definitions of informa-tion unit (n-grams with n = 1, 2, 3, 4).
Five topscores are boldfaced.
The baseline performanceis shown in underlined italics.
Correlations thatare significantly better (p < 0.05) than the na?
?ven = 1 model are marked with an asterisk.
Weuse McNemar (1955, p. 148) test for significanceof difference between same-sample correlations.N = 85, 252 for all correlations.6 DiscussionThe Na?
?ve model with n = 1 can be considered abaseline, corresponding to unweighted word over-lap between the lecture and the essay.
This modelattains a significant positive correlation with essayscore (r = 0.24), suggesting that, in general, bet-ter writers use more material from the lecture.Our next observation is that the Prob and Goodmodels do not improve over the baseline, that is,their weighting schemes generally assign higherweights to the wrong units.
We believe the rea-son for this is that the most highly used n-grams,in the lecture and in the essays, correspond to ge-neral topical and functional elements.
The impor-tance of these elements is discounted in the moreeffective Position, LectVsRead, and GoodVsBadmodels, highlighting subtler aspects of the lecture.Next, let us consider the granularity of the unitsof information.
We observe that 4-grams are in-ferior to trigrams for all models, suggesting thatdata sparsity is becoming a problem for matching4-word sequences.
For models that assign weightbased on one or two sources (lecture, or lectureand reading) ?
Na?
?ve, Position, LectVsRead ?
un-igram models are generally ineffective, while bi-249gram and trigram models significantly outperformthe baseline.
We interpret this as suggesting thatit is certain particular, detailed aspects of the top-ical concepts that constitute the important nuggetsin the lecture; these are usually realized by multi-word sequences.The GoodVsBad models show a different pat-tern, obtaining the best performance with a uni-gram version.
These models are sensitive to datasparsity not only when matching essays to thelecture (this problem is common to all models)but also during model building.
Recall that theweights in a GoodVsBad model are estimatedbased on differential use in samples of good andbad essays.
The estimation of use-in-a-corpus ismore accurate for smaller n, because longer n-grams are more susceptible to paraphrasing, whichleads to under-estimation of use.
Assuming thatparaphrasing behavior of good and bad writers isnot the same ?
in fact, there is corpus evidencethat better writers paraphrase more (Burstein etal., 2012) ?
the resulting inaccuracies might im-pact the estimation of differential use in a sys-tematic manner, making the n > 1 models lesseffective than the unigrams.
Given that (a) theGoodVsBad bigram model is the second best over-all in spite of the shortcomings of the estimationprocess, and (b) that the bigram models workedbetter than unigram models for all the other con-tent importance models, the GoodVsBad bigrammodel could probably be improved significantlyby using a more flexible information realizationmechanism.To illustrate the information assigned high im-portance by different models, consider a lec-ture discussing advantages of fish farming.
Thetop-scoring Good bigrams are topical expressions(fish farming), functional bigrams around fish andfarming,4aspects of content dealt with at lengthin the lecture (wild fish, commercial fishing), bi-grams referencing some of the claims ?
fish con-taining less fat and being used for fish meal.
Inaddition, this model picks out some sequences offunction words and punctuation (of the, are not,?, and?, ?, the?)
that suggest that better essaystend to give more detail (hence have more com-plex noun phrases and coordinated constructions)and to draw contrast.For the bigram GoodVsBad model, the topi-cal bigram fish farming is not in the top 20 bi-4such as that fish, of fish, farming is, ?, fish?grams.
Although some bigrams are shared withthe Good model, the GoodVsBad model selectsadditional details about the claims, such as thecontrast between inedible fish and edible fish thatis eaten by humans, as well as reference to chemi-cals used in farming and to the claim that wild fishare already endangered by other practices.The most important bigrams according to theLectVsRead model include functional bigramsaround fish and farming, functional sequences(that the, is a), as well as commercial fishing andedible fish.
Also selected are functional bigramsaround consumption and species, hinting, indi-rectly, at the edibility differences between species.Finally, this model selects almost all bigrams inthe reading passage makes, the reading makesclaims that and the reading says.
While distin-guishing the lecture from the reading, these do notcapture topic-relevant content of the lecture.The GoodVsBad unigram model selects poul-try, endangered, edible, chemicals among its top 6unigrams,5effectively touching upon the connec-tion with other farm-raised foods (poultry, chemi-cals), with wild fish (endangered) and with humanbenefit (edible) that are made in the lecture.7 Related workModern essay scoring systems are complex andcover various aspects of the writing construct,such as grammar, organization, vocabulary (Sher-mis and Burstein, 2013).
The quality of contentis often addressed by features that quantify thesimilarity between the vocabulary used in an es-say and reference essays from given score points(Attali and Burstein, 2006; Foltz et al, 2013; At-tali, 2011).
For example, Attali (2011) proposed ameasure of differential use of words in higher andlower scoring essays defined similarly to Good-VsBad, without, however, considering the sourcetext at all.
Such features can be thought of as con-tent quality features, as they implicitly assume thatwriters of better essays use better content.
How-ever, there are various kinds of better content, onlyone of them being selection of important informa-tion from the source; other elements of contentoriginate with the writer, such as examples, dis-course markers, evaluations, introduction and con-clusion, etc.
Our approach allows focusing on aparticular aspect of content quality, namely, selec-tion of appropriate materials from the source.5the other two being fishing and used.250Our results are related to the findings of Gure-vich and Deane (2007) who studied the differencebetween the reading and the lecture in their im-pact on essay scores for this test.
Using data froma single prompt, they showed that the differencebetween the essay?s average cosine similarity tothe reading and its average cosine similarity to thelecture is predictive of the score for non-nativespeakers of English, thus using a model similarto LectVsRead, although they took all lecture,reading, and essay words into account, in contrastto our model that looks only at n-grams that ap-pear in the lecture.
Our study shows that the ef-fectiveness of lecture-reading contrast models foressay scoring generalizes to a large set of prompts.Similarly, Evanini et al (2013) found that over-lap with material that is unique to the lecture (notshared with the reading) was predictive of scoresin a spoken source-based question answering task.In the vast literature on summarization, ourwork is closest to Hong and Nenkova (2014) whostudied models of word importance for multi-document summarization of news.
The Prob, Po-sition, and Good models are inspired by theirfindings of the effectiveness of similar models intheir setting.
We found that, in our setting, Proband Good models performed worse than assigninga uniform weight to all words.
We note, however,that models from Hong and Nenkova (2014) arenot strictly comparable, since their word proba-bility models were calculated after stopword ex-clusion, and their model that inspired our Goodmodel was defined somewhat differently and val-idated using content words only.
The defini-tion of our Position model and its use in the es-say scoring function S (equation 2) correspond toHong and Nenkova (2014) average first locationmodel for scoring summaries.
Differently fromtheir findings, this model is not effective for sin-gle words in our setting.
Position models over n-grams with n > 1 are effective, but their predic-tion is in the opposite direction of that found forthe news data ?
the more important materials tendto appear later in the lecture, as indicated by thepositive r between average first position and essayscore.
These findings underscore the importanceof paying attention to the genre of the source ma-terial when developing summarization systems.Our summarization task incorporates elementsof contrastive opinion summarization (Paul et al,2010; Kim and Zhai, 2009), since the lecture andthe reading sometimes interpret the same facts ina positive or negative light (for example, the factthat chemicals are used in fish farms is negativeif compared to wild fish, but not so if comparedto other farm-raised foods like poultry).
Relation-ships between aspect and sentiment (Brody andElhadad, 2010; Lazaridou et al, 2013) are alsorelevant, since aspects of the same fact are em-phasized with different evaluations (the quantityvs the variety of species that go into fish meal forfarmed fish).
We hypothesize that units participat-ing in sentiment and aspect contrasts are of higherimportance; this is a direction for future work.8 ConclusionIn this paper, we addressed the task of automati-cally assigning importance scores to parts of a lec-ture that is to be summarized as part of an Englishlanguage proficiency test.
We investigated the op-timal units of information to which importanceshould be assigned, as well as a variety of impor-tance scoring models, drawing on the news sum-marization and essay scoring literature.We found that bigrams and trigrams were ge-nerally more effective than unigrams and 4-gramsacross importance models, with some exceptions.We also found that the most effective impor-tance models are those that equate importanceof an n-gram with its preferential use in higher-scoring essays than in lower-scoring ones, aboveand beyond merely looking at the n-grams used ingood essays.
This demonstrates the utility of usingnot only gold, high-quality human summaries, butalso sub-standard ones when developing contentimportance models.Additional importance criteria that are intrinsicto the lecture, as well as those that capture contrastwith a different source discussing the same topic,were also found to be reasonably effective.
Sincedifferent importance models often select differentitems as most important, we intend to investigatecomplementarity of the different models.Finally, our results highlight that the effective-ness of an importance model depends on the genreof the source text.
Thus, while a first sentencebaseline is very competitive in news summariza-tion, we found that important information tendsnot to be located in the opening sentences in ourdata (these tend to provide general, introductoryinformation), but appears later on, when more de-tailed, specific claims are put forward.251ReferencesYigal Attali and Jill Burstein.
2006.
Automated EssayScoring With e-raterR?V.2.
Journal of Technology,Learning, and Assessment, 4(3).Yigal Attali.
2011.
A Differential Word Use Measurefor Content Analysis in Automated Essay Scoring.ETS Research Report, RR-11-36.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 804?812, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Jill Burstein, Michael Flor, Joel Tetreault, Nitin Mad-nani, and Steven Holtzman.
2012.
Examining Lin-guistic Characteristics of Paraphrase in Test-TakerSummaries.
ETS Research Report, RR-12-18.Keelan Evanini, Shasha Xie, and Klaus Zechner.
2013.Prompt-based content scoring for automated spokenlanguage assessment.
In Proceedings of the EighthWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 157?162, Atlanta,Georgia, June.
Association for Computational Lin-guistics.Peter Foltz, Lynn Streeter, Karen Lochbaum, andThomas Landauer.
2013.
Implementation and Ap-plication of the Intelligent Essay Assessor.
In MarkShermis and Jill Burstein, editors, Handbook of au-tomated essay evaluation: Current applications andnew directions, pages 68?88.
New York: Routh-ledge.Olga Gurevich and Paul Deane.
2007.
Documentsimilarity measures to distinguish native vs. non-native essay writers.
In Human Language Technolo-gies 2007: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics; Companion Volume, Short Papers, pages49?52, Rochester, New York, April.
Association forComputational Linguistics.Kai Hong and Ani Nenkova.
2014.
Improvingthe estimation of word importance for news multi-document summarization.
In The Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, Gottenberg, Sweden, April.
As-sociation for Computational Linguistics.Hyun Duk Kim and ChengXiang Zhai.
2009.
Gener-ating comparative summaries of contradictory opin-ions in text.
In Proceedings of the 18th ACM Confer-ence on Information and Knowledge Management,CIKM ?09, pages 385?394, New York, NY, USA.ACM.Angeliki Lazaridou, Ivan Titov, and CarolineSporleder.
2013.
A bayesian model for jointunsupervised induction of sentiment, aspect anddiscourse representations.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1630?1639, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Chin-Yew Lin.
2004.
ROUGE: A package for auto-matic evaluation of summaries.
In Proceedings ofACL workshop: Text summarization branches out,pages 74?81, Barcelona, Spain, July.
Association forComputational Linguistics.Quinn McNemar.
1955.
Psychological Statistics.
NewYork: J. Wiley and Sons, 2nd edition.Ani Nenkova and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In Human Language Technologies2004: The Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 145?152, Boston, Massachusetts, USA, May2 - May 7.
Association for Computational Linguis-tics.Michael J. Paul, ChengXiang Zhai, and Roxana Girju.2010.
Summarizing contrastive viewpoints in opin-ionated text.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 66?76, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Lia Plakans and Atta Gebril.
2013.
Using multipletexts in an integrated writing assessment: Sourcetext use as a predictor of score.
Journal of SecondLanguage Writing, 22:217?230.Mark Shermis and Jill Burstein, editors.
2013.
Hand-book of Automated Essay Evaluation: Current Ap-plications and Future Directions.
New York: Rout-ledge.252
