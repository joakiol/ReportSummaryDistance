Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1691?1702,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsImproving Sparse Word Representations with Distributional Inference forSemantic CompositionThomas Kober, Julie Weeds, Jeremy Reffin and David WeirTAG laboratory, Department of Informatics, University of SussexBrighton, BN1 9RH, UK{t.kober, j.e.weeds, j.p.reffin, d.j.weir}@sussex.ac.ukAbstractDistributional models are derived from co-occurrences in a corpus, where only asmall proportion of all possible plausible co-occurrences will be observed.
This resultsin a very sparse vector space, requiring amechanism for inferring missing knowledge.Most methods face this challenge in waysthat render the resulting word representa-tions uninterpretable, with the consequencethat semantic composition becomes hard tomodel.
In this paper we explore an alter-native which involves explicitly inferring un-observed co-occurrences using the distribu-tional neighbourhood.
We show that distribu-tional inference improves sparse word repre-sentations on several word similarity bench-marks and demonstrate that our model is com-petitive with the state-of-the-art for adjective-noun, noun-noun and verb-object composi-tions while being fully interpretable.1 IntroductionThe aim of distributional semantics is to derivemeaning representations based on observing co-occurrences of words in large text corpora.
Howevernot all plausible co-occurrences will be observedin any given corpus, resulting in word representa-tions that only capture a fragment of the meaningof a word.
For example the verbs ?walking?
and?strolling?
may occur in many different and pos-sibly disjoint contexts, although both verbs wouldbe equally plausible in numerous cases.
This sub-sequently results in incomplete representations forboth lexemes.
In addition, models based on countingco-occurrences face the general problem of sparsityin a very high-dimensional vector space.
The mostcommon approaches to these challenges have in-volved the use of various techniques for dimension-ality reduction (Bullinaria and Levy, 2012; Lapesaand Evert, 2014) or the use of low-dimensionaland dense neural word embeddings (Mikolov et al,2013; Pennington et al, 2014).
The common prob-lem in both of these approaches is that compositionbecomes a black-box process due to the lack of inter-pretability of the representations.
Count-based mod-els are therefore a very attractive line of work withregards to a number of important long-term researchchallenges, most notably the development of an ade-quate model of distributional compositional seman-tics.
In this paper we propose the use of distribu-tional inference (DI) to inject unobserved but plau-sible distributional semantic knowledge into the vec-tor space by leveraging the intrinsic structure of thedistributional neighbourhood.
This results in richerword representations and furthermore mitigates thesparsity effect common in high-dimensional vectorspaces, while remaining fully interpretable.Our contributions are as follows: we show that typedand untyped sparse word representations, enrichedby distributional inference, lead to performance im-provements on several word similarity benchmarks,and that a higher-order dependency-typed vectorspace model, based on ?Anchored Packed Depen-dency Trees (APTs)?
(Weir et al, 2016), is com-petitive with the state-of-the-art for adjective-noun,noun-noun and verb-object compositions.
Using ourmethod, we are able to bridge the gap in perfor-mance between high dimensional interpretable mod-1691els and low dimensional non-interpretable modelsand offer evidence to support a possible explanationof why high-dimensional models usually performworse, together with a simple, practical method forover-coming this problem.
We furthermore demon-strate that intersective approaches to compositionbenefit more from distributional inference than com-position by union and highlight the ability of com-position by intersection to disambiguate the mean-ing of a phrase in a local context.The remainder of this paper is structured as follows:we discuss related work in section 2, followed byan introduction of the APT framework for semanticcomposition in section 3.
We describe distributionalinference in section 4 and present our experimentalwork, together with our results in section 5.
We con-clude this paper and outline future work in section 6.2 Related WorkOur method follows the distributional smoothingapproach of Dagan et al (1994) and Dagan etal.
(1997).
In these works the authors are con-cerned with smoothing the probability estimate forunseen words in bigrams.
This is achieved by mea-suring which unobserved bigrams are more likelythan others on the basis of the Kullback-Leibler di-vergence between bigram distributions.
This hasled to significantly improved performance on a lan-guage modelling for speech recognition task, as wellas for word-sense disambiguation in machine trans-lation (Dagan et al, 1994; Dagan et al, 1997).More recently Pado?
et al (2013) used a distribu-tional approach for smoothing derivationally relatedwords, such as oldish ?
old, as a back-off strategyin case of data sparsity.
However, none of theseapproaches have used distributional inference as ageneral technique for directly enriching sparse dis-tributional vector representations, or have exploredits behaviour for semantic composition.Compositional models of distributional semanticshave become an increasingly popular topic in the re-search community.
Starting from simple pointwiseadditive and multiplicative approaches to composi-tion, such as Mitchell and Lapata (2008; 2010), andBlacoe and Lapata (2012), to tensor based models,such as Baroni and Zamparelli (2010), Coecke etal.
(2010), Grefenstette et al (2013) and Papernoet al (2014), and neural network based approaches,such as Socher et al (2012), Le and Zuidema (2015),Mou et al (2015) and Tai et al (2015).
Zanzottoet al (2015) provide a decompositional analysis ofhow similarity is affected by distributional compo-sition, and link compositional models to convolu-tion kernels.
Most closely related to our approachof composition are the works of Thater et al (2010),Thater et al (2011) and Weeds et al (2014), whichaim to provide a general model of compositionalityin a typed distributional vector space.
In this paperwe adopt the approach to distributional compositionintroduced by Weir et al (2016), whose APT frame-work is based on a higher-order dependency-typedvector space, however they do not address the issueof sparsity in their work.3 BackgroundDistributional vector space models can broadlybe categorised into untyped proximity basedmodels and typed models (Baroni and Lenci,2010).
Examples of the former include Deer-wester et al (1990); Lund and Burgess (1996);Curran (2004); Sahlgren (2006); Bullinaria andLevy (2007) and Turney and Pantel (2010).
Thesemodels count the number of times every word in alarge corpus co-occurs with other words within aspecified spatial context window, without leveragingthe structural information of the text.
Typed modelson the other hand, take the grammatical relationbetween two words for a co-occurrence eventinto account.
Early proponents of that approachare Grefenstette (1994) and Lin (1998).
Morerecent work by Pado?
and Lapata (2007), Erk andPado?
(2008) and Weir et al (2016) uses dependencypaths to build a structured vector space model.
Inboth kinds of models, the raw counts are usuallytransformed by Positive Pointwise Mutual Informa-tion (PPMI) or a variant of it (Church and Hanks,1990; Niwa and Nitta, 1994; Scheible et al, 2013;Levy and Goldberg, 2014).In the following we will give an explanation of thetheory of composition with APTs as introduced byWeir et al (2016), which we adopt in this paper.In addition to direct relations between two words,the APT model also considers inverse and higher1692order relations.
Inverse relations are denoted witha horizontal bar above the dependency relation,such as amod for an inverse adjectival modifier.Higher order dependencies are separated by acolon as in the second order distributional featuredobj:nsubj.
The example below illustrateshow raw text is processed to retrieve elementaryrepresentations in our APT model.
As an examplewe consider a lowercased corpus consisting of thesentences:we folded the clean clothesi like your clotheswe bought white shoes yesterdayhe folded the white sheetsWe dependency parse the raw sentences and,following Weir et al (2016), align and aggregate theresulting parse trees according to their dependencytype as shown in Figure 1.
For example the lexemeclothes has the distributional features amod:dryand dobj:nsubj:we among others.
Over a largecorpus, this results in a very high-dimensional andsparse vector space, which due to its typed nature ismuch sparser than for untyped models.we folded the ... clean clothes ...i like ... your ... clothes ...we bought ... ... white shoes yesterdayhe folded the ... white sheets ...nsubj amoddetdobjnmod:tmodnmod:possFigure 1: Aligned Packed Dependency Tree representation ofthe example sentences.Composition with APTsComposition is linguistically motivated by the prin-ciple of compositionality, which states that themeaning of a complex expression is fully deter-mined by its structure and the meanings of its con-stituents (Frege, 1884).
Many simple approaches tosemantic composition neglect the structure and loseinformation in the composition process.
For exam-ple, the phrases house boat and boat house havethe exact same representation when composition isdone via a pointwise arithmetic operation.
Despiteperforming well in a number of studies, this com-mutativity is not desirable for a fine grained un-derstanding of the semantics of natural language.When performing composition with APTs, we adoptthe method introduced by Weir et al (2016) whichviews distributional composition as a process of con-textualisation.
For composing the adjective whitewith the noun clothes via the dependency relationamod we need to consider how the adjective inter-acts with the noun in the vector space.
The distri-butional features of white describe things that arewhite via their first order relations such as amod,and things that can be done to white things, such asbought via amod:dobj in the example above.Table 1 shows a number of features extracted fromthe aligned dependency trees in Figure 1 and high-lights that adjectives and nouns do not share manyfeatures if only first order dependencies would beconsidered.
However through the inclusion of in-verse and higher order dependency paths we can ob-serve that the second order features of the adjectivealign with the first order features of the noun.
Forcomposition, the adjective white needs to be offsetby its inverse relation to clothes1 making it distribu-tionally similar to a noun that has been modified bywhite.
Offsetting can be seen as shifting the currentviewpoint in the APT data structure and is necessaryfor aligning the feature spaces for composition (Weiret al, 2016).
We are then in a position to composethe offset representation of white with the vector forclothes by the union or the intersection of their fea-tures.Table 2 shows the resulting feature spaces of thecomposed vectors.
It is worth noting that any arith-metic operation can be used to combine the countsof the aligned features, however for this paper weuse pointwise addition for both composition func-tions.
One of the advantages of this approach tocomposition is that the inherent interpretability ofcount-based models naturally expands beyond theword level, allowing us to study the distributional se-mantics of phrases in the same space as words.
Dueto offsetting one of the constituents, the compositionoperation is not commutative and hence avoids iden-tical representations for house boat and boat house.However, the typed nature of our vector space re-1The inverse of amod is just amod.1693white clothesDistributional Features Offset Features (by amod) Co-occurrence Count Distributional Features Co-occurrence Countamod:shoes :shoes 1 amod:clean 1amod:dobj:bought dobj:bought 1 dobj:like 1amod:dobj:folded dobj:folded 1 dobj:folded 1amod:dobj:nsubj:we dobj:nsubj:we 1 dobj:nsubj:we 1Table 1: Example feature spaces for the lexemes white and clothes extracted from the dependency tree of Figure 1.
Not all featuresare displayed for space reasons.
Offsetting amod:shoes by amod results in an empty dependency path, leaving just the wordco-occurrence :shoes as feature.Composition by union Composition by intersectionDistributional Features Co-occurrence Count Distributional Features Co-occurrence Count:shoes 1amod:clean 1dobj:bought 1dobj:folded 2 dobj:folded 2dobj:like 1dobj:nsubj:we 2 dobj:nsubj:we 2Table 2: Comparison of composition by union and composition by intersection.
Not all features are displayed for space reasons.sults in extreme sparsity, for example while the un-typed VSM has 130k dimensions, our APT modelcan have more than 3m dimensions.
We thereforeneed to enrich the elementary vector representationswith the distributional information of their nearestneighbours to ease the sparsity effect and infer miss-ing information.
Due to the syntactic nature of ourcomposition operation it is not straightforward toapply common dimensionality reduction techniquessuch as SVD, as the type information needs to bepreserved.4 Distributional InferenceFollowing Dagan et al (1994) and Dagan etal.
(1997), we propose a simple unsupervised al-gorithm for enriching sparse vector representationswith their nearest neighbours.
We show that ourdistributional inference algorithm improves perfor-mance for untyped and typed models on severalword similarity benchmarks, as well as being com-petitive with the state-of-the-art on semantic com-position.
As shown in algorithm 1 below, we iter-ate over all word vectors w in a given distributionalmodel M , and add the vector representations of thenearest neighbours n, determined by cosine similar-ity, to the representation of the enriched word vectorw?.
The parameter ?
in line 4 scales the contribu-tion of the original word vector to the resulting en-riched representation.
In this work we always chose?
to be identical to the number of neighbours usedfor distributional inference.
For example, if we used10 neighbours for DI, we would set ?
= 10, whichwe found sufficient to prevent the neighbours fromdominating the vector representation.
In our exper-iments we kept the input distributional model fixed,however it is equally possible to update the givenmodel in an online fashion, adding some amountof stochasticity to the enriched word vector repre-sentations.
There is a number of possibilities forthe neighbour retrieval function neighbours() andwe explore several options in this paper.
The algo-rithm furthermore is agnostic to the input distribu-tional model, for example it is possible to use com-pletely different vector space models for queryingneighbours and enrichment.Algorithm 1 Distributional Inference1: procedure DIST INFERENCE(M )2: init M ?3: for all w in M do4: w?
?
w ?
?5: for all n in neighbours(M,w) do6: w?
?
w?
+ n7: add w?
to M ?8: end for9: end for10: return M ?11: end procedure1694Static Top n Neighbour RetrievalThe perhaps simplest way is to choose the top nmost similar neighbours for each word in the vec-tor space and enrich the respective vector represen-tations with them.Density based Neighbour RetrievalThis approach has its roots in kernel density esti-mation (Parzen, 1962), however instead of defininga static global parzen window, we set the windowsize for every word individually, depending on thedistance to its nearest neighbour, plus a threshold.For example if the cosine distance between the targetvector and its top neighbour is 0.5, we use a windowsize of 0.5 +  for that word.
In our experiments wetypically define  to be proportional to the distanceof the nearest neighbour (e.g.
 = 0.5?
0.1).WordNet based Neighbour RetrievalInstead of leveraging the intrinsic structure of ourdistributional vector space, we retrieve neighboursby querying WordNet (Fellbaum, 1998), and treatsynsets with agreeing PoS tags as the nearest neigh-bours of any target vector.
This restricts the retrievedneighbours to synonyms only.5 ExperimentsOur model is based on a cleaned October 2013Wikipedia dump, which excludes all pages withfewer than 20 page views, resulting in a corpus ofapproximately 0.6 billion tokens (Wilson, 2015).The corpus is lowercased, tokenised, lemmatised,PoS tagged and dependency parsed with the Stan-ford NLP tools, using universal dependencies (Man-ning et al, 2014; de Marneffe et al, 2014).
We thenbuild our APT model with first, second and third or-der relations.
We remove distributional features witha count of less than 10, and vectors containing fewerthan 50 non-zero entries.
The raw counts are subse-quently transformed to PPMI weights.
The untypedvector space model is built from the same lower-cased, tokenised and lemmatised Wikipedia corpus.We discard terms with a frequency of less than 50and apply PPMI to the raw co-occurrence counts.Shifted PPMIWe explore a range of different values for shiftingthe PPMI scores as these have a significant impacton the performance of the APT model.
The effectof shifting PPMI scores for untyped vector spacemodels has already been explored in Levy and Gold-berg (2014), and Levy et al (2015), thus we onlypresent results for the APT model.
As shown inequation 1, PMI is defined as the log of the ratioof the joint probability of observing a word w anda context c together, and the product of the respec-tive marginals of observing them separately.
In ourAPT model, a context c is defined as a dependencyrelation together with a word.PMI(w, c) = log P (w, c)P (w)P (c)SPPMI(w, c) = max(PMI(w, c)?
log k, 0)(1)As PMI is negatively unbounded, PPMI is used toensure that all values are greater than or equal to0.
Shifted PPMI (SPPMI) subtracts a constant fromany PMI score before applying the PPMI threshold.We experiment with values of 1, 5, 10, 40 and 100for the shift parameter k.5.1 Word Similarity ExperimentsWe first evaluate our models on 3 word similar-ity benchmarks, MEN (Bruni et al, 2014), whichis testing for relatedness (e.g.
meronymy orholonymy) between terms, SimLex-999 (Hill et al,2015), which is testing for substitutability (e.g.
syn-onymy, antonymy, hyponymy and hypernymy), andWordSim-353 (Finkelstein et al, 2001), where weuse the version of Agirre et al (2009), who split thedataset into a relatedness and a substitutability sub-set.
Baroni and Lenci (2011) have shown that un-typed models are typically better at capturing relat-edness, whereas typed models are better at encodingsubstitutability.
Performance is measured by com-puting Spearman?s ?
between the cosine similaritiesof the vector representations and the correspondingaggregated human similarity judgements.
For theseexperiments we keep the number of neighbours thata word vector can consume fixed at 30.
This value isbased on preliminary experiments on WordSim-353(see Figure 2) using the static top n neighbour re-trieval function and a PPMI shift of k = 40.
Figure 2shows that distributional inference improves perfor-mance for any number of neighbours over a modelwithout DI (marked as horizontal dashed lines foreach WordSim-353 subset) and peaks at a value of169530.
Performance slightly degrades with more neigh-bours.
For the untyped VSM we use a symmetricwindow of 5 on either side of the target word.Figure 2: Effect of the number of neighbours on WordSim-353.Table 3 highlights the effect of the SPPMI shiftparameter k, while keeping the number of neigh-bours fixed at 30 and using the static top n neigh-bour retrieval function.
For the APT model, a valueof k = 40 performs best (except for SimLex-999,where smaller shifts give better results), with a per-formance drop-off for larger shifts.
In our experi-ments we find that a shift of k = 1 results in top per-formance for the untyped vector space model.
It ap-pears that shifting the PPMI scores in the APT modelhas the effect of cleaning the vectors from noisyPPMI artefacts, which reinforces the predominantsense, while other senses get suppressed.
Sub-sequently, this results in a cleaner neighbourhoodaround the word vector, dominated by a single sense.This explains why distributional inference slightlydegrades performance for smaller values of k.Table 4 shows that distributional inference suc-cessfully infers missing information for both modeltypes, resulting in improved performance over mod-els without the use of DI on all datasets.
The im-provements are typically larger for the APT model,suggesting that it is missing more distributionalknowledge in its elementary representations than un-typed models.
The density window and static top nneighbour retrieval functions perform very similar,however the static approach is more consistent andnever underperforms the baseline for either modeltype on any dataset.
The WordNet based neigh-bour retrieval function performs particularly well onSimLex-999.
This can be explained by the fact thatantonyms, which frequently happen to be among thenearest neighbours in distributional vector spaces,are regarded as dissimilar in SimLex-999, whereasthe WordNet neighbour retrieval function only re-turns synonyms.
The results furthermore confirmthe effect that untyped models perform better ondatasets modelling relatedness, whereas typed mod-els work better for substitutability tasks (Baroni andLenci, 2011).5.2 Composition ExperimentsOur approach to semantic composition as describedin section 3 requires the dimensions of our vectorspace models to be meaningful and interpretable.However, the problem of missing information is am-plified in compositional settings as many compati-ble dimensions between words are not observed inthe source corpus.
It is therefore crucial that dis-tributional inference is able to inject some of themissing information in order to improve the com-position process.
For the experiments involving se-mantic composition, we enrich the elementary rep-resentations of the phrase constituents before com-position.We first conduct a qualitative analysis for ourAPT model and observe the effect of distributionalinference on the nearest neighbours of composedadjective-noun, noun-noun and verb-object com-pounds.
In these experiments, we show how dis-tributional inference changes the neighbourhood inwhich composed phrases are embedded, and high-light the difference between composition by unionand composition by intersection.
For this exper-iment we use the static top n neighbour retrievalfunction with 30 neighbours and k = 40.Table 5 shows a small number of example phrasestogether with their top 3 nearest neighbours, com-puted from the union of all words in the Wikipediacorpus and all phrase pairs in the Mitchell and La-pata (2010) dataset.
As can be seen, nearest neigh-bours of phrases can be either single words or othercomposed phrases.
Words or phrases marked with?*?
in Table 5 mean that DI introduced, or failedto downrank, a spurious neighbour, while boldfacemeans that performing distributional inference re-sulted in a neighbourhood more coherent with thequery phrase than without DI.Table 5 shows that composition by union is unable to1696APTs MEN SimLex-999 WordSim-353 (rel) WordSim-353 (sub)without DI with DI without DI with DI without DI with DI without DI with DIk = 1 0.54 0.52 0.31 0.30 0.34 0.27 0.62 0.60k = 5 0.64 0.65 0.35 0.36 0.56 0.51 0.74 0.73k = 10 0.63 0.66 0.35 0.36 0.56 0.55 0.75 0.74k = 40 0.63 0.68 0.30 0.32 0.55 0.61 0.75 0.76k = 100 0.61 0.67 0.26 0.29 0.47 0.60 0.71 0.72Table 3: Effect of the magnitude of the shift parameter k in SPPMI on the word similarity tasks.
Boldface means best performanceper dateset.APTs (k = 40) No Distributional Inference Density Window Static Top n WordNetMEN 0.63 0.67 0.68 0.63SimLex-999 0.30 0.32 0.32 0.38WordSim-353 (rel) 0.55 0.62 0.61 0.56WordSim-353 (sub) 0.75 0.78 0.76 0.77Untyped VSM (k = 1) No Distributional Inference Density Window Static Top n WordNetMEN* 0.71 0.71 0.71 0.71SimLex-999 0.30 0.29 0.30 0.36WordSim-353 (rel) 0.60 0.64 0.64 0.52WordSim-353 (sub) 0.70 0.73 0.72 0.67Table 4: Neighbour retrieval function comparison.
Boldface means best performance on a dataset per VSM type.
*) With 3significant figures, the density window approach (0.713) is slightly better than the baseline without DI (0.708), static top n (0.710)and WordNet (0.710).downrank unrelated neighbours introduced by dis-tributional inference.
For example large quantityis incorrectly introduced as a top ranked neighbourfor the phrase small house, due to the proximity ofsmall and large in the vector space.
The phrasesmarket leader and television programme are two ex-amples of incoherent neighbours, which the compo-sition function was unable to downrank and whereDI could not improve the neighbourhood.
Compo-sition by intersection on the other hand vastly ben-efits from distributional inference.
Due to the in-creased sparsity induced by the composition pro-cess, a neighbourhood without DI produces numer-ous spurious neighbours as in the case of the verbhave as a neighbour for win battle.
Distributionalinference introduces qualitatively better neighboursfor almost all phrases.
For example, governmentleader and opposition member are introduced as topranked neighbours for the phrase party leader, andstress importance and underline are introduced asnew top neighbours for the phrase emphasise need.These results show that composition by union doesnot have the ability to disambiguate the meaning ofa word in a given phrasal context, whereas composi-tion by intersection has that ability but requires dis-tributional inference to unleash its full potential.For a quantitative analysis of distributional in-ference for semantic composition, we evaluate ourmodel on the composition dataset of Mitchell andLapata (2010), consisting of 108 adjective-noun,108 noun-noun, and 108 verb-object pairs.
The taskis to compare the model?s similarity estimates withthe human judgements by computing Spearman?s?.
For comparing the performance of the differentneighbour retrieval functions, we choose the sameparameter settings as in the word similarity experi-ments (k = 40 and using 30 neighbours for DI).Table 6 shows that the static top n and den-sity window neighbour retrieval functions performvery similar again.
The density window retrievalfunction outperforms static top n for compositionby intersection and vice versa for composition byunion.
The WordNet approach is competitive forcomposition by union, but underperfoms the otherapproaches for composition by intersection signifi-cantly.
For further experiments we use the static topn approach as it is computationally cheap and easyto interpret due to the fixed number of neighbours.Table 6 also shows that while composition by in-tersection is significantly improved by distributional1697Phrase Comp.
Union Union (with DI) Intersection Intersection (with DI)national AN government, regime, government, regime*, federal assembly, federal assembly, government,government ministry european state* government, monopoly local officesmall AN house, public building, house, public building, apartment, cottage, cottage, apartment, cabinhouse building large quantity* cabinparty NN leader, market leader, leader, government leader, party official, NDP, government leader, party official,leader government leader market leader* leader opposition membertraining NN programme, action programme, programme, action programme*, training college, trainee, training college,programme television programme television programme* education course education course, seminarwin battle VO win, win match, ties win, win match, fight war win match, win, have fight war, fight, win matchemphasise VO emphasise, underline, emphasise, underline, emphasise, prioritize, emphasise, stress importance,need underscore underscore negate underlineTable 5: Nearest neighbours AN, NN and VO pairs in the Mitchell and Lapata (2010) dataset, with and without distributionalinference.
Words and phrases marked with * denote spurious neighbours, boldfaced words and phrases mark improved neighbours.inference, composition by union does not appear tobenefit from it.Composition by Union or IntersectionBoth model types in this study support compositionby union as well as composition by intersection.
Inuntyped models, composition by union and com-position by intersection can be achieved by point-wise addition and pointwise multiplication respec-tively.
The major difference between compositionin the APT model and the untyped model is that inthe former, composition is not commutative due tooffsetting the modifier in a dependency relation (seesection 3).
Blacoe and Lapata (2012) showed thatan intersective composition function such as point-wise multiplication represents a competitive and ro-bust approach in comparison to more sophisticatedcomposition methods.
For the final set of experi-ments on the Mitchell and Lapata (2010) dataset,we present results the APT model and the untypedmodel, using composition by union and composi-tion by intersection, with and without distributionalinference.
We compare our models with the bestperforming untyped VSMs of Mitchell and Lap-ata (2010), and Blacoe and Lapata (2012), the bestperforming APT model of Weir et al (2016), aswell as with the recently published state-of-the-artmethods by Hashimoto et al (2014), and Wietinget al (2015), who are using neural network basedapproaches.
For our models, we use the static topn approach as neighbour retrieval function and tunethe remaining parameters, the SPPMI shift k (1, 5,10, 40, 100) and the number of neighbours (10, 30,50, 100, 500, 1000, 5000), for both model types,and the sliding window size for the untyped VSM(1, 2, 5), on the development portion of the Mitchelland Lapata (2010) dataset.
We keep the vector con-figuration (k and window size) fixed for all phrasetypes and only tune the number of neighbours usedfor DI individually.
The best vector configurationfor the APT model is achieved with k = 10 andfor the untyped VSM with k = 1.
For compositionby intersection best performance on the dev set wasachieved with 1000 neighbours for ANs, 10 for NNsand 50 for VOs with DI.
For composition by union,top performance was obtained with 100 neighboursfor ANs, 30 neighbours for NNs and 50 for VOs.The best results for the untyped model on the devset are achieved with a symmetric window size of 1and using 5000 neighbours for ANs, 10 for NNs and1000 for VOs with composition by pointwise multi-plication, and 30 neighbours for ANs, 5000 for NNsand 5000 for VOs for composition by pointwise ad-dition.
The validated numbers of neighbours on thedevelopment set show that the problem of missinginformation appears to be more severe for seman-tic composition than for word similarity tasks.
Eventhough a neighbour at rank 1000 or lower does notappear to have a close relationship to the target word,it still can contribute useful co-occurrence informa-tion not observed in the original vector.Table 7 shows that composition by intersection withdistributional inference considerably improves uponthe best results for APT models without distribu-tional inference and for untyped count-based mod-els, and is competitive with the state-of-the-art neu-ral network based models of Hashimoto et al (2014)and Wieting et al (2015).
Distributional inferencealso improves upon the performance of an untypedVSM where composition by pointwise multiplica-tion is outperforming the models of Mitchell and La-pata (2010), and Blacoe and Lapata (2012).
Table 7furthermore shows that DI has a smaller effect on1698APTs No Distributional Inference Density Window Static Top n WordNetintersection union intersection union intersection union intersection unionAdjective-Noun 0.10 0.41 0.31 0.39 0.25 0.40 0.12 0.41Noun-Noun 0.18 0.42 0.34 0.38 0.37 0.45 0.24 0.36Verb-Object 0.17 0.36 0.36 0.36 0.34 0.35 0.25 0.36Average 0.15 0.40 0.34 0.38 0.32 0.40 0.20 0.38Table 6: Neighbour retrieval function.
Underlined means best performance per phrase type, boldface means best average perfor-mance overall.Model Adjective-Noun Noun-Noun Verb-Object AverageAPT ?
union 0.45 (0.45) 0.45 (0.43) 0.38 (0.37) 0.43 (0.42)APT ?
intersect 0.50 (0.38) 0.49 (0.44) 0.43 (0.36) 0.47 (0.39)Untyped VSM ?
addition 0.46 (0.46) 0.40 (0.41) 0.38 (0.33) 0.41 (0.40)Untyped VSM ?
multiplication 0.46 (0.42) 0.48 (0.45) 0.40 (0.39) 0.45 (0.42)Mitchell and Lapata (2010) (untyped VSM & multiplication) 0.46 0.49 0.37 0.44Blacoe and Lapata (2012) (untyped VSM & multiplication) 0.48 0.50 0.35 0.44Hashimoto et al (2014) (PAS-CLBLM & Addnl) 0.52 0.46 0.45 0.48Wieting et al (2015) (Paragram word embeddings & RNN) 0.51 0.40 0.50 0.47Weir et al (2016) (APT & union) 0.45 0.42 0.42 0.43Table 7: Results for the Mitchell and Lapata (2010) dataset.
Results in brackets denote the performance of the respective modelswithout the use of distributional inference.
Underlined means best within group, boldfaced means best overall.the APT model based on composition by union andthe untyped model based on composition by point-wise addition.
The reason, as pointed out in the dis-cussion for Table 5, is that the composition functionhas no disambiguating effect and thus cannot elim-inate unrelated neighbours introduced by distribu-tional inference.
An intersective composition func-tion on the other hand is able to perform the disam-biguation locally in any given phrasal context.
Thisfurthermore suggests that for the APT model it is notnecessary to explicitly model different word sensesin separate vectors, as composition by intersectionis able to disambiguate any word in context individ-ually.
Unlike the models of Hashimoto et al (2014)and Wieting et al (2015), the elementary word rep-resentations, as well as the representations for com-posed phrases and the composition process in ourmodels are fully interpretable2.6 Conclusion and Future WorkOne of the major challenges in count-based mod-els is dealing with sparsity and missing information.To address this challenge, we contribute an unsu-pervised algorithm for enriching sparse word rep-resentations by exploiting the distributional neigh-bourhood.
We have demonstrated its benefit to typed2We release the APT vectors and our code at https://github.com/tttthomasssss/apt-toolkit.and untyped vector space models on a range of tasksand have shown that with distributional inferenceour APT model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-objectcompositions while being fully interpretable.
Withour method, we are able to bridge the gap in perfor-mance between low-dimensional non-interpretableand high-dimensional interpretable representations.Lastly, we have investigated the different behaviourof composition by union and composition by inter-section and have shown that an intersective com-position function, together with distributional in-ference, has the ability to locally disambiguate themeaning of a phrase.In future work we aim to scale our approach to se-mantic composition with distributional inference tolonger phrases and full sentences.
We furthermoreplan to investigate whether the number of neigh-bours required for improving elementary vector rep-resentations remains as high for other compositionaltasks and longer phrases as in this study.AcknowledgmentsThis work was funded by UK EPSRC projectEP/IO37458/1 ?A Unified Model of Compositionaland Distributional Compositional Semantics: The-ory and Applications?.
We would like to thank ouranonymous reviewers for their helpful comments.1699ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In Proceedings ofNAACL-HLT, pages 19?27, Boulder, Colorado, June.Association for Computational Linguistics.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721, December.Marco Baroni and Alessandro Lenci.
2011.
How weblessed distributional semantic evaluation.
In Pro-ceedings of GEMS Workshop, GEMS ?11, pages 1?10,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Cam-bridge, MA, October.
Association for ComputationalLinguistics.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for semanticcomposition.
In Proceedings of EMNLP, pages 546?556, Jeju Island, Korea, July.
Association for Compu-tational Linguistics.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
J. Artif.
Int.
Res.,49(1):1?47, January.John A. Bullinaria and Joseph P. Levy.
2007.
Extract-ing semantic representations from word co-occurrencestatistics: A computational study.
Behavior ResearchMethods, pages 510?526.John A. Bullinaria and Joseph P. Levy.
2012.
Extract-ing semantic representations from word co-occurrencestatistics: stop-lists, stemming, and svd.
Behavior Re-search Methods, 44(3):890?907.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Computational Linguistics, 16(1):22?29, March.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a composi-tional distributional model of meaning.
CoRR,abs/1003.4394.James Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.Ido Dagan, Fernando Pereira, and Lillian Lee.
1994.Similarity-based estimation of word cooccurrenceprobabilities.
In Proceedings of ACL, pages 272?278,Las Cruces, New Mexico, USA, June.
Association forComputational Linguistics.Ido Dagan, Lillian Lee, and Fernando Pereira.
1997.Similarity-based methods for word sense disambigua-tion.
In Proceedings of ACL, pages 56?63, Madrid,Spain, July.
Association for Computational Linguis-tics.Marie-Catherine de Marneffe, Timothy Dozat, NataliaSilveira, Katri Haverinen, Filip Ginter, Joakim Nivre,and Christopher D. Manning.
2014.
Universal stan-ford dependencies: A cross-linguistic typology.
InProceedings of LREC, pages 4585?4592, Reykjavik,Iceland, May.
European Language Resources Associ-ation (ELRA).
ACL Anthology Identifier: L14-1045.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
J. Amer.
Soc.Inf.
Sci., 41(6):391?407.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP, pages 897?906, Honolulu,Hawaii, October.
Association for Computational Lin-guistics.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The conceptrevisited.
In Proceedings of WWW, WWW ?01, pages406?414, New York, NY, USA.
ACM.Gottlob Frege.
1884.
Die Grundlagen der Arithmetik:Eine logisch mathematische Untersuchung u?ber denBegriff der Zahl.
W. Koebner.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for composi-tional distributional semantics.
Proceedings of IWCS.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers,Norwell, MA, USA.Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,and Yoshimasa Tsuruoka.
2014.
Jointly learningword representations and composition functions us-ing predicate-argument structures.
In Proceedings ofEMNLP, pages 1544?1555, Doha, Qatar, October.
As-sociation for Computational Linguistics.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguis-tics, 41(4):665?695, December.Gabriella Lapesa and Stefan Evert.
2014.
A large scaleevaluation of distributional semantic models: Parame-ters, interactions and model selection.
TACL, 2:531?545.1700Phong Le and Willem Zuidema.
2015.
The forest con-volutional network: Compositional distributional se-mantics with a neural chart and without binarization.In Proceedings of EMNLP, pages 1155?1164, Lisbon,Portugal, September.
Association for ComputationalLinguistics.Omer Levy and Yoav Goldberg.
2014.
Neural word em-bedding as implicit matrix factorization.
In Proceed-ings of NIPS, pages 2177?2185.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
TACL, 3:211?225.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of ACL, pages 768?774, Montreal, Quebec, Canada, August.
Associationfor Computational Linguistics.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,& Computers, 28(2):203?208.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of ACL - SystemDemonstrations, pages 55?60.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Proceedings of NIPS, pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In In Proceedings ofACL-08: HLT, pages 236?244.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and ZhiJin.
2015.
Discriminative neural sentence modelingby tree-based convolution.
In Proceedings of EMNLP,pages 2315?2325, Lisbon, Portugal, September.
Asso-ciation for Computational Linguistics.Yoshiki Niwa and Yoshihiko Nitta.
1994.
Co-occurrencevectors from corpora vs. distance vectors from dic-tionaries.
In Proceedings of Coling, COLING ?94,pages 304?309, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Sebastian Pado?, Jan S?najder, and Britta Zeller.
2013.Derivational smoothing for syntactic distributional se-mantics.
In Proceedings of ACL, pages 731?735,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Denis Paperno, Nghia The Pham, and Marco Baroni.2014.
A practical and linguistically-motivated ap-proach to compositional distributional semantics.
InProceedings of ACL, pages 90?99, Baltimore, Mary-land, June.
Association for Computational Linguistics.Emanuel Parzen.
1962.
On estimation of a probabil-ity density function and mode.
Ann.
Math.
Statist.,33(3):1065?1076, 09.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of EMNLP, pages 1532?1543, Doha, Qatar, October.
Association for Compu-tational Linguistics.Magnus Sahlgren.
2006.
The Word-space model.
Ph.D.thesis, University of Stockholm (Sweden).Silke Scheible, Sabine Schulte im Walde, and SylviaSpringorum.
2013.
Uncovering distributional dif-ferences between synonyms and antonyms in a wordspace model.
In Proceedings of IJCNLP, pages 489?497, Nagoya, Japan, October.
Asian Federation of Nat-ural Language Processing.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Korea,July.
Association for Computational Linguistics.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
In Proceedings of ACL, pages 1556?1566,Beijing, China, July.
Association for ComputationalLinguistics.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In Proceedingsof ACL, pages 948?957, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effec-tive vector model.
In Proceedings of IJCNLP, pages1134?1143, Chiang Mai, Thailand, November.
AsianFederation of Natural Language Processing.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.J.
Artif.
Int.
Res., 37(1):141?188, January.Julie Weeds, David Weir, and Jeremy Reffin.
2014.Distributional composition using higher-order depen-dency vectors.
In Proceedings of the 2nd Workshopon Continuous Vector Space Models and their Compo-sitionality, pages 11?20, Gothenburg, Sweden, April.Association for Computational Linguistics.David Weir, Julie Weeds, Jeremy Reffin, and ThomasKober.
2016.
Aligning packed dependency trees:1701a theory of composition for distributional seman-tics.
Computational Linguistics, in press (http://arxiv.org/abs/1608.07115).John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2015.
From paraphrase database to composi-tional paraphrase model and back.
TACL, 3:345?358.Benjamin Wilson.
2015.
The unknown perils ofmining wikipedia.
https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/, June.Fabio Massimo Zanzotto, Lorenzo Ferrone, and MarcoBaroni.
2015.
Squibs: When the whole is not greaterthan the combination of its parts: A ?decompositional?look at compositional distributional semantics.
Com-putational Linguistics, 41(1):165?173.1702
