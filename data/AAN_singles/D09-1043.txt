Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410?419,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPPerceptron Reranking for CCG RealizationMichael White and Rajakrishnan RajkumarDepartment of LinguisticsThe Ohio State UniversityColumbus, OH, USA{mwhite,raja}@ling.osu.eduAbstractThis paper shows that discriminativereranking with an averaged perceptronmodel yields substantial improvements inrealization quality with CCG.
The paperconfirms the utility of including languagemodel log probabilities as features in themodel, which prior work on discrimina-tive training with log linear models forHPSG realization had called into question.The perceptron model allows the combina-tion of multiple n-gram models to be opti-mized and then augmented with both syn-tactic features and discriminative n-gramfeatures.
The full model yields a state-of-the-art BLEU score of 0.8506 on Sec-tion 23 of the CCGbank, to our knowledgethe best score reported to date using a re-versible, corpus-engineered grammar.1 IntroductionIn this paper, we show how discriminative train-ing with averaged perceptron models (Collins,2002) can be used to substantially improve surfacerealization with Combinatory Categorial Gram-mar (Steedman, 2000, CCG).
Velldal and Oepen(2005) and Nakanishi et al (2005) have shown thatdiscriminative training with log-linear (maximumentropy) models is effective in realization rank-ing with Head-Driven Phrase Structure Grammar(Pollard and Sag, 1994, HPSG).
Here we showthat averaged perceptron models also perform wellfor realization ranking with CCG.
Averaged per-ceptron models are very simple, just requiring adecoder and a simple update function, yet despitetheir simplicity they have been shown to achievestate-of-the-art results in Treebank and CCG pars-ing (Huang, 2008; Clark and Curran, 2007a) aswell as on other NLP tasks.Along the way, we address the question ofwhether it is beneficial to incorporate n-gram logprobabilities as baseline features in a discrimina-tively trained realization ranking model.
On a lim-ited domain corpus, Velldal & Oepen found thatincluding the n-gram log probability of each can-didate realization as a feature in their log-linearmodel yielded a substantial boost in ranking per-formance; on the Penn Treebank (PTB), however,Nakanishi et al found that including an n-gram logprob feature in their model was of no benefit (withthe use of bigrams instead of 4-grams suggested asa possible explanation).
With these mixed results,the utility of n-gram baseline features for PTB-scale discriminative realization ranking has beenunclear.
In our particular setting, the question is:Do n-gram log prob features improve performancein broad coverage realization ranking with CCG,where factored language models over words, part-of-speech tags and supertags have previously beenemployed (White et al, 2007; Espinosa et al,2008)?We answer this question in the affirmative, con-firming the results of Velldal & Oepen, despitethe differences in corpus size and kind of lan-guage model.
We show that including n-gram logprob features in the perceptron model is highlybeneficial, as the discriminative models we testedwithout these features performed worse than thegenerative baseline.
These findings are in linewith Collins & Roark?s (2004) results with incre-mental parsing with perceptrons, where it is sug-gested that a generative baseline feature providesthe perceptron algorithm with a much better start-ing point for learning.
We also show that discrim-inative training allows the combination of multi-ple n-gram models to be optimized, and that thebest model augments the n-gram log prob fea-tures with both syntactic features and discrimina-tive n-gram features.
The full model yields a state-of-the-art BLEU (Papineni et al, 2002) score of0.8506 on Section 23 of the CCGbank, which isto our knowledge the best score reported to date410using a reversible, corpus-engineered grammar.The paper is organized as follows.
Section 2 re-views previous work on broad coverage realizationwith OpenCCG.
Section 3 describes our approachto realization reranking with averaged perceptronmodels.
Section 4 presents our evaluation of theperceptron models, comparing the results of dif-ferent feature sets.
Section 5 compares our resultsto those obtained by related systems and discussesthe difficulties of cross-system comparisons.
Fi-nally, Section 6 concludes with a summary anddiscussion of future directions for research.2 Background2.1 Surface Realization with CCGCCG (Steedman, 2000) is a unification-based cat-egorial grammar formalism which is defined al-most entirely in terms of lexical entries that encodesub-categorization information as well as syntacticfeature information (e.g.
number and agreement).Complementing function application as the stan-dard means of combining a head with its argu-ment, type-raising and composition support trans-parent analyses for a wide range of phenomena,including right-node raising and long distance de-pendencies.
An example syntactic derivation ap-pears in Figure 1, with a long-distance depen-dency between point and make.
Semantic com-position happens in parallel with syntactic compo-sition, which makes it attractive for generation.OpenCCG is a parsing/generation library whichworks by combining lexical categories for wordsusing CCG rules and multi-modal extensions onrules (Baldridge, 2002) to produce derivations.Surface realization is the process by which logicalforms are transduced to strings.
OpenCCG usesa hybrid symbolic-statistical chart realizer (White,2006) which takes logical forms as input and pro-duces sentences by using CCG combinators tocombine signs.
Edges are grouped into equiva-lence classes when they have the same syntacticcategory and cover the same parts of the input log-ical form.
Alternative realizations are ranked us-ing integrated n-gram or perceptron scoring, andpruning takes place within equivalence classes ofedges.
To more robustly support broad coveragesurface realization, OpenCCG greedily assemblesfragments in the event that the realizer fails to finda complete realization.To illustrate the input to OpenCCG, considerthe semantic dependency graph in Figure 2.
Inaa1heh3he h2<Det><Arg0> <Arg1><TENSE>pres<NUM>sg<Arg0>w1 want.01m1<Arg1><GenRel><Arg1><TENSE>presp1pointh1have.03make.03<Arg0>s[b]\np/npnp/nnpns[dcl]\np/nps[dcl]\np/(s[to]\np)npFigure 2: Semantic dependency graph from theCCGbank for He has a point he wants to make[.
.
.
], along with gold-standard supertags (cate-gory labels)the graph, each node has a lexical predication(e.g.
make.03) and a set of semantic features(e.g.
?NUM?sg); nodes are connected via depen-dency relations (e.g.
?ARG0?).
(Gold-standard su-pertags, or category labels, are also shown; seeSection 2.4 for their role in hypertagging.)
In-ternally, such graphs are represented using Hy-brid Logic Dependency Semantics (HLDS), adependency-based approach to representing lin-guistic meaning (Baldridge and Kruijff, 2002).
InHLDS, each semantic head (corresponding to anode in the graph) is associated with a nominalthat identifies its discourse referent, and relationsbetween heads and their dependents are modeledas modal relations.2.2 Realization from an Enhanced CCGbankOur starting point is an enhanced version of theCCGbank (Hockenmaier and Steedman, 2007)?acorpus of CCG derivations derived from the PennTreebank?with Propbank (Palmer et al, 2005)roles projected onto it (Boxwell and White, 2008).To engineer a grammar from this corpus suitablefor realization with OpenCCG, the derivations arefirst revised to reflect the lexicalized treatmentof coordination and punctuation assumed by themulti-modal version of CCG that is implementedin OpenCCG (White and Rajkumar, 2008).
Fur-ther changes are necessary to support semantic de-pendencies rather than surface syntactic ones; in411He has a point he wants to makenp sdcl\np/np np/n n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np> >T >Bnp s/(s\np) sto\np/np>Bsdcl\np/np>Bsdcl/npnp\np<np>sdcl\np<sdclFigure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [.
.
.
]particular, the features and unification constraintsin the categories related to semantically emptyfunction words such complementizers, infinitival-to, expletive subjects, and case-marking preposi-tions are adjusted to reflect their purely syntacticstatus.In the second step, a grammar is extracted fromthe converted CCGbank and augmented with log-ical forms.
Categories and unary type chang-ing rules (corresponding to zero morphemes) aresorted by frequency and extracted if they meet thespecified frequency thresholds.
A separate trans-formation then uses a few dozen generalized tem-plates to add logical forms to the categories, in afashion reminiscent of (Bos, 2005).
As shown inFigure 2, numbered semantic roles are taken fromPropBank when available, and more specific rela-tions are introduced in the categories for closed-class items such as determiners.After logical form insertion, the extracted andaugmented grammar is loaded and used to parsethe sentences in the CCGbank according to thegold-standard derivation.
If the derivation canbe successfully followed, the parse yields a log-ical form which is saved along with the corpussentence in order to later test the realizer.
Cur-rently, the algorithm succeeds in creating logicalforms for 98.85% of the sentences in the develop-ment section (Sect.
00) of the converted CCGbank,and 97.06% of the sentences in the test section(Sect.
23).
Of these, 95.99% of the developmentLFs are semantic dependency graphs with a sin-gle root, while 95.81% of the test LFs have a sin-gle root.
The remaining cases, with multiple roots,are missing one or more dependencies required toform a fully connected graph.
Such missing de-pendencies usually reflect remaining inadequaciesin the logical form templates.An error analysis of OpenCCG output by Ra-jkumar et al (2009) recently revealed that out of2331 named entities (NEs) annotated by the BBNcorpus (Weischedel and Brunstein, 2005), 238were not realized correctly.
For example, multi-word NPs like Texas Instruments Japan Ltd. wererealized as Japan Texas Instruments Ltd. Accord-ingly, inspired by Hogan et al?s (2007)?s Experi-ment 1, Rajkumar et al used the BBN corpus NEannotation to collapse certain classes of NEs.
Butunlike Hogan et al?s experiment where all the NEsannotated by the BBN corpus were collapsed, Ra-jkumar et al chose to collapse into single tokensonly NEs whose exact form can be reasonably ex-pected to be specified in the input to the realizer.For example, while some quantificational or com-paratives phrases like more than $ 10,000 are an-notated as MONEY in the BBN corpus, Rajkumaret al only collapse $ 10,000 into an atomic unit,with more than handled compositionally accord-ing to the semantics assigned to it by the gram-mar.
Thus, after transferring the BBN annotationsto the CCGbank corpus, Rajkumar et al (partially)collapsed NEs which are CCGbank constituentsaccording to the following rules: (1) completelycollapse the PERSON, ORGANIZATION, GPE,WORK OF ART major class type entitites; (2) ig-nore phrases like three decades later, which areannotated as DATE entities; and (3) collapse allphrases with POS tags CD or NNP(S) or lexicalitems % or $, ensuring that all prototypical namedentities are collapsed.It is worth noting that improvements in ourcorpus-based grammar engineering process?including a more precise treatment of punctuation,better named entity handling and the addition ofcatch-all logical form templates?have resulted ina 13.5 BLEU point improvement in our baselinerealization scores on Section 00 of the CCGbank,from a score of 0.6567 in (Espinosa et al, 2008)to 0.7917 in (Rajkumar et al, 2009), contribut-ing greatly to the state-of-the-art results reported412in Section 4.
A further 4.5 point improvement isobtained from the use of named entity classes inlanguage modeling and hypertagging (Rajkumaret al, 2009), as described next, and from our per-ceptron reranking model, described in Section 3.2.3 Factored Language ModelsAs in (White et al, 2007; Rajkumar et al, 2009),we use factored language models (Bilmes andKirchhoff, 2003) over words, part-of-speech tagsand supertags1to score partial and complete real-izations.
The trigram models were created usingthe SRILM toolkit (Stolcke, 2002) on the standardtraining sections (02?21) of the CCGbank, withsentence-initial words (other than proper names)uncapitalized.
While these models are consider-ably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005), the train-ing data does have the advantage of being in thesame domain and genre.
The models employ in-terpolated Kneser-Ney smoothing with the defaultfrequency cutoffs.
The best performing modelinterpolates three component models using rank-order centroid weights: (1) a word trigram model;(2) a word model with semantic classes replac-ing named entities; and (3) a trigram model thatchains a POS model with a supertag model, wherethe POS model (P ) conditions on the previous twoPOS tags, and the supertag model (S) conditionson the previous two POS tags as well as the currentone, as shown below:pPS(~Fi|~Fi?1i?2) = p(Pi| Pi?1i?2)p(Si| Pii?2) (1)Training data for the semantic class?replacedmodel was created by replacing (collapsed) wordswith their NE classes, in order to address data spar-sity issues caused by rare words in the same se-mantic class.
For example, the Section 00 sen-tence Pierre Vinken , 61 years old , will join theboard as a nonexecutive director Nov. 29 .
be-comes PERSON , DATE:AGE DATE:AGE old ,will join the ORG DESC:OTHER as a nonexecu-tive PER DESC DATE:DATE DATE:DATE .
Dur-ing realization, word forms are generated, but arethen replaced by their semantic classes for scoringusing the semantic class?replaced model, similarto Oh and Rudnicky (2002).Note that the use of supertags in the factoredlanguage model to score possible realizations is1With CCG, supertags (Bangalore and Joshi, 1999) arelexical categories considered as fine-grained syntactic labels.distinct from the prediction of supertags for lexicalcategory assignment: the former takes the wordsin the local context into account (as in supertag-ging for parsing), while the latter takes features ofthe logical form into account.
This latter processwe call hypertagging, to which we now turn.2.4 HypertaggingA crucial component of the OpenCCG realizer isthe hypertagger (Espinosa et al, 2008), or su-pertagger for surface realization, which uses amaximum entropy model to assign the most likelylexical categories to the predicates in the input log-ical form, thereby greatly constraining the real-izer?s search space.2Figure 2 shows gold-standardsupertags for the lexical predicates in the graph;such category labels are predicted by the hyper-tagger at run-time.
As in recent work on usingsupertagging in parsing, the hypertagger operatesin a multitagging paradigm (Curran et al, 2006),where a variable number of predictions are madeper input predicate.
Instead of basing category as-signment on linear word and POS context, how-ever, the hypertagger predicts lexical categoriesbased on contexts within a directed graph structurerepresenting the logical form (LF) of the sentenceto be realized.
The hypertagger generalizes Ban-galore and Rambow?s (2000) method of using su-pertags in generation by using maximum entropymodels with a larger local context.During realization, the hypertagger returns a ?-best list of supertags in order of decreasing prob-ability.
Increasing the number of categories re-turned clearly increases the likelihood that themost-correct supertag is among them, but at a cor-responding cost in chart size.
Accordingly, the hy-pertagger begins with a highly restrictive value for?, and backs off to progressively less-restrictivevalues if no complete realization can be found us-ing the set of supertags returned.
Clark and Curran(2007b) have shown this iterative relaxation strat-egy to be highly effective in CCG parsing.3 Perceptron RerankingAs Collins (2002) observes, perceptron traininginvolves a simple, on-line algorithm, with few it-erations typically required to achieve good perfor-mance.
Moreover, averaged perceptrons?which2The approach has been dubbed hypertagging since it op-erates at a level ?above?
the syntax, moving from semanticrepresentations to syntactic categories.413Input: training examples (xi, yi)Initialization: set ?
= 0, or use optional inputmodelAlgorithm:for t = 1 .
.
.
T , i = 1 .
.
.
Nzi= argmaxy?GEN(xi)?
(xi, y) ?
?if zi6= yi?
= ?
+ ?
(xi, yi) ?
?
(xi, zi)Output: ?
=?Tt=1?Ni=1?ti/TNFigure 3: Averaged perceptron training algorithmapproximate voted perceptrons, a maximum-margin method with attractive theoreticalproperties?seem to work remarkably well inpractice, while adding little further complexity.Additionally, since features only take on non-zero values when they appear in training itemsrequiring updates, perceptrons integrate featureselection with, and often produce quite smallmodels, especially when starting with a goodbaseline.The generic averaged perceptron training algo-rithm appears in Figure 3.
In our case, the algo-rithm trains a model for reranking the n-best real-izations generated using our existing factored lan-guage model for scoring, with the oracle-best re-alization considered the correct answer.
Accord-ingly, the input to the algorithm is a list of pairs(xi, yi), where xiis a logical form, GEN(xi) arethe n-best realizations for xi, and yiis the oracle-best member of GEN(xi).
The oracle-best realiza-tion is determined using a 4-gram precision metric(approximating BLEU) against the reference sen-tence.We have followed Huang (2008) in usingoracle-best targets for training, rather than goldstandard ones, in order to better approximate testconditions during training.
However, followingClark & Curran (2007a), during training we seedthe realizer with the gold-standard supertags, aug-menting the hypertagger?s ?-best list, in order toensure that the n-best realizations are generally ofhigh quality; consequently, the gold standard real-ization (i.e., the corpus sentence) usually appearsin the n-best list.3In addition, we use a hyper-tagger trained on all the training data, to improvehypertagger performance, while excluding the cur-3As in Clark & Curran?s approach, we use a single ?
valueduring training, rather than iteratively loosening the ?
value;the chosen ?
value determines the size of the discrimationspace.rent training section (in jack-knifed fashion) fromthe word-based parts of the language model, in or-der to make the language model scores more re-alistic.
It remains for future work to determinewhether using a different compromise between en-suring high-quality training data and remainingfaithful to the test conditions would yield betterresults.Since realization of the n-best lists for train-ing is the most time-consuming part of the pro-cess, in our current implementation we performthis step once, generating event files along the waycontaining feature vectors for each candidate real-ization.
The event files are used to calculate thefrequency distribution for the features, and mini-mum cutoffs are chosen to trim the feature alpha-bet to a reasonable size.
Training then takes placeby iterating over the event files, ignoring featuresthat do not appear in the alphabet.
As Figure 3indicates, training consists of calculating the top-ranked realization according to the current model?, and performing an update when the top-rankedrealization does not match the oracle-best realiza-tion.
Updates to the model add the feature vec-tor ?
(xi, yi) for the missed oracle-best realiza-tion, and subtract the feature vector ?
(xi, zi) forthe mistakenly top-ranked realization.
The finalmodel averages the models across the T iterationsover the training data, andN test cases within eachiteration.Note that while training the perceptron modelinvolves n-best reranking, realization with the re-sulting model can be viewed as forest rescoring,since scoring of all partial realizations is integratedinto the realizer?s beam search.
In future work, weintend to investigate saving the realizer?s packedcharts, rather than event files, and integrating theunpacking of the charts with the perceptron train-ing algorithm.The features we employ in our perceptron mod-els are of three kinds.
First, as in the log-linearmodels of Velldal & Oepen and Nakanishi et al,we incorporate the log probability of the candidaterealization?s word sequence according to our fac-tored language model as a single feature in the per-ceptron model.
Since our language model linearlyinterpolates three component models, we also in-clude the log prob from each component languagemodel as a feature, so that the combination ofthese components can be optimized.Second, we include syntactic features in our414Feature Type ExampleLexCat + Word s/s/np + beforeLexCat + POS s/s/np + INRule sdcl?
np sdcl\npRule + Word sdcl?
np sdcl\np + boughtRule + POS sdcl?
np sdcl\np + VBDWord-Word ?company, sdcl?
np sdcl\np, bought?Word-POS ?company, sdcl?
np sdcl\np, VBD?POS-Word ?NN, sdcl?
np sdcl\np, bought?Word + ?w?bought, sdcl?
np sdcl\np?
+ dwPOS + ?w?VBD, sdcl?
np sdcl\np?
+ dwWord + ?p?bought, sdcl?
np sdcl\np?
+ dpPOS + ?p?VBD, sdcl?
np sdcl\np?
+ dpWord + ?v?bought, sdcl?
np sdcl\np?
+ dvPOS + ?v?VBD, sdcl?
np sdcl\np?
+ dvTable 1: Basic and dependency features fromClark & Curran?s (2007b) normal form model;distances are in intervening words, punctuationmarks and verbs, and are capped at 3, 3 and 2,respectivelymodel by implementing Clark & Curran?s (2007b)normal form model in OpenCCG.4The features ofthis model are listed in Table 1; they are integer-valued, representing counts of occurrences in aderivation.
These syntactic features are quite com-parable to the dominance-oriented features in theunion of the Velldal & Oepen and Nakanishi etal.
models, except that our feature set does notinclude grandparenting, which has been found tohave limited utility in CCG parsing.
Our syntac-tic features also include ones that measure the dis-tance between headwords in terms of interveningwords, punctuation marks or verbs; these featuresgeneralize the ones in Nakanishi et al?s model.Note that in contrast to parsing, in realization dis-tance features are non-local, since different partialrealizations in the same equivalence class typicallydiffer in word order; as we are working in a rerank-ing paradigm though, the non-local nature of thesefeatures is unproblematic.Third, we include discriminative n-gram fea-tures in our model, following Roark et al?s (2004)approach to discriminative n-gram modeling forspeech recognition.
By discriminative n-gram fea-tures, we mean features counting the occurrencesof each n-gram that is scored by our factored lan-guage model, rather than a feature whose value isthe log prob determined by the language model.As Roark et al note, discriminative training withn-gram features has the potential to learn to nega-4We have omitted Clark & Curran?s root features, sincethe category we use for the full stop ensures that it must ap-pear at the root of any complete derivation.Model #Alph-feats #Feats Acc Timefull-model 2402173 576176 96.40% 08:53lp-ngram 1127437 342025 94.52% 05:19lp-syn 1274740 291728 85.03% 05:57Table 2: Perceptron Training Details?number offeatures in the alphabet, number of features in themodel, training accuracy and training time (hours)for 10 iterations on a single commodity servertively weight n-grams that appear in some of theGEN(xi) candidates, but which never appear inthe naturally occurring corpus used to train a stan-dard, generative language model.
Since our fac-tored language model considers words, semanticclasses, part-of-speech tags and supertags, our n-gram features represent a considerable generaliza-tion of the sequence-oriented features in Velldal& Oepen?s model, which never contain more thanone word and do not include semantic classes.4 Evaluation4.1 Experimental ConditionsFor the experiments reported below, we used alexico-grammar extracted from Sections 02?21 ofour enhanced CCGbank, a hypertagging model in-corporating named entity class features, and a tri-gram factored language model over words, namedentity classes, part-of-speech tags and supertags,as described in the preceding section.
BLEUscores were calculated after removing the under-scores between collapsed NEs.Events were generated for each training sectionseparately.
As already noted, the hypertagger andPOS/supertag language model was trained on allthe training sections, while separate word-basedmodels were trained excluding each of the train-ing sections in turn.
Event files for 26530 trainingsentences with complete realizations were gener-ated in 7 hours and 16 minutes on a cluster us-ing one commodity server per section, with an av-erage n-best list size of 18.2.
Perceptron modelswere trained on single machines; details for threeof the models appear in Table 2.
The complete setof models is listed in Table 3.4.2 ResultsRealization results on the development section aregiven in Table 4.
As the first block of rows af-ter the baseline shows, of the models incorporatinga single kind of feature, only the one with the n-gram log prob features beats the baseline BLEU415Model Descriptionbaseline-w3 No perceptron (3g wd only)baseline No perceptronsyn-only-nodist All syntactic features except distancengram-only Just ngram featuressyn-only Just syntactic featureslp-only Just log prob featureslp-ngram Log prob + Ngram featureslp-syn Log prob + Syntactic featuresfull-model Log prob + Ngram +Syntactic featuresTable 3: Legend for Experimental Conditionsscore, with the other models falling well belowthe baseline (though faring better than the trigram-word LM baseline).
This result confirms the im-portance of including n-gram log prob features indiscriminative realization ranking models, in linewith Velldal & Oepen?s findings, and contra thoseof Nakanishi et al, even though it was Nakanishiet al who experimented with the Penn Treebankcorpus, while Velldal &Oepen?s experiments wereon a much smaller, limited domain corpus.
Thesecond block of rows shows that both the discrim-inative n-gram features and the syntactic featuresprovide a substantial boost when used with the n-gram log prob features, with the syntactic featuresyielding a more than 3 BLEU point gain.
Thefinal row shows that the full model works best,though the boosts provided by the syntactic anddiscriminative n-gram features are clearly not in-dependent.
The BLEU point trends are mirrored inthe percentage of exact match realizations, whichgoes up by more than 10% from the baseline.
Thepercentage of complete (i.e., non-fragmentary) re-alizations, however, goes down; we expect thatthis is due to the time taken up by our currentnaive method of feature extraction, which does notcache the features calculated for partial realiza-tions.
Realization results on the standard test sec-tion appear in Table 5, confirming the gains madeby the full model over the baseline.5We calculated statistical significance for themain results on the development section usingbootstrap random sampling.6After re-sampling1000 times, significance was calculated using apaired t-test (999 d.f.).
The results indicated thatlp-only exceeded the baseline, lp-ngram and lp-5Note that the baseline for Section 23 uses 4-grams and afilter for balanced punctuation (White and Rajkumar, 2008),unlike the other reported configurations, which would explainthe somewhat smaller increase seen with this section.6Scripts for running these tests are available athttp://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htmModel %Exact %Compl.
BLEU Timebaseline-w3 26.00 83.15 0.7646 1.8baseline 29.00 83.28 0.7963 2.0syn-only-nodist 26.02 82.69 0.7754 3.2ngram-only 27.67 82.95 0.7777 3.0syn-only 28.34 82.74 0.7838 3.4lp-only 32.01 83.02 0.8009 2.1lp-ngram 36.31 80.47 0.8183 3.1lp-syn 39.47 79.74 0.8323 3.5full-model 40.11 79.63 0.8373 3.6Table 4: Section 00 Results (98.9% coverage)?percentage of exact match and grammaticallycomplete realizations, BLEU scores and averagetimes, in secondsModel %Exact %Complete BLEUbaseline 33.74 85.04 0.8173full-model 40.45 83.88 0.8506Table 5: Section 23 Results (97.06% coverage)syn exceeded lp-only, and the full model exceededlp-syn, with p < 0.0001 in each case.4.3 ExamplesTable 6 presents four examples where the fullmodel improves upon the baseline.
Example sen-tence wsj 0020.10 in Table 6 is a case where theperceptron successfully weights the componentngram models, as the lp-ngram model and thosethat build on it get it right.
Note that here, the mod-ifier ordering in small video-viewing is not speci-fied in the logical form and either ordering is pos-sible syntactically.
In wsj 0024.2, number agree-ment between the conjoined subject noun phraseand verb is obtained only with the full model.
Thissuggests that the full model is more robust to caseswhere the grammar is insufficiently precise (num-ber agreement is enforced by the grammar in onlythe simplest cases).
Example wsj 0034.9 correctsa VP ordering mismatch, where the corpus sen-tence is clearly preferred to the one where intooblivion is shifted to the end.
Finally, wsj 0047.13corrects an animacy mismatch on the wh-pronoun,in large part due to the high negative weight as-signed to the discriminative n-gram feature PER-SON , which.
Note that the full model still dif-fers from the original sentence in its placement ofthe adverb reportedly, choosing the arguably morenatural position following the auxiliary.4.4 Comparison to Other SystemsTable 7 lists our results in the context of those re-ported for other systems on PTB Section 23.
The416Ref-wsj 0020.10 that measure could compel Taipei ?s growing number of small video-viewing parlors to pay ...baseline,syn-only,ngram-only that measure could compel Taipei ?s growing number of video-viewing small parlors to ...lp-only, lp-ngram, full-model that measure could compel Taipei ?s growing number of small video-viewing parlors to ...Ref-wsj 0024.2 Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty.
operate the fields ...all except full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty.
operates the fields ...full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty.
operate the fields ...Ref-wsj 0034.9 they fell into oblivion after the 1929 crash .baseline, lp-ngram they fell after the 1929 crash into oblivion .lp-only, ngram-only, syn-only, full-model they fell into oblivion after the 1929 crash .Ref-wsj 0047.13 Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , reportedly has assured .
.
.baseline,baseline-w3, lp-syn, lp-only Antonio Novello , which Mr. Bush nominated to serve as surgeon general , has reportedly assured .
.
.full-model, lp-ngram, syn-only, ngram-syn Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , has reportedly assured .
.
.Table 6: Examples of realized outputSystem Coverage BLEU %ExactCallaway (05) 98.5% 0.9321 57.5OpenCCG (09) 97.1% 0.8506 40.5Ringger et al (04) 100.0% 0.836 35.7Langkilde-Geary (02) 83% 0.757 28.2Guo et al (08) 100.0% 0.7440 19.8Hogan et al (07) ?100.0% 0.6882OpenCCG (08) 96.0% 0.6701 16.0Nakanishi et al (05) 90.8% 0.7733Table 7: PTB Section 23 BLEU scores and exactmatch percentages in the NLG literature (Nakan-ishi et al?s results are for sentences of length 20 orless)most similar systems to ours are those of Nakan-ishi et al (2005) and Hogan et al (2007), as theyboth involve chart realizers for reversible gram-mars engineered from the Penn Treebank.
Whiledirect comparisons across systems cannot reallybe made when inputs vary in their semantic depthand specificity, we observe that our all-sentencesBLEU score of 0.8506 exceeds that of Hogan etal., who report a top score of 0.6882 (though withcoverage near 100%), and also surpasses Nakan-ishi et al?s score of 0.7733, despite their results be-ing limited to sentences of length 20 or less (with91% coverage).
Velldal & Oepen?s (2005) systemis also closely related, as noted in the introduc-tion, but as their experiments are on a limited do-main corpus, their results cannot be compared atall meaningfully.5 Related Work and DiscussionAs alluded to above, realization systems cannot beeasily compared, even on the same corpus, whentheir inputs are not the same.
This point is dra-matically illustrated in Langkilde-Geary?s (2002)system, where a BLEU score of 0.514 is reportedfor minimally specified inputs on PTB Section 23,while a score of 0.757 is reported for the ?Per-mute, no dir?
case (which perhaps most closelyresembles our inputs), and a score of 0.924 is re-ported for the most fully specified inputs; note,however, that in the latter case word order is deter-mined by sibling order in the inputs, an assump-tion not commonly made.
As another example,Guo et al (2008) report a competitive result of0.7440 (with 100% coverage) using a dependency-based approach; however, their inputs, like thoseof Hogan et al, include more surface syntactic in-formation than ours, as they specify case-markingprepositions, wh-pronouns and complementizers.In a recent experiment to assess the impact ofinput specificity, we found that including pred-icates for all prepositions in our logical formsboosted our baseline results by more than 3 BLEUpoints, with complete realizations found in morethan 90% of the test cases, indicating that generat-ing from a more surfacy input is indeed an easiertask than generating from a deeper representation.Given the current lack of consensus on realizer in-put specificity, we believe it is important to keepin mind that within-system comparisons (such asthose in the preceding section) are the ones thatshould be given the most credence.Returning to our cross-system comparison, it isperhaps surprising that Callaway (2005) reportsthe best PTB BLEU score to date, 0.9321, with98.5% coverage, using a purely symbolic, hand-crafted grammar augmented to handle the mostfrequent coverage issues for the PTB.
While Call-away?s inputs are unordered, word order is oftendetermined by positional features (e.g.
front) orby the type of modification (e.g.
describer vs.qualifier), and parts-of-speech are includedfor lexical items.
Additionally, in contrast to ourapproach, Callaway makes use of a generation-only grammar, rather than a reversible one, and hisapproach is less well-suited to producing n-best417outputs.
Nevertheless, his high scores do suggestthe potential for precise grammar engineering toimprove realization quality.While we have yet to perform a thorough er-ror analysis, our impression is that although thecurrent set of syntactic features substantially im-proves clausal constituent ordering, a variety ofdisfluent cases remain.
More thorough inves-tigations of features for constituent ordering inEnglish have been performed by Ringger et al(2004), Filippova and Strube (2009) and Zhongand Stent (2009), all of whom develop classifiersfor determining linear order.
In future work, weplan to investigate whether features inspired bythese approaches can be usefully integrated intoour perceptron reranker.Also related to the present work is discrimina-tive training in syntax-based MT (Turian et al,2007; Watanabe et al, 2007; Blunsom et al, 2008;Chiang et al, 2009).
Not surprisingly, since MT isa harder problem than surface realization, syntax-based MT systems have made use of less precisegrammars and more impoverished (target-side)feature sets than those tackling realization rank-ing.
With progress on discriminative training withlarge numbers of features in syntax-based MT, thefeatures found to be useful for high-quality sur-face realization may become increasingly relevantfor MT as well.6 ConclusionsIn this paper, we have shown how discriminativereranking with an averaged perceptron model canbe used to achieve substantial improvements in re-alization quality with CCG.
Using a comprehen-sive feature set, we have also confirmed the util-ity of including language model log probabilitiesas features in the model, which prior work ondiscriminative training with log linear models forHPSG realization had called into question.
Theperceptron model allows the combination of mul-tiple n-gram models to be optimized and then aug-mented with both syntactic features and discrim-inative n-gram features, inspired by related workin discriminative parsing and language modelingfor speech recognition.
The full model yields astate-of-the-art BLEU score of 0.8506 on Section23 of the CCGbank, to our knowledge the bestscore reported to date using a reversible, corpus-engineered grammar, despite our use of deeper,less specific inputs.
Finally, the perceptron modelpaves the way for exploring the utility of richerfeature spaces in statistical realization, includingthe use of linguistically-motivated and non-localfeatures, a topic which we plan to investigate infuture work.AcknowledgementsThis work was supported in part by NSF grant IIS-0812297 and by an allocation of computing timefrom the Ohio Supercomputer Center.
Our thanksalso to the OSU Clippers group and the anony-mous reviewers for helpful comments and discus-sion.ReferencesJason Baldridge and Geert-Jan Kruijff.
2002.
Cou-pling CCG and Hybrid Logic Dependency Seman-tics.
In Proc.
ACL-02.Jason Baldridge.
2002.
Lexically Specified Deriva-tional Control in Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh.Srinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: An Approach to Almost Parsing.
Com-putational Linguistics, 25(2):237?265.Srinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gener-ation.
In Proc.
COLING-00.Jeff Bilmes and Katrin Kirchhoff.
2003.
Factored lan-guage models and general parallelized backoff.
InProc.
HLT-03.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
ACL-08: HLT.Johan Bos.
2005.
Towards wide-coverage semanticinterpretation.
In Proc.
IWCS-6.Stephen Boxwell andMichaelWhite.
2008.
ProjectingPropbank roles onto the CCGbank.
In Proc.
LREC-08.Charles Callaway.
2005.
The types and distributions oferrors in a wide coverage surface realizer evaluation.In Proceedings of the 10th European Workshop onNatural Language Generation.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proc.
NAACL HLT 2009.Stephen Clark and James Curran.
2007a.
Perceptrontraining for a wide-coverage lexicalized-grammarparser.
In ACL 2007 Workshop on Deep LinguisticProcessing.418Stephen Clark and James R. Curran.
2007b.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Proc.ACL-04.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and ex-periments with perceptron algorithms.
In Proc.EMNLP-02.James R. Curran, Stephen Clark, and David Vadas.2006.
Multi-tagging for lexicalized-grammar pars-ing.
In Proc.
COLING/ACL-06.Dominic Espinosa, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proc.
ACL-08: HLT.Katja Filippova and Michael Strube.
2009.
Tree lin-earization in English: Improving language modelbased approaches.
In Proc.
NAACL HLT 2009 ShortPapers.Yuqing Guo, Josef van Genabith, and Haifeng Wang.2008.
Dependency-based n-gram models forgeneral purpose sentence realisation.
In Proc.COLING-08.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396.Deirdre Hogan, Conor Cafferkey, Aoife Cahill, andJosef van Genabith.
2007.
Exploiting multi-wordunits in history-based probabilistic generation.
InProc.
EMNLP-CoNLL.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
ACL-08:HLT.Irene Langkilde-Geary.
2002.
An empirical verifi-cation of coverage and correctness for a general-purpose sentence generator.
In Proc.
INLG-02.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic methods for disambiguation ofan HPSG-based chart generator.
In Proc.
IWPT-05.Alice H. Oh and Alexander I. Rudnicky.
2002.Stochastic natural language generation for spokendialog systems.
Computer, Speech & Language,16(3/4):387?407.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: A corpus annotated with se-mantic roles.
Computational Linguistics, 31(1).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL), Philadelphia, PA.Carl Pollard and Ivan Sag.
1994.
Head-Driven PhraseStructure Grammar.
University Of Chicago Press.Rajakrishnan Rajkumar, Michael White, and DominicEspinosa.
2009.
Exploiting named entity classes inCCG surface realization.
In Proc.
NAACL HLT 2009Short Papers.Eric Ringger, Michael Gamon, Robert C. Moore,David Rojas, Martine Smets, and Simon Corston-Oliver.
2004.
Linguistically informed statisticalmodels of constituent structure for ordering in sen-tence realization.
In Proc.
COLING-04.Brian Roark, Murat Saraclar, Michael Collins, andMark Johnson.
2004.
Discriminative languagemodeling with conditional random fields and theperceptron algorithm.
In Proc.
ACL-04.Mark Steedman.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.Andreas Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proc.
ICSLP-02.Joseph Turian, Benjamin Wellington, and I. DanMelamed.
2007.
Scalable discriminative learn-ing for natural language parsing and translation.
InProc.
NIPS 19.Erik Velldal and Stephan Oepen.
2005.
Maximum en-tropy models for realization ranking.
In Proc.
MTSummit X.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin trainingfor statistical machine translation.
In Proc.
EMNLP-CoNLL-07.RalphWeischedel and Ada Brunstein.
2005.
BBN pro-noun coreference and entity type corpus.
Technicalreport, BBN.Michael White and Rajakrishnan Rajkumar.
2008.A more precise analysis of punctuation for broad-coverage surface realization with CCG.
In Proc.of the Workshop on Grammar Engineering AcrossFrameworks (GEAF08).Michael White, Rajakrishnan Rajkumar, and ScottMartin.
2007.
Towards broad coverage surface re-alization with CCG.
In Proc.
of the Workshop onUsing Corpora for NLG: Language Generation andMachine Translation (UCNLG+MT).Michael White.
2006.
Efficient Realization of Coor-dinate Structures in Combinatory Categorial Gram-mar.
Research on Language and Computation,4(1):39?75.Huayan Zhong and Amanda Stent.
2009.
Determiningthe position of adverbial phrases in English.
In Proc.NAACL HLT 2009 Short Papers.419
