Proceedings of the NAACL HLT 2013 Demonstration Session, pages 20?23,Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational LinguisticsA Web Application for the Diagnostic Evaluation of Machine Translationover Specific Linguistic PhenomenaAntonio Toral Sudip Kumar Naskar Joris Vreeke Federico Gaspari Declan GrovesSchool of ComputingDublin City UniversityIreland{atoral, snaskar, fgaspari, dgroves}@computing.dcu.ie joris.vreeke@dcu.ieAbstractThis paper presents a web application and aweb service for the diagnostic evaluation ofMachine Translation (MT).
These web-basedtools are built on top of DELiC4MT, an open-source software package that assesses the per-formance of MT systems over user-definedlinguistic phenomena (lexical, morphological,syntactic and semantic).
The advantage of theweb-based scenario is clear; compared to thestandalone tool, the user does not need to carryout any installation, configuration or mainte-nance of the tool.1 Automatic Evaluation of MachineTranslation beyond Overall ScoresMachine translation (MT) output can be evaluatedusing different approaches, which can essentially bedivided into human and automatic, both of which,however, present a number of shortcomings.
Hu-man evaluation tends to be more reliable in a num-ber of ways and can be tailored to a variety of situ-ations, but is rather expensive (both in terms of re-sources and time) and is difficult to replicate.
Onthe other hand, standard automatic MT evaluationmetrics such as BLEU (Papineni et al 2002) andMETEOR (Banerjee and Lavie, 2005) are consid-erably cheaper and provide faster results, but returnrather crude scores that are difficult to interpret forMT users and developers alike.
Crucially, currentstandard automatic MT evaluation metrics also lackany diagnostic value, i.e.
they cannot identify spe-cific weaknesses in the MT output.
Diagnostic in-formation can be extremely valuable for MT devel-opers and users, e.g.
to improve the performance ofthe system or to decide which output is more suitedfor particular scenarios.An interesting alternative to the traditional MTevaluation metrics is to evaluate the performanceof MT systems over specific linguistic phenomena.While retaining the main advantage of automaticmetrics (low cost), this approach provides more fine-grained linguistically-motivated evaluation.
The lin-guistic phenomena, also referred to as linguisticcheckpoints, can be defined in terms of linguistic in-formation at different levels (lexical, morphological,syntactic, semantic, etc.)
that appear in the sourcelanguage.
Examples of such linguistic checkpoints,what translation information they can represent, andtheir relevance for MT are provided in Table 1.Checkpoint Relevance for MTLexical Words that can have multiple translations inthe target.
For example, the preposition ?de?in Spanish can be translated into English as?of?
or ?from?
depending on the context.Syntactic Syntactic constructs that are difficult to trans-late.
E.g., a checkpoint containing the se-quence a noun (noun1) followed by thepreposition ?de?, followed by another noun(noun2) when translating from Spanish toEnglish.
The equivalent English constructwould be noun2?s noun1, the translation thusinvolving some reordering.Semantic Words with multiple meanings, which possi-bly correspond to different translations in thetarget language.
Polysemous words can becollected from electronic dictionaries such asWordNet (Miller, 1995).Table 1: Linguistic CheckpointsCheckpoints can also be built by combining el-20ements from different categories.
For example, bycombining lexical and syntantic elements, we coulddefine a checkpoint for prepositional phrases (syn-tactic element) which start with the preposition ?de?
(lexical element).Woodpecker (Zhou et al 2008) is a tool that per-forms diagnostic evaluation of MT systems over lin-guistic checkpoints for English?Chinese.
Probablydue to its limitation to one language pair, its pro-prietary nature as well as rather restrictive licensingconditions, Woodpecker does not seem to have beenwidely used in the community, in spite of its abilityto support diagnostic evaluation.DELiC4MT1 is an open-source software that fol-lows the same approach as Woodpecker.
However,DELiC4MT is easily portable to any language pair2and provides additional functionality such as filter-ing of noisy checkpoint instances and support forstatistical significance tests.
This paper focuses onthe usage of this tool through a web application anda web service from the user?s perspective.
Detailsregarding its implementation, evaluation, etc.
canbe found in (Toral et al 2012; Naskar et al 2011).2 Web Services for Language TechnologyToolsThere exist many freely available language pro-cessing tools, some of which are distributed underopen-source licenses.
In order to use these tools,they need to be downloaded, installed, configuredand maintained, which results in high cost both interms of manual effort and computing resources.The requirement for in-depth technical knowledgeseverely limits the usability of these tools amongstnon-technical users, particularly in our case amongsttranslators and post-editors.Web services introduce a new paradigm in theway we use software tools where only providersof the tools are required to have knowledge re-garding their installation, configuration and mainte-nance.
This enables wider adoption of the tools andreduces the learning curve for users as the only infor-mation needed is basic knowledge of the functional-1http://www.computing.dcu.ie/?atoral/delic4mt/2It has already been tested on language pairs involvingthe following languages: Arabic, Bulgarian, Dutch, English,French, German, Hindi, Italian, Turkish and Welsh.ity and input/output parameters (which can be easilyincluded, e.g.
as part of an online tutorial).
Whilethis paradigm is rather new in the field of compu-tational linguistics, it is quite mature and successfulin other fields such as bioinformatics (Oinn et al2004; Labarga et al 2007).Related work includes two web applications in thearea of MT evaluation.
iBLEU (Madnani, 2011) or-ganises BLEU scoring information in a visual man-ner.
Berka et al(2012) perform automatic error de-tection and classification of MT output.Figure 1: Web interface for the web service.3 DemoThe demo presented in this paper consists of aweb service and a web application built on top ofDELiC4MT that allow to assess the performance ofMT systems on different linguistic phenomena de-21Figure 2: Screenshot of the web application (visualisation of results).fined by the user.
The following subsections detailboth parts of the demo.3.1 Web ServiceA SOAP-compliant web service3 has been built ontop of DELiC4MT.
It receives the following inputparameters (see Figure 1):1.
Word alignment between the source and targetsides of the testset, in the GIZA++ (Och andNey, 2003) output format.2.
Linguistic checkpoint defined as a Ky-bot4 (Vossen et al 2010) profile.3.
Output of the MT system to be evaluated, inplain text, tokenised and one sentence per line.4.
Source and target sides of the testset (orgold standard), in KAF format (Bosma et al2009).5The tool then evaluates the performance of theMT system (input parameter 3) on the linguistic phe-nomenon (parameter 2) by following this procedure:3http://registry.elda.org/services/3014Kybot profiles can be understood as regular expressionsover KAF documents, http://kyoto.let.vu.nl/svn/kyoto/trunk/modules/mining_module/5An XML format for text analysis based on representationstandards from ISO TC37/SC4.?
Occurrences of the linguistic phenomenon (pa-rameter 2) are identified in the source side ofthe testset (parameter 4).?
The equivalent tokens of these occurrences inthe target side (parameter 5) are found by usingword alignment information (parameter 1).?
For each checkpoint instance, the tool checkshow many of the n-grams present in the refer-ence of the checkpoint instance are containedin the output produced by the MT system (pa-rameter 3).3.2 Web ApplicationThe web application builds a graphical interface ontop of the web service.
It allows the user to visualisethe results in a fine-grained manner, the user can seethe performance of the MT system for each singleoccurrence of the linguistic phenomenon.Sample MT output for the ?noun?
checkpoint forthe English to French language direction is shownin Figure 2.
Two occurrences of the checkpoint areshown.
The first one regards the source noun ?mr.
?and its translation in the reference ?monsieur?, iden-tified through word alignments.
The alignment (4-4) indicates that both the source and target tokensappear at the fifth position (0-based index) in thesentence.
The reference token (?monsieur?)
is notfound in the MT output and thus a score of 0/122(0 n-gram matches out of a total of 1 possible n-gram) is assigned to the MT system for this noun in-stance.
Conversely, the score for the second occur-rence (?speaker?)
is 1/1 since the MT output con-tains the 1-gram of the reference translation (?ora-teur?
).The recall-based overall score is shown at the bot-tom of the figure (0.5025).
This is calculated bysumming up the scores (matching n-grams) for allthe occurrences (803) and dividing the result by thetotal number of possible n-grams (1598).4 ConclusionsIn this paper we have presented a web applica-tion and a web service for the diagnostic evalua-tion of MT output over linguistic phenomena usingDELiC4MT.
The tool allows users and developersof MT systems to easily receive fine-grained feed-back on the performance of their MT systems overlinguistic checkpoints of their interest.
The applica-tion is open-source, freely available and adaptable toany language pair.AcknowledgmentsThe research leading to these results has re-ceived funding from the European Union Sev-enth Framework Programme FP7/2007-2013 undergrant agreements FP7-ICT-4-248531 and PIAP-GA-2012-324414 and through Science Foundation Ire-land as part of the CNGL (grant 07/CE/I1142)ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Intrin-sic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, Proceedings of theACL-05 Workshop, pages 65?72, University of Michi-gan, Ann Arbor, Michigan, USA.Jan Berka, Ondej Bojar, Mark Fishel, Maja Popovi, andDaniel Zeman.
2012.
Automatic MT Error Anal-ysis: Hjerson Helping Addicter.
In Proceedings ofthe Eight International Conference on Language Re-sources and Evaluation (LREC?12), Istanbul, Turkey.European Language Resources Association (ELRA).W.
E. Bosma, Piek Vossen, Aitor Soroa, German Rigau,Maurizio Tesconi, Andrea Marchetti, Monica Mona-chini, and Carlo Aliprandi.
2009.
KAF: a genericsemantic annotation format.
In Proceedings of theGL2009 Workshop on Semantic Annotation, Septem-ber.Alberto Labarga, Franck Valentin, Mikael Andersson,and Rodrigo Lopez.
2007.
Web services at the euro-pean bioinformatics institute.
Nucleic Acids Research,35(Web-Server-Issue):6?11.Nitin Madnani.
2011. iBLEU: Interactively Debuggingand Scoring Statistical Machine Translation Systems.In Proceedings of the 2011 IEEE Fifth InternationalConference on Semantic Computing, ICSC ?11, pages213?214, Washington, DC, USA.
IEEE Computer So-ciety.George A. Miller.
1995.
WordNet: a lexical database forEnglish.
Commun.
ACM, 38(11):39?41, November.Sudip Kumar Naskar, Antonio Toral, Federico Gaspari,and Andy Way.
2011.
A Framework for DiagnosticEvaluation of MT based on Linguistic Checkpoints.
InProceedings of the 13th Machine Translation Summit,pages 529?536, Xiamen, China, September.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29:19?51, March.Tom Oinn, Matthew Addis, Justin Ferris, Darren Marvin,Martin Senger, Mark Greenwood, Tim Carver, KevinGlover, Matthew R. Pocock, Anil Wipat, and Peter Li.2004.
Taverna: a tool for the composition and en-actment of bioinformatics workflows.
Bioinformatics,20(17):3045?3054, November.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Antonio Toral, Sudip Kumar Naskar, Federico Gaspari,and Declan Groves.
2012.
DELiC4MT: A Tool forDiagnostic MT Evaluation over User-defined Linguis-tic Phenomena.
The Prague Bulletin of MathematicalLinguistics, pages 121?132.Piek Vossen, German Rigau, Eneko Agirre, Aitor Soroa,Monica Monachini, and Roberto Bartolini.
2010.
KY-OTO: an open platform for mining facts.
In Proceed-ings of the 6th Workshop on Ontologies and LexicalResources, pages 1?10, Beijing, China.Ming Zhou, Bo Wang, Shujie Liu, Mu Li, DongdongZhang, and Tiejun Zhao.
2008.
Diagnostic evalu-ation of machine translation systems using automati-cally constructed linguistic check-points.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics - Volume 1, COLING ?08, pages1121?1128, Stroudsburg, PA, USA.
Association forComputational Linguistics.23
