Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 333?338,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Bayesian Method to Incorporate Background Knowledge duringAutomatic Text SummarizationAnnie LouisILCC, School of Informatics,University of Edinburgh,Edinburgh EH8 9AB, UKalouis@inf.ed.ac.ukAbstractIn order to summarize a document, it isoften useful to have a background setof documents from the domain to serveas a reference for determining new andimportant information in the input doc-ument.
We present a model based onBayesian surprise which provides an in-tuitive way to identify surprising informa-tion from a summarization input with re-spect to a background corpus.
Specifically,the method quantifies the degree to whichpieces of information in the input changeone?s beliefs?
about the world representedin the background.
We develop sys-tems for generic and update summariza-tion based on this idea.
Our method pro-vides competitive content selection perfor-mance with particular advantages in theupdate task where systems are given asmall and topical background corpus.1 IntroductionImportant facts in a new text are those which devi-ate from previous knowledge on the topic.
Whenpeople create summaries, they use their knowl-edge about the world to decide what content in aninput document is informative to include in a sum-mary.
Understandably in automatic summariza-tion as well, it is useful to keep a background setof documents to represent general facts and theirfrequency in the domain.For example, in the simplest setting of multi-document summarization of news, systems areasked to summarize an input set of topically-related news documents to reflect its central con-tent.
In this GENERIC task, some of the best re-ported results were obtained by a system (Conroyet al, 2006) which computed importance scoresfor words in the input by examining if the wordoccurs with significantly higher probability in theinput compared to a large background collectionof news articles.
Other specialized summarizationtasks explicitly require the use of background in-formation.
In the UPDATE summarization task, asystem is given two sets of news documents on thesame topic; the second contains articles publishedlater in time.
The system should summarize theimportant updates from the second set assuming auser has already read the first set of articles.In this work, we present a Bayesian model forassessing the novelty of a sentence taken from asummarization input with respect to a backgroundcorpus of documents.Our model is based on the idea of Bayesian Sur-prise (Itti and Baldi, 2006).
For illustration, as-sume that a user?s background knowledge com-prises of multiple hypotheses about the currentstate of the world and a probability distributionover these hypotheses indicates his degree of be-lief in each hypothesis.
For example, one hypoth-esis may be that the political situation in Ukraineis peaceful, another where it is not.
Apriori as-sume the user favors the hypothesis about a peace-ful Ukraine, i.e.
the hypothesis has higher prob-ability in the prior distribution.
Given new data,the evidence can be incorporated using Bayes Ruleto compute the posterior distribution over the hy-potheses.
For example, upon viewing news reportsabout riots in the country, a user would update hisbeliefs and the posterior distribution of the user?sknowledge would have a higher probability for ariotous Ukraine.
Bayesian surprise is the differ-ence between the prior and posterior distributionsover the hypotheses which quantifies the extent towhich the new data (the news report) has changeda user?s prior beliefs about the world.In this work, we exemplify how Bayesian sur-prise can be used to do content selection for textsummarization.
Here a user?s prior knowledgeis approximated by a background corpus and we333show how to identify sentences from the inputset which are most surprising with respect to thisbackground.
We use the method to do two typesof summarization tasks: a) GENERIC news sum-marization which uses a large random collectionof news articles as the background, and b) UP-DATE summarization where the background is asmaller but specific set of news documents onthe same topic as the input set.
We find thatour method performs competitively with a previ-ous log-likelihood ratio approach which identifieswords with significantly higher probability in theinput compared to the background.
The Bayesianapproach is more advantageous in the update task,where the background corpus is smaller in size.2 Related workComputing new information is useful in many ap-plications.
The TREC novelty tasks (Allan et al,2003; Soboroff and Harman, 2005; Schiffman,2005) tested the ability of systems to find novelinformation in an IR setting.
Systems were givena list of documents ranked according to relevanceto a query.
The goal is to find sentences in eachdocument which are relevant to the query, and atthe same time is new information given the contentof documents higher in the relevance list.For update summarization of news, methodsrange from textual entailment techniques (Ben-tivogli et al, 2010) to find facts in the input whichare not entailed by the background, to Bayesiantopic models (Delort and Alfonseca, 2012) whichaim to learn and use topics discussed only in back-ground, those only in the update input and thosethat overlap across the two sets.Even for generic summarization, some of thebest results were obtained by Conroy et al (2006)by using a large random corpus of news articlesas the background while summarizing a new arti-cle, an idea first proposed by Lin and Hovy (2000).Central to this approach is the use of a likelihoodratio test to compute topic words, words that havesignificantly higher probability in the input com-pared to the background corpus, and are hencedescriptive of the input?s topic.
In this work,we compare our system to topic word based onessince the latter is also a general method to find sur-prising new words in a set of input documents butis not a bayesian approach.
We briefly explain thetopic words based approach below.Computing topic words: Let us call the inputset I and the background B.
The log-likelihoodratio test compares two hypotheses:H1: A word t is not a topic word and occurswith equal probability in I and B, i.e.
p(t|I) =p(t|B) = pH2: t is a topic word, hence p(t|I) = p1andp(t|B) = p2and p1> p2A set of documents D containing N tokens isviewed as a sequence of words w1w2...wN.
Theword in each position i is assumed to be generatedby a Bernoulli trial which succeeds when the gen-erated word wi= t and fails when wiis not t.Suppose that the probability of success is p. Thenthe probability of a word t appearing k times in adataset of N tokens is the binomial probability:b(k,N, p) =(Nk)pk(1?
p)N?k(1)The likelihood ratio compares the likelihood ofthe data D = {B, I} under the two hypotheses.?
=P (D|H1)P (D|H2)=b(ct, N, p)b(cI, NI, p1) b(cB, NB, p2)(2)p, p1and p2are estimated by maximum likeli-hood.
p = ct/N where ctis the number of timesword t appears in the total set of tokens compris-ing {B, I}.
p1= cIt/NIand p2= cBt/NBare theprobabilities of t estimated only from the input andonly from the background respectively.A convenient aspect of this approach is that?2 log ?
is asymptotically ?2distributed.
So for aresulting?2 log ?
value, we can use the ?2table tofind the significance level with which the null hy-pothesis H1can be rejected.
For example, a valueof 10 corresponds to a significance level of 0.001and is standardly used as the cutoff.
Words with?2 log ?
> 10 are considered topic words.
Con-roy et al (2006)?s system gives a weight of 1 to thetopic words and scores sentences using the numberof topic words normalized by sentence length.3 Bayesian SurpriseFirst we present the formal definition of Bayesiansurprise given by Itti and Baldi (2006) without ref-erence to the summarization task.Let H be the space of all hypotheses represent-ing the background knowledge of a user.
The userhas a probability P (H) associated with each hy-pothesis H ?
H. Let D be a new observation.
Theposterior probability of a single hypothesis H canbe computed as:P (H|D) =P (D|H)P (H)P (D)(3)334The surprise S(D,H) created by D on hypoth-esis space H is defined as the difference betweenthe prior and posterior distributions over the hy-potheses, and is computed using KL divergence.S(D,H) = KL(P (H|D), P (H))) (4)=?HP (H|D) logP (H|D)P (H)(5)Note that since KL-divergence is not symmet-ric, we could also compute KL(P (H), P (H|D))as the surprise value.
In some cases, surprise canbe computed analytically, in particular when theprior distribution is conjugate to the form of thehypothesis, and so the posterior has the same func-tional form as the prior.
(See Baldi and Itti (2010)for the surprise computation for different familiesof probability distributions).4 Summarization with Bayesian SurpriseWe consider the hypothesis space H as the set ofall the hypotheses encoding background knowl-edge.
A single hypothesis about the backgroundtakes the form of a multinomial distribution overword unigrams.
For example, one multinomialmay have higher word probabilities for ?Ukraine?and ?peaceful?
and another multinomial has higherprobabilities for ?Ukraine?
and ?riots?.
P (H) givesa prior probability to each hypothesis based onthe information in the background corpus.
In ourcase, P (H) is a Dirichlet distribution, the conju-gate prior for multinomials.
Suppose that the vo-cabulary size of the background corpus is V andwe label the word types as (w1, w2, ... wV).
Then,P (H) = Dir(?1, ?2, ...?V) (6)where ?1:Vare the concentration parameters ofthe Dirichlet distribution (and will be set using thebackground corpus as explained in Section 4.2).Now consider a new observation I (a text, sen-tence, or paragraph from the summarization input)and the word counts in I given by (c1, c2, ..., cV).Then the posterior over H is the dirichlet:P (H|I) = Dir(?1+ c1, ?2+ c2, ..., ?V+ cV)(7)The surprise due to observing I , S(I,H) is theKL divergence between the two dirichlet distribu-tions.
(Details about computing KL divergencebetween two dirichlet distributions can be foundin Penny (2001) and Baldi and Itti (2010)).Below we propose a general algorithm for sum-marization using surprise computation.
Then wedefine the prior distribution P (H) for each of ourtwo tasks, GENERIC and UPDATE summarization.4.1 Extractive summarization algorithmWe first compute a surprise value for each wordtype in the summarization input.
Word scores areaggregated to obtain a score for each sentence.Step 1: Word score.
Suppose that word typewiappears citimes in the summarization inputI .
We obtain the posterior distribution after see-ing all instances of this word (wi) as P (H|wi) =Dir(?1, ?2, ...?i+ ci, ...?V).
The score for wiisthe surprise computed as KL divergence betweenP (H|wi) and the prior P (H) (eqn.
6).Step 2: Sentence score.
The compositionfunctions to obtain sentence scores from wordscores can impact content selection performance(Nenkova et al, 2006).
We experiment with sumand average value of the word scores.1Step 3: Sentence selection.
The goal is to se-lect a subset of sentences with high surprise val-ues.
We follow a greedy approach to optimize thesummary surprise by choosing the most surprisingsentence, the next most surprising and so on.
Atthe same time, we aim to avoid redundancy, i.e.selecting sentences with similar content.
After asentence is selected for the summary, the surprisefor words from this sentence are set to zero.
We re-compute the surprise for the remaining sentencesusing step 2 and the selection process continuesuntil the summary length limit is reached.The key differences between our Bayesian ap-proach and a method such as topic words are: (i)The Bayesian approach keeps multiple hypothe-ses about the background rather than a single one.Surprise is computed based on the changes inprobabilities of all of these hypotheses upon see-ing the summarization input.
(ii) The computationof topic words is local, it assumes a binomial dis-tribution and the occurrence of a word is indepen-dent of others.
In contrast, word surprise althoughcomputed for each word type separately, quantifiesthe surprise when incorporating the new counts ofthis word into the background multinomials.4.2 Input and backgroundHere we describe the input sets and backgroundcorpus used for the two summarization tasks and1An alternative algorithm could directly compute the sur-prise of a sentence by incorporating the words from the sen-tence into the posterior.
However, we found this specificmethod to not work well probably because the few and un-repeated content words from a sentence did not change theposterior much.
In future, we plan to use latent topic modelsto assign a topic to a sentence so that the counts of all thesentence?s words can be aggregated into one dimension.335define the prior distribution for each.
We use datafrom the DUC2and TAC3summarization evalua-tion workshops conducted by NIST.Generic summarization.
We use multidocumentinputs from DUC 2004.
There were 50 inputs,each contains around 10 documents on a commontopic.
Each input is also provided with 4 manuallywritten summaries created by NIST assessors.
Weuse these manual summaries for evaluation.The background corpus is a collection of 5000randomly selected articles from the English Giga-word corpus.
We use a list of 571 stop words fromthe SMART IR system (Buckley, 1985) and the re-maining content word vocabulary has 59,497 wordtypes.
The count of each word in the backgroundis calculated and used as the ?
parameters of theprior Dirichlet distribution P (H) (eqn.
6).Update summarization.
This task uses data fromTAC 2009.
An input has two sets of documents, Aand B, each containing 10 documents.
Both A andB are on same topic but documents in B were pub-lished at a later time than A (background).
Therewere 44 inputs and 4 manual update summariesare provided for each.The prior parameters are the counts of wordsin A for that input (using the same stoplist).
Thevocabulary of these A sets is smaller, ranging from400 to 3000 words for the different inputs.In practice for both tasks, a new summarizationinput can have words unseen in the background.So new words in an input are added to the back-ground corpus with a count of 1 and the counts ofexisting words in the background are incrementedby 1 before computing the prior parameters.
Thesummary length limit is 100 words in both tasks.5 Systems for comparisonWe compare against three types of systems, (i)those which similarly to surprise, use a back-ground corpus to identify important sentences, (ii)a system that uses information from the input setonly and no background, and (iii) systems thatcombine scores from the input and background.KLback: represents a simple baseline for sur-prise computation from a background corpus.
Asingle unigram probability distribution B is cre-ated from the background using maximum like-lihood.
The summary is created by greedilyadding sentences which maximize KL divergence2http://www-nlpir.nist.gov/projects/duc/index.html3http://www.nist.gov/tac/between B and the current summary.
Supposethe set of sentences currently chosen in the sum-mary is S. The next step chooses the sentencesl= argmaxsiKL({S ?
si}||B) .TSsum, TSavg: use topic words computed as de-scribed in Section 2 and utilizing the same back-ground corpus for the generic and update tasksas the surprise-based methods.
For the generictask, we use a critical value of 10 (0.001 signif-icance level) for the ?2distribution during topicword computation.
In the update task however, thebackground corpus A is smaller and for most in-puts, no words exceeded this cutoff.
We lower thesignificance level to the generally accepted valueof 0.05 and take words scoring above this as topicwords.
The number of topic words is still small(ranging from 1 to 30) for different inputs.The TSsumsystem selects sentences with greatercounts of topic words and TSavgcomputes thenumber of topic words normalized by sentencelength.
A greedy selection procedure is used.
Toreduce redundancy, once a sentence is added, thetopic words contained in it are removed from thetopic word list before the next sentence selection.KLinp: represents the system that does not usebackground information.
Rather the method cre-ates a summary by optimizing for high similarityof the summary with the input word distribution.Suppose the input unigram distribution is I andthe current summary is S, the method chooses thesentence sl= argminsiKL({S ?
si}||I) at eachiteration.
Since {S ?
si} is used to compute diver-gence, redundancy is implicitly controlled in thisapproach.
Such a KL objective was used in com-petitive systems in the past (Daum?e III and Marcu,2006; Haghighi and Vanderwende, 2009).Input + background: These systems com-bine (i) a score based on the background (KLback,TS or SR) with (ii) the score based on the inputonly (KLinp).
For example, to combine TSsumandKLinp: for each sentence, we compute its scoresbased on the two methods.
Then we normalize thetwo sets of scores for candidate sentences using z-scores and compute the best sentence as argmaxsi(TSsum(si) - KLinp(si)).
Redundancy control isdone similarly to the TS only systems.6 Content selection resultsFor evaluation, we compare each summary to thefour manual summaries using ROUGE (Lin andHovy, 2003; Lin, 2004).
All summaries were trun-cated to 100 words, stemming was performed and336ROUGE-1 ROUGE-2KLback0.2276 (TS, SR) 0.0250 (TS, SR)TSsum0.3078 0.0616TSavg0.2841 (TSsum, SRsum) 0.0493 (TSsum)SRsum0.3120 0.0580SRavg0.3003 0.0549KLinp0.3075 (KLinp+TSavg) 0.0684KLinp+TSsum0.3250 0.0725KLinp+TSavg0.3410 0.0795KLinp+SRsum0.3187 (KLinp+TSavg) 0.0660 (KLinp+TSavg)KLinp+SRavg0.3220 (KLinp+TSavg) 0.0696Table 1: Evaluation results for generic summaries.Systems in parentheses are significantly better.stop words were not removed, as is standard inTAC evaluations.
We report the ROUGE-1 andROUGE-2 recall scores (average over the inputs)for each system.
We use the Wilcoxon signed-ranktest to check for significant differences in meanscores.
Table 1 shows the scores for generic sum-maries and 2 for the update task.
For each system,the peer systems with significantly better scores(p-value < 0.05) are indicated within parentheses.We refer to the surprise-based summaries asSRsumand SRavgdepending on the type of com-position function (Section 4.1).First, consider GENERIC summarization and thesystems which use the background corpus only(those above the horizontal line).
The KLbackbaseline performs significantly worse than topicwords and surprise summaries.
Numerically,SRsumhas the highest ROUGE-1 score and TSsumtops according to ROUGE-2.
As per the Wilcoxontest, TSsum, SRsumand SRavgscores are statisti-cally indistinguishable at 95% confidence level.Systems below the horizontal line in Table 1use an objective which combines both similaritywith the input and difference from the background.The first line here shows that a system optimiz-ing only for input similarity, KLinp, by itself hashigher scores (though not significant) than thoseusing background information only.
This result isnot surprising for generic summarization where allthe topical content is present in the input and thebackground is a non-focused random collection.At the same time, adding either TS or SR scoresto KLinpalmost always leads to better results withKLinp+ TSavggiving the best score.In UPDATE summarization, the surprise-basedmethods have an advantage over the topic wordones.
SRavgis significantly better than TSavgfor both ROUGE-1 and ROUGE-2 scores andbetter than TSsumaccording to ROUGE-1.
Infact, the surprise methods have numerically higherROUGE-1 ROUGE-2KLback0.2246 (TS, SR) 0.0213 (TS, SR)TSsum0.3037 (SRavg) 0.0563TSavg0.2909 (SRsum, SRavg) 0.0477 (SRsum, SRavg)SRsum0.3201 0.0640SRavg0.3226 0.0639KLinp0.3098 (KLinp+SRavg) 0.0710KLinp+TSsum0.3010 (KLinp+SRsum, avg) 0.0635KLinp+TSavg0.3021 (KLinp+SRsum, avg) 0.0543 (KLinp,KLinp+SRsum, avg)KLinp+SRsum0.3292 0.0721KLinp+SRavg0.3379 0.0767Table 2: Evaluation results for update summaries.Systems in parentheses are significantly better.ROUGE-1 scores compared to input similarity(KLinp) in contrast to generic summarization.When combined with KLinp, the surprise meth-ods provide improved results, significantly betterin terms of ROUGE-1 scores.
The TS methods donot lead to any improvement, and KLinp+ TSavgis significantly worse than KLinponly.
The limi-tation of the TS approach arises from the paucityof topic words that exceed the significance cutoffapplied on the log-likelihood ratio.
But Bayesiansurprise is robust on the small background corpusand does not need any tuning for cutoff values de-pending on the size of the background set.Note that these models do not perform on parwith summarization systems that use multiple in-dicators of content importance, involve supervisedtraining and which perform sentence compression.Rather our goal in this work is to demonstrate asimple and intuitive unsupervised model.7 ConclusionWe have introduced a Bayesian summarizationmethod that strongly aligns with intuitions abouthow people use existing knowledge to identify im-portant events or content in new observations.Our method is especially valuable when a sys-tem must utilize a small background corpus.While the update task datasets we have used werecarefully selected and grouped by NIST assesorsinto initial and background sets, for systems onthe web, there is little control over the number ofbackground documents on a particular topic.
Asystem should be able to use smaller amounts ofbackground information and as new data arrives,be able to incorporate the evidence.
Our Bayesianapproach is a natural fit in such a setting.AcknowledgementsThe author was supported by a Newton Interna-tional Fellowship (NF120479) from the Royal So-ciety and the British Academy.337ReferencesJ.
Allan, C. Wade, and A. Bolivar.
2003.
Retrieval andnovelty detection at the sentence level.
In Proceed-ings of SIGIR, pages 314?321.P.
Baldi and L. Itti.
2010.
Of bits and wows: abayesian theory of surprise with applications to at-tention.
Neural Networks, 23(5):649?666.L.
Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo.2010.
The sixth PASCAL recognizing textual en-tailment challenge.
Proceedings of TAC.C.
Buckley.
1985.
Implementation of the SMART in-formation retrieval system.
Technical report, Cor-nell University.J.
Conroy, J. Schlesinger, and D. O?Leary.
2006.Topic-focused multi-document summarization usingan approximate oracle score.
In Proceedings ofCOLING-ACL, pages 152?159.H.
Daum?e III and D. Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of ACL,pages 305?312.J.
Delort and E. Alfonseca.
2012.
DualSum: A topic-model based approach for update summarization.
InProceedings of EACL, pages 214?223.A.
Haghighi and L. Vanderwende.
2009.
Exploringcontent models for multi-document summarization.In Proceedings of NAACL-HLT, pages 362?370.L.
Itti and P. F. Baldi.
2006.
Bayesian surprise attractshuman attention.
In Proceedings of NIPS, pages547?554.C.
Lin and E. Hovy.
2000.
The automated acquisitionof topic signatures for text summarization.
In Pro-ceedings of COLING, pages 495?501.C.
Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of HLT-NAACL, pages 1085?1090.C.
Lin.
2004.
ROUGE: A package for automatic eval-uation of summaries.
In Proceedings of Text Sum-marization Branches Out Workshop, ACL, pages 74?81.A.
Nenkova, L. Vanderwende, and K. McKeown.2006.
A compositional context sensitive multi-document summarizer: exploring the factors that in-fluence summarization.
In Proceedings of SIGIR,pages 573?580.W.
D Penny.
2001.
Kullback-liebler divergencesof normal, gamma, dirichlet and wishart densities.Wellcome Department of Cognitive Neurology.B.
Schiffman.
2005.
Learning to Identify New Infor-mation.
Ph.D. thesis, Columbia University.I.
Soboroff and D. Harman.
2005.
Novelty detection:the trec experience.
In Proceedings of HLT-EMNLP,pages 105?112.338
