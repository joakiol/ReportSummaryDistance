Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810?815,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTowards Accurate Distant Supervision for Relational Facts ExtractionXingxing Zhang1 Jianwen Zhang2?
Junyu Zeng3 Jun Yan2 Zheng Chen2 Zhifang Sui11Key Laboratory of Computational Linguistics (Peking University), Ministry of Education,China2Microsoft Research Asia3Beijing University of Posts and Telecommunications1{zhangxingxing,szf}@pku.edu.cn2{jiazhan,junyan,zhengc}@microsoft.com3junyu.zeng@gmail.comAbstractDistant supervision (DS) is an appealinglearning method which learns from exist-ing relational facts to extract more froma text corpus.
However, the accuracy isstill not satisfying.
In this paper, we pointout and analyze some critical factors inDS which have great impact on accuracy,including valid entity type detection,negative training examples constructionand ensembles.
We propose an approachto handle these factors.
By experimentingon Wikipedia articles to extract the facts inFreebase (the top 92 relations), we showthe impact of these three factors on theaccuracy of DS and the remarkable im-provement led by the proposed approach.1 IntroductionRecently there are great efforts on building largestructural knowledge bases (KB) such as Free-base, Yago, etc.
They are composed of relationalfacts often represented in the form of a triplet,(SrcEntity, Relation, DstEntity),such as ?
(Bill Gates, BornIn, Seattle)?.
An impor-tant task is to enrich such KBs by extracting morefacts from text.
Specifically, this paper focuses onextracting facts for existing relations.
This is dif-ferent from OpenIE (Banko et al 2007; Carlson etal., 2010) which needs to discover new relations.Given large amounts of labeled sentences,supervised methods are able to achieve goodperformance (Zhao and Grishman, 2005; Bunescuand Mooney, 2005).
However, it is difficult tohandle large scale corpus due to the high costof labeling.
Recently an approach called distantsupervision (DS) (Mintz et al 2009) was pro-posed, which does not require any labels on thetext.
It treats the extraction problem as classifying?
The contact author.a candidate entity pair to a relation.
Then anexisting fact in a KB can be used as a labeledexample whose label is the relation name.
Thenthe features of all the sentences (from a given textcorpus) containing the entity pair are merged asthe feature of the example.
Finally a multi-classclassifier is trained.However, the accuracy of DS is not satisfying.Some variants have been proposed to improvethe performance (Riedel et al 2010; Hoffmannet al 2011; Takamatsu et al 2012).
They ar-gue that DS introduces a lot of noise into thetraining data by merging the features of all thesentences containing the same entity pair, becausea sentence containing the entity pair of a relationmay not talk about the relation.
Riedel et al(2010) and Hoffmann et al(2011) introducehidden variables to indicate whether a sentenceis noise and try to infer them from the data.Takamatsu et al(2012) design a generative modelto identify noise patterns.
However, as shown inthe experiments (Section 4), the above variants donot lead to much improvement in accuracy.In this paper, we point out and analyze somecritical factors in DS which have great impact onthe accuracy but has not been touched or well han-dled before.
First, each relation has its own schemadefinition, i.e., the source entity and the destina-tion entity should be of valid types, which is over-looked in DS.
Therefore, we propose a componentof entity type detection to check it.
Second, DSintroduces many false negative examples into thetraining set and we propose a new method to con-struct negative training examples.
Third, we find itis difficult for a single classifier to achieve high ac-curacy and hence we train multiple classifiers andensemble them.We also notice that Nguyen and Moschitti(2011a) and Nguyen and Moschitti (2011b) utilizeexternal information such as more facts from Yagoand labeled sentences from ACE to improve the810performance.
These methods can also be equippedwith the approach proposed in this paper.2 Critical Factors Affecting the AccuracyDS has four steps: (1) Detect candidate entitypairs in the corpus.
(2) Label the candidate pairsusing the KB.
(3) Extract features for the pairfrom sentences containing the pair.
(4) Train amulti-class classifier.
Among these steps, we findthe following three critical factors have greatimpact on the accuracy (see Section 4 for theexperimental results).Valid entity type detection.
In DS, a sentencewith a candidate entity pair a sentence with twocandidate entities is noisy.
First, the schema ofeach relation in the KB requires that the sourceand destination entities should be of valid types,e.g., the source and destination entity of therelation ?DirectorOfFilm?
should be of the types?Director?
and ?Film?
respectively.
If the twoentities in a sentence are not of the valid types, thesentence is noisy.
Second, the sentence may nottalk about the relation even when the two entitiesare of the valid types.
The previous works (Riedelet al 2010; Hoffmann et al 2011; Takamatsu etal., 2012) do not distinguish the two types of noisebut directly infer the overall noise from the data.We argue that the first type of noise is very difficultto be inferred just from the noisy relational labels.Instead, we decouple the two types of noise, andutilize external labeled data, i.e., the Wikipediaanchor links, to train an entity type detection mod-ule to handle the first type of noise.
We notice thatwhen Ling and Weld (2012) studied a fine-grainedNER method, they applied the method to relationextraction by adding the recognized entity tags tothe features.
We worry that the contribution of theentity type features may be drowned when manyother features are used.
Their method works wellon relatively small relations, but not that well onbig ones (Section 4.2).Negative examples construction.
DS treats therelation extraction as a multi-class classificationtask.
For a relation, it implies that the facts of allthe other relations together with the ?Other?
classare negative examples.
This introduces many falsenegative examples into the training data.
First,many relations are not exclusive with each other,e.g., ?PlaceOfBorn?
and ?PlaceOfDeath?, theborn place of a person can be also the death place.Second, in DS, the ?Other?
class is composedof all the candidate entity pairs not existed inthe KB, which actually contains many positivefacts of non-Other relations because the KB isnot complete.
Therefore we use a different way toconstruct negative training examples.Feature space partition and ensemble.
Thefeatures used in DS are very sparse and manyexamples do not contain any features.
Thus weemploy more features.
However we find it isdifficult for a single classifier on all the featuresto achieve high accuracy and hence we dividethe features into different categories and traina separate classifier for each category and thenensemble them finally.3 Accurate Distant Supervision (ADS)Different from DS, we treat the extractionproblem as N binary classification problems,one for each relation.
We modify the four stepsof DS (Section 2).
In step (1), when detectingcandidate entity pairs in sentences, we use ourentity type detection module (Section 3.1) to filterout the sentences where the entity pair is of invalidentity types.
In step (2), we use our new methodto construct negative examples (Section 3.2).
Instep (3), we employ more features and design anensemble classifier (Section 3.3).
In step (4), wetrain N binary classifiers separately.3.1 Entity Type DetectionWe divide the entity type detection into two steps.The first step, called boundary detection, is todetect phrases as candidate entities.
The secondstep, called named entity disambiguation, mapsa detected candidate entity to some entity types,e.g., ?FilmDirector?.
Note that an entity might bemapped to multiple types.
For instance, ?VenturaPons?
is a ?FilmDirector?
and a ?Person?.Boundary Detection Two ways are used forboundary detection.
First, for each relation, fromthe training set of facts, we get two dictionaries(one for source entities and one for destination en-tities).
The two dictionaries are used to detect thesource and destination entities.
Second, an exist-ing NER tool (StanfordNER here) is used with thefollowing postprocessing to filter some unwantedentities, because a NER tool sometimes producestoo many entities.
We first find the compatible N-ER tags for an entity type in the KB.
For example,811for the type ?FilmDirector?, the compatible NERtag of Standford NER is ?Person?.
To do this,for each entity type in the KB, we match all theentities of that type (in the training set) back to thetraining corpus and get the probability Ptag(ti) ofeach NER tag (including the ?NULL?
tag meaningnot recognized as a named entity) recognizedby the NER tool.
Then we retain the top k tagsStags = {t1, ?
?
?
, tk} with the highest probabil-ities to account for an accumulated mass z:k = argmink(( k?i=1Ptag(ti))?
z)(1)In the experiments we set z = 0.9.
The compati-ble ner tags are Stags\{?NULL?}.
If the retainedtags contain only ?NULL?, the candidate entitiesrecognized by NER tool will be discarded.Named Entity Disambiguation (NED) Witha candidate entity obtained by the boundarydetection, we need a NED component to assignsome entity types to it.
To obtain such a NED, weleverage the anchor text in Wikipedia to generatetraining data and train a NED component.
Thereferred Freebase entity and the types of an anchorlink in Wikipedia can be obtained from Freebase.The following features are used to train theNED component.
Mention Features: Uni-grams,Bi-grams, POS tags, word shapes in the mention,and the length of the mention.
Context Features:Uni-grams and Bi-grams in the windows of themention (window size = 5).3.2 Negative Examples ConstructionTreating the problem as a multi-class classificationimplies introducing many false negative examplesfor a relation; therefore, we handle each relationwith a separate binary classifier.
However, a KBonly tells us which entity pairs belong to a relation,i.e., it only provides positive examples for each re-lation.
But we also need negative examples to traina binary classifier.
To reduce the number of falsenegative examples, we propose a new methodto construct negative examples by utilizing the1-to-1/1-to-n/n-to-1/n-to-n property of a relation.1-to-1/n-to-1/1-to-n Relation A 1-to-1 or n-to-1 relation is a functional relation: for a relation r,for each valid source entity e1, there is only oneunique destination entity e2 such that (e1, e2) ?
r.However, in a real KB like Freebase, very fewrelations meet the exact criterion.
Thus we use thefollowing approximate criterion instead: relationr is approximately a 1-to-1/n-to-1 relation if theInequalities (2,3) hold, where M is the number ofunique source entities in relation r, and ?(?)
is anindicator function which returns 1 if the conditionis met and returns 0 otherwise.
Inequality (2)says the proportion of source entities which haveexactly one counterpart destination entity shouldbe greater than a given threshold.
Inequality (3)says the average number of destination entities ofa source entity should be less than the threshold.To check whether r is a 1-to-n relation, we simplyswap the source and destination entities of therelation and check whether the reversed relationis a n-to-1 relation by the above two inequalities.In experiments we set ?
= 0.7 and ?
= 1.1.1MM?i=1?(??
{e?|(ei, e?)
?
r}??
= 1)?
?
(2)1MM?i=1??
{e?|(ei, e?)
?
r}??
?
?
(3)n-to-n Relation Relations other than 1-to-1/n-to-1/1-to-n are n-to-n relations.
We approximatelycategorize a n-to-n relation to n-to-1 or 1-to-n bychecking which one it is closer to.
This is doneby computing the following two values ?src and?dst.
r is treated as a 1-to-n relation if ?src > ?dstand as a 1-to-n relation otherwise.
?src =1MsrcMsrc?i=1??
{e?|(ei, e?)
?
r}??
?dst =1MdstMdst?i=1??
{e?|(e?, ei) ?
r}??
(4)Negative examples For a candidate entity pair(e1, e2) not in the relation r of the KB, we firstdetermine whether it is 1-to-n or n-to-1 using theabove method.
If r is 1-to-1/n-to-1 and e1 exists insome fact of r as the source entity, then (e1, e2) isa negative example as it violates the 1-to-1/n-to-1constraint.
If r is 1-to-n, the judgement is similarand just simply swap the source and destinationentities of the relation.3.3 Feature Space Partition and EnsembleThe features of DS (Mintz et al 2009) are verysparse in the corpus.
We add some features in (Yaoet al 2011): Trigger Words (the words on thedependency path except stop words) and EntityString (source entity and destination entity).812Relation Taka Ensembleworks written 0.76 0.98river/basin countries 0.48 1/film/director/film 0.82 1Average 0.79 0.89Table 1: Manual evaluation of top-ranked 50 rela-tion instances for the most frequent 15 relations.We find that without considering the reversedorder of entity pairs in a sentence, the precisioncan be higher, but the recall decreases.
For exam-ple, for the entity pair ?Ventura Pons, Actrius?, weonly consider sentences with the right order (e.g.Ventura Pons is directed by Actrius.).
For each re-lation, we train four classifiers: C1 (without con-sidering reversed order), C2 (considering reversedorder), C1more (without considering reversed or-der and employ more feature) and C2more (con-sidering reversed order and employ more feature).We then ensemble the four classifiers by averagingthe probabilities of predictions:P (y|x) = P1 + P2 + P1more + P2more4 (5)4 Experiments4.1 Dataset and ConfigurationsWe aimed to extract facts of the 92 most frequentrelations in Freebase 2009.
The facts of eachrelation were equally split to two parts for trainingand testing.
Wikipedia 2009 was used as the targetcorpus, where 800,000 articles were used fortraining and 400,000 for testing.
During the NEDphrase, there are 94 unique entity types (they arealso relations in Freebase) for the source and desti-nation entities.
Note that some entity types containtoo few entities and they are discarded.
We used500,000 Wikipedia articles (2,000,000 sentences)for generating training data for the NED compo-nent.
We used Open NLP POS tagger, StandfordNER (Finkel et al 2005) and MaltParser (Nivreet al 2006) to label/tag sentences.
We employedliblinear (Fan et al 2008) as classifiers for NEDand relation extraction and the solver is L2LR.4.2 Performance of Relation ExtractionHeld-out Evaluation.
We evaluate the perfor-mance on the half hold-on facts for testing.
Wecompared performance of the n = 50, 000 best ex-tracted relation instances of each method and thePrecision-Recall (PR) curves are in Figure 1 and0 0.1 0.2 0.3 0.4 0.5 0.6 0.700.20.40.60.81RecallPrecisionOrigDSMultiRTakaADSFigure 1: Performance of different methods.0 0.1 0.2 0.3 0.4 0.5 0.6 0.70.250.40.60.81RecallPrecisionOrigDSDS_FigerETDETD+NegMoreEnsemble(ADS)Figure 2: Contributions of different components.Figure 2.
For a candidate fact without any enti-ty existing in Freebase, we are not able to judgewhether it is correct.
Thus we only evaluate thecandidate facts that at least one entity occurs asthe source or destination entity in the test fact set.In Figure 1, we compared our method withtwo previous methods: MultiR (Hoffmann et al2011) and Takamatsu et al(2012) (Taka).
ForMultiR, we used the author?s implementation1.We re-implemented Takamatsu?s algorithm.
AsTakamatsu?s dataset (903,000 Wikipedia articlesfor training and 400,000 for testing) is very similarto ours, we used their best reported parameters.Our method leads to much better performance.Manual Evaluation.
Following (Takamatsu etal., 2012), we selected the top 50 ranked (accord-ing to their classification probabilities) relationfacts of the 15 largest relations.
We compared ourresults with those of Takamatsu et al(2012) andwe achieved greater average precision (Table 1).1available at http://www.cs.washington.edu/ai/raphaelh/mrWe set T = 120, which leads to the best performance.813Pmicro Rmicro Pmacro Rmacro0.950 0.845 0.947 0.626Table 2: Performance of the NED component4.3 Contribution of Each ComponentIn Figure 2, with the entity type detection (ETD),the performance is better than the original DSmethod (OrigDS).
As for the performance of NEDin the Entity Type Detection, the Micro/MacroPrecision-Recall of our NED component are inTable 2.
ETD is also better than adding the entitytypes of the pair to the feature vector (DS Figer)2as in (Ling and Weld, 2012).
If we also employ thenegative example construction strategy in Section3.2 (ETD+Neg), the precision of the top rankedinstances is improved.
By adding more features(More) and employing the ensemble learning(Ensemble(ADS)) to ETD+Neg, the performanceis further improved.5 ConclusionThis paper dealt with the problem of improving theaccuracy of DS.
We find some factors are crucial-ly important, including valid entity type detection,negative training examples construction and en-sembles.
We have proposed an approach to handlethese issues.
Experiments show that the approachis very effective.ReferencesMichele Banko, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In Pro-ceedings of the 20th international joint conferenceon Artifical intelligence, IJCAI?07, pages 2670?2676, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.Razvan Bunescu and Raymond Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proceedings of Human Language Technolo-gy Conference and Conference on Empirical Meth-ods in Natural Language Processing, pages 724?731, Vancouver, British Columbia, Canada, October.Association for Computational Linguistics.Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-r Settles, Estevam R Hruschka Jr, and Tom MMitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth Conference on Artificial Intelligence(AAAI 2010), volume 2, pages 3?3.2We use Figer (Ling and Weld, 2012) to detect entity typesRong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL-05).
Association for Computational Linguis-tics.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 541?550, Portland, Oregon, USA,June.
Association for Computational Linguistics.X.
Ling and D.S.
Weld.
2012.
Fine-grained entityrecognition.
In Proceedings of the 26th Conferenceon Artificial Intelligence (AAAI).Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages1003?1011, Suntec, Singapore, August.
Associationfor Computational Linguistics.Truc-Vien T. Nguyen and Alessandro Moschitti.2011a.
End-to-end relation extraction using distantsupervision from external semantic repositories.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 277?282, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Truc-Vien T Nguyen and AlessandroMoschitti.
2011b.Joint distant and direct supervision for relation ex-traction.
In Proceeding of the International JointConference on Natural Language Processing, pages732?740.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In In Proc.
of LREC-2006, pages2216?2219.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Proceedings of the Sixteenth Eu-ropean Conference on Machine Learning (ECML-2010), pages 148?163.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervisionfor relation extraction.
In Proceedings of the 50th814Annual Meeting of the Association for Computation-al Linguistics (Volume 1: Long Papers), pages 721?729, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Limin Yao, Aria Haghighi, Sebastian Riedel, and An-drew McCallum.
2011.
Structured relation dis-covery using generative models.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing, pages 1456?1466, Edin-burgh, Scotland, UK., July.
Association for Compu-tational Linguistics.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistic-s (ACL?05), pages 419?426, Ann Arbor, Michigan,June.
Association for Computational Linguistics.815
