Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 466?474,Honolulu, October 2008. c?2008 Association for Computational LinguisticsMultimodal Subjectivity Analysis of Multiparty ConversationStephan RaaijmakersTNO Information andCommunication TechnologyDelft, The Netherlandsstephan.raaijmakers@tno.nlKhiet TruongTNO Defense, Security and SafetySoesterberg, The Netherlandskhiet.truong@tno.nlTheresa WilsonSchool of InformaticsUniversity of EdinburghEdingburgh, UKtwilson@inf.ed.ac.ukAbstractWe investigate the combination of severalsources of information for the purpose of sub-jectivity recognition and polarity classificationin meetings.
We focus on features from twomodalities, transcribed words and acoustics,and we compare the performance of three dif-ferent textual representations: words, charac-ters, and phonemes.
Our experiments showthat character-level features outperform word-level features for these tasks, and that a care-ful fusion of all features yields the best perfor-mance.
11 IntroductionOpinions, sentiments and other types of subjectivecontent are an important part of any meeting.
Meet-ing participants express pros and cons about ideas,they support or oppose decisions, and they makesuggestions that may or may not be adopted.
Whenrecorded and archived, meetings become a part ofthe organizational knowledge, but their value is lim-ited by the ability of tools to search and summa-rize meeting content, including subjective content.While progress has been made on recognizing pri-marily objective meeting content, for example, in-formation about the topics that are discussed (Hsuehand Moore, 2006) and who is assigned to work ongiven tasks (Purver et al, 2006), there has been1This work was supported by the Dutch BSIK-project Mul-timediaN, and the European IST Programme Project FP6-0033812.
This paper only reflects the authors?
views and fund-ing agencies are not liable for any use that may be made of theinformation contained herein.fairly little work specifically directed toward recog-nizing subjective content.In contrast, there has been a wealth of researchover the past several years on automatic subjectiv-ity and sentiment analysis in text, including on-linemedia.
Partly inspired by the rapid growth of so-cial media, such as blogs, as well as on-line newsand reviews, researchers are now actively address-ing a wide variety of new tasks, ranging from blogmining (e.g., finding opinion leaders in an on-linecommunity), to reputation management (e.g.
find-ing negative opinions about a company on the web),to opinion-oriented summarization and question an-swering.
Yet many challenges remain, includinghow best to represent and combine linguistic infor-mation for subjectivity analysis.
With the additionalmodalities that are present when working with face-to-face spoken communication, these challenges areeven more pronounced.The work in this paper focuses on two tasks: (1)recognizing subjective utterances and (2) discrimi-nating between positive and negative subjective ut-terances.
An utterance may be subjective becausethe speaker is expressing an opinion, because thespeaker is discussing someone else?s opinion, or be-cause the speaker is eliciting the opinion of someoneelse with a question.We approach the above tasks as supervised ma-chine learning problems, with the specific goal offinding answers to the following research questions:?
Given a variety of information sources, suchas text arising from (transcribed) speech,phoneme representations of the words in an ut-terance, and acoustic features extracted from466the audio layer, which of these sources are par-ticularly valuable for subjectivity analysis inmultiparty conversation??
Does the combination of these sources lead tofurther improvement??
What are the optimal representations of theseinformation sources in terms of feature designfor a machine learning component?A central tenet of our approach is that subwordrepresentations, such as character and phoneme n-grams, are beneficial for the tasks at hand.2 Subword FeaturesPrevious work has demonstrated that textual unitsbelow the word level, such as character n-grams,are valuable sources of information for varioustext classification tasks.
An example of charactern-grams is the set of 3-grams {#se, sen, ent,nti, tim, ime, men, ent, nt#, t#a,#an, ana, nal, aly, lys, ysi, sis,is#} for the two-word phrase sentiment analysis.The special symbol # represents a word boundary.While it is not directly obvious that there is muchinformation in these truncated substrings, charactern-grams have successfully been used for fine-grained classification tasks, such as named-entityrecognition (Klein et al, 2003) and subjectivesentence recognition (Raaijmakers and Kraaij,2008), as well as a variety of document-level tasks(Stamatatos, 2006; Zhang and Lee, 2006; Kanarisand Stamatatos, 2007).The informativeness of these low-level featurescomes in part from a form of attenuation (Eisner,1996): a slight abstraction of the underlying datathat leads to the formation of string equivalenceclasses.
For instance, words in a sentence will in-variably share many character n-grams.
Since ev-ery unique character n-gram in an utterance consti-tutes a separate feature, this leads to the formationof string classes, which is a form of abstraction.
Forexample, Zhang and Lee (2006) investigate similarsubword representations, called key substring groupfeatures.
By compressing substrings in a corpus in atrie (a prefix tree), and labeling entire sets of distri-butionally equivalent substrings with one group la-bel, an attenuation effect is obtained that proves verybeneficial for a number of text classification tasks.Aside from attenuation effects, character n-grams, especially those that represent word bound-aries, have additional benefits.
Treating wordboundaries as characters captures micro-phrasal in-formation: short strings that express the transitionof one word to another.
Stemming occurs naturallywithin the set of initial character n-grams of a word,where the suffix is left out.
Also, some part-of-speech information is captured.
For example, themodals could, would, should can be represented bythe 4-gram, ould, and the set of adverbs ending in-ly can be represented by the 3-gram ly#.A challenging thought is to extend the use of n-grams to the level of phonemes, which comprisethe first symbolic level in the process of sound tographeme conversion.
If n-grams of phonemes com-pare favorably to word n-grams for the purpose ofsentiment classification, then significant speedupscan be obtained for online sentiment classification,since tokenization of the raw speech signal can makea halt at the phoneme level.3 DataFor this work we use 13 meetings from the AMIMeeting Corpus (Carletta et al, 2005).
Each meet-ing has four participants and is approximately 30minutes long.
The participants play specific roles(e.g., Project Manager, Marketing Expert) and to-gether function as a design team.
Within the set of13 meetings, there are a total of 20 participants, witheach participant taking part in two or three meet-ings as part of the same design team.
Meetings withthe same set of participants represent different stagesin the design process (e.g., Conceptual Design, De-tailed Design).The meetings used in the experiments have beenannotated for subjective content using the AMIDAannotation scheme (Wilson, 2008).
Table 1 liststhe types of annotations that are marked in the data.There are three main categories of annotations, sub-jective utterances, subjective questions, and objec-tive polar utterances.
A subjective utterance is aspan of words (or possibly sounds) where a pri-vate state is being expressed either through choice ofwords or prosody.
A private state (Quirk et al, 1985)467is an internal mental or emotional state, includingopinions, beliefs, sentiments, emotions, evaluations,uncertainties, and speculations, among others.
Al-though typically when a private state is expressedit is the private state of the speaker, as in example(1) below, an utterance may also be subjective be-cause the speaker is talking about the private stateof someone else.
For example, in (2) the negativeopinion attributed to the company is what makes theutterance subjective.
(1) Finding them is really a pain, you know(2) The company?s decided that teletext is out-datedSubjective questions are questions in which thespeaker is eliciting the private state of someone else.In other words, the speaker is asking about whatsomeone else thinks, feels, wants, likes, etc., and thespeaker is expecting a response in which the otherperson expresses what he or she thinks, feels, wants,or likes.
For example, both (3) and (4) below aresubjective questions.
(3) Do you like the large buttons?
(4) What do you think about the large buttons?Objective polar utterances are statements or phrasesthat describe positive or negative factual informationabout something without conveying a private state.The sentence The camera broke the first time I usedit gives an example of negative factual information;generally, something breaking the first time it is usedis not good.For the work in this paper, we focus on recog-nizing subjectivity in general and distinguishing be-tween positive and negative subjective utterances.Positive subjective utterances are those in which anyof the following types of private states are expressed:agreements, positive sentiments, positive sugges-tions, arguing for something, beliefs from whichpositive sentiments can be inferred, and positive re-sponses to subjective questions.
Negative subjectiveutterances express private states that are the oppo-site of those represented by the positive subjectivecategory: disagreements, negative sentiments, nega-tive suggestions, arguing against something, beliefsfrom which negative sentiments can be inferred, andnegative responses to subjective questions.
Example(5) below contains two positive subjective utterancesTable 1: AMIDA Subjectivity Annotation TypesSubjective Utterancespositive subjectivenegative subjectivepositive and negative subjectiveuncertaintyother subjectivesubjective fragmentSubjective Questionspositive subjective questionnegative subjective questiongeneral subjective questionObjective Polar Utterancespositive objectivenegative objectiveand one negative subjective utterance.
Each annota-tion is indicated by a pair of angle brackets.
(5) Um ?POS-SUBJ it?s very easy to use?.Um ?NEG-SUBJ but unfortunately it doeslack the advanced functions?
?POS-SUBJwhich I I quite like having on the controls?.The positive and negative subjective category is formarking cases of positive and negative subjectivitythat are so closely interconnected that it is difficultor impossible to separate the two.
For example, (6)below is marked as both positive and negative sub-jective.
(6) Um ?POS-AND-NEG-SUBJ they?ve alsosuggested that we um we only use the remotecontrol to control the television, not the VCR,DVD or anything else?.In (Wilson, 2008), agreement is measured for eachclass separately at the level of dialogue act segments.If a dialogue act overlaps with an annotation of aparticular type, then the segment is considered tobe labelled with that type.
Table 2 gives the Kappa(Cohen, 1960) and % agreement for subjective seg-ments, positive and negative subjective segments,2and subjective questions.2A positive subjective segment is any dialogue act segmentthat overlaps with a positive subjective utterance or a positive-and-negative subjective utterance.
The negative subjective seg-ments are defined similarly.468Table 2: Interannotator agreement for the AMIDA sub-jectivity annotationsKappa % AgreeSubjective 0.56 79Pos Subjective 0.58 84Neg Subjective 0.62 92Subjective Question 0.56 954 ExperimentsWe conduct two sets of classification experiments.For the first set of experiments (Task 1), we auto-matically distinguish between subjective and non-subjective utterances.
For the second set of ex-periments (Task 2), we focus on distinguishing be-tween positive and negative subjective utterances.For both tasks, we use the manual dialogue act seg-ments available as part of the AMI Corpus as the unitof classification.
For Task 1, a segment is consideredsubjective if it overlaps with either a subjective utter-ance or subjective question annotation.
For Task 2,the segments being classified are those that overlapwith positive or negative subjective utterances.
Forthis task, we exclude segments that are both positiveand negative.
Although limiting the set of segmentsto be classified to just those that are positive or nega-tive makes the task somewhat artificial, it also allowsus to focus in on the performance of features specifi-cally for this task.3 We use 6226 subjective and 8707non-subjective dialog acts for Task 1 (with an aver-age duration of 1.9s, standard deviation of 2.0s), and3157 positive subjective and 1052 negative subjec-tive dialog acts for Task 2 (average duration of 2.6s,standard deviation of 2.3s).The experiments are performed using 13-foldcross validation.
Each meeting constitutes a separatefold for testing, e.g., all the segments from meeting 1make up the test set for fold 1.
Then, for a given fold,the segments from the remaining 12 meetings areused for training and parameter tuning, with roughlya 85%, 7%, and 8% split between training, tuning,and testing sets for each fold.
The assignment totraining versus tuning set was random, with the onlyconstraint being that a segment could only be in thetuning set for one fold of the data.3In practice, this excludes about 7% of the positive/negativesegments.The experiments we perform involve two steps.First, we train and optimize a classifier for each typeof feature using BoosTexter (Schapire and Singer,2000) AdaBoost.MH.
Then, we investigate the per-formance of all possible combinations of featuresusing linear combinations of the individual featureclassifiers.4.1 FeaturesThe two modalities that are investigated, prosodic,and textual, are represented by four differentsets of features: prosody (PROS), word n-grams (WORDS), character n-grams (CHARS), andphoneme n-grams (PHONES).Based on previous research on prosody modellingin a meeting context (Wrede and Shriberg, 2003)and on the literature in emotion research (Banse andScherer, 1996) we extract PROS features that aremainly based on pitch, energy and the distribution ofenergy in the long-term averaged spectrum (LTAS)(see Table 3).
These features are extracted at theword level and aggregated to the dialogue-act levelby taking the average over the words per dialogueact.
We then normalize the features per speaker permeeting by converting the raw feature values to z-scores (z = (x ?
?)/?
).Table 3: Prosodic features used in experiments.pitch mean, standard deviation, min-imum, maximum, range, meanabsolute slopeintensity (en-ergy)mean, standard deviation, min-imum, maximum, range, RMSenergydistribution en-ergy in LTASslope, Hammerberg index, cen-tre of gravity, skewnessThe textual features, WORDS and CHARS, andthe PHONES features are based on a manual tran-scription of the speech.
The PHONES were pro-duced through dictionary lookup on the words in thereference transcription.
Both CHARS and PHONESrepresentations include word boundaries as informa-tive tokens.
The textual features for a given seg-ment are simply all the WORDS/CHARS/PHONESin that segment.
Selection of n-grams is performedby the learning algorithm.4694.2 Single Source ClassifiersWe train four single source classifiers using BoosT-exter, one for each type of feature.
For the WORDS,CHARS, and PHONES, we optimize the classi-fier by performing a grid search over the parame-ter space, varying the number of rounds of boosting(100, 500, 1000, 2000, 5000), the length of the n-gram (1, 2, 3, 4, 5), and the type of n-gram.
Boos-Texter can be run with three different n-gram con-figurations: n-gram, s-gram, and f -gram.
For thedefault configuration (n-gram), BoosTexter searchesfor n-grams up to length n. For example, if n = 3,BoosTexter will consider 1-grams, 2-grams, and 3-grams.
For the s-gram configuration, BoosTexterwill in addition consider sparse n-grams (i.e., n-grams containing wildcards), such as the * idea.
Forthe f -gram configuration, BoosTexter will only con-sider n-grams of a maximum fixed length, e.g., ifn = 3 BoosTexter will only consider 3-grams.
Forthe PROS classifier, only the number of rounds ofboosting was varied.
The parameters are selectedfor each fold separately; the parameter set that pro-duces the highest subjective F1 score on the tuningset for Task 1, and the highest positive subjective F1score for Task 2, is used to train the final classifierfor that fold.4.3 Classifier combinationAfter the single source classifiers have been trained,they have to be combined into an aggregate classi-fier.
To this end, we decided to apply a simple linearinterpolation strategy.
Linear interpolation of mod-els is the weighted combination of simple models toform complex models, and has its roots in generativelanguage models (Jelinek and Mercer, 1980).
(Raai-jmakers, 2007) has demonstrated its use for discrim-inative machine learning.In the present binary class setting, BoosTexterproduces two decision values, one for every class.For every individual single-source classifier (i.e.,PROS, WORDS, CHARS and PHONES), separateweights are estimated that are applied to the decisionvalues for the two classes produced by these classi-fiers.
These weights express the relative importanceof the single-source classifiers.The prediction of an aggregate classifier for aclass c is then simply the sum of all weights forall participating single-source classifiers applied tothe decision values these classifiers produce for thisclass.
The class with the maximum score wins, justas in the simple non-aggregate case.Formally, then, this linear interpolation strategyfinds for n single-source classifiers n interpolationweights ?1, .
.
.
?n that minimize the empirical loss(measured by a loss function L), with ?j the weightof classifier j (?
?
[0, 1]), and C jc (xi) the decisionvalue of class c produced by classifier j for datum xi(a feature vector).
The two classes are denoted with0, 1.
The true class for datum xi is denoted with x?i.The loss function is in our case based on subjectiveF-measure (Task 1) or positive subjective F-measure(Task 2) measured on heldout development trainingand test data.The aggregate prediction x?i for datum xi on thebasis of n single-source classifiers then becomesx?i = arg maxc (n?j=1?j ?
Cjc=0(xi),n?j=1?j ?
Cjc=1(xi))(1)and the lambdas are defined as?nj = arg min?nj ?
[0,1]k?iL(x?i, x?i;?j , .
.
.
, ?n) (2)The search process for these weights can easily beimplemented with a simple grid search over admis-sible ranges.In the experiments described below, we investi-gate all possible combinations of the four differ-ent sets of features (PROS, WORDS, CHARS, andPHONES) to determine which combination yieldsthe best performance for subjectivity and subjectivepolarity recognition.5 Results and DiscussionResults for the two tasks are given in Tables 4 and 5and in Figures 1 and 2.
We use two baselines, listedat the top of each table.
The bullets in a given rowindicate the features that are being evaluated for agiven experiment.
In Table 4, subjective F1, recall,and precision are reported as well as overall accu-racy.
In Table 4, the F1, recall, and precision scoresare for the positive subjective class.
All values in thetables are averages over the 13 folds.470Table 4: Results Task 1: Subjective vs. Non-Subjective.PROS WORDS CHARS PHONES F1 PREC REC ACCBASE-SUBJ always chooses subjective class 60.3 43.4 100 43.4BASE-RAND randomly chooses a class based on priors 41.8 42.9 41.3 50.6single?
54.6 55.3 54.5 63.1?
60.5 68.5 54.5 71.0?
61.7 67.5 57.2 71.1?
60.3 66.4 55.5 70.2double?
?
63.9 72.1 57.6 73.4?
?
65.6 71.9 60.3 74.0?
?
64.6 72.3 58.4 73.7?
?
66.2 73.8 60.1 74.9?
?
65.2 73.2 58.8 74.3?
?
66.1 72.8 60.7 74.5triple?
?
?
66.5 74.3 60.3 75.1?
?
?
65.5 73.5 59.0 74.5?
?
?
66.5 73.3 60.8 74.8?
?
?
66.9 74.3 60.9 75.3quartet ?
?
?
?
67.1 74.5 61.2 75.4Table 5: Results Task 2: Positive Subjective vs.
Negative Subjective.PROS WORDS CHARS PHONES F1 PREC REC ACCBASE-POS-SUBJ always chooses positive subjective class 85.6 75.0 100 75.0BASE-RAND randomly chooses a class based on priors 75.1 74.4 76.1 62.4single?
84.8 74.8 98.1 73.9?
85.6 79.6 93.1 76.8?
85.9 81.9 90.5 78.0?
85.5 80.5 91.3 77.0double?
?
88.7 83.0 95.4 81.9?
?
88.7 83.1 95.1 81.8?
?
88.5 83.3 94.4 81.6?
?
89.5 84.2 95.7 83.3?
?
89.2 83.7 95.5 82.8?
?
89.0 84.2 94.6 82.6triple?
?
?
89.6 84.0 96.1 83.4?
?
?
89.3 83.6 95.8 82.8?
?
?
89.2 83.7 95.5 82.7?
?
?
89.8 84.4 96.0 83.8quartet ?
?
?
?
89.9 84.4 96.2 83.8It is quite obvious that the combination of differ-ent sources of information is beneficial, and in gen-eral, the more information the better the results.
Thebest performing classifier for Task 1 uses all the fea-tures, achieving a subjective F1 of 67.1.
For Task 2,the best performing classifier also uses all the fea-tures, although it does not perform significantly bet-ter than the classifier using only WORDS, CHARS,and PHONES.4 This classifier achieves a positive-subjective F1 of 89.9.We measured the effects of adding more infor-mation to the single source classifiers.
These re-sults are listed in Table 6.
Of the various featuretypes, prosody seems to be the least informative forboth subjectivity and polarity classification.
In ad-dition to producing the single-source classifier withthe lowest performance for both tasks, Table 6 showsthat when prosody is added, of all the features it isleast likely to yield significant improvements.4We measured significance with the non-parametricWilcoxon signed rank test, p < 0.05.Throughout the experiments, adding an additionaltype of textual feature always yields higher results.In all cases but two, these improvements are sig-nificant.
The best performing of the features arethe character n-grams.
Of the single-source exper-iments, the character n-grams achieve the best per-formance, with significant improvements in F1 overthe other single-source classifiers for both Task 1and Task 2.
Also, adding character n-grams to otherfeature combinations always gives significant im-provements in performance.An obvious question that remains is what the ef-fect is of classifier interpolation on the results.
Toanswer this question, we conducted two additionalexperiments for both tasks.
First, we investigatedthe performance of an uninterpolated combinationof the four single-source classifiers.
In essence, thiscombines the separate feature spaces without explic-itly weighting them.
Second, we investigated the re-sults of training a single BoosTexter model using allthe features, essentially merging all feature spaces471Table 6: Addition of features separately (for Task 1 and 2): ?+?
for a row-column pair (r, c) means that the additionof column feature c to the row features r significantly improved r?s F1; ?-?
indicates no significant improvement; ?X?means ?not applicable?+PROS + WORDS +CHARS +PHONESTask 1 2 1 2 1 2 1 2PROS X X + + + + + +WORDS - + X X + + + +CHARS - + - + X X - +PHONES - + + + + + X XPROS+WORDS X X X X + + + +PROS+CHARS X X + + X X + +PROS+PHONES X X + + + + X XWORDS+CHARS + - X X X X + +WORDS+PHONES + - X X + + X XCHARS+PHONES + + + + X X X XPROS+WORDS+CHARS X X X X X X + +PROS+WORDS+PHONES X X X X + + X XPROS+CHARS+PHONES X X + + X X X XWORDS+CHARS+PHONES + - X X X X X XF1measuremajorityrandproswordscharsphonespros+wordspros+charspros+phoneswords+charswords+phoneschars+phonespros+words+charspros+words+phonespros+phones+charswords+chars+phonespros+words+chars+phones40455055606570Figure 1: Results (F1) experiment 1: subjective vs. non-subjective.into one agglomerate feature space.
The results forthese experiments are given in Table 7, along withthe results from the all-feature interpolated classifi-cation for comparison.The results in Table 7 show that interpolationoutperforms both the unweighted and single-modelcombinations for both tasks.
For Task 1, the ef-fect of interpolation compared to a single model ismarginal (a .03 point difference in F1).
However,compared to the uninterpolated combination, inter-polation gives a clear 3.1 points improvement of F1.For Task 2, interpolation outperforms both the unin-terpolated and single-model classifiers, with 2 and 3points improvements in F1, respectively.F1measuremajorityrandproswordscharsphonespros+wordspros+charspros+phoneswords+charswords+phoneschars+phonespros+words+charspros+words+phonespros+phones+charswords+chars+phonespros+words+chars+phones707580859095100Figure 2: Results (F1) experiment 2: positive subjectivevs.
negative subjective.6 Related WorkPrevious work has demonstrated that textual unitsbelow the word level, such as character n-grams,are valuable sources of information.
Character-level models have successfully been used for named-entity recognition (Klein et al, 2003), predictingauthorship (Keselj et al, 2003; Stamatatos, 2006),text categorization (Zhang and Lee, 2006), web pagegenre identification (Kanaris and Stamatatos, 2007),and sentence-level subjectivity recognition (Raaij-makers and Kraaij, 2008) In spoken-language data,Hsueh (2008) achieves good results using chainsof phonemes to automatically segment meetings ac-cording to topic.
However, to the best of our knowl-edge there has been no investigation to date on the472Table 7: Results of interpolated classifiers compared touninterpolated and single-model classifiers for all fea-tures.Task Combination ACC REC PREC F11interpolated 75.4 61.2 74.5 67.1uninterpolated 73.0 58.7 70.6 64.0single model 74.7 62.1 72.7 66.82interpolated 83.8 96.2 84.4 89.9uninterpolated 79.8 98.0 79.7 87.9single model 79.5 91.0 83.3 86.9combination of character-level, phoneme-level, andword-level models for any natural language classifi-cation tasks.In text, there has been a significant amount ofresearch on subjectivity and sentiment recognition,ranging from work at the phrase level to work onclassifying sentences and documents.
Sentence-level subjectivity classification (e.g., (Riloff andWiebe, 2003; Yu and Hatzivassiloglou, 2003)) andsentiment classification (e.g., (Yu and Hatzivas-siloglou, 2003; Kim and Hovy, 2004; Hu and Liu,2004; Popescu and Etzioni, 2005)) is the researchin text most closely related to our work.
Of thesentence-level research, the most similar is workby Raaijmakers and Kraaij (2008) comparing word-spanning character n-grams to word-internal char-acter n-grams for subjectivity classification in newsdata.
They found that character n-grams spanningwords perform the best.Research on recognizing subjective content inmultiparty conversation includes work by Somasun-daran et al (2007) on recognizing sentiments andarguing in meetings, work by Neiberg el al.
(2006)on recognizing positive, negative, and neutral emo-tions in meetings, work on recognizing agreementsand disagreements in meetings (Hillard et al, 2003;Galley et al, 2004; Hahn et al, 2006), and workby Wrede and Shriberg (2003) on recognizing meet-ing hotspots.
Somasundaran et al use lexical anddiscourse features to recognize sentences and turnswhere meeting participants express sentiments or ar-guing.
They also use the AMI corpus in their work;however, the use of different annotations and taskdefinitions makes it impossible to directly comparetheir results and ours.
Neiberg et al use acoustic?prosodic features (Mel-frequency Cepstral Coeffi-cients (MFCCs) and pitch features) and lexical n-grams for recognizing emotions in the ISL MeetingCorpus (Laskowski and Burger, 2006).Agreements and disagreements are a subset of theprivate states represented by the positive and neg-ative subjective categories used in this work.
Torecognise agreements and disagreements automati-cally, Hillard et al train 3-way decision tree clas-sifiers (agreement, disagreement, other) using bothword-based and prosodic features.
Galley et almodel this task as a sequence tagging problem, andinvestigate whether features capturing speaker inter-actions are useful for recognizing agreements anddisagreements.
Hahn et al investigate the use ofcontrast classifiers (Peng et al, 2003) for the task,using only lexical features.Hotspots are places in a meeting in which the par-ticipants are highly involved in the discussion.
Al-though high involvement does not necessarily equatesubjective content, in practice, we expect more sen-timents, opinions, and arguments to be expressedwhen participants are highly involved in the discus-sion.
In their work on recognizing meeting hotspots,Wrede and Shriberg focus on evaluating the contri-bution of various prosodic features, ignoring lexi-cal features completely.
The results of their studyhelped to inform our choice of prosodic features forthe experiments in this paper.7 ConclusionsIn this paper, we investigated the use of prosodicfeatures, word n-grams, character n-grams, andphoneme n-grams for subjectivity recognition andpolarity classification of dialog acts in multipartyconversation.
We show that character n-gramsoutperform prosodic features, word n-grams andphoneme n-grams in subjectiviy recognition and po-larity classification.
Combining these features sig-nificantly improves performance.
Comparing theadditive value of the four information sources avail-able, prosodic information seem to be least in-formative while character-level information indeedproves to be a very valuable source.
For subjectiv-ity recognition, a combination of prosodic, word-level, character-level, and phoneme-level informa-tion yields the best performance.
For polarity clas-sification, the best performance is achieved with a473combination of words, characters and phonemes.ReferencesR.
Banse and K. R. Scherer.
1996.
Acoustic profiles invocal emotion expression.
Journal of Personality andSocial Psychology, pages 614?636.J.
Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,M.
Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,I.
McCowan, W. Post, D. Reidsma, and P. Wellner.2005.
The AMI meeting corpus.
In Proceedings ofthe Measuring Behavior Symposium on ?Annotatingand Measuring Meeting Behavior?.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20:37?46.J.
Eisner.
1996.
An empirical comparison of probabilitymodels for dependency grammar.
In Technical ReportIRCS-96-11, Institute for Research in Cognitive Sci-ence, University of Pennsylvania.M.
Galley, K. McKeown, J. Hirschberg, and E. Shriberg.2004.
Identifying agreement and disagreement in con-versational speech: Use of bayesian networks to modelpragmatic dependencies.
In Proceedings of ACL.S.
Hahn, R. Ladner, and M. Ostendorf.
2006.
Agree-ment/disagreement classification: Exploiting unla-beled data using contrast classifiers.
In Proceedingsof HLT/NAACL.D.
Hillard, M. Ostendorf, and E. Shriberg.
2003.
De-tection of agreement vs. disagreement in meetings:Training with unlabeled data.
In Proceedings ofHLT/NAACL.P.
Hsueh and J. Moore.
2006.
Automatic topic seg-mentation and lablelling in multiparty dialogue.
InProceedings of IEEE/ACM Workshop on Spoken Lan-guage Technology.P.
Hsueh.
2008.
Audio-based unsupervised segmentationof meeting dialogue.
In Proceedings of ICASSP.M.
Hu and B. Liu.
2004.
Mining and summarizing cus-tomer reviews.
In Proceedings of KDD.F.
Jelinek and R. L. Mercer.
1980.
Interpolated estima-tion of Markov source parameters from sparse data.In Proceedings, Workshop on Pattern Recognition inPractice, pages 381?397.I.
Kanaris and E. Stamatatos.
2007.
Webpage genre iden-tification using variable-length character n-grams.
InProceedings of ICTAI.V.
Keselj, F. Peng, N. Cercone, and C. Thomas.
2003.
N-gram-based author profiles for authorship attribution.In Proceedings of PACLING.S.
Kim and Eduard Hovy.
2004.
Determining the senti-ment of opinions.
In Proceedings of Coling.D.
Klein, J. Smarr, H. Nguyen, and C.D.
Manning.
2003.Named entity recognition with character-level models.In Proceedings of CoNLL.K.
Laskowski and S. Burger.
2006.
Annotation and anal-ysis of emotionally relevant behavior in the ISL meet-ing corpus.
In Proceedings of LREC 2006.D.
Neiberg, K. Elenius, and K. Laskowski.
2006.
Emo-tion recognition in spontaneous speech using GMMs.In Proceedings of INTERSPEECH.K.
Peng, S. Vucetic, B. Han, H. Xie, and Z Obradovic.2003.
Exploiting unlabeled data for improving ac-curacy of predictive data mining.
In Proceedings ofICDM.A.
Popescu and O. Etzioni.
2005.
Extracting productfeatures and opinions from reviews.
In Proceedings ofHLT/EMNLP.M.
Purver, P. Ehlen, and J. Niekrasz.
2006.
Detectingaction items in multi-party meetings: Annotation andinitial experiments.
In Proceedings of MLMI.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.A Comprehensive Grammar of the English Language.Longman, New York.S.
Raaijmakers and W. Kraaij.
2008.
A shallow ap-proach to subjectivity classification.
In Proceedingsof ICWSM.S.
Raaijmakers.
2007.
Sentiment classification with in-terpolated information diffusion kernels.
In Proceed-ings of the First International Workshop on Data Min-ing and Audience Intelligence for Advertising (AD-KDD?07).E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proceedings ofEMNLP.R.
E. Schapire and Y.
Singer.
2000.
BoosTexter: Aboosting-based system for text categorization.
Ma-chine Learning, 39(2/3):135?168.S.
Somasundaran, J. Ruppenhofer, and J. Wiebe.
2007.Detecting arguing and sentiment in meetings.
In Pro-ceedings of SIGdial.E.
Stamatatos.
2006.
Ensemble-based author identifica-tion using character n-grams.
In Proceedings of TIR.T.
Wilson.
2008.
Annotating subjective content in meet-ings.
In Proceedings of LREC.B.
Wrede and E. Shriberg.
2003.
Spotting ?hot spots?in meetings: Human judgments and prosodic cues.
InProceedings of EUROSPEECH.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answer-ing opinion questions: Separating facts from opinionsand identifying the polarity of opinion sentences.
InProceedings of EMNLP.D.
Zhang and W. S. Lee.
2006.
Extracting key-substring-group features for text classification.
In Proceedingsof KDD.474
