INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49?58,Utica, May 2012. c?2012 Association for Computational LinguisticsOptimising Incremental Generation for Spoken Dialogue Systems:Reducing the Need for FillersNina Dethlefs, Helen Hastie, Verena Rieser and Oliver LemonHeriot Watt UniversityEH14 4AS, Edinburghn.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.ukAbstractRecent studies have shown that incrementalsystems are perceived as more reactive, nat-ural, and easier to use than non-incrementalsystems.
However, previous work on incre-mental NLG has not employed recent ad-vances in statistical optimisation using ma-chine learning.
This paper combines the twoapproaches, showing how the update, revokeand purge operations typically used in in-cremental approaches can be implemented asstate transitions in a Markov Decision Process.We design a model of incremental NLG thatgenerates output based on micro-turn inter-pretations of the user?s utterances and is ableto optimise its decisions using statistical ma-chine learning.
We present a proof-of-conceptstudy in the domain of Information Presen-tation (IP), where a learning agent faces thetrade-off of whether to present information assoon as it is available (for high reactiveness)or else to wait until input ASR hypotheses aremore reliable.
Results show that the agentlearns to avoid long waiting times, fillers andself-corrections, by re-ordering content basedon its confidence.1 IntroductionTraditionally, the smallest unit of speech processingfor interactive systems has been a full utterance withstrict, rigid turn-taking.
Components of these inter-active systems, including NLG systems, have so fartreated the utterance as the smallest processing unitthat triggers a module into action.
More recently,work on incremental systems has shown that pro-cessing smaller ?chunks?
of user input can improvethe user experience (Skantze and Schlangen, 2009;Buss et al, 2010; Skantze and Hjalmarsson, 2010;Baumann et al, 2011).
Incrementality in NLG sys-tems enables the system designer to model severaldialogue phenomena that play a vital role in hu-man discourse (Levelt, 1989) but have so far beenabsent from NLG systems.
These include morenatural turn-taking through rapid system responses,grounding through the generation of backchannelsand feedback, and barge-ins (from both user and sys-tem).
In addition, corrections and self-correctionsthrough constant monitoring of user and system ut-terances play an important role, enabling the systemto recover smoothly from a recognition error or achange in the user?s preferences.
Some examples ofthe phenomena we are targeting are given in Fig.
1.Skantze and Hjalmarsson (2010) present a modelof incremental speech generation in which input pro-cessing and output planning are parallel processesand the system can self-monitor its own genera-tion process.
In an evaluation with human usersthey showed that their incremental system startedto speak significantly faster than a non-incrementalsystem (roughly 600 ms) and was perceived as sig-nificantly more polite and efficient.
Users also in-dicated that they knew better when to start speakingthemselves.
Alternative approaches to incrementalNLG include Kilger and Finkler (1995) who presentan early approach based on Tree-Adjoining Gram-mar, and Purver and Otsuka (2003) who define anincremental generator based on Dynamic Syntax.Both of these generators can monitor their own out-put and initiate corrections if necessary.Over recent years, adaptive and data-driven ap-49Self-correction (the system made a mistake)USR I want Italian food in the centre of town .
.
.SYS OK.
I found 35 Indian restaurants .
.
.USR No, I want Italian.SYS oh sorry .
.
.SYS I have 24 Italian restaurants in the city centre .
.
.Correction (the user changed their mind)USR I want Indian food in the centre of town .
.
.SYS There are 35 Indian .
.
.USR Oh sorry, I meant Italian.SYS OK, Italian .
.
.SYS I have 24 Italian restaurants .
.
.Re-ordering (from high to low confidence)USR I want Italian food .
.
.SYS [waits]USR in the city centre.SYS I have 120 places in the city centre .
.
.USR I love Italian .
.
.SYS 24 of them Italian .
.
.Holding the floorUSR I want cheap Italian food .
.
.SYS ok let me seeSYS I have 3 cheap Italian places .
.
.Figure 1: Example phenomena generated with the trainedpolicy.
The agent has learnt to produce backchannelswhen the user pauses, monitor and (self-)correct its out-put, and present information according to its confidence.proaches to NLG have also been developed andshown to outperform the previous (handcrafted,rule-based) methods for specific problems (Rieser etal., 2010; Janarthanam and Lemon, 2010; Dethlefsand Cuaya?huitl, 2011).
This work has establishedthat NLG can fruitfully be treated as a data-drivenstatistical planning process, where the objective isto maximise expected utility of the generated utter-ances (van Deemter, 2009), by adapting them to thecontext and user.
Statistical approaches to sentenceplanning and surface realisation have also been ex-plored (Stent et al, 2004; Belz, 2008; Mairesse etal., 2010; Angeli et al, 2010).
The advantages ofdata-driven methods are that NLG is more robust inthe face of noise, can adapt to various contexts and,trained on real data, can produce more natural anddesirable variation in system utterances.This paper describes an initial investigation into anovel NLG architecture that combines incrementalprocessing with statistical optimisation.
In order tomove away from conventional strict-turn taking, wehave to be able to model the complex interactionsobserved in human-human conversation.
Doing thisin a deterministic fashion through hand-written ruleswould be time consuming and potentially inaccu-rate, with no guarantee of optimality.
In this paper,we demonstrate that it is possible to learn incremen-tal generation behaviour in a reward-driven fashion.2 Previous Work: Incremental ProcessingArchitecturesThe smallest unit of processing in incremental sys-tems is called incremental unit (IU).
Its instantia-tion depends on the particular processing module.
Inspeech recognition, IUs can correspond to phonemesequences that are mapped onto words (Baumannand Schlangen, 2011).
In dialogue management, IUscan correspond to dialogue acts (Buss et al, 2010).In speech synthesis, IUs can correspond to speechunit sequences which are mapped to segments andspeech plans (Skantze and Hjalmarsson, 2010).
IUsare typically linked to other IUs by two types of rela-tions: same-level links connect IUs sequentially andexpress relationships at the same level; grounded-inlinks express hierarchical relations between IUs.2.1 Buffer-Based Incremental ProcessingA general abstract model of incremental process-ing based on buffers and a processor was devel-oped by Schlangen and Skantze (2009) and is illus-trated in Figure 2.
It assumes that the left bufferof a module, such as the NLG module, receivesIUs from one or more other processing modules,such as the dialogue manager.
These input IUs arethen passed on to the processor, where they aremapped to corresponding (higher-level) IUs.
Foran NLG module, this could be a mapping from thedialogue act present(cuisine=Indian) to the realisa-tion ?they serve Indian food?.
The resulting IUs arepassed on to the right buffer which co-incides withthe left buffer of another module (for example thespeech synthesis module in our example).
Same-level links are indicated as dashed arrows in Figure2 and grounded-in links as stacked boxes of IUs.The figure also shows that the mapping betweenIUs can be a one-to-many mapping (IU1 and IU2are mapped to IU3) or a one-to-one mapping (IU3 is50IU1 IU2IU1 IU2IU3IU3 IU3IU4IU4Left buffer Processor Right bufferLeft buffer Processor Right bufferFigure 2: The buffer-based model showing two connectedmodules (from Skantze and Hjalmarsson (2010).IU1IU2 IU3 IU4 IU5IU6 IU7 IU8 IU9 .
.
.Figure 3: The ISU-model for incremental processing(adapted from Buss and Schlangen (2011)).mapped to IU4).
The model distinguishes four op-erations that handle information processing: update,revise, purge and commit.
Whenever new IUs en-ter the module?s left buffer, the module?s knowledgebase is updated to reflect the new information.
Suchinformation typically corresponds to the current besthypothesis of a preceding processing module.
Asa property of incremental systems, however, suchhypotheses can be revised by the respective preced-ing module and, as a result, the knowledge bases ofall subsequent modules need to be purged and up-dated to the newest hypothesis.
Once a hypothesisis certain to not be revised anymore, it is commit-ted.
For concrete implementations of this model, seeSkantze and Schlangen (2009), Skantze and Hjal-marsson (2010), Baumann and Schlangen (2011).An implementation of an incremental dialoguemanager is based on the Information State Update(ISU) model (Buss et al, 2010; Buss and Schlangen,2011).
The model is related in spirit to the buffer-based architecture, but all of its input processing andoutput planning is realised by ISU rules.
This is truefor the incremental ?house-keeping?
actions update,revise, etc.
and all types of dialogue acts.
The in-cremental ISU model is shown in Figure 3.
Notethat this hierarchical architecture transfers well tothe ?classical?
division of NLG levels into utterance(IU1), content selection (IU2 - IU5) and surface re-alisations (IU6 - IU9, etc.
).2.2 Beat-Driven Incremental ProcessingIn contrast to the buffer-based architectures, alterna-tive incremental systems do not reuse previous par-tial hypotheses of the user?s input (or the system?sbest output) but recompute them at each process-ing step.
We follow Baumann et al (2011) in call-ing them ?beat-driven?
systems.
Raux and Eskenazi(2009) use a cost matrix and decision theoretic prin-ciples to optimise turn-taking in a dialogue systemunder the constraint that users prefer no gaps and nooverlap at turn boundaries.
DeVault et al (2009) usemaximum entropy classification to support respon-sive overlap in an incremental system by predictingthe completions of user utterances.2.3 Decision-making in Incremental SystemsSome of the main advantages of the buffer- and ISU-based approaches include their inherently incremen-tal mechanisms for updating and revising system hy-potheses.
They are able to process input of varyingsize and type and, at the same time, produce arbi-trarily complex output which is monitored and canbe modified at any time.
On the other hand, currentmodels are based on deterministic decision makingand thus share some of the same drawbacks that non-incremental systems have faced: (1) they rely onhand-written rules which are time-consuming andexpensive to produce, (2) they do not provide amechanism to deal with uncertainty introduced byvarying user behaviour, and (3) they are unable togeneralise and adapt flexibly to unseen situations.For NLG in particular, we have seen that incre-mentality can enhance the responsiveness of sys-tems and facilitate turn-taking.
However, this ad-vantage was mainly gained by the system produc-ing semantically empty fillers such as um, let mesee, well, etc.
(Skantze and Hjalmarsson, 2010).
Itis an open research question whether such markersof planning or turn-holding can help NLG systems,but for now it seems that they could be reduced toa minimum by optimising the timing and order ofInformation Presentation.
In the following, we de-velop a model for incremental NLG that is based onreinforcement learning (RL).
It learns the best mo-ment to present information to the user, when facedwith the options of presenting information as soonas it becomes available or else waiting until the in-51Type ExampleComparison The restaurant Roma is in the medium price range, but does not have great food.
The Firenzeand Verona both have great food but are more expensive.
The Verona has good service, too.Recommendation Restaurant Verona has the best overall match with your query.
It is a bit more expensive,but has great food and service.Summary I have 43 Italian restaurants in the city centre that match your query.
10 of them are in themedium price range, 5 are cheap and 8 are expensive.Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants inthe city centre that have a good price for value.put hypotheses of the system are more stable.
Thisalso addresses the general trade-off that exists in in-cremental systems between the processing speed ofa system and the output quality.3 Information Presentation StrategiesOur domain of application will be the Informa-tion Presentation phase in an interactive systemfor restaurant recommendations, extending previouswork by Rieser et al (2010), (see also Walker etal.
(2004) for an alternative approach).
Rieser etal.
incrementally construct IP strategies accordingto the predicted user reaction, whereas our approachfocuses on timing and re-ordering of informationaccording to dynamically changing input hypothe-ses.
We therefore implement a simplified versionof Rieser et al?s model.
Their system distinguishedtwo steps: the selection of an IP strategy and theselection of attributes to present to the user.
We as-sume here that the choice of attributes is determinedby matching the types specified in the user input,so that our system only needs to choose a strategyfor presenting its results (in the future, though, wewill include attribute selection into the decision pro-cess).
Attributes include cuisine, food quality, lo-cation, price range and service quality of a restau-rant.
The system then performs a database lookupand chooses among three main IP strategies sum-mary, comparison, recommendation and several or-dered combinations of these.
Please see Rieser et al(2010) for details.
Table 1 shows examples of themain types of presentation strategies we address.4 Optimising Incremental NLGTo optimise the NLG process within an incremen-tal model of dialogue processing, we define an RLagent with incremental states and actions for the IPtask.
An RL agent is formalised as a Markov De-cision Process, or MDP, which is characterised as afour-tuple < S,A, T,R >, where S is a set of statesrepresenting the status of the NLG system and all in-formation available to it, A is a set of NLG actionsthat combine strategies for IP with handling incre-mental updates in the system, T is a probabilistictransition function that determines the next state s?from the current state s and the action a accordingto a conditional probability distribution P (s?|s, a),and R is a reward function that specifies the reward(a numeric value) that an agent receives for takingaction a in state s. Using such an MDP, the NLGprocess can be seen as a finite sequence of states,actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},where t is the time step.
Note that a learning episodefalls naturally into a number of time steps at each ofwhich the agent observes the current state of the en-vironment st, takes an action at and makes a tran-sition to state st+1.
This organisation into discretetime steps, and the notion of a state space that is ac-cessible to the learning agent at any time allows us toimplement the state update, revoke and purge opera-tions typically assumed by incremental approachesas state updates and transitions in an MDP.
Anychange in the environment, such as a new best hy-pothesis of the recogniser, can thus be representedas a transition from one state to another.
At eachtime step, the agent then takes the currently best ac-tion according to the new state.
The best action inan incremental framework can include correcting aprevious output, holding the floor as a marker ofplanning, or to wait until presenting information.11We treat these actions as part of NLG content selectionhere, but are aware that in alternative approaches, they could52StatesincrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}presStrategy {0=unfilled,1=filled}statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}statusFood {0=unfilled,1=low,2=medium,3=high,4=realised}statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}statusService {0=unfilled,1=low,2=medium,3=high,4=realised}userReaction {0=none,1=select,2=askMore,3=other}userSilence={0=false,1=true}ActionsIP: compare, recommend, summarise, summariseCompare,summariseRecommend, summariseCompareRecommend,Slot-ordering: presentCuisine, presentFood, presentLocation,presentPrice, presentService,Incremental: backchannel, correct, selfCorrect, holdFloor,waitMoreGoal State 0, 1, 0 ?
4, 0 ?
4, 0 ?
4, 0 ?
4, 0 ?
4, 1, 0 ?
1Figure 4: The state and action space of the learning agent.The goal state is reached when all items (that the user maybe interested in) have been presented.Once information has been presented to the user,it is committed or realised.
We again represent re-alised IUs in the agent?s state representation, so thatit can monitor its own output.
The goal of an MDPis to find an optimal policy pi?
according to whichthe agent receives the maximal possible reward foreach visited state.
We use the Q-Learning algorithm(Watkins, 1989) to learn an optimal policy accordingto pi?
(s) = argmaxa?A Q?
(s, a), where Q?
speci-fies the expected reward for executing action a instate s and then following policy pi?.5 Experimental Setting5.1 The State and Action SpaceThe agent?s state space needs to contain all infor-mation relevant for choosing an optimal IP strat-egy and an optimal sequence of incremental ac-tions.
Figure 4 shows the state and action spaceof our learning agent.
The states contain infor-mation on the incremental and presentation sta-tus of the system.
The variable ?incrementalSta-tus?
characterises situations in which a particular(incremental) action is triggered.
For example, aholdFloor is generated when the user has finishedspeaking, but the system has not yet finished itsdatabase lookup.
A correction is needed whenalso be the responsibility of a dialogue manager.the system has to modify already presented infor-mation (because the user changed their preferences)and a selfCorrection is needed when previouslypresented information is modified because the sys-tem made a mistake (in recognition or interpreta-tion).
The variable ?presStrategy?
indicates whethera strategy for IP has been chosen.
It is ?filled?
whenthis is the case, and ?unfilled?
otherwise.
The vari-ables representing the status of the cuisine, food, lo-cation, price and service indicate whether the slotis of interest to the user (0 means that the user doesnot care about it), and what input confidence score iscurrently associated with its value.
Once slots havebeen presented, they are realised and can only bechanged through a correction or self-correction.The variable ?userReaction?
shows the user?s re-action to an IP episode.
The user can select a restau-rant, provide more information to further constrainthe search or do something else.
The ?userSilence?variable indicates whether the user is speaking ornot.
This can be relevant for holding the floor orgenerating backchannels.
The action set comprisesIP actions, actions which enable us to learn the or-dering of slots, and actions which allow us to cap-ture incremental phenomena.
The complete state-action space size of this agent is roughly 3.2 mil-lion.
The agent reaches its goal state (defined w.r.t.the state variables in Figure 4) when an IP strategyhas been chosen and all relevant attributes have beenpresented.5.2 The Simulated EnvironmentSince a learning agent typically needs several thou-sand interactions to learn a reasonable policy, wetrain it in a simulated environment with two compo-nents.
The first one deals with different IP strategiesgenerally (not just for the incremental case), and thesecond one focuses on incrementally updated userinput hypothesis during the interaction.To learn a good IP strategy, we use a user simula-tion by Rieser et al (2010),2 which was estimatedfrom human data and uses bi-grams of the formP (au,t|IPs,t), where au,t is the predicted user re-action at time t to the system?s IP strategy IPs,t instate s at time t. We distinguish the user reactions of2The simulation data are available from http://www.classic-project.org/.53select a restaurant, addMoreInfo to the current queryto constrain the search, and other.
The last categoryis considered an undesired user reaction that the sys-tem should learn to avoid.
The simulation uses lin-ear smoothing to account for unseen situations.
Inthis way, we can then predict the most likely userreaction to each system action.While the IP strategies can be used for incremen-tal and non-incremental NLG, the second part of thesimulation deals explicitly with the dynamic envi-ronment updates during an interaction.
We assumethat for each restaurant recommendation, the userhas the option of filling any or all of the attributescuisine, food quality, location, price range and ser-vice quality.
The possible values of each attributeand possible confidence scores are shown in Table 2and denote the same as described in Section 5.1.At the beginning of a learning episode, we as-sign each attribute a possible value and confidencescore with equal probability.
For food and servicequality, we assume that the user is never interestedin bad food or service.
Subsequently, confidencescores can change at each time step.
(In future workthese transition probabilities will be estimated froma data collection, though the following assumptionsare realistic, based on our experience.)
We assumethat a confidence score of 0 changes to any othervalue with a likelihood of 0.05.
A confidence scoreof 1 changes with a probability of 0.3, a confidencescore of 2 with a probability of 0.1 and a confidencescore of 3 with a probability of 0.03.
Once slotshave been realised, their value is set to 4.
Theycannot be changed then without an explicit correc-tion.
We also assume that realised slots change witha probability of 0.1.
If they change, we assumethat half of the time, the user is the origin of thechange (because they changed their mind) and halfof the time the system is the origin of the change(because of an ASR or interpretation error).
Eachtime a confidence score is changed, it has a proba-bility of 0.5 to also change its value.
The resultinginput to the NLG system are data structures of theform present(cuisine=Indian), confidence=low.5.3 The Reward FunctionThe main trade-off to optimise for IP in an incre-mental setting is the timing and order of presenta-tion.
The agent has to decide whether to presentAttribute Values ConfidenceCuisine Chinese, French, German, In-, 0, 1, 2, 3, 4dian, Italian, Japanese, Mexi-can, Scottish, Spanish, ThaiFood bad, adequate, good, very good 0, 1, 2, 3, 4Location 7 distinct areas of the city 0, 1, 2, 3, 4Price cheap, expensive, good-price-for-value, very expensive 0, 1, 2, 3, 4Service bad, adequate, good, very good 0, 1, 2, 3, 4Table 2: User goal slots for restaurant queries with possi-ble values and confidence scores.information as soon as it becomes available or elsewait until confidence for input hypotheses is morestable.
Alternatively, it can reorder information toaccount for different confidence scores.
We assignthe following rewards3: +100 if the user selectsan item, 0 if the user adds more search constraints,?100 if the user does something else or the sys-tem needs to self-correct,?0.5 for holding the floor,and ?1 otherwise.
In addition, the agent receivesan increasing negative reward for the waiting time,waiting time2 (to the power of two), in terms of thenumber of time steps passed since the last item waspresented.
This reward is theoretically ??.
Theagent is thus penalised stronger the longer it delaysIP.
The rewards for user reactions are assigned at theend of each episode, all other rewards are assignedafter each time step.
One episode stretches from themoment that a user specifies their initial preferencesto the moment in which they choose a restaurant.The agent was trained for 10 thousand episodes.6 Experimental ResultsAfter training, the RL agent has learnt the followingincremental IP strategy.
It will present informationslots as soon as they become available if they havea medium or high confidence score.
The agent willthen order attributes so that those slots with the high-est confidence scores are presented first and slotswith lower confidence are presented later (by whichtime they may have achieved higher confidence).
Ifno information is known with medium or high con-3Handcrafted rewards are sufficient for this proof-of-concept study, and can be learned from data for future models(Rieser and Lemon, 2011).54101 102 103 104?100?80?60?40?20020406080100AverageRewardEpisodesRLBase1Base2Base3Figure 5: Performance in terms of rewards (averaged over10 runs) for the RL agent and its baselines.fidence, the agent will hold the floor or wait.
In thisway, it can prevent self-corrections and minimisewaiting time?both of which yield negative rewards.It can thus start speaking very early (avoiding longpauses or semantically empty utterances) and stillhas a low likelihood of having to self-correct.For a comparison of the learnt policy with possi-ble hand-crafted policies (because current incremen-tal NLG systems are rule-based), we designed threebaselines.
Baseline 1 always presents informationas soon as it is available, i.e.
never waits.
Base-line 2 always waits until all information is knownwith high confidence (i.e.
until all confidence scoresare 3).
Baseline 3 was chosen to be more ambi-tious.
It always presents information as soon aspossible, using a decreasing order of confidence tominimise self-corrections (i.e.
very similar to thelearnt policy).
It chooses randomly among slots withequal confidence.
All baseline policies have an op-timised IP strategy (recommend/summary etc.)
anddiffer only in their incremental processing strategies.Baseline 1 is most similar to the current approachused in spoken dialogue systems, where the Dia-logue Manager triggers an NLG component as soonas a task-relevant user utterance is processed.Here we do not compare the different strategiesfor IP generally because this has been done byRieser et al (2010), even if not for incrementalNLG.
Figure 5 shows the performance of all be-haviours in terms of average rewards.
Baseline 1obtains on average roughly 100 reward points lessthan the RL agent.
This corresponds to the (neg-ative) reward of one self-correction (?100).
Sinceinformation is always presented as soon as it is avail-able, this baseline needs to produce on average oneBaseline 1 (present information as soon asit becomes available)USR Please show me Italian places(cuisine = Indian;conf score = 0.2)SYS I have found 35 Indian .
.
.USR No, ITALIAN(cuisine = Italian; conf score = 0.8)SYS Oh, you meant Italian.
I have 104 Italian places in Edin-burgh.
24 in the city centre, 21 in the New Town, 14 in .
.
.USR Show me some in the city centre(location = centre; conf score = 0.6)SYS OK.
I found 24 Italian restaurants in the city centre .
.
.Baseline 2 (always wait until confidence is high)USR Do you have Italian restaurants in the centre of town?cuisine = Italian; conf score = 0.4location = centre; conf score = 0.2SYS waitsUSR Italian in the centre.cuisine = Italian, conf score = 0.7location = centre, conf score = 0.5SYS I have 104 Italian restaurants.USR waitsSYS waitsUSR city centre pleaselocation = centre, conf score = 0.7SYS I have 24 Italian restaurants in the city centre .
.
.Baseline 3 (present information in decreasingorder of confidence)USR I want Italian food .
.
.cuisine = Indian, conf score = 0.2location = centre, conf score = 0.3SYS hmm (holding turn) .
.
.USR in the centre of townlocation = centre, conf score = 0.9SYS In the centre, let me see, Indian .
.
.USR Italian, please.cuisine = Italian, conf score = 0.7SYS Oh I see.
I have 24 Italian places in the centre .
.
.Figure 6: Example dialogues generated with the baselinepolicies for a user who wants Italian food in the city cen-tre.
Confidence scores for cuisine and location variablesfor the restaurants are shown as updated.self-correction per episode.
Baseline 2 needs to waituntil all information is known with high confidenceand obtains on average 125 to 130 rewards less thanthe RL agent.
This corresponds to approximately11 time steps of waiting (for input to reach higherconfidence) before presentation since 11 is (approxi-mately) the square root of 130.
Baseline 3 is roughlya reward of ?10 worse than the RL agent?s be-55haviour, which is due to a combination of more self-corrections, even if they just occur occasionally, anda higher number of turn holding markers.
The latteris due to the baseline starting to present as soon aspossible, so that whenever all confidence scores aretoo low to start presenting, a turn holding markeris generated.
The learning agent learns to outper-form all baselines significantly, by presenting infor-mation slots in decreasing order of confidence, com-bined with waiting and holding the floor at appro-priate moments.
Anticipating the rewards for wait-ing vs. holding the floor at particular moments is themain reason that the learnt policy outperforms Base-line 3.
Subtle moments of timing as in this case aredifficult to hand-craft and more appropriately bal-anced using optimisation.
An absolute comparisonof the last 1000 episodes of each behaviour showsthat the improvement of the RL agent correspondsto 126.8% over Baseline 1, to 137.7% over Baseline2 and to 16.76% over Baseline 3.
All differences aresignificant at p < 0.001 according to a paired t-testand have a high effect size r > 0.9.
The high per-centage improvement of the learnt policy over Base-lines 1 and 2 is mainly due to the high numeric val-ues chosen for the rewards as can be observed fromtheir qualitative behaviour.
Thus, if the negative nu-meric values of, e.g., a self-correction were reduced,the percentage reward would reduce, but the pol-icy would not change qualitatively.
Figure 1 showssome examples of the learnt policy including severalincremental phenomena.
In contrast, Figure 6 showsexamples generated with the baselines.7 Conclusion and Future DirectionsWe have presented a novel framework combining in-cremental and statistical approaches to NLG for in-teractive systems.
In a proof-of-concept study in thedomain of Information Presentation, we optimisedthe timing and order of IP.
The learning agent op-timises the trade-off of whether to present informa-tion as soon as it becomes available (for high respon-siveness) or else to wait until input hypotheses weremore stable (to avoid self-corrections).
Results in asimulated environment showed that the agent learnsto avoid self-corrections and long waiting times, of-ten by presenting information in order of decreas-ing confidence.
It outperforms three hand-craftedbaselines due to its enhanced adaptivity.
In pre-vious work, incremental responsiveness has mainlybeen implemented by producing semantically emptyfillers such as um, let me see, well, etc.
(Skantze andHjalmarsson, 2010).
Our work avoids the need forthese fillers by content reordering.Since this paper has focused on a proof-of-concept study, our goal has not been to demonstratethe superiority of automatic optimisation over hand-crafted behaviour.
Previous studies have shownthe advantages of optimisation (Janarthanam andLemon, 2010; Rieser et al, 2010; Dethlefs et al,2011).
Rather, our main goal has been to demon-strate that incremental NLG can be phrased as an op-timisation problem and that reasonable action poli-cies can be learnt so that an application within anincremental framework is feasible.
This observationallows us to take incremental systems, which so farhave been restricted to deterministic decision mak-ing, one step further in terms of their adaptabilityand flexibility.
To demonstrate the effectiveness ofa synergy between RL and incremental NLG on alarge scale, we would like to train a fully incrementalNLG system from human data using a data-drivenreward function.
Further, an evaluation with humanusers will be required to verify the advantages of dif-ferent policies for Information Presentation.Regarding the scalability of our optimisationframework, RL systems are known to suffer from thecurse of dimensionality, the problem that their statespace grows exponentially according to the numberof variables taken into account.
While the appli-cation of flat RL is therefore limited to small-scaleproblems, we can use RL with a divide-and-conquerapproach, hierarchical RL, which has been shown toscale to large-scale NLG applications (Dethlefs andCuaya?huitl, 2011), to address complex NLG tasks.Future work can take several directions.
Cur-rently, we learn the agent?s behaviour offline, be-fore the interaction, and then execute it statistically.More adaptivity towards individual users and situ-ations could be achieved if the agent was able tolearn from ongoing interactions using online learn-ing.
In addition, current NLG systems tend to as-sume that the user?s goals and situational circum-stances are known with certainty.
This is often anunrealistic assumption that future work could ad-dress using POMDPs (Williams and Young, 2007).56AcknowledgementsThe research leading to this work has received fund-ing from EC?s FP7 programmes: (FP7/2011-14)under grant agreement no.
287615 (PARLANCE);(FP7/2007-13) under grant agreement no.
216594(CLASSiC); (FP7/2011-14) under grant agreementno.
270019 (SPACEBOOK); (FP7/2011-16) undergrant agreement no.
269427 (STAC).ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approach togeneration.
In Proc.
of EMNLP, pages 502?512.Timo Baumann and David Schlangen.
2011.
Predict-ing the Micro-Timing of User Input for an IncrementalSpoken Dialogue System that Completes a User?s On-going Turn.
In Proc.
of 12th Annual SIGdial Meetingon Discourse and Dialogue, Portland, OR.Timo Baumann, Okko Buss, and David Schlangen.
2011.Evaluation and Optimisation of Incremental Proces-sors.
Dialogue and Discourse, 2(1).Anja Belz.
2008.
Automatic Generation of WeatherForecast Texts Using Comprehensive ProbabilisticGeneration-Space Models.
Natural Language Engi-neering, 14(4):431?455.Okko Buss and David Schlangen.
2011.
DIUM?An In-cremental Dialogue Manager That Can Produce Self-Corrections.
In Proc.
of the Workshop on the Seman-tics and Pragmatics of Dialogue (SemDIAL / Los An-gelogue), Los Angeles, CA.Okko Buss, Timo Baumann, and David Schlangen.
2010.Collaborating on Utterances with a Spoken DialogueSysten Using an ISU-based Approach to IncrementalDialogue Management.
In Proc.
of 11th Annual SIG-dial Meeting on Discourse and Dialogue.Nina Dethlefs and Heriberto Cuaya?huitl.
2011.Combining Hierarchical Reinforcement Learning andBayesian Networks for Natural Language Generationin Situated Dialogue.
In Proc.
of the 13th EuropeanWorkshop on Natural Language Generation (ENLG),Nancy, France.Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.2011.
Optimising Natural Language Generation Deci-sion Making for Situated Dialogue.
In Proceedings ofthe 12th Annual Meeting on Discourse and Dialogue(SIGdial), Portland, Oregon, USA.David DeVault, Kenji Sagae, and David Traum.
2009.Can I finish?
Learning when to respond to incrementalinterpretation result in interactive dialogue.
In Proc.of the 10th Annual SigDial Meeting on Discourse andDialogue, Queen Mary University, UK.Srini Janarthanam and Oliver Lemon.
2010.
Learning toAdapt to Unknown Users: Referring Expression Gen-eration in Spoken Dialogue Systems.
In Proc.
of the48th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 69?78, July.Anne Kilger and Wolfgang Finkler.
1995.
Incremen-tal generation for real-time applications.
Technical re-port, DFKI Saarbruecken, Germany.Willem Levelt.
1989.
Speaking: From Intenion to Artic-ulation.
MIT Press.Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc??
?c?ek, SimonKeizer, Blaise Thomson, Kai Yu, and Steve Young.2010.
Phrase-based statistical language generation us-ing graphical models and active learning.
In Proc.
ofthe Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 1552?1561.Matthew Purver and Masayuki Otsuka.
2003.
Incremen-tal Generation by Incremental Parsing.
In Proceedingsof the 6th UK Special-Interesting Group for Computa-tional Linguistics (CLUK) Colloquium.Antoine Raux and Maxine Eskenazi.
2009.
A Finite-State Turn-Taking Model for Spoken Dialog Sys-tems.
In Proc.
of the 10th Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics?Human Language Technologies(NAACL-HLT), Boulder, Colorado.Verena Rieser and Oliver Lemon.
2011.
ReinforcementLearning for Adaptive Dialogue Systems: A Data-driven Methodology for Dialogue Management andNatural Language Generation.
Book Series: The-ory and Applications of Natural Language Processing,Springer, Berlin/Heidelberg.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising Information Presentation for Spoken Dia-logue Systems.
In Proc.
of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL),Uppsala, Sweden.David Schlangen and Gabriel Skantze.
2009.
A General,Abstract Model of Incremental Dialogue Processing.In Proc.
of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics,Athens, Greece.Gabriel Skantze and Anna Hjalmarsson.
2010.
TowardsIncremental Speech Generation in Dialogue Systems.In Proc.
of the 11th Annual SigDial Meeting on Dis-course and Dialogue, Tokyo, Japan.Gabriel Skantze and David Schlangen.
2009.
Incre-mental Dialogue Processing in a Micro-Domain.
InProc.
of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics,Athens, Greece.Amanda Stent, Rashmi Prasad, and Marilyn Walker.2004.
Trainable sentence planning for complex infor-mation presentation in spoken dialogue systems.
In57Proc.
of the Annual Meeting of the Association forComputational Linguistics.Kees van Deemter.
2009.
What game theory can do forNLG: the case of vague language.
In 12th EuropeanWorkshop on Natural Language Generation (ENLG).Marilyn Walker, Steve Whittaker, Amanda Stent, Pre-taam Maloor, Johanna Moore, and G Vasireddy.2004.
Generation and Evaluation of User Tailored Re-sponses in Multimodal Dialogue.
Cognitive Science,28(5):811?840.Chris Watkins.
1989.
Learning from Delayed Rewards.PhD Thesis, King?s College, Cambridge, UK.Jason Williams and Steve Young.
2007.
PartiallyObservable Markov Decision Processes for SpokenDialog Systems.
Computer Speech and Language,21(2):393?422.58
