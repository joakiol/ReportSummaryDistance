Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 115?120,Beijing, China, July 26-31, 2015.c?2015 ACL and AFNLPMulti-level Translation Quality Prediction with QUEST++Lucia Specia, Gustavo Henrique Paetzold and Carolina ScartonDepartment of Computer ScienceUniversity of Sheffield, UK{l.specia,ghpaetzold1,c.scarton}@sheffield.ac.ukAbstractThis paper presents QUEST++ , an opensource tool for quality estimation whichcan predict quality for texts at word, sen-tence and document level.
It also providespipelined processing, whereby predictionsmade at a lower level (e.g.
for words) canbe used as input to build models for pre-dictions at a higher level (e.g.
sentences).QUEST++ allows the extraction of a va-riety of features, and provides machinelearning algorithms to build and test qual-ity estimation models.
Results on recentdatasets show that QUEST++ achievesstate-of-the-art performance.1 IntroductionQuality Estimation (QE) of Machine Translation(MT) have become increasingly popular over thelast decade.
With the goal of providing a predic-tion on the quality of a machine translated text, QEsystems have the potential to make MT more use-ful in a number of scenarios, for example, improv-ing post-editing efficiency (Specia, 2011), select-ing high quality segments (Soricut and Echihabi,2010), selecting the best translation (Shah andSpecia, 2014), and highlighting words or phrasesthat need revision (Bach et al., 2011).Most recent work focuses on sentence-level QE.This variant is addressed as a supervised machinelearning task using a variety of algorithms to in-duce models from examples of sentence transla-tions annotated with quality labels (e.g.
1-5 likertscores).
Sentence-level QE has been covered inshared tasks organised by the Workshop on Statis-tical Machine Translation (WMT) annually since2012.
While standard algorithms can be used tobuild prediction models, key to this task is workof feature engineering.
Two open source featureextraction toolkits are available for that: ASIYA1and QUEST2(Specia et al., 2013).
The latter hasbeen used as the official baseline for the WMTshared tasks and extended by a number of partic-ipants, leading to improved results over the years(Callison-Burch et al., 2012; Bojar et al., 2013;Bojar et al., 2014).QE at other textual levels have received muchless attention.
Word-level QE (Blatz et al., 2004;Luong et al., 2014) is seemingly a more challeng-ing task where a quality label is to be producedfor each target word.
An additional challenge isthe acquisition of sizable training sets.
Althoughsignificant efforts have been made, there is con-siderable room for improvement.
In fact, mostWMT13-14 QE shared task submissions were un-able to beat a trivial baseline.Document-level QE consists in predicting a sin-gle label for entire documents, be it an absolutescore (Scarton and Specia, 2014) or a relativeranking of translations by one or more MT sys-tems (Soricut and Echihabi, 2010).
While certainsentences are perfect in isolation, their combina-tion in context may lead to an incoherent docu-ment.
Conversely, while a sentence can be poor inisolation, when put in context, it may benefit frominformation in surrounding sentences, leading toa good quality document.
Feature engineering isa challenge given the little availability of tools toextract discourse-wide information.
In addition,no datasets with human-created labels are avail-able and thus scores produced by automatic met-rics have to be used as approximation (Scarton etal., 2015).Some applications require fine-grained, word-level information on quality.
For example, onemay want to highlight words that need fixing.Document-level QE is needed particularly for gist-ing purposes where post-editing is not an option.1http://nlp.lsi.upc.edu/asiya/2http://www.quest.dcs.shef.ac.uk/115For example, for predictions on translations ofproduct reviews in order to decide whether or notthey are understandable by readers.
We believethat the limited progress in word and document-level QE research is partially due to lack of a basicframework that one can be build upon and extend.QUEST++ is a significantly refactored andexpanded version of an existing open sourcesentence-level toolkit, QUEST.
Feature extrac-tion modules for both word and document-levelQE were added and the three levels of predictionwere unified into a single pipeline, allowing for in-teractions between word, sentence and document-level QE.
For example, word-level predictions canbe used as features for sentence-level QE.
Finally,sequence-labelling learning algorithms for word-level QE were added.
QUEST++ can be easily ex-tended with new features at any textual level.
Thearchitecture of the system is described in Section2.
Its main component, the feature extractor, ispresented in Section 3.
Section 4 presents experi-ments using the framework with various datasets.2 ArchitectureQUEST++ has two main modules: a feature ex-traction module and a machine learning module.The first module is implemented in Java and pro-vides a number of feature extractors, as well asabstract classes for features, resources and pre-processing steps so that extractors for new fea-tures can be easily added.
The basic functioningof the feature extraction module requires raw textfiles with the source and translation texts, and afew resources (where available) such as the MTsource training corpus and source and target lan-guage models (LMs).
Configuration files are usedto indicate paths for resources and the features thatshould be extracted.
For its main resources (e.g.LMs), if a resource is missing, QUEST++ can gen-erate it automatically.Figure 1 depicts the architecture of QUEST++ .Document and Paragraph classes are used fordocument-level feature extraction.
A Document isa group of Paragraphs, which in turn is a groupof Sentences.
Sentence is used for both word- andsentence-level feature extraction.
A Feature Pro-cessing Module was created for each level.
Eachprocessing level is independent and can deal withthe peculiarities of its type of feature.Machine learning QUEST++ provides scriptsto interface the Python toolkit scikit-learn3(Pedregosa et al., ).
This module is indepen-dent from the feature extraction code and usesthe extracted feature sets to build and test QEmodels.
The module can be configured to rundifferent regression and classification algorithms,feature selection methods and grid search forhyper-parameter optimisation.
Algorithms fromscikit-learn can be easily integrated bymodifying existing scripts.For word-level prediction, QUEST++ providesan interface for CRFSuite (Okazaki, 2007), a se-quence labelling C++ library for Conditional Ran-dom Fields (CRF).
One can configure CRFSuitetraining settings, produce models and test them.3 FeaturesFeatures in QUEST++ can be extracted from eithersource or target (or both) sides of the corpus at agiven textual level.
In order describe the featuressupported, we denote:?
S and T the source and target documents,?
s and t for source and target sentences, and?
s and t for source and target words.We concentrate on MT system-independent(black-box) features, which are extracted based onthe output of the MT system rather than any ofits internal representations.
These allow for moreflexible experiments and comparisons across MTsystems.
System-dependent features can be ex-tracted as long they are represented using a pre-defined XML scheme.
Most of the existing fea-tures are either language-independent or dependon linguistic resources such as POS taggers.
Thelatter can be extracted for any language, as longas the resource is available.
For a pipelined ap-proach, predictions at a given level can becomefeatures for higher level model, e.g.
features basedon word-level predictions for sentence-level QE.3.1 Word levelWe explore a range of features from recent work(Bicici and Way, 2014; Camargo de Souza et al.,2014; Luong et al., 2014; Wisniewski et al., 2014),totalling 40 features of seven types:Target context These are features that explorethe context of the target word.
Given a word tiin position i of a target sentence, we extract: ti,3http://scikit-learn.org/116Figure 1: Architecture of QUEST++i.e., the word itself, bigrams ti?1tiand titi+1, andtrigrams ti?2ti?1ti, ti?1titi+1and titi+1ti+2.Alignment context These features explore theword alignment between source and target sen-tences.
They require the 1-to-N alignment be-tween source and target sentences to be provided.Given a word tiin position i of a target sentenceand a word sjaligned to it in position j of a sourcesentence, the features are: the aligned word sjit-self, target-source bigrams sj?1tiand tisj+1, andsource-target bigrams ti?2sj, ti?1sj, sjti+1andsjti+2.Lexical These features explore POS informa-tion on the source and target words.
Giventhe POS tag Ptiof word tiin position i of atarget sentence and the POS tag Psjof wordsjaligned to it in position j of a source sen-tence, we extract: the POS tags Ptiand Psjthemselves, the bigrams Pti?1Ptiand PtiPti+1and trigrams Pti?2Pti?1Pti, Pti?1PtiPti+1andPtiPti+1Pti+2.
Four binary features are also ex-tracted with value 1 if tiis a stop word, punctua-tion symbol, proper noun or numeral.LM These features are related to the n-gram fre-quencies of a word?s context with respect to an LM(Raybaud et al., 2011).
Six features are extracted:lexical and syntactic backoff behavior, as well aslexical and syntactic longest preceding n-gram forboth a target word and an aligned source word.Given a word tiin position i of a target sentence,the lexical backoff behavior is calculated as:f (ti) =????????????????????????
?7 if ti?2, ti?1, tiexists6 if ti?2, ti?1and ti?1, tiexist5 if only ti?1, tiexists4 if ti?2, ti?1and tiexist3 if ti?1and tiexist2 if tiexists1 if tiis out of the vocabularyThe syntactic backoff behavior is calculated inan analogous fashion: it verifies for the existenceof n-grams of POS tags in a POS-tagged LM.
ThePOS tags of target sentence are produced by theStanford Parser4(integrated in QUEST++ ).Syntactic QUEST++ provides one syntactic fea-ture that proved very promising in previous work:the Null Link (Xiong et al., 2010).
It is a binaryfeature that receives value 1 if a given word tiina target sentence has at least one dependency linkwith another word tj, and 0 otherwise.
The Stan-ford Parser is used for dependency parsing.Semantic These features explore the polysemyof target and source words, i.e.
the number ofsenses existing as entries in a WordNet for a giventarget word tior a source word si.
We employthe Universal WordNet,5which provides access toWordNets of various languages.4http://nlp.stanford.edu/software/lex-parser.shtml5http://www.lexvo.org/uwn/117Pseudo-reference This binary feature exploresthe similarity between the target sentence and atranslation for the source sentence produced by an-other MT system.
The feature is 1 if the givenword tiin position i of a target sentence S is alsopresent in a pseudo-reference translation R. In ourexperiments, the pseudo-reference is produced byMoses systems trained over parallel corpora.3.2 Sentence levelSentence-level QE features have been extensivelyexplored and described in previous work.
Thenumber of QUEST++ features varies from 80 to123 depending on the language pair.
The completelist is given as part of QUEST++ ?s documentation.Some examples are:?
number of tokens in s & t and their ratio,?
LM probability of s & t,?
ratio of punctuation symbols in s & t,?
ratio of percentage of numbers, content-/non-content words, nouns/verbs/etc in s & t,?
proportion of dependency relations between(aligned) constituents in s & t,?
difference in depth of syntactic trees of s & t.In our experiments, we use the set of 80 fea-tures, as these can be extracted for all languagepairs of our datasets.3.3 Document levelOur document-level features follow from those inthe work of (Wong and Kit, 2012) on MT evalua-tion and (Scarton and Specia, 2014) for document-level QE.
Nine features are extracted, in additionto aggregated values of sentence-level features forthe entire document:?
content words/lemmas/nouns repetition inS/T ,?
ratio of content words/lemmas/nouns in S/T ,4 ExperimentsIn what follows, we evaluate QUEST++?s perfor-mance for the three prediction levels and variousdatasets.4.1 Word-level QEDatasets We use five word-level QE datasets:the WMT14 English-Spanish, Spanish-English,English-German and German-English datasets,and the WMT15 English-Spanish dataset.Metrics For the WMT14 data, we evaluate per-formance in the three official classification tasks:?
Binary: A Good/Bad label, where Bad indi-cates the need for editing the token.?
Level 1: A Good/Accuracy/Fluency label,specifying the coarser level categories of er-rors for each token, or Good for tokens withno error.?
Multi-Class: One of 16 labels specifying theerror type for the token (mistranslation, miss-ing word, etc.
).The evaluation metric is the average F-1 of allbut the Good class.
For the WMT15 dataset, weconsider only the Binary classification task, sincethe dataset does not provide other annotations.Settings For all datasets, the models weretrained with the CRF module in QUEST++ .
Whilefor the WMT14 German-English dataset we usethe Passive Aggressive learning algorithm, for theremaining datasets, we use the Adaptive Reg-ularization of Weight Vector (AROW) learning.Through experimentation, we found that this setupto be the most effective.
The hyper-parameters foreach model were optimised through 10-fold crossvalidation.
The baseline is the majority class inthe training data, i.e.
a system that always pre-dicts ?Unintelligible?
for Multi-Class, ?Fluency?for Level 1, and ?Bad?
for the Binary setup.Results The F-1 scores for the WMT14 datasetsare given in Tables 1?4, for QUEST++ and sys-tems that oficially participated in the task.
The re-sults show that QUEST++ was able to outperformall participating systems in WMT14 except for theEnglish-Spanish baseline in the Binary and Level1 tasks.
The results in Table 5 also highlight theimportance of selecting an adequate learning al-gorithm in CRF models.System Binary Level 1 MulticlassQUEST++ 0.502 0.392 0.227Baseline 0.525 0.404 0.222LIG/BL 0.441 0.317 0.204LIG/FS 0.444 0.317 0.204FBK-1 0.487 0.372 0.170FBK-2 0.426 0.385 0.230LIMSI 0.473 ?
?RTM-1 0.350 0.299 0.268RTM-2 0.328 0.266 0.032Table 1: F-1 for the WMT14 English-Spanish task4.2 Pipeline for sentence-level QEHere we evaluate the pipeline of using word-levelpredictions as features for sentence-level QE.118System Binary Level 1 MulticlassQUEST++ 0.386 0.267 0.161Baseline 0.299 0.151 0.019RTM-1 0.269 0.219 0.087RTM-2 0.291 0.239 0.081Table 2: F-1 for the WMT14 Spanish-English taskSystem Binary Level 1 MulticlassQUEST++ 0.507 0.287 0.161Baseline 0.445 0.117 0.086RTM-1 0.452 0.211 0.150RTM-2 0.369 0.219 0.124Table 3: F-1 for the WMT14 English-German taskDataset We use the WMT15 dataset for word-level QE.
The split between training and test setswas modified to allow for more sentences for train-ing the sentence-level QE model.
The 2000 lastsentences of the original training set were usedas test along with the original 1000 dev set sen-tences.
Therefore, word predictions were gener-ated for 3000 sentences, which were later split in2000 sentences for training and 1000 sentences fortesting the sentence-level model.Features The 17 QUEST++ baseline featuresare used alone (Baseline) and in combination withfour word-level prediction features:?
count & proportion of Good words,?
count & proportion of Bad words.Oracle word level labels, as given in the originaldataset, are also used in a separate experiment tostudy the potential of this pipelined approach.Settings For learning sentence-level models, theSVR algorithm with RBF kernel and hyperparam-eters optimised via grid search in QUEST++ isused.
Evaluation is done using MAE (Mean Ab-solute Error) as metric.Results As shown in Table 6, the use of word-level predictions as features led to no improve-ment.
However, the use of the oracle word-levellabels as features substantially improved the re-sults, lowering the baseline error by half.
We notethat the method used in this experiments is thesame as that in Section 4.1, but with fewer in-stances for training the word-level models.
Im-System Binary Level 1 MulticlassQUEST++ 0.401 0.230 0.079Baseline 0.365 0.149 0.069RTM-1 0.261 0.082 0.023RTM-2 0.229 0.085 0.030Table 4: F-1 for the WMT14 German-English taskAlgorithm BinaryAROW 0.379PA 0.352LBFGS 0.001L2SGD 0.000AP 0.000Table 5: F-1 for the WMT15 English-Spanish taskproving word-level prediction could thus lead tobetter results in the pipeline for sentence-level QE.MAEBaseline 0.159Baseline+Predicted 0.158Baseline+Oracle 0.07Table 6: MAE values for sentence-level QE4.3 Pipeline for document-level QEHere we evaluate the pipeline of using sentence-level predictions as features for QE of documents.Dataset For training the sentence-level model,we use the English-Spanish WMT13 training setfor sentence-level QE.
For the document-levelmodel, we use English-Spanish WMT13 datafrom the translation shared task.
We mixed theoutputs of all MT systems, leading to 934 trans-lated documents.
560 randomly selected docu-ments were used for training and 374 for test-ing.
As quality labels, for sentence-level trainingwe consider both the HTER and the Likert labelsavailable.
For document-level prediction, BLEU,TER and METEOR are used as quality labels (notas features), given the lack of human-target qualitylabels for document-level prediction.Features The 17 QUEST++ baseline featuresare aggregated to produce document-level fea-tures (Baseline).
These are then combined withdocument-level features (Section 3.3) and finallywith features from sentence-level predictions:?
maximum/minimum predicted HTER or Lik-ert score,?
average predicted HTER or Likert score,?
Median, first quartile and third quartile pre-dicted HTER or Likert score.Oracle sentence labels are not possible as theydo not exist for the test set documents.Settings For training and evaluation, we use thesame settings as for sentence-level.Results Table 7 shows the results in terms ofMAE.
The best result was achieved with the119baseline plus HTER features, but no significantimprovements over the baseline were observed.Document-level prediction is a very challengingtask: automatic metric scores used as labels donot seem to reliably distinguish translations of dif-ferent source documents, since they were primar-ily designed to compare alternative translations forthe same source document.BLEU TER METEORBaseline 0.049 0.050 0.055Baseline+Doc-level 0.053 0.057 0.055Baseline+HTER 0.053 0.048 0.054Baseline+Likert 0.054 0.056 0.054Baseline+Doc-level+HTER 0.053 0.054 0.054Baseline+Doc-level+Likert 0.053 0.056 0.054Table 7: MAE values for document-level QE5 RemarksThe source code for the framework, the datasetsand extra resources can be downloaded fromhttps://github.com/ghpaetzold/questplusplus.The license for the Java code, Python andshell scripts is BSD, a permissive license withno restrictions on the use or extensions of thesoftware for any purposes, including commer-cial.
For pre-existing code and resources, e.g.,scikit-learn, their licenses apply.AcknowledgmentsThis work was supported by the European Associ-ation for Machine Translation, the QT21 project(H2020 No.
645452) and the EXPERT project(EU Marie Curie ITN No.
317471).ReferencesN.
Bach, F. Huang, and Y. Al-Onaizan.
2011.
Good-ness: a method for measuring MT confidence.
InACL11.E.
Bicici and A.
Way.
2014.
Referential Transla-tion Machines for Predicting Translation Quality.
InWMT14.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,C.
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.2004.
Confidence Estimation for Machine Transla-tion.
In COLING04.O.
Bojar, C. Buck, C. Callison-Burch, C. Federmann,B.
Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,and L. Specia.
2013.
Findings of the 2013 Work-shop on SMT.
In WMT13.O.
Bojar, C. Buck, C. Federmann, B. Haddow,P.
Koehn, J.
Leveling, C. Monz, P. Pecina, M. Post,H.
Saint-Amand, R. Soricut, L. Specia, and A. Tam-chyna.
2014.
Findings of the 2014 Workshop onSMT.
In WMT14.C.
Callison-Burch, P. Koehn, C. Monz, M. Post,R.
Soricut, and L. Specia.
2012.
Findings of the2012 Workshop on SMT.
In WMT12.J.
G. Camargo de Souza, J. Gonz?alez-Rubio, C. Buck,M.
Turchi, and M. Negri.
2014.
FBK-UPV-UEdin participation in the WMT14 Quality Estima-tion shared-task.
In WMT14.N.
Q. Luong, L. Besacier, and B. Lecouteux.
2014.LIG System for Word Level QE task.
In WMT14.N.
Okazaki.
2007.
CRFsuite: a fast implementationof Conditional Random Fields.
http://www.chokkan.org/software/crfsuite/.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
Scikit-learn: Machine learning inPython.
Journal of Machine Learning Research, 12.S.
Raybaud, D. Langlois, and K. Smali.
2011.
Thissentence is wrong.
Detecting errors in machine-translated sentences.
Machine Translation, 25(1).C.
Scarton and L. Specia.
2014.
Document-level trans-lation quality estimation: exploring discourse andpseudo-references.
In EAMT14.C.
Scarton, M. Zampieri, M. Vela, J. van Genabith, andL.
Specia.
2015.
Searching for Context: a Studyon Document-Level Labels for Translation QualityEstimation.
In EAMT15.K.
Shah and L. Specia.
2014.
Quality estimation fortranslation selection.
In EAMT14.R.
Soricut and A. Echihabi.
2010.
Trustrank: Induc-ing trust in automatic translations via ranking.
InACL10.L.
Specia, K. Shah, J. G. C. de Souza, and T. Cohn.2013.
Quest - a translation quality estimation frame-work.
In ACL13.L.
Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
InEAMT11.G.
Wisniewski, N. Pcheux, A. Allauzen, and F. Yvon.2014.
LIMSI Submission for WMT?14 QE Task.
InWMT14.B.
T. M. Wong and C. Kit.
2012.
Extending machinetranslation evaluation metrics with lexical cohesionto document level.
In EMNLP/CONLL.D.
Xiong, M. Zhang, and H. Li.
2010.
Error detectionfor SMT using linguistic features.
In ACL10.120
