Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 151?158,Prague, June 2007. c?2007 Association for Computational LinguisticsA Compositional Approach toward Dynamic Phrasal ThesaurusAtsushi Fujita Shuhei Kato Naoki Kato Satoshi SatoGraduate School of Engineering, Nagoya University{fujita,ssato}@nuee.nagoya-u.ac.jp{shuhei,naoki}@sslab.nuee.nagoya-u.ac.jpAbstractTo enhance the technology for computingsemantic equivalence, we introduce the no-tion of phrasal thesaurus which is a naturalextension of conventional word-based the-saurus.
Among a variety of phrases thatconveys the same meaning, i.e., paraphrases,we focus on syntactic variants that are com-positionally explainable using a small num-ber of atomic knowledge, and develop a sys-tem which dynamically generates such vari-ants.
This paper describes the proposed sys-tem and three sorts of knowledge developedfor dynamic phrasal thesaurus in Japanese:(i) transformation pattern, (ii) generationfunction, and (iii) lexical function.1 IntroductionLinguistic expressions that convey the same mean-ing are called paraphrases.
Handling paraphrasesis one of the key issues in a broad range of nat-ural language processing tasks, including machinetranslation, information retrieval, information ex-traction, question answering, summarization, textmining, and natural language generation.Conventional approaches to computing semanticequivalence between two expressions are five-fold.The first approximates it based on the similaritiesbetween their constituent words.
If two words be-long to closer nodes in a thesaurus or semantic net-work, they are considered more likely to be similar.The second uses the family of tree kernels (Collinsand Duffy, 2001; Takahashi, 2005).
The degree ofequivalence of two trees (sentences) is defined asthe number of common subtrees included in bothtrees.
The third estimates the equivalence based onword alignment composed using templates or trans-lation probabilities derived from a set of paralleltext (Barzilay and Lee, 2003; Brockett and Dolan,2005).
The fourth espouses the distributional hy-pothesis (Harris, 1968): given two words are likelyto be equivalent if distributions of their surroundingwords are similar (Lin and Pantel, 2001; Weeds etal., 2005).
The final regards two expressions equiva-lent if they can be associated by using a set of lexico-syntactic paraphrase patterns (Mel?c?uk, 1996; Dras,1999; Yoshikane et al, 1999; Takahashi, 2005).Despite the results previous work has achieved,no system that robustly recognizes and generatesparaphrases is established.
We are not convinced ofa hypothesis underlying the word-based approachesbecause the structure of words also conveys somemeaning.
Even tree kernels, which take structuresinto account, do not have a mechanism for iden-tifying typical equivalents: e.g., dative alternationand passivization, and abilities to generate para-phrases.
Contrary to the theoretical basis, the twolines of corpus-based approaches have problems inpractice, i.e., data sparseness and computation cost.The pattern-based approaches seem steadiest.
Yetno complete resource or methodology for handlinga wide variety of paraphrases has been developed.On the basis of this recognition, we introduce thenotion of phrasal thesaurus to directly compute se-mantic equivalence of phrases such as follows.
(1) a. be in our favor / be favorable for usb.
its reproducibility / if it is reproduciblec.
decrease sharply / show a sharp decreased.
investigate the cause of a fire /investigate why there was a fire /investigate what started a fire /make an investigation into the cause of a fire151Phrasal thesaurus is a natural extension of conven-tional word-based thesaurus.
It is thus promised thatit will bring us the following benefits:Enhancement of NLP applications: As conven-tional thesauri, phrasal thesaurus must beuseful to handle paraphrases having differentstructures in a wide range of NLP applications.Reading and writing aids: Showing more appro-priate alternative phrases must be a power-ful aid at certain situations such as writingtext.
Controlling readability of text by alteringphrases must also be beneficial to readers.Our aim is to develop resources and mechanismsfor computing semantic equivalence on the workinghypothesis that phrase is the appropriate unit for thatpurpose.
This paper describes the first version of ourparaphrase generation system and reports on our on-going work on constructing resources for realizingphrasal thesaurus.The following sections describe the range of phe-nomena we treat (Section 2), the overall architec-ture of our paraphrase generation system whichfunctions as phrasal thesaurus (Section 3), the im-plementation of knowledge bases (Section 4) fol-lowed by discussion (Section 5), and conclusion(Section 6).2 Dynamic phrasal thesaurus2.1 IssueToward realizing phrasal thesaurus, the followingtwo issues should be discussed.?
What sorts of phrases should be treated?
How to cope with a variety of expressionsAlthough technologies of shallow parsing havebeen dramatically improved in the last decade, itis still difficult to represent arbitrary expression inlogical form.
We therefore think it is reasonable todefine the range relying on lexico-syntactic struc-ture instead of using particular semantic representa-tion.
According to the work of (Chklovski and Pan-tel, 2004; Torisawa, 2006), predicate phrase (sim-ple sentence) is a reasonable unit because it approx-imately corresponds to the meaning of single event.Combination of words and a variety of construc-tion coerce us into handling an enormous numberof expressions than word-based approaches.
Onemay think taking phrase is like treading a thornypath because one of the arguments in Section 1 isabout coverage.
On this issue, we speculate thatone of the feasible approach to realize a robust sys-tem is to divide phenomena into compositional andnon-compositional (idiosyncratic) ones1, and sepa-rately develop resources to handle them as describedin (Fujita and Inui, 2005).To compute semantic equivalence of idiosyncraticparaphrases, pairs or groups of paraphrases have tobe statically compiled into a dictionary as word-based thesaurus.
The corpus-based approach is valu-able for that purpose, although they are not guaran-teed to collect all idiosyncratic paraphrases.
On theother hand, compositional paraphrases can be cap-tured by a relatively small number of rules.
Thus itseems tolerable approach to generate them dynam-ically by applying such rules.
Our work is targetedat compositional paraphrases and the system can becalled dynamic phrasal thesaurus.
Hereafter, werefer to paraphrases that are likely to be explainedcompositionally as syntactic variants.2.2 Target language: JapaneseWhile the discussion above does not depend on par-ticular language, our implementation of dynamicphrasal thesaurus is targeted at Japanese.
Sev-eral methods for paraphrasing Japanese predicatephrases have been proposed (Kondo et al, 1999;Kondo et al, 2001; Kaji et al, 2002; Fujita et al,2005).
The range they treat is, however, relativelynarrow because they tend to focus on particular para-phrase phenomena or to rely on existing resources.On the other hand, we define the range of phenom-ena from a top-down viewpoint.
As a concrete defi-nition of predicate phrase in Japanese,noun phrase + case marker + predicateis employed which is hereafter referred to ?phrase.
?Noun phrase and predicate in Japanese them-selves subcategorize various syntactic variants asshown in Figure 1 and paraphrase phenomena forabove phrase also involve those focused on their in-teraction.
Thus the range of phenomena is not sonarrow, and intriguing ones, such as shown in exam-ples2 (2) and (3), are included.1We regard lexical paraphrases (e.g., ?scope?
?
?range?
)and idiomatic paraphrases (e.g., ?get the sack??
?be dismissedfrom employment?)
as idiosyncratic.2In each example, ?s?
and ?t?
denote an original sentenceand its paraphrase, respectively.
SMALLCAPS strings indicatethe syntactic role of their corresponding Japanese expressions.
[N] indicates a nominalizer.152(2) Head switchings.
kakunin-o isogu.checking-ACC to hurry-PRESWe hurry checking it.t.
isoide kakunin-suru.in a hurry to check-PRESWe check it in a hurry.
(3) Noun phrase ?
sub-clauses.
kekka-no saigensei-o kenshou-suru.result-GEN reproducibility-ACC to validate-PRESWe validate its reproducibility.t.
[ kekka-o saigen-dekiru ]result-ACC to reproduce-to be ableka-douka-o kenshou-suru.
[N]-whether-ACC to validate-PRESWe validate whether it is reproducible.We focus on syntactic variants at least one side ofwhich is subcategorized into the definition of phraseabove.
For the sake of simplicity, we hereafter rep-resent those expressions using part-of-speech (POS)patterns.
For instance, (2s) is called N : C : V type,and (3s) is N1: no : N2: C : V type.3 Paraphrase generation systemGiven a phrase, the proposed system generates itssyntactic variants in the following four steps:1.
Morphological analysis2.
Syntactic transformation3.
Surface generation with lexical choice4.
SLM-based filteringwhere no particular domain, occasion, and media isassumed3.
Candidates of syntactic variants are firstover-generated in step 2 and then anomalies amongthem are filtered out in steps 3 and 4 using rule-basedlexical choice and statistical language model.The rest of this section elaborates on each compo-nent in turn.3.1 Morphological analysisTechnologies of morphological analysis in Japanesehave matured by introducing machine learning tech-niques and large-scale annotated corpus, and thereare freely available tools.
Since the structure of inputphrase is assumed to be quite simple, employment ofdependency analyzer was put off.
We simply use amorphological analyzer MeCab4.3This corresponds to the linguistic transformation layer ofKURA (Takahashi et al, 2001).4http://mecab.sourceforge.net/noun phrase8>>>>>>><>>>>>>>:formal noun8<:?koto??mono?
?no?content8>>>><>>>>:single wordcompoundjN1N2N + suffixesmodified8><>:N1+ ?no?
+N2Adj+NAdjectival verb+Nclause+Npredicate8>>>>>>>>>><>>>>>>>>>>:verb phrase8>>>>><>>>>>:single word8>><>>:original verbSino-Japanese verblexical compoundlight verbAdv+ ?suru?compound8><>:original + originalSino + originalSino + SinoN + SinoAdjjsingle wordcompoundAdjectival verb+ ?da?Adv+ ?da?CopulaFigure 1: Classification of syntactic variants of nounphrase and predicate in Japanese.Our system has a post-analysis processing.
If ei-ther of Sino-Japanese verbal nouns (e.g., ?kenshou(validation)?
and ?kandou (impression)?)
or translit-eration of verbs in foreign language (e.g., ?doraibu(to drive)?
and ?shifuto (to shift)?)
is immediatelyfollowed by ?suru (to do)?
or ?dekiru (to be able),?these adjacent two morphemes are joined into a sin-gle morpheme to avoid incorrect transformation.3.2 Syntactic transformationThe second step over-generates syntactic variantsusing the following three sorts of knowledge:(i) Transformation pattern: It gives skeletons ofsyntactic variants.
Each variant is representedby POS symbols designating the input con-stituents and triggers of the generation functionand lexical function below.
(ii) Generation function: It enumerates differentexpressions that are constituted with the sameset of words and subcategorized into the re-quired syntactic category.
Some of generationfunctions handle base phrases, while the restgenerates functional words.
Base phrases theformer generates are smaller than that transfor-mation patterns treat.
Since some functionalwords are disjunctive, the latter generates allcandidates with a separator ?/?
and leaves theselection to the following step.153Table 1: Grammar in Backus-Naur form, example, and instantiation for each knowledge.Knowledge type Grammar / Example / Instantiation(i) Transformation <transformation pattern> ::= <left pattern> ?
<right pattern>pattern <left pattern> ::= (<POS symbol>|<word form>)+<POS symbol> ::= (N |C|V |Adj|Adv)<word form> ::= (<hiragana>|<katakana>|<kanji>)+<right pattern> ::=(<POS symbol>|<word form>|<function definition>|<lexical function>)+(a) N : C : V ?
adv(V ) : vp(N)(b) N1: no : N2: C : V ?N1: genCase() : vp(N2) : ka-douka : C : V(a) kakunin : o : isogu ?
adv (isogu) : vp(kakunin)checking ACC to hurry adv(to hurry) vp(checking)(b) kekka : no : saigensei : o : kenshou-sururesult GEN reproducibility ACC to validate-PRES?
kekka : genCase() : vp(saigensei) : ka-douka : o : kenshou-sururesult case marker vp(reproducibility) [N]-whether ACC to validate-PRES(ii) Generation <generation function> ::= <function definition> ?
?
{?<right pattern>+?
}?function <function definition> ::= <syntactic category>?
(?<POS symbol>*?
)?<syntactic category> ::= (np | vp | lvc)(a) vp(N) ?
{v(N) : genVoice() : genTense()}(b) np(N1, N2) ?
{N1, N2, N1: N2, N1: no : N2, vp(N1) : N2,wh(N2) : vp(N1) : ka, .
.
.
}(a) vp(kakunin) ?
{ v(kakunin) : genVoice() : genTense() }vp(verification) v(verification) verbal suffix for voice verbal suffix for tense(b) np(shukka, gen-in)np(starting fire, reason)?
{ shukka , gen-in , shukka : gen-in , shukka : no : gen-in ,starting fire reason starting fire reason starting fire GEN reasonvp(shukka) : gen-in , wh(gen-in) : vp(shukka) : ka , .
.
.
}vp(starting fire) reason wh(reason) vp(starting fire) [N](iii) Lexical <lexical function> ::= <relation>?
(?<POS symbol>?
)?function <relation> ::= (n | v | adj | adjv | adv | wh)(a) adv(V )(b) wh(N)(a) adv (isogu)adv(to hurry)??
isoidein a hurry(given by a verb?adverb dictionary)?
(b) wh(gen-in)wh(reason)??
{ naze , doushite }why why(given by a noun?interrogative dictionary)?
(iii) Lexical function: It generates different lexi-cal items in certain semantic relations, suchas derivative form, from a given lexical item.The back-end of this knowledge is a setof pre-compiled dictionaries as described inSection 4.2.Table 1 gives a summary of grammar in Backus-Naur form, examples, and instantiations of eachknowledge.
Figure 2 illustrates an example ofknowledge application flow for transforming (4s)into (4t), where ?:?
denotes delimiter of con-stituents.
(4) s. ?kakunin:o:isogu?t.
?isoide:{kakunin-suru:{?, reru/rareru, seru/saseru}:{?, ta/da}}?First, transformation patterns that match to the giveninput are applied.
Then, the skeletons of syntacticvariants given by the pattern are lexicalized by con-secutively invoking generation functions and lexicalfunctions.
Plural number of expressions that gen-eration function and lexical function generate areenumerated within curly brackets.
Transformationis ended when the skeletons are fully lexicalized.In fact, knowledge design for realizing the trans-formation is not really new, because we have beeninspired by the previous pattern-based approaches.Transformation pattern is thus alike that in theMeaning-Text Theory (MTT) (Mel?c?uk, 1996), Syn-chronous Tree Adjoining Grammar (STAG) (Dras,1999), meta-rule for Fastr (Yoshikane et al, 1999),154{v(kakunin) : genVoice() : genTense()}okakuninN:C: isoguVTrans.
Pat.N:C:V?
adv(V):vp(N)adv(isogu) : vp(kakunin)Gen. Func.vp(N)kakunin-suruLex.
Func.v(N)Gen. Func.genVoice()Gen. Func.genTense()isoideLex.
Func.adv(V){?, reru/rareru, seru/saseru} {?, ta/da}isoide : {kakunin-suru : {?, reru/rareru, seru/saseru} : {?, ta/da}}Figure 2: Syntactic transformation (for (2)).and transfer pattern for KURA (Takahashi et al,2001).
Lexical function is also alike that in MTT.However, our aim in this research is beyond thedesign.
In other words, as described in Section 1,we are aiming at the following two: to develop re-sources for handling syntactic variants in Japanese,and to confirm if phrasal thesaurus really contributeto computing semantic equivalence.3.3 Surface generation with lexical choiceThe input of the third component is a bunch of candi-date phrases such as shown in (4t).
This componentdoes the following three processes in turn:Step 1.
Unfolding: All word sequences are gener-ated by removing curly brackets one by one.Step 2.
Lexical choice: Disjunctive words are con-catenated with ?/?
(e.g., ?reru/rareru?
in (4t)).One of them is selected based on POS and con-jugation types of the preceding word.Step 3.
Conjugation: In the transformation step,conjugative words are moved to different po-sitions and some of them are newly generated.Inappropriate conjugation forms are corrected.3.4 SLM-based filteringIn the final step, we assess the correctness of eachcandidate of syntactic variants using a statistical lan-guage model.
Our model simply rejects candidatephrases that never appear in a large size of raw textcorpus consisting of 15 years of newspaper articles(Mainichi 1991?2005, approximately 1.8GB).
Al-though it is said that Japanese language has a degreeN:C:VN1:N2:C:V+NN:C:V1:V2+VN:C:Adv:V+AdvAdj:N:C:V+AdjN:C:Adjswitch V with AdjFigure 3: Derivations of phrase types.of freedom in word ordering, current implementa-tion does not yet employ structured language modelsbecause phrases we handle are simple.4 Knowledge implementation4.1 Transformation patterns and generationfunctionsAn issue of developing resources is how to ensuretheir coverage.
Our approach to this issue is to de-scribe transformation patterns by extending those forsimpler phrases.
We first described following threepatterns for N : C : V type phrases which we con-sider the simplest according to Figure 1.
(5) a. N : C : V ?
vp(N)b. N : C : V ?
N : genCase() : lvc(V )c. N : C : V ?
adv(V ) : vp(N)While the pattern (5c) is induced from example (2),the patterns (5a-b) are derived from examples (6)and (7), respectively.
(6) s. shigeki-o ukeruinspiration-ACC to receiveto receive an inspirationt.
shigeki-sareruto inspire-PASSto be inspired(7) s. hada-o shigeki-suruskin-ACC to stimulateto stimulate skint.
hada-ni shigeki-o ataeruskin-DAT stimulus-ACC to giveto give skin a stimulusRegarding the patterns in (8) as the entire set ofcompositional paraphrases for N : C : V typephrases, we then extended them to a bit more com-plex phrases as in Figure 3.
For instance, 10 patterns155Table 2: Transformation patterns.Target phrase # of patternsN : C : V 3N1: N2: C : V 10N : C : V1: V210N : C : Adv : V 7Adj : N : C : V 4N : C : Adj 3Total 37Table 3: Generation functions.Definition Syntactic category # of returned valuenp(N1, N2) noun phrase 9vp(N) verb phrase 1vp(N1, N2) verb phrase 2vp(V1, V2) verb phrase 3lvc(V ) light verb construction 1genCase() case marker 4genVoice() verbal suffix for voice 3genTense() verbal suffix for tense 2genAspect () verbal suffix for aspect 2for N1: N2: C : V type phrases shown in (8) havebeen described based on patterns in (5), mainly fo-cusing on interactions between newly introduced N1and other constituents.
(8) a. N1: N2: C : V ?
vp(N1, N2) (5a)b. N1: N2: C : V ?N1: genCase() : vp(N2) (5a)c. N1: N2: C : V ?N2: genCase() : vp(N1) (5a)d. N1: N2: C : V ?np(N1, N2) : genCase() : lvc(V ) (5b)e. N1: N2: C : V ?
N1: genCase() :N2: genCase() : lvc(V ) (5b)f. N1: N2: C : V ?
N2: genCase() :N1: genCase() : lvc(V ) (5b)g. N1: N2: C : V ?adv (V ) : vp(N1, N2) (5c)h. N1: N2: C : V ?adv (V ) : N1: genCase() : vp(N2) (5c)i. N1: N2: C : V ?adv (V ) : N2: genCase() : vp(N1) (5c)j. N1: N2: C : V ?np(N1, N2) : C : V (new)The number of transformation patterns we have sofar developed is shown in Table 2.Generation functions shown in Table 3 are devel-oped along with creating transformation patterns.Although this is the heart of the proposed model,two problems are remained: (i) the granularity ofeach generation function is determined according toTable 4: Dictionaries for lexical functions.ID POS-pair |D| |C| |D ?
C| |J |(a) noun?verb 3,431 - 3,431 3,431(b) noun?adjective 308 667 906 475 ?
(c) noun?adjectival verb 1,579 - 1,579 1,579(d) noun?adverb 271 - 271 271(e) verb?adjective 252 - 252 192 ?
(f) verb?adjectival verb 74 - 74 68 ?
(g) verb?adverb 74 - 74 64 ?
(h) adjective?adjectival verb 66 95 159 146 ?
(i) adjective?adverb 33 - 33 26 ?
(j) adjectival verb?adverb 70 - 70 70Total 6,158 762 6,849 6,322our linguistic intuition, and (ii) they do not ensure ofgenerating all possible phrases.
Therefore, we haveto establish the methodology to create this knowl-edge more precisely.4.2 Lexical functionsExcept wh(N), which generates interrogatives asshown in the bottom line of Table 1, the relationswe have so far implemented are lexical derivations.These roughly correspond to S, V, A, and Adv inMTT.
The back-end of these lexical functions is aset of dictionaries built by the following two steps:Step 1.
Automatic candidate collection: Mostderivatives in Japanese share the beginningof words and are characterized by the corre-spondences of their suffixes.
For example,?amai (be sweet)?
and ?amami (sweetness)?has a typical suffix correspondence ?
?-i:?-mi?of adjective?noun derivation.
Using this clue,candidates are collected by two methods.?
From dictionary: Retrieve all word pairs fromthe given set of words those satisfying thefollowing four conditions: (i) beginning withkanji character, (ii) having different POSs,(iii) sharing at least the first character and thefirst sound, and (iv) having a suffix patternwhich corresponds to at least two pairs.?
Using dictionary and corpus: Generate candi-dates from a set of words by applying a set oftypical suffix patterns, and then check if eachcandidate is an actual word using corpus.
Thisis based on (Langkilde and Knight, 1998).Step 2.
Manual selection: The set of word pairscollected in the previous step includes those donot have particular semantic relationship.
Thisstep involves human to discard noises.156Table 4 shows the size of 10 dictionaries, whereeach column denotes the number of word pairs re-trieved from IPADIC5 (|D|), those using IPADIC,seven patterns and the same corpus as in Section 3.4(|C|), their union (|D ?
C|), and those manu-ally judged correct (|J |), respectively.
The sets ofword pairs J are used as bi-directional lexical func-tions, although manual screening for four dictionar-ies without dagger (?)
are still in process.5 Discussion5.1 Unit of processingThe working hypothesis underlying our work is thatphrase is the appropriate unit for computing seman-tic equivalence.
In addition to the arguments inSection 1, the hypothesis is supported by what isdone in practice.
Let us see two related fields.The first is the task of word sense disambigua-tion (WSD).
State-of-the-art WSD techniques referto context as a clue.
However, the range of contextis usually not so wide: words and their POSs withinsmall window centered the target word and contentwords within the same sentence of the target word.The task therefore can be viewed as determining themeaning of phrase based on its constituent wordsand surrounding content words.Statistical language model (SLM) is another field.SLMs usually deal with various things within wordsequence (or structure) at the same time.
How-ever, relations within a phrase should be differen-tiated from that between phrases, because checkingthe former is for grammaticality, while the latter forcohesion.
We think SLMs should take the phrase todetermine boundaries for assessing the correctnessof generated expressions more accurately.5.2 CompositionalityWe examined how large part of manually createdparaphrases could be generated in our compositionalapproach.
First, a set of paraphrase examples werecreated in the following procedure:Step 1.
Most frequent 400 phrases typed N1: N2:C : V were sampled from one year of newspa-per articles (Mainichi 1991).Step 2.
An annotator produced paraphrases for eachphrase.
We allowed to record more than one5http://mecab.sourceforge.jp/paraphrase for a given phrase and to give upproducing paraphrases.
As a result, we ob-tained 211 paraphrases for 170 input phrases.Manual classification revealed that 42% (88 / 211)of paraphrases could be compositionally explain-able, and the (theoretical) coverage increases to 86%(182 / 211) if we have a synonym dictionary.
Thisratio is enough high to give these phenomena pref-erence as the research target, although we cannot re-ject a possibility that data has been biased.5.3 Sufficient condition of equivalenceIn our system, transformation patterns and genera-tion functions offer necessary conditions for gener-ating syntactic variants for given input.
However,we have no sufficient condition to control the appli-cation of such a knowledge.It has not been thoroughly clarified what clue canbe sufficient condition to ensure semantic equiva-lence, even in a number of previous work.
Though,at least, roles of participants in the event have to bepreserved by some means, such as the way presentedin (Pantel et al, 2007).
Kaji et al (2002) introduceda method of case frame alignment in paraphrase gen-eration.
In the model, arguments of main verb in thesource are taken over by that of the target accordingto the similarities between arguments of the sourceand target.
Fujita et al (2005) employed a semanticrepresentation of verb to realize the alignment of therole of participants governed by the source and tar-get verbs.
According to an empirical experiment in(Fujita et al, 2005), statistical language models donot contribute to calculating semantic equivalence,but to filtering out anomalies.
We therefore plan toincorporate above alignment-based models into oursystem, for example, within or after the syntactictransformation step (Figure 2).5.4 Ideas for improvementThe knowledge and system presented in Section 3are quite simple.
Thus the following features shouldbe incorporated to improve the system in addition tothe one described in Section 5.3.?
Dependency structure: To enable flexiblematching, such as Adv : N : C : V type inputand transformation pattern for N : C : Adv :V type phrases.?
Sophisticated SLM: The generation phaseshould also take the structure into account to157evaluate generated expressions flexibly.?
Knowledge development: Although we havenot done intrinsic evaluation of knowledge, weare aware of its incompleteness.
We are con-tinuing manual screening for the dictionariesand planning to enhance the methodology ofknowledge development.6 ConclusionTo enhance the technology for computing seman-tic equivalence, we have introduced the notion ofphrasal thesaurus, which is a natural extension ofconventional word-based thesaurus.
Plausibility oftaking phrase as the unit of processing has been dis-cussed from several viewpoints.
On the basis ofthat, we have been developing a system to dynam-ically generate syntactic variants in Japanese predi-cate phrases which utilizes three sorts of knowledgethat are inspired by MTT, STAG, Fastr, and KURA.Future work includes implementing more precisefeatures and larger resources to compute semanticequivalence.
We also plan to conduct an empiricalevaluation of the resources and the overall system.AcknowledgmentsThis work was supported in part by MEXT Grants-in-Aid for Young Scientists (B) 18700143, and forScientific Research (A) 16200009, Japan.ReferencesRegina Barzilay and Lillian Lee.
2003.
Learning to paraphrase:an unsupervised approach using multiple-sequence align-ment.
In Proceedings of the 2003 Human Language Tech-nology Conference and the North American Chapter of theAssociation for Computational Linguistics (HLT-NAACL),pages 16?23.Chris Brockett and William B. Dolan.
2005.
Support VectorMachines for paraphrase identification and corpus construc-tion.
In Proceedings of the 3rd International Workshop onParaphrasing (IWP), pages 1?8.Timothy Chklovski and Patrick Pantel.
2004.
VerbOcean: min-ing the Web for fine-grained semantic verb relations.
In Pro-ceedings of the 2004 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 33?40.Michael Collins and Nigel Duffy.
2001.
Convolution kernelsfor natural language.
In Advances in Neural InformationProcessing Systems 14: Proceedings of the 2001 Confer-ence, pages 625?632.Mark Dras.
1999.
Tree adjoining grammar and the reluctantparaphrasing of text.
Ph.D. thesis, Division of Informationand Communication Science, Macquarie University.Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto.
2005.
Ex-ploiting Lexical Conceptual Structure for paraphrase gener-ation.
In Proceedings of the 2nd International Joint Con-ference on Natural Language Processing (IJCNLP), pages908?919.Atsushi Fujita and Kentaro Inui.
2005.
A class-oriented ap-proach to building a paraphrase corpus.
In Proceedingsof the 3rd International Workshop on Paraphrasing (IWP),pages 25?32.Zellig Harris.
1968.
Mathematical structures of language.New York: Interscience Publishers.Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi, andSatoshi Sato.
2002.
Verb paraphrase based on case framealignment.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL), pages215?222.Keiko Kondo, Satoshi Sato, and Manabu Okumura.
1999.Paraphrasing of ?sahen-noun + suru?.
IPSJ Journal,40(11):4064?4074.
(in Japanese).Keiko Kondo, Satoshi Sato, and Manabu Okumura.
2001.Paraphrasing by case alternation.
IPSJ Journal, 42(3):465?477.
(in Japanese).Irene Langkilde and Kevin Knight.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In Proceedings ofthe 36th Annual Meeting of the Association for Computa-tional Linguistics and the 17th International Conference onComputational Linguistics (COLING-ACL), pages 704?710.Dekang Lin and Patrick Pantel.
2001.
Discovery of inferencerules for question answering.
Natural Language Engineer-ing, 7(4):343?360.Igor Mel?c?uk.
1996.
Lexical functions: a tool for the descrip-tion of lexical relations in a lexicon.
In Leo Wanner, editor,Lexical Functions in Lexicography and Natural LanguageProcessing, pages 37?102.
John Benjamin Publishing Com-pany.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, TimothyChklovski, and Eduard Hovy.
2007.
Isp: Learning infer-ential selectional preferences.
In Proceedings of HumanLanguage Technologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computational Lin-guistics (NAACL-HLT), pages 564?571.Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, Atsushi Fujita,and Kentaro Inui.
2001.
KURA: a transfer-based lexico-structural paraphrasing engine.
In Proceedings of the 6thNatural Language Processing Pacific Rim Symposium (NL-PRS) Workshop on Automatic Paraphrasing: Theories andApplications, pages 37?46.Tetsuro Takahashi.
2005.
Computation of semantic equiva-lence for question answering.
Ph.D. thesis, Graduate Schoolof Information Science, Nara Institute of Science and Tech-nology.Kentaro Torisawa.
2006.
Acquiring inference rules with tem-poral constraints by using Japanese coordinated sentencesand noun-verb co-occurrences.
In Proceedings of the Hu-man Language Technology Conference of the North Ameri-can Chapter of the Association for Computational Linguis-tics (HLT-NAACL), pages 57?64.Julie Weeds, David Weir, and Bill Keller.
2005.
The distribu-tional similarity of sub-parses.
In Proceedings of the ACLWorkshop on Empirical Modeling of Semantic Equivalenceand Entailment, pages 7?12.Fuyuki Yoshikane, Keita Tsuji, Kyo Kageura, and ChristianJacquemin.
1999.
Detecting Japanese term variation in tex-tual corpus.
In Proceedings of the 4th International Work-shop on Information Retrieval with Asian Languages, pages97?108.158
