Proceedings of ACL-08: HLT, pages 879?887,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAnalyzing the Errors of Unsupervised LearningPercy Liang Dan KleinComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{pliang,klein}@cs.berkeley.eduAbstractWe identify four types of errors that unsu-pervised induction systems make and studyeach one in turn.
Our contributions include(1) using a meta-model to analyze the incor-rect biases of a model in a systematic way,(2) providing an efficient and robust methodof measuring distance between two parametersettings of a model, and (3) showing that lo-cal optima issues which typically plague EMcan be somewhat alleviated by increasing thenumber of training examples.
We conductour analyses on three models: the HMM, thePCFG, and a simple dependency model.1 IntroductionThe unsupervised induction of linguistic structurefrom raw text is an important problem both for un-derstanding language acquisition and for buildinglanguage processing systems such as parsers fromlimited resources.
Early work on inducing gram-mars via EM encountered two serious obstacles: theinappropriateness of the likelihood objective and thetendency of EM to get stuck in local optima.
With-out additional constraints on bracketing (Pereira andShabes, 1992) or on allowable rewrite rules (Carrolland Charniak, 1992), unsupervised grammar learn-ing was ineffective.Since then, there has been a large body of workaddressing the flaws of the EM-based approach.Syntactic models empirically more learnable thanPCFGs have been developed (Clark, 2001; Kleinand Manning, 2004).
Smith and Eisner (2005) pro-posed a new objective function; Smith and Eis-ner (2006) introduced a new training procedure.Bayesian approaches can also improve performance(Goldwater and Griffiths, 2007; Johnson, 2007;Kurihara and Sato, 2006).Though these methods have improved inductionaccuracy, at the core they all still involve optimizingnon-convex objective functions related to the like-lihood of some model, and thus are not completelyimmune to the difficulties associated with early ap-proaches.
It is therefore important to better under-stand the behavior of unsupervised induction sys-tems in general.In this paper, we take a step back and presenta more statistical view of unsupervised learning inthe context of grammar induction.
We identify fourtypes of error that a system can make: approxima-tion, identifiability, estimation, and optimization er-rors (see Figure 1).
We try to isolate each one in turnand study its properties.Approximation error is caused by a mis-matchbetween the likelihood objective optimized by EMand the true relationship between sentences and theirsyntactic structures.
Our key idea for understand-ing this mis-match is to ?cheat?
and initialize EMwith the true relationship and then study the waysin which EM repurposes our desired syntactic struc-tures to increase likelihood.
We present a meta-model of the changes that EM makes and show howthis tool can shed some light on the undesired biasesof the HMM, the PCFG, and the dependency modelwith valence (Klein and Manning, 2004).Identifiability error can be incurred when two dis-tinct parameter settings yield the same probabil-ity distribution over sentences.
One type of non-identifiability present in HMMs and PCFGs is labelsymmetry, which even makes computing a mean-ingful distance between parameters NP-hard.
Wepresent a method to obtain lower and upper boundson such a distance.Estimation error arises from having too few train-ing examples, and optimization error stems from879EM getting stuck in local optima.
While it is to beexpected that estimation error should decrease as theamount of data increases, we show that optimizationerror can also decrease.
We present striking experi-ments showing that if our data actually comes fromthe model family we are learning with, we can some-times recover the true parameters by simply run-ning EM without clever initialization.
This resultruns counter to the conventional attitude that EM isdoomed to local optima; it suggests that increasingthe amount of data might be an effective way to par-tially combat local optima.2 Unsupervised modelsLet x denote an input sentence and y denote the un-observed desired output (e.g., a parse tree).
We con-sider a model family P = {p?
(x,y) : ?
?
?}.
Forexample, if P is the set of all PCFGs, then the pa-rameters ?
would specify all the rule probabilities ofa particular grammar.
We sometimes use ?
and p?interchangeably to simplify notation.
In this paper,we analyze the following three model families:In the HMM, the input x is a sequence of wordsand the output y is the corresponding sequence ofpart-of-speech tags.In the PCFG, the input x is a sequence of POStags and the output y is a binary parse tree with yieldx.
We represent y as a multiset of binary rewrites ofthe form (y ?
y1 y2), where y is a nonterminal andy1, y2 can be either nonterminals or terminals.In the dependency model with valence (DMV)(Klein and Manning, 2004), the input x =(x1, .
.
.
, xm) is a sequence of POS tags and the out-put y specifies the directed links of a projective de-pendency tree.
The generative model is as follows:for each head xi, we generate an independent se-quence of arguments to the left and to the right froma direction-dependent distribution over tags.
At eachpoint, we stop with a probability parametrized by thedirection and whether any arguments have alreadybeen generated in that direction.
See Klein and Man-ning (2004) for a formal description.In all our experiments, we used the Wall StreetJournal (WSJ) portion of the Penn Treebank.
We bi-narized the PCFG trees and created gold dependencytrees according to the Collins head rules.
We trained45-state HMMs on all 49208 sentences, 11-statePCFGs on WSJ-10 (7424 sentences) and DMVson WSJ-20 (25523 sentences) (Klein and Manning,2004).
We ran EM for 100 iterations with the pa-rameters initialized uniformly (always plus a smallamount of random noise).
We evaluated the HMMand PCFG by mapping model states to Treebanktags to maximize accuracy.3 Decomposition of errorsNow we will describe the four types of errors (Fig-ure 1) more formally.
Let p?
(x,y) denote the distri-bution which governs the true relationship betweenthe input x and output y.
In general, p?
does notlive in our model family P .
We are presented witha set of n unlabeled examples x(1), .
.
.
,x(n) drawni.i.d.
from the true p?.
In unsupervised induction,our goal is to approximate p?
by some model p?
?
Pin terms of strong generative capacity.
A standardapproach is to use the EM algorithm to optimizethe empirical likelihood E?
log p?
(x).1 However, EMonly finds a local maximum, which we denote ?
?EM,so there is a discrepancy between what we get (p?
?EM)and what we want (p?
).We will define this discrepancy later, but for now,it suffices to remark that the discrepancy dependson the distribution over y whereas learning dependsonly on the distribution over x.
This is an importantproperty that distinguishes unsupervised inductionfrom more standard supervised learning or densityestimation scenarios.Now let us walk through the four types of er-ror bottom up.
First, ?
?EM, the local maximumfound by EM, is in general different from ??
?argmax?
E?
log p?
(x), any global maximum, whichwe could find given unlimited computational re-sources.
Optimization error refers to the discrep-ancy between ??
and ?
?EM.Second, our training data is only a noisy sam-ple from the true p?.
If we had infinite data, wewould choose an optimal parameter setting under themodel, ?
?2 ?
argmax?
E log p?
(x), where now theexpectation E is taken with respect to the true p?
in-stead of the training data.
The discrepancy between?
?2 and ??
is the estimation error.Note that ?
?2 might not be unique.
Let ?
?1 denote1Here, the expectation E?f(x) def= 1nPni=1 f(x(i)) denotesaveraging some function f over the training data.880p?
= true modelApproximation error (Section 4)?
?1 = Best(argmax?
E log p?
(x))Identifiability error (Section 5)?
?2 ?
argmax?
E log p?
(x)Estimation error (Section 6)??
?
argmax?
E?
log p?
(x)Optimization error (Section 7)?
?EM= EM(E?
log p?
(x)) PFigure 1: The discrepancy between what we get (?
?EM)and what we want (p?)
can be decomposed into four typesof errors.
The box represents our model family P , whichis the set of possible parametrized distributions we canrepresent.
Best(S) returns the ?
?
S which has the small-est discrepancy with p?.the maximizer of E log p?
(x) that has the smallestdiscrepancy with p?.
Since ?
?1 and ?
?2 have the samevalue under the objective function, we would not beable to choose ?
?1 over ?
?2, even with infinite data orunlimited computation.
Identifiability error refers tothe discrepancy between ?
?1 and ?
?2.Finally, the model family P has fundamental lim-itations.
Approximation error refers to the discrep-ancy between p?
and p?
?1 .
Note that ?
?1 is not nec-essarily the best in P .
If we had labeled data, wecould find a parameter setting in P which is closerto p?
by optimizing joint likelihood E log p?
(x,y)(generative training) or even conditional likelihoodE log p?
(y | x) (discriminative training).In the remaining sections, we try to study each ofthe four errors in isolation.
In practice, since it isdifficult to work with some of the parameter settingsthat participate in the error decomposition, we usecomputationally feasible surrogates so that the errorunder study remains the dominant effect.4 Approximation errorWe start by analyzing approximation error, the dis-crepancy between p?
and p?
?1 (the model found byoptimizing likelihood), a point which has been dis-20 40 60 80 100iteration-18.4-18.0-17.6-17.2-16.7log-likelihood20 40 60 80 100iteration0.20.40.60.81.0LabeledF 1Figure 2: For the PCFG, when we initialize EM with thesupervised estimate ?
?gen, the likelihood increases but theaccuracy decreases.cussed by many authors (Merialdo, 1994; Smith andEisner, 2005; Haghighi and Klein, 2006).2To confront the question of specifically howthe likelihood diverges from prediction accuracy,we perform the following experiment: we ini-tialize EM with the supervised estimate3 ?
?gen =argmax?
E?
log p?
(x,y), which acts as a surrogatefor p?.
As we run EM, the likelihood increases butthe accuracy decreases (Figure 2 shows this trendfor the PCFG; the HMM and DMV models behavesimilarly).
We believe that the initial iterations ofEM contain valuable information about the incor-rect biases of these models.
However, EM is chang-ing hundreds of thousands of parameters at once in anon-trivial way, so we need a way of characterizingthe important changes.One broad observation we can make is that thefirst iteration of EM reinforces the systematic mis-takes of the supervised initializer.
In the first E-step,the posterior counts that are computed summarizethe predictions of the supervised system.
If thesematch the empirical counts, then the M-step does notchange the parameters.
But if the supervised systempredicts too many JJs, for example, then the M-stepwill update the parameters to reinforce this bias.4.1 A meta-model for analyzing EMWe would like to go further and characterize thespecific changes EM makes.
An initial approach isto find the parameters that changed the most dur-ing the first iteration (weighted by the correspond-2Here, we think of discrepancy between p and p?
as the errorincurred when using p?
for prediction on examples generatedfrom p; in symbols, E(x,y)?ploss(y, argmaxy?
p?(y?
| x)).3For all our models, the supervised estimate is solved inclosed form by taking ratios of counts.881ing expected counts computed in the E-step).
Forthe HMM, the three most changed parameters arethe transitions 2:DT?8:JJ, START?0:NNP, and8:JJ?3:NN.4 If we delve deeper, we can see that2:DT?3:NN (the parameter with the 10th largestchange) fell and 2:DT?8:JJ rose.
After checkingwith a few examples, we can then deduce that somenouns were retagged as adjectives.
Unfortunately,this type of ad-hoc reasoning requires considerablemanual effort and is rather subjective.Instead, we propose using a general meta-modelto analyze the changes EM makes in an automaticand objective way.
Instead of treating parameters asthe primary object of study, we look at predictionsmade by the model and study how they change overtime.
While a model is a distribution over sentences,a meta-model is a distribution over how the predic-tions of the model change.Let R(y) denote the set of parts of a predic-tion y that we are interested in tracking.
Each part(c, l) ?
R(y) consists of a configuration c and a lo-cation l. For a PCFG, we define a configuration tobe a rewrite rule (e.g., c = PP?IN NP), and a loca-tion l = [i, k, j] to be a span [i, j] split at k, wherethe rewrite c is applied.In this work, each configuration is associated witha parameter of the model, but in general, a configu-ration could be a larger unit such as a subtree, allow-ing one to track more complex changes.
The size ofa configuration governs how much the meta-modelgeneralizes from individual examples.Let y(i,t) denote the model prediction on the i-thtraining example after t iterations of EM.
To sim-plify notation, we write Rt = R(y(i,t)).
The meta-model explains how Rt became Rt+1.5In general, we expect a part in Rt+1 to be ex-plained by a part in Rt that has a similar locationand furthermore, we expect the locations of the twoparts to be related in some consistent way.
The meta-model uses two notions to formalize this idea: a dis-tance d(l, l?)
and a relation r(l, l?).
For the PCFG,d(l, l?)
is the number of positions among i,j,k thatare the same as the corresponding ones in l?, andr((i, k, j), (i?, k?, j?))
= (sign(i ?
i?
), sign(j ?4Here 2:DT means state 2 of the HMM, which was greedilymapped to DT.5If the same part appears in both Rt and Rt+1, we removeit from both sets.j?
), sign(k ?
k?))
is one of 33 values.
We define amigration as a triple (c, c?, r(l, l?
)); this is the unit ofchange we want to extract from the meta-model.Our meta-model provides the following genera-tive story of how Rt becomes Rt+1: each new part(c?, l?)
?
Rt+1 chooses an old part (c, l) ?
Rt withsome probability that depends on (1) the distance be-tween the locations l and l?
and (2) the likelihood ofthe particular migration.
Formally:pmeta(Rt+1 | Rt) =?(c?,l?)?Rt+1?(c,l)?RtZ?1l?
e??d(l,l?)p(c?
| c, r(l, l?
)),where Zl =?
(c,l)?Rt e??d(l,l?)
is a normalizationconstant, and ?
is a hyperparameter controlling thepossibility of distant migrations (set to 3 in our ex-periments).We learn the parameters of the meta-model withan EM algorithm similar to the one for IBM model1.
Fortunately, the likelihood objective is convex, sowe need not worry about local optima.4.2 Results of the meta-modelWe used our meta-model to analyze the approxima-tion errors of the HMM, DMV, and PCFG.
For thesemodels, we initialized EM with the supervised es-timate ?
?gen and collected the model predictions asEM ran.
We then trained the meta-model on the pre-dictions between successive iterations.
The meta-model gives us an expected count for each migra-tion.
Figure 3 lists the migrations with the highestexpected counts.From these migrations, we can see that EM triesto explain x better by making the corresponding ymore regular.
In fact, many of the HMM migra-tions on the first iteration attempt to resolve incon-sistencies in gold tags.
For example, noun adjuncts(e.g., stock-index), tagged as both nouns and adjec-tives in the Treebank, tend to become consolidatedunder adjectives, as captured by migration (B).
EMalso re-purposes under-utilized states to better cap-ture distributional similarities.
For example, state 24has migrated to state 40 (N), both of which are nowdominated by proper nouns.
State 40 initially con-tained only #, but was quickly overrun with distribu-tionally similar proper nouns such as Oct. and Chap-ter, which also precede numbers, just as # does.882Iteration 0?1(A) START 4:NN24:NNP(B) 4:NN8:JJ 4:NN(C) 24:NNP 24:NNP36:NNPSIteration 1?2(D) 4:NN8:JJ 4:NN(E) START 4:NN24:NNP(F) 8:JJ11:RB 27:TOIteration 2?3(G) 24:NNP8:JJ U.S.(H) 24:NNP8:JJ 4:NN(I) 3:DT 24:NNP8:JJIteration 3?4(J) 11:RB32:RP up(K) 24:NNP8:JJ U.S.(L) 19:, 11:RB32:RPIteration 4?5(M) 24:NNP34:$ 15:CD(N) 2:IN 24:NNP40:NNP(O) 11:RB32:RP down(a) Top HMM migrations.
Example: migration (D) means a NN?NN transition is replaced by JJ?NN.Iteration 0?1 Iteration 1?2 Iteration 2?3 Iteration 3?4 Iteration 4?5(A) DT NN NN (D) NNP NNP NNP (G) DT JJ NNS (J) DT JJ NN (M) POS JJ NN(B) JJ NN NN (E) NNP NNP NNP (H) MD RB VB (K) DT NNP NN (N) NNS RB VBP(C) NNP NNP (F) DT NNP NNP (I) VBP RB VB (L) PRP$ JJ NN (O) NNS RB VBD(b) Top DMV migrations.
Example: migration (A) means a DT attaches to the closer NN.Iteration 0?1 Iteration 1?2 Iteration 2?3 Iteration 3?4 Iteration 4?5(A) RB 1:VP4:SRB 1:VP1:VP(D) NNP 0:NP0:NPNNP NNP0:NP(G) DT 0:NP0:NPDT NN0:NP(J) TO VB1:VPTO VB2:PP(M) CD NN0:NPCD NN3:ADJP(B) 0:NP 2:PP0:NP1:VP 2:PP1:VP(E) VBN 2:PP1:VP1:VP 2:PP1:VP(H) 0:NP 1:VP4:S0:NP 1:VP4:S(K) MD 1:VP1:VPMD VB1:VP(N) VBD 0:NP1:VPVBD 3:ADJP1:VP(C) VBZ 0:NP1:VPVBZ 0:NP1:VP(F) 0:NP 1:VP4:S0:NP 1:VP4:S(I) TO VB1:VPTO VB2:PP(L) NNP NNP0:NPNNP NNP6:NP(O) 0:NP NN0:NP0:NP NN0:NP(c) Top PCFG migrations.
Example: migration (D) means a NP?NNPNP rewrite is replaced by NP?NNPNNP,where the new NNP right child spans less than the old NP right child.Figure 3: We show the prominent migrations that occur during the first 5 iterations of EM for the HMM, DMV, andPCFG, as recovered by our meta-model.
We sort the migrations across each iteration by their expected counts underthe meta-model and show the top 3.
Iteration 0 corresponds to the correct outputs.
Blue indicates the new iteration,red indicates the old.DMV migrations also try to regularize model pre-dictions, but in a different way?in terms of thenumber of arguments.
Because the stop probabilityis different for adjacent and non-adjacent arguments,it is statistically much cheaper to generate one argu-ment rather than two or more.
For example, if wetrain a DMV on only DT JJ NN, it can fit the dataperfectly by using a chain of single arguments, butperfect fit is not possible if NN generates both DTand JJ (which is the desired structure); this explainsmigration (J).
Indeed, we observed that the varianceof the number of arguments decreases with more EMiterations (for NN, from 1.38 to 0.41).In general, low-entropy conditional distributionsare preferred.
Migration (H) explains how adverbsnow consistently attach to verbs rather than modals.After a few iterations, the modal has committeditself to generating exactly one verb to the right,which is statistically advantageous because theremust be a verb after a modal, while the adverb is op-tional.
This leaves the verb to generate the adverb.The PCFG migrations regularize categories in amanner similar to the HMM, but with the addedcomplexity of changing bracketing structures.
Forexample, sentential adverbs are re-analyzed as VPadverbs (A).
Sometimes, multiple migrations ex-plain the same phenomenon.6 For example, migra-tions (B) and (C) indicate that PPs that previouslyattached to NPs are now raised to the verbal level.Tree rotation is another common phenomenon, lead-ing to many left-branching structures (D,G,H).
Themigrations that happen during one iteration can alsotrigger additional migrations in the next.
For exam-ple, the raising of the PP (B,C) inspires more of the6We could consolidate these migrations by using larger con-figurations, but at the risk of decreased generalization.883same raising (E).
As another example, migration (I)regularizes TO VB infinitival clauses into PPs, andthis momentum carries over to the next iteration witheven greater force (J).In summary, the meta-model facilitates our anal-yses by automatically identifying the broad trends.We believe that the central idea of modeling the er-rors of a system is a powerful one which can be usedto analyze a wide range of models, both supervisedand unsupervised.5 Identifiability errorWhile approximation error is incurred when likeli-hood diverges from accuracy, identifiability error isconcerned with the case where likelihood is indiffer-ent to accuracy.We say a set of parameters S is identifiable (interms of x) if p?
(x) 6= p??
(x) for every ?, ??
?
Swhere ?
6= ?
?.7 In general, identifiability error isincurred when the set of maximizers of E log p?
(x)is non-identifiable.8Label symmetry is perhaps the most familiar ex-ample of non-identifiability and is intrinsic to mod-els with hidden labels (HMM and PCFG, but notDMV).
We can permute the hidden labels withoutchanging the objective function or even the natureof the solution, so there is no reason to prefer onepermutation over another.
While seemingly benign,this symmetry actually presents a serious challengein measuring discrepancy (Section 5.1).Grenager et al (2005) augments an HMM to al-low emission from a generic stopword distribution atany position with probability q.
Their model woulddefinitely not be identifiable if q were a free param-eter, since we can set q to 0 and just mix in the stop-word distribution with each of the other emissiondistributions to obtain a different parameter settingyielding the same overall distribution.
This is a casewhere our notion of desired structure is absent in thelikelihood, and a prior over parameters could helpbreak ties.7For our three model families, ?
is identifiable in terms of(x,y), but not in terms of x alone.8We emphasize that non-identifiability is in terms of x, sotwo parameter settings could still induce the same marginal dis-tribution on x (weak generative capacity) while having differentjoint distributions on (x,y) (strong generative capacity).
Recallthat discrepancy depends on the latter.The above non-identifiabilities apply to all param-eter settings, but another type of non-identifiabilityconcerns only the maximizers of E log p?(x).
Sup-pose the true data comes from a K-state HMM.
Ifwe attempt to fit an HMM with K + 1 states, wecan split any one of the K states and maintain thesame distribution on x.
Or, if we learn a PCFG onthe same HMM data, then we can choose either theleft- or right-branching chain structures, which bothmimic the true HMM equally well.5.1 Permutation-invariant distanceKL-divergence is a natural measure of discrepancybetween two distributions, but it is somewhat non-trivial to compute?for our three recursive models, itrequires solving fixed point equations, and becomescompletely intractable in face of label symmetry.Thus we propose a more manageable alternative:d?(?
|| ??
)def=?j ?j |?j ?
?
?j |?j ?j, (1)where we weight the difference between the j-thcomponent of the parameter vectors by ?j , the j-th expected sufficient statistic with respect to p?
(the expected counts computed in the E-step).9 Un-like KL, our distance d?
is only defined on distri-butions in the model family and is not invariant toreparametrization.
Like KL, d?
is asymmetric, withthe first argument holding the status of being the?true?
parameter setting.
In our case, the parametersare conditional probabilities, so 0 ?
d?(?
|| ??)
?
1,so we can interpret d?
as an expected difference be-tween these probabilities.Unfortunately, label symmetry can wreak havocon our distance measure d?.
Suppose we want tomeasure the distance between ?
and ??.
If ??
issimply ?
with the labels permuted, then d?(?
|| ??
)would be substantial even though the distance oughtto be zero.
We define a revised distance to correctfor this by taking the minimum distance over all la-bel permutations:D?(?
|| ??)
= minpid?(?
||pi(??
)), (2)9Without this factor, rarely used components could con-tribute to the sum as much as frequently used ones, thus, makingthe distance overly pessimistic.884where pi(??)
denotes the parameter setting result-ing from permuting the labels according to pi.
(TheDMV has no label symmetries, so just d?
works.
)For mixture models, we can compute D?(?
|| ??
)efficiently as follows.
Note that each term in thesummation of (1) is associated with one of the Klabels.
We can form aK?K matrixM , where eachentry Mij is the distance between the parameters in-volving label i of ?
and label j of ??.
D?(?
|| ??)
canthen be computed by finding a maximum weightedbipartite matching on M using the O(K3) Hungar-ian algorithm (Kuhn, 1955).For models such as the HMM and PCFG, com-putingD?
is NP-hard, since the summation in d?
(1)contains both first-order terms which depend on onelabel (e.g., emission parameters) and higher-orderterms which depend on more than one label (e.g.,transitions or rewrites).
We cannot capture theseproblematic higher-order dependencies in M .However, we can bound D?(?
|| ??)
as follows.We create M using only first-order terms and findthe best matching (permutation) to obtain a lowerbound D?
and an associated permutation pi0 achiev-ing it.
Since D?(?
|| ??)
takes the minimum over allpermutations, d?(?
||pi(??))
is an upper bound forany pi, in particular for pi = pi0.
We then use a localsearch procedure that changes pi to further tightenthe upper bound.
Let D?
denote the final value.6 Estimation errorThus far, we have considered approximation andidentifiability errors, which have to do with flaws ofthe model.
The remaining errors have to do withhow well we can fit the model.
To focus on theseerrors, we consider the case where the true model isin our family (p?
?
P).
To keep the setting as real-istic as possible, we do supervised learning on reallabeled data to obtain ??
= argmax?
E?
log p(x,y).We then throw away our real data and let p?
= p??
.Now we start anew: sample new artificial data from?
?, learn a model using this artificial data, and seehow close we get to recovering ?
?.In order to compute estimation error, we need tocompare ??
with ?
?, the global maximizer of the like-lihood on our generated data.
However, we cannotcompute ??
exactly.
Let us therefore first consider thesimpler supervised scenario.
Here, ?
?gen has a closedform solution, so there is no optimization error.
Us-ing our distanceD?
(defined in Section 5.1) to quan-tify estimation error, we see that, for the HMM, ?
?genquickly approaches ??
as we increase the amount ofdata (Table 1).# examples 500 5K 50K 500KD?(??
|| ?
?gen) 0.003 6.3e-4 2.7e-4 8.5e-5D?(??
|| ?
?gen) 0.005 0.001 5.2e-4 1.7e-4D?(??
|| ?
?gen-EM) 0.022 0.018 0.008 0.002D?(??
|| ?
?gen-EM) 0.049 0.039 0.016 0.004Table 1: Lower and upper bounds on the distance fromthe true ??
for the HMM as we increase the number ofexamples.In the unsupervised case, we use the followingprocedure to obtain a surrogate for ??
: initialize EMwith the supervised estimate ?
?gen and run EM for100 iterations.
Let ?
?gen-EM denote the final param-eters, which should be representative of ??.
Table 1shows that the estimation error of ?
?gen-EM is an orderof magnitude higher than that of ?
?gen, which is to ex-pected since ?
?gen-EM does not have access to labeleddata.
However, this error can also be driven downgiven a moderate number of examples.7 Optimization errorFinally, we study optimization error, which is thediscrepancy between the global maximizer ??
and?
?EM, the result of running EM starting from a uni-form initialization (plus some small noise).
As be-fore, we cannot compute ?
?, so we use ?
?gen-EM as asurrogate.
Also, instead of comparing ?
?gen-EM and ?
?with each other, we compare each of their discrep-ancies with respect to ?
?.Let us first consider optimization error in termsof prediction error.
The first observation is thatthere is a gap between the prediction accuraciesof ?
?gen-EM and ?
?EM, but this gap shrinks consider-ably as we increase the number of examples.
Fig-ures 4(a,b,c) support this for all three model fami-lies: for the HMM, both ?
?gen-EM and ?
?EM eventuallyachieve around 90% accuracy; for the DMV, 85%.For the PCFG, ?
?EM still lags ?
?gen-EM by 10%, but webelieve that more data can further reduce this gap.Figure 4(d) shows that these trends are not par-ticular to artificial data.
On real WSJ data, the gap885500 5K 50K 500K# examples0.60.70.80.91.0Accuracy500 5K 50K 500K# examples0.60.70.80.91.0DirectedF 1500 5K 50K# examples0.50.60.80.91.0LabeledF 11K 3K 10K 40K# examples0.30.40.60.70.8Accuracy(a) HMM (artificial data) (b) DMV (artificial data) (c) PCFG (artificial data) (d) HMM (real data)500 5K 50K 500K# examples0.020.050.070.10.12D ?(??
||?)
??gen-EM?
?EM (rand 1)?
?EM (rand 2)?
?EM (rand 3)20 40 60 80 100iteration-173.3-171.4-169.4-167.4-165.5log-likelihood20 40 60 80 100iteration0.20.40.60.81.0AccuracySup.
init.Unif.
init.
(e) HMM (artificial data) (f) HMM log-likelihood/accuracy on 500K examplesFigure 4: Compares the performance of ?
?EM (EM with a uniform initialization) against ?
?gen-EM (EM initialized with thesupervised estimate) on (a?c) various models, (d) real data.
(e) measures distance instead of accuracy and (f) shows asample EM run.between ?
?gen-EM and ?
?EM also diminishes for theHMM.
To reaffirm the trends, we also measure dis-tance D?.
Figure 4(e) shows that the distance from?
?EM to the true parameters ??
decreases, but the gapbetween ?
?gen-EM and ?
?EM does not close as deci-sively as it did for prediction error.It is quite surprising that by simply running EMwith a neutral initialization, we can accurately learna complex model with thousands of parameters.
Fig-ures 4(f,g) show how both likelihood and accuracy,which both start quite low, improve substantiallyover time for the HMM on artificial data.Carroll and Charniak (1992) report that EM faredpoorly with local optima.
We do not claim that thereare no local optima, but only that the likelihood sur-face that EM is optimizing can become smootherwith more examples.
With more examples, there isless noise in the aggregate statistics, so it might beeasier for EM to pick out the salient patterns.Srebro et al (2006) made a similar observationin the context of learning Gaussian mixtures.
Theycharacterized three regimes: one where EM was suc-cessful in recovering the true clusters (given lots ofdata), another where EM failed but the global opti-mum was successful, and the last where both failed(without much data).There is also a rich body of theoretical work onlearning latent-variable models.
Specialized algo-rithms can provably learn certain constrained dis-crete hidden-variable models, some in terms of weakgenerative capacity (Ron et al, 1998; Clark andThollard, 2005; Adriaans, 1999), others in term ofstrong generative capacity (Dasgupta, 1999; Feld-man et al, 2005).
But with the exception of Das-gupta and Schulman (2007), there is little theoreticalunderstanding of EM, let alne on complex modelfamilies such as the HMM, PCFG, and DMV.8 ConclusionIn recent years, many methods have improved unsu-pervised induction, but these methods must still dealwith the four types of errors we have identified inthis paper.
One of our main contributions of this pa-per is the idea of using the meta-model to diagnosethe approximation error.
Using this tool, we can bet-ter understand model biases and hopefully correctfor them.
We also introduced a method for mea-suring distances in face of label symmetry and ranexperiments exploring the effectiveness of EM as afunction of the amount of data.
Finally, we hope thatsetting up the general framework to understand theerrors of unsupervised induction systems will aid thedevelopment of better methods and further analyses.886ReferencesP.
W. Adriaans.
1999.
Learning shallow context-free lan-guages under simple distributions.
Technical report,Stanford University.G.
Carroll and E. Charniak.
1992.
Two experiments onlearning probabilistic dependency grammars from cor-pora.
In Workshop Notes for Statistically-Based NLPTechniques, pages 1?13.A.
Clark and F. Thollard.
2005.
PAC-learnabilityof probabilistic deterministic finite state automata.JMLR, 5:473?497.A.
Clark.
2001.
Unsupervised induction of stochasticcontext free grammars with distributional clustering.In CoNLL.S.
Dasgupta and L. Schulman.
2007.
A probabilisticanalysis of EM for mixtures of separated, sphericalGaussians.
JMLR, 8.S.
Dasgupta.
1999.
Learning mixtures of Gaussians.
InFOCS.J.
Feldman, R. O?Donnell, and R. A. Servedio.
2005.Learning mixtures of product distributions over dis-crete domains.
In FOCS, pages 501?510.S.
Goldwater and T. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
InACL.T.
Grenager, D. Klein, and C. D. Manning.
2005.
Un-supervised learning of field segmentation models forinformation extraction.
In ACL.A.
Haghighi and D. Klein.
2006.
Prototype-based gram-mar induction.
In ACL.M.
Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In EMNLP/CoNLL.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In ACL.H.
W. Kuhn.
1955.
The Hungarian method for the as-signment problem.
Naval Research Logistic Quar-terly, 2:83?97.K.
Kurihara and T. Sato.
2006.
Variational Bayesiangrammar induction for natural language.
In Interna-tional Colloquium on Grammatical Inference.B.
Merialdo.
1994.
Tagging English text with a prob-abilistic model.
Computational Linguistics, 20:155?171.F.
Pereira and Y. Shabes.
1992.
Inside-outside reestima-tion from partially bracketed corpora.
In ACL.D.
Ron, Y.
Singer, and N. Tishby.
1998.
On the learnabil-ity and usage of acyclic probabilistic finite automata.Journal of Computer and System Sciences, 56:133?152.N.
Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In ACL.N.
Smith and J. Eisner.
2006.
Annealing structural biasin multilingual weighted grammar induction.
In ACL.N.
Srebro, G. Shakhnarovich, and S. Roweis.
2006.
Aninvestigation of computational and informational lim-its in Gaussian mixture clustering.
In ICML, pages865?872.887
