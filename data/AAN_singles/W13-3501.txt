Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 1?9,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsOnline Active Learning for Cost Sensitive Domain AdaptationMin Xiao and Yuhong GuoDepartment of Computer and Information SciencesTemple UniversityPhiladelphia, PA 19122, USA{minxiao,yuhong}@temple.eduAbstractActive learning and domain adaptation areboth important tools for reducing labelingeffort to learn a good supervised model ina target domain.
In this paper, we inves-tigate the problem of online active learn-ing within a new active domain adapta-tion setting: there are insufficient labeleddata in both source and target domains,but it is cheaper to query labels in thesource domain than in the target domain.Given a total budget, we develop two cost-sensitive online active learning methods, amulti-view uncertainty-based method anda multi-view disagreement-based method,to query the most informative instancesfrom the two domains, aiming to learn agood prediction model in the target do-main.
Empirical studies on the tasks ofcross-domain sentiment classification ofAmazon product reviews demonstrate theefficacy of the proposed methods on re-ducing labeling cost.1 IntroductionIn many application domains, it is difficult or ex-pensive to obtain labeled data to train supervisedmodels.
It is critical to develop effective learningmethods to reduce labeling effort or cost.
Activelearning and domain adaptation are both impor-tant tools for reducing labeling cost on learninggood supervised prediction models.
Active learn-ing reduces the cost of labeling by selecting themost informative instances to label, whereas do-main adaptation obtains auxiliary label informa-tion by exploiting labeled data in related domains.Combining the efforts from both areas to furtherreduce the labeling cost is an important researchdirection to explore.In this paper, we consider online active learn-ing with domain adaptations.
Online learning hasbeen widely studied (Borodin and El-Yaniv, 1998)due to its advantages of low memory requirementand fast computation speed.
Dredze and Crammer(2008) applied online learning on domain adap-tation and proposed to combine multiple similarsource domains to perform online learning for thetarget domain, which provides a new opportunityfor conducting active learning with domain adap-tation.
Online active learning with domain adap-tation, to our knowledge, has just gained atten-tion recently and has been addressed in (Rai et al2010; Saha et al 2011).
The active online do-main adaptation methods developed in (Rai et al2010; Saha et al 2011) leverage information fromthe source domain by domain adaptation to intelli-gently query labels for instances only in the targetdomain in an online fashion with a given budget.They assumed a large amount of labeled data isreadily available in the source domain.In this work, we however tackle online activelearning with domain adaptation in a different set-ting, where source domains with a large amount offree labeled data are not available.
Instead we as-sume there are very few labeled instances in boththe source and target domains and labels in bothdomains can be acquired with a cost.
Moreover,we assume the annotation cost for acquiring la-bels in the source domain is much lower than theannotation cost in the target domain.
This is apractical setting in many domain adaptation sce-narios.
For example, one aims to learn a goodreview classification model for high-end comput-ers.
It may be expensive to acquire labels for suchproduct reviews.
However, but it might be rela-tively much cheaper (but not free) to acquire la-bels for reviews on movies or restaurants.
In suchan active learning scenario, will a source domainwith lower annotation cost still be helpful for re-ducing the labeling cost required to learn a goodprediction model in the target domain?
Our re-search result in this paper will answer this ques-1Figure 1: The framework of online active learningwith domain adaptation.tion.
Specifically, we address this online active do-main adaptation problem by extending the onlineactive learning framework in (Cesa-Bianchi et al2006) to consider active label acquirement in bothdomains.
We first initialize the prediction modelbased on the initial labeled data in both the sourceand target domains (LS and LT ).
Then in eachround of the online learning, we receive one un-labeled instance from each domain (DS and DT ),on which we need to decide whether to query la-bels.
Whenever a label is acquired, we update theprediction model using the newly labeled instanceif necessary.
The framework of this online activelearning setting is demonstrated in Figure 1.
Weexploit multi-view learning principles to measurethe informativeness of instances and propose twocost-sensitive online active learning methods, amulti-view uncertainty-based method and a multi-view disagreement-based method, to acquire la-bels for the most informative instances.
Our em-pirical studies on the tasks of cross-domain sen-timent classification of Amazon product reviewsshow the proposed methods can effectively ac-quire the most informative labels given a budget,comparing to alternative methods.2 Related WorkThe proposed work in this paper involves re-search developments in multiple areas, includingonline active learning, active domain adaptationand multi-view active learning.
In this section, wewill cover the most related work in the literature.Online active learning has been widely stud-ied in the literature, including the perceptron-typemethods in (Cesa-Bianchi et al 2006; Monteleoniand Ka?a?ria?inen, 2007; Dasgupta et al 2009).Cesa-Bianchi et al(2006) proposed a selectivesampling perceptron-like method (CBGZ), whichserves as a general framework of online activelearning.
Monteleoni and Ka?a?ria?inen (2007) em-pirically studied online active learning algorithms,including the CBGZ, for optical character recogni-tion applications.
Dasgupta et al(2009) analyzedthe label complexity of the perceptron algorithmand presented a combination method of a modifi-cation of the perceptron update with an adaptivefiltering rule.
Our proposed online active learn-ing methods are placed on an extended frameworkof (Cesa-Bianchi et al 2006), by incorporatingdomain adaptation and multi-view learning tech-niques in an effective way.Active domain adaptation has been studied in(Chan and Ng, 2007; Rai et al 2010; Saha et al2011; Li et al 2012).
Chan and Ng (2007) pre-sented an early study on active domain adaptationand empirically demonstrated that active learn-ing can be successfully applied on out-of-domainword sense disambiguation systems.
Li et al(2012) proposed to first induce a shared subspaceacross domains and then actively label instancesaugmented with the induced latent features.
On-line active domain adaptation, however, has onlybeen recently studied in (Rai et al 2010; Sahaet al 2011).
Nevertheless, the active online do-main adaptation method (AODA) and its vari-ant method, domain-separator based AODA (DS-AODA), proposed in these works assume a largeamount of labeled data in the source domain andconduct online active learning only in the targetdomain, which is different from our problem set-ting in this paper.Multi-view learning techniques have recentlybeen employed in domain adaptation (Tur, 2009;Blitzer et al 2011; Chen et al 2011).
In par-ticular, instead of using data with conditional in-dependent views assumed in standard multi-viewlearning (Blum and Mitchell, 1998), Blitzer et al(2011) and Chen et al(2011) randomly splitoriginal features into two disjoint subsets to pro-duce two views, and demonstrate the usefulnessof multi-view learning with synthetic two views.On the other hand, multi-view active learning hasbeen studied in (Muslea et al 2000, 2002; Wangand Zhou, 2008, 2010).
These works all suggestto query labels for contention points (instanceson which different views predict different labels).Our proposed methods will exploit this multi-view2principle and apply it in our multi-view online ac-tive domain adaptation setting.In addition, our proposed work is also relatedto cost-sensitive active learning.
But differentfrom the traditional cost-sensitive active learn-ing, which assumes multiple oracles with differentcosts exist for the same set of instances (Donmezand Carbonell, 2008; Arora et al 2009), we as-sume two oracles, one for the source domain andone for the target domain.
Overall, the problem westudy in this paper is novel, practical and impor-tant.
Our research will demonstrate a combinationof advances in multiple research areas.3 Multi-View Online Active Learningwith Domain AdaptationOur online active learning is an extension of theonline active perceptron learning framework of(Cesa-Bianchi et al 2006; Rai et al 2010) in thecost-sensitive online active domain adaption set-ting.
We will present two multi-view online ac-tive methods in this section under the frameworkshown in Figure 1.Assume we have a target domain (DT ) and arelated source domain (DS) with a few labeled in-stances, LT and LS , in each of them respectively.The instances in the two domains are drawn fromthe same input space but with two different distri-butions specified by each domain.
An initial pre-diction model (w0) can then be trained with thecurrent labeled data from both domains.
Manydomain adaptation techniques (Sugiyama, 2007;Blitzer et al 2011) can be used for training here.However, for simplicity of demonstrating the ef-fectiveness of online active learning strategies, weuse vanilla Perceptron to train the initial predictionmodel on all labeled instances, as the perceptronalgorithm is widely used in various works (Sahaet al 2011) and can be combined seamlessly withthe online perceptron updates.
It can be viewed asa simple supervised domain adaptation training.The very few initial labeled instances are farfrom being sufficient to train a good predictionmodel in the target domain.
Additional labeleddata needs to be acquired to reach a reasonableprediction model.
However it takes time, money,and effort to acquire labels in all problem domains.For simplicity of demonstration, we use money tomeasure the cost and effort of labeling instancesin each domain.
Assume the cost of labeling oneinstance in the source domain is cs and the costof labeling one instance in the target domain is ct,where ct > cs.
Note the condition ct > cs isone criterion to be guaranteed when selecting use-ful source domains.
It does not make sense to se-lect source domains with more expensive labelingcost.
Given a budget B, we need to make wise de-cisions about which instances to query in the on-line learning setting.
We aim to learn the best pre-diction model in the target domain with the labelspurchased under the given budget.Then online active learning will be conducted ina sequence of rounds.
In each round r, we will re-ceive two randomly sampled unlabeled instancesin parallel, xs,r and xt,r, one from each domain,xs,r ?
DS and xt,r ?
DT .
Active learning strate-gies will be used to judge the informativeness ofthe two instances in a cost-sensitive manner anddecide whether to query labels for any one of themto improve the prediction model in the target do-main.
After new labels being acquired, we use thenewly labeled instances to make online perceptronupdates if the true labels are different from the pre-dicted labels.In this work, we focus on binary predictionproblems where the labels have binary values, y ?{+1,?1}.
We adopt the online perceptron-stylelearning model of (Cesa-Bianchi et al 2006) forthe online updates of the supervised perceptronmodel.
Moreover, we extend principles of multi-view active learning into our online active learn-ing framework.
As we introduced before, syn-thetic multi-views produced by splitting the orig-inal feature space into disjoint subsets have beendemonstrated effective in a few previous work(Blitzer et al 2011; Chen et al 2011).
We adoptthis idea to generate two views of the instancesin both domains by randomly splitting the com-mon feature space into two disjoint feature sub-sets, such that xs,r = {x(1)s,r ,x(2)s,r} and xt,r ={x(1)t,r ,x(2)t,r }.
Thus the initial prediction model willinclude two predictors (f (1), f (2)) with model pa-rameters (w(1)0 ,w(2)0 ), each trained on one viewof the labeled data using the perceptron algorithm.Correspondingly, the online updates will be madeon the two predictors.The critical challenge of this cost-sensitive on-line active learning problem nevertheless lies inhow to select the most informative instances forlabeling.
Based on different measurements ofinstance informativeness, we propose two on-line active learning algorithms: a Multi-view3Uncertainty-based instance Selection (MUS) al-gorithm and a Multi-view Disagreement-basedinstance Selection (MDS) algorithm for cost-sensitive online active domain adaptation, whichwe will present below.3.1 Multi-View Uncertainty-based InstanceSelection AlgorithmWe use the initial model (f (1), f (2)), trained onthe two views of the initial labeled data and rep-resented by the model parameters (w(1)0 ,w(2)0 ), asthe starting point of the online active learning.In each round r of the online active learning,we receive two instances xs,r = {x(1)s,r ,x(2)s,r} andxt,r = {x(1)t,r ,x(2)t,r }, one for each domain.
For thereceived instances, we need to make two sequen-tial decisions:1.
Between the instance (xs,r) from the sourcedomain and the instance (xt,r) from the tar-get domain, which one should we select forfurther consideration?2.
For the selected instance, do we really needto query its label?We answer the first question based on the label-ing cost ratio, ct/cs, from the two domains anddefine the following probabilityPc = e??
(ct/cs?1) (1)where ?
is a domain preference weighting param-eter.
Then with a probability Pc we select the tar-get instance xt,r and with a probability 1 ?
Pc weselect the source instance xs,r.
Our intuition is thatone should query the less expensive source domainmore frequently.
Thus more labeled instances canbe collected within the fix budget.
On the otherhand, the more useful and relevant but expensiveinstances from the target domain should also bequeried at a certain rate.For the selected instance x?,r, we then use amulti-view uncertainty strategy to decide whetherto query its label.
We first calculate the predictionconfidence and predicted labels of the selected in-stance based on the current predictors trained fromeach viewmk = |w(k)?x(k)?,r |, y?
(k) = sign(w(k)?x(k)?,r ) (2)where k = 1 or 2, standing for each of the twoviews.
If the two predictors disagree over the pre-diction label, i.e., y?
(1) 6= y?
(2), the selected in-stance is a contention point and contains usefulAlgorithm 1 MUS AlgorithmInput: B, Pc, cs, ct, b,initial model (w(1)0 ,w(2)0 )Output: prediction model (w(1),w(2))Initialize: w(1) = w(1)0 , w(2) = w(2)0for each round r = 1, 2, ?
?
?
doReceive two instances xs,r, xt,rSample d ?
U(0, 1)if B < ct then d = 1 end ifif d > Pc then x?,r= xs,r, c = cselse x?,r= xt,r, c = ctend ifCompute m1,m2, y?
(1), y?
(2) by Eq.
(2)Compute z1, z2 by Eq.
(3)if z1 = 1 or z2 = 1 or y?
(1) 6= y?
(2) thenQuery label y for x?,r, B = B?
cUpdate (w(1),w(2)) by Eq (4)end ifif B < cs then break end ifend forinformation for at least one predictor, accordingto the principle of multi-view active learning.
Wethen decide to pay a cost (cs or ct) to query its la-bel.
Otherwise, we make the query decision basedon the two predictors?
uncertainty (i.e., the inverseof the prediction confidence mk) over the selectedinstance.
Specifically, we sample two numbers,one for each view, according tozk = Bernoulli(b/(b + mk)) (3)where b is a prior hyperparameter, specifying thetendency of querying labels.
In our experiments,we use b = 0.1.
If either z1 = 1 or z2 = 1,which means that at least one view is uncertainabout the selected instance, we will query for thelabel y.
The prediction model will be updated us-ing the new labeled instances when the true labelsare different from the predicted ones; i.e.,w(k) = w(k) + (yx(k)?,r )I[y 6= y?
(k)] (4)for k = 1, 2, where I[?]
is an indicator function.This multi-view uncertainty-based instance selec-tion algorithm (MUS) is given in Algorithm 1.3.2 Multi-View Disagreement-based InstanceSelection AlgorithmMUS is restrained to query at most one instanceat each round of the online active learning.
Inthis section, we present an alternative multi-view4disagreement-based instance selection algorithm(MDS) within the same framework.In each round r of the online active learning,given the two instances xs,r and xt,r we received,the MDS algorithm evaluates both instances forpotential label acquisition using the multi-view in-formation provided by the two per-view predic-tors.
Let y?
(1)s and y?
(2)s denote the predicted la-bels of instance xs,r produced by the two predic-tors according to Eq (2).
Similarly let y?
(1)t andy?
(2)t denote the predicted labels of instance xt,r.Follow the principle suggested in the multi-viewactive learning work (Muslea et al 2000, 2002;Wang and Zhou, 2008, 2010) that querying labelsfor contention points (instances on which differentviews predict different labels) can lead to superiorinformation gain than querying uncertain points,we identify the non-redundant contention pointsfrom the two domains for label acquisition.Specifically, there are three cases: (1) If onlyone of the instances is a contention point, we queryits label with probability Pc (Eq (1)) when the in-stance is from the target domain, and query its la-bel with probability 1 ?
Pc when the instance isfrom the source domain.
(2) If both instances arecontention points, i.e., y?
(1)s 6= y?
(2)s and y?
(1)t 6= y?
(2)t ,but the predicted labels for the two instances arethe same, i.e., y?
(k)s = y?
(k)t for k = 1, 2, it suggeststhe two instances contain similar information withrespect to the prediction model and we only needto query one of them.
We then select the instancein a cost-sensitive manner stated in the MUS algo-rithm by querying the target instance with a prob-ability Pc and querying the source instance with aprobability 1 ?
Pc.
(3) If both instances are con-tention points but with different predicted labels, itsuggests the two instances contain complementaryinformation with respect to the prediction model,and we thus query labels for both of them.For any new labeled instance from the targetdomain or the source domain, we update the pre-diction model of each review using Equation (4)when the acquired true label is different from thepredicted label.
The overall MDS algorithm isgiven in Algorithm 2.3.3 Multi-View PredictionAfter the training process, we use the two predic-tors to predict labels of the test instances fromthe target domain.
Given a test instance xt =Algorithm 2 MDS AlgorithmInput: B, Pc, cs, ct, b,initial model (w(1)0 ,w(2)0 )Output: prediction model (w(1),w(2))Initialize: w(1) = w(1)0 , w(2) = w(2)0for each round r = 1, 2, ?
?
?
doReceive two instances xs,r, xt,rCompute y?
(1)s , y?
(2)s , y?
(1)t , y?
(2)t by Eq (2)Let ds = I[y?
(1)s = y?
(2)s ], dt = I[y?
(1)t = y?
(2)t ]Let qs = 0, qt = 0if B < ct then dt = 0 end ifSample d ?
U(0, 1)if ds = 1 and dt = 0 thenif d > Pc then qs = 1 end ifelse if ds = 0 and dt = 1 thenif d ?
Pc then qt = 1 end ifelse if ds = 1 and dt = 1if y?
(1)s = y?
(1)t thenif d > Pc then qs = 1 else qt = 1 end ifelse qs = 1, qt = 1end ifend ifif qs = 1 thenQuery label ys for xs,r, B = B?
csUpdate (w(1),w(2)) by Eq (4)end ifif B < ct then qt = 0 end ifif qt = 1 thenQuery label yt for xt,r, B = B?
ctUpdate (w(1),w(2)) by Eq (4)end ifif B < cs then break end ifend for(x(1)t ,x(2)t ), we use the predictor that have largerprediction confidence to determine its label y?.The prediction confidence of the kth view predic-tor on xt is defined as the absolute prediction value|w(k)?x(k)t |.
We then select the most confidentpredictor for this instance ask?
= argmaxk?
{1,2}|w(k)?x(k)t | (5)The predicted label is final computed asy?
= sign(w(k?)?x(k?
)t ) (6)With this multi-view prediction on the test data,the multi-view strengths can be exploited in thetesting phase as well.54 ExperimentsIn this section, we present the empirical evaluationof the proposed online active learning methods onthe task of sentiment classification comparing toalternative baseline methods.
We first describe theexperimental setup, and then present the resultsand discussions.4.1 Experimental SetupDataset For the sentiment classification task, weuse the dataset provided in (Prettenhofer and Stein,2010).
The dataset contains reviews with fourdifferent language versions and in three domains,Books (B), DVD (D) and Music (M).
Each domaincontains 2000 positive reviews and 2000 negativereviews, with a term-frequency (TF) vector rep-resentation.
We used the English version and con-structed 6 source-target ordered domain pairs fromthe original 3 domains: B2D, D2B, B2M, M2B,D2M, M2D.
For example, for the task of B2D, weuse the Books reviews as the source domain andthe DVD reviews as the target domain.
For eachpair of domains, we built a unigram vocabularyover the combined 4000 source reviews and 4000target reviews.
We further preprocessed the databy removing features that appear less than twicein either domain, replacing TF with TFIDF, andnormalizing the attribute values into [0, 1].Approaches In the experiments, we mainlycompared the proposed MUS and MDS algorithmswith the following three baseline methods.
(1)MTS (Multi-view Target instance Selection): Itis a target-domain variant of the MUS algorithm,and selects the most uncertain instance receivedfrom the target domain to query according to theprocedure introduced for MUS method.
(2) TCS(Target Contention instance Selection): It is atarget-domain variant of the MDS algorithm, anduses multi-view predictors to query contention in-stances received from the target domain.
(3) SUS(Single-view Uncertainty instance Selection): Itselects target vs source instances according to Pc(see Eq.
(1)), and then uses uncertainty measure tomake query decision.
This is a single view vari-ant of the MUS algorithm.
In the experiments, weused ?
= 1 for the Pc computation in Eq.
(1).4.2 Classification AccuracyWe first conducted experiments over the 6 do-main adaptation tasks constructed from the sen-timent classification data with a fixed cost ratioct/cs = 3.
We set cs = 1 and ct = 3.
Given a bud-getB = 900, we measure the classification perfor-mance of the prediction model learned by each on-line active learning method during the process ofbudget being used.
We started with 50 labeled in-stances from the source domain and 10 labeled in-stances from the target domain.
The classificationperformance is measured over 1000 test instancesfrom the target domain.
All other instances areused as inputs in the online process.
We repeatedthe experiments 10 times using different randomonline instance input orders.
The average resultsare reported in Figure 2.The results indicate the proposed two algo-rithms, MUS and MDS, in general greatly out-perform the other alternative methods.
The SUSmethod, which is a single-view variant of MUS,presents very poor performance across all 6 taskscomparing to the other multi-view based methods,which demonstrates the efficacy of the multi-viewinstance selection mechanism.
Among the multi-view based active learning methods, the MTSmethod and TCS method, which only query labelsfor more relevant but expensive instances fromthe target domain, demonstrated inferior perfor-mance, comparing to their cost-sensitive counter-parts, MUS and MDS, respectively.
This suggeststhat a cheaper source domain is in general helpfulon reducing the labeling cost for learning a goodprediction model in the target domain and our pro-posed active learning strategies are effective.4.3 Domain DivergenceTo further validate and understand our experimen-tal results on the sentiment classification data, weevaluated the domain divergence over the threepairs of domains we used in the experimentsabove.
Note, if the domain divergence is verysmall, it will be natural that a cheaper source do-main should help on reducing the labeling cost inthe target domain.
If the domain divergence is verybig, the space of exploring a cheaper source do-main will be squeezed.The divergence of two domains can be mea-sured using the A-distance (Ben-David et al2006).
We adopted the method of (Rai et al 2010)to proximate the A-distance.
We train a linearclassifier over all 8000 instances, 4000 instancesfrom each domain, to separate the two domains.The average per-instance hinge-loss for this sepa-rator subtracted from 1 was used as the estimate60 200 400 600 8006466687072747678B2DTotal budgetAccuracyMTSTCSSUSMUSMDS0 200 400 600 8006264666870727476B2MTotal budgetAccuracyMTSTCSSUSMUSMDS0 200 400 600 800606264666870727476D2MTotal budgetAccuracyMTSTCSSUSMUSMDS(a) (b) (c)0 200 400 600 8006264666870727476D2BTotal budgetAccuracyMTSTCSSUSMUSMDS0 200 400 600 80064666870727476M2BTotal budgetAccuracyMTSTCSSUSMUSMDS0 200 400 600 8006466687072747678M2DTotal budgetAccuracyMTSTCSSUSMUSMDS(d) (e) (f)Figure 2: Online active learning results over the 6 domain adaptation tasks for sentiment classification,with a total budget B=900 and a fixed cost ratio ct/cs = 3.of the proxy A-distance.
A score of 1 means per-fectly separable distributions and 0 means the twodistributions from the two domains are identical.In general, a higher score means a larger diver-gence between the two domains.Table 1: Proxy A-distance over domain pairs.Domains A-distanceBooks vs. DVD 0.7221Books vs. Music 0.8562DVD vs. Music 0.7831The proxy A-distances over the 3 domain pairsfrom the sentiment classification dataset are re-ported in Table 1.
It shows that all the 3 pairsof domains are reasonably far apart.
This justi-fied the effectiveness of the online active domainadaptation methods we developed and the resultswe reported above.
It suggests the applicability ofthe proposed active learning scheme is not boundto the existence of highly similar source domains.Moreover, the A-distance between Books and Mu-sic is the largest among the three pairs.
Thus itis most challenging to exploit the source domainin the adaptation tasks, B2M and M2B.
This ex-plains the good performance of the target-domainmethod TCS on these two tasks.
Nevertheless, theproposed MUS and MDS maintained consistentgood performance even on these two tasks.4.4 Robustness to Cost RatioWe then studied the empirical behavior of the pro-posed online active domain adaptation algorithmswith different cost ratio values ct/cs.Given a fixed budget B = 900, we set cs = 1and run a few sets of experiments on the senti-ment classification data by setting ct as differentvalues from {1, 2, 3, 4}, under the same experi-mental setting described above.
In addition tothe five comparison methods used before, we alsoadded a baseline marker, SCS, which is a source-domain variant of the MDS algorithm and queriescontention instances from only the source domain.The final classification performance of the predic-tion model learned with each approach is recorded71 1.5 2 2.5 3 3.5 4666870727476788082B2Dct/csAccuracyMTSTCSSCSSUSMUSMDS1 1.5 2 2.5 3 3.5 46466687072747678B2Mct/csAccuracyMTSTCSSCSSUSMUSMDS1 1.5 2 2.5 3 3.5 46466687072747678D2Mct/csAccuracyMTSTCSSCSSUSMUSMDS(a) (b) (c)1 1.5 2 2.5 3 3.5 466687072747678D2Bct/csAccuracyMTSTCSSCSSUSMUSMDS1 1.5 2 2.5 3 3.5 4626466687072747678M2Bct/csAccuracyMTSTCSSCSSUSMUSMDS1 1.5 2 2.5 3 3.5 465707580M2Dct/csAccuracyMTSTCSSCSSUSMUSMDS(d) (e) (f)Figure 3: Online active learning results over the 6 domain adaptation tasks for sentiment classification,with different cost ratio values ct/cs = {1, 2, 3, 4}.after the whole budget being used.
The averageresults over 10 runs are reported in Figure 3.We can see that: (1) With the increasing ofthe labeling cost in the target domain, the perfor-mance of all methods except SCS decreases sincethe same budget can purchase fewer labeled in-stances from the target domain.
(2) The three cost-sensitive methods (SUS, MUS, and MDS), whichconsider the labeling cost when making query de-cisions, are less sensitive to the cost ratios than theMTS and TCS methods, whose performance de-grades very quickly with the increasing of ct/cs.
(3) It is reasonable that when ct/cs is very big,the SCS, which simply queries source instances,produces the best performance.
But the proposedtwo cost-sensitive active learning methods, MUSand MDS, are quite robust to the cost ratios acrossa reasonable range of ct/cs values, and outper-form both source-domain only and target-domainonly methods.
When ct = cs, the proposed cost-sensitive methods automatically favor target in-stances and thus achieve similar performance asTCS.
When ct becomes much larger than cs, theproposed cost-sensitive methods automatically ad-just to favor cheaper source instances and maintaintheir good performance.5 ConclusionIn this paper, we investigated the online active do-main adaptation problem in a novel but practicalsetting where we assume labels can be acquiredwith a lower cost in the source domain than in thetarget domain.
We proposed two multi-view on-line active learning algorithms, MUS and MDS, toaddress the proposed problem.
The proposed al-gorithms exploit multi-view active learning learn-ing principles to measure the informativeness ofinstances and select instances in a cost-sensitivemanner.
Our empirical studies on the task of cross-domain sentiment classification demonstrate theefficacy of the proposed methods.
This researchshows that a cheaper source domain can help onreducing labeling cost for learning a good pre-diction model in the related target domain, withproper designed active learning algorithms.8ReferencesS.
Arora, E. Nyberg, and C. P. Rose?.
Estimatingannotation cost for active learning in a multi-annotator environment.
In Proceedings of theNAACL-HLT 2009 Workshop on Active Learn-ing for Natural Language Processing, 2009.S.
Ben-David, J. Blitzer, K. Crammer, andF.
Pereira.
Analysis of representations for do-main adaptation.
In Advances in Neural Infor-mation Processing Systems (NIPS), 2006.J.
Blitzer, D. Foster, and S. Kakade.
Domain adap-tation with coupled subspaces.
In Proceedingsof the Conference on Artificial Intelligence andStatistics (AISTATS), 2011.A.
Blum and T. Mitchell.
Combining labeled andunlabeled data with co-training.
In Proceedingsof the Conference on Computational LearningTheory (COLT), 1998.A.
Borodin and R. El-Yaniv.
Online computationand competitive analysis.
Cambridge Univer-sity Press, 1998.N.
Cesa-Bianchi, C. Gentile, and L. Zaniboni.Worst-case analysis of selective sampling forlinear classification.
Journal of Machine Learn-ing Research (JMLR), 7:1205?1230, 2006.Y.
Chan and H. Ng.
Domain adaptation with activelearning for word sense disambiguation.
In Pro-ceedings of the Annual Meeting of the Assoc.
ofComputational Linguistics (ACL), 2007.M.
Chen, K. Weinberger, and J. Blitzer.
Co-training for domain adaptation.
In Advances inNeural Information Processing Systems (NIPS),2011.S.
Dasgupta, A. T. Kalai, and C. Monteleoni.Analysis of perceptron-based active learning.Journal of Machine Learning Research (JMLR),10:281?299, 2009.P.
Donmez and J. G. Carbonell.
Proactive learn-ing: cost-sensitive active learning with multi-ple imperfect oracles.
In Proceedings of theACM Conference on Information and knowl-edge management (CIKM), 2008.M.
Dredze and K. Crammer.
Online methods formulti-domain learning and adaptation.
In Pro-ceedings of the Conf.
on Empirical Methods inNatural Language Processing (EMNLP), 2008.L.
Li, X. Jin, S. Pan, and J.
Sun.
Multi-domain ac-tive learning for text classification.
In Proceed-ings of the ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining(KDD), 2012.C.
Monteleoni and M. Ka?a?ria?inen.
Practical on-line active learning for classification.
In Pro-ceedings of the IEEE Conference on ComputerVision and Pattern Recognition, Online Learn-ing for Classification Workshop, 2007.I.
Muslea, S. Minton, and C. Knoblock.
Selectivesampling with redundant views.
In Proceedingsof the National Conference on Artificial Intelli-gence (AAAI), 2000.I.
Muslea, S. Minton, and C. A. Knoblock.
Ac-tive + semi-supervised learning = robust multi-view learning.
In Proceedings of the In-ternational Conference on Machine Learning(ICML), 2002.P.
Prettenhofer and B. Stein.
Cross-language textclassification using structural correspondencelearning.
In Proceedings of the Annual Meetingfor the Association of Computational Linguis-tics (ACL), 2010.P.
Rai, A. Saha, H. Daume?
III, and S. Venkata-subramanian.
Domain adaptation meets activelearning.
In Proceedings of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL), 2010.A.
Saha, P. Rai, H. Daume?
III, S. Venkata-subramanian, and S. DuVall.
Active super-vised domain adaptation.
In Proceedings ofthe European Conference on Machine Learning(ECML), 2011.M.
Sugiyama.
Direct importance estimation withmodel selection and its application to covariateshift adaptation.
In Advances in Neural Infor-mation Processing Systems (NIPS), 2007.G.
Tur.
Co-adaptation: Adaptive co-training forsemi-supervised learning.
In Proceedings of theIEEE Inter.
Conference on Acoustics, Speechand Signal Processing (ICASSP), 2009.W.
Wang and Z. Zhou.
On multi-view active learn-ing and the combination with semi-supervisedlearning.
In Proceedings of the internationalconference on Machine learning (ICML), 2008.W.
Wang and Z. Zhou.
Multi-view active learn-ing in the non-realizable case.
In Advances inNeural Information Processing Systems (NIPS),2010.9
