Dependency Analyzer:A Knowledge-Based Approach to Structural DisambiguationKatashi  NagaoIBM Research, Tokyo Research Laboratory5--19 Sanbancho, Chiyoda-ku,  Tokyo 102, JapanE-mail:  nagao@jpntscvm.bitnetAbstractTo resolve structural ambiguities in syntactic analysis ofnatural anguage, which are caused by prepositional phraseattachment, relative clause attachment, and so on, we de-veloped an experimental system called tile Dependency An-al!lzcr.
The system uses instances of dependency structuresextracted froth a terminology dictionary as a knowledgeba.~e.
Structural (attachment) ambiguity is representedby showing that a word has several words as c;tndidatemodiliees.
Tim system resolves uch ambiguity as follows.First, it searches the knowledge base for modification re-lationships (dependencies) between the word and each ofits possible modifiees, then assigns an order of preferenceto these relationships, and finally seieets the most prefer-able deper.dency.
The knowledge base can be constructedsemi-automatically, since the source of knowledge xists inthe form of texts, and these sentences can be analyzed bythe parser and transformed into dependency structures bythe system.
We are realizing knowledge bootstrapping byadding the outputs of the system to its knowledge base.1 IntroductionThe bottleneck of sentence analysis, structural ambi-guity, occurs when a sentence has several alternativesfor modifier-modifiee relationships (dependencies) betweenwords or phrases.
This kind of ambiguity cannot be re-solved merely by applying grammatical knowledge: thereis a need for semantic processing.
Resolution of struc-tural ambiguities eems to be a problem of selecting themost preferable dependency from several candidates by us-ing large-scale knowledge on dependencies among words.There are two problems in realizing practical semantic pro-cessing: one is that knowledge must be large-scale, andmust be constructed automatically or semi-automatically;the other is that the mechanism for utilizing knowledge,inference, must be efficient or tractable.
We developed asystem called the Dependency Analyzer that resolves theseproblems.The Dependency Analyzer is a systenl fl)r structural dis-ambignation.
One of its characteristics is that it selectsthe most preferabledependency by using a knowledge basecontaining terminological knowledge in the form of depen-dency trees.
The knowledge base can be constructed semi-automatically, as described in Section 2.
The inputs ofthis system are parse trees, which are outputs of the PEGparser, a broad coverage English parser \[5\].
The systemtranslates the phrase structures into dependency strut-282tures that explicitly represent modifier-modifiee r lation-ships between words.
The main processes of the systemare executed if attachment ambiguities are included in thephrase structures.
In the dependency structures, attach-ment ambiguities are represented by showing that somewords have several candidate modiliees.
From these de-pe.ndency structures, several candidate dependencies areextracted.
The system decides which of these should beadopted by using background knowledge an,l context.
Thedecision is made via tim mechanisms of path search anddistance calculation.
A precise description of path search isgiven in Section 3.
An explanation of distance calculationis given in Section 4.
Another problem for disambigua-tion, namely interaction (or constraints) between attach-ment ambiguities, is discussed in Section 5.2 Knowledge BaseThe knowledge must be large=scale, since natural anguagesemantics hould have a broad coverage of lexical items.Since dependency structures are built by analyzing sen-tences and by tra:nsforming phr~e structures in a straight-forward way, if knowledge is assumed to consist of depen-dency structures, a knowledge base is easily constructedby using already-existing on-line dictionaries.
This ideaof using on-line dictionary definitions as a knowledge basewas originally proposed by Karen Jensen and Jean-LouisBinot \[6\].
Jun-ichi Nakamura nd Makoto Nagao \[101 eval-uated tile automatic extraction of semantic relationshipsbetween words from the on-line dictionary.
We emphasizethat a data structure for representing knowledge should beas simple as possible, because it must be easy to constructand efficient.We selected the tree structure as a means of representingknowledge, because it is a very simple and manageabledata structure, and because tree structures are suitablefor describing dependency structures.Tile tree structure is defined as follows.
A Tree consistsof a Node and reeursions (or null) of Tree, and a Node con-sists of repetitions of a paired attribute name and u.ttributevalue.For example, Figure 1 shows a tree (dependency) strnc-ture for the clause "the operating system stores the files inthe disk."
In this tree, "WORD," "POS (part of speech),"and "CASE" are att,qbute names, and "store," "VERB,"and "AGENT" are attribute values.In our system, the knowledge can be extracted fi:om dic-tionaries of terminology, and is of two types: (1) depen-dency structures and (2) synonym and taxonym relation-( ((WORD .
"store") (POS .
VERB))(((WORD .
"operat ing system")(CASE .
AGENT) (POS .
NOUN)))(((WORD .
"fi le") (CASE , PATIENT)(POS .
NOUN)))(((WORD .
"disk") (CASE .
LOCATION)(POS .
NOUN))) )Table h Tree Index Tabled synonym and taxonyra treesto(O)-qo(o) t~2(0)tsO) t~(o) tea(o)t~(0) t ~(0) tn (1)t s0)  t25(1) t82(o)dependency treestlol(O 1) tlso(1 O)tu (1)  tr io(t)tlOl(1 1) t350(0 2 3)tas(1 O) tllo(1 1 O)lqgure h Tree structure for the clause "the operating sys-tem stores the files in the disk"ships.The process of knowledge xtraction is as follows.
First,dictionary statements are rewritten manually as simple:~entences.
Next, sentences are parsed into phrase strue-tm'es by tile PEG parser.
Then.
phrase structures aretransformed into dependency structm'es by the Depen-de'nc:q Str~zctu're Builder, which is a component of tile De-pendency Analyzer.
Finally, sernantie case markers aremanually added to the modification links in dependency.structures.
Synonym and taxollym relationships are ex-tracted from sentences of the form "X is a synonynt forY" and "X is a Y" respectively.
These sentences are au-tomatically transformed into tree structures each of whichhas two nodes R)r tile words "X" and "Y" and a link from"X" to "Y" with the label "isa."
In the case of "X is asynonym for Y," since "Y" is also a synonym for "X," "Y'"is connected with "X" at the same time by a link with thelabel "isa.'"
\Ve developed an interactive tree managementtool, the Tree Editor.
which makes it easy for users to dealwith trees.Another problem of natural language processiug is theknowledge acquisition bottleneck.
Some ideas on how toa.cquire knowledge fi'om a!ready-existing dictionaries auto-matically or semi-automatically haw~ be.en proposed \[10,41.But it is still difficult to develop a knowledge base hilly au-tomatically because of ambiguities in the natural languageanalysis of dictionary definitions.
A more practical way, toovercome the t)otthmeck is so-called kno'wledge bootstrap-tn'a~.\].
By knowledge bootstrapping, the Dependency Ana-lyzer extends its knowledge automatically by using a eor'eknowledge base that includes mammllv edited dependency.~;truetures.
Since the De.pendency Analyzer uses depen-dency structures as knowledge and outputs a dependencystructure with no ambiguity (case ambiguity is also re-solved by the system), tile output can be added to theknowledge base.
Of course we still need to evaluate theautomatically constructed knowledge base.
But the relia-bility (performance) of the knowledge base is rising grad-ua\[ly, so it is expected that human interw,ntion wilI begreatly reduced in the near future.
"t Path  Search  An  Et I ic ient  A lgo -r i thmPath search is a process for finding relationships betweenthe words in a candidate dependency by using a knowledgebase.
Since relationships between words in these candi-dates do not always exist in the knowledge base, relation-/ \(3 (o) ( ?O (2)t .5  (2 1 0)Figm'e 2: Tree and Node Locationships between synonyms and taxouyms of these words eaualso /)e targets.
Path search is done ill the following steps:1.
Synonyms and t~u,:onyms of words in the candidatedependencies are found by using the knowledge base.In the knowledge base, synonym and taxonym rela-tionships are also defined in the form of trees.
All thesynonyms and taxonyms can be collected by transitingrelationships.2.
Dependencies between elements of each synonym andtaxonym set (including the original words) are alsofound by using the knowledge base.We developed an efficient algorithm for path search, us-ing the table of indices shown in Table 1.
In this table, t~,represents the pointer of the tree in which the word on thesame line appears, and the numbers in parentheses repre-sent the node location of the word in the tree.
Relation-ships between the numbers amt the node are shown in Fig-ure 2.
The left side of tile table shows trees in which a syn-onym or a taxonym of the word on the same line appears asits parent node.
For example, in the tree to, the word a ison the node of location (0), and by traversing to up by onenode fl'om location (0) we can find that the word b is on thenode of location (), so b is a synonym or a taxonym of a, asshown in Figure 3.
Thus, in order to find a synonym or ataxonym of a word, we just traverse up tile tree on the leftside of the table by one node.
We assume that synonymand taxonym relationships are transitive, that is.
that asynonym/taxonym of one of the synonyms/taxonyms of aword is also a synonym/taxonym of the word itself.
We canto ~ bl synonym/tmxonym (isa)aFigure 3: Synonym/Taxonym Tree283tl.
tO 0 /0~/ bd"keep" .~,.."VM/SP .
.
.
.
information" /"virtual disk"Figure 6: Ambiguous Dependency StructureFigure 4: Dependency Tree~110 ~ O~to ~ bq.,, -.
dACFigure 5: Pathcollect all its synonyms/taxonyms by iteration of that pro-cess.
The next stage of path search is to find whether thereare dependencies between words within each set of syn-onyms/taxonyms.
This process searches trees that involveboth words and checks whether there is a path from oneword to the other.
In the dependency trees, the words' lo-cations show whether there is a dependency between them.For example, we can see that tile word b is a dominatorof the word d frmn the locations of these words in theemnmou tree tH0 (shown in Figure 4), which is included inboth the set of dependency trees that include b.
{ttl, ttl0},and that of dependency trees that include d, {tas, t110}.
Inthe tree structures, if the node a is an ancestor of the nodeb, then there is a unique path front b to a.
Thus, findingdependency between words is equivalent o checking theirnode locations in the dependency trees.
A path betweenwords wl and w2 is found by the following processes:1.
The synonym/taxonym sets of these words, S~,~ andS~,  are collected.
"keep"I on"virtual disk"Figure 7: Candidate Dependency4 D is tance  Ca lcu la t ion  - A Heur i s t i cP rocess  for  Se lec t ion  o f  the  MostPreferable DependencySeveral conditions are added to path.s, and the ch)senessof dependevcy in a path is computed according to theseconditions.
The degree of closeness of dependenc.y is calledthe dependency distance.
This is calculated by using thenumber of dependencies inclnded in a path and the valuesof the conditions.
Three conditions are used to calculatethe dependency distartce:I.
Case consistencyFor example, in the sentence "VM/SP kceps the infor-mation on the virtual disk," there is a prepositionalphrase attachment ambiguity, as shown in Figure 6.
Ifthe path shown in Figure 8 is found together with thecandidate dependency shown in Figure 7, then tile se-mantic case of the path's dependency between "store"and "disk" must be consistent with the grammaticalcase of the sentence's dependency between "keep" and"virtual disk."
lIere, the case consistency betweenthe sentence and the path holds, since the grammat-ical case "on" call have the role of the semantic case"location."
If this consistency holds, then the value ofcase consistency is 1; otherwise, it is 0.2.
Co-occurrence consistencyThis is the consistency between the other modifiers ofthe modifiee of tile candidate dependency, called theco-occurrent modifiers, and those of a path.2.
The common trees tz .
.
.
that involve both elenmnts,ei 6 Swl and ej ~ Sw2, of each set are found.3.
Tile node locations of ei and ej in t~.
.
.
are checked.For example, a path between the words a and c is shownin Figure 5.
"store""keep .
.
.
.
disk"l i sa"virtual disk"Figure 8: Path2843.
"keep""VM/SP" '~virtual disk"Figure 9: Co-Occurrence"operating system" "file .
.
.
.
disk"Figure 10: Dependency TreeIn tile example sentence, for instance, there is a co-occurrent modifier "VM/SP"  of the candidate depen-dency between "keep" and "virtual disk," as shown inFigure 9.
In this case, "VM/SP"  has the grammaticalcase subject.
On the other hand, if the path is given bythe dependency tree shown in Figure 10, then there isalso a eo-oceurrent modilicr "operating systenf' thathas the semantic ease of agent.
In addition, there is atmxonym relationship between "VM/SP"  and "oper-ating system" in the knowledge base, as shown in Fig-ure \].I.
In this case, the co-occurrence consistency be-tween "VM/SP"  and "operating systenf' holds, sincethere is a relationship between the words and bothcase,; are consistent (the grammatical case subject callhave a semantic case agent), as shown in Figure 12.The vahm of co-occurrence consistency is tile num-ber of co-oceurrent modifiers that are consistent be-tween the path and the sentence.
Here, the value is1, since only one co-oecurrent modifier "VM/SP"  isconsistent.Context consistencyContext consistency holds if dependencies in a pathalready exist in previous sentences.
For example, ifthe sentence "the data is stored in the storage device"comes before tile above sentence, then the dependencystructure shown in Figure 13 is in the context base inwhich the dependency structures of previous entencesare stored.
Then the other path (shown in Figure 14),which corresponds to the dependency between "store"and "disk" in the "path," is found by using the con-text base.
Thus the dependency between "store" and"disk" is defined by the context.
The vahn', of contextconsistmmy is the number of dependencies in the paththat are.
defined by tile context.
In this case, the wflue"operating system"l isa"VM/SP"Figure lh  Taxonym Relationshipagent location"operating system" ~ "store" .i .... "disk"subject on"VM/SP" " * "keep" ~ "virtual disk"Figure 12: Diagram of Co-Occurrence Consistency"store"St-->"data .
.
.
.
storage device"Figure 13: Dependency Tree in tile Context Baseis 1, since there is one dependency in the pa& and itis de.fined in the context.The dependency distance is computed from the followingformula:Distance = \[Depl + ~c'o,,, x (n -  1)(t~, ..... + 1) x (l@oo~ + 1) 'where \]Dep\] represents the number of dependencies in-cluded in the palh, i"c ..... is the value of case consistency,1 ~'oo~.
is that of co-occurrence cm~sistency, and l'C,o,,t is thatof context consistency.This formula assumes that case and co-occurrence consis-tency affect the distance of the whole path, but that contextconsistency affects the distance of each dependency in thepath.n is a real number in tile range 0 < n < 1; it is a heuristicparameter that represents the degree of unimportance ofcontext consistency.The dependency distance between "keep" and "virtualdisk" that is calculated by using the path in the example is0.125, because the number of depenttencies is 1, the valueof case consistency is 1, that of co-occurrence consistencyis 1, and that of context consistency is 1 (n is defined0.5).The ambiguity of an attachment is resolved by selectingthe candidate dependency that is separated by the shortestdistance.
"store" -..<storage device"disk"Figure 14: Path of Context285Table 2: Constraint TablesConstraint Table T5.6 Constraint Table 7~5,7 Cons~r~dnt Table T6.rtl'~ 0 I l I 0 1 ~6/7 3 61 0 13k~__~l_L2 o V3 f l  i 212o0 \[5 ~ 10 Ic~0 I m I1 2 ~  m,o ):% _'7_'7,0 / ,Q /~ "-- , / 7~\0~,~) I~O/{3,s}Figure 15: Ambigu,ms Dependency Slructure5 P lann ing ,  Const ra in t  P ropagat ion ,and  Process  o f  D isambiguat ionWhen there are several attachment ambiguities in onesentence, the relationships of each pair of ambiguities arerepresented by a constraint network \[91.
The idea that am-Mguous syntactic structures can be represented by a datastructure of constraint network was originally developedby Hiroshi Maruyama \[7 t. A constraint network consistsof constraint ables.For example, the constraint ables shown in Table 2are constructed from tile ambiguous dependency structureshown in Figure 15.
In this dependency structure, words5, 6, and 7 have attachment ambiguities, o their possi-ble modifiees are {1,3}, {1,5}, and {3,6} respectively.
Theconstraint table is a two-dimensional matrix that repre-sents the.
possibility of simultaneous modification of twoambiguous attachments.
The rows and columns of the ma-trix show the candidate modifiees of each modifier, and anelement in the matrix means the possibility (1 or 0) thatboth dependencies can exist simultaneously.
For example,constraint table T5.7 indicates that if word 5 modifies word1, then word 7 cannot modify word 3 because of the ruleof no-crossing.By using the constraint tables, the system decides whichambiguity should be resolved first.
This process is calledplanning.
In the above example, words 5, 6, and 7 have twocandidate modifiees each.
But from the constraint tables,we can see that if word 7 modifies word 3, then words 5 and6 cannot modify word 1.
Thus, in this case, the ambiguityconcerning the modification of word 7 should be resolvedfirst.
The algorithm for plauning consists of the followingsteps:1.
On each row of the constraint table Ti.j, sum up theelement values (Ai in Table 2), and subtract he sumfrom the size of the row (Bi).
Then sum up the resultson all rows (Ci).
The result is the value of merit of286the ambiguity of word i.2.
Do the same in each cohmm.
The result is the valueof merit of the ambiguity of word j.3.
In all the constraint tables, sum up all the values ofmerit of each ambiguity, and divide each of these val-ues by the number of their candidate modifiees.4.
The expected values of meTit of all ambiguities aregiven by the above process.
Select the ambiguity thathas the highest expected value.When an ambiguity is resolved, the system updates theconstraint ables by tile filtering algorithm called con-straint propagation.
We apply Mohr and Henderson's AC-4 algorithm \[8\] for constraint propagation.
We reduce thecomputational cost of disambiguation by using planningand constraint propagation.Structural disambiguation f a sentence is done as fol-lows.
The PEG parser tmrses a sentence and constructs itsphrase structure.
The Dependency Stracturc Builder trans-httes the phrase structure int.o the dependency strm:ture,and constructs the constraint tables when the phrase struc-ture contains ew~ral structural ambiguities.
The Plan-ner, which is the component for planning, gives the Dis-ambiguator the information on an ambiguous dependencyand its candidate modifiees.
The Disambiguator decideswhich modifiee is the most preferable by doing path searchand distance calculation.
After resolving one ambiguousattachment, it calls the constraint propagation routine tofilter the other ambiguities' candidates.
After filtering, theTransformer t ansforms the dependency structure into onethat has correc t dependencies forall resolved attachments.These processes are iterated until no ambiguity remains.6 Re la ted  WorkThere are several approaches to structural disambigua-tion, including resolution of prepositional phrase attach-ment.
Wilks et al \[12\] discussed some strategies for dis-ambiguation based on preference semantics.
Our frame-work is closely related to their ideas.
While their strate-gies need hand-coded semantic formulas called preplatesto decide preferences, our system can construct depen-dency knowledge semi-automatically.
Dahlgren and Mc-Dowell \[2\] proposed another preference strategy for prepo-sitional phrase disambiguation.
It is based on ontologicalknowledge, which is manually constructed.
Whereas thisframework (and also that of Wilks et al) was aimed at dis-ambiguating single prepositional phrases in sentences, ourapproach can handle the attachments of multiple preposi-tional phrases in sentences, ttirst \[3\] developed a mech-anism for structural disambiguation, called the SemanticEnquiry Desk, which is based on Chraniak's marker pass-ing paradigm \[1\].
Our path Search is partially equivalentto marker passing.
While marker passing involves a highcomputational cost and finds ninny meaningless relations,our path search is restricted and finds only paths that in-elude synonym/taxonym relationships and dependencies.Our system can reduce the computational cost by using alimited knowledge search.
Jensen and Binot \[6\] developeda heuristic method of prepositional phrase disambiguationusinp, on-line dictionary definitions.
Our approach is sire--liar t,o theirs in the sense that both use dictiouaries asknowledge sources.
The differences are in tile ways inwhich dictionary definitions are used.
While their methodsear{:hes for knowledge by phrasal pattern matching andcalculates certainty factors by complex procedures, oursuses knowledge it: a simt)le and efficient way, searchingtree:: and traversing nodes, and calculates t)referenees byafe, w simplified processes.
Wernlter \[11\] t)rop{}sed a e:}n-neeliol:ist approach to 8lrllctllra.\[ disan:biguation f nounphrases.
He integrated syntaclic and semantic onslraintson lhe relaxation etwork.
~el:laI:tic {2OllS'{l'ailltS ol: prepo-s i t ional  reh:tionshil)s betweetl words are learned by a back-l}ro\]}agation algorithm.
Learned semantics is often veryt:seful for natural language processing, when sexnantic re-htti,mshit)s cannot be represented explicitly.
\\2: represm:tsemantic relationships between words by explicit relation-ship chains, al:d therefore do not need learning by back-propagation.
We integrate sem.mti{: preferences and syn-tactic eonstrailllS t}y using e(mstraint t}ropagathm.
}n:t itis a sequential {:o::ue{'tion a d does not allow their iilterac-.ti{m. \\k!
are thii:king of desigIfinp a frau:ework that dealswilh both syntactic and semanli{: constraints simultam'-ousty.7 Concluding RemarksWe deveh)ped the DepeT~dcrtcy Anal:/zer to re:olve struc-tural ambiguity by sen:antic processing.
It aims t<~ over-come two serious problems in realizing pr:'a'tical semanticpr,~,cessing: : en::@u'::nm:ie conslru{tion of knowledgeand efficient use nf that knowledge.
The key ideas, path..sea~'ch and distance calcuiatiora, ~~e.re shown to be feasible.\Ve now have a knowledge base constructed by usingdefi:,.itions giver~ in the "IBM Dictionary of Computing/'which inch:des about 20.009 instance.s ofdependency :true-turc'.s, h: addilion, we evaluated the system by disan:-biguat.ing the prepositional phrase attachment of about2,0()0 sentence.':.
The results were as follows: (1) :he num-ber of arzflJiguou:-; prepositio::al phrases wa> 4.290, (2) thenumbe, of correctly (lisanfi}iguat.ed a'~ta{:hm{'n:s was 3,569,and (3) the success ratio of disambiguatio:: was 83.2%.Further enhancement plans arc listed be.low:,, ~,?~'.
are exploring the formalization of dependency dis-tar, ce with reference to graph theory.
I)epe.ndeney dis-tMlee is aSsl lnled to be a score lbr the (:OIlSistel~cy of adependency with tim background knowledge and con-text.
The background knowledge and context are rep-resented as trees (special ca.~es of graphs), and c(msis-tency might be defined by a degree of matching be-tween trees.,L.
We are planning to enhance tile system for other prob-lems such as adverb attachment and scope of eonj'unc-tion.s.
To resolve general struetmal ambiguity prob-lems, we must design a general ambiguity-packed syn-tactic strncture, since the system can deal wilh locallypacked ambiguities.Acknowledgementsi would like to thank members of the IBM Tokyo Re-search Laboratory, Karen Jensen of the IBM Thomas J.Watson Research Ceqter, and tile reviewers for their vain-able comments on a draft of this paper, Hiroshi Nomiyamafor his help in implementing tile system, Mizuho Tanaka,~%hko Kobayashi, Mitsuyo Sadohara, and Xbmoko Uehidafor their kind support it: constructing the knowledge baseand evaluating the system, and Michael McDonakl for hishelpful adviee on the wordi\[:g of this paper.References\[1\] Charniak, E., "A Neat Theory of Marker Pressing,"Procec.dirzgs of AAAI-86, 584-588, 1986.\[2\] Dahlgren.
K. and McDowe!l, J., "Using CommonsenseKm~wledge to Disambiguate Prepositional Phr~u~e Xiod-ifiers," Proceedin.q.s of A.4A\[-a< 589-593, !986.\[3\] tlirst, G., Scmanlic hlterpre.t,Ltion and Zhe Rc.s,)hLtio.nof A'mbiguity, Cambridge University Press, 1!357.\[.1\] Jacob:, P. and Zernik.
U., "Acquiring Lexical K'aowl-edge from Text: A Case Study," Proceedings (;f AAAL88, 739-7-1.1, 988.\[5\] Jensen, K., HeMorn, G.E., Richardson, S.D., and Haas,N., "PLNLP, PEG, and CRYI'IQUE: Th,'ee Contribn-tions to Computing in the Humanities."
IBM ResearchReport, EC 11841.
1986.\[6\] Jensen, K. and Binot J-L., "Disambiguating Prep(>sitional Phrase Attachments by Using On-Line Dictio-nary Definitions," Coraputational Ling'u?stics~ 13:251-260, 1987.\[7\] Maruyama, H., "Structural Disambiguation with Con-strain~, Propagation," Proceedings o/ ~l~e 28th AnnualMeeting of the A CL, 1990.\[8\] Mohr, E. and Henderson, T.. "Are and Path Con-siste.ney IIevisited."
Artificial Intelligence, 28:225-233.1986.\[9\] Montanari, U., "Networks of Constraints: Fundamen-tal Properties and Applications to Picture Processing,"Inf~imation Sciences, 7:95-132, 1974.\[10\] Nakamura, J. and Nagao, M., "Extraction of Seman-tic information from an Ordinary English Dictionaryand its Evaluation."
Proceedings of COLING-88.
459-46,1, 1!
)88.\[ll\] Wermter, S., "Integration of Semantic and Syntac-tic Constraints for Structural Noun Phrase t)isambigua-tion," P'roceedirLgs of IJCA\[-89, 1-186-1491, 989.\[i\[2\] Wilks, Y., Huang, X., and Fass, D., "Syntax, Preflu-ence and Right Attachment," Proceedings of I.ICAL85,779 78.1, \]!
)85.287
