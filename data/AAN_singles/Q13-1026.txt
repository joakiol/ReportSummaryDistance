Parsing entire discourses as very long strings: Capturing topic continuity ingrounded language learningMinh-Thang LuongDepartment of Computer ScienceStanford UniversityStanford, Californialmthang@stanford.eduMichael C. FrankDepartment of PsychologyStanford UniversityStanford, Californiamcfrank@stanford.eduMark JohnsonDepartment of ComputingMacquarie UniversitySydney, AustraliaMark.Johnson@MQ.edu.auAbstractGrounded language learning, the task of map-ping from natural language to a representationof meaning, has attracted more and more in-terest in recent years.
In most work on thistopic, however, utterances in a conversationare treated independently and discourse struc-ture information is largely ignored.
In thecontext of language acquisition, this indepen-dence assumption discards cues that are im-portant to the learner, e.g., the fact that con-secutive utterances are likely to share the samereferent (Frank et al 2013).
The current pa-per describes an approach to the problem ofsimultaneously modeling grounded languageat the sentence and discourse levels.
We com-bine ideas from parsing and grammar induc-tion to produce a parser that can handle longinput strings with thousands of tokens, creat-ing parse trees that represent full discourses.By casting grounded language learning as agrammatical inference task, we use our parserto extend the work of Johnson et al(2012),investigating the importance of discourse con-tinuity in children?s language acquisition andits interaction with social cues.
Our modelboosts performance in a language acquisitiontask and yields good discourse segmentationscompared with human annotators.1 IntroductionLearning mappings between natural language (NL)and meaning representations (MR) is an importantgoal for both computational linguistics and cognitivescience.
Accurately learning novel mappings is cru-cial in grounded language understanding tasks andsuch systems can suggest insights into the nature ofchildren language learning.Two influential examples of grounded languagelearning tasks are the sportscasting task, RoboCup,where the NL is the set of running commentary andthe MR is the set of logical forms representing ac-tions like kicking or passing (Chen and Mooney,2008), and the cross-situational word-learning task,where the NL is the caregiver?s utterances and theMR is the set of objects present in the context(Siskind, 1996; Yu and Ballard, 2007).
Workin these domains suggests that, based on the co-occurrence between words and their referents incontext, it is possible to learn mappings between NLand MR even under substantial ambiguity.Nevertheless, contexts like RoboCup?where ev-ery single utterance is grounded?are extremelyrare.
Much more common are cases where a sin-gle topic is introduced and then discussed at lengththroughout a discourse.
In a television news show,for example, a topic might be introduced by present-ing a relevant picture or video clip.
Once the topicis introduced, the anchors can discuss it by nameor even using a pronoun without showing a picture.The discourse is grounded without having to groundevery utterance.Moreover, although previous work has largelytreated utterance order as independent, the order ofutterances is critical in grounded discourse contexts:if the order is scrambled, it can become impossibleto recover the topic.
Supporting this idea, Frank etal.
(2013) found that topic continuity?the tendencyto talk about the same topic in multiple utterancesthat are contiguous in time?is both prevalent andinformative for word learning.
This paper examinesthe importance of topic continuity through a gram-matical inference problem.
We build on Johnson etal.
(2012)?s work that used grammatical inference to315Transactions of the Association for Computational Linguistics, 1 (2013) 315?326.
Action Editor: Mark Steedman.Submitted 2/2013; Revised 6/2013; Published 7/2013.
c?2013 Association for Computational Linguistics.	   		  		 																					Figure 1: Unigram Social Cue PCFGs (Johnson et al 2012) ?
shown is a parse tree of the input utterance ?wheresthe piggie?
accompanied with social cue prefixes, indicating that the caregiver is holding a pig toy while the child islooking at it; at the same time, a dog toy is present in the screen.learn word-object mappings and to investigate therole of social information (cues like eye-gaze andpointing) in a child language acquisition task.Our main contribution lies in the novel integra-tion of existing techniques and algorithms in parsingand grammar induction to offer a complete solutionfor simultaneously modeling grounded language atthe sentence and discourse levels.
Specifically, we:(1) use the Earley algorithm to exploit the specialstructure of our grammars, which are deterministicor have at most bounded ambiguity, to achieve ap-proximately linear parsing time; (2) suggest a rescal-ing approach that enables us to build a PCFG parsercapable of handling very long strings with thou-sands of tokens; and (3) employ Variational Bayesfor grammatical inference to obtain better grammarsthan those given by the EM algorithm.By parsing entire discourses at once, we shed lighton a scientifically interesting question about why thechild?s own gaze is a positive cue for word learn-ing (Johnson et al 2012).
Our data provide supportfor the hypothesis (from previous work) that care-givers ?follow in?
: they name objects that the childis already looking at (Tomasello and Farrar, 1986).In addition, our discourse model produces a perfor-mance improvement in a language acquisition taskand yields good discourse segmentations comparedwith human annotators.2 Related WorkSupervised semantic parsers.
Previous work hasdeveloped supervised semantic parsers to map sen-tences to meaning representations of various forms,including meaning hierarchies (Lu et al 2008) and,most dominantly, ?-calculus expressions (Zettle-moyer and Collins, 2005; Zettlemoyer, 2007; Wongand Mooney, 2007; Kwiatkowski et al 2010).These approaches rely on training data of annotatedsentence-meaning pairs, however.
Such data arecostly to obtain and are quite different from the ex-perience of language learners.Grounded Language Learning.
In contrast to se-mantic parsers, grounded language learning systemsaim to learn the meanings of words and sentencesgiven an observed world state (Yu and Ballard, 2004;Gorniak and Roy, 2007).
A growing body of workin this field employs distinct techniques from a widevariety of perspectives from text-to-record align-ment using structured classification (Barzilay andLapata, 2005; Snyder and Barzilay, 2007), iterativeretraining (Chen et al 2010), and generative modelsof segmentation and alignment (Liang et al 2009)to text-to-interaction mapping using reinforcementlearning (Branavan et al 2009; Vogel and Juraf-sky, 2010), graphical model semantics representa-tion (Tellex et al 2011a; Tellex et al 2011b), andCombinatory Categorial Grammar (Artzi and Zettle-moyer, 2013).
A number of systems have also usedalternative forms of supervision, including sentencespaired with responses (Clarke et al 2010; Gold-wasser and Roth, 2011; Liang et al 2011) andno supervision (Poon and Domingos, 2009; Gold-316wasser et al 2011).Recent work has also introduced an alternativeapproach to grounded learning by reducing it to agrammatical inference problem.
Bo?rschinger et al(2011) casted the problem of learning a semanticparser as a PCFG induction task, achieving state-ofthe art performance in the RoboCup domain.
Kimand Mooney (2012) extended the technique to makeit tractable for more complex problems.
Later, Kimand Mooney (2013) adapted discriminative rerank-ing to the grounded learning problem using a formof weak supervision.
We employ this general gram-matical inference approach in the current work.Children Language Acquisition.
In the context oflanguage acquisition, Frank et al(2008) proposeda system that learned words and jointly inferredspeakers?
intended referent (utterance topic) usinggraphical models.
Johnson et al(2012) used gram-matical inference to demonstrate the importance ofsocial cues in children?s early word learning.
We ex-tend this body of work by capturing discourse-baseddependencies among utterances rather than treatingeach utterance independently.Discourse Parsing.
A substantial literature has ex-amined formal representations of discourse acrossa wide variety of theoretical perspectives (Mannand Thompson, 1988; Scha and Polanyi, 1988;Hobbs, 1990; Lascarides and Asher, 1993; Knottand Sanders, 1997).
Although much of this workwas highly influential, Marcu (1997)?s work on dis-course parsing brought this task to special promi-nence.
Since then, more and more sophisticatedmodels of discourse analysis have been developed:,e.g., (Marcu, 1999; Soricut andMarcu, 2003; Forbeset al 2003; Polanyi et al 2004; Baldridge and Las-carides, 2005; Subba and Di Eugenio, 2009; Her-nault et al 2010; Lin et al 2012; Feng and Hirst,2012).
Our contribution to work on this task isto examine latent discourse structure specifically ingrounded language learning.3 A Grounded Learning TaskOur focus in this paper is to develop computationalmodels that help us better understand children?s lan-guage acquisition.
The goal is to learn both thelong term lexicon of mappings between words andobjects (language learning) as well as the intendedtopic of individual utterances (language comprehen-sion).
We consider a corpus of child-directed speechannotated with social cues, described in (Frank etal., 2013).
There are a total of 4,763 utterancesin the corpus, each of which is orthographically-transcribed from videos of caregivers playing withpre-linguistic children of various ages (6, 12, and18 months) during home visits.1 Each utterancewas hand-annotated with objects present in the(non-linguistic) context, e.g.
dog and pig (Fig-ure 1), together with sets of social cues, one setper object.
The social cues describe objects thecare-giver is looking at (mom.eyes), holding onto(mom.hands), or pointing to (mom.point); sim-ilarly, for (child.eyes) and (child.hands).3.1 Sentence-level ModelsMotivated by the importance of social informationin children?s early language acquisition (Carpenteret al 1998), Johnson et al(2012) proposed a jointmodel of non-linguistic information including thephysical context and social cues, and the linguis-tic content of individual utterances.
They framedthe joint inference problem of inferring word-objectmappings and inferring sentence topics as a gram-mar induction task where input strings are utterancesprefixed with non-linguistic information.
Objectspresent in the non-linguistic context of an utteranceare considered its potential topics.
There is also aspecial null topic, None, to indicate non-topical ut-terances.
The goal of the model is then to select themost probable topic for each utterance.Top-level rules, Sentence ?
Topict Wordst(unigram PCFG) or Sentence ?
TopictCollocst (collocation Adaptor Grammar), are tai-lored to link the two modalities (t ranges over T ?,the set of all available topics (T ) and None).
Theserules enforce sharing of topics between prefixes(Topict) and words (Wordst or Collocst).
Eachword in the utterance is drawn from either a topic-specific distribution Wordt or a general ?null?
dis-tribution WordNone.As illustrated in Figure 1, the selected topic, pig,is propagated down to the input string through twopaths: (a) through topical nodes until an object is1Caregivers were given pairs of toys to play with, e.g.
astuffed dog and pig, or a wooden car and truck.317reached, in this case the .pig object, and (b) throughlexical nodes to topical word tokens, e.g.
piggie.
So-cial cues are then generated by a series of binarydecisions as detailed in Johnson et al(2012).
Thekey feature of these grammars is that parameter in-ference corresponds both to learning word-topic re-lations and learning the salience of social cues ingrounded learning.In the current work, we restrict our attention toonly the unigram PCFG model to focus on investi-gating the role of topic continuity.
Unlike the ap-proach of Johnson et al(2012), which uses MarkovChain Monte Carlo techniques to perform gram-matical inference, we experiment with VariationalBayes methods, detailed in Section 6.3.2 A Discourse-level ModelTopic continuity?the tendency to group utterancesinto coherent discourses about a single topic?maybe an important source of information for childrenlearning the meanings of words (Frank et al 2013).To address this issue, we consider a new discourse-level model of grounded language that captures de-pendencies between utterances.
By linking multipleutterances in a single parse, our proposed grammati-cal formalism is a bigram Markov process that mod-els transitions among utterance topics.Our grammar starts with a root symbolDiscourse, which then selects a startingtopic through a set of discourse initial rules,Discourse ?
Discourset for t ?
T ?.
Eachof the Discourset nodes generates an utter-ance of the same topic, and advances into othertopics through transition rules, Discourset ?Sentencet Discourset?
for t?
?
T ?.
Dis-courses terminate by ending rules, Discourset?
Sentencet.
Other rules in the unigram PCFGmodel by Johnson are reused except for the top-level rules in which we replace the non-terminalSentence by topic-specific ones Sentencet.3.3 Parsing Discourses and ChallengesUsing a discourse-level grammar, we must parsea concatenation of all the utterances (with annota-tions) in each conversation.
This concatenation re-sults in an extremely long string: in the social-cuecorpus (Frank et al 2013), the average length ofthese per-recording concatenations is 2152 tokens(?=972).
Parsing such strings poses many chal-lenges for existing algorithms.For familiar algorithms such as CYK, runtimequickly becomes enormous: the time complexity ofCYK is O(n3) for an input of length n. Fortunately,we can take advantage of a special structural prop-erty of our grammars.
The shape of the parse tree iscompletely determined by the input string; the onlyvariation is in the topic annotations in the nonter-minal labels.
So even though the number of possi-ble parses grows exponentially with input length n,the number of possible constituents grows only lin-early with input length, and the possible constituentscan be identified from the left context.2 These con-straints ensure that the Earley algorithm3 (Earley,1970) will parse an input of length n with this gram-mar in time O(n).A second challenge in parsing very long strings isthat the probability of a parse is the product of theprobabilities of the rules involved in its derivation.As the length of a derivation grows linearly with thelength of the input, the parse probabilities decreaseexponentially as a function of sentence length, caus-ing floating-point underflow on inputs of even mod-erate length.
The standard method for handling thisis to compute log probabilities (which decrease lin-early as a function of input length, rather than ex-ponentially), but as we explain later (Section 5), wecan use the ability of the Earley algorithm to com-pute prefix probabilities (Stolcke, 1995) to rescalethe probability of the parse incrementally and avoidfloating-point underflows.In the next section, we provide background in-formation on the Earley algorithm for PCFGs, theprefix probability scheme we use, and the inside-outside algorithm in the Earley context.4 Background4.1 Earley Algorithm for PCFGsThe Earley algorithm was developed by Earley(1970) and known to be efficient for certain kindsof CFGs (Aho and Ullman, 1972).
An Earley parser2The prefix markers # and ## and the topic markers suchas ?.dog?
enable a left-to- right parser to unambiguously iden-tify its location in the input string.3In order to achieve linear time the parsing chart must havesuitable indexing; see Aho and Ullman (1972), Leo (1991) andAycock and Horspool (2002) for details.318constructs left-most derivations of strings, using dot-ted productions to keep track of partial derivations.Specifically, each state in an Earley parser is rep-resented as [l, r]: X??
.
?
to indicate that inputsymbols xl, .
.
.
, xr?1 have been processed and theparser is expecting to expand ?.
States are gen-erated on the fly using three transition operations:predict (add states to charts), scan (shift dots acrossterminals), and complete (merge two states).
Fig-ure 2 shows an example of a completion step whichalso illustrates the implicit binarization automati-cally done in Earley algorithm.  		  		    Figure 2: Completion step ?
merging two states [l,m]:X??
.
Y ?
and [m, r]:Y??
.
to produce a new state[l, r]: X?
?Y .
?.In order to handle PCFGs, Stolcke (1995) extendsthe Earley parsing algorithm to introduce the no-tion of an Earley path being a sequence of stateslinked by Earley operations.
By establishing a one-to-one mapping between partial derivations and Ear-ley paths, Stolcke could then assign each path aderivation probability, that is the product of the allrule probabilities used in the predicted states of thatpath.
Here, each production X??
corresponds to apredicted state [l, l] : X?.
?.Besides parsing, being able to compute string andprefix probabilities by summing derivation probabil-ities is also of great importance.
To compute thesesums efficiently, each Earley state is attached with aforward and an inner probability which are updatedincrementally as new states are spawned by the threetransition operations.4.2 Forward and Prefix ProbabilitiesIntuitively, the forward probability of a state [l, r]:X??
.
?
is the probability of an Earley paththrough that state, generating input up to positionr-1.
This probability generalizes a similar conceptin HMM and lends itself to the computation of pre-fix probabilities, sums of forward probabilities overscanned states yielding a prefix x.Computing prefix probabilities is important be-cause it enables probabilistic prediction of pos-sible follow-words xi+1 as P (xi+1|x0 .
.
.
xi) =P (x0...xixi+1)P (x0...xi) (Jelinek and Lafferty, 1991).
Theseconditional probabilities allow estimation of the in-cremental costs of a stack decoder (Bahl et al1983).
In (Huang and Sagae, 2010), a conceptu-ally similar prefix cost is defined to order states ina beam search decoder.
Moreover, the negative log-arithm of such conditional probabilities are termedas surprisal values in the psycholinguistics literature(e.g., Hale, 2001; Levy, 2008), to describe how dif-ficult a word is in a given context.
Interestingly, weshow that prefix probabilities lead us to construct aparser that could parse extremely long strings next.4.3 Inside Outside AlgorithmTo extend the Inside Outside (IO) algorithm (Baker,1979) to the Earley context, Stolcke introduced in-ner and outer probabilities which generalize the in-side and outside probabilities in the IO algorithm.Specifically, the inner probability of a state [l, r]:X??
.
?
is the probability of generating an inputsubstring xl, .
.
.
, xr?1 from a non-terminalX usinga production X??
?.4  		  		    Figure 3: Inner and outer probabilities.
The outerprobability of X??
.
Y ?
is a sum of all products ofits parent outer probability (X?
?Y .
?)
and its siblinginner probability (Y??
.).
Similarly, the outer proba-bility of Y??
.
is derived from the outer probability ofX?
?Y .
?
and the inner probability of X??
.
Y ?.Once all inner probabilities have been populatedin a forward pass, outer probabilities are derivedbackward, starting from the outer probability of thegoal state [0, n] :?
S .
being 1.
Here, each Earleystate is associated with an outer probability whichcomplements the inner probability by referring pre-cisely to those parts (not covered by the correspond-ing inner probability) of the complete paths generat-ing the input string x.
The implicit binarization in4Summing up inner probabilities of all states Y ??
.
exactlyyields Baker?s inside probability for Y .319Earley parsing allows outer probabilities to be accu-mulated in a similar way as its counterpart in the IOalgorithm (see Figure 3).These quantities allow for efficient grammaticalinference in which the expected count of each ruleX??
given a string x is computed as:c(X?
?|x) =?s:[l,r]X?.?
outer(s) ?
inner(s)P (S ??
x) .
(1)5 A Rescaling Approach for ParsingOur parser originated from the prefix probabilityparser by Levy (2008), but has diverged markedlysince then.
The parser, called Earleyx5, is ca-pable of producing Viterbi parses and performinggrammatical induction based on the expectation-maximization and variational Bayes algorithms.To tackle the underflow problem posed whenparsing discourses (?3.3), we borrow the rescal-ing concept from HMMs (Rabiner, 1990) to extendthe probabilistic Earley algorithm.
Specifically, theprobability of each Earley path is scaled by a con-stant ci each time it passes through a scanned stategenerating the input symbol xi.
In fact, each pathpasses through each scanned state exactly once, sowe consistently accumulate scaling factors for theforward and inner probabilities of a state [l, r] :X??
.
?
as c0 .
.
.
cr?1 and cl .
.
.
cr?1 respectively.Arguably, the most intuitive choice of the scal-ing factors are the prefix probabilities, which essen-tially resets the probability of any Earley path start-ing from any position i to 1.
Concretely, we setc0 = 1P (x0) and ci =P (x0...xi?1)P (x0...xi) for i=1, .
.
.
, n-1 where n is the input length.
As noted in section?4.2, the logarithm of ci gives us the surprisal valuefor the input symbol xi.Rescaling factors are only introduced in the for-ward pass, during which the outer probability of astate [l, r]: X??
.
?
has already been scaled by fac-tors c0 .
.
.
cl?1cr .
.
.
cn?1.6 More importantly, when5Parser code is available at http://nlp.stanford.edu/?lmthang/earleyx.6The outer probability of a state is essentially the product ofinner probabilities covering all input symbols outside the spanof that state.
For grammars containing cyclic unit productions,we also need to multiply with terms from the unit-productionrelation matrix (Stolcke, 1995).computing expected counts, scaling factors in theouter and inner terms cancel out with those in thestring probability in Eq.
(1), implying that rule prob-ability estimation is unaffected by rescaling.5.1 Parsing Time on Dense GrammarsWe compare in Table 1 the parsing time (on a2.4GHz Xeon CPU) of our parser (Earleyx) andLevy?s.
The task is to compute surprisal values fora 22-word sentence over a dense grammar.7 Giventhat our parser is now capable of performing scalingto avoid underflow, we avoid converting probabili-ties to logarithmic form, which yields a speedup ofabout 4 times compared to Levy?s parser.Parser Time (s)(Levy, 2008) 640Earleyx + scaling 145Table 1: Parsing time (dense grammars) ?
to computesurprisal values for a 22-word sentence using Levy?sparser and ours (Earleyx).5.2 Parsing Time on Sparse Grammars200 400 600 800 1000 1200 1400 1600 1800 20001234567Parsing Time on Sparse Grammars# WordssecondsFigure 4: Parsing time (sparse grammars) ?
to computeViterbi parses for sentences of increasing lengths.Figure 4 shows the time taken (as a function ofthe input length) for Earleyx to compute a Viterbiparses over our sparse grammars (?3.2).
The plotconfirmed our analysis in that the special structureof our grammars yields approximately linear parsingtime in the input length (see ?3.3).7MLE estimated from the English Penn Treebank.3206 Grammar InductionWe employ a Variational Bayes (VB) approach toperform grammatical inference instead of the stan-dard Inside Outside (IO) algorithm, or equivalentlythe Expectation Maximization (EM) algorithm, forseveral reasons: (1) it has been shown to be lesslikely to cause over-fitting for PCFGs than EM(Kurihara and Sato, 2004) and (2) implementation-wise, VB is a straightforward extension from EMas they both share the same process of computingthe expected counts (the IO part) and only differat how rule probabilities are reestimated.
At thesame time, VB has also been demonstrated to dowell on large datasets and is competitive with Gibbssamplers while having the fastest convergence timeamong these estimators (Gao and Johnson, 2008).The rule reestimation in VB is carried as fol-lows.
Let ?r be the prior hyperparameter of arule r in the rule set R and cr be its expectedcount accumulated over the entire corpus after anIO iteration.
The posterior hyperparameter for r is?
?r = ?r + cr.
Let ?
be the digamma function,the rule parameter update formula is: ?r:X??
=exp[?
(??r)?
?(?r?:X???
??r?
)].Whereas IO minimizes the negative log-likelihood of the observed data (sentences),-log p(x), VB minimizes a quantity called freeenergy, which we will use later to monitor con-vergence.
Here x denotes the observed data and?
represents the model parameters (PCFG ruleprobabilities).
Following (Kurihara and Sato, 2006),we compute the free energy as:F(x,?)
= ?
log p(x) +?X?Nlog ?
(?r:X??
??r)?
(?r:X??
?r)?
?r?R(log ?
(??r)?
(?r)+ cr log ?r)where ?
denotes the gamma function.6.1 Sparse Dirichlet PriorsIn our application, since each topic should only beassociated with a few words rather than the entirevocabulary, we impose sparse Dirichlet priors overthe Wordt distributions by setting a symmetric prior?<1 for all rules Wordt?w (?t ?
T,w ?
W ),where W is the set of all words in the corpus.
Thisbiases the model to select only a few rules per non-terminal Wordt.8 For all other rules, a uniform hy-perparameter value of 1 is used.
We initialized ruleprobabilities with uniform distributions plus randomnoise.
It is worthwhile to mention that sparse Dirich-let priors were proposed in Johnson (2010)?s workthat learns Latent Dirchlet Allocation topic modelsusing Bayesian inference for PCFGs.7 ExperimentsOur experiments apply sentence- and discourse-level models to the annotated corpus of child-directed speech described in Section 3.
Each modelis evaluated on (a) topic accuracy?how many utter-ances are labeled with correct topics (including thenull), (b) topic metrics (f-scores/precision/recall)?how well the model predicts non-null topical utter-ances, (c) word metrics?how well the model pre-dicts topical words,9 and (d) lexicon metrics?howwell word types are assigned to the topic that theyattach to most frequently.
For example, in Figure 1,the model assigns topic pig to the entire utterance.At the word level, it labels piggiewith topic pig andassigns null topic to wheres and the.
See (Johnson etal., 2012) for more details of these metrics.In Section 7.1, we examine baseline models thatdo not make use of social cues (mother and child?seye-gaze and hand position) to discover the topic;these baselines are contrasted with a range of socialcues (?7.2 and ?7.3).
In Section 7.4, we evaluate thediscourse structures discovered by our models.7.1 Baseline Models (No Social Cues)To create baselines for later experiments, we eval-uate our models without social information.
Wecompare sentence-level models using three differentinference procedures?Markov Chain Monte Carlo(MCMC) (Johnson et al 2012), Expectation Max-imization (EM), and Variational Bayes (VB)10?aswell as the discourse-level model described above.8It is important to not sparsify the WordNone distributionsince WordNone could expand into many non-topical words.9Topics assigned by the model are compared with thosegiven by the gold dictionary provided by (Johnson et al 2012).10To determine the best sparsity hyperparameter ?
forlexical rules (?6.1), we performed a line search over{1,0.1, 0.01, 0.001, 0.0001}.
As ?
decreases, performance im-proves, peaking at 0.001, the value used for all reported results321Model Topic Word Lexicon EnergyAcc.
F1 P R F1 P R F1 P RMCMC 49.07 60.64 48.67 80.43 29.50 17.63 90.31 14.83 8.10 88.10VB 53.14 60.89 50.53 76.59 25.62 14.94 89.91 16.71 9.25 85.71 156719discourse 51.02 59.40 48.60 76.35 23.86 13.82 87.33 15.05 8.27 83.33 150023discourse+init 55.78 60.91 52.15 73.22 29.75 17.91 87.65 21.11 11.95 90.48 149458Table 2: Social-cue models.
Comparison of sentence- and discourse-level models (init: initialized from the VBsentence-level model) over full metrics.
Free energies are shown to compare VB-based models.Model Acc.
Topic F1 Word F1 Lexicon F1MCMC 33.95 40.44 20.07 10.37EM 32.08 39.76 13.31 6.09VB 39.64 39.22 17.40 12.27discourse 40.63 42.01 19.31 12.72Table 3: Baseline (non-social) models.
Comparison ofsentence-level models (MCMC (Johnson et al 2012),EM, VB) and the discourse-level model.Results in Table 3 suggest that incorporatingtopic continuity through the discourse model boostsperformance compared to sentence-level models.Within sentence-level models, EM is inferior to bothMCMC and VB (in accordance with the consensusthat EM is likely to overfit for PCFGs).
ComparingVB and MCMC, VB is significantly better at topicaccuracy but is worse at topic F1.
This result sug-gests that VB predicts that more utterances are non-topical compared with MCMC, perhaps explainingwhy MCMC has the highest word F1.
Nevertheless,unlike VB, the discourse model outperformsMCMCin all topic metrics, indicating that topic continuityhelps in predicting both null and topical utterances.The discourse model is also capable of captur-ing topical transitions.
Examining one instance ofa learned grammar reveals that the distribution un-der Discourset is often dominated by a fewmajortransitions.
For example, car tends to have transi-tions into car (0.72) and truck (0.19); while pigprefers to transit into pig (0.69) and dog (0.24).These learned transitions nicely recover the struc-ture of the task that caregivers were given: to playwith toy pairs like car/truck and pig/dog.7.2 Social-cue ModelsWe next explore how topic continuity interacts withsocial information via a set of simulations mirroringthose in the previous section.
Results are shown inTable 2.
For the sentence-level models using socialcues, VB now outperformsMCMC in topic accuracyand F1, as well as lexicon evaluations, suggestingthat VB is overall quite competitive with MCMC.11Turning to the discourse models, social informa-tion and topic continuity both independently boostlearning performance (as evidenced in Johnson et al(2012) and in Section 7.1).
Nevertheless, joint infer-ence using both information sources (discourse row)resulted in a performance decrement.
Rather thanreflecting issues in the model itself, perhaps the in-creased complexity of the inference problem mighthave led to this performance decrement.To test this explanation, we initialized ourdiscourse-level model with the VB sentence-levelmodel.
Results are shown in the discourse+initrow.
With a sentence-level initialization, perfor-mance improved substantially, yielding the best re-sults over most metrics.
In addition, the discoursemodel with sentence-level initialization achievedlower free energy than the standard initialization dis-course model.
Both of these results support the hy-pothesis that initialization facilitated inference in themore complex discourse model.
From a cognitivescience perspective, this sort of result may point tothe utility of beginning the task of discourse segmen-tation with some initial sentence-level expectations.7.3 Effects of Individual Social CuesThe importance of particular social cues and theirrelationship to discourse continuity is an additionaltopic of interest from the cognitive science per-spective (Frank et al 2013).
Returning to one ofthe questions that motivated this work, we can use11Detailed breakdown of word f-scores reveals that MCMCis much better at precision, indicating that VB predicts morewords as topical than MCMC.
An explanation for such effectis that we use the same ?
for all lexical rules, which results insuboptimal sparsity levels for Wordt distributions.
For MCMC,Johnson et al(2012) used the adaptor grammar software tolearn the hyperparameters automatically from data.322all no.child.eyes no.child.hands no.mom.eyes no.mom.hands no.mom.pointMCMC 49.1/60.6/29.5/14.8 38.4/46.6/21.5/11.1 49.1/60.6/29.6/15.3 48.0/59.7/29.0/15.5 48.7/60.0/29.3/15.6 48.8/60.3/29.3/15.6VB 53.1/60.9/25.62/16.71 49.3/56.0/22.6/15.1 52.9/60.4/26.2/16.2 51.5/59.1/24.6/16.3 51.9/59.2/25.3/16.3 52.9/60.6/25.5/16.6discourse+init 55.8/60.9/29.8/21.1 53.7/59.2/27.8/19.7?+ 55.2/60.7/29.0/21.4+ 54.7/60.0/29.0/21.6 55.2/60.1/29.1/21.4 55.6/60.8/29.5/21.7Table 4: Social cue influence.
Ablation test results across models without discourse (MCMC, VB) and with discourse(discourse+init).
We start with the full set of social cues and drop one at a time.
Each cell contains results for metrics:topic accuracy/topic F1/word F1/lexicon F1.
For row discourse+init, we compare models with/without a social cueusing chi-square tests and denote statistically significant results (p < .05) at the utterance (?)
and word (+) levels.none child.eyes child.hands mom.eyes mom.hands mom.pointMCMC 34.0/40.4/20.1/10.4 45.7/57.3/28.9/13.6 34.0/40.1/20.1/9.7 33.8/40.2/19.9/9.7 35.6/42.8/19.8/10.0 30.6/35.5/18.1/9.2VB 39.6/39.2/17.4/12.27 47.2/53.0/21.9/13.9 43.0/45.8/15.4/12.9 42.9/46.5/14.6/12.4 41.1/43.8/17.1/12.4 39.7/39.7/17.5/13.4discourse 40.7/41.8/19.2/12.1 47.8/55.4/22.8/14.2?+ 44.6/50.8/20.3/13.1?+ 44.7/50.1/21.7/14.3?+ 42.7/46.4/19.0/11.6+ 38.7/40.2/16.6/11.9?+Table 5: Social cue influence.
Add-one test results across models without discourse (MCMC, VB) and with discourse(discourse).
We start with no social information and add one cue at a time.
Each cell contains results for metrics:topic accuracy/topic F1/word F1/lexicon F1.
For row discourse, we compare models with/without a social cue usingchi-square tests and denote statistically significant results (p < .05) at the utterance (?)
and word (+) levels.our discourse model to answer the question aboutthe role that the child.eyes cue plays in child-directed discourses.
Johnson et al(2012) raisedtwo hypotheses that could explain the importance ofchild.eyes as a social cue: (1) caregivers ?fol-low in?
on the child?s gaze: they tend to talk aboutwhat the child is looking at (Baldwin, 1993), or (2)the child.eyes cue encodes the topic of the pre-vious sentence, inadvertently giving a non-discoursemodel access to rudimentary discourse information.To address this question, we conduct two tests:(1) ablation ?
eliminating each social cue in turn(e.g.
child.eyes), and (2) add-one, using a sin-gle social cue per turn.
Table 4 and 5 show corre-sponding results for models without discourse (theMCMC and VB sentence-level models) and withdiscourse (discourse+init for the ablation test anddiscourse for the add-one test).
We observe simi-lar trends to Johnson et al(2012): the childs gaze isthe most important cue.
Removing it from the fullmodel with all social cues or adding it to the basemodel with no cues both result in the largest perfor-mance change; in both cases this change is statisti-cally reliable.12 The large performance differencesfor child.eyes are consistent with the hypothe-sis that caregivers are following in, or discussing theobject that children are interested in ?
even control-12It is somewhat surprising when child.eye has much lessinfluence on VB than onMCMC in the ablation test.
Though re-sults in the add-one test reveal that VB generalizes much betterthan MCMC when presented with a single social cue, it remainsinteresting to find out internally what causes the difference.ling for the continuity of discourse, a confound inprevious analyses.
In other words, the importanceof child.eyes in the discourse model suggeststhat this cue encodes useful information in additionto the intersentential discourse topic.7.4 Discourse Structure EvaluationWhile the discourse model performs well using met-rics from previous work, these metrics do not fullyreflect an important strength of the model: its abil-ity to capture inter-utterance structure.
For exam-Raw Discourse Utterancecar car come here lets find the carcar therecar car is that a carcar car the car goes vroom vroom vroomTable 6: Topic annotation examples.
raw (previousmetrics) and discourse (new metrics).ple, consider the sequence of utterances in Table 6.Our previous evaluation is based on the raw annota-tion, which labels as topical only utterances contain-ing topical words or pronouns referring to an object.As a result, classifying ?there?
as car is incorrect.From the perspective of a human listener, however,?there?
is part of a broader discourse about the car,and labeling it with the same topic captures the factthat it encodes useful information for learners.
Todifferentiate these cases, Frank and Rohde (under re-view) added a new set of annotations (to the datasetused in Section 7) based on the discourse structureperceived by human, similar to column discourse, .323We utilize these new annotations to judge topicspredicted by our discourse model and adopt previ-ous metrics for discourse segmentation evaluation:a=b, a simple proportion equivalence of discourseassignments; pk, a window method (Beeferman etal., 1999) to measure the probability of two randomutterances correctly classified as being in the samediscourse; and WindowDiff (Pevzner and Hearst,2002), an improved version of pk which gives ?par-tial credit?
to boundaries close to the correct ones.Results in Table 7 demonstrate that our model isin better agreement with human annotation (model-human) than the raw annotation (raw-human) acrossall metrics.
As is visible from the limited changein the a=b metric, relatively few topic assignmentsare altered; yet these alterations create much morecoherent discourses that allow for far better segmen-tation performance under pk and WindowDiff.raw-human model-humana=b 63.6 69.3pk 57.0 83.6WindowDiff 36.2 61.2Table 7: Discourse evaluation.
Single annotator sample,comparison between topics assigned by the raw annota-tion, our discourse model, and a human coder.To put an upper bound on possible discourse seg-mentation results, we further evaluated performanceon a subset of 634 utterances for which multiple an-notations were collected.
Results in Table 8 demon-strate that our model predicts discourse topics (m-h1,m-h1) at a level quite close to the level of agreementbetween human annotators (column h1-h2).r-h1 r-h2 m-h1 m-h2 h1-h2a=b 60.1 65.6 70.4 72.4 81.7pk 50.7 51.8 85.1 84.9 89.7WindowDiff 29.0 30.1 60.1 66.9 72.7Table 8: Discourse evaluation.
Multiple annotator sam-ple, comparison between raw annotations (r), our model(m), and two independent human coders (h1, h2).8 Conclusion and Future WorkIn this paper, we proposed a novel integration ofexisting techniques in parsing and grammar induc-tion to offer a complete solution for simultaneouslymodeling grounded language at the sentence anddiscourse levels.
Specifically, we used the Ear-ley algorithm to exploit the special structure of ourgrammars to achieve approximately linear parsingtime, introduced a rescaling approach to handle verylong input strings, and utilized Variational Bayes forgrammar induction to obtain better solutions thanthe Expectation Maximization algorithm.By transforming a grounded language learningproblem into a grammatical inference task, we usedour parser to study how discourse structure couldfacilitate children?s language acquisition.
In ad-dition, we investigate the interaction between dis-course structure and social cues, both importantand complementary sources of information in lan-guage learning (Baldwin, 1993; Frank et al 2013).We also examined why individual children?s gazewas an important predictor of reference in previ-ous work (Johnson et al 2012).
Using ablationtests, we showed that information provided by thechild?s gaze is still valuable even in the presence ofdiscourse continuity, supporting the hypothesis thatparents ?follow in?
on the particular focus of chil-dren?s attention (Tomasello and Farrar, 1986).Lastly, we showed that our models can produceaccurate discourse segmentations.
Our system?s out-put is considerably better than the raw topic anno-tations provided in the previous social cue corpus(Frank et al 2013) and is in good agreement withdiscourse topics assigned by human annotators inFrank and Rohde (under review).In conclusion, although previous work ongrounded language learning has treated individualutterances as independent entities, we have shownthat the ability to incorporate discourse informationcan be quite useful for such problems.
Discoursecontinuity is an important source of information inchildren language acquisition and may be a valuablepart of future grounded language learning systems.AcknowledgementsWe thank the TACL action editor, Mark Steed-man, and the anonymous reviewers for their valu-able feedback, as well as Chris Manning for helpfuldiscussions.
This research was supported under theAustralian Research Council?s Discovery Projectsfunding scheme (project numbers DP110102506and DP110102593).324ReferencesAlfred V. Aho and Jeffery D. Ullman.
1972.
The The-ory of Parsing, Translation and Compiling; Volume 1:Parsing.
Prentice-Hall, Englewood Cliffs, New Jersey.Yoav Artzi and Luke S. Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mapping in-structions to actions.
Transactions of the Associationfor Computational Linguistics, 1:49?62.John Aycock and R. Nigel Horspool.
2002.
Practical ear-ley parsing.
The Computer Journal, 45(6):620?630.Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.1983.
A maximum likelihood approach to continu-ous speech recognition.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 5(2):179 ?190.James K. Baker.
1979.
Trainable grammars for speechrecognition.
The Journal of the Acoustical Society ofAmerica, 65(S1):S132.Jason Baldridge and Alex Lascarides.
2005.
Proba-bilistic head-driven parsing for discourse structure.
InCONLL.Dare A. Baldwin.
1993.
Infants?
ability to consult thespeaker for clues to word reference.
Journal of ChildLanguage, 20:395?418.Regina Barzilay and Mirella Lapata.
2005.
Collectivecontent selection for concept-to-text generation.
InHLT-EMNLP.Doug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34(1-3):177?210.Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-son.
2011.
Reducing grounded learning tasks to gram-matical inference.
In EMNLP.S.
R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In ACL-IJCNLP.Malinda Carpenter, Katherine Nagell, MichaelTomasello, George Butterworth, and Chris Moore.1998.
Social cognition, joint attention, and com-municative competence from 9 to 15 months of age.Monographs of the society for research in childdevelopment, 63(4).David L. Chen and Raymond J. Mooney.
2008.
Learningto sportscast: A test of grounded language acquisition.In ICML.David L. Chen, Joohyun Kim, and Raymond J. Mooney.2010.
Training a multilingual sportscaster: Using per-ceptual context to learn language.
Journal of ArtificialIntelligence Research, 37:397?435.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing from theworld?s response.
In CoNLL.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-leveldiscourse parsing with rich linguistic features.
In ACL.Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad,Anoop Sarkar, Joshi Aravind, and Bonnie Webber.2003.
D-ltag system: Discourse parsing with a lexical-ized tree adjoining grammar.
Journal of Logic, Lan-guage and Information, 12:261?279.Michael C. Frank and Hannah Rohde.
under review.Markers of topical discourse in child-directed speech.Michael C. Frank, Noah D. Goodman, and Josh B. Tenen-baum.
2008.
A Bayesian framework for cross-situational word-learning.
Advances in Neural Infor-mation Processing Systems 20.Michael C. Frank, Joshua B. Tenenbaum, and AnneFernald.
2013.
Social and discourse contributionsto the determination of reference in cross-situationalword learning.
Language Learning and Development,9(1):1?24.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofBayesian estimators for unsupervised Hidden MarkovModel POS taggers.
In EMNLP.Dan Goldwasser and Dan Roth.
2011.
Learning fromnatural instructions.
In IJCAI.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised semanticparsing.
In ACL.Peter Gorniak and Deb Roy.
2007.
Situated languageunderstanding as filtering perceived affordances.
Cog-nitive Science, 31(2):197?231.Hugo Hernault, Helmut Prendinger, David A. duVerle,and Mitsuru Ishizuk.
2010.
HILDA: A discourseparser using support vector machine classification.
Di-alogue and Discourse, 1(3):1?33.Jerry R. Hobbs.
1990.
Literature and Cognition.
CSLILecture Notes 21.Liang Huang and Kenji Sagae.
2010.
Dynamic program-ming for linear-time incremental parsing.
In ACL.Frederick Jelinek and John D. Lafferty.
1991.
Compu-tation of the probability of initial substring generationby stochastic context-free grammars.
ComputationalLinguistics, 17(3):315?323.Mark Johnson, Katherine Demuth, and Michael Frank.2012.
Exploiting social information in grounded lan-guage learning via grammatical reduction.
In ACL.Mark Johnson.
2010.
Pcfgs, topic models, adaptor gram-mars and learning topical collocations and the struc-ture of proper names.
In ACL.Joohyun Kim and Raymond J. Mooney.
2012.
Unsu-pervised pcfg induction for grounded language learn-ing with highly ambiguous supervision.
In EMNLP-CoNLL.Joohyun Kim and Raymond J. Mooney.
2013.
Adaptingdiscriminative reranking to grounded language learn-ing.
In ACL.325Alistair Knott and Ted Sanders.
1997.
The classificationof coherence relations and their linguistic markers: Anexploration of two languages.
Journal of Pragmatics,30(2):135?175.Kenichi Kurihara and Taisuke Sato.
2004.
An appli-cation of the variational bayesian approach to proba-bilistic contextfree grammars.
In IJCNLP WorkshopBeyond Shallow Analyses.Kenichi Kurihara and Taisuke Sato.
2006.
Variationalbayesian grammar induction for natural language.
InICGI.Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-water, and Mark Steedman.
2010.
Inducing proba-bilistic ccg grammars from logical form with higher-order unification.
In EMNLP.Alex Lascarides and Nicholas Asher.
1993.
Temporalinterpretation, discourse relations, and common senseentailment.
Linguistics and Philosophy, 16(5):437?493.Joop M. I. M. Leo.
1991.
A general context-free parsingalgorithm running in linear time on every lr(k) gram-mar without using lookahead.
Theoretical ComputerScience, 82(1):165?176.Roger Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106(3):1126?1177.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In ACL-IJCNLP, pages 91?99.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional semantics.In Association for Computational Linguistics (ACL).Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012.
Apdtb-styled end-to-end discourse parser.
Natural Lan-guage Engineering, FirstView:1?34.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-moyer.
2008.
A generative model for parsing naturallanguage to meaning representations.
In EMNLP.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Daniel Marcu.
1997.
The rhetorical parsing of naturallanguage texts.
In ACL.Daniel Marcu.
1999.
A decision-based approach torhetorical parsing.
In ACL.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28(1):19?36.Livia Polanyi, Chris Culy, Martin Van Den Berg,Gian Lorenzo Thione, and David Ahn.
2004.
A rulebased approach to discourse parsing.
In SigDIAL.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In EMNLP.Lawrence R. Rabiner.
1990.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.Remko Scha and Livia Polanyi.
1988.
An augmentedcontext free grammar for discourse.
In COLING.Jeffrey M. Siskind.
1996.
A computational studyof cross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1-2):39?91.Benjamin Snyder and Regina Barzilay.
2007.
Database-text alignment via structured multilabel classification.In IJCAI.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical informa-tion.
In NAACL.Andreas Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computes prefixprobabilities.
Computational Linguistics, 21(2):165?201.Rajen Subba and Barbara Di Eugenio.
2009.
An effec-tive discourse parser that uses rich linguistic informa-tion.
In NAACL.Stefanie Tellex, Thomas Kolla, Steven Dickerson,Matthew R. Walter, Ashis G. Banerjee, Seth Teller,and Nicholas.
Roy.
2011a.
Approaching the symbolgrounding problem with probabilistic graphical mod-els.
AI Magazine, 32(4):64?76.Stefanie Tellex, Thomas Kolla, Steven Dickerson,Matthew R.Walter, Ashis G. Banerjee, Seth Teller, andNicholas Roy.
2011b.
Understanding Natural Lan-guage Commands for Robotic Navigation and MobileManipulation.
In AAAI.Michael Tomasello and Michael Jeffrey Farrar.
1986.Joint attention and early language.
Child development,pages 1454?1463.Adam Vogel and Daniel Jurafsky.
2010.
Learning to fol-low navigational directions.
In ACL.Yuk Wah Wong and Raymond J. Mooney.
2007.
Learn-ing synchronous grammars for semantic parsing withlambda calculus.
In ACL.Chen Yu and Dana H. Ballard.
2004.
On the integrationof grounding language and learning objects.
In AAAI.Chen Yu and Dana H Ballard.
2007.
A unified model ofearly word learning: Integrating statistical and socialcues.
Neurocomputing, 70(13):2149?2165.Luke S. Zettlemoyer and Michael Collins.
2005.
Learn-ing to Map Sentences to Logical Form: StructuredClassification with Probabilistic Categorial Grammars.In UAI.Michael Zettlemoyer, Luke S.and Collins.
2007.
Onlinelearning of relaxed CCG grammars for parsing to log-ical form.
In EMNLP-CoNLL.326
