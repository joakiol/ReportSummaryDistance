Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1?27,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsCoNLL-2011 Shared Task:Modeling Unrestricted Coreference in OntoNotesSameer PradhanBBN Technologies,Cambridge, MA 02138pradhan@bbn.comLance RamshawBBN Technologies,Cambridge, MA 02138lramshaw@bbn.comMitchell MarcusUniversity of Pennsylvania,Philadelphia, 19104mitch@linc.cis.upenn.eduMartha PalmerUniversity of Colorado,Boulder, CO 80309martha.palmer@colorado.eduRalph WeischedelBBN Technologies,Cambridge, MA 02138weischedel@bbn.comNianwen XueBrandeis University,Waltham, MA 02453xuen@cs.brandeis.eduAbstractThe CoNLL-2011 shared task involved pre-dicting coreference using OntoNotes data.
Re-sources in this field have tended to be lim-ited to noun phrase coreference, often on arestricted set of entities, such as ACE enti-ties.
OntoNotes provides a large-scale corpusof general anaphoric coreference not restrictedto noun phrases or to a specified set of en-tity types.
OntoNotes also provides additionallayers of integrated annotation, capturing ad-ditional shallow semantic structure.
This pa-per briefly describes the OntoNotes annota-tion (coreference and other layers) and thendescribes the parameters of the shared taskincluding the format, pre-processing informa-tion, and evaluation criteria, and presents anddiscusses the results achieved by the partic-ipating systems.
Having a standard test setand evaluation parameters, all based on a newresource that provides multiple integrated an-notation layers (parses, semantic roles, wordsenses, named entities and coreference) thatcould support joint models, should help to en-ergize ongoing research in the task of entityand event coreference.1 IntroductionThe importance of coreference resolution for theentity/event detection task, namely identifying allmentions of entities and events in text and clusteringthem into equivalence classes, has been well recog-nized in the natural language processing community.Automatic identification of coreferring entities andevents in text has been an uphill battle for severaldecades, partly because it can require world knowl-edge which is not well-defined and partly owing tothe lack of substantial annotated data.
Early workon corpus-based coreference resolution dates backto the mid-90s by McCarthy and Lenhert (1995)where they experimented with using decision treesand hand-written rules.
A systematic study wasthen conducted using decision trees by Soon et al(2001).
Significant improvements have been madein the field of language processing in general, andimproved learning techniques have been developedto push the state of the art in coreference resolu-tion forward (Morton, 2000; Harabagiu et al, 2001;McCallum and Wellner, 2004; Culotta et al, 2007;Denis and Baldridge, 2007; Rahman and Ng, 2009;Haghighi and Klein, 2010).
Various different knowl-edge sources from shallow semantics to encyclo-pedic knowledge are being exploited (Ponzetto andStrube, 2005; Ponzetto and Strube, 2006; Versley,2007; Ng, 2007).
Researchers continued findingnovel ways of exploiting ontologies such as Word-Net.
Given that WordNet is a static ontology andas such has limitation on coverage, more recently,there have been successful attempts to utilize in-formation from much larger, collaboratively builtresources such as Wikipedia (Ponzetto and Strube,2006).
In spite of all the progress, current techniquesstill rely primarily on surface level features such asstring match, proximity, and edit distance; syntac-tic features such as apposition; and shallow seman-tic features such as number, gender, named entities,semantic class, Hobbs?
distance, etc.
A better ideaof the progress in the field can be obtained by read-ing recent survey articles (Ng, 2010) and tutorials(Ponzetto and Poesio, 2009) dedicated to this sub-ject.Corpora to support supervised learning of thistask date back to the Message Understanding Con-ferences (MUC).
These corpora were tagged withcoreferring entities identified by noun phrases in thetext.
The de facto standard datasets for current coref-erence studies are the MUC (Hirschman and Chin-1chor, 1997; Chinchor, 2001; Chinchor and Sund-heim, 2003) and the ACE1 (G. Doddington et al,2000) corpora.
The MUC corpora cover all nounphrases in text, but represent small training and testsets.
The ACE corpora, on the other hand, have muchmore annotation, but are restricted to a small subsetof entities.
They are also less consistent, in terms ofinter-annotator agreement (ITA) (Hirschman et al,1998).
This lessens the reliability of statistical ev-idence in the form of lexical coverage and seman-tic relatedness that could be derived from the dataand used by a classifier to generate better predic-tive models.
The importance of a well-defined tag-ging scheme and consistent ITA has been well rec-ognized and studied in the past (Poesio, 2004; Poe-sio and Artstein, 2005; Passonneau, 2004).
Thereis a growing consensus that in order for these to bemost useful for language understanding applicationssuch as question answering or distillation ?
both ofwhich seek to take information access technologyto the next level ?
we need more consistent anno-tation of larger amounts of broad coverage data fortraining better automatic techniques for entity andevent identification.
Identification and encoding ofricher knowledge ?
possibly linked to knowledgesources ?
and development of learning algorithmsthat would effectively incorporate them is a neces-sary next step towards improving the current stateof the art.
The computational learning community,in general, is also witnessing a move towards eval-uations based on joint inference, with the two pre-vious CoNLL tasks (Surdeanu et al, 2008; Hajic?
etal., 2009) devoted to joint learning of syntactic andsemantic dependencies.
A principle ingredient forjoint learning is the presence of multiple layers ofsemantic information.One fundamental question still remains, and thatis ?
what would it take to improve the state of the artin coreference resolution that has not been attemptedso far?
Many different algorithms have been tried inthe past 15 years, but one thing that is still lackingis a corpus comprehensively tagged on a large scalewith consistent, multiple layers of semantic infor-mation.
One of the many goals of the OntoNotesproject2 (Hovy et al, 2006; Weischedel et al, 2011)is to explore whether it can fill this void and helppush the progress further ?
not only in coreference,but with the various layers of semantics that it triesto capture.
As one of its layers, it has created acorpus for general anaphoric coreference that cov-1http://projects.ldc.upenn.edu/ace/data/2http://www.bbn.com/nlp/ontonotesers entities and events not limited to noun phrasesor a limited set of entity types.
A small portion ofthis corpus from the newswire and broadcast newsgenres (?120k) was recently used for a SEMEVALtask (Recasens et al, 2010).
As mentioned earlier,the coreference layer in OntoNotes constitutes justone part of a multi-layered, integrated annotation ofshallow semantic structure in text with high inter-annotator agreement, which also provides a uniqueopportunity for performing joint inference over asubstantial body of data.The remainder of this paper is organized asfollows.
Section 2 presents an overview of theOntoNotes corpus.
Section 3 describes the coref-erence annotation in OntoNotes.
Section 4 then de-scribes the shared task, including the data providedand the evaluation criteria.
Sections 5 and 6 then de-scribe the participating system results and analyzethe approaches, and Section 7 concludes.2 The OntoNotes CorpusThe OntoNotes project has created a corpus of large-scale, accurate, and integrated annotation of multi-ple levels of the shallow semantic structure in text.The idea is that this rich, integrated annotation cov-ering many layers will allow for richer, cross-layermodels enabling significantly better automatic se-mantic analysis.
In addition to coreference, thisdata is also tagged with syntactic trees, high cov-erage verb and some noun propositions, partial verband noun word senses, and 18 named entity types.However, such multi-layer annotations, with com-plex, cross-layer dependencies, demands a robust,efficient, scalable mechanism for storing them whileproviding efficient, convenient, integrated access tothe the underlying structure.
To this effect, it uses arelational database representation that captures boththe inter- and intra-layer dependencies and also pro-vides an object-oriented API for efficient, multi-tiered access to this data (Pradhan et al, 2007a).This should facilitate the creation of cross-layer fea-tures in integrated predictive models that will makeuse of these annotations.Although OntoNotes is a multi-lingual resourcewith all layers of annotation covering three lan-guages: English, Chinese and Arabic, for the scopeof this paper, we will just look at the English por-tion.
Over the years of the development of this cor-pus, there were various priorities that came into play,and therefore not all the data in the English portion isannotated with all the different layers of annotation.There is a core portion, however, which is roughly21.3M words which has been annotated with all thelayers.
It comprises ?450k words from newswire,?150k from magazine articles, ?200k from broad-cast news, ?200k from broadcast conversations and?200k web data.OntoNotes comprises the following layers of an-notation:?
Syntax ?
A syntactic layer representing a re-vised Penn Treebank (Marcus et al, 1993;Babko-Malaya et al, 2006).?
Propositions ?
The proposition structure ofverbs in the form of a revised PropBank(Palmeret al, 2005; Babko-Malaya et al, 2006).?
Word Sense ?
Coarse grained word sensesare tagged for the most frequent polysemousverbs and nouns, in order to maximize cov-erage.
The word sense granularity is tailoredto achieve 90% inter-annotator agreement asdemonstrated by Palmer et al (2007).
Thesesenses are defined in the sense inventory filesand each individual sense has been connectedto multiple WordNet senses.
This provides adirect access to the WordNet semantic struc-ture for users to make use of.
There is also amapping from the word senses to the PropBankframes and to VerbNet (Kipper et al, 2000) andFrameNet (Fillmore et al, 2003).?
Named Entities ?
The corpus was tagged witha set of 18 proper named entity types thatwere well-defined and well-tested for inter-annotator agreement by Weischedel and Burn-stein (2005).?
Coreference ?
This layer captures generalanaphoric coreference that covers entities andevents not limited to noun phrases or a limitedset of entity types (Pradhan et al, 2007b).
Wewill take a look at this in detail in the next sec-tion.3 Coreference in OntoNotesGeneral anaphoric coreference that spans a rich setof entities and events ?
not restricted to a few types,as has been characteristic of most coreference dataavailable until now ?
has been tagged with a highdegree of consistency.
Attributive coreference istagged separately from the more common identitycoreference.Two different types of coreference are distin-guished in the OntoNotes data: Identical (IDENT),and Appositive (APPOS).
Appositives are treatedseparately because they function as attributions, asdescribed further below.
The IDENT type is usedfor anaphoric coreference, meaning links betweenpronominal, nominal, and named mentions of spe-cific referents.
It does not include mentions ofgeneric, underspecified, or abstract entities.Coreference is annotated for all specific entitiesand events.
There is no limit on the semantic typesof NP entities that can be considered for coreference,and in particular, coreference is not limited to ACEtypes.The mentions over which IDENT coreference ap-plies are typically pronominal, named, or definitenominal.
The annotation process begins by auto-matically extracting all of the NP mentions from thePenn Treebank, though the annotators can also addadditional mentions when appropriate.
In the fol-lowing two examples (and later ones), the phrasesnotated in bold form the links of an IDENT chain.
(1) She had a good suggestion and it was unani-mously accepted by all.
(2) Elco Industries Inc. said it expects net incomein the year ending June 30, 1990, to fall below arecent analyst?s estimate of $ 1.65 a share.
TheRockford, Ill. maker of fasteners also said itexpects to post sales in the current fiscal yearthat are ?slightly above?
fiscal 1989 sales of $155 million.3.1 VerbsVerbs are added as single-word spans if they canbe coreferenced with a noun phrase or with an-other verb.
The intent is to annotate the VP, but wemark the single-word head for convenience.
This in-cludes morphologically related nominalizations (3)and noun phrases that refer to the same event, evenif they are lexically distinct from the verb (4).
In thefollowing two examples, only the chains related tothe growth event are shown.
(3) Sales of passenger cars grew 22%.
The stronggrowth followed year-to-year increases.
(4) Japan?s domestic sales of cars, trucks and busesin October rose 18% from a year earlier to500,004 units, a record for the month, the JapanAutomobile Dealers?
Association said.
Thestrong growth followed year-to-year increasesof 21% in August and 12% in September.33.2 PronounsAll pronouns and demonstratives are linked to any-thing that they refer to, and pronouns in quotedspeech are also marked.
Expletive or pleonastic pro-nouns (it, there) are not considered for tagging, andgeneric you is not marked.
In the following exam-ple, the pronoun you and it would not be marked.
(Inthis and following examples, an asterisk (*) before aboldface phrase identifies entity/event mentions thatwould not be tagged as coreferent.
)(5) Senate majority leader Bill Frist likes to tell astory from his days as a pioneering heart sur-geon back in Tennessee.
A lot of times, Frist re-calls, *you?d have a critical patient lying therewaiting for a new heart, and *you?d want tocut, but *you couldn?t start unless *you knewthat the replacement heart would make *it tothe operating room.3.3 Generic mentionsGeneric nominal mentions can be linked with refer-ring pronouns and other definite mentions, but arenot linked to other generic nominal mentions.
Thiswould allow linking of the bracketed mentions in (6)and (7), but not (8).
(6) Officials said they are tired of making the samestatements.
(7) Meetings are most productive when they areheld in the morning.
Those meetings, however,generally have the worst attendance.
(8) Allergan Inc. said it received approval tosell the PhacoFlex intraocular lens, the firstfoldable silicone lens available for *cataractsurgery.
The lens?
foldability enables it to beinserted in smaller incisions than are now pos-sible for *cataract surgery.Bare plurals, as in (6) and (7), are always consid-ered generic.
In example (9) below, there are twogeneric instances of parents.
These are marked asdistinct IDENT chains (with separate chains distin-guished by subscripts X, Y and Z), each containinga generic and the related referring pronouns.
(9) ParentsX should be involved with theirX chil-dren?s education at home, not in school.
TheyXshould see to it that theirX kids don?t play tru-ant; theyX should make certain that the childrenspend enough time doing homework; theyXshould scrutinize the report card.
ParentsY aretoo likely to blame schools for the educationallimitations of theirY children.
If parentsZ aredissatisfied with a school, theyZ should havethe option of switching to another.In (10) below, the verb ?halve?
cannot be linkedto ?a reduction of 50%?, since ?a reduction?
is in-definite.
(10) Argentina said it will ask creditor banks to*halve its foreign debt of $64 billion ?
thethird-highest in the developing world .
Ar-gentina aspires to reach *a reduction of 50%in the value of its external debt.3.4 Pre-modifiersProper pre-modifiers can be coreferenced, butproper nouns that are in a morphologically adjecti-val form are treated as adjectives, and not corefer-enced.
For example, adjectival forms of GPEs suchas Chinese in ?the Chinese leader?, would not belinked.
Thus we could coreference United States in?the United States policy?
with another referent, butnot American ?the American policy.?
GPEs and Na-tionality acronyms (e.g.
U.S.S.R. or U.S.).
are alsoconsidered adjectival.
Pre-modifier acronyms can becoreferenced unless they refer to a nationality.
Thusin the examples below, FBI can be coreferenced toother mentions, but U.S.
cannot.
(11) FBI spokesman(12) *U.S. spokesmanDates and monetary amounts can be consideredpart of a coreference chain even when they occur aspre-modifiers.
(13) The current account deficit on France?s balanceof payments narrowed to 1.48 billion Frenchfrancs ($236.8 million) in August from a re-vised 2.1 billion francs in July, the FinanceMinistry said.
Previously, the July figure wasestimated at a deficit of 613 million francs.
(14) The company?s $150 offer was unexpected.The firm balked at the price.3.5 Copular verbsAttributes signaled by copular structures are notmarked; these are attributes of the referent they mod-ify, and their relationship to that referent will becaptured through word sense and propositional ar-gument tagging.4(15) JohnX is a linguist.
PeopleY are nervousaround JohnX, because heX always correctstheirY grammar.Copular (or ?linking?)
verbs are those verbs thatfunction as a copula and are followed by a sub-ject complement.
Some common copular verbs are:be, appear, feel, look, seem, remain, stay, become,end up, get.
Subject complements following suchverbs are considered attributes, and not linked.
SinceCalled is copular, neither IDENT nor APPOS corefer-ence is marked in the following case.
(16) Called Otto?s Original Oat Bran Beer, the brewcosts about $12.75 a case.3.6 Small clausesLike copulas, small clause constructions are notmarked.
The following example is treated as if thecopula were present (?John considers Fred to be anidiot?
):(17) John considers *Fred *an idiot.3.7 Temporal expressionsTemporal expressions such as the following arelinked:(18) John spent three years in jail.
In that time...Deictic expressions such as now, then, today, to-morrow, yesterday, etc.
can be linked, as well asother temporal expressions that are relative to thetime of the writing of the article, and which maytherefore require knowledge of the time of the writ-ing to resolve the coreference.
Annotators were al-lowed to use knowledge from outside the text in re-solving these cases.
In the following example, theend of this period and that time can be coreferenced,as can this period and from three years to sevenyears.
(19) The limit could range from three years toseven yearsX, depending on the compositionof the management team and the nature of itsstrategic plan.
At (the end of (this period)X)Y,the poison pill would be eliminated automati-cally, unless a new poison pill were approvedby the then-current shareholders, who wouldhave an opportunity to evaluate the corpora-tion?s strategy and management team at thattimeY.In multi-date temporal expressions, embeddeddates are not separately connected to to other men-tions of that date.
For example in Nov. 2, 1999, Nov.would not be linked to another instance of Novemberlater in the text.3.8 AppositivesBecause they logically represent attributions, appos-itives are tagged separately from Identity corefer-ence.
They consist of a head, or referent (a nounphrase that points to a specific object/concept in theworld), and one or more attributes of that referent.An appositive construction contains a noun phrasethat modifies an immediately-adjacent noun phrase(separated only by a comma, colon, dash, or paren-thesis).
It often serves to rename or further definethe first mention.
Marking appositive constructionsallows us to capture the attributed property eventhough there is no explicit copula.
(20) Johnhead, a linguistattributeThe head of each appositive construction is distin-guished from the attribute according to the followingheuristic specificity scale, in a decreasing order fromtop to bottom:Type ExampleProper noun JohnPronoun HeDefinite NP the manIndefinite specific NP a man I knowNon-specific NP manThis leads to the following cases:(21) Johnhead, a linguistattribute(22) A famous linguistattribute, hehead studied at ...(23) a principal of the firmattribute, J. SmithheadIn cases where the two members of the appositiveare equivalent in specificity, the left-most member ofthe appositive is marked as the head/referent.
Defi-nite NPs include NPs with a definite marker (the) aswell as NPs with a possessive adjective (his).
Thusthe first element is the head in all of the followingcases:(24) The chairman, the man who never gives up(25) The sheriff, his friend(26) His friend, the sheriffIn the specificity scale, specific names of diseasesand technologies are classified as proper names,whether they are capitalized or not.
(27) A dangerous bacteria, bacillium, is found5Type DescriptionAnnotator Error An annotator error.
This is a catch-all category for cases of errors that do not fit in the othercategories.Genuine Ambiguity This is just genuinely ambiguous.
Often the case with pronouns that have no clear an-tecedent (especially this & that)Generics One person thought this was a generic mention, and the other person didn?tGuidelines The guidelines need to be clear about this exampleCallisto Layout Something to do with the usage/design of CallistoReferents Each annotator thought this was referring to two completely different thingsPossessives One person did not mark this possessiveVerb One person did not mark this verbPre Modifiers One person did not mark this Pre ModifierAppositive One person did not mark this appositiveExtent Both people marked the same entity, but one person?s mention was longerCopula Disagreement arose because this mention is part of a copular structurea) Either each annotator marked a different half of the copulab) Or one annotator unnecessarily marked bothFigure 1: Description of various disagreement typesFigure 1: The distribution of disagreements across the various types in Table 2Sheet1Page 1Copulae 2%Appositives 3%Pre Modifiers 3%Verbs 3%Possessives 4%Refer nts 7%Callisto Layout 8%Guidelines 8%Generics 11%Genuine Ambiguity 25%Annotator Error 26%CopulaeAppositivesPre ModifiersVerbsPossessivesReferentsCallisto LayoutGuidelinesGenericsGenuine AmbiguityAnnotator Error0% 5% 10% 15% 20% 25% 30%Figure 2: The distribution of disagreements across the various types in Table 1When the entity to which an appositive refers isalso mentioned elsewhere, only the single span con-taining the entire appositive construction is includedin the larger IDENT chain.
None of the nested NPspans are linked.
In the example below, the en-tire span can be linked to later mentions to RichardGodown.
The sub-spans are not included separatelyin the IDENT chain.
(28) Richard Godown, president of the Indus-trial Biotechnology AssociationAges are tagged as attributes (as if they were el-lipses of, for example, a 42-year-old):(29) Mr.Smithhead, 42attribute,3.9 Special IssuesIn addition to the ones above, there are some specialcases such as:?
No coreference is marked between an organi-zation and its members.Genre ANN1-ANN2 ANN1-ADJ ANN2-ADJNewswire 80.9 85.2 88.3Broadcast News 78.6 83.5 89.4Broadcast Conversation 86.7 91.6 93.7Magazine 78.4 83.2 88.8Web 85.9 92.2 91.2Table 1: Inter Annotator and Adjudicator agreement forthe Coreference Layer in OntoNotes measured in termsof the MUC score.?
GPEs are linked to references to their govern-ments, even when the references are nestedNPs, or the modifier and head of a single NP.3.10 Annotator Agreement and AnalysisTable 1 shows the inter-annotator and annotator-adjudicator agreement on all the genres ofOntoNotes.
We also analyzed about 15K dis-agreements in various parts of the data, and groupedthem into one of the categories shown in Figure 1.Figure 2 shows the distribution of these differenttypes that were found in that sample.
It can be6seen that genuine ambiguity and annotator errorare the biggest contributors ?
the latter of which isusually captured during adjudication, thus showingthe increased agreement between the adjudicatedversion and the individual annotator version.4 CoNLL-2011 Coreference TaskThis section describes the CoNLL-2011 Corefer-ence task, including its closed and open track ver-sions, and characterizes the data used for the taskand how it was prepared.4.1 Why a Coreference Task?Despite close to a two-decade history of evaluationson coreference tasks, variation in the evaluation cri-teria and in the training data used have made it dif-ficult for researchers to be clear about the state ofthe art or to determine which particular areas requirefurther attention.
There are many different parame-ters involved in defining a coreference task.
Lookingat various numbers reported in literature can greatlyaffect the perceived difficulty of the task.
It can seemto be a very hard problem (Soon et al, 2001) or onethat is somewhat easier (Culotta et al, 2007).
Giventhe space constraints, we refer the reader to Stoy-anov et al (2009) for a detailed treatment of theissue.Limitations in the size and scope of the availabledatasets have also constrained research progress.The MUC and ACE corpora are the two that havebeen used most for reporting comparative results,but they differ in the types of entities and corefer-ence annotated.
The ACE corpus is also one thatevolved over a period of almost five years, with dif-ferent incarnations of the task definition and dif-ferent corpus cross-sections on which performancenumbers have been reported, making it hard to un-tangle and interpret the results.The availability of the OntoNotes data offered anopportunity to define a coreference task based on alarger, more broad-coverage corpus.
We have triedto design the task so that it not only can support thecurrent evaluation, but also can provide an ongoingresource for comparing different coreference algo-rithms and approaches.4.2 Task DescriptionThe CoNLL-2011 shared task was based on the En-glish portion of the OntoNotes 4.0 data.
The taskwas to automatically identify mentions of entitiesand events in text and to link the coreferring men-tions together to form entity/event chains.
The targetcoreference decisions could be made using automat-ically predicted information on the other structurallayers including the parses, semantic roles, wordsenses, and named entities.As is customary for CoNLL tasks, there were twotracks, closed and open.
For the closed track, sys-tems were limited to using the distributed resources,in order to allow a fair comparison of algorithm per-formance, while the open track allowed for almostunrestricted use of external resources in addition tothe provided data.4.2.1 Closed TrackIn the closed track, systems were limited to the pro-vided data, plus the use of two pre-specified externalresources: i) WordNet and ii) a pre-computed num-ber and gender table by Bergsma and Lin (2006).For the training and test data, in addition to theunderlying text, predicted versions of all the supple-mentary layers of annotation were provided, wherethose predictions were derived using off-the-shelftools (parsers, semantic role labelers, named entitytaggers, etc.)
as described in Section 4.4.2.
For thetraining data, however, in addition to predicted val-ues for the other layers, we also provided manualgold-standard annotations for all the layers.
Partici-pants were allowed to use either the gold-standard orpredicted annotation for training their systems.
Theywere also free to use the gold-standard data to traintheir own models for the various layers of annota-tion, if they judged that those would either providemore accurate predictions or alternative predictionsfor use as multiple views, or wished to use a latticeof predictions.More so than previous CoNLL tasks, corefer-ence predictions depend on world knowledge, andmany state-of-the-art systems use information fromexternal resources such as WordNet, which canadd a layer that helps the system to recognize se-mantic connections between the various lexical-ized mentions in the text.
Therefore, the use ofWordNet was allowed, even for the closed track.Since word senses in OntoNotes are predominantly3coarse-grained groupings of WordNet senses, sys-tems could also map from the predicted or gold-standard word senses provided to the sets of under-lying WordNet senses.
Another significant piece ofknowledge that is particularly useful for coreferencebut that is not available in the layers of OntoNotes isthat of number and gender.
There are many different3There are a few instances of novel senses introduced inOntoNotes which were not present in WordNet, and so lack amapping back to the WordNet senses7ways of predicting these values, with differing accu-racies, so in order to ensure that participants in theclosed track were working from the same data, thusallowing clearer algorithmic comparisons, we spec-ified a particular table of number and gender predic-tions generated by Bergsma and Lin (2006), for useduring both training and testing.Following the recent CoNLL tradition, partici-pants were allowed to use both the training and thedevelopment data for training the final model.4.2.2 Open TrackIn addition to resources available in the closed track,the open track, systems were allowed to use externalresources such as Wikipedia, gazetteers etc.
Thistrack is mainly to get an idea of a performance ceil-ing on the task at the cost of not getting a compar-ison across all systems.
Another advantage of theopen track is that it might reduce the barriers to par-ticipation by allowing participants to field existingresearch systems that already depend on external re-sources ?
especially if there were hard dependen-cies on these resources.
They can participate in thetask with minimal or no modification to their exist-ing system.4.3 Coreference Task DataSince there are no previously reported numbers onthe full version of OntoNotes, we had to createa train/development/test partition.
The only por-tion of OntoNotes that has a previously determined,widely used, standard split is the WSJ portion of thenewswire data.
For that subcorpus, we maintainedthe same partition.
For all the other portions we cre-ated stratified training, development and test parti-tions over all the sources in OntoNotes using the pro-cedure shown in Algorithm 1.
The list of training,development and test document IDs can be found onthe task webpage.44.4 Data PreparationThis section gives details of the different annota-tion layers including the automatic models that wereused to predict them, and describes the formats inwhich the data were provided to the participants.4.4.1 Manual Annotation Gold LayersWe will take a look at the manually annotated, orgold layers of information that were made availablefor the training data.4http://conll.bbn.com/download/conll-train.idhttp://conll.bbn.com/download/conll-dev.idhttp://conll.bbn.com/download/conll-test.idAlgorithm 1 Procedure used to create OntoNotestraining, development and test partitions.Procedure: GENERATE PARTITIONS(ONTONOTES) returns TRAIN,DEV, TEST1: TRAIN?
?2: DEV?
?3: TEST?
?4: for all SOURCE ?
ONTONOTES do5: if SOURCE = WALL STREET JOURNAL then6: TRAIN?
TRAIN ?
SECTIONS 02 ?
217: DEV?
DEV ?
SECTIONS 00, 01, 22, 248: TEST?
TEST ?
SECTION 239: else10: if Number of files in SOURCE ?
10 then11: TRAIN?
TRAIN ?
FILE IDS ending in 1 ?
812: DEV?
DEV ?
FILE IDS ending in 013: TEST?
TEST ?
FILE IDS ending in 914: else15: DEV?
DEV ?
FILE IDS ending in 016: TEST?
TEST ?
FILE ID ending in the highest number17: TRAIN?
TRAIN ?
Remaining FILE IDS for theSOURCE18: end if19: end if20: end for21: return TRAIN, DEV, TESTCoreference The manual coreference annotationis stored as chains of linked mentions connectingmultiple mentions of the same entity.
Coreference isthe only document-level phenomenon in OntoNotes,and the complexity of annotation increases non-linearly with the length of a document.
Unfortu-nately, some of the documents ?
especially ones inthe broadcast conversation, weblogs, and telephoneconversation genre ?
are very long which prohib-ited us from efficiently annotating them in entirety.These had to be split into smaller parts.
We con-ducted a few passes to join some adjacent parts, butsince some documents had as many as 17 parts, thereare still multi-part documents in the corpus.
Sincethe coreference chains are coherent only within eachof these document parts, for this task, each such partis treated as a separate document.
Another thingto note is that there were some cases of sub-tokenannotation in the corpus owing to the fact that to-kens were not split at hyphens.
Cases such as pro-WalMart had the sub-span WalMart linked with anotherinstance of the same.
The recent Treebank revisionwhich split tokens at most hyphens, made a majorityof these sub-token annotations go away.
There werestill some residual sub-token annotations.
Sincesubtoken annotations cannot be represented in theCoNLL format, and they were a very small quantity?
much less than even half a percent ?
we decided toignore them.For various reasons, not all the documents inOntoNotes have been annotated with all the differ-8Corpora Words DocumentsTotal Train Dev Test Total Train Dev TestMUC-6 25K 12K 13K 60 30 30MUC-7 40K 19K 21K 67 30 37ACE (2000-2004) 1M 775K 235K - - -OntoNotes5 1.3M 1M 136K 142K 2,083(2,999) 1,674(2,374) 202(303) 207(322)Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets.
Thenumbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.Each part was considered a separate document for evaluation purposes.Syntactic category Train Development TestCount % Count % Count %NP 60,345 59.71 8,463 59.31 8,629 53.09PRP 25,472 25.21 3,535 24.78 5,012 30.84PRP$ 8,889 8.80 1,208 8.47 1,466 9.02NNP 2,643 2.62 468 3.28 475 2.92NML 900 0.89 151 1.06 118 0.73Vx 1,915 1.89 317 2.22 314 1.93Other 893 0.88 126 0.88 239 1.47Overall 101,057 100.00 14,268 100.00 16,253 100.00Table 3: Distribution of mentions in the data by their syn-tactic category.Train Development TestEntities/Chains 26,612 3,752 3,926Links 74,652 10,539 12,365Mentions 101,264 14,291 16,291Table 4: Number of entities, links and mentions in theOntoNotes 4.0 data.ent layers of annotation, with full coverage.6 Thereis a core portion, however, which is roughly 1.3Mwords which has been annotated with all the layers.This is the portion that we used for the shared task.The number of documents in the corpus for thistask, for each of the different genres, are shown inTable 2.
Tables 3 and 4 shows the distribution ofmentions by the syntactic categories, and the countsof entities, links and mentions in the corpus respec-tively.
All of this data has been Treebanked andPropBanked either as part of the OntoNotes effortor some preceding effort.For comparison purposes, Table 2 also lists thenumber of documents in the MUC-6, MUC-7, andACE (2000-2004) corpora.
The MUC-6 data wastaken from the Wall Street Journal, whereas theMUC-7 data was from the New York Times.
TheACE data spanned many different genres similar to6Given the nature of word sense annotation, and changes inproject priorities, we could not annotate all the low frequencyverbs and nouns in the corpus.
Furthermore, PropBank annota-tion currently only covers verb predicates.the ones in OntoNotes.Parse Trees This represents the syntactic layerthat is a revised version of the Penn Treebank.
Forpurposes of this task, traces were removed from thesyntactic trees, since the CoNLL-style data format,being indexed by tokens, does not provide any goodmeans of conveying that information.
Function tagswere also removed, since the parsers that we usedfor the predicted syntax layer did not provide them.One thing that needs to be dealt with in conversa-tional data is the presence of disfluencies (restarts,etc.).
In the original OntoNotes parses, these aremarked using a special EDITED7 phrase tag ?
as wasthe case for the Switchboard Treebank.
Given thefrequency of disfluencies and the performance withwhich one can identify them automatically,8 a prob-able processing pipeline would filter them out be-fore parsing.
Since we did not have a readily avail-able tagger for tagging disfluencies, we decided toremove them using oracle information available inthe Treebank.Propositions The propositions in OntoNotes con-stitute PropBank semantic roles.
Most of the verbpredicates in the corpus have been annotated withtheir arguments.
Recent enhancements to the Prop-Bank to make it synchronize better with the Tree-bank (Babko-Malaya et al, 2006) have enhancedthe information in the proposition by the addition oftwo types of LINKs that represent pragmatic corefer-ence (LINK-PCR) and selectional preferences (LINK-SLC).
More details can be found in the addendum tothe PropBank guidelines9 in the OntoNotes 4.0 re-7There is another phrase type ?
EMBED in the telephone con-versation genre which is similar to the EDITED phrase type, andsometimes identifies insertions, but sometimes contains logicalcontinuation of phrases, so we decided not to remove that fromthe data.8A study by Charniak and Johnson (2001) shows that onecan identify and remove edits from transcribed conversationalspeech with an F-score of about 78, with roughly 95 Precisionand 67 recall.9doc/propbank/english-propbank.pdf9lease.
Since the community is not used to this rep-resentation which relies heavily on the trace struc-ture in the Treebank which we are excluding, we de-cided to unfold the LINKs back to their original rep-resentation as in the Release 1.0 of the PropositionBank.
This functionality is part of the OntoNotesDB Tool.10Word Sense Gold word sense annotation wassupplied using sense numbers as specified inthe OntoNotes list of senses for each lemma.11The sense inventories that were provided in theOntoNotes 4.0 release were not all mapped to the lat-est version 3.0 of WordNet, so we provided a revisedversion of the sense inventories, containing mappingto WordNet 3.0, on the task page for the participants.Named Entities Named Entities in OntoNotesdata are specified using a catalog of 18 Name types.Other Layers Discourse plays a vital role incoreference resolution.
In the case of broadcast con-versation, or telephone conversation data, it partiallymanifests in the form of speakers of a given utter-ance, whereas in weblogs or newsgroups it does soas the writer, or commenter of a particular articleor thread.
This information provides an importantclue for correctly linking anaphoric pronouns withthe right antecedents.
This information could be au-tomatically deduced, but since it would add addi-tional complexity to the already complex task, wedecided to provide oracle information of this meta-data both during training and testing.
In other words,speaker and author identification was not treatedas an annotation layer that needed to be predicted.This information was provided in the form of an-other column in the .conll table.
There were somecases of interruptions and interjections that ideallywould associate parts of a sentence to two differentspeakers, but since the frequency of this was quitesmall, we decided to make an assumption of onespeaker/writer per sentence.4.4.2 Predicted Annotation LayersThe predicted annotation layers were derived usingautomatic models trained using cross-validation onother portions of OntoNotes data.
As mentioned ear-lier, there are some portions of the OntoNotes corpusthat have not been annotated for coreference but thathave been annotated for other layers.
For training10http://cemantix.org/ontonotes.html11It should be noted that word sense annotation in OntoNotesis note complete, so only some of the verbs and nouns haveword sense tags specified.Senses Lemmas1 1,5062 1,046> 2 1,016Table 6: Word sense polysemy over verb and noun lem-mas in OntoNotesmodels for each of the layers, where feasible, weused all the data that we could for that layer fromthe training portion of the entire OntoNotes release.Parse Trees Predicted parse trees were producedusing the Charniak parser (Charniak and Johnson,2005).12 Some additional tag types used in theOntoNotes trees were added to the parser?s tagset,including the NML tag that has recently been addedto capture internal NP structure, and the rules used todetermine head words were appropriately extended.The parser was then re-trained on the training por-tion of the release 4.0 data using 10-fold cross-validation.
Table 5 shows the performance of there-trained Charniak parser on the CoNLL-2011 testset.
We did not get a chance to re-train the re-ranker,and since the stock re-ranker crashes when run on n-best parses containing NMLs, because it has not seenthat tag in training, we could not make use of it.Word Sense We trained a word sense tagger us-ing a SVM classifier and contextual word and partof speech features on all the training portion of theOntoNotes data.
The OntoNotes 4.0 corpus com-prises a total of 14,662 sense definitions across 4877verb and noun lemmas13.
The distribution of sensesper lemma is as shown in Table 6.
Table 7 showsthe performance of this classifier over both the verbsand nouns in the CoNLL-2011 test set.
Again thisperformance is not directly comparable to any re-ported in the literature before, and it seems lowerthen performances reported on previous versionsof OntoNotes because this is over all the genresof OntoNotes, and aggregated over both verbs andnouns in the CoNLL-2011 test set.Propositions To predict propositional structure,ASSERT14 (Pradhan et al, 2005) was used, re-trained also on all the training portion of the release12http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz13The number of lemmas in Table 6 do not add up to thisnumber because not all of them have examples in the trainingdata, where the total number of instantiated senses amounts to7933.14http://cemantix.org/assert.html10All Sentences Sentence len < 40N POS R P F N R P FBroadcast Conversation (BC) 2,194 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94Table 5: Parser performance on the CoNLL-2011 test setFrameset Total Total % Perfect Argument ID + ClassAccuracy Sentences Propositions Propositions P R FBroadcast Conversation (BC) 0.92 2,037 5,021 52.18 82.55 64.84 72.63Broadcast News (BN) 0.91 1,252 3,310 53.66 81.64 64.46 72.04Magazine (MZ) 0.89 780 2,373 47.16 79.98 61.66 69.64Newswire (NW) 0.93 1,898 4,758 39.72 80.53 62.68 70.49Weblogs and Newsgroups (WB) 0.92 929 2,174 39.19 81.01 60.65 69.37Overall 0.91 6,896 17,636 46.82 81.28 63.17 71.09Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.AccuracyBroadcast Conversation (BC) 0.70Broadcast News (BN) 0.68Magazine (MZ) 0.60Newswire (NW) 0.62Weblogs and Newsgroups (WB) 0.63Overall 0.65Table 7: Word sense performance over both verbs andnouns in the CoNLL-2011 test set4.0 data.
Given time constraints, we had to per-form two modifications: i) Instead of a single modelthat predicts all arguments including NULL argu-ments, we had to use the two-stage mode where theNULL arguments are first filtered out and the remain-ing NON-NULL arguments are classified into one ofthe argument types, and ii) The argument identifi-cation module used an ensemble of ten classifiers?
each trained on a tenth of the training data andperformed an unweighted voting among them.
Thisshould still give a close to state of the art perfor-mance given that the argument identification perfor-mance tends to start to be asymptotic around 10ktraining instances.
At first glance, the performanceon the newswire genre is much lower than what hasbeen reported for WSJ Section 23.
This could beattributed to two factors: i) the fact that we had tocompromise on the training method, but more im-portantly because ii) the newswire in OntoNotes notonly contains WSJ data, but also Xinhua news.
Onecould try to verify using just the WSJ portion of thedata, but it would be hard as it is not only a sub-set of the documents that the performance has beenreported on previously, but also the annotation hasbeen significantly revised; it includes propositionsfor be verbs missing from the original PropBank,and the training data is a subset of the original dataas well.
Table 8 shows the detailed performancenumbers.In addition to automatically predicting the argu-ments, we also trained a classifier to tag PropBankframeset IDs in the data using the same word sensemodule as mentioned earlier.
OntoNotes 4.0 con-tains a total of 7337 framesets across 5433 verblemmas.15 An overwhelming number of them aremonosemous, but the more frequent verbs tend to bepolysemous.
Table 9 gives the distribution of num-ber of framesets per lemma in the PropBank layer ofthe OntoNotes 4.0 data.During automatic processing of the data, wetagged all the tokens that were tagged with a partof speech VBx.
This means that there would be caseswhere the wrong token would be tagged with propo-sitions.
The CoNLL-2005 scorer was used to gener-ate the scores.Named Entities BBN?s IdentiFinderTMsystemwas used to predict the named entities.
Given the15The number of lemmas in Table 9 do not add up to thisnumber because not all of them have examples in the trainingdata, where the total number of instantiated senses amounts to4229.11Framesets Lemmas1 2,7222 321> 2 181Table 9: Frameset polysemy across lemmasOverall BC BN MZ NW TC WBF F F F F F FALL Named Entities 71.8 64.8 72.2 61.5 84.3 39.5 55.2Cardinal 68.7 51.8 71.1 66.1 82.8 34.0 68.7Date 76.1 63.7 77.9 66.7 83.7 60.5 56.0Event 27.6 00.0 34.8 30.8 47.6 - 13.3Facility 41.9 55.0 16.7 23.1 66.7 00.0 22.9GPE 87.9 87.5 90.3 73.7 92.9 65.9 88.7Language 41.2 - 50.0 50.0 00.0 20.0 75.0Law 63.0 00.0 85.7 00.0 67.9 00.0 50.0Location 58.4 59.1 59.6 53.3 68.0 00.0 23.5Money 74.6 16.7 66.7 73.2 79.4 30.8 61.5NORP 00.0 00.0 00.0 00.0 00.0 00.0 00.0Ordinal 73.4 73.8 73.4 78.1 78.4 88.9 37.0Organization 71.0 57.8 67.1 52.9 86.9 21.2 32.1Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6Table 10: Named Entity performance on the CoNLL-2011 test settime constraints, we could not re-train it on theOntoNotes data and so an existing, pre-trainedmodel was used, therefore the results are not agood indicator of the model?s best performance.The pre-trained model had also used a somewhatdifferent catalog of name types, which did notinclude the OntoNotes NORP type (for nationalities,organizations, religions, and political parties),so that category was never predicted.
Table 10shows the overall performance of the tagger on theCoNLL-2011 test set, as well as the performancebroken down by individual name types.
IdentiFinderperformance has been reported to be in the low 90?son WSJ test set.Other Layers As noted above, systems were al-lowed to make use of gender and number predic-tions for NPs using the table from Bergsma and Lin(Bergsma and Lin, 2006).4.4.3 Data FormatIn order to organize the multiple, rich layers of anno-tation, the OntoNotes project has created a databaserepresentation for the raw annotation layers alongwith a Python API to manipulate them (Pradhan etal., 2007a).
In the OntoNotes distribution the data isorganized as one file per layer, per document.
TheAPI requires a certain hierarchical structure withdocuments at the leaves inside a hierarchy of lan-guage, genre, source and section.
It comes with var-ious ways of cleanly querying and manipulating thedata and allows convenient access to the sense in-ventory and propbank frame files instead of havingto interpret the raw .xml versions.
However, main-taining format consistency with earlier CoNLL taskswas deemed convenient for sites that already hadtools configured to deal with that format.
Therefore,in order to distribute the data so that one could makethe best of both worlds, we created a new file typecalled .conll which logically served as another layerin addition to the .parse, .prop, .name and .coreflayers.
Each .conll file contained a merged repre-sentation of all the OntoNotes layers in the CoNLL-style tabular format with one line per token, and withmultiple columns for each token specifying the inputannotation layers relevant to that token, with the fi-nal column specifying the target coreference layer.Because OntoNotes is not authorized to distributethe underlying text, and many of the layers containinline annotation, we had to provide a skeletal form(.skel of the .conll file which was essentially the.conll file, but with the word column replaced witha dummy string.
We provided an assembly scriptthat participants could use to create a .conll file tak-ing as input the .skel file and the top-level directoryof the OntoNotes distribution that they had sepa-rately downloaded from the LDC16 Once the .conllfile is created, it can be used to create the individuallayers such as .parse, .name, .coref etc.
using an-other set of scripts.
Since the propositions and wordsense layers are inherently standoff annotation, theywere provided as is, and did not require that extramerging step.
One thing thing that made this datacreation process a bit tricky was the fact that we haddissected some of the trees for the conversation datato remove the EDITED phrases.
Table 11 describesthe data provided in each of the column of the .conllformat.
Figure 3 shows a sample from a .conll file.4.5 EvaluationThis section describes the evaluation criteria used.Unlike for propositions, word sense and named en-tities, where it is simply a matter of counting thecorrect answers, or for parsing, where there are sev-eral established metrics, evaluating the accuracy ofcoreference continues to be contentious.
Various al-16OntoNotes is deeply grateful to the Linguistic Data Con-sortium for making the source data freely available to the taskparticipants.12Column Type Description1 Document ID This is a variation on the document filename2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.3 Word number This is the word index in the sentence4 Word The word itself5 Part of Speech Part of Speech of the word6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and theword/part-of-speech leaf replaced with a *.
The full parse can be created by substitutingthe asterix with the ([pos] [word]) string (or leaf) and concatenating the items in therows of that column.7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role informa-tion.
All other rows are marked with a -8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7.9 Word sense This is the word sense of the word in Column 3.10 Speaker/Author This is the speaker or author name where available.
Mostly in Broadcast Conversation andWeb Log data.11 Named Entities These columns identifies the spans representing various named entities.12:N Predicate Arguments There is one column each of predicate argument structure information for the predicatementioned in Column 7.N Coreference Coreference chain information encoded in a parenthesis structure.Table 11: Format of the .conll file used on the shared task#begin document (nw/wsj/07/wsj_0771); part 000......nw/wsj/07/wsj_0771 0 0 ??
??
(TOP(S(S* - - - - * * (ARG1* * * -nw/wsj/07/wsj_0771 0 1 Vandenberg NNP (NP* - - - - (PERSON) (ARG1* * * * (8|(0)nw/wsj/07/wsj_0771 0 2 and CC * - - - - * * * * * -nw/wsj/07/wsj_0771 0 3 Rayburn NNP *) - - - - (PERSON) *) * * *(23)|8)nw/wsj/07/wsj_0771 0 4 are VBP (VP* be 01 1 - * (V*) * * * -nw/wsj/07/wsj_0771 0 5 heroes NNS (NP(NP*) - - - - * (ARG2* * * * -nw/wsj/07/wsj_0771 0 6 of IN (PP* - - - - * * * * * -nw/wsj/07/wsj_0771 0 7 mine NN (NP*)))) - - 5 - * *) * * * (15)nw/wsj/07/wsj_0771 0 8 , , * - - - - * * * * * -nw/wsj/07/wsj_0771 0 9 ??
??
*) - - - - * * *) * * -nw/wsj/07/wsj_0771 0 10 Mr. NNP (NP* - - - - * * (ARG0* (ARG0* * (15nw/wsj/07/wsj_0771 0 11 Boren NNP *) - - - - (PERSON) * *) *) * 15)nw/wsj/07/wsj_0771 0 12 says VBZ (VP* say 01 1 - * * (V*) * * -nw/wsj/07/wsj_0771 0 13 , , * - - - - * * * * * -nw/wsj/07/wsj_0771 0 14 referring VBG (S(VP* refer 01 2 - * * (ARGM-ADV* (V*) * -nw/wsj/07/wsj_0771 0 15 as RB (ADVP* - - - - * * * (ARGM-DIS* * -nw/wsj/07/wsj_0771 0 16 well RB *) - - - - * * * *) * -nw/wsj/07/wsj_0771 0 17 to IN (PP* - - - - * * * (ARG1* * -nw/wsj/07/wsj_0771 0 18 Sam NNP (NP(NP* - - - - (PERSON* * * * * (23nw/wsj/07/wsj_0771 0 19 Rayburn NNP *) - - - - *) * * * * -nw/wsj/07/wsj_0771 0 20 , , * - - - - * * * * * -nw/wsj/07/wsj_0771 0 21 the DT (NP(NP* - - - - * * * * (ARG0* -nw/wsj/07/wsj_0771 0 22 Democratic JJ * - - - - (NORP) * * * * -nw/wsj/07/wsj_0771 0 23 House NNP * - - - - (ORG) * * * * -nw/wsj/07/wsj_0771 0 24 speaker NN *) - - - - * * * * *) -nw/wsj/07/wsj_0771 0 25 who WP (SBAR(WHNP*) - - - - * * * * (R-ARG0*) -nw/wsj/07/wsj_0771 0 26 cooperated VBD (S(VP* cooperate 01 1 - * * * * (V*) -nw/wsj/07/wsj_0771 0 27 with IN (PP* - - - - * * * * (ARG1* -nw/wsj/07/wsj_0771 0 28 President NNP (NP* - - - - * * * * * -nw/wsj/07/wsj_0771 0 29 Eisenhower NNP *))))))))))) - - - - (PERSON) * *) *) *) 23)nw/wsj/07/wsj_0771 0 30 .
.
*)) - - - - * * * * * -nw/wsj/07/wsj_0771 0 0 ??
??
(TOP(S* - - - - * * * -nw/wsj/07/wsj_0771 0 1 They PRP (NP*) - - - - * (ARG0*) * (8)nw/wsj/07/wsj_0771 0 2 allowed VBD (VP* allow 01 1 - * (V*) * -nw/wsj/07/wsj_0771 0 3 this DT (S(NP* - - - - * (ARG1* (ARG1* (6nw/wsj/07/wsj_0771 0 4 country NN *) - - 3 - * * *) 6)nw/wsj/07/wsj_0771 0 5 to TO (VP* - - - - * * * -nw/wsj/07/wsj_0771 0 6 be VB (VP* be 01 1 - * * (V*) (16)nw/wsj/07/wsj_0771 0 7 credible JJ (ADJP*))))) - - - - * *) (ARG2*) -nw/wsj/07/wsj_0771 0 8 .
.
*)) - - - - * * * -#end documentFigure 3: Sample portion of the .conll file.13ternative metrics have been proposed, as mentionedbelow, which weight different features of a proposedcoreference pattern differently.
The choice is notclear in part because the value of a particular set ofcoreference predictions is integrally tied to the con-suming application.A further issue in defining a coreference metricconcerns the granularity of the mentions, and howclosely the predicted mentions are required to matchthose in the gold standard for a coreference predic-tion to be counted as correct.Our evaluation criterion was in part driven by theOntoNotes data structures.
OntoNotes coreferencedistinguishes between identity coreference and ap-positive coreference, treating the latter separatelybecause it is already captured explicitly by other lay-ers of the OntoNotes annotation.
Thus we evaluatedsystems only on the identity coreference task, whichlinks all categories of entities and events togetherinto equivalent classes.The situation with mentions for OntoNotes is alsodifferent than it was for MUC or ACE.
OntoNotesdata does not explicitly identify the minimum ex-tents of an entity mention, but it does include hand-tagged syntactic parses.
Thus for the official evalua-tion, we decided to use the exact spans of mentionsfor determining correctness.
The NP boundariesfor the test data were pre-extracted from the hand-tagged Treebank for annotation, and events trig-gered by verb phrases were tagged using the verbsthemselves.
This choice means that scores for theCoNLL-2011 coreference task are likely to be lowerthan for coref evaluations based on MUC, where themention spans are specified in the input,17 or thosebased on ACE data, where an approximate match isoften allowed based on the specified head of the NPmention.4.5.1 MetricsAs noted above, the choice of an evaluation met-ric for coreference has been a tricky issue and theredoes not appear to be any silver bullet approach thataddresses all the concerns.
Three metrics have beenproposed for evaluating coreference performanceover an unrestricted set of entity types: i) The linkbased MUC metric (Vilain et al, 1995), ii) The men-tion based B-CUBED metric (Bagga and Baldwin,1998) and iii) The entity based CEAF (ConstrainedEntity Aligned F-measure) metric (Luo, 2005).
Veryrecently BLANC (BiLateral Assessment of Noun-Phrase Coreference) measure (Recasens and Hovy,17as is the case in this evaluation with Gold Mentions2011) has been proposed as well.
Each of the met-ric tries to address the shortcomings or biases of theearlier metrics.
Given a set of key entities K, anda set of response entities R, with each entity com-prising one or more mentions, each metric generatesits variation of a precision and recall measure.
TheMUC measure if the oldest and most widely used.
Itfocuses on the links (or, pairs of mentions) in thedata.18 The number of common links between en-tities in K and R divided by the number of linksin K represents the recall, whereas, precision is thenumber of common links between entities in K andR divided by the number of links in R. This met-ric prefers systems that have more mentions per en-tity ?
a system that creates a single entity of allthe mentions will get a 100% recall without signifi-cant degradation in its precision.
And, it ignores re-call for singleton entities, or entities with only onemention.
The B-CUBED metric tries to addressesMUCS?s shortcomings, by focusing on the mentionsand computes recall and precision scores for eachmention.
If K is the key entity containing mention M,and R is the response entity containing mention M,then recall for the mention M is computed as |K?R||K|and precision for the same is is computed as |K?R||R| .Overall recall and precision are the average of theindividual mention scores.
CEAF aligns every re-sponse entity with at most one key entity by findingthe best one-to-one mapping between the entities us-ing an entity similarity metric.
This is a maximumbipartite matching problem and can be solved bythe Kuhn-Munkres algorithm.
This is thus a entitybased measure.
Depending on the similarity, thereare two variations ?
entity based CEAF ?
CEAFe anda mention based CEAF ?
CEAFe.
Recall is the totalsimilarity divided by the number of mentions in K,and precision is the total similarity divided by thenumber of mentions in R. Finally, BLANC uses avariation on the Rand index (Rand, 1971) suitablefor evaluating coreference.
There are a few othermeasures ?
one being the ACE value, but since thisis specific to a restricted set of entities (ACE types),we did not consider it.4.5.2 Official Evaluation MetricIn order to determine the best performing systemin the shared task, we needed to associate a singlenumber with each system.
This could have beenone of the metrics above, or some combination ofmore than one of them.
The choice was not sim-ple, and while we consulted various researchers in18The MUC corpora did not tag single mention entities.14the field, hoping for a strong consensus, their con-clusion seemed to be that each metric had its prosand cons.
We settled on the MELA metric by Denisand Baldridge (2009), which takes a weighted av-erage of three metrics: MUC, B-CUBED, and CEAF.The rationale for the combination is that each of thethree metrics represents a different important dimen-sion, the MUC measure being based on links, theB-CUBED based on mentions, and the CEAF basedon entities.
For a given task, a weighted averageof the three might be optimal, but since we don?thave an end task in mind, we decided to use the un-weighted mean of the three metrics as the score onwhich the winning system was judged.
We decidedto use CEAFe instead of CEAFm.4.5.3 Scoring Metrics ImplementationWe used the same core scorer implementation19 thatwas used for the SEMEVAL-2010 task, and whichimplemented all the different metrics.
There were acouple of modifications done to this scorer after itwas used for the SEMEVAL-2010 task.1.
Only exact matches were considered cor-rect.
Previously, for SEMEVAL-2010 non-exactmatches were judged partially correct with a0.5 score if the heads were the same and themention extent did not exceed the gold men-tion.2.
The modifications suggested by Cai and Strube(2010) were incorporated in the scorer.Since there are differences in the version used forCoNLL and the one available on the download site,and it is possible that the latter would be revised inthe future, we have archived the version of the scoreron the CoNLL-2011 task webpage.205 Systems and ResultsAbout 65 different groups demonstrated interest inthe shared task by registering on the task webpage.Of these, 23 groups submitted system outputs on thetest set during the evaluation week.
18 groups sub-mitted only closed track results, 3 groups only opentrack results, and 2 groups submitted both closed andopen track results.
2 participants in the closed track,did not write system papers, so we don?t use their re-sults in the discussion.
Their results will be reportedon the task webpage.19http://www.lsi.upc.edu/ esapena/downloads/index.php?id=320http://conll.bbn.com/download/scorer.v4.tar.gzThe official results for the 18 systems that submit-ted closed track outputs are shown in Table 12, withthose for the 5 systems that submitted open trackresults in Table 13.
The official ranking score, thearithmetic mean of the F-scores of MUC, B-CUBEDand CEAFe, is shown in the rightmost column.
Forconvenience, systems will be referred to here usingthe first portion of the full name, which is uniquewithin each table.For completeness, the tables include the raw pre-cision and recall scores from which the F-scoreswere derived.
The tables also include two additionalscores (BLANC and CEAFm) that did not factor intothe official ranking score.
Useful further analysismay be possible based on these results beyond thepreliminary results presented here.As discussed previously in the task description,we will consider three different test input conditions:i) Predicted only (Official), ii) Predicted plus goldmention boundaries, and iii) Predicted plus goldmentions5.1 Predicted only (Official)For the official test, beyond the raw source text,coreference systems were provided only with thepredictions from automatic engines as to the otherannotation layers (parses, semantic roles, wordsenses, and named entities).In this evaluation it is important to note that themention detection score cannot be considered in iso-lation of the coreference task as has usually been thecase.
This is mainly owing to the fact that there areno singleton entities in the OntoNotes data.
Mostsystems removed singletons from the response as apost-processing step, so not only will they not getcredit for the singleton entities that they correctly re-moved from the data, but they will be penalized forthe ones that they accidentally linked with anothermention.
What this number does indicate is the ceil-ing on recall that a system would have got in absenceof being penalized for making mistakes in corefer-ence resolution.
A close look at the Table 12 indi-cates a possible outlier in case of the sapena system.The recall for this system is very high, and precisionway lower than any other system.
Further investi-gations uncovered that the reason for this aberrantbehavior was that fact that this system opted to keepsingletons in the response.
By design, the scorer re-moves singletons that might be still present in thesystem, but it does so after the mention detectionaccuracy is computed.The official scores top out in the high 50?s.
Whilethis is lower than the figures cited in previous coref-15SystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33lee75.0766.8170.7061.7657.5359.5768.4068.2368.3156.3756.3756.3743.4147.7545.4870.6376.2173.0257.79sapena92.3928.1943.2056.3263.1659.5562.7572.0867.0953.5153.5153.5144.7538.3841.3269.5073.0771.1055.99chang68.0861.9664.8857.1557.1557.1567.1470.5368.7954.4054.4054.4041.9441.9441.9471.1977.0973.7155.96nugues69.8768.0868.9660.2057.1058.6166.7464.2365.4651.4551.4551.4538.0941.0639.5271.9970.3171.1154.53santos67.8063.2565.4559.2154.3056.6568.7962.8165.6649.5449.5449.5435.8640.2137.9173.3766.9169.4653.41song57.8180.4167.2653.7367.7959.9560.6566.0563.2346.2946.2946.2943.3730.7135.9669.4959.7161.4753.05stoyanov70.8464.9867.7863.6154.0458.4372.5853.2761.4446.0846.0846.0832.0040.8235.8873.2158.9360.8851.92sobha67.8262.0964.8351.0849.8850.4862.6365.4364.0049.4849.4849.4840.6541.8241.2361.4068.3563.8851.90kobdani62.0660.0461.0355.6451.5053.4969.6662.4365.8542.7042.7042.7032.3335.4033.7961.8663.5162.6151.04zhou61.0863.5962.3145.6552.7948.9657.1472.9164.0747.5347.5347.5343.1936.7939.7461.1073.9464.7250.92charton65.9062.7764.3055.0950.0552.4566.2658.4462.1046.8246.8246.8234.3339.0536.5469.9462.2364.8050.36yang71.9257.5363.9359.9146.4352.3171.6455.1462.3246.5546.5546.5530.2842.3935.3371.1161.7564.6349.99hao64.5064.1164.3057.8951.4254.4767.8355.4361.0145.0745.0745.0730.0835.7632.6772.6162.3765.3549.38xinxin65.4958.7161.9248.5444.8546.6261.5962.2861.9344.7544.7544.7535.1938.6236.8363.0465.8364.2748.46zhang55.3568.2561.1342.0355.6247.8852.5773.0561.1444.4644.4644.4642.0030.2835.1962.8469.2265.2148.07kummerfeld69.7756.9762.7246.3939.5642.7063.6057.3060.2945.3545.3545.3535.0542.2638.3258.7461.5859.9147.10zhekova67.4937.6048.2928.8720.6624.0867.1456.6761.4640.4340.4340.4331.5741.2135.7552.7757.0553.7740.43irwin17.0661.0926.6712.4550.6019.9835.0789.9050.4631.6831.6831.6845.8417.3825.2151.4856.8351.1231.88Table12:Performanceofsystemsintheofficial,closedtrackusingallpredictedinformationSystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33lee74.3167.8770.9462.8359.3461.0368.8569.0168.9356.7056.7056.7043.2946.8044.9871.9076.5573.9658.31cai67.1567.6467.4056.7358.9057.8064.6071.0367.6653.3753.3753.3742.7140.6841.6769.7773.9671.6255.71uryupina70.6066.3168.3959.7055.7057.6366.2964.1265.1851.4251.4251.4238.3442.1740.1669.2368.5468.8854.32klenner64.4160.2862.2849.0450.7149.8661.7068.6164.9750.0350.0350.0341.2839.7040.4866.0573.9069.0551.77irwin24.6062.2735.2718.5651.0127.2138.9785.5753.5533.8633.8633.8643.3319.3626.7651.6252.9151.7635.84Table13:Performanceofsystemsintheofficial,opentrackusingallpredictedinformationSystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33lee79.5271.2575.1665.8762.0563.9069.5270.5570.0359.2659.2659.2646.2950.4848.3072.0078.5574.7760.74nugues74.1870.7472.4264.3360.0562.1268.2665.1766.6853.8453.8453.8439.8644.2341.9372.5371.0471.7556.91chang63.3773.1867.9255.0065.5059.7962.1676.6568.6554.9554.9554.9546.7737.1741.4270.9779.3074.2956.62santos65.8269.9067.8057.7661.3959.5264.4970.2767.2651.8751.8751.8741.4238.1639.7272.7271.9772.3455.50kobdani67.1165.0966.0862.6356.8059.5773.2062.2267.2744.4944.4944.4932.8737.2534.9264.0764.1364.1053.92stoyanov76.9064.7370.2969.8155.0161.5477.0752.5462.4848.0848.0848.0830.9744.8436.6476.5760.3362.9653.55zhang59.6271.1964.8946.0658.7551.6453.8973.4162.1646.6246.6246.6243.4932.1136.9564.1170.4766.5450.25song58.4377.6466.6846.6668.4055.4854.4070.1961.2943.6243.6243.6243.7725.8832.5366.2958.7660.2249.77zhekova69.1957.2762.6733.4837.1535.2255.4768.2361.2041.3141.3141.3138.2934.6536.3853.4563.3354.7944.27Table14:PerformanceofsystemsinthesupplementaryclosedtrackusingpredictedinformationplusgoldboundariesSystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33lee78.7172.3375.3966.9363.9165.3970.0971.4970.7859.7859.7859.7846.3449.6247.9273.3879.0075.8361.36Table15:Performanceofsystemsinthesupplementaryopentrackusingpredictedinformationplusgoldboundaries16SystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33chang10010010080.4684.7582.5572.8474.5773.7069.7169.7169.7170.4560.7565.2478.0176.5777.2673.83Table16:Performanceofsystemsinthesupplementary,closedtrackusingpredictedinformationplusgoldmentionsSystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33lee83.3710090.9374.7989.6881.5667.4686.8875.9570.7370.7370.7377.7551.0561.6476.6585.8580.3573.05Table17:Performanceofsystemsinthesupplementary,opentrackusingpredictedinformationplusgoldmentionsSystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33lee76.7968.3472.3263.2958.9661.0568.8468.7268.7857.2857.2857.2844.1948.7546.3670.9376.5873.3658.73sapena95.2729.0744.5556.9963.9160.2562.8972.3167.2753.9053.9053.9045.2238.7041.7169.7173.3271.3256.41chang69.8863.6166.6058.4858.4858.4867.4270.9169.1255.2155.2155.2142.6642.6642.6671.4277.3673.9656.75nugues72.9671.0872.0162.6859.4661.0367.2464.8966.0452.8252.8252.8239.2542.5040.8172.5770.8671.6855.96santos70.3965.6767.9561.2856.2058.6369.2563.1666.0750.4750.4750.4736.5141.1538.6973.9267.3269.9354.46song59.2482.3968.9254.9269.2961.2760.8966.2763.4646.9746.9746.9744.4931.1536.6569.7359.8761.6153.79stoyanov74.4368.2871.2267.1857.0861.7274.0653.4562.0947.4047.4047.4032.7842.5237.0274.1059.3461.3153.61sobha71.0665.0667.9353.9152.6453.2763.1766.1464.6250.8050.8050.8041.7743.0342.3961.9169.1564.4953.43kobdani65.9863.8364.8959.2254.8156.9370.4963.1266.6044.1744.1444.1533.1936.5034.7762.5264.2563.3252.77zhou64.1166.7465.4048.0055.5151.4857.1873.7164.4048.4048.4048.4044.1837.3540.4861.5474.8665.3052.12charton71.0167.6469.2859.2453.8256.4067.1059.0262.8048.9148.9148.9135.9641.3938.4870.6562.7165.3452.56yang73.7358.9765.5361.2347.4553.4771.8855.1362.4047.0547.0547.0530.5443.1635.7771.3961.9264.8350.55hao66.7966.3866.5959.5552.8956.0268.2755.4661.2045.9545.9545.9530.7636.8133.5173.2262.7365.7850.24xinxin69.0561.9165.2850.9947.1148.9761.5962.7062.1445.6445.6445.6435.8639.5737.6263.4266.2964.6849.58zhang57.4170.7863.4043.4857.5349.5352.4473.6061.2444.9744.9744.9742.7130.4435.5563.1269.6365.5348.77kummerfeld71.0558.0163.8747.4240.4443.6563.7357.3960.3945.7645.7645.7635.3042.7238.6658.8961.7760.0747.57zhekova72.6540.4851.9931.7322.7026.4666.9256.6861.3741.0441.0441.0431.9342.1736.3453.0957.8654.2241.39irwin17.5862.9627.4912.6951.5920.3734.8889.9850.2731.7131.7131.7146.1317.3325.2051.5156.9351.1431.95Table18:Headwordbasedperformanceofsystemsintheofficial,closedtrackusingallpredictedinformationSystemMentionDetectionMUCB-CUBEDCEAFmCEAFeBLANCOfficialRPFRPF1RPF2RPFRPF3RPFF1+F2+F33lee76.0169.4372.5764.4060.8362.5769.3469.5769.4557.6857.6857.6844.1547.8545.9272.2376.9474.3259.31cai69.3269.8269.5758.3960.6359.4964.8871.5368.0454.3654.3654.3643.7441.5842.6470.1374.3972.0156.72uryupina72.1067.7269.8460.7456.6858.6466.4364.2565.3252.0052.0052.0038.8742.8540.7669.4368.7369.0754.91klenner71.7367.1469.3655.1757.0456.0962.6770.6966.4453.2553.2553.2544.2742.3943.3167.4575.9270.6855.28irwin25.2463.8736.1818.9051.9427.7138.7985.6453.4033.8933.8933.8943.5919.3126.7651.6652.9851.8035.96Table19:Headwordbasedperformanceofsystemsintheofficial,opentrackusingallpredictedinformation17erence evaluations, that is as expected, given that thetask here includes predicting the underlying men-tions and mention boundaries, the insistence on ex-act match, and given that the relatively easier appos-itive coreference cases are not included in this mea-sure.
The top-performing system (lee) had a scoreof 57.79 which is about 1.8 points higher than thatof the second (sapena) and third (chang) rankingsystems, which scored 55.99 and 55.96 respectively.Another 1.5 points separates them from the fourthbest score of 54.53 (nugues).
Thus the performancedifferences between the better-scoring systems werenot large, with only about three points separating thetop four systems.This becomes even clearer if we merge in the re-sults of systems that participated only in the opentrack but that made relatively limited use of outsideresources.21 Comparing that way, the cai systemscores in the same ball park as the second rank sys-tems (sapena and chang).
The uryupina system sim-ilarly scores very close to nugues?s 54.53Given that our choice of the official metric wassomewhat arbitrary, if is also useful to look atthe individual metrics, including the mention-basedCEAFm and BLANC metrics that were not part ofthe official metric.
The lee system which scoredthe best using the official metric does slightly worsethan song on the MUC metric, and also does slightlyworse than chang on the B-CUBED and BLANC met-rics.
However, it does much better than every othergroup on the entity-based CEAFe, and this is the pri-mary reason for its 1.8 point advantage in the offi-cial score.
If the CEAFe measure does indicate theaccuracy of entities in the response, this suggeststhat the lee system is doing better on getting coher-ent entities than any other system.
This could bepartly due to the fact that that system is primarilya precision-based system that would tend to createpurer entities.
The CEAFe measure also seems to pe-nalize other systems more harshly than do the othermeasures.We cannot compare these results to the ones ob-tained in the SEMEVAL-2010 coreference task usinga small portion of OntoNotes data because it wasonly using nominal entities, and had heuristicallyadded singleton mentions to the OntoNotes data2221The cai system specifically mentions that, and the only re-source that the uryupina system used outside of the closed tracksetting was the Stanford named entity tagger.22The documentation that comes with the SEMEVAL datapackage from LDC (LDC2011T01) states: ?Only nominalmentions and identical (IDENT) types were taken from theOntoNotes coreference annotation, thus excluding coreference5.2 Predicted plus gold mention boundariesWe also explored performance when the systemswere provided with the gold mention boundaries,that is, with the exact spans (expressed in terms oftoken offsets) for all of the NP constituents in thehuman-annotated parse trees for the test data.
Sys-tems could use this additional data to ensure that theoutput mention spans in their entity chains would notclash with those in the answer set.
Since this wasa secondary evaluation, it was an optional element,and not all participants ran their systems on this taskvariation.
The results for those systems that did par-ticipate in this optional task are shown in Tables 14(closed track) and 15 (open track).Most of the better scoring systems did supplythese results.
While all systems did slightly betterhere in terms of raw scores, the performance wasnot much different from the official task, indicatingthat mention boundary errors resulting from prob-lems in parsing do not contribute significantly to thefinal output.23One side benefit of performing this supplementalevaluation was that it revealed a subtle bug in theautomatic scoring routine that we were using thatcould double-count duplicate correct mentions in agiven entity chain.
These can occur, for example, ifthe system considers a unit-production NP-PRP com-bination as two mentions that identify the exact sametoken in the text, and reports them as separate men-tions.
Most systems had a filter in their processingthat selected only one of these duplicate mentions,but the kobdani system considered both as potentialmentions, and its developers tuned their algorithmusing that flawed version of the scorer.When we fixed the scorer and re-evaluated all ofthe systems, the kobdani system was the only onewhose score was affected significantly, dropping byabout 8 points, which lowered that system?s rankfrom second to ninth.
It is not clear how much ofthis was owing to the fact that the system?s param-relations with verbs and appositives.
Since OntoNotes is onlyannotated with multi-mention entities, singleton referential ele-ments were identified heuristically: all NPs and possessive de-terminers were annotated as singletons excluding those func-tioning as appositives or as pre-modifiers but for NPs in thepossessive case.
In coordinated NPs, single constituents as wellas the entire NPs were considered to be mentions.
There is noreliable heuristic to automatically detect English expletive pro-nouns, thus they were (although inaccurately) also annotated assingletons.
?23It would be interesting to measure the overlap between theentity clusters for these two cases, to see whether there wasany substantial difference in the mention chains, besides the ex-pected differences in boundaries for individual mentions.18eters had been tuned using the scorer with the bug,which double-credited duplicate mentions.
To findout for sure, one would have to re-tune the systemusing the modified scorer.One difficulty with this supplementary evaluationusing gold mention boundaries is that those bound-aries alone provide only very partial information.For the roughly 10% of mentions that the automaticparser did not correctly identify, while the systemsknew the correct boundaries, they had no hierarchi-cal parser or semantic role label information, andthey also had to further approximate the alreadyheuristic head word identification.
This incompletedata complicated the systems?
task and also compli-cates interpretation of the results.5.3 Predicted plus gold mentionsThe final supplementary condition that we exploredwas if the systems were supplied with the manually-annotated spans for exactly those mentions that didparticipate in the gold standard coreference chains.This supplies significantly more information thanthe previous case, where exact spans were suppliedfor all NPs, since the gold mentions list here will alsoinclude verb headwords that are linked to event NPs,but will not include singleton mentions, which donot end up as part of any chain.
The latter constraintmakes this test seem somewhat artificial, since it di-rectly reveals part of what the systems are designedto determine, but it still has some value in quanti-fying the impact that mention detection has on theoverall task and what the results are if the mentiondetection is perfect.Since this was a logical extension of the task andsince the data was available to the participants forthe development set, a few of the sites did run ex-periments of this type.
Therefore we decided to pro-vide the gold mentions data to a few sites who hadreported these scores, so that we could compute theperformance on the test set.
The results of these ex-periments are shown in Tables 16 and 17.
The resultsshow that performance does go up significantly, in-dicating that it is markedly easier for the systemsto generate better entities given gold mentions.
Al-though, ideally, one would expect a perfect mentiondetection score, it is the case that one of the two sys-tems ?
lee ?
did not get a 100% Recall.
This couldpossibly be owing to unlinked singletons that wereremoved in post-processing.The lee system developers also ran a further ex-periment where both gold mentions for the elementsof the coreference chains and also gold annota-tions for all the other layers were available to thesystem.
Surprisingly, the improvement in corefer-ence performance from having gold annotation ofthe other layers was almost negligible.
This sug-gests that either: i) the automatic models are pre-dicting those layers well enough that switching togold doesn?t make much difference; ii) informationfrom the other layers does not provide much lever-age for coreference resolution; or iii) current coref-erence models are not capable of utilizing the infor-mation from these other layers effectively.
Giventhe performance numbers on the individual layerscited earlier, (i) seems unlikely, and we hope thatfurther research in how best to leverage these lay-ers will result in models that can benefit from themmore definitively.5.4 Head word based scoringIn order to check how stringent the official, exactmatch scoring is, we also performed a relaxed scor-ing.
Unlike ACE and MUC, the OntoNotes data doesnot have manually annotated minimum spans thata mention must contain to be considered correct.However, OntoNotes does have manual syntacticanalysis in the form of the Treebank.
Therefore, wedecided to approximate the minimum spans by usingthe head words of the mentions using the gold stan-dard syntax tree.
If the response mention containedthe head word and did not exceed the true mentionboundary, then it was considered correct ?
both fromthe point of view of mention detection, and corefer-ence resolution.
The scores using this relaxed strat-egy for the open and closed track submissions usingpredicted data are shown in Tables 18 and 19.
Itcan be observed that the relaxed, head word based,scoring does not improve performance very much.The only exception was the klenner system whoseperformance increased from 51.77 to 55.28.
Over-all, the ranking remained quite stable, though it didchange for some adjacent systems which had veryclose exact match scores.5.5 Genre variationIn order to check how the systems did on variousgenres, we scored their performance per genre aswell.
Tables 20 and 21 summarize genre based per-formance for the closed and open track participantsrespectively.
System performance does not seemto vary as much across the different genres as isnormally the case with language processing tasks,which could suggest that coreference is relativelygenre insensitive, or it is possible that scores aretwo low for the difference to be apparent.
Compar-isons are difficult, however, because the spoken gen-19MD MUC BCUB Cm Ce BLANC O MD MUC BCUB Cm Ce BLANC OF F F F F F F F F F F F F Flee GENRE zhou GENREBC 72.2 60.0 66.2 53.9 43.7 71.7 56.7 BC 64.1 49.5 62.1 45.3 38.8 61.8 50.1BN 72.0 59.0 68.7 57.6 48.7 68.8 58.8 BN 60.8 45.9 64.4 49.5 41.2 66.8 50.5MZ 70.1 58.0 72.2 61.6 50.9 75.0 60.4 MZ 58.8 44.4 66.9 50.1 41.8 64.6 51.0NW 65.4 54.3 69.4 56.5 45.5 70.4 56.4 NW 57.7 44.8 65.7 48.7 40.3 63.1 50.2TC 75.9 66.8 69.5 59.3 41.3 81.6 59.2 TC 69.2 58.1 60.8 43.1 35.7 62.6 51.5WB 73.0 63.9 65.7 54.2 42.7 73.4 57.5 WB 67.4 55.4 62.8 47.9 39.2 69.1 52.5sapena chartonBC 48.7 58.8 64.6 50.8 39.4 70.4 54.3 BC 65.8 53.1 59.1 44.6 35.2 64.4 49.1BN 47.1 60.0 69.1 57.4 45.0 74.3 58.0 BN 65.5 52.0 64.0 50.0 39.6 65.9 51.9MZ 35.3 59.2 72.3 60.4 48.2 75.0 59.9 MZ 61.7 46.3 64.6 49.7 39.9 64.1 50.3NW 35.2 57.9 69.7 55.3 41.9 73.8 56.5 NW 57.6 44.6 64.5 48.2 37.7 67.0 48.9TC 60.4 64.3 63.3 48.3 35.1 68.8 54.2 TC 73.1 66.8 56.2 42.8 29.9 58.1 51.0WB 46.3 60.1 62.5 49.1 37.4 67.4 53.3 WB 67.6 57.6 59.3 45.1 33.3 66.6 50.0chang yangBC 65.5 56.4 67.1 51.5 39.8 71.6 54.4 BC 65.7 53.8 62.3 46.8 35.0 67.5 50.3BN 66.6 57.4 69.1 56.0 45.6 70.5 57.4 BN 66.0 53.1 63.8 49.1 40.0 63.1 52.3MZ 61.6 52.7 71.3 57.6 46.4 72.9 56.8 MZ 58.8 43.9 59.7 42.6 32.8 55.5 45.5NW 61.0 53.3 69.1 54.1 42.1 71.9 54.8 NW 57.2 44.7 62.9 45.3 35.0 62.7 47.6TC 72.2 68.5 71.4 59.6 37.7 81.7 59.2 TC 74.2 66.8 66.3 55.3 36.0 76.1 56.4WB 66.4 59.7 66.7 52.7 39.4 74.7 55.3 WB 67.6 57.6 57.0 42.6 32.1 60.1 48.9nugues haoBC 71.4 59.2 62.4 48.2 37.2 68.4 52.9 BC 68.9 58.7 58.9 44.8 31.7 64.9 49.8BN 70.0 58.5 67.4 54.5 43.1 73.1 56.3 BN 62.0 51.1 63.0 46.2 35.5 64.1 49.9MZ 65.4 53.6 68.6 54.2 42.2 70.1 54.8 MZ 60.3 46.7 61.5 46.3 34.3 61.9 47.5NW 61.8 51.9 67.0 51.3 39.2 69.4 52.7 NW 57.2 47.7 63.3 45.5 32.9 66.0 48.0TC 77.2 69.2 63.9 53.0 37.9 72.2 57.0 TC 67.9 60.4 58.8 44.7 30.3 68.3 49.8WB 72.9 64.2 63.4 51.1 38.5 74.3 55.4 WB 71.4 61.8 55.7 42.6 30.0 64.4 49.2santos xinxinBC 66.6 57.2 64.8 48.5 37.2 68.6 53.0 BC 64.8 47.8 60.2 43.9 35.5 65.1 47.9BN 66.9 57.3 66.9 52.3 41.0 71.8 55.1 BN 61.5 44.7 63.2 47.0 38.9 65.8 48.9MZ 62.7 51.0 65.9 48.9 37.8 64.5 51.6 MZ 54.6 35.5 64.5 45.7 37.7 61.0 45.9NW 58.4 49.5 66.2 48.1 37.4 66.9 51.0 NW 54.3 39.5 64.0 45.0 37.5 61.1 47.0TC 74.2 66.9 65.9 52.5 35.5 72.5 56.1 TC 74.2 62.0 57.9 45.4 33.4 66.5 51.1WB 70.4 63.2 63.4 49.5 38.2 70.3 55.0 WB 66.9 52.6 58.5 42.2 35.9 63.4 49.0song zhangBC 68.9 61.4 61.0 44.1 34.3 59.5 52.2 BC 65.8 50.6 61.1 45.3 35.5 67.3 49.1BN 66.2 58.4 64.8 49.0 38.2 65.2 53.8 BN 56.3 43.9 61.0 45.8 35.8 66.8 46.9MZ 63.7 53.4 65.5 49.9 39.0 63.4 52.6 MZ 57.1 35.1 62.2 44.4 36.1 59.4 44.5NW 62.4 53.6 64.3 48.0 37.2 62.7 51.7 NW 49.9 37.8 61.8 43.2 35.2 59.8 44.9TC 76.9 74.4 62.0 43.3 33.2 58.1 56.5 TC 75.4 65.9 60.2 46.0 32.1 67.1 52.7WB 70.0 63.0 60.1 43.3 31.8 60.8 51.6 WB 69.2 55.4 57.4 42.5 34.6 64.7 49.1stoyanov kummerfieldBC 69.5 59.1 57.6 43.5 34.0 58.7 50.2 BC 66.4 41.5 55.6 41.7 36.2 57.9 44.4BN 69.2 59.1 65.4 50.4 40.0 65.5 54.8 BN 68.3 48.2 63.4 51.7 44.7 61.6 52.1MZ 66.7 55.1 65.5 51.0 39.9 63.7 53.5 MZ 58.0 39.9 65.8 51.0 43.4 64.1 49.7NW 61.8 52.0 63.3 46.2 36.1 62.0 50.5 NW 55.2 41.3 64.7 46.8 37.0 63.5 47.6TC 72.6 66.6 57.6 42.3 31.0 57.6 51.7 TC 61.8 34.5 51.5 34.7 30.0 54.1 38.7WB 71.5 63.9 58.3 44.8 33.1 61.1 51.8 WB 68.2 48.1 56.0 44.4 38.6 59.6 47.6sobha zhekovaBC 68.3 51.7 61.4 47.8 40.4 62.9 51.2 BC 50.5 23.8 60.6 39.4 35.1 53.4 39.8BN 66.5 51.9 66.5 53.7 45.5 66.3 54.6 BN 51.2 26.0 62.4 42.5 37.5 54.3 42.0MZ 68.8 54.9 70.3 58.9 49.3 69.8 58.1 MZ 44.0 22.6 63.4 43.3 37.3 56.0 41.1NW 55.1 43.1 65.8 48.6 39.0 64.9 49.3 NW 39.7 19.4 62.8 41.0 35.8 53.7 39.3TC 71.5 55.1 57.5 44.2 36.7 60.5 49.7 TC 59.4 31.6 58.2 37.7 33.6 54.1 41.1WB 70.5 55.7 59.2 46.6 39.8 62.6 51.6 WB 54.1 27.8 58.7 38.5 34.7 53.0 40.4kobdani irwinBC 63.2 56.3 65.8 40.6 32.4 61.9 51.5 BC 23.5 16.1 46.0 29.4 23.6 49.8 28.6BN 63.5 55.7 68.5 46.9 37.5 64.6 53.9 BN 24.9 20.0 49.7 34.2 27.1 52.9 32.3MZ 57.5 52.2 69.8 45.7 36.4 61.7 52.8 MZ 23.2 17.9 55.9 36.2 28.5 53.0 34.1NW 52.2 41.7 64.4 43.2 33.7 62.6 46.6 NW 27.5 21.6 56.4 33.9 27.3 52.6 35.1TC 67.7 60.2 65.3 36.6 28.5 57.6 51.3 TC 28.0 19.3 38.2 24.5 18.7 49.0 25.4WB 68.7 62.8 62.4 42.5 32.9 64.0 52.7 WB 33.6 24.8 47.6 29.7 23.0 50.2 31.8Table 20: Detailed look at the performance per genre for the official, closed track using automatic performance.
MDrepresents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe and Orepresents the OFFICIAL score.20res were treated here with perfect speech recognitionaccuracy and perfect speaker turn information.
Un-der more realistic application conditions, the spreadin performance between genres might be greater.MD MUC BCUB Cm Ce BLANC OF F F F F F Flee GENREBC 72.7 61.7 67.0 54.5 43.6 72.7 57.4BN 72.0 60.6 69.4 57.9 48.1 70.3 59.3MZ 69.9 58.4 72.1 61.2 50.1 75.2 60.2NW 65.3 55.8 70.0 56.7 44.9 71.7 56.9TC 76.6 68.4 70.4 59.6 40.8 82.1 59.9WB 73.8 65.5 66.2 54.5 42.1 74.2 57.9caiBC 69.7 59.1 66.0 50.5 39.9 69.2 55.0BN 68.6 57.6 67.8 55.4 45.5 68.2 56.9MZ 64.0 51.1 69.5 55.9 45.6 71.2 55.4NW 60.3 49.9 67.8 52.7 41.2 69.1 53.0TC 75.6 70.5 72.2 59.6 38.0 80.3 60.2WB 71.7 63.9 65.0 51.8 39.8 72.8 56.2uryupinaBC 70.2 58.3 62.7 48.7 38.0 68.7 53.0BN 69.0 57.6 66.8 53.6 43.1 69.2 55.8MZ 65.7 52.4 68.3 54.3 43.6 68.8 54.8NW 62.6 52.1 68.3 53.2 41.2 71.3 53.9TC 75.7 67.1 61.0 50.7 34.6 67.1 54.2WB 72.0 61.7 60.9 48.8 38.3 67.6 53.6klennerBC 63.2 50.3 63.4 48.2 38.9 66.8 50.8BN 63.1 48.6 65.0 51.0 42.6 66.0 52.1MZ 59.1 43.7 67.1 52.9 45.3 65.0 52.0NW 55.3 41.3 65.0 48.0 39.6 64.5 48.7TC 73.9 64.9 67.9 56.4 39.0 78.0 57.3WB 66.8 58.1 64.0 50.1 39.6 72.7 53.9irwinBC 36.6 27.6 50.9 32.0 25.5 50.2 34.7BN 30.8 24.6 51.9 36.4 28.6 54.8 35.0MZ 26.1 20.0 57.3 37.6 29.4 54.3 35.6NW 32.3 24.7 58.4 34.7 27.9 51.1 37.0TC 46.4 34.3 44.6 29.4 21.9 51.7 33.6WB 41.7 32.9 50.5 32.9 25.1 53.2 36.2Table 21: Detailed look at the performance per genre forthe official, open track using predicted information.
MDrepresents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe andO represents the OFFICIAL score.6 ApproachesTables 22 and 23 summarize the approaches of theparticipating systems along with some of the impor-tant dimensions.Most of the systems broke the problem into twophases, first identifying the potential mentions in thetext and then linking the mentions to form corefer-ence chains.
Most participants also used rule-basedapproaches for mention detection, though two diduse trained models.
While trained morels seem ableto better balance precision and recall, and thus toachieve a higher F-score on the mention task itself,their recall tends to be quite a bit lower than thatachievable by rule-based systems designed to fa-vor recall.
This impacts coreference scores becausethe full coreference system has no way to recoverif the mention detection stage misses a potentiallyanaphoric mention.Only one of the participating systems cai at-tempted to do joint mention detection and corefer-ence resolution.
While it did not happen to be amongthe top-performing systems, the difference in perfor-mance could be due to the richer features used byother systems rather than to the use of a joint model.Most systems represented the markable mentionsinternally in terms of the parse tree NP constituentspan, but some systems used shared attribute mod-els, where the attributes of the merged entity aredetermined collectively by heuristically merging theattribute types and values of the different constituentmentions.Various types of trained models were used for pre-dicting coreference.
It is interesting to note thatsome of the systems, including the best-performingone, used a completely rule-based approach even forthis component.Most participants appear not to have focusedmuch on eventive coreference, those coreferencechains that build off verbs in the data.
This usu-ally meant that mentions that should have linked tothe eventive verb were instead linked in with someother entity.
Participants may have chosen not to fo-cus on events because they pose unique challengeswhile making up only a small portion of the data.Roughly 91% of mentions in the data are NPs andpronouns.In the systems that used trained models, manysystems used the approach described in Soon et al(2001) for selecting the positive and negative train-ing examples, while others used some of the al-ternative approaches that have been introduced inthe research literature more recently.
Many of thetrained systems also were able to improve their per-formance by using feature selection, though thingsvaried some depending on the example selectionstrategy and the classifier used.
Almost half of thetrained systems used the feature selection strategyfrom Soon et al (2001) and found it beneficial.
It isnot clear whether the other systems did not explorethis path, or whether it just did not prove as useful intheir case.7 ConclusionsIn this paper we described the anaphoric coreferenceinformation and other layers of annotation in the21TaskSyntaxLearningFrameworkMarkableIdentificationMarkableVerbFeatureSelection#FeaturesTrainingleeC+OPRule-basedRulestoexcludeCopularconstruction,Appositives,Pleonasticit,etc.Featuredependentwithsharedattributes???sapenaCPDecisionTree+RelaxationLabelingNP(maximalspan)+PRP+NE+CapitalizednounheuristicFullphrase??Train+DevchangCPLearningBasedJavaNP,NE,PRP,PRP$Fullphrase??Train+DevcaiOPComputehyperedgeweightson30%oftrainingdataNP,PRP,PRP$,Basephrasechunks,PleonasticitfilterFullphrase???nuguesCDLogisticRegression(LIBLINEAR)NP,PRP$andsequenceofNNP(s)inpostprocessingusingALIASandSTRINGMATCHHeadword?Forward+BackwardstartingfromSoonfeatureset24Train+DevuryupinaOPDecisionTree.DifferentclassifiersforPronominalandnon-PronominalmentionsNP,NE,PRP,PRP$,andrulestoexcludesomespecificcasesFullphrase?Multi-ObjectiveOptimizationonthreesplits.NSGA-II46Train+DevsantosCPETL(EntropyguidedTransformationalLearning)committeeandRandomForest(WEKA)AllNPandallpronounsandPER,ORG,GPEinNPFullphrase?InherenttotheclassifiersTrain+DevsongCPMaxEnt(OpenNLP)MentiondetectionclassifierFullphrase?Samefeatureset,butperclassifier40TrainstoyanovCPAveragedperceptronNEandpossessivesinadditiontoACEbasedsystemFullphrase??76?sobhaCPCRFfornon-pronominalandsaliencefactorforpronominalresolutionMachinelearnedpleonasticit,plusNP,PRP,PRP$andNEMinimal(Chunk/NE)andMaximumspan??TrainklennerODRule-based.SaliencemeasureusingdependenciesgeneratedfromtrainingdataNP,NE,PRP,PRP$Sharedattributed/transitivitybyusingavirtualprototype???kobdaniCPDecisionTreeNP(nomentionofPRP$)Startword,EndwordandHeadofNP?InformationgainratioTrainzhouCPSVMtreekernelusingBCportionofthedataRule-based;Fiverules:PRP$,PRP,NE,smallestNPsubsumingNEandDET+NPFullphrase??17Train+DevchartonCPMulti-layerperceptronRulesbasedonPOS,NEandfilteroutpleonasticitusingrule-basedfilterFullphrase??22TrainyangCPMaxEnt(MALLET)NP,PRP,PRP$,pre-modifiersandverbsFullphrase??40Train+DevhaoCPMaxEntNP,PRP,PRP$,VBDfullphrase??Train+DevxinxinCPILP/InformationgainNP,PRP,PRP$Fullphrase?Informationgainratio65?zhangCPSVMIOBclassificationFullphrase???kummerfieldCPUnsupervisedgenerativemodelNP,PRP,PRP$withmaximalspanFullphrase???zhekovaCPTIMBLmemorybasedlearnerNP,Propernouns,PRP,PRP$,plusverbwithpredicatelemmaHeadword??Train+DevirwinC+OPClassification-basedrankerNP,PRP,PRP$Sharedattributes???Table22:Participatingsystemprofiles?PartI.IntheTaskcolumn,C/Orepresentswhetherthesystemparticipatedintheclosed,openorbothtracks.IntheSyntaxcolumn,aPrepresentsthatthesystemsusedaphrasestructuregrammarrepresentationofsyntax,whereasaDrepresentsthattheyusedadependencyrepresentation.22PositiveTrainingExamplesNegativeTrainingExamplesDecodingParseConfigurationlee??Multi-passSievessapenaAllmentionpairsandlongerofnestedmentionswithcommonheadkeptMentionpairswithlessthanthreshold(5)numberofdifferentattributevaluesareconsidered(22%outof99%originalarediscarded)Iterative1-bestchangClosestantecedentAllprecedingmentionsinaunionofofgoldandpredictedmentions.MentionswherethefirstispronounandothernotarenotconsideredBestlinkandAlllinksstrategy;withandwithoutconstraints?BestlinkwithoutconstraintswasselectedfortheofficialruncaiWeightsaretrainedonpartofthetrainingdataRecursive2-waySpectralclustering(Agarwal,2005)nuguesClosestAntecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)Closest-firstclusteringforpronounsandBest-firstclusteringfornon-pronouns1-besturyupinaClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)mentionpairmodelwithoutrankingasinSoon2001santosExtendedversionofSoon(2001)whereinadditiontotheirstrategy,positiveandnegativeexamplesfrommentionsinthesentenceoftheclosestprecedingantecedentareconsideredLimitednumberofprecedingmentions60forautomaticand40givengoldboundaries;Aggressive-mergeclustering(MccarthyandLenhert,1995)songPre-clusterpairmodelsseparateforeachpairNP-NP,NP-PRPandPRP-PRPPre-clusters,withsingletonpronounpre-clusters,anduseclosest-firstclustering.Differentlinkmodelsbasedonthetypeoflinkingmentions?NP-PRP,PRP-PRPandNP-NPstoyanovSmartPairGeneration(SmartPG)wherethetypeofantecedentisdeterminedbythetypeofanaphorusingasetofrulesSingle-linkclusteringbycomputingtransitiveclosurebetweenpairwisepositives.sobhaClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)Pronominal:allprecedingNPsinthesentenceandpreceding4sentencesklenner??IncrementalentitycreationkobdaniClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)Best-firstclustering.Thresholdof100wordsusedforlongdocuments1-bestzhouClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)?chartonFromtheendofthedocument,untilanantecedentisfound,or10mentionsNegativeexamplesinbetweenanaphorandclosestantecedentMLPwithscoreof0.5usedforlinkingand10mentionsyangClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)Maximum23sentencestotheleft;ConstrainedclusteringhaoClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)Beamsearch(Luo,2004)PackedforestxinxinClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)Best-firstclusteringfollowedbyILPoptimizationzhangClosestantecedent(Soon,2001)Negativeexamplesinbetweenanaphorandclosestantecedent(Soon,2001)Windowof100markableskummerfield?
?Pre-andpost-resolutionfiltersGiven+Berkeleyparserparses;parseswithoutNMLsimprovedperformanceslightly;re-trainedBerkeleyparserzhekovaExamplesinthepastthreesentencesFromlastpossiblementionindocumentirwinClusterquerywithNULLclusterfordiscoursenewmentionsCluster-rankingapproach(rahman,2009)Table23:Participatingsystemprofiles?PartII.Thisfocusesonthewaypositiveandnegativeexamplesweregeneratedandthedecodingstrategyused.23OntoNotes corpus, and presented the results from anevaluation on learning such unrestricted entities andevents in text.
The following represent our conclu-sions on reviewing the results:?
Perhaps the most surprising finding was that thebest-performing system (lee) was completelyrule-based, rather than trained.
This suggeststhat their rule-based approach was able to doa more effective job of combining the multiplesources of evidence than the trained systems.The features for coreference prediction are cer-tainly more complex than for many other lan-guage processing tasks, which makes it morechallenging to generate effective feature com-binations.
The rule-based approach used bythe best-performing system seemed to benefitfrom a heuristic that captured the most con-fident links before considering less confidentones, and also made use of the information inthe guidelines in a slightly more refined man-ner than other systems.
They also included ap-positives and copular constructions in their cal-culations.
Although OntoNotes does not countthose as instances of IDENT coreference, usingthat information may have helped their systemdiscover additional useful links.?
It is interesting to note that the developers ofthe lee system also did the experiment of run-ning their system using gold standard informa-tion on the individual layers, rather than auto-matic model predictions.
The somewhat sur-prising result was that using perfect informa-tion for the other layers did not end up improv-ing coreference performance much, if at all.
Itis not clear whether this means that: i) Auto-matic predictors for the individual layers areaccurate enough already; ii) Information cap-tured by those supplementary layers actuallydoes not provide much leverage for resolvingcoreference; or iii) researchers have yet havefound an effective way of capturing and utiliz-ing the extra information provided by these lay-ers.?
It does seem that collecting information aboutan entity by merging information across thevarious attributes of the mentions that compriseit can be useful, though not all systems that at-tempted this achieved a benefit.?
System performance did not seem to vary asmuch across the different genres as is nor-mally the case with language processing tasks,which could suggest that coreference is rela-tively genre insensitive, or it is possible thatscores are two low for the difference to be ap-parent.
Comparisons are difficult, however, be-cause the spoken genres were treated here withperfect speech recognition accuracy and perfectspeaker turn information.
Under more realis-tic application conditions, the spread in perfor-mance between genres might be greater.?
It is noteworthy that systems did not seem toattempt the kind of joint inference that couldmake use of the full potential of various layersavailable in OntoNotes, but this could well havebeen owing to the limited time available for theshared task.?
We had expected to see more attention paid toevent coreference, which is a novel feature inthis data, but again, given the time constraintsand given that events represent only a smallportion of the total, it is not surprising that mostsystems chose not to focus on it.?
Scoring coreference seems to remain a signif-icant challenge.
There does not seem to be anobjective way to establish one metric in prefer-ence to another in the absence of a specific ap-plication.
On the other hand, the system rank-ings do not seem terribly sensitive to the par-ticular metric chosen.
It is interesting that bothversions of the CEAF metric ?
which tries tocapture the goodness of the entities in the out-put ?
seem much lower than the other metric,though it is not clear whether that means thatour systems are doing a poor job of creatingcoherent entities or whether that metric is justespecially harsh.Finally, it is interesting to note that the problem ofcoreference does not seem to be following the samekind of learning curve that we are used to with otherproblems of this sort.
While performance has im-proved somewhat, it is not clear how far we will beable to go given the strategies at hand, or whethernew techniques will be needed to capture additionalinformation from the texts or from world knowl-edge.
We hope that this corpus and task will providea useful resource for continued experimentation tohelp resolve this issue.AcknowledgmentsWe gratefully acknowledge the support of theDefense Advanced Research Projects Agency24(DARPA/IPTO) under the GALE program,DARPA/CMO Contract No.
HR0011-06-C-0022.We would like to thank all the participants.
Withouttheir hard work, patience and perseverance this eval-uation would not have been a success.
We wouldalso like to thank the Linguistic Data Consortiumfor making the OntoNotes 4.0 corpus freely andtimely available to the participants.
Emili Sapena,who graciously allowed the use of his scorerimplementation, and made available enhancementsand immediately fixed issues that were uncoveredduring the evaluation.
Finally, we offer our specialthanks to Llu?
?s Ma`rquez and Joakim Nivre for theirwonderful support and guidance without which thistask would not have been successful.ReferencesOlga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,Martha Palmer, Mitch Marcus, Seth Kulick, and Li-bin Shen.
2006.
Issues in synchronizing the Englishtreebank and propbank.
In Workshop on Frontiers inLinguistically Annotated Corpora 2006, July.Amit Bagga and Breck Baldwin.
1998.
Algorithms forScoring Coreference Chains.
In The First Interna-tional Conference on Language Resources and Eval-uation Workshop on Linguistics Coreference, pages563?566.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 33?40, Sydney,Australia, July.Jie Cai and Michael Strube.
2010.
Evaluation metricsfor end-to-end coreference resolution systems.
In Pro-ceedings of the 11th Annual Meeting of the Special In-terest Group on Discourse and Dialogue, SIGDIAL?10, pages 28?36.Eugene Charniak and Mark Johnson.
2001.
Edit detec-tion and parsing for transcribed speech.
In Proceed-ings of the Second Meeting of North American Chapterof the Association of Computational Linguistics, June.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL), AnnArbor, MI, June.Nancy Chinchor and Beth Sundheim.
2003.
Messageunderstanding conference (MUC) 6.
In LDC2003T13.Nancy Chinchor.
2001.
Message understanding confer-ence (MUC) 7.
In LDC2001T02.Aron Culotta, Michael Wick, Robert Hall, and AndrewMcCallum.
2007.
First-order probabilistic models forcoreference resolution.
In HLT/NAACL, pages 81?88.Pascal Denis and Jason Baldridge.
2007.
Joint de-termination of anaphoricity and coreference resolu-tion using integer programming.
In Proceedings ofHLT/NAACL.Pascal Denis and Jason Baldridge.
2009.
Global jointmodels for coreference resolution and named entityclassification.
Procesamiento del Lenguaje Natural,(42):87?96.Charles Fillmore, Christopher Johnson, and Miriam R. L.Petruck.
2003.
Background to framenet.
Interna-tional Journal of Lexicography, 16(3).G.
G. Doddington, A. Mitchell, M. Przybocki,L.
Ramshaw, S. Strassell, and R. Weischedel.2000.
The automatic content extraction (ACE)program-tasks, data, and evaluation.
In Proceedingsof LREC.Aria Haghighi and Dan Klein.
2010.
Coreference reso-lution in a modular, entity-centered model.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 385?393, Los An-geles, California, June.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2009): Shared Task, pages1?18, Boulder, Colorado, June.Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.Maiorano.
2001.
Text and knowledge mining forcoreference resolution.
In NAACL.L.
Hirschman and N. Chinchor.
1997.
Coreference taskdefinition (v3.0, 13 jul 97).
In Proceedings of the Sev-enth Message Understanding Conference.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% solution.
In Proceedings of HLT/NAACL,pages 57?60, New York City, USA, June.
Associationfor Computational Linguistics.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2000.
A large-scale classification ofenglish verbs.
Language Resources and Evaluation,42(1):21 ?
40.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of Human LanguageTechnology Conference and Conference on Empirical25Methods in Natural Language Processing, pages 25?32, Vancouver, British Columbia, Canada, October.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn treebank.
ComputationalLinguistics, 19(2):313?330, June.Andrew McCallum and Ben Wellner.
2004.
Conditionalmodels of identity uncertainty with application to nouncoreference.
In Advances in Neural Information Pro-cessing Systems (NIPS).Joseph McCarthy and Wendy Lehnert.
1995.
Using de-cision trees for coreference resolution.
In Proceedingsof the Fourteenth International Conference on Artifi-cial Intelligence, pages 1050?1055.Thomas S. Morton.
2000.
Coreference for nlp applica-tions.
In Proceedings of the 38th Annual Meeting ofthe Association for Computational Linguistics, Octo-ber.Vincent Ng.
2007.
Shallow semantics for coreferenceresolution.
In Proceedings of the IJCAI.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1396?1411, Uppsala, Swe-den, July.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum.
2007.
Making fine-grained and coarse-grainedsense distinctions, both manually and automatically.R.
Passonneau.
2004.
Computing reliability for corefer-ence annotation.
In Proceedings of LREC.Massimo Poesio and Ron Artstein.
2005.
The reliabilityof anaphoric annotation, reconsidered: Taking ambi-guity into account.
In Proceedings of the Workshop onFrontiers in Corpus Annotations II: Pie in the Sky.Massimo Poesio.
2004.
The mate/gnome scheme foranaphoric annotation, revisited.
In Proceedings ofSIGDIAL.Simone Paolo Ponzetto and Massimo Poesio.
2009.State-of-the-art nlp approaches to coreference resolu-tion: Theory and practical recipes.
In Tutorial Ab-stracts of ACL-IJCNLP 2009, page 6, Suntec, Singa-pore, August.Simone Paolo Ponzetto and Michael Strube.
2005.
Se-mantic role labeling for coreference resolution.
InCompanion Volume of the Proceedings of the 11thMeeting of the European Chapter of the Associa-tion for Computational Linguistics, pages 143?146,Trento, Italy, April.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceedingsof the HLT/NAACL, pages 192?199, New York City,N.Y., June.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James Martin, and Dan Jurafsky.
2005.Support vector learning for semantic argument classi-fication.
Machine Learning Journal, 60(1):11?39.Sameer Pradhan, Eduard Hovy, Mitchell Marcus, MarthaPalmer, Lance Ramshaw, and Ralph Weischedel.2007a.
OntoNotes: A Unified Relational SemanticRepresentation.
International Journal of SemanticComputing, 1(4):405?419.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007b.Unrestricted Coreference: Indentifying Entities andEvents in OntoNotes.
In in Proceedings of theIEEE International Conference on Semantic Comput-ing (ICSC), September 17-19.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977, Singapore, Au-gust.
Association for Computational Linguistics.W.
M. Rand.
1971.
Objective criteria for the evaluationof clustering methods.
Journal of the American Statis-tical Association, 66(336).Marta Recasens and Eduard Hovy.
2011.
Blanc: Im-plementing the rand index for coreference evaluation.Natural Language Engineering.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.
Anto`nia Mart?
?, Mariona Taule?, Ve?roniqueHoste, Massimo Poesio, and Yannick Versley.
2010.Semeval-2010 task 1: Coreference resolution inmultiple languages.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, pages 1?8,Uppsala, Sweden, July.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learn-ing approach to coreference resolution of noun phrase.Computational Linguistics, 27(4):521?544.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: Making sense of the state-of-the-art.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 656?664, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In CoNLL 2008: Proceedings26of the Twelfth Conference on Computational Natu-ral Language Learning, pages 159?177, Manchester,England, August.Yannick Versley.
2007.
Antecedent selection techniquesfor high-recall coreference resolution.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model theoretic coreferencescoring scheme.
In Proceedings of the Sixth MessageUndersatnding Conference (MUC-6), pages 45?52.Ralph Weischedel and Ada Brunstein.
2005.
BBN pro-noun coreference and entity type corpus LDC catalogno.
: LDC2005T33.
BBN Technologies.Ralph Weischedel, Eduard Hovy, Martha Palmer, MitchMarcus, Robert Belvin, Sameer Pradhan, LanceRamshaw, and Nianwen Xue.
2011.
OntoNotes: ALarge Training Corpus for Enhanced Processing.
InJoseph Olive, Caitlin Christianson, and John McCary,editors, Handbook of Natural Language Processingand Machine Translation.
Springer.27
