Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 51?57Manchester, August 2008Human Judgements in Parallel Treebank AlignmentMartin Volk and Torsten MarekUniversity of ZurichInstitute of Computational Linguistics8050 Zurich, Switzerlandvolk@cl.uzh.chYvonne SamuelssonStockholm UniversityDepartment of Linguistics106 91 Stockholm, Swedenyvonne.samuelsson@ling.su.seAbstractWe have built a parallel treebank thatincludes word and phrase alignment.The alignment information was manuallychecked using a graphical tool that al-lows the annotator to view a pair of treesfrom parallel sentences.
We found thecompilation of clear alignment guidelinesto be a difficult task.
However, experi-ments with a group of students have shownthat we are on the right track with up to89% overlap between the student annota-tion and our own.
At the same time theseexperiments have helped us to pin-pointthe weaknesses in the guidelines, many ofwhich concerned unclear rules related todifferences in grammatical forms betweenthe languages.1 IntroductionEstablishing translation correspondences is a dif-ficult task.
This task is traditionally called align-ment and is usually performed on the paragraphlevel, sentence level and word level.
Alignmentanswers the question: Which part of a text in lan-guage L1 corresponds in meaning to which part ofa text in language L2 (under the assumption thatthe two texts represent the same meaning in differ-ent languages).
This may mean that one text is thetranslation of the other or that both are translationsderived from a third text.There is considerable interest in automating thealignment process.
Automatic sentence alignmentc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.of legacy translations helps to fill translation mem-ories.
Automatic word alignment is a crucial stepin training statistical machine translation systems.Both sentence and word alignment have to dealwith 1:many alignments, i.e.
sometimes a sentencein one language is translated as two or three sen-tences in the other language.In other respects sentence alignment and wordalignment are fundamentally different.
It is rela-tively safe to assume the same sentence order inboth languages when computing sentence align-ment.
But such a monotonicity assumption is notpossible for word alignment which needs to allowfor word order differences and thus for crossingalignments.
And while algorithms for sentencealignment usually focus on length comparisons (interms of numbers of characters), word alignmentalgorithms use cross-language cooccurrence fre-quencies as a key feature.Our work focuses on word alignment and on anintermediate alignment level which we call phrasealignment.
Phrase alignment encompasses thealignment from simple noun phrases and preposi-tional phrases all the way to complex clauses.
Forexample, on the word alignment level we want toestablish the correspondence of the German ?verbform plus separated prefix?
fing an with the Eng-lish verb form began.
While in phrase alignmentwe mark the correspondence of the verb phrasesihn in den Briefkasten gesteckt and dropped it inthe mail box.We regard phrase alignment as alignment be-tween linguistically motivated phrases, in con-trast to some work in statistical machine trans-lation where phrase alignment is defined as thealignment between arbitrary word sequences.
Ourphrase alignment is alignment between nodes inconstituent structure trees.
See figure 1 for an ex-51ample of a tree pair with word and phrase align-ment.We believe that such linguistically motivatedphrase alignment provides useful phrase pairs forexample-based machine translation, and providesinteresting insights for translation science andcross-language comparisons.
Phrase alignmentsare particularly useful for annotating correspon-dences of idiomatic or metaphoric language use.2 The Parallel TreebankWe have built a trilingual parallel treebank in Eng-lish, German and Swedish.
The treebank consistsof around 500 trees from the novel Sophie?s Worldand 500 trees from economy texts (an annual re-port from a bank, a quarterly report from an inter-national engineering company, and the banana cer-tification program of the Rainforest Alliance).
Thesentences in Sophie?s World are relatively short(14.8 tokens on average in the English version),while the sentences in the economy texts are muchlonger (24.3 tokens on average; 5 sentences in theEnglish version have more than 100 tokens).The treebanks in English and German consist ofconstituent structure trees that follow the guide-lines of existing treebanks, the NEGRA/TIGERguidelines for German and the Penn treebankguidelines for English.
There were no guidelinesfor Swedish constituent structure trees.
We havetherefore adapted the German treebank guidelinesfor Swedish.
Both German trees and Swedish treesare annotated with flat structures but subsequentlyautomatically deepened to result in richer and lin-guistically more plausible tree structures.When the monolingual treebanks were finished,we started with the word and phrase alignment.For this purpose we have developed a special toolcalled the Stockholm TreeAligner (Lundborg etal., 2007) which displays two trees and allows theuser to draw alignment lines by clicking on nodesand words.
This tool is similar to word alignmenttools like ILink (Ahrenberg et al, 2003) or Cairo(Smith and Jahr, 2000).
As far as we know our toolis unique in that it allows the alignments of lin-guistically motivated phrases via node alignmentsin parallel constituent structure trees (cf.
(Samuels-son and Volk, 2007)).After having solved the technical issues, thechallenge was to compile precise and comprehen-sive guidelines to ensure smooth and consistentalignment decisions.
In (Samuelsson and Volk,2006) we have reported on a first experiment toevaluate inter-annotator agreement from our align-ment tasks.In this paper we report on another recently con-ducted experiment in which we tried to identifythe weaknesses in our alignment guidelines.
Weasked 12 students to alignment 20 tree pairs (Eng-lish and German) taken from our parallel treebank.By comparing their alignments to our Gold Stan-dard and to each other we gained valuable insightsinto the difficulty of the alignment task and thequality of our guidelines.3 Related ResearchOur research on word and phrase alignment is re-lated to previous work on word alignment as e.g.in the Blinker project (Melamed, 1998) or in theUPLUG project (Ahrenberg et al, 2003).
Align-ment work on parallel treebanks is rare.
Mostnotably there is the Prague Czech-English tree-bank (Kruijff-Korbayova?
et al, 2006) and theLinko?ping Swedish-English treebank (Ahrenberg,2007).
There has not been much work on the align-ment of linguistically motivated phrases.
Tinsleyet al (2007) and Groves et al (2004) report onsemi-automatic phrase alignment as part of theirresearch on example-based machine translation.Considering the fact that the alignment task isessentially a semantic annotation task, we mayalso compare our results to other tasks in seman-tic corpus annotation.
For example, we may con-sider the methods for resolving annotation con-flicts and the figures for inter-annotator agreementin frame-semantic annotation as found in the Ger-man SALSA project (cf.
(Burchardt et al, 2006)).4 Our Alignment GuidelinesWe have compiled alignment guidelines for wordand phrase alignment between annotated syntaxtrees.
The guidelines consist of general principles,concrete rules and guiding principles.The most important general principles are:1.
Align items that can be re-used as units in amachine translation system.2.
Align as many items (i.e.
words and phrases)as possible.3.
Align as close as possible to the tokens.The first principle is central to our work.
Itdefines the general perspective for our alignment.52Figure 1: Tree pair German-English with word and phrase alignments.We do not want to know which part of a sentencehas possibly given rise to which part of the cor-respondence sentence.
Instead our perspective ison whether a phrase pair is general enough to bere-used as translation unit in a machine translationsystem.
For example, we do not want to align dieVerwunderung u?ber das Leben with their astonish-ment at the world although these two phrases werecertainly triggered by the same phrase in the orig-inal and both have a similar function in the twocorresponding sentences.
These two phrases seenin isolation are too far apart in meaning to licensetheir re-use.
We are looking for correspondenceslike was fu?r eine seltsame Welt and what an ex-traordinary world which would make for a goodtranslation in many other contexts.Some special rules follow from this principle.For example, we have decided that a pronoun inone language shall never be aligned with a fullnoun in the other, since such a pair is not directlyuseful in a machine translation system.Principles 2 and 3 are more technical.
Princi-ple 2 tells our annotators that alignment should beexhaustive.
We want to re-use as much as pos-sible from the treebank, so we have to look foras many alignments as possible.
And principle 3says that in case of doubt the alignment should goto the node that is closest to the terminals.
Forexample, our German treebank guidelines requirea multi-word proper noun to first be grouped ina PN phrase which is a daughter node of a nounphrase [[Sofie Amundsen]PN ]NP whereasthe English guidelines only require the NP node[Sophie Amundsen]NP.
When we align thetwo names, principle 3 tells us to draw the align-ment line between the German PN node and theEnglish NP node since the PN node is closer to thetokens than the German NP node.Often we are confronted with phrases that arenot exact translation correspondences but approx-imate translation correspondences.
Consider thephrases mehr als eine Maschine and more than apiece of hardware.
This pair does not represent theclosest possible translation but it represents a pos-sible translation in many contexts.
In a way wecould classify this pair as the ?second-best?
trans-lation.
To allow for such distinctions we provideour annotators with a choice between exact transla-tion correspondences and approximate correspon-dences.
We also use the term fuzzy correspon-dence to refer to and give an intuitive picture ofthese approximate correspondences.
The option to53distinguish between different alignment strengthssounded very attractive at the start but it turned outto be the source for some headaches later.
Whereand how can we draw the line between exact andfuzzy translation correspondences?We have formulated some clear-cut rules:1.
If an acronym is to be aligned with a spelled-out term, it is always an approximate align-ment.
For example, in our economy reportsthe English acronym PT stands for PowerTechnology and is aligned to the German En-ergietechnik as a fuzzy correspondence.2.
Proper names shall be aligned as exact align-ments (even if they are spelled differentlyacross languages; e.g.
Sofie vs. Sophie).But many open questions persist.
Is einer derersten Tage im Mai an exact or rather a fuzzy trans-lation correspondence of early May?
We decidedthat it is not an exact correspondence.
How shallwe handle zu dieser Jahreszeit vs. at this time ofthe year where a literal translation would be in thisseason?
We decided that the former is still an exactcorrespondence.
These examples illustrate the dif-ficulties that make us wonder how useful the dis-tinction between exact and approximate translationcorrespondence really is.Automatically ensuring the overall consistencyof the alignment decisions is a difficult task.But we have used a tool to ensure the consis-tency within the exact and approximate alignmentclasses.
The tool computes the token span for eachalignment and checks if the same tokens pairs havealways received the same alignment type.
For ex-ample, if the phrase pair mit einer blitzschnellenBewegung and with a lightning movement is onceannotated as exact alignment, then it should alwaysbe annotated as exact alignment.
Figure 1 showsapproximate alignments between the PPs in derHand and in her hand.
It was classified as approxi-mate rather than exact alignment since the GermanPP lacks the possessive determiner.Currently our alignment guidelines are 6 pageslong with examples for English-German andEnglish-Swedish alignments.5 Experiments with Student AnnotatorsIn order to check the inter-annotator agreement forthe alignment task we performed the following ex-periment.
We gave 20 tree pairs in German andEnglish to 12 advanced undergraduate students ina class on ?Machine Translation and Parallel Cor-pora?.
Half of the tree pairs were taken from ourSophie?s World treebank and the other half fromour Economy treebank.
We made sure that therewas one 1:2 sentence alignment in the sample.
Thestudents did not have access to the Gold Standardalignment.In class we demonstrated the alignment tool tothe students and we introduced the general align-ment principles to them.
Then the students weregiven a copy of the alignment guidelines.
Weasked them to do the alignments independently ofeach other and to the best of their knowledge ac-cording to the guidelines.In our own annotation of the 20 tree pairs (= theGold Standard alignment) we have the followingnumbers of alignments:type exact fuzzy totalSophie part word 75 3 78phrase 46 12 58Economy part word 159 19 178phrase 62 9 71In the Sophie part of the experiment treebank wehave 78 word-to-word alignments and 58 phrase-to-phrase alignments.
Note that some phrases con-sist only of one word and thus the same alignmentinformation is represented twice.
We have deliber-ately kept this redundancy.The alignments in the Sophie part consist of125 times 1:1 alignments, 4 times 1:2 alignmentsand one 1:3 alignment (wa?re vs. would have been)when viewed from the German side.
There are 3times 1:2 alignments (e.g.
introducing vs. stelltevor) and no other 1:many alignment when viewedfrom the English side.In the Economy part the picture is similar.
Thevast majority are 1:1 alignments.
There are 207times 1:1 alignments and 21 times 1:2 alignments(many of which are German compound nouns)when viewed from German.
And there are 235times 1:1 alignments, plus 4 times 1:2 alignments,plus 2 times 1:3 alignments when viewed fromEnglish (e.g.
the Americas was aligned to the threetokens Nord- und Su?damerika).The student alignments showed a huge vari-ety in terms of numbers of alignments.
In theSophie part they ranged from 125 alignments tobare 47 alignments (exact alignments and fuzzyalignments taken together).
In the Economy partthe variation was between 259 and 62 alignments.54On closer inspection we found that the studentwith the lowest numbers works as a translatorand chose to use a very strict criterion of transla-tion equivalence rather than translation correspon-dence.
Three other students at the end of the list arenot native speakers of either German and English.We therefore decided to exclude these 4 studentsfrom the following comparison.The student alignments allow for the investiga-tion of a number of interesting questions:1.
How did the students?
alignments differ fromthe Gold Standard?2.
Which were the alignments done by all stu-dents?3.
Which were the alignments done by singlestudents only?4.
Which alignments varied most between exactand fuzzy alignment?When we compared each student?s alignmentsto the Gold Standard alignments, we computedthree figures:1.
How often did the student alignment and theGold Standard alignment overlap?2.
How many Gold Standard alignments did thestudent miss?3.
How many student alignments were not in theGold Standard?The remaining 8 students reached between 81%and 48% overlap with the Gold Standard on theSophie part, and between 89% and 66% overlapwith the Gold Standard on the Economy texts.
Thiscan be regarded as their recall values if we assumethat the Gold Standard represents the correct align-ments.
These same 8 students additionally hadbetween 2 and 22 own alignments in the Sophiepart and between 12 and 55 own alignments in theEconomy part.So the interesting question is: What kind ofalignments have they missed, and which werethe additional own alignments that they suggested(alignments that are not in the gold standard)?
Wefirst checked the students with the highest numbersof own alignments.
We found that some of thesealignments were due to the fact that students hadignored the rule to align as close to the tokens aspossible (principle 3 above).Another reason was that students sometimesaligned a word (or some words) with a node.For example, one student had aligned the wordnatu?rlich to the phrase of course instead of to theword sequence of course.
Our alignment tool al-lows that, but the alignment guidelines discour-age such alignments.
There might be exceptionalcases where a word-to-phrase alignment is neces-sary in order to keep valuable information, but ingeneral we try to stick to word-to-word and phrase-to-phrase alignments.Another discrepancy occurred when the stu-dents aligned a German verb group with a singleverb form in English (e.g.
ist zuru?ckzufu?hren vs.reflecting).
We have decided to only align the fullverb to the full verb (independent of the inflection).This means that we align only zuru?ckzufu?hren toreflecting in this example.The uncertainties on how to deal with differentgrammatical forms led to the most discrepancies.Shall we align the definite NP die Umsa?tze withthe indefinite NP revenues since it is much morecommon to drop the article in an English plural NPthan in German?
Shall we align a German genitiveNP with an of-PP in English (der beiden Divisio-nen vs. of the two divisions)?
We have decided togive priority to form over function and thus to alignthe NP der beiden Divisionen with the NP the twodivisions.
But of course this choice is debatable.When we compute the intersection of the align-ments done by all students (ignoring the differencebetween exact and fuzzy alignments), we find thatabout 50% of the alignments done by the studentwith the smallest number of alignments is sharedby all other students.
All of the alignments in theintersection are in our Gold Standard file.
This in-dicates that there is a core of alignments that areobvious and uncontroversial.
Most of them areword alignments.When we compute the union of the alignmentsdone by all students (again ignoring the differencebetween exact and fuzzy alignments), we find thatthe number of alignments in the union is 40% to50% higher than the number of alignments done bythe student with the highest number of alignments.It is also about 40% to 50% higher than the numberof alignments in the Gold Standard.
This meansthat there is considerable deviation from the GoldStandard.Comparing the union of the students?
align-ments to the Gold Standard points to some weak-55nesses of the guidelines.
For example, one align-ment in the Gold Standard that was missed by allstudents concerns the alignment of a German pro-noun (wenn sie die Hand ausstreckte) to an emptytoken in English (herself shaking hands).
Ourguidelines recommend to align such cases as fuzzyalignments, but of course it is difficult to determinethat the empty token really corresponds to the Ger-man word.Other discrepancies concern cases of differinggrammatical forms, e.g.
a German definite singu-lar noun phrase (die Hand) that was aligned to anEnglish plural noun phrase (Hands) in the GoldStandard but missed by all students.
Finally thereare a few cases where obvious noun phrase cor-respondences were simply overlooked by all stu-dents (sich - herself ) although the tokens them-selves were aligned.
Such cases should be handledby an automated process in the alignment tool thatprojects from aligned tokens to their mother nodes(in particular in cases of single token phrases).We also investigated how many exact align-ments and how many fuzzy alignments the stu-dents had used.
The following table gives the fig-ures.exact fuzzy overlap totalSophie part 152 106 69 189Economy part 296 188 119 366The alignments done by all students resulted in aunion set of 189 alignments for the Sophie part and366 alignments for the Economy part.
The align-ments in the Sophie part consisted of 152 exactalignments and 106 fuzzy alignments.
This meansthat 69 alignments were marked as both exact andfuzzy.
In other words, in 69 cases at least one stu-dent has marked an alignment as fuzzy while atleast one other student has marked the same align-ment as good.
So there is still considerable con-fusion amongst the annotators on how to decidebetween exact and fuzzy alignments.
And in caseof doubt many students have decided in favor offuzzy alignments.6 ConclusionsWe have shown the difficulties in creating cross-language word and phrase alignments.
Experi-ments with a group of students have helped to iden-tify the weaknesses in our alignment guidelinesand in our Gold Standard alignment.
We have re-alized that the guidelines need to contain a hostof fine-grained alignment rules and examples thatwill clarify critical cases.In order to evaluate a set of alignment experi-ments with groups of annotators it is important tohave good visualization tools to present the results.We have worked with Perl scripts for the compar-ison and with our own TreeAligner tool for the vi-sualization.
For example we have used two colorsto visualize a student?s alignment overlap with theGold Standard in one color and his own alignments(that are not in the Gold Standard) in another color.In order to visualize the agreements of the wholegroup it would be desirable to have the option to in-crease the alignment line width in proportion to thenumber of annotators that have chosen a particularalignment link.
This would give an intuitive im-pression of strong alignment links and weak align-ment links.Another option for future extension of this workis an even more elaborate classification of thealignment links.
(Hansen-Schirra et al, 2006) havedemonstrated how a fine-grained distinction be-tween different alignment types could look like.Annotating such a corpus will be labor-intensivebut provide for a wealth of cross-language obser-vations.ReferencesAhrenberg, Lars, Magnus Merkel, and Michael Petter-stedt.
2003.
Interactive word alignment for languageengineering.
In Proc.
of EACL-2003, Budapest.Ahrenberg, Lars.
2007.
LinES: An English-Swedishparallel treebank.
In Proc.
of Nodalida, Tartu.Burchardt, A., K. Erk, A. Frank, A. Kowalski, S. Pado?,and M. Pinkal.
2006.
The SALSA corpus: A Ger-man corpus resource for lexical semantics.
In Pro-ceedings of LREC 2006, pages 969?974, Genoa.Groves, Declan, Mary Hearne, and Andy Way.
2004.Robust sub-sentential alignment of phrase-structuretrees.
In Proceedings of Coling 2004, pages 1072?1078, Geneva, Switzerland, Aug 23?Aug 27.
COL-ING.Hansen-Schirra, Silvia, Stella Neumann, and MihaelaVela.
2006.
Multi-dimensional annotation andalignment in an English-German translation corpus.In Proceedings of the EACL Workshop on Multidi-mensional Markup in Natural Language Processing(NLPXML-2006), pages 35?
42, Trento.Kruijff-Korbayova?, Ivana, Kla?ra Chva?talova?, and OanaPostolache.
2006.
Annotation guidelines for theCzech-English word alignment.
In Proceedings ofLREC, Genova.56Lundborg, Joakim, Torsten Marek, Mae?l Mettler,and Martin Volk.
2007.
Using the StockholmTreeAligner.
In Proc.
of The 6th Workshop on Tree-banks and Linguistic Theories, Bergen, December.Melamed, Dan.
1998.
Manual annotation of transla-tional equivalence: The blinker project.
TechnicalReport 98-06, IRCS, Philadelphia PA.Samuelsson, Yvonne and Martin Volk.
2006.
Phrasealignment in parallel treebanks.
In Hajic, Jan andJoakim Nivre, editors, Proc.
of the Fifth Workshop onTreebanks and Linguistic Theories, pages 91?102,Prague, December.Samuelsson, Yvonne and Martin Volk.
2007.
Align-ment tools for parallel treebanks.
In Proceedings ofGLDV Fru?hjahrstagung 2007.Smith, Noah A. and Michael E. Jahr.
2000.
Cairo:An alignment visualization tool.
In Proc.
of LREC-2000, Athens.Tinsley, John, Ventsislav Zhechev, Mary Hearne, andAndy Way.
2007.
Robust language pair-independentsub-tree alignment.
In Machine Translation SummitXI Proceedings, Copenhagen.57
