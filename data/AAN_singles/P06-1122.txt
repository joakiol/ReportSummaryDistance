Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 969?976,Sydney, July 2006. c?2006 Association for Computational LinguisticsModelling lexical redundancy for machine translationDavid Talbot and Miles OsborneSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh, EH8 9LW, UKd.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.ukAbstractCertain distinctions made in the lexiconof one language may be redundant whentranslating into another language.
Wequantify redundancy among source typesby the similarity of their distributions overtarget types.
We propose a language-independent framework for minimisinglexical redundancy that can be optimiseddirectly from parallel text.
Optimisationof the source lexicon for a given target lan-guage is viewed as model selection over aset of cluster-based translation models.Redundant distinctions between types mayexhibit monolingual regularities, for ex-ample, inflexion patterns.
We define aprior over model structure using a Markovrandom field and learn features over setsof monolingual types that are predictiveof bilingual redundancy.
The prior makesmodel selection more robust without theneed for language-specific assumptions re-garding redundancy.
Using these mod-els in a phrase-based SMT system, weshow significant improvements in transla-tion quality for certain language pairs.1 IntroductionData-driven machine translation (MT) relies onmodels that can be efficiently estimated from par-allel text.
Token-level independence assumptionsbased on word-alignments can be used to decom-pose parallel corpora into manageable units for pa-rameter estimation.
However, if training data isscarce or language pairs encode significantly dif-ferent information in the lexicon, such as Czechand English, additional independence assumptionsmay assist the model estimation process.Standard statistical translation models use sep-arate parameters for each pair of source and targettypes.
In these models, distinctions in either lex-icon that are redundant to the translation processwill result in unwarranted model complexity andmake parameter estimation from limited paralleldata more difficult.
A natural way to eliminatesuch lexical redundancy is to group types into ho-mogeneous clusters that do not differ significantlyin their distributions over types in the other lan-guage.
Cluster-based translation models capturethe corresponding independence assumptions.Previous work on bilingual clustering has fo-cused on coarse partitions of the lexicon thatresemble automatically induced part-of-speechclasses.
These were used to model genericword-alignment patterns such as noun-adjectivere-ordering between English and French (Och,1998).
In contrast, we induce fine-grained parti-tions of the lexicon, conceptually closer to auto-matic lemmatisation, optimised specifically to as-sign translation probabilities.
Unlike lemmatisa-tion or stemming, our method specifically quanti-fies lexical redundancy in a bilingual setting anddoes not make language-specific assumptions.We tackle the problem of redundancy in thetranslation lexicon via Bayesian model selectionover a set of cluster-based translation models.
Wesearch for the model, defined by a clustering ofthe source lexicon, that maximises the marginallikelihood of target tokens in parallel data.
In thisoptimisation, source types are combined into clus-ters if their distributions over target types are toosimilar to warrant distinct parameters.Redundant distinctions between types may ex-hibit regularities within a language, for instance,inflexion patterns.
These can be used to guidemodel selection.
Here we show that the inclusionof a model ?prior?
over the lexicon structure leadsto more robust translation models.
Although a pri-ori we do not know which monolingual featurescharacterise redundancy for a given language pair,by defining a model over the prior monolingual969space of source types and cluster assignments, wecan introduce an inductive bias that allows cluster-ing decisions in different parts of the lexicon to in-fluence one another via monolingual features.
Weuse an EM-type algorithm to learn weights for aMarkov random field parameterisation of this priorover lexicon structure.We obtain significant improvements in transla-tion quality as measured by BLEU, incorporatingthese optimised model within a phrase-based SMTsystem for three different language pairs.
TheMRF prior improves the results and picks up fea-tures that appear to agree with linguistic intuitionsof redundancy for the language pairs considered.2 Lexical redundancy between languagesIn statistical MT, the source and target lexiconsare usually defined as the sets of distinct types ob-served in the parallel training corpus for each lan-guage.
Such models may not be optimal for cer-tain language pairs and training regimes.A word-level statistical translation model ap-proximates the probability Pr(E|F ) that a sourcetype indexed by F will be translated as a targettype indexed by E. Standard models, e.g.
Brownet al (1993), consist of discrete probability distri-butions with separate parameters for each uniquepairing of a source and target types; no attempt ismade to leverage structure within the event spacesE and F during parameter estimation.
This resultsin a large number of parameters that must be esti-mated from limited amounts of parallel corpora.We refer to distinctions made between lexicaltypes in one language that do not result in differentdistributions over types in the other language aslexically redundant for the language pair.
Sincethe role of the translation model is to determine adistribution over target types given a source type,when the corresponding target distributions do notvary significantly over a set of source types, themodel gains nothing by maintaining a distinct setof parameters for each member of this set.Lexical redundancy may arise when languagesdiffer in the specificity with which they refer to thesame concepts.
For instance, colours of the spec-trum may be partitioned differently (e.g.
blue inEnglish v.s.
sinii and goluboi in Russian).
It willalso arise when languages explicitly encode differ-ent information in the lexicon.
For example, trans-lating from French to English, a standard modelwould treat the following pairs of source and tar-get types as distinct events with entirely unre-lated parameters: (vert, green), (verte, green),(verts, green) and (vertes, green).
Here theFrench types differ only in their final suffixes dueto adjectival agreement.
Since there is no equiva-lent mechanism in English, these distinctions areredundant with respect to this target language.Distinctions that are redundant in the sourcelexicon when translating into one language may,however, be significant when translating into an-other.
For instance, the French adjectival numberagreement (the addition of an s) may be significantwhen translating to Russian which also marks ad-jectives for number (the inflexion to -ye).We can remove redundancy from the translationmodel by conflating redundant types, e.g.
vert .={vert, verte, verts, vertes}, and averaging bilin-gual statistics associated with these events.3 Eliminating redundancy in the modelRedundancy in the translation model can beviewed as unwarranted model complexity.
Acluster-based translation model defined via a hard-clustering of the lexicon can reduce this com-plexity by introducing additional independence as-sumptions: given the source cluster label, cj , thetarget type, ei, is assumed to be independent of theexact source type, fj , observed, i.e., p(ei|fj) ?p(ei|cj).
Optimising the model for lexical redun-dancy can be viewed as model selection over a setof such cluster-based translation models.We formulate model search as a maximum aposteriori optimisation: the data-dependent term,p(D|C), quantifies evidence provided for a model,C, by bilingual training data, D, while the prior,p(C), can assert a preference for a particularmodel structure (clustering of the source lexicon)on the basis of monolingual features.
Both termshave parameters that are estimated from data.
For-mally, we search for C?,C?
= argmaxC p(C|D)= argmaxC p(C)p(D|C).
(1)Evaluating the data-dependent term, p(D|C), fordifferent partitions of the source lexicon, we cancompare how well different models predict the tar-get tokens aligned in a parallel corpus.
This termwill prefer models that group together source typeswith similar distributions over target types.
Byusing the marginal likelihood (integrating out theparameters of the translation model) to calculate970p(D|C), we can account explicitly for the com-plexity of the translation model and compare mod-els with different numbers of clusters as well asdifferent assignments of types to clusters.In addition to an implicit uniform prior overcluster labels as in k-means clustering (e.g.
Chou(1991)), we also consider a Markov random field(MRF) parameterisation of the p(C) term to cap-ture monolingual regularities in the lexicon.
TheMRF induces dependencies between clusteringdecisions in different parts of the lexicon via amonolingual feature space biasing the search to-wards models that exhibit monolingual regulari-ties.
Rather than assuming a priori knowledge ofredundant distinctions in the source language, weuse an EM algorithm to update parameters for fea-tures defined over sets of source types on the basisof existing cluster assignments.
While initially themodel search will be guided only by informationfrom the bilingual statistics in p(D|C), monolin-gual regularities in the lexicon, such as inflexionpatterns, may gradually be propagated through themodel as p(C) becomes informative.
Our exper-iments suggest that the MRF prior enables morerobust model selection.As stated, the model selection procedure ac-counts for redundancy in the source lexicon us-ing the target distributions.
The target lexiconcan be optimised analogously.
Clustering targettypes allows the implementation of independenceassumptions asserting that the exact specificationof a target type is independent of the source typegiven knowledge of the target cluster label.
For ex-ample, when translating an English adjective intoFrench it may be more efficient to use the trans-lation model to specify only that the translationlies within a certain set of French adjectives, corre-sponding to a single lemma, and have the languagemodel select the exact form.
Our experiments sug-gest that it can be useful to account for redundancyin both languages in this way; this can be incorpo-rated simply within our optimisation procedure.In Section 3.1 we describe the bilingualmarginal likelihood, p(D|C), clustering proce-dure; in Section 3.2 we introduce the MRF param-eterisation of the prior, p(C), over model struc-ture; and in Section 3.3, we describe algorithmicapproximations.3.1 Bilingual model selectionAssume we are optimising the source lexicon (thetarget lexicon is optimised analogously).
A clus-tering of the lexicon is a unique mapping CF :F ?
CF defined for all f ?
F where, in additionto all source types observed in the parallel trainingcorpus, F may include items seen in other mono-lingual corpora (and, in the case of the source lex-icon only, the development and test data).
Thestandard SMT lexicon can be viewed as a cluster-ing with each type observed in the parallel trainingcorpus assigned to a distinct cluster and all othertypes assigned to a single ?unknown word?
cluster.We optimise a conditional model of target to-kens from word-aligned parallel corpora, D ={Dc0 , ..., DcN }, where Dci represents the set oftarget words that were aligned to the set of sourcetypes in cluster ci.
We assume that each target to-ken in the corpus is generated conditionally i.i.d.given the cluster label of the source type to whichit is aligned.
Sufficient statistics for this modelconsist of co-occurrence counts of source and tar-get types summed across each source cluster,#cf (e).=?f ?
?cf#(e, f ?).
(2)Maximising the likelihood of the data under thismodel would require us to specify the number ofclusters (the size of the lexicon) in advance.
In-stead we place a Dirichlet prior parameterised by?1 over the translation model parameters of eachcluster, ?cf ,e, defining the conditional distribu-tions over target types.
Given a clustering, theDirichlet prior, and independent parameters, thedistribution over data and parameters factorises,p(D,?|CF , ?)
=?cf?CFp(Dcf , ?cf |cf , ?)??cf?CF?e?E??
?1+#cf (e)cf ,eWe optimise cluster assignments with respect tothe marginal likelihood which averages the like-lihood of the set of counts assigned to a cluster,Dcf , under the current model over the prior,p(Dcf |?, cf ) =?p(?cf |?
)p(Dcf |?cf , cf )d?cf .This can be evaluated analytically for a Dirichletprior with multinomial parameters.Assuming a (fixed) uniform prior over modelstructure, p(C), model selection involves itera-tively re-assigning source types to clusters suchas to maximise the marginal likelihood.
Re-assignments may alter the total number of clusters1Distinct from the prior over model structure, p(C).971at any point.
Updates can be calculated locally, forinstance, given the sets of target tokens Dci andDcj aligned to source types currently in clustersci and cj , the change in log marginal likelihood ifclusters ci and cj are merged into cluster c?
is,?ci,cj?c?
= logp(Dc?|?, c?
)p(Dci |?, ci)p(Dcj |?, cj), (3)which is a Bayes factor in favour of the hypothe-sis that Dci and Dcj were sampled from the samedistribution (Wolpert, 1995).
Unlike its equivalentin maximum likelihood clustering, Eq.
(3) may as-sume positive values favouring a smaller numberof clusters when the data does not support a morecomplex hypothesis.
The more complex model,with ci and cj modelled separately, is penalisedfor being able to model a wider range of data sets.The hyperparameter, ?, is tied across clustersand taken to be proportional to the marginal (the?background?)
distribution over target types in thecorpus.
Under this prior, source types aligned tothe same target types, will be clustered togethermore readily if these target types are less frequentin the corpus as a whole.3.2 Markov random field model priorAs described above we consider a Markov randomfield (MRF) parameterisation of the prior overmodel structure, p(C).
This defines a distributionover cluster assignments of the source lexicon as awhole based solely on monolingual characteristicsof the lexical types and the relations between theirrespective cluster assignments.Viewed as graph, each variable in the MRF ismodelled as conditionally independent of all othervariables given the values of its neighbours (theMarkov property; (Geman and Geman, 1984)).Each variable in the MRF prior corresponds to alexical source type and its cluster assignment.
Fig.1 shows a section of the complete model includingthe MRF prior for a Welsh source lexicon; shad-ing denotes cluster assignments and English tar-get tokens are shown as directed nodes.2 From theMarkov property it follows that this prior decom-poses over neighbourhoods,pMRF(C)?
e?
?f?F?f ?
?Nf?i?i?i(f,f ?,cf ,c?f )Here Nf is the set of neighbours of source type f ;i indexes a set of functions ?i(?)
that pick out fea-tures of a clique; each function has a parameter ?i2The plates represent repeated sampling; each Welshsource type may be aligned to multiple English tokens.Figure 1: Model with Markov random field prior#(f)#(f)#(f) #(f)carcar#(f)waleswalescargar cymrugymrubarmarthat we learn from the data; these are tied acrossthe graph.
?
is a free parameter used to control theoverall contribution of the prior in Eq.
(1).
Herefeatures are defined over pairs of types but higher-order interactions can also be modelled.
We onlyconsider ?positive?
prior knowledge that is indica-tive of redundancy among source types.
Hence allfeatures are non-zero only when their argumentsare assigned to the same cluster.Features can be defined over any aspects of thelexicon; in our experiments we use binary featuresover constrained string edits between types.
Thefollowing feature would be 1, for instance, if theWelsh types cymru and gymru (see Fig.
1), wereassigned to the same cluster.3?1(fi = (c ?)
?
fj = (g ?)
?
ci = cj)Setting the parameters of the MRF prior overthis feature space by hand would require a prioriknowledge of redundancies for the language pair.In the absence of such knowledge, we use an it-erative EM algorithm to update the parameters onthe basis of the previous solution to the bilingualclustering procedure.
EM parameter estimationforces the cluster assignments of the MRF prior toagree with those obtained on the basis of bilingualdata using monolingual features alone.
Since fea-tures are tied across the MRF, patterns that char-acterise redundant relations between types will bere-enforced across the model.
For instance (seeFig.
1), if cymru and gymru are clustered to-gether, the parameter for feature ?1, shown above,may increase.
This induces a prior preference forcar and gar to form a cluster on subsequent it-erations.
A similar feature defined for mar andgar in the a priori string edit feature space, onthe other hand, may remain uninformative if notobserved frequently on pairs of types assigned tothe same clusters.
In this way, the model learns to3Here?
matches a common substring of both arguments.972generalise language-specific redundancy patternsfrom a large a priori feature space.
Changes in theprior due to re-assignments can be calculated lo-cally and combined with the marginal likelihood.3.3 Algorithmic approximationsThe model selection procedure is an EM algo-rithm.
Each source type is initially assigned toits own cluster and the MRF parameters, ?i, areinitialised to zero.
A greedy E-step iteratively re-assigns each source type to the cluster that max-imises Eq.
(1); cluster statistics are updated af-ter any re-assignment.
To reduce computation, weonly consider re-assignments that would cause atleast one (non-zero) feature in the MRF to fire, orto clusters containing types sharing target word-alignments with the current type; types may alsobe re-assigned to a cluster of their own at any iter-ation.
When clustering both languages simultane-ously, we average ?target?
statistics over the num-ber of events in each ?target?
cluster in Eq.
(2).We re-estimate the MRF parameters after eachpass through the vocabulary.
These are updatedaccording to MLE using a pseudolikelihood ap-proximation (Besag, 1986).
Since MRF parame-ters can only be non-zero for features observed ontypes clustered together during an E-step, we uselazy instantiation to work with a large implicit fea-ture set defined by a constrained string edit.The algorithm has two free parameters: ?
deter-mining the strength of the Dirichlet prior used inthe marginal likelihood, p(D|C), and ?
which de-termines the contribution of pMRF(C) to Eq.
(1).4 ExperimentsPhrase-based SMT systems have been shown tooutperform word-based approaches (Koehn et al,2003).
We evaluate the effects of lexicon modelselection on translation quality by considering twoapplications within a phrase-based SMT system.4.1 Applications to phrase-based SMTA phrase-based translation model can be estimatedin two stages: first a parallel corpus is aligned atthe word-level and then phrase pairs are extracted(Koehn et al, 2003).
Aligning tokens in paral-lel sentences using the IBM Models (Brown etal., 1993), (Och and Ney, 2003) may require lessinformation than full-blown translation since thetask is constrained by the source and target tokenspresent in each sentence pair.
In the phrase-leveltranslation table, however, the model must assignSource Tokens Types Singletons Test OOVCzech 468K 54K 29K 6K 469French 5682K 53K 19K 16K 112Welsh 4578K 46K 18K 15K 64Table 1: Parallel corpora used in the experiments.probabilities to a potentially unconstrained set oftarget phrases.
We anticipate the optimal modelsizes to be different for these two tasks.We can incorporate an optimised lexicon at theword-alignment stage by mapping tokens in thetraining corpus to their cluster labels.
The map-ping will not change the number of tokens in asentence, hence the word-alignments can be asso-ciated with the original corpus (see Exp.
1).To extrapolate a mapping over phrases from ourtype-level models we can map each type withina phrase to its corresponding cluster label.
This,however, results in a large number of distinctphrases being collapsed down to a single ?clus-tered phrase?.
Using these directly may spreadprobability mass too widely.
Instead we usethem to smooth the phrase translation model (seeExp.
2).
Here we consider a simple interpolationscheme; they could also be used within a backoffmodel (Yang and Kirchhoff, 2006).4.2 Experimental set-upThe system we use is described in (Koehn,2004).
The phrase-based translation model in-cludes phrase-level and lexical weightings in bothdirections.
We use the decoder?s default behaviourfor unknown words copying them verbatim to theoutput.
Smoothed trigram language models are es-timated on training sections of the parallel corpus.We used the parallel sections of the PragueTreebank (Cmejrek et al, 2004), French and En-glish sections of the Europarl corpus (Koehn,2005) and parallel text from the Welsh Assem-bly4 (see Table1).
The source languages, Czech,French and Welsh, were chosen on the basis thatthey may exhibit different degrees of redundancywith respect to English and that they differ mor-phologically.
Only the Czech corpus has explicitmorphological annotation.4.3 ModelsAll models used in the experiments are defined asmappings of the source and target vocabularies.The target vocabulary includes all distinct types4This Welsh-English parallel text is in the public domain.Contact the first author for details.973seen in the training corpus; the source vocabu-lary also includes types seen only in developmentand test data.
Free parameters were set to max-imize our evaluation metric, BLEU, on develop-ment data.
The results are reported on the test sets(see Table 1).
The baseline mappings used were:?
standard: the identity mapping;?
max-pref : a prefix of no more than n letters;?
min-freq: a prefix with a frequency of at leastn in the parallel training corpus.?
lemmatize: morphological lemmas (Czech)standard corresponds to the standard SMT lexi-con.
max-pref and min-freq are both simple stem-ming algorithms that can be applied to raw text.These mappings result in models defined overfewer distinct events that will have higher frequen-cies; min-freq optimises the latter directly.
Weoptimise over (possibly different) values of n forsource and target languages.
The lemmatize map-ping which maps types to their lemmas was onlyapplicable to the Czech corpus.The optimised lexicon models define mappingsdirectly via their clusterings of the vocabulary.
Weconsider the following four models:?
src: clustered source lexicon;?
src+mrf : as src with MRF prior;?
src+trg: clustered source and target lexicons;?
src+trg+mrf : as src+trg with MRF priors.In each case we optimise over ?
(a single value forboth languages) and, when using the MRF prior,over ?
(a single value for both languages).4.4 ExperimentsThe two sets of experiments evaluate the base-line models and optimised lexicon models dur-ing word-alignment and phrase-level translationmodel estimation respectively.?
Exp.
1: map the parallel corpus, performword-alignment; estimate the phrase transla-tion model using the original corpus.?
Exp.
2: smooth the phrase translation model,p(e|f) =#(e, f) + ?#(ce, cf )#(f) + ?#(cf )Here e, f and ce, cf are phrases mapped un-der the standard model and the model be-ing tested respectively; ?
is set once for allexperiments on development data.
Word-alignments were generated using the optimalmax-pref mapping for each training set.5 ResultsTable 2 shows the changes in BLEU when we in-corporate the lexicon mappings during the word-alignment process.
The standard SMT lexiconmodel is not optimal, as measured by BLEU, forany of the languages or training set sizes consid-ered.
Increases over this baseline, however, di-minish with more training data.
For both Czechand Welsh, the explicit model selection procedurethat we have proposed results in better translationsthan all of the baseline models when the MRFprior is used; again these increases diminish withlarger training sets.
We note that the stemmingbaseline models appear to be more effective forCzech than for Welsh.
The impact of the MRFprior is also greater for smaller training sets.Table 3 shows the results of using these modelsto smooth the phrase translation table.5 With theexception of Czech, the improvements are smallerthan for Exp 1.
For all source languages and mod-els we found that it was optimal to leave the tar-get lexicon unmapped when smoothing the phrasetranslation model.Using lemmatize for word-alignment on theCzech corpus gave BLEU scores of 32.71 and37.21 for the 10K and 21K training sets respec-tively; used to smooth the phrase translation modelit gave scores of 33.96 and 37.18.5.1 DiscussionModel selection had the largest impact for smallerdata sets suggesting that the complexity of thestandard model is most excessive in sparse dataconditions.
The larger improvements seen forCzech and Welsh suggest that these languages en-code more redundant information in the lexiconwith respect to English.
Potential sources could begrammatical case markings (Czech) and mutationpatterns (Welsh).
The impact of the MRF prior forsmaller data sets suggests it overcomes sparsity inthe bilingual statistics during model selection.The location of redundancies, in the form ofcase markings, at the ends of words in Czech asassumed by the stemming algorithms may explainwhy these performed better on this language than5The standard model in Exp.
2 is equivalent to the opti-mised max-pref in Exp.
1.974Table 2: BLEU scores with optimised lexicon applied during word-alignment (Exp.
1)Czech-English French-English Welsh-EnglishModel 10K sent.
21K 10K 25K 100K 250K 10K 25K 100K 250Kstandard 32.31 36.17 20.76 23.17 26.61 27.63 35.45 39.92 45.02 46.47max-pref 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11min-freq 33.95 36.98 21.22 23.77 26.74 27.98 36.23 40.65 45.38 46.35src 33.95 37.27 21.43 24.42 26.99 27.82 36.98 40.98 45.81 46.45src+mrf 33.97 37.89 21.63 24.38 26.74 28.39 37.36 41.13 46.50 46.56src+trg 34.24 38.28 22.05 24.02 26.53 27.80 36.83 41.31 45.22 46.51src+trg+mrf 34.70 38.44 22.33 23.95 26.69 27.75 37.56 42.19 45.18 46.48Table 3: BLEU scores with optimised lexicon used to smooth phrase-based translation model (Exp.
2)Czech-English French-English Welsh-EnglishModel 10K sent.
21K 10K 25K 100K 250K 10K 25K 100K 250K(standard)5 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11max-pref 35.63 38.81 22.49 24.10 26.99 28.26 37.31 40.09 45.57 46.41min-freq 34.65 37.75 21.14 23.41 26.29 27.47 36.40 40.84 45.75 46.45src 34.38 37.98 21.28 24.17 26.88 28.35 36.94 39.99 45.75 46.65src+mrf 36.24 39.70 22.02 24.10 26.82 28.09 37.81 41.04 46.16 46.51Table 4: System output (Welsh 25K; Exp.
2)Src ehangu o ffilm i deledu.Ref an expansion from film into television.standard expansion of footage to deledu.max-pref expansion of ffilm to television.src+mrf expansion of film to television.Src yw gwarchod cymru fel gwlad brydferthRef safeguarding wales as a picturesque countrystandard protection of wales as a country brydferthmax-pref protection of wales as a country brydferthsrc+mrf protecting wales as a beautiful countrySrc cynhyrchu canlyniadau llai na pherffaithRef produces results that are less than perfectstandard produce results less than pherffaithmax-pref produce results less than pherffaithsrc+mrf generates less than perfect resultsSrc y dynodiad o graidd y broblemRef the identification of the nub of the problemstandard the dynodiad of the heart of the problemmax-pref the dynodiad of the heart of the problemsrc+mrf the identified crux of the problemon Welsh.
The highest scoring features in theMRF (see Table 5) show that Welsh redundancies,on the other hand, are primarily between initialcharacters.
Inspection of system output confirmsthat OOV types could be mapped to known Welshwords with the MRF prior but not via stemming(see Table 4).
For each language pair the MRFlearned features that capture intuitively redundantpatterns: adjectival endings for French, case mark-ings for Czech, and mutation patterns for Welsh.The greater improvements in Exp.
1 were mir-rored by higher compression rates for these lex-icons (see Table.
6) supporting the conjecturethat word-alignment requires less information thanfull-blown translation.
The results of the lemma-Table 5: Features learned by MRF priorCzech French Welsh(?,?
m) (?,?
s) (c ?, g ?)(?,?
u) (?,?
e) (d ?, dd ?)(?,?
a) (?,?
es) (d ?, t ?)(?,?
ch) (?
e,?
es) (b ?, p ?)(?,?
ho) (?
e,?
er) (c ?, ch ?)(?
a,?
u) (?
e,?
ent) (b ?, f ?
)Note: Features defined over pairs of source types assigned tothe same cluster; here ?
matches a common substring.Table 6: Optimal lexicon size (ratio of raw vocab.
)Czech French WelshWord-alignment 0.26 0.22 0.24TM smoothing 0.28 0.38 0.51tizemodel on Czech show the model selection pro-cedure improving on a simple supervised baseline.6 Related WorkPrevious work on automatic bilingual word clus-tering has been motivated somewhat differentlyand not made use of cluster-based models to as-sign translation probabilities directly (Wang etal., 1996), (Och, 1998).
There is, however, alarge body of work using morphological analy-sis to define cluster-based translation models sim-ilar to ours but in a supervised manner (Zens andNey, 2004), (Niessen and Ney, 2004).
Theseapproaches have used morphological annotation(e.g.
lemmas and part of speech tags) to pro-vide explicit supervision.
They have also involvedmanually specifying which morphological distinc-975tions are redundant (Goldwater and McClosky,2005).
In contrast, we attempt to learn both equiv-alence classes and redundant relations automat-ically.
Our experiments with orthographic fea-tures suggest that some morphological redundan-cies can be acquired in an unsupervised fashion.The marginal likelihood hard-clustering algo-rithm that we propose here for translation modelselection can be viewed as a Bayesian k-means al-gorithm and is an application of Bayesian modelselection techniques, e.g., (Wolpert, 1995).
TheMarkov random field prior over model structureextends the fixed uniform prior over clusters im-plicit in k-means clustering and is common incomputer vision (Geman and Geman, 1984).
Re-cently Basu et al (2004) used an MRF to embodyhard constraints within semi-supervised cluster-ing.
In contrast, we use an iterative EM algo-rithm to learn soft constraints within the ?prior?monolingual space based on the results of cluster-ing with bilingual statistics.7 Conclusions and Future WorkWe proposed a framework for modelling lexicalredundancy in machine translation and tackled op-timisation of the lexicon via Bayesian model se-lection over a set of cluster-based translation mod-els.
We showed improvements in translation qual-ity incorporating these models within a phrase-based SMT sytem.
Additional gains resulted fromthe inclusion of an MRF prior over model struc-ture.
We demonstrated that this prior could beused to learn weights for monolingual features thatcharacterise bilingual redundancy.
Preliminaryexperiments defining MRF features over morpho-logical annotation suggest this model can alsoidentify redundant distinctions categorised lin-guistically (for instance, that morphological caseis redundant on Czech nouns and adjectives withrespect to English, while number is redundant onlyon adjectives).
In future work we will investigatethe use of linguistic resources to define feature setsfor the MRF prior.
Lexical redundancy would ide-ally be addressed in the context of phrases, how-ever, computation and statistical estimation maythen be significantly more challenging.AcknowledgementsThe authors would like to thank Philipp Koehn for providingtraining scripts used in this work; and Steve Renals, MirellaLapata and members of the Edinburgh SMT Group for valu-able comments.
This work was supported by an MRC Prior-ity Area Studentship to the School of Informatics, Universityof Edinburgh.ReferencesSugato Basu, Mikhail Bilenko, and Raymond J. Mooney.2004.
A probabilistic framework for semi-supervisedclustering.
In Proc.
of the 10th ACM SIGKDD Inter-national Conference on Knowledge Discovery and DataMining (KDD-2004).Julian Besag.
1986.
The statistical analysis of dirty pictures.Journal of the Royal Society Series B, 48(2):259?302.Peter Brown, Stephen Della Pietra, Vincent Della Pietra, andRobert Mercer.
1993.
The mathematics of machine trans-lation: Parameter estimation.
Computational Linguistics,19(2):263?311.Philip A. Chou.
1991.
Optimal partitioning for classificationand regression trees.
IEEE Trans.
on Pattern Analysis andMachine Intelligence, 13(4).M.
Cmejrek, J. Curin, J. Havelka, J. Hajic, and V. Kubon.2004.
Prague Czech-English dependency treebank: Syn-tactically annotated resources for machine translation.
In4th International Conference on Language Resources andEvaluation, Lisbon, PortugalS.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration of im-ages.
IEEE Trans.
on Pattern Analysis and Machine Intel-ligence, 6:721?741.Sharon Goldwater and David McClosky.
2005.
Improvingstatistical MT through morphological analysis.
In Proc.of the 2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002).Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings of theHLT/NAACL 2003.Philipp Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.
InProceedings of the AMTA 2004.Philipp Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT Summit 2005.S.
Niessen and H. Ney.
2004.
Statistical machine transla-tion with scarce resources using morpho-syntactic infor-mation.
Computational Linguistics, 30(2):181?204.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.F.-J.
Och.
1998.
An efficient method for determining bilin-gual word classes.
In Proc.
of the European Chapter ofthe Association for Computational Linguistics 1998.Ye-Yi Wang, John Lafferty, and Alex Waibel.
1996.
Wordclustering with parallel spoken language corpora.
In Proc.of 4th International Conference on Spoken Language Pro-cessing, ICSLP 96, Philadelphia, PA.D.H.
Wolpert.
1995.
Determining whether two data sets arefrom the same distribution.
In 15th international work-shop on Maximum Entropy and Bayesian Methods.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-based back-off models for machine translation of highly inflected lan-guages.
In Proc.
of the the European Chapter of the Asso-ciation for Computational Linguistics 2006.R.
Zens and H. Ney.
2004.
Improvements in phrase-basedstatistical machine translation.
In Proc.
of the HumanLanguage Technology Conference (HLT-NAACL 2004).976
