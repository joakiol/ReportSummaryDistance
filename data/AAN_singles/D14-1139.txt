Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1329?1341,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsWeakly-Supervised Learning withCost-Augmented Contrastive EstimationKevin Gimpel Mohit BansalToyota Technological Institute at Chicago, IL 60637, USA{kgimpel,mbansal}@ttic.eduAbstractWe generalize contrastive estimation intwo ways that permit adding more knowl-edge to unsupervised learning.
The firstallows the modeler to specify not only theset of corrupted inputs for each observa-tion, but also how bad each one is.
Thesecond allows specifying structural prefer-ences on the latent variable used to explainthe observations.
They require setting ad-ditional hyperparameters, which can beproblematic in unsupervised learning, sowe investigate new methods for unsuper-vised model selection and system com-bination.
We instantiate these ideas forpart-of-speech induction without tag dic-tionaries, improving over contrastive esti-mation as well as strong benchmarks fromthe PASCAL 2012 shared task.1 IntroductionUnsupervised NLP aims to discover useful struc-ture in unannotated text.
This structure mightbe part-of-speech (POS) tag sequences (Merialdo,1994), morphological segmentation (Creutz andLagus, 2005), or syntactic structure (Klein andManning, 2004), among others.
Unsupervisedsystems typically improve when researchers incor-porate knowledge to bias learning to capture char-acteristics of the desired structure.1There are many successful examples of addingknowledge to improve learning without labeledexamples, including: sparsity in POS tag distri-butions (Johnson, 2007; Ravi and Knight, 2009;Ganchev et al., 2010), short attachments fordependency parsing (Smith and Eisner, 2006),1We note that doing so strains the definition of the termunsupervised.
Hence we will use the term weakly-supervisedto refer to methods that do not explicitly train on labeled ex-amples for the task of interest, but do use some form of task-specific knowledge.agreement of word alignment models (Liang etal., 2006), power law effects in lexical distribu-tions (Blunsom and Cohn, 2010; Blunsom andCohn, 2011), multilingual constraints (Smith andEisner, 2009; Ganchev et al., 2009; Snyder et al.,2009; Das and Petrov, 2011), and orthographiccues (Spitkovsky et al., 2010c; Spitkovsky et al.,2011b), inter alia.Contrastive estimation (CE; Smith and Eisner,2005) is a general approach to weakly-supervisedlearning with a particular way of incorporatingknowledge.
CE increases the likelihood of the ob-servations at the expense of those in a particularneighborhood of each observation.
The neighbor-hood typically contains corrupted versions of theobservations.
The latent structure is marginalizedout for both the observations and their corruptions;the intent is to learn latent structure that helps toexplain why the observation was generated ratherthan any of the corrupted alternatives.In this paper, we present a new objective func-tion for weakly-supervised learning that general-izes CE by including two types of cost functions,one on observations and one on output structures.The first (?4) allows us to specify not only the setof corrupted observations, but also how bad eachcorruption was.
We use n-gram language modelsto measure the severity of each corruption.The second (?5) allows us to specify prefer-ences on desired output structures, regardless ofthe input sentence.
For POS tagging, we attemptto learn language-independent tag frequencies bycomputing counts from treebanks for 11 languagesnot used in our POS induction experiments.
Forexample, we encourage tag sequences that containadjacent nouns and penalize those that contain ad-jacent adpositions.We consider several unsupervised ways to sethyperparameters for these cost functions (?7), in-cluding the recently-proposed log-likelihood esti-mator of Bengio et al.
(2013).
We also circumvent1329hyperparameter selection via system combination,developing a novel voting scheme for POS induc-tion that aligns tag identifiers across runs.We evaluate our approach, which we call cost-augmented contrastive estimation (CCE), onPOS induction without tag dictionaries for fivelanguages from the PASCAL shared task (Gellinget al., 2012).
We find that CCE improves over bothstandard CE as well as strong baselines from theshared task.
In particular, our final average accu-racies are better than all entries in the shared taskthat use the same number of tags.2 Related WorkWeakly-supervised techniques can be roughly cat-egorized in terms of whether they influence themodel, the learning procedure, or explicitly targetthe output structure.
Examples abound in NLP;we focus on those that have been applied to POStagging.There have been many efforts at biasingmodels, including features (Smith and Eisner,2005a; Berg-Kirkpatrick et al., 2010), sparsepriors (Johnson, 2007; Goldwater and Griffiths,2007; Toutanova and Johnson, 2007), sparsityin tag transition distributions (Ravi and Knight,2009), small models via minimum descriptionlength criteria (Vaswani et al., 2010; Poon et al.,2009), a one-tag-per-type constraint (Blunsom andCohn, 2011), and power law effects via Bayesiannonparametrics (Van Gael et al., 2009; Blunsomand Cohn, 2010; Blunsom and Cohn, 2011).We focus below on efforts that induce bias intothe learning (?2.1) or more directly in the outputstructure (?2.2), as they are more closely relatedto our contributions in this paper.2.1 Biasing LearningSome unsupervised methods do not change themodel or attempt to impose structural bias; rather,they change the learning.
This may involve op-timizing a different objective function for thesame model, e.g., by switching from soft to hardEM (Spitkovsky et al., 2010b).
Or it may in-volve changing the objective during learning viaannealing (Smith and Eisner, 2004) or more gen-eral multi-objective techniques (Spitkovsky et al.,2011a; Spitkovsky et al., 2013).Other learning modifications relate to automaticdata selection, e.g., choosing examples for genera-tive learning (Spitkovsky et al., 2010a) or automat-ically generating negative examples for discrimi-native unsupervised learning (Li et al., 2010; Xiaoet al., 2011).CE does both, automatically generating nega-tive examples and changing the objective functionto include them.
Our observation cost function al-ters CE?s objective function, sharpening the effec-tive distribution of the negative examples.2.2 Structural BiasOur output cost function is used to directly spec-ify preferences on desired output structures.
Sev-eral others have had similar aims.
For dependencygrammar induction, Smith and Eisner (2006) fa-vored short attachments using a fixed-weight fea-ture whose weight was optionally annealed duringlearning.
Their bias could be implemented as anoutput cost function in our framework.Posterior regularization (PR; Ganchev et al.,2010) is a general framework for declarativelyspecifying preferences on model outputs.
Naseemet al.
(2010) proposed universal syntactic rules forunsupervised dependency parsing and used themin a PR regime; we use analogous universal tagsequences in our cost function.Our output cost is similar to posterior regular-ization.
The difference is that we specify pref-erences via an arbitrary cost function on outputstructures, while PR uses expectation constraintson posteriors of the model.
We compare to the PRtag induction system of Grac?a et al.
(2011) in ourexperiments, improving over it in several settings.2.3 Exploiting ResourcesMuch of the work mentioned above also benefitsfrom leveraging existing resources.
These may becurated or crowdsourced resources like the Wik-tionary (Li et al., 2012), or traditional annotatedtreebanks for languages other than those under in-vestigation (Cohen et al., 2011).
In this paper, weuse tag statistics from treebanks for 11 languagesto impose our structural bias for a different set oflanguages used in our POS induction experiments.Substantial recent work has improved manyNLP tasks by leveraging multilingual or paral-lel text (Cohen and Smith, 2009; Snyder et al.,2009; Wang and Manning, 2014), including un-supervised POS tagging (Naseem et al., 2009; Dasand Petrov, 2011; T?ackstr?om et al., 2013; Ganchevand Das, 2013).
This sort of multilingual guidancecould also be captured by particular output costfunctions, though we leave this to future work.13303 Unsupervised Structure LearningWe consider a structured unsupervised learningsetting.
We use X to denote our set of possiblestructured inputs, and for a particular x ?
X,we use Y(x) to denote the set of valid structuredoutputs for x.
We are given a dataset of inputs{x(i)}Ni=1.
To map inputs to outputs, we start bybuilding a model of the joint probability distribu-tion p?(x,y).
We use a log-linear parameteriza-tion with feature vector f and weight vector ?:p?
(x,y) =exp{?>f(x,y)}?x??X,y??Y(x?)
exp{?>f(x?,y?
)}where the sum in the denominator ranges over allpossible inputs and all valid outputs for them.In this paper, we consider ways of learning theparameters ?.
Given ?, at test time we output a yfor a new x using, e.g., Viterbi or minimum Bayesrisk decoding; we use the latter in this paper.3.1 EM and Contrastive EstimationWe start by reviewing two ways of choosing?.
The expectation-maximization algorithm (EM;Dempster et al., 1977) finds a local optimum ofthe marginal (log-)likelihood of the observations{x(i)}Ni=1.
The marginal log-likelihood is a sumover all x(i)of the gain function ?EM(x(i)):?EM(x(i)) = log?y?Y(x(i))p?
(x(i),y)= log?y?Y(x(i))exp{?>f(x(i),y)}?
log?x??X,y??Y(x?)exp{?>f(x?,y?)}?
??
?Z(?
)The difficulty is the final term, logZ(?
), whichrequires summing over all possible inputs andall valid outputs for them.
This summation istypically intractable for structured problems, andmay even diverge.
For this reason, EM is typi-cally only used to train log-linear model weightswhen Z(?)
= 1, e.g., for hidden Markov models,probabilistic context-free grammars, and modelscomposed of locally-normalized log-linear mod-els (Berg-Kirkpatrick et al., 2010), among others.There have been efforts at approximating thesummation over elements of X, whether by limit-ing sequence length (Haghighi and Klein, 2006),only summing over observations in the trainingdata (Riezler, 1999), restricting the observationspace based on the task (Dyer et al., 2011), or us-ing Gibbs sampling to obtain an unbiased sampleof the full space (Della Pietra et al., 1997; Rosen-feld, 1997).Contrastive estimation (CE) addresses this chal-lenge by using a neighborhood function N : X?2Xthat generates a set of inputs that are ?corrup-tions?
of an input x; N(x) always includes x. Us-ing shorthand Nifor N(x(i)), CE corresponds tomaximizing the sum over inputs x(i)of the gain?CE(x(i))= log Pr(x(i)| Ni)= log?y?Y(x(i)) p?(x(i),y)?x??Ni?y??Y(x?)
p?(x?,y?
)= log?y?Y(x(i))exp{?>f(x(i),y)}?log?x??Ni?y??Y(x?)exp{?>f(x?,y?
)}Two logZ(?)
terms cancel out, leaving the sum-mation over input/output pairs in the neighbor-hood instead of the full summation over pairs.Two desiderata govern the choice of N. One isto make the summation over its elements computa-tionally tractable.
If N(x) = X for all x ?
X, weobtain EM, so a smaller neighborhood typicallymust be used in practice.
The second considera-tion is to target learning for the task of interest.
ForPOS tagging and dependency parsing, Smith andEisner (2005a, 2005b) used neighborhood func-tions that corrupted the observations in systematicways, e.g., their TRANS1 neighborhood containsthe original sentence along with those that resultfrom transposing a single pair of adjacent words.The intent was to force the learner to explain whythe given sentences were observed at the expenseof the corrupted sentences.Next we present our modifications to con-trastive estimation.
Both can be viewed as addingspecialized cost functions that penalize some partof the structured input/output pair.4 Modeling Corruption CostsWhile CE allows us to specify a set of corruptedx for each x(i)via the neighborhood function N,it says nothing about how bad each corruption is.The same type of corruption might be harmful inone context and not harmful in another.This fact was suggested as the reason why cer-tain neighborhoods did not work as well for POS1331tagging as others (Smith and Eisner, 2005a).
Onepoorly-performing neighborhood consisted of sen-tences in which a single word of the originalwas deleted.
Deleting a single word in a sen-tence might not harm grammaticality.
By contrast,neighborhoods that transpose adjacent words ledto better results.
These kinds of corruptions are ex-pected to be more frequently harmful, at least forlanguages with relatively rigid word order.
How-ever, there may still be certain transpositions thatare benign, at least for grammaticality.To address this, we introduce an observationcost function ?
: X ?
X ?
R?0that indicateshow much two observations differ.
Using ?, wedefine the following gain function ?CCE1(x(i)) =log?y?Y(x(i))exp{?>f(x(i),y)}?log?x??Ni?y??Y(x?)exp{?>f(x?,y?)
+ ?(x(i),x?
)}The function ?
inflates the score of neighbor-hood entries with larger differences from the ob-served x(i).
This gain function is inspired by ideasfrom structured large-margin learning (Taskar etal., 2003; Tsochantaridis et al., 2005), specifi-cally softmax-margin (Povey et al., 2008; Gimpeland Smith, 2010).
Softmax-margin extends con-ditional likelihood by allowing the user to specifya cost function to give partial credit for structuresthat are partially correct.
Conditional likelihood,by contrast, treats all incorrect structures equally.While softmax-margin uses a cost function tospecify how two output structures differ, our gainfunction ?CCE1uses a cost function ?
to specifyhow two inputs differ.
But the motivations are sim-ilar: since poor structures have their scores artifi-cially inflated by ?, learning pays more attentionto them, choosing weights that penalize them morethan the lower-cost structures.4.1 Observation Cost FunctionsWhat types of cost functions should we consider?For efficient inference, we want to ensure that?
decomposes additively across parts of the cor-rupted input x?in the same way as the features; weassume unigram and bigram features in this paper.In addition, the choice of the observation costfunction ?
is tied to the choice of neighborhoodfunction.
In our experiments, we use neighbor-hoods that change the order of words in the obser-vation but not the set of words.
Our first cost func-tion simply counts the number of novel bigramsintroduced when corrupting the original:?I(x(i),x) = ?|x|+1?j=1I[xj?1xj/?
2grams(x(i))]where xjis the jth word of sentence x, x0isthe start-of-sentence marker, x|x|+1 is the end-of-sentence marker, 2grams(x) returns the set of bi-grams in x, I[] returns 1 if its argument is true and0 otherwise, and ?
is a constant to be tuned.
Wecall this cost function MATCH.
Only x(i)(whichis always contained in Ni) is guaranteed to havecost 0.
In the TRANS1 neighborhood, corruptedsequences will be penalized more if their transpo-sitions occur in the middle of the sentence ratherthan at the beginning or end.We also consider a version that weights the in-dicator by the negative log probability of the novelbigram: ?LM(x(i),x) =?|x|+1?j=1?log P(xj|xj?1)I[xj?1xj/?
2grams(x(i))]where P(xj|xj?1) is obtained from a bigram lan-guage model.
Among novel bigrams in the cor-ruption x, if the second word is highly surprisingconditioned on the first, the bigram will incur highcost.
We refer to ?LM(x(i),x) as MATLM.5 Expressing Structural PreferencesOur second modification to CE allows us to spec-ify structural preferences for outputs y.
We firstnote that there exist objective functions for su-pervised structure prediction that never requirecomputing the feature vector for the true outputy(i).
Examples include Bayes risk (Kaiser et al.,2000; Povey and Woodland, 2002) and structuredramp loss (Do et al., 2008).
These two objec-tives do, however, need to compute a cost func-tion cost(y(i),y), which requires the true outputy(i).
We start with the following form of struc-tured ramp loss from Gimpel and Smith (2012),transformed here to a gain function:maxy?Y(x(i))(?>f(x(i),y)?
cost(y(i),y))?maxy??Y(x(i))(?>f(x(i),y?)
+ cost(y(i),y?
))(1)Maximizing this gain function for supervisedlearning corresponds to increasing the model score1332of outputs that have both high model score (?>f )and low cost, while decreasing the model score ofoutputs with high model score and high cost.For unsupervised learning, we do not have y(i),so we simply drop y(i)from the cost function.
Theresult is an output cost function pi : Y ?
R?0which captures our a priori knowledge about de-sired output structures.
The value of pi(y) shouldbe large for outputs y that are far from the ideal.In this paper, we consider POS induction and useintrinsic evaluation; however, in a real-world sce-nario, the output cost function could use signalsderived from the downstream task in which thetags are being used.Given pi, we convert each max to a log?exp inEq.
1 and introduce the contrastive neighborhoodinto the second term, defining our new gain func-tion ?CCE2(x(i)) =log?y?Y(x(i))exp{?>f(x(i),y)?
pi(y)}?log?x??Ni?y??Y(x?)exp{?>f(x?,y?)
+ pi(y?
)}Gimpel (2012) found that using such ?softened?versions of the ramp losses worked better than theoriginal versions (e.g., Eq.
1) when training ma-chine translation systems.5.1 Output Cost FunctionsThe output cost pi should capture our desider-ata about y for the task of interest.
We con-sider universal POS tag subsequences analogousto the universal syntactic rules of Naseem et al.(2010).
In doing so, we use the universal tags ofPetrov et al.
(2012): NOUN, VERB, ADJ (ad-jective), ADV (adverb), PRON (pronoun), DET(determiner), ADP (pre/postposition), NUM (nu-meral), CONJ (conjunction), PRT (particle), ?.?
(punctuation), and X (other).We aimed for a set of rules that would be ro-bust across languages.
So, we used treebanks for11 languages from the CoNLL 2006/2007 sharedtasks (Buchholz and Marsi, 2006; Nivre et al.,2007) other than those used in our POS induc-tion experiments.
In particular, we used Arabic,Bulgarian, Catalan, Czech, English, Spanish, Ger-man, Hungarian, Italian, Japanese, and Turkish.We replicated shorter treebanks a sufficient num-ber of times until they were a similar size as thelargest treebank.
Then we counted gold POS tagunigrams and bigrams from the concatenation.tag unigram count costX 50783 3.83NUM 174613 2.59PRT 179131 2.57ADV 330210 1.96CONJ 436649 1.68PRON 461880 1.62DET 615284 1.33ADJ 694685 1.21ADP 906922 0.95VERB 1018989 0.83.
1042662 0.81NOUN 2337234 0tag bigram count costDET PRT 109 84.41DET CONJ 518 68.82NUM ADV 1587 57.63NOUN NOUN 409828 2.09DET NOUN 454980 1.04NOUN .
504897 0Table 1: Counts and costs for universal tags basedon treebanks for 11 languages not used in POS in-duction experiments.Where #(y) is the count of tag y in the treebankconcatenation, the cost of y isu(y) = log(maxy?#(y?
)#(y))and, where #(?y1, y2?)
is the count of tag bigram?y1, y2?, the cost of ?y1, y2?
isu(?y1, y2?)
= 10?log(max?y?1,y?2?#(?y?1, y?2?
)#(?y1, y2?
))We use a multiplier of 10 in order to exaggeratecount differences among bigrams, which gener-ally are closer together than unigram counts.
InTable 1, we show counts and costs for all tag uni-grams and selected tag bigrams.2Given these costs for individual tag unigramsand bigrams, we use the following pi function,which we call UNIV:pi(y) = ?|y|+1?j=1u(yj) + u(?yj?1, yj?
)where ?
is a constant to be tuned and yjis thejth tag of y.
We define y0to be the beginning-of-sentence marker and y|y|+1 to be the end-of-sentence marker (which has unigram cost 0).Many POS induction systems use one-tag-per-type constraints (Blunsom and Cohn, 2011;Gelling et al., 2012), which often lead to higher2The complete tag bigram list is provided in the supple-mentary material.1333max?N?i=1log?y?Y(x(i))exp{?>f(x(i),y)}?
log?x??Ni?y??Y(x?)exp{?>f(x?,y?)}(2)max?N?i=1log?y?Y(x(i))exp{?>f(x(i),y)?
pi(y)}?
log?x??Ni?y??Y(x?)exp{?>f(x?,y?)
+ ?(x(i),x?)
+ pi(y?
)}(3)Figure 1: Contrastive estimation (Eq.
2) and cost-augmented contrastive estimation (Eq.
3).
L2 regular-ization terms (C2?|?|j=1?2j) are not shown here but were used in our experiments.accuracies even though the gold standard is notconstrained in this way.
This constraint can be en-coded as an output cost function, though it wouldrequire approximate inference (Poon et al., 2009).6 Cost-Augmented CEWe extended the objective function underlyingCE by defining two new types of cost functions,one on observations (?4) and one on outputs (?5).We combine them into a single objective, whichwe call cost-augmented contrastive estimation(CCE), shown as Eq.
3 in Figure 1.If the cost functions ?
and pi factor in the sameway as the features f , then it is straightforwardto implement CCE atop an existing CE implemen-tation.
The additional terms in the cost functionscan be implemented as features with fixed weights(albeit where the weight differs depending on thecontext).7 Model SelectionOur modifications give increased flexibility, butrequire setting new hyperparameters.
In additionto the choice of the cost functions, each has aweight: ?
for ?
and ?
for pi.
We need ways toset these weights that do not require labeled data.Smith and Eisner (2005a) chose the hyperpa-rameter values that yielded the best CE objec-tive on held-out development data.
We use theirstrategy, though we experiment with two others aswell.3In particular, we estimate held-out data log-likelihood via the method of Bengio et al.
(2013)and also consider ways of combining outputs frommultiple models.7.1 Estimating Held-Out Log-LikelihoodBengio et al.
(2013) recently proposed ways toefficiently estimate held-out data log-likelihood3When using their strategy for CCE, we compute the CEcriterion only, omitting the costs.
We do so because theweights of the cost terms can have a large impact on the mag-nitude of the objective, making it difficult to do a fair com-parison of models with different cost weights.for generative models.
They showed empiricallythat a simple, biased version of their conserva-tive sampling-based log-likelihood (CSL) estima-tor can be useful for model selection.The biased CSL requires a Markov chain on thevariables in the model (i.e., x and y) as well asthe ability to compute p?(x|y).
It generates con-secutive samples of y from a Markov chain ini-tialized at each x in a development set D, withS Markov chains run for each x.
We computeand sum p?
(x|yj) for each sampled yj, then sumover all x in D. The result is a biased estimate forthe log-likelihood of D. Bengio et al.
showed thatthese biased estimates could give the same modelranking as unbiased estimates, though more effi-ciently.
They also showed that taking the single,initial sample from the S Markov chains resultedin the same model ranking as using many samplesfrom each chain.
We follow suit here.Our Markov chain is a blocked Gibbs sam-pler in which we alternate between sampling fromp?
(y|x) and p?(x|y).
Since we only use a sin-gle sample from each Markov chain and initializeeach chain to x, this simply amounts to drawing Ssamples from p?(y|x).
To sample from p?
(y|x),we use the exact algorithm obtained by runningthe backward algorithm and then performing left-to-right sampling of tags using the local featuresand requisite backward terms to define the localtag distributions.We then compute p?
(x|y) for each sampled y.If there are no features in f that look at more thanone word (which is the case with the features usedin our experiments), then this probability factors:p?
(x|y) =?|y|k=1p?
(xk|yk)This is easily computable assuming that we havenormalization constants Z(y) cached for each tagy.
To compute each Z(y), we sum over all wordsobserved in the training data (replacing some witha special UNK token; see below).
We can thencompute likelihoods for individual words and mul-1334tiply them across the words in the sentence to com-pute p?
(x|y).To summarize, we get a log-likelihood estimatefor development setD = {x(i)}|D|i=1by sampling Stimes from p?
(y|x(i)) for each x(i), getting sam-ples {{y(i),j}Sj=1}|D|i=1, then we compute?|D|i=1?Sj=1log p?
(x(i)|y(i),j)We used values of S ?
{1, 10, 100}, finding thatthe ranking of models was consistent across S val-ues.
We used S = 10 in all results reported below.We note that this estimator was originally pre-sented for generative models, and that (C)CE isnot a generative training criterion.
It seeks to max-imize the conditional probability of an observationgiven its neighborhood.
Nonetheless, when imple-menting our log-likelihood estimator, we treat themodel as a generative model, computing the Z(y)constants by summing over all words in the vocab-ulary.7.2 System CombinationWe can avoid choosing a single model by com-bining the outputs of multiple models via systemcombination.
We decode test data by using poste-rior decoding.
To combine the outputs of multiplemodels, we find the max-posterior tag under eachmodel, then choose the highest vote-getter, break-ing ties arbitrarily.However, when doing POS induction without atag dictionary, the tags are simply unique identi-fiers and may not have consistent meaning acrossruns.
To address this, we propose a novel votingscheme that is inspired by the widely-used 1-to-1accuracy metric for POS induction (Haghighi andKlein, 2006).
This metric maps system tags togold tags to maximize accuracy with the constraintthat each gold tag is mapped to at most once.
Theoptimal mapping can be found by solving a maxi-mum weighted bipartite matching problem.We adapt this idea to map tags between two sys-tems, rather than between system tags and goldtags.
Given k systems that we want to combine,we choose one to be the backbone and map the re-maining k ?
1 systems?
outputs to the backbone.4After mapping each system?s output to the back-bone system, we perform simple majority votingamong all k systems.
To choose the backbone, we4We use the LEMON C++ toolkit (Dezs et al., 2011) tosolve the maximum weighted bipartite matching problems.consider each of the k systems in turn as back-bone and maximize the sum of the weights of theweighted bipartite matching solutions found.
Thisis a heuristic that attempts to choose a backbonethat is similar to all other systems.
We foundthat highly-weighted matchings often led to highPOS tagging accuracy metrics.
We call this vot-ing scheme ALIGN.
To see the benefit of ALIGN,we also compare to a simple scheme (NA?IVE) thatperforms majority voting without any tag map-ping.8 ExperimentsTask and Datasets We consider POS inductionwithout tag dictionaries using five freely-availabledatasets from the PASCAL shared task (Gellinget al., 2012).5These include Danish (DA), usingthe Copenhagen Dependency Treebank v2 (Buch-Kromann et al., 2007); Dutch (NL), using theAlpino treebank (Bouma et al., 2001); Por-tuguese (PT), using the Floresta Sint?a(c)tica tree-bank (Afonso et al., 2002); Slovene (SL), us-ing the jos500k treebank (Erjavec et al., 2010);and Swedish (SV), using the Talbanken tree-bank (Nivre et al., 2006).
We use their providedtraining, development, and test sets.Evaluation We fix the number of tags in ourmodels to 12, which matches the number of uni-versal tags from Petrov et al.
(2012).
We useboth many-to-1 (M-1) and 1-to-1 (1-1) accuracyas our evaluation metrics, using the universal tagsfor the gold standard (which was done for the of-ficial evaluation for the shared task).6We notethat our pi function assigns identities to tags (e.g.,tag 1 is assumed to be NOUN), so we could useactual tagging accuracy when training with the picost function.
But we use M-1 and 1-1 accuracyto enable easier comparison both among differentsettings and to prior work.Baselines From the shared task, we compareto all entries that used 12 tags.
These include5http://wiki.cs.ox.ac.uk/InducingLinguisticStructure/SharedTask6It is common to use a greedy algorithm to com-pute 1-to-1 accuracy, e.g., as in the shared task scor-ing script (http://www.dcs.shef.ac.uk/?tcohn/wils/eval.tar.gz), though the optimal mapping canbe computed efficiently via the maximum weighted bipartitematching algorithm, as stated above.
We use the shared taskscorer for all results here for ease of comparison.
When weinstead evaluate using the optimal mapping, we find that ac-curacies are usually only slightly higher than those found bythe greedy algorithm.1335BROWN clusters (Brown et al., 1992), clusters ob-tained using the mkcls tool (Och, 1995), and thefeaturized HMM with sparsity constraints trainedusing posterior regularization (PR), described byGrac?a et al.
(2011).
The PR system achieved thehighest average 1-1 accuracy in the shared task.We restrict our attention to systems that use 12tags because the M-1 and 1-1 metrics are highlydependent upon the number of hypothesized tags.In general, using more tags leads to higher M-1and lower 1-1 (Gelling et al., 2012).
By keep-ing the number of tags fixed, we hope to provide acleaner comparison among approaches.We compare to two other baselines: an HMMtrained with 500 iterations of EM and an HMMtrained with 100 iterations of stepwise EM (Liangand Klein, 2009).
We used random initializationas done by Liang and Klein: we set each param-eter in each multinomial to exp{1 + c}, wherec ?
U [0, 1], then normalized to get probabilitydistributions.
For stepwise EM, we used mini-batch size 3 and stepsize reduction power 0.7.For all models we trained, including both base-lines and CCE, we used only the training dataduring training and used the unannotated devel-opment data for certain model selection criteria.No labels were used except for final evaluation onthe test data.
Therefore, we need a way to handleunknown words in test data.
When running EMand stepwise EM, while reading in the final 10%of sentences in the training set, we replace novelwords with the special token UNK.
We then re-place unknown words in test data with UNK.8.1 CCE SetupFeatures We use standard indicator features ontag-tag transitions and tag-word emissions, thespelling features from Smith and Eisner (2005a),and additional emission features based on Brownclusters.
The latter features are simply indicatorsfor tag-cluster pairs?analogous to tag-word emis-sions in which the word is replaced by its Browncluster identifier.
We run Brown clustering (Liang,2005) on the POS training data for each language,once with 12 clusters and once with 40, then addtag-cluster emission features for each clusteringand one more for their conjunction.77To handle unknown words: for words that only appearin the final 10% of training sentences, we replace them withUNK when firing their tag-word emission features.
We usespecial Brown cluster identifiers reserved for UNK.
But westill use all spelling features derived from the actual wordLearning We solve Eq.
2 and Eq.
3 by runningLBFGS until convergence on the training data, upto 100 iterations.
We tag the test data with mini-mum Bayes risk decoding and evaluate.We use two neighborhood functions:?
TRANS1: the original sentence along with allsentences that result from doing a single trans-position of adjacent words.?
SHUFF10: the original sentence along with 10random permutations of it.We use L2 regularization, addingC2?|?|j=1?2jtothe objectives shown in Figure 1.
We use a fixed(untuned) C = 0.0001 for all experiments re-ported below.8We initialize each CE model bysampling weights from N(0, 1).Cost Functions The cost functions ?
and pihave constants ?
and ?
which balance their con-tributions relative to the model score and must betuned.
We consider the ways proposed in Sec-tion 7, namely tuning based on the contrastive es-timation criterion computed on development data(CE), the log-likelihood estimate on developmentdata with S = 10 (LL), and our two system com-bination algorithms: na?
?ve voting (NA?IVE) andaligned voting (ALIGN), both of which use as in-put the 4 system outputs whose hyperparametersled to the highest values for the CE criterion ondevelopment data.We used ?
?
{3 ?
10?4, 10?3, 3 ?10?3, 0.01, 0.03, 0.1, 0.3} and ?
?
{3 ?10?6, 10?5, 3 ?
10?5, 10?4, 3 ?
10?4}.
Setting?
= ?
= 0 gives us CE, which we also compareto.
When using both MATLM and UNIV simul-taneously, we first choose the best two ?
valuesby the LL criterion and the best two ?
values bythe CE criterion when using only those individualcosts.
This gives us 4 pairs of values; we run ex-periments with these pairs and choose the pair toreport using each of the model selection criteria.For system combination, we use the 4 system out-puts resulting from these 4 pairs.For training bigram language models for theMATLM cost, we use the language?s POS train-ing data concatenated with its portion of the Eu-roparl v7 corpus (Koehn, 2005) and the text of itstype.
For unknown words at test time, we use the UNK emis-sion feature, the Brown cluster features with the special UNKcluster identifiers, and the word?s actual spelling features.8In subsequent experiments we tried C ?
{0.01, 0.001}for the baseline CE setting and found minimal differences.1336neigh-costmod.
DA NL PT SL SV avgborhood sel.
M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1SHUFF10none N/A 45.0 38.0 55.1 45.7 54.2 38.0 54.7 45.7 47.4 31.3 51.3 39.7MATCHCE 48.9 31.5 56.5 46.4 54.2 37.7 55.9 46.8 48.9 33.8 52.9 39.2LL 49.9 34.4 56.5 46.4 54.1 38.9 57.2 48.9 48.9 33.8 53.3 40.5MATLMCE 49.1 34.3 59.6 50.4 53.6 37.1 55.0 46.2 48.8 33.1 53.2 40.2LL 50.2 40.0 59.6 50.4 53.1 36.0 58.0 48.4 48.8 33.1 53.9 41.6TRANS1none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2MATCHCE 58.5 42.5 66.3 53.3 70.6 43.3 59.1 45.6 59.3 54.2 62.7 47.8LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6MATLMCE 59.4 43.5 63.8 50.1 70.2 43.0 58.5 46.1 59.2 54.8 62.2 47.5LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9Table 2: Results for observation cost functions.
The CE baseline corresponds to rows where cost=?none?.Other rows are CCE.
Best score for each column and each neighborhood is bold.Wikipedia.
The word counts for the Wikipediasused range from 18M for Slovene to 1.9B forDutch.
We used modified Kneser-Ney smoothingas implemented by SRILM (Stolcke, 2002).8.2 ResultsWe present two sets of results.
First we compareour MATCH and MATLM observation cost func-tions for our two neighborhoods and two ways ofdoing model selection.
Then we do a broader com-parison, comparing both types of costs and theircombination to our full set of baselines.Observation Cost Functions In Table 2, weshow results for observation cost functions.
Wenote that the TRANS1 neighborhood works muchbetter than the SHUFF10 neighborhood, but wefind that using cost functions can close the gap incertain cases, particularly for Dutch and Slovenefor which the SHUFF10 MATLM scores approachor exceed the TRANS1 scores without a cost.Since the SHUFF10 neighborhood exhibitsmore diversity than TRANS1, we expect to seelarger gains from using observation cost functions.We do in fact see larger gains in M-1, e.g., averageimprovements are 1.6-2.6 for SHUFF10 and 0.4-1.3 for TRANS1, though 1-1 gains are closer.For TRANS1, while MATCH does reach aslightly higher average M-1 than MATLM, the lat-ter does much better in 1-1 (49.9 vs. 47.6 whenusing LL for model selection).
For SHUFF10,MATLM consistently does better than MATCH.Nonetheless, we suspect MATCH works as well asit does because it at least differentiates the obser-vation (which is always part of the neighborhood)from the corruptions.We find that the LL model selection criterionconsistently works better than the CE criterion formodel selection.
When using LL model selectionand fixing the neighborhood, all average scores arebetter than their CE baselines.
For M-1, the aver-age improvement is 1.0 to 2.6 points, and for 1-1the average improvement ranges from 0.4 to 2.7.We find the best overall performance when us-ing MATLM with LL model selection with theTRANS1 neighborhood, and we report this settingin our subsequent experiments.Output Cost Function Table 3 shows resultswhen using our UNIV output cost function, as wellas our full set of baselines.
All (C)CE experimentsused the TRANS1 neighborhood.We find that our contrastive estimation baseline(cost=?none?)
has a higher average M-1 (61.8)than all results from the shared task, but its average1-1 accuracy is lower than that reached by poste-rior regularization, the best system in the sharedtask according to 1-1.
Using an observation costfunction increases both M-1 and 1-1: MATLMyields an average 1-1 of 49.9, nearing the 50.1 ofPR while exceeding it in M-1 by nearly 2 points.When using the UNIV cost function, we seesome variation in performance across model selec-tion criteria, but we find improvements in both M-1 and 1-1 accuracy under most settings.
When do-ing model selection via ALIGN voting, we roughlymatch the average 1-1 of PR, and when using theCE criterion, we beat it by 1 point on average (51.3vs.
50.1).Combined Costs When using the UNIV cost,we find that model selection via CE works bet-ter than LL.
So for the combined costs, we tookthe two best MATLM weights (?
values) accord-ing to LL and the two best UNIV weights (?
val-ues) according to CE and ran combined cost ex-periments (MATCHLM+UNIV) with the four pairsof hyperparameters.
Then from among these four,1337systemDA NL PT SL SV avgM-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1HMM, EM 42.5 28.1 53.0 40.6 59.4 33.7 50.3 34.7 49.3 33.9 50.9 34.2HMM, stepwise EM 51.7 38.2 61.6 45.2 66.5 46.7 53.6 35.7 55.3 39.6 57.7 41.1BROWN 47.1 39.2 57.3 43.1 67.6 51.6 58.3 42.3 57.6 51.3 57.6 45.5mkcls 53.1 44.2 63.0 54.1 68.1 46.3 50.4 40.6 57.3 43.6 58.4 45.8posterior regularization 53.8 45.6 57.6 45.4 74.4 56.1 60.0 48.5 58.8 54.9 60.9 50.1contrastive estimationcost model sel.none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2MATCH LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6MATLM LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9UNIVCE 59.7 45.6 60.6 51.1 70.0 62.7 60.9 44.1 57.1 52.8 61.7 51.3LL 59.5 42.2 62.1 56.3 70.7 43.1 60.9 44.1 57.1 52.8 62.1 47.7NA?IVE 59.2 45.6 62.2 52.8 72.7 52.7 60.0 43.8 56.2 53.0 62.2 49.6ALIGN 61.6 47.3 63.7 54.5 74.4 53.1 59.7 42.1 56.6 53.2 63.2 50.0MATLM CE 59.8 45.7 60.4 48.4 70.0 62.8 52.9 45.0 59.4 54.9 60.5 51.4+LL 59.3 42.5 61.9 56.2 70.8 43.1 59.3 41.9 60.0 55.1 62.3 47.8NA?IVE 58.5 44.4 64.9 60.3 65.4 52.1 55.5 45.9 59.0 54.4 60.6 51.4UNIV ALIGN 61.1 45.4 66.2 60.9 75.8 49.8 59.5 48.2 59.0 54.4 64.3 51.7Table 3: Unsupervised POS tagging accuracies for five languages, showing results for three systems fromthe PASCAL shared task as well as three other baselines (EM, stepwise EM, and contrastive estimation).All (C)CE results use the TRANS1 neighborhood.
The best score in each column is bold.we again chose results by CE, LL, and both votingschemes.The results are shown in the lower part of Ta-ble 3.
We find different trends in M-1 and 1-1 depending on whether we use CE or LL formodel selection, which may be due to our lim-ited hyperparameter search stemming from com-putational constraints.
However, by comparingNA?IVE to ALIGN, we see a consistent benefitfrom aligning tags before voting, leading to ourhighest average accuracies.
In particular, usingMATCHLM+UNIV and ALIGN, we improve overCE by 2.5 in M-1 and 4.5 in 1-1, also improvingover the best results from the shared task.9 ConclusionWe have shown how to modify contrastive estima-tion to use additional sources of knowledge, bothin terms of observation and output cost functions.We adapted a recently-proposed technique for es-timating the log-likelihood of held-out data, find-ing it to be effective as a model selection criterionwhen using observation cost functions.
We im-proved tagging accuracy by using weak supervi-sion in the form of universal tag frequencies.
Weproposed a system combination method for POSinduction systems that consistently performs bet-ter than na?
?ve voting and circumvents hyperpa-rameter selection.
We reported results on par withor exceeding the best systems from the PASCAL2012 shared task.Contrastive estimation has been shown effectivefor numerous NLP tasks, including dependencygrammar induction (Smith and Eisner, 2005b),bilingual part-of-speech induction (Chen et al.,2011), morphological segmentation (Poon et al.,2009), and machine translation (Xiao et al., 2011).The hope is that our contributions can benefit theseand other applications of weakly-supervised learn-ing.AcknowledgmentsWe thank the anonymous reviewers for their in-sightful comments and Waleed Ammar, ChrisDyer, David McAllester, Sasha Rush, NathanSchneider, Noah Smith, and John Wieting forhelpful discussions.ReferencesS.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.Floresta sint?a(c)tica: a treebank for Portuguese.
InProc.
of LREC.Y.
Bengio, L. Yao, and K. Cho.
2013.
Boundingthe test log-likelihood of generative models.
arXivpreprint arXiv:1311.6184.T.
Berg-Kirkpatrick, A. Bouchard-C?ot?e, J. DeNero,and D. Klein.
2010.
Painless unsupervised learn-ing with features.
In Proc.
of NAACL.P.
Blunsom and T. Cohn.
2010.
Unsupervised induc-tion of tree substitution grammars for dependencyparsing.
In Proc.
of EMNLP.1338P.
Blunsom and T. Cohn.
2011.
A hierarchical Pitman-Yor process HMM for unsupervised part of speechinduction.
In Proc.
of ACL.G.
Bouma, G. Van Noord, and R. Malouf.
2001.Alpino: Wide-coverage computational analysis ofDutch.
Language and Computers, 37(1).P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. DellaPietra, and J. C. Lai.
1992.
Class-based N-grammodels of natural language.
Computational Lin-guistics, 18(4).M.
Buch-Kromann, J. Wedekind, and J. Elming.2007.
The Copenhagen Danish-English dependencytreebank v. 2.0. code.google.com/p/copenhagen-dependency-treebank.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.of CoNLL.D.
Chen, C. Dyer, S. B. Cohen, and N. A. Smith.
2011.Unsupervised bilingual POS tagging with Markovrandom fields.
In Proc.
of the First Workshop onUnsupervised Learning in NLP.S.
Cohen and N. A. Smith.
2009.
Shared logistic nor-mal distributions for soft parameter tying in unsu-pervised grammar induction.
In Proc.
of NAACL.S.
B. Cohen, D. Das, and N. A. Smith.
2011.
Unsu-pervised structure prediction with non-parallel mul-tilingual guidance.
In Proc.
of EMNLP.M.
Creutz and K. Lagus.
2005.
Unsupervised mor-pheme segmentation and morphology induction fromtext corpora using Morfessor 1.0.
Helsinki Univer-sity of Technology.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proc.
of ACL.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
1997.Inducing features of random fields.
IEEE Trans.Pattern Anal.
Mach.
Intell., 19(4).A.
Dempster, N. Laird, and D. Rubin.
1977.
Maxi-mum likelihood estimation from incomplete data viathe EM algorithm.
Journal of the Royal StatisticalSociety B, 39:1?38.B.
Dezs, A. J?uttner, and P. Kov?acs.
2011.
LEMON - anopen source C++ graph template library.
Electron.Notes Theor.
Comput.
Sci., 264(5).C.
B.
Do, Q.
Le, C. H. Teo, O. Chapelle, and A. Smola.2008.
Tighter bounds for structured estimation.
InAdvances in NIPS.C.
Dyer, J. H. Clark, A. Lavie, and N. A. Smith.
2011.Unsupervised word alignment with arbitrary fea-tures.
In Proc.
of ACL.T.
Erjavec, D. Fiser, S. Krek, and N. Ledinek.
2010.The JOS linguistically tagged corpus of Slovene.
InProc.
of LREC.K.
Ganchev and D. Das.
2013.
Cross-lingual discrim-inative learning of sequence models with posteriorregularization.
In Proc.
of EMNLP.K.
Ganchev, J. Gillenwater, and B. Taskar.
2009.
De-pendency grammar induction via bitext projectionconstraints.
In Proc.
of ACL.K.
Ganchev, J. V. Grac?a, J. Gillenwater, and B. Taskar.2010.
Posterior regularization for structured latentvariable models.
Journal of Machine Learning Re-search, 11.D.
Gelling, T. Cohn, P. Blunsom, and J. V. Grac?a.2012.
The PASCAL challenge on grammar induc-tion.
In Proc.
of NAACL-HLT Workshop on the In-duction of Linguistic Structure.K.
Gimpel and N. A. Smith.
2010.
Softmax-marginCRFs: Training log-linear models with cost func-tions.
In Proc.
of NAACL.K.
Gimpel and N. A. Smith.
2012.
Structured ramploss minimization for machine translation.
In Proc.of NAACL.K.
Gimpel.
2012.
Discriminative Feature-Rich Mod-eling for Syntax-Based Machine Translation.
Ph.D.thesis, Carnegie Mellon University.S.
Goldwater and T. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
InProc.
of ACL.J.
V. Grac?a, K. Ganchev, L. Coheur, F. Pereira, andB.
Taskar.
2011.
Controlling complexity in part-of-speech induction.
J. Artif.
Int.
Res., 41(2).A.
Haghighi and D. Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proc.
of HLT-NAACL.M.
Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proc.
of EMNLP-CoNLL.J.
Kaiser, B. Horvat, and Z. Kacic.
2000.
A novel lossfunction for the overall risk criterion based discrimi-native training of HMM models.
In Proc.
of ICSLP.D.
Klein and C. D. Manning.
2004.
Corpus-basedinduction of syntactic structure: Models of depen-dency and constituency.
In Proc.
of ACL.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In Proc.
of MT Summit.Z.
Li, Z. Wang, S. Khudanpur, and J. Eisner.
2010.Unsupervised discriminative language model train-ing for machine translation using simulated confu-sion sets.
In Proc.
of COLING.S.
Li, J. V. Grac?a, and B. Taskar.
2012.
Wiki-ly super-vised part-of-speech tagging.
In Proc.
of EMNLP.1339P.
Liang and D. Klein.
2009.
Online EM for unsuper-vised models.
In Proc.
of NAACL.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proc.
of HLT-NAACL.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Instituteof Technology.B.
Merialdo.
1994.
Tagging English text with a proba-bilistic model.
Computational Linguistics, 20(2).T.
Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.2009.
Multilingual part-of-speech tagging: Two un-supervised approaches.
JAIR, 36.T.
Naseem, H. Chen, R. Barzilay, and M. Johnson.2010.
Using universal linguistic knowledge to guidegrammar induction.
In Proc.
of EMNLP.J.
Nivre, J. Nilsson, and J.
Hall.
2006.
Talbanken05: ASwedish treebank with phrase structure and depen-dency annotation.
In Proc.
of LREC.J.
Nivre, J.
Hall, S. K?ubler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.of CoNLL.F.
J. Och.
1995.
Maximum-likelihood-sch?atzungvon wortkategorien mit verfahren der kombina-torischen optimierung.
Bachelor?s thesis (Studien-arbeit), Friedrich-Alexander-Universit?at Erlangen-N?urnburg, Germany.S.
Petrov, D. Das, and R. McDonald.
2012.
A univer-sal part-of-speech tagset.
In Proc.
of LREC.H.
Poon, C. Cherry, and K. Toutanova.
2009.
Unsuper-vised morphological segmentation with log-linearmodels.
In Proc.
of HLT: NAACL.D.
Povey and P. C. Woodland.
2002.
Minimumphone error and I-smoothing for improved discrima-tive training.
In Proc.
of ICASSP.D.
Povey, D. Kanevsky, B. Kingsbury, B. Ramabhad-ran, G. Saon, and K. Visweswariah.
2008.
BoostedMMI for model and feature space discriminativetraining.
In Proc.
of ICASSP.S.
Ravi and K. Knight.
2009.
Minimized models forunsupervised part-of-speech tagging.
In Proc.
ofACL.S.
Riezler.
1999.
Probabilistic Constraint Logic Pro-gramming.
Ph.D. thesis, Universit?at T?ubingen.R.
Rosenfeld.
1997.
A whole sentence maximum en-tropy language model.
In Proc.
of ASRU.N.
A. Smith and J. Eisner.
2004.
Annealing techniquesfor unsupervised statistical language learning.
InProc.
of ACL.N.
A. Smith and J. Eisner.
2005a.
Contrastive estima-tion: Training log-linear models on unlabeled data.In Proc.
of ACL.N.
A. Smith and J. Eisner.
2005b.
Guiding unsuper-vised grammar induction using contrastive estima-tion.
In Proc.
of IJCAI Workshop on GrammaticalInference Applications.N.
A. Smith and J. Eisner.
2006.
Annealing structuralbias in multilingual weighted grammar induction.
InProc.
of COLING-ACL.D.
A. Smith and J. Eisner.
2009.
Parser adaptationand projection with quasi-synchronous features.
InProc.
of EMNLP.B.
Snyder, T. Naseem, and R. Barzilay.
2009.
Unsu-pervised multilingual grammar induction.
In Proc.of ACL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2010a.From Baby Steps to Leapfrog: How ?Less is More?in unsupervised dependency parsing.
In Proc.
ofNAACL-HLT.V.
I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.Manning.
2010b.
Viterbi training improves unsu-pervised dependency parsing.
In Proc.
of CoNLL.V.
I. Spitkovsky, D. Jurafsky, and H. Alshawi.
2010c.Profiting from mark-up: Hyper-text annotations forguided parsing.
In Proc.
of ACL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011a.Lateen EM: Unsupervised training with multiple ob-jectives, applied to dependency grammar induction.In Proc.
of EMNLP.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011b.Punctuation: Making a point in unsupervised depen-dency parsing.
In Proc.
of CoNLL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2013.Breaking out of local optima with count transformsand model recombination: A study in grammar in-duction.
In Proc.
of EMNLP.A.
Stolcke.
2002.
SRILM?an extensible languagemodeling toolkit.
In Proc.
of ICSLP.O.
T?ackstr?om, D. Das, S. Petrov, R. McDonald, andJ.
Nivre.
2013.
Token and type constraints for cross-lingual part-of-speech tagging.
Transactions of theAssociation for Computational Linguistics, 1.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-margin Markov networks.
In Advances in NIPS 16.K.
Toutanova and M. Johnson.
2007.
A BayesianLDA-based model for semi-supervised part-of-speech tagging.
In Advances in NIPS.I.
Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-tun.
2005.
Large margin methods for structured andinterdependent output variables.
Journal of MachineLearning Research, 6.1340J.
Van Gael, A. Vlachos, and Z. Ghahramani.
2009.The infinite HMM for unsupervised POS tagging.In Proc.
of EMNLP.A.
Vaswani, A. Pauls, and D. Chiang.
2010.
Efficientoptimization of an MDL-inspired objective functionfor unsupervised part-of-speech tagging.
In Proc.
ofACL.M.
Wang and C. D. Manning.
2014.
Cross-lingualprojected expectation regularization for weakly su-pervised learning.
Transactions of the Associationfor Computational Linguistics, 2.X.
Xiao, Y. Liu, Q. Liu, and S. Lin.
2011.
Fast gen-eration of translation forest for large-scale SMT dis-criminative training.
In Proc.
of EMNLP.1341
