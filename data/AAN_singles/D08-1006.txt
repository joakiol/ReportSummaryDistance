Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 51?58,Honolulu, October 2008. c?2008 Association for Computational LinguisticsRefining Generative Language Models using Discriminative LearningBen SandbankBlavatnik School of Computer ScienceTel-Aviv UniversityTel-Aviv 69978, Israelsandban@post.tau.ac.ilAbstractWe propose a new approach to language mod-eling which utilizes discriminative learningmethods.
Our approach is an iterative one:starting with an initial language model, ineach iteration we generate 'false' sentencesfrom the current model, and then train a clas-sifier to discriminate between them and sen-tences from the training corpus.
To the extentthat this succeeds, the classifier is incorpo-rated into the model by lowering the probabil-ity of sentences classified as false, and theprocess is repeated.
We demonstrate the effec-tiveness of this approach on a natural lan-guage corpus and show it provides an 11.4%improvement in perplexity over a modifiedkneser-ney smoothed trigram.1 IntroductionLanguage modeling is a fundamental task in natu-ral language processing and is routinely employedin a wide range of applications, such as speechrecognition, machine translation, etc?.
Tradition-ally, a language model is a probabilistic modelwhich assigns a probability value to a sentence or asequence of words.
We refer to these as generativelanguage models.
A very popular example of agenerative language model is the n-gram, whichconditions the probability of the next word on theprevious (n-1)-words.Although simple and widely-applicable, it hasproven difficult to allow n-grams, and other formsof generative language models as well, to take ad-vantage of non-local and overlapping features.1These sorts of features, however, pose no problemfor standard discriminative learning methods, e.g.large-margin classifiers.
For this reason, a newclass of language model, the discriminative lan-guage model, has been proposed recently to aug-ment generative language models (Gao et al,2005; Roark et al, 2007).
Instead of providingprobability values, discriminative language modelsdirectly classify sentences as either correct or in-correct, where the definition of correctness de-pends on the application (e.g.
grammatical /ungrammatical, correct translation / incorrect trans-lation, etc').Discriminative learning methods requirenegative samples.
Given that the corpora used fortraining language models contain only realsentences, i.e.
positive samples, obtaining thesecan be problematic.
In most work ondiscriminative language modeling this was not amajor issue as the work was concerned withspecific applications, and these provided a naturaldefinition of negative samples.
For instance,(Roark et al, 2007) proposed a discriminativelanguage model for a speech recognition task.Given an acoustic sequence, a baseline recognizerwas used to generate a set of possibletranscriptions.
The correct transcription was takenas a positive sample, while the rest were taken asnegative samples.
More recently, however,Okanohara and Tsujii (2007) showed that a1Conditional maximum entropy models (Rosenfeld, 1996)provide somewhat of a counter-example, but there, too, manykinds of global and non-local features are difficult to use(Rosenfeld, 1997).51discriminative language model can be trainedindependently of a specific application by using agenerative language model to obtain the negativesamples.
Using a non-linear large-margin learningalgorithm, they successfully trained a classifier todiscriminate real sentences from sentencesgenerated by a trigram.In this paper we extend this line of work tostudy the extent to which discriminative learningmethods can lead to better generative languagemodels per-se.
The basic intuition is the following:if a classifier can be used to discriminate real sen-tences from 'false' sentences generated by a lan-guage model, then it can also be used to improvethat language model by taking probability massaway from sentences classified as false and trans-ferring it to sentences classified as real.
If the re-sulting language model can be efficiently sampledfrom, then this process can be repeated, until gen-erated sentences can no longer be distinguishedfrom real ones.The remainder of the paper is structured asfollows: In the next section we formally developthis intuition, providing a quick overview of thewhole-sentence maximum-entropy model and ofself-supervised boosting, two previous works onwhich we rely.
We also present the method we usefor sampling from the current model, which for thepresent work is far more efficient than the classicalGibbs sampling.
Our experimental results arepresented in section 3, and section 4 concludeswith a discussion and a future outlook.2 Learning Framework2.1 Whole-sentence maximum-entropy modelThe vast majority of statistical language modelsestimate the probability of a given sentence as aproduct of conditional probabilities via the chainrule:11P( ) P( ... ) P( | )ndef defn i iis w w w h== = ?
(1)where 1 1...defi ih w w ?=  is called the history of theword wi.
Most work on language modelingtherefore is directed at the estimation of( | )i iP w h .
While this is theoretically correct, itmakes it difficult to incorporate global informationabout the sentence into the model, e.g.
length,grammaticality, etc'.
For this reason, the whole-sentence maximum-entropy model was proposedin (Rosenfeld, 1997).
In the WSME model theprobability of a sentence is defined directly as:01( ) ( ) exp( ( ))i iiP s P s f sZ?= ?
?
(2)Where 0 ( )P s  is some baseline model,0 ( ) exp( ( ))defi is iZ P s f s?= ??
?
is a normalizationconstant and the {fi}'s are features encoding someinformation about the sentence.
Most generally, afeature is a function from the set of wordsequences to R, the set of real numbers.
However,in most applications, as in our work, the featuresare taken to be binary.
Lastly, the {?i}'s are realcoefficients encoding the relative importance oftheir corresponding features.
In the WSMEframework the set of features {fi} is given ahead oftraining by the modeler, and learning consists ofestimating the coefficients {?i}.
This is done bystipulating the constraints11( ) ( ) ( )Ndefp i p i i jjE f E f f sN == = ??
(3)where p?
is the empirical distribution defined bythe training set {s1, ... sN}.2 If these constraints areconsistent then there is a unique solution in {?i}that satisfies them.
This solution is guaranteed tobe the one closest to P0 in the Kullback-Lebliersense among all solutions satisfying (3).
It is alsoguaranteed to be the maximum likelihood solutionfor the exponential family.
For more details, see(Chen and Rosenfeld, 1999a).2.2 Self-supervised boostingA different approach to learning the same sortof model as in (2) was proposed in (Welling et al,2003).
Here, instead of having all the features pre-given, they are learned one at a time along withtheir corresponding coefficients.
Welling et alshow that adding a new feature to (2) can be2Sometimes a smoothed version of (3) is used instead (e.g.Chen and Rosenfeld, 1999b).52interpreted as gradient ascent on the log-likelihoodfunction, and show that the optimal feature is theone that best discriminates real data from datasampled from the current model.
To see this, let0( ) ln( ( )) ( )i iiE s P s f s?= +?
(4)denote the energy associated with sentence s.3Equation (2) can now be rewritten as -1( ) exp( ( ))P s E sZ=  (5)where Z is a normalization constant as before.
Thederivative of the log-likelihood with respect to anarbitrary parameter ?
is then ?1( )1 ( )( )Nii s SE sL E sP sN?
?
?= ???
?= ?
+?
?
??
?
(6)where {s1, ... sN} is once again the training corpus,and the second sum runs over the set of all wordsequences.Now, suppose we change the energy function byadding an infinitesimal multiple of a new featuref*.
The log-likelihood after adding the feature canbe approximated by ?
*( ( ) ( )) ( ( )) LL E s f s L E s?
??
?+ ?
+?
(7)where the derivative of L is taken at 0?
= .Because the optimal feature is the one thatmaximizes the increase in log-likelihood, we aresearching for a feature that maximizes thisderivative.
Using equation (6) and noting that*E f??
=?we have ?
* *11 ( ) ( ) ( )Nii s SL f s P s f sN?
= ??
= ?
+?
?
?
(8)This expression cannot be computed in practice,because the set of all word sequences S is infinite.The second term however can approximated usingsamples {ui} from the current model ?
* *1 11 1( ) ( )N Ni ii iL f s f uN N?
= =?
?
?
+?
?
?
(9)3In (Welling et al, 2003) the term for P0 does not appear,which is equivalent to taking the uniform distribution as thebaseline model.In other words, given a set of N samples {ui}from the model, the optimal feature to add is onethat gives high scores to sampled sentences andlow ones to real sentences.
By labeling realsentences with 0 and sampled sentences with 1, thetask of learning the feature translates into the taskof training a classifier to discriminate betweenthese two classes of sentences.In the remainder of the paper we will use featureand classifier interchangeably.2.3 Rejection samplingSelf-supervised boosting was presented as ageneral method for density estimation, and was nottested in the context of language modeling.
Rather,Welling at al.
demonstrated its effectiveness inmodeling hand-written digits and on synthetic data.
?n both cases essentially linear classifiers wereused as features.
As these are computationally veryefficient, the authors could use a variant of Gibbssampling for generating negative samples.Unfortunately, as shown in (Okanohara and Tsujii,2007), with the represetation of sentences that weuse, linear classifiers cannot discriminate realsentences from sentences sampled from a trigram,which is the model we use as a baseline, so herewe resort to a non-linear large-margin classifier(see section 3 for details).
While large-marginclassifiers consistently out-perform other learningalgorithms in many NLP tasks, their non-linearvariations are also notoriously slow when it comesto computing their decision function ?
taking timethat can be linear in the size of their training data.This means that MCMC techniques like Gibbssampling quickly become intractable, even forsmall corpora, as they require performing verylarge numbers of classifications.
For this reason weuse a different sampling scheme which we refer toas rejection sampling.
This allows us to samplefrom the true model distribution while requiring adrastically smaller number of classifications, aslong as the current model isn't too far removedfrom the baseline.We will start by describing the samplingprocess, and then show that the probabilitydistribution it samples from has the form ofequation (2).
To sample a sentence from the cur-rent model, we generate one from the baselinemodel, and then pass it through each of the classi-fiers in the model.
If a given classifier classifies the53sentence as a model sentence, then it is rejectedwith a certain probability associated with this clas-sifier.
Only if a sentence is accepted by all classifi-ers is it taken as a sample sentence.
Otherwise, thesampling process is restarted.Let us derive an expression for the probability ofa sentence s generated in this manner.
To simplifynotation, assume that at this point we added but asingle feature f to the baseline model P0, andletrejp  stand for the rejection probabilityassociated with it.
Furthermore, let p-stand for theaccuracy of f in classifying sentences sampledfrom P0 (negative samples).
Formally,0( )Pp E f?
=  (10)First let's assume that ( ) 1f s = .
The probabilityfor generating s is a sum of the probabilities of twodisjoint outcomes ?
the probability of generating sas the first sentence and having it survive therejection, plus the probability of generating in thefirst iteration some sentence s' such that ( ') 1f s = ,rejecting that, and then generating s in one of thesubsequent iterations.
Formally, this means that ?1 0 1( ) (1 ) ( ) ( )rej rejP s p P s p p P s?= ?
+  (11)Rearranging, we have ?1 01( ) ( )1rejrejpP s P sp p??=?
(12)Similarly, the probability for a sentence s forwhich ( ) 0f s =  is the probability of generating sas the first sentence, plus the probability ofgenerating some other sentence s' for which( ') 1f s = , rejecting it, and then generating s in afuture iteration.
Formally,1 0 1( ) ( ) ( )rejP s P s p p P s?= +  (13)and hence ?1 01( ) ( )1rejP s P sp p?=?
(14)Letting 1rejZ p p?= ?
, and lettingln(1 )rejp?
= ?
, we have, for all s ?1 01( ) ( ) exp( ( ))P s P s f sZ?= ?
?
(15)This process can be trivially generalized for Nfeatures.
Let ?1( )iiP ip E f??
=  (16)stand for fi's accuracy in classifying sentencesgenerated from Pi-1, and let irejp  be the rejectionprobability associated with the i'th feature.Sampling from the model then proceeds bysampling a sentence s from P0.
For each 1 i N?
?
,in order, if ( ) 1if s = , then we attempt to reject swith probability irejp .
If s survives all the rejectionattempts, it is returned as the next sample.
Usingsimilar arguments as before it's possible to showthat if we take ln(1 )ii rejp?
= ?
and ?1(1 )Ni irejiZ p p?== ??
(17)then the probability of a sentence s sampled by thisprocess is given by equation (2).
Conversely, thisshows that rejection sampling can be used forobtaining negative samples from the model givenin (2) by taking 1 exp( )irej ip ?= ?
, as long as0 exp( ) 1i?< ?
.
In section 3 we show that in ourexperimental setup, rejection sampling bringsabout enormous savings in the number ofclassifications necessary during training, ascompared with Gibbs sampling.2.4 Adding a new featureGiven the current model Pi and a new featurefi+1, we wish to find the optimal ?i+1, orequivalently its optimal rejection probability 1irejp+.In the WSME framework, the weights of thefeatures are set in such a way that the expectedvalue of the features on sentences sampled fromthe model equals their expected value on realsentences.
A possible way to set the weight of anew feature is therefore to set 1irejp+such that theconstraint:1 1 1( ) ( )iP i p iE f E f+ + += ?
(18)is satistfied, where p?
is once again the empiricaldistribution defined by the training set.
Intuitively,this means that the new feature could no longer beused to discriminate between sentences sampledfrom Pi+1 and real sentences.
However, setting541irejp+ in this manner may violate the constraints (18)associated with the features already existing in themodel, thus hampering the model's performance.Therefore, we set the new feature's rejectionprobability by directly searching for the one thatminimizes an estimate of Pi+1's perplexity on a setof held out real sentences.
To do this, we firstsample a new set of sentences from Pi,independently of the set that was used for training1if + , and use it to estimate 1ip +?
.
For anyarbitrarily determined 1irejp+, this enables us tocalculate an estimate for the normalization constantZ (equation 17), and therefore an estimate for  Pi+1.We do this for a range of possible values for 1irejp+and pick the one that leads to the largest reductionin perplexity on the held out data.43 Experimental workWe tested our approach on the ATIS naturallanguage corpus (Hemphill et al, 1990).
We splitthe corpus into a training set of 11,000 sentences, aheld-out set containing 1,045 sentences, and a testset containing 1,000 sentences which werereserved for measuring perplexity.
The corpus waspre-processed so that every word appearing lessthan three times was replaced by a special UNKsymbol.
The resulting lexicon contained 603 wordtypes.Our learning framework leaves open a numberof design choices:1.
Baseline language model: For P0 we used atrigram with modified kneser-ney smoothing[Chen and Goodman, 1998], which is stillconsidered one of the best smoothing methods forn-gram language models.2.
Sentence representation: Each sentence wasrepresented as the collection of unigrams, bigramsand trigrams it contained.
A coordinate wasreserved for each such n-gram which appeared inthe data, whether real or sampled.
The value of then'th coordinate in the vector representation of4Interestingly, in practice both methods result in near identicalrejection probabilities, within a precision of 0.0001.
Thisindicates that satisfying the constraint (18) for the new featureis more important, in terms of perplexity, than preserving theconstraints of the previous features, insofar as those getviolated.sentence s was set to the number of times thecorresponding n-gram appeared in s.3.
Type of classifiers: For our features we usedlarge-margin classifiers trained using the onlinealgorithm described in (Crammer et al, 2006).
Thecode for the classifier was generously provided byDaisuke Okanohara.
This code was extensivelyoptimized to take advantage of the very sparsesentence representation described above.
As shownin (Okanohara and Tsujii, 2007), using thisrepresentation, a linear classifier cannot distinguishsentences sampled from a trigram and realsentences.
Therefore, we used a 3rd orderpolynomial kernel, which was found to give goodresults.
No special effort was otherwise made inorder to optimize the parameters of the classifiers.4.
Stopping criterion: The process of addingfeatures to the model was continued until theclassification performance of the next feature waswithin 2% of chance performance.We refer to the language model obtained by thisapproach as the boosted model to distinguish itfrom the baseline model.
To estimate the boostedmodel's perplexity we needed to estimate thenormalization constant Z in equation (2).
Since thisconstant is equal to0(exp( ))P i iiE f??
it can beestimated from a large-enough sample from P0.
Weused 10,000,000 sentences generated from thebaseline trigram and took the upper bound of the95% confidence interval of the sample mean as anupper bound for Z.
This means the perplexityestimates we report are upper bounds for the realmodel perplexity with 95% confidence.5The algorithm converged after 21 features wereadded to the model.
Figure 1 presents the model'sperplexity on the test set estimated after eachiteration.
The perplexity of the final model is 9.02.In comparison, the perplexity of the modifiedkneser-ney smoothed trigram on this corpus is10.18.
This is an 11.4% improvement relative tothe baseline model.5Alternatively we could have used our estimate for PN(s) de-scribed in section 2.4.
A large sample of sentences would stillbe necessary though, to get a good estimate for equation (16).55Figure 1.
Model perplexity during training.
The x-axis denotes the number of features added to themodel.
The final perplexity after 21 features is 9.02.Figure 2.
Classifier accuracy during training,assessed on held-out data.
0.5 signifies chanceperformance.Figure 2 shows the accuracy of the trainedfeatures on held-out data.
The held-out data wascomposed of equal parts real and model sentences,so 50% accuracy is chance performance.
As mighthave been expected, the classifiers start out with arelatively high accuracy of 68%, which dwindlesdown to little over 50% as more features are addedto the model.
Not surprisingly, there is a strongcorrelation between the accuracy of a feature andthe reduction in perplexity it engenders (spearmancorrelation coefficient r=0.89, p<10-5.
)In tables 1 and 2 we show a representativesample of sentences from the baseline model andfrom the final model.
As the baseline model is atrigram, it cannot capture dependencies that span arange longer than two words.
Hence sentences thatstart out seemingly in one topic and then veer offto another are common.
The global informationavailable to the features used by the boosted modelgreatly reduces this phenomenon.
To get aquantitative sense of this, we generated 200sentences from each model and submitted them forgrammaticality testing by a proficient (though non-native) English speaker.
Of the trigram-generatedplease list costs in at pittsburghwhat type of airplane is have an early morningwhat types of aircraft is that a mealwhat not nineteen forty twobetween boston and atlanta on august fifteenthwhich airlines fly american flying onwhat is the flight leaving pittsburgh after six p mTable 1.
A sample of sentences generated by thebaseline model, a trigram smoothed with modifiedkneser-ney smoothing.what is the cost of flight d l three seventeensixty fivewhat time does flight at eight thirty eight a mand six p mwhat does fare code q w meanwhat kind of aircraft will i be flying onflights from philadelphia on saturdaywhat is the fare for flight two nine sixwhat is the cost of coach transcontinental flightu a three oh two from denver to san franciscoTable 2.
A sample of sentences generated by the finalmodelsentences, 86 were deemed grammatical (43%),while of those generated by the boosted model 132were grammatical (66%).
This difference isstatistically significant with p<10-5.Finally, let us quantify the computationalsavings obtained from using rejection sampling.Let |V| stand for the lexicon size (here |V|=603)and |L| for the average sentence length (|L|=14).
InGibbs sampling, a sentence is sampled by startingout with a random sequence of words.
For eachword position, the current word is replaced witheach word in the lexicon, and the probability of theresulting sentence is calculated.
Then one of thewords is randomly selected for this position inproportion to the calculated probabilities.
Thesentence has to be scanned in this manner severaltimes for the sample to approximate the modeldistribution.
Assuming we perform only 3 scansfor each sentence, Gibbs sampling would have thusrequired us to classify 3 | || | 25,000V L ?sentences per sampled sentence.
Given that in eachiteration we generate 12,045 sentences, and that inthe n'th iteration each sentence has to be classifiedby n features, this gives a total of roughly56107 10?
classifications after 21 iterations.
Incontrast, using rejection sampling, we used only76.7 10?
classifications in total ?
a difference ofover three orders of magnitude.4 DiscussionIn this work we presented a method that enablesusing discriminative learning methods for refininggenerative language models.
Utilizing large-margin classifiers that are trained to discriminatereal sentences from model sentences we showedthat sizeable improvements in perplexity over astate-of-the-art smoothed trigram are possible.Our method bears some similarity to the recentlydeveloped Contrastive Estimation method (Smithand Eisner, 2004).
Contrastive estimation (CE) wasproposed as a means for training log-linear prob-abilistic models.
As all training methods, contras-tive estimation pushes probability mass untopositive samples.
Unlike other methods, CE takesthis probability mass from the 'neighborhood' ofeach positive sample.
For example, given a realsentence s, CE might give it more probability bytaking away probability from similar sentenceswhich are likely to be ungrammatical, for instancesentences that are formed by taking s and switch-ing the order of two adjacent words in it.
This isintuitively similar to our approach ?
effectively,our model gives probability mass to positive sam-ples, taking it away from sentences classified asmodel sentences.
A major difference between thetwo approaches, however, is that in CE the defini-tion of the sentence's neighborhood must be speci-fied in advance by the modeler.
In our work, the'neighborhood' is determined automatically anddynamically as learning proceeds, according to thecapabilities of the classifiers used.The sentence representation we chose for thiswork is rather simple, and was intended primarilyto demonstrate the efficacy of our approach.
Infuture work we plan to experiment with richerrepresentations, e.g.
including long-range n-grams(Rosenfeld, 1996), class n-grams (Brown et al,1992), grammatical features (Amaya and Benedy,2001), etc'.The main computational bottleneck in ourapproach is the generation of negative samplesfrom the current model.
Rejection samplingallowed us to use computationally intensiveclassifiers as our features by reducing the numberof classifications that had to be performed duringthe sampling process.
However, if the boostedmodel strays too far from the baseline P0, thesesavings will be negated by the very large sentencerejection probabilities that will ensue.
This is likelyto be the case when richer representations assuggested above are used, necessitating a return toGibbs sampling.
Therefore, in future work we planto experiment with classifiers whose decisionfunction is cheaper to compute, such as neuralnetworks and decision trees.
Another possibledirection would be using the recently proposedDeep Belief Network formalism (Hinton et al,2006).
DBNs utilize semi-linear features which arestacked recursively and thus very efficiently modelnon-linearities in their data.
These have been usedin the past for language modeling (Mnih andHinton, 2007), but not within the whole-sentenceframework.AcknowledgementsWe would like to thank Daisuke Okanohara forhis generosity in providing the code for the large-margin classifier.
The author is supported by theYeshaya Horowitz association through the centerof Complexity Science.ReferencesFredy Amaya and Jose Miguel Benedi, 2001, Improve-ment of a whole sentence maximum entropy model us-ing grammatical features.
In Proceedings of the 39thAnnual Meeting of the Association of ComputationalLinguistics.Peter.
F. Brown, Vincent.
J. Della Pietra, Peter.
V.deSouza, Jenifer.
C. Lai and Robert.
L. Mercer.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479, Dec. 1992.Stanley F. Chen and Joshua.
Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical Report TR?10?98, Center for Re-search in Computing Technology, Harvard UniversityStanley F. Chen and Ronald Rosenfeld.
1999a.
Efficientsampling and feature selection in whole sentence maxi-mum entropy language models.
In Proceedings ofICASSP?99.
IEEE.Stanley F. Chen and Ronald Rosenfeld.
1999b.
A Gaus-sian prior for smoothing maximum entropy models.Technical Report CMUCS-99-108, Carnegie MellonUniversity.57Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Online pas-sive-aggressive algorithms.
Journal of Machine Learn-ing Research.Jianfeng Gao, Hao Yu, Wei Yuan, and Peng Xu.
2005.Minimum sample risk methods for language modeling.In Proc.
of HLT/EMNLP.Charles T. Hemphill, John J. Godfrey and George R.Doddington.
1990.
The ATIS Spoken Language Sys-tems Pilot Corpus.
The workshop on speech and naturallanguage, Morgan Kaufmann.Geoffrey E. Hinton, Simon Osindero and Yee-WhyeTeh, 2006.
A fast learning algorithm for deep beliefnets, Neural Computation, 18(7):1527?1554.Andriy Mnih and Geoffrey E. Hinton.
2007.
Three newgraphical models for statistical language modeling.
InProceedings of the 24th international conference onMachine Learning.Daisuke Okanohara and Jun'ichi Tsujii.
2007.
A dis-criminative language model with pseudo-negative sam-ples.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics.Brian Roark, Murat Saraclar, and Michael Collins.2007.
Discriminative n-gram language modeling.
Com-puter Speech and Language, 21(2):373?392.Ronald Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
Computerspeech and language, 10:187?228Ronald Rosenfeld.
1997.
A whole sentence maximumentropy language model.
In Proc.
of the IEEE Workshopon Automatic Speech Recognition and Understanding.Noah A. Smith and Jason Eisner.
2005.
Contrastive es-timation: training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting of the Asso-ciation of Computational Linguistics.Max Welling., Richard Zemel and Geoffrey E. Hinton.2003.
Self-Supervised Boosting.
Advances in NeuralInformation Processing Systems, 15, MIT Press, Cam-bridge, MA58
