Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1612?1623, Dublin, Ireland, August 23-29 2014.Minimally Supervised Classification to Semantic Categories usingAutomatically Acquired Symmetric PatternsRoy Schwartz1Roi Reichart21Institute of Computer Science, The Hebrew University{roys02|arir}@cs.huji.ac.il2Technion IITroiri@ie.technion.ac.ilAri Rappoport1AbstractClassifying nouns into semantic categories (e.g., animals, food) is an important line of researchin both cognitive science and natural language processing.
We present a minimally supervisedmodel for noun classification, which uses symmetric patterns (e.g., ?X and Y?)
and an iterativevariant of the k-Nearest Neighbors algorithm.
Unlike most previous works, we do not use apredefined set of symmetric patterns, but extract them automatically from plain text, in an unsu-pervised manner.
We experiment with four semantic categories and show that symmetric patternsconstitute much better classification features compared to leading word embedding methods.
Wefurther demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-of-the-art label propagation alternatives for this task.
In experiments, our model obtains 82%-94%accuracy using as few as four labeled examples per category, emphasizing the effectiveness ofsimple search and representation techniques for this task.1 IntroductionThe role of language is to express meaning.
In the field of NLP, there has been an increasingly grow-ing number of approaches that deal with semantics.
Among these are vector space models (Turney andPantel, 2010; Baroni and Lenci, 2010), lexical acquisition (Hearst, 1992; Dorow et al., 2005; Davidovand Rappoport, 2006), universal cognitive conceptual annotation (Abend and Rappoport, 2013) and au-tomatic induction of feature representations (Collobert et al., 2011).
In this paper, we utilize extremelyweak supervision to classify words into fundamental cognitive semantic categories.There are several types of semantic categories expressed by languages, e.g., objects, actions, andproperties.
We follow human development, acquiring coarse-grained categories and distinctions beforedetailed ones (Mandler, 2004).
Specifically, we focus on the major class of concrete ?things?
(Langacker,2008, Ch.
4), roughly corresponding to nouns ?
the main participants in linguistic clauses ?
that areuniversally present in the semantics of virtually all languages (Dixon, 2005).Most works on noun classification to semantic categories require large amounts of human annotationto build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) orrely on language-specific resources such as WordNet (Evans and Or?asan, 2000; Or?asan and Evans, 2007).Such heavy supervision is labor intensive and makes these models domain and language dependent.Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can com-pensate for the lack of input from the senses in text corpora.
Our model therefore performs semanticcategory classification using only a small number of labeled seed words per category.
The experimentswe conduct show that such weak supervision is sufficient to construct a high quality classifier.A key component of our model is the application of symmetric patterns.
We define patterns to besequences of words and wildcards (e.g., ?X is a dog?, ?both X and Y?, etc.).
Accordingly, symmet-ric patterns are patterns that contain exactly two wildcards, where both wildcards are interchangeable.Examples of symmetric patterns include ?X and Y?, ?X as well as Y?
and ?neither X nor Y?.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/.1612Works that apply symmetric patterns in their model generally require expert knowledge in the form of apre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008).
In this work, we extractsymmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm.
Thisalgorithm automatically extracts a set of symmetric patterns from plain text using simple statistics abouthigh and low frequency word co-occurrences.
The unsupervised nature of our approach makes it domainand language independent.Our model addresses semantic classification in a transductive setup.
It takes advantage of word sim-ilarity scores that are computed based on symmetric pattern features, and propagates information fromconcepts with known classes to the rest of the concepts.
For this aim we apply an iterative variant of thek-Nearest Neighbors algorithm (denoted with I-k-NN) to a graph in which vertices correspond to nounsand word pairs are connected with edges based on their participation in symmetric patterns.We experiment with a subset of 450 nouns from the CSLB dataset (Devereux et al., 2013), which wereannotated with semantic categories by thirty human subjects.
From the set of semantic categories in thisdataset, we select categories that are both frequent and have a high inter-annotator agreement (Section 2).This results in a set of four semantic categories ?
animacy, edibility, is a tool and is worn.Our experiments show that our model performs very well even when only a small number of labeledseed words are available.
For example, on the task of binary classification with respect to a singlecategory, when using as few as four labeled seed words, classification accuracy reaches 82%-94%.Furthermore, our model outperforms several strong baselines for this task.
First, we compare ourmodel against a model that uses a deep neural network word embedding baseline (Collobert et al., 2011)instead of our symmetric pattern based features, and applies the exact same I-k-NN algorithm.
In recentyears, deep networks word embeddings obtained state-of-the-art results in several NLP tasks (Collobertand Weston, 2008; Socher et al., 2013).
However, in our task, features based on simple, intuitive andeasy to compute symmetric patterns, lead to substantially better performance (average improvement of0.15 F1 points).
Second, our model outperforms two baseline models that utilize the same symmetricpattern classification features as in our model, but replace our simple I-k-NN algorithm with two leadinglabel propagation alternatives (the normalized graph cut (N-Cut) algorithm (Yu and Shi, 2003) and theModified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)).
The average improvement overthese two baselines is 0.21 and 0.03 F1 points .The rest of the paper is organized as follows.
Section 2 describes our semantic classification taskand, particularly, the semantic classes that we aim to learn.
Section 3 presents our method for automaticsymmetric patterns acquisition.
Sections 4, 5 and 6 describe our model, experimental setup and results,respectively.
Related work is finally surveyed in Section 7.2 Task DefinitionThe task we tackle in this paper is the classification of nouns into semantic categories.
This sectiondefines the categories we address and the dataset we use.Semantic Categorization of Concrete Nouns.
We focus on concrete ?things?
(Langacker, 2008),which correspond to noun categories.
Nouns are interesting because they are the most basic lexicalsemantic categories.
Specifically, children acquire nouns before any other category (Clark, 2009).
More-over, noun categories are generally not subjective.
For example, it is hard to argue that a dog is notan animal, or that an apple is inedible, in most reasonable contexts.
The context independent nature ofnouns makes them appropriate for a type level classification task, such as the one we tackle.
In order toprovide a better description of the categories we aim to predict, we now turn to discuss the CSLB dataset,with which we experiment.Dataset.
We experiment with the CSLB property norms dataset (Devereux et al., 2013).
In order toprepare this data set, thirty human subjects were presented with 638 concrete nouns and were asked towrite the categories associated with each concept.
Table 1 presents the top five categories for the nounsapple and horse.1613Noun CategoriesApple is a fruit, does grow on trees, is green, is red, has pips seedsHorse is ridden, is an animal, has four legs, has legs, has hoovesTable 1: Five most frequent semantic categories for the words apple and horse in the CSLB dataset.Category Selection.
The CSLB dataset consists of a total of 2725 semantic categories.
We applya selection mechanism that provides us with a dataset in which (1) only noun categories (things) areincluded; and (2) only semantic categories that are prominent across humans are considered.
For this,we apply the following filtering stages.
First, since the vast majority of annotated categories are rare (forexample, 1691 categories are assigned to a single noun only), we set a minimum threshold of 35 nounsper category (5% of the nouns).
After removing highly infrequent categories, 28 are left.
We then applyan inter-annotator agreement criterion: for each semantic category c, we compute the average numberof human annotators that associated this category with a given noun, across the nouns annotated with c.We select the category c only if the value of this statistic is higher than 10 subjects (1/3 of the subjects),which results in a semantic category set of size 18.
Finally, we discard categories, such as color and size,that do not correspond to things.
We are left with four noun semantic categories: animacy (animals),edibility (food items), is a tool (tools), and is worn (clothes).Interestingly, the resulting semantic categories can also be justified from a cognitive perspective.
Thereis a large body of work indicating that our categories relate to brain organization principles.
For example,Just et al.
(2010) showed that food products and tools arouse different brain activation patterns.
More-over, a number of works showed that both animate objects and tools are represented in specific brain re-gions.
These works used neuroimaging methods such as functional magnetic resonance imaging (fMRI)(Naselaris et al., 2012), electroencephalography (EEG) (Chan et al., 2011) and magnetoencephalogra-phy (MEG) (Sudre et al., 2012).
See (Martin, 2007) for a detailed survey.
This parallel evidence to theprominence of our categories provides substance for intriguing future research.3 Symmetric PatternsPatterns.
In this work, patterns are combinations of words and wildcards, which provide a structuralphrase representation.
Examples of patterns include ?X and Y?, ?X such as Y?, ?X is a country?, etc.Patterns can be used to extract various relations between words.
For example, patterns such as ?X of aY?
(?basement of a building?)
can be useful for detecting the meronymy (part-of) relation (Berland andCharniak, 1999).
Symmetric patterns (e.g., ?X and Y?, ?France and Holland?
), which we use in thispaper, can be used to detect semantic similarity between words (Widdows and Dorow, 2002).Symmetric Patterns.
Symmetric patterns are patterns that contain exactly two wildcards, and wherethese wildcards are interchangeable.
Examples of symmetric patterns include ?X and Y?, ?X or Y?
and?X as well as Y?.
Previous works have shown that word pairs that participate in symmetric patterns barestrong semantic resemblance, and consequently, that these patterns can be used to cluster words intosemantic categories, where a high precision, but low coverage (recall) solution is good enough (Dorowet al., 2005; Davidov and Rappoport, 2006).
A key observation of this paper is that symmetric patternscan be also used for semantic classification, where recall is as important as precision.Flexible Patterns.
It has been shown in previous work (Davidov and Rappoport, 2006; Turney, 2008;Tsur et al., 2010; Schwartz et al., 2013) that patterns can be extracted from plain text in a fully unsu-pervised manner.
The key idea that makes this procedure possible is the concept of ?flexible patterns?,which are composed of high frequency words (HFW) and content words (CW).
Every word in the lan-guage is defined as either HFW or CW, based on the number of times this word appears in a large corpus.This clustering procedure is applied by traversing a large corpus, and marking words that appear withcorpus frequency higher than a predefined threshold t1as HFWs, and words with corpus frequency lowerthan t2as CWs.11We follow (Davidov and Rappoport, 2006) and set t1= 10?5, t2= 10?3.
Note that some words are marked both as HFWand as CW.
See (Davidov and Rappoport, 2008) for discussion.1614The resulting clusters have a desired property: HFWs are comprised mostly of function words (prepo-sitions, determiners, etc.)
while CWs are comprised mostly of content words (nouns, verbs, adjectivesand adverbs).
This coarse grained clustering is useful for pattern extraction from plain text, since lan-guage patterns tend to use fixed function words, while content words change from one instance of thepattern to another (Davidov and Rappoport, 2006).Flexible patterns are extracted by traversing a large corpus and, based on the clustering of words toCWs and HFWs, extracting all pattern instances.
An extracted pattern instance consists of CW wildcardsand the actual words replacing the HFWs in the pattern type.
Consider the sentence ?The boy is happyand joyful?.
Replacing the content words with the CW wildcard results in ?The CW is CW and CW?.From this intermediate representation, we extract word sequences of a given length constraint and denotethem as flexible patterns.2The flexible patterns of length 5 extracted from this sentence are ?The CW isCW and?
and ?CW is CW and CW?.
The reader is referred to (Davidov and Rappoport, 2006) for moredetails.Automatically Extracted Symmetric Patterns.
Most models that incorporate symmetric patterns usea predefined set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008).
In this work, we applyan automatic, completely unsupervised procedure for symmetric pattern extraction.
This procedure,described in Algorithm 1, is adopted from (Davidov and Rappoport, 2006).The procedure first extracts flexible patterns that contain exactly two CW wildcards.
It then selectsthose flexible patterns in which both CWs are interchangeable.
That is, it selects a pattern p if everyword pair CW1, CW2that participates in p indicates with high probability that the word pair C2, C1also participates in p. For example, for the symmetric pattern ?CW and CW?, both ?cats and dogs?and ?dogs and cats?
are semantically plausible expressions, and are therefore likely to appear in a largecorpus.
On the other hand, the flexible pattern ?CW such as CW?
is asymmetric, as exemplified inexpressions like ?countries such as France?, where replacing the CWs does not result in a semanticallyplausible expression (# ?France such as countries?).
The selection process is done by computing theproportion of CW1, CW2pairs that participate in p for which CW2, CW1also participates in p. Patternsfor which this proportion exceed a certain threshold are selected.We apply Algorithm 1 on the google books 5-gram corpus (Michel et al., 2011)3and extract 20 sym-metric patterns.
Some of the more interesting symmetric patterns extracted using this algorithm include?CW and the CW?, ?from CW to CW?, ?CW rather than CW?
and ?CW versus CW?.
In the next sectionwe present our approach to semantic classification, which makes use of automatically acquired symmet-ric patterns for word similarity computations.4 ModelIn this section we present our model for binary word classification according to a single semantic categoryin a minimally-supervised, transductive setup.
Given a set of words, we label a small number of wordswith their correct label according to the category at hand (+1 for words that belong to the category, -1for words that do not belong to it).
Our model is based on an undirected weighted graph, in whichvertices correspond to words, and edges correspond to relations between words.
Our goal is to classifythe unlabeled words (vertices) in the graph through a label propagation process.
We now turn to describeour model in detail.Graph Construction.
We construct our graph such that an edge is added between two words (vertices)if both words participate in a symmetric pattern.
The edge generation process is performed as follows.We first apply our symmetric pattern extraction procedure (Algorithm 1), and denote the set of selectedsymmetric patterns with P .
We then traverse a large corpus4and extract all word pairs that participatein any pattern p ?
P .
We denote the number of occurrences of a word pair (w1, w2) in such patternswith fw1,w2.
Finally, we select all word pairs (w1, w2) for which min(fw1,w2, fw2,w1) > ?.
Each such2We set the maximal flexible pattern length to be 5.3https://books.google.com/ngrams4We use google books 5-grams (Michel et al., 2011).1615Algorithm 1 Symmetric pattern extraction1: procedure EXTRACT SYMMETRIC PATTERNS(C,W )2: .
C is a large corpus, W is a lexicon3: .
Traverse C and extract all flexible patterns of length 3-5 that appear in C and contain exactly two content words4: P ?
extract flexible patterns(C,W )5: for p ?
P do6: if p appears in <10?6of the sentences in C then7: Discard p and continue8: end if9: Gp?
a directed graph s.t.
V (Gp)?W ,E(Gp)?
{(w1, w2)?W2:w1,w2participate in at least one instance of p}10: .
An undirected graph based on the bidirectional edges of the Gp11: symGp?
an undirected graph: {(w1), (w1,w2) : (w1,w2) ?
E(Gp) ?
(w2, w1) ?
E(Gp)}12: .
Two measures of symmetry13: M1?|V (symGp)||V (Gp)|,M2?|E(symGp)||E(Gp)|14: .
Symmetric pattern candidates are those with high M1and M2values15: if min (M1,M2) < 0.05 then16: Discard p17: end if18: end for19: for p ?
P do20: .
E.g., ?CW and CW?
is contained in ?both CW and CW?21: if ?p??
P s.t.
p?is contained in p then22: Discard p23: end if24: end for25: return The top 20 members of P with the highest M1value26: end procedurepair is connected with an edge ew1,w2in the graph, where the edge weight (denoted with ww1,w2) is thegeometric mean between fw1,w2and fw2,w1.Label Propagation.
Given a small number of annotated words (vertices), our goal is to propagate theinformation these words convey to other words in the graph.
To do so, we apply an iterative variant of thek-Nearest Neighbors algorithm (I-k-NN).
This iterative variant is required due to graph sparsity; whenstarting with a small set of labeled vertices, most unlabeled vertices do not have any labeled neighbor, andthus running the standard k-NN algorithm would result in classifying a very small number of vertices.Our approach is to run iterations of the k-NN algorithm, and thus propagate information to additionalvertices at each iteration.
At each k-NN step, the algorithm selects words that have at least one labeledneighbor.
From this set, only the words that have the highest ratio of neighbors with the same label areselected, and are assigned with this label.Consider a simple example.
Say we have three candidate vertices a, b and c, where a has one neighborwith label +1 (ratio(a) = 1/1 = 1.0), b has two neighbors with label -1 (ratio(b) = 2/2 = 1.0) andc has three neighbors with label +1 and one neighbor with label -1 (ratio(c) = max(3, 1)/4 = 3/4).Then, a and b are selected and are assigned with +1 and ?1, respectively.Seed Expansion.
In minimally supervised setups like ours, the model is initialized with a small set oflabeled seed examples.
A natural approach in such settings is to apply a seed expansion step, in order toobtain a larger set of labeled seeds.
Our method uses the same graph construction procedure describedabove, but uses a larger edge generation threshold ?
>> ?.5We then apply an iterative procedure thatlabels a vertex v with a label l if either (a) v is directly connected to ?
of the vertices labeled with l or (b)v is connected to ?lof the neighbors of vertices labeled with l.6This procedure is run iteratively until nomore vertices meet any of the criteria (a) or (b).5Using a larger threshold results in a sparser graph.
Nevertheless, each edge in this graph is more likely to represent a realsemantic relation.6?
and ?lare hyperparameters tuned on our development set (see Section 5.2).16165 Experimental Setup5.1 BaselinesWe compare our model to two types of baselines.
The first (Classification Features Baselines) utilizesthe I-k-NN algorithm, along with a different set of classification features.
The second (Label Propa-gation Baselines) utilizes the same classification features as we do, but replaces I-k-NN with a moresophisticated label propagation algorithm.5.1.1 Classification Features BaselinesIn this set of baselines, we use different methods for building our graph.
Concretely, instead of addingedges for pairs of words that appear in the same symmetric pattern, we use word similarity measuresbased on different feature sets as described below.
The process of building the graph using the baselineword similarity measures is described in Section 5.2.SENNA.
Deep neural networks have gained recognition as leading feature extraction methods for wordrepresentation (Collobert and Weston, 2008; Socher et al., 2013).
We use SENNA,7a deep network basedword embedding method, which has been used to produce state-of-the-art results in several NLP tasks,including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011).
We use the cosinesimilarity between two word embeddings as a word similarity measure.Brown.
This baseline is derived from the clustering induced by the Brown algorithm (Brown et al.,1992).8This clustering, in which words share a cluster if they tend to appear in the same lexical con-text, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al.,2004) and dependency parsing (Koo et al., 2008).
We use it in order to control for the possibility that asimple contextual preference similarity correlates with similarity in semantic categorization better thansymmetric pattern features.The Brown algorithm builds a binary tree, where words are located at leaf nodes.
We use the graphdistance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word simi-larity measure for building our graph.5.1.2 Label Propagation BaselinesIn this type of baselines, we replace I-k-NN with a different label propagation algorithm, while still usingthe symmetric pattern features for word similarity computations.N-Cut.
This baseline applies the normalized graph cut algorithm (Yu and Shi, 2003)9for label propa-gation.
Given a graphG = (V,E) and two sets of verticesA,B ?
V , this algorithm defines links(A,B)to be the sum of edge weights between A and B.
The objective of the algorithm is to find the clustersA, V \ A that minimizelinks(A,V \A)links(A,V ).
The algorithm of (Yu and Shi, 2003) is particularly efficient forthis problem as it avoids eigenvector computations which may become computationally prohibitive forlarge graphs (for more details, see their paper).
In order to encode information about our labeled seedwords, we hard-code a large negative value (-100000) to the weights of edges between seed words withdifferent labels (positive and negative).MAD.
The Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)10is an extensionof the Adsorption algorithm (Baluja et al., 2008).
MAD is a stochastic graph-based label propagationalgorithm which has shown to have a number of attractive theoretical properties and demonstrated goodexperimental results.7The word embeddings were downloaded from http://ml.nec-labs.com/senna/8We use the clusters induced by (Koo et al., 2008), who applied the Brown algorithm implementation of (Liang,2005) to the BLLIP corpus (Charniak et al., 2000).
http://www.people.csail.mit.edu/maestro/papers/bllip-clusters.gz9http://www.cis.upenn.edu/?jshi/software/Ncut_9.zip10http://github.com/parthatalukdar/junto16175.2 ExperimentsGraph Construction.
We experiment with the CSLB dataset (Devereux et al., 2013), consisting of 638nouns, annotated with their semantic categories by thirty human subjects.
We first omit all nouns thatare annotated as having more than one sense, and use the remaining 603 nouns to build our graph.
Fromthese nouns, 146 nouns are annotated as animate, 115 as edible, 50 as wearable and 35 as tools.11Wethen discard nouns that have less than two neighbors, which results in a final set of 450 nouns (vertices).The graphs used in the classification features baselines are different than those used by the models thatuse our symmetric pattern classification features, since the features define the graph structure (Section 4).In order to provide a meaningful comparison, we build graphs with the same number of vertices for eachof these baselines.
We do so by selecting the n edges with the highest weight, together with the set ofvertices connected by these edges, such that the resulting graph has 450 vertices.
Working with thesesets of vertices is the optimal setting for these baselines, as the resulting graphs are the ones with thehighest possible edge weights for graphs with 450 vertices.12Parameter Tuning.
In order to avoid adding additional labeled examples for the sake of parametertuning, we set the hyperparameter values to the ones for which each model performs best on an auxiliarysemantic classification task.
Concretely, we experiment with a fifth semantic category (audibility),13which is not part of our evaluation setting, for parameter tuning.
Note that this results in our modelhaving the same hyperparamter values for all four classification tasks.In order to ensure that the models assign all participating words with labels, we set ?=3, where ?
isthe minimal number of times a word pair should appear in the same symmetric pattern in order to havean edge in our graph (See Section 4).
In our seed expansion procedure, where we search for seeds whoselabel is predicted with high confidence, only word pairs that appear at least ?=50 times in the samesymmetric pattern are assigned an edge in the graph.
We set the seed expansion procedure parameters tobe ?
= 0.6, ?+1= 0.5, ?
?1= 0.2.Evaluation.
For each classification task, we run experiments with 4, 10, 20 and 40 labeled seed words.In each setting, half of the labeled seed words are assigned a positive label and the other half are assigneda negative label.
For each semantic category and labeled seed set size, we repeat our experiment 1000times, each of which with a different set of randomly selected labeled seed examples, and report theaverage results.
We report both accuracy (number of correct labels divided by number of vertices inthe graph) and F1 score, which is the harmonic mean of p (the average precision across labels) and r(average recall across labels).These two measures represent complementary aspects of our results.
On the one hand, accuracy isthe most natural classification performance measure.
On the other hand, the number of positive labels issubstantially smaller than the number of negative labels,14and thus this measure can be manipulated: adummy model that always assigns the negative label gets a high accuracy.
The F1 score controls againstsuch models by assigning them low scores.6 ResultsOur experiments are designed to explore two main questions: (a) the value of symmetric patterns assemantic classification features, compared to state-of-the-art word clustering and embedding methods;and (b) the required complexity of an algorithm that can propagate information about semantic simi-larity.
Particularly, we test the value of our simple I-k-NN algorithm compared to more sophisticatedalternatives.A Minimally Supervised Setting.
Our first set of experiments is in a minimally supervised settingwhere only two positive and two negative examples are available for each binary classification task.
This11Some words are classified as belonging to more than one category (e.g., ?chicken?
is both animate and edible).12The resulting graphs are actually denser than the symmetric patterns-based graph: 14K and 9K edges for the Brown andSENNA graphs, respectively, compared to < 5K edges in the symmetric patterns graph.13We used four labeled seed words in these experiments.14Only 6-25% of the nouns have a positive label.1618Animacy Edibility is worn is a toolSP SENNA Brown SP SENNA Brown SP SENNA Brown SP SENNA BrownAcc.MAD 80.4% 77.7% 12.0% 75.0% 56.5% 14.8% 82.7% 66.8% 14.7% 73.3% 67.7% 12.2%N-Cut 71.4% 60.4% 51.2% 75.5% 59.4% 50.9% 83.3% 71.5% 51.4% 82.7% 77.1% 52.0%I-k-NN 85.1% 76.0% 55.5% 82.2% 56.8% 68.0% 94.1% 70.9% 66.7% 82.0% 75.7% 65.0%F1MAD 0.77 0.76 0.18 0.69 0.55 0.24 0.71 0.56 0.22 0.58 0.47 0.17N-Cut 0.49 0.45 0.46 0.51 0.44 0.45 0.61 0.56 0.41 0.56 0.50 0.38I-k-NN 0.78 0.70 0.48 0.71 0.53 0.62 0.86 0.59 0.55 0.64 0.52 0.51Table 2: Accuracy and F1 score comparison between our model and the baselines.
The columns cor-respond to the type of classification features used by the model: SP ?
symmetric patterns, SENNA ?word embeddings extracted using deep networks (Collobert et al., 2011), Brown ?
Brown word clus-tering (Brown et al., 1992).
The rows correspond to the algorithms applied by the model: N-Cut ?
thenormalized graph cut algorithm (Yu and Shi, 2003), MAD ?
the modified adsorption algorithm (Talukdarand Crammer, 2009), I-k-NN ?
our iterative k-NN algorithm.
Our model (I-k-NN + SP) is superior in allcases, except for the accuracy of the ?is a tool?
semantic category, where it is second only to N-Cut+SP.5 10 15 200.550.60.650.70.750.80.85#training_samplesF1 ScoreSENNA (MAD)Brown (I?k?NN)Symmetric Patterns (I?k?NN)(a) Classification Features Comparison5 10 15 200.450.50.550.60.650.70.750.80.85#training_samplesF1 ScoreN?Cut (Symmetric Patterns)MAD (Symmetric Patterns)I?k?NN (Symmetric Patterns)(b) Algorithm Comparison5 10 15 200.50.550.60.650.70.750.80.85#training_samplesF1 ScoreI?k?NN, SENNAMAD, SENNAMAD, Symmetric PatternsI?k?NN, Symmetric Patterns(c) Top four Best ModelsFigure 1: (a) Comparison of the different classification features.
The figure shows the F1 scores of thebest model that uses each of the feature sets (the label propagation algorithm used in each model appearsin parentheses).
(b) Comparison of the different label propagation algorithms.
The figure shows the F1scores of the best model that uses each of the algorithms (the classification feature sets used in each modelappears in parentheses.
It is always symmetric patterns).
(c) The four best overall models (algorithm +classification feature set).
The figures show that the symmetric pattern feature set is superior to the otherfeature sets, and that I-k-NN is superior or comparable to the other label propagation algorithms.setup enables us to explore the performance of our model when the amounts of labeled training data istaken to the possible minimum.Table 2 presents our results.
With respect to objective (a), the table clearly demonstrates that symmetricpatterns lead to much better results compared to the alternatives.
Particularly, for all four semanticcategories, and across both evaluation measures, it is a model that utilizes symmetric pattern classificationfeatures that achieves the best results.
The average difference between the best model that uses symmetricpatterns and the best model that does not is 12.5% accuracy and 0.13 F1 points.
The dominance ofsymmetric pattern classification features is further demonstrated by the fact that a model that uses thesefeatures always performs better than a model that uses the same algorithm but different features.With respect to objective (b) the table shows that I-k-NN provides a large improvement in seven outof eight (category ?
evaluation measure) settings.
The average difference between the best model thatutilizes I-k-NN and the best model that applies a different algorithm is 5.4% accuracy and 0.06 F1 points.Analysis of Labeled Seed Set Size.
In order to get a wider perspective on our model, we repeated ourexperiments with various sizes of the labeled seed set: 5,10 and 20 positive and negative labeled examplesper semantic category.
For brevity, only the F1 score results of the edibility category are presented.
Thetrends observed on the other semantic categories (as well as when using the accuracy measure) are verysimilar.Figure 1a compares the different classification features.
For each feature f , results of the best per-forming model that uses f are shown.
The results reveal that symmetric patterns clearly outperform theother features.
The average differences between the best symmetric patterns-based model and the best1619models that use the other features are 0.15 (SENNA) and 0.16 (Brown) F1 points.Figure 1b compares the different label propagation algorithms.
For each algorithm a, results for thebest performing model that uses a are presented.
The results reveal that the I-k-NN algorithm outper-forms both algorithms by 0.03 (MAD) and 0.21 (N-Cut) F1 points.
The results also show that for allalgorithms, the best performing model uses symmetric patterns classification features, which furtherdemonstrates the dominance of these features.Finally, Figure 1c presents the four top performing models (algorithm + classification feature).
Inaccordance with the other findings presented in this section, the top two models, which outperform theother models by a large margin, apply symmetric pattern classification features.Seed Expansion Effect.
Our model uses a seed expansion procedure in order to expand a small set oflabeled seed words to a larger set (see Section 4).
In order to assess the quality of this procedure wecompute, for each semantic category, the average size of the expanded set and the accuracy of the newseeds (i.e., the proportion of new seeds that are labeled correctly).
Results show that the initial set isincreased from four seeds (two positive + two negative) to 48-52, and that the accuracy of the new seedsis as high as 88-99%.
Our experiments also show that this procedure provides a substantial performanceboost to our I-k-NN algorithm, which obtains a 7.2% accuracy and 0.05 F1 points improvement (averagedover the four semantic categories) when applied with the expanded set of labeled seed words comparedto the original set of size four.7 Related WorkClassification into Semantic Categories.
Several works tackled the task of semantic classification,mostly focusing on animacy, concreteness and countability.
The vast majority of these works are eithersupervised (Hatzivassiloglou and McKeown, 1997; Baldwin and Bond, 2003; Peng and Araki, 2005;?vrelid, 2005; Nagata et al., 2006; Xing et al., 2010; Kwong, 2011; Bowman and Chopra, 2012) ormake use of external, language-specific resources such as WordNet (Or?asan and Evans, 2001; Or?asanand Evans, 2007; Moore et al., 2013).
Our work, in contrast, is minimally supervised, requiring only asmall set of labeled seed words.Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrencesin instances of hand-crafted patterns such as ?X who Y?
and ?X and his Y?.
While their model usespatterns that are tailored to the animacy and gender categories, our model uses automatically inducedpatterns and is thus applicable to a range of semantic categories.Finally, Turney et al.
(2011) built a label propagation model that utilizes LSA (Landauer and Dumais,1997) based classification features.
They used their model to classify nouns into the concrete/abstractcategory using 40 labeled seed words .
Unlike our model, which requires only a small set of labeled seeds,their algorithm is actually heavily supervised, requiring thousands of labeled examples for selecting theseed set of labeled words that are used for propagation.
Our model, on the other hand, does not requireany seed selection procedure, and utilizes a randomly selected set of labeled seed words.Lexical Acquisition.
Another line of work focused on the acquisition of semantic categories.
In thissetup, a model aims to find a core seed of words belonging to a given category, sacrificing recall forprecision.
Our model tackles a different task, namely the classification of words according to a givencategory where both recall and precision are to be optimized.Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use ofsymmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, languagespecific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva etal., 2008; Wang and Cohen, 2009).
Other models require syntactic annotation derived from a supervisedparser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005).
Our modelautomatically induces symmetric patterns, obtaining high quality results without relying on any type oflanguage specific knowledge or annotation.
Moreover, some of the works mentioned above (Riloff andShepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label1620seeds to achieve good performance; in contrast, our work performs very well with a randomly selectedset of labeled seed words.8 ConclusionWe presented a minimally supervised model for noun classification into coarse grained semantic cate-gories.
Our model obtains 82%-94% accuracy on four semantic categories even when using only fourlabeled seed words per category.
We showed that our modeling decisions ?
using symmetric patterns asclassification features and a simple iterative k-NN algorithm for label propagation ?
lead to a substantialperformance gain compared to state-of-the-art, more sophisticated, alternatives.
Our results demonstratethe applicability of minimally supervised methods for semantic classification tasks.
Future work willinclude modifying our model to support other, more fine-grained types of semantic categories, includ-ing adjectival categories (properties).
We also plan to work on token-level word classification, and thussupport multi-sense words, as well as demonstrate the power of unsupervised patterns acquisition formultilingual setups.AcknowledgmentsThis research was funded (in part) by the Harry and Sylvia Hoffman leadership and responsibility pro-gram (for the first author), the Google Faculty research award (for the second author), the Intel Collab-orative Research Institute for Computational Intelligence (ICRI-CI) and the Israel Ministry of Scienceand Technology Center of Knowledge in Machine Learning and Artificial Intelligence (Grant number3-9243).ReferencesO.
Abend and A. Rappoport.
2013.
Universal conceptual cognitive annotation (UCCA).
In Proc.
of ACL.T.
Baldwin and F. Bond.
2003.
A plethora of methods for learning English countability.
In Proc.
of EMNLP.S.
Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly.
2008.
Videosuggestion and discovery for youtube: taking random walks through the view graph.
In Proc.
of WWW, pages895?904.
ACM.M.
Baroni and A. Lenci.
2010.
Distributional memory: A general framework for corpus-based semantics.
Com-putational Linguistics, 36(4):673?721.M.
Berland and E. Charniak.
1999.
Finding parts in very large corpora.
In Proc.
of ACL.S.
R. Bowman and H. Chopra.
2012.
Automatic Animacy Classification.
In Proc.
of NAACL-HLT StudentResearch Workshop.P.
F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai.
1992.
Class-based n-gram models ofnatural language.
Computational linguistics, 18(4):467?479.A.
M. Chan, J. M. Baker, E. Eskandar, D. Schomer, I. Ulbert, K. Marinkovic, S. S. Cash, and E. Halgren.
2011.First-pass selectivity for semantic categories in human anteroventral temporal lobe.
The Journal of Neuro-science, 31(49):18119?18129.E.
Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, and M. Johnson.
2000.
BLLIP 198789 WSJ Corpus Release 1,LDC No.
LDC2000T43.
Linguistic Data Consortium.A.
Clark.
2000.
Inducing syntactic categories by context distribution clustering.
In Proc.
of CoNLL.E.
V. Clark.
2009.
First language acquisition.
Cambridge University Press.R.
Collobert and J. Weston.
2008.
A unified architecture for natural language processing: Deep neural networkswith multitask learning.
In Proc.
of ICML.R.
Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa.
2011.
Natural language processing(almost) from scratch.
JMLR, 12:2493?2537.1621D.
Davidov and A. Rappoport.
2006.
Efficient unsupervised discovery of word categories using symmetric pat-terns and high frequency words.
In Proc.
of ACL-Coling.D.
Davidov and A. Rappoport.
2008.
Unsupervised discovery of generic relationships using pattern clusters andits evaluation by automatically generated SAT analogy questions.
In Proc.
of ACL-HLT.B.
J. Devereux, L. K. Tyler, J. Geertzen, and B. Randall.
2013.
The centre for speech, language and the brain(CSLB) concept property norms.
Behavior research methods, pages 1?9.R.
M. Dixon.
2005.
A semantic approach to English grammar.
Oxford University Press.B.
Dorow, D. Widdows, K. Ling, J. P. Eckmann, D. Sergi, and E. Moses.
2005.
Using Curvature and MarkovClustering in Graphs for Lexical Acquisition and Word Sense Discrimination.R.
Evans and C. Or?asan.
2000.
Improving anaphora resolution by identifying animate entities in texts.
In Proc.
ofDAARC.V.
Hatzivassiloglou and K. R. McKeown.
1997.
Predicting the semantic orientation of adjectives.
In Proc.
of ACL.M.
A. Hearst.
1992.
Automatic acquisition of hyponyms from large text corpora.
In Proc.
of Coling ?
Volume 2.H.
Ji and D. Lin.
2009.
Gender and Animacy Knowledge Discovery from Web-Scale N-Grams for UnsupervisedPerson Mention Detection.
In Proc.
of PACLIC.M.
A.
Just, V. L. Cherkassky, S. Aryal, and T. M. Mitchell.
2010.
A neurosemantic theory of concrete nounrepresentation based on the underlying brain codes.
PloS one, 5(1):e8622.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proc.
of ACL-HLT.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Semantic class learning from the web with hyponym pattern linkagegraphs.
In Proc.
of ACL-HLT.O.
Y. Kwong.
2011.
Measuring concept concreteness from the lexicographic perspective.
In Proc.
of PACLIC.T.
K. Landauer and S. T. Dumais.
1997.
A solution to plato?s problem: The latent semantic analysis theory ofacquisition, induction, and representation of knowledge.
Psychological review, 104(2):211.R.
W. Langacker.
2008.
Cognitive grammar: A basic introduction.
Oxford University Press.P.
Liang.
2005.
Semi-supervised learning for natural language.
Master?s thesis, Massachusetts Institute of Tech-nology.J.
M. Mandler.
2004.
The foundations of mind: Origins of conceptual thought.
Oxford University Press NewYork.A.
Martin.
2007.
The representation of object concepts in the brain.
Annual Review of Psychology, 58:25?45.J.
B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Or-want, et al.
2011.
Quantitative analysis of culture using millions of digitized books.
Science, 331(6014):176?182.S.
Miller, J. Guinness, and A. Zamanian.
2004.
Name tagging with word clusters and discriminative training.
InProc.
of NAACL.J.
L. Moore, C. J. Burges, E. Renshaw, and W.-t. Yih.
2013.
Animacy Detection with Voting Models.
In Proc.
ofEMNLP.R.
Nagata, A. Kawai, K. Morihiro, and N. Isu.
2006.
Reinforcing English countability prediction with onecountability per discourse property.
Proc.
of ACL-Coling.T.
Naselaris, D. E. Stansbury, and J. L. Gallant.
2012.
Cortical representation of animate and inanimate objects incomplex natural scenes.
Journal of Physiology-Paris, 106(5):239?249.C.
Or?asan and R. Evans.
2001.
Learning to identify animate references.
In Proc.
of the Workshop on Computa-tional Natural Language.C.
Or?asan and R. Evans.
2007.
NP Animacy Identification for Anaphora Resolution.
JAIR, 29:79?103.1622L.
?vrelid.
2005.
Animacy classification based on morphosyntactic corpus frequencies: some experiments withNorwegian nouns.
In Proc.
of the Workshop on Exploring Syntactically Annotated Corpora, pages 1?11.J.
Peng and K. Araki.
2005.
Detecting the countability of english compound nouns using web-based models.
InProc.
of IJCNLP.E.
Riloff and J. Shepherd.
1997.
A corpus-based approach for building semantic lexicons.
In Proc.
of EMNLP.R.
Schwartz, O. Tsur, A. Rappoport, and M. Koppel.
2013.
Authorship Attribution of Micro-Messages.
In Proc.of EMNLP.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantic taxonomy induction from heterogenous evidence.
In Proc.of ACL-Coling.R.
Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.
In Proc.
of EMNLP.G.
Sudre, D. Pomerleau, M. Palatucci, L. Wehbe, A. Fyshe, R. Salmelin, and T. Mitchell.
2012.
Tracking neuralcoding of perceptual and semantic features of concrete nouns.
NeuroImage, 62(1):451?463.P.
P. Talukdar and K. Crammer.
2009.
New regularized algorithms for transductive learning.
In ECML-PKDD,pages 442?457.
Springer.O.
Tsur, D. Davidov, and A. Rappoport.
2010.
ICWSM ?
a great catchy name: Semi-supervised recognition ofsarcastic sentences in online product reviews.
In Proc.
of ICWSM.P.
D. Turney and P. Pantel.
2010.
From frequency to meaning: Vector space models of semantics.
Journal ofartificial intelligence research, 37(1):141?188.P.
Turney, Y. Neuman, D. Assaf, and Y. Cohen.
2011.
Literal and metaphorical sense identification throughconcrete and abstract context.
In Proc.
of EMNLP.P.
D. Turney.
2008.
The latent relation mapping engine: Algorithm and experiments.
Journal of Artificial Intelli-gence Research, 33:615?655.R.
C. Wang and W. W. Cohen.
2009.
Automatic set instance extraction using the web.
In Proc.
of ACL-IJCNLP.D.
Widdows and B. Dorow.
2002.
A graph model for unsupervised lexical acquisition.
In Proc.
of Coling.X.
Xing, Y. Zhang, and M. Han.
2010.
Query difficulty prediction for contextual image retrieval.
In Advances inInformation Retrieval, pages 581?585.
Springer.S.
X. Yu and J. Shi.
2003.
Multiclass spectral clustering.
In Proc.
of ICCV.1623
