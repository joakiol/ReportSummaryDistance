Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411?1416,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Walk-based Semantically Enriched Tree KernelOver Distributed Word RepresentationsShashank Srivastava1 Dirk Hovy2 Eduard Hovy1(1) Carnegie Mellon University, Pittsburgh(2) Center for Language Technology, University of Copenhagen, Denmark{ssrivastava,hovy}@cmu.edu, mail@dirkhovy.comAbstractIn this paper, we propose a walk-based graphkernel that generalizes the notion of tree-kernels to continuous spaces.
Our proposedapproach subsumes a general framework forword-similarity, and in particular, provides aflexible way to incorporate distributed repre-sentations.
Using vector representations, suchan approach captures both distributional se-mantic similarities among words as well as thestructural relations between them (encoded asthe structure of the parse tree).
We show an ef-ficient formulation to compute this kernel us-ing simple matrix operations.
We present ourresults on three diverse NLP tasks, showingstate-of-the-art results.1 IntroductionCapturing semantic similarity between sentencesis a fundamental issue in NLP, with applications ina wide range of tasks.
Previously, tree kernels basedon common substructures have been used to modelsimilarity between parse trees (Collins and Duffy,2002; Moschitti, 2004; Moschitti, 2006b).
Thesekernels encode a high number of latent syntacticfeatures within a concise representation, and com-pute the similarity between two parse trees basedon the matching of node-labels (words, POS tags,etc.
), as well as the overlap of tree structures.
Whilethis is sufficient to capture syntactic similarity, itdoes not capture semantic similarity very well, evenwhen using discrete semantic types as node labels.This constrains the utility of many traditionaltree kernels in two ways: i) two sentences thatare syntactically identical, but have no semanticsimilarity can receive a high matching score (seeTable 1, top) while ii) two sentences with only localsyntactic overlap, but high semantic similarity canreceive low scores (see Table 1, bottom).tree pairs semantic syntactic score??high?
?lowlovewe toyscrushthey puppieskissedshe catgavesheherkissafriendfelinegreen littleherTable 1: Traditional tree kernels do not capture se-mantic similarityIn contrast, distributional vector representationsof words have been successful in capturing fine-grained semantics, but lack syntactic knowledge.Resources such as Wordnet, dictionaries and on-tologies that encode different semantic perspectivescan also provide additional knowledge infusion.In this paper, we describe a generic walk-basedgraph kernel for dependency parse trees that sub-sumes general notions of word-similarity, whilefocusing on vector representations of words tocapture lexical semantics.
Through a convolutionalframework, our approach takes into account thedistributional semantic similarities between wordsin a sentence as well as the structure of the parsetree.
Our main contributions are:1.
We present a new graph kernel for NLP that ex-tends to distributed word representations, anddiverse word similarity measures.2.
Our proposed approach provides a flexibleframework for incorporating both syntax andsemantics of sentence level constructions.3.
Our generic kernel shows state-of-the-art per-formance on three eclectic NLP tasks.14112 Related WorkTree kernels in NLP Tree kernels have been ex-tensively used to capture syntactic information aboutparse trees in tasks such as parsing (Collins andDuffy, 2002), NER (Wang et al 2010; Cumby andRoth, 2003), SRL (Moschitti et al 2008) and rela-tion extraction (Qian et al 2008).
These kernels arebased on the paradigm that parse trees are similar ifthey contain many common substructures, consist-ing of nodes with identical labels (Vishwanathan andSmola, 2003; Collins and Duffy, 2002).
Moschitti(2006a) proposed a partial tree kernel that adds flex-ibility in matching tree substructures.
Croce et al(2011) introduce a lexical semantic tree kernel thatincorporates continuous similarity values betweennode labels, albeit with a different focus than oursand would not match words with different POS.
Thiswould miss the similarity of ?feline friend?
and ?cat?in our examples, as it requires matching the adjective?feline?
with ?cat?, and verb ?kissed?
with ?kiss?.Walk based kernels Kernels for structured dataderive from the seminal Convolution Kernel for-malism by Haussler (1999) for designing kernelsfor structured objects through local decompositions.Our proposed kernel for parse trees is most closelyassociated with the random walk-based kernels de-fined by Gartner et al(2003) and Kashima et al(2003).
The walk-based graph kernels proposed byGartner et al(2003) count the common walks be-tween two input graphs, using the adjacency matrixof the product graph.
This work extends to graphswith a finite set of edge and node labels by appro-priately modifying the adjacency matrix.
Our kerneldiffers from these kernels in two significant ways: (i)Our method extends beyond label matching to con-tinuous similarity metrics (this conforms with thevery general formalism for graph kernels in Vish-wanathan et al(2010)).
(ii) Rather than using theadjacency matrix to model edge-strengths, we mod-ify the product graph and the corresponding adja-cency matrix to model node similarities.3 Vector Tree KernelsIn this section, we describe our kernel and an al-gorithm to compute it as a simple matrix multiplica-tion formulation.3.1 Kernel descriptionThe similarity kernel K between two dependencytrees can be defined as:K(T1, T2) =?h1?T1,h2?T2len(h1)=len(h2)k(h1, h2)where the summation is over pairs of equal lengthwalks h1 and h2 on the trees T1 and T2 respec-tively.
The similarity between two n length walks,k(h1, h2), is in turn given by the pairwise similari-ties of the corresponding nodes vih in the respectivewalks, measured via the node similarity kernel ?
:k(h1, h2) =n?i:1?
(vh1i , vh2i )In the context of parse trees, nodes vh1i and vh2i cor-respond to words in the two parse trees, and thus canoften be conveniently represented as vectors overdistributional/dependency contexts.
The vector rep-resentation allows us several choices for the nodekernel function ?.
In particular, we consider:1.
Gaussian : ?
(v1, v2) = exp(?
?v1?v2?22?2)2.
Positive-Linear: ?
(v1, v2) = max(vT1 v2, 0)3.
Sigmoid: ?
(v1, v2) =(1 + tanh(?vT1 v2))/2We note that the kernels above take strictly non-negative values in [0, 1] (assuming word vector rep-resentations are normalized).
Non-negativity is nec-essary, since we define the walk kernel to be theproduct of the individual kernels.
As walk kernelsare products of individual node-kernels, bounded-ness by 1 ensures that the kernel contribution doesnot grow arbitrarily for longer length walks.The kernel function K puts a high similarityweight between parse trees if they contain com-mon walks with semantically similar words in corre-sponding positions.
Apart from the Gaussian kernel,the other two kernels are based on the dot-productof the word vector representations.
We observe thatthe positive-linear kernel defined above is not a Mer-cer kernel, since the max operation makes it non-positive semidefinite (PSD).
However, this formu-lation has desirable properties, most significant be-ing that all walks with one or more node-pair mis-matches are strictly penalized and add no score to1412the tree-kernel.
This is a more selective conditionthan the other two kernels, where mediocre walkcombinations could also add small contributions tothe score.
The sigmoid kernel is also non-PSD, butis known to work well empirically (Boughorbel etal., 2005).
We also observe while the summation inthe kernel is over equal length walks, the formalismcan allow comparisons over different length paths byincluding self-loops at nodes in the tree.With a notion of similarity between words thatdefines the local node kernels, we need computa-tional machinery to enumerate all pairs of walksbetween two trees, and compute the summationover products in the kernel K(T1, T2) efficiently.We now show a convenient way to compute this asa matrix geometric series.3.2 Matrix Formulation for KernelComputationWalk-based kernels compute the number of com-mon walks using the adjacency matrix of the prod-uct graph (Gartner et al 2003).
In our case, thiscomputation is complicated by the fact that insteadof counting common walks, we need to compute aproduct of node-similarities for each walk.
Sincewe compute similarity scores over nodes, rather thanedges, the product for a walk of length n involvesn+ 1 factors.However, we can still compute the tree kernel Kas a simple sum of matrix products.
Given two treesT (V,E) and T ?
(V ?, E?
), we define a modified prod-uct graph G(Vp, Ep) with an additional ghost nodeu added to the vertex set.
The vertex and edge setsfor the modified product graph are given as:Vp := {(vi1, vj1?)
: vi1 ?
V, vj1?
?
V ?}
?
uEp := {((vi1, vj1?
), (vi2, vj2?))
: (vi1, vi2) ?
E,(vj1?, vj2?))
?
E?}?
{(u, (vi1, vj1?))
: vi1 ?
V, vj1?
?
V ?
}The modified product graph thus has additionaledges connecting u to all other nodes.
In our for-mulation, u now serves as a starting location for allrandom walks on G, and a k + 1 length walk of Gcorresponds to a pair of k length walks on T and T ?.We now define the weighted adjacency matrixW forG, which incorporates the local node kernels.W(vi1,vj1?),(vi2,vj2?)
={0 : ((vi1,vj1?),(vi2,vj2?))
/?
Ep?
(vi2, vj2?)
: otherwiseWu,(vi1,vj1?)
= ?
(vi1, vj1?
)W(v,u) = 0 ?
v ?
VpThere is a straightforward bijective mapping fromwalks on G starting from u to pairs of walks on Tand T ?.
Restricting ourselves to the case when thefirst node of a k + 1 length walk is u, the next ksteps allow us to efficiently compute the products ofthe node similarities along the k nodes in the corre-sponding k length walks in T and T ?.
Given this ad-jacency matrix for G, the sum of values of k lengthwalk kernels is given by the uth row of the (k+1)thexponent of the weighted adjacency matrix (denotedasW k+1).
This corresponds to k+1 length walks onG starting from u and ending at any node.
Specif-ically, Wu,(vi,v?j) corresponds to the sum of similar-ities of all common walks of length n in T and T ?that end in vi in T and v?j in T?.
The kernel K forwalks upto length N can now be calculated as :K(T, T ?)
=|Vp|?iSu,iwhereS = W +W 2 + ...WN+1We note that in out formulation, longer walks arenaturally discounted, since they involve products ofmore factors (generally all less than unity).The above kernel provides a similarity measurebetween any two pairs of dependency parse-trees.Depending on whether we consider directional re-lations in the parse tree, the edge set Ep changes,while the procedure for the kernel computation re-mains the same.
Finally, to avoid larger trees yield-ing larger values for the kernel, we normalize thekernel by the number of edges in the product graph.4 ExperimentsWe evaluate the Vector Tree Kernel (VTK) onthree NLP tasks.
We create dependency trees usingthe FANSE parser (Tratz and Hovy, 2011), anduse distribution-based SENNA word embeddingsby Collobert et al(2011) as word representations.These embeddings provide low-dimensional vector1413representations of words, while encoding distribu-tional semantic characteristics.
We use LibSVM forclassification.
For sake of brevity, we only reportresults for the best performing kernel.We first consider the Cornell Sentence Polaritydataset by Pang and Lee (2005).
The task is toidentify the polarity of a given sentence.
Thedata consists of 5331 sentences from positive andnegative movie reviews.
Many phrases denotingsentiments are lexically ambiguous (cf.
?terriblyentertaining?
vs ?terribly written?
), so simple lexi-cal approaches are not expected to work well here,while syntactic context could help disambiguation.Next, we try our approach on the MSR paraphrasecorpus.
The data contains a training set of 4077pairs of sentences, annotated as paraphrases andnon-paraphrases, and a test-set of 1726 sentencepairs.
Each instance consists of a pair of sentences,so the VTK cannot be directly used by a kernelmachine for classification.
Instead, we generate16 kernel values based for each pair on differentparameter settings of the kernel, and feed these asfeatures to a linear SVM.We finally look at the annotated Metaphor corpusby (Hovy et al 2013).
The dataset consists of sen-tences with specified target phrases.
The task here isto classify the target use as literal or metaphorical.We focus on target phrases by upweighting walksthat pass through target nodes.
This is done bysimply multiplying the corresponding entries in theadjacency matrix by a constant factor.5 Results5.1 Sentence Polarity DatasetPrec Rec F1 AccAlbornoz et al.63 ?
?
0.63WNA+synsets 0.61 ?
?
0.61WNA 0.53 ?
?
0.51DSM 0.54 0.55 0.55 0.54SSTK 0.49 0.48 0.48 0.49VTK 0.65 0.58 0.62 0.67Table 2: Results on Sentence Polarity datasetOn the polarity data set, Vector Tree Kernel(VTK) significantly outperforms the state-of-the-artmethod by Carrillo de Albornoz et al(2010), whouse a hybrid model incorporating databases of af-fective lexicons, and also explicitly model the ef-fect of negation and quantifiers (see Table 2).
Lex-ical approaches using pairwise semantic similarityof SENNA embeddings (DSM), as well as Word-net Affective Database-based (WNA) labels performpoorly (Carrillo de Albornoz et al 2010), showingthe importance of syntax for this particular problem.On the other hand, a syntactic tree kernel (SSTK)that ignores distributional semantic similarity be-tween words, fails as expected.5.2 MSR Paraphrase DatasetPrec Rec F1 AccBASE 0.72 0.86 0.79 0.69Zhang et al.74 0.88 0.81 0.72Qiu et al.73 0.93 0.82 0.72Malakasiotis 0.74 0.94 0.83 0.74Finch 0.77 0.90 0.83 0.75VTK 0.72 0.95 0.82 0.72Table 3: Results on MSR Paraphrase corpusOn the MSR paraphrase corpus, VTK performscompetitively against state-of-the-art-methods.
Weexpected paraphrasing to be challenging to ourmethod, since it can involve little syntactic overlap.However, data analysis reveals that the corpus gener-ally contains sentence pairs with high syntactic sim-ilarity.
Results for this task are encouraging sinceours is a general approach, while other systems usemultiple task-specific features like semantic role la-bels, active-passive voice conversion, and synonymyresolution.
In the future, incorporating such featuresto VTK should further improve results for this task .5.3 Metaphor IdentificationAcc P R F1CRF 0.69 0.74 0.50 0.59SVM+DSM 0.70 0.63 0.80 0.71SSTK 0.75 0.70 0.80 0.75VTK 0.76 0.67 0.87 0.76Table 4: Results on Metaphor datasetOn the Metaphor corpus, VTK improves the pre-vious score by Hovy et al(2013), whose approachuses an conjunction of lexical and syntactic tree ker-nels (Moschitti, 2006b), and distributional vectors.VTK identified several templates of metaphor usagesuch as ?warm heart?
and ?cold shoulder?.
We looktowards approaches for automatedly mining suchmetaphor patterns from a corpus.6 ConclusionWe present a general formalism for walk-basedkernels to evaluate similarity of dependency trees.1414Our method generalizes tree kernels to take dis-tributed representations of nodes as input, and cap-ture both lexical semantics and syntactic structuresof parse trees.
Our approach has tunable parame-ters to look for larger or smaller syntactic constructs.Our experiments shows state-of-the-art performanceon three diverse NLP tasks.
The approach can gen-eralize to any task involving structural and local sim-ilarity, and arbitrary node similarity measures.ReferencesSabri Boughorbel, Jean-Philippe Tarel, and Nozha Bouje-maa.
2005.
Conditionally positive definite kernels forsvm based image recognition.
In ICME, pages 113?116.Jorge Carrillo de Albornoz, Laura Plaza, and PabloGerva?s.
2010.
A hybrid approach to emotional sen-tence polarity and intensity classification.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning, pages 153?161.
Associa-tion for Computational Linguistics.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of the 40th annual meeting on association forcomputational linguistics, pages 263?270.
Associationfor Computational Linguistics.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.Danilo Croce, Alessandro Moschitti, and Roberto Basili.2011.
Structured lexical similarity via convolutionkernels on dependency trees.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 1034?1046.
Association forComputational Linguistics.Chad Cumby and Dan Roth.
2003.
On kernel methodsfor relational learning.
In In Proc.
of the InternationalConference on Machine Learning, pages 107?114.Andrew Finch.
2005.
Using machine translation evalu-ation techniques to determine sentence-level semanticequivalence.
In In IWP2005.Thomas Gartner, Peter Flach, and Stefan Wrobel.
2003.On graph kernels: Hardness results and efficient al-ternatives.
In Proceedings of the Annual Conferenceon Computational Learning Theory, pages 129?143.Springer.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical Report Technical Report UCS-CRL-99-10, UC Santa Cruz.Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identifyingmetaphorical word use with tree kernels.
In Proceed-ings of NAACL HLT, Meta4NLP Workshop.Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.2003.
Marginalized kernels between labeled graphs.In Proceedings of the Twentieth International Con-ference on Machine Learning, pages 321?328.
AAAIPress.Prodromos Malakasiotis.
2009.
Paraphrase recognitionusing machine learning to combine similarity mea-sures.
In Proceedings of the ACL-IJCNLP 2009 Stu-dent Research Workshop, ACLstudent ?09, pages 27?35, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow semantic parsing.
In Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics, pages 335?es.
Association forComputational Linguistics.Alessandro Moschitti.
2006a.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Machine Learning: ECML 2006, pages 318?329.Springer.Alessandro Moschitti.
2006b.
Making Tree KernelsPractical for Natural Language Learning.
In In Pro-ceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the ACL.Longhua Qian, Guodong Zhou, Fang Kong, QiaomingZhu, and Peide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semantic relationextraction.
In Proceedings of the 22nd InternationalConference on Computational Linguistics-Volume 1,pages 697?704.
Association for Computational Lin-guistics.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2006.Paraphrase recognition via dissimilarity significanceclassification.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?06, pages 18?26, Stroudsburg, PA,USA.
Association for Computational Linguistics.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Processing,pages 142?149.1415Stephen Tratz and Eduard Hovy.
2011.
A fast, accu-rate, non-projective, semantically-enriched parser.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP ?11, pages1257?1268, Stroudsburg, PA, USA.
Association forComputational Linguistics.S.
V. N. Vishwanathan and Alexander J. Smola.
2003.Fast kernels for string and tree matching.
In AdvancesIn Neural Information Processing Systems 15, pages569?576.
MIT Press.S.
V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kon-dor, and Karsten M. Borgwardt.
2010.
Graph kernels.J.
Mach.
Learn.
Res., 99:1201?1242, August.Xinglong Wang, Jun?ichi Tsujii, and Sophia Ananiadou.2010.
Disambiguating the species of biomedicalnamed entities using natural language parsers.
Bioin-formatics, 26(5):661?667.Yitao Zhang and Jon Patrick.
2005.
Paraphrase identi-fication by text canonicalization.
In In Proceedingsof the Australasian Language Technology Workshop2005.1416
