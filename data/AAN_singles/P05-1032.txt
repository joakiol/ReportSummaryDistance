Proceedings of the 43rd Annual Meeting of the ACL, pages 255?262,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsScaling Phrase-Based Statistical Machine Translationto Larger Corpora and Longer PhrasesChris Callison-Burch Colin BannardUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW{chris,colin}@linearb.co.ukJosh SchroederLinear B Ltd.39 B Cumberland StreetEdinburgh EH3 6RAjosh@linearb.co.ukAbstractIn this paper we describe a novel datastructure for phrase-based statistical ma-chine translation which allows for the re-trieval of arbitrarily long phrases while si-multaneously using less memory than isrequired by current decoder implementa-tions.
We detail the computational com-plexity and average retrieval times forlooking up phrase translations in our suf-fix array-based data structure.
We showhow sampling can be used to reduce theretrieval time by orders of magnitude withno loss in translation quality.1 IntroductionStatistical machine translation (SMT) has an advan-tage over many other statistical natural languageprocessing applications in that training data is reg-ularly produced by other human activity.
For somelanguage pairs very large sets of training data arenow available.
The publications of the EuropeanUnion and United Nations provide gigbytes of databetween various language pairs which can be eas-ily mined using a web crawler.
The LinguisticsData Consortium provides an excellent set of offthe shelf Arabic-English and Chinese-English paral-lel corpora for the annual NIST machine translationevaluation exercises.The size of the NIST training data presents a prob-lem for phrase-based statistical machine translation.Decoders such as Pharaoh (Koehn, 2004) primarilyuse lookup tables for the storage of phrases and theirtranslations.
Since retrieving longer segments of hu-man translated text generally leads to better trans-lation quality, participants in the evaluation exer-cise try to maximize the length of phrases that arestored in lookup tables.
The combination of largecorpora and long phrases means that the table sizecan quickly become unwieldy.A number of groups in the 2004 evaluation exer-cise indicated problems dealing with the data.
Cop-ing strategies included limiting the length of phrasesto something small, not using the entire training dataset, computing phrases probabilities on disk, and fil-tering the phrase table down to a manageable sizeafter the testing set was distributed.
We present adata structure that is easily capable of handling thelargest data sets currently available, and show that itcan be scaled to much larger data sets.In this paper we:?
Motivate the problem with storing enumeratedphrases in a table by examining the memory re-quirements of the method for the NIST data set?
Detail the advantages of using long phrases inSMT, and examine their potential coverage?
Describe a suffix array-based data structurewhich allows for the retrieval of translationsof arbitrarily long phrases, and show that it re-quires far less memory than a table?
Calculate the computational complexity andaverage time for retrieving phrases and showhow this can be sped up by orders of magnitudewith no loss in translation accuracy2 Related WorkKoehn et al (2003) compare a number of differ-ent approaches to phrase-based statistical machine255length num uniq(mil)average #translationsavg translength1 .88 8.322 1.372 16.5 1.733 2.353 42.6 1.182 3.444 58.7 1.065 4.585 65.0 1.035 5.756 66.4 1.022 6.917 65.8 1.015 8.078 64.3 1.012 9.239 62.2 1.010 10.410 59.9 1.010 11.6Table 1: Statistics about Arabic phrases in the NIST-2004 large data track.translation including the joint probability phrase-based model (Marcu and Wong, 2002) and a vari-ant on the alignment template approach (Och andNey, 2004), and contrast them to the performance ofthe word-based IBM Model 4 (Brown et al, 1993).Most relevant for the work presented in this paper,they compare the effect on translation quality of us-ing various lengths of phrases, and the size of theresulting phrase probability tables.Tillmann (2003) further examines the relationshipbetween maximum phrase length, size of the trans-lation table, and accuracy of translation when in-ducing block-based phrases from word-level align-ments.
Venugopal et al (2003) and Vogel et al(2003) present methods for achieving better transla-tion quality by growing incrementally larger phrasesby combining smaller phrases with overlapping seg-ments.3 Scaling to Long PhrasesTable 1 gives statistics about the Arabic-English par-allel corpus used in the NIST large data track.
Thecorpus contains 3.75 million sentence pairs, and has127 million words in English, and 106 million wordsin Arabic.
The table shows the number of uniqueArabic phrases, and gives the average number oftranslations into English and their average length.Table 2 gives estimates of the size of the lookuptables needed to store phrases of various lengths,based on the statistics in Table 1.
The number ofunique entries is calculated as the number uniquelength entries(mil)words(mil)memory(gigs)includingalignments1 7.3 10 .1 .112 36 111 .68 .823 86 412 2.18 2.644 149 933 4.59 5.595 216 1,645 7.74 9.466 284 2,513 11.48 14.077 351 3,513 15.70 19.308 416 4,628 20.34 25.059 479 5,841 25.33 31.2610 539 7,140 30.62 37.85Table 2: Estimated size of lookup tables for theNIST-2004 Arabic-English datalength coverage length coverage1 93.5% 6 4.70%2 73.3% 7 2.95%3 37.1% 8 2.14%4 15.5% 9 1.99%5 8.05% 10 1.49%Table 3: Lengths of phrases from the training datathat occur in the NIST-2004 test setphrases times the average number of translations.The number of words in the table is calculated as thenumber of unique phrases times the phrase lengthplus the number of entries times the average transla-tion length.
The memory is calculated assuming thateach word is represented with a 4 byte integer, thateach entry stores its probability as an 8 byte doubleand that each word alignment is stored as a 2 byteshort.
Note that the size of the table will vary de-pending on the phrase extraction technique.Table 3 gives the percent of the 35,313 word longtest set which can be covered using only phrases ofthe specified length or greater.
The table shows theefficacy of using phrases of different lengths.
The ta-ble shows that while the rate of falloff is rapid, thereare still multiple matches of phrases of length 10.The longest matching phrase was one of length 18.There is little generalization in current SMT imple-mentations, and consequently longer phrases gener-ally lead to better translation quality.2563.1 Why use phrases?Statistical machine translation made considerableadvances in translation quality with the introductionof phrase-based translation.
By increasing the sizeof the basic unit of translation, phrase-based ma-chine translation does away with many of the prob-lems associated with the original word-based for-mulation of statistical machine translation (Brownet al, 1993), in particular:?
The Brown et al (1993) formulation doesn?thave a direct way of translating phrases; insteadthey specify a fertility parameter which is usedto replicate words and translate them individu-ally.?
With units as small as words, a lot of reorderinghas to happen between languages with differentword orders.
But the distortion parameter is apoor explanation of word order.Phrase-based SMT overcomes the first of theseproblems by eliminating the fertility parameterand directly handling word-to-phrase and phrase-to-phrase mappings.
The second problem is alleviatedthrough the use of multi-word units which reducethe dependency on the distortion parameter.
Lessword re-ordering need occur since local dependen-cies are frequently captured.
For example, commonadjective-noun alternations are memorized.
How-ever, since this linguistic information is not encodedin the model, unseen adjective noun pairs may stillbe handled incorrectly.By increasing the length of phrases beyond afew words, we might hope to capture additionalnon-local linguistic phenomena.
For example, bymemorizing longer phrases we may correctly learncase information for nouns commonly selected byfrequently occurring verbs; we may properly han-dle discontinuous phrases (such as French negation,some German verb forms, and English verb particleconstructions) that are neglected by current phrase-based models; and we may by chance capture someagreement information in coordinated structures.3.2 Deciding what length of phrase to storeDespite the potential gains from memorizing longerphrases, the fact remains that as phrases get longerlength coverage length coverage1 96.3% 6 21.9%2 94.9% 7 11.2%3 86.1% 8 6.16%4 65.6% 9 3.95%5 40.9% 10 2.90%Table 4: Coverage using only repeated phrases ofthe specified lengththere is a decreasing likelihood that they will be re-peated.
Because of the amount of memory requiredto store a phrase table, in current implementations achoice is made as to the maximum length of phraseto store.Based on their analysis of the relationship be-tween translation quality and phrase length, Koehnet al (2003) suggest limiting phrase length to threewords or less.
This is entirely a practical sugges-tion for keeping the phrase table to a reasonablesize, since they measure minor but incremental im-provement in translation quality up to their maxi-mum tested phrase length of seven words.1Table 4 gives statistics about phrases which oc-cur more than once in the English section of the Eu-roparl corpus (Koehn, 2002) which was used in theKoehn et al (2003) experiments.
It shows that thepercentage of words in the corpus that can be cov-ered by repeated phrases falls off rapidly at length6, but that even phrases up to length 10 are able tocover a non-trivial portion of the corpus.
This drawsinto question the desirability of limiting phrase re-trieval to length three.The decision concerning what length of phrasesto store in the phrase table seems to boil down toa practical consideration: one must weigh the like-lihood of retrieval against the memory needed tostore longer phrases.
We present a data structurewhere this is not a consideration.
Our suffix array-based data structure allows the retrieval of arbitrar-ily long phrases, while simultaneously requiring farless memory than the standard table-based represen-tation.1While the improvements to translation quality reported inKoehn et al (2003) are minor, their evaluation metric may nothave been especially sensitive to adding longer phrases.
Theyused the Bleu evaluation metric (Papineni et al, 2002), butcapped the n-gram precision at 4-grams.2570123456789spain declined to confirm that spain declined to aid moroccodeclined to confirm that spain declined to aid moroccoto confirm that spain declined to aid moroccoconfirm that spain declined to aid moroccothat spain declined to aid moroccospain declined to aid moroccodeclined to aid moroccoto aid moroccoaid moroccomoroccospain declined to confirm that spain declined aidto morocco0 1 2 3 4 5 6 87 9s[0]s[1]s[2]s[3]s[4]s[5]s[6]s[7]s[8]s[9]Initialized, unsortedSuffix ArraySuffixes denoted by s[i]CorpusIndex ofwords:Figure 1: An initialized, unsorted suffix array for avery small corpus4 Suffix ArraysThe suffix array data structure (Manber and Myers,1990) was introduced as a space-economical way ofcreating an index for string searches.
The suffix ar-ray data structure makes it convenient to computethe frequency and location of any substring or n-gram in a large corpus.
Abstractly, a suffix array isan alphabetically-sorted list of all suffixes in a cor-pus, where a suffix is a substring running from eachposition in the text to the end.
However, rather thanactually storing all suffixes, a suffix array can beconstructed by creating a list of references to eachof the suffixes in a corpus.
Figure 1 shows how asuffix array is initialized for a corpus with one sen-tence.
Each index of a word in the corpus has a cor-responding place in the suffix array, which is identi-cal in length to the corpus.
Figure 2 shows the finalstate of the suffix array, which is as a list of the in-dices of words in the corpus that corresponds to analphabetically sorted list of the suffixes.The advantages of this representation are that it iscompact and easily searchable.
The total size of thesuffix array is a constant amount of memory.
Typ-ically it is stored as an array of integers where thearray is the same length as the corpus.
Because it isorganized alphabetically, any phrase can be quicklylocated within it using a binary search algorithm.Yamamoto and Church (2001) show how to usesuffix arrays to calculate a number of statistics thatare interesting in natural language processing appli-cations.
They demonstrate how to calculate term fre-8361950472to aid moroccoto confirm that spain declined to aid moroccomoroccospain declined to aid moroccodeclined to confirm that spain declined to aid moroccodeclined to aid moroccoconfirm that spain declined to aid moroccoaid moroccothat spain declined to aid moroccospain declined to confirm that spain declined to aid moroccoSortedSuffix ArraySuffixes denoted by s[i]s[0]s[1]s[2]s[3]s[4]s[5]s[6]s[7]s[8]s[9]Figure 2: A sorted suffix array and its correspondingsuffixesquency / inverse document frequency (tf / idf) for alln-grams in very large corpora, as well as how to usethese frequencies to calculate n-grams with high mu-tual information and residual inverse document fre-quency.
Here we show how to apply suffix arrays toparallel corpora to calculate phrase translation prob-abilities.4.1 Applied to parallel corporaIn order to adapt suffix arrays to be useful for sta-tistical machine translation we need a data structurewith the following elements:?
A suffix array created from the source languageportion of the corpus, and another created fromthe target language portion of the corpus,?
An index that tells us the correspondence be-tween sentence numbers and positions in thesource and target language corpora,?
An alignment a for each sentence pair in theparallel corpus, where a is defined as a subsetof the Cartesian product of the word positionsin a sentence e of length I and a sentence f oflength J :a ?
{(i, j) : i = 1...I; j = 1...J}?
A method for extracting the translationallyequivalent phrase for a subphrase given analigned sentence pair containing that sub-phrase.The total memory usage of the data structure isthus the size of the source and target corpora, plusthe size of the suffix arrays (identical in length to the258corpora), plus the size of the two indexes that cor-relate sentence positions with word positions, plusthe size of the alignments.
Assuming we use intsto represent words and indices, and shorts to repre-sent word alignments, we get the following memoryusage:2 ?
num words in source corpus ?
sizeof(int)+2 ?
num words in target corpus ?
sizeof(int)+2 ?
number sentence pairs ?
sizeof(int)+number of word alignments ?
sizeof(short)The total amount of memory required to store theNIST Arabic-English data using this data structureis2 ?
105,994,774 ?
sizeof(int)+2 ?
127,450,473 ?
sizeof(int)+2 ?
3,758,904 ?
sizeof(int)+92,975,229 ?
sizeof(short)Or just over 2 Gigabytes.4.2 Calculating phrase translationprobabilitiesIn order to produce a set of phrase translation prob-abilities, we need to examine the ways in whichthey are calculated.
We consider two common waysof calculating the translation probability: using themaximum likelihood estimator (MLE) and smooth-ing the MLE using lexical weighting.The maximum likelihood estimator for the proba-bility of a phrase is defined asp(f?
|e?)
=count(f?
, e?)?f?
count(f?
, e?
)(1)Where count(f?
, e?)
gives the total number of timesthe phrase f?
was aligned with the phrase e?
in theparallel corpus.
We define phrase alignments as fol-lows.
A substring e?
consisting of the words at po-sitions l...m is aligned with the phrase f?
by way ofthe subalignments = a ?
{(i, j) : i = l...m, j = 1...J}The aligned phrase f?
is the subphrase in f whichspans from min(j) to max(j) for j|(i, j) ?
s.The procedure for generating the counts that areused to calculate the MLE probability using our suf-fix array-based data structures is:1.
Locate all the suffixes in the English suffix ar-ray which begin with the phrase e?.
Since thesuffix array is sorted alphabetically we can eas-ily find the first occurrence s[k] and the last oc-currence s[l].
The length of the span in the suf-fix array l?k+1 indicates the number of occur-rences of e?
in the corpus.
Thus the denominator?f?
count(f?
, e?)
can be calculated as l ?
k + 1.2.
For each of the matching phrases s[i] in thespan s[k]...s[l], look up the value of s[i] whichis the word index w of the suffix in the Englishcorpus.
Look up the sentence number that in-cludes w, and retrieve the corresponding sen-tences e and f , and their alignment a.3.
Use a to extract the target phrase f?
that alignswith the phrase e?
that we are searching for.
In-crement the count for < f?, e?
>.4.
Calculate the probability for each uniquematching phrase f?
using the formula in Equa-tion 1.A common alternative formulation of the phrasetranslation probability is to lexically weight it as fol-lows:plw(f?
|e?, s) =n?i=11|{i|(i, j) ?
s}|??
(i,j)?sp(fj |ei)(2)Where n is the length of e?.In order to use lexical weighting we would needto repeat steps 1-4 above for each word ei in e?.
Thiswould give us the values for p(fj |ei).
We would fur-ther need to retain the subphrase alignment s in or-der to know the correspondence between the words(i, j) ?
s in the aligned phrases, and the total num-ber of foreign words that each ei is aligned with(|{i|(i, j) ?
s}|).
Since a phrase alignment < f?, e?
>may have multiple possible word-level alignments,we retain a set of alignments S and take the maxi-mum:259p(f?
|e?, S) = p(f?
|e?)
?
argmaxs?Splw(f?
|e?, s) (3)Thus our suffix array-based data structure can beused straightforwardly to look up all aligned trans-lations for a given phrase and calculate the proba-bilities on-the-fly.
In the next section we turn tothe computational complexity of constructing phrasetranslation probabilities in this way.5 Computational ComplexityComputational complexity is relevant because thereis a speed-memory tradeoff when adopting our datastructure.
What we gained in memory efficiencymay be rendered useless if the time it takes to cal-culate phrase translation probabilities is unreason-ably long.
The computational complexity of lookingup items in a hash table, as is done in current table-based data structures, is extremely fast.
Looking upa single phrase can be done in unit time, O(1).The computational complexity of our method hasthe following components:?
The complexity of finding all occurrences ofthe phrase in the suffix array?
The complexity of retrieving the associatedaligned sentence pairs given the positions of thephrase in the corpus?
The complexity of extracting all alignedphrases using our phrase extraction algorithm?
The complexity of calculating the probabilitiesgiven the aligned phrasesThe methods we use to execute each of these, andtheir complexities are as follow:?
Since the array is sorted, finding all occur-rences of the English phrase is extremely fast.We can do two binary searches: one to find thefirst occurrence of the phrase and a second tofind the last.
The computational complexity istherefore bounded by O(2 log(n)) where n isthe length of the corpus.?
We use a similar method to look up the sen-tences ei and fi and word-level alignment aiphrase freq O time (ms)respect for thedead3 80 24since the end ofthe cold war19 240 136the parliament 1291 4391 1117of the 290921 682550 218369Table 5: Examples of O and calculation times forphrases of different frequenciesthat are associated with the position wi in thecorpus of each phrase occurrence e?i.
The com-plexity is O(k ?
2 log(m)) where k is the num-ber of occurrences of e?
and m is the number ofsentence pairs in the parallel corpus.?
The complexity of extracting the aligned phrasefor a single occurrence of e?i is O(2 log(|ai|) toget the subphrase alignment si, since we storethe alignments in a sorted array.
The complex-ity of then getting f?i from si is O(length(f?i)).?
The complexity of summing over all alignedphrases and simultaneously calculating theirprobabilities is O(k).Thus we have a total complexity of:O(2 log(n) + k ?
2 log(m) (4)+e?1...e?k?ai,f?i|e?i(2 log(|ai|) + length(f?i)) + k) (5)for the MLE estimation of the translation probabil-ities for a single phrase.
The complexity is domi-nated by the k terms in the equation, when the num-ber of occurrences of the phrase in the corpus ishigh.
Phrases with high frequency may cause exces-sively long retrieval time.
This problem is exacer-bated when we shift to a lexically weighted calcula-tion of the phrase translation probability.
The com-plexity will be multiplied across each of the compo-nent words in the phrase, and the component wordsthemselves will be more frequent than the phrase.Table 5 shows example times for calculating thetranslation probabilities for a number of phrases.
Forfrequent phrases like of the these times get unaccept-ably long.
While our data structure is perfect for260overcoming the problems associated with storing thetranslations of long, infrequently occurring phrases,it in a way introduces the converse problem.
It hasa clear disadvantage in the amount of time it takesto retrieve commonly occurring phrases.
In the nextsection we examine the use of sampling to speed upthe calculation of translation probabilities for veryfrequent phrases.6 SamplingRather than compute the phrase translation proba-bilities by examining the hundreds of thousands ofoccurrences of common phrases, we instead sam-ple from a small subset of the occurrences.
It isunlikely that we need to extract the translations ofall occurrences of a high frequency phrase in orderto get a good approximation of their probabilities.We instead cap the number of occurrences that weconsider, and thus give a maximum bound on k inEquation 5.In order to determine the effect of different lev-els of sampling, we compare the translation qualityagainst cumulative retrieval time for calculating thephrase translation probabilities for all subphrases inan evaluation set.
We translated a held out set of430 German sentences with 50 words or less intoEnglish.
The test sentences were drawn from the01/17/00 proceedings of the Europarl corpus.
Theremainder of the corpus (1 million sentences) wasused as training data to calculate the phrase trans-lation probabilities.
We calculated the translationquality using Bleu?s modified n-gram precision met-ric (Papineni et al, 2002) for n-grams of up to lengthfour.
The framework that we used to calculate thetranslation probabilities was similar to that detailedin Koehn et al (2003).
That is:e?
= argmaxeI1p(eI1|fI1) (6)= argmaxeI1pLM (eI1) ?
(7)I?i=1p(f?i|e?i)d(ai ?
bi?1)plw(f?i|e?i,a) (8)Where pLM is a language model probability and d isa distortion probability which penalizes movement.Table 6 gives a comparison of the translation qual-ity under different levels of sampling.
While the ac-sample size time qualityunlimited 6279 sec .29050000 1051 sec .28910000 336 sec .2915000 201 sec .2891000 60 sec .288500 35 sec .288100 10 sec .288Table 6: A comparison of retrieval times and trans-lation quality when the number of translations iscapped at various sample sizescuracy fluctuates very slightly it essentially remainsuniformly high for all levels of sampling.
There area number of possible reasons for the fact that thequality does not decrease:?
The probability estimates under sampling aresufficiently good that the most probable trans-lations remain unchanged,?
The interaction with the language model prob-ability rules out the few misestimated probabil-ities, or?
The decoder tends to select longer or less fre-quent phrases which are not affected by thesampling.While the translation quality remains essentiallyunchanged, the cumulative time that it takes to cal-culate the translation probabilities for all subphrasesin the 430 sentence test set decreases radically.
Thetotal time drops by orders of magnitude from an hourand a half without sampling down to a mere 10 sec-onds with a cavalier amount of sampling.
This sug-gests that the data structure is suitable for deployedSMT systems and that no additional caching needbe done to compensate for the structure?s computa-tional complexity.7 DiscussionThe paper has presented a super-efficient data struc-ture for phrase-based statistical machine translation.We have shown that current table-based methods areunwieldily when used in conjunction with large datasets and long phrases.
We have contrasted this withour suffix array-based data structure which provides261a very compact way of storing large data sets whilesimultaneously allowing the retrieval of arbitrarilylong phrases.For the NIST-2004 Arabic-English data set,which is among the largest currently assembled forstatistical machine translation, our representationuses a very manageable 2 gigabytes of memory.
Thisis less than is needed to store a table containingphrases with a maximum of three words, and is tentimes less than the memory required to store a tablewith phrases of length eight.We have further demonstrated that while compu-tational complexity can make the retrieval of trans-lation of frequent phrases slow, the use of samplingis an extremely effective countermeasure to this.We demonstrated that calculating phrase translationprobabilities from sets of 100 occurrences or less re-sults in nearly no decrease in translation quality.The implications of the data structure presentedin this paper are significant.
The compact rep-resentation will allow us to easily scale to paral-lel corpora consisting of billions of words of text,and the retrieval of arbitrarily long phrases will al-low experiments with alternative decoding strate-gies.
These facts in combination allow for an evengreater exploitation of training data in statistical ma-chine translation.ReferencesPeter Brown, Stephen Della Pietra, Vincent Della Pietra,and Robert Mercer.
1993.
The mathematics of ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311, June.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT/NAACL.Philipp Koehn.
2002.
Europarl: A multilingual corpusfor evaluation of machine translation.
UnpublishedDraft.Philipp Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of AMTA.Udi Manber and Gene Myers.
1990.
Suffix arrays:A new method for on-line string searches.
In TheFirst Annual ACM-SIAM Symposium on Dicrete Algo-rithms, pages 319?327.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of EMNLP.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?450, De-cember.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic evalu-ation of machine translation.
In Proceedings of ACL.Christoph Tillmann.
2003.
A projection extension algo-rithm for statistical machine translation.
In Proceed-ings of EMNLP.Ashish Venugopal, Stephan Vogel, and Alex Waibel.2003.
Effective phrase translation extraction fromalignment models.
In Proceedings of ACL.Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.2003.
The CMU statistical machine translation sys-tem.
In Proceedings of MT Summit 9.Mikio Yamamoto and Kenneth Church.
2001.
Using suf-fix arrays to compute term frequency and documentfrequency for all substrings in a corpus.
Compuata-tional Linguistics, 27(1):1?30.262
