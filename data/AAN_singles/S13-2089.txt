Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 535?538, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUoM: Using Explicit Semantic Analysis for Classifying SentimentsSapna NegiUniversity of MaltaMsida, MSD2080, MALTAsapna.negi13@gmail.comMike RosnerUniversity of MaltaMsida, MSD2080, MALTAmike.rosner@um.edu.mtAbstractIn this paper, we describe our system sub-mitted for the Sentiment Analysis task at Se-mEval 2013 (Task 2).
We implemented a com-bination of Explicit Semantic Analysis (ESA)with Naive Bayes classifier.
ESA representstext as a high dimensional vector of explicitlydefined topics, following the distributional se-mantic model.
This approach is novel in thesense that ESA has not been used for Senti-ment Analysis in the literature, to the best ofour knowledge.1 IntroductionSemantic relatedness measure gives the comparisonof different terms or texts on the basis of theirmeaning or the content.
For instance, it can besaid that the word ?computer?
is semanticallymore related to ?laptop?
than ?flute?.
Sentimentanalysis refers to the task of determining the overallcontextual polarity of the written text.
In this paper,we propose the use of semantic relatedness models,specifically Explicit Semantic Analysis (ESA),to identify textual polarity.
There are differentapproaches to model semantic relatedness likeWordNet based models (Banerjee and Banerjee,2002), distributional semantic models (DSMs) etc.DSMs follow the distributional hypothesis, whichsays that words occurring in the same contexts tendto have similar meanings (Harris, 1954).
There-fore, considering sentiment classification problem,distributional hypothesis suggests that the words orphrases referring to positive polarity would tend toco-occur, and similar assumptions can be made forthe negative terms.DSMs generally utilize large textual corpora toextract the distributional information relying onthe co-occurrence information and distribution ofthe terms.
These models represent the text in theform of high-dimensional vectors highlighting theco-occurrence information.
Semantic relatednessbetween two given texts is calculated by usingthese vectors, thus, following that the the semanticmeaning of a text can be inferred from its usagein different contexts.
There are several differentcomputational models following distributionalsemantics hypothesis.
Latent Semantic Analysis(LSA), Latent Dirichlet Allocation (LDA) (Blei et.al., 2003), Explicit Semantic Analysis (ESA) aresome examples of such models.
However, in thiswork, we investigated the use of ESA for the giventask of sentiment analysis (SA).There are two sub-tasks defined in Task 2 atSemEval 2013 (SemEval, 2013).
We participated inMessage Polarity Classification sub-task, where weare required to automatically classify the sentimentof a given message into positive, negative, orneutral.
The task deals with the short texts comingfrom Twitter and SMS (Short Message Service).
Weare provided with 8,000 - 12,000 twitter messagesannotated with their sentiment label for the purposeof training the models.
In this work, we present ourapproach for sentiment classification which uses acombination of ESA and Naive Bayes classifier.
Therest of the paper is structured as follows : Section 2discusses some related work in this context.
Section5353 briefly explains ESA.
Section 4 describes ourapproaches while Section 5 explains the submittedruns for our system to the task.
Section 6 reports theresults, and we conclude in section 7.2 Related WorkThe research in SA initiated with the classical ma-chine learning algorithms like Naive Bayes, Maxi-mum Entropy etc.
using intuitive features like un-igrams, bigrams, parts of speech information, posi-tion of words, adjectives etc.
(Pang et.
al., 2002).However, such approaches are heavily dependentupon the given training data, and therefore can bevery limited for SA due to out of vocabulary wordsand phrases, and different meanings of words indifferent contexts (Pang and Lee, 2008).
Due tothese problems, several methods have been investi-gated to use some seed words for extracting morepositive and negative terms with the help of lexi-cal resources like WordNet etc., for instance, Senti-WordNet, which defines the polarity of the wordalong with the intensity.
In this paper, we model thesentiment classification using DSMs based on ex-plicit topic models (Cimiano et.
al., 2009), whichincorporate correlation information from a corpuslike Wikipedia, to generalize from a few known pos-itive or negative terms.
There have been some otherattempts to utilize topic models in this regards, butthey mainly focussed on latent topic models (Lin andHe, 2009) (Maas et.
al., 2011).
Joint sentiment topicmodel introduced LDA based unsupervised topicmodels in sentiment analysis by pointing out thatsentiments are often topic dependent because sameword/phrase could represent different sentiments fordifferent topics (Lin and He, 2009).
The recent workby Maas et.
al.
(Maas et.
al., 2011) on using latentconcept models presented a mixture model of un-supervised and supervised techniques to learn wordvectors capturing semantic term-document informa-tion along with the sentiment content.3 Explicit Semantic AnalysisExplicit Semantic Analysis (ESA) is a techniquefor computing semantic relatedness between textsusing distributional information (Gabrilovich andMarkovitch, 2007).
ESA represents text as vec-tors of concepts explicitly defined by humans, likeWikipedia articles.
This provides an intuitive andeasily understandable topic space for humans, incontrast to the latent topic space in latent mod-els.Input texts are represented as multidimensionalvectors of weighted concepts.
The procedure ofcomputing semantic relatedness involves comparingthe vectors corresponding to the given texts e.g.
us-ing cosine product.
The magnitude of each dimen-sion in the vector is the associativity weight of thetext to that explicit concept/dimension.
To quantifythis associativity, the textual content related to theexplicit concept/dimension is utilized.
This weightcan be calculated by considering different methods,for instance, tf-idf score.
ESA has been proved tobe a generalized vector space model (Gottron et.
al.,2011).4 MethodologyWe implemented a combination of traditional ma-chine learning based approach for SA using NaiveBayes algorithm, and ESA based sentiment identifi-cation.
To perform sentiment classification solelyusing ESA, we asses the similarity of a new textagainst the text whose sentiment is already known,using ESA.
More similar is a text to a particular sen-timent annotated text, better are its chances to be-long to the same sentiment class.
On the other hand,we followed a standard classification approach bylearning Naive Bayes over the given training data.Finally, we consult both ESA and Naive Bayes forclassifying the text.
The overall probability of a textbelonging to a particular sentiment class was deter-mined by weighted sum of ESA similarity score,and the scores given by Naive Bayes classifier.
Thesentiment class with the highest total score was ac-cepted as the sentiment of the input text.
The indi-vidual weights of ESA and Naive Bayes were deter-mined by linear regression for our experiments.5 System DescriptionWe created three bags of words (BOW) correspond-ing to the different sentiment classes (positive,negative, and neutral) annotated in the trainingdata.
These BOWs were used as the definition ofthe particular sentiment class for making the ESAcomparisons, and for learning Naive Bayes.
Weused unigrams and bigrams as features for the Naive536Task Approach F score Highest F score RankTwitter, with constrained data ESA with Naive Bayes .5182 .6902 24/35SMS, with constrained data ESA with Naive Bayes .422 .6846 24/28Twitter, with unconstrained data ESA with Naive Bayes .4507 .6486 16/16SMS, with unconstrained data ESA with Naive Bayes .3522 .4947 15/15Twitter, with constrained data ESA .35 .6902 NATable 1: ResultsBayes algorithm.
The ESA implementation wasreplicated from the version available on Github1,replacing the Wikipedia dump by the versionreleased in February 2013.We submitted two runs each for Twitter andSMS test data.
The first run (constrained) usedonly the provided training data for learning whilethe second run (unconstrained) used a combinationof external training data coming from the popularmovie review dataset (Pang et.
al., 2002), and thedata provided with the task.6 Results and discussionThe first four entries provided in the table 1 corre-spond to the four runs submitted in SemEval-2013Task 2.
The fifth entry corresponds to the resultsof a separate experiment performed by us, to esti-mate the influence of ESA on SA.
According to theF-scores, ESA is unable to identify the sentiment inthe texts following the mentioned approach.
The re-sults suggest that combining Naive Bayes to the sys-tem improved the overall scores.
However, even thecombined system could not perform well.
Also, themixing of external data lowered the scores indicat-ing incompatibility of the external training data withthe provided data.7 ConclusionWe presented an approach of using ESA for senti-ment classification.
The submitted system followa combination of standard Naive Bayes model andESA based classification.
The results of the tasksuggests that the approach we used for ESA basedclassification is unable to identify the sentiment ac-curately.
As a future step, we plan to investigate1https://github.com/kasooja/clesamore on the usability of ESA for sentiment classifi-cation, for instance, by using suitable features in theconcept definitions, and weighing them according tothe different sentiment classes.ReferencesMaas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y.,Potts, C.: Learning word vectors for sentiment anal-ysis.
In: Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1. pp.
142?150.HLT ?11, Association for Computational Linguistics,Stroudsburg, PA, USA (2011), http://dl.acm.org/citation.cfm?id=2002472.2002491Lin, C., He, Y.: Joint sentiment/topic model for sen-timent analysis.
In: Proceedings of the 18th ACMconference on Information and knowledge manage-ment.
pp.
375?384.
CIKM ?09, ACM, New York, NY,USA (2009), http://doi.acm.org/10.1145/1645953.1646003Cimiano, P., Schultz, A., Sizov, S., Sorg, P., Staab,S.
: Explicit versus latent concept models for cross-language information retrieval.
In: Proceedings ofthe 21st international jont conference on Artifi-cal intelligence.
pp.
1513?1518.
IJCAI?09, Mor-gan Kaufmann Publishers Inc., San Francisco, CA,USA (2009), http://dl.acm.org/citation.cfm?id=1661445.1661688Pang, B., Lee, L.: Opinion mining and sentiment analy-sis.
Found.
Trends Inf.
Retr.
2(1-2), 1?135 (Jan 2008),http://dx.doi.org/10.1561/1500000011Pang, B., Lee, L., Vaithyanathan, S.: Thumbs up?
:sentiment classification using machine learning tech-niques.
In: Proceedings of the ACL-02 conferenceon Empirical methods in natural language process-ing - Volume 10. pp.
79?86.
EMNLP ?02, Associa-tion for Computational Linguistics, Stroudsburg, PA,USA (2002), http://dx.doi.org/10.3115/1118693.1118704Wilson, T., Kozareva, Z., Nakov, P., Rosenthal, S., Stoy-anov, V., Ritter, A.: SemEval-2013 task 2: Sentiment537analysis in twitter.
In: Proceedings of the InternationalWorkshop on Semantic Evaluation.
SemEval ?13 (June2013)Banerjee, S., Banerjee, S.: An adapted lesk algorithm forword sense disambiguation using wordnet.
In: In Pro-ceedings of the Third International Conference on In-telligent Text Processing and Computational Linguis-tics.
pp.
136?145 (2002)David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
La-tent dirichlet alcation.
J. Mach.
Learn.
Res., 3:993?1022, March 2003.P.
W. Foltz, W. Kintsch, and T. K. Landauer.
The mea-surement of textual coherence with latent semanticanalysis.
Discourse Processes, 25:285?307, 1998.Evgeniy Gabrilovich and Shaul Markovitch.
Computingsemantic relatedness using wikipedia-based explicitsemantic analysis.
In In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence,pages 1606?1611, 2007.Thomas Gottron, Maik Anderka, and Benno Stein.
In-sights into explicit semantic analysis.
In Proceedingsof the 20th ACM international conference on Informa-tion and knowledge management, CIKM ?11, pages1961?1964, New York, NY, USA, 2011.
ACM.Zellig Harris.
Distributional structure.
Word,10(23):146?162, 1954.538
