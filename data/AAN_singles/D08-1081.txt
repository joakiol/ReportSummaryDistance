Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773?782,Honolulu, October 2008. c?2008 Association for Computational LinguisticsSummarizing Spoken and Written ConversationsGabriel Murray and Giuseppe CareniniDepartment of Computer ScienceUniversity of British ColumbiaVancouver, BC V6T 1Z4 CanadaAbstractIn this paper we describe research on sum-marizing conversations in the meetings andemails domains.
We introduce a conver-sation summarization system that works inmultiple domains utilizing general conversa-tional features, and compare our results withdomain-dependent systems for meeting andemail data.
We find that by treating meet-ings and emails as conversations with generalconversational features in common, we canachieve competitive results with state-of-the-art systems that rely on more domain-specificfeatures.1 IntroductionOur lives are increasingly comprised of multimodalconversations with others.
We email for businessand personal purposes, attend meetings in personand remotely, chat online, and participate in blog orforum discussions.
It is clear that automatic summa-rization can be of benefit in dealing with this over-whelming amount of interactional information.
Au-tomatic meeting abstracts would allow us to preparefor an upcoming meeting or review the decisions of aprevious group.
Email summaries would aid corpo-rate memory and provide efficient indices into largemail folders.When summarizing in each of these domains,there will be potentially useful domain-specific fea-tures ?
e.g.
prosodic features for meeting speech,subject headers for emails ?
but there are also un-derlying similarites between these domains.
Theyare all multiparty conversations, and we hypothe-size that effective summarization techniques can bedesigned that would lead to robust summarizationperformance on a wide array of such conversationtypes.
Such a general conversation summarizationsystem would make it possible to summarize a widevariety of conversational data without needing todevelop unique summarizers in each domain andacross modalities.
While progress has been made insummarizing conversations in individual domains,as described below, little or no work has been doneon summarizing unrestricted, multimodal conversa-tions.In this research we take an extractive approachto summarization, presenting a novel set of conver-sational features for locating the most salient sen-tences in meeting speech and emails.
We demon-strate that using these conversational features in amachine-learning sentence classification frameworkyields performance that is competitive or superiorto more restricted domain-specific systems, whilehaving the advantage of being portable across con-versational modalities.
The robust performance ofthe conversation-based system is attested via severalsummarization evaluation techniques, and we givean in-depth analysis of the effectiveness of the indi-vidual features and feature subclasses used.2 Related Work on Meetings and EmailsIn this section we give a brief overview of previousresearch on meeting summarization and email sum-marization, respectively.7732.1 Meeting SummarizationAmong early work on meeting summarization,Waibel et al (1998) implemented a modified versionof the Maximal Marginal Relevance algorithm (Car-bonell and Goldstein, 1998) applied to speech tran-scripts, presenting the user with the n best sentencesin a meeting browser interface.
Zechner (2002) in-vestigated summarizing several genres of speech, in-cluding spontaneous meeting speech.
Though rele-vance detection in his work relied largely on tf.idfscores, Zechner also explored cross-speaker infor-mation linking and question/answer detection.More recently, researchers have investigatedthe utility of employing speech-specific featuresfor summarization, including prosodic information.Murray et al (2005a; 2005b) compared purelytextual summarization approaches with feature-based approaches incorporating prosodic features,with human judges favoring the feature-based ap-proaches.
In subsequent work (2006; 2007), theybegan to look at additional speech-specific char-acteristics such as speaker status, discourse mark-ers and high-level meta comments in meetings, i.e.comments that refer to the meeting itself.
Galley(2006) used skip-chain Conditional Random Fieldsto model pragmatic dependencies between pairedmeeting utterances (e.g.
QUESTION-ANSWER re-lations), and used a combination of lexical, prosodic,structural and discourse features to rank utterancesby importance.
Galley found that while the mostuseful single feature class was lexical features, acombination of acoustic, durational and structuralfeatures exhibited comparable performance accord-ing to Pyramid evaluation.2.2 Email SummarizationWork on email summarization can be divided intosummarization of individual email messages andsummarization of entire email threads.
Muresan etal.
(2001) took the approach of summarizing indi-vidual email messages, first using linguistic tech-niques to extract noun phrases and then employ-ing machine learning methods to label the extractednoun phrases as salient or not.
Corston-Oliver et al(2004) focused on identifying speech acts within agiven email, with a particular interest in task-relatedsentences.Rambow et al (2004) addressed the challenge ofsummarizing entire threads by treating it as a binarysentence classification task.
They considered threetypes of features: basic features that simply treat theemail as text (e.g.
tf.idf, which scores words highly ifthey are frequent in the document but rare across alldocuments), features that consider the thread to be asequence of turns (e.g.
the position of the turn in thethread), and email-specific features such as numberof recipients and subject line similarity.Carenini et al (2007) took an approach to threadsummarization using the Enron corpus (describedbelow) wherein the thread is represented as afragment quotation graph.
A single node in thegraph represents an email fragment, a portion ofthe email that behaves as a unit in a fine-grainrepresentation of the conversation structure.
Afragment sometimes consists of an entire email andsometimes a portion of an email.
For example, if agiven email has the structureA> BCwhere B is a quoted section in the middle ofthe email, then there are three email fragments intotal: two new fragments A and C separated byone quoted fragment B. Sentences in a fragmentare weighted according to the Clue Word Score(CWS) measure, a lexical cohesion metric basedon the recurrence of words in parent and childnodes.
In subsequent work, Carenini et al (2008)determined that subjectivity detection (i.e., whetherthe sentence contains sentiments or opinions fromthe author) gave additional improvement for emailthread summaries.Also on the Enron corpus, Zajic et al (2008) com-pared Collective Message Summarization (CMS)to Individual Message Summarization (IMS) andfound the former to be a more effective techniquefor summarizing email data.
CMS essentially treatsthread summarization as a multi-document summa-rization problem, while IMS summarizes individualemails in the thread and then concatenates them toform a thread summary.In our work described below we also address thetask of thread summarization as opposed to sum-774marization of individual email messages, followingCarenini et al and the CMS approach of Zajic et al3 Experimental SetupIn this section we describe the classifier employedfor our machine learning experiments, the corporaused, the relevant summarization annotations foreach corpus, and the evaluation methods employed.3.1 Statistical ClassifierOur approach to extractive summarization viewssentence extraction as a classification problem.
Forall machine learning experiments, we utilize logisticregression classifiers.
This choice was partly moti-vated by our earlier summarization research, wherelogistic regression classifiers were compared along-side support vector machines (SVMs) (Cortes andVapnik, 1995).
The two classifier types yielded verysimilar results, with logistic regression classifiersbeing much faster to train and thus expediting fur-ther development.The liblinear toolkit 1 implements simple featuresubset selection based on the F statistic (Chen andLin, 2006) .3.2 Corpora DescriptionFor these experiments we utilize two corpora, theEnron corpus for email summarization and the AMIcorpus for meeting summarization.3.2.1 The Enron Email CorpusThe Enron email corpus2 is a collection of emailsreleased as part of the investigation into the Enroncorporation (Klimt and Yang, 2004).
It has becomea popular corpus for NLP research (e.g.
(Bekkermanet al, 2004; Yeh and Harnly, 2006; Chapanond et al,2005; Diesner et al, 2005)) due to being realistic,naturally-occurring data from a corporate environ-ment, and moreover because privacy concerns meanthat there is very low availability for other publiclyavailable email data.39 threads have been annotated for extractivesummarization, with five annotators assigned toeach thread.
The annotators were asked to select30% of the sentences in a thread, subsequently la-beling each selected sentence as either ?essential?
or1http://www.csie.ntu.edu.tw/?cjlin/liblinear/2http://www.cs.cmu.edu/?enron/?optional.?
Essential sentences are weighted threetimes as highly as optional sentences.
A sentencescore, or GSValue, can therefore range between 0and 15, with the maximum GSValue achieved whenall five annotators consider the sentence essential,and a score of 0 achieved when no annotator selectsthe given sentence.
For the purpose of training a bi-nary classifier, we rank the sentences in each emailthread according to their GSValues, then extract sen-tences until our summary reaches 30% of the to-tal thread word count.
We label these sentences aspositive instances and the remainder as the negativeclass.
Approximately 19% of sentences are labeledas positive, extractive examples.Because the amount of labeled data available forthe Enron email corpus is fairly small, for our classi-fication experiments we employ a leave-one-out pro-ceedure for the 39 email threads.
The labeled data asa whole total just under 1400 sentences.3.2.2 The AMI Meetings CorpusFor our meeting summarization experiments, weuse the scenario portion of the AMI corpus (Carlettaet al, 2005).
The corpus consists of about 100 hoursof recorded and annotated meetings.
In the scenariomeetings, groups of four participants take part in aseries of four meetings and play roles within a ficti-tious company.
While the scenario given to them isartificial, the speech and the actions are completelyspontaneous and natural.
There are 96 meetings inthe training set, 24 in the development set, and 20meetings for the test set.For this corpus, annotators wrote abstract sum-maries of each meeting and extracted transcript dia-logue act segments (DAs) that best conveyed or sup-ported the information in the abstracts.
A many-to-many mapping between transcript DAs and sen-tences from the human abstract was obtained foreach annotator, with three annotators assigned toeach meeting.
It is possible for a DA to be extractedby an annotator but not linked to the abstract, but fortraining our binary classifiers, we simply consider adialogue act to be a positive example if it is linkedto a given human summary, and a negative exampleotherwise.
This is done to maximize the likelihoodthat a data point labeled as ?extractive?
is truly aninformative example for training purposes.
Approx-imately 13% of the total DAs are ultimately labeled775as positive, extractive examples.The AMI corpus contains automatic speechrecognition (ASR) output in addition to manualmeeting transcripts, and we report results on bothtranscript types.
The ASR output was provided bythe AMI-ASR team (Hain et al, 2007), and the worderror rate for the AMI corpus is 38.9%.3.3 Summarization EvaluationFor evaluating our extractive summaries, we imple-ment existing evaluation schemes from previous re-search, with somewhat similar methods for meet-ings versus emails.
These are described and com-pared below.
We also evaluate our extractive classi-fiers more generally by plotting the receiver operatorcharacteristic (ROC) curve and calculating the areaunder the curve (AUROC).
This allows us to gaugethe true-positive/false-positive ratio as the posteriorthreshold is varied.We use the differing evaluation metrics for emailsversus meetings for two primary reasons.
First,the differing summarization annotations in the AMIand Enron corpora naturally lend themselves toslightly divergent metrics, one based on extract-abstract links and the other based on the essen-tial/option/uninformative distinction.
Second, andmore importantly, using these two metrics allow usto compare our results with state-of-the-art resultsin the two fields of speech summarization and emailsummarization.
In future work we plan to use a sin-gle evaluation metric.3.3.1 Evaluating Meeting SummariesTo evaluate meeting summaries we use theweighted f-measure metric (Murray et al, 2006).This evaluation scheme relies on the multiple humanannotated summary links described in Section 3.2.2.Both weighted precision and recall share the samenumeratornum =M?i=1N?j=1L(si, aj) (1)where L(si, aj) is the number of links for a DAsi in the machine extractive summary according toannotator ai, M is the number of DAs in the ma-chine summary, and N is the number of annotators.Weighted precision is defined as:precision = numN ?
M (2)and weighted recall is given byrecall = num?Oi=1?Nj=1 L(si, aj)(3)where O is the total number of DAs in the meeting,N is the number of annotators, and the denominatorrepresents the total number of links made betweenDAs and abstract sentences by all annotators.
Theweighted f-measure is calculated as the harmonicmean of weighted precision and recall.
The intuitionbehind weighted f-score is that DAs that are linkedmultiple times by multiple annotators are the mostinformative.3.3.2 Evaluating Email SummariesFor evaluating email thread summaries, we followCarenini et al (2008) by implementing their pyra-mid precision scheme, inspired by Nenkova?s pyra-mid scheme (2004).
In Section 3.2.1 we introducedthe idea of a GSValue for each sentence in an emailthread, based on multiple human annotations.
Wecan evaluate a summary of a given length by com-paring its total GSValues to the maximum possibletotal for that summary length.
For instance, if in athread the three top scoring sentences had GSValuesof 15, 12 and 12, and the sentences selected by agiven automatic summarization method had GSVal-ues of 15, 10 and 8, the pyramid precision would be0.85.Pyramid precision and weighted f-score are simi-lar evaluation schemes in that they are both sentencebased (as opposed to, for example, n-gram based)and that they score sentences based on multiple hu-man annotations.
Pyramid precision is very simi-lar to equation 3 normalized by the maximum scorefor the summary length.
For now we use these twoslightly different schemes in order to maintain con-sistency with prior art in each domain.4 A Conversation Summarization SystemIn our conversation summarization approach, wetreat emails and meetings as conversations com-prised of turns between multiple participants.
Wefollow Carenini et al (2007) in working at the finer776granularity of email fragments, so that for an emailthread, a turn consists of a single email fragment inthe exchange.
For meetings, a turn is a sequence ofdialogue acts by one speaker, with the turn bound-aries delimited by dialogue acts from other meet-ing participants.
The features we derive for summa-rization are based on this view of the conversationalstructure.We calculate two length features.
For each sen-tence, we derive a word-count feature normalizedby the longest sentence in the conversation (SLEN)and a word-count feature normalized by the longestsentence in the turn (SLEN2).
Sentence length haspreviously been found to be an effective feature inspeech and text summarization (e.g.
(Maskey andHirschberg, 2005; Murray et al, 2005a; Galley,2006)).There are several structural features used, in-cluding position of the sentence in the turn (TLOC)and position of the sentence in the conversation(CLOC).
We also include the time from the begin-ning of the conversation to the current turn (TPOS1)and from the current turn to the end of the conversa-tion (TPOS2).
Conversations in both modalities canbe well-structured, with introductory turns, generaldiscussion, and ultimate resolution or closure, andsentence informativeness might significantly corre-late with this structure.
We calculate two pause-stylefeatures: the time between the following turn and thecurrent turn (SPAU), and the time between the cur-rent turn and previous turn (PPAU), both normalizedby the overall length of the conversation.
These fea-tures are based on the email and meeting transcripttimestamps.
We hypothesize that pause features maybe useful if informative turns tend to elicit a largenumber of responses in a short period of time, or ifthey tend to quickly follow a preceding turn, to givetwo examples.There are two features related to the conversationparticipants directly.
One measures how dominantthe current participant is in terms of words in theconversation (DOM), and the other is a binary fea-ture indicating whether the current participant ini-tiated the conversation (BEGAUTH), based simplyon whether they were the first contributor.
It is hy-pothesized that informative sentences may more of-ten belong to participants who lead the conversationor have a good deal of dominance in the discussion.There are several lexical features used in theseexperiments.
For each unique word, we calculatetwo conditional probabilities.
For each conversationparticipant, we calculate the probability of the par-ticipant given the word, estimating the probabilityfrom the actual term counts, and take the maximumof these conditional probabilities as our first termscore, which we will call Sprob.Sprob(t) = maxSp(S|t)where t is the word and S is a participant.
For ex-ample, if the word budget is used ten times in total,with seven uses by participant A, three uses by par-ticipant B and no uses by the other participants, thenthe Sprob score for this term is 0.70.
The intuitionis that certain words will tend to be associated withone conversation participant more than the others,owing to varying interests and expertise between thepeople involved.Using the same procedure, we calculate a scorecalled Tprob based on the probability of each turngiven the word.Tprob(t) = maxTp(T |t)The motivating factor for this metric is that certainwords will tend to cluster into a small number ofturns, owing to shifting topics within a conversation.Having derived Sprob and Tprob, we then calcu-late several sentence-level features based on theseterm scores.
Each sentence has features related tomax, mean and sum of the term scores for thewords in that sentence (MXS, MNS and SMS forSprob, and MXT, MNT and SMT for Tprob).
Us-ing a vector representation, we calculate the cosinebetween the conversation preceding the given sen-tence and the conversation subsequent to the sen-tence, first using Sprob as the vector weights (COS1)and then using Tprob as the vector weights (COS2).This is motivated by the hypothesis that informativesentences might change the conversation in somefashion, leading to a low cosine between the preced-ing and subsequent portions.
We similarly calculatetwo scores measuring the cosine between the cur-rent sentence and the rest of the converation, usingeach term-weight metric as vector weights (CENT1for Sprob and CENT2 for Tprob).
This measures777Feature ID DescriptionMXS max Sprob scoreMNS mean Sprob scoreSMS sum of Sprob scoresMXT max Tprob scoreMNT mean Tprob scoreSMT sum of Tprob scoresTLOC position in turnCLOC position in conv.SLEN word count, globally normalizedSLEN2 word count, locally normalizedTPOS1 time from beg.
of conv.
to turnTPOS2 time from turn to end of conv.DOM participant dominance in wordsCOS1 cos. of conv.
splits, w/ SprobCOS2 cos. of conv.
splits, w/ TprobPENT entro.
of conv.
up to sentenceSENT entro.
of conv.
after the sentenceTHISENT entropy of current sentencePPAU time btwn.
current and prior turnSPAU time btwn.
current and next turnBEGAUTH is first participant (0/1)CWS rough ClueWordScoreCENT1 cos. of sentence & conv., w/ SprobCENT2 cos. of sentence & conv., w/ TprobTable 1: Features Keywhether the candidate sentence is generally similarto the conversation overall.There are three word entropy features, calculatedusing the formulawent(s) =?Ni=1 p(xi) ?
?
log(p(xi))( 1N ?
?
log( 1N )) ?
Mwhere s is a string of words, xi is a word typein that string, p(xi) is the probability of the wordbased on its normalized frequency in the string, Nis the number of word types in the string, and M isthe number of word tokens in the string.Note that word entropy essentially captures infor-mation about type-token ratios.
For example, if eachword token in the string was a unique type then theword entropy score would be 1.
We calculate theword entropy of the current sentence (THISENT),as well as the word entropy for the conversation upuntil the current sentence (PENT) and the word en-tropy for the conversation subsequent to the currentsentence (SENT).
We hypothesize that informativesentences themselves may have a diversity of wordtypes, and that if they represent turning points in theconversation they may affect the entropy of the sub-sequent conversation.Finally, we include a feature that is a rough ap-proximation of the ClueWordScore (CWS) used byCarenini et al (2007).
For each sentence we removestopwords and count the number of words that occurin other turns besides the current turn.
The CWS istherefore a measure of conversation cohesion.For ease of reference, we hereafter refer to thisconversation features system as ConverSumm.5 Comparison Summarization SystemsIn order to compare the ConverSumm system withstate-of-the-art systems for meeting and email sum-marization, respectively, we also present results us-ing the features described by Murray and Renals(2008) for meetings and the features described byRambow (2004) for email.
Because the work byMurray and Renals used the same dataset, we cancompare our scores directly.
However, Rambow car-ried out summarization work on a different, unavail-able email corpus, and so we re-implemented theirsummarization system for our current email data.In their work on meeting summarization, Murrayand Renals creating 700-word summaries of eachmeeting using several classes of features: prosodic,lexical, structural and speaker-related.
While thereare two features overlapping between our systems(word-count and speaker/participant dominance),their system is primarily domain-dependent in itsuse of prosodic features while our features representa more general conversational view.Rambow presented 14 features for the summa-rization task, including email-specific informationsuch as the number of recipients, number of re-sponses, and subject line overlap.
There is again aslight overlap in features between our two systems,as we both include length and position of the sen-tence in the thread/conversation.6 ResultsHere we present, in turn, the summarization resultsfor meeting and email data.6.1 Meeting Summarization ResultsFigure 1 shows the F statistics for each Conver-summ feature in the meeting data, providing a mea-sure of the usefulness of each feature in discriminat-ing between the positive and negative classes.
Some77800.050.10.150.20.25CENT2CENT1CWSBEGAUTHSPAUPPAUTHISENTSENTPENTCOS2COS1DOMTPOS2TPOS1SLEN2SLENCLOCTLOCSMTMNTMXTSMSMNSMXSf statisticfeature ID (see key)manualASRFigure 1: Feature F statistics for AMI meeting corpusSystem Weighted F-Score AUROCSpeech - Man 0.23 0.855Speech - ASR 0.24 0.850Conv.
- Man 0.23 0.852Conv.
- ASR 0.22 0.853Table 2: Weighted F-Scores and AUROCs for MeetingSummariesfeatures such as participant dominance have verylow F statistics because each sentence by a givenparticipant will receive the same score; so while thefeature itself may have a low score because it doesnot discriminate informative versus non-informativesentences on its own, it may well be useful in con-junction with the other features.
The best individualConverSumm features for meeting summarizationare sentence length (SLEN), sum of Sprob scores,sum of Tprob scores, the simplified CWS score(CWS), and the two centroid measures (CENT1 andCENT2).
The word entropy of the candidate sen-tence is very effective for manual transcripts butmuch less effective on ASR output.
This is due tothe fact that ASR errors can incorrectly lead to highentropy scores.Table 2 provides the weighted f-scores for allsummaries of the meeting data, as well as AUROCscores for the classifiers themselves.
For our 700-word summaries, the Conversumm approach scorescomparably to the speech-specific approach on bothmanual and ASR transcripts according to weightedf-score.
There are no significant differences accord-ing to paired t-test.
For the AUROC measures, thereare again no significant differences between the con-00.20.40.60.810  0.2  0.4  0.6  0.8  1TPFPlexical featuresstructural featuresparticipant featureslength featuresFea.
Subset AUROCStructural 0.652Participant 0.535Length 0.837Lexical 0.852Figure 2: AUROC Values for Feature Subclasses, AMICorpusversation summarizers and speech-specific summa-rizers.
The AUROC for the conversation systemis slightly lower on manual transcripts and slightlyhigher when applied to ASR output.For all systems the weighted f-scores are some-what low.
This is partly owing to the fact that out-put summaries are very short, leading to high pre-cision and low recall.
The low f-scores are also in-dicative of the difficulty of the task.
Human perfor-mance, gauged by comparing each annotator?s sum-maries to the remaining annotators?
summaries, ex-hibits an average weighted f-score of 0.47 on thesame test set.
The average kappa value on the test setis 0.48, showing the relatively low inter-annotatoragreement that is typical of summarization annota-tion.
There is no additional benefit to combining theconversational and speech-specific features.
In thatcase, the weighted f-scores are 0.23 for both manualand ASR transcripts.
The overall AUROC is 0.85for manual transcripts and 0.86 for ASR.We can expand the features analysis by consid-ering the effectiveness of certain subclasses of fea-tures.
Specifically, we group the summarization fea-tures into lexical, structural, participant and lengthfeatures.
Figure 2 shows the AUROCs for the fea-ture subset classifiers, illustrating that the lexicalsubclass is very effective while the length featuresalso constitute a challenging baseline.
A weakness779System Pyramid Precision AUROCRambow 0.50 0.64Conv.
0.46 0.75Table 3: Pyramid Precision and AUROCs for Email Sum-mariesof systems that depend heavily on length features,however, is that recall scores tend to decrease be-cause the extracted units are much longer - weightedrecall scores for the 700 word summaries are sig-nificantly worse according to paired t-test (p<0.05)when using just length features compared to the fullfeature set.6.2 Email Summarization ResultsFigure 3 shows the F statistic for each ConverSummfeature in the email data.The two most useful fea-tures are sentence length and CWS.
The Sprob andTprob features rate very well according to the Fstatistic.
The two centroid features incorporatingSprob and Tprob are comparable to one another andare very effective features as well.00.010.020.030.040.050.060.070.080.09CENT2CENT1CWSBEGAUTHSPAUPPAUTHISENTSENTPENTCOS2COS1DOMTPOS2TPOS1SLEN2SLENCLOCTLOCSMTMNTMXTSMSMNSMXSf statisticfeature ID (see key)Figure 3: Feature F statistics for Enron email corpusAfter creating 30% word compression summariesusing both the ConverSumm and Rambow ap-proaches, we score the 39 thread summaries usingPyramid Precision.
The results are given in Table 3.On average, the Rambow system is slightly higherwith a score of 0.50 compared with 0.46 for the con-versational system, but there is no statistical differ-ence according to paired t-test.The average AUROC for the Rambow system is0.64 compared with 0.75 for the ConverSumm sys-00.20.40.60.810  0.2  0.4  0.6  0.8  1TPFPlexical featuresstructural featuresparticipant featureslength featuresFea.
Subset AUROCStructural 0.63Participant 0.51Length 0.71Lexical 0.71Figure 4: AUROC Values for Feature Subclasses, EnronCorpustem, with ConverSumm system significantly betteraccording to paired t-test (p<0.05).
Random classi-fication performance would yield an AUROC of 0.5.Combining the Rambow and ConverSumm fea-tures does not yield any overall improvement.
ThePyramid Precision score in that case is 0.47 whilethe AUROC is 0.74.Figure 4 illustrates that the lexical and lengthfeatures are the most effective feature subclasses,though the best results overall are derived from acombination of all feature classes.7 DiscussionAccording to multiple evaluations, the ConverSummfeatures yield competitive summarization perfor-mance with the comparison systems.
There is a clearset of features that are similarly effective in both do-mains, especially CWS, the centroid features, theSprob features, the Tprob features, and sentencelength.
There are other features that are more ef-fective in one domain than the other.
For exam-ple, the BEGAUTH feature, indicating whether thecurrent participant began the conversation, is moreuseful for emails.
It seems that being the first per-son to speak in a meeting is not as significant asbeing the first person to email in a given thread.SLEN2, which normalizes sentence length by thelongest sentence in the turn, also is much more ef-780fective for emails.
The reason is that many meet-ing turns consist of a single, brief utterance such as?Okay, yeah.
?The finding that the summary evaluations arenot significantly worse on noisy ASR comparedwith manual transcripts has been previously attested(Valenza et al, 1999; Murray et al, 2005a), and it isencouraging that our ConverSumm features are sim-ilarly robust to this noisy data.8 ConclusionWe have shown that a general conversation summa-rization approach can achieve results on par withstate-of-the-art systems that rely on features specificto more focused domains.
We have introduced aconversation feature set that is similarly effective inboth the meetings and emails domains.
The use ofmultiple summarization evaluation techniques con-firms that the system is robust, even when appliedto the noisy ASR output in the meetings domain.Such a general conversation summarization systemis valuable in that it may save time and effort re-quired to implement unique systems in a variety ofconversational domains.We are currently working on extending our sys-tem to other conversation domains such as chats,blogs and telephone speech.
We are also investigat-ing domain adaptation techniques; for example, wehypothesize that the relatively well-resourced do-main of meetings can be leveraged to improve emailresults, and preliminary findings are encouraging.ReferencesR.
Bekkerman, A. McCallum, and G. Huang.
2004.
Au-tomatic categorization of email into folders: Bench-mark experiments on Enron and SRI corpora.
Tech-nical Report IR-418, Center of Intelligent InformationRetrieval, UMass Amherst.J.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Proc.
of ACM SIGIRConference on Research and Development in Informa-tion Retrieval 1998, Melbourne, Australia, pages 335?336.G.
Carenini, R. Ng, and X. Zhou.
2007.
Summarizingemail conversations with clue words.
In Proc.
of ACMWWW 07, Banff, Canada.G.
Carenini, X. Zhou, and R. Ng.
2008.
Summarizingemails with conversational cohesion and subjectivity.In Proc.
of ACL 2008, Columbus, Ohio, USA.J.
Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,M.
Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,I.
McCowan, W. Post, D. Reidsma, and P. Well-ner.
2005.
The AMI meeting corpus: A pre-announcement.
In Proc.
of MLMI 2005, Edinburgh,UK, pages 28?39.A.
Chapanond, M. Krishnamoorthy, and B. Yener.
2005.Graph theoretic and spectral analysis of enron emaildata.
Comput.
Math.
Organ.
Theory, 11(3):265?281.Y-W. Chen and C-J.
Lin.
2006.
Combining SVMswith various feature selection strategies.
In I. Guyon,S.
Gunn, M. Nikravesh, and L. Zadeh, editors, Featureextraction, foundations and applications.
Springer.S.
Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-bell.
2004.
Integration of email and task lists.
In Proc.of CEAS 2004, Mountain View, CA, USA.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine Learning, 20(3):273?297.J.
Diesner, T. Frantz, and K. Carley.
2005.
Communi-cation networks from the enron email corpus ?it?s al-ways about the people.
enron is no different?.
Comput.Math.
Organ.
Theory, 11(3):201?228.M.
Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.
InProc.
of EMNLP 2006, Sydney, Australia, pages 364?372.T.
Hain, L. Burget, J. Dines, G. Garau, V. Wan,M.
Karafiat, J. Vepa, and M. Lincoln.
2007.
TheAMI system for transcription of speech in meetings.In Proc.
of ICASSP 2007,, pages 357?360.B.
Klimt and Y. Yang.
2004.
Introducing the enron cor-pus.
In Proc.
of CEAS 2004, Mountain View, CA, USA.S.
Maskey and J. Hirschberg.
2005.
Comparing lexial,acoustic/prosodic, discourse and structural features forspeech summarization.
In Proc.
of Interspeech 2005,Lisbon, Portugal, pages 621?624.S.
Muresan, E. Tzoukermann, and J. Klavans.
2001.Combining linguistic and machine learning techniquesfor email summarization.
In Proc.
of ConLL 2001,Toulouse, France.G.
Murray and S. Renals.
2008.
Meta comments forsummarizing meeting speech.
In Proc.
of MLMI 2008,Utrecht, Netherlands.G.
Murray, S. Renals, and J. Carletta.
2005a.
Extrac-tive summarization of meeting recordings.
In Proc.
ofInterspeech 2005, Lisbon, Portugal, pages 593?596.G.
Murray, S. Renals, J. Carletta, and J. Moore.
2005b.Evaluating automatic summaries of meeting record-ings.
In Proc.
of the ACL 2005 MTSE Workshop, AnnArbor, MI, USA, pages 33?40.781G.
Murray, S. Renals, J. Moore, and J. Carletta.
2006.
In-corporating speaker and discourse features into speechsummarization.
In Proc.
of the HLT-NAACL 2006,New York City, USA, pages 367?374.G.
Murray.
2007.
Using Speech-Specific Features forAutomatic Speech Summarization.
Ph.D. thesis, Uni-versity of Edinburgh.A.
Nenkova and B. Passonneau.
2004.
Evaluating con-tent selection in summarization: The Pyramid method.In Proc.
of HLT-NAACL 2004, Boston, MA, USA,pages 145?152.O.
Rambow, L. Shrestha, J. Chen, and C. Lauridsen.2004.
Summarizing email threads.
In Proc.
of HLT-NAACL 2004, Boston, USA.R.
Valenza, T. Robinson, M. Hickey, and R. Tucker.1999.
Summarization of spoken audio through infor-mation extraction.
In Proc.
of the ESCA Workshop onAccessing Information in Spoken Audio, CambridgeUK, pages 111?116.A.
Waibel, M. Bett, M. Finke, and R. Stiefelhagen.
1998.Meeting browser: Tracking and summarizing meet-ings.
In D. E. M. Penrose, editor, Proc.
of the Broad-cast News Transcription and Understanding Work-shop, Lansdowne, VA, USA, pages 281?286.J.
Yeh and A. Harnly.
2006.
Email thread reassemblyusing similarity matching.
In Proc of CEAS 2006.D.
Zajic, B. Dorr, and J. Lin.
2008.
Single-document andmulti-document summarization techniques for emailthreads using sentence compression.
Information Pro-cessing and Management, to appear.K.
Zechner.
2002.
Automatic summarization of open-domain multiparty dialogues in diverse genres.
Com-putational Linguistics, 28(4):447?485.782
