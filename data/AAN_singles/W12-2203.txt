NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 17?24,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsOffline Sentence Processing Measures for testing Readability with UsersAdvaith Siddharthan Napoleon KatsosDepartment of Computing Science Department of Theoretical and Applied LinguisticsUniversity of Aberdeen University of Cambridgeadvaith@abdn.ac.uk nk248@cam.ac.ukAbstractWhile there has been much work on compu-tational models to predict readability basedon the lexical, syntactic and discourse prop-erties of a text, there are also interesting openquestions about how computer generated textshould be evaluated with target populations.In this paper, we compare two offline methodsfor evaluating sentence quality, magnitude es-timation of acceptability judgements and sen-tence recall.
These methods differ in the ex-tent to which they can differentiate betweensurface level fluency and deeper comprehen-sion issues.
We find, most importantly, thatthe two correlate.
Magnitude estimation canbe run on the web without supervision, andthe results can be analysed automatically.
Thesentence recall methodology is more resourceintensive, but allows us to tease apart the flu-ency and comprehension issues that arise.1 IntroductionIn Natural Language Generation, recent approachesto evaluation tend to consider either ?naturalness?
or?usefulness?.
Following evaluation methodologiescommonly used for machine translation and sum-marisation, there have been attempts to measure nat-uralness in NLG by comparison to human generatedgold standards.
This has particularly been the casein evaluating referring expressions, where the gen-erated expression can be treated as a set of attributesand compared with human generated expressions(Gatt et al, 2009; Viethen and Dale, 2006), but therehave also been attempts at evaluating sentences thisway.
For instance, Langkilde-Geary (2002) gener-ate sentences from a parsed analysis of an existingsentence, and evaluate by comparison to the origi-nal.
However, this approach has been criticised atmany levels (see for example, Gatt et al (2009) orSripada et al (2003)); for instance, because there aremany good ways to realise a sentence, because typi-cal NLG tasks do not come with reference sentences,and because fluency judgements in the monolingualcase are more subtle than for machine translation.Readability metrics, by comparison, do not relyon reference texts, and try to model the linguisticquality of a text based on features derived from thetext.
This body of work ranges from the Flesch Met-ric (Flesch, 1951), which is based on average wordand sentence length, to more systematic evaluationsof various lexical, syntactic and discourse charac-teristics of a text (cf.
Pitler et al (2010), who as-sess readability of textual summaries).
Some re-searchers have also suggested measuring edit dis-tance by using a human to revise a system generatedtext and quantifying the revisions made (Sripada etal., 2003).
This does away with the need for ref-erence texts and is quite suited to expert domainssuch as medicine or weather forecasting, where a do-main expert can easily correct system output.
Anal-ysis of these corrections can provide feedback onproblematic content and style.
We have previouslyevaluated text reformulation applications by askingreaders which version they prefer (Siddharthan etal., 2011), or through the use of Likert scales (Lik-ert, 1932) for measuring meaning preservation andgrammaticality (Siddharthan, 2006).
However, noneof these approaches tell us very much about the com-prehensibility of a text for an end reader.To address this, there has been recent interest intask based evaluations.
Task based evaluations di-rectly evaluate generated utterances for their utility17to the hearer.
However, while for some generationareas like reference (Gatt et al, 2009), the real worldevaluation task is obvious, it is less so for other gen-eration tasks such as surface realisation or text-to-text regeneration or paraphrase.
We are thus keento investigate psycholinguistic methods for investi-gating sentence processing as an alternative to taskbased evaluations.In the psycholinguistics literature, various offlineand online techniques have been used to investigatesentence processing by readers.
Online techniques(eye-tracking (Duchowski, 2007), neurophysiologi-cal (Friederici, 1995), etc.)
offer many advantagesin studying how readers process a sentence.
Butas these are difficult to set up and also resource in-tensive, we would prefer to evaluate NLG using of-fline techniques.
Some offline techniques, such asCloze tests (Taylor, 1953) or question answering, re-quire careful preparation of material (choice of textsand questions, and for Cloze, the words to leaveout).
Other methods, such as magnitude estima-tion and sentence recall (cf.
Sec 3 for details), aremore straightforward to implement.
In this paper,we investigate magnitude estimation of acceptabil-ity judgements and delayed sentence recall in thecontext of an experiment investigating generationchoices when realising causal relations.
Our goalis to study how useful these methods are for evaluat-ing surface level fluency and deeper comprehensibil-ity.
We are interested in whether they can distinguishbetween similar sentences, and whether they can beused to test hypotheses regarding the effect of com-mon generation decisions such as information orderand choice of discourse marker.
We briefly discussthe data in Section 2, before describing our experi-ments (Sections 3.1 and 3.2).
We finish with a dis-cussion of their suitability for more general evalua-tion of NLG with target readers.2 DataWe use a dataset created to explore generationchoices in the context of expressing causal re-lations; specifically, the choice of periphrasticcausative (Wolff et al, 2005) and information order.The dataset considers four periphrastic causatives(henceforth referred to as discourse markers): ?be-cause?, ?because of ?, the verb ?cause?
and the noun?cause?
with different lexico-syntactic properties.We present an example from this dataset below (cf.Siddharthan and Katsos (2010) for details):(1) a. Fructose-induced hypertension is causedby increased salt absorption by the intestineand kidney.
[b caused-by a]b.
Increased salt absorption by the intestineand kidney causes fructose-induced hyper-tension.
[a caused b]c. Fructose-induced hypertension occurs be-cause of increased salt absorption by the in-testine and kidney.
[b because-of a]d. Because of increased salt absorption by theintestine and kidney, fructose-induced hy-pertension occurs.
[because-of ab]e. Fructose-induced hypertension occurs be-cause there is increased salt absorption bythe intestine and kidney.
[b because a]f. Because there is increased salt absorp-tion by the intestine and kidney, fructose-induced hypertension occurs.
[because ab]g. Increased salt absorption by the intestineand kidney is the cause of fructose-inducedhypertension.
[a cause-of b]h. The cause of fructose-induced hypertensionis increased salt absorption by the intestineand kidney.
[cause-of ba]In this notation, ?a?
represents the cause,?b?
rep-resents the effect and the remaining string indi-cates the discourse marker; their ordering reflectsthe information order in the sentence, for exam-ple, ?a cause-of b?
indicates a cause-effect informa-tion order using ?cause of?
as the discourse marker.The dataset consists of 144 sentences extracted fromcorpora (18 sentences in each condition (discoursemarker + information order), reformulated manuallyto generated the other seven conditions, resulting in1152 sentences in total.Clearly, different formulations have different lev-els of fluency.
In this paper we explore what two of-fline sentence processing measures can tell us abouttheir acceptability and ease of comprehension.3 Method3.1 Magnitude estimation of acceptabilityHuman judgements for acceptability for each of the1152 sentences in the dataset were obtained usingthe WebExp package (Keller et al, 2009).
Note that18the reformulations are, strictly speaking, grammati-cal according to the authors?
judgement.
We are test-ing violations of acceptability, rather than grammat-icality per se.
This mirrors the case of NLG, wherea grammar is often used for surface realisation, en-suring grammaticality.Acceptability is a measure which reflectsboth ease of comprehension and surface well-formedness.
We later compare this experimentwith a more qualitative comprehension experimentbased on sentence recall (cf.
Section 3.2).
Ratherthan giving participants a fixed scale, we used themagnitude estimation paradigm, which is moresuitable to capture robust or subtle differencesbetween the relative strength of acceptability orgrammaticality violations (see, for example, Bardet al (1996); Cowart (1997); Keller (2000)).
Oneadvantage of magnitude estimation is that theresearcher does not make any assumptions aboutthe number of linguistic distinctions allowed.
Eachparticipant makes as many distinctions as they feelcomfortable.
Participants were given the followinginstructions (omitting those that relate to the webinterface):1.
Judge acceptability of construction, not ofmeaning;2.
There is no limit to the set of numbers you canuse, but they must all be positive - the lowestscore you can assign is 0.
In other words, makeas many distinctions as you feel comfortable;3.
Always score the new sentence relative to thescore you gave the modulus sentence, whichyou will see on the top of the screen;4.
Acceptability is a continuum, do not just makeyes/no judgements on grammaticality;5.
Try not to use a fixed scale, such as 1?5, whichyou might have used for other linguistic taskspreviously.Design: The propositional content of 144 sen-tences was presented in eight conditions.
Eight par-ticipant groups (A?H) consisting of 6 people eachwere presented with exactly one of the eight formu-lations of each of 144 different sentences, as per aLatin square design.
This experimental design al-lows all statistical comparisons between the eighttypes of causal formulations and the three genres tobe within-participant.
The participants were Uni-versity of Cambridge students (all native Englishspeakers).
Participants were asked to score how ac-ceptable a modulus sentence was, using any positivenumber.
They were then asked to score other sen-tences relative to this modulus, so that higher scoreswere assigned to more acceptable sentences.
Scoreswere normalised to allow comparison across partic-ipants, following standard practice in the literature,by using the z-score: For each participant, each sen-tence score was normalised so that the mean scoreis 0 and the standard deviation is 1 (zih =xih?
?h?h),where zih is participant h?s z-score for the sentencei when participant h gave a magnitude estimationscore of xih to that sentence.
?h is the mean and ?hthe standard deviation of the set of magnitude esti-mation scores for user h.3.2 Sentence RecallAcceptability ratings are regarded as a useful mea-sure because they combine surface judgements ofgrammaticality with deeper judgements about howeasy a sentence is to understand.
However, onemight want to know whether an inappropriate for-mulation can cause a breakdown in comprehensionof the content of a sentence, which would go beyondthe (perhaps) non-detrimental effect of a form that isdispreferred at the surface level.
To try and learnmore about this, we conducted a second behaviouralexperiment using a sentence recall methodology.
Asthese experiments are harder to conduct and have tobe supervised in a lab (to ensure that participantshave similar conditions of attention and motivation,and to prevent ?cheating?
using cut-and-paste ornote taking techniques), we selected a subset of 32pairs of items from the previous experiment.
Eachpair consisted of two formulations of the same sen-tence.
The pairs were selected in a manner that ex-hibited a variation in the within-pair difference ofacceptability.
In other words, we wanted to explorewhether two formulations of a sentences with sim-ilar acceptability ratings were recalled equally welland whether two formulations of a sentence with dif-ferent acceptability ratings were recalled differently.Design: 32 students at the University of Cam-bridge were recruited (these are different partici-19pants from those in the acceptability experiment inSection 3.1, but were also all native speakers).
Wecreated four groups A?D, each with eight partici-pants.
Each Group saw 16 sentences in exactly oneof the two formulation types, such that groups A?Bformed one Latin square and C?D formed anotherLatin square.
These 16 sentences were interleavedwith 9 filler sentences that did not express causal-ity.
For each item, a participant was first showna sentence on the screen at the rate of 0.5 secondsper word.
Then, the sentence was removed from thescreen, and the participant was asked to do two arith-metic tasks (addition and subtraction of numbers be-tween 10 and 99).
The purpose of these tasks was toadd a load between target sentence and recall so thatthe recall of the target sentence could not rely oninternal rehearsal of the sentence.
Instead, researchsuggests that in such conditions recall is heavily de-pendent on whether the content and form was ac-tually comprehended (Lombardi and Potter, 1992;Potter and Lombardi, 1990).
Participants then typedwhat they recalled of the sentence into a box on thescreen.We manually coded the recalled sentences for sixerror types (1?6) or perfect recall (0) as shown inTable 1.
Further, we scored the sentences basedon our judgements of how bad each error-type was.The table also shows the weight for each error type.For any recalled sentences, only one of (0,1,5,6) iscoded, i.e., these codes are mutually exclusive, butif none of the positive scores (0,1,5,6) have beencoded, any combination of error types (2,3,4) canbe coded for the same sentence.4 Results4.1 Correlation between the two methodsThe correlation between the differences in accept-ability (using average z-scores for each formula-tion from the magnitude estimation experiment) andrecall scores (scored as described above) for the32 pairs of sentences was found to be significant(Spearman?s rho=.43; p=.01).
A manual inspec-tion of the data showed up one major issue regard-ing the methodologies: our participants appear topenalise perceived ungrammaticalities in short sen-tences quite harshly when rating acceptability, butthey have no trouble recalling such sentences ac-curately.
For example, sentence a. in Example 2below had an average acceptability score of 1.41,while sentence b. only scored .13, but both sentenceswere recalled perfectly by all participants in the re-call study:(2) a.
It is hard to imagine that it was the cause ofmuch sadness.b.
It is hard to imagine that because of it therewas much sadness.Indeed the sentence recall test failed to discrimi-nate at all for sentences under 14 words in length.When we removed pairs with sentences under 14words (there were eight such pairs), the correlationbetween the differences in magest and recall scoresfor the 24 remaining pairs of sentences was evenstronger (Spearman?s rho=.64; p<.001).Summary: The two methodologies give very dif-ferent results for short sentences.
This is becausecomprehension is rarely an issue for short sentences,while surface level disfluencies are more jarring toparticipants in such short sentences.
For longer sen-tences, the two methods correlate strongly; for suchsentences, magnitude estimations of acceptabilitybetter reflect ease of comprehension.
In retrospect,this suggests that the design of an appropriate load(we used two arithmetic sums) is an important con-sideration that can affect the usefulness of recallmeasures.
One could argue that acceptability is amore useful metric for evaluating NLG as it com-bines surface level fluency judgements with ease ofcomprehension issues.
In Siddharthan and Katsos(2010), we described how this data could be used totrain an NLG component to select the most accept-able formulation of a sentence expressing a causalrelation.
We now enumerate other characteristicsof magnitude estimation of acceptability that makethem useful for evaluating sentences.
Then, in Sec-tion 4.3, we discuss what further information can begleaned from sentence recall studies.4.2 Results of magnitude estimation studyDistinguishing between sentences: We foundthat magnitude estimation judgements are very goodat distinguishing sentences expressing the same con-tent.
Consider Table 2, which shows the average ac-ceptability for the n-best formulation of each of the20Weight Error Code Error Description+0.5 0 Recalled accurately (clauses A and B can be valid paraphrases, but the discourse con-nective (TYPE) is the same)+0.4 1 Clauses A and B are recalled accurately but the relation is reformulated using a differ-ent but valid discourse marker-0.25 2 The discourse marker has been changed in a manner that modifies the original causalrelation-0.5 3 Clause B (effect) recall error (clause is garbled)-0.5 4 Clause A (cause) recall error (clause is garbled)+0.25 5 Causal relation and A and B are recalled well, but some external modifying clause isnot recalled properly+0.25 6 Causality is quantified (e.g., ?major cause?)
and this modifier is lost or changed inrecall (valid paraphrases are not counted here)Table 1: Weighting function for error types.144 sentences (n=1?8).
We see that the best formu-lation averages .89, the second best .57 and the worstformulation -.90.
Note that it is not always the sameformulation types that are deemed acceptable ?
if wealways select the most preferred type (a caused b)for each of the 144 sentences, the average accept-ability is only .12.n = 1 2 3 4 5 6 7 8Av.
Z = .89 .57 .33 .13 -.12 -.33 -.58 -.90Table 2: Average acceptability for the nth best formula-tion of each of the 144 sentences.Testing hypotheses: In addition to distinguishingbetween different formulations of a sentence, vary-ing generation choices systematically allows us totest any hypotheses we might have about their ef-fect on acceptability.
Indeed, hypothesis testing wasan important consideration in the design of this ex-periment.
For instance, various studies (Clark andClark, 1968; Katz and Brent, 1968; Irwin, 1980)suggest that for older school children, college stu-dents and adults, comprehension is better for thecause-effect presentation, both when the relation isimplicit (no discourse marker) and explicit (with adiscourse marker).
We can then test specific predic-tions about which formulations are likely to be moreacceptable.H1 We expect the cause-effect information orderto be deemed more acceptable than the corre-sponding effect-cause information order.H2 As all four discourse markers are commonlyused in language, we do not expect any par-ticular marker to be globally preferred to theothers.We ran a 4 (discourse marker) x 2 (information or-der) repeated measures ANOVA.
We found a maineffect of information order (F1(1, 49) = 5.19, p =.017) and discourse marker (F1(3, 147) = 3.48, p= .027).
Further, we found a strong interaction be-tween information order and formulation type, F1(3,147) = 19.17, p<.001.
We now discuss what theseresults mean.Understanding generation decisions: The maineffect of discourse marker was not predicted (Hy-pothesis H2).
We could try and explain this em-pirically.
For instance, in the BNC ?because?
asa conjunction occurs 741 times per million words,while ?cause?
as a verb occurs 180 times per mil-lion words, ?because of?
140 per million words and?cause?
as a noun 86 per million words.
We mightexpect the more common markers to be judged moreacceptable.
However, there was no significant corre-lation between participants?
preference for discoursemarker and the BNC corpus frequencies of the mark-ers (Spearman?s rho=0.4, p>0.75).
This suggeststhat corpus frequencies need not be a reliable indica-tor of reader preferences, at least for discourse con-nectives.
The mean z-scores for the four discoursemarkers are presented in Table 3To explore the interaction between discoursemarker and information order, a post-ANOVATukey HSD analysis was performed.
The significant21Discourse Marker Average Z-scoreCause (verb) 0.036Because of 0.028Because -0.011Cause (noun) -0.028Table 3: Av.
z-scores for the four discourse markerseffects are listed in Table 4.
There is a significantpreference for using ?because?
and ?because of?
inthe effect-cause order (infix) over the cause-effectorder (prefix) and for using ?cause?
as a verb inthe cause-effect order (active voice) over the effect-cause order (passive voice).
Thus, hypothesis H1is not valid for ?because?
and ?because of?, wherethe canonical infix order is preferred, and thoughthere are numerical preferences for the cause-effectorder for ?cause?
as a noun we found support forhypothesis H1 to be significant only for ?cause?
as averb.
Table 4 also tells us that if the formulation is incause-effect order, there is a preference for ?cause?as a verb over ?because?
and ?because of?.
On theother hand, if the formulation is in the reverse effect-cause order, there is a preference for ?because?
or?because of?
over ?cause?
as a verb or as a noun.Summary: This evaluation provides us with someinsights into how generation decisions interact,which can be used prescriptively to, for example, se-lect a discourse marker, given a required informationorder.4.3 Results of sentence recall studyWhile magnitude estimation assessments of accept-ability can be used to test some hypotheses about theeffect of generation decisions, it cannot really teaseapart cases where there are surface level disfluenciesfrom those that result in a breakdown in comprehen-sion.
To test such hypotheses, we use the sentencerecall study.Testing hypotheses: Previous research (e.g., En-gelkamp and Rummer (2002)) suggests that recallfor the second clause is worse when clauses arecombined through coordination (such as ?therefore?or ?and?)
than through subordination such as ?be-cause?.
The explanation is that subordination bet-ter unifies the two clauses in immediate memory.We would expect this unification to be even greaterwhen the cause and effect are arguments to a verb.Thus, compared to ?because?, we would expect re-call of the second clause to be higher for ?cause?
asa verb or a noun, due to the tighter syntactic bindingto the discourse marker (object of a verb).
Likewise,compared to ?cause?, we would expect to see morerecall errors for the second clause when using ?be-cause?
as a conjunction.
Our hypotheses are listedbelow:H3 For ?cause?
as a verb or a noun, there will befewer recall errors in ?a?
and ?b?
comparedto ?because?
or ?because of?, because of thetighter syntactic binding.H4 For ?because?
as a conjunction, there will bemore recall errors in the second clause than inthe first clause; i.e., for ?b because a?, clause?a?
will have more recall errors than ?b?
and for?because ab?, clause ?a?
will have fewer recallerrors than ?b?.Table 5 shows the average incidence of each errortype per sentences in that formulation (cf.
Table 1).Note that the totals per row might add up to slightlymore than 1 because multiple errors can be codedfor the same sentence.Table 5 shows that ?because?
and ?because of?constructs result in more type 3 and 4 recall errorsin clauses ?a?
and/or ?b?
compared with ?cause?
aseither a noun or a verb.
This difference is significant(z-test; p<.001), thus supporting hypothesis H3.Further, for ?because?, the recall errors for thefirst clause are significantly fewer than for the sec-ond clause (z-test; p<.01), thus supporting hypoth-esis H4.
In contrast, for the cases with ?cause?
as averb or noun, both A and B are arguments to a verb(either ?cause?
or a copula), and the tighter syntacticbinding helps unify them in immediate memory, re-sulting in fewer recall errors that are also distributedmore evenly between the first and the second argu-ment to the verb.We make one further observation: passive voicesentences appear to be reformulated at substantiallevels (19%), but in a valid manner (type 1 errors).This suggests that the dispreference for passives inthe acceptability study is about surface level formrather than deeper comprehension.
This would be a22(a) Ordering EffectsMarker Preference p-valuebecause effect-cause (.12) is preferred over cause-effect (-.14) p<.001because of effect-cause (.13) is preferred over cause-effect (-.11) p<.001cause (verb) cause-effect (.12) is preferred over effect-cause (-.05) p=.0145cause (noun) cause-effect (.01) is preferred over effect-cause (-.11) p=.302(b) Discourse Marker EffectsOrder Preference p-valueeffect-cause ?because?
(.12) is preferred over ?cause (noun)?
(-.11) p<.001effect-cause ?because-of?
(.13) is preferred over ?cause (noun)?
(-.11) p<.001effect-cause ?because-of?
(.13) is preferred over ?cause (verb)?
(-.05) p=.001effect-cause ?because?
(.12) is preferred over ?cause (verb)?
(-.05) p=.002effect-cause ?cause (verb)?
(-.05) is preferred over ?cause (noun)?
(-.11) p=.839effect-cause ?because?
(.12) is preferred over ?because-of?
(.13) p=.999cause-effect ?cause (verb)?
(.13) is preferred over ?because?
(-.14) p<.001cause-effect ?
cause (verb)?
(.13) is preferred over ?because-of?
(-.06) p=.006cause-effect ?cause (verb)?
(.13) is preferred over ?cause (noun)?
(.01) p=.165cause-effect ?cause (noun)?
(.01) is preferred over ?because?
(-.14) p=.237cause-effect ?because-of?
(-.06) is preferred over ?because?
(-.14) p=.883cause-effect ?cause (noun)?
(.01) is preferred over ?because-of?
(-.06) p=.961Table 4: Interaction effects between information order and discourse marker (mean z-scores in parentheses; significanteffects in bold face).reasonable conclusion, given that all our participantsare university students.Summary: Overall we conclude that sentence re-call studies provide insights into the nature of thecomprehension problems encountered, and they cor-roborate acceptability ratings in general, and partic-ularly so for longer sentences.5 ConclusionsIn this paper, we have tried to separate out surfaceform aspects of acceptability from breakdowns incomprehension, using two offline psycholinguisticmethods.We believe that sentence recall methodologies cansubstitute for task based evaluations and highlightbreakdowns in comprehension at the sentence level.However, like most task based evaluations, recall ex-periments are time consuming as they need to beconducted in a supervised setting.
Additionally, theyrequire manual annotation of error types, thoughperhaps this could be automated.Acceptability ratings on the other hand are easy toacquire.
Based on our experiments, we believe thatacceptability ratings are reliable indicators of com-prehension for longer sentences and, particularly forshorter sentences, combine surface form judgementswith ease of comprehension in a manner that is veryrelevant for evaluating sentence generation or regen-eration, including simplification.Both methods are considerably easier to set upand interpret than online methods such as self pacedreading, eye tracking or neurophysiological meth-ods.AcknowledgementsThis work was supported by the Economic and So-cial Research Council (Grant Number RES-000-22-3272).ReferencesE.G.
Bard, D. Robertson, and A. Sorace.
1996.
Magni-tude estimation for linguistic acceptability.
Language,72(1):32?68.H.H.
Clark and E.V.
Clark.
1968.
Semantic distinctionsand memory for complex sentences.
The QuarterlyJournal of Experimental Psychology, 20(2):129?138.23type err0 err1 err2 err3 err4 err5 err6b because a 0.62 0.18 0.02 0.10 0.18 0.00 0.05because ab 0.80 0.03 0.03 0.20 0.10 0.00 0.00b because-of a 0.78 0.11 0.02 0.06 0.07 0.00 0.06because-of ab 0.73 0.00 0.00 0.17 0.17 0.10 0.00a cause-of b 0.89 0.04 0.06 0.00 0.00 0.00 0.07cause-of ba 0.75 0.06 0.04 0.06 0.08 0.00 0.06a caused b 0.83 0.05 0.02 0.03 0.03 0.07 0.00b caused-by a 0.77 0.19 0.00 0.00 0.04 0.00 0.02Table 5: Table of recall errors per type.W.
Cowart.
1997.
Experimental Syntax: applying objec-tive methods to sentence judgement.
Thousand Oaks,CA: Sage Publications.A.T.
Duchowski.
2007.
Eye tracking methodology: The-ory and practice.
Springer-Verlag New York Inc.J.
Engelkamp and R. Rummer.
2002.
Subordinat-ing conjunctions as devices for unifying sentences inmemory.
European Journal of Cognitive Psychology,14(3):353?369.Rudolf Flesch.
1951.
How to test readability.
Harperand Brothers, New York.A.D.
Friederici.
1995.
The time course of syntactic ac-tivation during language processing: A model basedon neuropsychological and neurophysiological data.Brain and language, 50(3):259?281.A.
Gatt, A. Belz, and E. Kow.
2009.
The tuna-reg chal-lenge 2009: Overview and evaluation results.
In Pro-ceedings of the 12th European workshop on naturallanguage generation, pages 174?182.
Association forComputational Linguistics.J.W.
Irwin.
1980.
The effects of explicitness and clauseorder on the comprehension of reversible causal re-lationships.
Reading Research Quarterly, 15(4):477?488.E.W.
Katz and S.B.
Brent.
1968.
Understanding connec-tives.
Journal of Verbal Learning & Verbal Behavior.F.
Keller, S. Gunasekharan, N. Mayo, and M. Corley.2009.
Timing accuracy of web experiments: A casestudy using the WebExp software package.
BehaviorResearch Methods, 41(1):1.Frank Keller.
2000.
Gradience in Grammar: Experimen-tal and Computational Aspects of Degrees of Gram-maticality.
Ph.D. thesis, University of Edinburgh.I.
Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sen-tence generator.
In Proceedings of the 12th Interna-tional Natural Language Generation Workshop, pages17?24.
Citeseer.R.
Likert.
1932.
A technique for the measurement ofattitudes.
Archives of psychology.L.
Lombardi and M.C.
Potter.
1992.
The regeneration ofsyntax in short term memory* 1.
Journal of Memoryand Language, 31(6):713?733.E.
Pitler, A. Louis, and A. Nenkova.
2010.
Automaticevaluation of linguistic quality in multi-documentsummarization.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 544?554.
Association for ComputationalLinguistics.M.C.
Potter and L. Lombardi.
1990.
Regeneration in theshort-term recall of sentences* 1.
Journal of Memoryand Language, 29(6):633?654.Advaith Siddharthan and Napoleon Katsos.
2010.
Refor-mulating discourse connectives for non-expert readers.In Proceedings of the 11th Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-HLT 2010), Los Ange-les, CA.A.
Siddharthan, A. Nenkova, and K. McKeown.
2011.Information status distinctions and referring expres-sions: An empirical study of references to peo-ple in news summaries.
Computational Linguistics,37(4):811?842.Advaith Siddharthan.
2006.
Syntactic simplification andtext cohesion.
Research on Language and Computa-tion, 4(1):77?109.S.
Sripada, E. Reiter, and I. Davy.
2003.
SumTime-Mousam: Configurable marine weather forecast gen-erator.
Expert Update, 6(3):4?10.W.L.
Taylor.
1953. ?
cloze procedure?
: a new tool formeasuring readability.
Journalism Quarterly; Jour-nalism Quarterly.Jette Viethen and Robert Dale.
2006.
Algorithms forgenerating referring expressions: do they do what peo-ple do?
In Proceedings of the Fourth InternationalNatural Language Generation Conference, pages 63?70.P.
Wolff, B. Klettke, T. Ventura, and G. Song.
2005.Expressing causation in English and other languages.Categorization inside and outside the laboratory: Es-says in honor of Douglas L. Medin, pages 29?48.24
