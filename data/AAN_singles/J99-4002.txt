Lexical Rules in Constraint-basedGrammarsTed Br iscoe*University of CambridgeAnn Copestake  tCSLI, Stanford UniversityLexical rules have been used to cover a very diverse range of phenomena in constraint-basedgrammars.
Examination of the full range of rules proposed shows that Carpenter's (1991) postu-lated upper bound on the length of list-valued attributes uch as SUBCAT in the lexicon cannot bemaintained, leading to unrestricted generative capacity in constraint-based formalisms utilizingHPSG-style lexical rules.
We argue that it is preferable to subdivide such rules into a class ofsemiproductive lexically governed genuinely lexical rules, and a class of fully productive unarysyntactic rules.We develop arestricted approach to lexical rules in a typed default feature structure (TDFS)framework (Lascarides et al 1995; Lascarides and Copestake 1999), which has enough expressivityto state, for example, rules of verb diathesis alternation, but which does not allow arbitrarymanipulation of list-valued features.
An interpretation ofsuch lexical rules within a probabilisticversion of a TDFS-based linguistic (lexical and grammatical) theory allows us to capture thesemiproductive nature of genuinely lexical rules, steering an intermediate course between fullygenerative or purely abbreviatory ules.We illustrate the utility of this approach with a treatment of dative constructions withina linguistic framework that borrows insights from the constraint-based theories: HPSG, UCG,(Zeevat, Klein, and Calder 1987) and construction grammar (Goldberg 1995).
We end by outlin-ing how our approach to lexical rules allows for a treatment of passive and recursive affixation,which are generally assumed to require unrestricted list manipulation operations.1.
IntroductionIn lexicalist approaches to grammar, lexical rules are a crucial component of the overalltheory and more and more generalizations have come to be stated within them.
Forexample, in the development of HPSG (Pollard and Sag 1987, 1994) from GPSG (Gaz-dar et al 1985) several syntactic metarules concerning the location of empty categoriesin unbounded ependency constructions are restated as lexical rules.
One example isthe Subject Extraction rule, which licenses ubject extractions from sentential comple-ments.
Inflectional morphological rules, such as Plural Formation, and rules of verbalternation, such as Passive, are also stated as lexical rules by Pollard and Sag.
Recently,Bouma and van Noord (1994) have proposed a lexical rule of Adjunct Introduction,which can recursively add adverbial categories to the SUBCAT list of a verbal category.There are three main problems with the treatment of lexical, or what might be bettertermed unary, rules as a homogeneous class.
* Computer Laboratory, University of Cambridge, Pembroke Street, Cambridge CB2 3QG, UK.
E-maihejb@cl.cam.ac.ukt Center for the Study of Language and Information, Stanford University, Ventura Hall, Stanford,CA 94305.
E-mail: aac@csli.stanford.edu(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 4Firstly, Carpenter (1991) demonstrates that if lexical rules are able to perform ar-bitrary manipulations (deletion, addition, and permutation) of potentially unboundedlists, any recursively enumerable language can be generated, even if the nonderivedlexicon and grammar only generate context-free languages.
However, once we arecommitted to treating rules such as Passive and Adjunct Introduction in a homoge-neous way, restrictions that prevent lexical rules from increasing enerative capacity,such as constraining the use of category variables, bounding the length of the suB-CAT list, or limiting recursive application, cannot be imposed.
Subdividing lexical andunary syntactic rules allows such restrictions to be naturally maintained in the lexicon.Secondly, within theories like HPSG, which utilize constraint logics over (typed)feature structures ((T)FSs) as the lexical and grammatical representation language,the formal status of lexical rules is unclear.
They do not have a straightforward in-terpretation as logical constraints and are normally treated as metalevel conditionalgeneralizations concerning the set of admissible lexical entries.
This leads to immedi-ate disadvantages, such as nondeclarativity, potential nontermination i application,and the need for the grammar writer to specify what stays unchanged as well as whatchanges in a derived entry.
Our approach to lexical rules improves this situation byformalizing them in terms of default unification utilizing existing operations in thetyped default feature structure (TDFS) representation language (Lascarides et al 1995;Lascarides and Copestake 1999).Thirdly, most lexical, though not unary, rules are semiproductive.
In HPSG, lexi-cal rules are interpreted as generative rules encoding putatively exceptionless condi-tional generalizations concerning the existence of derived lexical entries.
Exceptionsand irregularities must be marked explicitly in lexical entries.
While this approach isdefensible for rules of inflectional morphology, generalizations about unbounded e-pendency constructions and adjunct introduction, it is less plausible when we come toconsider morphological rules of derivation or most verb diathesis alternations.
Theselatter are semiproductive; that is, subject o blocking (preemption by synonymy or bylexical form), to arbitrary lexical gaps, and to varying degrees of conventionalization(see Jackendoff \[1997a\] for a recent discussion).
It is technically possible to reinterpret(some) lexical rules as (lexical) redundancy rules using constraint-based techniques(e.g., Sanfilippo 1993).
However, this approach reduces lexical rules to a purely abbre-viatory device even more thoroughly than Jackendoff's (1975) original proposal.
Wewill argue in Section 7 that the pure redundancy rule interpretation is not optimal,even for semiproductive l xical rules, as such rules are utilized in the production andinterpretation of nonce usages and neologisms.Throughout the paper, we assume a linguistic framework based on typed featurestructures like HPSG, but allowing for defeasible specification i  typed default featurestructures (e.g., Lascarides et al 1995).
We distinguish between lexical rules and otherunary rules and present an account of lexical rules compatible with this frameworkand, potentially, with other constraint-based (T)FS frameworks.
We argue that thisaccount satisfactorily addresses the issues of generative power, formal interpretation,and semiproductivity.
In Section 2 we discuss the use and interpretation of HPSG-stylelexical rules in more detail.
In Section 3 we present he TDFS framework for lexicaland grammatical representation, which extends monotonic theories of lexical organi-zation with default inheritance and defeasible specification.
In Section 4, we formalizelexical rules in terms of default unification and demonstrate hat this leads to a morerestricted operation fully defined in terms of the nonmonotonic conditional logic ofTDFSs.
In Section 5 we show that, despite this restricted capacity, a linguistically in-sightful account of English dative constructions can be formulated, which treats themas (mostly) deri, ed by lexical rule.
This draws on insights from Dowty's (1989) theory488Briscoe and Copestake Lexical RulesbasePHON: \[\]3RDSNG : \[\]SYN: fLOC: \[SUBCAT: \ [ \ ] \ ] \ ]  ~-~SEM CONT : \[\] \]Figure 1HPSG Third Singular Verb Formation lexical rule.3rdsngPHON: f3rdsng( \[\].
\[\] )SYN: \[LOC: \[SUBCAT : \ [ \ ] \ ] \ ]SEM: \[CONT : \[\]\]basepHON \] 3rdsng?
\[\] L PHON: f3rdsng( \[\] , \[\]) \[ 3RDSNG : \[-fl ~ \]Figure 2Abbreviated form of the Third Singular Verb Formation lexical rule.of proto-roles, Sanfilippo's (1990, 1992, 1993) treatment of verb alternations in UCG(Zeevat, Klein, and Calder 1987) and Goldberg's (1995) analysis in Construction Gram-mar.
In Section 6 we argue that the dative alternation is a semiproductive process inEnglish not fully reducible to an abbreviatory convention, and provide an accountof lexical rule application within a probabilistic version of the TDFS framework thatcaptures the variable acceptability of different verbs in the dative construction.
Thus,we modify and extend the analysis of dative in Lascarides et al (1995) by providinga more adequate and restrictive formalization of this type of semiproductive l xicalrule.
In Section 7 we consider the extent o which our approach to lexical rules willgeneralize to other types of putatively lexical processes.2.
Lexical Ru les  in HPSGLexical rules in HPSG are interpreted as conditional relationships between lexical en-tries represented as constraints on TFSs (e.g., Pollard and Sag 1994, p. 395, n. 1).
Sincelexical entries themselves are FS descriptions, not FSs per se, this interpretation makeslexical rules metalevel statements in an informally defined language.
For example, thelexical rule for Third Singular Verb Formation (Pollard and Sag 1987, 210-213) is givenin Figure 1.
In this notation, coindexation must be interpreted as a copying operatorand not as a specification of reentrancy within a single TFS description, and the de-scription language must be extended with a conditional implication operator elatingFSs.
A linguistic infelicity of this notation is that it requires explicit specification ofwhat is unchanged under lexical rule application as well as of the changes to the de-rived entry.
The informal interpretation of such a rule is that for lexical entries thatunify with the antecedent description, a derived entry can be created by copying ele-ments as specified into the consequent description.
However, in the HPSG literature,these explicit copying statements are often left unstated, with the notation being in-terpreted to mean that features that are omitted have their values copied over, givingthe notation shown in Figure 2.A number of modifications of this original proposal within the HPSG frameworkhave been proposed.
Copestake and Briscoe (1992) and Copestake (1992) representlexical rules as TFSs containing 1 and 0 attributes representing input and output de-scriptions of the lexical rule, respectively.
This enables lexical rules to be encoded ina type hierarchy and for relationships between rules to be expressed via type inher-itance.
The interpretation of lexical rules is analogous to that of grammar ules (e.g.,Shieber 1992), and such rules can be thought of as equivalent to unary grammar ules.489Computational Linguistics Volume 25, Number 43rdsng-lrIN: \[base\] \[ 3rdsn~: \]OUT : | PHOI~ :f3rdsng( \[\], \[\])\[ 3RDSNG: \[\]Figure 3Reformulated Third Singular Verb Formation lexical rule.Calcagno (1995) develops an algorithm for improving the notation for lexical rules byeliminating the need to specify what is copied from input to output.
Meurers (1995)also develops a similar algorithm but, although e augments the description languageto allow lexical rules to be written in this abbreviated notation, he interprets the resultas a relational constraint on lexical entries.
1 Thus, in Meurers' notation, the rule inFigure 1 would be written without coindexation as a TFS using IN/OUT attributes asin Figure 3, and appropriate coindexation would be algorithmically generated fromboth the rule specification and the type system to recover an expanded rule effec-tively equivalent to that in Figure 1 (though the entire values of SYN and SEM wouldbe carried over).
In Meurers' formulation the coindexation generated is interpreted asgenuine reentrancy since the lexicon is defined utilizing "junk slot" encoding (Ait-Kaci1984) of the set of possible words, as shown below:word --* L1 V .-- V Ln V \[\] STORE : < \[OUT : \[\]\] >This constraint states that a word must be subsumed by one of the basic descriptionsor by a description derived via one or more lexical rule applications (i.e., any de-scription tagged \[\] ).
This latter step integrates the interpretation f lexical rules intothe underlying constraint logic of TFS descriptions by structure sharing informationbetween descriptions of basic and derived lexical entries.In view of the unrestricted generative power of conventional HPSG-style lexicalrules (Carpenter 1991), naive generative application of recursive or cyclic rules canlead to nontermination during parsing.
Bouma and van Noord (1994) and Johnsonand Dorre (1995) propose techniques for delayed evaluation of lexical rules so thatthey apply "on demand" at parse time.
Meurers and Minnen (1997) present a compu-tational framework for efficient application of Meurers' (1995) formalization of lexicalrules.
In their covariation approach, a finite-state machine for the application of lexi-cal rules is derived by computing possible "follow" relations between the set of rules.Next, pruned finite-state machines are associated with classes of actual exical entriesrepresenting the restricted set of rules that can apply to those entries.
Finally, entriesthemselves are extended with information common to all their derived variants.
Thesetechniques achieve most of the advantages of lexicon expansion i  the face of recursiverules and cyclic rule interactions that preclude a full off-line expansion.Since their inception, the productivity of some types of lexical rule has been anissue (e.g., Jackendoff 1975).
Given the interpretations discussed above, lexical rulespredict hat derived entries will exist without exception for any basic entry subsumedby a lexical rule input description.
Pollard and Sag (1987, 210f.)
treat the blockingof regular rules of morphological ffixation by making the application of affixation1 Riehemann (1993) was the first to propose this interpretation of lexical rules, though she chose torepresent them directly in the type inheritance hierarchy.490Briscoe and Copestake Lexical Rulesfunctions ensitive to specification of irregular affixes in base entries.
Thus the rule ofThird Singular Verb Formation in Figure 1 contains a function f3rdsng that combinesthe value of the attribute 3RDSNG with that of PHON to create the PHON of the out-put sign.
If the base sign specifies an irregular form for third singular the functionwill simply return this form as opposed to the form produced by regular suffixationwith -s. However, preemption by synonymy with an irregularly derived lexical en-try is only one form of semiproductivity exemplified by lexical rules.
Preemption bysynonymy can also apply in cases where the blocking form is basic and does notinvolve affixation, such as glory blocking *gloriosity (see Aronoff \[1976\]).
Identity oflexical form between a derived and underived entry or a more irregular derived entrycan serve to block the more regular derived entry, as with sticker, which has a highlypreferred meaning derived via object, as opposed to subject, -er nominalization (com-pare bumper sticker to ?a sticker of offensive posters to billboards).
There are many othertypes of semiproductivity, such as that created by "accidental" gaps or idiosyncraticexceptions, for example, ?thinkable (compare unthinkable), as well as cases whose statusas "systematic" exceptions or gaps is unclear.
For example, verbs such as cost, weigh,or last, which take measure phrase NP objects do not undergo Passive.
These shouldprobably be blocked from undergoing the lexical rule by enriching its structural inputwith restrictions on the semantic nature of the object NP.
However, symmetric predi-cates, such as resemble and equal, which also appear not to passivize asily, are a morecomplex case (e.g., Wasow 1980).
Firstly, there are symmetric predicates that passivizerelatively easily, such as meet--Kim was usually met by someone from the Russian Embassyat the safe house.
Secondly, it is possible to create acceptable xamples for the verbsstandardly cited in the literature--The difficulty of the solution was only equalled by theobduracy of the research team.
And thirdly, there is a general pragmatic reason to believePassive might be (largely) redundant with symmetric predicates since in most usesthe subject and object can be reversed without changing meaning.
Nevertheless, theformulation of Passive given in Pollard and Sag (1987, 215) does not address eithertype of exception (under the assumption that these verbs are all of type transitive).Jackendoff (1975) and others have proposed that lexical rules be interpreted asredundancy statements hat abbreviate the statement of the lexicon but that are notapplied generatively.
This conception of lexical rules has been utilized in a constraint-based framework by Sanfilippo (1993) who adds a list-valued attribute to the inputdescription of a lexical rule that encodes the name of the lexical rule, so that the ruleinput specification will only unify with lexical entries that also specify the rule name asa member of this list-valued attribute.
We illustrate this approach in slightly modifiedform with respect o the causative-inchoative verb alternation i Figure 4.
2 The effectis that a verb will explicitly specify the alternations that apply to it, as values for thefeature ALTS, out of a set of alternations that are possible for its type, and that lexicalrules may only be applied when the lexical sign has the appropriate value for thatalternation i stantiated.In Sanfilippo's approach, a constraint-based encoding of categorial grammar (e.g.,Zeevat, Klein, and Calder 1987) is combined with Dowty's proto thematic role theoryand proto-roles are interpreted as predicates holding between event and participantvariables in a neo-Davidsonian semantic representation.
The version of the rule givenassumes proto-roles appropriate for movement verbs and could be used to relate lexicalentries for gallop as in John galloped the horse / The horse galloped.
For convenience here2 Subscripted coindexing is used to abbreviate the path to the semantic index of the verbal argumentcategory.491Computational Linguistics Volume 25, Number 4intrans-verbPHON: \[\]SYN r RESULT : ssign \]: \[ ACTIVE: npsign ?
Jr .
?
~ r v-a~t-cause-move 1 SEM .
< verD-reJ .
~V~NT: e\[EVENT:e\] \[ARG:  \] >L ALTS: \[\] \[ TRANS-ALT : caus-inchoat \]Figure 4?
trans-caus-verbPHON: \[\]SYN \[RESULT \[ RESUcw:ssign \ ] \ ]  : : ACTIVE npslgn mACTIVE npsign \[~r p'agt'cause \] \[ p'pat'm?ve \] .SEM :< \[,verb'relEvENT:e\]j.
|EVENT:e EVENT:e />LARG: \[~ ARG: \[\] J?
ALTS : \[\]The Causative-Inchoative lexical rule: Sanfilippo's approach.we are using an abbreviated version of the "minimally-recursive" style of encodingfor the semantics (MRS) described by Copestake t al.
(1995).
The semantics for thecausative form of gallop described is equivalent in linearized notation to:\[gallop(e) A p-agt-cause(e, x) A p-pat-move(e, y)\]However, the rule can only apply if the transitive ntry for gallop specifies caus-inchoatas the value of ALTS TRANS-ALT.
An immediate problem arises, because as Pinker (1989)and others have argued, the rule is semiproductive rather than purely abbreviatory,in the sense that nonce usages are clearly interpreted conventionally as being novel(mis)applications of such rules.
As in for example, Kim subscribed his friend to Byte for ayear or Don't fall my dolly over!Although the proposals reviewed considerably clarify and enrich the original con-ception of lexical rules in HPSG, problems remain.
The generative power of lexicalrules places no limit on possible variations in derived words.
A more restricted lexicalrule formalism would encourage finer-grained istinctions between rule classes anddevelopment of substantive criteria for characterizing a rule as lexical (as opposed tounary syntactic).
It would also allow an interpretation of lexical rules more conso-nant with constraint-based frameworks.
Neither the interpretation of lexical rules asfully generative nor as purely abbreviatory is adequate.
Although many lexical rulesare subject o exceptions, gaps, and variable degrees of conventionalization, most aresemiproductive b cause they play a role in the production and interpretation of nonceusage, errors, and neologisms.3.
The Typed Default  Feature Structure Formal ismOur alternative approach to lexical rules assumes the use of typed default feature struc-tures as the basic data structure.
The TDFS formalism extends typed feature structureformalisms, such as those described in Carpenter (1992) or King (1994), by allowingfor structures that include default information.
These are combined by typed (persis-tent) default unification.
In most previous accounts (see, for example, Bouma \[1992\],Carpenter \[1993\], Copestake \[1993\], and Russell et al \[1993\]), default unification isan asymmetric operation that combines two ordinary (T)FSs, one of which is treated492Briscoe and Copestake Lexical Rulesas default and one nondefault, o produce a normal TFS.
In contrast, TDFSs markdefault information explicitly within each structure, and are combined with a sym-metric operation, which retains the information about the defeasibility of particularpieces of information.
There are two advantages ofthe TDFS framework over previousapproaches that are important for our treatment of lexical rules:..Unlike asymmetric default unification, the unification operation onTDFSs is order-independent.
Thus inheritance hierarchies may be definedin which some inherited information may be overridden by more specificinformation, without he results being dependent on evaluation order.Structures can be represented ashaving persistent defaults: in particular,parts of the semantics of lexical signs can be marked as defeasible, in away that allows the information to be overridden either by sentential ordiscourse context.Lascarides et al (1995) provide an initial formalization of the TDFS framework,Lascarides and Copestake (1999: henceforth L&C) describe the version of default uni-fication assumed in the informal outline of the TDFS formalism that follows.3.1 Asymmetric Default UnificationIn L&C, a symmetric unification operation on TDFSs is defined in terms of an asym-metric default unification operation, so we begin by describing the latter operation.We will also use a variant of this operation in our formalization of lexical rules inSection 4.
Carpenter (1993) gives an elegant definition of asymmetric default unifica-tion of untyped feature structures in terms of maximal incorporation of information.Because of conflicts in defaults, this may give rise to multiple extensions.
There aretwo alternative approaches to this: the operation can either return all extensions asa disjunction or return their generalization.
This gives unification-based analogues ofcredulous and skeptical default logics.
Here and below, we use M to represent unifica-tion (i.e., conjunction of information), _ for subsumption (i.e., is more general than),T for the most general type, and K to indicate inconsistency.<Definition: Credulous Default Unification (he)The result of credulously adding the default information i  G to the strict informationin F is given by:FMcG = {Fm G' : G' 3 G is maximally specific such that F M G' is defined}where m is ordinary unification.Definition: Skeptical Default Unification (Ms)FMsG = U{F M G' : G' 3 G is maximally specific such that F m G' is defined}The result is the generalization (u) of the result of the credulous default unificationoperation.Figure 5 gives an example (here and in the examples below we assume that a, b,and c are pairwise incompatible and pairwise generalize to T).
Carpenter makes use493Computational Linguistics Volume 25, Number 4IF:T\] ~?
G: H':?\[~;=:aT\] ~8 \[F:c ~\]b\]H' = /H:c \]\[F:'a-rFigure 5Credulous and skeptical asymmetric default unification.of a characterization f FSs as sets of atomic feature structures: that is, FSs that consisteither of a single path with an associated atomic value (which would correspond to atype in a TFS framework), or of a pair of (possibly identical) paths that are shared.
AnyFS can be described in terms of the set of atomic FSs that are strictly more general thanitself.
Carpenter's definition of default unification can thus be understood in terms ofincorporation of maximal subsets of the set of atomic FSs into which the default FScan be decomposed.3.2 Well-formedness Constraints< <Because Rs and ~c are ultimately defined in terms of monotonic subsumption, we cancreate variant definitions corresponding to variant notions of subsumption.
This isparticularly relevant when we come to consider typed rather than untyped FSs.
Wecan consider the subsumption ordering on well-formed typed feature structures, wherewell-formed TFSs are a subset of TFSs in general.
There are a variety of proposals forwell-formedness conditions for typed FSs: here we assume conditions on appropriatefeatures based on Carpenter (1992).
We will not give these in detail here, but briefly,we assume that every type, t, has a set of appropriate features associated with it,AppFeat(t).
For a TFS F to be well-formed, every node, q, in the structure must havean associated type, t, and the features labeling the edges that come out of the node qdirectly must be equal to AppFeat(t).
For example, if the type ne-list has the appropriatefeatures HD and TL, and has no subtypes, then the FS \[SYN: intrans\] will not unifywith an FS of type ne-list.So, assume ~t indicates the partial order in the collection of well-formed TFSs,and ~t indicates the corresponding reatest lower bound, and so on.
Then we can< <define ~t, a straightforward variant on Carpenter's %, but defined on typed FSs.Definition: Skeptical Typed Default Unification (Rt)The result of skeptically adding the default information in the TFS G to the strictinformation in the TFS F is given by:FRtG = t.3t{F Rt G ~ : G' ~t G is maximally specific such that F Rt G ~ is defined}In what follows, we will omit the subscript t and in general assume well-formednessof TFSs.3.3 Symmetric Default UnificationAs we mentioned above, we want to make a distinction between default and nonde-fault information in a typed default FS (TDFS), and to allow the default informationto "persist" through a series of default unification operations.
This is done by makinga TDFS be a dual structure, of which the first component is a standard TFS indicatingnondefault information and the second is a structure known as a tail.
A tail is a set494Briscoe and Copestake Lexical Rulesof pairs, each consisting of an atomic FS and a type that indicates pecificity.
AtomicFSs associated with more specific types in the tail will be preferred over those asso-ciated with more general types in the case of conflict.
As L&C discuss in detail, ifwe want to maintain associativity of default unification (in order to guarantee order-independence) we must maintain some of the unification "history."
The tail does thisand may thus include conflicting elements that were introduced by different TDFSs.However, the atomic FSs in the tail are required to be consistent with the indefeasibleTFS.
We use the notation I/T to indicate a TDFS with indefeasible component, I, andtail, T. An example of a TDFS is shown in (1).
(1)<>The default unification operation, 9 ,  combines two TDFSs.
It simply involvestaking the unification of their indefeasible components, and the union of their tailswith all elements inconsistent with the indefeasible result removed.
The followingdefinition of the operation is slightly modified from L&C (pfs(T) is an operation thatextracts the atomic FS components of tail elements):<>Definition: Symmetric Default Unification RLet F1 =aei I1/T1 and F2 =aef I2/T 2 be two TDFSS, and let F12 ----eel F1R>F2.
Furthermore,assume F12 ---~ef I12/T12.
Then/12 and T 12 are calculated as follows:1..The Indefeasible Part:/12 ---- 11 R 12That is, the indefeasible TFS is the unification of the indefeasible parts ofthe arguments.The Tail T12:T 12 =eel Filter(I12, (T 1 U T2))where Filter(I12, T) includes only the elements of T where the atomic FSp/s(T) is compatible with/12.
That is:Filter(F, T) = {T' E T : Pds(T') R F  ?
}Of course, it will sometimes be necessary to know which default information"wins."
That is, we require an operation, DefFS, that will give us a consistent defaultTFS, given the tail and the indefeasible structure.
In the case of conflicts, the result ofDefFS is dependent on the types that introduced the default information, since infor-mation associated with more specific types is preferred over that from more generaltypes (the analogue of the "penguin principle").
L&C therefore define the notion of aspecificity partition of a tail, such that all the elements which are maximally specific(according to the partial order of their types in the type hierarchy) are in the firstpartition, all the next most specific elements are in the second partition, and so on.That is, the partition of a tail T is (#1 .
.
.
.
.
#n),  where the types associated with the tailelements in #1 are strictly more specific than the types in/*2, and so forth.DefFS proceeds by using credulous asymmetric default unification to combine allthe possible elements of the most specific partition with the indefeasible structure,495Computational Linguistics Volume 25, Number 4then incorporating the next most specific partition into the result of this, and so on.Each step may result in multiple TFSs.
Incorporation of information is defined by the< <credulous default unification operation, ncs, which is a slight modification of %, asdefined by Carpenter, because it has to allow for the nondefault argument being aset of TFSs and the default argument being a set of atomic FSs that may be mutuallyincompatible.<Definition: Credulous Default Unification Rcs on SetsLet 3vl be a set of TFSS {F1,...,Fn}, and 02 a set of atomic Ass.
Then< < <~1Rcs 02 = {F1Rca 02 .
.
.
.
, Fn %a 02 }where<F1Flca{G1 .
.
.
.
.
Gn} = {F1 R F2 : F2 is the unification of a maximal subset of{G1 .
.
.
.
.
Gn} such that F1 R F2 is defined}Definition: The Operation DefFSLet F be a TDrS I/T.
ThenDey~S(F)  : 11( ( I4cs~fs(~l)  )Rcs .
.
.
~cs~3fs(#n) )So multiple TFSs may result from the credulous asymmetric default unificationsteps, but a final generalization operation takes only the information that they allhave in common.
Thus, in the absence of type precedence, conflicting defaults aretreated skeptically overall.In the example below, we illustrate DefFS on the TDFS shown in (1).
On the as-sumption that the type t is strictly more specific than t', which is strictly more specificthan t' ,  each partition in the tail has a single element.
In the example, the tail elementsare shown in specificity order.
The most specific will unify with the indefeasible struc-ture, but the other elements conflict with the result (on the assumption that a, b, andc are incompatible).t{(\[F: \[G : b\]\])t/,<EF )= : :bThe tail notation is somewhat cumbersome, especially since we are often concernedwith TDFSs where the default information is mutually consistent.
We can adopt anabbreviated notation for cases where there are no conflicts in which default information(i.e., the atomic FSs in the tail) is indicated by means of a slash notation, following theappropriate path in the indefeasible structure.
For example, the TDFS shown in (2a)is represented in the abbreviated notation as shown in (2b).
(2)496Briscoe and Copestake Lexical RulesI' r / al\]L H : /~We will use this abbreviated notation in the examples that follow.
We refer the readerto L&C and Lascarides et al (1995) for formal specification and detailed motivationof the TDFS formalism.
33.4 Persistent DefaultsSymmetric default unification can be used to extend the functionality of any linguisticframework based on, or embedded in, TFSs in two ways.
Firstly, the organizationof the lexicon can be extended naturally to allow for default inheritance networksof constraints.
Default inheritance has been widely exploited in lexical descriptionsboth within and outside the TFS framework (e.g., Daelemans, de Smedt, and Gazdar1992).
However, the TDFS approach extends formalized and implemented proposalsfor default inheritance, such as DATR (Evans and Gazdar 1989, 1996) or any versionbased on asymmetric unification, in that the lexicon now consists of a set of TDFSs, asopposed to a set of (T)FSs; that is, it permits default specification to persist "beyondthe lexicon" for exploitation during syntactic, semantic, or discourse level operations.This is the most significant property of the TDFS framework for our use of lexicalrules, so we will give a simple example of its potential utility.Figure 6 gives a possible lexical entry for climb that exploits persistent defaultspecification to capture the defeasible nature of aspects of the meaning of this verb.
Inthis sense and pattern of realizations, climb can combine with a locative PP specifyingthe goal and/or  directional path of movement.
By default, the directional path isspecified to be upwards.
The preposition heading the PP may strengthen, override, orleave default this specification, as illustrated in (3).
(3) a. John climbed up the shaftb.
John climbed along the shaftc.
John climbed around the shaftd.
John climbed through the shafte.
John climbed down the shaftIn addition, lexical rules that create an intransitive form of the verb by removingthe requirement for the PP arguments must leave the default direction specificationunmodified, and in all cases the lexically specified default remains defeasible duringdiscourse processing in a context with which it is inconsistent, as illustrated in (4).
(4) a.
The shaft was very deep.
John climbed for several hours, beforereaching the bottom.b.
The shaft had two inspection hatches: the first was five feet above thesecond.
John climbed through the shaft entering from the first inspectionhatch and exiting via the second.3 Note, however, that although L&C conjecture that DefFS is worst-case exponential, Malouf (1999) hasdeveloped an algorithm that is efficient even for cases where the tail is large in proportion to theindefeasible structure and where complex reentrancies are involved.
Malouf's results o far suggest thatpractical complexity varies from linear to quadratic, and that, overall, recoding amonotonic system touse defaults can improve performance, chiefly because it reduces the complexity of the type hierarchy.497Computational Linguistics Volume 25, Number 4I transitive- -verbPHON: chPmPb| .
.
.
.
\[ climb-rel 1 r p-agt-move ~ ~ direction-rel/up-rel 1/a  .. .
.
.
< \[EVENT:el' |EVENT:e \].
|EVENT:e j >L J LARG:x J LARG'yFigure 6Lexical entry for climb.The fact that defeasible components of meaning interact subtly with both lexical rulesaffecting rammatical realization and discourse context supports a framework in whichdefeasible semantic specifications can be explicitly represented at the lexical level.Copestake and Briscoe (1995) argue at some length for this position, and Lascaridesand Copestake (1995) and Lascarides et al (1995) show how lexical defaults interactappropriately with nonmonotonic discourse reasoning within the formal frameworkof DICE (Lascarides and Asher 1991, 1993).4.
Lexical Rules in the TDFS FormalismLexical rules differ from (default) inheritance in several ways.
Firstly, a lexical rulerelates lexical entries whose types are unordered, and secondly, such rules often ap-ply recursively.
Nevertheless, there are similarities between default unification andthe process of creating a new lexical entry, conditionally, given the presence of an-other entry satisfying certain properties.
In particular, asymmetric default unificationpresents itself as a way of implementing the requirement that information shouldcarry over from input to output description in a lexical rule, provided that it is con-sistent with the output type and any further constraints pecified in the rule.
Fur-thermore, if we wish to express lexical rules in terms of the TDFS formalism outlinedin Section 3, it makes sense to explore the utility of the nonmonotonic componentdeveloped.Asymmetric default unification can provide the foundation for the carrying overof consistent information between input and output descriptions in lexical rules.
Forexample, the second formulation of the rule of Third Singular Verb Formation (withoutcoindexation) given in Section 2 can be reinterpreted as an application of asymmetricdefault unification, as illustrated in Figure 7 (an explicit encoding of function appli-cation has been introduced for clarity).
Under this new interpretation of the notation(and assuming for the moment that the structures have empty tails), if the inputdescription subsumes a lexical structure, then the result is given by asymmetricallydefault unifying the lexical structure with the output description.
The effect is that in-formation in the input that is inconsistent with the output description is lost, whetherbecause it conflicts with a path specification or because the path itself is inconsistentwith the output ype.
On the other hand, conflicting or new information in the outputspecification survives.
The typed variant of Carpenter's definition of asymmetric de-fault unification that was presented in Section 3.2 will support his approach to lexicalrules.4.1 The Definition of a TDFS Lexical RuleAll lexical rules are of the following form, where Ia/Ta and Ib/Tb are TDFSs:Z,/T, IdTb498Briscoe and Copestake Lexical RulesApplied to:r 3rdsg \]3RDSNG\[A~GS:<T.\[\]> \]FUNCTION ?
f3rdsng \[base \]~+ PHON:base \]PHON r FUNCTION : \]3RDSN:G\[ ARGS : < walk > \]SYN LOC:SUBCAT : < NP >is equivalent to:r 3rdsg \] F base 1 FUNCTION : FUNCTION f3rdsng /PHON: \ [ARGS:<walk>\]  PHON : \] \[ARGS : < T.\[\] > \] ~t |3RDSNG:L 3RDSNG : \[\] L SYN LOC SUBCAT : < NP >3rdsg \[ FUNCTION: f3rdsng \] = PHON: ARGS:<walk,\[\]> \]3RDSNG, \[\]SYN LOC SUBCAT : < NP >Figure 7Reinterpreted Third Singular Verb Formation lexical rule.The interpretation of the rule is as follows: Given a lexeme, 11/I"1, where I1 u Ia, thenthe result of applying the lexical rule is a TDFS, Io/To, such that:<1.
Io = Ib\[qtI\]The indefeasible output is given by asymmetrically default unifying theindefeasible part of the output specification with the indefeasible part ofthe input lexeme.2.
To = Filter(I0, (Tb U T1))The output tail is the union of the output specification's tail Tb and theinput tail, T1, minus any tail elements that are incompatible with thenew indefeasible information.<> <Note that this definition parallels the definition of m, with Rt replacing m. Notealso that although as we have described them here lexical rules apply to TDFSs, itshould be clear that their application to ordinary TFSs is just the special case wherethere are no default components in any of the feature structures involved.4.2 Maximal Incorporation of Information in a Type HierarchyWhen we extend atomic FSs as defined by Carpenter to a typed framework, if a TFShas a path ~r with value t, then \[~r : t\] is an atomic FS in the decomposition of the TFS.However, if t has supertypes r, s, and so on, in order to get maximal incorporation ofinformation, we must also include the atomic FSs \[Tr: r\], \[Tr: s\], and so on (see L&C,Sec.
6.3).
This is important in our use of asymmetric default unification to define lexicalrules, as we illustrate with the following example.Suppose we describe the inchoative-causative alternation using the lexical ruleshown in (5).
(5) \[intrans-verb\]/{} H \[trans-caus-verb\]/{}499Computational Linguistics Volume 25, Number 4intrans-verb movt-verbintrans-movt-verbFigure 8trans-verbtrans-caus-verbStrans-caus-movt-verbMultiple inheritance for movement verbs.This is intended to apply to a range of semantic subclasses and the specific semanticclass should be retained after application of the lexical rule: for example, if the inputis a movement verb then the result should also be a movement verb.
If semanticsubclasses cross-classify with types such as trans-verb and intrans-verb, this could beimplemented using multiple inheritance: i.e., the type intrans-movt-verb would inheritfrom intrans-verb and movt-verb, and the type trans-caus-movt-verb would inheritfrom trans-caus-verb and movt-verb as shown in Figure 8.
Assume we apply the rulein (5) to a lexeme of type intrans-movt-verb.
If the asymmetric default unificationoperation used in the definition of lexical rules did not consider the supertypes ofthis type, the application of the rule would give an output of type trans-verb, becausethe value of the type on the input is incompatible with trans-verb.
However, if wetake account of all the supertypes of the type that are compatible with the output, theresult is the desired one: trans-caus-movt-verb.
This is illustrated in Figure 9.4.3 Reversibility and BackformationGiven the arguments in Lascarides et al (1995) about he desirability of the symmetricform of default unification on the grounds of order-independence, it may seem surpris-ing to suggest that an order-dependent operation be the basis for the formalization oflexical rules.
But it is clear that lexical rule application must be order-dependent, forexample to distinguish truthfulness from *truthnessful.
However, bidirectionality andreversibility are required in order to model both analysis and generation, and also todeal with backformafion.
In fact, although conventional HPSG-style lexical rules ap-pear to be straightforwardly interpretable as bidirectional, there is no guarantee thatany given input can be recovered from knowledge of the rule and of its output, unlessall the information i  the input is copied over to the output.
The most that can be saidis that by reversing the rule a result will be obtained that is unifiable with the originalinput.
Of course, lexical rules could be written in such a way that all informationabout he input can be recovered, by copying all the information over (for example toa DTR-style attribute in the output).Reversibility of TDFS lexical rules is defined somewhat differently.
Given a ruleFS1 ~ FS2, and an input I that results in output O, we can recover 11 defined as theunification of FS1 with F, the most general FS such that F  FS2 ~- O.
By the definitionof default unification, I u F, and since TDFS lexical rules apply under subsumption,I I- FS1.
Thus I _U 11, that is, we can guarantee that the result of the "reverse" applicationsubsumes or is equal to the input (which is actually astronger result han we obtain forconventional lexical rules).
With both conventional nd TDFS lexical rules, therefore,500Briscoe and Copestake Lexical Rulesintrans-verbPHON : phon; v~.
\[ RESULT : ssign \].
.
.
.
.
\[ ACTIVE : npsign Jverb-rel -a t-cause / SEM:< \[ .
.
.
.
.
1.
I~.V~T : e 1~ L \[ t~vul'~l : e j L ARG .
x Jtrans-caus-verbPHON : phonI \[ RESULT : ssign \ ] t  SYN : RESULT : \[ ACTIVE , npsign \]k ACTIVE : npsign-a t-cause - at |.
.
.
.
rverb-rel 1.\[I V  T.el, ~.~NT o~lvl : < \[ EVENT : e\] > :e j  \ [ARG:x  \] ARG:yInput:intrans-movt-verbL \]PHON : gallopSYN \[ RESULT :ssign \]: ACTIVE npslgn\]\[ I r p'agt'cause'm?ve \] SEM gallop-rel , | EVENT : e >: < EVENT:e \ [ARG:xtrans-caus-movt-verbPHON : gallopSYN: \[ RESULT \[RESULT:ssign \] : \[ ACTIVE : npslgn 1Output: L ACTIVE npsign E sem?v l\[" SEM:< EVENT.e\]'  |EVENT:e , EVENT:e\[ARG : x ARG : yFigure 9Causative-Inchoative lexical rule revisited.additional assumptions about the relationship between the input and the output, atleast with respect o phonology/orthography and semantics, would have to be madeif we wanted to guarantee that a lexical entry will be fully retrievable.Thus, idiosyncratic information in the input lexical entry, which is overridden inthe output, cannot be recovered.
This actually seems to be a desirable property whenwe consider how backformations may be modeled.
For example, self-destruct ratherthan self-destroy is the backformation from self-destruction (see Bauer \[1983, 232\]).
It isthus apparently linguistically unmotivated to assume that the input to a lexical rule iscompletely recoverable from a derived form and the asymmetry of TDFS lexical rulesarguably captures ome of the markedness of backformation without precluding thepossibility that individual reversed rules faithfully model the effects of backformations.4.4 DiscussionUnlike the versions of lexical rules described by Riehemann (1993), Copestake andBriscoe (1995), and Meurers (1995), this interpretation cannot be incorporated into amonotonic onstraint-based formalism, since the operation of lexical rules is essentiallynonmonotonic, in that the incorporation of additional information into the input mayresult in loss of information from the output.
This treatment of lexical rules thereforerequires that there be an interface between the lexicon and the syntactic omponentso that operations are carried out in a predefined order.
This is not a disadvantage501Computational Linguistics Volume 25, Number 4\/\.Input1 F bt 2 F3 t FFigure 10ResultrF arF erF eAbstract lexical rule example.from our perspective, since the use of defaults to encode information that does notpersist beyond the lexicon requires uch an interface in any case.
The cost of the fullymonotonic version of lexical rules is that the complete history of rule application hasto be accessible, ither explicitly, as with Riehemann's approach, or implicitly (e.g., viajunk slots).
However, our current approach does mean that some potential applicationsof lexical rules are precluded, which we discuss further in Section 7.In contrast with Meurer's notation, it is not possible to straightforwardly compileout our lexical rules into equivalent rules that do not use defaults, even when TFSsrather than TDFSs are considered.
Consider the abstract example shown in Figure 10.To state this using conventional lexical rules would be extremely complex, since itwould require a whole series of rules to deal with the possible values of F, each withnegative conditions to prevent them applying incorrectly.
Contrast this to Meurer's ap-proach, where all inputs would give a result where F had the value a.
Essentially thedifference is that whereas our approach can be paraphrased as"transfer all informationfor a feature F that does not conflict with information on the output," Meurer's deft-nition can be paraphrased "transfer all values for a feature F unless any informationis stated about F." The example shown in Figure 9 illustrates one case for which ourapproach leads to desirable results.
Another example arises when inflection is imple-mented using lexical rules, as with the example of the Third Singular Verb Formationrule (Figure 7).
With conventional lexical rules it is impossible to carry over informa-tion about the type of the input to the output (unless the rule is totally monotonicin operation, in which case it is equivalent to the use of simple subsumption-basedinheritance).
Therefore lexical types cannot persist after inflectional rules are applied,unless the rule is split so that one subrule applies to each type (see, for example,Copestake \[1992\] for further discussion).
This class of problems is avoided with ourcurrent approach.In other respects, this version of lexical rules is intermediate in expressivity be-tween simple inheritance and conventional lexical rules.
It is more powerful thansimple inheritance, because the "input" and "output" types of lexical rules do nothave to be in a fixed inheritance hierarchy, and recursive application of rules is pos-sible.
Consider for example, an alternative encoding, using simple inheritance, of theCausative-Inchoative rule shown in Figure 9.
This requires that a type caus-or-inch-502Briscoe and Copestake Lexical Rulesverb exists that has intrans-verb and trans-caus-verb as subtypes.
However, in orderto encode further alternations, these types themselves have to have subtypes corre-sponding to each rule.
For example, if the goal PP in John galloped the horse to thebarn / The horse galloped to the barn is encoded as an argument, hen intrans-verb andtrans-caus-verb oth have to have subtypes for the goal PP and this configurationhas to be replicated for all subtypes of intrans-verb, such as intrans-movt-verb.
Thisquickly leads to highly complex type hierarchies, ince the configuration of the typesencodes "reachability," aswell as inheritance.
Furthermore, ncoding lexical rules byinheritance does not allow for any notion of semiproductivity of a rule.
Individualexceptions to a rule can be encoded, for example, if run is encoded as an intrans-verband not specified to be of the higher type caus-or-inch-verb, then causative forms ofrun will not be generated.
But this leaves it as an accident that, at least for this classof movement verbs, it is generally the causative rather than the inchoative form thatis impossible or marked.
We will discuss how we can exploit he asymmetry of TDFSlexical rules to allow for semiproductivity n Section 6.Our approach to lexical rules is, on the other hand, less powerful than conventionalHPSG-style lexical rules because it is not possible to arbitrarily "rearrange" materialbetween the input and the output structures: a value that is present on a particularpath in the input can either be unified with a value on the same path in the outputstructure, or be overridden, but it cannot be moved to a new position within theoutput feature structure.
This approach therefore shares some of the restrictions ofsimple inheritance with respect o encoding potentially recursive operations, and wediscuss the implications of this further in Section 7.It is clear that the notion of lexical rules that we have presented encodes omethinglike "type reachability" rules for lexical types.
This is an inherently more restrictednotion than that of HPSG-style lexical rules, which can also encode arbitrary operationson list-valued features.
A consequence of this more restricted notion is that lexical rulescannot be used to rearrange the order of list-valued features and cannot be appliedrecursively in a manner that makes such lists unbounded.
In fact, since lexical rulessimply relate lexical types predefined in the lexicon, they cannot increase the generativecapacity of the overall system in which they are embedded.
4In Section 5, nevertheless, we show that an insightful account of one rule thatwould be encoded via SUBCAT list manipulations can be stated in this more restrictiveframework.
In Section 7 we consider the extent o which our approach can captureother putative lexical rules and argue for a clearer distinction between lexical rulesand (unary) syntactic rules.
55.
Dative ConstructionsLascarides, Copestake, and Briscoe (1996) present an account of the dative alterna-tion that illustrates the utility of the TDFS framework for encoding defeasible l xicalsemantic entailments in terms of Dowty's (1989) proto thematic roles, and the interac-4 We omit a formal proof as this would require more detailed specification of the syntactic omponentthan is warranted in the rest of the paper.5 An alternative method of exploiting the TDFS formalism to encode rules was mentioned in L&C(page 87) and has been explored by Malouf (1999).
This technique uses the TDFS description languageto allow a very succinct statement of rules that use coindexation to relate their input and output:effectively, paths in the input and output structure can be specified to be coindexed by default.
Theexpanded rules have the same properties as the lexical rule variants described by Copestake (1992) orRiehemann (1993).
Thus, in terms of the current paper, this encoding is a way of improving therepresentation f syntactic rules.503Computational Linguistics Volume 25, Number 4tion of these with the dative alternation (e.g., Green 1974).
This account draws heavilyon Goldberg's (1995) analysis of dative in construction grammar (e.g., Fillmore, Kay,and O'Connor 1988), and attempts to integrate her insights and general approach intoa more formally explicit constraint-based framework.
In this section, we extend thisaccount by embedding the account in a syntactic framework based on UCG (Zeevat,Klein, and Calder 1987), as integrated with Dowty's approach to thematic roles andextended by Sanfilippo (1990, 1992, 1993), and augmented with linking theory (Chang1995).
6This allows us to utilize TDFS lexical rules in an insightful way to express analternation otherwise naturally treated in terms of rearrangement of a list-valued SUB-CAT feature.
In Section 8 we go on to propose a probabilistic interpretation f TDFSlexical rules that allows us to capture the semiproductivity of the dative alternation,and other lexical rules.Following Goldberg (1995) we argue that there is a family of dative constructionsthat exhibit he same syntactic properties and related semantic properties, exemplifiedin (6).
(6) a. Mary gave Joe a presentb.
Joe painted Sally a picturec.
Mary promised Joe a new card.
He tipped Bill two poundse.
The medicine brought him relieff.
The music lent the party a festive airg.
Jo gave Bob a punchh.
He blew his wife a kissi.
She smiled herself an upgradeThe core dative construction i volves a volitional agent and willing recipient andcarries the entailments hat the agent causes the recipient to receive the object denotedby the patient/affected-object argument, as in (6a).
Under this interpretation, (6b)involves a shift in meaning where the recipient may or may not receive the affected-object, but the agent acts with this intention, whilst (6c) involves a similar shift as theact of transfer is intended to take place at some point in the future and may not infact occur.
(6d), unlike the previous examples, does not have an oblique counterpart.The remaining examples all appear to involve metaphorical or idiomatic extensions tothe core dative construction.We represent the difference in the basic (abstract) meaning of the dative construc-tion in (6a) and (6b) in terms of entailments associated with proto thematic roles, sothat agent becomes p-agt-cause-transfer in (6a) and p-agt-cause-make-intend-transferin (6b).
There are lexical rules that relate the dative construction with the alternativeoblique complementation patterns involving to and for PP arguments, respectively,which alter the proto-agent role of a "creation" verb such as paint from p-agt-cause-make to p-agt-cause-make-intend-transfer.
We abbreviate the different entailments(willingness and successful transfer) concerning the first object in (6a) and (6b) as6 Nevertheless, the general pproach tolexical rules is equally compatible with HPSG, combinatorycategorial grammar (e.g., Steedman 1996), tree adjoining rammar (e.g., Joshi 1987) or indeed anygrammatical theory embeddable in the T(D)FS representation language.504Briscoe and Copestake Lexical Rules- dative-verbPHON : phonSYN:\ [  RESULT , \[ RESULT : \[RESULT " s s i g n  ACTIVE npsig ACTIVE npsign J i l lL ACTIVE : npsign.
.
.
.
\[ t~-a~,t-caus/p-agt-cause-transfer \] / verD-reJ / / ~Vl~NT /SEM : < \[ Eii~RN~;~':!a~RGb: i: i~A~EbGIN/~z-?ebj.
1-recip\]!
'L :Y J L  : JFigure 11The dative type constraint.oblique-transfer-verbPHON : phon\[ \[ \[RESULT:ssign \ ] \ ]  1 RESULT : RESULT : \[ ACTIVE npsign jSYN, ACTIVE npsignL ACTIVE : pptosignp-agt-cause-transfer \] \[ p-pat-aff-obj \] \[ p-obl-recip \]SEM:< \[\[EVENTVerb'rel e\] '  EVENT:e | , \ ]EVENT:e / .
|EVENT:e  / >: ARG:x J LARG:y \] LARG:z \]Figure 12The oblique-transfer type constraint.recipient and benefactive, respectively.
It should only be necessary to state the formof the dative construction once.
Furthermore, it should not be necessary to say thatverbs of creation, such as paint, are lexically ambiguous between two- and three-placepredicates; rather, it is participation in the dative construction itself that creates thethird benefactive argument for these inherently two-place predicates.
We also assumewithout explicit argument here that the for PP variant is produced by a lexical ruleintroducing the optional PP.In the TDFS framework we can state the semantics of the dative constructionindependently of specific lexical heads (or arguments) as a type constraint on the set ofdative lexical items.
The type constraint expressed as a TDFS in Figure 11 achieves thisand utilizes default specification of the proto-roles on the arguments so that specificverbs can override the core entailments.
This constraint stipulates that dative verbs willnecessarily have a p-agt-cause role, but, by default, this will be specialized to p-agt-cause-transfer, thus expressing the generalization that the dative construction usuallyimplies that some transfer has taken place.
Similarly, the role on the proto-patientis p-pat-aff-obj by default, since the object will usually be affected, and p-obl-recipcaptures the entailment that the oblique argument usually corresponds to a recipient.
(We assume that the second object is treated as an oblique \[indirect object\] argumentin the framework, though nothing particularly rests on this assumption.
)Most verbs will not be directly specified in the lexicon as being of type dative,but will become associated with this type via the lexical rules relating oblique-transferverbs and transitive-creation verbs to it.
The type constraints for these source typesare given in Figure 12 and Figure 13, respectively.
(We ignore the issue of how the in-formation represented atthese types might be factored between supertypes to capturefurther generalizations concerning verb classes; see, for example, Sanfilippo \[1993\].
)The two lexical rules required are given in Figure 14.505Computational Linguistics Volume 25, Number 4?
trans-create-verbPHON phon|RESULT: ssign 1 1 SYN : RESULT : L ACTIVE : npslgn jJ ACTIVE npsign i \] : < .
/EVENT : e .
EVENT: e > SEM \[ EVENT : e L ARG : x ARG : yFigure 13The transitive-creation type constraint.The Dative Lexical Ruleoblique-transfer-verb .-+ dative-verbCreate-to-Benef-Dative Lexical Ruler dative-verb| r verb-rel ~ F D-a~t-cause-make-intend-transfer 1trans-create-verb,-~ \]SEM:< \[EVENT:e\]'\[~AV~G NTe .\]| F p-pat-aff-obj \] F p-obl-benef| |EVENT:e | .
|EVENT:e  | >L LARG:y J LARG:z JFigure 14Dative lexical rules.These rules can be stated quite simply with reference to the type system for verbs.The first rule, which is the core (recipient) Dative rule, is simply stipulated as a "reach-ability" relationship between the two types oblique-transfer-verb and dative-verb.However, when this rule is applied to a lexical entry of type oblique-transfer-verb,the specifications of the proto-roles as p-agt-cause-transfer and p-obl-recip in the resultwill be indefeasible, in contrast o their defeasible status in dative-verb, because theyare indefeasible in the basic type.
We show the input and output TDFSs for a transferverb (give) undergoing this rule in Figure 15.
In contrast, the benefactive Dative rulespecializes the dative type so that the proto-role entailments stipulated override thedefaults on the type dative-verb.
Thus, p-agt-cause-make-intend-transfer overrides p-agt-cause-transfer and p-obl-benef overrides p-obl-recip.
The effect is that the resultdoes not imply that transfer of the affected object was necessarily successful.
Figure 16shows the input and output for a creation verb (paint) undergoing this rule.
Thus theserules correct predict that while (6a) implies successful transfer, (6b) only implies thattransfer was intended.The verb types shown so far do not make explicit the linking between proto-rolearguments and the semantic indices of syntactic arguments.
In HPSG and constructiongrammar, such linking is provided on a construction-by-construction basis in the typedefinitions for each construction/sign.
The standard way of linking arguments in a(T)FS framework is to make the semantic indices of the syntactic arguments reentrantwith the indices representing the arguments of the (proto) thematic roles.
However,if we adopted this approach, default unifying linked versions of oblique-transfer-verb with dative-verb would result in the incoherent structure shown in Figure 17because the reentrancies in the input type description are consistent with the outputtype description, and are thus incorrectly preserved.506Briscoe and Copestake Lexical RulesIn )ut:oblique-transfer-verbPHON : giveSYN : RESULT , RESULT : \[ ACTIVE : npsign 1L ACTIVE npsignk ACTIVE : pptosign,  WNT:: e LARG : x ARG : y \ [ARG : zOutput:dative-verbPHON : give \[ 1\] SYN : RESULT : \[ ACTIVE npslgn \] L ACTIVE npsign ACTIVE npsign\[ p-agt-cause-transfer \] \[ p-pat-aff-obj \] \[ p-obl-recip \]SEM:< \[EVENTgive'rel e \ ] '  EVENT:e / , |EVENT:e  | .
|EVENT:e: ARG; x J L ARG: y J LARG: zFigure 15giveIn 3ut:?
trans-create-verbPHON : paintSYN \[RESULT \[RESULT:ssign \ ] \ ]  : ACTIVE : npsign J: L ACTIVE npsign\] W .V NT:o :< EVENT:e ' ' L ARG : x~ -pat-aff-obj 1 VENT : eARG : yOutput:dative-verbPHON : paintI RESULT: ssign 1 \] \]SYN.
RESULT: IRESULT: \[ACTIVE: npsign\] / /L ACTIVE : npsign J |ACTIVE : npsign J .r " " r 1 ~ r v-a~t-cause-make-intend-transferSEM:< / ~ .~ e e / ' /~V~NT : e' : ~ L ARG: xr p-pat-aff-obj r p-obl-benef \]\ ]EVENT:e  ?
iEVENT:e  | >L ARG: y L ARG: z JFigure 16paintThere are ways  in which we could extend the formal ism to avoid this problem,for instance by al lowing specification of inequalities (Carpenter 1992), which could beused to explicitly prevent inappropriate coindexation (see Lascarides and Copestake\[1999\] for inequalit ies in the TDFS framework).
However ,  for the purposes of link-ing, this restriction on the expressivity of lexical rules is a virtue rather than a vice:Pinker (1989) argues on quite independent  grounds that l inking rules should apply507Computational Linguistics Volume 25, Number 4?
dative-verbPHON : phon\[ r ss' n  CTIV SYN : : L npslgn\[\]L ACTIVE npsign F~ACTIVE : npsign \[\]F verb-rel \] F p-agt-caus/p-agt-cause-transfer":< , \[EVENT:e SEM \[EVENT:ej \[ARG: \[\]\[ ~?P~I~'p eat'aff'?b j \ ] ~ A R G :  \[  J' \[ p-obl/p-obl-recip " A R G :  EVENT : e\[\] >Figure 17Incoherent linked dative TDFS.after lexical rules so that all lexical entries are subject o the same linking constraints.By factoring linking constraints out of the type constraints on specific constructionswe can eliminate redundancy and explicitly express the appropriate linking gener-alizations.
Chang (1995) follows Wechsler (1991) and Davis (1996) in assuming thatlinking generalizations are captured via constraints pecified as TFSs, but makes theassumption that linking applies after lexical rule application.
She shows how linkinggeneralizations can be captured within the TDFS framework assuming the linguisticframework of UCG/construction grammar and proto thematic roles outlined above.By reorganizing the semantic type hierarchy to take account of the distinction betweeninternal and external causation (see Levin and Rappaport Hovav \[1995\]), Chang is ableto define 11 partly defeasible verbal linking constraints, each specifying the link be-tween one thematic role and one argument position, correctly linking monadic, dyadic,and triadic predicates that may undergo causative-inchoative, passive, and/or dativealternations by persistent default unification of all linking constraints with each dis-tinct basic and derived verbal type.
Thus, our approach to lexical rules is similar to thatof Pinker (1989) in that all basic and derived lexical entries are subject o a few gen-eral linking constraints that coindex syntactic arguments with appropriate proto-roles.However, in common with Goldberg (1995), we adopt the position that such rules arenot fully reducible to operations on semantic representations, but rather concern theinterplay of syntax and semantics in (bounded ependency) constructions.Verbs such as promise in (6c) are treated as a subclass of transfer verbs, which wecall future transfer verbs, defined by a subtype of oblique-transfer-verb altering theproto-role ntailments analogously to the benefactive case already discussed, so that noentailment of actual transfer is made.
If the source type description is modified in thisfashion, the main dative lexical rule will apply and produce an output TDFS in whichthe dative type defeasible proto-role ntailments are overwritten to something like p-agt-cause-intend-fut-transfer and p-pat-benef, respectively.
Thus, all that is requiredis an extra source type description for this semantic subclass of verbs to producecorrect behavior.The verb tip in (6d) is an example of a small class of verbs (also including envy),which show that dative variants can exist without an oblique "source."
In an approachthat generated erived entries strictly from basic source entries, these cases would beproblematic.
However, in an approach such as ours where lexical rules relate inde-pendently defined (specialized) type descriptions, there is no prediction that outputtypes of lexical rules cannot be basic types for some verbs.
We can simply treat tipand similar cases as basic dative verbs and alter the defeasible proto-role ntailments508Briscoe and Copestake Lexical Rulesdative-met-verbPHON : phonSYN : I RESULTL ACTIVE : npsignL\[ RESU T : \[ RESULT ~ ss ignACT IVE  npsignACTIVE npslgn 1\]\]\]F verb-rel \] Fp-agt-caus/p-agt-cause-met-transfer":< , |EVENT:e SEM LEVENT:e. LARG:xEVENT ~ e .
EVENT: e >ARG : e' ARG : zFigure 18Metaphorical dative type constraint.of the dative type as appropriate.
Thus, we make no prediction that the set of dativeverbs will be a subset of the set of oblique verbs.Goldberg (1995) argues that (6e) and (6f) are licensed by a metaphorical extensionof the transfer elation by which causal events are viewed as transfers.
Causing aneffect in an entity is understood as transferring that effect to it.
We capture this byaltering the entailments a sociated with verbs such as lend by the proto-roles specifiedby oblique-transfer-verb using a lexical rule that creates an entry of a sister typemet-oblique-transfer-verb that specifies different proto-roles.
The Dative lexical rulewould also apply to this subtype, capturing the fact that this metaphorical extensionin the dative construction parallels a similar extension of the same verb set in theoblique to prepositional phrase construction.In contrast, (6g) and (6h) provide evidence that certain quasi-idiomatic expressionsneed to be associated irectly with dative-sign as they have no counterparts in suchoblique expressions.
We assume that (quasi) idioms are best represented assubtypes oflexical signs in which not only the syntactic head but also other arguments are severelyconstrained in terms of lexical selection.
Thus Goldberg claims the quasi-idiomaticexpressions in (6g) and (6h) are licensed by a metaphor that involves understandingactions intentionally directed at another person as being entities transferred to thatperson.
As a first approximation, we might represent this process in terms of thesubtype of dative-verb shown in Figure 18, in which we have overridden the defaultproto-role specifications with entailments specific to the metaphor, which we assumealso serve to constrain the range of acceptable arguments o the (transfer) verb.Finally, (6i) demonstrates that reflexive datives can sometimes be formed fromintransitive verbs.
The semantic restrictions on the source for such constructions arenot entirely clear to us, however, all the putative similar examples to the attested (6i)seem innovative and much less conventionalized than the core dative examples.
Itwould be straightforward to introduce a lexical rule mapping intransitive verbs to thedative construction.
However, the relative productivity of this putative rule shouldbe represented as being much lower than the two rules discussed above.
Nothingin the representation we have presented so far equips us to deal with the issue of(semi)productivity which also arises with the two rules introduced earlier; considerthe famous example of donate, which does not undergo the dative alternation althoughit is uncontroversially an oblique transfer verb.
In the next section we consider thisissue in detail.509Computational Linguistics Volume 25, Number 46.
Semiproductivity and Probabilistic TDFS GrammarThe search for a fully productive statement ofverb alternations such as dative has ledto an increasingly semantic perspective on such rules.
However, while a reasonablecase can be made that the conditions on the rules introduced in Figure 14 expressnecessary conditions for their application, they do not capture sufficient conditionsbecause of the existence of semantically similar nonalternating verbs, such as donateas opposed to give, or create as opposed to paint, as (7a) and (7b) illustrate.
(7) a.
*The president donated the club a trophyb.
*The architect created them a bridgePinker (1989) argues that so-called broad semantic lasses of the type identified in thedative rules given above (i.e., creation or transfer verbs) provide necessary conditionsfor lexical rule application, but that narrow class lexical rules should be specifiedbreaking down such rules into a number of fully productive subcases.
In the attempt todefine such subcases Pinker is forced to make subtle and often unintuitive distinctions,and to claim that the meaning components involved are features of universal grammarto which the grammars of any language may be sensitive.
For example, in attemptingto differentiate he dative alternating and nonalternating subclasses in (8a) and (8b),Pinker (1989, 110-111) characterizes those in (8a) as "verbs of continuous impartingof force in some manner causing accompanied motion" and those in (8b) as "verbs ofcontinuous causation of motion ... in a deictically-specified direction.
"(8) a.
*John dragged/pulled Bill the computer / the computer to Billb.
John brought/carried/took Bill the computer / the computer to Billc.
John pushed Bill his beer / ?calculator / *computerd.
John slid Bill his beer / ?calculator / *computerFurthermore, the continuous nature of force imparted seems crucial to the acceptabilityof dative with the first class: so (8c) is more acceptable, the lighter the affected objectand the more plausible it is to construct a scenario in which push is synonymous withslide, as in (8d).Notwithstanding the subtle distinctions Pinker makes and the often very tiny sizeof the narrow verb classes he identifies, there remain exceptions to his generalizations.For example, in British English, which is generally slightly more liberal with respect tothe dative alternation than American English, the examples in (9a) and (9b) are pairs,classified in the same narrow class by Pinker, where one is acceptable in the dativeand one is not.
(9) a. John designed / *created them a bridgeb.
I picked / *indicated her a dressSuch dialectal disparities would be less problematic f they applied uniformly to nar-row classes but instead they appear to be insensitive to Pinker's classifications.
Inaddition, Pirelli, Ruimy, and Montemagni (1994) and Nicholls (1995) document caseswhere Pinker's narrow classes do not generalize to equivalent alternations in Italianand French, respectively; demonstrating that cross-linguistic disparities are similarly510Briscoe and Copestake Lexical Rulesinsensitive to putative narrow classes.
More generally, Pinker's notion of a fully pro-ductive narrow-class lexical rule is falsified by many examples cited in Boguraev andBriscoe (1989), Levin (1992), and Sch/~tze (1997).
We conclude that the program oftreating all exceptions to dative as systematic, as opposed to accidental or idiosyn-cratic (Wasow 1980), fails for dative movement, and will, we suspect, fail for mostverb diathesis alternations.
Therefore, it is not possible to characterize such rules as afully productive generative operation.Within the generative tradition of work on lexical rules, the only alternative totreating such rules as fully productive generalizations is to treat them as redundancyrules of some form (e.g., Jackendoff \[1975\] and see the discussion in Section 2).
How-ever, this approach does not account for the semiproductive nature of such rules.
Forexample, Pinker notes with respect o the dative alternation that a variety of recentnonce verbs readily undergo dative because they are clearly members of the commu-nication subclass of transfer verbs, as (10a) illustrates.
(10) a. John faxed / xeroxed / emailed his colleagues a copy of the reportb.
Sun donated us a bunch of computersc.
John explained me the problemd.
He stripped him the ballA redundancy rule only relates existing lexical entries to achieve abbreviation i thestatement of the lexicon.
It cannot be generatively applied to nonce usages.
Similarly,the examples in (10b), (10c), and (10d) are all attested uses of the dative that violateputative (narrow-class) morphological or semantic onstraints on its application fromPinker (1989, 155-157).
The existence of creative or analogous application to nonceusages and attested exceptions to the narrow-class rules makes the redundancy ruleapproach unsatisfactory, at least, when formalized as a purely abbreviatory device (seeSection 2 above).The search for narrow classes and full productivity seems futile for rules of verbalternation because such rules are inherently semiproductive in the same manner thatderivational rules are often characterized assemiproductive (e.g., Bauer 1983).
Instead,we argue, following Goldberg (1995), who in turn is influenced by theories of semipro-ductivity developed for morphology, that rules of verb alternation are sensitive to bothtype and token frequency effects that determine language users' assessments of the de-gree of acceptability of a given derived form and also their willingness to apply a rulein producing or interpreting a novel form.
Bauer (1983, 71f.
), in supporting the viewthat lexical rules should be treated as fully productive generative rules analogous tothose employed in syntactic description, argues that it is this greater "item-familiarity"of lexical items that allows judgements of relative novelty/conventionality to be builtup.
He points out that there are simply too many combinatoric possibilities at thesentential level for the frequency of particular combinations to be assessed with anyconfidence by a language user.
However, in the case of words and, we might add,idioms, the range of possibilities, though large, is not so great that judgements ofnovelty based on frequency of use cannot be acquired.
Bauer argues, therefore, thataccounting for semiproductivity s an issue of performance, not competence.
There isconsiderable evidence that language users are very sensitive to the relative frequencyof variant forms and senses of lexical items.
Such assumptions underlie influentialtheories of language variation and change (e.g., Labov 1972) and psycholinguisticaccounts of preferences and misinterpretation during language comprehension (e.g.,Kelly and Martin 1994).511Computational Linguistics Volume 25, Number 4The frequency with which a given word form is associated with a particular lexicalentry (i.e., sense or grammatical realization) is often highly skewed; Church and Mer-cer (1993) point out that a model of part-of-speech assignment in context will be 90%accurate (for English) if it simply chooses the lexically most frequent part-of-speech fora given word.
In the LOB corpus, there are about 18 times as many instances of believein the most common subcategorization class (sentential complement) as in the fourleast common classes combined, and other multiple-complement-taking verbs showsimilar strong skews (e.g., Briscoe, Copestake, and Boguraev 1990).
In the absence ofother factors, it seems very likely that language users utilize frequency information toresolve indeterminacies in both generation and interpretation.
Such a strategy is com-patible with and may well underlie the Gricean maxim of manner, in that ambiguitiesin language will be more easily interpretable if there is a tacit agreement ot to utilizeabnormal or rare means of conveying particular messages.
We can model this aspectof language use as a conditional probability that a word form will be associated to aspecific lexical entry, derived using a maximum likelihood estimator: 7freq(lexical-entry with word-form)Pr?b(lexical-entry I w?rd-f?rm)= freq(word-form)This proposal is not novel and is the analogue of proposals to associate proba-bilities with initial trees in, for example, a lexicalized tree adjoining rammar (Resnik1992; Schabes 1992).
However, it differs from recent proposals by, for example, Brew(1995), to associate probabilities with values on paths in a TFS formalism underlyingHPSG, as the probabilistic nformation is much less fine-grained.
We associate a singleprobability with each complete TDFS that represents a lexical entry.
In a probabilisticgrammar based on this approach, the probability of a derivation must depend in parton details of the grammatical pproach adopted.
In a categorial framework it may bethere are only mutually exclusive schemata for combining lexical entries into phrasaland clausal signs, so the probability of a given derivation can be treated as the productof the probability of the lexical entries utilized in that derivation.
In this case, for wordforms i in sentence:Prob(sent-interp) = II(lex-entry { word-form/)iIn frameworks that incorporate alternative competing syntactic rule schemata or op-erations, it might be necessary to associate probabilities with such rules and treat theprobability of a derivation as the combined product of the probability of the syntac-tic operations applied and the lexical entries utilized (e.g., Schabes 1992).
Under thisformulation, the conditional probability of a lexical entry given a word form is inde-pendent of the larger context in which the word occurs (except o the extent hat thisis encoded in the lexical entry).
This approximation ties the number of probabilisticparameters tobe estimated by the language user to the size of the user's lexicon andis thus the probabilistic analogue of the item-familiarity approach described above.
It7 In what follows we assume familiarity with the basic axioms of probability theory and with statisticalestimation (e.g., Box and Tiao 1973).
We should emphasize that we are proposing a probabilistic versionof a grammatical theory developed in the TDFS framework, not as a solution to practical engineeringproblems of parsing, but as a theoretical ccount of the item-familiarity view of semiproductivity.
Wedoubt that this theory is psycholinguistically accurate in the sense that language users literally computeprobabilities and abide by the axioms of probability theory.
However, probability theory provides aprecise and clear framework in which to represent estimates of the relative likelihood of events.512Briscoe and Copestake Lexical Rulestrans-caus-verbPHON : fax \[-SYN : / RESULT/L ACTIVEfax-rel SEM: < EVENTRESULT : ssign \] 1 ACTIVE : npslgn \[\]npsign \[\]\] \[ p-agt-causel P p-pat-aff-obj\],\[EVENT:e \[, \]EVENT:e >e \[ARG : \[\] J LARG: \[\]create/transfer-lexeme-fsm(trans(0.2), oblique-tr(0.3)...)Figure 19Lexeme for fax.says, in effect, that users can track the relative frequency of words/lexemes but not of(most) phrases or sentences.We assume that lexical probabilities are acquired for both basic and derived lexi-cal entries independently of any lexical rules used to create derived entries.
Thus wemake no claim that a derived entry will necessarily be less frequent than a basic one.It might seem that this assumption commits us to a "full entry" theory of the lexicon(e.g., Aronoff 1976; Jackendoff 1997a) in which all possible words are present; that is,the consequences of lexical rules are precomputed.
In the limit, the full entry theorycannot be correct because of the presence of recursive lexical rules such as derivationalrules of re-, anti-, or great- prefixation in words such as rereprogram, anti-anti-missile orgreat-great-grandfather.
Instead we adopt an intermediate position in which basic entriesare augmented with a representation f the attested lexical rules that have applied tothem and any such derived chains, where both the basic entry and these "abbreviated"derived entries are associated with a probability.
One way of formalizing and imple-menting this approach is to adopt the covariation technique of Meurers and Minnen(1997) discussed in Section 2, in which finite-state machines (FSMs) representing thepossible lexical rules that can apply to each basic lexical entry are associated withequivalence classes of such entries and the entry is simplified to information commonbetween the variants.
If we assume a precompiled representation f this form, con-ditional probabilities that a word form will be associated with a particular (basic orderived) entry can be associated with states in the FSM, as illustrated in Figure 19.In this representation, the states of the FSM, which have been given mnemonicnames corresponding to their types, are each associated with a probability represent-ing the relative likelihood that fax will be associated with the derived entry that resultsfrom applying the rule to the source entry (the probabilities shown here are purelyfor illustrative purposes).
We call this representation the lexeme for a given word.Figure 20 shows part of the corresponding FSM explicitly.
Note that there are stateswith no associated probabilities, reflecting possible but unattested usages.
The topol-ogy of the FSM associated with a given word may be shared with other words, but thespecific probabilities associated with the states representing lexical entries will be id-iosyncratic, so that the each lexeme representation must minimally encode the uniquename of the relevant FSM and a probability for each attested state/lexical entry asshown in Figure 19.
If the derived form is irregular in some way, then the exceptionalinformation can be stipulated at the relevant state, and the feature structure calculatedby default unifying the specified information with the productive output of the lexicalrule.
For example, if beggar is treated as derived by the agentive -er rule (which isreasonable synchronically), then the irregular morphology can be stipulated and willoverride the predicted begger.513Computational Linguistics Volume 25, Number 4t r a ~ l  iq@ue_trF create-dative-i ~ r 0.3trans benef-dative0.2Figure 20FSM for fax.dative-lrrecip-dativeuse-subst-lrsubstance0.84Figure 21Lexeme for lacquer.agent ~ e r s o nS / 0.9 instrument-er~ O ~ Otr~ns instrumentresult-lr resultative0.01The resulting FSM is not equivalent to a Markov model because probabilities onstates represent output probabilities and not transition probabilities in the machine.In addition, since the probabilities encode the relative likelihood that a given wordform will associate with a particular lexical entry, the set of probabilities on states ofan FSM will not be globally normalized.
One FSM will represent the application ofboth rules of conversion (zero affixation) and rules of derivation to a given lexemeand the latter will change the form of the word, and thus participate in a differentdistribution.
See for example, Figure 21, which is intended to cover the noun andverb lacquer, plus the derived form, lacquerer (with agentive and instrument readingstaken as distinct).
Thus, probabilities on states in FSMs are not required to sum to one,though conditional probabilities of the set of possible (attested and unattested) lexicalentries for a given word form are.One immediate problem with the proposed representation is that certain rules mayapply cyclically or recursively, creating an infinite set of entries.
The FSM encodingdevised by Meurers and Minnen is specifically developed as a form of precompilationcompatible with this possibility.
The majority of clearly recursive or cyclic rules inthe literature are derivational, so it is clear from the word form how many times arule has applied.
We can extend the probabilistic encoding scheme to allow sets ofprobabilities to be encoded on states annotated with number of affixations (e.g., \[q4,\[anti,p1; anti-anti,p2; ...\]\]).
We assume for now that rules of conversion, such as mostverb alternation rules, do not apply cyclically or recursively and discuss apparentexceptions in Section 7.A second problem with the acquisition of reliable estimates of such probabilitiesfor a language user (or implemented parser) is that many possibilities will remain514Briscoe and Copestake Lexical Rulesunseen and will, therefore, be unattested.
For instance, the fact that donate has notbeen seen in the dative construction may indicate the ungrammaticality of this real-ization or merely reflect lack of linguistic exposure to the appropriate dialect, register,or whatever.
The simple maximum likelihood estimator shown above will assign zeroprobability to unseen events.
There are a variety of methods for estimating proba-bilistic parameters or smoothing probability distributions that avoid assigning zeroprobability to unseen events.
One standard approach assigns a hypothetical singleobservation to each unseen event in a distribution before normalizing frequencies toobtain probabilities.
This captures the intuition that the more frequent the observationof some events in a distribution, the less likely it is that the unseen possibilities willoccur.
Thus, a rare word with only a few observations may be more likely to be seenin an alternative realization than a very frequent word that has been observed manytimes in some subset of the possible realizations licensed by the grammar.
However,all unseen events will be assigned the same probability within each distinct distribu-tion and this is at best a gross estimate of their actual distribution.
(The technique isanalogous to assuming a uniform prior distribution within the framework of Bayesianestimation.
)In the case of unattested derived lexical entries for a given word form, the relativeproductivity of the lexical rule(s) required to produce the derived entry are the mostlikely source of information to estimate the probability of an unattested derived entrygiven a word form.
8 Within the probabilistic framework presented above lexical rulesare not directly associated with probabilities.
Nevertheless we can represent the relativeproductivity of each lexical rule by calculating the ratio of possible to attested outputsfor each rule (see Aronoff \[1976\]):M Prod(lexical-rule) =(where N is the number of attested lexical entries that match the lexical rule inputand M is the number of attested output entries).
This is a very simple estimate ofproductivity, and more complex accounts are considered below.The estimate for degree of productivity of a rule can be combined with smoothingto obtain a variant-enhanced smoothing method of the type discussed by Churchand Gale (1991), capable of assigning distinct probabilities to unseen events withinthe same distribution.
This can be achieved by estimating the held-back probabilitymass to be distributed between the unseen entries using the basic smoothing methodoutlined above and then distributing this mass differentially by multiplying the totalmass for unseen entries (expressed as a ratio of the total observations for a givenword) by a different ratio for each lexical rule.
This ratio is obtained by dividing theratio representing the productivity of the lexical rule(s) by the sum of the ratios of thelexical rules required to construct all the unseen entries.Unseen-pr-mass(word-form) =number-of-unattested-entries(word-form)freq(word-form)+number-of-unattested-entries(word-form)Est-freq(lex-entryi with word-formj) =Prod(lri) TT ,~ ~ ,4~ m.~ ~nseen_t,r_mass~wor _ r~,,jj x ~ Prod(lrl),...,Prod(Irn)8 An estimate of the relative productivity of a lexical rule would correspond to Goldberg's (1995) notionof type frequency, while the conditional probability of a lexical entry being associated with a specificword form corresponds to her token frequency.515Computational Linguistics Volume 25, Number 4(where lrl.., lrn are the n lexical rules needed to derive the n unattested entries forword-formj).
This will yield revised ratios for each given word, which can then benormalized to probabilities.To make this clearer, consider the use of the probabilities to drive interpretationin the case of a nonce usage; for example, a language user faced with an unattestedrealization (in their experience) of fax in a dative construction, such as fax me theminutes of the last meeting.
Given the assumptions made in the lexeme representationin Figure 19, fax may undergo either the benefactive Dative or recipient Dative rulesto yield a dative realization.
These rules would produce either a deputive readingwhere, although the speaker is a beneficiary of the action, the recipient is unspecified,or a reading where the speaker is also the recipient of the transfer action.
Choosingbetween these rules in the absence of clear contextual information could be achievedby choosing the derivation (and thus interpretation) with highest probability.
Thiswould depend solely on the relative probability of the unseen derived entries createdby applying these two rules to fax.
This would be (pre)computed by applying theformulas above to a representation f the lexeme for fax in which ratios represent thenumber of observations of an entry for a given word form over the total number ofobservations of that word form, and unattested entries are noted and assigned oneobservation each:create/transfer-lexeme-fsm (trans(~),  oblique-tr (1~0),recip-dative (1Y0), benef-dative (1~) .
.
.
.
)Now if we assume that the recipient Dative rule can apply to 100 source entries andthe resulting derived entries are attested in 60 cases, while the benefactive Dative canapply to 1,000 entries and the derived entries are attested in 100 cases, we can computethe revised estimates of the probabilities for the unseen entries for fax by instantiatingthe formula for estimated frequency as follows:E 'Est-freq(fax with recipient-dative) -- ~ x 6~- 100 xand similarly for the benefactive-dative case.
The resulting ratios can then be convertedto probabilities by normalizing them along with those for the attested entries for fax.In this case, the recipient reading will be preferred as the recipient Dative rule is moreproductive.This general approach could be refined in order to take account of Pinker's obser-vations concerning narrow-class rules, and already handles the possibility of special-ized subcases of more general rules.
For example, we could factor the computation ofproductivity between subtypes of the input type of a rule and derive more fine-grainedmeasures of productivity for each narrow class a rule applies to (assuming the typesystem is not recursive in such a way that the subclasses of the input type are infinite).In the case of specialized subcases of lexical rules that apply to a narrower ange oflexical items, but yield a more specific interpretation (such as the rules of Meat or Furgrinding, as opposed to general Grinding, proposed in Copestake and Briscoe \[1995\];see Section 7), the relative productivity of each rule will be estimated in the mannerdescribed above, but the more specialized rule is likely to be more productive sinceit will apply to fewer entries than the more general rule.
Similarly, in Figure 21, weassumed a Use-Substance l xical rule, but a more accurate stimation of probabilitiesmight be obtained by considering specialized subclasses.
This approach to derivingestimates of the productivity of lexical rules is applied to four denominal verb forma-516Briscoe and Copestake Lexical Rulestion rules in Briscoe and Copestake (1996), where the probabilities of the basic andderived word forms are estimated from part-of-speech tagged textual corpora.The probabilistic approach we have presented is part of a theory of language useor performance rather than one of competence orgrammatical representation.
As suchit is not a part of the T(D)FS representation language, which is intended as a generalformalism in which paradigmatic (lexical) and syntagmatic (syntactic and semantic)theories can be encoded or embedded.
This probabilistic approach to lexical rulesintegrates neatly with extant proposals to control application of lexical rules efficientlywithin a constraint-based framework, such as those of Meurers and Minnen (1997).To our knowledge it is the first attempt to formalize relatively informal accounts ofsemiproductivity based on the item-familiarity view of (morphological) lexical rules.As such it serves to highlight a potential difference between genuinely lexical rulesand unary syntactic rules, such as Adjunct Introduction, because in the probabilisticframework presented here the latter rule applied during construction of a sententialderivation will not affect the probability of the derivation, while lexical rules may,since they can output lexical entries with estimated conditional probabilities.The general claim we make here is that if we assume that speakers choose well-attested high-frequency forms to realize particular senses and listeners choose well-attested high-frequency senses when faced with ambiguity, then much of the semipro-ductivity of lexical rules is predicted.
This improves on the control principle suggestedin Copestake (1992), that lexical rules should only be applied if no interpretation wasapplicable that did not involve a lexical rule, since it allows for cases such as turkey,where the derived (meat) use is more frequent than the nonderived (animal) use inthe corpora which we have examined.
The two other control effects suggested inCopestake (1992) are both also superseded by the current proposal.
One of these wasto allow for blocking, which is discussed below.
The other was that more specificlexical rules should be preferred over more general ones.
We would expect hat, ingeneral, the more specialized rule will be more productive, as a natural consequenceof applying to a smaller class, but the earlier proposal would have had the undesir-able consequence that this was a fixed consequence, which could not be adjusted forcases where the generalization did not hold.
Thus the grammar writer was, in effect,required to consider both competence and performance when stipulating a rule.Blocking can be treated as a special case of this principle: if speakers use higher-frequency forms to convey a given meaning, an extended meaning will not becomeconventionalized if a common synonym exists.
This means that we do not have tostipulate a separate blocking principle in interpretation, since the blocked senses willnot be attested or will have a very low frequency.
And in generation, we assume thathigher-probability forms are preferred as a way of conveying a given meaning.
It isnecessary to allow for the possibility of unblocking, because of examples uch as thefollowing:(11) In the case of at least one county primary school..,  they were offered(with perfect iming) saute potatoes, carrots, runner beans and roast cow.
(Guardian ewspaper, May 16th 1990, in a story about mad cow disease.
)However, this is not the complete story, since we have not accounted formally for theextra implicatures that the use of a blocked form conveys, nor have we allowed for thegeneration of blocked forms (apart from in the circumstances where the generator'slexicon omits the synonym).
Both these problems require an account of the interfacewith pragmatics (see Copestake and Lascarides \[1997\] for one such account, whichintegrates probabilistic information i to pragmatic reasoning).517Computational Linguistics Volume 25, Number 4The method proposed above for estimating the probability of unattested but pos-sible derived lexical entries for given lexical items is simple.
Other more complexschemes could be developed, which, for example, took account of the average prob-ability of the output of a lexical rule.
This might be necessary, for example, to modelthe relative frequencies of -er versus -ee suffixation, since although the latter is moreproductive (by Baayen and Lieber's \[1991\] definition), tokens of the former are morefrequent overall (Barker 1995).
However, we have presented a simple approach ere,since we currently have no evidence that a more complex approach is justified, giventhat our main aim is to rank unseen senses by plausibility.
Another problem is theneed to ensure that classes have comparable frequency distributions.
This could mat-ter if there were competing lexical rules, defined on different but overlapping classes,since if one class has a high percentage of low-frequency words compared to the other,the estimate of its productivity will tend to be lower.
The productivity figure could beadjusted to allow for item frequency within classes.
We will not discuss this furtherhere, but see Baayen and Sproat (1996) for discussion of the related phenomenon ofambiguous derivational affixes.Schiitze (1997, 133f.)
argues, in the context of a detailed critique of Pinker (1989),that accounts of lexical rules that do not include a quantitative component cannotform the basis for a satisfactory theory of the acquisition of lexical rules by languagelearners.
The seed for the formation of a specific lexical rule must be comparisonof the semantics and alternation/derivation behavior of a class of lexical items, butsince there will always be noise in the form of exceptions because of the inherentsemiproductivity of the processes modeled by lexical rules, the induction of a rulemust be based in part on quantitative reasoning concerning the degree of generaliza-tion obtained from, or equivalently number of exceptions to, a putative lexical rule.The probabilistic approach proposed here could, we think, form the basis for suchreasoning (see Schiitze \[1997\] for a detailed discussion of the learning of lexical rules.
)Jackendoff (1997a, 115f.)
also notes that the learning of semiproductive l xical rulesmust be grounded in the prior existence of basic and derived lexical entries in thechild's lexicon.
Jackendoff (1997a, 124f.)
goes on to argue for a full entry theory oflexical organization in which the output of semiproductive l xical rules is entirelyspecified lexically.
He suggests that the advantage of positing semiproductive rulesremains because the "informational-cost" of learning such semiregular components ofthe lexicon is reduced.
The attraction of the current proposal, integrated with Meurersand Minnen's (1997) partial precompilation approach, is that we can do justice to thefacts of semiproductivity and also achieve an efficient and maximally nonredundantencoding of the lexicon.7.
Other Lexical ProcessesLexical rules should be able to account for processes of morphological inflection,derivation, and conversion.
Verb alternations are a class of morphological conversionrules that exhibit similar semiproductive behavior to other processes of derivationand conversion.
We have discussed ative in detail to demonstrate that a linguisti-cally adequate account of one such rule is possible in the proposed framework.
Asimilar approach to other alternations hould be feasible within the framework pre-sented.
However the formalism introduced so far provides no mechanism for buildingup any recursive structure.
There is no direct way of copying over information fromthe input into a different slot in the output structure.
The mechanism proposed willtherefore allow for at most one step of affixation as shown for the inflectional rulein Figure 7, because inputting the output to another ule would override the func-518Briscoe and Copestake Lexical Rulestion specification.
Similarly, the rules given in Section 5 rely on their semantic effectsapplying to known structures in the input.At first sight, this restriction might appear to preclude a treatment of rules such asPassive, which have been assumed to require manipulations of a list-valued SUBCATfeature (Pollard and Sag 1987).
However, since Passive is nonrecursive, the appropriateeffect can be expressed indirectly in our framework, by setting up a feature that issubsequently linked to the "real" SUBCAT.
We use ARGREAL (for argument realization)for this feature in the sketch of the account of Passive that follows.We assume the following types:unlinked-active : diffiist\] SYN  E R+  ALI unlinked-passive \ ] \ ]SYN: ARGREAL: \[LIST: \[HD: \[~ppbysign\]\[LAST: \[THL D :e~st\] \]linked-active \]SYN I ARGREAL: \[LIST: \[\]\]\]: LSUBCAT : \[\]linked-~assiveSYN: \[ARGREAL: \[LIST: \[TL: \[\]\]\]\]LSUBCAT : \[\]The Passive lexical rule simply states:unlinked-active ~-~ unlinked-passiveThe feature ARGREAL is encoded as a difference list on unlinked-active: that is, thereare two features, LIST and LAST, such that the value of the LIST feature is a list inthe usual HD/TL encoding and LAST is maintained as a pointer to the end of the list.Figure 22 shows an example of rule application to a lexical sign (equivalent to the signfor give shown in Figure 15 with a SUBCAT list instead of the categorial encoding).As above, we assume that linking occurs after lexical rule application, and thetypes linked-active and linked-passive provide the appropriate r entrancy statementsbetween ARGREAL and SUBCAT.
The effect is that the first element of the SUBCAT list inthe linked active form will be the last element in the linked passive, which is realizedas a PP\[by\] rather than an NP because of the constraints on the unlinked-passivetype.
An advantage of this approach to Passive is that linking generalizations canbe identical for active and passive forms, since they are expressed with respect othe ARGREAL slot rather than SUBCAT.
The feature ARGREAL has similarities with theargument structure feature used in more recent versions of HPSG, and we suspect itwould be possible to combine the functionality of both features.The formalism is therefore sufficiently expressive to encode some nonrecursivelist manipulation operations, if suitable pointers into the list, such as LAST, are setup lexically or on the output of rules that produce lists of sufficiently determinatestructure.
But this is inadequate for encoding rules that require that the entire semanticsof the input (which may itself arise from previous rule applications) be modified bythe semantics expressed by the rule.
To take a simple example, consider prefixationby re-, and assume that this is encoded by a lexical rule that can apply an arbitrarynumber of times, each time appending re- to a list of prefixes in the PHON value, and519Computational Linguistics Volume 25, Number 4In )ut:oblique-transfer-verbPHON : giveSYN ARGREAL :Output:"unlinked-passivePHON : give\[ \[HDLIST : TL :k LAST : \[\]SYN ARGREAL :Linked form:- linked-passivePHON giveSUBCAT : \[\]SYN :ARGREAL :Figure 22: npsign \] 1 \[ HD : npsign \]HD: pptosign\[ \[HD oppbysign 1\] r HD : npsignLIST: TL: L \ ]TL  \ [TL : f f l  \ [HD:ppt?s ign\ ]L LAS T ~\ [HD : TL ::e~st \]HD : npsign 1r HD : pptosign \] \] HD: \[\] ppbysign TL: TL: ff l\[TL:elis t \]LAsT:LIST: \[HD:\[\]\]\]\[\]TL: \[\]An example of Passive: oblique-transfer-verb is here assumed to be a subtype ofunlinked-active.I \[STEM : < tie > 1 PHON : ~ PREFIXES : < re, re > J1/EVENT:e | .
|EVENT:e ' /  1 L SEM: | re-rel 1 \[ re-rel \[ tie-rel < LARG:e' \] LARG:e" J 'LEVENT:e" J  >Figure 23reretieadding a re-rel to the semantics, as sketched in Figure 23, which shows a possiblerepresentation for reretie.
9In order to allow rules of this type to be expressed, an extension to the formalismis required to allow the values of list-valued features in the output of the rule tobe appended to the input values.
Figure 24 illustrates how this could be used toexpress a lexical rule for re- prefixation, with @ being used as the notation to indicatethat the value of the feature in the input be appended to the right of the structurestated in the output.
Figure 25 shows the equivalent rule in the conventional notation,using reentrancy to indicate copying.
However, this extension is more restricted inexpressivity than allowing arbitrary copying between input and output structures,and still does not make available the arbitrary list-manipulation operations that arepossible in conventional HPSG-style lexical rules.
This extension relies on the use of aflat representation such as minimal recursion semantics (MRS, Copestake t al.
1995),9 The semantic representation is ot intended to be taken too seriously, but the argument applies to anyrepresentation where it is assumed that, for example, reretie is not equivalent to retie.520Briscoe and Copestake Lexical RulesI PHON : I PREFIXES: < re >~ \]~-~ EVENT:e >~< re_ ,~: verb \] SEM : < \[ re-rel \] r r 1I_Figure 24Sketch of lexical rule for re- prefixation.verb \]PHON: \[PREFIXES: \[~\] ~-~SEM: \[\]Figure 25PHON: \[PREFIXES: \[ HD:reTL:\[\] ] \ ]r re-rel \]HD: |EVENT, e |SEM: ~ARG: e' J\[ TL : listCorresponding lexical rule using reentrancy notation for copying.since the reason we can simply use Append to construct the semantics of the output isthat the semantics of any sign is always encoded as a list, without any embedding ofstructures.
Thus we are exploiting the fact that semantic omposition in the grammaras a whole relies on the Append operation.The use of Append in conjunction with a flat semantic representation is alsoadequate to express potentially recursive rules of regular sense extension, such as"grinding" and "portioning," as lexical rules (Copestake and Briscoe 1992, 1995).
Ingeneral, we believe that our formulation of lexical rules is expressive nough to cap-ture inflectional, derivational, and conversion processes, to model both systematic andidiosyncratic exceptions, and preemption by both synonymy and lexical form.A rule such as Adjunct Introduction (Bouma and van Noord 1994), which addsadjunct categories to the SUBCAT lists of verb entries recursively creating a potentiallyinfinite set of derived entries, seems to us to be a clear example of a nonlexical unarysyntactic rule.
Firstly, it appears to be fully productive in that it is neither lexicallygoverned nor subject o idiosyncratic exceptions, blocking, or other forms of semipro-ductivity.
Secondly, its function is to add, or better interpolate, adjunct categories tothe SUBCAT list "on demand," given the syntactic ontext, and this is best achieved bythe syntactic omponent during syntactic analysis.It is possible that an account of Adjunct Introduction could be formulated as aTDFS lexical rule via Append, though it is difficult to see how such a formulationcould account naturally for argument-adjunct in erpolation of the kind found in ex-amples like: United flies from New York daily to the Gulf Coast.
However, once we adoptthe probabilistic approach to lexical rules, it becomes increasingly unnatural and un-motivated to attempt to interpret such processes as lexical.
The item-familiarity theoryof lexical productivity clearly should not extend to attempting to model whether aspecific verb is more likely to appear with one or two adjuncts, because this is in nosense a definition of a "possible lexeme.
"Furthermore, there are now clear theoretical advantages for creating a distinctclass of unary nonlexical rules.
In the TDFS framework, an interface between thelexical component and syntactic-semantic component of the grammar is required sothat some lexical default specification does not persist into the syntactic omponent (forexample, defaults concerning rammatical agreement; see Lascarides and Copestake\[1999\]).
Thus, such rules must necessarily apply after all genuinely lexical rules, and521Computational Linguistics Volume 25, Number 4creating such a separation means that it should be possible to ensure that the two typesof rule cannot interact in ways that lead to unrestricted generative capacity in the fullsystem.
The two main criteria for distinguishing such rules that we have identifiedso far are (semi)productivity and the creation of (un)bounded list structures in thesyntactic representation.8.
ConclusionsBoth Goldberg (1995) and Jackendoff (1997a, 1997b) contrast he lexical rule approachto bounded dependencies with one that treats each construction independently andcharacterizes relations between constructions, omewhat vaguely, in terms of "inheri-tance."
Jackendoff (1997b, 556f.)
makes the point that lexical rules in lexicalist frame-works are expressive nough to describe bounded constructions and any idiosyncraticmeanings they convey, so they can be used to capture relations among such construc-tions.
He argues against his approach though, because he suggests that the elementsof the constructions related by such rules are often not lexical.
Copestake and Briscoe(1995) make the same point with respect o examples of systematic metonymy, wheresuch semiproductive "lexical" rules apply to noun phrase constructions.
Jackendoffalso argues, however, that constructions need to be treated as a kind of phrasal exicalitem whose (idiosyncratic) meaning is learnt like that of a lexical item (Jackendoff1997b, 554).
For us, the defining characteristic of a lexical rule is that it requires omelisting of properties to accurately express its behavior, whether this be because it islexically governed, has exceptions, is underspecified in its effects, or whatever.
Thusidioms must be lexically specified, though they are best treated as particularly idiosyn-cratic phrases/constructions, rather than lexical items.For Jackendoff (1997a, 115f.
), the crucial distinction is not lexical/nonlexical butproductive/semiproductive rul .
Jackendoff's definition of a productive rule encom-passes rules such as Plural Noun Formation or Third Singular Verb Formation (seeSection 2 above), despite the existence of irregular derived forms, because he arguesthat such a rule's output need not be listed.
It is semiproductive only to the extentthat certain aspects of its output can be overridden or blocked by lexical specificationof exceptions.
He reserves the term semiproductive for rules, such as Denominal VerbFormation (shelf ~-+ shelve), where the exact output of the rule is underspecified andthe existence of the derived words is not guaranteed.
Thus, the precise meaning of thedenominal verb is partly systematic ('to put x in/on y') and partly idiosyncratic andunpredictable ( .g., to saddle (a horse) means 'to put a saddle on a horse's back,' while toshelve abook means 'to put a book on a shelf'), and the phonological form is not alwaysidentical with or entirely predictable from the nominal form (e.g., shelve).
Furthermore,there are many nouns that do not have corresponding denominal forms (mustard vs.butter, teapot vs. knife, etc.).
Jackendoff argues that the nature of the exceptions to thelatter type of rule requires (full) listing in the lexicon of the derived forms, while theformer does not.In our approach, lexical rules are those that require some element of lexical/listedspecification, whether it be the listing of irregular forms that override aspects of therule output or of idiosyncratic aspects of the resulting meaning, or the unattested sta-tus of the derived entry.
The approach to lexical rules we have advocated, integrating arestrictive default-based formalization with partial precompilation and a probabilisticaccount of item-familiarity and semiproductivity, is capable of expressing inflectional,derivational, and conversion rules whose domain is (within) that of a bounded epen-dency construction (i.e., includes "alternation" rules relating bounded constructions).This approach reintegrates construction-based generalizations with more traditional522Briscoe and Copestake Lexical Ruleslexical rules, provides a very general means for encoding semiproductivity, and makesa principled istinction between lexical and unary syntactic rules that should allow thegenerative power of the overall grammar to be restricted.
To summarize: lexical rulescannot perform arbitrary operations on unbounded lists; they are unidirectional, buthave limited reversibility properties appropriate to account for backformation; theyinvolve no extension to the underlying logic of the TDFS framework; they require thestatement of what changes, not what stays the same; they are subject o type con-straints and can exploit the default inheritance hierarchy to capture generalizations;they can be semiproductive and are predicted to be sensitive to blocking, exceptions,conventionalization, a d so forth; and they allow a linguistically elegant, and accurate,account of the dative construction/alternation, subsuming the insights emerging fromrecent detailed analyses of this specific lexical rule.Furthermore, we outlined ways in which this approach can be extended straight-forwardly to deal with rules apparently involving more complex SUBCAT list manipu-lations, and with recursive processes of derivation and conversion and their associatedsemantics, without sacrificing these desirable properties.
It follows from our approachthat some putative lexical rules should be treated as unary syntactic rules.
A poten-tial advantage of this division of labor is that it may even be possible to develop aseparate treatment of unary syntactic rules that does not utilize category-valued vari-ables over list-valued features.
In any case, if these rules only apply to the outputof the lexicon, this will avoid the increase in generative capacity identified by Car-penter (1991), resulting from the interaction of recursion, arbitrary list operations andunbounded lists, by keeping list-valued features bounded uring lexical rule appli-cation, and only allowing unbounded additions to, or limited modification of, suchfeatures during syntactic processing.AcknowledgmentsWe would like to thank Tony Kroch, MarkLiberman, Geoff Nunberg, Mark Steedmanand, especially, Annie Zaenen for helpfulinput and advice.
The content and structureof the paper is, we hope, much improvedon the basis of three anonymous referees'insightful comments on an earlier draft.
Allthe ideas and mistakes, nevertheless, remainour responsibility.ReferencesAit-Kaci, Hassan.
1984.
A Lattice-TheoreticApproach to Computation Based on a Calculuso/Partially Ordered Type Structures.
Doctoraldissertation, University of Pennsylvania.Aronoff, Mark.
1976.
Word Formation inGenerative Grammar.
Linguistic InquiryMonograph 1.
MIT Press, Cambridge,MA.Baayen, Harald and Rochelle Lieber.
1991.Productivity and English derivation: Acorpus-based study.
Linguistics,29:801-843.Baayen, Harald and Richard Sproat.
1996.Estimating lexical priors forlow-frequency morphologicallyambiguous forms.
ComputationalLinguistics, 22(2):155-166.Barker, Chris.
1995.
Episodic -ee in English:Thematic relations and new wordformation.
In Mandy Simons and TeresaGalloway, editors, Semantics and LinguisticTheory V. Cornell University, Ithaca, NY,pages 1-18.Bauer, Laurie.
1983.
English Word-Formation.Cambridge University Press, Cambridge,England.Boguraev, Bran and Ted Briscoe.
1989.Utilizing the LDOCE grammar codes.
InBran Boguraev and Ted Briscoe, editors,Computational Lexicography for NaturalLanguage Processing.
Longman, London,pages 85-116.Bouma, Gosse.
1992.
Feature structures andnonmonotonicity.
ComputationalLinguistics, 18(2):183-204.Bouma, Gosse and Gertjan van Noord.
1994.Constraint-based categorial grammar.
InProceedings ofthe 32nd Annual Meeting,pages 147-154.
Las Cruces, NM.Association for ComputationalLinguistics.Box, George E. P. and George C. Tiao.
1973.Bayesian I ference inStatistical Analysis.Addison-Wesley, Reading, MA.Brew, Chris.
1995.
Stochastic HPSG.
In523Computational Linguistics Volume 25, Number 4Proceedings ofthe 7th European Conference ofthe Association of Computational Linguistics,pages 83-89, Dublin, Ireland.Briscoe, Ted and Ann Copestake.
1996.Controlling the application of lexicalrules.
In Proceedings ofthe ACL SIGLEXWorkshop on Breadth and Depth of SemanticLexicons, pages 7-19, Santa Cruz.Briscoe, Ted, Ann Copestake, and BranBoguraev.
1990.
Enjoy the paper: Lexicalsemantics via lexicology.
In Proceedings ofthe 13th International Conference onComputational Linguistics, pages 42-47,Helsinki.Calcagno, Michael.
1995.
Interpreting lexicalrules.
In Proceedings ofthe Conference onFormal Grammar, Barcelona.Carpenter, Bob.
1991.
The generative powerof categorial grammars and head-drivenphrase structure grammars with lexicalrules.
Computational Linguistics,17(3):301-314.Carpenter, Bob.
1992.
The Logic of TypedFeature Structures.
Cambridge UniversityPress, Cambridge, England.Carpenter, Bob.
1993.
Skeptical andcredulous default unification withapplication to templates and inheritance.In Ted Briscoe, Ann Copestake, andValeria de Paiva, editors, Inheritance,Defaults and the Lexicon.
CambridgeUniversity Press, Cambridge, England,pages 13-37.Chang, Nancy.
1995.
A Constraint-BasedApproach to Linking.
M.Phil.
dissertation,Cambridge University.Church, Ken and William Gale.
1991.
Acomparison of the enhanced Good-Turingand deleted estimation methods forestimating probabilities of Englishbigrams.
Computer Speech and Language,5(1):19-54.Church, Ken and Robert Mercer.
1993.Introduction to the special issue oncomputational linguistics using largecorpora.
Computational Linguistics,19(1):1-24.Copestake, Ann.
1992.
The Representation fLexical Semantic Information.
Doctoraldissertation, University of Sussex.Cognitive Science Research Paper CSRP280.Copestake, Ann.
1993.
Defaults in lexicalrepresentation.
I  Ted Briscoe, AnnCopestake, and Valeria de Paiva, editors,Inheritance, Defaults and the Lexicon.Cambridge University Press, Cambridge,England, pages 223-245.Copestake, Ann and Ted Briscoe.
1992.Lexical operations in a unification basedframework.
In James Pustejovsky andSabine Bergler, editors, Lexical Semanticsand Knowledge Representation.
Proceedings ofthe first SIGLEX Workshop, Berkeley, CA.Springer-Verlag, Berlin, pages 101-119.Copestake, Ann and Ted Briscoe.
1995.Semi-productive polysemy and senseextension.
Journal of Semantics, 12:15-67.Copestake, Ann, Dan Flickinger, RobertMalouf, Suzanne Riehemann, and IvanSag.
1995.
Translation using minimalrecursion semantics.
In Proceedings oftheSixth International Conference on Theoreticaland Methodological Issues in MachineTranslation (TMI95), pages 15-32, Leuven,Belgium.Copestake, Ann and Alex Lascarides.
1997.Integrating symbolic and statisticalrepresentations: the lexicon-pragmaticsinterface.
In Proceedings ofthe 35th AnnualMeeting of the Association for ComputationalLinguistics and 8th Conference oftheEuropean Chapter of the Association forComputational Linguistics (ACL-EACL 97),Madrid, pages 136-143.Daelemans, Walter, Konrad de Smedt, andGerald Gazdar.
1992.
Inheritance innatural anguage processing.Computational Linguistics, 18(2):205-218.Davis, Antony.
1996.
Lexical semantics andlinking in the hierarchical lexicon.Doctoral dissertation, Stanford University.Dowty, David.
1989.
On the semanticcontent of the notion "thematic role."
InGennaro Chierchia, Barbara Partee, andRay Turner, editors, Property Theory, TypeTheory and Natural Language Semantics.Reidel, Dordrecht, The Netherlands,pages 69-129.Evans, Roger and Gerald Gazdar.
1989.Inference in DATR.
In Proceedings ofthe 4thConference ofthe European Chapter of theAssociation for Computational Linguistics(EACL-1989), pages 66-71, Manchester,England.Evans, Roger and Gerald Gazdar.
1996.DATR: A language for lexical knowledgerepresentation.
Computational Linguistics,22(2):167-216.Fillmore, Charles, Paul Kay, and MaryO'Connor.
1988.
Regularity andidiomaticity in grammatical constructions.Language, 64:501-538.Gazdar, Gerald, Ewan Klein, GeoffreyPullum, and Ivan Sag.
1985.
GeneralizedPhrase Structure Grammar.
Basil Blackwell,Oxford.Goldberg, Adele.
1995.
Constructions.Chicago University Press, Chicago, IL.Green, Georgia.
1974.
Semantics and SyntacticRegularity.
Indiana University Press.Jackendoff, Ray.
1975.
Morphological and524Briscoe and Copestake Lexical Rulessemantic regularities in the lexicon.Language, 51(3):639-671.Jackendoff, Ray.
1997a.
The Architecture oftheLanguage Faculty.
MIT Press, Cambridge,MA.Jackendoff, Ray.
1997b.
Twistin' the nightaway.
Language, 73(3):534--559.Johnson, Mark and Jochen Dorre.
1995.Memoization of coroutined constraints.Proceedings ofthe 33rd Annual Meeting,pages 100-107, Cambridge, MA.Association of Computational Linguistics.Joshi, Aravind.
1987.
An introduction totree-adjoining grammars.
In AlexisManaster-Ramer, ditor, Mathematics ofLanguage.
John Benjamins, Amsterdam,pages 87-115.Kelly, Martin and Steven Martin.
1994.Domain-general abilities applied todomain-specific tasks: Sensitivity toprobabilities in perception, cognition andlanguage.
Lingua, 92:105-140.King, Paul.
1994.
An expanded logicalformalism for head-driven phrasestructure grammar.
Arbeitspapiere desSFB 340/59, University of Ti~bingen.Labov, William.
1972.
Sociolinguistic Patterns.University of Pennsylvania Press,Philadelphia.Lascarides, Alex and Nicholas Ashen 1991.Discourse relations and defeasibleknowledge.
In Proceedings ofthe 29thAnnual Meeting, pages 55-63, Berkeley,CA.
Association for ComputationalLinguistics.Lascarides, Alex and Nicholas Asher.
1993.Temporal interpretation, discourserelations and common sense entailment.Linguistics and Philosophy, 16:437-493.Lascarides, Alex, Ted Briscoe, NicholasAsher, and Ann Copestake.
1995.
Orderindependent and persistent typed defaultunification.
Linguistics and Philosophy,19(1):1-89.Lascarides, Alex and Ann Copestake.
1995.The pragmatics of word meaning.
InMandy Simons and Teresa Galloway,editors, Semantics and Linguistic Theory V.Cornell University, Ithaca, NY,pages 204-221.Lascarides, Alex and Ann Copestake.
1999.Default representation in constraint-basedframeworks.
Computational Linguistics,25(1):55-106.Lascarides, Alex, Ann Copestake, and TedBriscoe.
1996.
Ambiguity and coherence.Journal of Semantics, 13:41-65.Levin, Beth.
1992.
Towards aLexicalOrganization of English Verbs.
University ofChicago Press, Chicago, IL.Levin, Beth and Malka Rappaport Hovav.1995.
Unaccusativity at the Syntax-LexicalSemantics Interface.
MIT Press, Cambridge,MA.Malouf, Robert.
1999.
Practical defaultinheritance in constraint-based grammars.Paper presented at Ohio State University.Meurers, Detmar.
1995.
Towards a semanticsfor lexical rules as used in HPSG.
InProceedings ofthe Conference on FormalGrammar, Barcelona.
Available on-line athttp://www.sfs.nphil.uni-tuebingen.de/Ndm/LR/sem.ps.gzMeurers, Detmar and Guido Minnen.
1997.A computational treatment of HPSGlexical rules as covariation i  lexicalentries.
Computational Linguistics,23(4):543-596.Nicholls, Diane.
1995.
Can fully productivelexical rules be defined and can theyapply cross-linguistically?
Acquilex-IIWorking Paper 79: http://www.cl.cam.ac.uk/Research/NL/acquilex/.Pinker, Steven.
1989.
Learnability andCognition: The Acquisition of ArgumentStructure.
MIT Press, Cambridge, MA.Pirelli, Vito, Nilda Ruimy, andSimonetta Montemagni.
1994.
Lexicalregularities and lexicon compilation.Acquilex-II Working Paper 36: http://www.cl.cam.ac.uk/Research/NL/acquilex/.Pollard, Carl and Ivan Sag.
1987.
AnInformation-based Approach to Syntax andSemantics: Volume 1Fundamentals.
CSLILecture Notes 13, CSLI Publications,Stanford, CA.Pollard, Carl and Ivan Sag.
1994.Head-driven Phrase Structure Grammar.Chicago University Press, Chicago.Resnik, Philip.
1992.
Probabilistic lexicalizedtree adjoining rammar.
In Proceedings ofthe 14th International Conference onComputational Linguistics (COLING-92),pages 418-424, Nantes, France.Riehemann, Suzanne.
1993.
Word Formationin Lexical Type Hierarchies.
M.Phil.dissertation, University of Tfibingen,Germany.Russell, Graham, Afzal Ballim, John Carroll,and Susan Warwick-Armstrong.
1993.
Apractical approach to multiple defaultinheritance for unification-based lexicons.In Ted Briscoe, Ann Copestake, andValeria de Paiva, editors, Inheritance,Defaults and the Lexicon.
CambridgeUniversity Press, Cambridge, England,pages 137-147.Sanfilippo, Antonio.
1990.
GrammaticalRelations, Thematic Roles and Verb Semantics.Doctoral dissertation, Centre forCognitive Science, University ofEdinburgh.525Computational Linguistics Volume 25, Number 4Sanfilippo, Antonio.
1992.
Verbal diathesis,knowledge acquisition, lexiconconstruction and dictionary compilation.Acquilex-II Working Papers 1: http://www.cl.cam.ac.uk/Research/NL/acquilex/.Sanfilippo, Antonio.
1993.
LKB encoding oflexical knowledge from machine-readabledictionaries.
In Ted Briscoe, AnnCopestake, and Valeria de Paiva, editors,Inheritance, Defaults and the Lexicon.Cambridge University Press, Cambridge,England, pages 190-222.Schabes, Yves.
1992.
Stochastic lexicalizedtree adjoining grammar.
In Proceedings ofthe 14th International Conference onComputational Linguistics (COLING-92),pages 425-432, Nantes, France.Schi~tze, Hinrich.
1997.
Ambiguity Resolutionin Language Learning: Computational andCognitive Models.
CSLI Lecture Notes 71,CSLI Publications, Stanford, CA.Shieber, Stuart.
1992.
Constraint-basedGrammar Formalisms.
MIT Press,Cambridge, MA.Steedman, Mark.
1996.
Surface Structure andInterpretation.
MIT Press, Cambridge, MA.Wasow, Tom.
1980.
Major and minor rulesin lexical grammar.
In Tuen Hoekstra,Harry van der Hulst, and MichaelMoortgat, editors, Lexical Grammar.
Foris,Dordrecht, pages 285-312.Wechsler, Steven.
1991.
Argument Structureand Linking.
Doctoral dissertation,Stanford University.Zeevat, Henk, Ewan Klein, and Jo Calder.1987.
An introduction to unificationcategorial grammar.
In Nicholas Haddock,Ewan Klein, and Glyn Morrill, editors,Categorial Grammar, Uni~cation Grammarand Parsing: Working Papers in CognitiveScience 1.
Centre for Cognitive Science,University of Edinburgh, pages 195-222.526
