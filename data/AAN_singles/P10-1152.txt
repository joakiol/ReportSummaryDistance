Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1502?1511,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsViterbi Training for PCFGs:Hardness Results and Competitiveness of Uniform InitializationShay B. Cohen and Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{scohen,nasmith}@cs.cmu.eduAbstractWe consider the search for a maximumlikelihood assignment of hidden deriva-tions and grammar weights for a proba-bilistic context-free grammar, the problemapproximately solved by ?Viterbi train-ing.?
We show that solving and even ap-proximating Viterbi training for PCFGs isNP-hard.
We motivate the use of uniform-at-random initialization for Viterbi EM asan optimal initializer in absence of furtherinformation about the correct model pa-rameters, providing an approximate boundon the log-likelihood.1 IntroductionProbabilistic context-free grammars are an essen-tial ingredient in many natural language process-ing models (Charniak, 1997; Collins, 2003; John-son et al, 2006; Cohen and Smith, 2009, interalia).
Various algorithms for training such modelshave been proposed, including unsupervised meth-ods.
Many of these are based on the expectation-maximization (EM) algorithm.There are alternatives to EM, and one such al-ternative is Viterbi EM, also called ?hard?
EM or?sparse?
EM (Neal and Hinton, 1998).
Insteadof using the parameters (which are maintained inthe algorithm?s current state) to find the true pos-terior over the derivations, Viterbi EM algorithmuses a posterior focused on the Viterbi parse ofthose parameters.
Viterbi EM and variants havebeen used in various settings in natural languageprocessing (Yejin and Cardie, 2007; Wang et al,2007; Goldwater and Johnson, 2005; DeNero andKlein, 2008; Spitkovsky et al, 2010).Viterbi EM can be understood as a coordinateascent procedure that locally optimizes a function;we call this optimization goal ?Viterbi training.
?In this paper, we explore Viterbi training forprobabilistic context-free grammars.
We firstshow that under the assumption that P 6= NP, solv-ing and even approximating the Viterbi trainingproblem is hard.
This result holds even for hid-den Markov models.
We extend the main hardnessresult to the EM algorithm (giving an alternativeproof to this known result), as well as the problemof conditional Viterbi training.
We then describea ?competitiveness?
result for uniform initializa-tion of Viterbi EM: we show that initialization ofthe trees in an E-step which uses uniform distri-butions over the trees is optimal with respect to acertain approximate bound.The rest of this paper is organized as follows.
?2gives background on PCFGs and introduces somenotation.
?3 explains Viterbi training, the declar-ative form of Viterbi EM.
?4 describes a hardnessresult for Viterbi training.
?5 extends this result toa hardness result of approximation and ?6 furtherextends these results for other cases.
?7 describesthe advantages in using uniform-at-random initial-ization for Viterbi training.
We relate these resultsto work on the k-means problem in ?8.2 Background and NotationWe assume familiarity with probabilistic context-free grammars (PCFGs).
A PCFGG consists of:?
A finite set of nonterminal symbols N;?
A finite set of terminal symbols ?;?
For each A ?
N, a set of rewrite rules R(A) ofthe form A ?
?, where ?
?
(N ?
?
)?, andR = ?A?NR(A);?
For each rule A ?
?, a probability ?A??.
Thecollection of probabilities is denoted ?, and theyare constrained such that:?(A?
?)
?
R(A), ?A??
?
0?A ?
N,??:(A??)?R(A)?A??
= 1That is, ?
is grouped into |N| multinomial dis-tributions.1502Under the PCFG, the joint probability of a stringx ?
??
and a grammatical derivation z is1p(x, z | ?)
=?(A??)?R(?A??)fA??
(z) (1)= exp?(A??)?RfA??
(z) log ?A?
?where fA??
(z) is a function that ?counts?
thenumber of times the rule A ?
?
appears inthe derivation z. fA(z) will similarly denote thenumber of times that nonterminal A appears in z.Given a sample of derivations z = ?z1, .
.
.
, zn?,let:FA??
(z) =n?i=1fA??
(zi) (2)FA(z) =n?i=1fA(zi) (3)We use the following notation forG:?
L(G) is the set of all strings (sentences) x thatcan be generated using the grammar G (the?language ofG?).?
D(G) is the set of all possible derivations z thatcan be generated using the grammarG.?
D(G, x) is the set of all possible derivations zthat can be generated using the grammarG andhave the yield x.3 Viterbi TrainingViterbi EM, or ?hard?
EM, is an unsupervisedlearning algorithm, used in NLP in various set-tings (Yejin and Cardie, 2007; Wang et al, 2007;Goldwater and Johnson, 2005; DeNero and Klein,2008; Spitkovsky et al, 2010).
In the context ofPCFGs, it aims to select parameters ?
and phrase-structure trees z jointly.
It does so by iterativelyupdating a state consisting of (?, z).
The stateis initialized with some value, then the algorithmalternates between (i) a ?hard?
E-step, where thestrings x1, .
.
.
, xn are parsed according to a cur-rent, fixed ?, giving new values for z, and (ii) anM-step, where the ?
are selected to maximize like-lihood, with z fixed.With PCFGs, the E-step requires running an al-gorithm such as (probabilistic) CKY or Earley?s1Note that x = yield(z); if the derivation is known, thestring is also known.
On the other hand, there may be manyderivations with the same yield, perhaps even infinitely many.algorithm, while the M-step normalizes frequencycounts FA??
(z) to obtain the maximum likeli-hood estimate?s closed-form solution.We can understand Viterbi EM as a coordinateascent procedure that approximates the solution tothe following declarative problem:Problem 1.
ViterbiTrainInput: G context-free grammar, x1, .
.
.
, xn train-ing instances from L(G)Output: ?
and z1, .
.
.
, zn such that(?, z1, .
.
.
, zn) = argmax?,zn?i=1p(xi, zi | ?)
(4)The optimization problem in Eq.
4 is non-convex and, as we will show in ?4, hard to op-timize.
Therefore it is necessary to resort to ap-proximate algorithms like Viterbi EM.Neal and Hinton (1998) use the term ?sparseEM?
to refer to a version of the EM algorithmwhere the E-step finds the modes of hidden vari-ables (rather than marginals as in standard EM).Viterbi EM is a variant of this, where the E-step finds the mode for each xi?s derivation,argmaxz?D(G,xi) p(xi, z | ?
).We will refer toL(?, z) =n?i=1p(xi, zi | ?)
(5)as ?the objective function of ViterbiTrain.
?Viterbi training and Viterbi EM are closely re-lated to self-training, an important concept insemi-supervised NLP (Charniak, 1997; McCloskyet al, 2006a; McClosky et al, 2006b).
With self-training, the model is learned with some seed an-notated data, and then iterates by labeling new,unannotated data and adding it to the original an-notated training set.
McClosky et al consider self-training to be ?one round of Viterbi EM?
with su-pervised initialization using labeled seed data.
Werefer the reader to Abney (2007) for more details.4 Hardness of Viterbi TrainingWe now describe hardness results for Problem 1.We first note that the following problem is knownto be NP-hard, and in fact, NP-complete (Sipser,2006):Problem 2.
3-SATInput: A formula ?
=?mi=1 (ai ?
bi ?
ci) in con-junctive normal form, such that each clause has 31503S?2cccccccccccccccccccccccccccccccTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTS?1A1eeeeeeeeeeeeeeeeeeeYYYYYYYYYYYYYYYYYYYA2eeeeeeeeeeeeeeeeeeeYYYYYYYYYYYYYYYYYYYUY1,0qqqqqqqMMMMMMMUY2,1qqqqqqqMMMMMMMUY4,0qqqqqqqMMMMMMMUY1,0qqqqqqqMMMMMMMUY2,1qqqqqqqMMMMMMMUY3,1qqqqqqqMMMMMMMVY?1 VY1 VY2 VY?2 VY?4 VY4 VY?1 VY1 VY2 VY?2 VY3 VY?31 0 1 0 1 0 1 0 1 0 1 0Figure 1: An example of a Viterbi parse tree which represents a satisfying assignment for ?
= (Y1?Y2?
Y?4)?
(Y?1?
Y?2?Y3).In ?
?, all rules appearing in the parse tree have probability 1.
The extracted assignment would be Y1 = 0, Y2 = 1, Y3 =1, Y4 = 0.
Note that there is no usage of two different rules for a single nonterminal.literals.Output: 1 if there is a satisfying assignment for ?and 0 otherwise.We now describe a reduction of 3-SAT to Prob-lem 1.
Given an instance of the 3-SAT problem,the reduction will, in polynomial time, create agrammar and a single string such that solving theViterbiTrain problem for this grammar and stringwill yield a solution for the instance of the 3-SATproblem.Let ?
=?mi=1 (ai ?
bi ?
ci) be an instance ofthe 3-SAT problem, where ai, bi and ci are liter-als over the set of variables {Y1, .
.
.
, YN} (a literalrefers to a variable Yj or its negation, Y?j).
Let Cjbe the jth clause in ?, such that Cj = aj ?
bj ?
cj .We define the following context-free grammarG?and string to parse s?:1.
The terminals of G?
are the binary digits ?
={0, 1}.2.
We create N nonterminals VYr , r ?
{1, .
.
.
, N} and rules VYr ?
0 and VYr ?
1.3.
We create N nonterminals VY?r , r ?
{1, .
.
.
, N} and rules VY?r ?
0 and VY?r ?
1.4.
We create UYr,1 ?
VYrVY?r and UYr,0 ?VY?rVYr .5.
We create the rule S?1 ?
A1.
For each j ?
{2, .
.
.
,m}, we create a rule S?j ?
S?j?1Ajwhere S?j is a new nonterminal indexed by?j ,?ji=1Ci and Aj is also a new nonterminalindexed by j ?
{1, .
.
.
,m}.6.
Let Cj = aj ?
bj ?
cj be clause j in ?.
LetY (aj) be the variable that aj mentions.
Let(y1, y2, y3) be a satisfying assignment for Cjwhere yk ?
{0, 1} and is the value of Y (aj),Y (bj) and Y (cj) respectively for k ?
{1, 2, 3}.For each such clause-satisfying assignment, weadd the rule:Aj ?
UY (aj),y1UY (bj),y2UY (cj),y3 (6)For each Aj , we would have at most 7 rules ofthat form, since one rule will be logically incon-sistent with aj ?
bj ?
cj .7.
The grammar?s start symbol is S?n .8.
The string to parse is s?
= (10)3m, i.e.
3mconsecutive occurrences of the string 10.A parse of the string s?
using G?
will be usedto get an assignment by setting Yr = 0 if the ruleVYr ?
0 or VY?r ?
1 are used in the derivation ofthe parse tree, and 1 otherwise.
Notice that at thispoint we do not exclude ?contradictions?
comingfrom the parse tree, such as VY3 ?
0 used in thetree together with VY3 ?
1 or VY?3 ?
0.
The fol-lowing lemma gives a condition under which theassignment is consistent (so contradictions do notoccur in the parse tree):Lemma 1.
Let ?
be an instance of the 3-SATproblem, and letG?
be a probabilistic CFG basedon the above grammar with weights ??.
If the(multiplicative) weight of the Viterbi parse of s?is 1, then the assignment extracted from the parsetree is consistent.Proof.
Since the probability of the Viterbi parseis 1, all rules of the form {VYr , VY?r} ?
{0, 1}which appear in the parse tree have probability 1as well.
There are two possible types of inconsis-tencies.
We show that neither exists in the Viterbiparse:15041.
For any r, an appearance of both rules of theform VYr ?
0 and VYr ?
1 cannot occur be-cause all rules that appear in the Viterbi parsetree have probability 1.2.
For any r, an appearance of rules of the formVYr ?
1 and VY?r ?
1 cannot occur, becausewhenever we have an appearance of the ruleVYr ?
0, we have an adjacent appearance ofthe rule VY?r ?
1 (because we parse substringsof the form 10), and then again we use the factthat all rules in the parse tree have probability 1.The case of VYr ?
0 and VY?r ?
0 is handledanalogously.Thus, both possible inconsistencies are ruled out,resulting in a consistent assignment.Figure 1 gives an example of an application ofthe reduction.Lemma 2.
Define ?, G?
as before.
There exists??
such that the Viterbi parse of s?
is 1 if and onlyif ?
is satisfiable.
Moreover, the satisfying assign-ment is the one extracted from the parse tree withweight 1 of s?
under ??.Proof.
(=?)
Assume that there is a satisfying as-signment.
Each clause Cj = aj ?
bj ?
cj is satis-fied using a tuple (y1, y2, y3) which assigns valuefor Y (aj), Y (bj) and Y (cj).
This assignment cor-responds the following ruleAj ?
UY (aj),y1UY (bj),y2UY (cj),y3 (7)Set its probability to 1, and set al other rules ofAj to 0.
In addition, for each r, if Yr = y, set theprobabilities of the rules VYr ?
y and VY?r ?
1?yto 1 and VY?r ?
y and VYr ?
1?
y to 0.
The restof the weights for S?j ?
S?j?1Aj are set to 1.This assignment of rule probabilities results in aViterbi parse of weight 1.
(?=) Assume that the Viterbi parse has prob-ability 1.
From Lemma 1, we know that we canextract a consistent assignment from the Viterbiparse.
In addition, for each clause Cj we have aruleAj ?
UY (aj),y1UY (bj),y2UY (cj),y3 (8)that is assigned probability 1, for some(y1, y2, y3).
One can verify that (y1, y2, y3)are the values of the assignment for the corre-sponding variables in clause Cj , and that theysatisfy this clause.
This means that each clause issatisfied by the assignment we extracted.In order to show an NP-hardness result, we needto ?convert?
ViterbiTrain to a decision problem.The natural way to do it, following Lemmas 1and 2, is to state the decision problem for Viter-biTrain as ?given G and x1, .
.
.
, xn and ?
?
0,is the optimized value of the objective functionL(?, z) ?
???
and use ?
= 1 together with Lem-mas 1 and 2.
(Naturally, an algorithm for solvingViterbiTrain can easily be used to solve its deci-sion problem.
)Theorem 3.
The decision version of the Viterbi-Train problem is NP-hard.5 Hardness of ApproximationA natural path of exploration following the hard-ness result we showed is determining whether anapproximation of ViterbiTrain is also hard.
Per-haps there is an efficient approximation algorithmfor ViterbiTrain we could use instead of coordi-nate ascent algorithms such as Viterbi EM.
Recallthat such algorithms?
main guarantee is identify-ing a local maximum; we know nothing about howfar it will be from the global maximum.We next show that approximating the objectivefunction of ViterbiTrain with a constant factor of ?is hard for any ?
?
(12 , 1] (i.e., 1/2 +  approxima-tion is hard for any  ?
1/2).
This means that, un-der the P 6= NP assumption, there is no efficient al-gorithm that, given a grammar G and a sample ofsentences x1, .
.
.
, xn, returns ??
and z?
such that:L(?
?, z?)
?
?
?max?,zn?i=1p(xi, zi | ?)
(9)We will continue to use the same reduction from?4.
Let s?
be the string from that reduction, andlet (?, z) be the optimal solution for ViterbiTraingiven G?
and s?.
We first note that if p(s?, z |?)
< 1 (implying that there is no satisfying as-signment), then there must be a nonterminal whichappears along with two different rules in z.This means that we have a nonterminal B ?
Nwith some rule B ?
?
that appears k times,while the nonterminal appears in the parse r ?k + 1 times.
Given the tree z, the ?
that maxi-mizes the objective function is the maximum like-lihood estimate (MLE) for z (counting and nor-malizing the rules).2 We therefore know thatthe ViterbiTrain objective function, L(?, z), is at2Note that we can only make p(z | ?, x) greater by using?
to be the MLE for the derivation z.1505most(kr)k, because it includes a factor equalto(fB??(z)fB(z))fB??
(z), where fB(z) is the num-ber of times nonterminal B appears in z (hencefB(z) = r) and fB??
(z) is the number of timesB ?
?
appears in z (hence fB??
(z) = k).
Forany k ?
1, r ?
k + 1:(kr)k?
(kk + 1)k?12(10)This means that if the value of the objective func-tion of ViterbiTrain is not 1 using the reductionfrom ?4, then it is at most 12 .
If we had an efficientapproximate algorithm with approximation coeffi-cient ?
> 12 (Eq.
9 holds), then in order to solve3-SAT for formula ?, we could run the algorithmon G?
and s?
and check whether the assignmentto (?, z) that the algorithm returns satisfies ?
ornot, and return our response accordingly.If ?
were satisfiable, then the true maximalvalue of L would be 1, and the approximation al-gorithm would return (?, z) such that L(?, z) ??
> 12 .
z would have to correspond to a satisfy-ing assignment, and in fact p(z | ?)
= 1, becausein any other case, the probability of a derivationwhich does not represent a satisfying assignmentis smaller than 12 .
If ?
were not satisfiable, thenthe approximation algorithm would never return a(?, z) that results in a satisfying assignment (be-cause such a (?, z) does not exist).The conclusion is that an efficient algorithm forapproximating the objective function of Viterbi-Train (Eq.
4) within a factor of 12 +  is unlikelyto exist.
If there were such an algorithm, we coulduse it to solve 3-SAT using the reduction from ?4.6 Extensions of the Hardness ResultAn alternative problem to Problem 1, a variant ofViterbi-training, is the following (see, for exam-ple, Klein and Manning, 2001):Problem 3.
ConditionalViterbiTrainInput: G context-free grammar, x1, .
.
.
, xn train-ing instances from L(G)Output: ?
and z1, .
.
.
, zn such that(?, z1, .
.
.
, zn) = argmax?,zn?i=1p(zi | ?, xi) (11)Here, instead of maximizing the likelihood, wemaximize the conditional likelihood.
Note thatthere is a hidden assumption in this problem def-inition, that xi can be parsed using the grammarG.
Otherwise, the quantity p(zi | ?, xi) is notwell-defined.
We can extend ConditionalViterbi-Train to return ?
in the case of not having a parsefor one of the xi?this can be efficiently checkedusing a run of a cubic-time parser on each of thestrings xi with the grammarG.An approximate technique for this problem issimilar to Viterbi EM, only modifying the M-step to maximize the conditional, rather than joint,likelihood.
This new M-step will not have a closedform and may require auxiliary optimization tech-niques like gradient ascent.Our hardness result for ViterbiTrain applies toConditionalViterbiTrain as well.
The reason isthat if p(z, s?
| ??)
= 1 for a ?
with a satisfyingassignment, thenL(G) = {s?}
andD(G) = {z}.This implies that p(z | ?
?, s?)
= 1.
If ?
is unsat-isfiable, then for the optimal ?
of ViterbiTrain wehave z and z?
such that 0 < p(z, s?
| ??)
< 1and 0 < p(z?, s?
| ??)
< 1, and therefore p(z |?
?, s?)
< 1, which means the conditional objec-tive function will not obtain the value 1.
(Notethat there always exist some parameters ??
thatgenerate s?.)
So, again, given an algorithm forConditionalViterbiTrain, we can discern betweena satisfiable formula and an unsatisfiable formula,using the reduction from ?4 with the given algo-rithm, and identify whether the value of the objec-tive function is 1 or strictly less than 1.
We get theresult that:Theorem 4.
The decision problem of Condition-alViterbiTrain problem is NP-hard.where the decision problem of ConditionalViter-biTrain is defined analogously to the decisionproblem of ViterbiTrain.We can similarly show that finding the globalmaximum of the marginalized likelihood:max?1nn?i=1log?zp(xi, z | ?)
(12)is NP-hard.
The reasoning follows.
Using thereduction from before, if ?
is satisfiable, thenEq.
12 gets value 0.
If ?
is unsatisfiable, then wewould still get value 0 only if L(G) = {s?}.
IfG?
generates a single derivation for (10)3m, thenwe actually do have a satisfying assignment from1506Lemma 1.
Otherwise (more than a single deriva-tion), the optimal ?
would have to give fractionalprobabilities to rules of the form VYr ?
{0, 1} (orVY?r ?
{0, 1}).
In that case, it is no longer truethat (10)3m is the only generated sentence, whichis a contradiction.The quantity in Eq.
12 can be maximized ap-proximately using algorithms like EM, so thisgives a hardness result for optimizing the objec-tive function of EM for PCFGs.
Day (1983) pre-viously showed that maximizing the marginalizedlikelihood for hidden Markov models is NP-hard.We note that the grammar we use for all of ourresults is not recursive.
Therefore, we can encodethis grammar as a hidden Markov model, strength-ening our result from PCFGs to HMMs.37 Uniform-at-Random InitializationIn the previous sections, we showed that solvingViterbi training is hard, and therefore requires anapproximation algorithm.
Viterbi EM, which is anexample of such algorithm, is dependent on an ini-tialization of either ?
to start with an E-step or zto start with an M-step.
In the absence of a better-informed initializer, it is reasonable to initializez using a uniform distribution over D(G, xi) foreach i.
If D(G, xi) is finite, it can be done effi-ciently by setting ?
= 1 (ignoring the normaliza-tion constraint), running the inside algorithm, andsampling from the (unnormalized) posterior givenby the chart (Johnson et al, 2007).
We turn nextto an analysis of this initialization technique thatsuggests it is well-motivated.The sketch of our result is as follows: wefirst give an asymptotic upper bound for the log-likelihood of derivations and sentences.
Thisbound, which has an information-theoretic inter-pretation, depends on a parameter ?, which de-pends on the distribution from which the deriva-tions were chosen.
We then show that this boundis minimized when we pick ?
such that this distri-bution is (conditioned on the sentence) a uniformdistribution over derivations.Let q(x) be any distribution over L(G) and ?some parameters for G. Let f(z) be some featurefunction (such as the one that counts the numberof appearances of a certain rule in a derivation),and then:Eq,?
[f ] ,?x?L(G)q(x)?z?D(G,x)p(z | ?, x)f(z)3We thank an anonymous reviewer for pointing this out.which gives the expected value of the feature func-tion f(z) under the distribution q(x)?p(z | ?, x).We will make the following assumption aboutG:Condition 1.
There exists some ?I such that?x ?
L(G),?z ?
D(G, x), p(z | ?I , x) =1/|D(G, x)|.This condition is satisfied, for example, whenGis in Chomsky normal form and for all A,A?
?
N,|R(A)| = |R(A?)|.
Then, if we set ?A??
=1/|R(A)|, we get that all derivations of x willhave the same number of rules and hence the sameprobability.
This condition does not hold for gram-mars with unary cycles because |D(G, x)|may beinfinite for some derivations.
Such grammars arenot commonly used in NLP.Let us assume that some ?correct?
parameters??
exist, and that our data were drawn from a dis-tribution parametrized by ??.
The goal of this sec-tion is to motivate the following initialization for?, which we call UniformInit:1.
Initialize z by sampling from the uniform dis-tribution over D(G, xi) for each xi.2.
Update the grammar parameters using maxi-mum likelihood estimation.7.1 Bounding the ObjectiveTo show our result, we require first the followingdefinition due to Freund et al (1997):Definition 5.
A distribution p1 is within ?
?
1 ofa distribution p2 if for every event A, we have1??p1(A)p2(A)?
?
(13)For any feature function f(z) and any twosets of parameters ?2 and ?1 for G and for anymarginal q(x), if p(z | ?1, x) is within ?
ofp(z | ?2, x) for all x then:Eq,?1 [f ]??
Eq,?2 [f ] ?
?Eq,?1 [f ] (14)Let ?0 be a set of parameters such that we performthe following procedure in initializing Viterbi EM:first, we sample from the posterior distributionp(z | ?0, x), and then update the parameters withmaximum likelihood estimate, in a regular M-step.Let ?
be such that p(z | ?0, x) is within ?
ofp(z | ?
?, x) (for all x ?
L(G)).
(Later we willshow that UniformInit is a wise choice for making?
small.
Note that UniformInit is equivalent to theprocedure mentioned above with ?0 = ?I .
)1507Consider p?n(x), the empirical distribution overx1, .
.
.
, xn.
As n ?
?, we have that p?n(x) ?p?
(x), almost surely, where p?
is:p?
(x) =?zp?
(x, z | ??)
(15)This means that as n ?
?
we have Ep?n,?
[f ] ?Ep?,?
[f ].
Now, let z0 = (z0,1, .
.
.
, z0,n) be sam-ples from p(z | ?0, xi) for i ?
{1, .
.
.
, n}.
Then,from simple MLE computation, we know that thevaluemax?
?n?i=1p(xi, z0,i | ??)
(16)=?(A??)?R(FA??(z0)FA(z0))FA??
(z0)We also know that for ?0, from the consistency ofMLE, for large enough samples:FA??
(z0)FA(z0)?Ep?n,?0 [fA??
]Ep?n,?0 [fA](17)which means that we have the following as ngrows (starting from the ViterbiTrain objectivewith initial state z = z0):max?
?n?i=1p(xi, z0,i | ??)
(18)(Eq.
16)=?(A??)?R(FA??(z0)FA(z0))FA??(z0)(19)(Eq.
17)??(A??
)?R(Ep?n,?0 [fA??
]Ep?n,?0 [fA])FA??
(z0)(20)We next use the fact that p?n(x) ?
p?
(x) for largen, and apply Eq.
14, noting again our assumptionthat p(z | ?0, x) is within ?
of p(z | ?
?, x).
Wealso let B =?i|zi|, where |zi| is the number ofnodes in the derivation zi.
Note that FA(zi) ?B.
The above quantity (Eq.
20) is approximatelybounded above by?(A??)?R1?2B(Ep?,??
[fA??]Ep?,??
[fA])FA??(z0)(21)=1?2|R|B?(A??)?R(??A??)FA??
(z0) (22)Eq.
22 follows from:??A??
=Ep?,??
[fA??]Ep?,??
[fA](23)If we continue to develop Eq.
22 and applyEq.
17 and Eq.
23 again, we get that:1?2|R|B?(A??)?R(??A??)FA??(z0)=1?2|R|B?(A??)?R(??A??)FA??(z0)?FA(z0)FA(z0)?1?2|R|B?(A??)?R(??A??)Ep?,?0[fA??]Ep?,?0[fA]?FA(z0)?1?2|R|B?(A??)?R(??A??)?2??A??FA(z0)?1?2|R|B???(A??)?R(??A??)n??A?????
??
?T (?
?,n)B?2/n(24)=(1?2|R|B)T (?
?, n)B?2/n (25), d(?;?
?, |R|, B) (26)where Eq.
24 is the result of FA(z0) ?
B.For two series {an} and {bn}, let ?an ' bn?denote that limn??
an ?
limn??
bn.
In otherwords, an is asymptotically larger than bn.
Then,if we changed the representation of the objec-tive function of the ViterbiTrain problem to log-likelihood, for ??
that maximizes Eq.
18 (withsome simple algebra) we have:1nn?i=1log2 p(xi, z0,i | ??)
(27)' ?2|R|Bnlog2 ?+B?2n(1nlog2 T (?
?, n))= ?2|R|Bnlog2 ??
|N|B?2|N|n?A?NH(?
?, A)(28)whereH(?
?, A) = ??(A??)?R(A)??A??
log2 ??A??
(29)is the entropy of the multinomial for nonter-minal A.
H(?
?, A) can be thought of as theminimal number of bits required to encode achoice of a rule from A, if chosen independentlyfrom the other rules.
All together, the quantityB|N|n(?A?NH(?
?, A))is the average number ofbits required to encode a tree in our sample using1508?
?, while removing dependence among all rulesand assuming that each node at the tree is chosenuniformly.4 This means that the log-likelihood, forlarge n, is bounded from above by a linear func-tion of the (average) number of bits required tooptimally encode n trees of total size B, while as-suming independence among the rules in a tree.We note that the quantityB/nwill tend toward theaverage size of a tree, which, under Condition 1,must be finite.Our final approximate bound from Eq.
28 re-lates the choice of distribution, from which samplez0, to ?.
The lower bound in Eq.
28 is a monotone-decreasing function of ?.
We seek to make ?
assmall as possible to make the bound tight.
We nextshow that the uniform distribution optimizes ?
inthat sense.7.2 Optimizing ?Note that the optimal choice of ?, for a single xand for candidate initializer ?
?, is?opt(x,??
;?0) = supz?D(G,x)p(z | ?0, x)p(z | ?
?, x)(30)In order to avoid degenerate cases, we will add an-other condition on the true model, ??
:Condition 2.
There exists ?
> 0 such that, forany x ?
L(G) and for any z ?
D(G, x), p(z |?
?, x) ?
?
.This is a strong condition, forcing the cardinal-ity of D(G) to be finite, but it is not unreason-able if natural language sentences are effectivelybounded in length.Without further information about ??
(otherthan that it satisfies Condition 2), we may wantto consider the worst-case scenario of possible ?,hence we seek initializer ?0 such that?
(x;?0) , sup??opt(x,?
;?0) (31)is minimized.
If ?0 = ?I , then we have thatp(z | ?I , x) = |D(G, x)|?1 , ?x.
Together withCondition 2, this implies thatp(z | ?I , x)p(z | ?
?, x)??x?
(32)4We note that Grenander (1967) describes a (lin-ear) relationship between the derivational entropy andH(?
?, A).
The derivational entropy is defined as h(?
?, A) =?Px,z p(x, z | ??)
log p(x, z | ??
), where z ranges overtrees that have nonterminal A as the root.
It follows im-mediately from Grenander?s result thatPAH(?
?, A) ?PA h(?
?, A).and hence ?opt(x,??)
?
?x/?
for any ?
?, hence?
(x;?I) ?
?x/?
.
However, if we choose ?0 6=?I , we have that p(z?
| ?0, x) > ?x for some z?,hence, for ??
such that it assigns probability ?
onz?, we have thatsupz?D(G,x)p(z | ?0, x)p(z | ?
?, x)>?x?
(33)and hence ?opt(x,??;??)
> ?x/?
, so ?(x;??)
>?x/?
.
So, to optimize for the worst-case scenarioover true distributions with respect to ?, we aremotivated to choose ?0 = ?I as defined in Con-dition 1.
Indeed, UniformInit uses ?I to initializethe state of Viterbi EM.We note that if ?I was known for a specificgrammar, then we could have used it as a directinitializer.
However, Condition 1 only guaranteesits existence, and does not give a practical way toidentify it.
In general, as mentioned above, ?
= 1can be used to obtain a weighted CFG that sat-isfies p(z | ?, x) = 1/|D(G, x)|.
Since we re-quire a uniform posterior distribution, the num-ber of derivations of a fixed length is finite.
Thismeans that we can converted the weighted CFGwith ?
= 1 to a PCFG with the same posterior(Smith and Johnson, 2007), and identify the ap-propriate ?I .8 Related WorkViterbi training is closely related to the k-meansclustering problem, where the objective is to findk centroids for a given set of d-dimensional pointssuch that the sum of distances between the pointsand the closest centroid is minimized.
The ana-log for Viterbi EM for the k-means problem is thek-means clustering algorithm (Lloyd, 1982), a co-ordinate ascent algorithm for solving the k-meansproblem.
It works by iterating between an E-like-step, in which each point is assigned the closestcentroid, and an M-like-step, in which the cen-troids are set to be the center of each cluster.?k?
in k-means corresponds, in a sense, to thesize of our grammar.
k-means has been shown tobe NP-hard both when k varies and d is fixed andwhen d varies and k is fixed (Aloise et al, 2009;Mahajan et al, 2009).
An open problem relating toour hardness result would be whether ViterbiTrain(or ConditionalViterbiTrain) is hard even if we donot permit grammars of arbitrarily large size, orat least, constrain the number of rules that do notrewrite to terminals (in our current reduction, the1509size of the grammar grows as the size of the 3-SATformula grows).On a related note to ?7, Arthur and Vassilvit-skii (2007) described a greedy initialization al-gorithm for initializing the centroids of k-means,called k-means++.
They show that their ini-tialization is O(log k)-competitive; i.e., it ap-proximates the optimal clusters assignment by afactor of O(log k).
In ?7.1, we showed thatuniform-at-random initialization is approximatelyO(|N|L?2/n)-competitive (modulo an additiveconstant) for CNF grammars, where n is the num-ber of sentences, L is the total length of sentencesand ?
is a measure for distance between the truedistribution and the uniform distribution.5Many combinatorial problems in NLP involv-ing phrase-structure trees, alignments, and depen-dency graphs are hard (Sima?an, 1996; Good-man, 1998; Knight, 1999; Casacuberta and de laHiguera, 2000; Lyngs?
and Pederson, 2002;Udupa and Maji, 2006; McDonald and Satta,2007; DeNero and Klein, 2008, inter alia).
Ofspecial relevance to this paper is Abe and Warmuth(1992), who showed that the problem of findingmaximum likelihood model of probabilistic au-tomata is hard even for a single string and an au-tomaton with two states.
Understanding the com-plexity of NLP problems, we believe, is crucial aswe seek effective practical approximations whennecessary.9 ConclusionWe described some properties of Viterbi train-ing for probabilistic context-free grammars.
Weshowed that Viterbi training is NP-hard and, infact, NP-hard to approximate.
We gave motivationfor uniform-at-random initialization for deriva-tions in the Viterbi EM algorithm.AcknowledgmentsWe acknowledge helpful comments by the anony-mous reviewers.
This research was supported byNSF grant 0915187.ReferencesN.
Abe and M. Warmuth.
1992.
On the computationalcomplexity of approximating distributions by prob-5Making the assumption that the grammar is in CNF per-mits us to use L instead of B, since there is a linear relation-ship between them in that case.abilistic automata.
Machine Learning, 9(2?3):205?260.S.
Abney.
2007.
Semisupervised Learning for Compu-tational Linguistics.
CRC Press.D.
Aloise, A. Deshpande, P. Hansen, and P. Popat.2009.
NP-hardness of Euclidean sum-of-squaresclustering.
Machine Learning, 75(2):245?248.D.
Arthur and S. Vassilvitskii.
2007. k-means++: Theadvantages of careful seeding.
In Proc.
of ACM-SIAM symposium on Discrete Algorithms.F.
Casacuberta and C. de la Higuera.
2000.
Com-putational complexity of problems on probabilisticgrammars and transducers.
In Proc.
of ICGI.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proc.
of AAAI.S.
B. Cohen and N. A. Smith.
2009.
Shared logis-tic normal distributions for soft parameter tying inunsupervised grammar induction.
In Proc.
of HLT-NAACL.M.
Collins.
2003.
Head-driven statistical models fornatural language processing.
Computational Lin-guistics, 29(4):589?637.W.
H. E. Day.
1983.
Computationally difficult parsi-mony problems in phylogenetic systematics.
Jour-nal of Theoretical Biology, 103.J.
DeNero and D. Klein.
2008.
The complexity ofphrase alignment problems.
In Proc.
of ACL.Y.
Freund, H. Seung, E. Shamir, and N. Tishby.
1997.Selective sampling using the query by committee al-gorithm.
Machine Learning, 28(2?3):133?168.S.
Goldwater and M. Johnson.
2005.
Bias in learningsyllable structure.
In Proc.
of CoNLL.J.
Goodman.
1998.
Parsing Inside-Out.
Ph.D. thesis,Harvard University.U.
Grenander.
1967.
Syntax-controlled probabilities.Technical report, Brown University, Division of Ap-plied Mathematics.M.
Johnson, T. L. Griffiths, and S. Goldwater.
2006.Adaptor grammars: A framework for specifyingcompositional nonparameteric Bayesian models.
InAdvances in NIPS.M.
Johnson, T. L. Griffiths, and S. Goldwater.
2007.Bayesian inference for PCFGs via Markov chainMonte Carlo.
In Proc.
of NAACL.D.
Klein and C. Manning.
2001.
Natural lan-guage grammar induction using a constituent-context model.
In Advances in NIPS.K.
Knight.
1999.
Decoding complexity in word-replacement translation models.
ComputationalLinguistics, 25(4):607?615.S.
P. Lloyd.
1982.
Least squares quantization in PCM.In IEEE Transactions on Information Theory.R.
B. Lyngs?
and C. N. S. Pederson.
2002.
The con-sensus string problem and the complexity of com-paring hidden Markov models.
Journal of Comput-ing and System Science, 65(3):545?569.M.
Mahajan, P. Nimbhorkar, and K. Varadarajan.
2009.The planar k-means problem is NP-hard.
In Proc.
ofInternational Workshop on Algorithms and Compu-tation.1510D.
McClosky, E. Charniak, and M. Johnson.
2006a.Effective self-training for parsing.
In Proc.
of HLT-NAACL.D.
McClosky, E. Charniak, and M. Johnson.
2006b.Reranking and self-training for parser adaptation.
InProc.
of COLING-ACL.R.
McDonald and G. Satta.
2007.
On the complex-ity of non-projective data-driven dependency pars-ing.
In Proc.
of IWPT.R.
M. Neal and G. E. Hinton.
1998.
A view of theEM algorithm that justifies incremental, sparse, andother variants.
In Learning and Graphical Models,pages 355?368.
Kluwer Academic Publishers.K.
Sima?an.
1996.
Computational complexity of prob-abilistic disambiguation by means of tree-grammars.In In Proc.
of COLING.M.
Sipser.
2006.
Introduction to the Theory of Com-putation, Second Edition.
Thomson Course Tech-nology.N.
A. Smith and M. Johnson.
2007.
Weighted andprobabilistic context-free grammars are equally ex-pressive.
Computational Linguistics, 33(4):477?491.V.
I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.Manning.
2010.
Viterbi training improves unsuper-vised dependency parsing.
In Proc.
of CoNLL.R.
Udupa and K. Maji.
2006.
Computational com-plexity of statistical machine translation.
In Proc.
ofEACL.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
Whatis the Jeopardy model?
a quasi-synchronous gram-mar for question answering.
In Proc.
of EMNLP.C.
Yejin and C. Cardie.
2007.
Structured local trainingand biased potential functions for conditional ran-dom fields with application to coreference resolu-tion.
In Proc.
of HLT-NAACL.1511
