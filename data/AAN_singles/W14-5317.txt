Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146?154,Dublin, Ireland, August 23 2014.Experiments in Sentence Language Identification with Groups of SimilarLanguagesBen KingDepartment of EECSUniversity of MichiganAnn Arborbenking@umich.eduDragomir RadevDepartment of EECSSchool of InformationUniversity of MichiganAnn Arborradev@umich.eduSteven AbneyDepartment of LinguisticsUniversity of MichiganAnn Arborabney@umich.eduAbstractLanguage identification is a simple problem that becomes much more difficult when its usualassumptions are broken.
In this paper we consider the task of classifying short segments of text inclosely-related languages for the Discriminating Similar Languages shared task, which is brokeninto six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czechand Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish,and (F) American and British English.
We consider a number of different methods to boostclassification performance, such as feature selection and data filtering, but we ultimately find thata simple na?
?ve Bayes classifier using character and word n-gram features is a strong baseline thatis difficult to improve on, achieving an average accuracy of 0.8746 across the six tasks.1 IntroductionLanguage identification constitutes the first stage of many NLP pipelines.
Before applying tools trainedon specific languages, one must determine the language of the text.
It is also is often considered to be asolved task because of the high accuracy of language identification methods in the canonical formulationof the problem with long monolingual documents and a set of mostly dissimilar languages to choosefrom.
We consider a different setting with much shorter text in the form of single sentences drawn fromvery similar languages or dialects.This paper describes experiments related to and our submissions to the Discriminating Similar Lan-guages (DSL) shared task.
This shared task has six subtasks, each a classification task in which a sentencemust be labeled as belonging to a small set of related languages:?
Task A: Bosnian vs. Croatian vs. Serbian?
Task B: Indonesian vs. Malay?
Task C: Czech vs. Slovak?
Task D: Brazilian vs. European Portuguese?
Task E: Argentinian vs. Peninsular Spanish?
Task F: American vs. British EnglishThe first three tasks involve classes that could be rightly called separate languages or dialects.
Theclasses of each of the final three tasks have high mutual intelligibility and are so similar that somelinguists may not even classify them as separate dialects.
We will use the term ?language variant?
torefer to such classes.In this paper we experiment with several types of methods aimed at improving the classification ac-curacy of these tasks: machine learning methods, data pre-processing, feature selection, and additionaltraining data.
We find that a simple na?
?ve Bayes classifier using character and word n-gram features isa strong baseline that is difficult to improve on.
Because this paper covers so many different types ofmethods, its format eschews the standard ?Results?
section, instead providing comparisons of methodsas they are presented.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1462 Related WorkRecent directions in language identification have included finer-grained language identification (Kingand Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs(Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closelyrelated languages.Language identification for closely related languages has been considered by several researchers,though it has lacked a systematic evaluation before the DSL shared task.
The problem of distinguish-ing Croatian from Serbian and Slovenian is explored by Ljube?si?c et al.
(2007), who used a list of mostfrequent words along with a Markov model and a word blacklist, a list of words that are not allowedto appear in a certain language.
A similar approach was later used by Tiedemann and Ljube?si?c (2012)to distinguish Bosnian, Croatian, and Serbian.
They further develop the idea of a blacklist classifier,loosening the binary restriction of the earlier work?s blacklist and considering the frequencies of wordsrather than their absolute counts.
This blacklist classifier is able to outperform a na?
?ve Bayes classifierwith large amounts of training data.
They also find training on parallel data to be important, as it al-lows the machine learning methods to pick out features relating to the differences between the languagesthemselves, rather than learning differences in domain.Zampieri et al.
consider classes that would be most often classified as language varieties rather thanseparate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al.,2013).
A similar problem of distinguishing among Chinese text from mainland China, Singapore, andTaiwan is considered by Huang and Lee (2008) who approach the problem by computing similaritybetween a document and a corpus according to the size of the intersection between the sets of types ineach.A similar, but somewhat different problem of automatically identifying lexical variants betweenclosely related languages is considered in (Peirsman et al., 2010).
Using distributional methods, theyare able to identify Netherlandic Dutch synonyms for words from Belgian Dutch.3 DataThis paper?s training data and evaluation data both come from the DSL corpus collection (DSLCC)(Tan et al., 2014).
We use the training section of this data for training and the development section forevaluation.
The training section consists of 18,000 labeled instances per class, while the developmentsection has 2,000 labeled instances per class.In order to try to increase classifier accuracy (and to avoid the problems with the task F trainingdata), we decided to collect additional training data for each open-class task.
For each task, we collectednewspaper text from the appropriate websites for each of the 2?3 languages.
We used regular expressionsto split the text into sentences, and created a set of rules to filter out strings that were unlikely to be goodsentences.
Because the pages on the newspaper websites tended to have some boilerplate text, we collatedall the sentences and only kept one copy of each sentence.Task Language/Dialect Newspaper Sentences WordsABosnian Nezavisne Novine 175,741 3,250,648Croatian Novi List 231,271 4,591,318Serbian Ve?cernje Novosti 239,390 5,213,507BIndonesian Kompas 114,785 1,896,138Malay Berita Harian 36,144 695,597CCzech Den?
?k 160,972 2,432,393Slovak Denn?
?k SME 62,908 970,913DBrazilian Portuguese O Estado de S. Paulo 558,169 11,199,168European Portuguese Correio da Manh?a 148,745 2,979,904EArgentinian Spanish La Naci?on 333,246 7,769,941Peninsular Spanish El Pa?
?s 195,897 4,329,480FAmerican English The New York Times 473,350 10,491,641British English The Guardian 971,097 20,288,294Table 1: Sources and amounts of training data collected for the open track for each task.147In order to create balanced training data, for each task we downsampled the number of sentences ofthe larger collection(s) to match the number of sentences in the smaller collection.
For example, wedownsampled the British English collection to 473,350 sentences and combined it with the AmericanEnglish sentences to create the training data for English.
Figure 1 shows results of training using thisexternal data.3.1 FeaturesWe use many types of features that have been found to be useful in previous language identificationwork: word unigrams, word bigrams, and character n-grams (2 ?
n ?
6).
Character n-grams are simplysubstrings of the sentence and may include in addition to letters, whitespace, punctuation, digits, andanything else that might be in the sentence.
Words, for the purpose of word unigrams and bigrams, aresimply maximal tokens not containing any punctuation, digit, or whitespace.When instances are encoded into feature vectors, each feature has a value equal to the number of timesit occured in the corresponding sentence, so the majority of features have a value of 0 for any giveninstance, but it is possible for a feature to occur multiple times in a sentence and have a value greaterthan 1.0 in the feature vector.
Table 2 below compares the performance of a na?
?ve Bayes classifier usingeach of the different feature groups below.Word CharacterTask All 1 2 2 3 4 5 6Bosnian/Croatian/Serbian 0.9348 0.9290 0.8183 0.7720 0.8808 0.9412 0.9338 0.9323Indonesian/Malay 0.9918 0.9943 0.9885 0.8545 0.9518 0.9833 0.9908 0.9930Czech/Slovak 0.9998 1.0000 0.9985 0.9980 0.9998 0.9998 1.0000 1.0000Portuguese 0.9535 0.9468 0.9493 0.7935 0.8888 0.9318 0.9468 0.9570Spanish 0.8623 0.8738 0.8625 0.7673 0.8273 0.8513 0.8610 0.8660English 0.4970 0.4948 0.5005 0.4825 0.4988 0.5010 0.5048 0.4993Average 0.8732 0.8731 0.8529 0.7780 0.8412 0.8681 0.8729 0.8746Table 2: Accuracies compared for different sets of features compared.
The classifier used here is na?
?veBayes.4 MethodsOur baseline method against which we compare all other models is a na?
?ve Bayes classifier using wordunigram features trained on the DSL-provided training data.
The methods we compare to it can bebroken into three classes: other machine learning methods, feature selection methods, and data filteringmethods.The classification pipeline used here has the following stages: (1) data filtering, (2) feature extraction,(3) feature selection, (4) training, and (5) classification.4.1 Machine Learning MethodsWe will use the following notation throughout this section.
An instance x, that is, a sentence to beclassified, with a corresponding class label y is encoded into a feature vector f(x), where each entryis an integer denoting how many times the feature corresponding to that entry?s index occurred in thesentence.
The class label here is a language and it?s drawn from a small set y ?
Y .In addition to the na?
?ve Bayes classifier, we also experiment with two versions of logistic regressionand a support vector machine classifier.
The MALLET machine learning library implementations areused for the first three classifiers (McCallum, 2002) and SVMLight is used for the fourth (Joachims, ).Na?
?ve Bayes A na?
?ve Bayes classifier models the class label as an independent combination of inputfeatures.148P (y|f(x)) =1P (f(x))P (y)n?i=1P (f(x)i|y) (1)As na?
?ve Bayes is a generative classifier, it has been shown to be able to outperform discriminativeclassifiers when the number of training instances is small compared to the number of features (Ng andJordan, 2002).
This classifier is additionally advantageous in that it has a simple closed-form solutionfor maximizing its log likelihood.Logistic Regression A logistic regression classifier is a discriminative classifier whose parameters areencoded in a vector ?.
The conditional probability of a class label over an instance (x, y) is modeled asfollows:P (y|x; ?)
=1Z(x; ?
)exp {f(x, y) ?
?}
; Z(x, ?)
=?y?Yexp {f(x, y) ?
?}
(2)The parameter vector ?
is commonly estimated by maximizing the log-likelihood of this function overthe set of training instances (x, y) ?
T in the following way:?
= argmax??
(x,y)?TlogP (yi|xi; ?)?
?R(?)
(3)The term R(?)
above is a regularization term.
It is common for such a classifier to overfit the pa-rameters to the training data.
To keep this from happening, a regularization term can be added whichkeeps the parameters in ?
from growing too large.
Two common choices for this function are L2 and L1normalization:RL2= ||?||22=n?i=1?2i, RL1= ||?||1=n?i=1|?i| (4)L2 regularization is well-grounded theoretically, as it is equivalent to a model with a Gaussian prioron the parameters (Rennie, 2004).
But L1 regularization has a reputation for enforcing sparsity on theparameters.
In fact, it has been shown to be quite effective when the number of irrelevant dimensions isgreater than the number of training examples, which we expect to be the case with many of the tasks inthis paper (Ng, 2004).Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attemptsto find a boundary that linearly separates the training data with the maximum possible margin.
SVMshave been shown to be a very efficient and high accuracy method to classify data across a wide varietyof different types of tasks (Tsochantaridis et al., 2004).Table 3 below compares these machine learning methods.
Because of its consistently good perfor-mance across tasks, we use a na?
?ve Bayes classifier throughout the rest of the paper.4.2 Feature Selection MethodsWe expect that the majority of features are not relevant to the classification task, and so we experimentedwith several methods of feature selection, both manual and automatic.Information Gain As a fully automatic method of feature extraction, we used information gain toscore features according to their expected usefulness.
Information gain (IG) is an information theoreticconcept that (colloquially) measures the amount of knowledge about the class label that is gained byhaving access to a specific feature.
If f is the occurence an individual feature and?f the non-occurenceof a feature, we measure its information gain by the following formula:G(f) = P (f)??
?y?YP (y|f)logP (y|f)?
?+ P (?f)??
?y?YlogP (y|?f)logP (y|?f)??(5)149TaskLogisticRegression(L2-norm)LogisticRegression(L1-norm)Na?
?ve Bayes SVMBosnian/Croatian/Serbian 0.9138 0.9135 0.9290 0.9100Indonesian/Malay 0.9878 0.9810 0.9943 0.9873Czech/Slovak 0.9983 0.9958 1.0000 0.9985Portuguese 0.9383 0.9368 0.9468 0.9325Spanish 0.8843 0.8770 0.8738 0.8768English 0.5000 0.4945 0.4948 0.4958Average 0.8704 0.8648 0.8731 0.8668Table 3: Comparison of different machine learning methods using word unigram features on the sixtasks.To reduce the number of features being used in classification (and to hopefully remove irrelevantfeatures), we choose the 10,000 features with the highest IG scores.
IG considers each feature indepen-dently, so it is possible that redundant feature sets could be chosen.
For example, it might happen thatboth the quadrigram ther and the trigram the score highly according to IG and are both selected, eventhough they are highly correlated with one another.Parallel Text Feature Selection Because IG feature selection often seemed to choose features morerelated to differences in domain than to differences in language (see Table 7), we wanted to try to isolatefeatures that are specific to language differences.
It has been shown in previous work that training onparallel text can help to isolate language differences since the domains of the languages are identical(Tiedemann and Ljube?si?c, 2012).
For each of the tasks,1we use translations of the complete Bible as aparallel corpus, running IG feature selection exactly as above.
Table 4 below gives more details aboutthe texts used.Task Language/Dialect BibleBIndonesian Alkitab dalam Bahasa Indonesia Masa KiniMalay 2001 Today?s Malay VersionCCzech Cesk?y studijn??
prekladSlovak Slovensk?y Ekumenick?y BibliaDBrazilian Portuguese a B?IBLIA para todosEuropean Portuguese Almeida Revista e Corrigida (Portugal)EArgentinian Spanish La Palabra (versi?on hispanoamericana)Peninsular Spanish La Palabra (versi?on espa?nola)FAmerican English New International VersionBritish English New International Version AnglicizedTable 4: Bibles used as parallel corpora for feature selection.Manual Feature Selection We also used manual feature selection, selecting features to use in the clas-sifiers from lists published on Wikipedia comparing the two languages.
Of course some of the features inlists like these are features that are quite difficult to detect using NLP (especially before the language hasbeen identified) such as characteristic passive or genitive constructions.
But there are many features thatwe are able to detect and use in a list of manually selected features, such as character n-grams relatingto morphology and spelling and word n-grams relating to vocabulary differences.Table 5 below compares these feature selection methods on each task.
Since the manual feature selec-tion suggested all types of features, including character n-gram and word unigram and bigram features,the experiments in this section use all features described in Section 3.1.
The results show that any typeof feature selection consistently hurts performance, though IG hurts the least, and it should be notedthat in certain cases with other machine learning methods, IG feature selection actually yielded better1excluding Task A, for which we were unable to find a Bible in Latin-script Serbian or any Bible in Bosnian150performance than all features.
That the feature selection methods designed to isolate language-specificfeatures performed so poorly is one indicator that the labeled data has additional differences that are nottied to the languages themselves.
We discuss this idea further in Section 5.Task No feature selection IG Parallel ManualBosnian/Croatian/Serbian 0.9348 0.9300 ?
0.6328Indonesian/Malay 0.9918 0.9768 0.8093 0.8485Czech/Slovak 0.9998 0.9995 0.9940 0.8118Portuguese 0.9535 0.9193 0.7215 0.6888Spanish 0.8623 0.8310 0.5210 0.7023English 0.4970 0.4978 0.5020 0.5053Average 0.8732 0.8590 ?
0.6982Table 5: Comparison of manual and automatic feature selection methods.
IG and parallel feature selec-tion both use the 10,000 features with the highest IG scores.4.3 Data Filtering MethodsEnglish Word Removal In looking through the training data for the non-English tasks, we observedthat it was not uncommon for sentences in these languages to contain English words and phrases.
Be-cause foreign words should be independent of the language/dialect used, English words included in thesentences for other tasks should just be noise that, if removed will improve classification performance.For each of the non-English tasks (A, B, C, D, and E), we create a new training set for identifyingEnglish/non-English words by mixing together 1,000 random English words with 10,000 random task-language words.
The imbalance in the classes is a compromise, approximating the actual proportions inthe test without leading to a degenerate classifier.
Because English and the other classes are so dissimilar,the performance of the English word classifier is very insensitive to the actual ratio.
From this data, wetrain a na?
?ve Bayes classifier using character 3-grams, 4-grams, and 5-grams.We manually labeled the words of 150 sentences from the five non-English tasks in order to evaluatethe English word classifier.
Across the five tasks, the precision was 0.76 and the recall was 0.66, leadingto an F1-score of 0.70.
Any words labeled as English by the classifier were removed from the sentenceand it was passed on to the feature extraction, classification, and training stages.Named Entity Removal We also observed another common class of word that could potentially actas a noise source: named entities.
Across all the languages listed studied here, it is common for namedentities to begin with a capital letter.
Lacking named entity recognizers for all the languages here, weinstead used the property of having an initial capital letter as a surrogate for recognizing a word as anamed entity.
Because all the languaes studied here also have the convention of capitalizing the firstword of a sentence, we remove all words beginning with a capital letter except for the first and pass thisabridged sentence on to the feature extraction, classification, and training stages.Task No data filteringEnglish WordRemovalNamed EntityRemovalBosnian/Croatian/Serbian 0.9138 0.9105 0.9003Indonesian/Malay 0.9878 0.9885 0.9778Czech/Slovak 0.9983 0.9980 0.9973Portuguese 0.9383 0.9365 0.9068Spanish 0.8843 0.8835 0.8555English 0.5000 0.5000 0.5050Average 0.8704 0.8695 0.8571Table 6: Comparison of data filtering methods using word unigram features on the six tasks.151(A)0 0.2 0.4 0.6 0.8 1?1050.40.60.8Training Instances per ClassAccuracyDSLexternalexternal (CV)(B)0 0.5 1 1.5 2 2.5?1040.50.60.70.80.91Training Instances per ClassAccuracyDSLexternalexternal (CV)(C)0 0.5 1 1.5?1050.60.70.80.91Training Instances per ClassAccuracyDSLexternalexternal (CV)(D)0 0.2 0.4 0.6 0.8 1?1050.50.60.70.80.9Training Instances per ClassAccuracyDSLexternalexternal (CV)(E)0 0.5 1 1.5?1050.50.60.70.80.9Training Instances per ClassAccuracyDSLexternalexternal (CV)(F)0 0.5 1 1.5 2?1050.50.60.70.8Training Instances per ClassAccuracyDSLexternalexternal (CV)Figure 1: Learning curves for the six tasks as the number of training instances per language is varied.The line marked ?DSL?
is the learning curve for the DSL-provided training data evaluated against thedevelopement data.
The line marked ?external?
is our external newspaper training data evaluated againstthe development data.
The line marked ?external (CV)?
is our external training data evaluated using10-fold cross-validation.152Bosnian/Croatian/Serbian Indonesian/Malay Czech/Slovak Portuguese Spanish Englishda bisa sa Portugal the Ikako berkata se R Rosario yousa kerana aj euros han Thekazao karena ako Brasil euros saidtakode daripada ve cento Argentina Obamarekao saat pre governo PP yourevra dari pro Lusa Fe Iftijekom beliau ktor?e PSD Rajoy thatposle selepas s?u Ele Espa?na butposto bahwa ktor?y Governo Madrid ItTable 7: The ten word-unigram features given the highest weight by information gain feature selectionfor each of the six tasks.5 DiscussionAcross many of the tasks, there was evidence that performance was tied more strongly to domain-specificfeatures of the two classes rather than to language- (or language-variant-) specific features.
For example,Table 7 shows the best word-unigram features selected by information gain feature selection for each ofthe tasks.
The Portuguese, Spanish, and English tasks specifically have as many of their most importantfeatures named entities and other non-language specific features.It seems that for many of the tasks, it is easier to distinguish the subject matter written about than it is todistinguish the languages/dialects themselves.
With Portuguese, for example, Brazilian dialect speakerswere much more likely to discuss places in Brazil and mention Brazilian reais (currency, abbreviatedas R), while European speakers mentioned euros, places in Portugal, and discussed Portuguese politics.While there are definite linguistic differences between Brazilian and European Portuguese, these seemto be less pronounced than the superficial differences in subject matter.Practically, this is not necessarily a bad thing for this shared task, as the domain information gives extraclues that allow the task to be completed with higher accuracy than would otherwise be possible.
Thiswould become problematic if one wanted to apply a classifier trained on this data to general domains,where the classifier may not be able to rely on the speaker talking about a certain subject matter.
Toaddress this, the classifier would either need to focus on features specific to the language pair itself orwould need to be trained on data that spanned many domains.Further evidence of domain overfitting comes from the fact that the larger training sets drawn fromnewspaper text were not able to improve performance on the development set over the provided trainingdata, which is presumably drawn from the same collection as the development data.
Figure 1 showslearning curves for each of the six tasks.
Though all the external text is self-consistent (cross-validationresults in high accuracy), in none of the cases does training on a large amount of external data allow theclassifier to exceed the accuracy achieved by training on the DSL data.6 ConclusionIn this paper we experimented with several methods for classification of sentences in closely-related lan-guages for the DSL shared task.
Our analysis showed that, when dealing with closely related languages,the task of classifying text according to its language was difficult to untie from the taks of classifyingother text characteristics, such as the domain.
Across all our types of methods, we found that a na?
?veBayes classifier using character n-gram, word unigram, and word bigram features was a strong baseline.In future work, we would like to try to improve on these results by incorporating features that try tocapture syntactic relationships.
Certainly some of the pairs of languages considered here are close enoughthat they could be chunked, tagged, or parsed before knowing exactly which variety they belong to.
Thiswould allow for the inclusion of features related to transitivity, agreement, complementation, etc.
Forexample, in British English, the verb ?provide?
is monotransitive, but ditransitive in American English.
Itis unclear how much features like these would improve accuracy, but it is likely that they would ultimatelybe necessary to improve classification of similar languages to human levels of performance.153ReferencesShane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson.
2012.
Language identi-fication for creating language-specific twitter collections.
In Proceedings of the Second Workshop on Languagein Social Media, pages 65?74.
Association for Computational Linguistics.Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013.
Microblog language identification: Overcomingthe limitations of short, unedited and idiomatic text.
Language Resources and Evaluation, 47(1):195?215.Chu-Ren Huang and Lung-Hao Lee.
2008.
Contrastive approach towards text source classification based ontop-bag-of-word similarity.
pages 404?410.Thorsten Joachims.
Svmlight: Support vector machine.
http://svmlight.
joachims.
org/.Ben King and Steven Abney.
2013.
Labeling the languages of words in mixed-language documents using weaklysupervised methods.
In Proceedings of NAACL-HLT, pages 1110?1119.Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras.
2007.
Language identication: How to distinguish similarlanguages?
In Proceedings of the 29th International Conference on Information Technology Interfaces, pages541?546.Marco Lui, Jey Han Lau, and Timothy Baldwin.
2014.
Automatic detection and language identification of multi-lingual documents.
Transactions of the Association for Computational Linguistics, 2:27?40.Andrew K. McCallum.
2002.
Mallet: A machine learning for language toolkit.http://mallet.cs.umass.edu.Andrew Y Ng and Michael I Jordan.
2002.
On discriminative vs. generative classifiers: A comparison of logisticregression and naive bayes.
Advances in neural information processing systems, 2:841?848.Andrew Y Ng.
2004.
Feature selection, l 1 vs. l 2 regularization, and rotational invariance.
In Proceedings of thetwenty-first international conference on Machine learning, page 78.
ACM.Dong-Phuong Nguyen and A Seza Dogruoz.
2013.
Word level language identification in online multilingualcommunication.
Association for Computational Linguistics.Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010.
The automatic identification of lexical variationbetween language varieties.
Natural Language Engineering, 16(4):469?491.Jason Rennie.
2004.
On l2-norm regularization and the gaussian prior.http://people.csail.mit.edu/jrennie/writing.Liling Tan, Marcos Zampieri, Nikola Ljube?sic, and J?org Tiedemann.
2014.
Merging comparable data sourcesfor the discrimination of similar languages: The dsl corpus collection.
In Proceedings of The 7th Workshop onBuilding and Using Comparable Corpora (BUCC).J?org Tiedemann and Nikola Ljube?si?c.
2012.
Efficient discrimination between closely related languages.
InCOLING, pages 2619?2634.Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun.
2004.
Support vector ma-chine learning for interdependent and structured output spaces.
In Proceedings of the twenty-first internationalconference on Machine learning, page 104.
ACM.Marcos Zampieri and Binyam Gebrekidan.
2012.
Automatic identification of language varieties: The case ofportuguese.
In Proceedings of KONVENS, pages 233?237.Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy.
2012.
Classifying pluricentric languages:Extending the monolingual model.
In Proceedings of the Fourth Swedish Language Technlogy Conference(SLTC2012), pages 79?80.Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy.
2013.
N-gram language models and posdistribution for the identification of spanish varieties.
Proceedings of TALN2013, Sable dOlonne, France, pages580?587.154
