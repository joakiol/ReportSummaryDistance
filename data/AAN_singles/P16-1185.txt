Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1965?1974,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSemi-Supervised Learning for Neural Machine TranslationYong Cheng#, Wei Xu#, Zhongjun He+, Wei He+, Hua Wu+, Maosong Sun?and Yang Liu?
?#Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China?State Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and Technology, Tsinghua University, Beijing, China+Baidu Inc., Beijing, Chinachengyong3001@gmail.com weixu@tsinghua.edu.cn{hezhongjun,hewei06,wu hua}@baidu.com{sms,liuyang2011}@tsinghua.edu.cnAbstractWhile end-to-end neural machine transla-tion (NMT) has made remarkable progressrecently, NMT systems only rely on par-allel corpora for parameter estimation.Since parallel corpora are usually limitedin quantity, quality, and coverage, espe-cially for low-resource languages, it isappealing to exploit monolingual corporato improve NMT.
We propose a semi-supervised approach for training NMTmodels on the concatenation of labeled(parallel corpora) and unlabeled (mono-lingual corpora) data.
The central idea isto reconstruct the monolingual corpora us-ing an autoencoder, in which the source-to-target and target-to-source translationmodels serve as the encoder and decoder,respectively.
Our approach can not onlyexploit the monolingual corpora of thetarget language, but also of the sourcelanguage.
Experiments on the Chinese-English dataset show that our approachachieves significant improvements overstate-of-the-art SMT and NMT systems.1 IntroductionEnd-to-end neural machine translation (NMT),which leverages a single, large neural network todirectly transform a source-language sentence intoa target-language sentence, has attracted increas-ing attention in recent several years (Kalchbren-ner and Blunsom, 2013; Sutskever et al, 2014;Bahdanau et al, 2015).
Free of latent structuredesign and feature engineering that are critical inconventional statistical machine translation (SMT)(Brown et al, 1993; Koehn et al, 2003; Chi-ang, 2005), NMT has proven to excel in model-?Yang Liu is the corresponding author.ing long-distance dependencies by enhancing re-current neural networks (RNNs) with the gating(Hochreiter and Schmidhuber, 1993; Cho et al,2014; Sutskever et al, 2014) and attention mecha-nisms (Bahdanau et al, 2015).However, most existing NMT approaches suf-fer from a major drawback: they heavily relyon parallel corpora for training translation mod-els.
This is because NMT directly models theprobability of a target-language sentence given asource-language sentence and does not have a sep-arate language model like SMT (Kalchbrenner andBlunsom, 2013; Sutskever et al, 2014; Bahdanauet al, 2015).
Unfortunately, parallel corpora areusually only available for a handful of research-rich languages and restricted to limited domainssuch as government documents and news reports.In contrast, SMT is capable of exploiting abundanttarget-side monolingual corpora to boost fluencyof translations.
Therefore, the unavailability oflarge-scale, high-quality, and wide-coverage par-allel corpora hinders the applicability of NMT.As a result, several authors have tried to useabundant monolingual corpora to improve NMT.Gulccehre et al (2015) propose two methods,which are referred to as shallow fusion and deepfusion, to integrate a language model into NMT.The basic idea is to use the language model toscore the candidate words proposed by the transla-tion model at each time step or concatenating thehidden states of the language model and the de-coder.
Although their approach leads to signifi-cant improvements, one possible downside is thatthe network architecture has to be modified to in-tegrate the language model.Alternatively, Sennrich et al (2015) proposetwo approaches to exploiting monolingual corporathat is transparent to network architectures.
Thefirst approach pairs monolingual sentences withdummy input.
Then, the parameters of encoder1965bushi yu shalong juxing le huitanbushi yu shalong juxing le huitanBush held a talk with SharonBush held a talk with SharonBush held a talk with Sharonbushi yu shalong juxing le huitanencoderdecoderencoderdecoder(a) (b)Figure 1: Examples of (a) source autoencoder and (b) target autoencoder on monolingual corpora.
Ouridea is to leverage autoencoders to exploit monolingual corpora for NMT.
In a source autoencoder, thesource-to-target model P (y|x;??? )
serves as an encoder to transform the observed source sentence xinto a latent target sentence y (highlighted in grey), from which the target-to-source model P (x?|y;???
)reconstructs a copy of the observed source sentence x?from the latent target sentence.
As a result,monolingual corpora can be combined with parallel corpora to train bidirectional NMT models in asemi-supervised setting.and attention model are fixed when training onthese pseudo parallel sentence pairs.
In the sec-ond approach, they first train a nerual translationmodel on the parallel corpus and then use thelearned model to translate a monolingual corpus.The monolingual corpus and its translations con-stitute an additional pseudo parallel corpus.
Simi-lar ideas have also been suggested in conventionalSMT (Ueffing et al, 2007; Bertoldi and Federico,2009).
Sennrich et al (2015) report that their ap-proach significantly improves translation qualityacross a variety of language pairs.In this paper, we propose semi-supervisedlearning for neural machine translation.
Given la-beled (i.e., parallel corpora) and unlabeled (i.e.,monolingual corpora) data, our approach jointlytrains source-to-target and target-to-source trans-lation models.
The key idea is to append a re-construction term to the training objective, whichaims to reconstruct the observed monolingual cor-pora using an autoencoder.
In the autoencoder, thesource-to-target and target-to-source models serveas the encoder and decoder, respectively.
As theinference is intractable, we propose to sample thefull search space to improve the efficiency.
Specif-ically, our approach has the following advantages:1.
Transparent to network architectures: our ap-proach does not depend on specific architec-tures and can be easily applied to arbitraryend-to-end NMT systems.2.
Both the source and target monolingual cor-pora can be used: our approach can bene-fit NMT not only using target monolingualcorpora in a conventional way, but also themonolingual corpora of the source language.Experiments on Chinese-English NIST datasetsshow that our approach results in significant im-provements in both directions over state-of-the-artSMT and NMT systems.2 Semi-Supervised Learning for NeuralMachine Translation2.1 Supervised LearningGiven a parallel corpus D = {?x(n),y(n)?
}Nn=1,the standard training objective in NMT is to max-imize the likelihood of the training data:L(?)
=N?n=1logP (y(n)|x(n);?
), (1)where P (y|x;?)
is a neural translation model and?
is a set of model parameters.
D can be seenas labeled data for the task of predicting a targetsentence y given a source sentence x.As P (y|x;?)
is modeled by a single, large neu-ral network, there does not exist a separate targetlanguage model P (y;?)
in NMT.
Therefore, par-allel corpora have been the only resource for pa-rameter estimation in most existing NMT systems.Unfortunately, even for a handful of resource-rich1966languages, the available domains are unbalancedand restricted to government documents and newsreports.
Therefore, the availability of large-scale,high-quality, and wide-coverage parallel corporabecomes a major obstacle for NMT.2.2 Autoencoders on Monolingual CorporaIt is appealing to explore the more readily avail-able, abundant monolingual corpora to improveNMT.
Let us first consider an unsupervised set-ting: how to train NMT models on a monolingualcorpus T = {y(t)}Tt=1?Our idea is to leverage autoencoders (Vincent etal., 2010; Socher et al, 2011): (1) encoding an ob-served target sentence into a latent source sentenceusing a target-to-source translation model and (2)decoding the source sentence to reconstruct theobserved target sentence using a source-to-targetmodel.
For example, as shown in Figure 1(b),given an observed English sentence ?Bush helda talk with Sharon?, a target-to-source translationmodel (i.e., encoder) transforms it into a Chinesetranslation ?bushi yu shalong juxing le huitan?
thatis unobserved on the training data (highlighted ingrey).
Then, a source-to-target translation model(i.e., decoder) reconstructs the observed Englishsentence from the Chinese translation.More formally, let P (y|x;???)
and P (x|y;???
)be source-to-target and target-to-source transla-tion models respectively, where???
and???
are cor-responding model parameters.
An autoencoderaims to reconstruct the observed target sentencevia a latent source sentence:P (y?|y;???
,???
)=?xP (y?,x|y;???
,???
)=?xP (x|y;???
)?
??
?encoderP (y?|x;???
)?
??
?decoder, (2)where y is an observed target sentence, y?is acopy of y to be reconstructed, and x is a latentsource sentence.We refer to Eq.
(2) as a target autoencoder.1Likewise, given a monolingual corpus of sourcelanguage S = {x(s)}Ss=1, it is natural to introducea source autoencoder that aims at reconstructing1Our definition of auotoencoders is inspired by Ammar etal.
(2014).
Note that our autoencoders inherit the same spiritfrom conventional autoencoders (Vincent et al, 2010; Socheret al, 2011) except that the hidden layer is denoted by a latentsentence instead of real-valued vectors.the observed source sentence via a latent targetsentence:P (x?|x;???
,???
)=?yP (x?,y|x;???
)=?yP (y|x;???
)?
??
?encoderP (x?|y;???
)?
??
?decoder.
(3)Please see Figure 1(a) for illustration.2.3 Semi-Supervised LearningAs the autoencoders involve both source-to-targetand target-to-source models, it is natural to com-bine parallel corpora and monolingual corpora tolearn birectional NMT translation models in asemi-supervised setting.Formally, given a parallel corpus D ={?x(n),y(n)?
}Nn=1, a monolingual corpus of targetlanguage T = {y(t)}Tt=1, and a monolingual cor-pus of source language S = {x(s)}Ss=1, we intro-duce our new semi-supervised training objectiveas follows:J(???
,???
)=N?n=1logP (y(n)|x(n);???
)?
??
?source-to-target likelihood+N?n=1logP (x(n)|y(n);???
)?
??
?target-to-source likelihood+?1T?t=1logP (y?|y(t);???
,???
)?
??
?target autoencoder+?2S?s=1logP (x?|x(s);???
,???
)?
??
?source autoencoder, (4)where ?1and ?2are hyper-parameters for balanc-ing the preference between likelihood and autoen-coders.Note that the objective consists of four parts:source-to-target likelihood, target-to-source likeli-hood, target autoencoder, and source autoencoder.In this way, our approach is capable of exploitingabundant monolingual corpora of both source andtarget languages.1967The optimal model parameters are given by???
?= argmax{N?n=1logP (y(n)|x(n);??? )
+?1T?t=1logP (y?|y(t);???
,??? )
+?2S?s=1logP (x?|x(s);???
,???
)}(5)???
?= argmax{N?n=1logP (x(n)|y(n);??? )
+?1T?t=1logP (y?|y(t);???
,??? )
+?2S?s=1logP (x?|x(s);???
,???
)}(6)It is clear that the source-to-target and target-to-source models are connected via the autoencoderand can hopefully benefit each other in joint train-ing.2.4 TrainingWe use mini-batch stochastic gradient descent totrain our joint model.
For each iteration, be-sides the mini-batch from the parallel corpus, wealso construct two additional mini-batches by ran-domly selecting sentences from the source and tar-get monolingual corpora.
Then, gradients are col-lected from these mini-batches to update modelparameters.The partial derivative of J(???
,??? )
with respectto the source-to-target model???
is given by?J(???
,???
)????=N?n=1?
logP (y(n)|x(n);???
)????+?1T?t=1?
logP (y?|y(t);???
,???
)????+?2S?s=1?
logP (x?|x(s);???
,???
)????.
(7)The partial derivative with respect to???
can be cal-culated similarly.Unfortunately, the second and third terms in Eq.
(7) are intractable to calculate due to the exponen-tial search space.
For example, the derivative inChinese English# Sent.
2.56MParallel # Word 67.54M 74.82MVocab.
0.21M 0.16M# Sent.
18.75M 22.32MMonolingual # Word 451.94M 399.83MVocab.
0.97M 1.34MTable 1: Characteristics of parallel and monolin-gual corpora.the third term in Eq.
(7) is given by?x?X (y)P (x|y;???
)P (y?|x;???
)?
logP (y?|x;???
)????
?x?X (y)P (x|y;???
)P (y?|x;???
).
(8)It is prohibitively expensive to compute the sumsdue to the exponential search space of X (y).Alternatively, we propose to use a subset of thefull space?X (y) ?
X (y) to approximate Eq.
(8):?x?
?X (y)P (x|y;???
)P (y?|x;???
)?
logP (y?|x;???
)?????x?
?X (y)P (x|y;???
)P (y?|x;???
).
(9)In practice, we use the top-k list of candidatetranslations of y as?X (y).
As |?X (y)|  X |(y)|,it is possible to calculate Eq.
(9) efficiently byenumerating all candidates in?X (y).
In practice,we find this approximation results in significantimprovements and k = 10 seems to suffice tokeep the balance between efficiency and transla-tion quality.3 Experiments3.1 SetupWe evaluated our approach on the Chinese-English dataset.As shown in Table 1, we use both a parallelcorpus and two monolingual corpora as the train-ing set.
The parallel corpus from LDC consists of2.56M sentence pairs with 67.53M Chinese wordsand 74.81M English words.
The vocabulary sizesof Chinese and English are 0.21M and 0.16M, re-spectively.
We use the Chinese and English partsof the Xinhua portion of the GIGAWORD cor-pus as the monolingual corpora.
The Chinesemonolingual corpus contains 18.75M sentenceswith 451.94M words.
The English corpus contains22.32M sentences with 399.83M words.
The vo-cabulary sizes of Chinese and English are 0.97Mand 1.34M, respectively.1968Iterations0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0BLEU30.030.531.031.532.032.533.033.534.0?
104k=15k=10k=5k=1Figure 2: Effect of sample size k on the Chinese-to-English validation set.Iterations0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0BLEU15.015.516.016.517.017.5k=15k=10k=5k=1Figure 3: Effect of sample size k on the English-to-Chinese validation set.For Chinese-to-English translation, we use theNIST 2006 Chinese-English dataset as the vali-dation set for hyper-parameter optimization andmodel selection.
The NIST 2002, 2003, 2004,and 2005 datasets serve as test sets.
Each Chi-nese sentence has four reference translations.
ForEnglish-to-Chinese translation, we use the NISTdatasets in a reverse direction: treating the firstEnglish sentence in the four reference transla-tions as a source sentence and the original inputChinese sentence as the single reference trans-lation.
The evaluation metric is case-insensitiveBLEU (Papineni et al, 2002) as calculated by themulti-bleu.perl script.We compared our approach with two state-of-the-art SMT and NMT systems:1.
MOSES (Koehn et al, 2007): a phrase-basedSMT system;Iterations0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0BLEU30.030.531.031.532.032.533.033.534.0?
1040% OOV10% OOV20% OOV30% OOVFigure 4: Effect of OOV ratio on the Chinese-to-English validation set.Iterations0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0BLEU6.08.010.012.014.016.018.0?
1040% OOV10% OOV20% OOV30% OOVFigure 5: Effect of OOV ratio on the English-to-Chinese validation set.2.
RNNSEARCH (Bahdanau et al, 2015): anattention-based NMT system.For MOSES, we use the default setting to trainthe phrase-based translation on the parallel corpusand optimize the parameters of log-linear modelsusing the minimum error rate training algorithm(Och, 2003).
We use the SRILM toolkit (Stolcke,2002) to train 4-gram language models.For RNNSEARCH, we use the parallel corpus totrain the attention-based neural translation models.We set the vocabulary size of word embeddingsto 30K for both Chinese and English.
We followLuong et al (2015) to address rare words.On top of RNNSEARCH, our approach is capa-ble of training bidirectional attention-based neuraltranslation models on the concatenation of paralleland monolingual corpora.
The sample size k is setto 10.
We set the hyper-parameter ?1= 0.1 and1969?2= 0 when we add the target monolingual cor-pus, and ?1= 0 and ?2= 0.1 for source monolin-gual corpus incorporation.
The threshold of gra-dient clipping is set to 0.05.
The parameters ofour model are initialized by the model trained onparallel corpus.3.2 Effect of Sample Size kAs the inference of our approach is intractable, wepropose to approximate the full search space withthe top-k list of candidate translations to improveefficiency (see Eq.
(9)).Figure 2 shows the BLEU scores of various set-tings of k over time.
Only the English mono-lingual corpus is appended to the training data.We observe that increasing the size of the approx-imate search space generally leads to improvedBLEU scores.
There are significant gaps betweenk = 1 and k = 5.
However, keeping increas-ing k does not result in significant improvementsand decreases the training efficiency.
We find thatk = 10 achieves a balance between training effi-ciency and translation quality.
As shown in Fig-ure 3, similar findings are also observed on theEnglish-to-Chinese validation set.
Therefore, weset k = 10 in the following experiments.3.3 Effect of OOV RatioGiven a parallel corpus, what kind of monolingualcorpus is most beneficial for improving transla-tion quality?
To answer this question, we investi-gate the effect of OOV ratio on translation quality,which is defined asratio =?y?yJy /?
VDtK|y|, (10)where y is a target-language sentence in the mono-lingual corpus T , y is a target-language word in y,VDtis the vocabulary of the target side of the par-allel corpus D.Intuitively, the OOV ratio indicates how a sen-tence in the monolingual resembles the parallelcorpus.
If the ratio is 0, all words in the mono-lingual sentence also occur in the parallel corpus.Figure 4 shows the effect of OOV ratio onthe Chinese-to-English validation set.
Only En-glish monolingual corpus is appended to the par-allel corpus during training.
We constructed fourmonolingual corpora of the same size in terms ofsentence pairs.
?0% OOV?
means the OOV ra-tio is 0% for all sentences in the monolingual cor-pus.
?10% OOV?
suggests that the OOV ratio isno greater 10% for each sentence in the mono-lingual corpus.
We find that using a monolingualcorpus with a lower OOV ratio generally leads tohigher BLEU scores.
One possible reason is thatlow-OOV monolingual corpus is relatively easierto reconstruct than its high-OOV counterpart andresults in better estimation of model parameters.Figure 5 shows the effect of OOV ratio on theEnglish-to-Chinese validation set.
Only Englishmonolingual corpus is appended to the parallelcorpus during training.
We find that ?0% OOV?still achieves the highest BLEU scores.3.4 Comparison with SMTTable 2 shows the comparison between MOSESand our work.
MOSES used the monolingualcorpora as shown in Table 1: 18.75M Chinesesentences and 22.32M English sentences.
Wefind that exploiting monolingual corpora dramat-ically improves translation performance in bothChinese-to-English and English-to-Chinese direc-tions.Relying only on parallel corpus, RNNSEARCHoutperforms MOSES trained also only on par-allel corpus.
But the capability of makinguse of abundant monolingual corpora enablesMOSES to achieve much higher BLEU scores thanRNNSEARCH only using parallel corpus.Instead of using all sentences in the monolin-gual corpora, we constructed smaller monolingualcorpora with zero OOV ratio: 2.56M Chinese sen-tences with 47.51M words and 2.56M EnglishEnglish sentences with 37.47M words.
In otherwords, the monolingual corpora we used in theexperiments are much smaller than those used byMOSES.By adding English monolingual corpus, ourapproach achieves substantial improvements overRNNSEARCH using only parallel corpus (up to+4.7 BLEU points).
In addition, significant im-provements are also obtained over MOSES usingboth parallel and monolingual corpora (up to +3.5BLEU points).An interesting finding is that adding Englishmonolingual corpora helps to improve English-to-Chinese translation over RNNSEARCH using onlyparallel corpus (up to +3.2 BLEU points), sug-gesting that our approach is capable of improvingNMT using source-side monolingual corpora.In the English-to-Chinese direction, we ob-tain similar findings.
In particular, adding Chi-1970SystemTraining DataDirection NIST06 NIST02 NIST03 NIST04 NIST05CE C EMOSES??
?C?
E 32.48 32.69 32.39 33.62 30.23E?
C 14.27 18.28 15.36 13.96 14.11???C?
E 34.59 35.21 35.71 35.56 33.74?
??
E?
C 20.69 25.85 19.76 18.77 19.74RNNSEARCH??
?C?E 30.74 35.16 33.75 34.63 31.74E?C 15.71 20.76 16.56 16.85 15.14???C?
E 35.61??++38.78??++38.32??++38.49??++36.45?
?++E?C 17.59++23.99++18.95++18.85++17.91++?
?
?C?E 35.01++38.20??++37.99??++38.16??++36.07?
?++E?C 21.12?++29.52??++20.49??++21.59?
?++19.97++Table 2: Comparison with MOSES and RNNSEARCH.
MOSES is a phrase-based statistical machinetranslation system (Koehn et al, 2007).
RNNSEARCH is an attention-based neural machine translationsystem (Bahdanau et al, 2015).
?CE?
donates Chinese-English parallel corpus, ?C?
donates Chinesemonolingual corpus, and ?E?
donates English monolingual corpus.
???
means the corpus is included inthe training data and ?
means not included.
?NIST06?
is the validation set and ?NIST02-05?
are testsets.
The BLEU scores are case-insensitive.
?*?
: significantly better than MOSES (p < 0.05); ?**?
:significantly better than MOSES (p < 0.01);?+?
: significantly better than RNNSEARCH (p < 0.05);?++?
: significantly better than RNNSEARCH (p < 0.01).MethodTraining DataDirection NIST06 NIST02 NIST03 NIST04 NIST05CE C ESennrich et al (2015)??
?C?E 34.10 36.95 36.80 37.99 35.33?
??
E?C 19.85 28.83 20.61 20.54 19.17this work??
?C?E 35.61??38.78??38.32??38.49?36.45?
?E?C 17.59 23.99 18.95 18.85 17.91?
?
?C?E 35.01??38.20??37.99?
?38.16 36.07?
?E?C 21.12??29.52?
?20.49 21.59??19.97?
?Table 3: Comparison with Sennrich et al (2015).
Both Sennrich et al (2015) and our approach buildon top of RNNSEARCH to exploit monolingual corpora.
The BLEU scores are case-insensitive.
?*?
:significantly better than Sennrich et al (2015) (p < 0.05); ?**?
: significantly better than Sennrich et al(2015) (p < 0.01).nese monolingual corpus leads to more benefitsto English-to-Chinese translation than adding En-glish monolingual corpus.
We also tried to useboth Chinese and English monolingual corporathrough simply setting all the ?
to 0.1 but failedto obtain further significant improvements.Therefore, our findings can be summarized asfollows:1.
Adding target monolingual corpus improvesover using only parallel corpus for source-to-target translation;2.
Adding source monolingual corpus also im-proves over using only parallel corpus forsource-to-target translation, but the improve-ments are smaller than adding target mono-lingual corpus;3.
Adding both source and target monolingualcorpora does not lead to further significantimprovements.3.5 Comparison with Previous WorkWe re-implemented Sennrich et al (2015)?smethod on top of RNNSEARCH as follows:1.
Train the target-to-source neural translationmodel P (x|y;??? )
on the parallel corpusD ={?x(n),y(n)?}Nn=1.2.
The trained target-to-source model???
?isused to translate a target monolingual corpusT = {y(t)}Tt=1into a source monolingualcorpus?S = {?x(t)}Tt=1.3.
The target monolingual corpus is paired withits translations to form a pseudo parallel cor-pus, which is then appended to the originalparallel corpus to obtain a larger parallel cor-pus:?D = D ?
?
?S, T ?.4.
Re-train the the source-to-target neural trans-lation model on?D to obtain the final modelparameters???
?.1971Monolingual hongsen shuo , ruguo you na jia famu gongsi dangan yishenshifa , nametamen jiang zihui qiancheng .Reference hongsen said, if any logging companies dare to defy the law, then they willdestroy their own future .Translation hun sen said , if any of those companies dare defy the law , then they willhave their own fate .
[iteration 0]hun sen said if any tree felling company dared to break the law , then theywould kill themselves .
[iteration 40K]hun sen said if any logging companies dare to defy the law , they woulddestroy the future themselves .
[iteration 240K]Monolingual dan yidan panjue jieguo zuizhong queding , ze bixu zai 30 tian nei zhixing .Reference But once the final verdict is confirmed , it must be executed within 30 days.Translation however , in the final analysis , it must be carried out within 30 days .
[iteration 0]however , in the final analysis , the final decision will be carried out within30 days .
[iteration 40K]however , once the verdict is finally confirmed , it must be carried out within30 days .
[iteration 240K]Table 4: Example translations of sentences in the monolingual corpus during semi-supervised learning.We find our approach is capable of generating better translations of the monolingual corpus over time.Table 3 shows the comparison results.
Both thetwo approaches use the same parallel and mono-lingual corpora.
Our approach achieves signifi-cant improvements over Sennrich et al (2015) inboth Chinese-to-English and English-to-Chinesedirections (up to +1.8 and +1.0 BLEU points).One possible reason is that Sennrich et al (2015)only use the pesudo parallel corpus for parame-ter estimation for once (see Step 4 above) whileour approach enables source-to-target and target-to-source models to interact with each other itera-tively on both parallel and monolingual corpora.To some extent, our approach can be seen as aniterative extension of Sennrich et al (2015)?s ap-proach: after estimating model parameters on thepseudo parallel corpus, the learned model param-eters are used to produce a better pseudo parallelcorpus.
Table 4 shows example Viterbi transla-tions on the Chinese monolingual corpus over it-erations:x?= argmaxx{P (y?|x;???
)P (x|y;???
)}.
(11)We observe that the quality of Viterbi transla-tions generally improves over time.4 Related WorkOur work is inspired by two lines of research: (1)exploiting monolingual corpora for machine trans-lation and (2) autoencoders in unsupervised andsemi-supervised learning.4.1 Exploiting Monolingual Corpora forMachine TranslationExploiting monolingual corpora for conventionalSMT has attracted intensive attention in recentyears.
Several authors have introduced transduc-tive learning to make full use of monolingualcorpora (Ueffing et al, 2007; Bertoldi and Fed-erico, 2009).
They use an existing translationmodel to translate unseen source text, which canbe paired with its translations to form a pseudoparallel corpus.
This process iterates until con-vergence.
While Klementiev et al (2012) pro-pose an approach to estimating phrase translationprobabilities from monolingual corpora, Zhangand Zong (2013) directly extract parallel phrasesfrom monolingual corpora using retrieval tech-niques.
Another important line of research is totreat translation on monolingual corpora as a de-cipherment problem (Ravi and Knight, 2011; Douet al, 2014).1972Closely related to Gulccehre et al (2015) andSennrich et al (2015), our approach focuses onlearning birectional NMT models via autoen-coders on monolingual corpora.
The major ad-vantages of our approach are the transparency tonetwork architectures and the capability to exploitboth source and target monolingual corpora.4.2 Autoencoders in Unsupervised andSemi-Supervised LearningAutoencoders and their variants have been widelyused in unsupervised deep learning ((Vincent etal., 2010; Socher et al, 2011; Ammar et al, 2014),just to name a few).
Among them, Socher et al(2011)?s approach bears close resemblance to ourapproach as they introduce semi-supervised recur-sive autoencoders for sentiment analysis.
The dif-ference is that we are interested in making a bet-ter use of parallel and monolingual corpora whilethey concentrate on injecting partial supervisionto conventional unsupervised autoencoders.
Daiand Le (2015) introduce a sequence autoencoderto reconstruct an observed sequence via RNNs.Our approach differs from sequence autoencodersin that we use bidirectional translation models asencoders and decoders to enable them to interactwithin the autoencoders.5 ConclusionWe have presented a semi-supervised approach totraining bidirectional neural machine translationmodels.
The central idea is to introduce autoen-coders on the monolingual corpora with source-to-target and target-to-source translation models asencoders and decoders.
Experiments on Chinese-English NIST datasets show that our approachleads to significant improvements.As our method is sensitive to the OOVs presentin monolingual corpora, we plan to integrate Jeanet al (2015)?s technique on using very large vo-cabulary into our approach.
It is also necessary tofurther validate the effectiveness of our approachon more language pairs and NMT architectures.Another interesting direction is to enhance theconnection between source-to-target and target-to-source models (e.g., letting the two models sharethe same word embeddings) to help them benefitmore from interacting with each other.AcknowledgementsThis work was done while Yong Cheng was vis-iting Baidu.
This research is supported by the973 Program (2014CB340501, 2014CB340505),the National Natural Science Foundation of China(No.
61522204, 61331013, 61361136003), 1000Talent Plan grant, Tsinghua Initiative ResearchProgram grants 20151080475 and a Google Fac-ulty Research Award.
We sincerely thank theviewers for their valuable suggestions.ReferencesWaleed Ammar, Chris Dyer, and Noah Smith.
2014.Conditional random field autoencoders for unsuper-vised structred prediction.
In Proceedings of NIPS2014.Dzmitry Bahdanau, KyungHyun Cho, and YoshuaBengio.
2015.
Neural machine translation byjointly learning to align and translate.
In Proceed-ings of ICLR.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation.In Proceedings of WMT.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguisitics.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL.Kyunhyun Cho, Bart van Merri?enboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the propertiesof neural machine translation: Encoder-decoder ap-proaches.
In Proceedings of SSST-8.Andrew M. Dai and Quoc V. Le.
2015.
Semi-supervised sequence learning.
In Proceedings ofNIPS.Qing Dou, Ashish Vaswani, and Kevin Knight.
2014.Beyond parallel data: Joint word alignment and de-cipherment improves machine translation.
In Pro-ceedings of EMNLP.Caglar Gulccehre, Orhan Firat, Kelvin Xu, KyunghyunCho, Lo?
?c Barrault, Huei-Chi Lin, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2015.
Onusing monolingual corpora in neural machine trans-lation.
arXiv:1503.03535 [cs.CL].Sepp Hochreiter and J?urgen Schmidhuber.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguisitics.1973Sebastien Jean, Kyunghyun Cho, Roland Memisevic,and Yoshua Bengio.
2015.
On using very large tar-get vocabulary for neural machine translation.
InProceedings of ACL.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In Proceedingsof EMNLP.Diederik P Kingma, Shakir Mohamed, Danilo JimenezRezende, and Max Welling.
2014.
Semi-supervisedlearning with deep generative models.
In Advancesin Neural Information Processing Systems.Alexandre Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky.
2012.
Toward statisti-cal machine translation without paralel corpora.
InProceedings of EACL.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL (demo session).Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, OriolVinyals, and Wojciech Zaremba.
2015.
Addressingthe rare word problem in neural machine translation.In Proceedings of ACL.Franz Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a methof for automatic eval-uation of machine translation.
In Proceedings ofACL.Sujith Ravi and Kevin Knight.
2011.
Deciphering for-eign language.
In Proceedings of ACL.Rico Sennrich, Barry Haddow, and Alexandra Birch.2015.
Improving nerual machine translation modelswith monolingual data.
arXiv:1511.06709 [cs.CL].Richard Socher, Jeffrey Pennington, Eric Huang, An-drew Ng, and Christopher Manning.
2011.
Semi-supervised recursive autoencoders for predictingsentiment distributions.
In Proceedings of EMNLP.Andreas Stolcke.
2002.
Srilm - am extensible lan-guage modeling toolkit.
In Proceedings of ICSLP.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Proceedings of NIPS.Nicola Ueffing, Gholamreza Haffari, and AnoopSarkar.
2007.
Trasductive learning for statisticalmachine translation.
In Proceedings of ACL.Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,Yoshua Bengio, and Pierre-Autoine Manzagol.2010.
Stacked denoising autoencoders: Learninguseful representations in a deep network with a localdenoising criterion.
Journal of Machine LearningResearch.Jiajun Zhang and Chengqing Zong.
2013.
Learninga phrase-based translation model from monolingualdata with application to domain adaptation.
In Pro-ceedings of ACL.1974
