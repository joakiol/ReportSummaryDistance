Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 61?65,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLoss-Sensitive Discriminative Training of Machine Transliteration ModelsKedar BellareDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA 01003, USAkedarb@cs.umass.eduKoby CrammerDepartment of Computer ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USAcrammer@cis.upenn.eduDayne FreitagSRI InternationalSan Diego, CA 92130, USAdayne.freitag@sri.comAbstractIn machine transliteration we transcribe aname across languages while maintaining itsphonetic information.
In this paper, wepresent a novel sequence transduction algo-rithm for the problem of machine transliter-ation.
Our model is discriminatively trainedby the MIRA algorithm, which improves thetraditional Perceptron training in three ways:(1) It allows us to consider k-best translitera-tions instead of the best one.
(2) It is trainedbased on the ranking of these transliterationsaccording to user-specified loss function (Lev-enshtein edit distance).
(3) It enables the userto tune a built-in parameter to cope with noisynon-separable data during training.
On anArabic-English name transliteration task, ourmodel achieves a relative error reduction of2.2% over a perceptron-based model with sim-ilar features, and an error reduction of 7.2%over a statistical machine translation modelwith more complex features.1 Introduction and Related WorkProper names and other technical terms are fre-quently encountered in natural language text.
Bothmachine translation (Knight and Graehl, 1997) andcross-language information retrieval (Jeong et al,1999; Virga and Khudanpur, 2003; Abdul-Jaleel andLarkey, 2003) can benefit by explicitly translatingsuch words from one language into another.
Thisapproach is decidedly better than treating them uni-formly as out-of-vocabulary tokens.
The goal of ma-chine transliteration is to translate words betweenalphabets of different languages such that they arephonetically equivalent.Given a source language sequence f =f1f2 .
.
.
fm from an alphabet F , we want to producea target language sequence e = e1e2 .
.
.
en in the al-phabet E such that it maximizes some score functions(e, f),e = argmaxe?s(e?, f).Virga and Khudanpur (2003) model this scoringfunction using a separate translation and languagemodel, that is, s(e, f) = Pr(f |e)Pr(e).
In con-strast, Al-Onaizan and Knight (2002) directly modelthe translation probability Pr(e|f) using a log-linearcombination of several individually trained phraseand character-based models.
Others have treatedtransliteration as a phrase-based transduction (Sherifand Kondrak, 2007).
All these approaches are adap-tations of statistical models for machine transla-tion (Brown et al, 1994).
In general, the parame-ters of the scoring function in such approaches aretrained generatively and do not utilize complex fea-tures of the input sequence pairs.Recently, there has been interest in applyingdiscriminatively-trained sequence alignment mod-els to many real-world problems.
McCallum et al(2005) train a conditional random field model todiscriminate between matching and non-matchingstring pairs treating alignments as latent.
Learningaccurate alignments in this model requires finding?close?
non-match pairs which can be a challenge.A similar conditional latent-variable model has beenapplied to the task of lemmatization and genera-tion of morphological forms (Dreyer et al, 2008).61Zelenko and Aone (2006) model transliteration asa structured prediction problem where the letter eiis predicted using local and global features derivedfrom e1e2 .
.
.
ei?1 and f .
Bergsma and Kondrak(2007) address cognate identification by training aSVM classification model using phrase-based fea-tures obtained from a Levenshtein alignment.
Boththese models do not learn alignments that is neededto obtain high performance on transliteration tasks.Freitag and Khadivi (2007) describe a discrimina-tively trained sequence alignment model based onaveraged perceptron, which is closely related to themethod proposed in this paper.Our approach improves over previous directionsin two ways.
First, our system produces better k-besttransliterations than related approaches by trainingon multiple hypotheses ranked according to a user-specified loss function (Levenshtein edit distance).Hence, our method achieves a 19.2% error reductionin 5-best performance over a baseline only trainedwith 1-best transliterations.
This is especially help-ful when machine transliteration is part of a largermachine translation or information retrieval pipelinesince additional sentence context can be used tochoose the best among top-K transliterations.
Sec-ond, our training procedure accounts for noise andnon-separability in the data.
Therefore, our translit-eration system would work well in cases where per-son names were misspelled or in cases in which asingle name had many reasonable translations in theforeign language.The training algorithm we propose in this pa-per is based on the K-best MIRA algorithm whichhas been used earlier in structured prediction prob-lems (McDonald et al, 2005a; McDonald et al,2005b).
Our results demonstrate a significant im-provement in accuracy of 7.2% over a statisticalmachine translation (SMT) system (Zens et al,2005) and of 2.2% over a perceptron-based editmodel (Freitag and Khadivi, 2007).2 Sequence Alignment ModelLet e = e1e2 .
.
.
en and f = f1f2 .
.
.
fm be se-quences from the target alhabet E and source al-phabet F respectively.
Let a = a1a2 .
.
.
al be a se-quence of alignment operations needed to convert finto e. Each alignment operation either appends aletter to the end of the source sequence, the targetsequence or both sequences.
Hence, it is a memberof the cross-product ak ?
E?{?}?F?{?
}\{(?, ?
)},where ?
is the null character symbol.
Let ak1 =a1a2 .
.
.
ak denote the sequence of first k alignmentoperations.
Similarly ek1 and fk1 are prefixes of e andf of length k.We define the scoring function between a wordand its transliteration to be the a maximum over allpossible alignment sequences a,s(e, f) = maxas(a, e, f) ,where the score of a specific alignment a betweentwo words is given by a linear relation,s(a, e, f) = w ?
?
(a, e, f),for a parameter vector w and a feature vec-tor ?
(a, e, f).
Furthermore, let ?
(a, e, f) =?lk=1 ?
(ak, e, i, f , j) be the sum of feature vec-tors associated with individual alignment operations.Here i, j are positions in sequences e, f after per-forming operations ak1 .
For fixed sequences e and fthe function s(e, f) can be efficiently computed us-ing a dynamic programming algorithm,s(ei1, f j1 ) =max????
?s(ei?11 , f j1 ) + w ?
?
(?ei, ?
?, e, i, f , j)s(ei1, f j?11 ) + w ?
?(?
?, fj?, e, i, f , j)s(ei?11 , f j?11 ) + w ?
?
(?ei, fj?, e, i, f , j).
(1)Given a source sequence f computing the best scor-ing target sequence e = argmaxe?
s(e?, f) amongall possible sequences E?
requires a beam searchprocedure (Freitag and Khadivi, 2007).
This pro-cedure can also be used to produce K-best targetsequences {e?1, e?2, .
.
.
, e?K} such that s(e?1, f) ?s(e?2, f) ?
.
.
.
?
s(e?K , f).In this paper, we employ the same features asthose used by Freitag and Khadivi (2007).
All lo-cal feature functions ?
(ak, e, i, f , j) are conjunc-tions of the alignment operation ak and forward orbackward-looking character m-grams in sequencese and f at positions i and j respectively.
Forthe source sequence f both forward and backward-looking m-gram features are included.
We restrictthe m-gram features in our target sequence e to only62be backward-looking since we do not have access toforward-looking m-grams during beam-search.
Anorder M model is one that uses m-gram featureswhere m = 0, 1, .
.
.
M .Our training algorithm takes as input a data setD of source-target transliteration pairs and outputsa parameter vector u.
The algorithm pseudo-codeappears in Fig.
(1).
In the algorithm, the functionL(e?, e) defines a loss incurred by predicting e?
in-stead of e. In most structured prediction problems,the targets are of equal length and in such cases theHamming loss function can be used.
However, inour case the targets may differ in terms of length andthus we use the Levenshtein edit distance (Leven-shtein, 1966) with unit costs for insertions, deletionsand substitutions.
Since the targets are both in thesame alphabet E this loss function is well-defined.The user also supplies three paramters: (1) T - thenumber of training iterations (2) K - the numberof best target hypotheses used (3) C - a complex-ity parameter.
A low C is useful if the data is non-separable and noisy.The final parameter vector u returned by the al-gorithm is the average of the intermediate parametervectors produced during training.
We find that av-eraging helps to improve performance.
At test time,we use the beam search procedure to produce K-best hypotheses using the parameter vector u.3 Experimental ResultsWe apply our model to the real-world Arabic-English name transliteration task on a data set of10,084 Arabic names from the LDC.
The data setconsists of Arabic names in an ASCII-based alpha-bet and its English rendering.
Table 1 shows afew examples of Arabic-English pairs in our dataset.
We use the same training/development/testing(8084/1000/1000) set as the one used in a previ-ous benchmark study (Freitag and Khadivi, 2007).The development and testing data were obtainedby randomly removing entries from the trainingdata.
The absence of short vowels (e.g.
?a?
in?NB?I, nab?i?
), doubled consonants (e.g.
?ww?in ?FWAL, fawwal?)
and other diacritics in Arabicmake the transliteration a hard problem.
Therefore,it is hard to achieve perfect accuracy on this data set.For training, we set K = 20 best hypotheses andInput parametersTraining Data DComplexity parameter C > 0Number of epochs TInitialize w0 = 0 (zero vector) ; ?
= 0 ; u = 0Repeat T times:For Each (e, f) ?
D :1. a = argmaxa?w?
?
?
(a?, e, f) (Find best scoringalignment between e and f using dynamic program-ming)2.
Generate a list of K-best target hypotheses{e?1, e?2, .
.
.
, e?K} given the current parameters w?
.Let the corresponding alignments for the targets be{a?1,a?2, .
.
.
,a?K}.3.
Set w?+1 to be the solution of :minw 12 ||w ?w?
||2 + C?Kk=1 ?ksubject to (for k = 1 .
.
.K) :w ?
(?
(a, e, f)?
?
(a?k, e?k, f)) ?
L(e, e?k)?
?k?k ?
04. u?
u+ w?+15.
?
?
?
+ 1Output Scoring function s(a, e, f) = u ?
?
(a, e, f)Figure 1: The k-best MIRA algorithm for discriminativelearning of transliterations.Arabic EnglishNB?I nab?iHNBLI hanbaliFRIFI furayfiMLKIAN malikianBI;ANT bizantFWAL fawwalOALDAWI khalidawiBUWUI battutiH;?
hazzahTable 1: Examples of Arabic names in the ASCII alpha-bet and their English transliterations.C = 1.0 and run the algorithm for T = 10 epochs.To evaluate our algorithm, we generate 1-best (or 5-best) hypotheses using the beam search procedureand measure accuracy as the percentage of instancesin which the target sequence e is one of the 1-best(or 5-best) targets.
The input features are based oncharacter m-grams for m = 1, 2, 3.
Unlike previ-63ous generative transliteration models, no additionallanguage model feature is used.We compare our model against a state-of-the-artstatistical machine translation (SMT) system (Zenset al, 2005) and an averaged perceptron editmodel (PTEM) with identical features (Freitag andKhadivi, 2007).
The SMT system directly modelsthe posterior probability Pr(e|f) using a log-linearcombination of several sub-models: a character-based phrase translation model, a character-basedlexicon model, a character penalty and a phrasepenalty.
In the PTEM model, the update rule onlyconsiders the best target sequence and modifies theparameters w?+1 = w?
+ ?
(a, e, f) ?
?
(a?, e?, f)if the score s(e?, f) ?
s(e, f).Model (train+dev) 1-best 5-bestSMT 0.528 0.824PTEM 0.552 0.803MIRA 0.562 0.841Table 2: The 1-best and 5-best accuracy of differ-ent models on the Arabic-English transliteration task.At 95% confidence level, MIRA/PTEM outperform theSMT model in 1-best accuracy and MIRA outperformsPTEM/SMT in 5-best accuracy.Table 2 shows the 1-best and 5-best accuracy ofeach model trained on the combined train+dev dataset.
All the models are evaluated on the same testset.
Both MIRA and PTEM algorithms outperformthe SMT model in terms of 1-best accuracy.
Thedifferences in accuracy are significant at 95% con-fidence level, using the bootstrapping method forhypothesis testing.
The difference in 1-best per-formance of MIRA and PTEM is not significant.At 5-best, the MIRA model outperforms both SMTand PTEM model.
We conjecture that using theproblem-specific Levenshtein loss function helps fil-ter bad target sequences from the K-best outputsduring training.In a second experiment we studied the effectof changing C on the performance of the algo-rithm.
We ran the algorithm with the above set-tings, except varying the value of the complexityparameter to one of 7 values in the range C =0.00001, 0.0001, .
.
.
, 0.1, 1.0, training only usingthe train set, and evaluating the resulting model onModel (train) 1-best 5-bestC = 1.0 0.545?
0.832C = 0.5 0.548?
0.83C = 0.2 0.549?
0.834C = 0.01 0.545 0.852?C = 0.001 0.518 0.843C = 0.0001 0.482 0.798C = 0.00001 0.476 0.798Table 3: The effect of varying model parameter C on 1,5-best accuracy on the test set.
All the models are trainedwith Levenshtein loss and 20-best targets.
The super-script ?
indicates the models that achieved the greatestperformance on the dev set for a particular column.the test set.
The results are summarized in Table 3.The entry marked with a star ?
indicates the modelthat achieved the best performance on the dev set fora particular choice of evaluation measure (1-best or5-best).
We find that changing C does have an effecton model performance.
As the value of C decreases,the performance at lower ranks improves: C = 0.01is good for 5-best accuracy and C = 0.001 for 20-best accuracy (not in table).
As C is further reduced,a greater number of iterations are needed to con-verge.
In our model, where the alignments are notobserved but inferred during training, we find thatmaking small incremental updates makes our algo-rithm more robust.
Indeed, setting C = 0.01 andtraining on the train+dev set improves 5-best per-formance of our model from 0.841 to 0.861.
Hence,the choice of C is important.4 Conclusions and Future WorkWe have shown a significant improvement in accu-racy over state-of-the-art transliteration models bytaking into consideration the ranking of multiplehypotheses (top-K) by Levenshtein distance, andmaking the training algorithm robust to noisy non-separable data.
Our model does consistently wellat high (K = 1) and low ranks (K = 5), and cantherefore be used in isolation or in a pipelined sys-tem (e.g.
machine translation or cross-language in-formation retrieval) to achieve better performance.In a pipeline system, more features of names aroundproper nouns and previous mentions of the name canbe used to improve scoring of K-best outputs.64In our experiments, the Levenshtein loss functionuses only unit costs for edit operations and is notspecifically tuned towards our application.
In fu-ture work, we may imagine penalizing insertionsand deletions higher than substitutions and othernon-uniform schemes for better transliteration per-formance.
Our K-best framework can also be easilyextended to cases where one name has multiple for-eign translations that are equally likely.ReferencesNasreen Abdul-Jaleel and Leah S. Larkey.
2003.
Statis-tical transliteration for English-Arabic cross languageinformation retrieval.
In CIKM ?03, pages 139?146,New York, NY, USA.
ACM.Yaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in arabic text.
In Proceed-ings of the ACL-02 Workshop on Computational Ap-proaches to Semitic Languages, pages 1?13.Shane Bergsma and Greg Kondrak.
2007.
Alignment-based discriminative string similarity.
In ACL, pages656?663, June.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1994.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Markus Dreyer, Jason Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1080?1089, Honolulu, Hawaii, Oc-tober.
Association for Computational Linguistics.Dayne Freitag and Shahram Khadivi.
2007.
A sequencealignment model based on the averaged perceptron.
InEMNLP-CoNLL, pages 238?247.K.S.
Jeong, S. H. Myaeng, J.S.
Lee, and K.-S.Choi.
1999.
Automatic identification and back-transliteration of foreign words for information re-trieval.
Information Processing and Management,35:523?540.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In Philip R. Cohen and WolfgangWahlster, editors, Proceedings of the Thirty-Fifth An-nual Meeting of the Association for ComputationalLinguistics and Eighth Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 128?135, Somerset, New Jersey.
Associa-tion for Computational Linguistics.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.Andrew McCallum, Kedar Bellare, and Fernando Pereira.2005.
A conditional random field for discriminatively-trained finite-state string edit distance.
In UAI.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Flexible text segmentation with structuredmultilabel classification.
In HLT-EMNLP, pages 987?994, Vancouver, BC, Canada, October.
Association forComputational Linguistics.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005b.
Online large-margin training of dependencyparsers.
In ACL, pages 91?98, Ann Arbor, Michigan,June.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In ACL, pages 944?951, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Paola Virga and Sanjeev Khudanpur.
2003.
Translit-eration of proper names in cross-lingual informationretrieval.
In Proceedings of the ACL 2003 workshopon Multilingual and Mixed-language Named EntityRecognition, pages 57?64, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Dmitry Zelenko and Chinatsu Aone.
2006.
Discrimi-native methods for transliteration.
In EMNLP, pages612?617, Sydney, Australia, July.
Association forComputational Linguistics.R.
Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,J.
Xu, Y. Zhang, and H. Ney.
2005.
The RWTHPhrase-based Statistical Machine Translation System.In Proceedings of the International Workshop on Spo-ken Language Translation (IWSLT), Pittsburgh, PA,USA.65
