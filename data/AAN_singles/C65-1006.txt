61965 International Conference on Computational LinguisticsAUTOmaTIC LINGUISTIC CLASSIFICATIONE.
D. Pendergraft and N. DaleLinguistics Research CenterThe University of TexasAustin i Texasl U. S. A.yABSTRACTMany investisators have recognized that so-phisticated mechanical translation or other computationallinguistic systems will require language learning capa-bilities, both in the ability to discover and verify newdescriptive rules, and to adapt rules to new situationsof use.The work plan of a long-range series of ex-periments in automatic linguistic classification is de-scribed, together with a discussion of the first experi-ment now in progress.
The latter is concerned with categoryidentification.
In particular the data resulting fromautomatic syntactic analysis of English texts will beused to identify syntactic categories which have similarmembership.The series of experiments will accordingly combinethe use of automatic linguistic analysis and automaticclassification techniques.
Automatic syntactic analysis,and in later experiments semantic analysis, will be performedwithin the Linguistics Research System (LRS).
Automaticclassification will be carried out within the AutomaticClassification System (ACS).
Both of these computer systemshave been developed by the Linguistics Research Center.LRS is a large-capacity system designed especiallyto support research in computational linguistics, Itcurrently has the capability of performing several typesof generalized translation, and of transferring informationamong any number of languages through the use of interlingualdescriptions.
This system operates under its own monitor.ACS is a Fortran IV system operating under theIBSYS monitor.
It performs a variety of classificationoperations on large universes of objects having specifiedproperties.
Either objects or properties may be classified,and correlations may be computed among resulting classesA programming interface is being constructed betweenthese two systems so that their combined capabilities can beused for automatic linguistic classification.Pendergra f t ,  Dale 1-1INTRODUCTIONMechanical translation research over almosttwo decades has led to a broader discipline, computationallinguistics, which already includes within its concern theautomated processes that collect, store, retrieve orcommunicate information conveyed in or about language,as well as translate one language into another.
Uithprogress in automatic classification, another possibilityis being explored, that of creating new information ratherthan merely gathering, maintaining or distributing theresults of human intellectual activities.Many investigators have noted that sophisticatedlinguistic systems must be capable of learning.
A newterm may be defined within a text being translated me-chanically.
Or, more commonly, a new meaning may be givena term used in close communication among colleagues.Bar-Hillel, for one, has waxed and waned in enthusiasmfor mechanized linguistic learning, finally relegatingeven its investigators to the "lunatic fringe" ofcomputational linguistics.
Some thoughtful work hasbeen done by Lamb \[i\] .
Certainly Solomonoff \[2\] shouldbe mentioned, as should Knowlton \[3\] and Sparck-Jones \[4\],but each for a different reason.
There is hardly aliterature to cite, unless it be that unruly assemblage wehave come to call "artificial intelligence.
"The name "self-organizing system" has also comeinto use.
We will adopt it, so that "learning" and"adapting" may distinguish different kinds of self-organization.We observe, furthermore, that to process informationone must first process the language (or symbolic system)Pendergraft, Dale I-~in which the information is given.
As a consequence,every information processing system has a componentthat processes linguistic information.
And thatcomponent, we now know, may have a subcomponent whichprocesses meta-linguistic information.Self-organizing linguistic systems properlyfall within the scope of meta-linguistic processing.The information being processed is about some language,the "object-language" of the communication; hence thevehicle by which the information is conveyed is a"meta-language."
If the self-organizing system haschanged the description of the conventional alternativesavailable within the object-language, then we will saythat the system is "learning."
Whether or not thealternatives remain unchanged, if some alteration hasbeen made in the conventions of their use, the systemwill be "adapting.
"Thus, roughly speaking, learning will involvesome change in linguistic rules that describe a set ofwell-defined alternatives in the object-language.Adaptation will involve some change in a set of probabilitiesthat describe how those alternatives are being used.A self-organizing information system, in contrastto one learning or adapting by meta-linguistic processing,would employ linguistic processing to create new informationabout some subject-matter not necessarily linguistic.
Butsince the information so processed might indeed be aboutlanguage, we anticipate that linguistic self-organizationmay be based either meta-linguistically or linguistically.For the present, however, our system will be basedon meta-linguistic processing.
Work in meta-syntacticsPendergraft ,  Dale i-3is progressing rapidly; researchers in computationallinguistics now face the obligation of testing hypo-theses more rigorously than with heuristic argumentsor typical linguistic examples.
More careful investi-gation is need in meta-semantics, i.e.
in the relationsbetween meta-linguistic and linguistic information.In essence, then, we will try with automaticlinguistic classification to bridge the gap between thedesign of language and the events of spoken or writtendiscourse.
What we have to report is only a smallbeginning toward that objective.We recognize that these are difficult problemsrequiring long-range commitments.
They are neverthelesscentral to improving the language data used in automatedanalysis, synthesis and translation.
Moreover, they leadto the concept of a dynamic language data base in lin-guistic processing.Already it is clear that the amount ofinformation contained in a language description greatlyinfluences efficiency in linguistic processing.
Contraryto our former intuition, a simple description may merelybe deficient in information so that the search in automatedanalysis will be extended unduly.
There appears, furthermore,to be an optimal size in the syntactical descriptive unit.Thus, in making the transition from syntactical to semanticaldescription (at least for the theories \[5\] we are studying),the basic question is analogous to that in the transitionfrom lexical to syntactical description which gives rise tomorphology: viz.
what objects are to be classified?
Weare attacking these semological and morphological problemsPendergraft, Dale 1-4within the same theoretical structure that determines howthe resulting objects are  to be classified.
Indeed, thetwo questions appear inseparable.Pendergraf t ,  Dale 2-1BASIC PROGRAHSThe Automatic Classification System (ACS), aFortran IV programming package (7) based on the classifi-cation theories of Needham and Parker-Rhodes (8) has beendeveloped by the Linguistics Research Center of TheUniversity of Texas (under support of the National ScienceFoundation and the U. S. Army Electronics Laboratories),and has been made generally available for classificationresearch.
The version of the system used in our ownfacility has been augmented with list-processing routinesand other specialized programming which greatly increasedits efficiency and data-capacity.ACS is a generalized classification system whichcan be applied to non-linguistic as well as linguisticproblems.
Its basic inputs are data describing the inci-dence (or the frequency of incidence) of particular propertiesupon particular objects.
These incidence data may be trans-posed, so that either the properties or the objects can beclassified.
Various measures of the similarity between pairsof the objects (or properties) are available, permitting theincidence data to be used in computations of the connectionsbetween object (or property) pairs.
Using these connectiondata, other routines group together "clumps" of objects withsimilar properties (or of properties occurring similarly inthe objects).
Various kinds of the clumps can be discovered.ACS has a section which controls the selection of similaritymeasures and clumping methods in classification experiments (6).To formalize the concepts of distributional classi-fication, e.g.
those investigated by Hockett (8) and by Harris(9) we have extended the general classification theory to binaryas well as singulary relations.
In linguistic classificationthese can be interpreted as constitutive relations (e.g.Pendergra f t ,  Dale 2-2concatenation).
This interpretation, more exactly, assumes thatthe incidence data describe pairs of objects standing in thatparticular relation.
Clumps of similar objects are then foundin both the domain and counterdomain of the relation.
Finally,individual clumps in the domain are paired with individualclumps in the counterdomain according as the connections betweenmembers of the two clumps are dense (in a precise sense) rela-tive to the entire set of connections.
Pairs of clumps may alsobe found by using a measure of relative sparseness in the connec-tions.
These capabilities have been added to ACS, and programmingis being done to prepare incidence data mechanically from theresults of automatic analysis in LRS (i0).The automatic analysis algorithms in LRS are linguisti-cally generalized, i.e.
they will recognize the expressions ofany object-language according to the exact specifications describedin particular meta-languages.
These language data, furthermore,are operationally generalized; they will be given relationally(solely in terms of relations and not as a process) so that syn-thesis as well as analysis algorithms may refer to the samedescriptions.
Object-language descriptions are conveyed by ahierarchy o?
meta-languages rather than by a single meta-language(5).
The complete data hierarchy will be given by lexical, syn-tactical, semantical and pragmatical meta-languages; the firstthree are currently available in LRS.
Lexical, syntactic, semanticand pragmatic analysis (or synthesis) algorithms will be orientedto the corresponding levels of monolingual data.
Analysis willaffect a transfer to the next higher level of processing; syn-thesis to the next lower level.
Automated lexical and syntacticanalysis (as well as synthesis and translation) are operationalin LRS, and the semantic algorithms will be later this year.
Allof the algorithms are parallel, stochastic, heuristic and machine-independent; that is to say, they have the following design fea-tures which we believe to be important in automatic linguisticclassification experiments:Pcndergra f t ,  Dale 2-3(a) They carry forward a search for all possible lin-guistic alternatives in parallel, instead of following tocompletion one sequence of alternatives before beginning an-other.
As a result, all of the available linguistic evidenceis represented in the analysis output.
(b) They compute a probab i l i ty  for  each l ingu is t i ca l te rnat ive  be ing  processed .
The probab i l i ty  w i l l  be thesame in ana lys i s  or synthes is ;  i t  represents  the l i ke l ihoodof occur rence  in tile language ra ther  than in the process .
(c) They {may or may not ,  as a mat ter  of cho ice)  usethe probab i l i t i es  as heur i s t i c  c r i te r ia  to l im i t  the ana lys i soutput to the most likely alternatives.
(d) They are o r iented  ent i re ly  to par t i cu la r  meta-syntactical and meta-semantical relations, not to the com-ponents of a particular computer.
All processing decisionsand results will be the same on every computer large enoughto do the linguistic processing.Pendergraft, Dale 3-1MORPHOLOGICAL CLASSIFICATIONConstitutive relations may, of course, be usedas the basis of classification of individual objects(i.e.
for singulary as opposed to binary classification).A formal distinction can also be made between the classesof objects which are identical and those which are to somedegree equivalent in distribution.
The former, to be speci-fic, have identical clump membership and are thus indistin-guishable with respect to the particular constitutive rela-tion described in the incidence data.
The latter have commonmembership in a particular set of clumps and, as a consequence,share certain distributional properties which are representedby those clumps.Morphological classification, therefore, willinvolve the following basic operations within our theories.
(These will also be pertinent to our remarks below aboutsemological classification.)
For the given constitutiverelation, the classification algorithm will have to:(a) Recognize, among the objects potentially madeavailable by segmentation, those which are to be classified.
(b) Perform singulary classification of the recognizedobjects to determine which subsets of them have identicaldistribution relative to that constitutive relation.We assume, in the morphological problem, that theobjects to be classified are lexical units (whether phoneticor orthographic) and that concatenation is the constitutiverelation.
Our working hypothesis is that the morphologicalobjects are those which maximize the connection entropy.This seems intuitively reasonable, since more or less homo-geneous connections would be anticipated among objectsPendergraft, Dale 3-2having e lementary s ta tus .
Converse ly ,  a re la t ive ly  s t rongconnect ion between two ob jec ts  would be evidence that  theywere par ts  of a s ing le  const ruct .Accordingly, a routine has been added to ACS tonormalize the connection data and,nfOr the normalized con-nections PI' P2' "''' P2 (i ?
Pi; r Pi = i), to compute- i=  1 the connection entropynH(PI'  P2' " ' ' '  Pn ) = -r pj log pjj=lfor  a convenient  logar i thmic  base \[11\].
The second operat ion ,that  of determin ing what ob jec ts  have ident ica l  d i s t r ibut iona lp roper t ies ,  w i l l  be handled by a rout ine  which f inds the setsin the in tersect ions  def ined by the co l lec t ion  of a l l  clumps.
(We will say that the members of the identit Z classes form"component sets" of the universe of objects being classified,because the sets partition that universe.
)Some of the strategies which may be used in mor-phological classification have been compared by Hockett \[12\]who concluded that different classification methods couldsucceed in establishing the same relation between morphemesand phonemes.
A strategy chosen for automation must, aboveall, be computationally tractable.
The two methods whichHockett calls the "morph approach" and the "morphophonemicapproach" would have inherent advantages or disadvantagescomputationally.The "morph approach," according to Hockett, sup=poses that morphemes are represented by morphs and that morphsare composed by phonemes.
In consequence, the constitutivePendergraft, Dale 3-3relation (concatenation) must obtain between constructionsof phonemes.
By our hypothesis, a set of morphs would beany set of the constructions maximizing (perhaps locally)the connection entropy.
And members of each identity classof the morphs would be the allomorphs of a particular morpheme.Computationally, then, we might carry out the fol-lowing morphological classification algorithm:(a) Perform lexical analysis, using the current set ofphoneme constructs as the lexical data (lexicon).
(b) Prepare incidence data describing the constitutiverelation between the contructs.
(c) Compute the connections between pairs of the con-structs and the connection entropy, comparing the result withthe entropy of the preceding cycle.
(d) I f  the entropy has increased ,  combine the (one ormore, depending upon the ra te  of increase)  s t rong ly  connectedpa i rs  in to  s ing le  const ructs ;  re turn  to (a).
(e) If the entropy has not increased, perform singularyclassification and find the component sets of morphs whichwill represent morphemes.l l ocket t ' s  "morphophonemic approach,"  in cont ras t ,would take morphemes to be composed of morphophonemes andmorphophonemes to be represented  by phonemes.
One in terpre -ta t ion  of these re la t ions  in terms of the c lass i f i ca t iontheor ies  (among severa l )  would involve the suppos i t ion  thatthe members of each ident i ty  c lass  of the phonemes representPendergra f t ,  Dale 3-4a par t i cu la r  morphophoneme.
Consequent ly ,  any phoneme re-p resent ing  the morphophoneme M would be d i s t ingu ished  in theinc idence data only as an M. A phoneme const ruct ,  s imi la r ly ,would be d i s t ingu ished  only as the const ruct  of representedmorphophonemes.
Any set  of the morphophoneme const ructs  max-imiz ing (maybe loca l ly )  the connect ion entropy would be re-cognized as morphemes.That these relations would call for a differentcomputational strategy should be evident.
Because of thehigher level of abstraction, one might anticipate that (a)there would be fewer morphophoneme than phoneme constructs,and (b) the latter would occur more frequently than theformer in the outputs of lexical analysis.
But our aim isnot to prejudge the computational advantages of one approachabove another; the schemes which are feasible within ouranalysis and classification capabilities will be tested.Pendergra f t ,  Dale 4-1SYNTACTICAL CLASSIFICATIONThe advantages of abstraction in classification arenevertheless striking in syntactical applications.
Indeed thepossible gains seem so promising that we have bypassed automatedmorphological classification in our first experiments to investi-gate the following operations of syntactical classification.
Eachoperation presupposes not only the existence of a set of morphemes,but an assignment of the morphemes to syntactical equivalenceclasses relative to concatenation, as already described.
(i) Identification of classes.
If, in the outputs of syn-tactical analysis, it is found that some expression has been(ambiguously) recognized both as an A and as a B, then this coin-cidence of A and B will be the event counted.
Singulary classi-fication will then be performed to determine whether an A and aB are distinguishable distributionally relative to coincidence.If not, we will induce that the predicates "A" and "B" are co-extensive, i.e.
they denote the same objects \[13\].
The two pre-dicates will therefore be replaced (wherever they occur in thesyntactical description) by a single predicate.
(2) Generalization of classes.
During the class identifi-cation operation (i), the event of being an A will be assigned toa set of (zero or more) clumps.
If being an A entails being inthe clump C, then we introduce the new predicate "C".
We induce,further, that the predicate "C" comprehends the predicate "A",i.e.
"C" denotes every object that "A" does \[13\].
And, since theextensions of the new predicates are clumps of objects sharingsome distributional property, we characterize "A" and "C" asostensive and distributional predicates, respectively, relativeto the constitutive relation.Taking a new incidence data to describe the relationof comprehension between the distributional and ostensive ,red-Pendergra f t ,  Dale 4-2icates, we will next perform singulary classification tobring together predicates which are similar relative tocomprehension.
Thus) we induce that the predicates havesimilar extensions.
The ostensive predicates in each clumpwill be replaced (in all their occurrences in the syntacticaldescription) by the distributional predicate of that clump,i.e.
by the predicate whose extension is the union of theextensions of the extensionally-similar predicates.
(K-clumping is convenient for generalizing classes because itprovides a parameter for the degree of generalization.
)(3) Rul_.__~e generation.
The aim of this operation willbe to find new syntactic rules (i.e.
taxonomic axioms) tobe added to the syntactical description.
The events to becounted in preparing the incidence data will be those inwhich an A is found to be concatenated to a B in the outputsof automated syntactic analysis.
Binary classification willbe used to pair clumps on the basis of dense connections, asexplained above.
For any resulting pair of densely connectedclumps C) D classifying an A as a C) and a B as a D) respec-tively) we generate the syntactic rules A ~ C, B = D andC D ~ E. The predicate "C D" will have as its extension anyC concatenated to any D. "E" will be a new predicate compre-hending "C'~D.
''Rules generated inductively will tend to be overlygeneral.
There will be an operation) however) by which syn-tactical classes can be specialized to conform to the empiricalanalysis data.
(4) Specialization of classes.
From the rules A ~ Cand C D ~ E we may infer the derived rule A D ~ E. Hence theapplication of A c C to C~D ~ E at the first (left-most) place\['endergraft, Dale 4-3in the la t te r  may be symbol ized a lgebra ica l ly  \[14\] as fo l lows :f-a "/'7~ ,.-~(c D F) (A CO)  = (A D c..E)Incidence data, prepared for one particular class Cp will de-scribe tile frequency of application of rules at places mention-iag that class.
The events counted, specifically, will be thosein which a rule X is found (in the analysis outputs) to be appliedat a place p in rule Y (i.e.
for the event "~xfPaY, the pair of ob-jects YP~x will be regarded as standing in the constitutive rela-tion).
Different places in the same rule will be treated as dif-ferent objects relative to application.
Binary classificationwill be used to pair densely-connected clumps of distributionallysimilar (in the domain) places of application~ and (in the coun-terdomain) rules being applied to the places.
The predicate "C"will be replaced (in those particular occurrences in the syntac-tical description) by a new predicate denoting that subclass of C.These syntactic classification operations will be opera=tional in the combined LRS-ACS programming system before the endof this year.
We plan an extension of the system to include auto-mated morphological, semological and semantical classification.The last will be restricted to a distributional semantics withoutidentification of references, i.e.
to the restricted form of ourtheoretical hypothesis \[5\] which assumes that applications atdifferent places in the same rule are independent events.Pendergraft, Dale 5-1SEMOLOGICAL CLASSIFICATIONRecently we observed \[iS\] that a small informationalunit in language data seems convenient for the descriptivelinguist, but a large informational unit would optimize lin-guistic processing.
In retrospect it appears likely that, inour project and elsewhere, different approaches to syntacticaldescription have too often been concerned with different infor-mational units rather than different information.
As antici-pated above, we have come to questions in syntactical classi-fication which are analogous to those in lexical classifica-tion which gave rise to morphology; viz.
what objects are tobe classified semantically?Joos \[16\] has stimulated our thinking about semology,as has La~nb \[12\].
Undoubtedly the latter's own interest inautomated syntactical classification \[i\] has contributed tothe similarity of our theories; the study of automatic lin-guistic classification brings one to consider informationalunits which are small enough to be discovered mechanically.Adopting Bloomfield's terminology \[18\], we willrefer to the elemental units of syntactical description (i.e.those rules conveying minimal units of information) as tasmemes.The elementary units to be classified semantically will besemes.
Between the two, we will posit semological relationsanalogous to those which Hockett presented for morphology.
(i) The first hypothesis would be that sememes arerepresented by semes, and that semes are composed of tagmemes.Within the frame of our classification theories, therefore,the constitutive relation would be application: the semeswould be regarded as the representatives of a particularsememe.
This is the approach we will take in our firstsemological experiments.Pendergraft, Dale  5-2(2) Semes would be composed of semotagmemes, in tilesecond hypothesis, and semotagmemes represented by tagmemes.Consequently, for the purposes of automated classification,the members of an identity class relative to applicationwould be regarded as the representatives of a particularsemotagmeme.
A set of semotagmeme constructs (locally)maximizing the connection entropy would be recognized assemes.
This approach to automated semological classifica-tion may have the advantage of a higher level of abstraction,like the analogous morphophoneme approach in morphologicalclassification.Both semological hypotheses will be tested when wehave the additional data-capacity which a magnetic disk willprovide in ACS early next year.
LRS programs that maintaineither type of semological data are already operational.Pendergraft, Dale 6-16 SEMANTICAL CLASSIFICATIONSememes, in the sense which may be formalized assuggested above, are regarded in our working hypothesis asdescribing signs in the object-language.
To be specificthey will have two epistemological functions:(a) They w i l l  convey the ( fo rmat iona l )  syntax of theob jec t - language,  i .e .
the in fo rmat ion  needed to const ructcomplex s igns from the bas ic  ones.
(b) They will be units substituted in translation,paraphrasing and other transformations based on semanticalcriteria.A fundamental principle leading to distributionalsemantics was cited by Martin \[15\] in 1958.
In discussing"translational" and "non-translational" semantical meta-languages, he presents a thesis which we will paraphrasevery roughly for our present purpose:Semantical relations (e.g.
denotation, designation),in requiring as their arguments both signs and their objects(denotata, designata), make it necessary that the semanticalmeta-language itself have signs for the same objects as theobject-language.
The meta-language signs are, accordingly,translations of the object-language signs, since the two setshave common objects.
As a consequence of this, semanticalrelations in the meta-language will be at least as complexas those in the object-language.
However a "non-translational"semantical meta-language may describe a relation between signs,but one defined in semantical terms (e.g.
comprehension, whereone sign will comprehend another if the former denotes everyPendergraf t ,  Dale 6-2object  the la t te r  does).
This second type of meta-languagewill be semantically less complex than the object-language.Furthermore, as we have suggested above, it isprobable that comprehension of signs may be induced fromdistributional evidence.
A distributional semantics, inaddition to being a non-translational in Martin's sense,would define comp\[ehension or some alternative relation be-tween signs in purely distributional terms) leaving aside alltheoretical references to objects which the signs may or maynot have.
This is the approach we \]lave taken) by employingthe concepts of classification theory to formalize those ofdistribution.With few exceptions the computational strategiesin semantical classification will be the same as in thedistributional syntactics.
Analogous operations of classidentification, generalization and specialization will beavailable.
But the members of syntactical classes will besyntactic rules.
And the rules in a given class will berequired to have the same "degree," i.e.
the same numberof those predicates with the equivalence (but not theidentity) classes as their extensions \[5\].Generation of semantic rules will likewise beanalogous to the syntactical operation.
But our semanticalhypothesis requires that all of the syntactic rules in theextensions of two semantical classes be applied (pairwise)at~laces of application with the same name.
For instanceA B would describe the applications of the rules in seman-tical class B to those in the class A at the places named bythe numeral 2.
When the syntactic rules are first generated,Pendergraft, Dale 6-3their places of application will be named positionally (fromleft to right) and, in the restricted theory, uniquely (notwo places will have the same name).
Binary semantical classi-fication, as part of the rule generation operation, will showhow the places should be renamed to satisfy the above semanti-cal convention.
(In LRS this is the information conveyed by"superscripts" associated with the appropriate predicates insyntactic rules.)
Otherwise the generated semantic rules willbe formally the same as the syntactic (e.g.
A = C, B c D,C~D c E).
The numeral naming the place of application oftwo semantical classes is given in our notations as part ofthe connective symbolizing application.
Conventions for re-naming the places during deductive inference \]lave been reportedelsewhere \[19\].Pendergraft, Dale 7-1SELF-ORGANIZING LINGUISTIC SYSTE~4SAutomatic linguistic classification will give usvarious capabilities for changing language descriptions.l%e plan to study each capability separately so that it willreceive its own development.
Coordination of the capabil-ities into an integrated system will be approached as adifferent problem, that of self-organization.
The systemas a whole must not only change, but change for the better.I Iomeostasis ,  as exp la ined by Ashby \[20"\], is thefundamental cont ro l  p r inc ip le  we w i l l  invest igate .
Roughlyspeaking,  i t  ca l l s  for  reorgan izat ion  when the s i tuat ion(according to some cr i te r ion)  is get t ing  worse and s tab i l i tywhen i t  is  get t ing  bet ter .
Hence the a lgor i thms we descr ibedfor  morphologica l  (or semologica l )  c lass i f i ca t ion  were toosimple.
I f  a decrease in connect ion entropy def ines  "get t ingworse" in morphologica l  (or semolog ica l )  c lass i f i ca t ion ,  thesystem must be able to de l iver  smal le r  as well  as la rger  con-s t ruc ts  dur ing i t s  reorgan izat ion .
In syntact i ca l  (or seman-t i ca l )  c lass i f i ca t ion ,  s tab i l i ty  or reorgan izat ion  (in responseto decreas ing  or inc reas ing  ent ropy ,  respect ive ly )  may be ob-ta ined by a choice between the class identification and general-ization operations.
With K-clumping, class generalization mayalso be parameterized to specify a greater or lesser reorgani-zation in descriptive categories.These basic control techniques will be tried towardthe end Of this year.
To control class specialization andrule generation, we will use the following processing sequenceafter each cycle of syntactic (or semantic) analysis.Pendergraft, Dale 7-2(a) Compute the connections and connection entropyfor each class.
(h) Sort the classes so that those with the lowestentropy come first.
(c) Perform the class specialization operation on thesuccessive classes until one is reached which cannot be spe-cialized.
(d) Use only that class and the ones following it forrule generation.Underlying this processing strategy is the  assump-tion that stable classes will be characterized by high con-nection entropy.
(Though plausible, this must be tested.
)Rule generation will thus be limited, as a result of thestrategy, to those classes which are found to be the moststable.
Broadly effective control strategies are our pres-ent concern; we believe it will be possible to supplementthese with more selective controls later on.Incidence data for our first automatic linguisticclassification experiments }lave been prepared mechanicallyfrom statistics brought directly to ACS from the analysisoutputs in LRS.
For the self-organizing linguistic systemwe felt that the statistics should he accumulated from anal-ysis statistics from LRS to the Information ?laintenance Sys-tem (IHS), a coordinate information storage and retrievalsystem \[21\] which we have programmed fol the AeronauticalSystems Division, Air Force Systems Command.
This systemhas been released by its sponsor for use in linguisticresearch.
Classification statistics from ACS will also bePendergraf t ,  Dale 7-3stored in IMS.
A report generator will be added to I~S sothat the analysis and classification statistics can be dis-played in formats suitable for publication.Programming to implement the Self-organizing Lin-guistic System (SLS) will include the following routines:7.1 LRS-IMS InterfaceTransportation of the analysis statistics on coin-cidence, concatenation and application at the different lin-guistic levels will be performed by these programs.
In addi-tion to collecting and organizing the statistics, they willupdate the stores in I~|S, also handling the additions anddeletions of rules or classes.
Normalizing factors will bemaintained cumulatively so that statistics collected duringdifferent periods of time may be compared.
These programsare now almost completed.7.2 I~!S-ACS InterfaceThis set of programs will carry out tile controlstrategies we have mentioned.
They are being written underIBSYS so that they will be compatible with ACS Programming.The 151S store has been designed so that it can be manipulatedunder either the LRS operating system or IBSYS.
It is antic-i~ated that most of these routines will be in operation be-fore the end of 1965.7.3 ACS=IMS InterfaceClassification results will be collected, organizedand transported to IMS by these routines.
They will also up-date the I~4S store.
Their completion will coincide with rou-tines in the IMS-ACS interface.Pendergra f t ,  Dale 7-47.4 IMS-LRS InterfaceThe same request  formats  which the l ingu is t  usesin add ing ,  chang ing  or de le t ing  language data  in LRS w i l lbe used by the se l f -o rgan iz ing  system, l lowever, languagedata process ing  in LRS may be per formed e i ther  wi th  mnemonicsymbols or numera ls  as the names of syntact i ca l  (or semant ica l )c lasses .
The automated system wi l l  use the numera ls ,  re fe r -enc ing i t s  requests  to the resu l t s  of automat ic  c lass i f i ca t ion .Because the self-organizing system will be able tomake extensive changes in the data base, which would be pro-hibitive by manual coding, we plan to provide macro-requests(e.g.
a request to eliminate the distinction between the pred-icates grouped together by the generalization operation).Pendergra f t~ Dale 8-1AN EXPERIMENTThis experiment in class identification willexemplify the type of research we are performing.Although the operations performed are those describedabove as class generalization, by setting the K-clumpingparameter to 1 we obtain component sets as the classifi-cation output.8.1 Experimental DesignGeneral DefinitionsGiven a binary matrix:l(i,j):l(i):l(j):the number of l's in the intersectionof columns i and j.the number of l's in column i.the  number of  l ' s  in column j .Phase I:Part I:Forming Connection MatrixCo,struct Incidence Array AA = (ai,j)ai, j - 1such that.
th  the ~ object isdescribed by the i thproperty.a i)j = 0 0 otherwisePendergra f t ,  Dale 8-2Part 2: Compute Frequency Matrix from APart 3:Y = (fi,j) such thatfi,j = l(i,j) i# jfi,i = l(i)fj,j = l(j)Reduce Dimensions of FRemove rows and columns of P as follows~Part 4:irow (column) i is deleted(~> fi,j = 0 for all j, j~iNormalize columns of FN = (n i ,  j )  such that(Ni) j = fi,j/fj,jPart 5 : Compute Connect ion  ),.latrix CC = (c i~  j )  such thatrain k ) where m is  the  number(ni k'n~'~ of columns (rows) in the  Ci,j k=lnormalized reduced matrixF.Discussion: The frequency matrix which forms the incidencedata for the experiment is a table showing how many times anobject i coincides with an object j.
Reducing the matrixby removing columns which are all zero on all off-diagonalcells, deletes from the set of objects those objects forPendergraft, Dale 8-3which there are no coincidence data.
Normalizing thecolumns of F by the diagonal--which contains the numberof instances of the object in the sample--produces thenormalized incidence matrix.Connection matrix C is a symmetric matrix whichdescribes the relation of object i to object j based onthe normalized incidence data.
Matrix C constitutes thedata for the next stage of processing?Phase 2: Locat ing  GR-ClumpsDefinitionsC: Connection Hatrix computed in Phase i.U: Universe set (set of objects characterized inconnection matrix C.)A: A subset of U.~: U-A=~, complement of A.x: An element of U.An element of A (al,a2, ai:~i: An element of ~ (KI,~2)(Note: r+t=m)?
.a  t )?
.
F  r)c(a i )a j ) :  c i connect ion  of  ob jec t  i to ob jec t  j as de f ined  ,Jin the i j th  ce l l  of  C,tC{x,A):  r c(x ,a i )i=-irr c(x, i)i= lb(x ,A}:  C(x,A} = C(x)~) The bia_===~s of  ob jec ts  x to the set  Ais the excess (either positive or negative} of itsconnections to A less its connections to A.Pendergraft, Dale 8-4t tA X A: Z z c(a i ,a .
)Ji=l j= lr r7~ x ~: z z c(:2 i ,~ j )i=l j=lt ra X ~: z r c (a  i , F j )i=l j= lGR-clump: U set A is a GR-clump of U (----'> it is a localminimum for the following functionF(A) = A x 7~i iAXA+~X~In terms of individual elements, the definitioncan be stated as follows:A = {xIb(x,A)>_0VxeA andb (y,A)e OVye~}D iscuss ion :  There is  no known way to pred ic t  how many GR-clumpsex is t  in a g iven space .
The GR-clump f ind ing  procedures  \[22\]produce a set  of h igh ly  over lapp ing  GR-clumps.Phase 3:Par t  1:Forming K-Clumps of  ObjectsForra an ob ject -GR-c lump Inc idence  Array  AA = (a i , j )  such that?
th  = 1 i f  ob jec t  j is  in the 1 a i , ja.
= 0 o therwise  1 , jGR-clump.Pendergra f t ,  Da le  8-5Part 2: Form a Connection ~latrix FF = ( f i  ) such  that  ,Jfi,j = l(iIJ)l(i)+l(j)-l(i,j)fi,i = fj,j = 0Part 3: Locate K-clumps in FDiscussion: The K-clumps located in F will be thoseelements which are highly similar in ti~eir distributionalproperties.
The threshold value can be used to vary theamount of similarity.Pendergra f t ,  Dale 8-68.2 Tlle Exper imentPhase i:Data Base: Six paragraphs of Engl ish text weresyntact ical ly analyzed in LRS.
The outputs were on magnetictape.
A computer program was written to take this data andform a binary incidence array as follows:c lassesstrings oftext  i,ji, j = 1 if string i was in class jThe list of classes was generated at the same time.In the six paragraphs, 129 classes were found.
Graph 1 showsthe rate at which classes were found.In the  next  s tage  o f  p rocess ing ,  th i s  inc idence  ar raywas used to make a co - inc idence  f requency  count .
For ty - f i ve  ofthe  129 c lasses  occur red  un ique ly ,  i .e .
d id  not  co inc ide  w i thanother  c lass .
These 45 were de le ted  from the  data  set ,  l eav ing84 c lasses .The next  s tep  was to normal i ze  the  f requency  matr ixand compute the  connect ion  matr ix  as exp la ined  in  the  exper imenta ldes ign .
In the  84 x 84 matr ix  there  were 1012 nonzero  ent r iesg iv ing  a matr ix  dens i ty  of  14.3%.
The connect ion  va lues  rangedfrom zero  to 3 .33283.Classes-- PO ~ .I~ 0"~ -4 I~ I~0 0 0 0 0 ~ 0 0 0 0oOu00B!0I'00I0,100 C'J0~0C~CnO~000000L6")~U"13mQ ?Cm F-IC"130-17(DV-OoO0moo00I I IPendergraft, Dale 8-8Phase 2:GR-clumping was done in the connection matrix de-scribe in Phase I.
Using the pivot variable method of initialpartitioning \[22\] 44 GR-clumps were located.
Graph 2 displaysthe distribution (by size) of the GR-clumps found.Phase 3:The connect ion matr ix  was computed as descr ibed  inthe exper imenta l  des ign.
K-clumps par t i t ioned  the set  of84 categor ies  in tocomponent  sets .
The K-clumps ranged insize from 2-14 classes.
Graph 3 shows the number of classesby size of the K-clumps.0m0Percent of Total0 0 0 0GR-Clump Set03 "-4 (30O O.  O(,0OOO03u oN(1)OGb::0!Cbm ?..3O1mOolroO01o,IO0101E}O1(.,1103O03 (.,11-'4OU"b:::0"13POor)l oN(1)OGbIOm ?..3 "E}03OmOi'oOPercentoJ -1~O Oof K-Clump Set01 0~ ~1 0130 0 0 0 OEOOO,IO3~ oNd)OIOt -3(/}ol0ba:)ODP0moJ)>"0O,IO0i oN(1)OC~m ?-3 "OPendergraft, Dale R-IREFERENCES..S..S .....i0.ii.12.13.14.S.
~I.
Lamb, "On the Mechanization of Linguistic Learning,"University of California, Berkeley, California, 1961.R.
J. Solomonoff, "The Mechanization of Linguistic Learning,"Zator Company, Cambridge, Massachusetts, 1959.K.
C. Knowlton, "Sentence Parsing With A Self-OrganizingHeuristic Program," M.I.T., Cambridge, :4assachusetts, 1962.K.
Sparck Jones, "Synonymy and Semantic Classification,"Cambridge Language Research Unit, Cambridge, England, 1964.E.
D. Pendergraft, "Basic Methodology," Symposium on theCurrent Status of Research, University of Texas, Au-~tl~,Texas, 1963N.
Dale, "Automatic Classification System Users' Manual,"University of Texas, Austin, Texas, 1964.A.
F. Parker-Rhodes and R. ~I.
Needham, "The Theory of Clumps,"Cambridge Language Research Unit, Cambridge, England, 1960.C.
F. Ilockett, A Course in Modern Linguistics, New York City,New York, 1958.Z.
S. Harris, ~lethods in Structural Linguistics, Chicago,Illinois, 1951.A.
G. Dale, N. Dale and E. D. Pendergraft, "A ProgrammingSystem for Automatic Classification with Applications inLinguistic and Information Retrieval Research," Universityof Texas, Austin, Texas, 1964.A.
I. Khinchin, Mathematical Foundations of InformationTheory, New York City, New York, 1957.C.
F. Hockett, "Linguistic Elements and Their Relations,"Language, January 1961.R.
M. Martin, Truth and Denotation, Chicago, Illinois, 1958.W.
B. Estes, W. A. Holley and E. D. Pendergraft, "Formationand Transformation Structures," University of Texas, Austin,Texas, 1963.Pendergraft, Dale R-215.16.17.18.19.20.21.22.Linguistics Research Center, Re|~ort No.
23, Universityof Texas, Austin, Texas, 1965."
Summer Institute of Linguistics M. Joos, "Semology,University of Texas, Austin, Texas, 1961."
University "On the Nature of the Sememe, S. M. Lamb,of California, Berkeley, California, 1961.L.
Bloomfield, Language, Chicago, Illinois, 1953.D.
A. Senechalle, "Q-Collections and Concatenation,"University of Texas, Austin, Texas, 1963.W.
R. Ashby, Design for a Brain, New York City, New York,1960.
"Information Maintenance System ~(anual, B. Foster,University of Texas, Austin, Texas, 1964.A.
G. Dale and N. Dale, "Some ClumpiDg Experiments forInformation Retrieval," University of Texas, Austin,Texas, 1964.
