Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2368?2378,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning Language Games through InteractionSida I. Wang Percy Liang Christopher D. ManningComputer Science DepartmentStanford University{sidaw,pliang,manning}@cs.stanford.eduAbstractWe introduce a new language learningsetting relevant to building adaptive nat-ural language interfaces.
It is inspiredby Wittgenstein?s language games: a hu-man wishes to accomplish some task(e.g., achieving a certain configuration ofblocks), but can only communicate with acomputer, who performs the actual actions(e.g., removing all red blocks).
The com-puter initially knows nothing about lan-guage and therefore must learn it fromscratch through interaction, while the hu-man adapts to the computer?s capabilities.We created a game called SHRDLURN ina blocks world and collected interactionsfrom 100 people playing it.
First, we an-alyze the humans?
strategies, showing thatusing compositionality and avoiding syn-onyms correlates positively with task per-formance.
Second, we compare computerstrategies, showing that modeling prag-matics on a semantic parsing model accel-erates learning for more strategic players.1 IntroductionWittgenstein (1953) famously said that languagederives its meaning from use, and introduced theconcept of language games to illustrate the fluid-ity and purpose-orientedness of language.
He de-scribed how a builder B and an assistant A can usea primitive language consisting of four words?
?block?, ?pillar?, ?slab?, ?beam?
?to successfullycommunicate what block to pass from A to B. Thisis only one such language; many others would alsowork for accomplishing the cooperative goal.This paper operationalizes and explores the ideaof language games in a learning setting, which wecall interactive learning through language gamesFigure 1: The SHRDLURN game: the objectiveis to transform the start state into the goal state.The human types in an utterance, and the computer(which does not know the goal state) tries to in-terpret the utterance and perform the correspond-ing action.
The computer initially knows nothingabout the language, but through the human?s feed-back, learns the human?s language while makingprogress towards the game goal.(ILLG).
In the ILLG setting, the two parties do notinitially speak a common language, but nonethe-less need to collaboratively accomplish a goal.Specifically, we created a game called SHRD-LURN,1in homage to the seminal work of Wino-grad (1972).
As shown in Figure 1, the objectiveis to transform a start state into a goal state, butthe only action the human can take is entering anutterance.
The computer parses the utterance andproduces a ranked list of possible interpretationsaccording to its current model.
The human scrollsthrough the list and chooses the intended one, si-multaneously advancing the state of the blocks andproviding feedback to the computer.
Both the hu-man and the computer wish to reach the goal state1Demo: http://shrdlurn.sidaw.xyz2368(only known to the human) with as little scrollingas possible.
For the computer to be successful, ithas to learn the human?s language quickly over thecourse of the game, so that the human can accom-plish the goal more efficiently.
Conversely, the hu-man must also accommodate the computer, at leastpartially understanding what it can and cannot do.We model the computer in the ILLG as a se-mantic parser (Section 3), which maps natural lan-guage utterances (e.g., ?remove red?)
into logicalforms (e.g., remove(with(red))).
The seman-tic parser has no seed lexicon and no annotatedlogical forms, so it just generates many candidatelogical forms.
Based on the human?s feedback, itperforms online gradient updates on the parame-ters corresponding to simple lexical features.During development, it became evident thatwhile the computer was eventually able to learnthe language, it was learning less quickly thanone might hope.
For example, after learning that?remove red?
maps to remove(with(red)),it would think that ?remove cyan?
also mappedto remove(with(red)), whereas a humanwould likely use mutual exclusivity to rule out thathypothesis (Markman and Wachtel, 1988).
Wetherefore introduce a pragmatics model in whichthe computer explicitly reasons about the human,in the spirit of previous work on pragmatics (Gol-land et al, 2010; Frank and Goodman, 2012;Smith et al, 2013).
To make the model suitablefor our ILLG setting, we introduce a new onlinelearning algorithm.
Empirically, we show that ourpragmatic model improves the online accuracy by8% compared to our best non-pragmatic model onthe 10 most successful players (Section 5.3).What is special about the ILLG setting is thereal-time nature of learning, in which the humanalso learns and adapts to the computer.
Whilethe human can teach the computer any language?English, Arabic, Polish, a custom programminglanguage?a good human player will choose touse utterances that the computer is more likely tolearn quickly.
In the parlance of communicationtheory, the human accommodates the computer(Giles, 2008; Ireland et al, 2011).
Using Ama-zon Mechanical Turk, we collected and analyzedaround 10k utterances from 100 games of SHRD-LURN.
We show that successful players tend touse compositional utterances with a consistent vo-cabulary and syntax, which matches the inductivebiases of the computer (Section 5.2).
In addition,through this interaction, many players adapt to thecomputer by becoming more consistent, more pre-cise, and more concise.On the practical side, natural language systemsare often trained once and deployed, and usersmust live with their imperfections.
We believethat studying the ILLG setting will be integral forcreating adaptive and customizable systems, es-pecially for resource-poor languages and new do-mains where starting from close to scratch is un-avoidable.2 SettingWe now describe the interactive learning of lan-guage games (ILLG) setting formally.
There aretwo players, the human and the computer.
Thegame proceeds through a fixed number of levels.In each level, both players are presented with astarting state s ?
Y , but only the human seesthe goal state t ?
Y .
(e.g.
in SHRDLURN, Yis the set of all configurations of blocks).
Thehuman transmits an utterance x (e.g., ?removered?)
to the computer.
The computer then con-structs a ranked list of candidate actions Z =[z1, .
.
.
, zK] ?
Z (e.g., remove(with(red)),add(with(orange)), etc.
), where Z is allpossible actions.
For each zi?
Z, it computesyi= JziKs, the successor state from executing ac-tion zion state s. The computer returns to the hu-man the ordered list Y = [y1, .
.
.
, yK] of succes-sor states.
The human then chooses yifrom the listY (we say the computer is correct if i = 1).
Thestate then updates to s = yi.
The level ends whens = t, and the players advance to the next level.Since only the human knows the goal state t andonly the computer can perform actions, the onlyway for the two to play the game successfully isfor the human to somehow encode the desired ac-tion in the utterance x.
However, we assume thetwo players do not have a shared language, so thehuman needs to pick a language and teach it to thecomputer.
As an additional twist, the human doesnot know the exact set of actions Z (although theymight have some preconception of the computer?scapabilities).2Finally, the human only sees theoutcomes of the computer?s actions, not the actuallogical actions themselves.We expect the game to proceed as follows: Inthe beginning, the computer does not understand2This is often the case when we try to interact with a newsoftware system or service before reading the manual.2369what the human is saying and performs arbitraryactions.
As the computer obtains feedback andlearns, the two should become more proficient atcommunicating and thus playing the game.
Hereinlies our key design principle: language learningshould be necessary for the players to achievegood game performance.SHRDLURN.
Let us now describe the detailsof our specific game, SHRDLURN.
Each states ?
Y consists of stacks of colored blocks ar-ranged in a line (Figure 1), where each stackis a vertical column of blocks.
The actionsZ are defined compositionally via the gram-mar in Table 1.
Each action either adds toor removes from a set of stacks, and a set ofstacks is computed via various set operationsand selecting by color.
For example, the actionremove(leftmost(with(red))) removes thetop block from the leftmost stack whose topmostblock is red.
The compositionality of the actionsgives the computer non-trivial capabilities.
Ofcourse, the human must teach a language to har-ness those capabilities, while not quite knowingthe exact extent of the capabilities.
The actualgame proceeds according to a curriculum, wherethe earlier levels only need simpler actions withfewer predicates.We designed SHRDLURN in this way for sev-eral reasons.
First, visual block manipulations areintuitive and can be easily crowdsourced, and itcan be fun as an actual game that people wouldplay.
Second, the action space is designed to becompositional, mirroring the structure of naturallanguage.
Third, many actions z lead to the samesuccessor state y = JzKs; e.g., the ?leftmost stack?might coincide with the ?stack with red blocks?
forsome state s and therefore an action involving ei-ther one would result in the same outcome.
Sincethe human only points out the correct y, the com-puter must grapple with this indirect supervision,a reflection of real language learning.3 Semantic parsing modelFollowing Zettlemoyer and Collins (2005) andmost recent work on semantic parsing, we usea log-linear model over logical forms (actions)z ?
Z given an utterance x:p?
(z | x) ?
exp(?T?
(x, z)), (1)where ?
(x, z) ?
Rdis a feature vector and ?
?
Rdis a parameter vector.
The denotation y (succes-sor state) is obtained by executing z on a state s;formally, y = JzKs.Features.
Our features are n-grams (includingskip-grams) conjoined with tree-grams on the log-ical form side.
Specifically, on the utteranceside (e.g., ?stack red on orange?
), we use uni-grams (?stack?, ?, ?
), bigrams (?red?, ?on?, ?
), tri-grams (?red?, ?on?, ?orange?
), and skip-trigrams(?stack?, ?, ?on?).
On the logical form side, fea-tures corresponds to the predicates in the logicalforms and their arguments.
For each predicate h,let h.i be the i-th argument of h. Then, we de-fine tree-gram features ?
(h, d) for predicate h anddepth d = 0, 1, 2, 3 recursively as follows:?
(h, 0) = {h},?
(h, d) = {(h, i, ?
(h.i, d?
1)) | i = 1, 2, 3}.The set of all features is just the cross productof utterance features and logical form features.For example, if x = ?enlever tout?
and z =remove(all()), then features include:(?enlever?,all) (?tout?,all)(?enlever?,remove) (?tout?,remove)(?enlever?, (remove, 1,all))(?tout?, (remove, 1,all))Note that we do not model an explicit alignmentor derivation compositionally connecting the utter-ance and the logical form, in contrast to most tradi-tional work in semantic parsing (Zettlemoyer andCollins, 2005; Wong and Mooney, 2007; Lianget al, 2011; Kwiatkowski et al, 2010; Berantet al, 2013), instead following a looser model ofsemantics similar to (Pasupat and Liang, 2015).Modeling explicit alignments or derivations isonly computationally feasible when we are learn-ing from annotated logical forms or have a seedlexicon, since the number of derivations is muchlarger than the number of logical forms.
In theILLG setting, neither are available.Generation/parsing.
We generate logical formsfrom smallest to largest using beam search.Specifically, for each size n = 1, .
.
.
, 8, we con-struct a set of logical forms of size n (with ex-actly n predicates) by combining logical forms ofsmaller sizes according to the grammar rules in Ta-ble 1.
For each n, we keep the 100 logical forms zwith the highest score ?T?
(x, z) according to thecurrent model ?.
Let Z be the set of logical formson the final beam, which contains logical formsof all sizes n. During training, due to pruning at2370Rule Semantics DescriptionSet al() all stacksColor cyan|brown|red|orange primitive colorColor?
Set with(c) stacks whose top block has color cSet?
Set not(s) all stacks except those in sSet?
Set leftmost|rightmost(s) leftmost/rightmost stack in sSet Color?
Act add(s, c) add block with color c on each stack in sSet?
Act remove(s) remove the topmost block of each stack in sTable 1: The formal grammar defining the compositional action space Z for SHRDLURN.We use c to denote a Color, and s to denote a Set.
For example, one action that wehave in SHRDLURN is: ?add an orange block to all but the leftmost brown block?
7?add(not(leftmost(with(brown))),orange).intermediate sizes, Z is not guaranteed to containthe logical form that obtains the observed state y.To mitigate this effect, we use a curriculum so thatonly simple actions are needed in the initial levels,giving the human an opportunity to teach the com-puter about basic terms such as colors first beforemoving to larger composite actions.The system executes all of the logical forms onthe final beam Z, and orders the resulting denota-tions y by the maximum probability of any logicalform that produced it.3Learning.
When the human provides feedbackin the form of a particular y, the system forms thefollowing loss function:`(?, x, y) = ?
log p?
(y | x, s) + ?||?||1, (2)p?
(y | x, s) =?z:JzKs=yp?
(z | x).
(3)Then it makes a single gradient update using Ada-Grad (Duchi et al, 2010), which maintains a per-feature step size.4 Modeling pragmaticsIn our initial experience with the semantic pars-ing model described in Section 3, we foundthat it was able to learn reasonably well, butlacked a reasoning ability that one finds in hu-man learners.
To illustrate the point, considerthe beginning of a game when ?
= 0 in thelog-linear model p?
(z | x).
Suppose thathuman utters ?remove red?
and then identifies3We tried ordering based on the sum of the probabilities(which corresponds to marginalizing out the logical form),but this had the degenerate effect of assigning too much prob-ability mass to y being the set of empty stacks, which canresult from many actions.zrm-red= remove(with(red)) as the cor-rect logical form.
The computer then performsa gradient update on the loss function (2), up-weighting features such as (?remove?,remove)and (?remove?,red).Next, suppose the human utters ?remove cyan?.Note that zrm-redwill score higher than all otherformulas since the (?remove?,red) feature willfire again.
While statistically justified, this be-havior fails to meet our intuitive expectations fora smart language learner.
Moreover, this behav-ior is not specific to our model, but applies to anystatistical model that simply tries to fit the datawithout additional prior knowledge about the spe-cific language.
While we would not expect thecomputer to magically guess ?remove cyan?
7?remove(with(cyan)), it should at least pushdown the probability of zrm-redbecause zrm-redintuitively is already well-explained by another ut-terance ?remove red?.This phenomenon, mutual exclusivity, was stud-ied by Markman and Wachtel (1988).
They foundthat children, during their language acquisitionprocess, reject a second label for an object andtreat it instead as a label for a novel object.The pragmatic computer.
To model mutual ex-clusivity formally, we turn to probabilistic mod-els of pragmatics (Golland et al, 2010; Frank andGoodman, 2012; Smith et al, 2013; Goodman andLassiter, 2015), which operationalize the ideas ofGrice (1975).
The central idea in these models isto treat language as a cooperative game betweena speaker (human) and a listener (computer) aswe are doing, but where the listener has an ex-plicit model of the speaker?s strategy, which inturn models the listener.
Formally, let S(x | z) bethe speaker?s strategy and L(z | x) be the listener?s2371zrm-redzrm-cyanz3, z4, .
.
.p?
(z | x)?remove red?
0.8 0.1 0.1?remove cyan?
0.6 0.2 0.2S(x | z)?remove red?
0.57 0.33 0.33?remove cyan?
0.43 0.67 0.67L(z | x)?remove red?
0.46 0.27 0.27?remove cyan?
0.24 0.38 0.38Table 2: Suppose the computer saw one exam-ple of ?remove red?
7?zrm-red, and then the hu-man utters ?remove cyan?.
top: the literal lis-tener, p?
(z | x), mistakingly chooses zrm-redover zrm-cyan.
middle: the pragmatic speaker,S(x | z), assigns a higher probability to to ?removecyan?
given zrm-cyan; bottom: the pragmatic lis-tener, L(z | x) correctly assigns a lower probabil-ity to zrm-redwhere p(z) is uniform.strategy.
The speaker takes into account the literalsemantic parsing model p?
(z | x) as well as a priorover utterances p(x), while the listener considersthe speaker S(x | z) and a prior p(z):S(x | z) ?
(p?
(z | x)p(x))?, (4)L(z | x) ?
S(x | z)p(z), (5)where ?
?
1 is a hyperparameter that sharpensthe distribution (Smith et al, 2013).
The com-puter would then use L(z | x) to rank candidatesrather than p?.
Note that our pragmatic model onlyaffects the ranking of actions returned to the hu-man and does not affect the gradient updates ofthe model p?.Let us walk through a simple example to see theeffect of modeling pragmatics.
Table 2 shows thatthe literal listener p?
(z | x) assigns high probabil-ity to zrm-redfor both ?remove red?
and ?removecyan?.
Assuming a uniform p(x) and ?
= 1, thepragmatic speaker S(x | z) corresponds to normal-izing each column of p?.
Note that if the pragmaticspeaker wanted to convey zrm-cyan, there is a de-cent chance that they would favor ?remove cyan?.Next, assuming a uniform p(z), the pragmatic lis-tener L(z | x) corresponds to normalizing eachrow of S(x | z).
The result is that conditioned on?remove cyan?, zrm-cyanis now more likely thanzrm-red, which is the desired effect.The pragmatic listener models the speaker as acooperative agent who behaves in a way to max-imize communicative success.
Certain speakerbehaviors such as avoiding synonyms (e.g., not?delete cardinal?)
and using a consistent word or-dering (e.g, not ?red remove?)
fall out of the gametheory.4For speakers that do not follow this strat-egy, our pragmatic model is incorrect, but as weget more data through game play, the literal lis-tener p?
(z | x) will sharpen, so that the literal lis-tener and the pragmatic listener will coincide inthe limit.
?z, C(z)?
0?z,Q(z)?
repeatreceive utterance x from humanL(z | x) ?P (z)Q(z)p?
(z | x)?send human a list Y ranked by L(z | x)receive y ?
Y from human?
?
?
?
??
?`(?, x, y)Q(z)?
Q(z) + p?
(z | x)?C(z)?
C(z) + p?
(z | x, JzKs= y)P (z)?C(z)+??z?:C(z?)>0(C(z?)+?
)until game endsAlgorithm 1: Online learning algorithm thatupdates the parameters of the semantic parser?
as well as counts C,Q required to performpragmatic reasoning.Online learning with pragmatics.
To imple-ment the pragmatic listener as defined in (5), weneed to compute the speaker?s normalization con-stant?xp?
(z | x)p(x) in order to compute S(x |z) in (4).
This requires parsing all utterances xbased on p?
(z | x).
To avoid this heavy computa-tion in an online setting, we propose Algorithm 1,where some approximations are used for the sakeof efficiency.
First, to approximate the intractablesum over all utterances x, we only use the exam-ples that are seen to compute the normalizationconstant?xp?
(z | x)p(x) ??ip?
(z | xi).Then, in order to avoid parsing all previous exam-ples again using the current parameters for eachnew example, we store Q(z) =?ip?i(z | xi)?,where ?iis the parameter after the model updateson the ithexample xi.
While ?iis different fromthe current parameter ?, p?
(z | xi) ?
p?i(z | xi)for the relevant example xi, which is accounted for4Of course, synonyms and variable word order occur inreal language.
We would need a more complex game com-pared to SHRDLURN to capture this effect.2372by both ?iand ?.In Algorithm 1, the pragmatic listener L(z | x)can be interpreted as an importance-weighted ver-sion of the sharpened literal listener p?
?, where itis downweighted by Q(z), which reflects whichz?s the literal listener prefers, and upweighted byP (z), which is just a smoothed estimate of the ac-tual distribution over logical forms p(z).
By con-struction, Algorithm 1 is the same as (4) exceptthat it uses the normalization constant Q based onstale parameters ?iafter seeing example, and ituses samples to compute the sum over x. Follow-ing (5), we also need p(z), which is estimated byP (z) using add-?
smoothing on the counts C(z).Note that Q(z) and C(z) are updated after themodel parameters are updated for the current ex-ample.Lastly, there is a small complication due to onlyobserving the denotation y and not the logicalform z.
We simply give each consistent logicalform {z | JzKs= y} a pseudocount based onthe model: C(z) ?
C(z) + p?
(z | x, JzKs= y)where p?
(z | x, JzKs= y) ?
exp(?T?
(x, z)) forJzKs= y (0 otherwise).Compared to prior work where the setting isspecifically designed to require pragmatic infer-ence, pragmatics arises naturally in ILLG.
Wethink that this form of pragmatics is the most im-portant during learning, and becomes less impor-tant if we had more data.
Indeed, if we have a lotof data and a small number of possible zs, thenL(z|x) ?
p?
(z|x) as?xp?
(z|x)p(x) ?
p(z)when ?
= 1.5However, for semantic parsing,we would not be in this regime even if we havea large amount of training data.
In particular, weare nowhere near that regime in SHRDLURN, andmost of our utterances / logical forms are seen onlyonce, and the importance of modeling pragmaticsremains.5 Experiments5.1 SettingData.
Using Amazon Mechanical Turk (AMT),we paid 100 workers 3 dollars each to play SHRD-LURN.
In total, we have 10223 utterances alongwith their starting states s. Of these, 8874 ut-terances are labeled with their denotations y; therest are unlabeled, since the player can try any ut-terance without accepting an action.
100 playerscompleted the entire game under identical settings.5Technically, we also need p?to be well-specified.We deliberately chose to start from scratch for ev-ery worker, so that we can study the diversity ofstrategies that different people used in a controlledsetting.Each game consists of 50 blocks tasks dividedinto 5 levels of 10 tasks each, in increasing com-plexity.
Each level aims to reach an end goalgiven a start state.
Each game took on average89 utterances to complete.6It only took 6 hoursto complete these 100 games on AMT and eachgame took around an hour on average according toAMT?s work time tracker (which does not accountfor multi-tasking players).
The players were pro-vided minimal instructions on the game controls.Importantly, we gave no example utterances in or-der to avoid biasing their language use.
Around20 players were confused and told us that the in-structions were not clear and gave us mostly spamutterances.
Fortunately, most players understoodthe setting and some even enjoyed SHRDLURNas reflected by their optional comments:?
That was probably the most fun thing I haveever done on mTurk.?
Wow this was one mind bending games [sic].Metrics.
We use the number of scrolls as a mea-sure of game performance for each player.
Foreach example, the number of scrolls is the positionin the list Y of the action selected by the player.
Itwas possible to complete this version of SHRD-LURN by scrolling (all actions can be found inthe first 125 of Y )?22 of the 100 players failed toteach an actual language, and instead finished thegame mostly by scrolling.
Let us call them spamplayers, who usually typed single letters, randomwords, digits, or random phrases (e.g.
?how areyou?).
Overall, spam players had to scroll a lot:21.6 scrolls per utterance versus only 7.4 for thenon-spam players.5.2 Human strategiesSome example utterances can be found in Table 3.Most of the players used English, but vary in theiradherence to conventions such as use of determin-ers, plurals, and proper word ordering.
5 playersinvented their own language, which are more pre-cise, more consistent than general English.
Oneplayer used Polish, and another used Polish nota-tion (bottom of Table 3).6This number is not 50 because some block tasks needmultiple steps and players are also allowed to explore withoutreaching the goal.2373Most successful players (1st?20th)rem cy pos 1, stack or blk pos 4, remblk pos 2 thru 5, rem blk pos 2 thru 4,stack bn blk pos 1 thru 2, fill bn blk,stack or blk pos 2 thru 6, rem cy blkpos 2 fill rd blk (3.01)remove the brown block, remove allorange blocks, put brown block onorange blocks, put orange blocks onall blocks, put blue block on leftmostblue block in top row (2.78)Remove the center block, Remove thered block, Remove all red blocks,Remove the first orange block, Put abrown block on the first brown block,Add blue block on first blue block(2.72)Average players (21th?50th)reinsert pink, take brown, put in pink,remove two pink from second layer,Add two red to second layer in oddintervals, Add five pink to secondlayer, Remove one blue and onebrown from bottom layer (9.17)remove red, remove 1 red, remove 2 4orange, add 2 red, add 1 2 3 4 blue,emove 1 3 5 orange, add 2 4 orange,add 2 orange, remove 2 3 brown, add1 2 3 4 5 red, remove 2 3 4 5 6,remove 2, add 1 2 3 4 6 red (8.37)move second cube, double red withblue, double first red with red, triplesecond and fourth with orange, addred, remove orange on row two, addblue to column two, add brown onfirst and third (7.18)Least successful players (51th?
)holdleftmost, holdbrown,holdleftmost, blueonblue,brownonblue1, blueonorange,holdblue, holdorange2, blueonred2 ,holdends1, holdrightend, hold2,orangeonorangerightmost (14.15)?add red cubes on center left, centerright, far left and far right?, ?removeblue blocks on row two column two,row two column four?, remove redblocks in center left and center righton second row (12.6)laugh with me, red blocks with oneaqua, aqua red alternate, brown redred orange aqua orange, red brownred brown red brown, space redorange red, second level red space redspace red space (14.32)Spam players (?
85th?100)next, hello happy, how are you, move, gold, build goal blocks, 23,house, gabboli, x, run?xav, d, j, xcv, dulicate goal (21.7)Most interestingusu?n bra?zowe klocki, postawpomara?nczowy klocek na pierwszymklocku, postaw czerwone klocki napomara?nczowych, usu?npomara?nczowe klocki w g?rnymrze?dzierm scat + 1 c, + 1 c, rm sh, + 1 2 4 sh,+ 1 c, - 4 o, rm 1 r, + 1 3 o, full fill c,rm o, full fill sh, - 1 3, full fill sh, rmsh, rm r, + 2 3 r, rm o, + 3 sh, + 2 3sh, rm b, - 1 o, + 2 c,mBROWN,mBLUE,mORANGERED+ORANGE?ORANGE,BROWN+BROWNm1+BROWNm3,ORANGE +BROWN+ORANGE?m1+ ORANGE?m3 +BROWN?
?2 + BROWN?
?4Table 3: Example utterances, along with the average number of scrolls for that player in parentheses.Success is measured by the number of scrolls, where the more successful players need less scrolls.
1)The 20 most successful players tend to use consistent and concise language whose semantics is similarto our logical language.
2) Average players tend to be slightly more verbose and inconsistent (left andright), or significantly different from our logical langauge (middle).
3) Reasons for being unsuccessfulvary.
Left: no tokenization, middle: used a coordinate system and many conjunctions; right: confused inthe beginning, and used a language very different from our logical language.Overall, we find that many players adapt inILLG by becoming more consistent, less verbose,and more precise, even if they used standard En-glish at the beginning.
For example, some playersbecame more consistent over time (e.g.
from us-ing both ?remove?
and ?discard?
to only using ?re-move?).
In terms of verbosity, removing functionwords like determiners as the game progresses isa common adaptation.
In each of the followingexamples from different players, we compare anutterance that appeared early in the game to a sim-ilar utterance that appeared later: ?Remove the redones?
became ?Remove red.?
; ?add brown on top ofred?
became ?add orange on red?
; ?add red blocksto all red blocks?
became ?add red to red?
; ?darkred?
became ?red?
; one player used ?the?
in all ofthe first 20 utterances, and then never used ?the?
inthe last 75 utterances.Players also vary in precision, ranging fromoverspecified (e.g.
?remove the orange cube at theleft?, ?remove red blocks from top row?)
to under-specified or requiring context (e.g.
?change col-ors?, ?add one blue?, ?Build more blocus?, ?Movethe blocks fool?,?Add two red cubes?).
We foundthat some players became more precise over time,as they gain a better understanding of ILLG.Most players use utterances that actually do notmatch our logical language in Table 1, even thesuccessful players.
In particular, numbers are of-ten used.
While some concepts always have thesame effect in our blocks world (e.g.
?first block?means leftmost), most are different.
More con-cretely, of the top 10 players, 7 used numbers ofsome form and only 3 players matched our logicallanguage.
Some players who did not match thelogical language performed quite well neverthe-2374less.
One possible explanation is because the ac-tion required is somewhat constrained by the logi-cal language and some tokens can have unintendedinterpretations.
For example, the computer cancorrectly interpret numerical positional references,as long as the player only refers to the leftmostand rightmost positions.
So if the player says ?remblk pos 4?
and ?rem blk pos 1?, the computer caninterpret ?pos?
as rightmost and interpret thebigram (?pos?, ?1?)
as leftmost.
On the otherhand, players who deviated significantly by de-scribing the desired state declaratively (e.g.
?redorange red?, ?246?)
rather than using actions, ora coordinate system (e.g.
?row two column two?
)performed poorly.
Although players do not haveto match our logical language exactly to performwell, being similar is definitely helpful.Compositionality.
As far as we can tell, allplayers used a compositional language; no one in-vented unrelated words for each action.
Interest-ingly, 3 players did not put spaces between words.Since we assume monomorphemic words sepa-rated by spaces, they had to do a lot of scrollingas a result (e.g., 14.15 with utterances like ?or-angeonorangerightmost?
).5.3 Computer strategiesWe now present quantitative results on howquickly the computer can learn, where our goal isto achieve high accuracy on new utterances as wemake just a single pass over the data.
The num-ber of scrolls used to evaluate player is sensitive tooutliers and not as intuitive as accuracy.
Instead,we consider online accuracy, described as follows.Formally, if a player produced T utterances x(j)and labeled them y(j), thenonline accuracydef=1TT?j=1I[y(j)= Jz(j)Ks(j)],where z(j)= argmaxzp?
(j?1)(z|x(j)) is themodel prediction based on the previous parame-ter ?(j?1).
Note that the online accuracy is de-fined with respect to the player-reported labels,which only corresponds to the actual accuracy ifthe player is precise and honest.
This is not truefor most spam players.Compositionality.
To study the importance ofcompositionality, we consider two baselines.First, consider a non-compositional model (mem-0.0 0.1 0.2 0.3 0.4 0.5 0.6full model accuracy0.00.10.20.30.40.50.6full+pragmatics accuracy(a)0.0 0.1 0.2 0.3 0.4 0.5 0.6half model accuracy0.00.10.20.30.40.50.6half+pragmatics accuracy(b)Figure 2: Pragmatics improve online accuracy.
Inthese plots, each marker is a player.
red o: play-ers who ranked 1?20 in terms of minimizing num-ber of scrolls, green x: players 20?50; blue +:lower than 50 (includes spam players).
Markersizes correspond to player rank, where better play-ers are depicted with larger markers.
2a: onlineaccuracies with and without pragmatics on the fullmodel; 2b: same for the half model.players ranked by # of scrollsMethod top 10 top 20 top 50 all 100memorize 25.4 24.5 22.5 17.6half model 38.7 38.4 36.0 27.0half + prag 43.7 42.7 39.7 29.4full model 48.6 47.8 44.9 33.3full + prag 52.8 49.8 45.8 33.8Table 4: Average online accuracy under vari-ous settings.
memorize: featurize entire utter-ance and logical form non-compositionally; halfmodel: featurize the utterances with unigrams, bi-grams, and skip-grams but conjoin with the entirelogical form; full model: the model described inSection 3; +prag: the models above, with our on-line pragmatics algorithm described in Section 4.Both compositionality and pragmatics improve ac-curacy.orize) that just remembers pairs of complete ut-terance and logical forms.
We implement thisusing indicator features on features (x, z), e.g.,(?remove all the red blocks?, zrm-red), and use alarge learning rate.
Second, we consider amodel (half ) that treats utterances composition-ally with unigrams, bigrams, and skip-trigramsfeatures, but the logical forms are regarded asnon-compositional, so we have features such as(?remove?, zrm-red), (?red?, zrm-red), etc.Table 4 shows that the full model (Section 3)significantly outperforms both the memorize andhalf baselines.
The learning rate ?
= 0.1 is se-lected via cross validation, and we used ?
= 1and ?
= 3 following Smith et al (2013).2375Pragmatics.
Next, we study the effect of prag-matics on online accuracy.
Figure 2 showsthat modeling pragmatics helps successful players(e.g., top 10 by number of scrolls) who use preciseand consistent languages.
Interestingly, our prag-matics model did not help and can even hurt theless successful players who are less precise andconsistent.
This is expected behavior: the prag-matics model assumes that the human is coopera-tive and behaving rationally.
For the bottom halfof the players, this assumption is not true, in whichcase the pragmatics model is not useful.6 Related Work and DiscussionOur work connects with a broad body of work ongrounded language, in which language is used insome environment as a means towards some goal.Examples include playing games (Branavan et al,2009, 2010; Reckman et al, 2010) interacting withrobotics (Tellex et al, 2011, 2014), and followinginstructions (Vogel and Jurafsky, 2010; Chen andMooney, 2011; Artzi and Zettlemoyer, 2013) Se-mantic parsing utterances to logical forms, whichwe leverage, plays an important role in these set-tings (Kollar et al, 2010; Matuszek et al, 2012;Artzi and Zettlemoyer, 2013).What makes this work unique is our new inter-active learning of language games (ILLG) setting,in which a model has to learn a language fromscratch through interaction.
While online gradientdescent is frequently used, for example in seman-tic parsing (Zettlemoyer and Collins, 2007; Chen,2012), we using it in a truly online setting, takingone pass over the data and measuring online accu-racy (Cesa-Bianchi and Lugosi, 2006).To speed up learning, we leverage computa-tional models of pragmatics (J?ger, 2008; Gollandet al, 2010; Frank and Goodman, 2012; Smithet al, 2013; Vogel et al, 2013).
The main differ-ence is these previous works use pragmatics witha trained base model, whereas we learn the modelonline.
Monroe and Potts (2015) uses learningto improve the pragmatics model.
In contrast,we use pragmatics to speed up the learning pro-cess by capturing phenomena like mutual exclu-sivity (Markman and Wachtel, 1988).
We also dif-fer from prior work in several details.
First, wemodel pragmatics in the online learning settingwhere we use an online update for the pragmat-ics model.
Second, unlikely the reference gameswhere pragmatic effects plays an important role bydesign, SHRDLURN is not specifically designedto require pragmatics.
The improvement we getis mainly due to players trying to be consistent intheir language use.
Finaly, we treat both the utter-ance and the logical forms as featurized composi-tional objects.
Smith et al (2013) treats utterances(i.e.
words) and logical forms (i.e.
objects) as cat-egories; Monroe and Potts (2015) used features,but also over flat categories.Looking forward, we believe that the ILLG set-ting is worth studying and has important implica-tions for natural language interfaces.
Today, thesesystems are trained once and deployed.
If thesesystems could quickly adapt to user feedback inreal-time as in this work, then we might be ableto more readily create systems for resource-poorlanguages and new domains, that are customizableand improve through use.AcknowledgmentsDARPA Communicating with Computers (CwC)program under ARO prime contract no.
W911NF-15-1-0462.
The first author is supported by aNSERC PGS-D fellowship.
In addition, we thankWill Monroe, and Chris Potts for their insightfulcomments and discussions on pragmatics.ReproducibilityAll code, data, and experiments for this paper areavailable on the CodaLab platform:https://worksheets.codalab.org/worksheets/0x9fe4d080bac944e9a6bd58478cb05e5eThe client side code is here:https://github.com/sidaw/shrdlurn/tree/acl16-demoand a demo: http://shrdlurn.sidaw.xyzReferencesY.
Artzi and L. Zettlemoyer.
2013.
Weakly super-vised learning of semantic parsers for mappinginstructions to actions.
Transactions of the As-sociation for Computational Linguistics (TACL)1:49?62.J.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on Freebase from question-answer pairs.
In Empirical Methods in NaturalLanguage Processing (EMNLP).S.
Branavan, H. Chen, L. S. Zettlemoyer, andR.
Barzilay.
2009.
Reinforcement learning for2376mapping instructions to actions.
In Associa-tion for Computational Linguistics and Inter-national Joint Conference on Natural LanguageProcessing (ACL-IJCNLP).
pages 82?90.S.
Branavan, L. Zettlemoyer, and R. Barzilay.2010.
Reading between the lines: Learningto map high-level instructions to commands.In Association for Computational Linguistics(ACL).
pages 1268?1277.N.
Cesa-Bianchi and G. Lugosi.
2006.
Predic-tion, learning, and games.
Cambridge Univer-sity Press.D.
L. Chen.
2012.
Fast online lexicon learning forgrounded language acquisition.
In Associationfor Computational Linguistics (ACL).D.
L. Chen and R. J. Mooney.
2011.
Learningto interpret natural language navigation instruc-tions from observations.
In Association for theAdvancement of Artificial Intelligence (AAAI).pages 859?865.J.
Duchi, E. Hazan, and Y.
Singer.
2010.
Adap-tive subgradient methods for online learningand stochastic optimization.
In Conference onLearning Theory (COLT).M.
Frank and N. D. Goodman.
2012.
Predictingpragmatic reasoning in language games.
Sci-ence 336:998?998.H.
Giles.
2008.
Communication accommodationtheory.
Sage Publications, Inc.D.
Golland, P. Liang, and D. Klein.
2010.
A game-theoretic approach to generating spatial descrip-tions.
In Empirical Methods in Natural Lan-guage Processing (EMNLP).N.
Goodman and D. Lassiter.
2015.
ProbabilisticSemantics and Pragmatics: Uncertainty in Lan-guage and Thought.
The Handbook of Contem-porary Semantic Theory, 2nd Edition Wiley-Blackwell.H.
P. Grice.
1975.
Logic and conversation.
Syntaxand semantics 3:41?58.M.
E. Ireland, R. B. Slatcher, P. W. Eastwick, L. E.Scissors, E. J. Finkel, and J. W. Pennebaker.2011.
Language style matching predicts rela-tionship initiation and stability.
PsychologicalScience 22(1):39?44.G.
J?ger.
2008.
Game theory in semantics andpragmatics.
Technical report, University ofT?bingen.T.
Kollar, S. Tellex, D. Roy, and N. Roy.
2010.Grounding verbs of motion in natural languagecommands to robots.
In International Sympo-sium on Experimental Robotics (ISER).T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater,and M. Steedman.
2010.
Inducing probabilisticCCG grammars from logical form with higher-order unification.
In Empirical Methods inNatural Language Processing (EMNLP).
pages1223?1233.P.
Liang, M. I. Jordan, and D. Klein.
2011.Learning dependency-based compositional se-mantics.
In Association for Computational Lin-guistics (ACL).
pages 590?599.E.
Markman and G. F. Wachtel.
1988.
Children?suse of mutual exclusivity to constrain the mean-ings of words.
Cognitive Psychology 20:125?157.C.
Matuszek, N. FitzGerald, L. Zettlemoyer,L.
Bo, and D. Fox.
2012.
A joint model oflanguage and perception for grounded attributelearning.
In International Conference on Ma-chine Learning (ICML).
pages 1671?1678.W.
Monroe and C. Potts.
2015.
Learning in theRational Speech Acts model.
In Proceedings of20th Amsterdam Colloquium.P.
Pasupat and P. Liang.
2015.
Compositional se-mantic parsing on semi-structured tables.
In As-sociation for Computational Linguistics (ACL).H.
Reckman, J. Orkin, and D. Roy.
2010.
Learningmeanings of words and constructions, groundedin a virtual game.
In Conference on NaturalLanguage Processing (KONVENS).N.
J. Smith, N. D. Goodman, and M. C. Frank.2013.
Learning and using language via re-cursive pragmatic reasoning about other agents.In Advances in Neural Information ProcessingSystems (NIPS).S.
Tellex, R. Knepper, A. Li, D. Rus, and N. Roy.2014.
Asking for help using inverse semantics.In Robotics: Science and Systems (RSS).S.
Tellex, T. Kollar, S. Dickerson, M. R. Walter,A.
G. Banerjee, S. J. Teller, and N. Roy.
2011.Understanding natural language commands forrobotic navigation and mobile manipulation.
InAssociation for the Advancement of ArtificialIntelligence (AAAI).A.
Vogel, M. Bodoia, C. Potts, and D. Juraf-sky.
2013.
Emergence of gricean maxims from2377multi-agent decision theory.
In North Ameri-can Association for Computational Linguistics(NAACL).
pages 1072?1081.A.
Vogel and D. Jurafsky.
2010.
Learning to fol-low navigational directions.
In Association forComputational Linguistics (ACL).
pages 806?814.T.
Winograd.
1972.
Understanding Natural Lan-guage.
Academic Press.L.
Wittgenstein.
1953.
Philosophical Investiga-tions.
Blackwell, Oxford.Y.
W. Wong and R. J. Mooney.
2007.
Learn-ing synchronous grammars for semantic parsingwith lambda calculus.
In Association for Com-putational Linguistics (ACL).
pages 960?967.L.
S. Zettlemoyer and M. Collins.
2005.
Learn-ing to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Uncertainty in Artificial Intelli-gence (UAI).
pages 658?666.L.
S. Zettlemoyer and M. Collins.
2007.
Onlinelearning of relaxed CCG grammars for parsingto logical form.
In Empirical Methods in Nat-ural Language Processing and ComputationalNatural Language Learning (EMNLP/CoNLL).pages 678?687.2378
