Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 45?53,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUnsupervised Linguistically-Driven Reliable Dependency ParsesDetection and Self-Training for Adaptation to the Biomedical DomainFelice Dell?Orletta, Giulia Venturi, Simonetta MontemagniIstituto di Linguistica Computazionale ?Antonio Zampolli?
(ILC?CNR)Via G. Moruzzi, 1 ?
Pisa (Italy){felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.itAbstractIn this paper, a new self?training methodfor domain adaptation is illustrated, wherethe selection of reliable parses is car-ried out by an unsupervised linguistically?driven algorithm, ULISSE.
The methodhas been tested on biomedical texts withresults showing a significant improve-ment with respect to considered baselines,which demonstrates its ability to captureboth reliability of parses and domain?specificity of linguistic constructions.1 IntroductionAs firstly demonstrated by (Gildea, 2001), pars-ing systems have a drop of accuracy when testedagainst domain corpora outside of the data fromwhich they were trained.
This is a real prob-lem in the biomedical domain where, due to therapidly expanding body of biomedical literature,the need for increasingly sophisticated and effi-cient biomedical text mining systems is becom-ing more and more pressing.
In particular, the ex-istence of natural language parsers reliably deal-ing with biomedical texts represents the prerequi-ste for identifying and extracting knowledge em-bedded in them.
Over the last years, this prob-lem has been tackled within the biomedical NLPcommunity from different perspectives.
The de-velopment of a domain?specific annotated corpus,i.e.
the Genia Treebank (Tateisi, Yakushiji, Ohta,& Tsujii, 2005), played a key role by providing asound basis for empirical performance evaluationas well as training of parsers.
On the other hand,several attempts have been made to adapt generalparsers to the biomedical domain.
First experi-ments in this direction are reported in (Clegg &Shepherd, 2005) who first compared the perfor-mance of three different parsers against the Ge-nia treebank and a sample of the Penn Treebank(PTB) (Mitchell P. Marcus & Santorini, 1993) inorder to carry out an inter?domain analysis ofthe typology of errors made by each parser anddemonstrated that by integrating the output of thethree parsers they achieved statistically significantperformance gains.
Three different methods ofparser adaptation for the biomedical domain havebeen proposed by (Lease & Charniak, 2005) who,starting from the results of unknown word rateexperiments carried out on the Genia treebank,adapted a PTB?trained parser by improving thePart?Of?Speech tagging accuracy and by relyingon an external domain?specific lexicon.
More re-cently, (McClosky, Charniak, & Johnson, 2010)and (Plank & van Noord, 2011) devised adaptationmethods based on domain similarity measures.
Inparticular, both of them adopted lexical similar-ity measures to automatically select from an anno-tated collection of texts those training data whichis more relevant, i.e.
lexically closer, to adapt theparser to the target domain.A variety of semi?supervised approaches,where unlabeled data is used in addition to labeledtraining data, have been recently proposed in theliterature in order to adapt parsing systems to newdomains.
Among these approaches, the last fewyears have seen a growing interest in self?trainingfor domain adaptation, i.e.
a method for using au-tomatically annotated data from a target domainwhen training supervised models.
Self?trainingmethods proposed so far mainly differ at the levelof the selection of parse trees to be added to thein?domain gold trees as further training data.
De-pending on whether or not external supervisedclassifiers are used to select the parses to be addedto the gold?training set, two types of methods areenvisaged in the literature.
The first is the case,among others, of: (Kawahara & Uchimoto, 2008),using a machine learning classifier to predict thereliability of parses on the basis of different fea-ture types; or (Sagae & Tsujii, 2007), selecting45identical analyses for the same sentence within theoutput of different parsing models trained on thesame dataset; or (McClosky, Charniak, & John-son, 2006), using a discriminative reranker againstthe output of a n?best generative parser for select-ing the best parse for each sentence to be usedas further training data.
Yet, due to the fact thatseveral supervised classifiers are resorted to forimproving the base supervised parser, this classof methods cannot be seen as a genuine istanceof self?training.
The second type of methods isexemplified, among others, by (Reichart & Rap-poport, 2007) who use the whole set of automat-ically analyzed sentences, and by (McClosky &Charniak, 2008) and (Sagae, 2010) who add dif-ferent amounts of automatically parsed data with-out any selection strategy.
Note that (McClosky& Charniak, 2008) tested their self?training ap-proach on the Genia Treebank: they self?traineda PTB?trained costituency parser using a randomselection of Medline abstracts.In this paper, we address the second scenariowith a main novelty: we use an unsupervised ap-proach to select reliable parses from automaticallyparsed target domain texts to be combined with thegold?training set.
Two unsupervised algorithmshave been proposed so far in the literature forselecting reliable parses, namely: PUPA (POS?based Unsupervised Parse Assessment Algorithm)(Reichart & Rappoport, 2009) and ULISSE (Un-supervised LInguiStically?driven Selection of dE-pendency parses) (Dell?Orletta, Venturi, & Mon-temagni, 2011).
Both algorithms assign a qual-ity score to each parse tree based on statisticscollected from a large automatically parsed cor-pus, with a main difference: whereas PUPA oper-ates on costituency trees and uses statistics aboutsequences of part?of?speech tags, ULISSE usesstatistics about linguistic features checked againstdependency?based representations.
The self?training strategy presented in this paper is basedon an augmented version of ULISSE.
The reasonsfor this choice are twofold: if on the one handULISSE appears to outperform PUPA (namely, adependency?based version of PUPA implementedin (Dell?Orletta et al 2011)), on the other hand thelinguistically?driven nature of ULISSE makes ourself?training strategy for domain adaptation ableto capture reliable parses which are also represen-tative of the syntactic peculiarities of the target do-main.After introducing the in?
and out?domain cor-pora used in this study (Section 2), we discussthe results of the multi?level linguistic analysis ofthese corpora carried out (Section 3) with a viewto identifying the main features differentiating thebiomedical language from ordinary language.
InSection 4, the algorithm used to select reliableparses from automatically parsed domain?specifictexts is described.
In Section 5 the proposed self?training method is illustrated, followed by a dis-cussion of achieved results (Section 6).2 CorporaUsed domain corpora include i) the two out?domain datasets used for the ?Domain AdaptationTrack?
of the CoNLL 2007 Shared Task (Nivreet al 2007) and ii) the dependency?based versionof the Genia Treebank (Tateisi et al 2005).
TheCoNLL 2007 datasets are represented by chemical(CHEM) and biomedical abstracts (BIO), made of5,001 tokens (195 sentences) and of 5,017 tokens(200 sentences) respectively.
The dependency?based version of Genia includes?493k tokens and?18k sentences which was generated by convert-ing the PTB version of Genia created by Illes Solt1using the (Johansson & Nugues, 2007) tool withthe -conll2007 option to produce annotations inline with the CoNLL 2007 data set2.
As unla-belled data, we used the datasets distributed in theframework of the CoNLL 2007 Domain Adapta-tion Track.
For CHEM the set of unlabelled dataconsists of 10,482,247 tokens (396,128 sentences)and for BIO of 9,776,890 tokens (375,421 sen-tences).
For the experiments using Genia as testset, we used the BIO unlabelled data.
This waspossible due to the fact that both the Genia Tree-bank and the BIO dataset consist of biomedicalabstracts extracted (though using different queryterms) from PubMed.com.As in?domain training data we have used theCoNLL 2007 dependency?based version of Sec-tions 2?11 of the Wall Street Journal (WSJ) par-tition of the Penn Treebank (PTB), for a total of447,000 tokens and about 18,600 sentences.
Fortesting, we used the subset of Section 23 of WSJconsisting of 5,003 tokens (214 sentences).All corpora have been morpho?syntacticallytagged and lemmatized by a customized version1http://categorizer.tmit.bme.hu/?illes/genia ptb/2In order to be fully compliant with the PTB PoS tagset,we changed the PoS label of all punctuation marks.46of the pos?tagger described in (Dell?Orletta, n.d.)and dependency parsed by the DeSR parser us-ing Multi?Layer Perceptron (MLP) as learning al-gorithm (Attardi, Dell?Orletta, Simi, & Turian,n.d.
), a state?of?the?art linear?time Shift?Reducedependency parser following a ?stepwise?
ap-proach (Buchholz & Marsi, 2006).3 Linguistic analysis of biomedicalabstrats vs newspaper articlesFor the specific concerns of this study, we carriedout a comparative linguistic analysis of four dif-ferent corpora, taken as representative of ordinarylanguage and biomedical language.
In each case,we took into account a gold (i.e.
manually an-notated) corpus, and an unlabelled corpus, whichwas automatically annotated.
By comparing theresults obtained with respect to gold and automat-ically annotated texts, we intend to demonstratethe reliability of features extracted from automat-ically annotated texts.
As data representative ofordinary language we took i) the whole WSJ sec-tion of the Penn Treebank3 including 39,285,425tokens (1,625,606 sentences) and ii) the sections2?11 of the WSJ.
For what concerns the biomed-ical domain, we relied on the Genia Treebank inorder to guarantee comparability of the results ofour linguistic analysis with previous studies car-ried out on this reference corpus.
As automaticallyannotated data we used the corpus of biomedicalabstract (BIO) distributed as out?domain datasetused for the ?Domain Adaptation Track?
of theCoNLL 2007 Shared Task.In order to get evidence of the differences hold-ing between the WSJ newspaper articles and theselected biomedical abstracts, the four corporahave been compared with respect to a wide typol-ogy of features (i.e.
raw text, lexical, morpho?syntactic and syntactic).
Let us start from rawtext features, in particular from average sen-tence length (calculated as the average numberof words per sentence): as Figure 1 shows, boththe corpus of automatically parsed newspaper ar-ticles (WSJ unlab) and the manually annotatedone (WSJ gold) contain shorter sentences with re-spect to both the automatically parsed biomedi-cal abstrats (BIO unlab) and the manually anno-tated ones (Genia gold), a result which is in line3This corpus represents to the unlabelled data set dis-tributed for the CoNLL 2007 Shared Task on DependencyParsing, domain adaptation track.Figure 1: Average sentence length in biomedicaland newspaper corpora.with (Clegg & Shepherd, 2005) findings.
Whenwe focus on the lexical level, BIO unlab andGenia gold appear to have quite a similar per-centage of lexical items which is not containedin WSJ gold (23.13% and 26.14% respectively)while the out?of?vocabulary rate of WSJ unlabis much lower, i.e.
8.69%.
Similar results wererecorded by (Lease & Charniak, 2005) who reportthe unknown word rate for various genres of tec-nical literature.Let us focus now on the morpho?syntactic level.If we consider the distribution of nouns, verbsand adjectives, three features typically represent-ing stylistic markers associated with different lin-guistic varieties (Biber & Conrad, 2009), it canbe noticed (see Figures 2(a) and 2(c)) that thebiomedical abstracts contain a higher percentageof nouns and adjectives while showing a signif-icantly lower percentage of verbs (Figure 2(b)).The syntactic counterpart of the different distri-bution of morpho?syntactic categories can be ob-served in Table 1, reporting the percentage distri-bution of the first ten Parts?of?Speech dependencytriplets occurring in the biomedical and newspapercorpora: each triplet is described as the sequenceof the PoS of a dependent and a head linked by adepedency arc, by also considering the PoS of thehead father.
It turned out that biomedical abstractsare characterized by nominal dependency triplets,e.g.
two nouns and a preposition (NN?NN?IN)or noun, preposition, noun (NN?IN?NN) or ad-jective, noun and preposition (JJ?NN?IN), whichoccur more frequently than verbal triplets, suchas determiner, noun and verb (DT?NN?VBZ) or averbal root (.?VBD?ROOT)4.
Interestingly, in Ge-nia gold no verbal triplet occurs within the top tentriplets, which cover the 21% of the total amount4We named ?ROOT?
the artificial root node.47(a) Distribution of Nouns (b) Distribution of Verbs (c) Distribution of AdjectivesFigure 2: Distribution of some Parts?of?Speech in biomedical and newspaper corpora.of triplets occurring in the corpus.
By contrast,the same top ten triplets represent only ?11% inWSJ gold, testifying the wider variety of syntac-tic constructions occurring in newspaper articleswith respect to texts of the biomedical domain.This is also proved by the total amount of differ-ent PoS dependency triplets occurring in the twogold datasets, i.e.
7,827 in WSJ gold and 5,064in Genia gold even though the Genia Treebank is?50,000?tokens bigger.Further differences can be observed at a deepersyntactic level of analysis.
This is the case of theaverage depth of embedded complement ?chains?governed by a nominal head.
Figure 3(a) showsthat biomedical abstracts are characterized by anaverage depth which is higher than the one ob-served in newspaper articles.
A similar trendcan be observed for what concerns the distribu-tion of ?chains?
by depth.
In Figure 3(b) showsthat WSJ unlab and WSJ gold ?chains?, on the onehand, and BIO unlab and Genia gold ?chains?,on the other hand, overlap.
The corpora havealso been compared with respect to i) the av-erage length of dependency links, measured interms of the words occurring between the syntac-tic head and the dependent (excluding punctua-tion marks), and ii) the average depth of the wholeparse tree, calculated in terms of the longest pathfrom the root of the dependency tree to a leaf.
Inboth cases it can be noted that i) the biomedicalabstracts contain much longer dependency linksthan newswire texts (Figure 3(c)) and ii) the av-erage depth of BIO unlab and Genia gold parsetrees is higher than in the case of the WSJ unlaband WSJ gold (Figure 3(d)).
A further distin-guishing feature of the biomedical abstracts con-cerns the average depth of ?chains?
of embed-ded subordinate clauses, calculated here by takinginto account both clausal arguments and comple-ments linked to a verbal sentence root.
As Figure3(e) shows, both BIO unlab and Genia gold haveshorter ?chains?
with respect to the ones containedin the newspaper articles.
Interestingly, a carefulanalysis of the distributions by depth of ?chains?of embedded subordinate clauses shows that thebiomedical abstracts appear to have i) a higher oc-currence of ?chains?
including just one subordinateclause and ii) a lower percentage of deep ?chains?with respect to newswire texts.
Finally, we com-pared the two types of corpora with respect to thedistribution of verbal roots.
The biomedical ab-stracts resulted to be characterised by a lower per-centage of verbal roots with respect to newspaperarticles (see Figure 3(f)).
This is in line with thedistribution of verbs as well as of the nominal de-pendency triplets observed in the biomedical ab-stracts at the morpho?syntactic level of analysis.Interestingly, the results obtained with respectto automatically parsed and manually annotateddata show similar trends for both considered in?and out?domain corpora, thus demonstrating thereliability of features monitored against automat-ically annotated data.
In what follows, we willshow how detected linguistic peculiarities can beexploited in a domain adaptation scenario.4 Linguistically?driven UnsupervisedRanking of Parses for Self?trainingIn the self?training approach illustrated in this pa-per, the selection of parses from the automaticallyannotated target domain dataset is guided by anaugmented version of ULISSE, an unsupervisedlinguistically?driven algorithm to select reliableparses from the output of dependency annotatedtexts (Dell?Orletta et al 2011) which has showna good performance for two different languages48WSJ gold WSJ unlab Genia gold BIO unlabTriplet % Freq Triplet % Freq Triplet % Freq Triplet % FreqDT-NN-IN 2.03 DT-NN-IN 1.72 NN-NN-IN 3.66 DT-NN-IN 2.87.-VBD-ROOT 1.61 .-VBD-ROOT 1.30 NN-IN-NN 2.93 NN-IN-NN 2.39NN-IN-NN 1.11 JJ-NN-IN 0.99 DT-NN-IN 2.48 JJ-NN-IN 2.08JJ-NN-IN 1.10 NN-IN-NN 0.97 JJ-NN-IN 1.96 NN-NN-IN 1.73.-VBZ-ROOT 1.09 NNP-NNP-IN 0.87 NN-NNS-IN 1.88 IN-NN-IN 1.72NNP-NNP-IN 0.95 DT-NN-VBD 0.85 JJ-NNS-IN 1.77 JJ-NNS-IN 1.36DT-NN-VBZ 0.89 NN-VBD-ROOT 0.80 IN-NN-IN 1.65 .-VBD-ROOT 1.33DT-NN-VBD 0.87 JJ-NNS-IN 0.79 NN-CC-IN 1.64 NNS-IN-NN 1.13JJ-NNS-IN 0.87 NNP-NNP-VBD 0.78 NNS-IN-NN 1.56 NNP-NN-IN 1.03IN-NN-IN 0.87 .-VBZ-ROOT 0.75 NN-NN-CC 1.47 NN-IN-VBN 0.93Table 1: Frequency distribution of the first ten Parts?of?Speech dependency triplets in biomedical andnewspaper corpora.
(a) Depth of embedded complement?chains?
(b) Distribution of embedded com-plement ?chains?
by depth(c) Length of dependency links(d) Parse tree depth (e) Depth of embedded subordinateclauses ?chains?
(f) Distribution of verbal rootsFigure 3: Syntactic features in biomedical and newspaper corpora.
(English and Italian) against the output of two su-pervised parsers (MST, (McDonald, Lerman, &Pereira, 2006) and DeSR, (Attardi, 2006)) se-lected for their behavioral differences (McDonald& Nivre, 2007).
ULISSE assigns to each depen-dency tree a score quantifying its reliability basedon a wide range of linguistic features.
After col-lecting statistics about selected features from acorpus of automatically parsed sentences, for eachnewly parsed sentence ULISSE computes a relia-bility score using the previously extracted featurestatistics.
In its reliability assessment, ULISSE ex-ploits both global and local features, where globalfeatures (listed in Table 2 and discussed in Sec-tion 3) are computed with respect to each sen-tence and averaged over all sentences in the cor-pus, and the local features with respect to indi-vidual dependency links and averaged over all ofthem.
Local features include the plausibility ofa dependency link calculated by considering se-lected features of the dependent and its govern-ing head as well as of the head father: whereasin ULISSE the selected features were circum-scribed to part?of?speech information (so?called?ArcPOSFeat?
feature), in this version of the al-gorithm a new local feature has been introduced,named ?ArcLemmaFeat?, which exploits lemmainformation.
?ArcPOSFeat?
is able to capture thedifferent distribution of PoS dependency triplets(see Table 1), along with the type of dependencylink, while the newly introduced ?ArcLemmaFeat?is meant to capture the lexical peculiarities of thetarget domain (see Section 3).
As demonstratedin (Dell?Orletta et al 2011), both global and lo-49cal linguistic features contribute to the selectionof reliable parses.
Due to the typology of linguis-tic features underlying ULISSE, selected reliableparses typically include domain?specific construc-tions.
This peculiarity of the ULISSE algorithmturned out to be particularly useful to maximizethe self?training effect in improving the parsingperformance in a domain adaptation scenario.The reliability score assigned by this augmentedversion of ULISSE to newly parsed sentences re-sults from a combination of the weights associ-ated with individual features, both global and localones.
In this study, the reliability score was com-puted as a simple product of the individual featureweights: in this way, one low weight feature is suf-ficient to qualify a parse as low quality and thus toexclude it from the self?training dataset5.FeatureParse tree depthEmbedded complement ?chains?
headed by a noun- Average depth- Distribution by depthVerbal rootsArity of verbal predicates- Distribution by aritySubordinate vs main clauses- Relative ordering of subordinate clauses with respect to the main clause- Average depth of ?chains?
of embedded subordinate clauses- Distribution of embedded subordinate clauses ?chains?
by depthLength of dependency linksTable 2: Global features underlying ULISSE.5 Experimental set?upIn the reported experiments, we used the DeSRparser.
Its performance using the proposed domainadaptation strategy was tested against i) the twoout?domain datasets distributed for the ?DomainAdaptation Track?
of the CoNLL 2007 SharedTask and ii) the dependency?based version of theGenia Treebank, described in Section 2.
For test-ing purposes, we selected from the dependency?based version of the Genia Treebank sentenceswith a maximum length of 39 tokens (for a totalof 375,912 tokens and 15,623 sentences).Results achieved with respect to the CHEM andBIO test sets were evaluated in terms of ?LabelledAttachment Score?
(LAS), whereas for Genia theonly possible evaluation was in terms of ?Un-labelled Attachment Score?
(UAS).
This followsfrom the fact that, as reported by Illes, this ver-sion of Genia is annotated with a Penn Treebank?style phrase?structure, where a number of func-tional tags are missing: this influences the type5See (Dell?Orletta et al 2011) for a detailed descriptionof the quality score computation.Test corpus LAS UASPTB 86.09% 87.29%CHEM 78.50% 81.10%BIO 78.65% 79.97%GENIA n/a 80.25%Table 3: The BASE model tested on PTB, CHEM,BIO and GENIA.of evaluation which can be carried out against theGenia test set.Achieved results were compared with two base-lines, represented by: i) the Baseline model(BASE), i.e.
the parsing model trained on the PTBtraining set only; ii) the Random Selection (RS) ofparses from automatically parsed out?domain cor-pora, calculated as the mean of a 10?fold cross?validation process.
As proved by (Sagae, 2010)and by (McClosky & Charniak, 2008) for thebiomedical domain, the latter represents a strongunsupervised baseline showing a significant accu-racy improvement which was obtained by addingincremental amounts of automatically parsed out?domain data to the training dataset without any se-lection strategy.The experiments we carried out to test the ef-fectiveness of our self?training strategy were or-ganised as follows.
ULISSE and the baseline al-gorithms were used to produce different rankingsof parses of the unlabelled target domain corpora.From the top of these rankings different pools ofparses were selected to be used for training.
Inparticular, two different sets of experiments werecarried out, namely: i) using only automaticallyparsed data as training corpus and ii) combiningautomatically parsed data with the PTB trainingset.
For each set of experiments, different amountsof unlabelled data were used to create the self?training models.6 ResultsTable 3 reports the results of the BASE modeltested on PTB, CHEM, BIO and GENIA.
Whenapplied without adaptation to the out?domainCHEM, BIO and GENIA test sets, the BASE pars-ing model has a drop of about 7.5% of LAS in bothCHEM and BIO cases.
For what concerns UAS,the drop is about 6% for CHEM and about 7% forBIO and GENIA.The results of the performed experiments areshown in Figures 4 and 5, where each plot re-ports the accuracy scores (LAS and UAS respec-tively) of the self?trained parser using the ULISSE50(a) LAS for CHEM without PTB training set.
(b) LAS for BIO without the PTB training set.
(c) LAS for CHEM with the PTB training set.
(d) LAS for BIO with the PTB training set.Figure 4: LAS of the different self?training models in the two sets of performed experiments.
(a) UAS for GENIA without the PTB training set.
(b) UAS for GENIA with the PTB training set.Figure 5: UAS of the different self?training models for GENIA.algorithm (henceforth, ULISSE?Stp) and of thebaseline models (RS and BASE).
The parser ac-curacy was computed with respect to differentamounts of automatically parsed data which wereused to create the self?trained parsing model.
Forthis purpose, we considered six different pools of250k, 450k, 900k, 1000k, 1350k and 1500k to-kens.
Plots are organized by experiment type: i.e.the results in subfigures 4(a), 4(b) and 5(a) areachieved by using only automatically parsed dataas training corpus, whereas those reported in theother subfigures refer to models trained on auto-matically parsed data combined with PTB.
Notethat in all figures the line named best?RS repre-sents the best RS score for each pool of k tokens inthe 10?fold cross?validation process.For what concerns BIO and CHEM, in the firstset of experiments ULISSE?Stp turned out to bethe best self?training algorithm: this is always thecase for CHEM (see subfigure 4(a)), whereas forBIO (see subfigure 4(b)) it outperforms all base-lines only when pools of tokens >= 900k areadded.
When 900k tokens are used, ULISSE?Stpallows a LAS improvement of 0.81% on CHEMand of 0.61% on BIO with respect to RS, andof 0.62% on CHEM and of 0.48% on BIO withrespect to BASE.
It is interesting to note thatULISSE?Stp using only automatically parsed datafor training achieves better results than BASE (us-ing the PTB gold training set): to our knowledge,a similar result has never been reported in the lit-erature.
The behaviour is similar also when the51experiments are evaluated in terms of UAS6.The results achieved in the first set of experi-ments carried out on the GENIA test set (see 5(a))differ significantly from what we observed forCHEM and BIO: in this case, the BASE model ap-pears to outperform all the other algorithms, withthe ULISSE?Stp algorithm being however moreeffective than the RS baselines.Figures 4(c), 4(d) and 5(b) report the results ofthe second set of experiments, i.e.
those carriedout by also including PTB in the training set.
Notethat in these plots the RS+PTB lines represent thescore of the parser models trained on the poolsof tokens used to obtain the best?RS line com-bined with the PTB gold training set.
It can beobserved that the ULISSE?Stp+PTB self?trainingmodel outperforms all baselines for CHEM, BIOand GENIA for all the different sizes of pools ofselected tokens.
For each pool of parsed data, Ta-ble 4 records the improvement and the error reduc-tion observed with respect to the BASE model.Pool of tokens CHEM Err.
red.
BIO Err.
red.
GENIA Err.
red.250k 0.8 3.72 0.76 3.55 0.97 4.91450k 1.1 5.12 0.54 2.53 1.52 7.7900k 1.14 5.3 1.02 4.77 1.31 6.631000k 0.8 3.72 1.56 7.29 1.2 6.081350k 0.4 1.49 1.46 6.82 0.94 4.761500k 0.78 3.62 0.75 3.37 0.66 3.34Table 4: % improvement of ULISSE?Stp+PTB vsBASE reported in terms of LAS for CHEM andBIO and of UAS for GENIA.Differently from (Sagae, 2010) (with aconstituency?based parser), in this set of experi-ments the self?training approach based on randomselection of sentences (i.e.
the best?RS+PTBbaseline) doesn?t achieve any improvement withrespect to the BASE model with only minorexceptions (observed e.g.
with 250k and 450kpools of added tokens for CHEM and with 250kfor GENIA).
Moreover, even when the best?RSLAS is higher than the ULISSE?Stp score (e.g.in the first pools of k of Figure 4(b)), ULISSE?Stp+PTB turns out to be more effective thanthe best?RS+PTB baseline (Figure 4(d)).
Theseresults may follow from the fact that ULISSE?Stpis able to capture not only reliable parses but also,and more significantly here, parses which reflectthe syntactic peculiarities of the target domain.Table 5 shows the results of the differentULISSE?Stp+PTB models tested on the PTB test6In this paper, for CHEM and BIO experiments we reportonly the LAS scores since this is the standard evaluation met-ric for dependency parsing.set: no LAS improvement is observed with respectto the results obtained with the BASE model, i.e.86.09% (see Table 3).
This result is in line with(McClosky et al 2010) and (Plank & van No-ord, 2011) who proved that parsers trained on theunion of gold corpora belonging to different do-mains achieve a lower accuracy with respect to thesame parsers trained on data belonging to a sin-gle target domain.
Last but not least, it should benoted that the performance of ULISSE?Stp acrossthe experiments carried out with pools of automat-ically parsed tokens of different sizes is in linewith the behaviour of the ULISSE ranking algo-rithm (Dell?Orletta et al 2011), where increas-ingly wider top lists of parsed tokens show de-creasing LAS scores.
This helps explaining whythe performance of ULISSE?Stp starts decreasingafter a certain threshold when wider top?lists oftokens are added to the parser training data.Pool of tokens CHEM BIO250k 83.53 85.55450k 85.53 86.01900k 85.95 84.791000k 86.03 85.451350k 85.49 85.711500k 85.67 86.39Table 5: ULISSE?Stp+PTB on PTB test set withautomatically parsed data.ConclusionIn this paper we explored a new self?trainingmethod for domain adaptation where the selec-tion of reliable parses within automatically an-notated texts is carried out by an unsupervisedlinguistically?driven algorithm, ULISSE.
Resultsachieved for the CoNLL 2007 datasets as wellas for the larger test set represented by GENIAshow a significant improvement with respect toconsidered baselines.
This demonstrates a two?fold property of ULISSE, namely its reliablityand effectiveness both in capturing peculiaritiesof biomedical texts, and in selecting high qualityparses.
Thanks to these properties the proposedself?training method is able to improve the parserperformances when tested in an out?domain sce-nario.
The same approach could in principle beapplied to deal with biomedical sub?domain varia-tion: as reported by (Lippincott, Se?aghdha, & Ko-rhonen, 2011), biomedical texts belonging to dif-ferent sub?domains do vary along many linguisticdimensions, with a potential negative impact onbiomedical NLP tools.52ReferencesAttardi, G. (2006).
Experiments with a multi-language non-projective dependency parser.In Proceedings of CoNLL-X ?06 (pp.
166?170).
New York City, New York.Attardi, G., Dell?Orletta, F., Simi, M., & Turian, J.(n.d.).
Accurate dependency parsing with astacked multilayer perceptron.
In Proceed-ings of EVALITA 2009.Biber, D., & Conrad, S. (2009).
Genre, regis-ter, style.
Cambridge: Cambridge Univer-sity Press.Buchholz, S., & Marsi, E. (2006).
CoNLL-X shared task on multilingual dependencyparsing.
In Proceedings of CoNLL 2006.Clegg, A.
B., & Shepherd, A. J.
(2005).
Evalu-ating and integrating treebank parsers on abiomedical corpus.
In In proceedings of theACL 2005 workshop on software.Dell?Orletta, F. (n.d.).
Ensemble system forpart-of-speech tagging.
In Proceedings ofEVALITA 2009.Dell?Orletta, F., Venturi, G., & Montemagni, S.(2011).
Ulisse: an unsupervised algorithmfor detecting reliable dependency parses.
InProceedings of CoNLL 2011 (pp.
115?124).Portland, Oregon.Gildea, D. (2001).
Corpus variation and parserperformance.
In Proceedings of EMNLP2001 (p. 167-202).
Pittsburgh, PA.Johansson, R., & Nugues, P. (2007).
Ex-tended constituent-to-dependency conver-sion for english.
In Proceedings of NODAL-IDA 2007.
Tartu, Estonia.Kawahara, D., & Uchimoto, K. (2008).
Learningreliability of parses for domain adaptationof dependency parsing.
In Proceedings ofIJCNLP 2008 (pp.
709?714).Lease, M., & Charniak, E. (2005).
Parsingbiomedical literature.
In Proceedings ofthe second international joint conference onnatural language processing (IJCNLP-05),Jeju Island, Korea (pp.
58?69).Lippincott, T., Se?aghdha, D.
O?., & Korhonen, A.(2011).
Exploring subdomain variation inbiomedical language.
BMC Bioinformatics,12, 212?233.McClosky, D., & Charniak, E. (2008).
Self-training for biomedical parsing.
In Proceed-ings of ACL?HLT 2008 (pp.
101?104).McClosky, D., Charniak, E., & Johnson, M.(2006).
Reranking and self-training forparser adaptation.
In Proceedings of ACL2006 (pp.
337?344).
Sydney, Australia.McClosky, D., Charniak, E., & Johnson, M.(2010).
Automatic domain adaptation forparsing.
In Proceedings of NAACL?HLT2010 (pp.
28?36).
Los Angeles, California.McDonald, R., Lerman, K., & Pereira, F. (2006).Multilingual dependency analysis with atwo-stage discriminative parser.
In Proceed-ings of CoNLL 2006.McDonald, R., & Nivre, J.
(2007).
Character-izing the errors of data-driven dependencyparsing models.
In Proceedings of EMNLP-CoNLL 2007 (p. 122-131).Mitchell P. Marcus, M. A. M., & Santorini, B.(1993).
Building a large annotated corpusof english: the penn treebank.
Comput.
Lin-guist., 19(2), 313?330.Nivre, J., Hall, J., Ku?bler, S., McDonald, R., Nils-son, J., Riedel, S., & Yuret, D. (2007).The conll 2007 shared task on dependencyparsing.
In Proceedings of EMNLP-CoNLL2007 (pp.
915?932).Plank, B., & van Noord, G. (2011).
Effective mea-sures of domain similarity for parsing.
InProceedings of ACL 2011 (pp.
1566?1576).Portland, Oregon.Reichart, R., & Rappoport, A.
(2007).
Self?training for enhancement and domain adap-tation of statistical parsers trained on smalldatasets.
In Proceedings of ACL 2007 (pp.616?623).Reichart, R., & Rappoport, A.
(2009).
Automaticselection of high quality parses created by afully unsupervised parser.
In Proceedings ofCoNLL 2009 (pp.
156?164).Sagae, K. (2010).
Self-training without rerankingfor parser domain adaptation and its impacton semantic role labeling.
In Proceedingsof the 2010 workshop on domain adaptationfor natural language processing (DANLP2010) (pp.
37?44).
Uppsala, Sweden.Sagae, K., & Tsujii, J.
(2007).
Dependency pars-ing and domain adaptation with lr modelsand parser ensemble.
In Proceedings ofEMNLP?CoNLL 2007 (pp.
1044?1050).Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J.(2005).
Syntax annotation for the GENIAcorpus.
In Proceedings of IJCNLP?05 (pp.222?227).
Jeju Island, Korea.53
