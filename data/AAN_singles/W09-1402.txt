Proceedings of the Workshop on BioNLP: Shared Task, pages 10?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExtracting Complex Biological Events with Rich Graph-Based Feature SetsJari Bjo?rne,1 Juho Heimonen,1,2 Filip Ginter,1 Antti Airola,1,2Tapio Pahikkala1 and Tapio Salakoski1,21Department of Information Technology, University of Turku2Turku Centre for Computer Science (TUCS)Joukahaisenkatu 3-5, 20520 Turku, Finlandfirstname.lastname@utu.fiAbstractWe describe a system for extracting com-plex events among genes and proteins frombiomedical literature, developed in context ofthe BioNLP?09 Shared Task on Event Extrac-tion.
For each event, its text trigger, class, andarguments are extracted.
In contrast to the pre-vailing approaches in the domain, events canbe arguments of other events, resulting in anested structure that better captures the under-lying biological statements.
We divide the taskinto independent steps which we approach asmachine learning problems.
We define a widearray of features and in particular make ex-tensive use of dependency parse graphs.
Arule-based post-processing step is used to re-fine the output in accordance with the restric-tions of the extraction task.
In the shared taskevaluation, the system achieved an F-score of51.95% on the primary task, the best perfor-mance among the participants.1 IntroductionIn this paper, we present the best-performing systemin the primary task of the BioNLP?09 Shared Taskon Event Extraction (Kim et al, 2009).1 The pur-pose of this shared task was to competitively eval-uate information extraction systems targeting com-plex events in the biomedical domain.
Such an eval-uation helps to establish the relative merits of com-peting approaches, allowing direct comparability ofresults in a controlled setting.
The shared task was1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTaskthe first competitive evaluation of its kind in theBioNLP field as the extraction of complex eventsbecame possible only recently with the introductionof corpora containing the necessary annotation: theGENIA event corpus (Kim et al, 2008a) and theBioInfer corpus (Pyysalo et al, 2007).The objective of the primary task (Task 1) wasto detect biologically relevant events such as pro-tein binding and phosphorylation, given only anno-tation of named entities.
For each event, its class,trigger expression in the text, and arguments need tobe extracted.
The task follows the recent movementin BioNLP towards the extraction of semanticallytyped, complex events the arguments of which canalso be other events.
This results in a nested struc-ture that captures the underlying biological state-ments more accurately compared to the prevailingapproach of merely detecting binary interactions ofpairs of biological entities.Our system is characterized by heavy relianceon efficient, state-of-the-art machine learning tech-niques and a wide array of features derived froma full dependency analysis of each sentence.
Thesystem is a pipeline of three major processing steps:trigger recognition, argument detection and seman-tic post-processing.
By separating trigger recog-nition from argument detection, we can use meth-ods familiar from named entity recognition to tagwords as event triggers.
Event argument detectionthen becomes the task of predicting for each trigger?trigger or trigger?named entity pair whether it cor-responds to an actual instantiation of an event argu-ment.
Both steps can thus be approached as classi-fication tasks.
In contrast, semantic post-processing10Sentence?splittingTokenizationParsingConversion?to?graph?representationTrigger?detection(multi?class?SVM)System?outputSemantic?post?processing(rule?based)Edge?detection(multi?class?SVM)Input?dataFigure 1: The main components of the system.is rule-based, directly implementing argument typeconstraints following from the definition of the task.In the following sections, we present the imple-mentation of the three stages of our information ex-traction system in detail, and provide insights intowhy we chose the approach we did.
We also discussalternate directions we followed but that did not im-prove performance.
Finally, we analyze the overallperformance of our system in the shared task as wellas evaluate its components individually.2 The system descriptionThe overall architecture of the system is shownin Figure 1.
All steps in the system process onesentence at a time.
Since 95% of all annotatedevents are fully contained within a single sentence,this does not incur a large performance penalty butgreatly reduces the size and complexity of the ma-chine learning problems.2.1 Graph representationWe represent the extraction target in terms of seman-tic networks, graphs where the nodes correspondto named entities and events, and the edges corre-spond to event arguments.
The shared task can thenbe viewed as the problem of finding the nodes andedges of this graph.
For instance, nested events arenaturally represented through edges connecting twoevent nodes.
The graph representation of an exam-ple sentence is illustrated in Figure 2D.We have previously used this graph representa-tion for information extraction (Heimonen et al,2008; Bjo?rne et al, 2009) as well as for establishingthe connection between events and syntactic depen-dency parses in the Stanford scheme of de Marneffeand Manning (2008) (Bjo?rne et al, 2008).2.2 Trigger detectionWe cast trigger detection as a token labeling prob-lem, that is, each token is assigned to an event class,or a negative class if it does not belong to a trig-ger.
Triggers are then formed based on the predictedclasses of the individual tokens.
Since 92% of alltriggers in the data consist of a single token, adjacenttokens with the same class prediction form a singletrigger only in case that the resulting string occursas a trigger in the training data.
An event node iscreated for each detected trigger (Figure 2B).In rare cases, the triggers of events of differentclass share a token, thus the token belongs to sev-eral separate classes.
To be able to approach triggerdetection as a multi-class classification task whereeach token is given a single prediction, we intro-duce combined classes as needed.
For instance theclass gene expression/positive regulation denotes to-kens that act as a trigger to two events of the tworespective classes.
Note that this implies that thetrigger detection step produces at most one eventnode per class for any detected trigger.
In the sharedtask, however, multiple events of the same class canshare the same trigger.
For instance, the trigger in-volves in Figure 2 corresponds to two separate regu-lation events.
A separate post-processing step is in-troduced after event argument detection to duplicateevent nodes as necessary (see Section 2.4).Due to the nature of the GENIA event annota-tion principles, trigger detection cannot be easily re-duced to a simple dictionary lookup of trigger ex-pressions for two main reasons.
First, a number ofcommon textual expressions act as event triggers insome cases, but not in other cases.
For example,only 28% of the instances of the expression activatesare triggers for a positive regulation event while theremaining 72% are not triggers for any event.
Sec-ond, a single expression may be associated with var-ious event classes.
For example, the instances of thetoken overexpression are evenly distributed among11RegulationProteinIL-4 geneRegulationregulationRegulationinvolvesProteinNFAT1 andProteinNFAT2 .<Theme <Theme Cause> Cause>NN NN NN VBZ NN CC .<nn conj_and><nn dobj><nsubjNNProteinIL-4 geneRegulationregulationRegulationinvolvesProteinNFAT1 andProteinNFAT2 .<Theme <Theme Cause>Cause><Themenode duplicationparseT29   Regulation   regulationT30   Regulation   involvesE10   Regulation:T29   Theme:T7E11   Regulation:T30   Theme:E10   Cause:T9E12   Regulation:T30   Theme:E10   Cause:T8equivalentDCET7     Protein   IL-4T8     Protein   NFAT1T9     Protein   NFAT2ProteinIL-4 geneRegulationregulationRegulationinvolvesProteinNFAT1 andProteinNFAT2 .ProteinIL-4 gene regulation involvesProteinNFAT1 andProteinNFAT2 .edge detectiontrigger recognitionTrainingData PreparationBAEvent Extractiondobj>Figure 2: An example sentence from Shared Task document 10069428 (simplified).
A) Named entities are given.B) Triggers are detected and corresponding event nodes are created.
C) Event argument edges are predicted betweennodes.
The result is a sentence-level semantic network.
D) One node may denote multiple events of the same class,therefore nodes are duplicated in the semantic post-processing step.
E) The resulting graph can be losslessly trans-formed into the Shared Task event annotation.
Training data for the trigger recognizer includes named entity annotation(A) and for the edge detector the semantic network with no node duplication (C).gene expression, positive regulation, and the nega-tive class.
In light of these properties, we addresstrigger detection with a multi-class support vectormachine (SVM) classifier that assigns event classesto individual tokens, one at a time.
This is in con-trast to sequence labeling problems such as namedentity recognition, where a sequential model is typ-ically employed.
The classifier is trained on gold-standard triggers from the training data and incorpo-rates a wide array of features capturing the proper-ties of the token to be classified, both its linear anddependency context, and the named entities withinthe sentence.Token features include binary tests for capital-ization, presence of punctuation or numeric charac-ters, stem using the Porter stemmer (Porter, 1980),character bigrams and trigrams, and presence of thetoken in a gazetteer of known trigger expressionsand their classes, extracted from the training data.Token features are generated not only for the tokento be classified, but also for tokens in the immediatelinear context and dependency context (tokens thatgovern or depend on the token to be classified).Frequency features include the number ofnamed entities in the sentence and in a linear win-dow around the token in question as well as bag-of-word counts of token texts in the sentence.Dependency chains up to depth of three areconstructed, starting from the token to be classified.At each depth, both token features and dependencytype are included, as well as the sequence of depen-dency types in the chain.The trigger detector used in the shared task isin fact a weighted combination of two indepen-12dent SVM trigger detectors, both based on the samemulti-class classification principle and somewhatdifferent feature sets.2 The predictions of the twotrigger detectors are combined as follows.
For eachtrigger detector and each token, the classifier confi-dence scores of the top five classes are re-normalizedinto the [0, 1] interval.
The renormalized confidencescores of the two detectors are then linearly inter-polated using a parameter ?, 0 ?
?
?
1, whosevalue is set experimentally on the development set,as discussed below.Setting the correct precision?recall trade-off intrigger detection is very important.
On one hand,any trigger left undetected directly implies a falsenegative event.
On the other hand, the edge detec-tor is trained on gold standard data where there areno event nodes without arguments, which creates abias toward predicting edges for any event node theedge detector is presented with.
On the develop-ment set, essentially all predicted event nodes aregiven at least one argument edge.
We optimize theprecision?recall trade-off explicitly by introducing aparameter ?, 0 ?
?, that multiplies the classifierconfidence score given to the negative class, that is,the ?no trigger?
class.
When ?
< 1, the confidenceof the negative class is decreased, thus increasingthe possibility of a given token forming a trigger,and consequently increasing the recall of the triggerdetector (naturally, at the expense of its precision).Both trigger detection parameters, the interpola-tion weight ?
and the precision?recall trade-off pa-rameter ?, are set experimentally using a grid searchto find the globally optimal performance of the en-tire system on the development set, using the sharedtask performance metric.
The parameters are thusnot set to optimize the performance of trigger detec-tion in isolation; they are rather set to optimize theperformance of the whole system.2.3 Edge detectionAfter trigger detection, edge detection is used to pre-dict the edges of the semantic graph, thus extractingevent arguments.
Like the trigger detector, the edgedetector is based on a multi-class SVM classifier.We generate examples for all potential edges, which2This design should be considered an artifact of the time-constrained, experiment-driven development of the systemrather than a principled design choice.051015202530354045500 1 2 3 4 5 6 7 8 9 10 >10Proportionofedges [%]Edge lengthdependency distancelinear distanceFigure 3: The distribution of event argument edge lengthsmeasured as the number of dependencies on the shortestdependency path between the edge terminal nodes, con-trasted with edge lengths measured as the linear tokendistance.are always directed from an event node to anotherevent node (event nesting) or from an event node toa named entity node.
Each example is then classifiedas theme, cause, or a negative denoting the absenceof an edge between the two nodes in the given di-rection.
It should be noted that even though eventnodes often require multiple outgoing edges corre-sponding to multiple event arguments, all edges arepredicted independently and are not affected by pos-itive or negative classifications of other edges.The feature set makes extensive use of syntac-tic dependencies, in line with many recent stud-ies in biomedical information extraction (see, e.g.
(Kim et al, 2008b; Miwa et al, 2008; Airola et al,2008; Van Landeghem et al, 2008; Katrenko andAdriaans, 2008)).
The central concept in generat-ing features of potential event argument edges is theshortest undirected path of syntactic dependenciesin the Stanford scheme parse of the sentence whichwe assume to accurately capture the relationship ex-pressed by the edge.
In Figure 3, we show that thedistances among event and named entity nodes interms of shortest dependency path length are con-siderably shorter than in terms of their linear order inthe sentence.
The end points of the path are the syn-tactic head tokens of the two named entities or eventtriggers.
The head tokens are identified using a sim-ple heuristic.
Where multiple shortest paths exist,all are considered.
Most features are built by com-bining the attributes of multiple tokens (token text,13POS tag and entity or event class, such as protein orbinding) or dependencies (type such as subject anddirection relative to surrounding tokens).N-grams are generated by merging the at-tributes of 2?4 consecutive tokens.
N-grams are alsobuilt for consecutive dependencies.
Additional tri-grams are built for each token and its two flank-ing dependencies, as well as for each dependencyand its two flanking tokens.
These N-grams are de-fined in the direction of the potential event argumentedge.
To take into account the varying directionsof the dependencies, each pair of consecutive tokensforms an additional bigram defining their governor-dependent relationship.Individual component features are defined foreach token and edge in a path based on theirattributes which are also combined with the to-ken/edge position at either the interior or the end ofthe path.
Edge attributes are combined with their di-rection relative to the path.Semantic node features are built by directlycombining the attributes of the two terminalevent/entity nodes of the potential event argumentedge.
These features concatenate both the specifictypes of the nodes (e.g.
protein or binding) as wellas their categories (event or named entity).
Finally,if the events/entities have the same head token, thisself-loop is explicitly defined as a feature.Frequency features include the length of theshortest path as an integer-valued feature as well asan explicit binary feature for each length.
The num-ber of named entities and event nodes, per type, inthe sentence are defined for each example.We have used this type of edge detector with alargely similar feature set previously (Bjo?rne et al,2009).
Also, many of these features are standardin relation extraction studies (see, e.g., Buyko et al(2008)).2.4 Semantic post-processingThe semantic graph produced by the trigger andedge detection steps is not final.
In particular, itmay contain event nodes with an improper combi-nation of arguments, or no arguments whatsoever.Additionally, as discussed in Section 2.2, if there areevents of the same class with the same trigger, theyare represented by a single node.
Therefore, we in-troduce a rule-based post-processing step to refineFigure 4: Example of event duplication.
A) All theme?cause combinations are generated for regulation events.B) A heuristic is applied to decide how theme argumentsof binding events should be grouped.the graph, using the restrictions on event argumenttypes and combinations defined in the shared task.In Task 1, the allowed argument edges in thegraph are 1) theme from an event to a named en-tity, 2) theme or cause from a regulation event (or itssubclasses) to an event or a named entity.
Edges cor-responding to invalid arguments are removed.
Also,directed cycles are broken by removing the edgewith the weakest classification confidence score.After pruning invalid edges, event nodes are du-plicated so that all events have a valid combinationof arguments.
For example, the regulation event in-volves in Figure 2C has two cause arguments andtherefore represents two distinct events.
We thusduplicate the event node, obtaining one regulationevent for each of the cause arguments (Figure 2D).Events of type gene expression, transcription,translation, protein catabolism, localization, andphosphorylation must have exactly one theme argu-ment, which makes the duplication process trivial:duplicate events are created, one for each of the ar-guments.
Regulation events must have one themeand can additionally have one cause argument.
Forthese classes we use a heuristic, generating a newevent for each theme?cause combination of outgo-ing edges (Figure 4A).
Binding is the only eventclass that can have multiple theme arguments.
Thereis thus no simple way of determining how multi-ple outgoing theme edges should be grouped (Fig-ure 4B).
We apply a heuristic that first groups the ar-guments by their syntactic role, defined here as xthefirst dependency in the shortest path from the event14to the argument.
It then generates an event for eachpair of arguments that are in different groups.
In thecase of only one group, all single-argument eventsare generated.Finally, all events with no arguments as well asregulation events without a theme argument are iter-atively removed until no such event remains.
Theresulting graph is the output of our event extrac-tion system and can be losslessly converted into theshared task format (Figure 2D&E).2.5 Alternative directionsWe now briefly describe some of the alternative di-rections explored during the system development,which however did not result in increased perfor-mance, and were thus not included in the final sys-tem.
Whether the reason was due to the consideredapproaches being inadequate for the extraction task,or simply a result of the tight time constraints en-forced by the shared task is a question only furtherresearch can shed light on.For the purpose of dividing the extraction prob-lem into manageable subproblems, we make strongindependence assumptions.
This is particularly thecase in the edge detection phase where each edgeis considered in isolation from other edges, someof which may actually be associated with the sameevent.
Similar assumptions are made in the triggerdetection phase, where the classifications of individ-ual tokens are independent.A common way to relax independence assump-tions is to use N -best re-ranking where N most-likely candidates are re-ranked using global featuresthat model data dependencies that could not be mod-elled in the candidate generation step.
The best can-didate with respect to this re-ranked order is thenthe final prediction of the system.
N -best re-rankinghas been successfully applied for example in statisti-cal parsing (Charniak and Johnson, 2005).
We gen-erated the ten most likely candidate graphs, as de-termined by the confidence scores of the individualedges given by the multi-class SVM.
A perfect re-ranking of these ten candidates would lead to 11.5percentage point improvement in the overall systemF-score on the development set.
While we were un-able to produce a re-ranker sufficiently accurate toimprove the system performance in the time given,the large potential gain warrants further research.In trigger word detection, we experimented witha structural SVM incorporating Hidden MarkovModel type of sequential dependencies (Altun et al,2003; Tsochantaridis et al, 2004), which allow con-ditioning classification decisions on decisions madefor previous tokens as well as with a conditional ran-dom field (CRF) sequence classifier (Lafferty et al,2001).
Neither of these experiments led to a perfor-mance gain over the multiclass SVM classifier.As discussed previously, 4.8% of all annotatedevents cross sentence boundaries.
This problemcould be approached using coreference resolutiontechniques, however, the necessary explicit corefer-ence annotation to train a coreference resolution sys-tem is not present in the data.
Instead, we attemptedto build a machine-learning based system to detectcross-sentence event arguments directly, rather thanvia their referring expression, but were unable to im-prove the system performance.3 Tools and resources3.1 Multi-class SVMWe use a support vector machine (SVM) multi-classclassifier which has been shown to have state-of-the-art classification performance (see e.g.
(Cram-mer and Singer, 2002; Tsochantaridis et al, 2004)).Namely, we use the SVMmulticlass implementa-tion3 which is one of the fastest multi-class SVMimplementations currently available.
Analogouslyto the binary SVMs, multi-class SVMs have a reg-ularization parameter that determines the trade-offbetween the training error and the complexity of thelearned concept.
We select the value of the parame-ter on the development set.
Multi-class SVMs scalelinearly with respect to both the amount of trainingdata and the average number of nonzero features pertraining example, making them an especially suit-able learning method for our purposes.
They alsoprovide a real-valued prediction for each exampleto be classified which is used as a confidence scorein trigger detection precision?recall trade-off adjust-ment and event argument edge cycle breaking in se-mantic post-processing.
We use the linear kernel,the only practical choice to train the classifier withthe large training sets available.
For example, the3http://svmlight.joachims.org/svm_multiclass.html15010203040506070800  10  20  30  40  50  60  70  80Recall [%]Precision [%]Figure 5: Performance of the 24 systems that participatedin Task 1, together with an F-score contour plot for refer-ence.
Our system is marked with a full circle.final training data of the edge detector (8932 sen-tences) consists of 31792 training examples with295034 unique features.
Training with even thisamount of data is computationally feasible, typicallytaking less than an hour.All classifiers used in the system are trained asfollows.
First we optimize the regularization param-eter C by training on the shared task training set andtesting on the shared task development set.
We thenre-train the final classifier on the union of the train-ing and development sets, using the best value of Cin the previous step.
The same protocol is followedfor the ?
and ?
parameters in trigger detection.3.2 Dependency parsesBoth trigger detection and edge prediction rely ona wide array of features derived from full depen-dency parses of the sentence.
We use the McClosky-Charniak domain-adapted parser (McClosky andCharniak, 2008) which is among the best perform-ing parsers trained on the GENIA Treebank corpus.The native constituency output of the parser is trans-formed to the ?collapsed?
form of the Stanford de-pendency scheme (de Marneffe and Manning, 2008)using the Stanford parser tools.4 The parses wereprovided by the shared task organizers.4 Results and discussionThe final evaluation of the system was performed bythe shared task organizers using a test set whose an-4http://nlp.stanford.edu/software/notation was at no point available to the task partici-pants.
By the main criterion of Task 1, approximatespan matching with approximate recursive match-ing, our system achieved an F-score of 51.95%.
Fig-ure 5 shows the performance of all systems partic-ipating in Task 1.
The per-class results in Table 1show that regulation events (including positive andnegative regulation) as well as binding events are thehardest to extract.
These classes have F-scores inthe 31?44% range, while the other classes fall intothe 50?78% range.
This is not particularly surpris-ing since binding and regulation are the only classesin which events can have multiple arguments, whichmeans that for an event to be detected correctly, theedge detector often must make several correct pre-dictions.
Additionally, these classes have the lowesttrigger recognition performance on the developmentset.
It is interesting to note that the per-class perfor-mance in Table 1 shows no clear correlation betweenthe number of events of a class and its F-score.Table 2 shows the performance of the system us-ing various other evaluation criteria defined in theshared task.
The most interesting of these is thestrict matching criterion, which, in order to consideran event correctly extracted, requires exact triggerspan as well as all its nested events to be recursivelycorrect.
The performance of the system with respectto the strict criterion is 47.41% F-score, only 4.5 per-centage points lower than the relaxed primary mea-sure.
As seen in Table 2, this difference is almostexclusively due to triggers with incorrect span.To evaluate the performance impact of each sys-tem component individually, we report in Table 3overall system performance on the development set,obtained by progressively replacing the processingsteps with gold-standard data.
The results show thatthe errors of the system are almost evenly distributedbetween the trigger and edge detectors.
For instance,a perfect trigger detector would decrease the overallsystem error of 46.5% by 18.58 percentage points,a relative decrease of 40%.
A perfect edge detec-tor would, in combination with a perfect trigger de-tector, lead to system performance of 94.69%.
Theimprovement that could be gained by further devel-opment of the semantic post-processing step is thuslimited, indicating that the strict argument combina-tion restrictions of Task 1 are sufficient to resolve themajority of post-processing cases.16Event Class # R P FProtein catabolism 14 42.86 66.67 52.17Phosphorylation 135 80.74 74.66 77.58Transcription 137 39.42 69.23 50.23Localization 174 49.43 81.90 61.65Regulation 291 25.43 38.14 30.52Binding 347 40.06 49.82 44.41Negative regulation 379 35.36 43.46 38.99Gene expression 722 69.81 78.50 73.90Positive regulation 983 38.76 48.72 43.17Total 3182 46.73 58.48 51.95Table 1: Per-class performance in terms of Recall, Preci-sion, and F-score on the test set (3182 events) using ap-proximate span and recursive matching, the primary eval-uation criterion of Task 1.Matching R P FStrict 42.65 53.38 47.41Approx.
Span 46.51 58.24 51.72Approx.
Span&Recursive 46.73 58.48 51.95Table 2: Performance of our system on the test set (3182events) with respect to other evaluation measures in theshared task.5 ConclusionsWe have described a system for extracting complex,typed events from biomedical literature, only assum-ing named entities as given knowledge.
The highrank achieved in the BioNLP?09 Shared Task com-petitive evaluation validates the approach taken inbuilding the system.
While the performance is cur-rently the highest achieved on this data, the F-scoreof 51.95% indicates that there remains considerableroom for further development and improvement.We use a unified graph representation of the datain which the individual processing steps can be for-mulated as simple graph transformations: adding orremoving nodes and edges.
It is our experience thatsuch a representation makes handling the data fast,easy and consistent.
The choice of graph representa-tion is further motivated by the close correlation ofthese graphs with dependency parses.
As we are go-ing to explore the interpretation and applications ofthese graphs in the future, the graph representationwill likely provide a flexible base to build on.Dividing the task of event extraction into multi-ple subtasks that can be approached by well-studiedTrig Edge PP R P F ?Fpred pred pred 51.54 55.62 53.50GS pred pred 71.66 72.51 72.08 18.58GS GS pred 97.21 92.30 94.69 22.61GS GS GS 100.0 100.0 100.0 5.31Table 3: Effect of the trigger detector (Trig), edge detec-tor (Edge), and post-processing (PP) on performance onthe development set (1789 events).
The ?F column in-dicates the effect of replacing the predictions (pred) ofa component with the corresponding gold standard data(GS), i.e.
the maximal possible performance gain obtain-able from further development of that component.methods proved to be an effective approach in de-veloping our system.
We relied on state-of-the-artmachine learning techniques that scale up to the taskand allow the use of a considerable number of fea-tures.
We also carefully optimized the various pa-rameters, a vital step when using machine learningmethods, to fine-tune the performance of the system.In Section 2.5, we discussed alternative directionspursued during the development of the current sys-tem, indicating possible future research directions.To support this future work as well as complementthe description of the system in this paper we intendto publish our system under an open-source license.This shared task represents the first competi-tive evaluation of complex event extraction in thebiomedical domain.
The prior research has largelyfocused on binary interaction extraction, achievingafter a substantial research effort F-scores of slightlyover 60% (see, e.g., Miwa et al (2008)) on AIMed,the de facto standard corpus for this task.
Even ifa direct comparison of these results is difficult, theysuggest that 52% F-score in complex event extrac-tion is a non-trivial achievement, especially consid-ering the more detailed semantics of the extractedevents.
Further, complex event extraction is still anew problem ?
relevant corpora having been avail-able for only a few years.AcknowledgmentsThis research was funded by the Academy of Fin-land.
Computational resources were provided byCSC ?
IT Center for Science Ltd. We thank theshared task organizers for their efforts in data prepa-ration and system evaluation.17ReferencesAntti Airola, Sampo Pyysalo, Jari Bjo?rne, TapioPahikkala, Filip Ginter, and Tapio Salakoski.
2008.All-paths graph kernel for protein-protein interactionextraction with evaluation of cross-corpus learning.BMC Bioinformatics, 9(Suppl 11):S2.Yasemin Altun, Ioannis Tsochantaridis, and ThomasHofmann.
2003.
Hidden Markov support vector ma-chines.
In Proceedings of the Twentieth InternationalConference on Machine Learning (ICML?03), pages3?10.
AAAI Press.Jari Bjo?rne, Sampo Pyysalo, Filip Ginter, and TapioSalakoski.
2008.
How complex are complexprotein-protein interactions?
In Proceedings of theThird International Symposium on Semantic Mining inBiomedicine (SMBM?08), pages 125?128.
TUCS.Jari Bjo?rne, Filip Ginter, Juho Heimonen, SampoPyysalo, and Tapio Salakoski.
2009.
Learning to ex-tract biological event and relation graphs.
In Proceed-ings of the 17th Nordic Conference on ComputationalLinguistics (NODALIDA?09).Ekaterina Buyko, Elena Beisswanger, and Udo Hahn.2008.
Testing different ACE-style feature sets forthe extraction of gene regulation relations from MED-LINE abstracts.
In Proceedings of the Third Interna-tional Symposium on Semantic Mining in Biomedicine(SMBM?08), pages 21?28.
TUCS.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 173?180.
ACL.Koby Crammer and Yoram Singer.
2002.
On the al-gorithmic implementation of multiclass kernel-basedvector machines.
Journal of Machine Learning Re-search, 2:265?292.Marie-Catherine de Marneffe and Christopher Manning.2008.
Stanford typed hierarchies representation.
InProceedings of the COLING?08 Workshop on Cross-Framework and Cross-Domain Parser Evaluation,pages 1?8.Juho Heimonen, Sampo Pyysalo, Filip Ginter, and TapioSalakoski.
2008.
Complex-to-pairwise mapping ofbiological relationships using a semantic network rep-resentation.
In Proceedings of the Third Interna-tional Symposium on Semantic Mining in Biomedicine(SMBM?08), pages 45?52.
TUCS.Sophia Katrenko and Pieter Adriaans.
2008.
A localalignment kernel in the context of NLP.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics (Coling?08).Jin-Dong Kim, Tomoko Ohta, and Tsujii Jun?ichi.
2008a.Corpus annotation for mining biomedical events fromliterature.
BMC Bioinformatics, 9(1):10.Seonho Kim, Juntae Yoon, and Jihoon Yang.
2008b.Kernel approaches for genic interaction extraction.Bioinformatics, 24(1):118?126.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 shared task on event extraction.
InProceedings of the NAACL-HLT 2009 Workshopon Natural Language Processing in Biomedicine(BioNLP?09).
ACL.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the 18th International Conference on Ma-chine Learning (ICML?01), pages 282?289.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In Proceedings ofACL-08: HLT, Short Papers, pages 101?104.
Associa-tion for Computational Linguistics.Makoto Miwa, Rune S?tre, Yusuke Miyao, TomokoOhta, and Jun?ichi Tsujii.
2008.
Combiningmultiple layers of syntactic information for protein-protein interaction extraction.
In Proceedings of theThird International Symposium on Semantic Mining inBiomedicine (SMBM?08), pages 101?108.
TUCS.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Ja?rvinen, and TapioSalakoski.
2007.
BioInfer: A corpus for informationextraction in the biomedical domain.
BMC Bioinfor-matics, 8(1):50.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In Proceedings of the Twenty-first Inter-national Conference on Machine Learning (ICML?04),pages 104?111.
ACM.Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,and Yves Van de Peer.
2008.
Extracting protein-protein interactions from text using rich feature vec-tors and feature selection.
In Proceedings of theThird International Symposium on Semantic Mining inBiomedicine (SMBM?08), pages 77?84.
TUCS.18
