Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569?578,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSemi-supervised learning of morphological paradigms and lexiconsMalin AhlbergSpr?akbankenUniversity of Gothenburgmalin.ahlberg@gu.seMarkus ForsbergSpr?akbankenUniversity of Gothenburgmarkus.forsberg@gu.seMans HuldenUniversity of Helsinkimans.hulden@helsinki.fiAbstractWe present a semi-supervised approachto the problem of paradigm inductionfrom inflection tables.
Our system ex-tracts generalizations from inflection ta-bles, representing the resulting paradigmsin an abstract form.
The process is in-tended to be language-independent, andto provide human-readable generalizationsof paradigms.
The tools we provide canbe used by linguists for the rapid cre-ation of lexical resources.
We evaluate thesystem through an inflection table recon-struction task using Wiktionary data forGerman, Spanish, and Finnish.
With noadditional corpus information available,the evaluation yields per word form ac-curacy scores on inflecting unseen baseforms in different languages ranging from87.81% (German nouns) to 99.52% (Span-ish verbs); with additional unlabeled textcorpora available for training the scoresrange from 91.81% (German nouns) to99.58% (Spanish verbs).
We separatelyevaluate the system in a simulated task ofSwedish lexicon creation, and show thaton the basis of a small number of inflectiontables, the system can accurately collectfrom a list of noun forms a lexicon with in-flection information ranging from 100.0%correct (collect 100 words), to 96.4% cor-rect (collect 1000 words).1 IntroductionLarge scale morphologically accurate lexicon con-struction for natural language is a very time-consuming task, if done manually.
Usually, theconstruction of large-scale lexical resources pre-supposes a linguist who constructs a detailed mor-phological grammar that models inflection, com-pounding, and other morphological and phonolog-ical phenomena, and additionally performs a man-ual classification of lemmas in the language ac-cording to their paradigmatic behavior.In this paper we address the problem of lexiconconstruction by constructing a semi-supervisedsystem that accepts concrete inflection tables as in-put, generalizes inflection paradigms from the ta-bles provided, and subsequently allows the use ofunannotated corpora to expand the inflection ta-bles and the automatically generated paradigms.1In contrast to many machine learning ap-proaches that address the problem of paradigm ex-traction, the current method is intended to producehuman-readable output of its generalizations.
Thatis, the paradigms provided by the system can beinspected for errors by a linguist, and if neces-sary, corrected and improved.
Decisions made bythe extraction algorithms are intended to be trans-parent, permitting morphological system develop-ment in tandem with linguist-provided knowledge.Some of the practical tasks tackled by the sys-tem include the following:?
Given a small number of known inflection ta-bles, extract from a corpus a lexicon of thoselemmas that behave like the examples pro-vided by the linguist.?
Given a large number of inflection tables?such as those provided by the crowdsourcedlexical resource, Wiktionary?generalize thetables into a smaller number of abstractparadigms.2 Previous workAutomatic learning of morphology has long been aprominent research goal in computational linguis-tics.
Recent studies have focused on unsupervisedmethods in particular?learning morphology from1Our programs and the datasets used, including theevaluation procedure for this paper, are freely avail-able at https://svn.spraakbanken.gu.se/clt/eacl/2014/extract569unlabeled data (Goldsmith, 2001; Schone and Ju-rafsky, 2001; Chan, 2006; Creutz and Lagus,2007; Monson et al., 2008).
Hammarstr?om andBorin (2011) provides a current overview of unsu-pervised learning.Previous work with similar semi-supervisedgoals as the ones in this paper include Yarowskyand Wicentowski (2000), Neuvel and Fulop(2002), Cl?ement et al.
(2004).
Recent machinelearning oriented work includes Dreyer and Eis-ner (2011) and Durrett and DeNero (2013), whichdocuments a method to learn orthographic trans-formation rules to capture patterns across inflec-tion tables.
Part of our evaluation uses the samedataset as Durrett and DeNero (2013).
Eskanderet al.
(2013) shares many of the goals in this paper,but is more supervised in that it focuses on learn-ing inflectional classes from richer annotation.A major departure from much previous workis that we do not attempt to encode variationas string-changing operations, say by string edits(Dreyer and Eisner, 2011) or transformation rules(Lind?en, 2008; Durrett and DeNero, 2013) thatperform mappings between forms.
Rather, ourgoal is to encode all variation within paradigmsby presenting them in a sufficiently generic fash-ion so as to allow affixation processes, phonolog-ical alternations as well as orthographic changesto naturally fall out of the paradigm specificationitself.
Also, we perform no explicit alignment ofthe various forms in an inflection table, as in e.g.Tchoukalov et al.
(2010).
Rather, we base our al-gorithm on extracting the longest common subse-quence (LCS) shared by all forms in an inflectiontable, from which alignment of segments falls outnaturally.
Although our paradigm representationis similar to and inspired by that of Forsberg et al.
(2006) and D?etrez and Ranta (2012), our methodof generalizing from inflection tables to paradigmsis novel.3 Paradigm learningIn what follows, we adopt the view that wordsand their inflection patterns can be organizedinto paradigms (Hockett, 1954; Robins, 1959;Matthews, 1972; Stump, 2001).
We essentiallytreat a paradigm as an ordered set of functions(f1, .
.
.
, fn), where fi:x1, .
.
.
, xn7?
?
?, that is,where each entry in a paradigm is a function fromvariables to strings, and each function in a partic-ular paradigm shares the same variables.3.1 Paradigm representationWe represent the functions in what we call ab-stract paradigm.
In our representation, an ab-stract paradigm is an ordered collection of strings,where each string may additionally contain in-terspersed variables denoted x1, x2, .
.
.
, xn.
Thestrings represent fixed, obligatory parts of aparadigm, while the variables represent mutableparts.
These variables, when instantiated, mustcontain at least one segment, but may otherwisevary from word to word.
A complete abstractparadigm captures some generalization where themutable parts represented by variables are instan-tiated the same way for all forms in one particu-lar inflection table.
For example, the fairly simpleparadigmx1x1+s x1+ed x1+ingcould represent a set of English verb forms, wherex1in this case would coincide with the infinitiveform of the verb?walk, climb, look, etc.For more complex patterns, several variableparts may be invoked, some of them discontinu-ous.
For example, part of an inflection paradigmfor German verbs of the type schreiben (to write)verbs may be described as:x1+e+x2+x3+en INFINITIVEx1+e+x2+x3+end PRESENT PARTICIPLEge+x1+x2+e+x3+en PAST PARTICIPLEx1+e+x2+x3+e PRESENT 1P SGx1+e+x2+x3+st PRESENT 2P SGx1+e+x2+x3+t PRESENT 3P SGIf the variables are instantiated as x1=schr,x2=i, and x3=b, the paradigm corresponds tothe forms (schreiben, schreibend, geschrieben,schreibe, schreibst, schreibt).
If, on the otherhand, x1=l, x2=i, and x3=h, the same paradigm re-flects the conjugation of leihen (to lend/borrow)?
(leihen, leihend, geliehen, leihe, leihst, leiht).It is worth noting that in this representation, noparticular form is privileged in the sense that allother forms can only be generated from some spe-cial form, say the infinitive.
Rather, in the cur-rent representation, all forms can be derived fromknowing the variable instantiations.
Also, givenonly a particular word form and a hypotheticalparadigm to fit it in, the variable instantiations canoften be logically deduced unambiguously.
Forexample, let us say we have a hypothetical formsteigend and need to fit it in the above paradigm,without knowing which slot it should occupy.
We570may deduce that it must represent the present par-ticiple, and that x1=st, x2=i, and x3=g.
From thisknowledge, all other forms can subsequently bederived.Although we have provided grammatical in-formation in the above table for illustrative pur-poses, our primary concern in the current work isthe generalization from inflection tables?whichfor our purposes are simply an ordered set ofword forms?to paradigms of the format dis-cussed above.3.2 Paradigm induction from inflection tablesThe core component of our method consists offinding, given an inflection table, the maximallygeneral paradigm that reflects the information inthat table.
To this end, we make the assumptionthat string subsequences that are shared by dif-ferent forms in an inflection table are incidentaland can be generalized over.
For example, giventhe English verb swim, and a simple inflection ta-ble swim#swam#swum,2we make the assump-tion that the common sequences sw and m are ir-relevant to the inflection, and that by disregardingthese strings, we can focus on the segments thatvary within the table?in this case the variationi?a?u.
In other words, we can assume sw andm to be variables that vary from word to wordand describe the table swim#swam#swum asx1+i+x2#x1+a+x2#x1+u+x2, where x1=sw andx2=m in the specific table.3.2.1 Maximally general paradigmsIn order to generalize as much as possible from aninflection table, we extract from it what we call themaximally general paradigm by:1.
Finding the longest common subsequence(LCS) to all the entries in the inflection table.2.
Finding the segmentation into variables ofthe LCS(s) (there may be several) in the in-flection table that results in(a) The smallest number of variables.
Twosegments xy in the LCS must be part ofthe same variable if they always occurtogether in every form in the inflectiontable, otherwise they must be assignedseparate variables.2To save space, we will henceforth use the #-symbol as adelimiter between entries in an inflection table or paradigm.ringrangrung[r]i[ng][r]a[ng][r]u[ng]rng?
Extract     LCS ?
Fit LCS      to table ?
Generalize     to paradigmsInput:inflectiontablesswimswamswumswm[sw]i[m][sw]a[m][sw]u[m]x1+i+x2x1+a+x2x1+u+x2x1+i+x2x1+a+x2x1+u+x2?
Collapse     paradigmsx1+i+x2x1+a+x2x1+u+x2}} }}Figure 1: Illustration of our paradigm generaliza-tion algorithm.
In step ?
we extract the LCS sep-arately for each inflection table, attempt to finda consistent fit between the LCS and the formspresent in the table (step ?
), and assign the seg-ments that participate in the LCS variables (step?).
Finally, resulting paradigms that turn out to beidentical may be collapsed (step ?)
(section 3.3).
(b) The smallest total number of infixednon-variable segments in the inflectiontable (segments that occur between vari-ables).3.
Replacing the discontinuous sequences thatare part of the LCS with variables (everyform in a paradigm will contain the samenumber of variables).These steps are illustrated in figure 1.
Thefirst step, extracting the LCS from a collection ofstrings, is the well-known multiple longest com-mon subsequence problem (MLCS).
It is knownto be NP-hard (Maier, 1978).
Although the num-ber of strings to find the LCS from may be ratherlarge in real-world data, we find that a few sensibleheuristic techniques allow us to solve this problemefficiently for practical linguistic material, i.e., in-flection tables.
We calculate the LCS by calculat-ing intersections of finite-state machines that en-code all subsequences of all words, using the fomafinite-state toolkit (Hulden, 2009).3While for most tables there is only one wayto segment the LCS in the various forms, someambiguous corner cases need to be resolved byimposing additional criteria for the segmentation,given in steps 2(a) and 2(b).
As an example,consider a snippet of a small conjugation tablefor the Spanish verb comprar (to buy), com-prar#compra#compro.
Obviously the LCS iscompr?however, this can be distributed in twodifferent ways across the strings, as seen below.3Steps 2 and 3 are implemented using more involvedfinite-state techniques that we plan to describe elsewhere.571comprarcompracompro{ x1comprarcompracompro{{x1 x2(a) (b){ {x1 x2x1 {The obvious difference here is that in the firstassignment, we only need to declare one vari-able x1=compr, while in the second, we needtwo, x1=comp, x2=r.
Such cases are resolved bychoosing the segmentation with the smallest num-ber of variables by step 2(a).Remaining ambiguities are resolved by mini-mizing the total number of infixed segments.
Asan illustration of where this is necessary, considera small extract from the Swedish noun table segel(sail): segel#seglen#seglet.
Here, the LCS, ofwhich there are two of equal length (sege/segl)must be assigned to two variables where eitherx1=seg and x2=e, or x1=seg and x2=l:segelseglenseglet{ {x1 x2segelseglenseglet{ {x1 x2(a) (b)However, in case (a), the number of infixedsegments?the l?s in the second and third form?total one more than in the distribution in (b), whereonly one e needs to be infixed in one form.
Hence,the representation in (b) is chosen in step 2(b).The need for this type of disambiguation strat-egy surfaces very rarely and the choice to mini-mize infix length is largely arbitrary?although itmay be argued that some linguistic plausibility isencoded in the minimization of infixes.
However,choosing a consistent strategy is important for thesubsequent collapsing of paradigms.3.3 Collapsing paradigmsIf several tables are given as input, and we extractthe maximally general paradigm from each, wemay collapse resulting paradigms that are identi-cal.
This is also illustrated in figure 1.As paradigms are collapsed, we record the in-formation about how the various variables wereinterpreted prior to collapsing.
That is, for theexample in figure 1, we not only store the result-ing single paradigm, but also the information thatx1=r, x2=ng in one table and that x1=sw, x2=min another.
This allows us to potentially recon-struct all the inflection tables seen during learn-Form Input Generalization[Inf] kaufen x1+en[PresPart] kaufend x1+end[PastPart] gekauft ge+x1+t[Pres1pSg] kaufe x1+e[Pres1pPl] kaufen x1+en[Pres2pSg] kaufst x1+st[Pres2pPl] kauft x1+t[Pres3pSg] kauft x1+t[Pres3pPl] kaufen x1+en.
.
.
.
.
.
.
.
.x1= kaufTable 1: Generalization from a German exampleverb kaufen (to buy) exemplifying typical render-ing of paradigms.ing.
Storing this information is also crucial forparadigm table collection from text, fitting unseenword forms into paradigms, and reasoning aboutunseen paradigms, as will be discussed below.3.4 MLCS as a language-independentgeneralization strategyThere is very little language-specific informationencoded in the strategy of paradigm generaliza-tion that focuses on the LCS in an inflectiontable.
That is, we do not explicitly prioritizeprocesses like prefixation, suffixation, or left-to-right writing systems.
The resulting algorithmthus generalizes tables that reflect concatenativeand non-concatenative morphological processesequally well.
Tables 1 and 2 show the outputs ofthe method for German and Arabic verb conjuga-tion reflecting the generalization of concatenativeand non-concatenative patterns.3.5 Instantiating paradigmsAs mentioned above, given that the variable in-stantiations of a paradigm are known, we may gen-erate the full inflection table.
The variable instan-tiations are retrieved by matching a word form toone of the patterns in the paradigms.
For example,the German word form b?ucken (to bend down)may be matched to three patterns in the paradigmexemplified in table 1, and all three matches yieldthe same variable instantiation, i.e., x1=b?uck.Paradigms with more than one variable maybe sensitive to the matching strategy of the vari-ables.
To see this, consider the pattern x1+a+x2and the word banana.
Here, two matches are pos-sible x1=b and x2=nana and x1=ban and x2=na.In other words, there are three possible matching572Form Input Generalization[Past1SG] katabtu (I.J?)
x1+a+x2+a+x3+tu[Past2SGM] katabta (I.J?)
x1+a+x2+a+x3+ta[Past2SGF] katabti (I.J?)
x1+a+x2+a+x3+ti[Past3SGM] kataba (I.J?)
x1+a+x2+a+x3+a[Past3SGF] katabat (I.J?)
x1+a+x2+a+x3+at.
.
.
.
.
.
.
.
.
[Pres1SG] aktubu (I.J?
@) a+x1+x2+u+x3+u[Pres2SGM] taktubu (I.J?K) ta+x1+x2+u+x3+u[Pres2SGF] taktub?
?na (?J.J?K) ta+x1+x2+u+x3+?
?na[Pres3SGM] yaktubu (I.J?K) ya+x1+x2+u+x3+u[Pres3SGF] taktubu (I.J?K) ta+x1+x2+u+x3+u.
.
.
.
.
.
.
.
.x1= k (?
), x2= t (H), x3= b (H.)Table 2: Generalization from an Arabic con-jugation table involving the root /k-t-b/ fromwhich the stems katab (to write/past) and ktub(present/non-past) are formed, conjugated in FormI, past and present tenses.
Extracting the longestcommon subsequence yields a paradigm wherevariables correspond to root radicals.strategies:41. shortest match (x1=b and x2=nana)2. longest match (x1=ban and x2=na)3. try all matching combinationsThe matching strategy that tends to be success-ful is somewhat language-dependent: for a lan-guage with a preference for suffixation, longestmatch is typically preferred, while for othersshortest match or trying all combinations may bethe best choice.
All languages evaluated in thisarticle have a preference for suffixation, so in ourexperiments we have opted for using the longestmatch for the sake of convenience.
Our imple-mentation allows for exploring all matches, how-ever.
Even though all matches were to be tried,?bad?
matches will likely result in implausible in-flections that can be discarded using other cues.4 Assigning paradigms automaticallyThe next problem we consider is assigning the cor-rect paradigms to candidate words automatically.4The number of matches may increase quickly for longerwords and many variables in the worst case: e.g.
caravanmatches x1+a+x2in three different ways.As a first step, we match the current word to a pat-tern.
In the general case, all patterns are tried for agiven candidate word.
However, we usually haveaccess to additional information about the candi-date words?e.g., that they are in the base form ofa certain part of speech?which we use to improvethe results by only matching the relevant patterns.From a candidate word, all possible inflectiontables are generated.
Following this, a decisionprocedure is applied that calculates a confidencescore to determine which paradigm is the mostprobable.
The score is a weighted combination ofthe following calculations:1.
Compute the longest common suffix for thegenerated base form (which may be the inputform) with previously seen base forms.
If ofequal length, select the paradigm where thesuffix occurs with higher frequency.2.
Compute frequency spread over the set ofunique word forms according to the follow-ing formula:?w?set(W )log(freq(w) + 1)3.
Use the most frequent paradigm as a tie-breaker.Step 1 is a simple memory-based approach,much in the same spirit as van den Bosch andDaelemans (1999), where we compare the currentbase form with what we have seen before.For step 2, let us elaborate further why thefrequency spread is computed on unique wordforms.
We do this to avoid favoring paradigmsthat have the same word forms for many or allinflected forms.
For example, the German nounAnanas (pineapple) has a syncretic inflection withone repeated word form across all slots, Ananas.When trying to assign a paradigm to an unknownword form that matches x1, it will surely fit theparadigm that Ananas has generated perfectlysince we have encountered every word form in thatparadigm, of which there is only one, namely x1.Hence, we want to penalize low variation of wordforms when assigning paradigms.The confidence score calculated is not only ap-plicable for selecting the most probable paradigmfor a given word-form; it may also be used to ranka list of words so that the highest ranked paradigmis the most likely to be correct.
Examples of suchrankings are found in section 5.3.5730 50 100 150 200Paradigms0.700.750.800.850.900.951.00Inflection table coverageFI-NOUNS-ADJSFI-VERBSES-VERBSDE-NOUNSDE-VERBSFigure 2: Degree of coverage with varying num-bers of paradigms.5 EvaluationTo evaluate the method, we have conducted threeexperiments.
First we repeat an experiment pre-sented in Durrett and DeNero (2013) using thesame data and experiment setup, but with ourgeneralization method.
In this experiment, weare given a number of complete inflection tablesscraped from Wiktionary.
The task is to recon-struct complete inflection tables from 200 held-outbase forms.
For this task, we evaluate per formaccuracy as well as per table accuracy for recon-struction.
The second experiment is the same asthe first, but with additional access to an unlabeledtext dump for the language from Wikipedia.In the last experiment we try to mimic the situa-tion of a linguist starting out to describe a new lan-guage.
The experiment uses a large-scale Swedishmorphology as reference and evaluates how reli-ably a lexicon can be gathered from a word list us-ing only a few manually specified inflection tablesgeneralized into abstract paradigms by our system.5.1 Experiment 1: WiktionaryIn our first experiment we start from the inflec-tion tables in the development and test set fromDurrett and DeNero (2013), henceforth D&DN13.Table 3 shows the number of input tables as wellas the number of paradigms that they result in af-ter generalization and collapsing.
For all cases,the number of output paradigms are below 10%of the number of input inflection tables.
Figure2 shows the generalization rate achieved with theparadigms.
For instance, the 20 most common re-sulting German noun paradigms are sufficient tomodel almost 95% of the 2,564 separate inflectiontables given as input.As described earlier, in the reconstruction task,the input base forms are compared to the abstractInput: Output:Data inflection abstracttables paradigmsDE-VERBS 1827 140DE-NOUNS 2564 70ES-VERBS 3855 97FI-VERBS 7049 282FI-NOUNS-ADJS 6200 258Table 3: Generalization of paradigms.
The num-ber of paradigms produced from Wiktionary in-flection tables by generalization and collapsing ofabstract paradigms.paradigms by measuring the longest common suf-fix length for each input base form compared tothe ones seen during training.
This approach ismemory-based: it simply measures the similarityof a given lemma to the lemmas encountered dur-ing the learning phase.
Table 4 presents our resultsjuxtaposed with the ones reported by D&DN13.While scoring slightly below D&DN13 for themajority of the languages when measuring formaccuracy, our method shows an advantage whenmeasuring the accuracy of complete tables.
In-terestingly, the only case where we improve uponthe form accuracy of D&DN13 is German verbs,where we get our lowest table accuracy.Table 4 further shows an oracle score, giv-ing an upper bound for our method that wouldbe achieved if we were always able to pick thebest fitting paradigm available.
This upper boundranges from 99% (Finnish verbs) to 100% (threeout of five tests).5.2 Experiment 2: Wiktionary andWikipediaIn our second experiment, we extend the previousexperiment by adding access to a corpus.
Apartfrom measuring the longest common suffix length,we now also compute the frequency of the hy-pothetical candidate forms in every generated ta-ble and use this to favor paradigms that generatea large number of attested forms.
For this, weuse a Wikipedia dump, from which we have ex-tracted word-form frequencies.5In total, the num-ber of word types in the Wikipedia corpus was8.9M (German), 3.4M (Spanish), 0.7M (Finnish),and 2.7M (Swedish).
Table 5 presents the results,5The corpora were downloaded and extracted as de-scribed at http://medialab.di.unipi.it/wiki/Wikipedia_Extractor574Data Per D&DN13 Per D&DN13 Oracle accuracytable form per form (per table)DE-VERBS 68.0 85.0 97.04 96.19 99.70 (198/200)DE-NOUNS 76.5 79.5 87.81 88.94 100.00 (200/200)ES-VERBS 96.0 95.0 99.52 99.67 100.00 (200/200)FI-VERBS 92.5 87.5 96.36 96.43 99.00 (195/200)FI-NOUNS-ADJS 85.0 83.5 91.91 93.41 100.00 (200/200)Table 4: Experiment 1: Accuracy of reconstructing 200 inflection tables given only base forms fromheld-out data when paradigms are learned from the Wiktionary dataset.
For comparison, figures fromDurrett and DeNero (2013) are included (shown as D&DN13).Data Per Per Oracle acc.table form per form (table)DE-VERBS 76.50 97.87 99.70 (198/200)DE-NOUNS 82.00 91.81 100.00 (200/200)ES-VERBS 98.00 99.58 100.00 (200/200)FI-VERBS 92.50 96.63 99.00 (195/200)FI-NOUNS-ADJS 88.00 93.82 100.00 (200/200)Table 5: Experiment 2: Reconstructing 200 held-out inflection tables with paradigms induced fromWiktionary and further access to raw text fromWikipedia.where an increased accuracy is noted for all lan-guages, as is to be expected since we have addedmore knowledge to the system.
The bold numbersmark the cases where we outperform the result inDurrett and DeNero (2013), which is now the casein four out of five tests for table accuracy, scoringbetween 76.50% for German verbs and 98.00% forSpanish verbs.Measuring form accuracy, we achieve scoresbetween 91.81% and 99.58%.
The smallest im-provement is noted for Finnish verbs, which hasthe largest number of paradigms, but also thesmallest corpus.5.3 Experiment 3: Ranking candidatesIn this experiment we consider a task where weonly have a small number of inflection tables,mimicking the situation where a linguist has man-ually entered a few inflection tables, allowed thesystem to generalize these into paradigms, andnow faces the task of culling from a corpus?inthis case labeled with basic POS information?thecandidate words/lemmas that best fit the inducedparadigms.
This would be a typical task duringlexicon creation.We selected the 20 most frequent nounparadigms (from a total of 346), with one in-flection table each, from our gold standard, theTop-1000 rank Correct/IncorrectTOP 10% 100/0 (100.0%)TOP 50% 489/11 (97.8%)TOP 100% 964/36 (96.4%)Table 6: Top-1000 rank for all nouns in SALDOSwedish lexical resource SALDO (Borin et al.,2013).
From this set, we discarded paradigmsthat lack plural forms.6We also removed fromthe paradigms special compounding forms thatSwedish nouns have, since compound informa-tion is not taken into account in this experiment.The compounding forms are part of the originalparadigm specification, and after a collapsing pro-cedure after compound-form removal, we wereleft with a total of 11 paradigms.In the next step we ranked all nouns in SALDO(79.6k lemmas) according to our confidence score,which indicates how well a noun fits a givenparadigm.
We then evaluated the paradigm assign-ment for the top-1000 lemmas.
Among these top-1000 words, we found 44 that were outside the20 most frequent noun paradigms.
These wordswere not necessarily incorrectly assigned, sincethey may only differ in their compound forms; asa heuristic, we considered them correct if they hadthe same declension and gender as the paradigm,and incorrect otherwise.Table 6 displays the results, including a total ac-curacy of 96.4%.Next, we investigated the top-1000 distributionfor individual paradigms.
This corresponds to thesituation where a linguist has just entered a newinflection table and is looking for words that fit theresulting paradigm.
The result is presented in two6The paradigms that lack plural forms are subsets of otherparadigms.
In other words: when no plural forms are attested,we would need a procedure to decide if plural forms are evenpossible, which is currently beyond the scope of our method.57510 20 30 40 50 60 70Top ranked H%L246810Error rate H%Lp_kikarep_meningp_flicka10 20 30 40 50 60 70Top ranked H%L10203040506070Error rate H%Lp_dikep_akademip_vingep_nyckelFigure 3: Top-1000: high and low precision paradigms.error rate plots: figure 3 shows the low precisionand high precision paradigms in two plots, whereerror rates range from 0-2% and 16-44% for thetop 100 words.We further investigated the worst-performingparadigm, p akademi (academy), to determinethe reason for the high error rate for this particularitem.
The main source of error (334 out of 1000) isconfusion with p akribi (accuracy), which has noplural.
However, it is on semantic grounds that theparadigm has no plural; a native Swedish speakerwould pluralize akribi like akademi (disregard-ing the fact that akribi is defective).
The secondmain type of error (210 out of 1000) is confusionwith the unseen paradigm of parti (party), whichinflects similarly to akademi, but with a differ-ence in gender?difficult to predict from surfaceforms?that manifests itself in two out of eightword forms.6 Future workThe core method of abstract paradigm represen-tation presented in this paper can readily be ex-tended in various directions.
One obvious topic ofinterest is to investigate the use of machine learn-ing techniques to expand the method to completelyunsupervised learning by first clustering similarwords in raw text into hypothetical inflection ta-bles.
The plausibility of these tables could then beevaluated using similar techniques as in our exper-iment 2.We also plan to explore ways to improve thetechniques for paradigm selection and ranking.
Inour experiments we have, for the sake of trans-parency, used a fairly simple strategy of suffixmatching to reconstruct tables from base forms.A more involved classifier may be trained for thispurpose.
An obvious extension is to use a clas-sifier based on n-gram, capitalization, and otherstandard features to ascertain that word forms inhypothetical reconstructed inflection tables main-tain similar shapes to ones seen during training.One can also investigate ways to collapseparadigms further by generalizing over phonolog-ical alternations and by learning alternation rulesfrom the induced paradigms (Koskenniemi, 1991;Theron and Cloete, 1997; Koskenniemi, 2013).Finally, we are working on a separate interactivegraphical morphological tool in which we plan tointegrate the methods presented in this paper.7 ConclusionWe have presented a language-independentmethod for extracting paradigms from inflectiontables and for representing and generalizing theresulting paradigms.7Central to the process ofparadigm extraction is the notion of maximallygeneral paradigm, which we define as the in-flection table, with all of the common stringsubsequences forms represented by variables.The method is quite uncomplicated and outputshuman-readable generalizations.
Despite the rel-ative simplicity, we obtain state-of-the art resultsin inflection table reconstruction tasks from baseforms.Because of the plain paradigm representationformat, we believe the model can be used prof-itably in creating large-scale lexicons from a fewlinguist-provided inflection tables.7The research presented here was supported by theSwedish Research Council (the projects Towards aknowledge-based culturomics, dnr 2012-5738, and SwedishFramenet++, dnr 2010-6013), the University of Gothenburgthrough its support of the Centre for Language Technologyand its support of Spr?akbanken, and the Academy of Finlandunder the grant agreement 258373, Machine learning ofrules in natural language morphology and phonology.576ReferencesLars Borin, Markus Forsberg, and Lennart L?onngren.2013.
SALDO: a touch of yin to WordNet?s yang.Language Resources and Evaluation, May.
Onlinefirst publication; DOI 10.1007/s10579-013-9233-4.Erwin Chan.
2006.
Learning probabilistic paradigmsfor morphology in a latent class model.
In Proceed-ings of the Eighth Meeting of the ACL Special Inter-est Group on Computational Phonology and Mor-phology, pages 69?78.
Association for Computa-tional Linguistics.Lionel Cl?ement, Bernard Lang, Beno?
?t Sagot, et al.2004.
Morphology based automatic acquisition oflarge-coverage lexica.
In LREC 04, pages 1841?1844.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing (TSLP), 4(1):3.Gr?egoire D?etrez and Aarne Ranta.
2012.
Smartparadigms and the predictability and complexity ofinflectional morphology.
In Proceedings of the 13thEACL, pages 645?653.
Association for Computa-tional Linguistics.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text usinga Dirichlet process mixture model.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 616?627.
Associationfor Computational Linguistics.Greg Durrett and John DeNero.
2013.
Supervisedlearning of complete morphological paradigms.
InProceedings of NAACL-HLT, pages 1185?1195.Ramy Eskander, Nizar Habash, and Owen Rambow.2013.
Automatic extraction of morphological lex-icons from morphologically annotated corpora.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages1032?1043.
Association for Computational Linguis-tics.Markus Forsberg, Harald Hammarstr?om, and AarneRanta.
2006.
Morphological lexicon extractionfrom raw text data.
In Advances in Natural Lan-guage Processing, pages 488?499.
Springer.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
Computationallinguistics, 27(2):153?198.Harald Hammarstr?om and Lars Borin.
2011.
Unsuper-vised learning of morphology.
Computational Lin-guistics, 37(2):309?350.Charles F Hockett.
1954.
Two models of grammati-cal description.
Morphology: Critical Concepts inLinguistics, 1:110?138.Mans Hulden.
2009.
Foma: a finite-state compiler andlibrary.
In Proceedings of the 12th Conference of theEuropean Chapter of the European Chapter of theAssociation for Computational Linguistics: Demon-strations Session, pages 29?32, Athens, Greece.
As-sociation for Computational Linguistics.Kimmo Koskenniemi.
1991.
A discovery procedurefor two-level phonology.
Computational Lexicol-ogy and Lexicography: A Special Issue Dedicatedto Bernard Quemada, 1:451?46.Kimmo Koskenniemi.
2013.
An informal discoveryprocedure for two-level rules.
Journal of LanguageModelling, 1(1):155?188.Krister Lind?en.
2008.
A probabilistic model for guess-ing base forms of new words by analogy.
In Compu-tational Linguistics and Intelligent Text Processing,pages 106?116.
Springer.David Maier.
1978.
The complexity of some problemson subsequences and supersequences.
Journal of theACM (JACM), 25(2):322?336.Peter H. Matthews.
1972.
Inflectional morphology:A theoretical study based on aspects of Latin verbconjugation.
Cambridge University Press.Christian Monson, Jaime Carbonell, Alon Lavie, andLori Levin.
2008.
Paramor: finding paradigmsacross morphology.
In Advances in Multilingualand Multimodal Information Retrieval, pages 900?907.
Springer.Sylvain Neuvel and Sean A Fulop.
2002.
Unsuper-vised learning of morphology without morphemes.In Proceedings of the ACL-02 workshop on Morpho-logical and phonological learning-Volume 6, pages31?40.
Association for Computational Linguistics.Robert H Robins.
1959.
In defence of WP.
Transac-tions of the Philological Society, 58(1):116?144.Patrick Schone and Daniel Jurafsky.
2001.Knowledge-free induction of inflectional mor-phologies.
In Proceedings of the second meetingof the North American Chapter of the Associationfor Computational Linguistics on Language tech-nologies, pages 1?9.
Association for ComputationalLinguistics.Gregory T. Stump.
2001.
A theory of paradigm struc-ture.
Cambridge University Press.Tzvetan Tchoukalov, Christian Monson, and BrianRoark.
2010.
Morphological analysis by mul-tiple sequence alignment.
In Multilingual Infor-mation Access Evaluation I.
Text Retrieval Experi-ments, pages 666?673.
Springer.Pieter Theron and Ian Cloete.
1997.
Automatic acqui-sition of two-level morphological rules.
In Proceed-ings of the fifth conference on Applied natural lan-guage processing, pages 103?110.
Association forComputational Linguistics.577Antal van den Bosch and Walter Daelemans.
1999.Memory-based morphological analysis.
In Proceed-ings of the 37th Annual Meeting of the Associationfor Computational Linguistics, pages 285?292.
As-sociation for Computational Linguistics.David Yarowsky and Richard Wicentowski.
2000.Minimally supervised morphological analysis bymultimodal alignment.
In Proceedings of the 38thAnnual Meeting on Association for ComputationalLinguistics, pages 207?216.
Association for Com-putational Linguistics.578
