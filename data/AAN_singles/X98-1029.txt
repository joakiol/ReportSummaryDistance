TIPSTER Information Extraction Evaluation:The MUC-7 WorkshopElaine MarshNavy Center  for  Appl ied Research in Art i f ic ial  Intel l igenceNaval  Research Laboratory4555 Over look  Ave. ,  SWWashington,  DC 20375-5337Emai l :  marsh @ aic.nr l .navy.mi lINTRODUCTIONThe last of the "Message UnderstandingConferences", which were designed to evaluate textextraction systems, was held in April 1998 in Fairfax,Virginia.
The workshop was co-chaired by ElaineMarsh and Ralph Grishman.
A group of 18organizations, both from the United States andabroad, participated in the evaluation.
MUC-7introduced a wider set of tasks with larger sets oftraining and formal data than previous MUCs.Results showed that while performance on the namedentity and template lements task remains relativelyhigh, additional research is still necessary forimproved performance on more difficult asks such ascoreference r solution and domain-specific templategeneration from textual sources.EVALUATION TASKSMUC-7 consisted of six informationextraction tasks.
The Named Entity Task \[NE\]required systems to insert SGML tags into the text tomark each string that represents a person,organization, or location name, or a date or timestamp, or a currency or percentage figure.
Theguidelines were brought in line with the multilingualtask.
The Multi-lingual Entity Task \[MET\] involvedthe execution of the NE task for Chinese andJapanese language texts.
The Template ElementTask \[TE\] required participants to extract basicinformation related to organization, person, andartifact entities, drawing evidence from anywhere inthe text.
The Template Relation Task \[TR\] was anew task for MUC-7, involving extracting relationalinformation on generic domain-independent relationssuch as employee_of, manufacturer_of, andlocation_of relations.
The Scenario Template Task\[ST\] consisted of extracting prespecified eventinformation and relating that event information toparticular organization, person, or artifact entitiesinvolved in that event.
The final task was theCoreference Task, which involved capturinginformation on coreferring expressions, i.e.
allmentions of a given entity, including those tagged inNE and TE tasks.18 sites participated in MUC-7.
8 of 18were university research groups.
8 were from outsidethe United States.
Sites could participate in one ormore of the tasks.
12 sites participated in the NE task,9 in TE, 5 in TR, 5 in TE, and 5 in CO. No siteparticipated in all of the tasks.TRAIN ING AND DATA SETSThe corpus for MUC-7 consisted of subsetsof articles selected from a set of approximately158,000 articles from the New York Times NewsService (supplied by the LDC).
The evaluationepoch of the articles was January 1- September 11,1996.
Training and test sets were retrieved from thecorpus using the Managing Gigabytes text retrievalsystem using domain relevant erms.
2 sets of 100articles from the aircraft accident domain were usedfor preliminary training, including the dryrun.
2 setsof 100 articles from the launch event domain wereselected for the formal run after having beenbalanced for relevancy, type and source.The training data set consisted of trainingkeys for NE, TE, and TR tasks made available from apreliminary set of 100 articles; CO from apreliminary training set of 30 articles.
A formaltraining set of 100 articles and answer keys wereprovided for the ST task.The test set for the evaluation consisted of100 articles and answer keys for NE (from theFormal Training data set) and 100 articles and answerkeys for TE, TR, and ST. A subset of 30 articles andanswer keys were provided for the CO task.233FORMAL EVALUATIONThe evaluation began with the distributionof the formal run test for NE at the beginning ofMarch 1998.
The training set of articles, STguidelines and keys were made available at thebeginning of March and one month afterward the testset of articles was made available by electronictransfer from SAIC.
The deadline for completing theTE, TR, ST, and CO tasks was 6 April 1998 viaelectronic file transfer of system outputs to SAIC.Tests were run by individual participatingsites at their own facilities, following a written testprocedure.
Sites could conduct official "optional"tests in addition to the basic test and adaptive systemswere permitted.
Each site's system output was scoredaccording to the following categories with respect othe answer keys: correct, incorrect, missing, spurious,possible (affected by inclusion and omission ofoptional data) and actual.
Metrics included recall (ameasure of how much of the key's fills wereproduced in the response), precision (a measure ofhow much of the response fills are actually in thekey), F-measure (combining recall and precision intoone measure, and ERR (error per response fill).Additional supporting metrics of undergeneration,overgeneration, and substitution were provided aswell.
The scoring procedure was completelyautomatic.
Initial results for five tasks are presentedin Figure 1.independent relations that hold between theseelements.
The hope was that this would lead toperformance improvements onthe Scenario Templatetask.
The evaluation domain for MUC-7 wasconcerned with vehicle launch events.
The templateconsisted of one high-level event object with 7 slots,including two relational objects, three set fills, andtwo pointers to low-level objects.
The domainrepresented a change from person-oriented domain ofMUC-6 to a more artifact-oriented domain.While there have been important advancesin information extraction for named entity tasks andsubstantial improvement in the other tasks for whichthese MUC evaluations were developed, muchremains to be done to put production-levelinformation extraction systems on users' desks.
Weleave these breakthroughs to future researchers withthanks and recognition of the groundbreaking effortsof all the MUC participants hroughout the years.ACKNOWLEDGMENTSThe MUC-7 evaluation was organized byNRL with technical support from SAIC and financialsupport from the DARPA TIPSTER Program.
Otherevaluation support was provided by the MUC-7Program Committee, DARPA/ITO and the TipsterExecutive Committee.
The Linguistic DataConsortium provided all textual data for dataselection and also provided the training and test datato participating sites at minimal cost.In MUC-7, the new Template Relation taskwas an attempt o move from identifying domain-independent elements to100908070O 6O?
G 50?1 40 I=m3020100identifying domain-x?
cp.
.
.
.
.
.
.
.
.
.
N4E.
x A~ X,ik .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ X X0 10 20 30 40 50 60 70 80 90 100Recall?
NE\[\] TEA STxTRx COFigure 1: Overall recall and precision on all tasks234
