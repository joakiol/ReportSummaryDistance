Proc.
of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 49?54,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsAgent-based modeling of language evolutionTorvald Lekvam Bj?orn Gamb?ack Lars BungumDepartment of Computer and Information Science, Sem S?lands vei 7?9Norwegian University of Science and Technology, 7491 Trondheim, Norwaytorvald@lekvam.no {gamback,larsbun}@idi.ntnu.noAbstractAgent-based models of language evolutionhave received a lot of attention in the lasttwo decades.
Researchers wish to under-stand the origin of language, and aim tocompensate for the lacking empirical evi-dence by utilizing methods from computerscience and artificial life.
The paper looksat the main theories of language evolution:biological evolution, learning, and culturalevolution.
In particular, the Baldwin effectin a naming game model is elaborated onby describing a set of experimental simu-lations.
This is on-going work and ideasfor further investigating the social aspectsof language evolution are also discussed.1 IntroductionWhat is language?
It is interesting how we cantake a train of thought and transfer this into an-other person?s mind by pushing the air around us.Human language, this complex medium that dis-tinctly separates humans from animals, has baf-fled scientists for centuries.
While animals alsouse language, even with a degree of syntax (Kako,1999), spoken human language exhibits a vastlymore complex structure and spacious variation.To understand how language works ?
how itis used, its origin and fundamentals ?
our bestinformation sources are the languages alive (andsome extinct but documented ones).
Depending ondefinition, there are 6,000?8,000 languages world-wide today, showing extensive diversity of syntax,semantics, phonetics and morphology (Evans andLevinson, 2009).
Still, these represent perhapsonly 2% of all languages that have ever existed(Pagel, 2000).
As this is a rather small window, wewant to look back in time.
But there is a problemin linguistic history: our reconstruction techniquescan only take us back some 6,000 to 7,000 years.Beyond this point, researchers can only speculateon when and how human language evolved: eitheras a slowly proceeding process starting millionsof years (Ma) ago, e.g., 7 Ma ago with the firstappearance of cognitive capacity or 2.5 Ma agowith the first manufacture of stone implements; orthrough some radical change taking place about100 ka ago with the appearance of the modern hu-mans or 50?60 ka ago when they started leavingAfrica (Klein, 2008; Tattersall, 2010).The rest of this introduction covers some keyaspects of language evolution.
Section 2 then fo-cuses on computational models within the field,while Section 3 describes a specific naming gamemodel.
Finally, Section 4 discusses the results andsome ideas for future work.1.1 Theories of origin: the biological aspectThere are two main ideas in biological evolutionas to why humans developed the capacity to com-municate through speech.
The first states that lan-guage (or more precisely the ability to bear the fullstructure of language) came as an epiphenomenon,a by-product (spandrel) of an unrelated mutation.This theory assumes that a mental language fac-ulty could not by itself evolve by natural selection;there would simply be too many costly adaptationsfor it to be possible.
Thus there should exist an in-nate capacity in the form of a universal grammar(Chomsky, 1986), which can hold a finite numberof rules enabling us to carry any kind of language.According to the second idea, languageemerged in a strictly adaptational process (Pinkerand Bloom, 1990).
That is, that language evolu-tion can be explained by natural selection, in thesame way as the evolution of other complex traitslike echolocation in bats or stereopsis in monkeys.Both ideas ?
innate capacity vs natural selection?
have supporters, as well as standpoints thathold both aspects as important, but at different lev-els (Deacon, 2010; Christiansen and Kirby, 2003).491.2 Theories of origin: the cultural aspectBiology aside, the forces behind the emergence ofhuman language are not strictly genetic (and donot operate only on a phylogenetic time scale).Kirby (2002) argues that, in addition to biologicalevolution, there are two more complex adaptive(dynamical) systems influencing natural language;namely cultural evolution (on the glossogenetictime scale) and learning (which operates on a in-dividual level, on the ontogenetic time scale).In addition, there is the interesting Darwinianidea that cultural learning can guide biologicalevolution, a process known as the Baldwin effect(Baldwin, 1896; Simpson, 1953).
This theory ar-gues that culturally learned traits (e.g., a univer-sal understanding of grammar or a defense mech-anism against a predator) can assimilate into thegenetic makeup of a species.
Teaching each mem-ber in a population the same thing over and overagain comes with great cost (time, faulty learn-ing, genetic complexity), and the overall popula-tion saves a lot of energy if a learned trait wouldbecome innate.
On the other hand, there is a costof genetic assimilation as it can prohibit plastic-ity in future generations and make individuals lessadaptive to unstable environments.There has been much debate recently whetherlanguage is a result of the Baldwin effect or not(Evans and Levinson, 2009; Chater et al., 2009;Baronchelli et al., 2012, e.g.
), but questions, hypo-theses, and simulations fly in both directions.2 Language evolution and computationSince the 90s, there has been much work on sim-ulation of language evolution in bottom-up sys-tems with populations of autonomous agents.
Thefield is highly influenced by the work of Steels andKirby, respectively, and has been summarized andreviewed both by themselves and others (Steels,2011; Kirby, 2002; Gong and Shuai, 2013, e.g.
).Computational research in this field is limitedto modeling very simplified features of humanlanguage in isolation, such as strategies for nam-ing colors (Bleys and Steels, 2011; Puglisi et al.,2008), different aspects of morphology (Dale andLupyan, 2012), and similar.
This simplicity is im-portant to keep in mind, since it is conceivable thatcertain features of language can be highly influ-enced by other features in real life.A language game simulation (Steels, 1995) isa model where artificial agents interact with eachother in turn in order to reach a cooperative goal;to make up a shared language of some sort, allwhile minimizing their cognitive effort.
All agentsare to some degree given the cognitive ability tobear language, but not given any prior knowledgeof how language should look like or how consen-sus should unfold.
No centralized anchors are in-volved: a simulation is all self-organized.Agents are chosen (mostly at random) as hearerand speaker, and made to exchange an utteranceabout a certain arbitrary concept or meaning intheir environment.
If the agents use the same lan-guage (i.e., the utterance is understood by bothparties), the conversation is a success.
If thespeaker utters something unfamiliar to the hearer,the conversation is termed a failure.
If an agentwants to express some concept without having anyutterances for it, the agent is assumed to have theability to make one up and add this to its memory.While interpretation in real life is a complex af-fair, it is mostly assumed that there is a fairly directconnection between utterance and actual meaningin language game models (emotions and social sit-uations do not bias how language is interpreted).A simple language game normally is charac-terized by many synonyms spawning among theagents.
As agents commence spreading their ownutterances around, high-weighted words start to bepreferred.
Consensus is reached when all agentsknow the highest weighted word for each concept.Commonly, the agents aim to reach a single co-herent language, but the emergence of multilin-gualism has also been simulated (Lipowska, 2011;Roberts, 2012).
Cultural evolution can be capturedby horizontal communication between individualsin the same generation or vertical communicationfrom adults to children.
The latter typically lets theagents breed, age and die, with the iterated learn-ing model (Smith et al., 2003) being popular.A variety of language games exist, from sim-ple naming games, where the agents?
only topicconcerns one specific object (Lipowska, 2011), tomore cognitive grounding games (Steels and Loet-zsch, 2012).
There have also been studies on somemore complex types of interaction, such as spa-tial games (Spranger, 2013), factual descriptiongames (van Trijp, 2012) and action games (Steelsand Spranger, 2009), where the agent communi-cation is about objects in a physical environment,about real-world events, and about motoric behav-iors, respectively.503 The Baldwin effect in a naming gameSeveral researchers have created simulations to in-vestigate the Baldwin effect, starting with Hintonand Nowlan (1987).
Cangelosi and Parisi (2002)simulate agents who evolve a simple grammaticallanguage in order to survive in a world filled withedible and poisonous mushrooms.
Munroe andCangelosi (2002) used this model to pursue theBaldwin effect, with partially blind agents initiallyhaving to learn features of edible mushrooms, butwith the learned abilities getting more and moreassimilated into the genome over the generations.Chater et al.
(2009) argue that only stable parts oflanguage may assimilate into the genetic makeup,while variation within the linguistic environmentis too unstable to be a target of natural selection.Watanabe et al.
(2008) use a similar model, but incontrast state that genetic assimilation not neces-sarily requires a stable linguistic environment.Lipowska (2011) has pursued the Baldwin ef-fect in a simple naming game model with the in-tention of mixing up a language game in a simu-lation that incorporates both learning, cultural andbiological evolution.
The model places a set ofagents in a square lattice of a linear size L, whereevery agent is allowed ?
by a given probability p?
to communicate with a random neighbor.At each time step, a random agent is chosen andp initially decides whether the agent is allowed tocommunicate or will face a ?population update?.Every agent has an internal lexicon of N wordswith associated weights (wj: 1 ?
j ?
N ).
When-ever a chosen speaker is to utter a word, the agentselects a word i from its lexicon with the probabil-ity wi/?Nj=1wj.
If the lexicon is empty (N = 0),a word is made up.
A random neighbor in the lat-tice is then chosen as the hearer.
If both agentsknow the uttered word, the dialog is deemed asuccess, and if not, a failure.
Upon success, bothagents increase the uttered word?s weight in theirlexica by a learning ability variable.
Each agent kis equipped with such a variable l (0 < lk< 1).This learning ability is meant to, in its simplicity,reflect the genetic assimilation.Instead of engaging in communication, the cho-sen agent is occasionally updated, by a probability1?
p. Agents die or survive with a probability pswhich is given by an equation that takes into ac-count age, knowledge (lexicon weights in respectto the population?s average weights), and simula-tion arguments.
If the agent has a high-weightedlexicon and is young of age, and therefore survivesat a given time step, the agent is allowed to breedif there are empty spaces in its neighborhood.All in all, each time step can terminate witheight different scenarios: in addition to the twocommunication scenarios (success or failure), thescenario where the agent dies, as well as the onewhere the agents lives but only has non-emptyneighbors (so that no change is possible), there arefour possibilities for breeding.
If the agent breeds,the off-spring either inherit the parent?s learningability or gain a new learning ability, with a proba-bility pm.
With the same mutation probability, theoff-spring also either gains a new word or inheritsthe parent?s highest-weight word.This model was implemented with the aim toreproduce Lipowska?s results.
She argues that hermodel is fairly robust to both population size andher given arguments; however, our experimentsdo not support this: as the Baldwin effect unfold,it does not follow the same abrupt course as inLipowska?s model.
This could be due to some as-sumptions that had to be made, since Lipowska(2011), for instance, presents no details on howage is calculated.
We thus assume that every timean agent is allowed to communicate, its age getsincremented.
Another possibility could be to in-crement every agent?s age at every time step, sothat agents get older even if they do not commu-nicate.
Furthermore, the initial values for learn-ability are not clearly stated.
Lipowska uses sev-eral different values in her analysis.
We have used0.5, which makes a decrease in learnability a partof the evolutionary search space as well.Simulations with parameters similar to thoseused by Lipowska (2011) [iterations = 200, 000,mutationchance= 0.01, L = 25, p = 0.4, l = 0.5],produce results as in Figure 1, showing the highestweighted word per agent after 50k and 150k timesteps, with each agent being a dot in a ?heat map?
;black dots indicate dead agents (empty space).The number of groups are reduced over time, andtheir sizes grow, as more agents agree on a lex-icon and as favorable mutations spread throughthe population, (as indicated by agent learnability;Figure 2).
Even after 200k iterations, consensus isnot reached (which it was in Lipowska?s simula-tion), but the agent population agrees on one wordif the simulation is allowed to run further.
It is nat-ural to assume that the difference lays in the detailsof how age is calculated, as noted above.51Figure 1: Ca 16 different words dominate the pop-ulation at iteration 50k and nine at iteration 150k.Figure 2: Mutations favoring learnability at itera-tion 50k spread substantially by iteration 150k.Diverting from Lipowska?s parameters andskewing towards faster turnover (higher mutationrate, higher possibility of survival with richer lex-icon/higher age, etc.
), gives behavior similar tohers, with faster and more abrupt genetic assim-ilation, as shown Figure 3.
The upper line in thefigure represents the fraction of agents alive in thelattice.
It is initially fully populated, but the popu-lation decreases with time and balances at a pointwhere death and birth are equally tensioned.Agents with higher learnability tend to livelonger, and the lower graph in Figure 3 shows theaverage learnability in the population.
It is roughlysigmoid (S-shaped; cf.
Lipowska?s experiment) asa result of slow mutation rate in the first phase,followed by a phase with rapid mutation rate (ca100k?170k) as the learnability also gets inherited,and decreasing rate towards the end when mu-tations are more likely to ruin agent learnability(when the learning ability l is at its upper limit).As can be seen in Figure 4, the agents rapidly getto a stable weighted lexicon before the Baldwineffect shows itself around time step 100k.As mentioned, Lipowska?s model did not reflectthe robustness argued in her paper: for other val-ues of p, the number of empty spots in the popu-lation lattice starts to diverge substantially, and forsome values all agents simply die.
As populationsizes vary, the number of iterations must also beadjusted to get similar results.
If not, the agentswill not reach the same population turn-over asFigure 3: Fraction of agents alive in the lattice andaverage learnability in the population (s-shaped).Figure 4: Average sum of weights in agent lexica.for smaller population sizes since only one agentmay be updated per iteration.
Lipowska (2011)compensated with higher mutation rate on simu-lations with different population sizes; however,these could be two variables somewhat more inde-pendent of each other.
The model would have beenmuch more stable if it contained aspects of a typi-cal genetic algorithm, where agents are allowed tointeract freely within generations.
This way, themodel could be acting more upon natural selec-tion (and in search of the Baldwin effect), insteadof relying on well-chosen parameters to work.4 Discussion and future workLanguage is a complex adaptive system with nu-merous variables to consider.
Thus we must makea number of assumptions when studying languageand its evolution, and can only investigate certainaspects at a time through simplifications and ab-stractions.
As this paper has concentrated on theagent-based models of the field, many studies re-flecting such other aspects had to be left out.In addition, there has lately been a lot of workstudying small adjustments to the agent-basedmodels, in order to make them more realistic by,for example, having multiple hearers in a lan-guage game conversations (Li et al., 2013), dif-ferent topologies (Lei et al., 2010; Lipowska andLipowski, 2012), and more heterogeneous popula-tions (Gong et al., 2006).52In general, though, simulations on languageevolution tend to have relatively small and fixedsizes (Baronchelli et al., 2006; Vogt, 2007) ?
andfew studies seem to take social dynamics (Gong etal., 2008; Kalampokis et al., 2007) or geographyinto account (Patriarca and Heinsalu, 2009).Further work is still needed to make existingmodels more realistic and to analyze relations be-tween different models (e.g., by combining them).Biological evolution could be studied with moreflexible (or plastic) neural networks.
Cultural evo-lution could be investigated under more realisticgeographical and demographical influence, whilelearning could be analyzed even further in light ofsocial dynamics, as different linguistic phenom-ena unfold.
Quillinan (2006) presented a modelconcerning how a network of social relationshipscould evolve with language traits.
This modelcould be taken further in combination with exist-ing language games or it could be used to showhow language responds to an exposure of continu-ous change in a complex social network.Notably, many present models have a ratherna?
?ve way of selecting cultural parents, and agenetic algorithm for giving fitness to agents interms of having (assimilated) the best strategiesfor learning (e.g., memory efficiency), social con-ventions (e.g., emotions, popularity), and/or sim-ple or more advanced grammar could be explored.A particular path we aim to pursue is to study alanguage game with a simple grammar under so-cial influence (e.g., with populations in differentfixed and non-fixed graphs, with multiple hearers),contained within a genetic algorithm.
In such asetting, the agents must come up with strategiesfor spreading and learning new languages, andneed to develop fault-tolerant models for speakingwith close and distant neighbors.
This could be arobust model where a typical language game couldbe examined, in respect to both biological and cul-tural evolution, with a more realistic perspective.AcknowledgmentsWe would like thank the three anonymous review-ers for several very useful comments.
Thanks alsoto Keith Downing for providing feedback on workunderlying this article.The third author is supported by a grant from theNorwegian University of Science and Technology.Part of this work was funded by the PRESEMTproject (EC grant number FP7-ICT-4-248307).ReferencesJames Mark Baldwin.
1896.
A new factor in evolution.The American Naturalist, 30(354):441?451.Andrea Baronchelli, Maddalena Felici, Vittorio Loreto,Emanuele Caglioti, and Luc Steels.
2006.
Sharptransition towards shared vocabularies in multi-agent systems.
Journal of Statistical Mechanics:Theory and Experiment, 2006(06):P06014.Andrea Baronchelli, Nick Chater, Romualdo Pastor-Satorras, and Morten H. Christiansen.
2012.
Thebiological origin of linguistic diversity.
PLoS ONE,7(10):e48029.Joris Bleys and Luc Steels.
2011.
Linguistic selec-tion of language strategies.
In G. Kampis, I. Kar-sai, and E. Szathm?ary, editors, Advances in ArtificialLife.
Darwin Meets von Neumann, volume 2, pages150?157.
Springer.Angelo Cangelosi and Domenico Parisi.
2002.
Com-puter simulation: A new scientific approach to thestudy of language evolution.
In Angelo Cangelosiand Domenico Parisi, editors, Simulating the Evolu-tion of Language, chapter 1, pages 3?28.
Springer,London.Nick Chater, Florencia Reali, and Morten H Chris-tiansen.
2009.
Restrictions on biological adaptationin language evolution.
Proceedings of the NationalAcademy of Sciences, 106(4):1015?1020.Noam Chomsky.
1986.
Knowledge of language: Itsnature, origins, and use.
Greenwood.Morten H. Christiansen and Simon Kirby.
2003.Language evolution: consensus and controversies.TRENDS in Cognitive Sciences, 7(7):300?307.Rick Dale and Gary Lupyan.
2012.
Understandingthe origins of morphological diversity: the linguis-tic niche hypothesis.
Advances in Complex Systems,15(03n04):1150017.Terrence W. Deacon.
2010.
A role for relaxed se-lection in the evolution of the language capacity.Proceedings of the National Academy of Sciences,107(Supplement 2):9000?9006.Nicholas Evans and Stephen C. Levinson.
2009.
Themyth of language universals: Language diversityand its importance for cognitive science.
Behavioraland Brain Sciences, 32(05):429?448.Tao Gong and Lan Shuai.
2013.
Computer simulationas a scientific approach in evolutionary linguistics.Language Sciences, 40:12?23.Tao Gong, James W. Minett, and William S-Y Wang.2006.
Language origin and the effects of individ-uals popularity.
In Proceedings of the 2006 IEEECongress on Evolutionary Computation, pages 999?1006, Vancouver, British Columbia, Jul.
IEEE.53Tao Gong, James W. Minett, and William S-Y Wang.2008.
Exploring social structure effect on languageevolution based on a computational model.
Connec-tion Science, 20(2-3):135?153.Geoffrey E Hinton and Steven J Nowlan.
1987.
Howlearning can guide evolution.
Complex systems,1(3):495?502.Edward Kako.
1999.
Elements of syntax in the sys-tems of three language-trained animals.
AnimalLearning & Behavior, 27(1):1?14.Alkiviadis Kalampokis, Kosmas Kosmidis, and PanosArgyrakis.
2007.
Evolution of vocabulary on scale-free and random networks.
Physica A: StatisticalMechanics and its Applications, 379(2):665 ?
671.Simon Kirby.
2002.
Natural language from artificiallife.
Artificial Life, 8(2):185?215.Richard G. Klein.
2008.
Out of Africa and the evo-lution of human behavior.
Evolutionary Anthropol-ogy: Issues, News, and Reviews, 17(6):267?281.Chuang Lei, Jianyuan Jia, Te Wu, and Long Wang.2010.
Coevolution with weights of names in struc-tured language games.
Physica A: Statistical Me-chanics and its Applications, 389(24):5628?5634.Bing Li, Guanrong Chen, and Tommy W.S.
Chow.2013.
Naming game with multiple hearers.
Com-munications in Nonlinear Science and NumericalSimulation, 18(5):1214?1228.Dorota Lipowska and Adam Lipowski.
2012.
Naminggame on adaptive weighted networks.
Artificial Life,18(3):311?323.Dorota Lipowska.
2011.
Naming game and compu-tational modelling of language evolution.
Compu-tational Methods in Science and Technology, 17(1?2):41?51.Steve Munroe and Angelo Cangelosi.
2002.
Learningand the evolution of language: the role of culturalvariation and learning costs in the Baldwin effect.Artificial Life, 8(4):311?339.Mark Pagel.
2000.
The history, rate and pattern ofworld linguistic evolution.
In Ch.
Knight, J.R. Hur-ford, and M. Studdert-Kennedy, editors, The Evo-lutionary Emergence of Language: Social Func-tion and the Origins of Linguistic Form, chapter 22,pages 391?416.
Cambridge University Press.Marco Patriarca and Els Heinsalu.
2009.
Influenceof geography on language competition.
Physica A:Statistical Mechanics and its Applications, 388(2?3):174?186.Steven Pinker and Paul Bloom.
1990.
Natural lan-guage and natural selection.
Behavioral and BrainSciences, 13:707?784.Andrea Puglisi, Andrea Baronchelli, and VittorioLoreto.
2008.
Cultural route to the emergence oflinguistic categories.
Proceedings of the NationalAcademy of Sciences, 105(23):7936?7940.Justin Quillinan.
2006.
Social networks and culturaltransmission.
Master of Science Thesis, Schoolof Philosophy, Psychology and Language Sciences,University of Edinburgh, Edinburgh, Scotland, Aug.Sean Geraint Roberts.
2012.
An evolutionary ap-proach to bilingualism.
Ph.D. thesis, School of Phi-losophy, Psychology and Language Sciences, Uni-versity of Edinburgh, Edinburgh, Scotland, Oct.George Gaylord Simpson.
1953.
The Baldwin effect.Evolution, 7(2):110?117.Kenny Smith, Simon Kirby, and Henry Brighton.2003.
Iterated learning: A framework for the emer-gence of language.
Artificial Life, 9(4):371?386.Michael Spranger.
2013.
Evolving grounded spa-tial language strategies.
KI-K?unstliche Intelligenz,27(2):1?10.Luc Steels and Martin Loetzsch.
2012.
The groundednaming game.
In L. Steels, editor, Experiments inCultural Language Evolution, pages 41?59.
JohnBenjamins.Luc Steels and Michael Spranger.
2009.
How ex-perience of the body shapes language about space.In Proceedings of the 21st International Joint Con-ference on Artificial Intelligence, pages 14?19,Pasadena, California, Jul.
IJCAI.Luc Steels.
1995.
A self-organizing spatial vocabulary.Artificial Life, 2(3):319?332.Luc Steels.
2011.
Modeling the cultural evolution oflanguage.
Physics of Life Reviews, 8(4):339?356.Ian Tattersall.
2010.
Human evolution and cognition.Theory in Biosciences, 129(2?3):193?201.Remi van Trijp.
2012.
The evolution of case systemsfor marking event structure.
In L. Steels, editor,Experiments in Cultural Language Evolution, pages169?205.
John Benjamins.Paul Vogt.
2007.
Group size effects on the emer-gence of compositional structures in language.
InF.
Almeida e Costa, L.M.
Rocha, E. Costa, I Har-vey, and A. Coutinho, editors, Advances in Artifi-cial Life: Proceedings of the 9th European Confer-ence (ECAL 2007), pages 405?414, Lisbon, Portu-gal, Sep. Springer.Yusuke Watanabe, Reiji Suzuki, and Takaya Arita.2008.
Language evolution and the Baldwin effect.Artificial Life and Robotics, 12(1-2):65?69.54
