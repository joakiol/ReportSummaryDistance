Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616?625,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAugmenting Translation Models with Simulated Acoustic Confusions forImproved Spoken Language TranslationYulia Tsvetkov Florian Metze Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213; U.S.A.{ytsvetko, fmetze, cdyer}@cs.cmu.eduAbstractWe propose a novel technique for adaptingtext-based statistical machine translationto deal with input from automatic speechrecognition in spoken language translationtasks.
We simulate likely misrecognitionerrors using only a source language pro-nunciation dictionary and language model(i.e., without an acoustic model), and usethese to augment the phrase table of a stan-dard MT system.
The augmented sys-tem can thus recover from recognition er-rors during decoding using synthesizedphrases.
Using the outputs of five differ-ent English ASR systems as input, we findconsistent and significant improvements intranslation quality.
Our proposed tech-nique can also be used in conjunction withlattices as ASR output, leading to furtherimprovements.1 IntroductionSpoken language translation (SLT) systems gen-erally consist of two components: (i) an auto-matic speech recognition (ASR) system that tran-scribes source language utterances and (ii) a ma-chine translation (MT) system that translates thetranscriptions into the target language.
These twocomponents are usually developed independentlyand then combined and integrated (Ney, 1999;Matusov et al., 2006; Casacuberta et al., 2008;Zhou, 2013; He and Deng, 2013).While this architecture is attractive since it re-lies only on components that are independentlyuseful, such systems face several challenges.
First,spoken language tends to be quite different fromthe highly edited parallel texts that are available totrain translation systems.
For example, disfluen-cies, such as repeated words or phrases, restarts,and revisions of content, are frequent in spon-taneous speech,1while these are usually absentin written texts.
In addition, ASR outputs typi-cally lack explicit segmentation into sentences, aswell as reliable casing and punctuation informa-tion, which are crucial for MT and other text-basedlanguage processing applications (Ostendorf et al.,2008).
Second, ASR systems are imperfect andmake recognition errors.
Even high quality sys-tems make recognition errors, especially in acous-tically similar words with similar language modelscores, for example morphological substitutionslike confusing bare stem and past tense forms, andin high-frequency short words (function words)which often lack both disambiguating context andare subject to reduced pronunciations (Goldwateret al., 2010).One would expect that training an MT systemon ASR outputs (rather than the usual written-style texts) would improve matters.
Unfortunately,there are few corpora of speech paired with texttranslations into a second language that could beused for this purpose.
This has been an incentiveto various MT adaptation approaches and devel-opment of speech-input MT systems.
MT adapta-tion has been done via input text pre-processing,by transformation of spoken language (ASR out-put) into written language (MT input) (Peitz etal., 2012; Xu et al., 2012); via decoding ASR n-best lists (Quan et al., 2005), or confusion net-works (Bertoldi et al., 2007; Casacuberta et al.,2008), or lattices (Dyer et al., 2008; Onishi et al.,2010); via additional translation features captur-ing acoustic information (Zhang et al., 2004); andwith methods that follow a paradigm of unified de-coding (Zhou et al., 2007; Zhou, 2013).
In linewith the previous research, we too adapt a standardMT system to a speech-input MT, but by alteringthe translation model itself so it is better able to1Disfluencies constitute about 6% of word tokens in spon-taneous speech, not including silent pauses (Tree, 1995; Kasland Mahl, 1965)616deal with ASR output (Callison-Burch et al., 2006;Tsvetkov et al., 2013a).We address speech translation in a resource-deficient scenario, specifically, adapting MT sys-tems to SLT when ASR is unavailable.
We aug-ment a discriminative set that translation modelsrescore with synthetic translation options.
Theseautomatically generated translation rules (hence-forth synthetic phrases) are noisy variants of ob-served translation rules with simulated plausiblespeech recognition errors (?2).
To simulate ASRerrors we generate acoustically and distribution-ally similar phrases to a source (English) phrasewith a phonologically-motivated algorithm (?4).Likely phonetic substitutions are learned with anunsupervised algorithm that produces clusters ofsimilar phones (?3).
We show that MT systemsaugmented with synthetic phrases increase thecoverage of input sequences that can be translated,and yield significant improvement in the quality oftranslated speech (?6).This work makes several contributions.
Primaryis our framework to adapt MT to SLT by popu-lating translation models with synthetic phrases.2Second, we propose a novel method to generateacoustic confusions that are likely to be encoun-tered in ASR transcription hypotheses.
Third, wedevise simple and effective phone clustering al-gorithm.
All aforementioned algorithms work ina low-resource scenario, without recourse to au-dio data, speech transcripts, or ASR outputs: ourmethod to predict likely recognition errors usesphonological rather than acoustic information anddoes not depend on a specific ASR system.
Sinceour source language is English, we operate on aphone level and employ a pronunciation dictionaryand a language model, but the algorithm can inprinciple be applied without pronunciation dictio-nary for languages with a phonemic orthography.2 MethodologyWe adopt a standard ASR-MT cascading approachand then augment translation models with syn-thetic phrases.
Our proposed system architectureis depicted in Figure 1.Synthetic phrases are generated from entries inthe original translation model?phrase translation2We augment phrase tables only with synthetic phrasesthat capture simulated ASR errors, the methodology that weadvocate, however, is applicable to many problems in transla-tion (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneauet al., 2013).ASRASR LM(source lang.
)AcousticModelcat ???
?MTMT LM(target lang.
)TM+TM'Translation Modelaugmented withsimulated ASR errorsFigure 1: SLT architecture: ASR and MT aretrained independently and then cascaded.
We im-prove SLT by populating MT translation modelwith synthetic phrases.
Each synthetic phrase isa variant of an original phrase pair with simulatedASR errors on the source side.pairs acquired from parallel data.
From a sourceside of an original phrase pair we generate list ofits plausible misrecognition variants (pseudo-ASRoutputs with recognition errors) and add them asa source side of a synthetic phrase.
For k-bestsimulated ASR outputs we construct k syntheticphrases: a simulated ASR output in the sourceside is coupled with its translation?an original tar-get phrase (identical for all k phrases).
Syntheticphrases are annotated with five standard phrasaltranslation features (forward and reverse phraseand lexical translation probabilities and phrasepenalty); these were found in the original phraseand remain unchanged.
In addition, we add threenew features to all phrase pairs, both synthetic andoriginal.
First, we add a boolean feature indi-cating the origin of a phrase: synthetic or origi-nal.
Two other features correspond to an ASR lan-guage model score of the source side.
One is LMscore of the synthetic phrase, another is a scoreof a phrase from which the source side was gener-ated.
We then append synthetic phrases to a phrasetable: k synthetic phrases for each original phrasepair, with eight features attached to each phrase.We show synthetic phrases example in Figure 2.3 Acoustically confusable phonesThe phonetic context of a given phone affects itsacoustic realization, and a variability in a produc-tion of the same phone is possible depending oncoarticulation with its neighboring phones.3In ad-dition, there are phonotactic constraints that canrestrict allowed sequences of phones.
English hasstrong constraints on sequences of consonants; thesequence [zdr], for example, cannot be a legal En-3These are the reasons why in context-dependent acous-tic modeling different HMM models are trained for differentcontexts.617SourcephraseTargetphraseOriginal phrasetranslation featuresSyntheticindicatorSyntheticLM scoreOriginalLM scoretells the story raconte l?histoire f1, f2, f3, f4, f50 3.9?10?33.9?10?3tell their story raconte l?histoire f1, f2, f3, f4, f51 5.9?10?33.9?10?3tells a story raconte l?histoire f1, f2, f3, f4, f51 2.2?10?33.9?10?3tell the story raconte l?histoire f1, f2, f3, f4, f51 1.7?10?33.9?10?3tell a story raconte l?histoire f1, f2, f3, f4, f51 1.3?10?33.9?10?3tell that story raconte l?histoire f1, f2, f3, f4, f51 1.0?10?33.9?10?3tell their stories raconte l?histoire f1, f2, f3, f4, f51 0.9?10?33.9?10?3tells the stories raconte l?histoire f1, f2, f3, f4, f51 0.8?10?33.9?10?3tells her story raconte l?histoire f1, f2, f3, f4, f51 0.7?10?33.9?10?3chelsea star raconte l?histoire f1, f2, f3, f4, f51 0.5?10?33.9?10?3Figure 2: Example of acoustically confusable synthetic phrases.
Phrases were synthesized from theoriginal phrase pair in Row 1 by generating acoustically similar phrases for the English phrase tells thestory.
All phrases have the same (target) French translation me raconte l?histoire and the same five basicphrase-based translation rule features.
To these, three additional features are added: a synthetic phraseindicator, the source language LM score of the source phrase, and the source language LM score of asource phrase in the original phrase pair.TleftTrightTWleftWIHright.
.
.T P (T |WIH) .
.
.W P (W |T ) .
.
.IH P (IH|T ) P (IH|TW ) .
.
.ER P (ER|T ) .
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 3: A fragment of the co-occurrence matrixfor phone sequence [T W IH T ER].
Rows corre-spond to phones; columns correspond to left/rightcontext phones of lengths one and two.glish syllable onset (Jurafsky and Martin, 2000).Motivated by the constraining effect of contexton phonetic distribution, we cluster phones using adistance-based measure.
To do so, we build a vec-tor space model representation of each phone bycreating a co-occurrence matrix from a corpus ofphonetic forms where each row represents a phoneand columns indicate the contextual phones.
Wetake into account left/right context windows oflengths one and two.
A cell rp,cin the vector spacedictionary matrix represents phone p and context cusing the empirical relative frequency f(p | c), asestimated from a pronunciation dictionary.
Fig-ure 3 shows a fragment of the co-occurrence ma-trix constructed from a dictionary containing justthe pronunciation of Twitter ?
[T W IH T ER].Under this representation, the similarity ofphones can be easily quantified by measuring theirdistance in the vector space, the cosine of the anglebetween them:Sim(p1, p2) =p1?p2||p1||?||p2||Armed with this similarity function, we apply theK-means algorithm4to partition the phones intodisjoint sets.4 Plausible misrecognition variantsFor an input English sequence we generate top-kpseudo-ASR outputs, that are added as a sourceside of a synthetic phrase.
Every ASR output thatwe simulate is a plausible misrecognition that hastwo distinguishing characteristics: it is acousti-cally and linguistically confusable with the inputsequence.
Former corresponds to phonetic simi-larity and latter to distributional similarity of thesetwo phrases in corpus.Given a reference string?a word or sequenceof words w in the source language, we generatek-best hypotheses v. This can be modeled as aweighted finite state transducer:{v} = G ?D?1?
T ?D ?
{w} (1)where?
D maps from words to pronunciations?
T is a phone confusion transducer?
D?1maps from pronunciations to words?
G is an ASR language modelD maps words to their phonetic representation5,or multiple representations for words with several4Value of K=12 was determined empirically.5Using the CMU pronounciation dictionaryhttp://www.speech.cs.cmu.edu/cgi-bin/cmudict618pronunciation variants.
To create a phone con-fusion transducer T maps source to target phonesequences by performing a number of edit opera-tions.
Allowed edits are:?
Deletion of a consonant (mapping to ).?
Doubling of a vowel.?
Insertion of one or two phones in the end of asequence from the list of possible suffixes: S(-s), IX NG (-ing), D (-ed).?
Substitution of a phone by an acousti-cally similar phone.
The clusters of thesimilar phones are {Z,S}, {XL,L,R},{AA,AO,EY,UH}, {AXR,AX}, {XN,XM},{P,B,F}, {DH,CH,ZH,T,SH}, {OY,AE},{IY,AY,OW}, {EH,AH,IH,AW,ER,UW}.The phone clustering algorithm that pro-duced these is detailed in the previoussection.After a series of edit operations, D?1trans-ducer maps new phonetic sequences from pronun-ciations to n-grams of words.
The k-best variantsresulting from the weighted composition are thek-best plausible misrecognitions.One important property of this method is that itmaps words in decoding vocabulary (41,487 typesare possible inputs to transducer D) into CMUdictionary which is substantially larger (141,304types are possible outputs of transducer D?1).This allows to generate out-of-vocabulary (OOV)words and phrases, which are not only recogni-tion errors, but also plausible variants of differentsource phrases that can be translated to one tar-get phrase, e.g., verb past tense forms or functionwords.Consider a bigram tells the from our syntheticphrase example in Figure 2.
We first obtainits phonetic representation [T EH L Z] [DH IY],and then a sequence of possible edit operationsis Substitute(T, CH), Substitute(Z, S), Delete(DH)and translation of phonetic sequence [CH EH L SIY] back to words brings us to chelsea.
See Fig-ure 4 for visualization.5 Experimental setupsTo establish the effectiveness and ro-bustness of our approach, we conductedtwo sets of experiments?expASR andexpMultilingual?with transcribed andtells the  T EH L Z DH IYchelsea CH EH L S    IYFigure 4: Pseudo-ASR output generation exam-ple for a bigram tells the.
Phonetic edits areSubstitute(T, CH), Substitute(Z, S), Delete(DH).translated TED talks (Cettolo et al., 2012b).6En-glish is the source language in all the experiments.In expASR we used tst2011?the official testset of the SLT track of the IWSLT 2011 evalu-ation campaign on the English-French languagepair (Federico et al., 2011).7This test set com-prises reference transcriptions of 8 talks (approx-imately 1.1h of speech, segmented to 818 utter-ances), 1-best hypotheses from five different ASRsystems, a ROVER combination of four systems(Fiscus, 1997), and three sets of lattices producedby the participants of the IWSLT 2011 ASR track.In this set of experiments we compare baselinesystems performance to a performance of systemsaugmented with synthetic phrases on (1) referencetranscriptions, (2) 1-best hypotheses from all re-leased ASR systems, and (3) a set of ASR latticesproduced by FBK (Ruiz et al., 2011).8Experi-ments with individual systems are aimed to val-idate that MT augmented with synthetic phrasescan better translate ASR outputs with recogni-tion errors and sequences that were not observedin the MT training data.
Consistency in perfor-mance across different ASRs is expected if our ap-proach to generate plausible misrecognition vari-ants is universal, rather than biased to a specificsystem.
Comparison of 1-best system with syn-thetic phrases to lattice decoding setup withoutsynthetic phrases should demonstrate whether n-best plausible misrecognition variants that we gen-erate assemble multiple paths through a lattice.The purpose of expMultilingual is toshow that translation improvement is consistentacross different target languages.
This multilin-gual experiment is interesting because typologi-cally different languages pose different challengesto translation (degree and locality of reordering,morphological richness, etc.).
By showing thatwe improve results across languages (even with6http://www.ted.com/7http://iwslt2011.org/doku.php?id=06_evaluation#slt_track_english_to_french8Pruning threshold for lattices is 0.08.619the same underlying ASR system), we show thatour technique is robust to the different demandsthat languages place on the translation model.
Wecould not find any publicly available multilingualdata sets of the translated speech,9therefore weconstructed a new test set.We use our in-house speech recognizer andevaluate on locally crawled and pre-processedTED audio and text data.
We build SLT systemsfor five target languages: French, German, Rus-sian, Hebrew, and Hindi.
Consequently, our testsystems are diverse typologically and trained oncorpora of different sizes.
We sample a test set ofseven talks, representing approximately two hoursof English speech, for which we have translationsto all five languages;10talks are listed in Table 1.Due to segmentation differences in the releasedTED (text) corpora and then several automaticpreprocessing stages, numbers of sentences forthe same talks are not identical across languages.Therefore, we select English-French system as anoracle (this is the largest dataset), and first align itwith the ASR output.
Then, we filter out test setsfor non-French MT systems, to retain only sen-tence pairs that are included in the English-Frenchtest set.
Thus, our test sets for non-French MTsystems are smaller, and source-side sentences inthe English-French MT is a superset of source-sidesentences in all five languages.
Training, tuning,and test corpora sizes are listed in Table 2.
Sametraining and development sets were used in bothexpASR and expMultilingual experiments.Training Dev TestEN?FR 140,816 2,521 843EN?DE 130,010 2,373 501EN?RU 117,638 2,380 735EN?HE 135,366 2,501 540EN?HI 126,117 2,000 300Table 2: Number of sentences in training, dev andexpMultilingual test corpora.5.1 ASRIn the expMultilingual set of experiments,we employ the JANUS Recognition Toolkit thatfeatures the IBIS single pass decoder (Soltau et9After we conducted our experiments, a new multilingualparallel corpus of translated speech was released for SLTtrack of IWSLT 2013 Evaluation Campaign, however, thisdata set does not include Russian, Hebrew and Hindi, whichare a subject of this research.10Since TED translation is a voluntary effort, not all talksare available in all languages.al., 2001).
The acoustic model is maximumlikelihood system, no speaker adaptation or dis-criminative training applied.
The acoustic modeltraining data is 186h of Broadcast News-styledata.
5-gram language model with modifiedKneser-Ney smoothing is trained with the SRILMtoolkit (Stolcke, 2002) on the EPPS, TED, News-Commentary, and the Gigaword corpora.
TheBroadcast News test set contains 4h of audio; weobtain 25.6% word error rate (WER) on this testset.We segment the TED test audio by the times-tamps of transcripts appearance on the screen.Then, we manually detect and discard noisy hy-potheses around segmentation boundaries, andmanually align the remaining hypotheses withthe references which are the source side of theEnglish-French MT test set.
The resulting testset of 843 hypotheses, sentence aligned with tran-scripts, yields 30.7% WER.
Higher error rates (rel-atively to the Broadcast News baseline) can beexplained by the idiosyncratic nature of the TEDgenre, and the fact that our ASR system was nottrained on the TED data.For the expASR set of experiments the ASRoutputs and lattices in standard lattice format(SLF) were produces by the participants of IWSLT2011 evaluation campaign.5.2 MTWe train and test MT using the TED corpora inall five languages.
For French, German and Rus-sian we use sentence-aligned training and develop-ment sets (without our test talks) released for theIWSLT 2012 evaluation campaign (Cettolo et al.,2012a); we split Hebrew and Hindi to training anddevelopment respectively.11We split Hebrew andHindi to sentences with simple heuristics, and thensentence-align with the Microsoft Bilingual Sen-tence Aligner (Moore, 2002).
Punctuation markswere removed, corpora were lowercased, and tok-enized using the cdec scripts (Dyer et al., 2010).In all MT experiments, both for sentence andlattice translation, we employ the Moses toolkit(Koehn et al., 2007), implementing the phrase-based statistical MT model (Koehn et al., 2003)and optimize parameters with MERT (Och, 2003).Target language 3-gram Kneser-Ney smoothed11Since TED Hindi corpus is very small (only about 6Ksentences) we augment it with additional parallel data (Bojaret al., 2010); however, this improved Hindi system qualityonly marginally, probably owing to domain mismatch.620TED id TED talk1 Al Gore, 15 Ways to Avert a Climate Crisis, 200639 Aubrey de Grey: A roadmap to end aging, 2005142 Alan Russell: The potential of regenerative medicine, 2006228 Alan Kay shares a powerful idea about ideas, 2007248 Alisa Miller: The news about the news, 2008451 Bill Gates: Mosquitos, malaria and education, 2009535 Al Gore warns on latest climate trends, 2009Table 1: Test set of TED talks.language models are trained on the training partof each corpus.
Results are reported using case-insensitive BLEU with a single reference and nopunctuation (Papineni et al., 2002).
To verifythat our improvements are consistent and are notjust an effect of optimizer instability (Clark et al.,2011), we train three systems for each MT setup.Statistical significance is measured with the Mul-tEval toolkit.12Reported BLEU scores are aver-aged over three systems.In MT adaptation experiments we augmentbaseline phrase tables with synthetic phrases.
Foreach entry in the original phrase table we add (atmost) five13best acoustic confusions, detailed inSection 4.
Table 3 contains sizes of phrase tables,original and augmented with synthetic phrases.Original SyntheticEN?FR 4,118,702 24,140,004EN?DE 2,531,556 14,807,308EN?RU 1,835,553 10,743,818EN?HE 2,169,397 12,692,641EN?HI 478,281 2,674,025Table 3: Sizes of phrase tables from the baselinesystems, and phrase tables with synthetic phrases.6 Experiments6.1 expASRWe first measure the phrasal coverage of recog-nition errors that our technique is able to predict.We compute a number of 1- and 2-gram phrasesin ASR hypotheses from the tst2011 that arenot in the references: these are ASR errors.
Then,we compare their OOV rate in the English-Frenchphrase tables, original vs. synthetic.
The pur-pose of synthetic phrases is to capture misrecog-nized sequences, ergo, reduction in OOV rate of12https://github.com/jhclark/multeval13This threshold is of course rather arbitrary.
In future ex-periments we are planning to conduct an in-depth investiga-tion of the threshold value, based on ASR LM score and pho-netic distance from the original phrase.ASR errors in synthetic phrase tables correspondsto the portion of errors that our method was ableto predict.
Table 4 shows that the OOV rate of n-grams in phrase tables augmented with syntheticphrases drops dramatically, up to 54%.
Consis-tent reduction of recognized errors across outputsfrom five different ASR systems confirms that ourerror-prediction approach is ASR-independent.tst2011 #1-grams #2-gramssystem0 29 (50.9%) 230 (20.3%)system1 27 (41.5%) 234 (21.3%)system2 36 (36.0%) 230 (20.1%)system3 34 (44.1%) 275 (20.1%)system4 46 (52.9%) 182 (16.8%)ROVER 30 (54.5%) 183 (18.7%)Table 4: Phrasal coverage of recognition errorsthat our technique is able to predict.
These areraw counts of 1-gram and 2-gram types that areOOVs in the baseline system and are recoveredby our method when we augment the system withplausible misrecognitions.
Percentages in paren-theses show OOV rate reduction due to recoveredn-grams.Next, we explore the effect of synthetic phraseson translation performance, across different (1-best) ASR outputs.
For references, ASR hypothe-ses, and ROVERed hypotheses we compare trans-lations produced by MT systems trained with andwithout synthetic phrases.
We detail our findingsin Table 5.Improvements in translation are significant forall systems with synthetic phrases.
This experi-ment corroborates the underlying assumption thatsimulated ASR errors are paired with correct tar-get phrases.
Moreover, this experiment supportsthe claim that incorporating noisier translations inthe translation model successfully adapts MT toSLT scenario and has indeed a positive effect onspeech translation.
Interestingly, improvement ofreference translations is also observed.
We spec-ulate that this stems from better lexical selectiondue to a smoothing effect that our technique may621WERBLEUBaselineBLEUSyntheticpreferences - 30.8 31.2 0.05system0 22.0 24.3 25.0 <0.01system1 23.3 23.8 24.3 <0.01system2 21.1 23.9 24.4 0.02system3 32.4 20.8 21.3 <0.01system4 19.5 24.5 25.0 0.01ROVER 17.4 25.0 25.6 0.01Table 5: Comparison of the baseline translationsystems with the systems augmented with syn-thetic phrases.
We measure EN?FR MT perfor-mance on the tst2011 test set: reference tran-scripts and ASR outputs on from five systemsand their ROVER combination.
Improvements intranslation of all ASR outputs are statistically sig-nificant.
This confirms the claim that incorporat-ing simulated ASR errors via synthetic phrases ef-fectively adapts MT to SLT scenario.have.Finally, we contrast the proposed approach oftranslation models adaptation to a conventionalmethod of lattice translation.
We decode FBK lat-tices produced for IWSLT 2011 Evaluation Cam-paign, and compare results to FBK 1-best transla-tion results, which correspond to system1 in Table5.
Table 6 summarizes our main finding: 1-bestsystem with synthetic phrases significantly outper-forms lattice decoding setup with baseline trans-lation table.14The additional small improvementin lattice decoding with synthetic phrases suggeststhat lattice decoding and phrase table adaptationare two complementary strategies and their com-bination is beneficial.6.2 expMultilingualIn the multilingual experiment we train ten MT se-tups: five baseline setups and five systems withsynthetic phrases, three systems per setup.
Foreach system we compare translations of the refer-ence transcripts and ASR hypotheses on the multi-lingual test set described in Section 6.
We evaluatetranslations produced by MT systems trained withand without synthetic phrases.
Table 7 summa-rizes experimental results, along with the test setWER for each language.14Automatic evaluation results (in terms of BLEU) pub-lished during the IWSLT 2011 Evaluation Campaign (Fed-erico et al., 2011) (p. 21) are 26.1 for FBK systems.
Unsur-prisingly, performance of our systems is lower, as we focusonly on translation table and do not optimize factors, such asLMs and others.BLEUBaselineBLEUSyntheticFBK 1-best 23.8 24.3FBK lattices 24.0 24.4Table 6: Comparison of the baseline EN?FR trans-lation systems with the systems augmented withsynthetic phrases, in 1-best and lattice decodingsetups.
1-best synthetic system significantly out-performs baseline lattice decoding setup.
Addi-tional improvement in lattice decoding with syn-thetic phrases suggests that lattice decoding andphrase table adaptation are two complementarystrategies.WER Baseline SyntheticRef ASR Ref ASREN?FR 30.723.3 17.8 23.9 18.1EN?DE 33.614.0 11.1 14.2 11.4EN?RU 30.712.3 10.7 12.2 10.6EN?HE 29.79.2 7.0 9.5 7.2EN?HI 32.15.5 4.5 5.6 4.8Table 7: Comparison of the baseline translationsystems with the systems augmented with syn-thetic phrases.
We measure MT performance onthe reference transcripts and ASR outputs.
Con-sistent improvements are observed in four out offive languages.Modest but consistent improvements are ob-served in four out of five setups with syntheticphrases.
Only French setup yielded statisticallysignificant improvement (p < .01).
However,if we concatenate the outputs of all languages,the improvement in translation of references withBLEU score averaged over all systems becomesstatistically significant (p = .03), improving from16.8 for the baseline system to 17.3 for the adaptedMT outputs.
While more careful evaluation is re-quired in order to estimate the effect of acous-tic confusions, the accumulated result show thatsynthetic phrases facilitate MT adaptation to SLTacross languages.7 AnalysisWe conducted careful manual analysis of actualusages of synthetic phrases in translation.
The pur-pose of this qualitative analysis is to verify thatpredicted ASR errors are paired with phrases thatcontribute to better translation to a target language.Table 8 shows some examples.
In the first sentencefrom the tst2011 test set (output from system 4)the word area was erroneously recognized as airy,622English ref so what they do is they move into an areaASR output so what they do is they move into an airyBaseline MT donc ce qu?ils font c?est qu?ils se d?placer dans un airySynthetic MT donc ce qu?ils font c?est qu?ils se d?placer dans une zoneFrench ref donc ce qu?ils font c?est qu?ils emm?nagent dans une zoneEnglish ref so i started thinking and listing what all it was that i thought would make a perfect biennialASR output so on i started a thinking and listing was all it was that i thought would make a pretty by neilBaseline MT donc j?ai commenc?
?
une pens?e et listing ?tait tout c?
?tait que je pensais ferait un assez par neilSynthetic MT donc j?ai commenc?
?
penser et une liste ?tait tout c?
?tait que je pensais ferait un assez par neilFrench ref alors j?ai commenc?
?
penser et ?
lister tout ce qui selon moi ferait une biennale parfaiteTable 8: Examples of translations improved with synthetic phrases.which is an OOV word for the baseline system.Our confusion generation algorithm also producedthe word airy as a plausible misrecognition variantfor the word area and attached it to a correct tar-get phrase zone, and this synthetic phrase was se-lected during decoding, yielding to a correct trans-lation for the ASR error.
Second example shows asimilar behavior for an indefinite article a. Thirdexample is taken from the English-Russian systemin the multilingual test set.
Gauge was producedas a plausible misrecognition variant to age, andtherefore correctly translated (albeit incorrectly in-flected) as ????????(age+sg+m+acc).
Syntheticphrases were also used in translations contain-ing misrecognized function words, segmentation-related examples, and longer n-grams.8 Related workPredicting ASR errors to improve speech recog-nition quality has been explored in several previ-ous studies.
Jyothi and Fosler-Lussier (2009) de-velop weighted finite-state transducer frameworkfor error prediction.
They build a confusion ma-trix FST between phones to model acoustic errorsmade by the recognizer.
Costs in the confusionmatrix combine acoustic variations in the HMMrepresentations of the phones (information fromthe acoustic model) and word-based phone confu-sions (information from the pronunciation model).In their follow-up work, Jyothi and Fosler-Lussier(2010) employ this error-predicting framework totrain the parameters of a global linear discrimina-tive language model that improves ASR.Sagae et al.
(2012) examined three protocolsfor ?hallucinating?
ASR n-best lists.
First ap-proach generates confusions on the phone level,with a phone-based finite-state transducer that em-ploys real n-best lists produced by the ASR sys-tem.
Second is generating confusions at the wordlevel with a MT-based approach.
Third is a phrasalcohorts approach, in which acoustically confus-able phrases are extracted from ASR n-best lists,based on pivots?identical left and right contexts ofa phrase.
All three methods were evaluated on thetask of ASR improvement through decoding withdiscriminative language models.
Discriminativelanguage models trained on simulated n-best listsproduced with phrasal cohorts method yielded thelargest WER reduction on the telephone speechrecognition task.Our approach to generating plausible ASR mis-recognitions is similar to previously explored FST-based methods.
The fundamental difference, how-ever, is in speech-free phonetic confusion trans-ducer that does not employ any data extractedfrom acoustic models or ASR outputs.
SimulatedASR errors are typically used to improve ASR ap-plications.
To the best of our knowledge no priorwork has been done on integrating ASR errors di-rectly in the translation models.9 ConclusionThe idea behind the novel ASR error-predictionalgorithm that we devise is to identify phonolog-ical neighbors with similar distributional proper-ties, i.e.
similar sounding words for which lan-guage model probabilities are insufficient for theirdisambiguation.
These sequences have been iden-tified as significant contributors to ASR errors(Goldwater et al., 2010).
Additional and evenmore important factors that cause recognition er-rors are disfluencies in speech (Tsvetkov et al.,2013b).
In the task of adapting MT to SLT theseand other irregularities can effectively be incor-porated in a useful general framework: syntheticphrases that augment phrase tables.
Our exper-iments show that simulated acoustic confusionscapture real ASR errors and that proposed frame-work effectively exploits them to improve transla-tion.623AcknowledgmentsWe are grateful to Jo?o Miranda and Alan Black for providingus the TED audio with transcriptions, and to Zaid Sheikh forhis help with ASR decoding.
This work was supported in partby the U. S. Army Research Laboratory and the U. S. ArmyResearch Office under contract/grant number W911NF-10-1-0533.ReferencesWaleed Ammar, Victor Chahuneau, MichaelDenkowski, Greg Hanneman, Wang Ling, AustinMatthews, Kenton Murray, Nicola Segall, YuliaTsvetkov, Alon Lavie, and Chris Dyer.
2013.The CMU machine translation systems at WMT2013: Syntax, synthetic translation options, andpseudo-references.Nicola Bertoldi, Richard Zens, and Marcello Federico.2007.
Speech translation by confusion network de-coding.
In Proc.
ICASSP, pages 1297?1300.
IEEE.Ondrej Bojar, Pavel Stranak, and Daniel Zeman.
2010.Data issues in English-to-Hindi machine translation.In Proceedings of LREC.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machinetranslation using paraphrases.
In Proceedings ofHLT/NAACL, pages 17?24.
Association for Compu-tational Linguistics.Francisco Casacuberta, Marcello Federico, HermannNey, and Enrique Vidal.
2008.
Recent effortsin spoken language translation.
Signal ProcessingMagazine, IEEE, 25(3):80?88.Mauro Cettolo, Marcello Federico, Luisa Bentivogli,Michael Paul, and Sebastian St?ker.
2012a.Overview of the IWSLT 2012 evaluation campaign.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012b.
WIT3: Web inventory of transcribedand translated talks.
In Proceedings of EAMT, pages261?268, Trento, Italy.Victor Chahuneau, Eva Schlinger, Noah A. Smith, andChris Dyer.
2013.
Translating into morphologicallyrich languages with synthetic phrases.
In Proceed-ings of EMNLP.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for op-timizer instability.
In Proceedings of ACL, pages176?181.
Association for Computational Linguis-tics.Chris Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In Pro-ceedings of ACL-08: HLT, pages 1012?1020.
Asso-ciation for Computational Linguistics.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of ACL.Marcello Federico, Luisa Bentivogli, Michael Paul,and Sebastian St?ker.
2011.
Overview of theIWSLT 2011 evaluation campaign.
In Proc.
IWSLT,pages 8?9.Jonathan G. Fiscus.
1997.
A post-processing system toyield reduced word error rates: Recognizer outputvoting error reduction (ROVER).
In Proc.
ASRU,pages 347?352.
IEEE.Sharon Goldwater, Dan Jurafsky, and Christopher DManning.
2010.
Which words are hard to rec-ognize?
prosodic, lexical, and disfluency factorsthat increase speech recognition error rates.
SpeechCommunication, 52(3):181?200.Xiaodong He and Li Deng.
2013.
Speech-centric in-formation processing: An optimization-oriented ap-proach.
IEEE, 101(5):1116?1135.Dan Jurafsky and James H Martin.
2000.
Speech &Language Processing.
Pearson Education India.Preethi Jyothi and Eric Fosler-Lussier.
2009.
A com-parison of audio-free speech recognition error pre-diction methods.
In Proc.
INTERSPEECH, pages1211?1214.Preethi Jyothi and Eric Fosler-Lussier.
2010.
Discrimi-native language modeling using simulated asr errors.In Proc.
INTERSPEECH, pages 1049?1052.Stanislav V Kasl and George F Mahl.
1965.
The re-lationship of disturbances and hesitations in spon-taneous speech to anxiety.
In Journal of Personal-ity and Social Psychology, volume 1(5), pages 425?433.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT/NAACL, pages 48?54.
Associationfor Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of ACL, pages 177?180.
Asso-ciation for Computational Linguistics.Evgeny Matusov, Stephan Kanthak, and Hermann Ney.2006.
Integrating speech recognition and machinetranslation: Where do we stand?
In Proc.
ICASSP,pages V?1217?V?1220.
IEEE.624Robert C. Moore.
2002.
Fast and accurate sentencealignment of bilingual corpora.
In Proceedingsof AMTA, pages 135?144, London, UK.
Springer-Verlag.Hermann Ney.
1999.
Speech translation: Coupling ofrecognition and translation.
In Proc.
ICASSP, vol-ume 1, pages 517?520.
IEEE.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.2010.
Paraphrase lattice for statistical machinetranslation.
In Proceedings of ACL.Mari Ostendorf, Beno?t Favre, Ralph Grishman, DilekHakkani-Tur, Mary Harper, Dustin Hillard, JuliaHirschberg, Heng Ji, Jeremy G Kahn, Yang Liu,Sameer Maskey, Evgeny Matusov, Hermann Ney,Andrew Rosenberg, Elizabeth Shriberg, Wen Wang,and Chuck Wooters.
2008.
Speech segmentationand spoken document processing.
Signal Process-ing Magazine, IEEE, 25(3):59?69.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.
Association for Computa-tional Linguistics.Stephan Peitz, Simon Wiesler, Markus Nu?baum-Thom, and Hermann Ney.
2012.
Spoken languagetranslation using automatically transcribed text intraining.
In Proc.
IWSLT.Vu H Quan, Marcello Federico, and Mauro Cettolo.2005.
Integrated n-best re-ranking for spoken lan-guage translation.
In Proc.
INTERSPEECH, pages3181?3184.
IEEE.Nick Ruiz, Arianna Bisazza, Fabio Brugnara, DanieleFalavigna, Diego Giuliani, Suhel Jaber, RobertoGretter, and Marcello Federico.
2011.
FBK@IWSLT 2011.
In Proc.
IWSLT.Kenji Sagae, M. Lehr, E. Prud?hommeaux, P. Xu,N.
Glenn, D. Karakos, S. Khudanpur, B. Roark,M.
Sara?lar, I. Shafran, D. Bikel, C. Callison-Burch,Y.
Cao, K. Hall, E. Hasler, P. Koehn, A. Lopez,M.
Post, and D. Riley.
2012.
Hallucinated n-bestlists for discriminative language modeling.
In Proc.ICASSP.
IEEE.H.
Soltau, F. Metze, C. F?gen, and A. Waibel.
2001.A one-pass decoder based on polymorphic linguisticcontext assignment.
In Proc.
ASRU.Andreas Stolcke.
2002.
SRILM?an extensible lan-guage modeling toolkit.
In Proc.
ICSLP, pages 901?904.Jean E Fox Tree.
1995.
The effects of false starts andrepetitions on the processing of subsequent words inspontaneous speech.
Journal of memory and lan-guage, 34(6):709?738.Yulia Tsvetkov, Chris Dyer, Lori Levin, and ArchnaBhatia.
2013a.
Generating English determiners inphrase-based translation with synthetic translationoptions.
In Proceedings of WMT.
Association forComputational Linguistics.Yulia Tsvetkov, Zaid Sheikh, and Florian Metze.2013b.
Identification and modeling of word frag-ments in spontaneous speech.
In Proc.
ICASSP.IEEE.Ping Xu, Pascale Fung, and Ricky Chan.
2012.Phrase-level transduction model with reordering forspoken to written language transformation.
In Proc.ICASSP, pages 4965?4968.
IEEE.Ruiqiang Zhang, Genichiro Kikui, Hirofumi Ya-mamoto, Taro Watanabe, Frank Soong, and Wai KitLo.
2004.
A unified approach in speech-to-speechtranslation: integrating features of speech recog-nition and machine translation.
In Proceedingsof COLING, page 1168.
Association for Computa-tional Linguistics.Bowen Zhou, Laurent Besacier, and Yuqing Gao.2007.
On efficient coupling of ASR and SMT forspeech translation.
In Proc.
ICASSP, volume 4,pages IV?101.
IEEE.Bowen Zhou.
2013.
Statistical machine translation forspeech: A perspective on structures, learning, anddecoding.
IEEE, 101(5):1180?1202.625
