Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387?397,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsData-driven sentence generation with non-isomorphic treesMiguel Ballesteros1Bernd Bohnet2Simon Mille1Leo Wanner1,31Natural Language Processing Group, Pompeu Fabra University, Barcelona, Spain2Google Inc.3Catalan Institute for Research and Advanced Studies (ICREA)1,3{name.lastname}@upf.edu2bohnetbd@google.comAbstractAbstract structures from which the generationnaturally starts often do not contain any func-tional nodes, while surface-syntactic struc-tures or a chain of tokens in a linearized treecontain all of them.
Therefore, data-drivenlinguistic generation needs to be able to copewith the projection between non-isomorphicstructures that differ in their topology andnumber of nodes.
So far, such a projectionhas been a challenge in data-driven genera-tion and was largely avoided.
We presenta fully stochastic generator that is able tocope with projection between non-isomorphicstructures.
The generator, which starts fromPropBank-like structures, consists of a cas-cade of SVM-classifier based submodules thatmap in a series of transitions the input struc-tures onto sentences.
The generator has beenevaluated for English on the Penn-Treebankand for Spanish on the multi-layered Ancora-UPF corpus.1 IntroductionApplications such as machine translation that inher-ently draw upon sentence generation increasinglydeal with deep meaning representations; see, e.g.,(Aue et al, 2004; Jones et al, 2012; Andreas et al,2013).
Deep representations tend to differ in theirtopology and number of nodes from the correspond-ing surface structures since they do not contain, e.g.,any functional nodes, while syntactic structures orchains of tokens in linearized trees do.
This meansthat sentence generation needs to be able to copewith the projection between non-isomorphic struc-tures.
However, most of the recent work in data-driven sentence generation still avoids this chal-lenge.
Some systems focus on syntactic generation(Bangalore and Rambow, 2000; Langkilde-Geary,2002; Filippova and Strube, 2008) or linearizationand inflection (Filippova and Strube, 2007; He etal., 2009; Wan et al, 2009; Guo et al, 2011a), andavoid thus the need to cope with this projection alltogether; some use a rule-based module to handlethe projection between non-isomorphic structures(Knight and Hatzivassiloglou, 1995; Langkilde andKnight, 1998; Bohnet et al, 2011); and some adaptthe meaning structures to be isomorphic with syn-tactic structures (Bohnet et al, 2010).
However, it isobvious that a ?syntacticization?
of meaning struc-tures can be only a temporary workaround and that arule-based module raises the usual questions of cov-erage, maintenance and portability.In this paper, we present a fully stochastic gener-ator that is able to cope with the projection betweennon-isomorphic structures.1Such a generator canbe used as a stand-alone application and also, e.g.,in text simplification (Klebanov et al, 2004) or deepmachine translation (Jones et al, 2012) (where thetransfer is done at a deep level).
In abstractive sum-marization, it facilitates the generation of the sum-maries, and in extractive summarization a better sen-tence fusion.21The data-driven sentence generator is availablefor public downloading at https://github.com/talnsoftware/deepgenerator/wiki.2For all of these applications, the deep representation canbe obtained by a deep parser, such as, e.g., (Ballesteros et al,2014a).387The generator, which starts from elementarypredicate-argument lexico-structural structures asused in sentence planning by Stent et al (2004),consists of a cascade of Support Vector Machines(SVM)-classifier based submodules that map the in-put structures onto sentences in a series of transi-tions.
Following the idea presented in (Ballesteros etal., 2014b), a separate SVM-classifier is defined forthe mapping of each linguistic category.
The genera-tor has been tested on Spanish with the multi-layeredAncora-UPF corpus (Mille et al, 2013) and on En-glish with an extended version of the dependencyPenn TreeBank (Johansson and Nugues, 2007).The remainder of the paper is structured asfollows.
In the next section, we briefly out-line the fundamentals of sentence generation aswe view it in our work, focusing in partic-ular on the most challenging part of it: thetransition between the non-isomorphic predicate-argument lexico-structural structures and surface-syntactic structures.
Section 3 outlines the setup ofour system.
Section 4 discusses the experiments wecarried out and the results we obtained.
In Section5, we briefly summarize related work, before in Sec-tion 6 some conclusions are drawn and future workis outlined.2 The FundamentalsSentence generation realized in this paper is partof the sentence synthesis pipeline argued for byMel?
?cuk (1988).
It consists of a sequence of twomappings:1.
Predicate-argument lexico-structural structure?
Syntactic structure2.
Syntactic Structure?
Linearized structureFollowing the terminology in (Mel?
?cuk, 1988),we refer to the predicate-argument lexico-structuralstructures as ?deep-syntactic structures?
(DSyntSs)and to the syntactic structures as ?surface-syntacticstructures?
(SSyntSs).While SSyntSs and linearized structures are iso-morphic, the difference in the linguistic abstractionof the DSyntSs and SSyntSs leads to divergencesthat impede the isomorphy between the two andmake the first mapping a challenge for statisticalgeneration.
Therefore, we focus in this section onthe presentation of the DSyntSs and SSyntSs and themapping between them.2.1 DSyntSs and SSyntSs2.1.1 Input DSyntSsDSyntSs are very similar to the PropBank(Babko-Malaya, 2005) structures and the structuresas used for the deep track of the First Surface Re-alization Shared Task (SRST, (Belz et al, 2011))annotations.
DSyntSs are connected trees that con-tain only meaning-bearing lexical items and bothpredicate-argument (indicated by Roman numbers:I, II, III, IV, .
.
. )
and lexico-structural, or deep-syntactic, (ATTR(ibutive), APPEND(itive) and CO-ORD(inative)) relations.
In other words, they donot contain any punctuation and functional nodes,i.e., governed elements, auxiliaries and determin-ers.
Governed elements such governed prepositionsand subordinating conjunctions are dropped becausethey are imposed by sub-categorization restrictionsof the predicative head and void of own meaning?as, for instance, to in give TO your friend or thatin I know that you will come.3Auxiliaries do notappear as nodes in DSyntSs.
Rather, the informa-tion they encode is captured in terms of tense, as-pect and voice attributes of the corresponding fullverbal nodes.
Equally, determiners are substitutedby attribute?value pairs of givenness they encode,assigned to their governors.
See Figure 1 (a) for asample DSyntS.42.1.2 SSyntSsSSyntSs are connected dependency trees in whichthe nodes are labeled by open or closed class lexicalitems and the edges by grammatical function rela-tions of the type ?subject?, ?oblique object?, ?adver-bial?, ?modifier?, etc.
A SSyntS is thus a typical de-pendency tree as used in data-driven syntactic pars-ing (Haji?c et al, 2009) and generation (Belz et al,2011).
See Figure 1 (b) for illustration of a SSyntS.3In contrast, on in the bottle is on the table is not droppedbecause it is semantic.4?That?
is considered a kind of determiner (to be derivedfrom the Information Structure).
This is the reason to omit it inthe deep structure.388(a) (b)Figure 1: A DSyntS (a) and its corresponding SSyntS (b) for the sentence Almost 1.2 million jobs have been createdby the state in that time2.2 Projection of DSyntS to SSyntSIn order to project a DSyntS onto its correspond-ing SSyntS in the course of generation (where bothDSyntSs and their corresponding SSyntSs are storedin the 14-column CoNLL?09 format), the followingtypes of actions need to be performed:51.
Project each node in the DSyntS onto itsSSynS-correspondence.
This correspondence can bea single node, as, e.g., job ?
[NN] (where NN is anoun), or a subtree (hypernode, known as syntagmin linguistics), as, e.g., time?
[DT NN] (where DTis a determiner and NN a noun) or create?
[VAUXVAUXVB IN] (where VAUXis an auxiliary, VB a fullverb and IN a preposition).
In formal terms, we as-sume any SSyntS-correspondence to be a hypernodewith a cardinality ?
1.2.
Generate the correct lemma for the nodes inSSyntS that do not have a 1:1 correspondence withan origin DSyntS node (as DT and VAUXabove).63.
Establish the dependencies within the individualSSyntS-hypernodes.4.
Establish the dependencies between the SSyntS-5For Spanish, we apply after the DSyntS?SSyntS transitionin a postprocessing stage rules for the generation of relative pro-nouns that are implied by the the SSyntS.
Since we cannot counton the annotation of coreference in the training data, we do nottreat other types of referring expressions.6The lemmas of nodes with 1:1 correspondence are the samein both structures.hypernodes (more precisely, between the nodes ofdifferent SSyntS-hypernodes) to obtain a connectedSSyntS-tree.2.3 Treebanks used in the experiments2.3.1 Spanish TreebankFor the validation of the performance of our gen-erator on Spanish, we use the AnCora-UPF tree-bank, which contains only about 100,000 tokens, butwhich has been manually annotated and validated onthe SSyntS- and DSyntS-layers, such that its qualityis rather high.
The deep annotation does not con-tain any functional prepositions since they have beenremoved for all predicates of the corpus, and theDSyntS-relations have been edited following anno-tation guidelines.
AnCora-UPF SSyntSs are anno-tated with fine-grained dependencies organized in ahierarchical scheme (Mille et al, 2012), in a similarfashion as the dependencies of the Stanford Scheme(de Marneffe et al, 2006).7Thus, it is possible touse the full set of labels or to reduce it according toour needs.
We performed preliminary experimentsin order to assess which tag granularity is bettersuited for generation and came up with the 31-labeltagset.7The main difference with the Stanford scheme is that inAnCora-UPF no distinction is explicitly made between argu-mental and non-argumental dependencies.3892.3.2 English TreebankFor the validation of the generator on English,we use the dependency Penn TreeBank (about1,000,000 tokens), which we extend by a DSyntlayer defined by the same deep dependency rela-tions, features and node correspondences as theSpanish DSynt layer.
The Penn TreeBank DSyntlayer is obtained by a rule-based graph transducer.The transducer removes definite and indefinite deter-miners, auxiliaries, THAT complementizers, TO in-finitive markers, and a finite list of functional prepo-sitions.
The functional prepositions have been man-ually compiled from the description and examples ofthe roles in the PropBank and NomBank annotationsof the 150 most frequent predicates of the corpus.
Adictionary has been built, which contains for each ofthe 150 predicates the argument slots (roles) and theprepositions associated to it, such that given a predi-cate and a preposition, we know to which role it cor-responds.
Consider, for illustration, Figure 2, whichindicates that for the nominal predicate plan 01, adependent introduced by the preposition to corre-sponds to the second argument of plan 01, while adependent introduced by for is its third argument.Figure 2: A sample (partial) mapping dictionary entryFor each possible surface dependency relation be-tween a governor and a dependent, a default map-ping is provided, which is applied if(i) The syntactic structure fulfills the conditions ofthe default mapping (e.g., ?subject?
is by de-fault mapped onto ?I?
unless it is the subject ofa passive verb, in which case it is mapped to thesecond argument ?II?
), and(ii) The pair governor?dependent is not found inthe dictionary; that is, if the dependent of theSSyntS dependency relation is a prepositionfound in the governor?s entry in the dictio-nary, the information provided in the dictio-nary is used instead of the default mapping.88In the PropBank annotation, a distinction is made betweenFor instance, in the sentence Sony announcedits plans to hire Mr. Guber, to is a dependentof plan with the SSyntS dependency NMOD.NMOD is by default mapped onto the deep re-lation ATTR, but since in the dictionary entryof plan it is stated that a dependent introducedby to is mapped to ?II?
(cf.
Figure 2), II is therelation that appears in the DSyntS-annotation.The features definiteness, voice, tense, aspect inthe FEATS column of the CoNLL format capturethe information conveyed by determiners and aux-iliaries.
The conversion procedure maps surface de-pendency relations as found in the Penn TreeBankonto the restricted set of deep dependency relationsas described in Section 2.1.1.The nodes in the original (surface-oriented) anddeep annotations are connected through their IDs.In the FEATS column of the output CoNLL file,id0 indicates the deep identifier of a word, whileid1 indicates the ID of the surface node it corre-sponds to.
There are less nodes in DSyntSs thanin SSyntSs since SSyntSs contain all the words ofa sentence.
Hence, a DSynt-node can correspond toseveral SSyntS nodes.
Multiple correspondences areindicated by the presence of the id2 (id3, id4, etc)feature in the FEATS column.3 Deep Generation3.1 BaselinesSince no available data-driven generator uses as in-put DSyntSs, we developed as baselines two rule-based graph transducer generators which producefor English respectively Spanish the best possibleSSyntSs, using only the information contained in thestarting DSyntS.The two baseline generators are structured simi-larly: both contain around 50 graph transducer rules,separated into two clusters.
The first cluster mapsDSyntS-nodes onto SSyntS-nodes, while the secondone handles the introduction of SSyntS dependencyrelations between the generated SSyntS-nodes.
Forinstance, in English, one rule maps DSyntS-nodesthat have a one-to-one correspondence in the SSyntSexternal and internal arguments, such that for some predicatesthe arguments are numbered starting from ?0?, and for otherstarting from ?1?.
This has been normalized in order to makeall arguments start from ?1?
for all predicates.390if N1is a Vfinand ((R1,2== I and N1is in activevoice and N2is not by)or (R1,2== II and N1is in passive voice))if ?
one-to-one correspondence between NDiand NSithen introduce SBJ between NS1and NS2elseif NS2is top node of the SSyntS hypernode and((NS1is top node of the SSynt hypernode and isAUX)or (NS1is the bottom node of the SSynthypernode and is Vfin)or (NS1is not top node or bottom node ofthe SSynt-hypernode and is AUX))then introduce SBJ between NS1and NS2endifendifFigure 3: Sample graph transducer rule(simple nouns, verbs, adverbs, adjectives, etc.
), 22rules map DSyntS-nodes that have a one-to-manycorrespondence in the SSyntS (N ?
DET+NN, N?DET+NN+governed PREP, V?AUX+VV, V?that COMPL+AUX+VV+governed PREP, etc.
), and25 rules generate the dependency relations.9Thetransduction rules apply in two phases, see Figure 3.During the first phase, all nodes and intra-hypernodedependencies are created in the output structure.During the second phase, all inter-hypernode depen-dencies are established.
Since there are one-to-manyDSyntS-SSyntS correspondences, the rules of thesecond phase have to ensure that the correct outputnodes are targeted, i.e., that jobs in Figure 1(b) ismade a dependent of have, and not of been or cre-ated, which all correspond to create in the input.Consider, for illustration of the complexity of therule-based generator, the transduction rule in Figure3.
The rule creates the SSynt dependency relationSBJ (subject) in a target SSyntS (with a governornode ND1and a dependent node ND2linked by adeep dependency relation R1,2in the input DSyntSand two nodes NS1and NS2which correspond toND1and ND2respectively in the target SSyntS).The evaluation shows that all straightforwardmappings are performed correctly; English auxil-iaries, that complementizers, infinitive markers and9?N?
stands for ?noun?, ?NN?
for ?common noun?, ?DET?for ?determiner?, ?PREP?
for ?preposition?, ?V?
for ?verb?,?AUX?
for ?auxiliary verb?, ?VV?
for ?main verb?, and?COMPL?
for ?complementizer?.determiners are introduced, and so are Spanish aux-iliaries, reflexive pronouns, and determiners.
Thatis, the rules produce well-formed SSyntSs of allpossible combinations of auxiliaries, conjunctionsand/or prepositions for verbs, determiners and/orprepositions for nouns, adjectives and adverbs.When there are several possible mappings, thebaseline takes decisions by default.
For example,when a governed preposition must be introduced, wealways introduce the most common one (of in En-glish, de ?of?
in Spanish).3.2 Data-Driven GeneratorThe data-driven generator is defined as a tree trans-ducer framework that consists of a cascade of 6 data-driven small tasks; cf.
Figure 4.
The first four taskscapture the actions 1.?4.
from Section 2.2; the 5thlinearizes the obtained SSyntS.
Figure 4 provides asample input and output of each submodule.
Thesystem outputs a 14 column CoNLL?09 linearizedformat without morphological inflections or punctu-ation marks.In the next sections, we discuss how these ac-tions are realized and how they are embedded intothe overall generation process.The intra- and inter-hypernode dependency deter-mination works as an informed dependency parserthat uses the DSyntS as input.
The search space isthus completely pruned.
Note also that for each step,the space of classes for the SVMs is based on lin-guistic facts extracted from the training corpus (forinstance, for the preposition generation SVM, theclasses are the possible prepositions; for the auxil-iary generation SVM, the possible auxiliaries, etc.
).3.2.1 Hypernode IdentificationGiven a node ndfrom the DSyntS, the systemmust find the shape of the surface hypernode thatcorresponds to ndin the SSyntS.
The hypernodeidentification SVMs use the following features:PoS of nd, PoS of nd?s head, verbal voice (active,passive) and aspect (perfective, progressive) of thecurrent node, lemma of nd, and nd?s dependencies.In order to simplify the task, we define the shapeof a surface hypernode as a list of surface PoS tags.This unordered list contains the PoS of each of thelemmas contained within the hypernode and a tagthat encodes the original deep node; for instance:391Figure 4: Workflow of the Data-Driven Generator.
[ NN(deep), DT]For each deep, i.e., DSyntS, PoS tag (which canbe one of the following four: N (noun), V (verb),Adv (adverb), A (adjective)), a separate multi-classclassifier is defined.10For instance, in the case ofN, the N-classifier will use the above features toassign to the a DSynt-node with PoS N the mostappropriate (most likely) hypernode?in this case,[NN(deep), DT].3.2.2 Lemma GenerationOnce the hypernodes of the SSyntS under con-struction have been produced, the functional nodesthat have been newly introduced in the hypernodesmust be assigned a lemma label.
The lemma gener-ation SVMs use the following features of the deepnodes ndin the hypernodes to select the most likelylemma:verbal finiteness (finite, infinitive, gerund, partici-ple) and aspect (perfective, progressive), degree ofdefiniteness of nouns, PoS of nd, lemma of nd, PoSof the head of ndAgain, for each surface PoS tag, a separate clas-sifier is defined.
Thus, the DT-classifier would pickfor the hypernode [NN(deep), DT] the most likelylemma for the DT-node (optimally, a determiner).10As will be seen in the discussion of the results, the strategyproposed by Ballesteros et al (2014b) to define a separate clas-sifier for each linguistic category here and in the other stageslargely pays off because it reduces the classification searchspace enormously and thus leads to a higher accuracy.3.2.3 Intra-hypernode Dependency GenerationGiven a hypernode and its lemmas provided bythe two previous stages, the dependencies (i.e., thedependency attachments and dependency labels) be-tween the elements of the created SSyntS hypern-odes must be determined (and thus also the gover-nors of the hypernodes).
For this task, the intra-hypernode dependency generation SVMs use thefollowing features:lemmas included in the hypernode, PoS-tags of thelemmas in the hypernode, voice of the head h of thehypernode, deep dependency relation to h.For each kind of hypernode, dynamically a sepa-rate classifier is generated.11In the case of the hy-pernode [NN(deep), DT], the corresponding classi-fier will create a link between the determiner and thenoun, with the noun as head and the determiner asdependent because it is the best link that it can find;cf.
Figure 5 for illustration.
We ensure that the out-put of the classifiers is a tree by controlling that ev-ery node (except the root) has one and only one gov-ernor.
The DSynt input is a tree; in the case of hy-pernodes of cardinality one, the governor/dependentrelation is maintained; in the case of hypernodes ofhigher cardinality, only one node receives an incom-ing arc and only one can govern another hypernode.3.2.4 Inter-hypernode Dependency GenerationOnce the individual hypernodes have been con-verted into connected dependency subtrees, the hy-11This implies that the number of classifiers varies dependingon the training set.
For instance, during the intra-hypernodedependency creation for Spanish, 108 SVMs are generated.392[ NN(deep), DT]detFigure 5: Internal dependency within a hypernodepernodes must be connected between each other,such that we obtain a complete SSyntS.
The inter-hypernode dependency generation SVMs use thefollowing features of a hypernode ssto determinefor each hypernode its governor.
For each hyper-node with a distinct internal dependency pattern, aseparate classifier is dynamically derived (for ourtreebanks, we obtained 114 different SVM classi-fiers because they also take into account hypernodeswith just one token).
:the internal dependencies of ss, the head of ss, thelemmas of ss, the PoS of the dependent of the headof ssin DSyntSFor instance, the classifier for the hypernode[JJ(deep)] is most likely to identify as its governorNN in the hypernode [NN(deep), DT]; cf.
Figure 6.The task faced by the inter-hypernode depen-dency classifiers is the same as that of a dependencyparser, only that its search space is very small (whichis favorably reflected in the accuracy figures).
[ NN(deep), DT] [ JJ(deep)]modifFigure 6: Surface dependencies between two hypernodes.3.3 LinearizationOnce we obtained a SSyntS, the linearizer must findthe correct order of the words.
There is already abody of work available on statistical linearization.Therefore, these tasks were not in the focus of ourwork.
Rather, we adopt the most successful tech-nique of the first SRST (Belz et al, 2011), a bottom-up tree linearizer that orders bottom-up each headand its children (Bohnet et al, 2011; Guo et al,2011a).
This has the advantage that the linear or-der obtained previously can provide context featuresfor ordering sub-trees higher up in the dependencytree.
Each head and its children are ordered with abeam search.The beam is initialized with entries of singlewords that are expanded in the next step by the re-maining words of the sub-tree, which results in anumber of new entries for the next iteration.
Afterthe expansion step, the new beam entries are sortedand pruned.
We keep the 30 best entries and con-tinue with the expansion and pruning steps until nofurther nodes of the subtree are left.
We take anSVM to obtain the scores for sorting the beam en-tries, using the same feature templates as in Guo etal.
(2011b) and Bohnet et al (2011).4 Experiments and ResultsIn our experiments, the Spanish treebank has beendivided into: (i) a development set of 219 sentences,with 3,437 tokens in the DSyntS treebank and 4,799tokens in the SSyntS treebank (with an average of21.91 words by sentence in SSynt); (ii) a trainingset of 3,036 sentences, with 57,665 tokens in theDSyntS treebank and 84,668 tokens in the SSyntStreebank (with an average of 27.89 words by sen-tence in SSynt); and a (iii) a held-out test for eval-uation of 258 sentences, with 5,878 tokens in theDSyntS treebank and 8,731 tokens in the SSyntStreebank (with an average of 33.84 words by sen-tence in SSynt).For the English treebank, we used a classicalsplit of (i) a training set of 39,279 sentences, with724,828 tokens in the DSynt treebank and 958,167tokens in the SSynt treebank (with an average of24.39 words by sentence in SSynt); and (ii) a testset of 2,399 sentences, with 43,245 tokens in theDSynt treebank and 57,676 tokens in the SSynt tree-bank (with an average of 24.04 words by sentence inSSynt).In what follows, we show the system performanceon both treebanks.
The Spanish treebank was usedfor development and testing, while the English tree-bank was only used for testing.4.1 ResultsIn this section, we present the performance of,first of all, the individual tasks of the data-drivenDSyntS?SSyntS projection, since these have beenthe challenging tasks that we addressed.
Table 1shows similar results for all tasks on the develop-ment and test sets with gold-standard input, that is,393the results of the classifiers as a stand-alone mod-ule, assuming that the previous module provides aperfect output.Spanish Dev.set # %Hypernode identification 3327/3437 96.80Lemma generation 724/767 94.39Intra-hypernode dep.
generation 756/756 100.00Inter-hypernode dep.
generation 2628/2931 89.66Spanish Test set # %Hyper-node identification 5640/5878 95.95Lemma generation 1556/1640 94.88Intra-hypernode dep.
generation 1622/1622 100.00Inter-hypernode dep.
generation 4572/5029 90.91Table 1: Results of the evaluation of the SVMs for thenon-isomorphic transition for the Spanish DSyntS devel-opment and test setsEnglish Test set # %Hyper-node identification 42103/43245 97.36Lemma generation 6726/7199 93.43Intra-hypernode dep.
generation 6754/7179 94.08Inter-hypernode dep.
generation 35922/40699 88.26Table 2: Results of the evaluation of the SVMs for thenon-isomorphic transition for the English DSyntS test setTo have the entire generation pipeline in place,we carried out several linearization experiments,starting from: (i) the SSyntS gold standard, (ii)SSyntSs generated by the rule-based baselines, and(iii) SSyntSs generated by the data-driven deep gen-erator; cf.
surface gen., baseline deep gen, and deepgen.
respectively in Tables 3 and 4).12Development Set BLEU NIST Exactsurface gen. 0.754 11.29 24.20 %baseline deep gen. 0.547 9.98 10.96 %deep gen. 0.582 10.78 12.33 %Test Set BLEU NIST Exactsurface gen. 0.762 12.08 15.89 %baseline deep gen. 0.515 10.60 2.33 %deep gen. 0.542 11.24 3.49 %Table 3: Overview of the results on the Spanish develop-ment and test sets excluding punctuation marks after thelinearization12Following (Langkilde-Geary, 2002; Belz et al, 2011) andother works on statistical text generation, we access the qualityof the linearization module via BLEU score, NIST and exactlymatched sentences.Test Set BLEU NIST Exactsurface gen. 0.91 15.26 56.02 %baseline deep gen. 0.69 13.71 12.38 %deep gen. 0.77 14.42 21.05 %Table 4: Overview of the results on the English test setexcluding punctuation marks after the linearization4.2 Discussion and Error AnalysisIn general, Tables 1?4 show that the quality of thepresented deep data-driven generator is rather goodboth during the individual stages of the DSyntS?SSyntS transition and as part of the DSyntS?linearized sentence pipeline.Two main problems impede an even better perfor-mance figures than those reflected in Tables 1 and 2.First, the introduction of prepositions causes mosterrors in hypernode detection and lemma genera-tion: when a preposition should be introduced or notand which preposition should be introduced dependsexclusively on the subcategorization frame of thegovernor of the DSyntS node.
A corpus of a limitedsize does not capture the subcategorization framesof ALL predicates.
This is especially true for ourSpanish treebank, which is particularly small.
Sec-ond, the inter-hypernode dependency suffers fromthe fact that the SSyntS tagset is quite fine-grained,at least in the case of Spanish, which makes the taskof the classifiers harder (e.g., there are nine differenttypes of verbal objects).
In spite of these problems,each set of classifiers achieves results above 88% onthe test sets.The results of deep generation in Tables 3 and4 can be explained by the fact of error propaga-tion: while (only) about 1 out of 10 hypernodes andabout 1 out of 10 lemmas are not correct and verylittle information is lost in the stage of the intra-hypernode dependencies determination, already al-most 1.75 out of 10 inter-hypernode dependencies,and finally 1 out 10 linear orderings are incorrect forEnglish and more than 2 out 10 for Spanish.As already mentioned above, the size of the train-ing corpus strongly affects the results.
Thus, forEnglish, for which the size of the training datasethas been 10 times bigger than for Spanish, the data-driven generator provides, without any tuning, morethan 0.2 BLEU points more that for Spanish.
A big-ger corpus also covers more linguistic phenomena394(lexical features, subcategorization frames, syntac-tic sentential constructions, etc.
)?which can be alsoexploited for rule-based generation.The linearizer also suffers from a small size of thetraining set.
Thus, while the small Spanish trainingcorpus leads to 0.754 BLEU and 0.762 BLEU for thedevelopment and test sets respectively, for English,we achieve 0.91 BLEU, which is a very competi-tive outcome compared to other English linearizers(Song et al, 2014).We also found that the data-driven generator tendsto output slightly shorter sentences, when comparedto the rule-based baseline.
It is always difficult tofind the best evaluation metric for plain text sen-tences (Smith et al, 2014).
In our experiments,we used BLEU, NIST and the exact match metric.BLEU is the average of n-gram precisions and in-cludes a brevity penalty, which reduces the scoreif the length of the output sentence is shorter thanthe gold.
In other words, BLEU favors longer sen-tences.
We believe that this is one of the reasonswhy the machine-learning based generator shows abigger difference for the English test set and theSpanish development set than the rule-based base-line.
Firstly, there are extremely long sentences inthe Spanish test set (31 words per sentence, in theaverage; the longest being 165 words).
Secondly,the English sentences and the Spanish developmentsentences are much shorter than the Spanish test sen-tences, such that the ML approach has the potentialto perform better.5 Related workThere is an increasing amount of work on statisticalsentence generation, although hardly any addressesthe problem of deep generation from semantic struc-tures that are not isomorphic with syntactic- struc-tures as a purely data-driven problem (as we do).
Tothe best of our knowledge, the only exception is ourearlier work in (Ballesteros et al, 2014b), where wediscuss the principles of classifiers for data-drivengenerators.
As already mentioned in Section 1, mostof the state-of-the-art work focuses on syntactic gen-eration; see, among others (Bangalore and Rambow,2000; Langkilde-Geary, 2002; Filippova and Strube,2008), or only on linearization and inflection (Filip-pova and Strube, 2007; He et al, 2009; Wan et al,2009; Guo et al, 2011a).
A number of proposalsare hybrid in that they combine statistical machinelearning-based generation with rule-based genera-tion.
Thus, some combine machine learning withpre-generated elements, as, e.g., (Marciniak andStrube, 2004; Wong and Mooney, 2007; Mairesse etal., 2010), or with handcrafted rules, as, e.g., (Ring-ger et al, 2004; Belz, 2005).
Others derive auto-matically grammars for rule-based generation mod-ules from annotated data, which can be used forsurface generation, as, e.g., (Knight and Hatzivas-siloglou, 1995; Langkilde and Knight, 1998; Oh andRudnicky, 2002; Zhong and Stent, 2005; Bohnet etal., 2011; Rajkumar et al, 2011) or for generationfrom ontology triples, as, e.g., (Gyawali and Gar-dent, 2013).6 ConclusionsWe presented a statistical deep sentence generatorthat successfully handles the non-isomorphism be-tween meaning representations and syntactic struc-tures in terms of a principled machine learning ap-proach.
This generator has been successfully testedon an English and a Spanish corpus, as a stand-aloneDSyntS?SSyntS generator and as a part of the gen-eration pipeline.
We are currently about to apply itto other languages?including Chinese, French andGerman.
Furthermore, resources are compiled touse it for generation of spoken discourse in Arabic,Polish and Turkish.We believe that our generator can be used not onlyin generation per se, but also, e.g., in machine trans-lation (MT), since MT could profit from using mean-ing representations such as DSyntSs, which abstractaway from the surface syntactic idiosyncrasies ofeach language, but are still linguistically motivated,as transfer representations.AcknowledgmentsOur work on deep stochastic sentence generationis partially supported by the European Commis-sion under the contract numbers FP7-ICT-610411(project MULTISENSOR) and H2020-RIA-645012(project KRISTINA).395ReferencesJ.
Andreas, A. Vlachos, and S. Clark.
2013.
Seman-tic Parsing as Machine Translation.
In Proceedings ofACL ?13.A.
Aue, A. Menezes, R. Moore, C. Quirk, and E. Ringger.2004.
Statistical Machine Translation Using LabeledSemantic Dependency Graphs.
In Proceedings of TMI?04.Olga Babko-Malaya, 2005.
Propbank Annotation Guide-lines.Miguel Ballesteros, Bernd Bohnet, Simon Mille, and LeoWanner.
2014a.
Deep-syntactic parsing.
In Proceed-ings of COLING?14, Dublin, Ireland.Miguel Ballesteros, Simon Mille, and Leo Wanner.2014b.
Classifiers for Data-Driven Deep SentenceGeneration.
In Proceedings of the International Con-ference of Natural Language Generation (INLG).Srinivas Bangalore and Owen Rambow.
2000.
Exploit-ing a probabilistic hierarchical model for generation.In Proceedings of the 18th conference on Computa-tional linguistics-Volume 1, pages 42?48.
Associationfor Computational Linguistics.Anja Belz, Mike White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
The firstsurface realisation shared task: Overview and evalu-ation results.
In Proceedings of the Generation Chal-lenges Session at the 13th European Workshop on Nat-ural Language Generation, pages 217?226.Anja Belz.
2005.
Statistical generation: Three meth-ods compared and evaluated.
In Proceedings of the10th European Workshop on Natural Language Gen-eration, pages 15?23.Bernd Bohnet, Leo Wanner, Simon Mille, and AliciaBurga.
2010.
Broad coverage multilingual deep sen-tence generation with a stochastic multi-level realizer.In Proceedings of COLING ?10, pages ?98?106?.Bernd Bohnet, Simon Mille, Beno?
?t Favre, and Leo Wan-ner.
2011.
StuMaBa: From deep representationto surface.
In Proceedings of ENLG 2011, Surface-Generation Shared Task, Nancy, France.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the 5th International Conference on Lan-guage Resources and Evaluation (LREC), volume 6,pages 449?454.Katja Filippova and Michael Strube.
2007.
Generatingconstituent order in german clauses.
In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics, volume 45, page 320.Katja Filippova and Michael Strube.
2008.
Sentence fu-sion via dependency graph compression.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?08, pages 177?185, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Yuqing Guo, Deirdre Hogan, and Josef van Genabith.2011a.
Dcu at generation challenges 2011 surfacerealisation track.
In Proceedings of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation, pages 227?229,Nancy, France, September.
Association for Computa-tional Linguistics.Yuqing Guo, Haifeng Wang, and Josef Van Genabith.2011b.
Dependency-based n-gram models for generalpurpose sentence realisation.
Natural Language Engi-neering, 17(04):455?483.B.
Gyawali and C. Gardent.
2013.
LOR-KBGEN,A Hybrid Approach To Generating from the KBGenKnowledge-Base.
In Proceedings of the KBGen Chal-lenge http://www.kbgen.org/papers/.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.
2009.Dependency based chinese sentence realization.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP: Volume 2-Volume 2, pages 809?816.
As-sociation for Computational Linguistics.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProceedings of the 16th Nordic Conference of Com-putational Linguistics (NODALIDA), pages 105?112,Tartu, Estonia, May 25-26.B.
Jones, J. Andreas, D. Bauer, K.M.
Hermann, andK.
Knight.
2012.
Semantics-Based Machine Transla-tion with Hyperedge Replacement Grammars.
In Pro-ceedings of COLING ?12.Beata Beigman Klebanov, Kevin Knight, and DanielMarcu.
2004.
Text simplification for information-seeking applications.
In On the Move to MeaningfulInternet Systems, Lecture Notes in Computer Science,pages 735?747.
Springer Verlag.Kevin Knight and Vasileios Hatzivassiloglou.
1995.Two-level, many-paths generation.
In Proceedings ofthe 33rd annual meeting on Association for Compu-tational Linguistics, pages 252?260.
Association forComputational Linguistics.396I.
Langkilde and K. Knight.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In Proceed-ings of the COLING/ACL, pages 704?710.Irene Langkilde-Geary.
2002.
An empirical verificationof coverage and correctness for a general-purpose sen-tence generator.
In Proceedings of the 12th Interna-tional Natural Language Generation Workshop, pages17?24.
Citeseer.Franc?ois Mairesse, Milica Ga?si?c, Filip Jur?c??
?cek, SimonKeizer, Blaise Thomson, Kai Yu, and Steve Young.2010.
Phrase-based statistical language generation us-ing graphical models and active learning.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 1552?1561.
Associ-ation for Computational Linguistics.Tomasz Marciniak and Michael Strube.
2004.Classification-based generation using tag.
In NaturalLanguage Generation, pages 100?109.
Springer.Igor Mel??cuk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press, Albany.Simon Mille, Alicia Burga, Gabriela Ferraro, and LeoWanner.
2012.
How does the granularity of an an-notation scheme influence dependency parsing perfor-mance?
In Proceedings of COLING 2012, pages 839?852, Mumbai, India.Simon Mille, Alicia Burga, and Leo Wanner.
2013.Ancora-upf: A multi-level annotation of spanish.
InProceedings of the Second International Conferenceon Dependency Linguistics, Prague, Czech Republic.Alice H Oh and Alexander I Rudnicky.
2002.
Stochasticnatural language generation for spoken dialog systems.Computer Speech & Language, 16(3):387?407.Rajakrishnan Rajkumar, Dominic Espinosa, and MichaelWhite.
2011.
The osu system for surface realizationat generation challenges 2011.
In Proceedings of the13th European workshop on natural language gener-ation, pages 236?238.
Association for ComputationalLinguistics.Eric Ringger, Michael Gamon, Robert C Moore, DavidRojas, Martine Smets, and Simon Corston-Oliver.2004.
Linguistically informed statistical models ofconstituent structure for ordering in sentence realiza-tion.
In Proceedings of the 20th international confer-ence on Computational Linguistics, page 673.
Associ-ation for Computational Linguistics.Aaron Smith, Christian Hardmeier, and J?org Tiedemann.2014.
Bleu is not the colour: How optimising bleureduces translation quality.Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.2014.
Joint morphological generation and syntacticlinearization.
In Proceedings of the Twenty-EighthAAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu?ebec City, Qu?ebec, Canada., pages 1522?1528.A.
Stent, R. Prasad, and M. Walker.
2004.
Trainable sen-tence planning for complex information presentationin spoken dialog systems.
In Proceedings of the ACL?04, pages 79?86.Stephen Wan, Mark Dras, Robert Dale, and C?ecile Paris.2009.
Improving grammaticality in statistical sentencegeneration: Introducing a dependency spanning treealgorithm with an argument satisfaction model.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 852?860.
Association for ComputationalLinguistics.Yuk Wah Wong and Raymond J Mooney.
2007.
Genera-tion by inverting a semantic parser that uses statisticalmachine translation.
In HLT-NAACL, pages 172?179.Huayan Zhong and Amanda Stent.
2005.
Building sur-face realizers automatically from corpora.
Proceed-ings of UCNLG, 5:49?54.397
