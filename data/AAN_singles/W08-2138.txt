CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 258?262Manchester, August 2008DeSRL: A Linear-Time Semantic Role Labeling SystemMassimiliano Ciaramita?
?massi@yahoo-inc.comFelice Dell?Orletta?dellorle@di.unipi.it?
: Yahoo!
Research Barcelona, Ocata 1, 08003, Barcelona, Catalunya, Spain?
: Dipartimento di Informatica, Universit`a di Pisa, L. B. Pontecorvo 3, I-56127, Pisa, Italy?
: Barcelona Media Innovation Center, Ocata 1, 08003, Barcelona, Catalunya, SpainGiuseppe Attardi?attardi@di.unipi.itMihai Surdeanu?,?mihai.surdeanu@barcelonamedia.orgAbstractThis paper describes the DeSRL sys-tem, a joined effort of Yahoo!
ResearchBarcelona and Universit`a di Pisa for theCoNLL-2008 Shared Task (Surdeanu etal., 2008).
The system is characterized byan efficient pipeline of linear complexitycomponents, each carrying out a differentsub-task.
Classifier errors and ambigui-ties are addressed with several strategies:revision models, voting, and reranking.The system participated in the closed chal-lenge ranking third in the complete prob-lem evaluation with the following scores:82.06 labeled macro F1 for the overall task,86.6 labeled attachment for syntactic de-pendencies, and 77.5 labeled F1 for se-mantic dependencies.1 System descriptionDeSRL is implemented as a sequence of compo-nents of linear complexity relative to the sentencelength.
We decompose the problem into three sub-tasks: parsing, predicate identification and clas-sification (PIC), and argument identification andclassification (AIC).
We address each of these sub-tasks with separate components without backwardfeedback between sub-tasks.
However, the use ofmultiple parsers at the beginning of the process,and re-ranking at the end, contribute beneficialstochastic aspects to the system.
Figure 1 summa-rizes the system architecture.
We detail the parsing?All authors contributed equally to this work.?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.sub-task in Section 2 and the semantic sub-tasks(PIC and AIC) in Section 3.2 ParsingIn the parsing sub-task we use a combination strat-egy on top of three individual parsing models,two developed in-house ?DeSRleft?to?rightandDeSRrevisionright?to?left?
and a third using an off-the-shelf parser, Malt 1.0.01.2.1 DeSRleft?to?rightThis model is a version of DeSR (Attardi, 2006),a deterministic classifier-based Shift/Reduceparser.
The parser processes input tokens advanc-ing on the input from left to right with Shift ac-tions and accumulates processed tokens on a stackwith Reduce actions.
The parser has been adaptedfor this year?s shared task and extended with addi-tional classifiers, e.g., Multi Layer Perceptron andmultiple SVMs.2The parser uses the following features:1.
SPLIT LEMMA: from tokens ?1, 0, 1, prev(0),leftChild(0), rightChild(0)2.
PPOSS: from ?2, ?1, 0, 1, 2, 3, prev(0), next(?1),leftChild(?1), leftChild(0), rightChild(?1),rightChild(0)3.
DEPREL: from leftChild(?1), leftChild(0),rightChild(?1)4.
HDIST: from ?1, 0In the above list negative numbers refer to tokenson the stack, positive numbers to tokens in the in-put queue.
We use the following path operators:leftChild(x) refers to the leftmost child of tokenx, rightChild(x) to the rightmost child of tokenx, prev(x) and next(x) respectively to the tokenpreceding or following x in the sentence.1http://w3.msi.vxu.se/?nivre/research/MaltParser.html2This parser is available for download at: http://sourceforge.net/projects/desr/.258Voting PIC AIC RerankingArgumentFrameDeSR left?to?rightMaltDeSR right?to?leftrevision OutputInputFigure 1: DeSRL system architecture.The first three types of features are directly ex-tracted from the attributes of tokens present in thetraining corpus.
The fourth feature represents thedistance of the token to the head of the noun phraseto which it belongs, or ?O?
if it does not belong toa noun phrase.
This distance is computed with asimple heuristic, based on a pattern of POS tags.Attardi and Dell?Orletta (2008) have shown thatthis feature improves the accuracy of a shift/reducedependency parser by providing approximate in-formation about NP chunks in the sentence.
In factno token besides the head of a noun phrase canhave a head referring to a token outside the nounphrase.
Hence the parser can learn to avoid creat-ing such links.
The addition of this feature yieldsan increase of 0.80% in Labeled Accuracy on thedevelopment set.2.2 Revision Parser: DeSRrevisionright?to?leftOur second individual parsing model implementsan alternative to the method of revising parse treesof Attardi and Ciaramita (2007) (see also (Hall &Novak, 2005)).
The original approach consisted intraining a classifier to revise the errors of a base-line parser.
The approach assumed that only lo-cal revisions to the parse tree would be needed,since the dependency parser mostly gets individualphrases correctly.
The experiments showed that in-deed most of the corrections can be expressed bya small set of (about 20) complex movement rules.Furthermore, there was evidence that one could gethigher improvements from the tree revision classi-fier if this was trained on the output of a lower ac-curacy parser.
The reason for this is that the num-ber of errors is higher and this provides a largeramount of training data.For the CoNLL 2008 shared task, we refined thisidea, but instead of using an independent classi-fier for the revision, we use the parser itself.
Thesecond parser is trained on the original corpus ex-tended with dependency information predicted bya lower accuracy parser.
To obtain the base parserwe use DeSR trained on half the training corpususing a Maximum Entropy (ME) classifier.
TheME classifier is considerably faster to train but hasa lower accuracy: this model achieved an LAS of76.49% on the development set.
Using the out-put of the ME-based parser we extend the originalcorpus with four additional columns: the lemmaof the predicted head (PHLEMMA), the PPOSS ofthe predicted head (PHPPOSS), the dependency ofthe predicted head (PHDEPREL), and the indica-tion of whether a token appears before or after itspredicted head.
A second parser is trained on thiscorpus, scanning sentences from right to left andusing the following additional features:1.
PHPPOSS: from ?1, 02.
PHLEMMA: from ?1, 03.
PHDEPREL: from ?1, 04.
PHHDIST: from 0Performing parsing in reverse order helps reduceseveral of the errors that a deterministic parsermakes when dependency links span a long distancein the input sequence.
Experiments on the CoNLL2007 corpora (Dell?Orletta, 2008) have shown thatthis indeed occurs, especially for distances in therange from 6 to 23.
In particular, the most signifi-cant improvements are for dependencies with labelCOORD (+ 6%) and P (+ 8%).The revision parser achieves an LAS of 85.81%on the development set.
Note that the extra fea-tures from the forward parser are indeed use-ful, since a simple backward parser only achieves82.56% LAS on the development set.2.3 Parser CombinationThe final step consists in combining the out-puts of the three individual models a simplevoting scheme: for each token we use major-ity voting to select its head and dependency la-bel.
In case of ties, we chose the dependencypredicted by our overall best individual model(DeSRrevisionright?to?left).3Note that typical approaches to parsercombination combine the outputs of inde-pendent parsers, while in our case one basemodel (DeSRrevisionright?to?left) is trained with3We tried several voting strategies but none performed bet-ter.259information predicted by another individualmodel(DeSRleft?to?right).
To the best of ourknowledge, combining individual parsing modelsthat are inter-dependent is novel.3 Semantic Role LabelingWe implement the Semantic Role Labeling (SRL)problem using three components: PIC, AIC, andreranking of predicted argument frames.3.1 Predicate Identification and ClassificationThe PIC component carries out the identificationof predicates, as well as their partial disambigua-tion, and it is implemented as a multiclass averagePerceptron classifier (Crammer & Singer, 2003).For each token i we extract the following features(?, ?
stands for token combination):1.
SPLIT LEMMA: from ?i?1, i?, i?1, i, i+1, ?i, i+1?2.
SPLIT FORM: from i?
2, i?
1, i, i+ 1.i+ 23.
PPOSS: from ?i?2, i?1?, ?i?1, i?, i?1, i, i+1, ?i, i+1?, ?i+ 1, i+ 2?4.
WORD SHAPE: e.g., ?Xx*?
for ?Brazil?, from ?i?2, i?1, i?, ?i?
1, i?, i?
1, i, i+1, ?i, i+1?, ?i, i+1, i+2?5.
Number of children of node i6.
For each children j of i: split lemmaj, ppossj,depreli,j, ?split lemmai, split lemmaj?, ?ppossi,ppossj?7.
Difference of positions: j ?
i, for each child j of i.The PIC component uses one single classifier map-ping tokens to one of 8 classes corresponding tothe rolesets suffixes 1 to 6, the 6 most frequenttypes, plus a class grouping all other rolesets, anda class for non predicates; i.e., Y = {0, 1, 2, .., 7}.Each token classified as y7is mapped by default tothe first sense y1.
This approach is capable of dis-tinguishing between different predicates based onfeatures 1 and 2, but it can also exploit informationthat is shared between predicates due to similarframe structures.
The latter property is intuitivelyuseful especially for low-frequency predicates.The classifier has an accuracy in the multiclassproblem, considering also the mistakes due to thenon-predicted classes, of 96.2%, and an F-score of92.7% with respect to the binary predicate iden-tification problem.
To extract features from trees(5-7) we use our parser?s output on training, devel-opment and evaluation data.3.2 Argument Identification andClassificationAlgorithm 1 describes our AIC framework.
The al-gorithm receives as input a sentence S where pred-icates have been identified and classified using theAlgorithm 1: AICinput : sentence S; inference strategy I; model wforeach predicate p in S doset frame Fin= {}foreach token i in S doif validCandidate(i) then?y = arg maxy?Yscore(?
(p, i),w, y)if?y 6= nil thenadd argument (i,?y) to FinFout= inference(Fin, I)output: set of all frames FoutPIC component, an inference strategy I is usedto guarantee that the generated best frames satisfythe domain constraints, plus an AIC classificationmodel w. We learn w using a multiclass Percep-tron, using as output label setY all argument labelsthat appear more than 10 times in training plus a nillabel assigned to all other tokens.During both training and evaluation we se-lect only the candidate tokens that pass thevalidCandidate filter.
This function requires thatthe length of the dependency path between pred-icate and candidate argument be less than 6, thelength of the dependency path between argumentand the first common ancestor be less than 3, andthe length of the dependency path between thepredicate and the first common ancestor be lessthan 5.
This heuristic covers over 98% of the ar-guments in training.In the worst case, Algorithm 1 has quadraticcomplexity in the sentence size.
But, on average,the algorithm has linear time complexity becausethe number of predicates per sentence is small (av-eraging less than five for sentences of 25 words).The function ?
generates the feature vector fora given predicate-argument tuple.
?
extracts thefollowing features from a given tuple of a predicatep and argument a:1. token(a)4, token(modifier of a) if a is thehead of a prepositional phrase, and token(p).2.
Patterns of PPOSS tags and DEPREL labelsfor: (a) the predicate children, (b) the childrenof the predicate ancestor across VC and IMdependencies, and (c) the siblings of the sameancestor.
In all paths we mark the position ofp, a and any of their ancestors.3.
The dependency path between p and a. Weadd three versions of this feature: just the4token extracts the split lemma, split form, and PPOSStag of a given token.260path, and the path prefixed with p and a?sPPOSS tags or split lemmas.4.
Length of the dependency path.5.
Distance in tokens between p and a.6.
Position of a relative to p: before or after.We implemented two inference strategies:greedy and reranking.
The greedy strategy sortsall arguments in a frame Finin descending orderof their scores and iteratively adds each argumentto the output frame Foutonly if it respects the do-main constraints with the other arguments alreadyselected.
The only domain constraint we use is thatcore arguments cannot repeat.3.3 Reranking of Argument FramesThe reranking inference strategy adapts the ap-proach of Toutanova et al (2005) to the depen-dency representation with notable changes in can-didate selection, feature set, and learning model.For candidate selection we modify Algorithm 1:instead of storing only y?
for each argument in Finwe store the top k best labels.
Then, from the ar-guments in Fin, we generate the top k frames withthe highest score, where the score of a frame is theproduct of all its argument probabilities, computedas the softmax function on the output of the Per-ceptron.
In this set of candidate frames we markthe frame with the highest F1score as the positiveexample and all others as negative examples.From each frame we extract these features:1.
Position of the frame in the set ordered byframe scores.
Hence, smaller positions in-dicate candidate frames that the local modelconsidered better (Marquez et al, 2007).2.
The complete sequence of arguments andpredicate for this frame (Toutanova, 2005).We add four variants of this feature: just thesequence and sequence expanded with: (a)predicate voice, (b) predicate split lemma,and (c) combination of voice and split lemma.3.
The complete sequence of arguments andpredicate for this frame combined with theirPPOSS tags.
Same as above, we add fourvariants of this feature.4.
Overlap with the PropBank or NomBankframe for the same predicate lemma andsense.
We add the precision, recall, and F1score of the overlap as features (Marquez etal., 2007).5.
For each frame argument, we add the featuresfrom the local AIC model prefixed with theWSJ + Brown WSJ BrownLabeled macro F182.69 83.83 73.51LAS 87.37 88.21 80.60Labeled F178.00 79.43 66.41Table 1: DeSRL results in the closed challenge,for the overall task, syntactic dependencies, andsemantic dependencies.Devel WSJ BrownDeSRleft?to?right85.61 86.54 79.74DeSRrevisionright?to?left85.81 86.19 78.91MaltParser 84.10 85.50 77.06Voting 87.37 88.21 80.60Table 2: LAS of individual and combined parsers.corresponding argument label in the currentframe (Toutanova, 2005).The reranking classifier is implemented as multi-layer perceptron with one hidden layer of 5 units,trained to solve a regression problem with a leastsquare criterion function.
Previously we experi-mented, unsuccessfully, with a multiclass Percep-tron and a ranking Perceptron.
The limited numberof hidden units guarantees a small computationaloverhead with respect to a linear model.4 Results and AnalysisTable 1 shows the overall results of our systemin the closed challenge.
Note that these scoresare higher than those of our submitted run mainlydue to improved parsing models (discussed be-low) whose training ended after the deadline.
Thescore of the submitted system is the third bestfor the complete task.
The system throughput inour best configuration is 28 words/second, or 30words/second without reranking.
In exploratoryexperiments on feature selection for the re-rankingmodel we found that several features classes donot contribute anything and could be filtered outspeeding up significantly this last SRL step.
Notehowever that currently over 90% of the runtime isoccupied by the syntactic parsers?
SVM classifiers.We estimate that we can increase throughput oneorder of magnitude simply by switching to a faster,multiclass classifier in parsing.4.1 Analysis of ParsingTable 2 lists the labeled attachment scores (LAS)achieved by each parser and by their combinationon the development set, the WSJ and Brown testsets.
The results are improved with respect to theofficial run, by using a revision parser trained onthe output of the lower accuracy ME parser, as261Labeled F1Unlabeled F1Syntax PIC Inference Devel WSJ Brown Devel WSJ Browngold gold greedy 88.95 90.21 84.95 93.71 94.34 93.29predicted gold greedy 85.96 86.70 78.68 90.60 90.98 88.02predicted predicted greedy 79.88 79.27 66.41 86.07 85.33 80.14predicted predicted reranking 80.13 79.43 66.41 86.33 85.62 80.41Table 3: Scores of the SRL component under various configurations.Devel WSJ BrownUnlabeled F192.69 90.88 86.96Labeled F1(PIC) 87.29 84.87 71.99Labeled F1(Sense 1) 79.62 78.94 70.11Table 4: Scores of the PIC component.mentioned earlier.
These results show that vot-ing helps significantly (+1.56% over the best singleparser) even though inter-dependent models wereused.
However, our simple voting scheme doesnot guarantee that a well-formed tree is generated,leaving room for further improvements; e.g., asin (Sagae & Lavie, 2006).4.2 Analysis of SRLTable 3 shows the labeled and unlabeled F1scoresof our SRL component as we move from gold topredicted information for syntax and PIC.
For theshared task setting ?predicted syntax and predictedPIC?
we show results for the two inference strate-gies implemented: greedy and reranking.
The firstline in the table indicates that the performance ofthe SRL component when using gold syntax andgold PIC is good: the labeled F1is 90 points for thein-domain corpus and approximately 85 points forthe out-of-domain corpus.
Argument classificationsuffers the most on out-of-domain input: there isa difference of 5 points between the labeled scoreson WSJ and Brown, even though the correspond-ing unlabeled scores are comparable.The second line in the table replicates the setupof the 2005 CoNLL shared task: predicted syntaxbut gold PIC.
This yields a moderate drop of 3 la-beled F1points on in-domain data and a larger dropof 6 points for out-of-domain data.We see larger drops when switching to predictedPIC (line 3): 5-6 labeled F1points in domain and12 points out of domain.
This drop is caused by thePIC component, e.g., if a predicate is missed thewhole frame is lost.
Table 4 lists the scores of ourPIC component, which we compare with a base-line system that assigns sense 1 to all identifiedpredicates.
The table indicates that, even thoughour disambiguation component improves signifi-cantly over the baseline, it performs poorly, espe-cially on out-of-domain data.
Same as SRL, theclassification sub-task suffers the most out of do-main (there is a difference of 15 points betweenunlabeled and labeled F1scores on Brown).Finally, the reranking inference strategy yieldsonly modest improvements (last line in Table 3).We attribute these results to the fact that, unlikeToutanova et al (2005), we use only one tree togenerate frame candidates, hence the variation inthe candidate frames is small.
Considering that theprocessing overhead of reranking is already large(it quadruples the runtime of our AIC component),we do not consider reranking a practical extensionto a SRL system when processing speed is a dom-inant requirement.ReferencesG.
Attardi.
2006.
Experiments with a Multilan-guage Non-Projective Dependency Parser.
In Proc.of CoNNL-X 2006.G.
Attardi and M. Ciaramita.
2007.
Tree Revi-sion Learning for Dependency Parsing.
In Proc.
ofNAACL/HLTC 2007.G.
Attardi, F. Dell?Orletta.
2008.
Chunking and De-pendency Parsing.
In Proc.
of Workshop on PartialParsing.K.
Crammer and Y.
Singer.
2003.
UltraconservativeOnline Algorithms for Multiclass Problems.
Journalof Machine Learning Research 3: pp.951-991.F.
Dell?Orletta.
2008.
Improving the Accuracy of De-pendency Parsing.
PhD Thesis.
Dipartimento di In-formatica, Universit`a di Pisa, forthcoming.K.
Hall and V. Novak.
2005.
Corrective Modelingfor Non-Projective Dependency Parsing.
In Proc.
ofIWPT.L.
Marquez, L. Padro, M. Surdeanu, and L. Villarejo.2007.
UPC: Experiments with Joint Learning withinSemEval Task 9.
In Proc.
of SemEval 2007.K.
Sagae and A. Lavie.
2006.
Parser Combination byreparsing.
In Proc.
of HLT/NAACL.M.
Surdeanu, R. Johansson, A. Meyers, L. M`arquezand J. Nivre.
2008.
The CoNLL-2008 Shared Taskon Joint Parsing of Syntactic and Semantic Depen-dencies.
In Proc.
of CoNLL-2008.K.
Toutanova, A. Haghighi, and C. Manning.
2005.Joint Learning Improves Semantic Role Labeling.
InProc.
of ACL.262
