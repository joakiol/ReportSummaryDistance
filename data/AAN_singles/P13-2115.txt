Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651?657,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDiverse Keyword Extraction from ConversationsMaryam HabibiIdiap Research Institute and EPFLRue Marconi 19, CP 5921920 Martigny, Switzerlandmaryam.habibi@idiap.chAndrei Popescu-BelisIdiap Research InstituteRue Marconi 19, CP 5921920 Martigny, Switzerlandandrei.popescu-belis@idiap.chAbstractA new method for keyword extractionfrom conversations is introduced, whichpreserves the diversity of topics that arementioned.
Inspired from summarization,the method maximizes the coverage oftopics that are recognized automaticallyin transcripts of conversation fragments.The method is evaluated on excerpts of theFisher and AMI corpora, using a crowd-sourcing platform to elicit comparativerelevance judgments.
The results demon-strate that the method outperforms twocompetitive baselines.1 IntroductionThe goal of keyword extraction from texts is toprovide a set of words that are representative ofthe semantic content of the texts.
In the applica-tion intended here, keywords are automatically ex-tracted from transcripts of conversation fragments,and are used to formulate queries to a just-in-timedocument recommender system.
It is thus impor-tant that the keyword set preserves the diversity oftopics from the conversation.
While the first key-word extraction methods ignored topicality as theywere based on word frequencies, more recent me-thods have considered topic modeling factors forkeyword extraction, but without specifically set-ting a topic diversity constraint, which is impor-tant for naturally-occurring conversations.In this paper, we propose a new method for key-word extraction that rewards both word similarity,to extract the most representative words, and worddiversity, to cover several topics if necessary.
Thepaper is organized as follows.
In Section 2 we re-view existing methods for keyword extraction.
InSection 3 we describe our proposal, which relieson topic modeling and a novel topic-aware diversekeyword extraction algorithm.
Section 4 presentsthe data and tasks for comparing sets of keywords.In Section 5 we show that our method outperformstwo existing ones.2 State of the Art in Keyword ExtractionNumerous studies have been conducted to auto-matically extract keywords from a text or a tran-scribed conversation.
The earliest techniques haveused word frequencies (Luhn, 1957), TFIDF val-ues (Salton et al, 1975; Salton and Buckley,1988), and pairwise word co-occurrence frequen-cies (Matsuo and Ishizuka, 2004) to rank wordsfor extraction.
These approaches do not con-sider word meaning, so they may ignore low-frequency words which together indicate a highly-salient topic (Nenkova and McKeown, 2012).To improve over frequency-based methods, se-veral ways to use lexical semantic informationhave been proposed.
Semantic relations be-tween words can be obtained from a manually-constructed thesaurus such as WordNet, or fromWikipedia, or from an automatically-built the-saurus using latent topic modeling techniques.Ye et al (2007) used the frequency of all wordsbelonging to the same WordNet concept set, whilethe Wikifier system (Csomai and Mihalcea, 2007)relied on Wikipedia links to compute a substituteto word frequency.
Harwath and Hazen (2012)used topic modeling with PLSA to build a the-saurus, which they used to rank words based ontopical similarity to the topics of a transcribed con-versation.
To consider dependencies among se-lected words, word co-occurrence has been com-bined with PageRank by Mihalcea and Tarau(2004), and additionally with WordNet by Wanget al (2007), or with topical information by Z. Liuet al (2010).
However, as shown empirically byMihalcea and Tarau (2004) and by Z. Liu et al(2010) with various co-occurrence windows, suchapproaches have difficulties modeling long-rangedependencies between words related to the same651topic.
Z. Liu et al (2009b) used part-of-speech in-formation and word clustering techniques, whileF.
Liu et al (2009a) added this information tothe TFIDF method so as to consider both worddependency and semantic information.
However,although they considered topical similarity, theabove methods did not explicitly reward diversityand might miss secondary topics.Supervised methods have been used to learn amodel for extracting keywords with various learn-ing algorithms (Turney, 1999; Frank et al, 1999;Hulth, 2003).
These approaches, however, rely onthe availability of in-domain training data, and theobjective functions they use for learning do notconsider yet the diversity of keywords.3 Diverse Keyword ExtractionWe propose to build a topical representation ofa conversation fragment, and then to select key-words using topical similarity while also reward-ing the diversity of topic coverage, inspired byrecent summarization methods (Lin and Bilmes,2011; Li et al, 2012).3.1 Representing Topic InformationTopic models such as Probabilistic Latent Seman-tic Analysis (PLSA) or Latent Dirichlet Allocation(LDA) can be used to determine the distributionover the topic z of a word w, noted p(z|w), from alarge amount of training documents.
LDA imple-mented in the Mallet toolkit (McCallum, 2002) isused in this paper because it does not suffer fromthe overfitting issue of PLSA (Blei et al, 2003).The distribution of each topic z in a given con-versation fragment t, noted p(z|t), can be com-puted by summing over all probabilities p(z|w) ofthe N words w spoken in the fragment:p(z|t) = 1N?w?tp(z|w).3.2 Selecting KeywordsThe problem of keyword extraction with maximaltopic coverage is formulated as follows.
If a con-versation fragment t mentions a set of topics Z,and each word w from the fragment t can evoke asubset of the topics in Z, then the goal is to finda subset of unique words S ?
t, with |S| ?
k,which maximzes the number of covered topics foreach number of keywords k.This problem is an instance of the maximumcoverage problem, which isNP -hard.
Nemhauseret al (1978) showed that a greedy algorithm canfind an approximate solution guaranteed to bewithin (1 ?
1e ) ' 0.63 of the optimal solutionif the coverage function is submodular and mono-tone nondecreasing1.To find a monotone submodular function forkeyword extraction, we used inspiration from re-cent work on extractive summarization methods(Lin and Bilmes, 2011; Li et al, 2012), which pro-posed a square root function for diverse selectionof sentences to cover the maximum number of keyconcepts of a given document.
The function re-wards diversity by increasing the gain of selectinga sentence including a concept that was not yetcovered by a previously selected sentence.
Thismust be adapted for keyword extraction by defin-ing an appropriate reward function.We first introduce rS,z , the topical similaritywith respect to topic z of the keyword set S se-lected from the fragment t, defined as follows:rS,z =?w?Sp(z|w) ?
p(z|t).We then propose the following reward functionfor each topic, where p(z|t) is the importance ofthe topic and ?
is a parameter between 0 and 1:f : rS,z ?
p(z|t) ?
r?S,z .This is clearly a submodular function with di-minishing returns as rS,z increases.Finally, the keywords S ?
t, with |S| ?
k,are chosen by maximizing the cumulative rewardfunction over all the topics, formulated as follows:R(S) =?z?Zp(z|t) ?
r?S,z .Since R(S) is submodular, the greedy algo-rithm for maximizing R(S) is shown as Algo-rithm 1 on the next page, with r{w},z being similarto rS,z with S = {w}.
If ?
= 1, the reward func-tion is linear and only measures the topical simila-rity of words with the main topics of t. However,when 0 < ?
< 1, as soon as a word is selectedfrom a topic, other words from the same topic starthaving diminishing gains.4 Data and Evaluation MethodThe proposed keyword extraction method wastested on two conversational corpora, the Fisher1A function F is submodular if ?A ?
B ?
T \ t, F (A+t) ?
F (A) ?
F (B + t) ?
F (B) (diminishing returns) andis monotone nondecreasing if ?A ?
B, F (A) ?
F (B).652(a) (b)Please select one of the following options:1.
Image (a) represents the conversation fragment better than (b).2.
Image (b) represents the conversation fragment better than (a).3.
Both (a) and (b) offer a good representation of the conversation.4.
None of (a) and (b) offer a good representation of the conversation.Figure 1: Example of a HIT based on an AMI discussion about the impact on sales of some features ofremote controls (the conversation transcript is given in the Appendix).
The word cloud was generatedusing WordleTM from the list produced by the diverse keyword extraction method with ?
= 0.75 (notedD(.75)) for image (a) and by a topic similarity method (TS) for image (b).
TS over-represents the topic?color?
by selecting three words related to it, but misses other topics such as ?remote control?, ?losing adevice?
and ?buying a device?
which are also representative of the fragment.Input : a given text t, a set of topics Z, thenumber of keywords kOutput: a set of keywords SS ?
?
;while |S| ?
k doS ?
S ?
{argmaxw?t\S(h(w))whereh(w) =?z?Z p(z|t)[r{w},z + rS,z]?
};endreturn S;Algorithm 1: Diverse keyword extraction.Corpus (Cieri et al, 2004), and the AMI MeetingCorpus (Carletta, 2007).
The former corpus con-tains about 11,000 topic-labeled telephone conver-sations, on 40 pre-selected topics (one per con-versation).
We created a topic model using Mal-let over two thirds of the Fisher Corpus, given itslarge number of single-topic documents, with 40topics.
The remaining data is used to build 11artificial ?conversations?
(1-2 minutes long) fortesting, by concatenating 11 times three fragmentsabout three different topics.The AMI Corpus contains 171 half-hour meet-ings about remote control design, which includeseveral topics each ?
so they cannot be directlyused for learning topic models.
While selectingfor testing 8 conversation fragments of 2-3 min-utes each, we trained topic models on a subset ofthe English Wikipedia (10% or 124,684 articles).Following several previous studies, the number oftopics was set to 100 (Boyd-Graber et al, 2009;Hoffman et al, 2010).To evaluate the relevance (or representative-ness) of extracted keywords with respect to aconversation fragment, we designed comparisontasks.
In each task, a fragment is shown, followedby three control questions about its content, andthen by two lists of nine keywords each, from twodifferent extraction methods.
To improve readabil-ity, the keyword lists are presented to the judgesusing a word cloud representation generated byWordleTM (http://www.wordle.net), in which thewords ranked higher are emphasized in the wordcloud (see example in Figure 1).
The judges hadto read the conversation transcript, answer the con-trol questions, and then decide which word cloudbetter represents the content of the conversation.The tasks were crowdsourced via Amazon?sMechanical Turk (AMT) as ?human intelligencetasks?
(HITs).
One of them is exemplified in Fig-ure 1, without the control questions, and the re-spective conversation transcript is given in the Ap-pendix.
Ten workers were recruited for each cor-pus.
An example of judgment counts for each ofthe 8 AMI HITs comparing two methods is shownin Table 1.
After collecting judgments, the com-parative relevance values were computed by firstapplying a qualification control factor to the hu-man judgments, and then averaging results overall judgments (Habibi and Popescu-Belis, 2012).Moreover, to verify the diversity of the key-6530.650.70.750.80.850.90.9511.051 2 3 4 5 6 7 8 9 10 11 12 13 14 15D(0.50)D(0.75)TSWFRanking?
?NDCG valuesFigure 2: Average ?-NDCG over the 11 conversations from the Fisher Corpus, for 1 to 15 extractedkeywords.word set, we use the ?-NDCG measure (Clarkeet al, 2008) proposed for information retrieval,which rewards a mixture of relevance and diver-sity ?
with equal weights when ?
= .5 as set here.We only apply ?-NDCG to the three-topic con-versation fragments from the Fisher Corpus, rel-evance of a keyword being set to 1 when it be-longs to the fragment corresponding to the topic.A higher value indicates that keywords are moreuniformly distributed across the three topics.5 Experimental ResultsWe have compared several versions of the diversekeyword extraction method, noted D(?
), for ?
?
{.5, .75, 1}, with two other methods.
The firstone uses only word frequency (not including stop-words) and is noted WF.
We did not use TFIDFbecause it sets low weights on keywords that arerepeated in many fragments but which are never-theless important to extract.
The second method isbased on topical similarity (noted TS) but does notspecifically enforce diversity (Harwath and Hazen,2012).
In fact TS coincides with D(1), so it isnoted TS.
As the relevance of keywords for D(.5)was already quite low, we did not test lower valuesof ?.
Similarly, we did not test additional valuesof ?
above .5 because the resulting word lists werevery similar to tested values.First of all, we compared the four methods withrespect to the diversity constraint over the con-HIT A B C D E F G HTS more relevant 4 1 1 1 2 2 1 1D(.75) more rel.
4 1 8 9 6 6 6 8Both relevant 2 5 1 0 2 2 3 1Both irrelevant 0 3 0 0 0 0 0 0Table 1: Number of answers for each of the fouroptions of the comparative evaluation task, fromten human judges.
The 8 HITs compare the D(.75)and TS methods on 8 AMI HITs.Corpus Compared methods Relevance (%)(m1 vs. m2) m1 m2Fisher D(.75) vs. TS 68 32TS vs. WF 82 18WF vs. D(.5) 95 5AMI D(.75) vs. TS 78 22TS vs. WF 60 40WF vs. D(.5) 78 22Table 2: Comparative relevance scores of keywordextraction methods based on human judgments.catenated fragments of the Fisher Corpus, by us-ing ?-NDCG to measure how evenly the extractedkeywords were distributed across the three topics.Figure 2 shows results averaged over 11 conversa-tions for various sizes of the keyword set (1?15).The average ?-NDCG values for D(.75) and D(.5)are similar, and clearly higher than WF and TSfor all ranks (except, of course, for a single key-word).
The values for TS are quite low, and onlyincrease for a large number of keywords, demon-strating that TS does not cope well with topic di-versity, but on the contrary first selects keywordsfrom the dominant topic.
The values for WF aremore uniform as it does not consider topics at all.To measure the overall representativeness ofkeywords, we performed binary comparisons be-tween the outputs of each method, using crowd-sourcing, over 11 fragments from the Fisher Cor-pus and 8 fragments from AMI.
The goal is torank the methods, so we only report here onthe comparisons required for complete ordering.AMT workers compared two lists of nine key-words each, with four options: X more represen-tative or relevant than Y , or vice-versa, or bothrelevant, or both irrelevant.
Table 1 shows thejudgments collected when comparing the output ofD(.75) with TS on the AMI Corpus.
Workers dis-agreed for the first two HITs, but then found thatthe keywords extracted by D(.75) were more rep-resentative compared to TS.
The consolidated rel-654evance (Habibi and Popescu-Belis, 2012) is 78%for D(.75) vs. 22% for TS.The averaged relevance values for all compar-isons needed to rank the four methods are shownin Table 2 separately for the Fisher and AMI Cor-pora.
Although the exact differences vary, the hu-man judgments over the two corpora both indi-cate the following ranking: D(.75) > TS > WF >D(.5).
The optimal value of ?
is thus around .75,and with this value, our diversity-aware methodextracts more representative keyword sets than TSand WF.
The differences between methods arelarger for the Fisher Corpus, due to the artificialfragments that concatenate three topics, but theyare still visible on the natural fragments of theAMI Corpus.
The low scores of D(.5) are foundto be due, upon inspection, to the low relevanceof keywords.
In particular, the comparative rele-vance of D(.75) vs. D(.5) on the Fisher Corpus isvery large (96% vs. 4%).6 ConclusionThe diverse keyword extraction method with ?
=.75 provides the keyword sets that are judged mostrepresentative of the conversation fragments (twoconversational datasets) by a large number of hu-man judges recruited via AMT, and has the high-est ?-NDCG value.
Therefore, enforcing both rel-evance and diversity brings an effective improve-ment to keyword extraction.Setting ?
for a new dataset remains an issue,and requires a small development data set.
How-ever, preliminary experiments with a third datasetshowed that ?
= .75 remains a good value.In the future, we will use keywords to re-trieve documents from a repository and recom-mend them to conversation participants by formu-lating topically-separate queries.Appendix: Conversation transcript ofAMI ES2005a meeting (00:00:5-00:01:52)The following transcript of a four-party conversa-tions (speakers noted A through D) was submittedto our keyword extraction method and a baselineone, generating respectively the two word cloudsshown in Figure 1.A: The only the only remote controlsI?ve used usually come with thetelevision, and they?re fairly basic.So uhD: Yeah.
Yeah.C: Mm-hmm.D: Yeah, I was thinking that as well,I think the the only ones that I?ve seenthat you buy are the sort of one forall type things where they?re, yeah.
Sopresumably that might be an idea toC: Yeah the universal ones.
Yeah.A: Mm.
But but to sell it for twentyfive you need a lot of neat features.For sure.D: put into.C: Yeah.D: Yeah, yeah.
Uh ?cause I mean, whatuh twenty five Euros, that?s about Idunno, fifteen Pounds or so?C: Mm-hmm, it?s about that.D: And that?s quite a lot for a remotecontrol.A: Yeah, yeah.C: Mm.
Um well my first thoughtswould be most remote controls are greyor black.
As you said they come withthe TV so it?s normally just your basicgrey black remote control functions, somaybe we could think about colour?
Makethat might make it a bit different fromthe rest at least.
Um, and as you say,we need to have some kind of gimmick, soum I thought maybe something like if youlose it and you can whistle, you knowthose things?D: Uh-huh.
Mm-hmm.
Okay.
The thekeyrings, yeah yeah.
Okay, that?s cool.C: Because we always lose our remotecontrol.B: Uh yeah uh, being as a MarketingExpert I will like to say like beforedeciding the cost of this remote controlor any other things we must see themarket potential for this product likewhat is the competition in the market?What are the available prices of theother remote controls in the prices?What speciality other remote controlsare having and how complicated it is touse these remote controls as compared toother remote controls available in themarket.D: Okay.B: So before deciding or beforefinalising this project, we must discussall these things, like and apart fromthis, it should be having a good lookalso, because people really uh liketo play with it when they are watchingmovies or playing with or playing withtheir CD player, MP three player likeany electronic devices.
They reallywant to have something good, having agood design in their hands, so, yes, allthis.AcknowledgmentsThe authors are grateful to the Swiss National Sci-ence Foundation for its financial support throughthe IM2 NCCR on Interactive Multimodal Infor-mation Management (see www.im2.ch).655ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish,Chong Wang, and David Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In Pro-ceedings of the 23rd Annual Conference on NeuralInformation Processing Systems (NIPS).Jean Carletta.
2007.
Unleashing the killer corpus:Experiences in creating the multi-everything AMIMeeting Corpus.
Language Resources and Evalu-ation Journal, 41(2):181?190.Christopher Cieri, David Miller, and Kevin Walker.2004.
The Fisher Corpus: a resource for the nextgenerations of speech-to-text.
In Proceedings of 4thInternational Conference on Language Resourcesand Evaluation (LREC), pages 69?71.Charles L. A. Clarke, Maheedhar Kolla, Gordon V.Cormack, Olga Vechtomova, Azin Ashkan, StefanBu?ttcher, and Ian MacKinnon.
2008.
Novelty anddiversity in information retrieval evaluation.
In Pro-ceedings of the 31st annual international ACM SI-GIR conference on Research and development in in-formation retrieval, pages 659?666.Andras Csomai and Rada Mihalcea.
2007.
Linkingeducational materials to encyclopedic knowledge.Frontiers in Artificial Intelligence and Applications,158:557.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ings of the 16th International Joint Conference onArtificial Intelligence (IJCAI 1999), pages 668?673,Stockholm, Sweden.Maryam Habibi and Andrei Popescu-Belis.
2012.
Us-ing crowdsourcing to compare document recom-mendation strategies for conversations.
In Work-shop on Recommendation Utility Evaluation: Be-yond RMSE (RUE 2011), page 15.David Harwath and Timothy J. Hazen.
2012.
Topicidentification based extrinsic evaluation of summa-rization techniques applied to conversational speech.In Proceedings of International Conference onAcoustics, Speech and Signal Processing (ICASSP),pages 5073?5076.
IEEE.Matthew D. Hoffman, David M. Blei, and FrancisBach.
2010.
Online learning for Latent DirichletAllocation.
Proceedings of 24th Annual Conferenceon Neural Information Processing Systems, 23:856?864.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP 2003),pages 216?223, Sapporo, Japan.Jingxuan Li, Lei Li, and Tao Li.
2012.
Multi-document summarization via submodularity.
Ap-plied Intelligence, 37(3):420?430.Hui Lin and Jeff Bilmes.
2011.
A class of submodularfunctions for document summarization.
In Proceed-ings of the 49th Annual Meeting of the ACL.Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.2009a.
Unsupervised approaches for automatic key-word extraction using meeting transcripts.
In Pro-ceedings of the 2009 Annual Conference of theNorth American Chapter of the ACL (HLT-NAACL),pages 620?628.Zhiyuan Liu, Wenyi Huang, Yabin Zheng, andMaosong Sun.
2009b.
Clustering to find exemplarterms for keyphrase extraction.
In Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2009), pages257?266.Zhiyuan Liu, Wenyi Huang, Yabin Zheng, andMaosong Sun.
2010.
Automatic keyphrase extrac-tion via topic decomposition.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2010), pages 366?376.Hans Peter Luhn.
1957.
A statistical approach tomechanized encoding and searching of literary in-formation.
IBM Journal of Research and Develop-ment, 1(4):309?317.Yutaka Matsuo and Mitsuru Ishizuka.
2004.
Key-word extraction from a single document using wordco-occurrence statistical information.
InternationalJournal on Artificial Intelligence Tools, 13(1):157?169.Andrew K. McCallum.
2002.
MALLET:A machine learning for language toolkit.http://mallet.cs.umass.edu.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2004), pages 404?411,Barcelona.George L. Nemhauser, Laurence A. Wolsey, and Mar-shall L. Fisher.
1978.
An analysis of approxi-mations for maximizing submodular set functions.Mathematical Programming Journal, 14(1):265?294.Ani Nenkova and Kathleen McKeown, 2012.
A Surveyof Text Summarization Techniques, chapter 3, pages43?76.
Springer.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.Information Processing and Management Journal,24(5):513?523.656Gerard Salton, Chung-Shu Yang, and Clement T. Yu.1975.
A theory of term importance in automatic textanalysis.
Journal of the American Society for Infor-mation Science, 26(1):33?44.Peter Turney.
1999.
Learning to extract keyphrasesfrom text.
Technical Report ERB-1057, NationalResearch Council Canada (NRC).Jinghua Wang, Jianyi Liu, and Cong Wang.
2007.Keyword extraction based on PageRank.
In Ad-vances in Knowledge Discovery and Data Mining(Proceedings of PAKDD 2007), LNAI 4426, pages857?864.
Springer-Verlag, Berlin.Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and LongQiu.
2007.
Document concept lattice for text un-derstanding and summarization.
Information Pro-cessing and Management, 43(6):1643?1662.657
