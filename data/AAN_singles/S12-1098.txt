First Joint Conference on Lexical and Computational Semantics (*SEM), pages 662?666,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguisticsjanardhan: Semantic Textual Similarity using Universal NetworkingLanguage graph matchingJanardhan SinghIIT Bombay,Mumbai, Indiajanardhan@cse.iitb.ac.inArindam BhattacharyaIIT Bombay,Mumbai, Indiaarindamb@cse.iitb.ac.inPushpak BhattacharyyaIIT Bombay,Mumbai, Indiapb@cse.iitb.ac.inAbstractSentences that are syntactically quite differentcan often have similar or same meaning.
TheSemEval 2012 task of Semantic Textual Sim-ilarity aims at finding the semantic similaritybetween two sentences.
The semantic repre-sentation of Universal Networking Language(UNL), represents only the inherent meaningin a sentence without any syntactic details.Thus, comparing the UNL graphs of two sen-tences can give an insight into how semanti-cally similar the two sentences are.
This paperpresents the UNL graph matching method forthe Semantic Textual Similarity(STS) task.1 IntroductionUniversal Networking language (UNL) gives thesemantic representation of sentences in a graphi-cal form.
By comparing the similarity of thesegraphs, we inherently compare only the semanticcontent of the two sentences, rather than compar-ing the similarities in the syntax.
Thus, the UNLgraph matching strategy is a natural choice for theSemantic Textual Similarity(STS) task of SemEval2012.
UNL graphs are also used in textual en-tailment and interlingua based machine translationtasks.
We use the UNL enconverter system at:http://www.cfilt.iitb.ac.in/UNL encoto generate the UNL graphs of the sentences.
For thetwo graphs, generated from the two sentences, wegive a similarity score by matching the two graphs.In the following sections we describe UNLmatching strategy.
section 2 describes the UNL sys-Figure 1: UNL graph for ?John eats rice?tem and why this approach is useful, section 3 de-scribes the matching algorithm, section 4 describesthe challenges faced in this approach, section 5 givesthe results and finally section 6 gives the conclusionand the future scope.2 Universal Networking LanguageThe Universal Networking Language gives a graph-ical representation of the semantics of a text in theform of hypergraphs.
The representation is at thesemantic level which allows mapping of the simi-lar meaning sentences having different syntax to thesame representation.
To exemplify this point, con-sider the UNL graphs generated for the followingsentences:Sentence 1: John ate rice.Sentence 2: Rice was eaten by John.The UNL graph generated from the system aregiven in figures 1 and 2 respectively.The UNL graph consists of three components:662Figure 2: UNL graph for ?Rice was eaten by John??
Universal Words?
Relations?
Attributes2.1 Universal WordsThe Universal Words (UWs) form the vocabulary ofthe Universal Networking Language.
They form thenodes of the UNL graph.
The words are normalizedto their basic lemma, for example, eats becomes eat.The Universal Word is, usually, followed by a dis-ambiguating constraint list which is mainly used fordisambiguating the sense of the Universal Word.
Forexample, John (iof > person), here the word John isdisambiguated as an instance of (iof) a person andrice is disambiguated to be in the class of (icl) propernoun.
The UNL generation system, uses a Universalword dictionary created using the wordnet.2.2 RelationsThe UNL manual describes 46 binary semantic re-lations among the Universal Words as given in UNLmanual.
These form the labelled arcs of the UNLgraph.
In the example of figures 1 and 2, the rela-tions agent (agt) and object (obj) are shown.
John isthe agent of the action eat and rice is the object ofthe action eat.
The UNL generation system gener-ated these relations using complex rules based on thedependency and constituency parser outputs, Word-net features and Named Entity recognizer output.2.3 AttributesAttributes are attached to the Universal Words toshow the speakers perspective for some subjectiveinformation in the text.
For the given example, withrespect to the speaker of the text, the action of eathappened in the past with respect to the speaker.This is represented by the attribute @past.The detailed description of the UNL standard canbe found in the UNL manual available online athttp://www.undl.org/unlsys/unl/unl2005/.The two sentences listed above, have the samesemantic content, although their syntax is different.One sentence is in the active voice, while the othersentence is in the passive.
But if we compare theUNL graphs of the two sentences, they are almostidentical, with an extra attribute @passive on themain verb eat in the second graph.
The graph match-ing of the two sentences results in a high score nearto 5.
Like voice, most of the syntactic variations aredropped when we move from syntactic to semanticrepresentation.
Thus, comparing the semantic rep-resentation of the sentences, is useful, to identifytheir semantic similarity.
The UNL generation sys-tem generates the attributes using similar features tothose for relation generation.3 UNL matchingThe UNL system available online at:http://www.cfilt.iitb.ac.in/UNL encoproduces graphs for the sentences by listing thebinary relations present in the graph.
An exampleof such a listing is :Sentence 3: A man is eating a banana by a tree.
[unl:1]agt ( eat(icl>eat>do, agt>thing,obj>thing):4.@present.@progress.@entry,man(icl>male>thing,equ>adult_male):2.
@indef )ins ( eat(icl>eat>do, agt>thing,obj>thing):4.@present.@progress.@entry,tree(icl>woody_plant>thing):9.
@indef )obj ( eat(icl>eat>do, agt>thing,obj>thing):4.@present.@progress.@entry,banana(icl>herb>thing,equ>banana_tree):6.
@indef )[\unl]663Sentence 4 : A man is eating a banana.
[unl:1]agt ( eat(icl>eat>do, agt>thing,obj>thing):4.@present.@progress.@entry,man(icl>male>thing,equ>adult_male):2.
@indef )obj ( eat(icl>eat>do, agt>thing,obj>thing):4.@present.@progress.@entry,banana(icl>herb>thing,equ>banana_tree):6.
@indef )[\unl]We treat the UNL graph of one sentence as goldunland the other as testunl.
The matching scorebetween the two is found using the followingformulation (Mohanty, 2008):score(testunl, goldunl)= (2?precision?recall)(precision+recall) (1)precision=?relation?testunl relation score(relation)(count(relations?testunl)) (2)recall=?relation?testunl relation score(relation)(count(relations?goldunl)) (3)relation score(relation)= avg(rel match, uw1score, uw2score) (4)rel match={1 if relation name matches0 otherwise(5)uwscore= avg(word score, attribute score) (6)word score={1 if universal word matches0 otherwise(7)attribute score= F1score(testunl attr, goldunl attr) (8)The matching scheme is based on the idea of theF1 score.
The two UNL graphs are a list of UNLrelations each.
Considering, one as the gold UNLgraph and the other as the test UNL graph, we canfind the precision and recall of the total relations thathave matched.
For the example given in section 2.4,the sentence 3 has three relations while sentence 4has two relations.
A correspondence between therelations agt of the two graphs and also the relationobj of the two graphs can be established based onthe universal words that they connect.
Each such re-lation match is given a score, explained later, whichis used in the calculation of the precision and recall.From the precision and recall the F1 score can beeasily calculated which becomes the total matchingscore of the two graphs.The relation score is obtained by averaging thescores of relation match, and the score of the twouniversal word matches.
The universal word matchscore has a component of the attributes that matchbetween the corresponding universal words.
Thisattribute matching is again the F1 score calculationsimilar to relation matching.
Matching the attributesof the universal words, contributes to the score of thematched universal word, which in turn contributesto the score of the matched relation.
Thus, matchingof the semantic relations has more weight than thematching of the attributes.The score obtained by this formulation is between0 and 1.
Another score between 0 and 1 is obtainedby flipping the goldunl graph to testunl and testunlto goldunl.
Average of these two scores is then mul-tiplied by 5 to give the final score.By this formulation, the score obtained by match-ing graphs for sentences 3 and 4 is 4.04 Challenges in the approachIn the UNL graph matching startegy we faced thefollowing challenges:4.1 Sentences with grammatical errorsMany of the sentences, especially, from the MSRpardataset, had minor grammatical errors.
The UNLgeneration requires grammatical correctness.
Someof the examples of such sentences are:?
The no-shows were Sens.
John Kerry of Mas-sachusetts and Bob Graham of Florida.664?
She countersued for $125 million, saying G+Jbroke its contract with her by cutting her outof key editorial decisions and manipulated themagazine?s financial figures.?
?She was crying and scared,?
said Isa Yasin, theowner of the store.Here, terms like G+J and punctuation errors as inthe third example lead to the generation of improperUNL graphs.
To handle such cases, the UNL gener-ation needs to get robust.4.2 Scoping errorsUNL graphs are hypergraphs, in which, a node canin itself be a UNL graph.
Scopes are given iden-tity numbers like :01,:02 and so on.
While matchingtwo different UNL graphs, this matching of scopeidentity numbers cannot be directly achieved.
Also,one graph may have different number of scopes ascompared to the other.
Hence, eventhough the UNLgraphs are generated correctly, due to scoping mis-matches the matching score goes down.
To tacklethis problem, the UNL graphs generated are con-verted into scopeless form before the matching isperformed.
Every UNL graph has an entry node,which is the starting node of the graph.
This is de-noed by an @entry attribute on the node.
Everyscope, too, has an entry node.
The idea for convert-ing the UNL graphs into scopeless form is to replacethe scope nodes by the graphs that these nodes repre-sent, with the connection to the original scope nodegoing to the entry node of the replacing graph.4.3 Incomplete or no graph generationIt was observed that for some of the sentences,the UNL generation system did not produce UNLgraphs or the generation was incomplete.
Some ofthese sentences are:?
The Metropolitan Transportation Authoritywas given two weeks to restore the $1.50 fareand the old commuter railroad rates, York de-clared.?
Long lines formed outside gas stations and peo-ple rushed to get money from cash machinesSunday as Israelis prepared to weather a strikethat threatened to paralyze the country.These, are due to some internal system errors ofthe UNL generation system.
To improve on this, theUNL generation system itself has to improve.5 ResultsBy adopting the methodology described in section 3,the following results were obtained on the differentdatasets.MSRpar 0.1936MSRvid 0.5504SMT-eur 0.3755On-WN 0.2888SMT-news 0.3387As observed, the performance is good for theMSRvid dataset.
This dataset consists of small andsimple sentences which are grammatically correct.The performance on this dataset should further im-prove by capturing the synonyms of the Univer-sal words while matching the UNL relations.
Theperformance for MSRpar dataset is low.
The sen-tences in this dataset are long and sometimes withminor grammatical errors resulting in incomplete orno UNL graphs.
As the UNL generation systembecomes more robust, the performance is expectedto improve quickly.
The overall result over all thedatasets is given in the following table.ALL ALLnrm Mean0.3431 0.6878 0.34816 Conclusion and Future ScopeThe UNL graph matching approach works well withgrammatically correct sentences.
The approach de-pends on the accuracy of the UNL generation sys-tem itself.
With the increase in the robustness of theUNL generation system, this approach seems natu-ral.
Since, the approach is unsupervised, it does notrequire any training data.
The matching algorithmcan be extended to include the synonyms of the Uni-versal Words while matching relations.ReferencesMohanty, R. and Limaye, S. and Prasad, M.K.
and Bhat-tacharyya, P. 2008.
Semantic Graph from EnglishSentences, Proceedings of ICON-2008: 6th Inter-national Conference on Natural Language ProcessingMacmillan Publishers, India665UNL Center of UNDL Foundation 2005 Uni-versal Networking Language (UNL) Spec-ifications Version 2005 Online URL:http://www.undl.org/unlsys/unl/unl2005/UNL enconversion system.
2012.
Online URL:http://www.cfilt.iitb.ac.in/UNL enco666
