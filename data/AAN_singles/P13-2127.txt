Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 725?730,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAnnotation of regular polysemy and underspecificationHe?ctor Mart?
?nez Alonso,Bolette Sandford PedersenUniversity of CopenhagenCopenhagen (Denmark)alonso@hum.ku.dk, bsp@hum.ku.dkNu?ria BelUniversitat Pompeu FabraBarcelona (Spain)nuria.bel@upf.eduAbstractWe present the result of an annotation taskon regular polysemy for a series of seman-tic classes or dot types in English, Dan-ish and Spanish.
This article describesthe annotation process, the results in termsof inter-encoder agreement, and the sensedistributions obtained with two methods:majority voting with a theory-compliantbackoff strategy, and MACE, an unsuper-vised system to choose the most likelysense from all the annotations.1 IntroductionThis article shows the annotation task of a corpusin English, Danish and Spanish for regular poly-semy.
Regular polysemy (Apresjan, 1974; Puste-jovsky, 1995; Briscoe et al 1995; Nunberg, 1995)has received a lot of attention in computationallinguistics (Boleda et al 2012; Rumshisky et al2007; Shutova, 2009).
The lack of available sense-annotated gold standards with underspecificationis a limitation for NLP applications that rely ondot types1 (Rumshisky et al 2007; Poibeau, 2006;Pustejovsky et al 2009).Our goal is to obtain human-annotated corpusdata to study regular polysemy and to detect it inan automatic manner.
We have collected a cor-pus of annotated examples in English, Danish andSpanish to study the alternation between sensesand the cases of underspecification, including acontrastive study between languages.
Here we de-scribe the annotation process, its results in termsof inter-encoder agreement, and the sense distri-butions obtained with two methods: majority vot-ing with a theory-compliant backoff strategy and,MACE an unsupervised system to choose the mostlikely sense from all the annotations.1The corpus is freely available athttp://metashare.cst.dk/repository/search/?q=regular+polysemy2 Regular polysemyVery often a word that belongs to a semantic type,like Location, can behave as a member of anothersemantic type, like Organization, as shown by thefollowing examples from the American NationalCorpus (Ide and Macleod, 2001) (ANC):a) Manuel died in exile in 1932 in England.b) England was being kept busy with other con-cernsc) England was, after all, an important winemarketIn case a), England refers to the English terri-tory (Location), whereas in b) it refers arguably toEngland as a political entity (Organization).
Thethird case refers to both.
The ability of certainwords to switch between semantic types in a pre-dictable manner is referred to as regular polysemy.Unlike other forms of meaning variation causedby metaphor or homonymy, regular polysemy isconsidered to be caused by metonymy (Apresjan,1974; Lapata and Lascarides, 2003).
Regular pol-ysemy is different from other forms of polysemyin that both senses can be active at the same in apredicate, which we refer to as underspecification.Underspecified instances can be broken down in:1.
Contextually complex: England was, afterall, an important wine market2.
Zeugmatic, in which two mutually exclusivereadings are coordinated: England is conser-vative and rainy3.
Vague, in which no contextual element en-forces a reading: The case of England is sim-ilar3 Choice of semantic classesThe Generative Lexicon (GL) (Pustejovsky, 1995)groups nouns with their most frequent metonymicsense in a semantic class called a dot type.
ForEnglish, we annotate 5 dot types from the GL:1.
Animal/Meat: ?The chicken ran away?
vs.725?the chicken was delicious?.2.
Artifact/Information : ?The book fell?
vs.?the book was boring?.3.
Container/Content: ?The box was red?
vs.?I hate the whole box?.4.
Location/Organization: ?England is far?vs.
?England starts a tax reform?.5.
Process/Result: ?The building took monthsto finish?
vs. ?the building is sturdy?.For Danish and Spanish, we have chosen Con-tainer/Content and Location/Organization.
Wechose the first one because we consider it themost prototypical case of metonymy from the oneslisted in the GL.
We chose the second one becausethe metonymies in locations are a common con-cern for Named-Entity Recognition (Johannessenet al 2005) and a previous area of research inmetonymy resolution (Markert and Nissim, 2009).4 Annotation SchemeFor each of the nine (five for English, two for Dan-ish, two for Spanish) dot types, we have randomlyselected 500 corpus examples.
Each example con-sists of a sentence with a selected headword be-longing to the corresponding dot type.
In spite ofa part of the annotation being made with a con-trastive study in mind, no parallel text was usedto avoid using translated text.
For English andDanish we used freely available reference corpora(Ide and Macleod, 2001; Andersen et al 2002)and, for Spanish, a corpus built from newswire andtechnical text (Vivaldi, 2009).For most of the English examples we used thewords in Rumshisky (2007), except for Loca-tion/Organization.
For Danish and Spanish wetranslated the words from English.
We expandedthe lists using each language?s wordnet (Pedersenet al 2009; Gonzalez-Agirre et al 2012) as the-saurus to make the total of occurrences reach 500after we had removed homonyms and other formsof semantic variation outside of the purview ofregular polysemy.For Location/Organization we have used high-frequency names of geopolitical locations fromeach of the corpora.
Many of them are corpus-specific (e.g.
Madrid is more frequent in theSpanish corpus) but a set of words is shared:Afghanistan, Africa, America, China, England,Europe,Germany, London.Every dot type has its particularities that we hadto deal with.
For instance, English has lexical al-ternatives for the meat of several common animals,like venison or pork instead of deer and pig.
Thislexical phenomenon does not impede metonymyfor the animal names, it just makes it less likely.In order to assess this, we have included 20 ex-amples of cow.
The rest of the dataset consists ofanimal names that do not participate in this lexicalalternation, like eel, duck, chicken, or sardine.We call the first sense in the pair of metonymsthat make up the dot type the literal sense, and thesecond sense the metonymic sense, e.g.
Locationis the literal sense in Location/Organization.Each block of 500 sentences belonging to adot type was an independent annotation subtaskwith an isolated description.
The annotator wasshown an example and had to determine whetherthe headword in the example had the literal,metonymic or the underspecified sense.
Figure 1shows an instance of the annotation process.Figure 1: Screen capture for a Mechanical Turkannotation instance or HITThis annotation scheme is designed with the in-tention of capturing literal, metonymic and under-specified senses, and we use an inventory of threepossible answers, instead of using Markert andNissim?s (Markert and Nissim, 2002; Nissim andMarkert, 2005) approach with fine-grained sensedistinctions, which are potentially more difficult toannotate and resolve automatically.
Markert andNissim acknowledge a mixed sense they define asbeing literal and metonymic at the same time.For English we used Amazon Mechanical Turk(AMT) with five annotations per example by turk-ers certified as Classification Masters.
Using AMTprovides annotations very quickly, possibly at theexpense of reliability, but it has been proven suit-able for sense-disambiguation task (Snow et al2008).
Moreover, it is not possible to obtain an-notations for every language using AMT.
Thus,for Danish and Spanish, we obtained annotationsfrom volunteers, most of them native or very pro-ficient non-natives.
See Table 1 for a summary ofthe annotation setup for each language.After the annotation task we obtained the agree-726Language annotators typeDanish 3-4 volunteerEnglish 5 AMTSpanish 6-7 volunteerTable 1: Amount and type of annotators per in-stance for each language.ment values shown in Table 2.
The table also pro-vides the abbreviated names of the datasets.Dot type Ao ?
?
?eng:animeat 0.86 ?
0.24 0.69eng:artinfo 0.48 ?
0.23 0.12eng:contcont 0.65 ?
0.28 0.31eng:locorg 0.72 ?
0.29 0.46eng:procres 0.5 ?
0.24 0.10da:contcont 0.32 ?
0.37 0.39da:locorg 0.73 ?
0.37 0.47spa:contcont 0.36 ?
0.3 0.42spa:locorg 0.52 ?0.28 0.53Table 2: Averaged observed agreement and itsstandard deviation and alphaAverage observed agreement (Ao) is the meanacross examples for the proportion of matchingsenses assigned by the annotators.
Krippendorff?salpha is an aggregate measure that takes chancedisagreement in consideration and accounts for thereplicability of an annotation scheme.
There arelarge differences in ?
across datasets.The scheme can only provide reliable (Artsteinand Poesio, 2008) annotations (?
> 0.6) for onedot type2.
This indicates that not all dot types areequally easy to annotate, regardless of the kind ofannotator.
In spite of the number and type of an-notators, the Location/Organization dot type givesfairly high agreement values for a semantic task,and this behavior is consistent across languages.5 Assigning sense by majority votingEach example has more than one annotation andwe need to determine a single sense tag for eachexample.
However, if we assign senses by major-ity voting, we need a backoff strategy in case ofties.The common practice of backing off to the mostfrequent sense is not valid in this scenario, wherethere can be a tie between the metonymic andthe underspecified sense.
We use a backoff thatincorporates our assumption about the relations2We have made the data freely available athttp://metashare.cst.dk/repository/search/?q=regular+polysemybetween senses, namely that the underspecifiedsense sits between the literal and the metonymicsenses:1.
If there is a tie between the underspecifiedand literal senses, the sense is literal.2.
If there is a tie between the underspec-ified and metonymic sense, the sense ismetonymic.3.
If there is a tie between the literal andmetonymic sense or between all three senses,the sense is underspecified.Dot type L M U V Beng:animeat 358 135 7 3 4eng:artinfo 141 305 54 8 48eng:contcont 354 120 25 0 25eng:locorg 307 171 22 3 19eng:procres 153 298 48 3 45da:contcont 328 82 91 53 38da:locorg 322 95 83 44 39spa:contcont 291 140 69 54 15soa:locorg 314 139 47 40 7Table 3: Literal, Metonymic and Underspecifiedsense distributions, and underspecified senses bro-ken down in Voting and BackoffThe columns labelled L, M and U in Table 3provide the sense distributions for each dot type.The preference for the underspecified sense variesgreatly, from the very infrequent for English inAnimal/Meat to the two Danish datasets where theunderspecified sense evens with the metonymicone.
However, the Danish examples have mostlythree annotators, and chance disagreement is thehighest for this language in this setup, i.e., thechance for an underspecified sense in Danish tobe assigned by our backoff strategy is the highest.Columns V and B show respectively whetherthe underspecified senses are a result of majorityvoting or backoff.
In contrast to volunteers, turk-ers disprefer the underspecified option and mostof the English underspecified senses are assignedby backoff.
However, it cannot be argued thatturkers have overused clicking on the first option(a common spamming behavior) because we cansee that two of the English dot types (eng:artinfo,eng:procres) have majority of metonymic senses,which are always second in the scheme (cf.
Fig.1).
Looking at the amount of underspecifiedsenses that have been obtained by majority vot-ing for Danish and Spanish, we suggest that thelevel of abstraction required by this annotation istoo high for turkers to perform at a level compara-727ble to that of our volunteer annotators.Figure 2: Proportion of non-literality in locationnames across languagesFigure 2 shows the proportion of non-literal(metonymic+underspecified) examples for theLocation/Organization words that are commonacross languages.
We can see that individualwords show sense skewdness.
This skewdness isa consequence of the kind of text in the corpus:e.g.
America has a high proportion of non-literalsenses in the ANC, where it usually means ?thepopulation or government of the US?.
Similarly,it is literal less than 50% of the times for the othertwo languages.
In contrast, Afghanistan is mostoften used in its literal location sense.6 Assigning senses with MACEBesides using majority voting with backoff , weuse MACE (Hovy et al 2013) to obtain the sensetag for each example.Dot type L M U D Ieng:animeat 340 146 14 .048 3eng:artinfo 170 180 150 .296 46eng:contcont 295 176 28 .174 0eng:locorg 291 193 16 .084 3eng:procres 155 210 134 .272 33da:contcont 223 134 143 .242 79da:locorg 251 144 105 .206 53spa:contcont 270 155 75 .074 56spa:locorg 302 146 52 .038 40Table 4: Sense distributions calculated withMACE, plus Difference and Intersection of under-specified senses between methodsMACE is an unsupervised system that usesExpectation-Maximization (EM) to estimate thecompetence of annotators and recover the mostlikely answer.
MACE is designed as a Bayesiannetwork that treats the ?correct?
labels as latentvariables.
This EM method can also be understoodas a clustering that assigns the value of the closestcalculated latent variable (the sense tag) to eachdata point (the distribution of annotations).Datasets that show less variation betweensenses calculated using majority voting and usingMACE will be more reliable.
Along the sense dis-tribution in the first three columns, Table 4 pro-vides the proportion of the senses that is differentbetween majority voting and MACE (D), and thesize of the intersection (I) of the set of underspec-ified examples by voting and by MACE, namelythe overlap of the U columns of Tables 3 and 4.Table 4 shows a smoother distribution of sensesthan Table 3, as majority classes are down-weighted by MACE.
It takes very different de-cisions than majority voting for the two En-glish datasets with lowest agreement (eng:artinfo,eng:procres) and for the Danish datasets, whichhave the fewest annotators.
For these cases, thediferences oscillate between 0.206 and 0.296.Although MACE increases the frequency ofthe underspecified senses for all datasets but one(eng:locorg), the underspecified examples in Ta-ble 3 are not subsumed by the MACE results.
Thevalues in the I column show that none of the un-derspecified senses of eng:contcont receive the un-derspecified sense by MACE.
All of these exam-ples, however, were resolved by backoff, as wellas most of the other underspecified cases in theother English datasets.
In contrast to the votingmethod, MACE does not operate with any theo-retical assumption about the relation between thethree senses and treats them independently whenassigning the most likely sense tag to each distri-bution of annotations.7 Comparison between methodsThe voting system and MACE provide differentsense tags.
The following examples (three fromeng:contcont and four from eng:locorg) show dis-agreement between the sense tag assigned by vot-ing and by MACE:d) To ship a crate of lettuce across the country,a trucker needed permission from a federalregulatory agency.e) Controls were sent a package containingstool collection vials and instructions for col-lection and mailing of samples.f) In fact, it was the social committee, andour chief responsibilities were to arrange forbands and kegs of beer .728g) The most unpopular PM in Canada?s mod-ern history, he introduced the Goods and Ser-vices Tax , a VAT-like national sales tax.h) This is Boston?s commercial and financialheart , but it s far from being an homogeneousdistrict [...]i) California has the highest number of peoplein poverty in the nation ?
6.4 million, includ-ing nearly one in five children.j) Under the Emperor Qianlong (Chien Lung),Kangxi?s grandson, conflict arose betweenEurope?s empires and the Middle Kingdom.All of the previous examples were tagged as un-derspecified by either the voting system or MACE,but not by both.
Table 5 breaks down the five an-notations that each example received by turkers inliteral, metonymic and underspecified.
The lasttwo columns show the sense tag provided by vot-ing or MACE.Example L M U VOTING MACEd) 2 2 1 U Le) 3 1 1 L Uf) 1 2 2 M Ug) 2 2 1 U Mh) 2 2 1 U Mi) 3 0 2 L Uj) 1 2 2 M UTable 5: Annotation summary and sense tags forthe examples in this sectionJust by looking at the table it is not immediatewhich method is preferable to assign sense tagsin cases that are not clear-cut.
In the case of i),we consider the underspecified sense more ade-quate than the literal one obtained by voting, justlike we are also more prone to prefer the under-specified meaning in f), which has been assignedby MACE.
In the case of h), we consider that thestrictly metonymic sense assigned by MACE doesnot capture both the organization- (?commercialand financial?)
and location-related (?district?)
as-pects of the meaning, and we would prefer the un-derspecifed reading.
However, MACE can alsoovergenerate the underspecified sense, as the vialsmentioned in example e) are empty and have nocontent yet, thereby being literal containers andnot their content.Examples d), g) and h) have the same dis-tribution of annotations?namely 2 literal, 2metonymic and 1 underspecified?but d) has re-ceived the literal sense from MACE, whereas theother two are metonymic.
This difference is a re-sult of having trained MACE independently foreach dataset.
The three examples receive the un-derspecified sense from the voting scheme, sinceneither the literal or metonymic sense is morepresent in the annotations.On the other hand, e) and i) are skewed towardsliterality and receive the literal sense by pluralitywithout having to resort to any backoff, but theyare marked as underspecified by MACE.8 ConclusionsWe have described the annotation process of aregular-polysemy corpus in English, Danish andSpanish which deals with five different dot types.After annotating the examples for their literal,metonymic or underspecified reading, we havedetermined that this scheme can provide reliable(?
over 0.60) annotations for one dot type.
Notall the dot types are equally easy to annotate.The main source of variation in agreement, andthus annotation reliability, is the dot type itself.While eng:animeat and eng:locorg appear the eas-iest, eng:artinfo and eng:procres obtain very low ?scores.9 Further workAfter collecting annotated data, the natural nextstep is to attempt class-based word-sense disam-biguation (WSD) to predict the senses in Tables 3and 4 using a state-of-the-art system like Nastaseet al(2012).
We will consider a sense-assignmentmethod (voting or MACE) as more appropriate ifit provides the sense tags that are easiest to learnby our WSD system.However, learnability is only one possible pa-rameter for quality, and we also want to developan expert-annotated gold standard to compare ourdata against.
We also consider the possibility ofdeveloping a sense-assignment method that reliesboth on the theoretical assumption behind the vot-ing scheme and the latent-variable approach usedby MACE.AcknowledgmentsThe research leading to these results has beenfunded by the European Commission?s 7th Frame-work Program under grant agreement 238405(CLARA).729ReferencesMette Skovgaard Andersen, Helle Asmussen, and J?rgAsmussen.
2002.
The project of korpus 2000 go-ing public.
In The Tenth EURALEX InternationalCongress: EURALEX 2002.J.
D. Apresjan.
1974.
Regular polysemy.
Linguistics.R.
Artstein and M. Poesio.
2008.
Inter-coder agree-ment for computational linguistics.
ComputationalLinguistics, 34(4):555?596.Gemma Boleda, Sabine Schulte im Walde, and ToniBadia.
2012.
Modeling regular polysemy: A studyon the semantic classification of catalan adjectives.Computational Linguistics, 38(3):575?616.Ted Briscoe, Ann Copestake, and Alex Lascarides.1995.
Blocking.
In Computational Lexical Seman-tics.
Citeseer.Aitor Gonzalez-Agirre, Egoitz Laparra, and GermanRigau.
2012.
Multilingual central repository ver-sion 3.0.
In Proceedings of the 8th InternationalConference on Language Resources and Evaluation(LREC 2012), pages 2525?2529.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,and Eduard Hovy.
2013.
Learning whom to trustwith mace.
In Proceedings of NAACL-HLT 2013.N.
Ide and C. Macleod.
2001.
The american nationalcorpus: A standardized resource of american en-glish.
In Proceedings of Corpus Linguistics 2001,pages 274?280.
Citeseer.J.
B. Johannessen, K. Haagen, K. Haaland, A. B.Jo?nsdottir, A. N?klestad, D. Kokkinakis, P. Meurer,E.
Bick, and D. Haltrup.
2005.
Named entity recog-nition for the mainland scandinavian languages.
Lit-erary and Linguistic Computing, 20(1):91.M.
Lapata and A. Lascarides.
2003.
A probabilisticaccount of logical metonymy.
Computational Lin-guistics, 29(2):261?315.K.
Markert and M. Nissim.
2002.
Towards a cor-pus annotated for metonymies: the case of locationnames.
In Proc.
of LREC.
Citeseer.K.
Markert and M. Nissim.
2009.
Data and modelsfor metonymy resolution.
Language Resources andEvaluation, 43(2):123?138.Vivi Nastase, Alex Judea, Katja Markert, and MichaelStrube.
2012.
Local and global context for super-vised and unsupervised metonymy resolution.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages183?193.
Association for Computational Linguis-tics.Malvina Nissim and Katja Markert.
2005.
Learningto buy a renault and talk to bmw: A supervised ap-proach to conventional metonymy.
In Proceedingsof the 6th International Workshop on ComputationalSemantics, Tilburg.Geoffrey Nunberg.
1995.
Transfers of meaning.
Jour-nal of semantics, 12(2):109?132.B.
S. Pedersen, S. Nimb, J. Asmussen, N. H. S?rensen,L.
Trap-Jensen, and H. Lorentzen.
2009.
Dan-net: the challenge of compiling a wordnet for danishby reusing a monolingual dictionary.
Language re-sources and evaluation, 43(3):269?299.Thierry Poibeau.
2006.
Dealing with metonymic read-ings of named entities.
arXiv preprint cs/0607052.J.
Pustejovsky, A. Rumshisky, J. Moszkowicz, andO.
Batiukova.
2009.
Glml: Annotating argumentselection and coercion.
In IWCS-8: Eighth Interna-tional Conference on Computational Semantics.J.
Pustejovsky.
1995.
The generative lexicon: a theoryof computational lexical semantics.A.
Rumshisky, VA Grinberg, and J. Pustejovsky.
2007.Detecting selectional behavior of complex types intext.
In Fourth International Workshop on Genera-tive Approaches to the Lexicon, Paris, France.
Cite-seer.Ekaterina Shutova.
2009.
Sense-based interpretationof logical metonymy using a statistical method.
InProceedings of the ACL-IJCNLP 2009 Student Re-search Workshop, pages 1?9.
Association for Com-putational Linguistics.R.
Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng.2008.
Cheap and fast?but is it good?
: evalu-ating non-expert annotations for natural languagetasks.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 254?263.
Association for Computational Lin-guistics.Jorge Vivaldi.
2009.
Corpus and exploitation tool:Iulact and bwananet.
In I International Confer-ence on Corpus Linguistics (CICL 2009), A surveyon corpus-based research, Universidad de Murcia,pages 224?239.730
