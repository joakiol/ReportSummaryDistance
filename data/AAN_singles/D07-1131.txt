Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1175?1181,Prague, June 2007. c?2007 Association for Computational LinguisticsMultilingual Deterministic Dependency Parsing Framework using Modi-fied Finite Newton Method Support Vector MachinesYu-Chieh Wu Jie-Chi Yang Yue-Shi LeeDept.
of Computer Science and In-formation EngineeringGraduate Institute of NetworkLearning TechnologyDept.
of Computer Science andInformation EngineeringNational Central University National Central University Ming Chuan UniversityTaoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwanbcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw lees@mcu.edu.twAbstractIn this paper, we present a three-step mul-tilingual dependency parser based on adeterministic shift-reduce parsing algo-rithm.
Different from last year, we sepa-rate the root-parsing strategy as sequentiallabeling task and try to link the neighborword dependences via a near neighborparsing.
The outputs of the root andneighbor parsers were encoded as featuresfor the shift-reduce parser.
In addition, thelearners we used for the two parsers andthe shift-reduce parser are quite different(conditional random fields and the modi-fied finite-Newton method support vectormachines).
We found that our methodcould benefit from the two-preprocessingstages.
To speed up training, in this year,we employ the MFN-SVM (modified fi-nite-Newton method support vector ma-chines) which can be learned in lineartime.
The experimental results show thatour method achieved the middle rank overthe 23 teams.
We expect that our methodcould be further improved via well-tunedparameter validations for different lan-guages.1 IntroductionThe target of dependency parsing is toautomatically recognize the head-modifierrelationships between words in natural languagesentences.
Usually, a dependency parser canconstruct a similar grammar tree with thedependency graph.
In this year, CoNLL-2007shared task (Nivre et al, 2007) focuses onmultilingual dependency parsing based on tendifferent languages (Hajic et al, 2004; Aduriz etal., 2003; Mart?
et al, 2007; Chen et al, 2003;B?hmova et al, 2003; Marcus et al, 1993;Johansson and Nugues, 2007; Prokopidis et al,2005; Czendes et al, 2005; Montemagni et al,2003; Oflazer et al, 2003) and domain adaptationfor English (Marcus et al, 1993; Johansson andNugues, 2007; Kulick et al, 2004; MacWhinney,2000; Brown, 1973) without taking the language-specific knowledge into consideration.
Theultimate goal of them is to design idealmultilingual and domain portable dependencyparsing systems.To accomplish the multilingual and domain ad-aptation tasks, we present a three-pass parsingmodel based on a shift-reducing algorithm (Ya-mada and Matsumoto, 2003; Chang et al, 2006),namely, neighbor parsing, root relation parsing,and shift-reduce parsing.
Our method favors exam-ining the ?un-parsed?
tokens, which incrementallyshrink.
At the beginning, the parsing direction ismainly determined by the amount of un-parsedtokens in the sentence with either forward orbackward parse.
In this step, the projective parsingmethod can be used to evaluate most of the non-projective Treebank datasets.
Once the direction isdetermined, the pseudo-projectivize transformationalgorithm (Nivre and Nilsson, 2005) converts mostnon-projective training data into projective anddecodes the parsed text into non-projective.
Here-after, both neighbor-parser and root-parser weretrained to discovery additional features for thedownstream shift-reduce parse model.
We foundthat the two additional features could improve theperformance.
Subsequently, the modified shift-reduce parsing algorithm starts to parse the finaldependencies with two-pass processing, i.e., pre-dict parse action and label the relations.1175In the remainder of this paper, Section 2 de-scribes the proposed parsing model, and Section 3lists the experimental settings and results.
Section4 presents the discussion and analysis of our parser.In Section 5, we draw the future direction and con-clusion.2 System DescriptionOver the past decades, many state-of-the-art pars-ing algorithm were proposed, such as head-wordlexicalized PCFG (Collins, 1998), Maximum En-tropy (Charniak, 2000), Maximum/Minimumspanning tree (MST) (McDonald et al, 2005),shift-reduce-based deterministic parsing (Yamadaand Matsumoto, 2003; Chang et al, 2006; Nivre,2003).
Among them, the shift-reduce methodswere shown to be the most efficient method, whichonly costs at most 2n~3n actions to parse a sen-tence (Chang et al, 2006; Nivre, 2003).
Chang etal.
(2006) further added the ?wait-right?
action tothe words that had children and could not be re-duced in current state.
This could avoid the so-called ?too early reduce?
problems.The overall parsing model can be found in Fig-ure 1.
Figure 2 illustrates the detail system spec ofour parsing model.Figure 1: System architecture2.1 Neighbor ParserAs shown in Figure 1, the first step is to identifythe neighbor head-modifier relations between twoconsecutive words.
Cheng et al (2006) also re-ported that the use of neighboring dependency at-tachment tagger enhance the unlabeled attachmentscores from 84.38 to 84.6 for 13 languages.
Usu-ally, it is the case that the select features are fixedand could not be tuned to capture the second orderfeatures (McDonald et al, 2006).
At each location,there the focus and next words are always com-pared.
It may fail to link the next and next+1 wordpair since the next word might be reduced due toan earlier wrong decision.?.
Parsing Algorithm:1.
Neighbor Parser2.
Root Parser3.
Shift-Reduce Algorithm (Yamadaand Matsumoto, 2003)?.
Parser Characteris-tics:1.
Deterministic2.
two-pass (Labeling separated)3.
Pseudo-Projective en(de)-coding(Nivre and Nilsson, 2005)?.
Learner: MFN-SVM(1) One-versus-All(2) Linear Kernel?.
Feature Set:1.
Lexical (Unigram/Bigram)2.
Fine-grained POS (and BiPOS)3.
Lemma/FEAT used?.
Post-Processing: Non-Used?.
Additional/External Resources: Non-UsedFigure 2: System specHowever, starting parsing based on the result ofneighbor parsing is not a good idea since it couldproduce error propagation problems.
Rather, weinclude the result of our neighbor parsing as fea-tures to increase the original feature set.
In the pre-liminary study, we found that the derived featuresare very useful for most languages.As conventional sequential tagging problems,such part-of-speech tagging and phrase chunking,we employ the conditional random fields (CRF) aslearners (Kudo et al, 2004).
The basic idea of theneighbor parsing can be shown in Figure 3.The first and second colums in Figure 3 repre-sents the basic word and fine-grained POS froms,while the third column indicates if this word hasthe LH (left-head) or RH (right-head) with associ-ated relations or O (no neighbor head in either leftor right neighbor word).
The used features are:Word, fine-grained POS, bigram, and bi-POS withcontext window = 2(left) and 4(right)1176Figure 3: Sequential tagging model for neighborparseUnfortunately, for some languages, like Chi-nese and Czech, training with CRF is because ofthe large number of features and the head relations.To make it practical, we focus on just three types:left head, right head, and out-of-neighbor.
Thiseffectively reduces most of the feature space forthe CRF.
The training time for the neighbor parserwith only three categories is less than 5 minuteswhile it takes three days with taking all the relationtag into account.2.2 Root ParserAfter the neighbor parse, the tagged labels aregood features for the root parse.
In the secondstage, the root parser identifies the root words inthe sentence.
Nevertheless, for some languages,such as Arabic and Czech, the roots might be sev-eral types as against to Chinese and English inwhich the number of labels of roots is merely one.Similar to the neighbor parser, we also take theroot label into account.
As noted, for Chinese andEnglish, the goal of the root parser can be reducedto determine whether the current word is root ornot.Figure 4: Sequential tagging model for neighborparseSimilar to the neighbor parse, the root parsingtask can also be treated as a sequential taggingproblem.
Figure 4 shows the basic concept of theroot parser.
The third column is mainly derivedfrom the neighbor parser, while the fourth columnrepresents whether the current word is a root withrelation or not.2.3 Parsing AlgorithmAfter adding the neighbor and root parser output asfeatures, in the final stage, the modified Yamada?sshift-reduce parsing algorithm (Yamada and Ma-tsumoto, 2003) is then run.
This method is deter-ministic and can deal with projective data only.There are three basic operation (action) types: Shift(S), Left (L), and Right (R).
The operation ismainly determined via the classifier according tothe selected features (see 2.4).
Each time, the op-eration is applied to two unparsed words, namely,focus and next.
If there exists an arc between thetwo words (either left or right), then the head offocus or next word is found; otherwise (i.e., shift),next two words are considered at next stage.
Thismethod could be economically performed viamaintaining two pointers, focus, and next withoutan explicit stack.
The parse operation is iterativelyrun until no more relation can be found in the sen-tence.In 2006, Chang et al (2006) further reportedthat the use of ?step-back?
in comparison to theoriginal ?stay?.
Furthermore, they also add the?wait-left?
operations to prevent the ?too early re-duce?
problems.
In this way, the parse actions canbe reduced to be bound in 3n where n is the num-ber of words in a sentence.Now we compare the adopted parsing algorithmin this year to the one we employed last year (Wuet al, 2006a).
The common characteristics are:1. the same number of parse operations (4)2. shift-reduce3.
linearly scaled4.
deterministic and projectiveOn the contrary, their parse actions are quite dif-ferent.
Therefore these two methods have differentrun time.
This gives the two methods rise to differ-ent iterative times.
The main reason is that thestep-back might trace back to previous words,which can be viewed as pop the top words on thestack back to the unparsed strings, while theNivre?s method does not trace-back any two words1177in the stack.
In other words, if a word is pushedinto the stack, it will no longer be compared withthe other deeper words inside the stack.
Hencesome of the non-root words in the stack remain tobe parsed.
A simple solution is to adopt an exhaus-tive post-processing step for the unparsed words inthe stack (details in (Wu et al, 2006a, 2006b)).A good advantage of the step-back is that it cantrace back to the unparsed words in the stack.
Buttheoretically, the required parse actions still morethan the Nivre?s algorithm (2n vs. 3n).By adopting the projectivized en/de-coding overthe modified Yamada?s algorithm, we can treat thewords that do not have a parent as roots.
Thus, forsome languages (e.g.
Czech and Arabic), the mul-tiple root problem can be easily solved.
In this yearwe separate the parse action and the relation labelinto two stages as opposed to having one pass lastyear.
In this way, we can simply adopt a sequentialtagger to auto-assign the relation labels after thewhole sentence is parsed.2.4 Features and LearnersUnlike last year, we did separate the action predic-tion and the label recognition into two stageswhere the one of the learners could provide moreinformation to another.
The used features of thetwo learners are quite similar and listed as follows:Basic feature type (for previous 2 and next 3 words):Word, POS (fine-grained), Lemma, FEAT, NParse,RParseEnhanced feature type:Bigram, BiPOS for focus and next wordsprevious two parse actionsFor label recognition:Label tag to its head, label tags for previous twowordsIn this paper, we replicate and modify the modi-fied finite Newton support vector machines (MFN-SVM) (Keerthi and DeCoste, 2005) as the learner.The MFN-SVM is a very efficient SVM opti-mization method which linearly scales with thenumber of training examples.
Usually, the trainedmodels from MFN-SVM are quite large that couldnot be processed in practice.
We therefore definedthe positive lower bound (10-10) and the negativeupper bound (-10-10) to eliminate values that tendto be zero.However, the SVM is a binary classifier whichonly recognizes true or false.
For multiclass prob-lem, we use the so-called one-versus-all (OVA)method with linear kernel to combine the results ofeach individual classifier.
The final class in testingphase is mainly determined by selecting the maxi-mum similarity.For all languages, our parser uses the same set-tings and features.
For all the languages (except forBasque and Turkish), we use backward parsingdirection to keep the un-parsed token rate low.3 Experimental Result3.1 Dataset and Evaluation MetricsThe testing data is provided by the (Nivre et al,2007) which consists of 10 language treebanks.More detailed descriptions of the dataset can befound at the web site1.
The experimental results aremainly evaluated by the unlabeled and labeled at-tachment scores.
CoNLL also provided a perlscript to automatic compute these rates.3.2 ResultsTable 1 presents the overall parsing performanceof the 10 languages.
As shown in Table 1, we listtwo parsing results at column B and column C(new and old).
It is worth to note that the result Bis produced by training the neighbor parser withfull labels instead of the three categories,left/right/out-of-neighbor.
A is the official pro-vided parse results.
Some of the parsing results inA did not include the enhanced feature type andneighbor/root parses due to the time limitation.
Forthe domain adaptation task, we directly use thetrained English model to classify the PChemtb andCHILDES corpora without further adjustment.In addition, we also apply the Maltparser 0.4,which is implemented with the Nivre?s algorithm(Nivre et al, 2006) to be compared.
The Maltpaseralso includes the SVM and memory-based learner(MBL).
Nevertheless, the training time complexityof the SVM in Maltparser is not linear time asMFN-SVM.
Therefore we use the default MBLand feature model 3 (M3) in this experiment.
Tomake a fair comparison, the input training data wasalso projectivized through the same pseudo-projective encoding/decoding methods.1 http://nextens.uvt.nl/depparse-wiki/SharedTaskWebsite1178To perform the significant test, we evaluate thestatistical difference among the three results.
If theanswer is ?Yes?, it means the two systems are sig-nificant difference under at least 95% confidencescore (p < 0.05).The final column of the Table 1 lists the non-root words unparsed rate of the modified Ya-mada?s method and the Nivre?s parsing modelwhich we employed last year.
Among 10 lan-guages, we can find that the modified Yamada?smethod outperform our old method in five lan-guages, while fail to win in three languages.
Wedid not report the comparative study between theforward parsing and backward parsing directionshere since only the two languages (Basque andTurkish) were better in performing forward direc-tion.4 DiscussionNow we turn to discuss the improvement of the useof the neighbor parse and root parse.
All of the ex-periments were conducted by additional runswhere we removed the neighbor and root parseoutputs from the feature set.
In this experiment, wereport four representative languages that tend toachieve the best and worst improvements.
Table 2lists the comparative study of the four languages.As listed in Table 2, both English and Chinesegot substantial benefit from the use of the twoparsers.
As observed by (Isozaki et al, 2004), in-corporating both top-down (root find) and bottom-up (base-NP) can yield better improvement overthe Yamada?s parsing algorithm.
Thus, instead ofpre-determining the root and base-phrase structures,the tagging results of the neighbor and root parserswere included as new features to add wider infor-mation for the shift-reduce parser.
It is also inter-esting to link neighbors and determine the rootbefore parsing.
We plan to compare it with outmethod in the future.Table 2: The effective of the used Neighbor/RootParser in the selected four languagesWith N/R Parser WithoutChinese 79.29 75.51English 84.27 79.49Basque 72.26 72.32Turkish 75.65 76.60On the other hand, we also found that 2 out ofthe 10 languages had been negatively affected bythe neighbor and root parsers.
In Basque they madea marginally negative improvement, and in theTurkish the two parsers did decrease the originalparsing models.
We further observed that the maincause is that the weak performance of the neighborparser.
In Turkish, the recall/precision rates of theneighbor dependence are 92.61/93.12 with includeneighbor parse outputs, while it achieved93.71/93.51 with purely run the modified Ya-mada?s method.
We can expect that the resultcould achieve higher LAS score when the neighborparser is improved.
As mentioned in section 2.1,2.2, the selected features for the two parsers areunified for the 10 languages.
It is not surprisingTable 1: A general statistical table of labeled attachment score, test and un-parsed rate (percentage)Statistic test  Un-Parsed Rate Language A (Official)B(Corrected)C(Malt-Parser 0.4) A vs B A vs C B vs C Old NewArabic 66.16 70.71 56.67 Yes No Yes 1.08% 0.69%Basque 70.71 72.26 57.79 Yes Yes Yes 3.04% 3.72%Catalan 81.44 81.44 76.36 Yes No No 0.45% 0.27%Chinese 74.69 79.29 68.15 Yes Yes Yes 0.00% 0.00%Czech 66.72 70.24 56.96 Yes No Yes 4.17% 3.87%English 79.49 84.27 75.53 Yes Yes Yes 1.66% 0.84%Greek 70.63 77.64 58.81 No Yes Yes 2.26% 2.12%Hungarian 69.08 71.98 59.41 Yes Yes Yes 3.88% 5.38%Italian 78.79 78.38 74.08 Yes No Yes 0.63% 0.63%Turkish 72.52 75.65 64.41 Yes Yes Yes 4.93% 5.54%pchemtb_closed 55.31** 73.35 - - - - - -*CHILDES_closed 52.89 58.29 - - - - - -* The CHILDES data does not contain the relation tag, instead, the unlabeled attachment score is listed** The original submission of the pchemtb_closed task can not pass through the evaluator and hence is not the official score.
After correctingthe format problems, the actual LAS score should be 55.31.1179that for certain data the fixed feature set might per-form even worse than the original shift-reduceparser.
A better way is to validate the features withvariant settings for different languages.
We left thefeature engine task as future work.5 Conclusion and Future RemarksMultilingual dependency parsing investigates onproposing a general framework of dependenceparsing algorithms.
This paper presents and ana-lyzes the impact of two preprocessing components,namely, neighbor parsing and root-parsing.
Thosetwo parsers provide very useful additional featuresfor downstream shift-reduce parser.
The experi-mental results also demonstrated that the use of thetwo components did improve results for the se-lected languages.
In the error-analysis, we also ob-served that for some languages, parameter tuningand feature selection is very important for systemperformance.In the future, we plan to report the actual per-formance with replacing the MFN-SVM by thepolynomial kernel SVM.
In our pilot study, the useof approximate-polynomial kernel (Wu et al, 2007)outperforms the linear kernel SVM in Chinese andArabic.
Also, we are investigating how to convertthe shift-reduce parser into approximate N-bestparser efficiently.
In this way, the parse rerankingalgorithm can be adopted to further improve theperformance.ReferencesA.
Abeill?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A.Diaz de Ilarraza, A. Garmendia and M. Oronoz.
2003.Construction of a Basque Dependency Treebank.
InProc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT), pages 201?204.A.
B?hmov?, J. Hajic, E. Hajicov?
and B. Hladk?.
2003.The PDT: a 3-level annotation scenario.
In Abeill?
(2003), chapter 7, 103?127.R.
Brown.
1973.
A First Language: The Early Stages.Harvard University Press.M.
W. Chang, Q.
Do, and D. Roth.
2006.
MultilingualDependency Parsing: A Pipeline Approach.
In RecentAdvances in Natural Language Processing, pages195-204.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C.Huang and Z. Gao.
2003.
Sinica Treebank: DesignCriteria, Representational Issues and Implementation.In Abeill?
(2003), chapter 13, pages 231?248.Y.
Cheng, M. Asahara and Y. Matsumoto.
2006.
Multi-lingual Dependency Parsing at NAIST.
In Proc.
ofthe 10th Conference on Natural Language Learning,pages 191-195.D.
Czendes, J. Csirik, T. Gyim?thy, and A. Kocsor.2005.
The Szeged Treebank.
Springer.J.
Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. Beska.2004.
Prague Arabic Dependency Treebank: Devel-opment in Data and Tools.
In Proc.
of the NEMLARIntern.
Conf.
on Arabic Language Resources andTools, pages 110?117.H.
Isozaki; H. Kazawa; T. Hirao.
2004.
A DeterministicWord Dependency Analyzer Enhanced With Prefer-ence Learning.
In Proc.
of the 20th InternationalConference on Computational Linguistics, pages275-281.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference onComputational Linguistics (NODALIDA).S.
Keerthi and D. DeCoste.
2005.
A modified finiteNewton method for fast solution of large scale linearSVMs.
Journal of Machine Learning Research.
6:341-361.S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-Donald, M. Palmer, A. Schein, and L. Ungar.
2004.Integrated annotation for biomedical information ex-traction.
In Proc.
of the Human LanguageTechnology Conference and the Annual Meeting ofthe North American Chapter of the Association forComputational Linguistics (HLT/NAACL).B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19(2):313?330.M.
A.
Mart?, M.
Taul?, L. M?rquez and M. Bertran.2007.
CESS-ECE: A Multilingual and MultilevelAnnotated Corpus.
Available for download from:http://www.lsi.upc.edu/~mbertran/cess-ece/.R.
McDonald, K. Lerman and F. Pereira.
2006.Multilingual Dependency Analysis with a Two-StageDiscriminative.
In Proc.
of the 10th Conference onNatural Language Learning, pages 216-220.1180S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari, O.Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M.Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D.Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R.Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeill?
(2003), chapter 11,pages 189?210.J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
In Proc.
of the InternationalWorkshop on Parsing Technology, pages 149-160.J.
Nivre, and J. Nilsson.
2005.
Pseudo-projectivedependency Parsing.
In Proc.
of the 43rd AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 99-106.J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit, and S. Marinov.2006.
Labeled pseudo-projective dependency parsingwith support vector machines.
In Proc.
of the 10thConference on Natural Language Learning, pages221-225.J.
Nivre, J.
Hall, S. K?bler, R. McDonald, J. Nilsson, S.Riedel, and D. Yuret.
2007.
The CoNLL 2007 sharedtask on dependency parsing.
In Proc.
of the JointConf.
on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL).K.
Oflazer, B.
Say, D. Zeynep Hakkani-T?r, and G. T?r.2003.
Building a Turkish treebank.
In Abeill?
(2003),chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H.Papageorgiou, and S. Piperidis.
2005.
Theoreticaland practical issues in the construction of a Greekdepen- dency treebank.
In Proc.
of the 4th Workshopon Treebanks and Linguistic Theories (TLT), pages149?160.T.
Kudo, K, Yamamoto, and Y. Matsumoto.
2004.Appliying conditional random fields to Japanesemorphological analysis, In Proc.
of the 2004Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2004), pages 230-237.Y.
C. Wu, Y. S. Lee, and J. C. Yang.
2006a.
Theexploration of deterministic and efficient dependencyparsing.
In Proc.
of the 10th Conference onComputational Natural Language Learning, pages241-245.Y.
C. Wu, J. C. Yang, and Q. X. Lin.
2006b.Description of the NCU Chinese word segmentationand named entity recognition system for SIGHANbakeoff 2006.
In Proc.
of the 5th SIGHAN Workshopon Chinese Language Processing, pages 209-212.Y.
C. Wu, J. C. Yang, and Y. S. Lee.
2007.
An Ap-proximate Approach for Training Polynomial KernelSVMs in Linear Time.
In Proc.
of the 45th AnnualMeeting of the Association for ComputationalLinguistics (ACL), in press.H.
Yamada and Y. Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.In Proc.
of the 8th International Workshop onParsing Technologies, pages 195?206.1181
