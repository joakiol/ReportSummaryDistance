Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2245?2254,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsKnowledge-Based Semantic Embedding for Machine TranslationChen Shi?
?Shujie Liu?Shuo Ren?Shi Feng?Mu Li?Ming Zhou?Xu Sun?Houfeng Wang??
?MOE Key Lab of Computational Linguistics, Peking University?Microsoft Research Asia?Shanghai Jiao Tong University?Collaborative Innovation Center for Language Ability{shichen, xusun, wanghf}@pku.edu.cn sjtufs@gmail.com{shujliu, v-shuren, muli, mingzhou}@microsoft.comAbstractIn this paper, with the help of knowl-edge base, we build and formulate a se-mantic space to connect the source andtarget languages, and apply it to thesequence-to-sequence framework to pro-pose a Knowledge-Based Semantic Em-bedding (KBSE) method.
In our KB-SE method, the source sentence is firstlymapped into a knowledge based seman-tic space, and the target sentence is gen-erated using a recurrent neural networkwith the internal meaning preserved.
Ex-periments are conducted on two transla-tion tasks, the electric business data andmovie data, and the results show that ourproposed method can achieve outstandingperformance, compared with both the tra-ditional SMT methods and the existingencoder-decoder models.1 IntroductionDeep neural network based machine translation,such as sequence-to-sequence (S2S) model (Choet al, 2014; Sutskever et al, 2014), try to learntranslation relation in a continuous vector space.As shown in Figure 1, the S2S framework containstwo parts: an encoder and a decoder.
To compressa variable-length source sentence into a fixed-sizevector, with a recurrent neural network (RNN), anencoder reads words one by one and generates asequence of hidden vectors.
By reading all thesource words, the final hidden vector should con-tain the information of source sentence, and it iscalled the context vector.
Based on the contextvector, another RNN-based neural network is usedto generate the target sentence.
?This work was done while the first author was visitingMicrosoft Research.yT?
y2 y1X1 X2 XTDecoderEncoderc?
?
??
?4G ??
?
???
?
?
????
?
?I want a white 4Gcellphone with abig screen.Figure 1: An illustration of the RNN-based neuralnetwork model for Chinese-to-English machinetranslationThe context vector plays a key role in the con-nection of source and target language spaces, andit should contain all the internal meaning extractedfrom source sentence, based on which, the decodercan generate the target sentence keeping the mean-ing unchanged.
To extract the internal meaningand generate the target sentence, S2S frameworkusually needs large number of parameters, and abig bilingual corpus is acquired to train them.In many cases, the internal meaning is not easyto learn, especially when the language is informal.For the same intention, there are various expres-sions with very different surface string, which ag-gravates the difficulty of internal meaning extrac-tion.
As shown in Table 1, there are three differentexpressions for a same intention, a customer wantsa white 4G cellphone with a big screen.
The firstand second expressions (Source1 and Source2) arewordy and contain lots of verbiage.
To extrac-t the internal meaning, the encoder should ignorethese verbiage and focus on key information.
Thisis hard for the encoder-decoder mechanism, sinceit is not defined or formulated that what kind ofinformation is key information.
The meaning s-2245X1S ource Grounding?
?
??
?
4G??
?
???
??
???
?
?
?I want a white 4Gcellphone with abig screen.X2 XT y1Target Generationy2 yTSemantic SpaceCategory.cellphoneAppearance.color.whiteAppearance.size.big_screenNetwork.4G_networkFunction.ability.smartPrice.$NUMPeople.my_fatherCarrier.China_UnicomBrand.iPhoneOS.AndroidPeople.students?
?Source Sentence Target SentenceFigure 2: An illustration of Knowledge-Based Semantic Embedding (KBSE).Source1 ?????????
4G????????
?Source2 ?????
4G?????????????
?Source3 ???????????
4G??
?Intention I want a white 4G cellphone with a big screen.Enc-Dec I need a 4G cellphone with a big screen.Table 1: An example of various expressions for asame intention.pace of the context vector is only a vector spaceof continuous numbers, and users cannot add ex-ternal knowledge to constrain the internal mean-ing space.
Therefore, the encoder-decoder system(Enc-Dec) does not generate the translation of ????
?/?white?, and fails to preserve the correctmeaning of Source1, shown in Table 1.No matter how different between the surfacestrings, the key information is the same (wan-t, white, 4G, big screen, cellphone).
Thisphenomenon motivates a translation process as:we firstly extract key information (such as en-tities and their relations) from the source sen-tence; then based on that, we generate target sen-tence, in which entities are translated with un-changed predication relations.
To achieve this,background knowledge (such as, phone/computer,black/white, 3G/4G) should be considered.In this paper, we propose a Knowledge-BasedSemantic Embedding (KBSE) method for ma-chine translation, as shown in Figure 2.
Our KBSEcontains two parts: a Source Grounding part toextract semantic information in source sentence,and a Target Generation part to generate targetsentence.
In KBSE, source monolingual data anda knowledge base is leveraged to learn an explic-it semantic vector, in which the grounding spaceis defined by the given knowledge base, then thesame knowledge base and a target monolingual da-ta are used to learn a natural language generator,which produce the target sentence based on thelearned explicit semantic vector.
Different fromS2S models using large bilingual corpus, our KB-SE only needs monolingual data and correspond-ing knowledge base.
Also the context/semanticvector in our KBSE is no longer implicit contin-uous number vector, but explicit semantic vector.The semantic space is defined by knowledge base,thus key information can be extracted and ground-ed from source sentence.
In such a way, users caneasily add external knowledge to guide the modelto generate correct translation results.We conduct experiments to evaluate our KB-SE on two Chinese-to-English translation tasks,one in electric business domain, and the other inmovie domain.
Our method is compared withphrasal SMT method and the encoder-decodermethod, and achieves significant improvement inboth BLEU and human evaluation.
KBSE is al-so combined with encoder-decoder method to getfurther improvement.In the following, we first introduce our frame-work of KBSE in section 2, in which the details ofSource Grounding and Target Generation are il-lustrated.
Experiments is conducted in Section 3.Discussion and related work are detailed in Sec-tion 4, followed by conclusion and future work.22462 KBSE: Knowledge-Based SemanticEmbeddingOur proposed KBSE contains two parts: SourceGrounding part (in Section 2.1) embeds thesource sentence into a knowledge semantic space,in which the grounded semantic information canbe represented by semantic tuples; and TargetGeneration part (in Section 2.2) generates the tar-get sentence based on these semantic tuples.2.1 Source GroundingSource ?????????
4G????????
?Category.cellphoneTuples Appearance.color.whiteAppearance.size.big screenNetwork.4G networkTable 2: Source sentence and the grounding result.Grounding result is organized as several tuples.As shown in Table 2, given the source sentence,Source Grounding part tries to extract the seman-tic information, and map it to the tuples of knowl-edge base.
It is worth noticing that the tuples arelanguage-irrelevant, while the name of the enti-ties inside can be in different languages.
To getthe semantic tuples, we first use RNN to encodethe source sentence into a real space to get thesentence embedding, based on which, correspond-ing semantic tuples are generated with a neural-network-based hierarchical classifier.
Since theknowledge base is organized in a tree structure, thetuples can be seen as several paths in the tree.
ForR ootCategory Network?
AppearanceComputer Cellphone 4G 3 G Size Shape ColorLaptop Desktop?
whitered?
?small ?big_screenFigure 3: Illustration of the tuple tree for Table2.
Each tuple extracted from source sentence canbe represented as a single path (solid line) in tupletree.
There are 4 solid line paths representing 4tuples of Table 2.
The path circled in dashed linesstands for the tuple Appearance.color.white.input layerembedding layer  fhidden layer  g ht ht-1HWetuple treextdotrtLR classifierFigure 4: Illustration of Source Grounding.
Theinput sentence x is transformed through an embed-ding layer f and a hidden layer g. Once we getthe sentence embedding H , we calculate the innerproduct of H and the weight Wefor the specificedge e, and use a logistic regression as the classi-fier to decide whether this edge should be chosen.tuples in Table 2, Figure 3 shows the correspond-ing paths (in solid lines).2.1.1 Sentence EmbeddingSentence embedding is used to compress thevariable-length source sentence into a fixed-sizecontext vector.
Given the input sentence x =(x1... xT), we feed each word one by one intoan RNN, and the final hidden vector is used as thesentence embedding.
In detail, as shown in Fig-ure 4, at time-stamp t, an input word xtis fed intothe neural network.
With the embedding layer f ,the word is mapped into a real vector rt= f(xt).Then the word embedding rtis fed into an RNNg to get the hidden vector ht= g(rt, ht?1).
Weinput the words one by one at time 1, 2, ..., T , andget the hidden vectors h1, h2, ..., hT.
The last hid-den state hTshould contain all the information ofthe input sentence, and it is used as the sentenceembedding H .
To model the long dependencyand memorize the information of words far fromthe end, Gated Recurrent Unit(GRU) (Cho et al,2014) is leveraged as the recurrent function g.2.1.2 Tuple GenerationIn our system, we need a tuple tree for tuple gen-eration.
For those knowledge base who is natural-ly organized as tree structure, such as Freebase,we use its own stucture.
Otherwise, we manu-ally build the tuple tree as the representation ofthe introduced knowledge base.
Given a knowl-2247edge base for a specific domain, we divide the in-tention of this domain into several classes, whileeach class has subclasses.
All the classes abovecan be organized as a tree structure, which is thetuple tree we used in our system, as shown in Fig-ure 3.
It is worth noticing that the knowledge basecaptures different intentions separately in differenttree structures.Following the hierarchical log-bilinear model(HLBL) (Mnih and Hinton, 2009; Mikolov et al,2013), based on the sentence embedding H , webuild our neural-network-based hierarchical clas-sifier as follows: Each edge e of tuple tree has aweight vector we, which is randomly initialized,and learned with training data.
We go through thetuple tree top-down to find the available paths.
Foreach current node, we have a classifier to decidewhich children can be chosen.
Since several chil-dren can be chosen at the same time independent-ly, we use logistic regression as the classifier foreach single edge, rather than a softmax classifierto choose one best child node.For the source sentence and corresponding tu-ples in table 2, in the first layer, we should choosethree children nodes: Category, Appearance andNetwork, and in the second layer with the paren-t node Appearance, two children nodes color andsize should be selected recursively.
As shown inFigure 4, the probability to choose an edge e withits connected child is computed as follows:p(1|e,H) =11 + e?we?H(1)where the operator ?
is the dot product function.The probability of the tuples conditioned on thesource sentence p(S|x1... xT) is the product ofall the edges probabilities, calculated as follows:p(S|x1... xT) = p(S|H)=?e?Cp(1|e,H)?e?/?Cp(0|e?, H)where p(1|e,H) is the probability for an edge ebelonging to the tuple set S, and p(0|e?, H) is theprobability for an edge e?not in the tuple set S.2.2 Target GenerationWith the semantic tuples grounded from sourcesentence, in this section, we illustrate how to gen-erate target sentence.
The generation of the targetsentence is another RNN, which predicts the nextword yt+1conditioned on the semantic vector Cand all the previously predicted words y1, ..., yt.Given current word yt, previous hidden vectorht?1, and the semantic vector C, the probabilityof next target word yt+1is calculated as:ht= g(ht?1, yt, C) (2)p(yt+1|y1...yt, C) =es(yt+1,ht)?y?es(y?,ht)(3)where equation (2) is used to generate the next hid-den vector ht, and equation (3) is the softmax func-tion to compute the probability of the next wordyt+1.
For the recurrent function g in equation (2),in order to generate target sentence preserving thesemantic meaning stored in C , we modified GRU(Cho et al, 2014) following (Wen et al, 2015;Feng et al, 2016):rt= ?
(Wryt+ Urht?1+ Vrct)h?t= tanh(Wyt+ U(rtht?1) + V ct)zt= ?
(Wzyt+ Uzht?1+ Vzct)dt= ?
(Wdyt+ Udht?1+ Vdct)ct= dtct?1ht= (1 ?
zt) h?t+ ztht?1+ tanh(Vhct)in which, ctis the semantic embedding at time t,which is initialized withC, and changed with a ex-traction gate dt.
The introduced extraction gate dtretrieve and remove information from the seman-tic vector C to generate the corresponding targetword.To force our model to generate the target sen-tence keeping information contained in C un-changed, two additional terms are introduced intothe cost function:?tlog(p(yt|C)) + ?cT?2+1TT?j=1?dt?
dt?1?2where the first term is log-likelihood cost, thesame as in the encoder-decoder.
And the other t-wo terms are introduced penalty terms.
?cT?2isfor forcing the decoding neural network to extractas much information as possible from the semanticvector C, thus the generated target sentence keepsthe same meaning with the source sentence.
Thethird term is to restrict the extract gate from ex-tracting too much information in semantic vectorC at each time-stamp.For the semantic tuples in Table 2, our modifiedRNN generates the target sentence word by word,until meets the end symbol character: ?I want awhite 4G cellphone with a big screen.
?.22482.3 CombinationThe two components of KBSE (Source Ground-ing and Target Generation) are separatelytrained, and can be used in three ways:?
Source Grounding can be used to do seman-tic grounding for a given sentence and get thekey information as a form of tuples;?
Target Generation can generate a naturallanguage sentence based on the existing se-mantic tuples;?
Combining them, KBSE can be used to trans-lation a source sentence into another lan-guage with a semantic space defined by a giv-en knowledge base.3 ExperimentsTo evaluate our proposed KBSE model, in thissection, we conduct experiments on two Chinese-to-English translation tasks.
One is from electricbusiness domain, and the other is from movie do-main.3.1 Baseline and Comparison SystemsWe select two baseline systems.
The first one is anin-house implementation of hierarchical phrase-based SMT (Koehn et al, 2003; Chiang, 2007)with traditional features, which achieves a similarperformance to the state-of-the-art phrase-baseddecoder in Moses1(Koehn et al, 2007).
The 4-gram language model is trained with target sen-tences from training set plus the Gigaword corpus2.
Our phrase-based system is trained with MERT(Och, 2003).
The other system is the encoder-decoder system (van Merri?enboer et al, 2015)3,based on which our KBSE is implemented.We also combine KBSE with encoder-decodersystem, by adding the knowledge-based semanticembedding to be another context vector.
Hence,for the decoder there are two context vectors, onefrom the encoder and the other is generated by theSemantic Grounding part.
We call this modelEnc-Dec+KBSE.For our proposed KBSE, the number of hiddenunits in both parts are 300.
Embedding size of bothsource and target are 200.
Adadelta (Zeiler, 2012)1http://www.statmt.org/moses/2https://catalog.ldc.upenn.edu/LDC2011T073The implementation is from https://github.com/mila-udem/blocks-examplesSource Sentence Semantic TuplesCategory.cellphone??
iPhone???
Carrier.China MobileBrand.iPhone????????
Name.The Matrix????????
Genre.science fiction????????
Director.Wachowski bro??????
Language.EnglishSemantic Tuples Target SentenceCategory.cellphoneAppearance.color.white I want a white 4G phoneAppearance.size.big screen with a big screen .Network.4G networkName.Pirates of Caribbean The Pirates of theReleased.2003 Caribbean is a 2003Country.America American film, starringStarring.Johnny Depp Johnny Depp .Table 3: Illustration of dataset structure in this pa-per.
We show one example for both corpus in bothpart, respectively.is leveraged as the optimizer for neural networktraining.
The batch size is set to 128, and learn-ing rate is initialized as 0.5.
The model weightsare randomly initialized from uniform distributionbetween [-0.005, 0.005].3.2 Dataset DetailsTo train our KBSE system, we only need two kind-s of pairs: the pair of source sentence and seman-tic tuples to train our Source Grounding, the pairof semantic tuples and target sentence to train ourTarget Generation.
Examples of our training da-ta in the electric business and movie domains areshown in Table 3.
To control the training pro-cess of KBSE, we randomly split 1000 instancesfrom both corpus for validation set and another1000 instances for test set.
Our corpus of elec-tric business domain consists of bilingual sentencepairs labeled with KB tuples manually4, which isa collection of source-KB-target triplets.
For theMovie domain, all the data are mined from web,thus we only have small part of source-KB-targettriplets.
In order to show the advantage of ourproposed KBSE, we also mined source-KB pairsand KB-target pairs separately.
It should be notedthat, similar as the encoder-decoder method, bilin-gual data is needed for Enc-Dec+KBSE, thus withthe added knowledge tuples, Enc-Dec+KBSE aretrained with source-KB-target triplets.4Due to the coverage problem, knowledge bases of com-mon domain (such as Freebase) are not used in this paper.2249Electric Business MovieModel BLEU HumanEval Tuple F-score BLEU HumanEval Tuple F-scoreSMT 54.30 78.6 - 42.08 51.4 -Enc-Dec 60.31 90.8 - 44.27 65.8 -KBSE 62.19 97.1 92.6 47.83 72.4 80.5Enc-Dec + KBSE 64.52 97.9 - 46.35 74.6 -KBSE upperbound 63.28 98.2 100 49.68 77.1 100Table 4: The BLEU scores, human evaluation accuracy, tuple F-score for the proposed KBSE model andother benchmark models.Our electric business corpus contains 50,169source-KB-target triplets.
For this data, we dividethe intention of electric business into 11 classes,which are Category, Function, Network, People,Price, Appearance, Carrier, Others, Performance,OS and Brand.
Each class above also has subclass-es, for example Category class has subclass com-puter and cellphone, and computer class can bedivided into laptop, tablet PC, desktop and AIO.Our movie corpus contains 44,826 source-KB-target triplets, together with 76,134 source-KBpairs and 85,923 KB-target pairs.
The data iscrawling from English Wikipedia5and the par-allel web page in Chinese Wikipedia6.
Simplerule method is used to extract sentences and KBpairs by matching the information in the infoboxand the sentences in the page content.
Since notall the entities from Chinese wikipedia has englishname, we have an extra entity translator to trans-late them.
For a fair comparison, this entity trans-lator are also used in other systems.
Due to thewhole process is semi-automatic, there may be afew irregular results within.
We divided the in-tention of movie data into 14 classes, which areBasedOn, Budget, Country, Director, Distributor,Genre, Language, Name, Producer, Released, S-tarring, Studio, Theme and Writer.3.3 EvaluationWe use BLEU (Papineni et al, 2002) as the au-tomatical evaluation matrix, significant testing iscarried out using bootstrap re-sampling method(Koehn, 2004) with a 95% confidence level.
As anaddition, we also do human evaluation for all thecomparison systems.
Since the first part SourceGrounding of our KBSE is separately trained, theF-score of KB tuples is also evaluated.
Table 45https://en.wikipedia.org6https://zh.wikipedia.orglists evaluation results for the electric business andmovie data sets.3.3.1 BLEU EvaluationFrom Table 4, we can find that our proposedmethod can achieve much higher BLEU than SMTsystem, and we can also achieve 1.9 and 3.6BLEU points improvement compared with the rawencoder-decoder system on both eletric businessand movies data.For the Enc-Dec+KBSE method, with the sametraining data on electric business domain, in-troducing knowledge semantic information canachieve about 4 BLEU points compared withthe encoder-decoder and more than 2 BLEUpoints compared with our KBSE.
Compared withencoder-decoder, Enc-Dec+KBSE method lever-ages the constrained semantic space, so that keysemantic information can be extracted.
Comparedwith KBSE, which relies on the knowledge base,Enc-Dec+KBSE method can reserve the informa-tion which is not formulated in the knowledgebase, and also may fix errors generated in thesource grounding part.Since Enc-Dec+KBSE can only be trained withsource-KB-target triplets, for the movie dataset,the performance is not as good as our KBSE,but still achieves a gain of more than 2 BLEUpoint compared with the raw Enc-Dec system.
Onmovie data, our KBSE can achieve significant im-provement compared with the models (SMT, Enc-Dec, Enc-Dec+KBSE ) only using bilingual data.This shows the advantage of our proposed method,which is our model can leverage monolingual datato learn Source Grounding and Target Genera-tion separately.We also separately evaluate the SourceGrounding and Target Generation parts.
Weevaluate the F-score of generated KB tuples225000.
20.
40.
60.
81I  w ant  a whi te  4 G  c e l lphone  w i th  a big s c reen  .tuple featurevaluescellphone 4G_network white big_screenFigure 5: An example showing how the KB tuples control the tuple features flowing into the network viaits learned semantic gates.compared with the golden KB tuples.
The resultshows that our semantic grounding performance isquite high (92.6%), which means the first part canextract the semantic information in high coverageand accuracy.
We evaluate the translation resultby feeding the Target Generation network withhuman labeled KB tuples.
The translation result(shown as KBSE upperbound in Table 4) withgolden KB tuples can achieve about 1.1 and 1.8BLEU scores improvement compared with KBSEwith generated KB tuples in both dataset.3.3.2 Human EvaluationFor the human evaluation, we do not need the w-hole sentence to be totally right.
We focus on thekey information, and if a translation is right bymain information and grammar correction, we la-bel it as correct translation, no matter how differ-ent of the translation compared with the referenceon surface strings.
Examples of correct and incor-rect translations are shown in Table 5.
As shownin Table 4, the human evaluation result shares thesame trend as in BLEU evaluation.
Our proposedmethod achieves the best results compared withSMT and raw encoder-decoder.
In our method,important information are extracted and normal-ized by encoding the source sentence into the se-mantic space, and the correct translation of impor-tant information is key for human evaluation, thusour method can generate better translation.3.4 Qualitative AnalysisIn this section, we compare the translation resultwith baseline systems.
Generally, since KB is in-troduced, our model is good at memorizing the keyinformation of the source sentence.
Also thanksto the strong learning ability of GRU, our modelrarely make grammar mistakes.
In many transla-tions generated by traditional SMT, key informa-Target I want a black Dell desktop.Correct I want a Dell black desktop.Could you please recommend me a blackDell desktop?I want a white Dell desktop.Incorrect I want a black Dell laptop.I want a black Dell desktop desktop.Table 5: Some examples of which kind of sentencecan be seen as a correct sentence and which will beseen as incorrect in the part of human evaluation.tion is lost.
Encoder-Decoder system does muchbetter, but some key information is also lost oreven repetitively generated.
Even for a long sourcesentence with a plenty of intentions, our model cangenerate the correct translation.To show the process of Target Generation, Fig-ure 5 illustrates how the KB-tuples control the tar-get sentence generation.
Taking the semantic tupleAppearance.color.white as an example, the GRUkeeps the feature value almost unchanged until thetarget word ?white?
is generated.
Almost all thefeature values drop from 1 to 0, when the corre-sponding words generated, except the tuple Ap-pearance.size.big screen.
To express the meaningof this tuple, the decoding neural network shouldgenerate two words, ?big?
and ?screen?.
When thesentence finished, all the feature values should be0, with the constraint loss we introduced in Sec-tion 2.2.Table 6 lists several translation example gener-ated by our system, SMT system and the Encoder-Decoder system.
The traditional SMT modelsometimes generate same words or phrases severaltimes, or some information is not translated.
Butour model rarely repeats or lose information.
Be-sides, SMT often generate sentences unreadable,since some functional words are lost.
But for KB-2251Source ?????????
4G????????
?Reference I want a 4G network cellphone with China Telecom supported.KBSE I need a white 4G cellphone with China Telecom supported.Enc-Dec I want a 3G cellphone with China Telecom.SMT Ah, that has a big screen, 4G network cellphone?
give white.Source ???????
2003???????????????????????????
?Reference The Matrix is a 2003 English film directed by Wachowski Brothers, starring Keanu Reeves.KBSE The Matrix is a 2003 English movie starring Keanu Reeves, directed by Wachowski Brothers.Enc-Dec The Matrix is a 2013 English movie directed by Wachowski, starring Johnny Depp.SMT The Matrix is directed by the Wachowski brothers film, and starring film language English.Table 6: Examples of some translation results for our proposed KBSE system and the baseline systems.SE, the target sentence is much easier to read.
TheEncoder-Decoder model learns the representationof the source sentence to a hidden vector, which isimplicit and hard to tell whether the key informa-tion is kept.
However KBSE learns the representa-tion of the source sentence to a explicit tuple em-bedding, which contains domain specific informa-tion.
So sometimes when encoder-decoder cannotmemorize intention precisely, KBSE can do better.3.5 Error AnalysisOur proposed KBSE relies on the knowledge base.To get the semantic vector of source sentence, oursemantic space should be able to represent anynecessary information in the sentence.
For ex-ample, since our designed knowledge base do nothave tuples for number of objects, some results ofour KBSE generate the entities in wrong plurali-ty form.
Since our KBSE consists of two separateparts, the Source Grounding part and the TargetGeneration part, the errors generated in the firstpart cannot be corrected in the following process.As we mentioned in Section 3.3.1, combining KB-SE with encoder-decoder can alleviate these twoproblems, by preserving information not capturedand correct the errors generated in source ground-ing part.4 Related WorkUnlike previous works using neural network tolearn features for traditional log-linear model (Li-u et al, 2013; Liu et al, 2014), Sutskever et al(2014) introduced a general end-to-end approachbased on an encoder-decoder framework.
In orderto compress the variable-sized source sentence in-to a fixed-length semantic vector, an encoder RNNreads the words in source sentence and generate ahidden state, based on which another decoder RN-N is used to generate target sentence.
Differentfrom our work using a semantic space defined byknowledge base, the hidden state connecting thesource and target RNNs is a vector of implicit andinexplicable real numbers.Learning the semantic information from a sen-tence, which is also called semantic grounding, iswidely used for question answering tasks (Liang etal., 2011; Berant et al, 2013; Bao et al, 2014; Be-rant and Liang, 2014).
In (Yih et al, 2015), witha deep convolutional neural network (CNN), thequestion sentence is mapped into a query graph,based on which the answer is searched in knowl-edge base.
In our paper, we use RNN to encode thesentence to do fair comparison with the encoder-decoder framework.
We can try using CNN to re-place RNN as the encoder in the future.To generate a sentence from a semantic vector,Wen et al (2015) proposed a LSTM-based natu-ral language generator controlled by a semanticvector.
The semantic vector memorizes what in-formation should be generated for LSTM, and itvaries along with the sentence generated.
Our Tar-get Generation part is similar with (Wen et al,2015), while the semantic vector is not predefined,but generated by the Source Grounding part.5 Conclusion and Future WorkIn this paper, we propose a Knowledge Based Se-mantic Embedding method for machine transla-tion, in which Source Grounding maps the sourcesentence into a semantic space, based on whichTarget Generation is used to generate the transla-tion.
Unlike the encoder-decoder neural network,in which the semantic space is implicit, the seman-tic space of KBSE is defined by a given knowl-edge base.
Semantic vector generated by KBSEcan extract and ground the key information, withthe help of knowledge base, which is preserved inthe translation sentence.
Experiments are conduct-ed on a electronic business and movie data sets,2252and the results show that our proposed method canachieve significant improvement, compared withconventional phrase SMT system and the state-of-the-art encoder-decoder system.In the future, we will conduct experiments onlarge corpus in different domains.
We also want tointroduce the attention method to leverage all thehidden states of the source sentence generated byrecurrent neural network of Source Grounding.AcknowledgementWe thank Dongdong Zhang, Junwei Bao, ZhiruiZhang, Shuangzhi Wu and Tao Ge for helpfuldiscussions.
This research was partly supportedby National Natural Science Foundation of China(No.61333018 No.61370117) and Major NationalSocial Science Fund of China (No.12&ZD227).ReferencesJunwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
In Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 967?976, Baltimore, Maryland, June.
Association forComputational Linguistics.Jonathan Berant and Percy Liang.
2014.
Semanticparsing via paraphrasing.
In Proceedings of the52nd Annual Meeting of the Association for Com-putational Linguistics, ACL 2014, June 22-27, 2014,Baltimore, MD, USA, Volume 1: Long Papers, pages1415?1425.Jonathan Berant, Andrew Chou, Roy Frostig, and Per-cy Liang.
2013.
Semantic parsing on Freebasefrom question-answer pairs.
In Proceedings of the2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1533?1544, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
computational linguistics, 33(2):201?228.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder?decoderfor statistical machine translation.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1724?1734, Doha, Qatar, October.
Association for Com-putational Linguistics.Shi Feng, Shujie Liu, Mu Li, and Ming Zhou.
2016.Implicit distortion and fertility models for attention-based encoder-decoder NMT model.
CoRR, ab-s/1601.03317.Philipp Koehn, Franz Josef Och, and Daniel Mar-cu.
2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertol-di, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th annual meeting of the ACL oninteractive poster and demonstration sessions, pages177?180.
Association for Computational Linguistic-s.Philipp Koehn, 2004.
Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing, chapter Statistical Significance Tests forMachine Translation Evaluation.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In ACL, pages 590?599.Lemao Liu, Taro Watanabe, Eiichiro Sumita, andTiejun Zhao.
2013.
Additive neural networks forstatistical machine translation.
In ACL (1), pages791?801.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou.
2014.A recursive recurrent neural network for statisticalmachine translation.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems, pages 3111?3119.Andriy Mnih and Geoffrey E Hinton.
2009.
A s-calable hierarchical distributed language model.
InAdvances in neural information processing systems,pages 1081?1088.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, S-apporo, Japan, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic e-valuation of machine translation.
In Proceedings of40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural network-s.
In Advances in neural information processing sys-tems, pages 3104?3112.2253Bart van Merri?enboer, Dzmitry Bahdanau, Vincent Du-moulin, Dmitriy Serdyuk, David Warde-Farley, JanChorowski, and Yoshua Bengio.
2015.
Blocks andfuel: Frameworks for deep learning.
arXiv preprintarXiv:1506.00619.Tsung-Hsien Wen, Milica Gasic, Nikola Mrk?si?c, Pei-Hao Su, David Vandyke, and Steve Young.
2015.Semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages1711?1721, Lisbon, Portugal, September.
Associa-tion for Computational Linguistics.Wen-tau Yih, Ming-Wei Chang, Xiaodong He, andJianfeng Gao.
2015.
Semantic parsing via stagedquery graph generation: Question answering withknowledge base.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 1321?1331, Beijing, China, July.
As-sociation for Computational Linguistics.Matthew D. Zeiler.
2012.
ADADELTA: an adaptivelearning rate method.
CoRR, abs/1212.5701.2254
