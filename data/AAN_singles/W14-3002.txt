Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 6?9,Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational LinguisticsThe Case for Empiricism (With and Without Statistics)Kenneth Church1101 Kitchawan RoadYorktown Heights, NY 10589USAKenneth.Ward.Church@gmail.comAbstractThese days we tend to use terms like empiricaland statistical as if they are interchangeable, butit wasn?t always this way, and probably for goodreason.
In A Pendulum Swung Too Far (Church,2011), I argued that graduate programs shouldmake room for both Empiricism and Rational-ism.
We don?t know which trends will dominatethe field tomorrow, but it is a good bet that itwon?t be what?s hot today.
We should preparethe next generation of students for all possiblefutures, or at least all probable futures.
This pa-per argues for a diverse interpretation of Empiri-cism, one that makes room for everything fromHumanities to Engineering (and then some).Figure 1: Lily Wong Fillmore (standing) andCharles (Chuck) Fillmore1 Lifetime Achievement Award (LTA)Since the purpose of this workshop is to cele-brate Charles (Chuck) Fillmore, I would like totake this opportunity to summarize some of thepoints that I made in my introduction to Chuck?sLTA talk at ACL-2012.I had the rather unusual opportunity to see histalk (a few times) before writing my introductionbecause Chuck video-taped his talk in advance.1I knew that he was unable to make the trip, but Ihad not appreciated just how serious the situationwas.
I found out well after the fact that the LTAmeant a lot to him, so much so that he postponedan operation that he probably shouldn?t havepostponed (over his doctor?s objection), so thathe would be able to answer live questions viaSkype after the showing of his video tape.I started my introduction by crediting LilyWong Fillmore, who understood just how muchChuck wanted to be with us in Korea, but also,just how impossible that was.
Let me take thisopportunity to thank her once again for her con-tributions to the video (technical lighting, edit-ing, encouragement and so much more).For many of us in my generation, C4C,Chuck?s ?The Case for Case?
(Fillmore, 1968)was the introduction to a world beyond Rational-ism and Chomsky.
This was especially the casefor me, since I was studying at MIT, where welearned many things (but not Empiricism).After watching Chuck?s video remarks, I wasstruck by just how nice he was.
He had nicethings to say about everyone from Noam Chom-sky to Roger Schank.
But I was also struck byjust how difficult it was for Chuck to explainhow important C4C was (or even what it saidand why it mattered).
To make sure that the in-ternational audience wasn?t misled by his up-bringing and his self-deprecating humor, Ishowed a page of ?Minnesota Nice?
stereotypes,while reminding the audience that stereotypesaren?t nice, but as stereotypes go, these stereo-types are about as nice as they get.1 The video is available online athttps://framenet.icsi.berkeley.edu/fndrupal/node/5489.6Chuck, of course, is too nice to mention thatFillmore (1967) had 6000 citations in GoogleScholar as of ACL-2012.2  He also didn?t men-tion that he has another half dozen papers with1000 or more citations including an ACL paperon FrameNet (Baker et al, 1998).3I encouraged the audience to read C4C.
Notonly is it an example of a great linguistic argu-ment, but it also demonstrates a strong commandof the classic literature as well as linguistic facts.Our field is too ?silo?-ed.
We tend to cite recentpapers by our friends, with too little discussionof seminal papers, fields beyond our own, andother types of evidence that go beyond the usualsuspects.
We could use more ?Minnesota Nice.
?I then spent a few slides trying to connect thedots between Chuck?s work and practical engi-neering apps, suggesting a connection betweenmorphology and Message Understanding Con-ference (MUC)-like tasks.
We tend to think toomuch about parsing (question 1), though ques-tion 2 is more important for tasks such as infor-mation extraction and semantic role labeling.1.
What is the NP (and the VP) under S?2.
Who did what to whom?Figure 2: An example of information extractionin commercial practice.Context-Free Grammars are attractive for lan-guages with more word order and less morphol-ogy (such as English), but Case Grammar maybe more appropriate for languages with moremorphology and less word order (such as Latin,Greek & Japanese).
I then gave a short (over-simplified) tutorial on Latin and Japanese gram-mar, suggesting a connection between Latin cas-es (e.g., nominative, accusative, ablative, etc.
)and Japanese function words (e.g., the subject2 Citations tend to increase over time, especially forimportant papers like Fillmore (1967), which hasmore than 7000 citations as of April 2014.3 See framenet.icsi.berkeley.edu for more recent pub-lications such as Ruppenhofer et al (2006).marker ga and the direct object marker wo, etc.
).From there, I mentioned a few historical connec-tions?
Case Grammar ?
Frames ?
FrameNet?
Valency4 ?
Scripts (Roger Schank)?
Chuck ?
Sue Atkins (Lexicography)The verb ?give,?
for example, requires threearguments: Jones (agent) gave money (object) tothe school (beneficiary).
In Latin, these argu-ments are associated with different cases (nomi-native, accusative, etc.).
Under the frame view,similar facts are captured with a commercialtransaction frame, which connects argumentsacross verbs such as: buy, sell, cost and spend.5VERBBUYERGOODSSELLERMONEYPLACEbuy subject object from for atsell tocostindirectobjectsubject  object atspend subject on  object atLexicographers such as Sue Atkins use patternssuch as:?
Risk <valued object> for <situation> |<purpose> | <beneficiary> | <motivation>to address similar alternations.
My colleaguePatrick Hanks uses a similar pattern to motivateour work on using statistics to find collocations:?
Save <good thing> from <bad situation>Lexicographers use patterns like this to accountfor examples such as:?
Save whales from extinction?
Ready to risk everything for what he be-lieves.where we can?t swap the arguments:?
*Save extinction from whalesThe challenge for the next generation is to movethis discussion from lexicography and generallinguistics to computational linguistics.
Whichof these representations are most appropriate forpractical NLP apps?
Should we focus on part ofspeech tagging statistics, word order or frames4 http://en.wikipedia.org/wiki/Valency_(linguistics)5 For more discussion of this table, see www.uni-stuttgart.de/ linguistik/ sfb732/ files/hamm_framesemantics.pdf7(typical predicate-argument relations and collo-cations)?Do corpus-based lexicography methods scaleup?
Are they too manually intensive?
If so,could we use machine learning methods to speedup manual methods?
Just as statistical parserslearn phrase structure rules such as S ?
NP VP,we may soon expect machine learning systems tolearn valency, collocations and typical predicate-argument relations.How large do the corpora have to be to learnwhat?
When can we expect to learn frames?
Inthe 1980s, corpora were about 1 million words(Brown Corpus).
That was large enough to makea list of common content words, and to train partof speech taggers.
A decade later, we had 100million word corpora such as the British NationalCorpus.
This was large enough to see associa-tions between common predicates and functionwords such as ?save?
+ ?from.?
Since then, withthe web, data has become more and more availa-ble.
Corpus growth may well be indexed to theprice of disks (improving about 1000x per dec-ade).
Coming soon, we can expect 1M2 wordcorpora.
(Google may already be there.)
Thatshould be large enough to see associations ofpairs of content words (collocations).
At thatpoint, machine learning methods should be ableto learn many of the patterns that lexicographershave been talking about such as: risk valued ob-ject for purpose.We should train the next generation with thetechnical engineering skills so they will be ableto take advantage of the opportunities, but moreimportantly, we should encourage the next gen-eration to read the seminal papers in a broadrange of disciplines so the next generation willknow about lots of interesting linguistic patternsthat will, hopefully, show up in the output oftheir machine learning systems.2 Empirical / Corpus-Based TraditionsAs mentioned above, there is a direct connectionbetween Fillmore and Corpus-Based Lexicogra-phers such as Sue Atkins (Fillmore and Atkins,1992).
Corpus-based work has a long traditionin lexicography, linguistics, psychology andcomputer science, much of which is documentedin the Newsletter of the International ComputerArchive of Modern English (ICAME).6  Accord-ing to Wikipedia,7 ICAME was co-founded by6http://icame.uib.no/archives/No_1_ICAME_News.pdf7 http://en.wikipedia.org/wiki/W._Nelson_FrancisNelson Francis, who is perhaps best known forhis collaboration with Henry Ku?era on theBrown Corpus.8   The Brown Corpus dates backto the 1960s, though the standard reference waspublished two decades later (Francis and Ku?era,1982).The Brown Corpus has been extremely influ-ential across a wide range of fields.
Accordingto Google Scholar, the Brown Corpus has morethan 3000 citations.
Many of these referenceshave been extremely influential themselves in anumber of different fields.
At least9 ten of thesereferences have at least 2000 citations in at leastfive fields:?
Information Retrieval (Baeza-Yates andRibeiro-Neto, 1999),?
Lexicography (Miller, 1995),?
Sociolinguistics (Biber, 1991),?
Psychology (MacWhinney, 2000)?
Computational Linguistics (Marcus et al,1993; Jurafsky and Martin, 2000; Churchand Hanks, 1990; Resnik, 1995)All of this work is empirical, though much ofit is not all that statistical.
The Brown Corpusand corpus-based methods have been particularlyinfluential in the Humanities, but less so in otherfields such as Machine Learning and Statistics.
Iremember giving talks at top engineering univer-sities and being surprised, when reporting exper-iments based on the Brown Corpus, that it wasstill necessary in the late 1990s to explain whatthe Brown Corpus was, as well as the researchdirection that it represented.
While many of the-se top universities were beginning to warm up tostatistical methods and machine learning, therehas always been less awareness of empiricismand less sympathy for the research direction.These days, I fear that the situation has not im-proved all that much.
In fact, there may be evenless room than ever for empirical work (unless itis statistical).It is ironic how much the field has changed(and how little it has changed).
Back in the early1990s, it was difficult to publish papers that di-gressed from the strict rationalist tradition thatdominated the field at the time.
We created theWorkshop on Very Large Corpora (WVLC8 http://en.wikipedia.org/wiki/Brown_Corpus9 Google Scholar is an amazing resource, but not per-fect.
There is at least one error of omission: Manningand Sch?tze (1999).8evolved into EMNLP) to make room for a littlework of a different kind.
But over the years, thedifferences between the main ACL conferenceand EMNLP have largely disappeared, and thesimilarities between EMNLP and ICAME havealso largely disappeared.
While it is nice to seethe field come together as it has, it is a shamethat these days, it is still difficult to publish apaper that digresses from the strict norms thatdominate the field today, just as it used to be dif-ficult years ago to publish papers that digressedfrom the strict norms that dominated the field atthe time.
Ironically, the names of our meetingsno longer make much sense.
There is less dis-cussion than there used to be of the E-word inEMNLP and the C-word in WVLC.One of the more bitter sweet moments at aWVLC/EMNLP meeting was the invited talk byKu?era and Francis at WVLC-1995, 10  whichhappened to be held at MIT.
Just a few yearsearlier, it would have been unimaginable thatsuch a talk could have been so appreciated atMIT of all places, given so many years of suchhostility to all things empirical.Their talk was the first and last time that I re-member a standing ovation at WVLC/EMNLP,mostly because of their contributions to the field,but also because they both stood up for the hourduring their talk, even though they were wellpast retirement (and standing wasn?t easy, espe-cially for Francis).Unfortunately, while there was widespreadappreciation for their accomplishments, it wasdifficult for them to appreciate what we weredoing.
I couldn?t help but notice that Henry wastrying his best to read other papers in theWVLC-1995 program (including one of mine),but they didn?t make much sense to him.
It wasalready clear then that the field had taken a hardturn away from the Humanities (and C4C andFrameNet) toward where we are today (moreStatistical than Empirical).3 ConclusionFads come and fads go, but seminal papers suchas ?Case for Case?
are here to stay.
As men-tioned above, we should train the next generationwith the technical engineering skills to take ad-vantage of the opportunities, but more important-ly, we should encourage the next generation toread seminal papers in a broad range of disci-10 http://aclweb.org/anthology//W/W95/W95-0100.pdfplines so they know about lots of interesting lin-guistic patterns that will, hopefully, show up inthe output of their machine learning systems.ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto.1999.Modern information retrieval.
Vol.
463.
ACMPress, New York, NY, USA.Collin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
?The berkeley framenet project,?ACL.Douglas Biber.
1991.
Variation across speech andwriting.
Cambridge University Press.Kenneth Church.
2011.
A pendulum swung too far,Linguistic Issues in Language Technology, 6(5).Kenneth Church and Patrick Hanks.
1990 "Word as-sociation norms, mutual information, and lexicog-raphy."
Computational linguistics 16(1): 22-29Charles J. Fillmore.
1968.
?The Case for Case.?
InBach and Harms (Ed.
): Universals in LinguisticTheory.
Holt, Rinehart, and Winston, New York,NY, USA, pp.
1-88.Charles J. Fillmore and Beryl TS Atkins.
1992.
?To-ward a frame-based lexicon: The semantics ofRISK and its neighbors.?
Frames, fields, and con-trasts, pp.
75-102, Lawrence Erlbaum Associates,Hillsdale, NJ, USA.W.
Nelson Francis and Henry Ku?era.
1982 Frequen-cy analysis of English usage.
Houghton Mifflin,Boston, MA, USA.Dan Jurafsky and James H. Martin.
2000 Speech &Language Processing.
Pearson Education India.Brian MacWhinney.
2000.
The CHILDES Project:The database.
Vol.
2.
Psychology Press.Christopher D. Manning and  Hinrich Sch?tze.1999.
Foundations of statistical natural languageprocessing.
MIT Press.
Cambridge, MA, USA.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
"Building a large anno-tated corpus of English: The Penn Tree-bank."
Computational linguistics 19(2): 313-330.George A. Miller.
1995.
"WordNet: a lexical databasefor English."
Communications of the ACM 38(11):39-41.Philip Resnik.
1995.
"Using information content toevaluate semantic similarity in a taxonomy."
arXivpreprint cmp-lg/9511007Josef Ruppenhofer, Michael Ellsworth, Miriam RLPetruck, Christopher R. Johnson, and JanScheffczyk.
2006.
FrameNet II: Extended theoryand practice.
framenet.icsi.berkeley.edu9
