Proceedings of the Fifth Law Workshop (LAW V), pages 47?55,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsAn Annotation Scheme for Automated Bias Detection inWikipediaLivnat Herzig, Alex Nunes and Batia SnirComputer Science DepartmentBrandeis UniversityWaltham, MA, U.S.A.lherzig, nunesa, bsnir @brandeis.eduAbstractBiasML is a novel annotation scheme withthe purpose of identifying the presence aswell as nuances of biased language withinthe subset of Wikipedia articles dedicatedto service providers.
Whereas Wikipediacurrently uses only manual flagging to de-tect possible bias, our scheme providesa foundation for the automating of biasflagging by improving upon the methodol-ogy of annotation schemes in classic sen-timent analysis.
We also address chal-lenges unique to the task of identifyingbiased writing within the specific contextof Wikipedia?s neutrality policy.
We per-form a detailed analysis of inter-annotatoragreement, which shows that although theagreement scores for intra-sentential tagswere relatively low, the agreement scoreson the sentence and entry levels wereencouraging (74.8% and 66.7%, respec-tively).
Based on an analysis of our firstimplementation of our scheme, we suggestpossible improvements to our guidelines, inhope that further rounds of annotation af-ter incorporating them could provide ap-propriate data for use within a machinelearning framework for automated detec-tion of bias within Wikipedia.1 IntroductionBiasML is an annotation scheme directed at de-tecting bias in the Wikipedia pages of serviceproviders.
Articles are judged as biased or non-biased at the sentential and document levels,and annotated on the intra-sentential level fora number of lexical and structural features.2 Motivation and Background2.1 MotivationNeutral Point of View (NPOV) is one ofthree core tenets of Wikipedia?s content pol-icy.
Wikipedia describes NPOV as ?represent-ing fairly, proportionately, and as far as possi-ble without bias, all significant views that havebeen published by reliable sources?
(Wikipedia,2011a).The collaborative design of Wikipedia is suchthat anyone can submit content, and so the de-tection and flagging of bias within articles isan essential and ongoing task in maintainingthe quality and utility of Wikipedia.
Currently,NPOV is enforced manually via the same openprocess that creates content on the site.
Userscan flag pages with suspect content as contain-ing a ?NPOV dispute?.
This is problematic:definitions of bias vary from editor to editor,and accusations of bias can themselves comefrom a biased perspective.
Additionally, thispractice is weighted towards the attention ofWikipedia users, such that the scrutiny an ar-ticle receives is proportional to its broader pop-ularity.
For example, though the pages for Landof Israel and restaurant franchise Fresh to Or-der have both been flagged for NPOV disputes,they have been edited 1,480 and 46 times by 536and 22 users, respectively (Wikipedia, 2011b;Wikipedia, 2011c).
The average Wikipedia pagereceives just under 20 edits (Wikipedia, 2011d).In light of this, an automated pass at bias de-tection is highly desirable.
Instead of wholesalereliance on human editors, a system based onour annotation scheme could serve as an initial47filter in monitoring user contributions.
If inte-grated into the Wikipedia framework, this sys-tem could aid in the regulation of NPOV pol-icy violations, e.g.
tracking repeat offenders.With this goal in mind we have designed Bi-asML to flag NPOV issues in a specific subsetof Wikipedia articles.
We have constrained ourtask to the pages of service providers such assmall businesses, schools, and hospitals.
As agenre, the pages of service providers are espe-cially worthy of scrutiny because they are bothless likely to be closely vetted, and more likely tobe edited by someone with a commercial interestin the reputation of the organization.In addition, service provider pages are partic-ularly appropriate for automatic POV-flaggingbecause the bias complaints leveled against themtend to be much more systematic and objectivecompared with those of an especially controver-sial or divisive topic.2.2 BackgroundSentiment analysis efforts usually rely on theprior polarity of words (their polarity out ofcontext).
For example, Turney (2002) pro-poses a method to classify reviews as ?rec-ommended?/?not recommended?, based on theaverage semantic orientation of the review.Semantic orientation is the mutual informa-tion measure of selected phrases with theword excellent minus their mutual informationwith the word poor.
However, as Wilson etal.
(2005) point out, even using a lexicon of pos-itive/negative words marked for their prior po-larity is merely a starting point, since a word?spolarity in context might differ from its priorpolarity.The distinction between prior and contextualpolarity is crucial for detecting bias, since wordswith a prior positive/negative polarity may ormay not convey bias, depending on their con-text.
Notably, the inverse is also true - generallyneutral words can be used to create a favorabletone towards a sentence?s topic, thereby express-ing bias.
An example of the latter case are thewords own and even in the sentence The hospi-tal has its own pharmacy, maternity ward, andeven a morgue.
Though generally neutral, theirusage here contributes to the sentence?s overallnon-neutrality.
In order to deal with contex-tual polarity, Wilson et al propose a two-stageprocess that first uses clues marked with contex-tual polarity to determine whether the phrasescontaining these clues are polar or neutral.
Thesecond stage then determines the actual polarityof the phrases deemed non-neutral.However, Wilson et al?s approach would notsuit our task of bias detection in Wikipedia,as the abovementioned example, taken from aWikipedia entry, shows.
Blatant expression ofopinions or emotions is rare in the Wikipediaentries of service providers.
Words which ex-plicitly convey that an opinion/emotion is beingexpressed are rarely used (e.g.
I think).
Rather,bias is introduced either in more subtle ways(e.g.
using words that are usually neutral) orin ways that differ from the ones addressed byprevious approaches.
For example, bias is intro-duced by preceding positive information aboutthe provided service by phrases such as it iswidely believed.
Clearly, this phrase does nothave contextual polarity, but it does introducebias.Within the realm of Wikipedia, phrases thatcreate an impression that something specific andmeaningful has been said when only a vague orambiguous claim has been communicated, suchas it is widely believed, are referred to as weasels(Wikipedia, 2011e).
The recent CoNLL-2010shared task (Farkas et al, 2010), aimed at de-tecting uncertainty cues in texts, focused onthese phrases in trying to determine whethersentences contain uncertain information.
In thesame vein, we include weasel words as part ofour annotation scheme to detect bias.Finally, as Blitzer et al (2007) point out, al-though the typical word-level analysis capturesthe finer-grained aspects of sentiment language,it falls short in capturing broader structurally orcontextually-based bias.
Bias can also be intro-duced by repetitive usage of words that in typ-ical usage do not have prior polarity, but whenused in a repetitive manner, create a favorabledepiction of a sentence?s topic.
This cannot becaptured by approaches such as those of Wilsonet al or Turney.48To tackle cases like those described above, ourannotation scheme extends beyond lexical tags,and includes tags that capture dependencies be-tween a word and its context, as well as tagsthat are aimed at capturing subtle expressionsof bias.3 Method3.1 Corpus Selection and PreparationThe POV Wikipedia entries were selected fromWikipedia?s list of entries that are classified as?NPOV dispute?.
Roughly 6,000 of the morethan 3 million existing Wikipedia entries havebeen flagged this way (Wikipedia, 2011f).
Wewent over these entries using a ?get random ar-ticle?
feature, choosing ones that met our ser-vice provider criterion, i.e., they were eitherabout a specific product or a service provider.The neutral entries were selected via a searchthrough pages of products/service providers onWikipedia that were evaluated by us as neutral.Our corpus ultimately consisted of 22 POV en-tries and 11 NPOV ones.3.2 Annotation SchemeAnnotation Procedure and Tags: The an-notation was performed using the MAE anno-tation tool (Stubbs, 2011), which is compliantwith LAF guidelines (Ide and Romary, 2006).The annotation scheme uses standoff annota-tion and includes tagging on multiple levels -tagging biased words and linguistic structures;tagging the neutrality of each sentence; taggingthe overall neutrality of the entry.
The annota-tor is instructed to read through each sentence,and decide if it is written in a neutral point ofview or not.
At this point in the annotationprocess, a sentence is considered non-neutral ifit is written in a non-neutral tone, or if it fa-vors/disfavors its topic (regardless of whetherthe sentence is sourced).
If a sentence is deemedneutral, it is tagged with a sentential level tagSENTENCE POV, with the attribute NPOV,and no further tagging of it is required.In the alternate case that a sentence is judgedto contain non-neutral language, the annotatoris asked to look for words/phrases that should betagged with the word/phrase level tags (elabo-rated below) only within the scope of the currentsentence.
After tagging the word/phrase leveltags, the sentence should be evaluated for itsneutrality, and tagged SENTENCE POV withone of two possible attributes (POV or NPOV),depending on the word/phrase level tags it has.After all the sentences are tagged with the SEN-TENCE POV tag, the entire entry is taggedwith the ENTRY POV tag, whose attribute val-ues are numeric, ranging between 1 and 4, where1 is completely neutral and 4 is clearly non-neutral (i.e., written as an advertisement).The annotation scheme is comprised of 4word/phrase level extent tags that aim to cap-ture biased language - POLAR PHRASE,WEASEL, REPETITION, and PER-SONAL TONE.
The POLAR PHRASE tag isused to mark words/phrases that are used toexpress favor or disfavor within the sententialcontext, and contribute to the non-neutralityof the sentence.
The annotator is advisedto examine whether replacing the suspectedword(s) results in a more neutral version of thesentence, without losing any of the sentence?scontent.
If so, the word(s) should be tagged asPOLAR PHRASE (with a positive or negativeattribute).
For example, in the sentence Thenew hospital even has a morgue, even is taggedwith the POLAR PHRASE tag (the attributevalue is positive), and the entire sentence?sSENTENCE POV tag receives the attributePOV.The PERSONAL TONE tag is used to tagwords/phrases that convey a personal tone,which is commonly used in advertisements but isinappropriate in encyclopedic entries.
The pos-sible attribute values are first person (e.g.
we,our), second person (e.g.
you, your) and other(e.g.
here).
The REPETITION tag is used fortwo possible cases - when similar words are un-necessarily used to describe the same thing, allwords except the first one should be considereda repetition; when there is unnecessary repeti-tion that does not add new information (i.e., itis not elaboration, but mere repetition) aboutthe service the service provider offers, or praiseof the service provider, the repeated elements49Figure 1: An annotated Wikipedia entry - POLAR PHRASEs are underlined in bold, all of the positivetype; WEASEL is italicized, and is of the pro type; REPETITION is underlined, receiving the attributevalue 3.
SENTENCE POV for sentences no.
1, 2, 5 & 6 is NPOV, while it is POV for sentences no.
3 & 4.The ENTRY POV is 3, which corresponds to POV.should be considered repetition.
For both cases,the attribute value will be the numeric value rep-resenting the number of repeated elements.
Toillustrate the former type of REPETITION andthe PERSONAL TONE tag, consider the sen-tence The councils work to enhance and improvethe quality of your local health service.
Im-prove is a case of REPETITION, since there isno need for both enhance and improve (the at-tribute value is 1).
In addition, your is taggedwith the PERSONAL TONE tag (second per-son), and the sentence?s SENTENCE POV tagreceives the attribute POV.
The other type ofREPETITION applies to cases where a sentencesuch as The funeral home also offers a flowershop, crematorium, family center and library,is subsequently followed by a sentence such asThis unique funeral home is built of naturallimestone, and has a modern cremation center,a family center and library, a flower shop and achapel.
While unique is tagged as a PO-LAR PHRASE, the other underlined elementsare all REPETITION, with the attribute valueset to 3, since 3 elements are repeated unnec-essarily, without adding new information.
Notethat although crematorium and cremation cen-ter refer to the same entity, it is not treated asa repetition, because the second mention addsthat it is a modern crematorium.
The secondsentence?s neutrality is therefore POV, while thefirst one?s is NPOV.As elaborated in the background section,weasel words also introduce bias, by presentingthe appearance of support for statements whiledenying the reader the possibility to assess theviewpoint?s source.
These are usually generalclaims about what people think or feel, or whathas been shown.
These words/phrases are cap-tured by the WEASEL tag.
This tag has twopossible attributes, pro, which captures ?classic?WEASELs such as is often credited, and con,which would capture negative portrayal, as in isnever believed.
In contrast to the previously de-scribed word/phrase level tags, we also includeda fifth tag, FACTIVE PHRASE, which is inher-ently different.
It is used to mark phrases thatgive objectivity to what is otherwise a biaseddescription, usually a source.
These phrases de-bias polar phrases and weasels.The relation between a FACTIVE PHRASEand the POLAR PHRASE or WEASEL thatit de-biases is captured by the LEGITIMIZElink tag.
A sentence that was initially judgedas non-neutral can eventually be tagged asNPOV, if each instance of its biased languageis backed up by sources.
Otherwise, it shouldbe tagged as POV.
For example, in the sentenceIt is widely believed that John Smith started thetradition of pro-bono work.
[1], the phrase iswidely believed is tagged WEASEL, whereas [1]is tagged FACTIVE PHRASE.
In addition, aLEGITIMIZE tag will link these two elements,50resulting in an overall neutral sentence, sinceits biased language is backed up by a source.The SENTENCE POV tag will therefore havethe attribute value NPOV (whereas it would bePOV if there were no FACTIVE PHRASE).
Tofurther illustrate this point, consider the sen-tence Jones and Sons ranked number one inThe American Lawyer?s Annual Survey.
Num-ber one is tagged as a POLAR PHRASE (pos-itive), The American Lawyer?s Annual Surveyis a FACTIVE PHRASE, and there is a LE-GITIMIZE link between them.
The entireSENTENCE POV tag?s neutrality is thereforeNPOV.
This is in contrast to the sentenceJones and Sons are the number one law firm inBoston., which would have the attribute valuePOV, because its polar phrases have no factivephrase to back them up.
Our framework alsoenables tagging a sentence as POV even if noneof the possible tags apply to them.
See Figure 1for an example of an annotated entry.BiasML Innovations: The annotation schemeelaborated above is an innovative yet practi-cal answer to the theoretical linguistic consid-erations of sentiment analysis within the genreof Wikipedia.
As previously mentioned, ourscheme improves upon approaches that relyupon prior polarity (e.g.
Turney, 2002) byidentifying cases of biased language that stemfrom intra-sentential and cross-sentential de-pendencies, rather than isolated words.
OurPOLAR PHRASE tag resembles phrases withnon-neutral contextual polarity that Wilson etal.
?s (2005) approach introduces, but it capturescases that their approach does not - namely, gen-erally neutral words that nevertheless make asentence biased.Another innovation of our framework isenabling the legitimization of weasel words.Whereas the CoNLL-2010 shared task (Farkaset al, 2010) annotated all occurrences of weaselsas uncertainty markers, we acknowledge the pos-sibility of sources (e.g.
citations) that actuallynullify the weasel.The multiple-level discourse association of ourtag scheme also allows observation of shifts inpolarity within the larger discourse of the arti-cle.
The sentence-level POV tag allows the an-notator to identify the overall neutrality of eachsentence, thus producing a landscape of how bi-ased language is distributed across the article.This landscape not only provides an indicator ofwhere to look for contextual clues and dependen-cies among more local tags, but it is particularlyrelevant to Wikipedia?s wiki platform, where it islikely that different authors contributed to dif-ferent portions of the article, making it moreprone to variance in biased tone.While developing this scheme, we wanted tomake sure it tapped into the capacity of theannotator to identify both subjective languageuse and objective linguistic phenomena.
Whiletags like PERSONAL TONE and WEASEL re-quire the annotator to mark precise occurrencesof language, the sentence and document-levelPOV tags allow the annotator to identify pointof view without having to explicitly point toa specific linguistic structure.
To preserve thevalue of the human annotator?s subjective judg-ments, our scheme permitted the co-occurrenceof a sentence or document POV tag with the ab-sence of any local lexical tags.
This allowed ourscheme to recognize the difficult cases in senti-ment analysis where one intuitively senses opin-ionated language, but is unable to formally de-fine what makes it so.Another aim of our work was to develop ascheme that captured the way information isportrayed in Wikipedia, while avoiding judg-ment on what information is actually commu-nicated.
A significant source of dispute withinWikipedia is disagreement as to the veracityof an article?s content; however, identificationof this is truly a different task then the onewe have defined here.
In order to tease apartthese distinct types of evaluation, annotatorswere instructed to identify citations that legit-imize statements that are potentially POV, butnot to consider the truthfulness of the statementor validity of the source when tagging.4 ResultsOur corpus of 33 articles of varying degrees ofneutrality was distributed among three annota-tors, each annotator receiving 2/3 of the entire51corpus.
The articles were presented as plaintext in the annotation environment, and werestripped of images, titles, section headings, orother information extraneous to the main bodyof the text (inline references, however, were pre-served).
The annotators were graduate linguis-tics students.
Their training consisted of a briefinformation session on the motivation of ourwork, a set of annotation guidelines, and op-tional question and answer sessions.
Adjudica-tion of the annotation was performed with theMAI adjudication tool (Stubbs, 2011).4.1 Tag AnalysisFor each tag, an average percent agreement scorewas calculated (for extents and attributes) perdocument, then averaged to get the agreementover all documents in the corpus.
Note that ex-tent agreement was defined as strictly as possi-ble, requiring an exact character index match,meaning cases of overlap would not be consid-ered agreement (e.g.
best and the best would notbe a match, even if they referred to the sameinstance of best).
The percent agreement scoresare displayed in Table 1.
Note that calculationswere not performed for the LEGITMIZE linktag, because it relies on the extent of other tags.Tag % Extent Agreement % Attribute AgreementPOLAR PHRASE 6.5 60FACTIVE PHRASE 9.3 NAWEASEL 4.9 13.6REPETITION 0 0PERSONAL TONE 33 57.1SENTENCE POV 94.6 74.8ENTRY POV 97 66.7Table 1: Tag Analysis of IAA: Mean % AgreementAgreement is notably stronger among thehigher level tags, ENTRY POV and SEN-TENCE POV.
For the ENTRY POV neutral-ity attribute, we had decided to measure over-all Entry POV neutrality along a 4-point scale,after noticing our own hesitation to assign thesame tag to both slightly preferential and fla-grantly biased entries.
However, this more nu-anced system was at odds with our original ob-jective of creating an annotation scheme for usein a binary classification of bias.
Though itmight manifest to different degrees, bias eitheris or is not present within an entry.
Our inten-tion in collapsing the scale after the fact was torecover a more organic division in Entry POVjudgments.
With the built-in 4-way division,inter-annotator agreement on Entry POV at-tributes stood at 42.42%.
This number roseconsiderably when the scale was reduced to a2-way division.
To reflect the notion that anybias is unacceptable, we chose to divide EN-TRY POV into two groups: not-at-all-biased(ENTRY POV=1) and containing bias (EN-TRY POV>1).
This division yielded an inter-annotator agreement of 66.7%.
In the case ofthe SENTENCE POV attribute, which is bi-nary, agreement on neutrality is even higher at74.8%.The strength of scores for attributes at thesentence and document levels suggest that an-notators had similar perceptions of what kindsof discourse entailed a bias not fit for an ency-clopedic entry.
This in turn suggests that thereis conceptual validity in our task on a higherlevel, as well as validity in how that concept wasdefined and conveyed to annotators.Interestingly, agreement numbers declinefor the intra-sentential tags.
Both PO-LAR PHRASE and PERSONAL TONE haveattribute agreement scores at or near 60%, butPERSONAL TONE has an extent agreement of33%, while POLAR PHRASE has only 6.5% forextent.
WEASEL and REPETITION have lowscores for both extent and attribute, with REP-ETITION being 0% for both (note that extentagreement is a prerequisite for attribute agree-ment).
FACTIVE PHRASE also has low extentagreement, making extent agreement generallylow across the board for intra-sentential tags.Attribute agreement is expected to be high forthe intra-sentential tags, given that attributesare almost always positive (pro/positive) withinthe service provider genre.
Based on the ad-judication process, we suspect that the maincontributor to instances of attribute disagree-ment for these tags was simply a failure52on the annotators?
part to specify the at-tribute at all, perhaps because they encoun-tered mainly positive/pro instances of PO-LAR PHRASEs/WEASELs, thereby forgettingthat an attribute is relevant.
The annotatorsalso reported confusion about cases where a gen-erally negative word/phrase is used to supportor promote the article?s topic (in these cases, theattribute should be positive).For POLAR PHRASE, the lack of extentagreement is not entirely unexpected, as thistag was difficult to define.
As previously dis-cussed, we chose not to use a lexicon of pos-itive/negative words with their prior polarity,because a word?s polarity in these documentswas highly contingent upon its context and par-ticular usage.
During adjudication, it was ob-served that one of the annotators consistentlymarked any term that was generally positive as aPOLAR PHRASE.
For example, the word mod-ern was chosen when used to describe architec-ture.
Although this word has some sort of posi-tive connotation, it does not meet the substitu-tion criteria outlined for POLAR PHRASE inthe guidelines (for a word to qualify as a PO-LAR PHRASE, there should be a comparablesubstitution possible that would reduce the non-neutrality of the sentence without losing any ofits content).
This annotator had set his/her ac-ceptability threshold for this tag too low, whichresulted in over-selection.
This could hopefullybe avoided in future annotation efforts by moreexposure to correct and incorrect examples ofpolar phrases.Low extent agreement for the WEASEL andREPETITION tags appears to be a result of apoor understanding of what the tags are meantto capture.
In the case of the WEASEL tag, an-notators tended to mark anything that had anobscured source, such as, being overlooked forthe position and a number of executives.
Al-though the passive voice in the first exampleand the vague specification in the second onedo obscure a source, they do not present sup-port for the topic at hand, which is part of theWEASEL definition.
To aid future annotation,it appears that further emphasis is needed toconvey the fact that a WEASEL consists of atargeted word/phrase (and not just a lack of ci-tation) that is used to conceal the source of afavorable or unfavorable statement.
A lexiconwould be useful in this case, as most weasels arecovered by just a handful of common phrases orconstructions.
For example, the famous is acommon WEASEL that was missed by all anno-tators throughout the corpus.The poor performance for the REPETITIONtag is probably a result of it not being just lit-eral echo, but rather a recurrence of informa-tion used for promotional purposes.
Like PO-LAR PHRASE, this makes its definition rathersubjective, and thus prone to different inter-pretations.
Throughout the corpus, all annota-tors tended to miss the REPETITION we hadidentified in the gold standard, and there werealso cases of annotators marking literal repeti-tions that did not match the guidelines?
crite-ria.
Although the linguistic phenomenon thatthe REPETITION tag was intended to captureis indeed indicative of bias (especially for ser-vice provider articles), it is relatively rare.
Itsrarity and elusiveness, combined with the factthat agreement was 0%, would motivate us toexclude this as a tag in future versions of theannotation scheme.4.2 Annotator AnalysisTable 2 reports how each annotator comparesto the gold standard (which was determinedby the authors).
Overall, annotator B clearlyoutperformed the other two, with both strongprecision and recall scores.
For all the intra-sentential tags with the exception of WEASEL,there seems to be a consistent trend where an-notator B has the highest scores, a second an-notator has somewhat lower scores (either A orC), and the third one has very low scores.
Thistrend suggests that for each of these tags, a sin-gle annotator tended to pull down its agreementscores, though not consistently the same anno-tator.
For example, annotator C performed rela-tively poorly on FACTIVE PHRASE and PER-SONAL TONE, while the same was true for an-notator A on the POLAR PHRASE and REP-ETITION tags.
For the higher level tags (SEN-TENCE POV and ENTRY POV), performance53was excellent for all annotators, which is con-sistent with the percent agreement scores fromTable 1.Tag annotator a annotator b annotator cpre., rec.
pre., rec.
pre., rec.POLAR PHRASE 0.2, 0.28 0.63, 0.89 0.55, 0.17FACTIVE PHRASE 0.29, 0.5 0.55, 0.86 0, 0WEASEL 0.33, 0.28 0.85, 0.92 0.33, 0.6REPETITION 0.06, 0.08 0.62, 1 0.44, 0.36PERSONAL TONE 0.64, 0.39 1, 1 0, 0SENTENCE POV 1, 0.97 1, 1 0.98, 0.97ENTRY POV 1, 1 1, 1 1, 1Table 2: Per-Annotator Analysis: Precision and Re-callWhile the low individual scores on intra-sentential tags is disconcerting, the overallhigher scores for annotator B are a positive indi-cation that a decent understanding and execu-tion of the scheme and guidelines are possible,and agreement could potentially improve greatlywith better training for adherence to the guide-lines in the case of the other two annotators.4.3 Proposed Annotation ChangesPost-annotation analyses have provided a basisfor changes to our annotation scheme, guide-lines, and implementation process for the fu-ture.
In addition to the changes to the guide-lines we have suggested in the previous section,we believe that the greatest amount of improve-ment for our tag agreement could be achieved byconducting a training session for annotators, inwhich they study and then practice with positiveand negative examples of the different tags.
Thiswould hopefully solidify understanding of thetagging scheme, since it became apparent duringcomparison with the gold standard that certainannotators had trouble with specific tags.
Fur-thermore, it would be worth experimenting withless rigorous forms of extent matching, and per-haps allowing extents with a certain degree ofoverlap to qualify as agreement.5 Conclusions and Future WorkThe work presented here offers a new annotationscheme for the automatic detection of bias in theunique genre of Wikipedia entries.
In additionto a tagset designed to identify linguistic charac-teristics associated with bias within an encyclo-pedic corpus, our scheme works beyond typicalsentiment analysis approaches to capture cross-sentential linguistic phenomena that lead to en-cyclopedia bias.
Strong agreement results forsentence and document levels bias tags (74.8%and 66.7%, respectively) indicate that there isconceptual validity in our task on a higher level,as well as validity in how that concept was de-fined and conveyed to annotators.
While agree-ment for intra-sentential tags was lower, the factthat one annotator consistently scored high onagreement with the gold standard suggests thatimproved annotator training, and specificationof unforeseen cases in the guidelines would pro-vide more reliable annotator performance forthese tags.
It is our hope that upon implement-ing the suggested improvements outlined in thiswork, further rounds of annotation could pro-vide appropriate data for use within a machinelearning framework for automated detection ofvarious sorts of bias within Wikipedia.AcknowledgmentsWe would like to thank James Pustejovsky, Lo-tus Goldberg and Amber Stubbs for feedback onearlier versions of this paper and helpful advicealong the execution of this project.
We wouldalso like to thank three anonymous reviewers fortheir comments.ReferencesJohn Blitzer, Mark Drezde , and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for Sentiment Clas-sification.
Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics,187?205.
Prague, Czech Republic.Richard Farkas, Veronika Vincze, Gyorgy Mora,Janos Csirik and Gyorgy Szarvas.
2010.
TheCoNLL-2010 Shared Task: Learning to DetectHedges and their Scope in Natural Language Text.54Proceedings of the Fourteenth Conference on Com-putational Natural Language Learning: SharedTask, 1?12.
Uppsala, Sweden.Nancy Ide and Laurent Romary.
2006.
RepresentingLinguistic Corpora and Their Annotations.
Pro-ceedings of the Fifth Language Resources and Eval-uation Conference, Genoa, Italy.Amber Stubbs.
2011.
MAE and MAI: LightweightAnnotation and Adjudication Tools.
Proceed-ings of the Fifth Linguistic Annotation Workshop.LAW V. Portland, Oregon.Peter D. Turney.
2002.
Thumbs Up or ThumbsDown?
Semantic Orientation Applied to Unsu-pervised Classification fo Reviews.
Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics, 417?424.
Philadelphia,Pennsylvania.Wikipedia.
2011a.
http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view.
Accessed May5, 2011.Wikipedia.
2011b.
http://toolserver.org/?soxred93/articleinfo/index.php?%20article=Land of Israel&lang=en&wiki=wikipedia.
Accessed May 5,2011.Wikipedia.
2011c.
http://toolserver.org/?soxred93/articleinfo/index.php?%20article=Fresh to Order&lang=en&wiki=wikipedia.
Accessed May 5,2011.Wikipedia.
2011d.
http://en.wikipedia.org/wiki/Special:Statistics.
Accessed May 5, 2011.Wikipedia.
2011e.
http://en.wikipedia.org/wiki/Weasel word.
Accessed May 5, 2011.Wikipedia.
2011f.
http://en.wikipedia.org/wiki/Category:NPOV disputes.
Accessed May 5, 2011.Theresa Wilson, Janyce Wiebe, and Paul Hoffman.2005.
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.
Joint Human Lan-guage Technology Conference and the Conferenceon Empirical Methods in Natural Language Pro-cessing, 347?354.
Vancouver, Canada.55
