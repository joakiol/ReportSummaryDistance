Proceedings of NAACL HLT 2007, Companion Volume, pages 197?200,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsChinese Named Entity Recognition with Cascaded Hybrid ModelXiaofeng YUInformation Systems LaboratoryDepartment of Systems Engineering & Engineering ManagementThe Chinese University of Hong KongShatin, N.T., Hong Kongxfyu@se.cuhk.edu.hkAbstractWe propose a high-performance cascaded hy-brid model for Chinese NER.
Firstly, we useBoosting, a standard and theoretically well-founded machine learning method to combine aset of weak classifiers together into a base sys-tem.
Secondly, we introduce various types ofheuristic human knowledge into Markov LogicNetworks (MLNs), an effective combinationof first-order logic and probabilistic graphi-cal models to validate Boosting NER hypothe-ses.
Experimental results show that the cas-caded hybrid model significantly outperformsthe state-of-the-art Boosting model.1 IntroductionNamed entity recognition (NER) involves the identifica-tion and classification of certain proper nouns in text,such as person names (PERs), locations (LOCs), orga-nizations (ORGs), miscellaneous names (MISCs), tem-poral, numerical and monetary phrases.
It is a well-established task in the NLP community and is regardedas crucial technology for many NLP applications, such asinformation extraction, question answering, informationretrieval and machine translation.Compared to European-language NER, Chinese NERseems to be more difficult (Yu et al, 2006).
Recent ap-proaches to Chinese NER are a shift away from man-ually constructed rules or finite state patterns towardsmachine learning or statistical methods.
However, rule-based NER systems lack robustness and portability, andmachine learning approaches might be unsatisfactory tolearn linguistic information in Chinese NEs.
In fact,Chinese NEs have distinct linguistic characteristics intheir composition and human beings usually use priorknowledge to recognize NEs.
For example, about 365of the highest frequently used surnames cover 99% Chi-nese surnames (Sun et al, 1995).
For the LOC ??/Beijing City?, ??/Beijing?
is the name part and?/City?
is the salient word.
For the ORG ???
?/Beijing City Government?, ??/Beijing?
is the LOCname part, ?/City?
is the LOC salient word and ???/Government?
is the ORG salient word.
Some ORGscontain one or more PERs, LOCs, MISCs and ORGs.
Amore complex example is the nested ORG ?VfIfI'f??
:fb/School of Computer Science,Wuhan University, Wuhan City, Hubei Province?
whichcontains two ORGs ?fI'f/Wuhan University?
and???
:fb/School of Computer Science?
and twoLOCs ?V/Hubei Province?
and ?fI/WuhanCity?.
The two ORGs contain ORG salient words ?'f/University?
and ?fb/School?, while the two LOCscontain LOC salient words ?/Province?
and ?/City?respectively.Inspired by the above observation, we propose a cas-caded hybrid model for Chinese NER 1.
First, we em-ploy Boosting, which has theoretical justification and hasbeen shown to perform well on other NLP problems,to combine weak classifiers into a strong classifier.
Wethen exploit a variety of heuristic human knowledge intoMLNs, a powerful combination of logic and probabil-ity, to validate Boosting NER hypotheses.
We also usethree Markov chain Monte Carlo (MCMC) algorithmsfor probabilistic inference in MLNs.
Experimental re-sults show that the cascaded hybrid model yields betterNER results than the stand-alone Boosting model by alarge margin.2 BoostingThe main idea behind the Boosting algorithm is that a setof many simple and moderately accurate weak classifiers(also called weak hypotheses) can be effectively com-bined to yield a single strong classifier (also called thefinal hypothesis).
The algorithm works by training weakclassifiers sequentially whose classification accuracy isslightly better than random guessing and finally combin-1In this paper we only focus on PERs, LOCs, ORGs andMISCs.
Since temporal, numerical and monetary phrases canbe well identified with rule-based approaches.197ing them into a highly accurate classifier.
Each weak clas-sifier searches for the hypothesis in the hypotheses spacethat can best classify the current set of training examples.Based on the evaluation of each iteration, the algorithmre-weights the training examples, forcing the newly gen-erated weak classifier to give higher weights to the exam-ples that are misclassified in the previous iteration.We use the AdaBoost.MH algorithm (Schapire andSinger, 1999) as shown in Figure 1, an n-ary classi-fication variant of the original well-known binary Ad-aBoost algorithm (Freund and Schapire, 1997).
It hasbeen demonstrated that Boosting can be used to buildlanguage-independent NER models that perform excep-tionally well (Wu et al (2002), Wu et al (2004), Carreraset al (2002)).
In particular, reasonable Chinese NERresults were still obtained using Boosting, even thoughthere was no Chinese-specific tuning and the model wasonly trained on one-third of the provided corpora inSIGHAN bakeoff-3 (Yu et al, 2006).3 Markov Logic NetworksA Markov Network (also known as Markov RandomField) is a model for the joint distribution of a set ofvariables (Pearl, 1988).
It is composed of an undirectedgraph and a set of potential functions.
A First-OrderKnowledge Base (KB) (Genesereth and Nislsson, 1987)is a set of sentences or formulas in first-order logic.
AMarkov Logic Network (MLN) (Richardson and Domin-gos, 2006) is a KB with a weight attached to each formula(or clause).
Together with a set of constants representingobjects in the domain, it species a ground Markov Net-work containing one feature for each possible groundingof a first-order formula in the KB, with the correspond-ing weight.
The weights associated with the formulas inan MLN jointly determine the probabilities of those for-mulas (and vice versa) via a log-linear model.
An MLNdefines a probability distribution over Herbrand interpre-tations (possible worlds), and can be thought of as a tem-plate for constructing Markov Networks.
The probabil-ity distribution over possible worlds x specified by theground Markov Network ML,C is given byP (X = x) =1Zexp(?wini(x )) =1Z?
?i(x{i})ni(x)(1)where Fi is the formula in first-order logic, wi is a realnumber, ni (x) is the number of true groundings of Fi inx, x{i} is the true value of the atoms appearing in Fi, and?i(x{i})= ewi .3.1 Learning WeightsGiven a relational database, MLN weights can in princi-ple be learned generatively by maximizing the likelihoodof this database.
The gradient of the log-likelihood withInput: A training set Tr = {< d1, C1 >, .
.
.
, < dg, Cg >}where Cj ?
C = {c1, ..., cm} for all j = 1, .
.
.
, g.Output: A final hypothesis ?
(d, c) =?Ss=1 ?s?s(d, c).Algorithm: LetD1(dj , ci) = 1mg for all j = 1, .
.
.
, g andfor all i = 1, .
.
.
,m. For s = 1, .
.
.
, S do:?
pass distribution Ds(dj , ci)to the weak classifier;?
derive the weak hypothesis ?s from the weakclassifier;?
choose ?s ?
R;?
set Ds+1(dj , ci) =Ds(dj ,ci)exp(?
?sCj [ci]?s(dj ,ci))ZswhereZs =?mi=1?gj=1 Ds(dj , ci )exp( ?
?sCj [ci] ?s(dj , ci))is a normalization factor chosen so that?mi=1?gj=1 Ds+1(dj , ci) = 1.Figure 1: The AdaBoost.MH algorithm.respect to the weights is?
?wilogPw(X = x) = ni (x) ?
?Pw(X = x?)ni(x?
)(2)where the sum is over all possible databases x?
, andPw(X = x?)
is P (X = x?)
computed using the currentweight vector w = (w1, ..., wi, ...).
Unfortunately, com-puting these expectations can be very expensive.
Instead,we can maximize the pseudo-likelihood of the data moreefficiently.
If x is a possible database and xl is the lthground atom?s truth value, the pseudo-log-likelihood of xgiven weights w islogP ?w(X = x) =n?l=1logPw(Xl=xl | MBx(Xl )) (3)where MBx (Xl) is the state of Xl?s Markov blan-ket in the data.
Computing Equation 3 and its gradientdoes not require inference over the model, and is there-fore much faster.
We optimize the pseudo-log-likelihoodusing the limited-memory BFGS algorithm (Liu and No-cedal, 1989).3.2 InferenceIf F1 and F2 are two formulas in first-order logic, C is afinite set of constants including any constants that appearin F1 or F2, and L is an MLN, thenP (F1 | F2, L, C) = P (F1 | F2,ML,C) (4)=P (F1 ?
F2 | ML,C)P (F2 | ML,C)(5)=?x??F1?
?F2P (X = x | ML,C)?x?
?F2P (X = x | ML,C)(6)198where ?Fi is the set of worlds where Fi holds, andP (x | ML,C) is given by Equation 1.
The ques-tion of whether a knowledge base entails a formula Fin first-order logic is the question of whether P (F |LKB, CKB,F ) = 1, where LKB is the MLN obtained byassigning infinite weight to all the formulas in KB, andCKB,F is the set of all constants appearing in KB or F .The most widely used approximate solution to proba-bilistic inference in MLNs is Markov chain Monte Carlo(MCMC) (Gilks et al, 1996).
In this framework, theGibbs sampling algorithm is to generate an instance fromthe distribution of each variable in turn, conditional on thecurrent values of the other variables.
One way to speedup Gibbs sampling is by Simulated Tempering (Marinariand Parisi, 1992), which performs simulation in a gener-alized ensemble, and can rapidly achieve an equilibriumstate.
Poon and Domingos (2006) proposed MC-SAT,an inference algorithm that combines ideas from MCMCand satisfiability.4 Heuristic Human KnowledgeEven though the Boosting model is able to accommodatea large number of features, some NEs, especially LOCs,ORGs and MISCs are difficult to identify due to lackof linguistic knowledge.
For example, some ORGs arepossibly mistagged as LOCs and/or MISCs.
We incor-porate heuristic human knowledge via MLNs to validatethe Boosting NER hypotheses.
We extract 151 locationsalient words and 783 organization salient words from theLDC Chinese-English bi-directional NE lists compiledfrom Xinhua News database.
We also make a punctua-tion list which contains 19 items.
We make the followingassumptions to validate the Boosting results:?
Obviously, if a tagged entity ends with a locationsalient word, it is a LOC.
If a tagged entity ends withan organization salient word, it is an ORG.?
If a tagged entity is close to a subsequent locationsalient word, probably they should be combined to-gether as a LOC.
The closer they are, the more likelythat they should be combined.?
Heuristically, if a series of consecutive tagged en-tities are close to a subsequent organization salientword, they should probably be combined togetheras an ORG because an ORG may contain multiplePERs, LOCs, MISCs and ORGs.?
Similarly, if there exists a series of consecutivetagged entities and the last one is tagged as an ORG,it is likely that all of them should be combined as anORG.?
Entity length restriction: all kinds of tagged entitiescannot exceed 25 Chinese characters.?
Punctuation restriction: in general, all tagged enti-ties cannot span any punctuation.?
Since all NEs are proper nouns, the tagged entitiesshould end with noun words.All the above human knowledge can be formulized asfirst-order logic to construct the structure of MLNs.
Andall the validated Boosting results are accepted as new NEcandidates (or common nouns).
We train an MLN to rec-ognize them.5 ExperimentsWe randomly selected 15,000 and 3,000 sentences fromthe People?s Daily corpus as training and test sets, respec-tively.
We used the decision stump2 as the weak classifierin Boosting to construct a character-based Chinese NERbaseline system.The features used were as follows:?
The current character and its POS tag.?
The characters within a window of 3 characters be-fore and after the current character.?
The POS tags within a window of 3 characters be-fore and after the current character.?
A small set of conjunctions of POS tags and charac-ters within a window of 3 characters of the currentcharacter.?
The BIO 3 chunk tags of the previous 3 characters.We declared 10 predicates and specified 9 first-order formulas according to the heuristic humanknowledge in Section 4.
For example, we usedperson(candidate) to predicate whether a candi-date is a PER.
Formulas are recursively constructed fromatomic formulas using logical connectives and quanti-fiers.
They are constructed using four types of sym-bols: constants, variables, functions, and predicates.Constant symbols represent objects in the domain ofinterest (e.g., ??/Beijing?
and ?w/Shanghai?
areLOCs).
Variable symbols range over the objects in thedomain.
Function symbols represent mappings from tu-ples of objects to objects.
Predicate symbols repre-sent relations among objects in the domain or attributesof objects.
For example, the formula endwith(r,p)?locsalientword(p)=>location(r) meansif r ends with a location salient word p, then it is a LOC.2A decision stump is basically a one-level decision treewhere the split at the root level is based on a specific at-tribute/value pair.3In this representation, each character is tagged as either thebeginning of a named entity (B tag), a character inside a namedentity (I tag), or a character outside a named entity (O tag).199We extracted all the distinct NEs (4,475 PERs, 2,170LOCs, 2,823 ORGs and 614 MISCs) from the 15,000-sentence training corpus.
An MLN training database,which consists of 10 predicates and 44,810 groundatoms was built.
A ground atom is an atomic formulaall of whose arguments are ground terms (terms con-taining no variables).
For example, the ground atomlocation(?) conveys that ??/BeijingCity?
is a LOC.During MLN learning, each formula is converted toConjunctive Normal Form (CNF), and a weight is learnedfor each of its clauses.
The weight of a clause is usedas the mean of a Gaussian prior for the learned weight.These weights reflect how often the clauses are actuallyobserved in the training data.We validated 352 Boosting results to construct theMLN testing database, which contains 1,285 entries andthese entries are used as evidence for inference.
Infer-ence is performed by grounding the minimal subset of thenetwork required for answering the query predicates.
Weapplied 3 MCMC algorithms: Gibbs sampling (GS), MC-SAT and Simulated Tempering (ST) for inference andthe comparative NER results are shown in Table 1.
Thecascaded hybrid model greatly outperforms the Boostingmodel.
We obtained the same results using GS and STalgorithms.
And GS (or ST) yields slightly better resultsthan the MC-SAT algorithm.6 ConclusionIn this paper we propose a cascaded hybrid modelfor Chinese NER.
We incorporate human heuristics viaMLNs, which produce a set of weighted first-orderclauses to validate Boosting NER hypotheses.
To the bestof our knowledge, this is the first attempt at using MLNsfor the NER problem in the NLP community.
Experi-ments on People?s Daily corpus illustrate the promise ofour approach.
Directions for future work include learningthe structure of MLNs automatically and using MLNs forinformation extraction and statistical relational learning(e.g., entity relation identification).ReferencesXavier Carreras, Llu?
?s Ma`rquez, and Llu?
?s Padro?.
Named en-tity extraction using AdaBoost.
In Computational NaturalLanguage Learning (CoNLL-2002), at COLING-2002, pages171?174, Taipei, Sep 2002.Yoav Freund and Robert E. Schapire.
A decision-theoretic gen-eralization of on-line learning and an application to boosting.Computer and System Sciences, 55(1):119?139, 1997.Michael R. Genesereth and Nils J. Nislsson.
Logical founda-tions of artificial intelligence.
Morgan Kaufmann PublishersInc., San Mateo, CA, 1987.W.R.
Gilks, S. Richardson, and D.J.
Spiegelhalter.
Markovchain Monte Carlo in practice.
Chapman and Hall, London,UK, 1996.Table 1: Comparative Chinese NER ResultsPrecision Recall F?=1BoostingPER 99.39% 99.06% 99.22LOC 87.55% 91.81% 89.63ORG 82.15% 66.61% 73.57MISC 80.00% 87.84% 83.74Overall 90.26% 89.42% 89.84Hybrid (MC-SAT)PER 99.39% 99.06% 99.22LOC 94.83% 91.81% 93.30ORG 87.82% 85.69% 86.74MISC 93.53% 85.10% 89.12Overall 95.01% 92.78% 93.88Hybrid (GS/ST)PER 99.39% 99.06% 99.22LOC 94.80% 91.91% 93.34ORG 87.82% 86.28% 87.04MISC 93.53% 85.10% 89.12Overall 94.99% 92.93% 93.95Dong C. Liu and Jorge Nocedal.
On the limited memory BFGSmethod for large scale optimization.
Mathematical Program-ming, 45:503?528, 1989.Enzo Marinari and Giorgio Parisi.
Simulated Tempering: Anew Monte Carlo scheme.
Europhysics Letters, 19:451?458,1992.Judea Pearl.
Probabilistic reasoning in intelligent systems: net-works of plausible inference.
Morgan Kaufmann PublishersInc., San Francisco, CA, 1988.Hoifung Poon and Pedro Domingos.
Sound and efficient infer-ence with probabilistic and deterministic dependencies.
In21st National Conference on Artificial Intelligence (AAAI-06), Boston, Massachusetts, July 2006.
The AAAI Press.Matthew Richardson and Pedro Domingos.
Markov logic net-works.
Machine Learning, 62(1-2):107?136, 2006.Robert E. Schapire and Yoram Singer.
Improved boosting algo-rithms using confidence-rated predictions.
Machine Learn-ing, 37(3):297?336, 1999.Maosong Sun, Changning Huang, Haiyan Gao, and Jie Fang.Identifying Chinese names in unrestricted texts.
Journal ofChinese Information Processing, 1995.Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, andYongsheng Yang.
Boosting for named entity recognition.
InComputational Natural Language Learning (CoNLL-2002),at COLING-2002, pages 195?198, Taipei, Sep 2002.Dekai Wu, Grace Ngai, and Marine Carpuat.
Why nitpickingworks: Evidence for Occam?s razor in error correctors.
In20th International Conference on Computational Linguistics(COLING-2004), Geneva, 2004.Xiaofeng Yu, Marine Carpuat, and Dekai Wu.
Boosting forChinese named entity recognition.
In 5th SIGHAN Workshopon Chinese Language Processing, Australia, July 2006.200
