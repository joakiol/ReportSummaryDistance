A Statistical NLP Approach forFeature and Sentiment Identification from Chinese ReviewsZhen Hai1 Kuiyu Chang1 Qinbao Song2 Jung-jae Kim11School of Computer Engineering, Nanyang Technological University, Singapore 639798{haiz0001, askychang, jungjae.kim}@ntu.edu.sg2Department of Computer Science and Technology, Xi?an Jiaotong University, Xi?an 710049, Chinaqbsong@mail.xjtu.edu.cnAbstractExisting methods for extracting featuresfrom Chinese reviews only usesimplistic syntactic knowledge, whilethose for identifying sentiments relyheavily on a semantic dictionary.
In thispaper, we present a systematic techniquefor identifying features and sentiments,using both syntactic and statistical anal-ysis.
We firstly identify candidate fea-tures using a proposed set of commonsyntactic rules.
We then prune irrelevantcandidates with topical relevance scoresbelow a cut-off point.
We also proposean association analysis method based onlikelihood ratio test to infer the polarityof opinion word.
The sentiment of a fea-ture is finally adjusted by analyzing thenegative modifiers in the local contextof the opinion word.
Experimental re-sults show that our system performs sig-nificantly better than a well-known opi-nion mining system.1 IntroductionThere were 420 million Internet users in Chinaby the end of June 2010.
As a result, online so-cial media in China has accumulated massiveamount of valuable peer reviews on almost any-thing.
Mining this pool of Chinese reviews todetect features (e.g.
????
mobile phone) andidentify the corresponding sentiment (e.g.
posi-tive, negative) has recently become a hot re-search area.
However, the vast majority of pre-vious work on feature detection only uses sim-plistic syntactic natural language processing(NLP) approaches, while those on sentimentidentification depend heavily on a semantic dic-tionary.
Syntactic approaches are often prone toerrors due to the informal nature of online re-views.
Dictionary-based approaches are morerobust than syntactic approaches, but must beconstantly updated with new terms and expres-sions, which are constantly evolving in onlinereviews.To overcome these limitations, we propose astatistical NLP approach for Chinese feature andsentiment identification.
The technique is in factthe core of our Chinese review mining system,called Idea Miner or iMiner.
Figure 1 shows thearchitectural overview of iMiner, which com-prises five modules, of which Module ?
(Opi-nion Feature Detection) and ?
(Contextual Sen-timent Identification) are the main focus of thispaper.?
Segmentation, POS Tagging,and Syntactic ParsingFeature?
CandidateFeature Pruning?
CandidateFeature Extraction?
ContextualSentiment Identification?
Polarity Inferencefor Opinion WordSentiment?
Review Crawling?
Feature-Sentiment SummaryFigure 1:  Overview of the iMiner system.2 Related WorkQiu et al (2007) used syntactic analysis to iden-tify features 1  in Chinese sentences, which issimilar to the methods proposed by Zhuang et al(2006) and Xia et al (2007).
However, syntacticanalysis alone tends to extract many invalid fea-tures due to the colloquial nature of online re-views, which are often abruptly concise or1 A feature refers to the subject of an opinion.grammatically incorrect.
To address the issue,our approach employs an additional step toprune candidates with low topical relevance,which is a statistical measure of how frequentlya term appears in one review and across differ-ent reviews.Pang et al (2002) examined the effectivenessof using supervised learning methods to identifydocument level sentiments.
But the techniquerequires a large amount of training data, andmust be re-trained whenever it is applied to anew domain.
Furthermore, it does not performwell at the sentence level.
Zhou et al (2008)and Qiu et al (2008) proposed dictionary-basedapproaches to infer contextual sentiments fromChinese sentences.
However, it is difficult tomaintain an up-to-date dictionary, as new ex-pressions emerge frequently online.
In contrast,to identify the sentiment expressed in a reviewregion2, our method first infers the polarity ofan opinion word by using statistical associationanalysis, and subsequently analyzes the localcontext of the opinion word.
Our method is do-main independent and uses only a small set of80 polarized words instead of a huge dictionary.2.1 Topic Detection and TrackingThe task of Topic Detection and Tracking is tofind and follow new events in a stream of newsstories.
Fukumoto and Suzuki (2000) proposeda domain dependence criterion to discriminate atopic from an event, and find all subsequentsimilar news stories.
Our idea of topical relev-ance is related but different; we only focus onthe relevance of a candidate feature with respectto a review topic, so as to extract the features onwhich sentiments are expressed.2.2 Polarity Inference for Opinion WordTurney (2002) used point-wise mutual informa-tion (PMI) to predict the polarity of an opinionword O, which is calculated as MI1-MI2, whereMI1 is the mutual information between word Oand positive word ?excellent?, and MI2 denotesthe mutual information between O and negativeword ?poor?.
Instead of PMI, our method usesthe likelihood ratio test (LRT) to compute thesemantic association between an opinion wordand each seed word, since LRT leads to better2A review region is a sentence or clause which con-tains one and only feature.results in practice.
Finally, the polarity is calcu-lated as the weighted sum of the polarity valuesof all seed words, where the weights are deter-mined by the semantic association.2.3 Feature-Sentiment Pair IdentificationTurney (2002) proposed an unsupervised learn-ing algorithm to identify the overall sentimentsof reviews.
However, his method does notdetect features to associate with the sentiments.Shi and Chang (2006) proposed to build a hugeChinese semantic lexicon to extract both fea-tures and sentiments.
Other lexicon-based workfor identifying feature-sentiment pair was pro-posed by Yi et al (2003) and Xia et al (2007).We propose a new statistical NLP approach toidentify feature-sentiment pairs, which uses notonly syntactic analysis but also data-centric sta-tistical analysis.
Most importantly, our approachrequires no semantic lexicon to be maintained.3 Feature DetectionModule ?
in iMiner aims to detect opinionfeatures, which are subjects of reviews, such asthe product itself like ????
(mobile phone) orspecific attributes like ????
(screen).Example 1: ????????????
(I likethe color of this mobile phone).In example 1, the noun ????
(color) indi-cates a feature.
Some features are expressedimplicitly in review sentences, as shown below.Example 2: ??????????
(Too expen-sive, I cannot afford it).In example 2, the noun ????
(price) is theopinion feature of this sentence, but it does notoccur explicitly.
In this paper, we do not dealwith implicit features, but instead focus on theextraction of explicit features only.3.1 Candidate Feature ExtractionAccording to our observation, features are gen-erally expressed as nouns and occur in certainpatterns in Chinese reviews.
Typically, a nounacting as the object or subject of a verb is a po-tential feature.
In addition, when a clause con-tains only a noun phrase without any verbs, theheadword of the noun phrase is also a candidate.Due to the colloquial nature of online reviews, itis complicated and nearly impossible to collectall possible syntactic roles of features.
Thus, weTable 1: Dependence relations and syntactic rules for candidate feature extraction.Relation Ruleonly use the aforementioned three primary pat-terns to extract an initial set of candidates.Dependence Grammar (Tesniere, 1959) expl-ores asymmetric governor-dependent relation-ship between words, which are then combinedinto the dependency structure of sentences.
Thethree dependency relations SBV, VOB, andHED correspond to the three aforementionedpatterns.
For each relation, we define a rule withadditional restrictions for candidate feature ex-traction, as shown in Table 1.Candidate features are extracted in the fol-lowing manner: for each word, we first deter-mine if it is a noun; if so, we apply the VOB,SBV, and HED rules sequentially.
A nounmatching any of the rules is extracted as a can-didate feature.3.2 Candidate Feature PruningDue to the informal nature of online reviews, alarge number of irrelevant candidates are ex-tracted by the three syntactic rules.
Thus, weneed to further prune them by using additionaltechniques.Intuitively, candidates that are found in manyreviews should be more representative com-pared to candidates that occur in only a few re-views.
This characteristic of candidates can becaptured by the topical relevance (TR) score.TR can be used to measure how strongly a can-didate feature is relevant to a review topic.
TheTR of a candidate is described by two indica-tors, i.e., dispersion and deviation.
Dispersionindicates how frequently a candidate occursacross different reviews, while deviation de-notes how many times a candidate appears inone review.
The topical relevance score (TRS)is calculated by combining both dispersion anddeviation.
Candidate features with high TRS aresupposed to be highly relevant, while those withTRS lower than a specified threshold are re-jected.Formally, let the i-th candidate feature be de-noted by Ti, and the j-th review document3 byDj.
The weight of feature Ti in document Dj isdenoted by Wij, which could be computed basedon TF.IDF (Luhn, 1957) shown in formula (1):(1 log ) * log 00ij ijiijNTF if TFDFWotherwise+      >=?????
(1)TFij denotes the term frequency of Ti in Dj, andDFi denotes the document frequency of Ti; Nindicates the number of documents in the cor-pus.
We compute the standard deviation Si:21( )1NiijjiW WSN=?= ??
(2)where the average weight of Ti across all docu-ments is calculated as follows:11 Ni ijW WN == j?
.The dispersion Dispi of Ti is then calculated:iiiWDispS=  (3)The deviation Deviij of Ti in Dj is computed:3 A review document refers to a forum review, whichtends to be shorter than full length editorial articles.Interpretation Example (3-5)VOB ( , ) ( , )N VOB N C ?If term is noun (N) anddepends on another com-ponent with relation VOB,extract as candidate (C).?????????
(I like the mobilephone).
The noun ????
relies on theword ????
with relation VOB, thus,????
is extracted as candidate.SBV ( , ) ( , )N SBV N C ?If term is noun (N) anddepends on another com-ponent with relation SBV,extract as candidate (C).???????
(The screen is toosmall).
The noun ????
depends onthe word ???
with relation SBV, thus????
is extracted as candidate.HED ( , ) ( , )N HED N C ?If term is noun (N) andgoverns another compo-nent with relation HED,extract as candidate (C).???????
(beautiful exterior).The noun ????
governs the word????
with relation HED, thus, ????
is extracted as candidate.
'ij ij jDevi W W= ?
(4)The average scalar weight of all candidate fea-tures in Dj is calculated as follows:1' 1 MjiijW WM == ?where M is the vocabulary size of Dj.We can obtain the topical relevance scoreTRSij of Ti in Dj finally as follows:*ij i ijTRS Disp Devi=  (5)By combining the dispersion and deviation,the quantity TRSij thus captures the topical re-levance strength of Ti with respect to the topic ofdocument Dj.All candidates of a document are then sortedin descending order of TRS, and those withTRS above a pre-specified threshold are ex-tracted as opinion features.
In fact, we can ex-tract candidates at the document, paragraph, orsentence resolution.
In practice, we observe nosignificant performance differences at the vari-ous resolutions.3.3 Experimental EvaluationWe collected 2,986 real-life review documentsabout mobile phones from major online Chineseforums.
Each document corresponds to a forumtopic, where each paragraph in a documentmatches a thread under the topic.
Of these, wemanually annotated the features and sentimentorientations expressed in 219 randomly selecteddocuments, which include 600 review sentences.To evaluate the performance of our approach,we first conducted an experiment for extractingcandidate features.
We then performed threeother experiments for pruning the candidates atthe document, paragraph, and sentence levels,respectively.
For each experiment, we tried sev-eral different thresholds, i.e., percentage of TRSmean (TRSM) of all candidates.
The average F-measure (F), precision (P), and recall (R) of theresults at the three levels are shown in Figure 2.The highest F-measure results of feature detec-tion with and without pruning are shown in Ta-ble 2 for easy comparison.Table 2: Feature detection results.Feature Detection P (%) R (%) F (%)No Pruning 71.61 90.69 80.03Pruning  (33% TRSM) 81.56 85.22 83.35As line 2 of Table 2 shows, feature detectionwithout pruning achieves 90.69% recall, whichshows that the proposed syntactic rules haveexcellent coverage.
However, its precision is notso promising, achieving only 71.61%, whichmeans that many irrelevant candidates are alsoextracted by our rules.
Thus, relying on syntac-tic analysis alone is not good enough, and weneed to take one more step to prune the candi-date features.As line 3 of Table 2 shows, after pruning thecandidate set, precision improved remarkablyby 10% to 81.56%, while recall dropped slightlyto 85.22%.
For online review mining, precisionis much more important than recall, becauseusers?
confidence in iMiner rely heavily on theaccuracy of the results they see (precision), andnot on what they don?t see (recall).Figure 2: iMiner feature pruning results.Figure 2 plots the results of pruning at vari-ous TRSM thresholds.
The best F-measure of83.35% was achieved with a 33% TRSM.
If weincrease the threshold to 43%, the precision in-creases to 83.19%, while the recall drops to81.57%.
By exploring the distribution of a can-didate in corpus, its topical relevance with re-spect to the review topic can be measured statis-tically, which allows the noisy candidates to bepruned effectively.
From the results in Figure 2,our idea of topical relevance is shown to behighly effective in detecting features.Table 3: Characteristics of FBS and iMiner.Aspects FBS iMinerCandidates Nouns from POS taggerNouns from syn-thetic analysisPruning Association MiningTopicalRelevanceOpinion word Adjectives Adjectives, verbsPolarityinferenceDictionarybasedLRT associationbasedSentimentResolution Sentence Sentence, clauseNegation Single Single, doubleWe compared our results with that of the as-sociation mining-based method in Feature-basedSummarization (FBS) (Hu and Liu, 2004) onthe same dataset.
Table 3 summarizes the dif-ferences between FBS and iMiner, parts ofwhich are elaborated in Section 4.
The results ofFBS with various support thresholds are shownin Figure 3.
The support corresponds to the per-centage of total number of review sentences.FBS attained the highest F-measure of 76.35%at a support of 0.4% with 79.6% precision and73.36% recall.
As the support increases, theprecision also increases from 62.99% to 86.92%,while the recall decreases from 91.61% to61.86%.
Comparing the best results of the twosystems, iMiner beats FBS by 7% in F-Measure,1.96% in precision, and 11.86% in recall.Figure 3: FBS feature extraction results.We find that FBS suffers from the followinglimitations: (1) FBS extracted an additional14.11% noisy candidate features due to the lackof syntactic analysis, which requires more ex-tensive pruning; and (2) FBS only considerssentence frequency in computing the support toidentify frequent candidate features, ignoringthe candidate frequency within the sentence.3.4 Feature Extraction Error AnalysisWe categorize our feature extraction errorsinto 4 main types, FE1 to FE4, as follows.FE1: When more than one candidate exists ina review region, our algorithm may pick thewrong features due to misplaced priorities.
Notethat we assume only one (dominant) feature perregion in both our algorithm and the labeleddataset.
A total of 43% errors were due to pick-ing the wrong dominant candidate.Example 6: ?????????????
(Thesound is too weak, people cannot listen clear-ly).In example 6, both ????
and ???
are ex-tracted as features.
However, the noun ???
isan incorrect feature detected by our algorithm.FE2: The proposed set of common syntacticrules is not comprehensive, missing out 23% oftrue features.Example 7: ????????????
(I amsick about this phone).In example 7, the noun ????
is a missedfeature.
This is in fact a POB dependence rela-tion, which is outside the scope of our threerules.FE3: About 22% errors are due to irrelevantfeatures possessing high TR scores, and there-fore which are not pruned subsequently.Example 8: ????????????
(I like itvery much, but I have no money to buy it).In example 8, the noun ???
is incorrectlyconfirmed as a feature due to its high TR score.FE4: About 9% errors are due to incorrectPOS tags.Example 9: ?????????
(Consistentinterruption during phone calls).In example 9, the verb ???
is extracted in-correctly as a feature, since it is incorrectlytagged as a noun.
The remaining 3% of the er-rors are due to the system incorrectly extractingfeatures from sentences that contain no opi-nions.4 Contextual Sentiment IdentificationThe main task of module ?
in iMiner is to iden-tify the contextual sentiment of a feature.
Atwo-step approach is proposed: (1) The polarityof an opinion word within a review region isinferred via association analysis based on thelikelihood ratio test; and (2) the sentiment isvalidated against the contextual information ofthe opinion word in the region and finalized.4.1 Polarity Inference for Opinion WordTo infer polarity, an opinion word is first identi-fied in a review region, as described in Figure 4.Note that we consider not only adjectives butalso verbs as opinion words.
We then measurethe association between the opinion word andeach seed word.
We calculate the polarity valueof the opinion word as the association weightedsum of polarities of all seed words.Example 10: ???????????
(Theprice of this mobile phone is very cheap).Example 10 contains an adjective ???
?
(cheap) that governs the feature ????
(price);thus ????
is extracted as an opinion word.1.
feature Ti and word Wj in the same region2.
if (Wj = adjective and depends on Ti)3.        extract Wj as opinion word;4.     else if (Wj = adjective and governs Ti)5.        extract Wj as opinion word;6.     else if (Wj = verb and governs Ti)7.        extract Wj as opinion word;Figure 4: Extracting Opinion WordA set of polarized words were collected fromcorpus as seed words, including 35 positivewords, 36 negative words, and 9 neutral words.Each seed word is assigned a polarity weightfrom -10 to 10.
For example, ????
(lovely)has a score of 10, ????
(common) has a scoreof 0, and ????
(lousy) has a score of -10.To measure the semantic association Aij be-tween an opinion word Oi and each seed word Sj,we propose a formula based on the likelihoodratio test (Dunning, 1993), as follows:1 1 1 2 2 21 1 2 22[ log ( , , ) log ( , , )log ( , , ) log ( , , ) ]ijA L p k n L p k nL p k n L p k n+?
?=(6)where( , , ) (1 )k nL p k n p p ?= ?
k ;1 21 2k kpn n+= + ,111kpn= , 222kpn= ;1 1n k k= + 3 2 2n k k= +, .
4The variable k1(O, S) in Table 4 refers to thecount of documents containing both opinionword O and seed word S, k2(O,?S) indicates thenumber of documents containing O but not S,k3(?O, S) counts the number of documents con-taining S but not O, while k4(?O,?
S) tallies thecount of documents containing neither O nor S.Table 4: Document counts.S ?
SO k1 (O, S) k2 (O,?
S)?O k3 (?O, S) k4 (?O,?
S)The higher the quantity Aij, the stronger thesemantic association is between the opinionword and the seed word.The polarity value OVi of the opinion word Oiis computed as the association weighted averageof all seed word polarity values:1*Liji jj iAOV SVA== ?
(7)The sum Ai of all association strength is calcu-lated as follows:1Li ijA A== j?
;where Aij denotes the association between Oiand Sj, SVj  indicates the polarity value of Sj, andL is the size of the seed word list.After performing association analysis, wethen classify the polarity value OVi using anupper bound V+ and lower bound V-, such thatif Vi is larger than V+, then the polarity is in-ferred as positive; conversely if Vi is smallerthan V-, then the polarity is inferred as negative;otherwise, it is neutral.
Here, the V+ and V-boundaries refer to thresholds that can be de-termined experimentally.4.2 Contextual Sentiment IdentificationApart from inferring the polarity of opinionwords, we also examine additional contextualinformation around the opinion words.
In fact,the final sentiment is determined by combiningthe polarity with the contextual information.
Inthis work, we focus on negative modifiers, asshown in the examples below.Example 11: ??????????
(I do notlike this mobile phone).In example 11, the polarity of the opinionword ????
(like) is inferred as positive, butthe review region expresses a negative orienta-tion to the feature ???
?, because a negationword ???
(not) modifies ????.
Thus, it isimportant to locate negative modifiers.Example 12: ???????????
(Thescreen of this mobile phone is not unlovely).In example 12, the polarity of opinion word????
(lovely) is inferred as positive.
By ex-amining its direct modifier, i.e., ???
(un-), weidentify the sentiment of ?????
(unlovely) asnegative.
However, the final sentiment about thefeature ????
(screen) is actually positive dueto the earlier negation ????
(not), which mod-ifies the latter ?????
(unlovely).
This is whatwe call a double negation sentence, which is notuncommon in reviews.
Therefore, it is necessaryto take two additional steps to capture thedouble negation as follows.Figure 5 shows the main steps of identifyingcontextual sentiment.
For an opinion word Oi inthe review region, we first determine if thereexists an adverb modifying it.
If so, we extractthe adverb as the direct modifier.
If the modifierhas a negative meaning, then we reverse theprior polarity of Oi.
Similarly, we can take oneadditional step to locate the double negationmodifier and finally identify the contextual sen-timent orientation.1.
for each opinion word Oi2.
if (a word Wj = adverb and depends on Oi)3.        extract Wj as direct modifier;4.        if (word Wj = negation word)5.           reverse the prior polarity of Oi;6.        if (word Wk = adverb and relies on Wj)7.           extract Wk as indirect modifier;8.           if (word Wk = negation word)9.              reverse the current polarity of Oi;10.   output the current polarity of Oi;Figure 5: Identifying the Contextual Sentiment4.3 Experimental EvaluationSince features are detected prior to the senti-ments, there is a possibility for an erroneousfeature (i.e., a false positive feature) to be asso-ciated with a sentiment.
We thus conducted twodifferent experiments.
In the first case, we enu-merate all extracted feature-sentiment pairs,including the wrong features.
In the second sce-nario, we enumerate the feature-sentiment pairsonly for those correctly extracted features.
Foreach experiment, we further evaluated the resultwith (C) and without (N.C.) contextual informa-tion.We select the best case of feature detectionand then run our sentiment identification algo-rithm on the review dataset described in section3.3; the polarity thresholds V- and V+ are set to0.45 and 0.5, respectively.Table 5: Results for all features.Systems P (%) R (%) F (%)iMinerN.C.
57.07 58.21 57.63C.
70.3 71.72 71FBS 49.70 45.80 47.67Table 5 shows the results for all detected fea-tures (correct and incorrect).
As shown in line 2,our method achieved an F-measure of 57.63%without considering contextual information,while precision and recall are 57% and 58.21%,respectively.
Adding contextual information, asline 3 shows, boosts the F-measure to 71%, aremarkable 13.37% improvement.Table 6 shows the results for just the correct-ly extracted features.
As shown in line 2, in thecase of not considering contextual information,our method achieved an F-measure of 63.17%,while precision and recall were 69.05% and58.21%, respectively.
By considering contextualinformation, line 3 shows that the F-measureimproved to 77.82% which is 14.65% better,with precision and recall at 85.06% and 71.72%,respectively.
The above results show that localcontextual analysis of double and single nega-tion can significantly improve the accuracy ofsentiment orientation identification.Table 6: Results for correctly detected features.Systems P (%) R (%) F (%)iMinerN.C.
69.05 58.21 63.17C.
85.06 71.72 77.82FBS 62.45 45.80 52.84By examining the results shown in line 3 (inbold) of both Tables 5 and 6, the F-measure oncorrectly identified features increases from 71%to 77.82%, while the precision increases drasti-cally from 70.3% to 85.06%.
The results showthat our two-step approach of identifying senti-ment orientation is reasonable and effective andthat a great many of sentiments can be identifiedcorrectly for related features, especially forthose correctly detected one.
However, in prac-tice there is no way to tell the correctly identi-fied features from the incorrect ones, thus Table5 is a more realistic gauge of our approach..Lastly, we compared our approach to senti-ment identification with FBS (see Table 3).
Thebest results are used, as shown in the last rowsof Table 5 and 6.
When considering all featuresextracted, the F-measure of FBS is only 47.67%,which is 23.33% lower than that of iMiner,where both precision and recall are 49.70% and45.80%, respectively.
Considering only the cor-rectly detected features, iMiner widens its leadover FBS to 25% in terms of F-measure.There are several explanations for the poorresults of FBS: (1) The inferior results of featuredetection affect the subsequent task of sentimentidentification; and (2) the polarity inference de-pends heavily on a semantic dictionary Word-Net.
In our experiments for FBS, we used anextended version of the ???????
Thesau-rus containing 77,492 words, and a sentimentlexicon with 8,856 words that is part of mini(free) HowNet, and lastly our seed word listcontaining 80 words.4.4 Sentiment Identification Error AnalysisWe classify our sentiment identification er-rors into 5 main types, SE1 to SE5, as follows.SE1: Sentiment identification relies heavilyon feature extraction, which means that if fea-tures are detected wrongly, it is impossible forthe sentiment identified to be correct.
About49% of false sentiments are due to incorrectlyextracted features.Even for the correctly extracted features,there are still several errors as listed below.SE2: Incorrectly identified opinion words canlead to mistakes in inferring sentiments, ac-counting for 14% of the errors.SE3: Errors in detecting contextual informa-tion about opinion words led to 12% of thewrong sentiment identification results.SE4: Both the quality and quantity of seedwords influence sentiment identification.SE5: The threshold choices for V+ and V- di-rectly impact the polarity inference of opinionwords, affecting the sentiment identification.SE4 and SE5 errors account for the remaining25% of the erroneous sentiment results.5 ConclusionThe main contribution of this paper is the pro-posed systematic technique of identifying bothfeatures and sentiments for Chinese reviews.Our proposed approach compares very favora-bly against the well-known FBS system on asmall-scale dataset.
Our feature detection is 7%better than FBS in terms of F-measure, withsignificantly higher recall.
Meanwhile, our ap-proach of identifying contextual sentimentachieved around 23% better F-measure thanFBS.We plan to further explore effective methodsto deal with the various feature and sentimenterrors.
In addition, we plan to explore the ex-traction of implicit features, since a significantnumber of reviews express opinion via implicitfeatures.
Lastly, we plan to test out these im-provements on a large-scale dataset.AcknowledgementWe thank Harbin Institute of Technology?s Cen-ter for Information Retrieval in providing theirLanguage Technology Platform (LTP) software.This research was supported in part by Singa-pore Ministry of Education?s Academic Re-search Fund Tier 1 grant RG 30/09.ReferencesDunning, T. E. 1993.
Accurate methods for the statistics ofsurprise and coincidence.
Computational Linguistics19(1).Fukumoto, Fumiyo, and Yoshimi Suzuki.
2000.
EventTracking based on Domain Dependence, SIGIR.Hu, Minqing, and Bing Liu.
2004.
Mining and summariz-ing customer reviews, SIGKDD, Seattle, WA, USA.Luhn, Hans Peter.
1957.
A statistical approach tomecha-nized encoding and searching of literary information.IBM Journal of Research and Development 1 (4):309-17.Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification using Ma-chine Learning Techniques, EMNLP.Qiu, Guang, Kangmiao Liu, Jiajun Bu, Chun Chen, andZhiming Kang.
2007.
Extracting opinion topics forChinese opinions using dependence grammar,ADKDD, California, USA.Qiu, Guang, Can Wang, Jiajun Bu, Kangmiao Liu, andChun Chen.
2008.
Incorporate the Syntactic Knowledgein Opinion Mining in User-generated Content, WWW,Beijing, China.Shi, Bin, and Kuiyu Chang.
2006.
Mining Chinese Re-views, ICDM Data Mining on Design and MarketingWorkshop.Tesniere, L. 1959.
Elements de Syntaxe Structurale: Li-brairie C. Klincksieck, Paris.Turney, Peter D. 2002.
Thumbs Up or Thumbs Down?Semantic Orientation Applied to Unsupervised Classi-fication of Reviews, ACL, Philadelphia.Xia, Yunqing, Ruifeng Xu, Kam-Fai Wong, and FangZheng.
2007.
The Unified Collocation Framework forOpinion Mining.
International Conference on MachineLearning and Cybernetics.Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment Analyzer: ExtractingSentiments about a Given Topic using Natural Lan-guage Processing Techniques, ICDM.Zhou, Chao, Guang Qiu, Kangmiao Liu, Jiajun Bu, Ming-cheng Qu, and Chun Chen.
2008.
SOPING : a Chinesecustomer review mining system, SIGIR, Singapore.Zhuang, Li, Feng Jing, and Xiaoyan Zhu.
2006.
MovieReview Mining and Summarization, CIKM.
