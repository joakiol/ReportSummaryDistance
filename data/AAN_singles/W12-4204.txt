Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 30?38,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsTowards a Predicate-Argument Evaluation for MTOndr?ej Bojar?, Dekai Wu??
Charles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied Linguistics?
HKUST, Human Language Technology Center,Department of Computer Science and EngineeringHong Kong University of Science and Technologybojar@ufal.mff.cuni.cz, dekai@cs.ust.hkAbstractHMEANT (Lo and Wu, 2011a) is a man-ual MT evaluation technique that focuses onpredicate-argument structure of the sentence.We relate HMEANT to an established lin-guistic theory, highlighting the possibilities ofreusing existing knowledge and resources forinterpreting and automating HMEANT.
Weapply HMEANT to a new language, Czechin particular, by evaluating a set of English-to-Czech MT systems.
HMEANT proves tocorrelate with manual rankings at the sentencelevel better than a range of automatic met-rics.
However, the main contribution of thispaper is the identification of several issuesof HMEANT annotation and our proposal onhow to resolve them.1 IntroductionManual evaluation of machine translation output isa tricky enterprise.
It has been long recognizedthat different evaluation techniques lead to differentoutcomes, e.g.
Blanchon et al (2004) mention anevaluation carried out in 1972 where the very sameRussian-to-English MT outputs were scored 4.5 outof the maximum 5 points by prospective users ofthe system but only 1 out of 5 by teachers of En-glish.
Throughout the years, many techniques wereexplored with more or less of a success.The two-scale scoring for adequacy and fluencyused in NIST evaluation has been abandoned bysome evaluation campaigns, most notably the WMTshared task series, see Koehn and Monz (2006)through Callison-Burch et al (2012)1.
Since 2008,WMT uses a simple relative ranking of MT out-puts as its primary manual evaluation technique:the annotator is presented with up to 5 MT out-puts for a given input sentence and the task is torank them from best to worst (ties allowed) on what-ever criteria he or she deems appropriate.
While thissingle-scale relative ranking is perhaps faster to an-notate and reaches a higher inter- and intra-annotatoragreement than the (absolute) fluency and adequacy(Callison-Burch et al, 2007), the technique and itsevaluation are still far from satisfactory.
Bojar etal.
(2011) observe several discrepancies in the in-terpretation of the rankings, partly due to the highload on human annotators (the comparison of sev-eral long sentences at once, among other issues) butpartly also due to technicalities of the calculation.Lo and Wu (2011a) present an interesting evalua-tion technique called MEANT (or HMEANT if car-ried out by humans), the core of which lies in as-sessing whether the key elements in the predicate-argument structure of the sentence have been pre-served.
In other words, lay annotators are check-ing, if they recognize who did what to whom, when,where and why from the MT outputs and whetherthe respective role fillers convey the same meaningas in the reference translation.
HMEANT has beenshown to correlate reasonably well with manual ad-equacy and ranking evaluations.
It is relatively fastand should lend itself to full automatization.
Onthe other hand, HMEANT was so far tested only ontranslation into English and with just three compet-ing MT systems.1http://www.statmt.org/wmt06 till wmt1230In this work, we extend the application ofHMEANT to evaluating MT into Czech, a mor-phologically rich language with relatively free wordorder.
The paper is structured as follows: Sec-tion 2 presents the technical details of HMEANTand relates HMEANT to an established linguistictheory that underlies the Prague dependency tree-banks (Hajic?
et al, 2006; Hajic?
et al, 2012) andseveral other works.
We also suggest possible ben-efits of this coupling such as the reuse of tools.
InSection 3, we describe the setup and results of ourHMEANT experiment.
Since this is the first timeHMEANT is applied to a new language, Section 4constitutes the main contribution of this work.
Wepoint out at several problems of HMEANT and pro-pose a remedy, the empirical evaluation of whichhowever remains for future work.
Section 5 con-cludes our observations.2 Relating HMEANT and Valency Theoryof FGD2.1 HMEANT Annotation ProcedureHMEANT is designed to be simple and fast.
Theannotation consists of two steps: (1) semantic rolelabelling, SRL in the sequel, and (2) alignment ofroles between the hypothesis and the reference.The annotation guidelines are deliberately mini-malistic, so that even inexpert people can learn themquickly.
The complete guidelines for SRL are givenin Figure 1 and it takes less than 15 minutes to trainan unskilled person.In the alignment task, the annotators first indicatewhich frames in the reference and the hypothesiscorrespond to each other.
In the second step, theyalign all matching role fillers to each other and alsomark the translation as ?Correct?
or ?Partial?.The HMEANT calculation then evaluates the f-score of the predicates and their role fillers in a givensentence.
An important aspect of the calculation isthat unmatched predicates with all their role fillersare excluded from the calculation.2.2 Functional Generative DescriptionThe core ideas of HMEANT follow the case gram-mar (Fillmore, 1968) or PropBank (Palmer et al,2005) and can be also directly related to an estab-lished linguistic theory which was primarily devel-Semantic frames summarize a sentence using a simple eventstructure that captures essential parts of the meaning like?who did what to whom, when, where, why and how?.Phrases or clauses that express meanings can be identified asplaying a particular semantic role in the sentence.
In otherwords, semantic frames are the systematic abstraction of themeanings in a sentence.The following is the list of the semantic roles to be used inHMEANT evaluation:Agent (who) Action (did)Experiencer or Patient (what) Benefactive (whom)Temporal (when) Locative (where)Purpose (why) Manner (how)Degree or Extent (how) Modal (how) [may, should, ...]Negation (how) [not] Other adverbial argument (how)You may consider the Action predicate to be the centralevent, while the other roles modify the Action to give a moredetailed description of the event.
Each semantic frame con-tains exactly one Action and any number of other roles.Please note that the Action predicate must be exactly ONEsingle word.There may be multiple semantic frames in one sentence, be-cause a sentence may be constructed to describe multipleevents and each semantic frame captures only one event.Figure 1: Semantic role labeling guidelines of HMEANT.oped for Czech, namely the Functional GenerativeDescription (Sgall, 1967; Sgall et al, 1986).
Thetheory defines so-called ?tectogrammatical?
layer (t-layer).
At the t-layer, each sentence is represented asa dependency tree with just content words as sepa-rate nodes.
All auxiliary words are ?hidden?
intoattributes of the corresponding t-nodes.
Moreover,ellipsis is restored to some extent, so e.g.
droppedsubject pronouns do have a corresponding t-node.An important element of FGD is the valency the-ory (Panevova?, 1980) which introduces empiricallinguistic tests to distinguish between what othertheories would call complements vs. adjuncts andpostulates the relationship between the set of verbmodifiers as observed in the sentence and the set ofvalency slots that should be listed in a valency dic-tionary.
This aspect could provide a further refine-ment of HMEANT, e.g.
weighing complements andadjuncts differently.FGD has been thoroughly tested and refined dur-ing the development of the Prague DependencyTreebank (Hajic?
et al, 2006)2 and the parallelPrague Czech-English Dependency Treebank (Hajic?2http://ufal.mff.cuni.cz/pdt2.0/31et al, 2012)3.
Note that the latter is a translationof all the 49k sentences of the Penn Treebank WSJsection.
Both English and Czech sentences are man-ually annotated at the tectogrammatical layer, wherethe English layer is based on the Penn annotationand manually adapted for t-layer.
Both languages in-clude their respective valency lexicons and the workon a bilingual valency lexicon is being developed(S?indlerova?
and Bojar, 2010).A range of automatic tools to convert plain text upto the t-layer exist for both English and Czech.
Mostof them are now part of the Treex platform (Popeland Z?abokrtsky?, 2010)4 and they were successfullyused in automatic annotation of 15 million parallelsentences (Bojar et al, 2012)5 as well as other NLPtasks including English-to-Czech MT.
Recently, sig-nificant effort was also invested in parsing not quitecorrect output of MT systems into Czech for thepurposes of rule-based grammar correction (Rosa etal., 2012).
Establishing the automatic pipeline forMEANT should be relatively easy with these toolsat hand.2.3 HMEANT vs. FGD ValencyThe formulation of HMEANT in terms of FGD isstraightforward: it is the f-score of matched t-nodesfor predicates and the subtrees of their immediatedependents in the t-trees of the hypothesis and thereference.HMEANT uses a simple web-based annotationinterface which operates on the surface form of thesentence.
Annotators mark the predicate and theircomplementations as contiguous spans in the sen-tence.
While this seems natural when we wantlay people to annotate, it brings some problems,see Section 4.
A linguistically adequate interfacewould allow to mark tectogrammatical nodes andsubtrees in the t-layer, however, the customizableeditor TrEd6 used for manual annotation of t-layeris too heavy for our purposes both in terms of speedand complexity of user interface.Perhaps the best option we plan to investigate infuture research is a mixed approach: the interfacewould display only the text version of the sentence3http://ufal.mff.cuni.cz/pcedt2.0/4http://ufal.mff.cuni.cz/treex/5http://ufal.mff.cuni.cz/czeng/6http://ufal.mff.cuni.cz/tred/HMEANT 0.2833METEOR 0.2167WER 0.1708CDER 0.1375NIST 0.1167TER 0.1167PER 0.0208BLEU 0.0125Table 1: Kendall?s ?
for sentence-level correlation withhuman rankings.but it would internally know the (automatic) t-layerstructure.
Selecting any word that corresponds tothe t-node of a verb would automatically extend theselection to all other belongings of the t-node, i.e.all auxiliaries of the verb.
For role fillers, select-ing any word from the role filler would select thewhole t-layer subtree.
In order to handle errors inthe automatic t-layer annotation, the interface wouldcertainly need to allow manual selection and de-selection of words, providing valuable feedback tothe automatic tools.3 An Experiment in English-Czech MTEvaluationIn this first study, we selected 50 sentences from theEnglish-to-Czech WMT12 manual evaluation.
Thesentences were chosen to overlap with the standardWMT ranking procedure (see Section 3.1) as muchas possible.In total, 13 MT systems participated in this trans-lation direction.
We allocated 14 annotators (oneannotator for the SRL of the reference) so that no-body saw the same sentence translated by more sys-tems.
The hypotheses were shuffled so every annota-tor got samples from all systems as well as the refer-ence.
Unfortunately, time constraints and the largenumber of MT systems prevented us from collect-ing overlapping annotations, so we cannot evaluateinter-annotator agreement.Following Lo and Wu (2011a) and Callison-Burch et al (2012), we report Kendall?s ?
rank cor-relation coefficients for sentence-level rankings asprovided by a range of automatic metrics and ourHMEANT.
The gold standard are the manual WMTrankings.
See Table 1.32We see that HMEANT achieves a better correla-tion than all the tested automatic metrics, althoughin absolute terms, the correlation is not very high.Lo and Wu (2011b) report ?
for HMEANT of up to0.49 and Lo and Wu (2011a) observe ?
in the range0.33 to 0.43.
These figures are not comparable to ourresult for several reasons: we evaluated 13 and notjust 3 MT systems, the gold standard for us are over-all system rankings, not just adequacy judgments asfor Lo and Wu (2011b), and we evaluate translationto Czech, not English.
Callison-Burch et al (2012)report ?
for several automatic metrics on the wholeWMT12 English-to-Czech dataset, the best of whichcorrelates at ?
= 0.18.
The only common metric isMETEOR and it reaches 0.16 on the whole WMT12set.7 In line with our observation, Czech-to-Englishcorrelations reported by Callison-Burch et al (2012)are higher: the best metric achieves 0.28 and aver-ages 0.25 across four source languages.The overall low sentence-level correlation ofour HMEANT and WMT12 rankings is obviouslycaused to some extent by the problems we identi-fied, see Section 4 below.
On the other hand, it isquite possible that the WMT-style rankings taken asthe gold standard are of a disputable quality them-selves, see Section 3.1 or the detailed report on inter-annotator agreement and a long discussion on inter-preting the rankings in Callison-Burch et al (2012).Last but not least, it is likely that HMEANT andmanual ranking simply measure different propertiesof MT outputs.
The Kendall?s ?
is thus not an ulti-mate meta-evaluation metric for us.3.1 WMT-Style RankingsThis section illustrates some issues with the WMTrankings when used for system-level evaluation.
Ob-viously, at the sentence level, the rankings can be-have differently but the system-level evaluation ben-efits from a large number of manual labels.In the WMT-style rankings, humans are providedwith no more than 5 system outputs for a given sen-tence at once.
The task is to rank these 5 systemsrelatively to each other, ties allowed.Following Bojar et al (2011), we report threepossible evaluation regimes (or ?interpretations?)
of7It is possible that Callison-Burch et al (2012) use some-what different METEOR settings apart from the different subsetof the data.these 5-fold rankings to obtain system-level scores.The first step is shared: all pairwise comparisonsimplied by the 5-fold ranking are extracted.
Foreach system, we then report the percentage of caseswhere the system won the pairwise comparison.
Ourdefault interpretation is to exclude all ties from thecalculation, labelled ?Ties Ignored?, i.e.
winswins + losses .The former WMT interpretation (up to 2011) was toinclude ties in both the numerator and the denomi-nator, i.e.
wins + tieswins+ties+losses denoted ??
Others?.
WMTsummary paper also reports ?> Others?
where theties are included in the denominator only, thus giv-ing credit to systems that are different.As we see in Table 2, each of the interpretationsleads to different rankings of the systems.
More im-portantly, the underlying set of sentences also affectsthe result.
For instance, the system ONLINEA jumpsto the second position in ?Ties Ignored?
if we con-sider only the 50 sentences used in our HMEANTevaluation.
To some extent, the differences arecaused by the lower number of observations.
While?All-No Ties?
is based on 2893?134 pairwise com-parisons per system, ?50-No Ties?
is based on just186?30 observations.
Moreover, not all systemscame up among the 5 ranked systems for a givensentence.
In our 50 sentences, only 7.3?2.1 systemswere compared per sentence.
On the full set of sen-tences, this figure drops to 5.9?1.7.4 Problems of HMEANT AnnotationWe asked our annotators to take notes and reportany problems.
On the positive side, some annota-tors familiar with the WMT ranking evaluation feltthat in both phases of HMEANT, they ?knew whatthey were doing and why?.
In the ranking task, itis unfortunately quite common that the annotator isasked to rank incomparably bad hypotheses.
In suchcases, the annotator probably tries to follow somesubjective and unspoken criteria, which often leadsto a lower in inter- and intra-annotator agreement.On the negative side, we observed many problemsof the current version of HMEANT, and we proposea remedy for all of them.
We disregard minor tech-nical issues of the annotation interface and focus onthe design decisions.
The only technical limitationworth mentioning was the inability to return to pre-vious sentences.
In some cases, this even caused the33Interpretation Ties Ignored ?
Others > OthersSentences All 50 All 50 All 50cu-depfix 66.4 72.5 73.0 77.5 53.3 59.4onlineB 63.0 61.4 70.5 69.3 50.3 49.0uedin-wmt12 55.8 60.3 63.6 66.3 46.0 o 51.1cu-tamch-boj 55.6 54.6 o 64.7 62.1 44.2 45.7cu-bojar 2012 54.3 53.2 o 64.1 o 62.2 42.6 43.0CU TectoMT 53.1 o 54.9 60.5 59.8 o 44.6 o 49.0onlineA 52.9 o 61.4 o 60.8 o 66.7 o 44.0 o 53.0pctrans2010 47.7 o 54.1 55.1 o 60.1 40.9 o 47.1commercial2 46.0 51.3 54.6 59.5 38.7 42.7cu-poor-comb 44.1 41.6 o 54.7 50.5 35.7 35.2uk-dan-moses 43.5 33.2 53.4 44.2 o 35.9 27.7SFU 36.1 31.0 46.8 43.0 30.0 25.6jhu-hiero 32.2 26.7 43.2 36.0 27.0 23.3Table 2: WMT12 system-level ranking results in three different evaluation regimes evaluated either on all sentencesor just the 50 sentences that were subject to our HMEANT annotation.
The table is sorted along the first column andthe symbol ?o?
in other columns marks items out of sequence.annotators to skip parts of the annotation altogether,because they clicked Next Sentence instead of theNext Frame button.Note that the impact of the problems on the finalHMEANT reliability varies.
What causes just minorhesitations in the SRL phase can lead to completeannotation failures in the Alignment phase and viceversa.
We list the problems in decreasing severity,based on our observations as well as the number ofannotators who complained about the given issue.4.1 Vague SRL GuidelinesThe first group of problems is caused by the SRLguidelines being (deliberately) too succinct and de-veloped primarily for English.Complex predicates.
Out of the many possiblecases where predicates are described using severalwords, SRL guidelines mention just modal verbs andreserve a label for them (assuming that the main verbwill be chosen as the Action, i.e.
the predicate it-self).
This goes against the syntactic properties ofCzech and other languages, where the modal verb isthe one that conjugates and it is only complementedby the content verb in infinitive.
Some annotatorsthus decided to mark such cases as a pair of nestedframes.The problem becomes more apparent for otherclasses of verbs, such as phasic verbs (e.g.
?to be-gin?
), which naturally lead to nested frames.A specific problem for Czech mentioned by al-most all annotators, was the copula verb ?to be?.Here, the meaning-bearing element is actually theadjective that follows (e.g.
?to be glad to .
.
.
?
).HMEANT forced the annotators to use e.g.
the Ex-periencer slot for the non-verbal part of this complexpredicate.
In the negated form, ?nen??
(is not)?, someannotators even marked the copula as Negation andthe non-verbal part as the Action.No verb at all.
HMEANT does not permit to an-notate frames with no predicate.
There are howeverat least two frequent cases that deserve this option:(1) the whole sentence can be a nominal construc-tion such as the title of a section, and (2) an MTsystem may erroneously omit the verb, while the re-maining slot fillers are understandable and the wholemeaning of the sentence can be also guessed.
Givingno credit to such a sentence at all seems too strict.
Insome cases, it was possible for the annotators to finda substitute word for the Action role, e.g.
a noun thatshould have been translated as the verb.A related issue was caused by the uncertainty towhat extent the frame annotation should go.
Thereare many nouns derived from verbs that also bear va-lency.
FGD acknowledges this and valency lexiconsfor Czech do include also many of such nouns.
If the34Reference Oblec?ky mus?
?me vystr??
?hat z c?asopisu?Gloss clothes we-must cut from magazinesRoles Experiencer Modal Action LocativeMeaning We must cut the clothes (assuming paper toys) from magazinesHypothesis Mus?
?me vyr??
?znout oblec?en??
z c?asopisu?Gloss We-must cut clothes from magazinesRoles Modal Action ExperiencerFigure 2: An example of PP-attachment mismatch.
While it is (almost) obvious from the word order of the referencethat the preposition phrase ?z c?asopisu??
is a separate filler, it was marked as part of the Experiencer role in thehypothesis.
In the alignment phase, there is no way to align the single Experiencer slot of the hypothesis onto the twoslots (Experiencer, Locative) if the reference.instructions are not clear in this respect, it is quitepossible that one annotator creates frames for suchnouns and the other does not, causing a mismatch inthe Alignment phase.PP-attachment.
The problem of attaching prepo-sitional phrases to verbs or to other noun phrasesis well acknowledged in many languages includingEnglish and Czech.
See an example in Figure 2.A complete solution of the problem in the SRLphase will never be possible, because there are nat-urally ambiguous cases where each annotator canprefer a different reading.
However, the Align-ment phase should be somehow prepared for the in-evitable mismatches.Unclear role labels.
Insufficient role labels.The set of role labels of HMEANT is very simplecompared to the set of edge labels (called ?func-tors?)
in the tectogrammatical annotation.
Severalannotators mentioned that the HMEANT roleset ishard to use especially for passive constructions orverbs with a secondary object.Because the final HMEANT calculation requiresaligned fillers to match in their role labels, the agree-ment on role labels is important.
We suggest experi-menting also with a variant of HMEANT that woulddisregard the labels altogether.Other problematic cases are sentences where sev-eral role fillers appear to belong to the same type,e.g.
Locative: ?Byl pr?evezen (He was transported)| do nemocnice (to the hospital) | v za?chranne?m vr-tuln?
?ku (in a helicopter)?.
While it is semanticallyobvious that the hospital is not in the helicopter, sothis is not a PP-attachment problem, some annota-tors still mark both Locatives jointly as a single slot,causing the same slot mismatch.
It is also possiblethat the annotator has actually assigned the Locativelabel twice but the annotation interface interpretedall the words as belonging to one filler only.Coreference.
The SRL guidelines are not specificon handling of slot fillers realized as pronouns (oreven dropped pronouns).
If we consider a sentencelike ?It is the man who wins?, it is not clear whichwords should be marked as the Agent of the Action?wins?.
There are three candidates, all equally cor-rect from the purely semantic point of view: ?it?,?the man?
and ?who?.A natural choice would be to select the closestword referring to the respective object, however, inconstructions of complex verbs or in pro-drop lan-guages the object may not be explicitly stated inthe syntactically closest position.
Depending on theannotators?
decisions, this can lead to a mismatchin the number of slots in the subsequent Alignmentphase.Other problems.
Some annotators mentioned afew other problems.
One of them were paratacticconstructions: the frame-labelling procedure doesnot allow to distinguish between sentences like ?Itis windy and it rains?
vs. ?It is windy but it rains?,because neither ?and?
nor ?but?
are a slot filler.
Sim-ilarly, expressions like ?for example?
do not seem toconstitute a slot filler but still somehow refine themeaning of the sentence and should be preserved inthe translation.One annotator suggested that the importance ofthe SRL phase should be emphasized and the anno-tators should be pushed towards annotating as muchas they can, e.g.
also by highlighting all verbs inthe sentence, in order to provide enough frames andfillers to align in the second phase.35Reference Opily?
r?idic?
te?z?ce zrane?nGloss A drunken driver seriously injuredRoles Agent Extent ActionMeaning A drunken driver is seriously injured.Hypothesis Opily?
r?idic?
va?z?ne?
zranilGloss A drunken driver seriously injured (active form)Roles Agent Extent ActionMeaning A drunken driver seriously injured (someone).Figure 3: A mismatch of the meanings of the predicates.
Other roles in the frames match perfectly.The following sections describe problems of theAlignment phase.4.2 Correctness of the PredicateHMEANT alignment phase allows the annotators toeither align or not align a pair of frames.
There isno option to indicate that the match of the predicatesthemselves is somewhat incorrect.
Once the predi-cates are aligned, the user can only match individualfillers, possibly penalizing partial mismatches.Figure 3 illustrates this issue on a real examplefrom our data.
Once the annotator decides to alignthe frames, there is no way to indicate that the mean-ing was reversed by the translation.What native speakers of Czech also feel is thatthe MT output in Figure 3 is incomplete, an Ex-periencer is missing.
A similar example from thedata is the hypothesis ?Sve?dek ozna?mil policii.
(Thewitness informed/announced the police.)?
The verb?ozna?mit (inform/announce)?
in Czech requires themessage (perhaps the Experiencer in the HMEANTterminology), similarly to the English ?announce?but unlike ?inform?.
The valency theory of FGD for-mally describes the problem as a missing slot fillerand given a valency dictionary, such errors can beeven identified automatically.On the other hand, it should be noted that a mis-match in the predicate alone does not mean that thetranslation is incorrect.
An example in our data wasthe phrase ?dokud se souc?asne?
ume?n??
nedoc?kalo veV?
?dni nove?ho sta?nku?
vs. ?nez?
souc?asne?
ume?n??
veV?
?dni dostalo novy?
domov?.
Both versions mean?until contemporary art in Vienna was given a newhome?
but due to the different conjunction chosen(?dokud/nez?, till/until?
), one of the verbs has to benegated.4.3 Need for M:N Frame AlignmentThe majority of our annotators complained thatcomplex predicates such as phasal verbs or copulaconstructions as well as muddled MT output withno verb often render the frame matching impossi-ble.
If the reference and the hypothesis differ in thenumber of frames, then it is also almost certain thatthe role fillers observed in the two sentences will bedistributed differently among the frames, prohibitingfiller alignment.A viable solution would be allow merging offrames during the Alignment phase, which is equiva-lent to allowing many-to-many alignment of frames.The sets of role fillers would be simply unioned, im-proving the chance for filler alignment.4.4 Need for M:N Slot AlignmentInherent ambiguities like PP-attachment or spuri-ous differences in SRL prevent from 1-1 slot align-ment rather frequently.
A solution would be to allowmany-to-many alignments of slot fillers.4.5 Partial Adequacy vs.
Partial FluencyThe original HMEANT Alignment guidelines say tomark an aligned slot pair as Correct or Partial match.
(Mismatching slots should not be aligned at all.)
APartial match is described as:Role fillers in MT express part of themeaning of the aligned role fillers in thereference translation.
Do NOT penalizeextra meaning unless it belongs in otherrole fillers in the reference translation.The second sentence of the instructions is prob-ably aimed at cases where the MT expresses morethan the reference does, which is possible because36the translator may have removed part of the contentor because the source and the reference are both notquite literal translations from a third language.
Aclarifying example of this case in the instructions ishighly desirable.What our annotators noticed were cases where thetranslation was semantically adequate but containede.g.
an agreement mismatch or another grammar er-ror.
The instructions should exemplify, if this is tobe treated as a Correct or Partial match.
Optionally,the Partial match could be split into three separatecases: partially inadequate, partially disfluent, andpartially inadequate and disfluent.4.6 Summary of Suggested HMEANT FixesTo summarize the observations above, our experi-ence with HMEANT was overall positive, but wepropose several changes in the design to improve thereliability of the annotations:SRL Phase:?
The SRL guidelines should be kept as simple asthey are, but more examples and especially ex-amples of incorrect MT output should be pro-vided.?
The Action should be allowed to consist of sev-eral words, including non-adjacent ones.?
The possibility of using automatic t-layer anno-tation tools should be explored, at least to pre-annotate which words form a multi-word pred-icate or role filler.Alignment Phase:?
The annotator must be able to indicate a partialor incorrect match of the predicates themselves.?
Both frames as well as fillers should supportM:N alignment to overcome a range of natu-rally appearing as well as spurious mismatchesin the two SRL annotations.?
Examples of anaphoric expressions should beincluded in the guidelines, stressing that any el-ement of the anaphora chain should be treatedas an appropriate representant of the role filler.?
The Partial match could distinguish betweenan error in adequacy or fluency, or rather, theAlignment guidelines should explicitly provideexamples of both types and ask the annotatorsto disregard the difference.Technical Changes:?
The annotators need to be able to go backwithin each phase.
(The division betweenthe SRL and Alignment phases should be pre-served.
)We do not expect any of the proposed changes tonegatively impact annotation time.
Actually, somespeedup may be obtained from the suggested pre-annotation and also from a reduced hesitation of theannotators in the alignment phase thanks to the M:Nalignment possibility.5 ConclusionWe applied HMEANT, a technique for manual eval-uation of MT quality based on predicate-argumentstructure, to a new language, Czech.
The experimentconfirmed that HMEANT is applicable in this set-ting, outperforming automatic metrics in sentence-level correlation with manual rankings.During our annotation, we identified a range ofproblems in the current HMEANT design.
We thuspropose a few modifications to the technique andalso suggest backing HMEANT with a linguistictheory of deep syntax, opening the avenue to au-tomating the metric using available tools.AcknowledgmentsWe would like to thank our annotators for all the com-ments and also Chi-kiu Lo, Karteek Addanki, AnandKarthik Tumuluru, and Avishek Kumar Thakur for ad-ministering the annotation interface.
This work was sup-ported by the project EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of theCzech Republic) and the Czech Science Foundationgrants P406/11/1499 and P406/10/P259 (Ondr?ej Bo-jar); and by the Defense Advanced Research ProjectsAgency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022and HR0011-06-C-0023; by the European Union underthe FP7 grant agreement no.
287658; and by the HongKong Research Grants Council (RGC) research grantGRF621008 (Dekai Wu).
Any opinions, findings andconclusions or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflectthe views of the RGC, EU, or DARPA.37ReferencesHerve?
Blanchon, Christian Boitet, and Laurent Besacier.2004.
Spoken Dialogue Translation Systems Evalua-tion: Results, New Trends, Problems and Proposals.In Proceedings of International Conference on SpokenLanguage Processing ICSLP 2004, Jeju Island, Korea,October.Ondr?ej Bojar, Milos?
Ercegovc?evic?, Martin Popel, andOmar Zaidan.
2011.
A Grain of Salt for the WMTManual Evaluation.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 1?11,Edinburgh, Scotland, July.
Association for Computa-tional Linguistics.Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???Mars??
?k, Michal Nova?k, Martin Popel, and Ales?
Tam-chyna.
2012.
The Joy of Parallelism with CzEng 1.0.In Proceedings of the Eighth International LanguageResources and Evaluation Conference (LREC?12), Is-tanbul, Turkey, May.
ELRA, European Language Re-sources Association.
In print.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)Evaluation of Machine Translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, pages 136?158, Prague, Czech Republic, June.Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, Montreal, Canada,June.
Association for Computational Linguistics.Charles J. Fillmore.
1968.
The Case for Case.
In E. Bachand R. Harms, editors, Universals in Linguistic The-ory, pages 1?90.
New York.Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc??
?kova?, MarieMikulova?, Petr Pajas, Jan Popelka, Jir???
Semecky?,Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?kaUres?ova?, and Zdene?k Z?abokrtsky?.
2012.
Announc-ing Prague Czech-English Dependency Treebank 2.0.In Proceedings of the Eighth International LanguageResources and Evaluation Conference (LREC?12), Is-tanbul, Turkey, May.
ELRA, European Language Re-sources Association.
In print.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, MarieMikulova?, Zdene?k Z?abokrtsky?, and Magda S?evc???kova?Raz??mova?.
2006.
Prague Dependency Treebank 2.0.LDC2006T01, ISBN: 1-58563-370-4.Philipp Koehn and Christof Monz.
2006.
Manual and au-tomatic evaluation of machine translation between eu-ropean languages.
In Proceedings on the Workshop onStatistical Machine Translation, pages 102?121, NewYork City, June.
Association for Computational Lin-guistics.Chi-kiu Lo and Dekai Wu.
2011a.
Meant: An inexpen-sive, high-accuracy, semi-automatic metric for evalu-ating translation utility based on semantic roles.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 220?229, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Chi-kiu Lo and Dekai Wu.
2011b.
Structured vs. flatsemantic role representations for machine translationevaluation.
In Proceedings of the Fifth Workshop onSyntax, Semantics and Structure in Statistical Trans-lation, SSST-5, pages 10?20, Stroudsburg, PA, USA.Association for Computational Linguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Jarmila Panevova?.
1980.
Formy a funkce ve stavbe?
c?eske?ve?ty [Forms and functions in the structure of the Czechsentence].
Academia, Prague, Czech Republic.Martin Popel and Zdene?k Z?abokrtsky?.
2010.
TectoMT:Modular NLP Framework.
In Hrafn Loftsson, EirikurRo?gnvaldsson, and Sigrun Helgadottir, editors, Lec-ture Notes in Artificial Intelligence, Proceedings of the7th International Conference on Advances in Natu-ral Language Processing (IceTAL 2010), volume 6233of Lecture Notes in Computer Science, pages 293?304, Berlin / Heidelberg.
Iceland Centre for LanguageTechnology (ICLT), Springer.Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.
2012.DEPFIX: A System for Automatic Correction ofCzech MT Outputs.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, Mon-treal, Canada, June.
Association for ComputationalLinguistics.
Submitted.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence and Its Semantic andPragmatic Aspects.
Academia/Reidel PublishingCompany, Prague, Czech Republic/Dordrecht, Nether-lands.Petr Sgall.
1967.
Generativn??
popis jazyka a c?eska?
dek-linace.
Academia, Prague, Czech Republic.Jana S?indlerova?
and Ondr?ej Bojar.
2010.
Buildinga Bilingual ValLex Using Treebank Token Align-ment: First Observations.
In Proceedings of the Sev-enth International Language Resources and Evalua-tion (LREC?10), pages 304?309, Valletta, Malta, May.ELRA, European Language Resources Association.38
