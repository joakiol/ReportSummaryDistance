Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198?207,Columbus, June 2008. c?2008 Association for Computational LinguisticsMaking Grammar-Based Generation Easier to Deploy in Dialogue SystemsDavid DeVault and David Traum and Ron ArtsteinUSC Institute for Creative Technologies13274 Fiji WayMarina del Rey, CA 90292{devault,traum,artstein}@ict.usc.eduAbstractWe present a development pipeline and asso-ciated algorithms designed to make grammar-based generation easier to deploy in imple-mented dialogue systems.
Our approach real-izes a practical trade-off between the capabili-ties of a system?s generation component andthe authoring and maintenance burdens im-posed on the generation content author for adeployed system.
To evaluate our approach,we performed a human rating study with sys-tem builders who work on a common large-scale spoken dialogue system.
Our resultsdemonstrate the viability of our approach andillustrate authoring/performance trade-offs be-tween hand-authored text, our grammar-basedapproach, and a competing shallow statisticalNLG technique.1 IntroductionThis paper gives an overview of a new example-based generation technique that is designed to makegrammar-based generation easier to deploy in dia-logue systems.
Dialogue systems present severalspecific requirements for a practical generation com-ponent.
First, the generator needs to be fast enoughto support real-time interaction with a human user.Second, the generator must provide adequate cover-age for the meanings the dialogue system needs toexpress.
What counts as ?adequate?
can vary be-tween systems, since the high-level purpose of a di-alogue system can affect priorities regarding outputfluency, fidelity to the requested meaning, varietyof alternative outputs, and tolerance for generationfailures.
Third, developing the necessary resourcesfor the generation component should be relativelystraightforward in terms of time and expertise re-quired.
This is especially important since dialoguesystems are complex systems with significant devel-opment costs.
Finally, it should be relatively easyfor the dialogue manager to formulate a generationrequest in the format required by the generator.Together, these requirements can reduce the at-tractiveness of grammar-based generation whencompared to simpler template-based or canned textoutput solutions.
In terms of speed, off-the-shelf, wide-coverage grammar-based realizers suchas FUF/SURGE (Elhadad, 1991) can be too slow forreal-time interaction (Callaway, 2003).In terms of adequacy of coverage, in principle,grammar-based generation offers significant advan-tages over template-based or canned text output byproviding productive coverage and greater variety.However, realizing these advantages can require sig-nificant development costs.
Specifying the neces-sary connections between lexico-syntactic resourcesand the flat, domain-specific semantic representa-tions that are typically available in implemented sys-tems is a subtle, labor-intensive, and knowledge-intensive process for which attractive methodologiesdo not yet exist (Reiter et al, 2003).One strategy is to hand-build an application-specific grammar.
However, in our experience,this process requires a painstaking, time-consumingeffort by a developer who has detailed linguisticknowledge as well as detailed domain knowledge,and the resulting coverage is inevitably limited.Wide-coverage generators that aim for applicabil-198ity across application domains (White et al, 2007;Zhong and Stent, 2005; Langkilde-Geary, 2002;Langkilde and Knight, 1998; Elhadad, 1991) pro-vide a grammar (or language model) for free.
How-ever, it is harder to tailor output to the desired word-ing and style for a specific dialogue system, andthese generators demand a specific input format thatis otherwise foreign to an existing dialogue system.Unfortunately, in our experience, the developmentburden of implementing the translation between thesystem?s available meaning representations and thegenerator?s required input format is quite substan-tial.
Indeed, implementing the translation might re-quire as much effort as would be required to build asimple custom generator; cf.
(Callaway, 2003; Buse-mann and Horacek, 1998).
This development cost isexacerbated when a dialogue system?s native mean-ing representation scheme is under revision.In this paper, we survey a new example-based ap-proach (DeVault et al, 2008) that we have devel-oped in order to mitigate these difficulties, so thatgrammar-based generation can be deployed morewidely in implemented dialogue systems.
Our de-velopment pipeline requires a system developer tocreate a set of training examples which directlyconnect desired output texts to available applica-tion semantic forms.
This is achieved through astreamlined authoring task that does not require de-tailed linguistic knowledge.
Our approach thenprocesses these training examples to automaticallyconstruct all the resources needed for a fast, high-quality, run-time grammar-based generation compo-nent.
We evaluate this approach using a pre-existingspoken dialogue system.
Our results demonstratethe viability of the approach and illustrate author-ing/performance trade-offs between hand-authoredtext, our grammar-based approach, and a competingshallow statistical NLG technique.2 Background and MotivationThe generation approach set out in this paper hasbeen developed in the context of a research pro-gram aimed at creating interactive virtual humansfor social training purposes (Swartout et al, 2006).Virtual humans are embodied conversational agentsthat play the role of people in simulations or games.They interact with human users and other virtual hu-Figure 1: Doctor Perez.mans using spoken language and non-verbal behav-ior such as eye gaze, gesture, and facial displays.The case study we present here is the genera-tion of output utterances for a particular virtual hu-man, Doctor Perez (see Figure 1), who is designedto teach negotiation skills in a multi-modal, multi-party, non-team dialogue setting (Traum et al, 2005;Traum et al, 2008).
The human trainee who talksto the doctor plays the role of a U.S. Army captainnamed Captain Kirk.
We summarize Doctor Perez?sgeneration requirements as follows.In order to support compelling real-time conver-sation and effective training, the generator must beable to identify an utterance for Doctor Perez to usewithin approximately 200ms on modern hardware.Doctor Perez has a relatively rich internal men-tal state including beliefs, goals, plans, and emo-tions.
As Doctor Perez attempts to achieve his con-versational goals, his utterances need to take a va-riety of syntactic forms, including simple declar-ative sentences, various modal constructions relat-ing to hypothetical actions or plans, yes/no and wh-questions, and abbreviated dialogue forms such aselliptical clarification and repair requests, ground-ing, and turn-taking utterances.
Doctor Perez cur-rently uses about 200 distinct output utterances inthe course of his dialogues.Doctor Perez is designed to simulate a non-nativeEnglish speaker, so highly fluent output is not a ne-cessity; indeed, a small degree of disfluency is evendesirable in order to increase the realism of talkingto a non-native speaker.Finally, in reasoning about user utterances, dia-logue management, and generation, Doctor Perez1992666666666666666666666666664addressee captain-kirkdialogue-act264addressee captain-kirktype assign-turnactor doctor-perez375speech-act26666666666666664actor doctor-perezaddressee captain-kirkaction assertcontent2666666664type statepolarity negativetime presentattribute resourceAttributevalue medical-suppliesobject-id market3777777775377777777777777753777777777777777777777777775addressee captain-kirkdialogue-act.addressee captain-kirkdialogue-act.type assign-turndialogue-act.actor doctor-perezspeech-act.actor doctor-perezspeech-act.addressee captain-kirkspeech-act.action assertspeech-act.content.type statespeech-act.content.polarity negativespeech-act.content.time presentspeech-act.content.attribute resourceAttributespeech-act.content.value medical-suppliesspeech-act.content.object-id market(a) Attribute-value matrix (b) Corresponding frameFigure 2: An example of Doctor Perez?s representations for utterance semantics: Doctor Perez tells the captain thatthere are no medical supplies at the market.exploits an existing semantic representation schemethat has been utilized in a family of virtual humans.This scheme uses an attribute-value matrix (AVM)representation to describe an utterance as a set ofcore speech acts and other dialogue acts.
Speechacts generally have semantic contents that describepropositions and questions about states and actionsin the domain, as well as other features such as po-larity and modality.
See (Traum, 2003) for somemore details and examples of this representation.For ease of interprocess communication, and certainkinds of statistical processing, this AVM structure islinearized so that each non-recursive terminal valueis paired with a path from the root to the final at-tribute.
Thus, the AVM in Figure 2(a) is representedas the ?frame?
in Figure 2(b).Because the internal representations that make upDoctor Perez?s mental state are under constant de-velopment, the exact frames that are sent to the gen-eration component change frequently as new rea-soning capabilities are added and existing capabil-ities are reorganized.
Additionally, while only hun-dreds of frames currently arise in actual dialogues,the number of potential frames is orders of magni-tude larger, and it is difficult to predict in advancewhich frames might occur.In this setting, over a period of years, a numberof different approaches to natural language gener-ation have been implemented and tested, includinghand-authored canned text, domain specific hand-built grammar-based generators (e.g., (Traum et al,2003)), shallow statistical generation techniques,and the grammar-based approach presented in thispaper.
We now turn to the details of our approach.3 Technical ApproachOur approach builds on recently developed tech-niques in statistical parsing, lexicalized syntax mod-eling, generation with lexicalized grammars, andsearch optimization to automatically construct allthe resources needed for a high-quality run-timegeneration component.The approach involves three primary steps: spec-ification of training examples, grammar induction,and search optimization.
In this section, we presentthe format that training examples take and then sum-marize the subsequent automatic processing steps.Due to space limitations, we omit the full detailsof these automatic processing steps, and refer thereader to (DeVault et al, 2008) for additional details.3.1 Specification of Training ExamplesEach training example in our approach speci-fies a target output utterance (string), its syn-tax, and a set of links between substrings withinthe utterance and system semantic representa-tions.
Formally, a training example takes the form(u, syntax(u), semantics(u)).
We will illustratethis format using the training example in Figure 3.In this example, the generation content author200Utterance we don?t have medical supplies here captainSyntaxcat: SA?
?cat: S?
?cat: NP?
?pos: PRP?
?wecat: VP?
?pos: AUX?
?dopos: RB?
?n?tcat: VP?
?pos: AUX?
?havecat: NP?
?pos: JJ?
?medicalpos: NNS?
?suppliescat: ADVP?
?pos: RB?
?herecat: NP?
?pos: NN?
?captainSemanticswe do n?t .
.
.
.
.
.
.
.
.
{speech-act.action = assertspeech-act.content.polarity = negativehave .
.
.
.
.
.
.
.
.
.
.
.
.
speech-act.content.attribute = resourceAttributemedical supplies .
.
speech-act.content.value = medical-supplieshere .
.
.
.
.
.
.
.
.
.
.
.
.
speech-act.content.object-id = marketcaptain .
.
.
.
.
.
.
.
.
.??
?addressee = captain-kirkdialogue-act.addressee = captain-kirkspeech-act.addressee = captain-kirkFigure 3: A generation training example for Doctor Perez.suggests the output utterance u = we don?t havemedical supplies here captain.
Each utterance u isaccompanied by syntax(u), a syntactic analysis inPenn Treebank format (Marcus et al, 1994).
In thisexample, the syntax is a hand-corrected version ofthe output of the Charniak parser (Charniak, 2001;Charniak, 2005) on this sentence; we discuss thishand correction in Section 4.To represent the meaning of utterances, our ap-proach assumes that the system provides some setM = {m1, ...,mj} of semantic representations.The meaning of any individual utterance is thenidentified with some subset of M .
For Doctor Perez,M comprises the 232 distinct key-value pairs thatappear in the system?s various generation frames.
Inthis example, the utterance?s meaning is captured bythe 8 key-value pairs indicated in the figure.Our approach requires the generation contentauthor to link these 8 key-value pairs to con-tiguous surface expressions within the utterance.The technique is flexible about which surface ex-pressions are chosen (e.g.
they need not corre-spond to constituent boundaries); however, they doneed to be compatible with the way the syntacticanalysis tokenizes the utterance, as follows.
Lett(u) = ?t1, ..., tn?
be the terminals in the syn-tactic analysis, in left-to-right order.
Formally,semantics(u) = {(s1,M1), ..., (sk,Mk)}, wheret(u) = s1@ ?
?
?
@sk (with @ denoting concatena-tion), and where Mi ?
M for all i ?
1..k. In thisexample, the surface expression we don?t, which to-kenizes as ?we,do,n?t?, is connected to key-valuesthat indicate a negative polarity assertion.This training example format has two features thatare crucial to our approach.
First, the semantics ofan utterance is specified independently of its syntax.This greatly reduces the amount of linguistic exper-tise a generation content author needs to have.
Italso allows making changes to the underlying syn-tax without having to re-author the semantic links.Second, the assignment of semantic representa-tions to surface expressions must span the entire ut-terance.
No words or expressions can be viewed as?meaningless?.
This is essential because, otherwise,the semantically motivated search algorithm used ingeneration has no basis on which to include thoseparticular expressions when it constructs its outpututterance.
Many systems, including Doctor Perez,lack some of the internal representations that wouldbe necessary to specify semantics down to the lex-ical level.
An important feature of our approach isthat it allows an arbitrary semantic granularity to beemployed, by mapping the representations availablein the system to appropriate multi-word chunks.2013.2 Automatic Grammar Induction and SearchOptimizationThe first processing step is to induce a productivegrammar from the training examples.
We adopt theprobabilistic tree-adjoining grammar (PTAG) for-malism and grammar induction technique of (Chi-ang, 2003).
We induce our grammar from trainingexamples such as Figure 3 using heuristic rules toassign derivations to the examples, as in (Chiang,2003).
Once derivations have been assigned, sub-trees within the training example syntax are incre-mentally detached.
This process yields the reusablelinguistic resources in the grammar, as well as thestatistical model needed to compute operation prob-abilities when the grammar is later used in genera-tion.
Figure 5 in the Appendix illustrates this pro-cess by presenting the linguistic resources inferredfrom the training example of Figure 3.Our approach uses this induced grammar to treatgeneration as a search problem: given a desired se-mantic representation M ?
?
M , use the grammarto incrementally construct an output utterance u thatexpressesM ?.
We treat generation as anytime searchby accruing multiple goal states up until a specifiedtimeout (200ms for Doctor Perez) and returning alist of alternative outputs ranked by their derivationprobabilities.The search space created by a grammar inducedin this way is too large to be searched exhaustivelyin most applications.
The second step of automatedprocessing, then, uses the training examples to learnan effective search policy so that good output sen-tences can be found in a reasonable time frame.
Thesolution we have developed employs a beam searchstrategy that uses weighted features to rank alterna-tive grammatical expansions at each step.
Our al-gorithm for selecting features and weights is basedon the search optimization algorithm of (Daum?and Marcu, 2005), which decides to update featureweights when mistakes are made during search ontraining examples.
We use the boosting approach of(Collins and Koo, 2005) to perform feature selectionand identify good weight values.4 Empirical EvaluationIn the introduction, we identified run-time speed, ad-equacy of coverage, authoring burdens, and NLG re-quest specification as important factors in the selec-tion of a technology for a dialogue system?s NLGcomponent.
In this section, we evaluate our tech-nique along these four dimensions.Hand-authored utterances.
We collected a sam-ple of 220 instances of frames that Doctor Perez?sdialogue manager had requested of the generationcomponent in previous dialogues with users.
Someframes occurred more than once in this sample.Each frame was associated with a single hand-authored utterance.
Some of these utterances arosein human role plays for Doctor Perez; some werewritten by a script writer; others were authoredby system builders to provide coverage for specificframes.
All were reviewed by a system builder forappropriateness to the corresponding frame.Training.
We used these 220 (frame, utterance)examples to evaluate both our approach and a shal-low statistical method called sentence retriever (dis-cussed below).
We randomly split the examplesinto 198 training and 22 test examples; we used thesame train/test split for our approach and sentenceretriever.To train our approach, we constructed training ex-amples in the format specified in Section 3.1.
Syntaxposed an interesting problem, because the Charniakparser frequently produces erroneous syntactic anal-yses for utterances in Doctor Perez?s domain, but itwas not obvious how detrimental these errors wouldbe to overall generated output.
We therefore con-structed two alternative sets of training examples ?one where the syntax of each utterance was the un-corrected output of the Charniak parser, and anotherwhere the parser output was corrected by hand (thesyntax in Figure 3 above is the corrected version).Hand correction of parser output requires consider-able linguistic expertise, so uncorrected output rep-resents a substantial reduction in authoring burden.The connections between surface expressions andframe key-value pairs were identical in both uncor-rected and corrected training sets, since they are in-dependent of the syntax.
For each training set, wetrained our generator on the 198 training examples.We then generated a single (highest-ranked) utter-ance for each example in both the test and trainingsets.
The generator sometimes failed to find a suc-cessful utterance within the 200ms timeout; the suc-cess rate of our generator was 95% for training ex-202amples and 80% for test examples.
The successfulutterances were rated by our judges.Sentence retriever is based on the cross-language information retrieval techniques describedin (Leuski et al, 2006), and is currently in use forDoctor Perez?s NLG problem.
Sentence retrieverdoes not exploit any hierarchical syntactic analy-sis of utterances.
Instead, sentence retriever viewsNLG as an information retrieval task in which a setof training utterances are the ?documents?
to be re-trieved, and the frame to be expressed is the query.At run-time, the algorithm functions essentially as aclassifier: it uses a relative entropy metric to selectthe highest ranking training utterance for the framethat Doctor Perez wishes to express.
This approachhas been used because it is to some extent robustagainst changes in internal semantic representations,and against minor deficiencies in the training corpus,but as with a canned text approach, it requires eachutterance to be hand-authored before it can be usedin dialogue.
We trained sentence retriever on the 198training examples, and used it to generate a single(highest-ranked) utterance for each example in boththe test and training sets.
Sentence retriever?s suc-cess rate was 96% for training examples and 90%for test examples.
The successful utterances wererated by our judges.Figure 7 in the Appendix illustrates the alternativeutterances that were produced for a frame present inthe test data but not in the training data.Run-time speed.
Both our approach and sentenceretriever run within the available 200ms window.Adequacy of Coverage.
To assess output quality,we conducted a study in which 5 human judges gaveoverall quality ratings for various utterances DoctorPerez might use to express specific semantic frames.In total, judges rated 494 different utterances whichwere produced in several conditions: hand-authored(for the relevant frame), generated by our approach,and sentence retriever.We asked our 5 judges to rate each of the 494 ut-terances, in relation to the specific frame for whichit was produced, on a single 1 (?very bad?)
to 5(?very good?)
scale.
Since ratings need to incorpo-rate accuracy with respect to the frame, our judgeshad to be able to read the raw system semantic rep-resentations.
This meant we could only use judgeswho were deeply familiar with the dialogue system;however, the main developer of the new generationalgorithms (the first author) did not participate asa judge.
Judges were blind to the conditions un-der which utterances were produced.
The judgesrated the utterances using a custom-built applicationwhich presented a single frame together with 1 to 6candidate utterances for that frame.
The rating inter-face is shown in Figure 6 in the Appendix.
The orderof candidate utterances for each frame was random-ized, and the order in which frames appeared wasrandomized for each judge.The judges were instructed to incorporate bothfluency and accuracy with respect to the frame intoa single overall rating for each utterance.
While itis possible to have human judges rate fluency andaccuracy independently, ratings of fluency alone arenot particularly helpful in evaluating Doctor Perez?sgeneration component, since for Doctor Perez, a cer-tain degree of disfluency can contribute to believ-ability (as noted in Section 2).
We therefore askedjudges to make an overall assessment of output qual-ity for the Doctor Perez character.The judges achieved a reliability of ?
= 0.708(Krippendorff, 1980); this value shows that agree-ment is well above chance, and allows for tentativeconclusions.
Agreement between subsets of judgesranged from ?
= 0.802 for the most concordant pairof judges to ?
= 0.593 for the most discordant pair.We also performed an ANOVA comparing threeconditions (generated, retrieved and hand-authoredutterances) across the five judges; we found sig-nificant main effects of condition (F (2, 3107) =55, p < 0.001) and judge (F (4, 3107) = 17, p <0.001), but no significant interaction (F (8, 3107) =0.55, p > 0.8).
We therefore conclude that the indi-vidual differences among the judges do not affect thecomparison of utterances across the different condi-tions, so we will report the rest of the evaluation onthe mean ratings per utterance.Due to the large number of factors and the dif-ferences in the number of utterances correspond-ing to each condition, we ran a small numberof planned comparisons.
The distribution of rat-ings across utterances is not normal; to validateour results we accompanied each t-test by a non-parametric Wilcoxon rank sum test, and signifi-cance always fell in the same general range.
Wefound a significant difference between generated203Generated (N = 90)Sentence retriever (N = 100)RatingFrequency(%)0102030401 2 3 4 5Figure 4: Observed ratings of generated (uncorrectedsyntax) vs. retrieved sentences for test examples.output for all examples, retrieved output for all ex-amples, and hand-authored utterances (F (2, 622) =16, p < 0.001); however, subsequent t-tests showthat all of this difference is due to the fact that hand-authored utterances (mean rating 4.4) are better thanretrieved (t(376) = 3.7, p < 0.001) and gener-ated (t(388) = 5.9, p < 0.001) utterances, whereasthe difference between generated (mean rating 3.8)and retrieved (mean rating 4.0) is non-significant(t(385) = 1.6, p > 0.1).Figure 4 shows the observed rating frequenciesof sentence retriever (mean 3.0) and our approach(mean 3.6) on the test examples.
While this datadoes not show a significant difference, it suggeststhat retriever?s selected sentences are most fre-quently either very bad or very good; this reflectsthe fact that the classification algorithm retrieveshighly fluent hand-authored text which is sometimessemantically very incorrect.
(Figure 7 in the Ap-pendix provides such an example, in which a re-trieved sentence has the wrong polarity.)
The qual-ity of our generated output, by comparison, appearsmore graded, with very good quality the most fre-quent outcome and lower qualities less frequent.
Ina system where there is a low tolerance for verybad quality output, generated output would likely beconsidered preferable to retrieved output.In terms of generation failures, our approach hadpoorer coverage of test examples than sentence re-triever (80% vs. 90%).
Note however that in thisstudy, our approach only delivered an output if itcould completely cover the requested frame.
In thefuture, we believe coverage could be improved, withperhaps some reduction in quality, by allowing out-puts that only partially cover requested frames.In terms of output variety, in this initial study ourjudges rated only the highest ranked output gener-ated or retrieved for each frame.
However, we ob-served that our generator frequently finds several al-ternative utterances of relatively high quality (seeFigure 7); thus our approach offers another poten-tial advantage in output variety.Authoring burdens.
Both canned text and sen-tence retriever require only frames and correspond-ing output sentences as input.
In our approach, syn-tax and semantic links are additionally needed.
Wecompared the use of corrected vs. uncorrected syn-tax in training.
Surprisingly, we found no significantdifference between generated output trained on cor-rected and uncorrected syntax (t(29) = 0.056, p >0.9 on test items, t(498) = ?1.1, p > 0.2 on allitems).
This is a substantial win in terms of reducedauthoring burden for our approach.If uncorrected syntax is used, the additional bur-den of our approach lies only in specifying the se-mantic links.
For the 220 examples in this study,one system builder specified these links in about 6hours.
We present a detailed cost/benefit analysis ofthis effort in (DeVault et al, 2008).NLG request specification.
Both our approachand sentence retriever accept the dialogue manager?snative semantic representation for NLG as input.Summary.
In exchange for a slightly increasedauthoring burden, our approach yields a generationcomponent that generalizes to unseen test problemsrelatively gracefully, and does not suffer from thefrequent very bad output or the necessity to authorevery utterance that comes with canned text or acompeting statistical classification technique.5 Conclusion and Future WorkIn this paper we have presented an approach to spec-ifying domain-specific, grammar-based generationby example.
The method reduces the authoring bur-den associated with developing a grammar-basedNLG component for an existing dialogue system.We have argued that the method delivers relativelyhigh-quality, domain-specific output without requir-ing that content authors possess detailed linguisticknowledge.
In future work, we will study the perfor-204mance of our approach as the size of the training setgrows, and assess what specific weaknesses or prob-lematic disfluencies, if any, our human rating studyidentifies in output generated by our technique.
Fi-nally, we intend to evaluate the performance of ourgeneration approach within the context of the com-plete, running Doctor Perez agent.AcknowledgmentsThanks to Arno Hartholt, Susan Robinson, ThomasRuss, Chung-chieh Shan, and Matthew Stone.
Thiswork was sponsored by the U.S. Army Research,Development, and Engineering Command (RDE-COM), and the content does not necessarily reflectthe position or the policy of the Government, and noofficial endorsement should be inferred.ReferencesStephen Busemann and Helmut Horacek.
1998.
A flex-ible shallow approach to text generation.
In Proceed-ings of INLG, pages 238?247.Charles B. Callaway.
2003.
Evaluating coverage forlarge symbolic NLG grammars.
Proceedings of theInternational Joint Conferences on Artificial Intelli-gence.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In ACL ?01: Proceedings of the39th Annual Meeting on Association for Computa-tional Linguistics, pages 124?131, Morristown, NJ,USA.
Association for Computational Linguistics.Eugene Charniak.
2005.ftp://ftp.cs.brown.edu/pub/nlparser/parser05Aug16.tar.gz.David Chiang.
2003.
Statistical parsing with an auto-matically extracted tree adjoining grammar.
In RensBod, Remko Scha, and Khalil Sima?an, editors, DataOriented Parsing, pages 299?316.
CSLI Publications,Stanford.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?70.Hal Daum?, III and Daniel Marcu.
2005.
Learning assearch optimization: approximate large margin meth-ods for structured prediction.
In ICML ?05: Proceed-ings of the 22nd international conference on Machinelearning, pages 169?176, New York, NY, USA.
ACM.David DeVault, David Traum, and Ron Artstein.
2008.Practical grammar-based NLG from examples.
InFifth International Natural Language GenerationConference (INLG).Michael Elhadad.
1991.
FUF: the universal unifier usermanual version 5.0.
Technical Report CUCS-038-91.Klaus Krippendorff, 1980.
Content Analysis: An Intro-duction to Its Methodology, chapter 12, pages 129?154.
Sage, Beverly Hills, CA.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InCOLING-ACL, pages 704?710.I.
Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sen-tence generator.Anton Leuski, Ronakkumar Patel, David Traum, andBrandon Kennedy.
2006.
Building effective questionanswering characters.
In The 7th SIGdial Workshopon Discourse and Dialogue.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.E.
Reiter, S. Sripada, and R. Robertson.
2003.
Acquir-ing correct knowledge for natural language generation.Journal of Artificial Intelligence Research, 18:491?516.William Swartout, Jonathan Gratch, Randall W. Hill, Ed-uard Hovy, Stacy Marsella, Jeff Rickel, and DavidTraum.
2006.
Toward virtual humans.
AI Mag.,27(2):96?108.David Traum, Michael Fleischman, and Eduard Hovy.2003.
Nl generation for virtual humans in a complexsocial environment.
In Working Notes AAAI SpringSymposium on Natural Language Generation in Spo-ken and Written Dialogue, March.David Traum, William Swartout, Jonathan Gratch,Stacy Marsella, Patrick Kenny, Eduard Hovy, ShriNarayanan, Ed Fast, Bilyana Martinovski, RahulBaghat, Susan Robinson, Andrew Marshall, DagenWang, Sudeep Gandhe, and Anton Leuski.
2005.Dealing with doctors: A virtual human for non-teaminteraction.
In SIGdial.D.
R. Traum, W. Swartout, J Gratch, and S Marsella.2008.
A virtual human dialogue model for non-teaminteraction.
In Laila Dybkjaer and Wolfgang Minker,editors, Recent Trends in Discourse and Dialogue.Springer.David Traum.
2003.
Semantics and pragmatics of ques-tions and answers for dialogue agents.
In proceedingsof the International Workshop on Computational Se-mantics, pages 380?394, January.Michael White, Rajakrishnan Rajkumar, and Scott Mar-tin.
2007.
Towards broad coverage surface realiza-tion with CCG.
In Proc.
of the Workshop on UsingCorpora for NLG: Language Generation and MachineTranslation (UCNLG+MT).Huayan Zhong and Amanda Stent.
2005.
Buildingsurface realizers automatically from corpora usinggeneral-purpose tools.
In Proc.
Corpus Linguistics ?05Workshop on Using Corpora for Natural LanguageGeneration.205syntax:cat: SA?
?fin: other,??
cat: Scat: NP,??
apr: VBP,apn: other?
?pos: PRP?
?wefin: yes,??
cat: VPapn: other,??
pos: VBPdopos: RB?
?n?tfin: yes,??
cat: VP,gra: obj1?
?fin: yes,??
cat: VP,gra: obj1?
?pos: VBP?
?havecat: NP,??
gra: obj1operations: initial tree compsemantics: speech-act.action = assertspeech-act.content.polarity = negativespeech-act.content.attribute = resourceAttributesyntax:cat: NP,??
apr: VBP,gra: obj1,??
apn: otherpos: JJ?
?medicalpos: NNS?
?suppliescat: ADVP,??
gra: adjpos: RB?
?herecat: NP,??
apr: VBZ,gra: adj,??
apn: 3pspos: NN?
?captainoperations: comp left/right adjunction left/right adjunctionsemantics: speech-act.content.value =medical-suppliesspeech-act.content.object-id =marketaddressee = captain-kirkdialogue-act.addressee =captain-kirkspeech-act.addressee =captain-kirkFigure 5: The linguistic resources automatically inferred from the training example in Figure 3.Figure 6: Human rating interface.206Input semantic formaddressee captain-kirkdialogue-act.actor doctor-perezdialogue-act.addressee captain-kirkdialogue-act.type assign-turnspeech-act.action assertspeech-act.actor doctor-perezspeech-act.addressee captain-kirkspeech-act.content.attribute acceptableAttributespeech-act.content.object-id clinicspeech-act.content.time presentspeech-act.content.type statespeech-act.content.value yesOutputsHand-authoredthe clinic is acceptable captainGenerated (uncorrected syntax)Rank Time (ms)1 16 the clinic is up to standard captain2 94 the clinic is acceptable captain3 78 the clinic should be in acceptable condition captain4 16 the clinic downtown is currently acceptable captain5 78 the clinic should agree in an acceptable condition captainGenerated (corrected syntax)Rank Time (ms)1 47 it is necessary that the clinic be in good condition captain2 31 i think that the clinic be in good condition captain3 62 captain this wont work unless the clinic be in good conditionSentence retrieverthe clinic downtown is not in an acceptable condition captainFigure 7: The utterances generated for a single test example by different evaluation conditions.
Generated outputswhose rank (determined by derivation probability) was higher than 1 were not rated in the evaluation reported in thispaper, but are included here to suggest the potential of our approach to provide a variety of alternative outputs for thesame requested semantic form.
Note how the output of sentence retriever has the opposite meaning to that of the inputframe.207
