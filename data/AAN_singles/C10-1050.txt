Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 439?446,Beijing, August 2010Hierarchical Phrase-based Machine Translation with Word-basedReordering ModelKatsuhiko Hayashi*, Hajime Tsukada**Katsuhito Sudoh**, Kevin Duh**, Seiichi Yamamoto**Doshisha Universitykatsuhiko-h@is.naist.jp, seyamamo@mail.doshisha.ac.jp**NTT Communication Science Laboratoriestsukada, sudoh, kevinduh@cslab.kecl.ntt.co.jpAbstractHierarchical phrase-based machine trans-lation can capture global reordering withsynchronous context-free grammar, buthas little ability to evaluate the correctnessof word orderings during decoding.
Wepropose a method to integrate word-basedreordering model into hierarchical phrase-based machine translation to overcomethis weakness.
Our approach extends thesynchronous context-free grammar rulesof hierarchical phrase-based model to in-clude reordered source strings, allowingefficient calculation of reordering modelscores during decoding.
Our experimen-tal results on Japanese-to-English basictravel expression corpus showed that theBLEU scores obtained by our proposedsystem were better than those obtained bya standard hierarchical phrase-based ma-chine translation system.1 IntroductionHierarchical phrase-based machine translation(Chiang, 2007; Watanabe et al, 2006) is one ofthe promising statistical machine translation ap-proaches (Brown et al, 1993).
Its model is for-mulated by a synchronous context-free grammar(SCFG) which captures the syntactic informationbetween source and target languages.
Althoughthe model captures global reordering by SCFG,it does not explicitly introduce reordering modelto constrain word order.
In contrast, lexicalizedreordering models (Tillman, 2004; Koehn et al,2005; Nagata et al, 2006) are extensively usedfor phrase-based translation.
These lexicalized re-ordering models cannot be directly applied to hi-erarchical phrased-based translation since the hi-erarchical phrase representation uses nonterminalsymbols.To handle global reordering in phrase-basedtranslation, various preprocessing approacheshave been proposed, where the source sentenceis reordered to target language order beforehand(Xia and McCord, 2004; Collins et al, 2005; Li etal., 2007; Tromble and Eisner, 2009).
However,preprocessing approaches cannot utilize other in-formation in the translation model and target lan-guage model, which has been proven helpful indecoding.This paper proposes a method that incorpo-rates word-based reordering model into hierarchi-cal phrase-based translation to constrain word or-der.
In this paper, we adopt the reordering modeloriginally proposed by Tromble and Eisner (2009)for the preprocessing approach in phrase-basedtranslation.
To integrate the word-based reorder-ing model, we added a reordered source stringinto the right-hand-side of SCFG?s rules.
By thisextension, our system can generate the reorderedsource sentence as well as target sentence and isable to efficiently calculate the score of the re-ordering model.
Our method utilizes the transla-tion model and target language model as well asthe reordering model during decoding.
This is anadvantage of our method over the preprocessingapproach.The remainder of this paper is organized asfollows.
Section 2 describes the concept of ourapproach.
Section 3 briefly reviews our pro-posed method on hierarchical phrase-based ma-439Standard SCFG X ?< X1 wa jinsei no X2 da , X1 is X2 of life>SCFG (move-to-front) X ?< X1 wa jinsei no X2 da , wa X1 da X2 no jinsei , X1 is X2 of life>SCFG (attach) X ?< X1 wa jinsei no X2 da , X1 wa da X2 no jinsei , X1 is X2 of life>Table 1: A Japanese-to-English example of various SCFG?s rule representations.
Japanese words areromanized.
Our proposed representation of rules has reordered source string to generate reorderedsource sentence S?
as well as target sentence T .
The ?move-to-front?
means Tromble and Eisner (2009)?s algorithm and the ?attach?
means Al-Onaizan and Papineni (2006) ?s algorithm.chine translation model.
We experimentally com-pare our proposed system to a standard hierarchi-cal phrase-based system on Japanese-to-Englishtranslation task in Section 4.
Then we discuss onrelated work in Section 5 and conclude this paperin Section 6.2 The Concept of Our ApproachThe preprocessing approach (Xia and McCord,2004; Collins et al, 2005; Li et al, 2007; Trombleand Eisner, 2009) splits translation procedure intotwo stages:S ?
S?
?
T (1)where S is a source sentence, S?
is a reorderedsource sentence with respect to the word order oftarget sentence T .
Preprocessing approach has thevery deterministic and hard decision in reorder-ing.
To overcome the problem, Li et al (2007)proposed k-best appoach.
However, even with ak-best approach, it is difficult to generate good hy-potheses S?
by using only a reordering model.In this paper, we directly integrated the reorder-ing model into the decoder in order to use thereordering model together with other informationin the hierarchical phrase-based translation modeland target language model.
Our approach is ex-pressed as the following equation.S ?
(S?
, T ).
(2)Our proposed method generates the reorderedsource sentence S?
by SCFG and evaluates thecorrectness of the reorderings using a word-basedreordering model of S?
which will be introducedin section 3.4.Figure 1: A derivation tree for Japanse-to-Englishtranslation.3 Hierarchical Phrase-based ModelExtension3.1 Hierarchical Phrase-based ModelHierarchical phrase-based model (Chiang, 2007)induces rules of the formX ?< ?, ?,?, w > (3)where X is a non-terminal symbol, ?
is a se-quence string of non-terminals and source termi-nals, ?
is a sequence string of non-terminals andtarget terminals.
?
is a one-to-one correspon-dence for the non-terminals appeared in ?
and ?.Given a source sentence S, the translation taskunder this model can be expressed asT?
= T(argmaxD:S(D)=Sw(D))(4)where D is a derivation and w(D) is a score ofthe derivation.
Decoder seeks a target sentence440Figure 2: Reordered source sentence generated byour proposed system.T (D) which has the highest score w(D).
S(D)is a source sentence under a derivation D. Fig-ure 1 shows the example of Japanese-to-Englishtranslation by hierarchical phrase-based machinetranslation model.3.2 Rule ExtensionTo generate reordered source sentence S?
as wellas target sentence T , we extend hierarchicalphrase rule expressed in Equation 3 toX ?< ?, ??
, ?,?, w > (5)where ??
is a sequence string of non-terminals andsource terminals, which is reordered ?
with re-spect to the word order of target string ?.
Thereason why we add ??
to rules is to efficiently cal-culate the reordering model scores.
If each ruledoes not have ??
, the decoder need to keep wordalignments because we cannot know word orderof S?
without them.
The calculation of reorder-ing model scores using word alignments is verywasteful when decoding.The translation task under our model extendsEquation 4 to the following equation:T?
= (S??
, T? )
= (S?
, T )(argmaxD:S(D)=Sw(D)).
(6)Our system generates the reordered source sen-tence S?
as well as target sentence T .
Figure 2shows the generated reordered source sentence S?Uni-gram Featuressr, s-posrsrs-posrsl, s-poslsls-poslBi-gram Featuressr, s-posr, sl, s-posls-posr, sl, s-poslsr, sl, s-poslsr, s-posr, s-poslsr, s-posr, slsr, sls-posr, s-poslTable 2: Features used by Word-based ReorderingModel.
pos means part-of-speech tag.when translating the example of Figure 1.
Notethat the structure of S?
is the same as that of targetsentence T .
The decoder generates both Figure 2and the right hand side of Figure 1, allowing us toscore both global and local word reorderings.To add ??
to rules, we permuted ?
into ??
afterrule extraction based on Grow-diag-final (Koehnet al, 2005) alignment by GIZA++ (Och and Ney,2003).
To do this permutation on rules, we ap-plied two methods.
One is the same algorithmas Tromble and Eisner (2009), which reordersaligned source terminals and nonterminals in thesame order as that of target side and moves un-aligned source terminals to the front of alignedterminals or nonterminals (move-to-front).
Theother is the same algorithm as AI-Onaizan andPapineni (2006), which differs from Tromble andEisner?s approach in attaching unaligned sourceterminals to the closest prealigned source termi-nals or nonterminals (attach).
This extension ofadding ??
does not increase the number of rules.Table 1 shows a Japanese-to-English exampleof the representation of rules for our proposed sys-tem.
Japanese words are romanized.
Suppose thatsource-side string is (X1 wa jinsei no X2 da) andtarget-side string is (X1 is X2 of life) and theirword alignments are a=((jinsei , life) , (no , of), (da , is)).
Source-side aligned words and non-terminal symbols are sorted into the same order oftarget string.
Source-side unaligned word (wa) ismoved to the front or right of the prealigned sym-bol (X1).441Surrounding Word Pos Featuress-posr, s-posr + 1, s-posl ?
1, s-posls-posr ?
1, s-posr, s-posl ?
1, s-posls-posr, s-posr + 1, s-posl, s-posl + 1s-posr ?
1, s-posr, s-posl, s-posl + 1Table 3: The Example of Context Features3.3 Word-based Reordering ModelWe utilize the following score(S?)
as a feature forthe word-based reordering model.
This is incor-polated into the log-linear model (Och and Ney,2002) of statistical machine translation.score(S?)
=?i,j:1?i<j?nB[s?i, s?j ] (7)B[s?l, s?r] = ?
?
?
(s?l, s?r) (8)where n is the length of reordered source sen-tence S?
(= (s?1 .
.
.
s?n)), ?
is a weight vector and?
is a vector of features.
This reordering model,which is originally proposed by Tromble and Eis-ner (2009), can assign a score to any possible per-mutation of source sentences.
Intuitively B[s?l, s?r]represents the score of ordering s?l before s?r; thehigher the value, the more we prefer word s?l oc-curs before s?r.
Whether S?l should occur before S?rdepends on how often this reordering occurs whenwe reorder the source to target sentence order.To train B, we used binary feature functions?
as used in (Tromble and Eisner, 2009), whichwere introduced for dependency parsing by Mc-Donald et al (2005).
Table 2 shows the kindof features we used in our experiments.
We didnot use context features like surrounding word posfeatures in Table 3 because they were not useful inour preliminary experiments and propose an effi-cient implementation described in the next sectionin order to calculate this reordering model whendecoding.
To train the parameter ?, we used theperceptron algorithm following Tromble and Eis-ner (2009).3.4 Integration to Cube PruningCKY parsing and cube-pruning are used for de-coding of hierarchical phrase-based model (Chi-ang, 2007).
Figure 3 displays that hierarchicalphrase-based decoder seeks new span [1,7] itemsFigure 3: Creating new items from subitems andrules, that have a span [1,7] in source sentence.with rules, utilizing subspan [1,3] items and sub-span [4,7] items.
In this example, we use 2-gramlanguage model and +LM decoding.
uni(?)
means1-gram language model cost for heuristics and in-teraction usually means language model cost thatcannot be calculated offline.
Here, we introduceour two implementations to calculate word-basedreordering model scores in this decoding algo-rithm.First, we explain a naive implementation shownin the left side of Figure 4.
This algorithm per-forms the same calculation of reordering model asthat of language model.
Each item keeps a part ofreordered source sentence.
The reordering scoreof new item can be calculated as interaction costwhen combining subitems with the rule.The right side of Figure 4 shows our pro-posed implementation.
This implementation canbe adopted to decoding only when we do not usecontext features like surrounding word pos fea-tures in Table 3 (and consider a distance betweenwords in features).
If a span is given, the reorder-ing scores of new item can be calculated for eachrule, being independent from the word order ofreordered source segment of a subitem.
So, thereordering model scores can be calculated for allrules with spans by using a part of the input sourcesentence before sorting them for cube pruning.We expect this sorting of rules with reordering442Figure 4: The ?naive?
and ?proposed?
implementation to calculate the reordering cost of new items.model scores will have good influence on cubepruning.
The right hand side of Figure 4 showsthe diffrence between naive and proposed imple-mentation (S?
is not shown to allow for a clear pre-sentation).
Note the difference is in where/whenthe reordering scores are inserted: together withthe N -gram scores in the case of naive implemen-tation; incorpolated into sorted rules for the pro-posed implementation.4 Experiment4.1 PurposeTo reveal the effectiveness of integrating the re-ordering model into decoder, we compared thefollowing setups:?
baseline: a standard hierarchical phrase-based machine translation (Hiero) system.?
preprocessing: applied Tromble and Eisner?sapproach, then translate by Hiero system.?
Hiero system + reordering model: integratedreordering model into Hiero system.We used the Joshua Decoder (Li and Khudanpur,2008) as the baseline Hiero system.
This decoderuses a log-linear model with seven features, whichconsist of N -gram language model PLM (T ), lex-ical translation model Pw(?|?
), Pw(?|?
), ruletranslation model P (?|?
), P (?|?
), word penaltyand arity penalty.The ?Hiero + Reordering model?
system hasword-based reordering model as an additional fea-ture to baseline features.
For this approach, weuse two systems.
One has ?move-to-front?
sys-tem and the other is ?attach?
system explained inSection 3.2.
We implemented our proposed algo-rithm in Section 3.4 to both ?Hiero + Reorderingmodel?
systems.
As for beam width, we use thesame setups for each system.4.2 Data SetData Sent.
Word.
Avg.
lengTraining ja 200.8K 2.4M 12.0en 200.8K 2.3M 11.5Development ja 1.0K 10.3K 10.3en 1.0K 9.8K 9.8Test ja 1.0K 14.2K 14.2en 1.0K 13.5K 13.5Table 4: The Data statisticsFor experiments we used a Japanese-Englishbasic travel expression corpus (BTEC).
Japaneseword order is linguistically very different fromEnglish and we think Japanese-English pair isa very good test bed for evaluating reorderingmodel.443XXXXXXXXXXXSystemMetrics BLEU PERBaseline (Hiero) 28.09 39.68Preprocessing 17.32 45.27Hiero + move-to-front 28.85 39.89Hiero + attach 29.25 39.43Table 5: BLEU and PER scores on the test set.Our training corpus contains about 200.8k sen-tences.
Using the training corpus, we extractedhierarchical phrase rules and trained 4-gram lan-guage model and word-based reordering model.Parameters were tuned over 1.0k sentences (devel-opment data) with single reference by minimumerror rate training (MERT) (Och, 2003).
Test dataconsisted of 1.0k sentences with single reference.Table 4 shows the condition of corpus in detail.4.3 ResultsTable 5 shows the BLEU (Papineni et al, 2001)and PER (Niesen et al, 2000) scores obtained byeach system.
The results clearly indicated thatour proposed system with word-based reorder-ing model (move-to-front or attach) outperformedbaseline system on BLEU scores.
In contrast,there is no significant improvement from baselineon PER.
This suggests that the improvement ofBLEU mainly comes from reordering.
In our ex-periment, preprocessing approach resulted in verypoor scores.4.4 DiscussionTable 6 displays examples showing the cause ofthe improvements of our system with reorderingmodel (attach) comparing to baseline system.
Wecan see that the outputs of our system are morefluent than those of baseline system because of re-ordering model.As a further analysis, we calculated the BLEUscores of Japanese S?
predicted from reorder-ing model against true Japanese S?
made fromGIZA++ alignments, were only 26.2 points on de-velopment data.
We think the poorness mainlycomes from unaligned words since they are un-tractable for the word-based reordering model.Actually, Japanese sentences in our training datainclude 34.7% unaligned words.
In spite of thepoorness, our proposed method effectively utilizethis reordering model in contrast to preprocessingapproach.5 Related WorkOur approach is similar to preprocessing approach(Xia and McCord, 2004; Collins et al, 2005; Liet al, 2007; Tromble and Eisner, 2009) in that itreorders source sentence in target order.
The dif-ference is this sentence reordering is done in de-coding rather than in preprocessing.A lot of studies on lexicalized reordering (Till-man, 2004; Koehn et al, 2005; Nagata et al,2006) focus on the phrase-based model.
Theseworks cannnot be directly applied to hierarchi-cal phrase-based model because of the differencebetween normal phrases and hierarchical phrasesthat includes nonterminal symbols.Shen et al (2008,2009) proposed a way to inte-grate dependency structure into target and sourceside string on hierarchical phrase rules.
This ap-proach is similar to our approach in extending theformalism of rules on hierarchical phrase-basedmodel in order to consider the constraint of wordorder.
But, our approach differs from (Shen et al,2008; Shen et al, 2009) in that syntax annotationis not necessary.6 Conclusion and Future WorkWe proposed a method to integrate word-basedreordering model into hierarchical phrase-basedmachine translation system.
We add ??
into thehiero rules, but this does not increase the num-ber of rules.
So, this extension itself does not af-fect the search space of decoding.
In this paperwe used Tromble and Eisner?s reordering modelfor our method, but various reordering model canbe incorporated to our method, for example S?N -gram language model.
Our experimental re-sults on Japanese-to-English task showed that oursystem outperformed baseline system and prepro-cessing approach.In this paper we utilize ??
only for reorder-ing model.
However, it is possible to use ??
forother modeling, for example we can use it forrule translation probabilities P (??
|?
), P (?|??)
foradditional feature functions.
Of course, we can444S america de seihin no hanbai wo hajimeru keikaku ga ari masu ka .
kono tegami wa koukuubin de nihon made ikura kakari masu ka .TB sales of product in america are you planning to start ?
this letter by airmail to japan .
how much is it ?TP are you planning to start products in the u.s. ?
how much does it cost to this letter by airmail to japan ?R do you plan to begin selling your products in the u.s. ?
how much will it cost to send this letter by air mail to japan ?Table 6: Examples of outputs for input sentence S from baseline system TB and our proposed sys-tem (attach) TP .
R is a reference.
The underlined portions have equivalent meanings and show thereordering differences.also utilize reordered target sentence T ?
for vari-ous modeling as well.
Addtionally we plan to useS?
for MERT because we hypothesize the fluentS?
leads to fluent T .ReferencesAI-Onaizan, Y. and K. Papineni.
2006.
Distortionmodels for statistical machine translation.
In Proc.the 44th ACL, pages 529?536.Brown, P. F., S. A. D. Pietra, V. D. J. Pietra, and R. L.Mercer.
1993.
The mathematics of statistical ma-chine translation: Parameter estimation.
Computa-tional Linguitics, 19:263?312.Chiang, D., K. Knight, and W. Wang.
2009.
11,001new features for statistical machine translation.
InProc.
NAACL, pages 216?226.Chiang, D. 2007.
Hierachical phrase-based transla-tion.
Computational Linguitics, 33:201?228.Collins, M., P. Koehn, and I. Kucerova.
2005.
Clauserestructuring for statistical machine translation.
InProc.
the 43th ACL, pages 531?540.Collins, M. 2002.
Discriminative training methods forhidden markov models.
In Proc.
of EMNLP.Freund, Y. and R. E. Schapire.
1996.
Experimentswith a new boosting algorithm.
In Proc.
of the 13thICML, pages 148?156.Koehn, P., A. Axelrod, A-B.
Mayne, C. Callison-Burch, M. Osborne, and D. Talbot.
2005.
Ed-inburgh system description for 2005 iwslt speechtranslation evaluation.
In Proc.
the 2nd IWSLT.Li, Z. and S. Khudanpur.
2008.
A scalable decoderfor parsing-based machine translation with equiv-alent language model state maintenance.
In Proc.ACL SSST.Li, C-H., D. Zhang, M. Li, M. Zhou, K. Li, andY.
Guan.
2007.
A probabilistic approach to syntax-based reordering for statistical machine translation.In Proc.
the 45th ACL, pages 720?727.McDonald, R., K. Crammer, and F. Pereira.
2005.Spanning tree methods for discriminative training ofdependency parsers.
In Thechnical Report MS-CIS-05-11, UPenn CIS.Nagata, M., K. Saito, K. Yamamoto, and K. Ohashi.2006.
A clustered global phrase reordering modelfor statistical machine translation.
In COLING-ACL, pages 713?720.Niesen, S., F.J. Och, G. Leusch, and H. Ney.
2000.An evaluation tool for machine translation: Fastevaluation for mt research.
In Proc.
the 2nd In-ternational Conference on Language Resources andEvaluation.Och, F. J. and H. Ney.
2002.
Discriminative train-ing and maximum entropy models for statistical ma-chine translation.
In Proc.
the 40th ACL, pages 295?302.Och, F. and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29:19?51.Och, F. J.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
the 41th ACL,pages 160?167.Papineni, K. A., S. Roukos, T. Ward, and W-J.
Zhu.2001.
Bleu: a method for automatic evaluation ofmachine translation.
In Proc.
the 39th ACL, pages311?318.Shen, L., J. Xu, and R. Weischedel.
2008.
A newstring-to-dependency machine translation algorithmwith a target dependency language model.
In Proc.ACL, pages 577?585.Shen, L., J. Xu, B. Zhang, S. Matsoukas, andR.
Weischedel.
2009.
Effective use of linguistic andcontextual information for statistical machine trans-lation.
In Proc.
EMNLP, pages 72?80.Tillman, C. 2004.
A unigram orientation modelfor statistical machine translation.
In Proc.
HLT-NAACL, pages 101?104.Tromble, R. and J. Eisner.
2009.
Learning linearordering problems for better translation.
In Proc.EMNLP, pages 1007?1016.445Watanabe, T., H. Tsukada, and H. Isozaki.
2006.
Left-to-right target generation for hierarchical phrase-based translation.
In Proc.
COLING-ACL, pages777?784.Xia, F. and M. McCord.
2004.
Improving a statis-tical mt system with automatically learned rewritepatterns.
In Proc.
the 18th ICON, pages 508?514.446
