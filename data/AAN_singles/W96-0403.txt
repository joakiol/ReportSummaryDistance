Paraphrasing and Aggregating Argumentative TextUsing Text StructureXiaorong Huang Armin FiedlerFachbereich Informatik, Universit/it des SaarlandesPostfach 15 11 50, D-66041 Saarbriicken, Germany{huang I a f ied le r}Ocs ,  un i - sb ,  deAbst rac tWe argue in this paper that sophisticated mi-croplanning techniques are required even formathematical proofs, in contrast to the beliefthat mathematical texts are only schematicand mechanical.
We demonstrate why para-phrasing and aggregation significantly en-hance the flexibility and the coherence ofthe text produced.
To this end, we adoptedthe Text Structure of Meteer as our basicrepresentation.
The type checking mecha-nism of Text Structure allows us to achieveparaphrasing by building comparable combi-nations of linguistic resources.
Specified interms of concepts in an uniform ontologicalstructure called the Upper Model, our se-mantic aggregation rules are more compactthan similar rules reported in the literature.1 In t roduct ionMany of the first NLG systems link theirinformation structure to the correspondinglinguistic resources either through predefinedtemplates or via careful engineering for a spe-cific application.
Therefore their expressivepower is restricted (see \[12\] for an extensivediscussion).
An increasing interest in moresophisticated microplanning techniques canbe clearly observed \[12, 14\], however.
Inthis paper, we first motivate the needs forparaphrasing and aggregation for the gener-ation of argumentative texts, in particular ofmathematical proofs, and then describe howour microplanning operations can be formu-lated in terms of Meteer's Text Structure.The work reported here is part of afully implemented system called PRO VERB,which produces natural language proofs fromproofs found by' automated reasoning sys-tems \[7\].
First experiments with PRO VERBresulted in very mechanical texts due to thelack of microplanning techniques.
Accordingto our analysis, there are at least two lin-guistic phenomena that call for appropriatemicroplanning techniques.First, naturally occurring proofs containparaphrases with respect o both rhetoricalrelations, as well as to logical functions orpredicates.
For instance, the derivation of Bfrom A can be verbalized as:"Since A, B."
or as"A leads to B.
"The logic predicate para(C1, C2), also,can be verbalized as:"Line C1 parallels line C2."
or as"The parallelism of the lines C1 andC2.
"Second, without microplanningPROVERB generates text structured ex-actly mirroring the information structure ofthe proof and the formulae.
This means thatevery step of derivation is translated into aseparate sentence, and formulae are recur-sively verbalized.
As an instance of the lat-ter, the formulaSet(F) A Subset(F, G) (1)is verbalized as21"F is a set.
F is a subset of G."although the following is much more natural:"The set F is a subset of G."Therefore, we came to the conclusion thatan intermediate l vel of representation is nec-essary that allows flexible combinations oflinguistic resources.
It is worth pointing outthat these techniques are required althoughthe input information chunks are of clausesize.
Another requirement is that this in-termediate representation is easy to control,since a mathematical text must conform tothe syntactic rules of its sublanguage.
In thenext section, we first give a brief overview ofPROVERB.
Then we describe the architec-ture of our microplanner, and illustrate howMeteer's Text Structure can be adopted asour central representation.
In Sec.
5 and 6we describe the handling of paraphrases andaggregation rules, two of the major tasks ofour microplanner.2 The Macroplanner ofP R 0 VERBThe macroplanner of PROVERB combineshierarchical planning \[13\] with local organi-zation \[15\] in a uniform planning framework\[6\].
The hierarchical planning is realizedby so-called top-down presentation operatorsthat split the task of presenting a particularproof into subtasks of presenting subproofs.While the overall planning mechanism is sim-ilar to the RST-based planning approach,the plan operators resemble the schemata inschema-based planning.
The output of themacroplanner is an ordered sequence of proofcommunicative acts (PCAs).PCAs are the primitive actions plannedduring macroplanning to achieve commu-nicative goals.
Like speech acts, PCAs canbe defined in terms of the communicativegoals they fulfill as well as in terms of theirpossible verbalizations.
Based on an analysisof proofs in mathematical textbooks, thereare mainly two types of goals:Conveying derivation step: In terms ofrhetorical relations, PCAs in this categoryrepresent a variation of the rhetorical rela-tion derive \[8\].
Below we examine the simplePCA called Der ive as an example.
(Der ive  Reasons:  (a 6F ,  F C G)Method : de f - subsetConclusion: a 6G)Depending on the reference choices, thefollowing is a possible verbalization:"Since a is an element of F and F is asubset of G, a is an element of G by thedefinition of subset.
"Updating the global attentional structure:These PCAs either convey a partial plan forthe forthcoming discourse or signal the endof a subproof.
PCAs of this sort are alsocalled meta-comments \[16\].The PCA(Beg in -Cases  Goal : FormulaAssumptions: (A B))produces the verbalization:"To prove Formula, let us consider thetwo cases by assuming A and B.
"3 Text Structure inP R 0 VERB3.1 In t roduct ion  and  Genera lS t ructureText Structure is first proposed by Meteer\[11, 12\] in order to bridge the generation gapbetween the representation i  the applicationprogram and the linguistic resources pro-vided by the language.
By abstracting overconcrete linguistic resources, Text Structureshould supply the planner with basic vocab-ularies, with which it chooses linguistic re-sources.
Meteer's text structure is organizedas a tree, in which each node represents aconstituent of the text.
In this form it con-tains three types of linguistic information:constituency, structural relations among con-stituents, and in particular, the semantic at-egories the constituents express.The main role of the semantic categoriesis to provide vocabularies which specify type22restrictions for nodes.
They define how sep-arate Text Structures can be combined, andensure that the planner only builds express-ible Text Structures.
For instance if tree Ashould be expanded at node n by tree B, theresulting type of B must be compatible tothe type restriction attached to n. Panaget\[14\] argues, however, that Meteer's emanticcategories mix the ideational and the textualdimension as argued in the systemic linguis-tic theory \[5\].
Here is one of his examples:"The ship sank" is an ideational event,and it is textually presented from an EVENT-PERSPECTIVE.
"The sinking of the ship" isstill an ideational event, but now presentedfrom an OBJECT-PERSPECTIVE.On account of this, Panaget split the typerestrictions into two orthogonal dimensions:the ideational dimension in terms of the Up-per Model \[1\], and the hierarchy of textualsemantic categories based on an analysis ofFrench and of English.
In our work, we ba-sically follow the approach of Panaget.Technically speaking, the Text Structurein PROVERB is a tree recursively composedof kernel subtrees or composite subtrees:An atomic kernel subtree has a head at theroot and arguments as children, representingbasically a predicate/argument structure.Composite subtrees can be divided into twosubtypes: the first has a special matrix childand zero or more adjunct children and rep-resents linguistic hypotaxis, the second hastwo or more coordinated children and standsfor parataxis.3.2 Type  Rest r i c t ionsEach node is typed both in terms of theUpper Model and the hierarchy of textualsemantic categories.
The Upper Model isa domain-independent property inheritancenetwork of concepts that are hierarchicallyorganized according to how they can be lin-guistically expressed.
Figure 1 shows a frag-ment of the Upper Model in PRO VERB.
Forevery domain of application, domain-specificconcepts must be identified and placed as anextension of the Upper Model.The hierarchy of textual semantic cate-gories is also a domain-independent propertyinheritance network.
The concepts axe or-ganized in a hierarchy based on their tex-tual realization.
For example, the conceptclause-modifier-rankingl t is realized as an ad-verb, clause-modifier-rankingll as a preposi-tional phrase, and clause-modifier-embeddedas an adverbial clause.
Fig.
2 shows a frag-ment of the hierarchy of textual semanticcategories.3.3 Mapp ing  APOs  to  UMOsThe mapping from the content o the linguis-tic resources now happens in a two-stagedway.
While Meteer associates the applica-tion program objects (APOs) directly withso-called resources trees, we map APOs intoUpper Model objects, which in turn are ex-panded to the Text Structures.
It is worthnoting that there is a practical advantage ofthis two-staged process.
Instead of having toconstruct resource trees for APOs, the userof our system only needs to define a map-ping from the APOs to Upper Model objects(UMOs).When mapping APOs to UMOs, the mi-croplanner must choose among available al-ternatives.
For example, the applicationprogram object para that stands for the log-ical predicate denoting the parallelism rela-tion between lines may map in five differentUpper Model concepts.
In the 0-place case,para can be mapped into object leading tothe noun "parallelism," or quality, leadingto the adjective "parallel."
In the binarycase, the choices are property-ascription thatmay be verbalized as "x and y are parallel,"quality-relation that allows the verbalizationas "x is parallel to y", or process-relation,that is the formula "x II Y.
"The mapping of Upper Model objects intothe Text Structure is defined by so-calledresource trees, i.e.
reified instances of textstructure subtrees.
The resource trees of anUpper Model concept are assembled in itsrealization class.~Concepts of the hierarchy of textual semanticcategories are noted in sans-serif text.23concept" mod i f ied -conceptf -  consc ious -be ing- ob jec t  ---q .
.t -  non-concious-Uung\ [ -  re la t iona l -p rocesses -- p rocesst.
menta l -p rocesses._~- moda l -qua l io 'qua l i ty  t.  mater ia l -word-qua l i tyr "  log ica l- a rb i t ra ty -p lace- re la t ion -~, -  sequence- genera l i zed-possess ion- quant i f i ca t ions  .
,  .d i screte -p lace- re la t ion  ~ .
.
r "  taent t ty  r "  p roper ty -ascr ip t iontn tenstve  - - - '1  .
.t -  ascript ion -1_ c i rcumstant ia l  c lass -ascr ip t ionFigure 1: A Fragment of Upper Model in PROVERB- text- sentence- c lausecategory  - vp-npmodi f ie r -c lause-modif iervp-modi f iernp-modi f ierintensif ier_ c lause-modi f ier - rank ing l  c lause-modi f ier - rank ing l lc lause-mod i f ie r -embeddedFigure 2: A Fragment of the Hierarchy of Textual Semantic Categories in PROVERB4 Architecture and ControlThe main tasks of our microplanner in-clude aggregation to remove redundancies,insertion of cue words to increase coher-ence, and reference choices, as well as lexicalchoices.
Apart from that, the microplanneralso handles entence scoping and layout.
Anoverview of the microplanner's architectureis provided in Figure 3.Our microplanner takes as input an or-dered sequence of PCAs, structured in anattentional hierarchy.
The first module,the derivation reference choice component(DRCC), suggests which parts of a PCA areto be verbalized.
This is done based on thehierarchical discourse structure as well as onthe textual distance.
PCAs annotated withthese decisions annotated are called preverbalmessages (PMs).Starting from a list of PMs as the initialText Structure, the microplanner progres-sively maps application program concepts inPMs into text structure objects of some tex-tual semantic type by referring to UpperModel objects as an intermediate level.
TheText Structure evolves by the expansion ofleaves top-down and left to right.
This pro-cess is controlled by the main module of ourmicroplanner, the Text Structure Generator(TSG), which carries out the following algo-rithm:?
When the current node is an APO withmore than one son, apply ordering and ag-gregation, in order to produce more con-cise and more coherent text.
The appli-cation of an aggregating rule before theexpansion of a leaf node may trigger theinsertion of cue words.?
An APO is mapped into an UMO, whichis in turn expanded into a Text Structureby choosing an appropriate resource tree.24Natural Deduction ProofMacroplanner.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R) k: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.gregation rules.5 Paraphrasing in PROVERBWith the help of a concrete example weM~c'r~p\]~e\[ .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
illustrate in this section how the Text Struc-Text StructureExpansionSentence StripingLexical ChoiceOrderingAggregationCue WordInsertionLayoutF~ \[ Realizatioa Closes\] Textual SemanticI CategoriesText StructureTransformer )6( )Figure 3: Architecture of the Microplanner?
A fully expanded Text Structure will betraversed again:- -  to choose the appropriate lexical items.- -  to make sentence scoping decisions bysingling out one candidate textual se-mantic category for each constituent.This in turn may trigger the executionof a cue word rule.
For instance, thechoice of the category sentence for aconstituent may lead to the insertion ofthe cue word "furthermore" in the nextsentence.- - to  determine the layout parameters,which will be realized later as ~TEX-commands in the final output text.A Text Structure constructed in this wayis the output of our microplanner, and willbe transformed into the input formalism ofTAG-GEN \[10\], our linguistic realizer.In the next two sections, we concentrateon two major tasks of the Text Structuregenerator: to choose compatible paraphrasesof application program concepts, and to im-prove the textual structure by applying ag-ture generator chooses among paraphrasesand avoids building inexpressible text struc-tures via type checking.Example  We examine a simple logic for-mula derive(para(C1,C2),B).
Note that Bstands for a conclusion which will not be ex-amined here.
We will also not follow the pro-cedure in detail.In the current implementation, the rhetor-ical relation derive is only connected to oneUpper Model concept derive, a subconceptof cause.relation.
The realization class as-sociated to the concept, however, containsseveral alternative resource trees leading todifferent patterns of verbalization.
We onlylist two variations below:?
B, since A.?
Because of A, B.The resource tree of the first alternative isgiven in Fig.
4.The logic predicate para(C1,  C2) can bemapped to one of the following Upper Modelconcepts, where we always include one pos-sible verbalization:?
quality-relation(para, C1, C2)(line C1 is parallel to C2)?
process-relation(para, C1,C2)(C1\[\[C2)?
property-ascription(para, Cl A C2)(lines C1 and C2 are parallel)Textually, the property-ascription versioncan be realized in two forms, represented bythe two resource trees in Fig.
5.Type checking during the construction ofthe Text Structure must ensure, that therealization be compatible along both theideational and the textual dimension.
In thisexample, the combination of the tree in Fig.
4and the first tree in Fig.
5 is compatible andwill lead to the verbalization:"B, since C1 and C2 are parallel.
"25(realization-class (derive :reason R :conclusion C)(resource-tree (composite-tree :content nil:tsc (sentence clause):matrix (leaf :content C: t sc  (clause)):adjunct (composite-tree :content since:tsc (clause):matrix (leaf :content R: t sc  (c lause) )  ) ) )(.further resource trees .
.
.
))Figure 4: The Realization Class for derive<lex be>vpheadarguraent  a rgu lnentconj(C,, C~) Parano noAs a verb phrase?
nilcomproPsite \[matrix adjunctPara conj(C,, CC~)no modifierAs a nominal phraseFigure 5: Textual Variations in form of Re-source TreesThe second tree in Fig.
5, however, canonly be combined with another ealization ofderive, resulting in:"Because of the parallelism of line C1and line C2, B.
"In our current system we concentrat onthe mechanism and are therefore still exper-imenting with heuristics which control thechoice of paraphrases.
One interesting ruleis to distinguish between general rhetoricalrelations and domain specific mathematicalconcepts.
While the former should be para-phrased to increase the flexibility, continuityof the latter helps the user to identify tech-nical concepts.6 Semantic AggregationRulesAlthough the handling of paraphrase gen-eration already increases the flexibility inthe text, the default verbalization strategywill still expand the Text Structure by re-cursively descending the proof and formulastructure, and thereby forced to keep thesestructures.
To achieve the second verbal-ization of equation (1) in the introduction,however, we have to combine Set(F) andSubset(F, G) to form an embedded structureSubset(Set(F), G).
Clearly, although still inthe same format, this is no more an Up-per Model object, since Set(F) is an UpperModel process, not an object.
Actually, thisdocuments a textual decision that no mat-ter how Subset and Set should be instanti-ated, the argument F in Subset(F, G) will bereplaced by Set(F).
This textual operationeliminates one of the duplicates of F.  Thissection is devoted to various textual reorgan-isations which eliminate such redundancies.Following the tradition, we call them aggre-gation rules.As it will become clear when handling con-crete aggregation rules, such rules may nar-row the realization choices of APOs by im-posing additional type restrictions.
Further-more, some realization choices block desir-able textual reorganisation.
On account ofthis we carry out aggregations before con-crete resources for the APOs like object andclass-ascription are chosen.APOs, before they are mapped to UMOs,can be viewed as variables for UMOs (forconvenience, we continue to refer to them asAPOs).
In this sense, our rules work withsuch variables at the semantic level of theUpper Model, and therefore differ from thosemore syntactic rules reported in the litera-ture.
For a comparison see Sec.
6.4.So far, we have investigated three types ofaggregation which will be addressed in thenext two subsections.
A categorization of theaggregation rules is given in Fig.
6.26Grouping (5)Logical Predicates (1)Aggregation(l 1)Embedding (2)PMs (2) Logical Connectives (2)Figure 6: Aggregation Rules in PROVERBPattern (4)Chaining (3) Others (1)6.1 Semant ic  Group ingWe use semantic grouping to characterize themerge of two parallel Text Structure objectswith the same top-concept by grouping theirarguments.
Two APOs are parallel in thesense that they have the same parent node.The general form of this type of rules can becharacterized by the pattern as given below:Rule Pat tern  AP\[a\] + P\[b\]P\[aCb\]The syntax of our rules means that a textstructure of the form above the bar will betransformed into one of the form below thebar.
Viewing Text Structure as a tree, P\[a\]and P\[b\] are both sons of +, they are mergedtogether by grouping the arguments a and bunder another operator ~.
In the first rulebelow, + and ~ are identical.Rule A.1 (Predicate Grouping)P\[a\] + P\[b\]P\[a + b\]where + can be either a logical A or a logicalV, and P stands for a logical predicate.
Thefollowing example illustrates the effect of thisrule.Set(F) A Set(G)"F is a set.
G is a set.
"are aggregated to:Set(F A G)"F and G are sets.
"The rule covers the predicate grouping rulereported in \[3\].
This is also the best placeto explain why we apply aggregation beforechoosing concrete linguistic resources.
If thetwo occurrences of Set are instantiated if-ferently, this rule will be blocked.Now let us examine another semanticgrouping rule, where + and ~ are no longeridentical.Rule A.2 ( Impl icat ion w i th  identicalconclus ion )c) A (P2 c)(& v P2) cHere +, ~,  and P are instantiated to A,V, and ~,  respectively.
By instantiating +,E\[~ and P in pattern A to different logicalconnectives and derivation relations, we havealltogether five rules in this category.
Thecorrectness of the rules in this category withrespect o the information conveyed is guar-anteed by the semantics of the Upper Modelconcerned.
In the case of rule A.2 for in-stance, (PiVP2) ~ C is a logical consequenceof (P1 ~ C) A (P2 ~ C).6.2 Semant ic  Embedd ingThe next category of aggregation rules han-dles parallel structures which are not identi-cal.
In this case, some of them may be con-verted to embedded structures, as is done bythe following rule.Rule B.1 (Ob ject  Embedd ing)P\[T\] A Q\[T\]Q\[P\[T\]\]where?
concepts(f,T) Mconcepts(P) # 027?
f is the innermost application programconcept.governing T in Q\[T\],?
concepts(f, T) denotes the Upper Modelconcepts the argument T of f may take,?
concepts(P) denotes the Upper Modelconcept P may result in.We require also that PIT\] is realized as anobject T with modifiers.
It is this intuitiveexplanation which guarantees the correctnessof this rule with respect o meaning.The following example illustrates this rule,in particular, how the decision made herenarrows the choices of linguistic resources forboth P and T as an argument of Q.
We beginwith the two APOs in a conjunction below,containing a common APO F.Set(F) A Subset(F, G)"F is a set.
F is a subset of G."Since F is directly governed by Subset,f and Q in our rule above coincide here.concepts(Subset, F) = {object), whileconcepts(Set) = (class-ascription, object).Therefore, their intersection is {object).This not only guarantees the expressibilityof the new APO, but also restricts the choiceof linguistic resources for Set, now restrictedto object.
The result as well as its verbaliza-tion is given below:Subset(Set(F), G)"The set F is a subset of G."Actually, for mathematical texts we haveonly used two embedding rules, with theother being the dual of rule B.1 where P andQ change their places.6.3 Pat tern -basedru lesOpt imizat ionRules in the third category involve morecomplex changes of the textual structure ina way which is neither a grouping nor an em-bedding.
They could be understood as somedomain-specific communicative conventions,and must be explored in every domain of ap-plication.
In PRO VERB, currently four suchrules are integrated.
Three of them build asequence of some transitive relations into achain.Rule C. 1 below addresses the problem thatevery step of derivation is mapped to a sepa-rate sentence in the default verbalization.
Itreflects the familiar phenomenon that whenseveral derivation steps form a chain, theyare verbalized in a more connected way.
Toaccommodate he phenomenon of a chain, wehave also added a slot called next in the do-main model concept derive-chain.
Now sup-pose that we have two consecutive deriva-tions with R1,M1,C1 and R2, M2, C2 as itspremises (called reasons), the rule of infer-ence (called method), and the conclusion.They form part of a chain if the conclusionC1 is used as a premise in the second step,namely C1 E R2.
In this case, the followingrule combines them into a chain by puttingthe second derivation into the next slot of thechain.
At the same time, C1 is removed fromR2 since it is redundant.Ru le  C.1 Der ivat ion  Cha in  2derive(R1, M1, C1), derive(R2, IVI2, C2)derive-chain(R1, M1, C1, derive( R2 \ C1, M2, C2, ))The following example illustrates how thisrule works.
We will only give the verbaliza-tion and omit the Text Structure.
Given asequence of two derivation steps which canbe verbalized as:"0 C_ o'*, by the definition of transitiveclosure."
and"Since (x, y) E a and o C o*, (x, y) E c*by the definition of subset.
"Rule C.1 will produce a chain which will beverbalized as"a C 0" by the definition of transitiveclosure, thus establishing (x, y) E 0" bythe definition of subset, since (x,y) E0.
"Note that the rule above is only a simpli-fication of a recursive definition, since chain-ing is not restricted to two derivation steps.2This is a simplified version of the original ruledefined recursively in \[4\]28Readers are referred to \[4\].
Although thisrule inserts the second derive into anotherText Structure, the resulting structure is nowa chain, no longer a plain derive.
Thereforeit distinguishes clearly f;om the rules in Sec-tion 6.2.There are two more chaining rules forthe logical connectors implication and equiv-alence.
A further rule removes redundanciesin some case analyses (see \[4\]).6.4 Discuss ionWhile many systems have some.
aggregationrules implemented \[9, 2\], there are compar-atively few detailed discussions in the liter-ature.
The most structured categorizationwe found is the work of Dalianis and Hovy\[3\], where they define aggregation as a wayof avoiding redundancy.
Some of their rules,nevertheless, make decisions which we wouldcall reference choice.
Since this is treated inanother module, we define our aggregation atthe semantic level.
The following are severalsignificant features of our aggregation rules.The first difference is that our aggregationrules are defined in terms of manipulationsof the Upper Model.
They remove redun-dancies by combining the linguistic resourcesof two adjacent APOs, which contain redun-dant content.
They cover the more syntacticrules reported in the literature at a more ab-stract level.Second, Text Structure provides usstronger means to specify textual operations.While rules reported in the literature typ-ically aggregate clauses, our rules operateboth above and beneath the level of clauseconstituents.Third, while most investigations have con-centrated on general purpose microplanningoperations, we came to the conclusion thatmicroplanning needs domain-specific rulesand patterns as well.7 A Running ExampleThe following example illustrates the mecha-nism of aggregation and its effect on resultingtext.
We start with the following sequence ofPMs:assume(Set(F))assume(Set(G))assume(Subset(F, G) )assume(element(a, F) )assume(element(b, F) )derive( (element(a, F) A Subset(F, G) ),e, element(a, G) )derive( (element(b, F) A Subset(F, G)),e, element(b, G) )Without aggregation, the system produces:"Let F be a set.
Let G be a set.
Let F C G.Let a E F. Let b E F. Since a E F andF C G, a E G. Since b E F and F C G,b E G."Aggregation of the assume-PMs results in:assume(Set(F) A Set(G) A Subset(F, G)i element(a, F) i element(b, F))whereas the application of the grouping rulefor independent derive-PMs provides:derive( ( element( a,F) A element(b, F)A Subset(F, G)), e,(element(a, G) A element(b, G) ) )After that, the predicate grouping rule A.1is applied to the arguments of assume, whichare grouped to:( Set( F A G) A Subset(F, G)A element(a A B, F A F)))Note that F A F is later reduced to F.Predicate grouping applies to the argumentsof derive in a similar way.
Finally, the systemproduces the following output:"Let F and G be sets, F C G, and a, b E F.Then a, b E G."8 ConclusionWe argued in this paper that sophisti-cated microplanning techniques are requiredeven for mathematical proofs, in contrastto the belief that mathematical texts areonly schematic and mechanical.
We demon-strated why paraphrasing and aggregationwill significantly enhance the flexibility andthe coherence of text produced.
In order to29carry out appropriate textual rearrangementwe need a representation formalism which al-lows flexible but principled manipulation oflinguistic resources.
To this end, we basi-cally adopted the Text Structure of Meteer,but split her semantic ategories into two di-mensions following Panaget.
The type check-ing mechanism ofText Structure allows us toachieve paraphrasing by building comparablecombinations of linguistic resources.
Speci-fied in terms of Upper Model concepts, oursemantic aggregation rules are more abstractthan similar rules reported in the literature.One important feature of our work is theintegration of microplanning knowledge spe-cific to our domain of application.
Thisbody of knowledge must be refined to furtherimprove the quality of the text produced.More experience is also required to formulatestrategies to choose among alternatives.References\[1\] John Bateman, Bob Kasper, JohannaMoore, and Richard Whitney.
The pen-man upper model.
Technical Report ISIresearch report, USC/Information Sci-ence institute, 1990.\[2\] Robert Dale.
Generating Referring Ex-pressions.
ACL-MIT PressSeries in Nat-ural Language Processing.
MIT Press,1992.\[3\]\[4\]Hercules Dalianis and Eduard Hovy.Aggregation in natural anguage gener-ation.
In Proc.
4th European Workshopon Natural Language Generation, 1993.Armin Fiedler.
Mikroplanungstechnikenzur PrEsentation mathematischer Be-weise.
Master's thesis, Fachbereich In-formatik, Universit?t des Saartandes,1996.\[5\]\[6\]M.A.K.
Halliday.
Introduction to func-tional grammar.
Edward Arnold, 1985.Xiaorong Huang.
Planning argumenta-tive texts.
In Proc.
of 15th InternationalConference on Computational Linguis-tics, 1994.\[7\] Xiaorong Huang.
PROVERB: A sys-tem explaining machine-found proofs.In Proc.
of 16th Annual Conference ofthe Cognitive Science Society, 1994.\[8\] Xiaorong Huang.
Human OrientedProof Presentation: A ReconstructiveApproach.
Infix, 1996.\[9\] Gerard Kempen.
Conjunction reduc-tion and gapping in clause-level coordi-nation: an inheritance-based approach.Computational Intelligence, 7(4):357-360, 1991.\[10\] Anne Kilger and Wolfgang Finkler.
In-cremental generation for real-time ap-plications.
Research Report RR-95-11,DFKI, Saarbriicken, 1995.\[11\] Marie W. Meteer.
Bridging the genera-tion gap between text planning linguis-tic realization.
Computational Intelli-gence, 7(4), 1991.\[12\] Marie W. Meteer.
Expressibility and theProblem of Efficient Text Planning.
Pin-ter Publishes, London, 1992.\[13\] Johanna Doris Moore and CEcile L.Paris.
Planning text for advisory dia-logues.
In Proc.
27th Annual Meeting ofthe Association for Computational Lin-guistics, 1989.\[14\] FFranck Panaget.
Using a textual repre-sentational level component in the con-text of discourse or dialogue generation.In Proc.
of 7th International Workshopon Natural Language Generation, 1994.\[15\] Penelope Sibun.
The local organiza-tion of text.
In Proe.
of the fifth in-ternational natural language generationworkshop, 1990.\[16\] Ingrid Zukerman.
Using meta-com-ments to generate fluent text in a techni-cal domain.
Computational Intelligence,7:276-295, 1991.30
