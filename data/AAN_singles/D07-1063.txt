Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
600?609, Prague, June 2007. c?2007 Association for Computational LinguisticsJapanese Dependency Analysis Using the Ancestor-Descendant RelationAkihiro Tamura??
Hiroya Takamura??
Manabu Okumura???
Common Platform Software Research Laboratories NEC Corporationa-tamura@ah.jp.nec.com??
Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan{takamura,oku}@pi.titech.ac.jpAbstractWe propose a novel method for Japanese de-pendency analysis, which is usually reducedto the construction of a dependency tree.
Indeterministic approaches to this task, depen-dency trees are constructed by series of ac-tions of attaching a bunsetsu chunk to one ofthe nodes in the tree being constructed.
Con-ventional techniques select the node basedon whether the new bunsetsu chunk and eachnode in the trees are in a parent-child rela-tion or not.
However, tree structures includerelations between two nodes other than theparent-child relation.
Therefore, we useancestor-descendant relations in addition toparent-child relations, so that the added re-dundancy helps errors be corrected.
Ex-perimental results show that the proposedmethod achieves higher accuracy.1 IntroductionJapanese dependency analysis has been recognizedas one of the basic techniques in Japanese process-ing.
A number of techniques have been proposedfor years.
Japanese dependency is usually repre-sented by the relation between phrasal units called?bunsetsu?
chunks, which are the smallest meaning-ful sequences consisting of an independent word andaccompanying words (e.g., a noun and a particle).Hereafter, a ?chunk?
means a bunsetsu chunk in thispaper.
The relation between two chunks has a di-?Akihiro Tamura belonged to Tokyo Institute of Technologywhen this work was done.Figure 1: Example of a dependency treerection from the modifier to the modifiee.
All de-pendencies in a sentence are represented by a de-pendency tree, where a node indicates a chunk, andnode B is the parent of node A when chunk B is themodifiee of chunk A.
Figure 1 shows an example ofa dependency tree.
The task of Japanese dependencyanalysis is to find the modifiee for each chunk in asentence.
The task is usually regarded as construc-tion of a dependency tree.In primitive approaches, the probabilities of de-pendencies are given by manually constructed rulesand the modifiee of each chunk is determined.
How-ever, those rule-based approaches have problems incoverage and consistency.
Therefore, a number ofstatistical techniques using machine learning algo-rithms have recently been proposed.
In most con-ventional statistical techniques, the probabilities ofdependencies between two chunks are learned in thelearning phase, and then the modifiee of each chunkis determined using the learned models in the anal-ysis phase.
In terms of dependency trees, the parentnode of each node is determined based on the likeli-ness of parent-child relations between two nodes.We here take notice of the characteristics of de-pendencies which cannot be captured well only by600the parent-child relation.
Consider, for example,Figure 1.
In Figure 1, ID 3(pizza-and) and ID4(salad-accusative) are in a parallel structure.
In thestructure, node 4 is a child of node 5(ate), but node3 is not a child of 5, although 3 and 4 are both foodsand should share a tendency of being subcategorizedby the verb ?eat?.
A number of conventional modelsuse the pair of 3(pizza-and) and 5(ate) as a nega-tive instance because 3 does not modify 5.
Conse-quently, those models cannot learn and use the sub-categorization preference of verbs well in the paral-lel structures.We focus on ancestor-descendant relations tocompensate for the weakness.
Two nodes are in theancestor-descendant relation when one of the twonodes is included in the path from the root node tothe other node.
The upper node of the two nodesis called an ?ancestor node?
and the lower node a?descendant node?.
When the ancestor-descendantrelation is used, both of the above two instancesfor nodes 3 and 4 can be considered as positive in-stances.
Therefore, it is expected that the ancestor-descendant relation helps the algorithm capture thecharacteristics that cannot be captured well by theparent-child relation.We aim to improve the performance of Japanesedependency analysis by taking the ancestor-descendant relation into account.
In exploitingancestor-descendant information, it came to us thatredundant information is effectively utilized in acoding problem in communications (Mackay, 2003).Therefore, we propose a method in which the prob-lem of determining the modifiee of a chunk is re-garded as a kind of a coding problem: dependency isexpressed as a sequence of values, each of which de-notes whether a parent-child relation or an ancestor-descendant relation holds between two chunks.In Section 2, we present the related work.
In Sec-tion 3, we explain our method.
In Section 4, we de-scribe our experiments and their results, where weshow the effectiveness of the proposed method.
InSection 5, we discuss the results of the experiments.Finally, we describe the summary of this paper andthe future work in Section 6.2 Conventional Statistical Methods forJapanese Dependency AnalysisFirst, we describe general formulation of theprobability model for dependency analysis.
Wedenote a sequence of chunks, ?b1, b2, ..., bm?,by B, and a sequence of dependency pat-terns, ?Dep(1), Dep(2), ..., Dep(m)?, by D, whereDep(i) = j means that bi modifies bj .
Given the se-quence B of chunks as an input, dependency analy-sis is defined as the problem of finding the sequenceD of the dependency patterns that maximizes theconditional probability P (D | B).
A number ofthe conventional methods assume that dependencyprobabilities are independent of each other and ap-proximate P (D | B) with?m?1i=1 P (Dep(i) | B).P (Dep(i) | B) is estimated using machine learn-ing algorithms.
For example, Haruno et al (1999)used Decision Trees, Sekine (2000) used MaximumEntropy Models, Kudo and Matsumoto (2000) usedSupport Vector Machines.Another notable method is Cascaded ChunkingModel by Kudo and Matsumoto (2002).
In theirmodel, a sentence is parsed by series of the fol-lowing processes: whether or not the current chunkmodifies the following chunk is estimated, and if itis so, the two chunks are merged together.
Sassano(2004) parsed a sentence efficiently using a stack.The stack controls the modifier being analyzed.These conventional methods determine the mod-ifiee of each chunk based on the likeliness of de-pendencies between two chunks (in terms of depen-dency tree, the likeliness of parent-child relationsbetween two nodes).
The difference between theconventional methods and the proposed method isthat the proposed method determines the modifieesbased on the likeliness of ancestor-descendant re-lations in addition to parent-child relations, whilethe conventional methods tried to capture charac-teristics that cannot be captured by parent-child re-lations, by adding ad-hoc features such as featuresof ?the chunk modified by the candidate modifiee?to features of the candidate modifiee and the mod-ifier.
However, these methods do not deal withancestor-descendant relations between two chunksdirectly, while our method uses that information di-rectly.
In Section 5, we empirically show that ourmethod uses the ancestor-descendant relation more601effectively than the conventional ones and explainthat our method is justifiable in terms of a codingproblem.3 Proposed MethodThe methods explained in this section construct adependency tree by series of actions of attachinga node to one of the nodes in the trees being con-structed.
Hence, when the parent node of a certainnode is being determined, it is required that the par-ent node should already be included in the tree beingconstructed.
To satisfy the requirement, we note thecharacteristic of Japanese dependencies: dependen-cies are directed from left to right.
(i.e., the par-ent node is closer to the end of a sentence than itschild node).
Therefore, our methods analyze a sen-tence backwards as in Sekine (2000) and Kudo andMatsumoto (2000).
Consider, for example, Figure1.
First, our methods determine the parent node ofID 4(salad-accusative), and then that of ID 3(pizza-and) is determined.
Next, the parent node of ID 2(atlunchtime), and finally, that of ID 1(he-nominative)is determined and dependencies in a sentence areidentified.
Please note that our methods are applica-ble only to dependency structures of languages thathave a consistent head-direction like Japanese.We explain three methods that are different inthe information used in determining the modifiee ofeach chunk.
In Section 3.1, we explain PARENTMETHOD and ANCESTOR METHOD, which de-termine the modifiee of each chunk based on thelikeliness of only one type of the relation.
PARENTMETHOD uses the parent-child relation, which isused in conventional Japanese dependency analy-sis.
ANCESTOR METHOD is novel in that ituses the ancestor-descendant relation which has notbeen used in the existing methods.
In Section3.2, we explain our method, PARENT-ANCESTORMETHOD, which determines the modifiees basedon the likeliness of both ancestor-descendant andparent-child relations.When the modifiee is determined using theancestor-descendant relation, it is necessary to takeinto account the relations with every node in the tree.Consider, for example, the case that the modifieeof ID 1(he-nominative) is determined in Figure 1.When using the parent-child relation, the modifieecan be determined based only on the relation be-tween ID 1 and 5.
On the other hand, when using theancestor-descendant relation, the modifiee cannot bedetermined based only on the relation between ID1 and 5.
This is because if one of ID 2, 3 and 4is the modifiee of ID 1, the relation between ID 1and 5 is ancestor-descendant.
ID 5 is determinedas the modifiee of ID 1 only after the relations witheach node of ID 2, 3 and 4 are recognized not tobe ancestor-descendant.
An elegant way to use theancestor-descendant relation, which we propose inthis paper, is to represent a dependency as a code-word where each bit indicates the relation with anode in the tree, and determine the modifiee basedon the relations with every node in the tree (for de-tails to the next section).3.1 Methods with a single relation: PARENTMETHOD and ANCESTOR METHODFigure 2 shows the pseudo code of the algo-rithm to construct a dependency tree using PAR-ENT METHOD or ANCESTOR METHOD.
Asmentioned above, the two methods analyze a sen-tence backwards.
We should note that node1 tonoden in the algorithm respectively correspond tothe last chunk to the first chunk of a sentence.MODEL PARENT(nodei,nodej) indicates the pre-diction whether nodej is the parent of nodei ornot, which is the output of the learned model.MODEL ANCESTOR(nodei,nodej) indicates theprediction whether nodej is the ancestor of nodei ornot.
String output indicates the sequence of the i?1 predictions stored in step 3.
The codeword denotedby string[k] is the binary sequence given to the ac-tion that nodei is attached to nodek.
Parent[nodei]indicates the node to which nodei is attached, andDis indicates a distance function.
Thus, our methodpredicts the correct actions by measuring the dis-tance between the codeword string[k] and the pre-dicted binary (later extended to real-valued) se-quences string output.
In other words, our methodselects the action that is the closest to the outputs ofthe learned model.Both models are learned from dependency treesgiven as training data as shown in Figure 3.
Eachrelation is learned from ordered pairs of two nodesin the trees.
However, our algorithm in Figure 2targets at dependencies directed from left to right.6021:for i = 1, 2, ..., n do2: for j = 1, 2, ..., i ?
1 do3: result parent[j]=MODEL PARENT(nodei,nodej)(in case of PARENT and PARENT-ANCESTOR METHOD)3: result ancestor[j]=MODEL ANCESTOR(nodei,nodej)(in case of ANCESTOR and PARENT-ANCESTOR METHOD)4: end5: Parent[nodei]=argmink Dis(string[k], string output)6:endFigure 2: Pseudo code of PARENT, ANCESTOR,and PARENT-ANCESTOR METHODSFigure 3: Example of training instancesTherefore, the instances with a right-to-left depen-dency are excluded from the training data.
For ex-ample, the instance with node4 being the candi-date parent (or ancestor) of node1 is excluded inFigure 3.
MODEL PARENT uses ordered pairsof a parent node and a child node as positive in-stances and the other ordered pairs as negative in-stances.
MODEL ANCESTOR uses ordered pairsof an ancestor node and a descendant node aspositive instances and the other ordered pairs asnegative instances.
From the above descriptionand Figure 3, the number of training instancesused in learning MODEL PARENT is the sameas the number of training instances used in learn-ing MODEL ANCESTOR.
However, the number ofpositive instances in learning MODEL ANCESTORis larger than in learning MODEL PARENT be-cause the set of parent-child relations is a subset ofancestor-descendant relations.As mentioned above, the two methods analyze asentence backwards.
We should note that node1 tonoden in the algorithm respectively correspond tothe last chunk to the first chunk of a sentence.Next, we illustrate the process of determining theparent node of a certain node nodem(with Figures 4and 5).
Hereafter, nodem is called a target node.The parent node is determined based on the like-liness of a relation; the parent-child and ancestor-descendant relation are used in PARENT METHODand ANCESTOR METHOD respectively.Our methods regard a dependency between thetarget node and its parent node as a set of relationsbetween the target node and each node in the tree.Each relation corresponds to one bit, which becomes1 if the relation holds, ?1 otherwise.
For example,a sequence (?1,?1,?1, 1) represents that the par-ent of node5 is node4 in PARENT METHOD (Fig-ure 4), since the relation holds only between nodes4 and 5.First, the learned model judges whether the tar-get node and each node in the current tree are ina certain relation or not; PARENT METHOD usesMODEL PARENT as the learned model and AN-CESTOR METHOD uses MODEL ANCESTOR.The sequence of the m?1 predictions by the learnedmodel is stored in string output.The codeword string[k] is the binary (?1 or 1)sequence that is to be output when the target nodeis attached to the nodek.
In Figures 4 and 5, theset of string[k] (for node5) is in the dashed square.For example, string[2] in ANCESTOR METHOD(Figure 5) is (1, 1,?1,?1) since nodes 1 and 2 arethe ancestor of node5 if node5 is attached to node2.Next, among the set of string[k], the codewordthat is the closest to the string output is selected.The target node is then attached to the node cor-responding to the selected codeword.
In Figure 4,the string[4], (?1,?1,?1, 1), is selected and thennode5 is attached to node4.Japanese dependencies have the non-crossingconstraint: dependencies do not cross one another.To satisfy the constraint, we remove the nodes thatwill break the non-crossing constraint from the can-didates of a parent node in step 5 of the algorithm.PARENT METHOD differs from conventionalmethods such as Sekine (2000) or Kudo and Mat-sumoto (2000), in the process of determining theparent node.
These conventional methods select thenode given by argmaxjP (nodej | nodei) as theparent node of nodei, setting the beam width to 1.However, their processes are essentially the same asthe process in PARENT METHOD.603Figure 4: Analysis example using PARENTMETHODFigure 5: Analysis example using ANCESTORMETHOD3.2 Proposed method: PARENT-ANCESTORMETHODThe proposed method determines the parent node ofa target node based on the likeliness of ancestor-descendant relations in addition to parent-childrelations.
The use of ancestor-descendant rela-tions makes it possible to capture the character-istics which cannot be captured by parent-childrelations alone.
The pseudo code of the pro-posed method, PARENT-ANCESTOR METHOD,is shown in Figure 2.
MODEL PARENT andMODEL ANCESTOR are learned as described inSection 3.1.
String output is the concatenationof the predictions by both MODEL PARENT andMODEL ANCESTOR.
In addition, string[k] isprovided based not only on parent-child relations butalso on ancestor-descendant relations.
An analysisexample using PARENT-ANCESTOR METHOD isshown in Figure 6.Figure 6: Analysis example using PARENT-ANCESTOR METHOD4 Experiment4.1 Experimental settingsWe used Kyoto University text corpus (Version2.0) (Kurohashi and Nagao, 1997) for training andtest data.
The articles on January 1st through 8th(7,958 sentences) were used as training data, and thearticles on January 9th (1,246 sentences) as test data.The dataset is the same as in leading works (Sekine,2000; Kudo and Matsumoto, 2000; Kudo and Mat-sumoto, 2002; Sassano, 2004).We used SVMs as the algorithm of learning andanalyzing the relations between nodes.
We used thethird degree polynomial kernel function and set thesoft margin parameter C to 1, which is exactly thesame setting as in Kudo and Matsumoto (2002).
Wecan obtain the real-valued score in step 3 of the al-gorithm, which is the output of the separating func-tion.
The score can be regarded as likeliness of thetwo nodes being in the parent-child (or the ancestor-descendant).
Therefore, we used the sequence ofthe outputs of SVMs as string output, instead ofconverting the scores into binary values indicatingwhether a certain relation holds or not.Two feature sets are used: static features and dy-namic features.
The static features used in the ex-periments are shown in Table 1.
The features are thesame as those used in Kudo and Matsumoto (2002).In Table 1, HeadWord means the rightmost con-tent word in the chunk whose part-of-speech is nota functional category.
FunctionalWord means the604Table 1: Static features used in experimentsHead Word (surface-form, POS, POS-subcategory,inflection-type, inflection-form), Functional Word (Modifier / surface-form, POS, POS-subcategory, inflection-type,Modifiee inflection-form), brackets, quotation-marks,punctuation-marks, position in sentence (beginning, end)Between two distance (1,2-5,6-), case-particles, brackets,chunks quotation-marks, punctuation-parksFigure 7: Dynamic featuresrightmost functional word or the inflectional form ofthe rightmost predicate if there is no functional wordin the chunk.Next, we explain the dynamic features used inthe experiments.
Three types of dynamic featureswere used in Kudo and Matsumoto (2002): (A)the chunks modifying the current candidate modi-fiee, (B) the chunk modified by the current candidatemodifiee, and (C) the chunks modifying the currentcandidate modifier.
The type C is not available in theproposed method because the proposed method an-alyzes a sentence backwards unlike Kudo and Mat-sumoto (2002).
Therefore, we did not use the typeC.
We used the type A?
and B?
which are recursiveexpansion of type A and B as the dynamic features(Figure 7).
The form of functional words or inflec-tion was used as a type A?
feature and POS and POS-subcategory of HeadWord as a type B?
feature.4.2 Experimental resultsIn this section, we show the effectiveness of the pro-posed method.
First, we compare the three methodsdescribed in Section 3: PARENT METHOD, AN-CESTOR METHOD, and PARENT-ANCESTORMETHOD.
The results are shown in Table 2.
Here,dependency accuracy is the percentage of correctdependencies (correct parent-child relations in treesin test data), and sentence accuracy is the percent-age of the sentences in which all the modifiees aredetermined correctly (correctly constructed trees intest data).Table 2 shows that PARENT-ANCESTORMETHOD is more accurate than the other twoTable 2: Result of dependency analysis using meth-ods described in Section 3Method Dependency SentenceAccuracy AccuracyPARENT 88.95% 44.87%ANCESTOR 87.64% 43.74%PARENT-ANCESTOR 89.54% 47.38%Table 3: Comparison to conventional methodsFeature Method Dependency SentenceAccuracy AccuracyOnly Proposed method 88.88% 46.33%static Kudo and Matsumoto (2002) 88.71% 45.19%Static + Proposed method 89.43% 47.94%Dynamic A,B Kudo and Matsumoto (2002) 89.19% 46.64%OriginalProposed method 89.54% 47.38%Sekine (2000) 87.20% 40.76%Kudo and Matsumoto (2000) 89.09% 46.17%Kudo and Matsumoto (2002) 89.29% 47.53%Sassano (2004) 89.56% 48.35%w/o Rich Sassano (2004) 89.19% 47.05%w/o Conj 89.41% 47.86%methods.
In other words, the accuracy of depen-dency analysis improves by utilizing the redundantinformation.
The improvement is statistically sig-nificant in the sign-test with 1% significance-level.Next, we compare the proposed method withconventional methods.
We compare the proposedmethod particularly with Kudo and Matsumoto(2002) with the same feature set.
The reasons arethat Cascaded Chunking Model proposed in Kudoand Matsumoto (2002) is used in a popular Japanesedependency analyzer, CaboCha 1, and the compari-son can highlight the effectiveness of our approachbecause we can experiment under the same condi-tions (e.g., dataset, feature set, learning algorithm).A summary of the comparison is shown in Table 3.Table 3 shows that the proposed methodoutperforms conventional methods except Sas-sano (2004)2, while Sassano (2004) used richer fea-tures which are not used in the proposed method,such as features for conjunctive structures based onKurohashi and Nagao (1994), features concerningthe leftmost content word in the candidate modi-fiee.
The comparison of the proposed method withSassano (2004)?s method without the features of1http://chasen.org/?taku/software/cabocha/2We have not tested the improvement statistically becausewe do not have access to the conventional methods.605Table 4: Accuracy of dependency analysis on paral-lel structuresParallel structures Other thanparallel structuresPARENT 74.18% 91.21%ANCESTOR 73.24% 90.01%PARENT-ANCESTOR 76.29% 91.63%conjunctive structures (w/o Conj) and without thericher features derived from the words in chunks(w/o Rich) suggests that the proposed method is bet-ter than or comparable to Sassano (2004)?s method.5 Discussion5.1 Performance on parallel structuresAs mentioned in Section 1, the ancestor-descendantrelation is supposed to help to capture parallel struc-tures.
In this section, we discuss the performance ofdependency analysis on parallel structures.
Parallelstructures such as those of nouns (e.g., Tom and Keneat hamburgers.)
and those of verbs (e.g., Tom eatshamburgers and drinks water.
), are marked in KyotoUniversity text corpus.
We investigate the accuracyof dependency analysis on parallel structures usingthe information.Table 4 shows that the accuracy on parallel struc-tures improves by adding the ancestor-descendantrelation.
The improvement is statistically significantin the sign-test with 1% significance-level.
Table 4also shows that error reduction rate on parallel struc-tures by adding the ancestor-descendant relation is8.3% and the rate on the others is 4.7%.
These showthat the ancestor-descendant relation work well es-pecially for parallel structures.In Table 4, the accuracy on parallel structuresusing PARENT METHOD is slightly better thanthat using ANCESTOR METHOD, while the dif-ference is not statistically significant in the sign-test.
It shows that the parent-child relation is alsonecessary for capturing the characteristics of paral-lel structures.
Consider the following two instancesin Figure 1 as an example: the ordered pair of ID3(pizza-and) and ID 5(ate), and the ordered pair ofID 4(salad-accusative) and ID 5.
In ANCESTORMETHOD, both instances are positive instances.
Onthe other hand, only the ordered pair of ID 4 andID 5 is a positive instance in PARENT METHOD.Table 5: Comparison between usages of theancestor-descendant relationDependency SentenceAccuracy AccuracyFeature 88.57% 44.71%Model 88.88% 46.33%Hence, PARENT METHOD can learn appropriatecase-particles in a modifier of a verb.
For exam-ple, the particle which means ?and?
does not mod-ify verbs.
However, it is difficult for ANCESTORMETHOD to learn the characteristic.
Therefore,both parent-child and ancestor-descendant relationsare necessary for capturing parallel structures.5.2 Discussion on usages of theancestor-descendant relationIn the proposed method, MODEL ANCESTOR,which judges whether the relation between twonodes is ancestor-descendant or not, is prepared,and the information on the ancestor-descendant re-lation is directly utilized.
On the other hand,conventional methods add the features regardingthe ancestor or descendant chunk to capture theancestor-descendant relation.
In this section, weempirically show that the proposed method utilizesthe information on the ancestor-descendant rela-tion more effectively than conventional methods.The results in the previous sections could not showthe effectiveness because MODEL PARENT andMODEL ANCESTOR in the proposed method usethe features regarding the ancestor-descendant rela-tion.Table 5 shows the result of dependency analy-sis using two types of usages of the informationon the ancestor-descendant relation.
?Feature?
indi-cates the conventional usage and ?Model?
indicatesour usage.
Please note that MODEL PARENT andMODEL ANCESTOR used in ?Model?
do not usethe features regarding the ancestor-descendant rela-tion.
Table 5 shows that our usage is more effec-tive than the conventional usage.
This is becauseour usage takes advantage of redundancy in termsof a coding problem as described in the next sec-tion.
Moreover, the learned features through the pro-posed method would include more information than606ad-hoc features that were manually added.5.3 Proposed method in terms of a codingproblemIn a coding problem, redundancy is effectively uti-lized so that information can be transmitted moreproperly (Mackay, 2003).
This idea is the same asthe main point of the proposed method.
In this sec-tion, we discuss the proposed method in terms of acoding problem.In a coding problem, when encoding information,the redundant bits are attached so that the added re-dundancy helps errors be corrected.
Moreover, thefollowing fact is known (Mackay, 2003):the error-correcting ability is higher when the dis-tances between the codewords are longer.
(1)For example, consider the following three typesof encodings: (A) two events are encoded respec-tively into the codewords ?1 and 1 (the simplestencoding), (B) into the codewords (?1,?1, 1) and(1, 1, 1) (hamming distance:2), and (C) into thecodewords (?1,?1,?1) and (1, 1, 1) (hammingdistance:3).
Please note that the hamming distance isdefined as the number of bits that differ between twocodewords.
In (A), the correct information is nottransmitted if a one-bit error occurs.
In (B), if an er-ror occurs in the third bit, the error can be correctedby assuming that the original codeword is closestto the received codeword.
In (C), any one-bit errorcan be corrected.
Thus, (B) has the higher error-correcting ability than (A), and (C) has the highererror-correcting ability than (B).We explain the problem of determining the par-ent node of a target node in the proposed method interms of the coding theory.
A sequence of numberscorresponds to a codeword.
It is assumed that thecodeword which expresses the correct parent nodeof the target node is transmitted.
The codeword istransmitted through the learned model through chan-nels to the receiver.
The receiver infers the parentnode from the received sequence (string output) inconsideration of the codewords that can be transmit-ted (string[k]).
Therefore, error-correcting ability,the ability of correcting the errors in predictions instep 3, is dependent on the distances between thecodewords (string[k]).The codewords in PARENT-ANCESTORMETHOD are the concatenation of the bits based onboth parent-child relations and ancestor-descendantrelations.
Consequently, the distances betweencodewords in PARENT-ANCESTOR METHOD arelonger than those in PARENT METHOD or AN-CESTOR METHOD.
From (1), the error-correctingability is expected to be higher.
In terms of a codingproblem, the proposed method exploits the essenceof (1), and utilizes ancestor-descendant relationseffectively.We assume that every bit added as redundancy iscorrectly transmitted for the above-mentioned dis-cussion.
However, some of these added bits may betransmitted wrongly in the proposed method.
In thatcase, the added redundancy may not help errors becorrected than cause an error.
In the experiments ofdependency analysis, the advantage prevails againstthe disadvantage because accuracy of each bit of thecodeword is 94.5%, which is high value.Discussion on applicability of existing codesA number of approaches use Error CorrectingOutput Coding (ECOC) (Dietterich and Bakiri,1995; Ghani, 2000) for solving multiclass classifica-tion problems as a coding problem.
The approachesassign a unique n-bit codeword to each class, andthen n classifiers are trained to predict each bit.
Thepredicted class is the one whose codeword is clos-est to the codeword produced by the classifiers.
Thecodewords in these approaches are designed to bewell-separated from one another and have sufficienterror-correcting ability (e.g., BCH code).However, these existing codewords are not ap-plicable to the proposed method.
In the proposedmethod, we have two models respectively derivedfrom the parent-child and ancestor-descendant rela-tion, which can be interpreted in terms of both lin-guistic aspects and tree structures.
If we use ECOC,however, pairs of nodes are divided into positive andnegative instances arbitrarily.
Since this divisionlacks linguistic or structural meaning, training in-stances will lose consistency and any proper modelwill not be obtained.
Moreover, we have to preparedifferent models for each stage in tree construction,because the length of the codewords vary accordingto the number of nodes in the current tree.607Table 6: Result of dependency analysis using vari-ous distance functionsDistance Method Dependency SentenceFunction Accuracy AccuracyHammingPARENT(n) 85.05% 35.35%PARENT(f) 85.48% 39.87%ANCESTOR(n) 87.54% 43.42%ANCESTOR(f) 86.97% 43.18%Proposed method(n) 88.36% 43.74%Proposed method(f) 88.45% 44.79%PARENT 88.95% 44.87%Cosine / ANCESTOR 87.64% 43.74%Euclidean Proposed method 89.54% 47.38%ManhattanPARENT(n) 88.74% 44.63%PARENT(f) 88.90% 44.79%ANCESTOR 87.64% 43.74%Proposed method 89.24% 46.89%5.4 Influence of distance functionsIn this section, we compare the performance of de-pendency analysis with various distance functions:hamming distance, euclidean distance, cosine dis-tance, and manhattan distance.
These distance func-tions between sequences X=?x1 x2 ... xn?
andY =?y1 y2 ... yn?
are defined as follows:?
Ham(X,Y ) =?ni=1(1 ?
?
(xi, yi)),?
Euc(X,Y ) =?
?ni=1(xi ?
yi)2,?
Cos(X,Y ) = 1 ?
?ni=1 xi?yi?
?ni=1 x2i?
?ni=1 y2i,?
Man(X,Y ) =?ni=1 | xi ?
yi |.In the hamming distance, string output is con-verted to a binary sequence with their elements be-ing of ?1 or 1.
The cosine distance is equivalent tothe Euclidean distance under the condition that theabsolute value of every component of string[k] is1.The results of dependency analysis using thesedistance functions are shown in Table 6.
In Table6, ?(n)?
means that the nearest chunk in a sentenceis selected as the modifiee in order to break a tie,which happens when the number of sequences satis-fying the condition in step 5 is two or more, while?(f)?
means that the furthest chunk is selected.
If theresults in case of (n) and (f) are the same, (n) and (f)are omitted and only one result is shown.Table 6 shows that the proposed method out-performs PARENT METHOD and ANCESTORMETHOD in any distance functions.
It means thatthe effectiveness of the proposed method does notdepend on distance functions.
The result using thehamming distance is much worse than using theother distance functions.
It means that using thescores output by SVMs as the likeliness of a certainrelation improves the accuracy.
The results of (n)and (f) in the hamming distance are different.
It isbecause the hamming distances are always positiveintegers and ties are more likely to happen.
Table6 also shows that the result of the cosine or the eu-clidean distance is better than that of the manhattandistance.6 ConclusionsWe proposed a novel method for Japanese depen-dency analysis, which determines the modifiee ofeach chunk based on the likeliness not only ofthe parent-child relation but also of the ancestor-descendant relation in a dependency tree.
Theancestor-descendant relation makes it possible tocapture the parallel structures in more depth.
Interms of a coding theory, the proposed methodboosts error-correcting ability by adding the redun-dant bits based on ancestor-descendant relations andincreasing the distance between two codewords.
Ex-perimental results showed the effectiveness of theproposed method.
In addition, the results showedthat the proposed method outperforms conventionalmethods.Future work includes the following.
In this pa-per, we use the features proposed in Kudo and Mat-sumoto (2002).
By extracting new features that aremore suitable for the ancestor-descendant relation,we can further improve our method.
The featuresused by Sassano (2004) are promising as well.
Weare also planning to apply the proposed method toother tasks which need to construct tree structures.For example, (zero-) anaphora resolution is consid-ered as a good candidate task for application.ReferencesThomas G. Dietterich and Ghulum Bakiri.
1995.
SolvingMulticlass Learning Problems via Error-CorrectingOutput Codes.
Journal of Artificial Intelligence Re-search, 2:263?286.Rayid Ghani.
2000.
Using Error-Correcting Codes For608Text Classification.
In Proc.
of ICML-2000, pages303?310.Masahiko Haruno, Satoshi Shirai, and YoshifumiOoyama.
1999.
Using Decision Trees to Constructa Practical Parser.
Machine Learning, 34:131?149.Taku Kudo and Yuji Matsumoto.
2000.
Japanese Depen-dency Analysis Based on Support Vector Machines.
InProc.
of EMNLP/VLC 2000, pages 18?25.Taku Kudo and Yuji Matsumoto.
2002.
Japanese Depen-dency Analysis using Cascaded Chunking.
In Proc.
ofCoNLL 2002, pages 63?69.Sadao Kurohashi and Makoto Nagao.
1994.
A syntacticanalysis method of long Japanese sentences based onthe detection of conjunctive structures.
ComputationalLinguistics, 20(4):507?534.Sadao Kurohashi and Makoto Nagao.
1997.
Kyoto Uni-versity text corpus project.
In Proc.
of ANLP, pages115?118, Japan.David J. C. Mackay.
2003.
Information Theory, Infer-ence, and Learning Algorithms.
Cambridge Univer-sity Press.Manabu Sassano.
2004.
Linear-Time Dependency Anal-ysis for Japanese.
In Proc.
of COLING 2004, pages8?14.Satoshi Sekine.
2000.
Japanese dependency analysis us-ing a deterministic finite state transducer.
In Proc.
ofCOLING 2000, pages 761?767.609
