Sub-event based multi-document summarizationNaomi Daniel,1    Dragomir Radev,1,2    Timothy Allison31School of Information2Department of Electrical Engineering and Computer Science3Department of Classical StudiesUniversity of Michigan{ndaniel, radev, tballiso}@umich.eduAbstractThe production of accurate and completemultiple-document summaries is challenged bythe complexity of judging the usefulness ofinformation to the user.
Our aim is to determinewhether identifying sub-events in a news topiccould help us capture essential information toproduce better summaries.
In our first experiment,we asked human judges to determine the relativeutility of sentences as they related to the sub-events of a larger topic.
We used this data tocreate summaries by three different methods, andwe then compared these summaries with threeautomatically created summaries.
In our secondexperiment, we show how the results of our firstexperiment can be applied to a cluster-basedautomatic summarization system.
Through bothexperiments, we examine the use of inter-judgeagreement and a relative utility metric thataccounts for the complexity of determiningsentence quality in relation to a topic.1.
IntroductionMultiple articles on a particular topic tend to containredundant information as well as information that is uniqueto each article.
For instance, different news sourcescovering the same topic may take different angles, or newinformation may become available in a later report.
So,while all the articles are related to the larger topic, eacharticle may be associated with any of several sub-events.We wanted to find a way to capture the unique sub-eventinformation that is characteristic in multiple-documentcoverage of a single topic.
We predicted that breakingdocuments down to their sub-events and capturing thosesentences in each sub-event with the highest utility wouldproduce an accurate, thorough, and diverse multi-document summary.In our first experiment, we compared sixmethods of summarization to see which produces the bestsummaries.
The methods included three automatic andthree manual methods of producing summaries.
We usedrelative utility to capture and measure subtleties indetermining sentence relevance.
We created multipledocument summaries using both a sub-event basedapproach and a topic-based approach.
Generally, weexpected to find that the manual summaries performedbetter than the automatic summaries.
In our secondexperiment, we designed a multi-document summarizerwhich relied on a clustering method, and we tested thethree policies we devised for creating summaries from themanual summarization technique developed in our firstexperiment.2.
Related WorkMuch work has preceded and informed this paper.
Allanet al?s (1998) work on summarizing novelty recognizesthat news topics consist of a series of events ?
what we call?sub-events,?
to distinguish the difference between a newstopic and its sub-events.
However, their method differs inits approach, which uses an algorithm to identify ?novel?sentences, rather than the use of human judges.
In otherrelated work, sentences are either judged ?on-topic?
or?off-topic?
(Allan et al, 2001a) (Allan et al, 2001b).Carbonell and Goldstein use Maximal Marginal Relevance(MMR) to identify ?novel?
information to improve queryanswering results, and they also apply this method tomultiple-document summarization (Carbonell andGoldstein, 1997 and Goldstein, 1999).
Success in the useof inter-judge agreement has led us to pursue the use of thecurrent evaluation methods.
However, this experimentdiffers from prior work in that we use judges to determinethe relevance of sentences to sub-events rather than toevaluate summaries (Radev et al, 2000).
Finally,McKeown et al (1999), Hatzivassiloglou et al (2001) andBoros et al (2001) have shown the challenges andpotential payoffs of using sentence clustering in extractivesummarization.3.
Article CorpusOur study involves two experiments carried out on onecorpus of news articles.
The article corpus was selectedfrom a cluster of eleven articles describing the 2000 crashof Gulf Air flight 072.
From these articles we chose acorpus of five articles, containing a total of 159 sentences.All the articles cover a single news event, the plane crashand its aftermath.
The articles were gathered on the webfrom sources reporting on the event as it unfolded, andcome from various news agencies, such as ABC News,Fox News, and the BBC.
All of the articles give somediscussion of the events leading up to and following thecrash, with particular articles focusing on areas of specialinterest, such as the toll on Egypt, from where many of thepassengers had come.
The article titles in Table 1, below,illustrate the range of sub-events that are covered under thecrash topic.Article ID Source Date Headline30 BBC Aug. 25 Bodies recovered fromGulf Air crash41 Fox News Aug. 25 Egyptians Suffer SecondAir Tragedy in a Year81 USA Today Aug. 25 One American among 143dead in crash87 ABC News Aug. 26 Prayers for victims ofBahrain crash97 Fox News Aug. 26 Did Pilot Error Cause AirCrashTable 1.
Corpus article characteristics.4.
Experiment 1: Sub-Event AnalysisOur first experiment involved having human judgesanalyze the sentences in our corpus for degree of saliencyto a series of sub-events comprising the topic.4.1 Description of Sub-Event User StudyThe goal of this experiment was to study the effectivenessof breaking a news topic down into sub-events, in order tocapture not simply salience, but also diversity (Goldstein,1998).The sub-events were chosen to cover all of thematerial in the reports and to represent the most significantaspects of the news topic.
For the Gulf Air crash, wedetermined that the sub-events were:1.
The plane takes off2.
Something goes wrong3.
The plane crashes4.
Rescue and recovery effort5.
Gulf Air releases information6.
Government agencies react7.
Friends, relatives and nations mourn8.
Black box(es) are searched for9.
Black box(es) are recovered10.
Black box(es) are sent for analysisWe instructed judges to rank the degree ofsentence relevance to each sub-event.
Judges wereinstructed to use a scale, such that a score of ten indicatedthat the sentence was critical to the sub-event, and a scoreof 0 indicated that the sentence was irrelevant.
Thus, thejudges processed the 159 sentences from 5 documents tentimes, once pertaining to each sub-event.
This experimentproduced for each judge 1590 data points which wereanalyzed according to the methods described in the nextsection.We used the data on the relevance of thesentences to the sub-events to calculate inter-judgeagreement.
In this manner, we determined whichsentences had the overall highest relevance to each sub-event.
We used this ranking to produce summaries atdifferent levels of compression.5.
Methods for Producing SummariesTo gather data about the effectiveness of dividingnews topics into their sub-events for creating summaries,we utilized data from human judges, upon which wemanually performed three algorithms.
These algorithmsand their application are described in detail below.
Wewere interested to determine if the Round Robin method(described below,) which has been used by McKeown etal.
(1999), Boros et al (2001) and by Hatzivassiloglou etal.
(2001), was the most effective.5.1 Sub-Event-Based AlgorithmsAfter collecting judges?
scores of relevance for eachsentence for each subtopic, we then ranked the sentencesaccording to three different algorithms to create multiple-document summaries.
From this data, we createdsummary extracts using three algorithms, as follows:  Algorithm 1) Highest Score Anywhere - pick thesentence which is most relevant to any subevent, no matterthe subevent; pick the next sentence which is most relevantto any subevent, etc. Algorithm 2) Sum of All Scores - for eachsentence, sum its relevance score for each cluster, pickthe sentence with the highest sum; then pick thesentence with the second highest sum, etc.  Algorithm 3) Round Robin - pick the sentencewhich has the most relevance for subevent 1, pick thesentence with the most relevance for subevent 2, etc.
Afterpicking 1 sentence from each subevent, pick the sentencewith the 2nd best relevance to subevent 1, etc.Judge 1 Judge 2 Judge 3 Judge 1 Judge 2 Judge 3 Judge 1 Judge 2 Judge 3Article 30,Sentence 1 1 0 0 5 0 5 8 8 102 1 0 0 7 4 7 10 10 103 4 0 0 10 10 10 10 5 74 1 0 3 5 0 2 8 0 25 0 0 0 3 0 0 5 0 26 0 0 0 3 0 0 6 0 27 0 0 0 3 0 0 6 0 28 0 0 0 3 4 2 10 10 109 0 0 2 0 0 0 8 0 010 0 0 0 3 0 0 6 0 2Sub-Event 1 Sub-Event 2 Sub-Event 3Table 2.
First ten sentences of article 30, shown with scores given by three judges for three sub-events.
Judges often disagreeon the degree of sentence relevancy.
Some sentences are used in more than one sub-event.Algorithm 1 - Highest ScoreAnywhere (HSA): This algorithm was producedby summing the data across all judges to produce a totalinter-judge score and keeping sub-events distinct, to seethe inter-judge utility scores given to sub-events.
Weordered the sentences by ranking these scores indescending order and omitting duplicates, to produce theten and twenty percent extracts.
For example, with datafrom seven judges on ten sub-events, the highest possiblescore for each sentence was seventy.
Thus seventy wasthe highest score.In the case that there was a tie betweensentences, we ordered them by sub-event number (firstsub-event first and tenth sub-event last).Algorithm 2 - Sum of All Scores(SAS): This algorithm was produced by summing thedata across all judges to produce a total inter-judge score,and combining events so that we could see the utilityscores given across sub-events.
We ordered thesentences by ranking these cross-event inter-judge utilityscores in descending order and omitting duplicates, toproduce the ten and twenty percent extracts.Algorithm 3 - Round Robin (RR): Thisalgorithm was produced by summing the data across alljudges to produce a total inter-judge score and keepingsub-events distinct, to see the inter-judge utility scoresgiven to sub-events.
We ordered the sentences byranking the inter-judge utility scores in descending orderwithin each sub-event.
We then chose the top sentencefrom each sub-event (one through ten), the secondhighest sentence from each sub-event, and so on,omitting duplicates, until we had produced the ten andtwenty percent extracts.In this manner, we created thirty-six sub-event-based summary extracts ?
six clusters, three algorithms,two compression rates ?
which we then analyzed.The Sum of All Scores algorithm most closelyreplicates a centroid-based summary by combining theten sub-event scores into one pan-topic score for eachsentence.
Further, the Sum of All Scores algorithm is thesub-event algorithm most likely to pick sentences with ahigh ?general relevance,?
which is what the baselinerelative utility scores are meant to capture.
In contrast,the Highest Score Anywhere algorithm maintains thestructure of the sub-event breakdown, preferring thehighest score in any sub-event.
Likewise, the RoundRobin algorithm maintains the sub-event breakdown, butrather than preferring the highest score in any event, itselects the highest score from each sub-event, serially;this algorithm most closely resembles the Lead-basedautomatic summarizer, and is at the heart ofHatzivassiloglou et al?s (2001) SimFinder.5.2 Automatic Multi-DocumentSummariesThe three automatic summarization methods that weused in our comparison have already been established.We compared our manual summaries to these establishedautomatic multiple-document summarization methods:Centroid-based (MEAD), Lead-based and Random.MEAD:  First, we produced summariesusing the MEAD system.
MEAD produces a centroid(vector) for all of the sentences and then selects thosesentences which are closest to the centroid.
MEADmeasures similarity with the cosine measurement andTF*IDF weighting.
Mead also adjusts a sentence?s scorebased on its length, its position in the original documentand its similarity to sentences already selected for theextract.
(Radev et al 2000).Lead-Based: We also produced summariesby the Lead-based method.
This method involvesassigning the highest score to the first sentence in eacharticle, then the second sentence in each article, and soon.Random: We created summaries with everypossible combination of sentences for each summarylength.
This allowed us to compute the average randomrelative utility score.6.
Relative UtilityFollowing (Radev et al, 2000), we used relative utility asour metric.
Relative utility was chosen for advantages ina couple of areas.Relative utility is a metric which measuressentence relevance.
It allows us to distinguish the degreeof importance between sentences, providing a moreflexible model for evaluating sentence utility (Radev etal., 2000).
Studies involving sentence extraction haveoften been predicated upon determining the usefulness ofsentences as either useful or non-useful (Allan et al2001b).
However, determining the usefulness ofsentences is more complex than a simple a binary choicecan account for.
We employ a relative utility metric toaccount for subtleties in determining the saliency ofsentences.Another advantage of the relative utility metricis that, although human judges have often agree verylittle on which sentences belong in a summary, they tendto agree on how important sentences are to a topic orevent; thus, relative utility makes it possible to leveragethis agreement.To calculate relative utility, we had humansubjects assign a score to each sentence in a corpus ofarticles.
The score reflects the subject?s perception of asentence?s relevance to the overall topic of the corpus.The scale our judges were instructed to use ranged fromzero to ten.
A score of zero indicated that the sentencewas irrelevant; whereas a score of ten indicated that thesentence was crucial to the understanding of the topic.So that judges?
scores can be fairly compared, eachjudge?s scores are normalized by the highest score andlowest score which that judge gives any sentence.Relative utility is determined by first addingtogether the utility scores given to each sentence by eachjudge.
Each sentence in a summary is then awarded thetotal of the judges?
scores for that sentence.
Finally, thesummary?s total score is divided by the best possiblescore, given the size of the summary.For example, let us assume that a cluster hasthree sentences (A, B and C) which have been judged bytwo judges in the following way: A 10, 9, B 8, 6 and C 6,5.
That is, judge 1 gives sentence A a 10, while judge 2gives sentence A a 9, and so on.
In the first step, we sumthe judges?
scores for each sentence, yielding (A 19, B14, C 11).
If a summarizer has to pick a 2 sentencesummary, and it picks A and C, its utility score is 30.We then divide this score by the best possible 2 sentencesummary, in this case A and B, whose utility is 33,yielding a final relative utility of .91.7.
Extract CreationSummaries can be created by abstracting or extracting[Mani, 2001].
For purposes of comparison with MEAD,an extractive summarizer, we used an extractive methodto create all six summary types: sum of all scores, highestscore anywhere, round robin, MEAD, lead-based, andrandom.7.1 ClustersEach of the summarization methods wasemployed at both ten and twenty percent compressionrates.
We used the summaries thus produced to considerhow compression rates could influence the effectivenessof the six summarization methods.
In our firstexperiment, we additionally looked at varyingcombinations of the five articles, such that we examinedthe corpus in six clusters, as shown in the figure below.We selected these article combinations to maximize thediversity of sources in each cluster, and to achieve avariable number of articles in a cluster.Combination 1) articles 30 + 41 + 81 + 87 + 97Combination 2) articles 30 + 41 + 81Combination 3) articles 41 + 81 + 87Combination 4) articles 81 + 87 + 97Combination 5) articles 87 + 97 + 30Combination 6) articles 97+ 30 + 41Figure 1.
Article clusters.10%      20%HSA SAS RR MEAD Lead Rand HAS SAS RR MEAD Lead RandCluster 1 0.641 0.686 0.717 0.617 0.795 0.480 0.542 0.745 0.683 0.621 0.722 0.521Cluster 2 0.629 0.739 0.716 0.629 0.800 0.459 0.637 0.786 0.659 0.623 0.741 0.490Cluster 3 0.568 0.698 0.544 0.672 0.701 0.435 0.572 0.735 0.631 0.647 0.629 0.470Cluster 4 0.406 0.669 0.651 0.662 0.714 0.489 0.539 0.722 0.596 0.653 0.738 0.521Cluster 5 0.646 0.675 0.698 0.604 0.797 0.549 0.598 0.739 0.733 0.631 0.749 0.575Cluster 6 0.622 0.698 0.693 0.595 0.880 0.508 0.623 0.762 0.717 0.552 0.817 0.536Average = 0.585 0.694 0.670 0.630 0.781 0.487 0.585 0.748 0.670 0.621 0.733 0.519Table 3.
Results: Best performing algorithm at each cluster/compression rate shown in bold.8.
Results from the first experimentSome of our results met our expectations, while otherssurprised us (see Table 3).
The Sum of All Scores manualalgorithm produces the best summaries at the twentypercent compression rate.
At the ten percent compressionrate, data shows Lead-based summaries performing best,with the Sum of All Scores algorithm coming in rightbehind.
Mead scores in the mid-range as expected, forboth compression rates, just behind the Round RobinAlgorithm.
In contrast, the random method leads in lowscores, with the Highest Score Anywhere algorithmcoming in only slightly higher.
Random sets the lowerbound.
Here, we discuss the details of our findings andtheir significance in more detail.8.1 Manual AlgorithmsBoth the Sum of All Scores, and Round Robin algorithmsperformed better than MEAD, with the highest scoreanywhere algorithm performing less well.
This result isreasonable, based upon the characteristics of thealgorithms.
Algorithm 2 (SAS), the best performer amongthe manual summaries, used the sum of all scores acrossevents and judges; thus, it tapped into which sentenceswere most popular overall.
Algorithm 3 (RR), also betterthan MEAD, used a round robin technique, which,similarly to the Lead-based results, tapped into thepyramid quality of news journalism.
Algorithm 1 (HSA),poorest performer second to Random, used the highestscore in any event by inter-judge score; its weakness wasin negating both the benefits of the pyramid structure ofthe judges?
sentence rankings, as well as the popularity ofsentences across events.8.2 Compression RateFor extracts at the ten percent compression rate, Lead-based sets the upper, and random the lower, bound.However, the Sum of All Scores algorithm performedbetter at the twenty percent compression rate, beatingLead-based for best summaries.
Each method producedbetter summaries overall at ten percent compression rate,except for Algorithm 2, which performed better at thetwenty percent compression rate.We believe that SAS performed better at thetwenty percent compression rate as a result of twocharacteristics: as the sum of scores across sub-events, thisalgorithm preferred both sentences that received higherscores, as well as sentences which were highly rankedmost frequently.
Therefore, it is weighted toward thosesentences that carry information essential to several sub-events.
Because of these sentences?
relevancy to morethan one sub-event, they are most likely to be important tothe majority of readers, regardless of the user?s particularinformation task.
This can also be seen as popularityweighting, with those sentences getting the most and bestscores from judges producing the most useful summaries.The patterns uncovered by this result should be leveragedfor future improvements to automatic summarizers.8.3 Lead-Based SummariesWe were not extremely surprised to find that Lead-basedsummaries produced better summaries at the 10%summary rate.
This result may be explained by thepyramid structure of news journalism, which, in a sense,pre-ranks document sentences in order of importance, inorder to convey the most critical information first.
As ourcorpus was comprised entirely of news articles, this effectcould be exaggerated in our results.
As expected, though,the Random summarizer set the lower bound.8.4 Manual Summaries and MEADMost significantly, among the mid-range performers, thedata demonstrates what we expected to find:  Two of thethree new sub-event-based algorithms perform better thanMEAD.
Identifying sub-events in news topic coverage isone method that we have shown can be utilized to helpcreate better summaries.9.
Automatic Clustering and ExtractionIn our second experiment, we were interested to see howthe different strategies would work with a simpleclustering-based multi-document summarizer.
We did notexpect our clustering algorithm to neatly partition the dataaccording to the subevents we identified in our firstexperiment, but we did want to see if our findings aboutSAS would hold true for automatically partitioned data.And so we turned to sentence clustering.
While Boros etal.
(2001) report poor performance but some promise tothis method, Hatzivassiloglou et al (2001) have exploitedclustering with very good results in SimFinder.
Both relyon the RR method, although SimFinder considers severalother important factors in sentence selection.9.1 Automatic ClusteringBecause of the vast number of variables associated withdesigning a cluster-based summarization algorithm, wechose to limit our system so that we could focus on RR,HSA and SAS.
To give a sense of our performance, wealso ran a purely centroid-based summarization algorithm.We used K-means clustering, and obtainedresults for K = 2-20, at both the 10% and 20% summarylevels.
By this process, we created K clusters, seeded themas discussed below, and then for each sentence, we foundthat cluster to which the sentence was closest.
After fillingthe clusters, we checked again to see if each sentence wasin its best cluster.
We kept doing this until equilibriumwas reached (usually no more than 6 cycles).For our similarity metric we used the cosinemeasure with inverse document frequency (IDF), inversesentence frequency (ISF) (following Neto et al (2000) andno term-weighting.
We ran all of these permutationstwice, once ignoring sentences with 9 words or fewer (as isMEAD?s default) and once ignoring sentences with 2words or just 1.
We did not use stop words, stemming, orsyntactic parsing.
Further, we did not factor in the locationof the sentences in their original documents, although bothMEAD and SimFinder do this.Initially, we used a method of randomly seedingthe clusters, but we found this method extremely unstable.We then devised the following method: 1) for the firstcluster, find the sentence which is closest to the centroid ofthe document cluster, 2) for each sentence after that, findthe sentence which is maximally different from thosesentences already picked as seeds.9.2 Automatic ExtractionAfter creating the clusters by this method, we extractedsentences with the same three methods of interest, HSA,SAS, and RR.
For this experiment, we also added asimple Centroid policy.
Under this policy, a centroidvector was created for all of the sentences, and then foreach sentence the cosine measure was computed againstthe centroid.
The sentences were then sorted by theircosine scores with the centroid.
The top 10% or 20% wereselected for the summary.For all policies, the extraction algorithm wouldnot select a sentence which had a cosine of 0.99 or higherwith any sentence already in the summary.
Forcomparison, MEAD?s default is 0.7.
In the future, wewould like to study the effect of this parameter oninformation diversity.10.
Results for Automatic ClusteringIn Table 4, we report our findings from the secondexperiment.
This table presents the average of theperformances across all of the clustering options (2clusters to 20 clusters) for the specified parameters.
Ingeneral for a 10% summary, the SAS method outperformsthe other methods, leading Centroid by only a smallamount.
At the 20% level, the Centroid policy beats allother algorithms, although SAS with ISF and a 2-wordsentence minimum comes close.Some other interesting findings emerge from thistable as well, namely term-weighting seems beneficial forall methods except for HSA, and ISF seems generallymore beneficial for SAS and Centroid than for RR orHSA.SAS RR HSA Centroid SAS RR HSA Centroidmin.
2 word IDF 0.602 0.560 0.481 0.546 0.639 0.570 0.533 0.617min.
2 word ISF 0.672 0.485 0.453 0.669 0.650 0.520 0.522 0.656min.
2 word none 0.531 0.550 0.528 0.515 0.581 0.557 0.576 0.588min.
9 word IDF 0.608 0.488 0.472 0.546 0.634 0.535 0.523 0.616min.
9 word ISF 0.609 0.501 0.460 0.670 0.630 0.529 0.525 0.656min.
9 word none 0.528 0.511 0.498 0.517 0.588 0.558 0.562 0.58210% 20%Table 4: Results from our automatic, cluster-based summarizerTable 4 is unable to capture, however, themarked variation in results depending on how manyclusters were initially selected.
In Table 5, we present ourfindings for the overall best parameters.
As can be seen,SAS is the most common policy.
In fact, SAS appears inthe top 22 out 25 combinations at the 10% level and 20 outof 25 at the 20% compression level.Top 10 performers, 10% summary Top 10 performers, 20% summary# clusters ISF/IDF min.
sent.
length policy rel.
util.
# clusters ISF/IDF min.
sent.
length policy rel.
util.15 ISF 2 SAS 0.718 4 ISF 2 SAS 0.68616 ISF 2 SAS 0.711 3 ISF 2 SAS 0.68214 ISF 2 SAS 0.710 2 ISF 2 SAS 0.68120 ISF 2 SAS 0.705 2 ISF 9 RR 0.66913 ISF 2 SAS 0.704 3 ISF 9 HSA 0.66517 ISF 2 SAS 0.704 5 ISF 2 SAS 0.66511 IDF 9 SAS 0.684 2 ISF 9 HSA 0.6648 IDF 9 SAS 0.681 7 ISF 2 SAS 0.6617 ISF 2 SAS 0.679 9 IDF 9 SAS 0.66019 ISF 2 SAS 0.678 Na ISF 9 CENTROID 0.656Table 5: Top 10 parameters for the both rates of summarizationTables 4 and Tables 5, taken together, suggestthat SAS should be leveraged to improve performanceover the pure centroid method.
More work needs to bedone to determine the appropriate number of clusters tobegin with, but it is interesting that there appears to be aninverse relationship, namely, the smaller summary seemsto benefit from small, tightly packed clusters, while thelarger summary benefits from a few noisy clusters.11.
ConclusionsWhile the Lead-based policy from our first experiment stilloutperforms all of our automatic cluster-based summariesat the 10% and 20% levels, our findings about SAS areimportant for future efforts to summarize by partitioning.As discussed, the pyramid structure of news articles mayhave boosted the scores of the lead-based policy.
Inapplications of summarizers, where the information is notpresorted, we believe that clustering and then extractionwith SAS could offer the best results.We conclude that multi-document summarizationis improved by two specific elements.
Firstly, taking intoaccount varying degrees of relevancy, as opposed to apolarized relevant/non-relevant metric.
Secondly,recognizing the sub-events that comprise a single newsevent is essential.12.
Future WorkIn future work, we see four areas for improvement.
Wewould like to improve our simple cluster-based algorithm.Hatzivassiloglou et al (2001) have shown several ways ofdoing this.
Second, we would like to have human judgesevaluate the final summaries and give scores based on howwell the summary captures the most relevant parts of thedocument cluster and how well the summary avoidsrepetition.
This would allow us to see how effective theRU method is as well as how well our summarizer isfunctioning.
Third, we would like to run a machinelearning algorithm on a number of different and variedclusters to find which parameter settings work best foreach type of cluster.
We suspect that the optimal numberof original clusters, and the choice of ISF or IDF, could bedetermined by the amount of redundancy in the cluster andthe desired size of the extract, but more work remains to bedone on this.
Finally, we need to test the best clusteringmethod against other methods -- centroid-based, MMR,lexical-chain, key-word to name a few.12.
AcknowledgementsThis work was partially supported by the National ScienceFoundation's Information Technology Research program(ITR) under grant IIS-0082884.
Our thanks go to theanonymous reviewers for their very helpful comments.The version of MEAD that we used wasdeveloped at the Johns Hopkins summer workshop in2001 under the direction of Dragomir Radev and laterupgraded at the University of Michigan.
We want to thankthe following individuals for their work on MEAD: SashaBlair-Goldensohn, Simone Teufel, Arda Celebi, Wai Lam,Hong Qi, John Blitzer, Horacio Saggion, Elliott Drabek,Danyu Liu, Michael Topper, and Adam Winkel.13.
References[1] Allan, J. et al, 1998.
?On-line New Event Detectionand Tracking.?
In Proceedings of the 21st annualinternational ACM SIGIR conference on Research anddevelopment in information retrieval.
Melbourne,Australia.
[2] Allan, J. et al, 2001a.
?Temporal summaries of newstopics.?
In Proceedings of the 24th annual internationalACM SIGIR conference on Research and development ininformation retrieval.
[3] Allan, J. et al, 2001b.
?Topic models for summarizingnovelty.?
ARDA Workshop on Language Modeling andInformation Retrieval.
Pittsburgh, Pennsylvania.
[4] Boros, E. et al  2001.
?A Clustering Based Approachto Creating Multi-Document Summaries.?
In Proceedings ofthe 24th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval, NewOrleans, LA, 2001.
[5] Carbonell, J. and J.G.
Goldstein, 1998.
?The use ofMMR, diversity-based reranking for reordering documentsand producing summaries.?
In Proceedings of the 21stannual international ACM SIGIR conference on Researchand development in information retrieval.
Melbourne,Australia.
[6] Goldstein, J.G., 1999.
?Automatic text summarizationof multiple documents.?
Carnegie Mellon University.
[7] Hatzivassiloglou et al, 2001.
?SimFinder: A FlexibleClustering Tool for Summarization.?
NAACL, Workshopon Automatic Summarization.
Pittsburgh, PA.[8] Mani, I., 2001.
?Automatic summarization.?
NaturalLanguage Processing, ed.
Ruslan Mitkov.
Philadelphia,PA: John Benjamins Publishing.
[9] Marcu, D., 2000.
The theory and practice of discourseparsing and summarization.
Cambridge, MA: MIT Press.
[10] McKeown, K. and J. Klavans, V. Hatzivassiloglou, R.Barzilay, E. Eskin, 1999.
?Towards multidocumentsummarization by reformulation: Progress and prospects.
?In Proceedings of AAAI-99, Orlando, Fl., pp.
453-60.
[11] Neto, Joel et al, 2000.
?Document Clustering andText Summarization.?
In N. Mackin, editor, Proc.
4thInternational Conference Practical Applications ofKnowledge Discovery and Data Mining (PADD-2000),pages 41-55, London, January.
The Practical ApplicationCompany.
[12] Radev D., H. Jing and M. Budzikowska, 2000.?Centroid-based summarization of multiple documents:sentence extraction, utility-based evaluation, and userstudies.?
ANLP/NAACL Workshop on Summarization.Seattle, WA.
[13] Radev, D., S. Blair-Goldensohn and Z. Zhang, 2001.?Experiments in single and multi-document summarizationusing MEAD.?
First Document UnderstandingConference.
New Orleans, LA.
