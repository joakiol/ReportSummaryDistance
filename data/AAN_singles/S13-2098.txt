Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 585?591, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsCNGL: Grading Student Answers by Acts of TranslationErgun Bic?iciCentre for Next Generation Localisation,Dublin City University, Dublin, Ireland.ebicici@computing.dcu.ieJosef van GenabithCentre for Next Generation Localisation,Dublin City University, Dublin, Ireland.josef@computing.dcu.ieAbstractWe invent referential translation machines(RTMs), a computational model for identify-ing the translation acts between any two datasets with respect to a reference corpus se-lected in the same domain, which can be usedfor automatically grading student answers.RTMs make quality and semantic similarityjudgments possible by using retrieved rele-vant training data as interpretants for reach-ing shared semantics.
An MTPP (machinetranslation performance predictor) model de-rives features measuring the closeness of thetest sentences to the training data, the diffi-culty of translating them, and the presence ofacts of translation involved.
We view questionanswering as translation from the question tothe answer, from the question to the referenceanswer, from the answer to the reference an-swer, or from the question and the answer tothe reference answer.
Each view is modeledby an RTM model, giving us a new perspectiveon the ternary relationship between the ques-tion, the answer, and the reference answer.
Weshow that all RTM models contribute and aprediction model based on all four perspec-tives performs the best.
Our prediction modelis the 2nd best system on some tasks accordingto the official results of the Student ResponseAnalysis (SRA 2013) challenge.1 Automatically Grading Student AnswersWe introduce a fully automated student answergrader that performs well in the student responseanalysis (SRA) task (Dzikovska et al 2013) and es-pecially well in tasks with unseen answers.
Auto-matic grading can be used for assessing the level ofcompetency for students and estimating the requiredtutoring effort in e-learning platforms.
It can alsobe used to adapt questions according to the averagestudent performance.
Low scored topics can be dis-cussed further in classrooms, enhancing the overallcoverage of the course material.The quality estimation task (QET) (Callison-Burch et al 2012) aims to develop quality indica-tors for translations at the sentence-level and pre-dictors without access to the reference.
Bicici etal.
(2013) develop a top performing machine transla-tion performance predictor (MTPP), which uses ma-chine learning models over features measuring howwell the test set matches the training set relying onextrinsic and language independent features.The student response analysis (SRA)task (Dzikovska et al 2013) addresses the fol-lowing problem.
Given a question, a known correctreference answer, and a student answer, assess thecorrectness of the student?s answer.
The studentanswers are categorized as correct, partially correctincomplete, contradictory, irrelevant, or non do-main, in the 5-way task; as correct, contradictory,or incorrect in the 3-way task; and as correct orincorrect in the 2-way task.The student answer correctness prediction prob-lem involves finding a function f approximating thestudent answer correctness given the question (Q),the answer (A), and the reference answer (R):f(Q,A,R) ?
q(A,R).
(1)We approach f as a supervised learning problemwith (Q, A, R, q(A,R)) tuples being the training585data and q(A,R) being the target correctness score.We model the problem as a translation task whereone possible interpretation is translating Q (sourceto translate, S) to R (target translation, T) and evalu-ating with A (as reference target, RT) (QRA).
Sincethe information appearing in the question may be re-peated in the reference answer or may be omitted inthe student answer, it also makes sense to concate-nate Q and A when translating to R (QARQA).
Weobtain 4 different perspectives on the ternary rela-tionship between Q, A, and R depending on how wemodel their relationship as an instance of translation:QAR : S = Q, T = A, RT = R.QRA : S = Q, T = R, RT = A.ARA : S = A, T = R, RT = A.QARQA : S = Q+A, T = R, RT = Q+A.2 The Machine Translation PerformancePredictor (MTPP)In machine translation (MT), pairs of source and tar-get sentences are used for training statistical MT(SMT) models.
SMT system performance is af-fected by the amount of training data used as wellas the closeness of the test set to the training set.MTPP (Bic?ici et al 2013) is a top performing ma-chine translation performance predictor, which usesmachine learning models over features measuringhow well the test set matches the training set to pre-dict the quality of a translation without using a ref-erence translation.
MTPP measures the coverage ofindividual test sentence features and syntactic struc-tures found in the training set and derives featurefunctions measuring the closeness of test sentencesto the available training data, the difficulty of trans-lating the sentence, and the presence of acts of trans-lation involved.Features for Translation ActsMTPP uses n-gram features defined over text orcommon cover link (CCL) (Seginer, 2007) struc-tures as the basic units of information over whichsimilarity calculations are made.
Unsupervisedparsing with CCL extracts links from base wordsto head words, which allow us to obtain structuresrepresenting the grammatical information instanti-ated in the training and test data.
Feature functionsuse statistics involving the training set and the testsentences to determine their closeness.
Since theyare language independent, MTPP allows quality es-timation to be performed extrinsically.
Categoriesfor the 283 features used are listed below and theirdetailed descriptions are presented in (Bic?ici et al2013) where the number of features are given in {#}.?
Coverage {110}: Measures the degree towhich the test features are found in the train-ing set for both S ({56}) and T ({54}).?
Synthetic Translation Performance {6}: Calcu-lates translation scores achievable according tothe n-gram coverage.?
Length {4}: Calculates the number of wordsand characters for S and T and their ratios.?
Feature Vector Similarity {16}: Calculates thesimilarities between vector representations.?
Perplexity {90}: Measures the fluency of thesentences according to language models (LM).We use both forward ({30}) and backward({15}) LM based features for S and T.?
Entropy {4}: Calculates the distributional sim-ilarity of test sentences to the training set.?
Retrieval Closeness {24}: Measures the de-gree to which sentences close to the test set arefound in the training set.?
Diversity {6}: Measures the diversity of co-occurring features in the training set.?
IBM1 Translation Probability {16}: Calculatesthe translation probability of test sentences us-ing the training set (Brown et al 1993).?
Minimum Bayes Retrieval Risk {4}: Calculatesthe translation probability for the translationhaving the minimum Bayes risk among the re-trieved training instances.?
Sentence Translation Performance {3}: Calcu-lates translation scores obtained according toq(T,R) using BLEU (Papineni et al 2002),NIST (Doddington, 2002), or F1 (Bic?ici andYuret, 2011b) for q.3 Referential Translation Machine (RTM)Referential translation machines (RTMs) we de-velop provide a computational model for quality andsemantic similarity judgments using retrieval of rel-evant training data (Bic?ici and Yuret, 2011a; Bic?ici,2011) as interpretants for reaching shared seman-tics (Bic?ici, 2008).
We show that RTM achieves586very good performance in judging the semantic sim-ilarity of sentences (Bic?ici and van Genabith, 2013)and we can also use RTM to automatically assessthe correctness of student answers to obtain betterresults than the baselines proposed by (Dzikovska etal., 2012), which achieve the best performance onsome tasks (Dzikovska et al 2013).RTM is a computational model for identifying theacts of translation for translating between any giventwo data sets with respect to a reference corpus se-lected in the same domain.
RTM can be used forautomatically grading student answers.
An RTMmodel is based on the selection of common train-ing data relevant and close to both the training setand the test set where the selected relevant set ofinstances are called the interpretants.
Interpretantsallow shared semantics to be possible by behavingas a reference point for similarity judgments andproviding the context.
In semiotics, an interpretantI interprets the signs used to refer to the real ob-jects (Bic?ici, 2008).
RTMs provide a model for com-putational semantics using interpretants as a refer-ence according to which semantic judgments withtranslation acts are made.
Each RTM model is a datatranslation model between the instances in the train-ing set and the test set.
We use the FDA (Feature De-cay Algorithms) instance selection model for select-ing the interpretants (Bic?ici and Yuret, 2011a) from agiven corpus, which can be monolingual when mod-eling paraphrasing acts, in which case the MTPPmodel is built using the interpretants themselves asboth the source and the target side of the parallel cor-pus.
RTMs map the training and test data to a spacewhere translation acts can be identified.
We viewthat acts of translation are ubiquitously used duringcommunication:Every act of communication is an act oftranslation (Bliss, 2012).Translation need not be between different languagesand paraphrasing or communication also containacts of translation.
When creating sentences, we useour background knowledge and translate informa-tion content according to the current context.Given a training set train, a test set test, andsome monolingual corpus C, preferably in the samedomain as the training and test sets, the RTM stepsare:1.
T = train ?
test.2.
select(T, C)?
I3.
MTPP(I,train)?
Ftrain4.
MTPP(I,test)?
FtestStep 2 selects the interpretants, I, relevant to theinstances in the combined training and test data.Steps 3 and 4 use I to map train and test toa new space where similarities between the transla-tion acts can be derived more easily.
RTM relies onthe representativeness of I as a medium for buildingtranslation models for translating between trainand test.Our encouraging results in the SRA task providesa greater understanding of the acts of translation weubiquitously use when communicating and how theycan be used to predict the performance of trans-lation, judging the semantic similarity of text, andevaluating the quality of student answers.
RTM andMTPP models are not data or language specific andtheir modeling power and good performance are ap-plicable across different domains and tasks.
RTMexpands the applicability of MTPP by making it fea-sible when making monolingual quality and simi-larity judgments and it enhances the computationalscalability by building models over smaller but morerelevant training data as interpretants.4 ExperimentsSRA involves the prediction on Beetle (studentinteractions when learning conceptual knowledgein the basic electricity and electronics domain)and SciEntsBank (science assessment questions)datasets.
SciEntsBank is harder due to contain-ing questions from multiple domains (Dzikovskaet al 2012).
SRA challenge results are eval-uated with the weighted average F1, Fw1 =1N?c?C NcF1(c) and the macro average F1, Fm1 =1|C|?c?C F1(c) (Dzikovska et al 2012).The lexical baseline system is based on measuresof lexical overlap using 4 features: the number ofoverlapping words, F1, Lesk (Lesk, 1986), and co-sine scores over the words when comparing A andR ({4}) and Q and R ({4}).
Lesk score is calculatedas: L(A,R) =?p?M |p|2/(|A||R|), where M con-tains the maximal overlapping phrases that match in587A and R and |p| is the length of a phrase 1.
This lex-ical baseline is highly competitive: no submissionperformed better in the 2-way Beetle unseen ques-tions task.4.1 RTM ModelsWe obtain CNGL results for the SRA task as fol-lows.
For each perspective described in Section 1,we build an RTM model.
Each RTM model viewsthe SRA task from a different perspective using the283 features extracted dependent on the interpre-tants using MTPP.
We extract the features both onthe training set of 4155 and the test set of 1258 (Q,A, R) sentence triples for the Beetle task and thetraining set of 5251 and the test set of 5835 (Q, A,R) sentence triples for the SciEntsBank task.
Theaddition of lexical overlap baseline features slightlyhelps.
We use the best reference answer if the refer-ence answer is not identified in the training set.The training corpus used is the English side ofan out-of-domain corpus on European parliamen-tary discussions, Europarl (Callison-Burch et al2012) 2, to which we also add the unique sentencesfrom R. In-domain corpora are likely to improve theperformance.
We do not perform any linguistic pro-cessing or use other external resources.
We use onlyextrinsic features, or features that are ignorant of anyinformation intrinsic to, and dependent on, a givenlanguage or domain.
We use the training corpus tobuild a 5-gram target LM.
We use ridge regression(RR) and support vector regression (SVR) with RBFkernel (Smola and Scho?lkopf, 2004).
Both of thesemodels learn a regression function using the featuresto estimate a numerical target value.
The parametersthat govern the behavior of RR and SVR are the reg-ularization ?
for RR and the C, , and ?
parametersfor SVR.
At testing time, the predictions are boundso as to have scores in the range [0, 1], [0, 2], or [0, 4]and rounded for finding the predicted category.4.2 Training ResultsTable 1 lists the 10-fold cross-validation (CV) re-sults on the training set for RR and SVR for dif-ferent RTM systems without the parameter op-timization.
As we combine different perspec-tives, the performance improves and we use the1http://search.cpan.org/dist/Text-Similarity/2We use WMT?13 corpora from www.statmt.org/wmt13/.QAR+QRA+ARA+QARQA system for our submis-sions using RR for run 1, SVR for run 2.
ARA per-forms the best among individual perspectives.
Eachadditional perspective adds another 283 features tothe representation.Fm1 / Fw1 Beetle SciEntsBankModel RR SVR RR SVRQAR .38/.49 .45/.57 .21/.30 .28/.36QRA .33/.50 .33/.53 .22/.31 .29/.42ARA .45/.54 .50/.60 .21/.30 .30/.38QARQA .35/.50 .40/.58 .20/.27 .27/.40QAR+ARA .47/.55 .49/.61 .26/.36 .32/.39QAR+ARA+QARQA .48/.57 .49/.62 .31/.38 .29/.40QAR+QRA+ARA+QARQA .48/.56 .48/.61 .31/.38 .29/.40Table 1: Performance on the training set without tuning.We perform tuning on a subset of the Beetleand SciEntsBank datasets separately after includingthe baseline lexical overlap features and optimizeagainst the performance evaluated withR2, the coef-ficient of determination.
SVR performance is givenin Table 2.
The CNGL system significantly outper-forms the lexical overlap baseline in all tasks forBeetle and in the 2-way task for SciEntsBank.
For3-way and 5-way, CNGL performs slightly better.Fm1 / Fw1 Beetle SciEntsBankSystem 2 3 5 2 3 5Lexical .74/.75 .53/.56 .46/.53 .61/.64 .43/.55 .29/.41CNGL .84/.84 .61/.63 .55/.63 .74/.75 .47/.56 .30/.41Table 2: Optimized SVR results vs. lexical overlap base-line on the training set for 2-way, 3-way, or 5-way tasks.4.3 SRA Challenge ResultsThe SRA task test set al contains instances that be-long to unseen questions (uQ) and unseen domains(uD), which make it harder to predict.
The train-ing data provided for the task correspond to learningwith unseen answers (uA).
Table 3 presents the SRAchallenge results containing the lexical overlap, ourCNGL SVR submission (RR is slightly worse), andthe maximum and mean results 3.According to the official results, CNGL SVR isthe 2nd best system based on 5-way evaluation (4th3Max is not the performance of the best performing systembut the maximum result obtained for each metric and subtask.588Fm1 / Fw1 Beetle SciEntsBankSystem uA uQ uA uQ uD2Lexical .80/.79 .74/.72 .64/.62 .65/.63 .66/.65CNGL .80/.81 .67/.68 .55/.57 .56/.58 .56/.57Mean .71/.72 .61/.62 .64/.66 .60/.62 .61/.63Max .84/.84 .72/.73 .77/.77 .74/.74 .70/.713Lexical .55/.58 .48/.50 .40/.52 .39/.52 .42/.55CNGL .57/.59 .45/.47 .33/.38 .31/.37 .31/.36Mean .54/.55 .41/.42 .48/.56 .39/.51 .39/.51Max .72/.73 .58/.60 .65/.71 .47/.63 .49/.625Lexical .42/.48 .41/.46 .30/.44 .26/.40 .25/.40CNGL .43/.55 .38/.47 .20/.27 .21/.30 .22/.29Mean .44/.51 .34/.40 .34/.46 .24/.38 .26/.37Max .62/.70 .55/.61 .48/.64 .31/.49 .38/.47Table 3: SRA challenge results: CNGL SVR submission,the lexical overlap baseline, and the maximum and meanresults for 2-way, 3-way, or 5-way tasks.
uA, uQ, and uDcorrespond to unseen answers, questions, and domains.result overall) and the 3rd best system based on 2-way and 3-way evaluation (5th result overall) on theuQ Beetle task.
The SVR model performs betterthan the lexical baseline and the mean result in theBeetle task but performs worse in the SciEntsBank.The lower performance is likely to be due to using anout-of-domain training corpus for building the RTMmodels and on the uQ and uD tasks, it may also bedue to optimizing on the uA task only.
The lowerperformance in SciEntsBank is also due to multiplequestion domains (Dzikovska et al 2012).SVR Beetle SciEntsBankFw1 2 3 5 2 3 5(a) QAR+ARA .86 .66 .64 .77 .56 .42(b) QAR+ARA+QARQA .86 .66 .65 .77 .57 .45(c) QAR+QRA+ARA+QARQA .85 .64 .63 .77 .58 .45Fm1 2 3 5 2 3 5(a) QAR+ARA .86 .64 .55 .76 .47 .34(b) QAR+ARA+QARQA .85 .64 .55 .76 .48 .36(c) QAR+QRA+ARA+QARQA .85 .62 .54 .76 .49 .35Table 4: Improved SVR performance on the training setwith tuning for 2-way, 3-way, or 5-way tasks.4.4 Improved RTM ModelsWe improve the RTM model with the expansion ofour representation by adding the following features:?
Character n-grams {4}: Calculates the cosinebetween the character n-grams (for n=2,3,4,5)obtained for S and T (Ba?r et al 2012).?
LIX {2}: Calculates the LIX readabilityscore (Wikipedia, 2013; Bjo?rnsson, 1968) forS and T. 4Table 4 lists the improved results on the training setafter tuning, which shows about 0.04 increase in allscores when compared with Table 1 and Table 2.Fm1 /Fw1 Beetle SciEntsBankModel uA uQ uA uQ uD2(a) .81/.82 .70/.71 .55/.57 .58/.58 .56/.57(b) .80/.81 .71/.72 .69/.70 .54/.56 .56/.58(c) .79/.79 .70/.71 .60/.59 .57/.58 .55/.573(a) .59/.61 .48/.49 .26/.34 .34/.40 .26/.32(b) .60/.62 .47/.48 .36/.43 .31/.38 .29/.34(c) .58/.60 .46/.48 .41/.48 .30/.39 .29/.345(a) .47/.56 .37/.45 .19/.22 .22/.33 .22/.29(b) .43/.56 .36/.45 .26/.37 .23/.33 .21/.30(c) .42/.52 .40/.48 .27/.39 .24/.33 .20/.30Table 5: Improved SVR results on the SRA task test set.Fm1 /Fw1 SciEntsBankModel uA uQ uD2(a) .56/.57 .54/.55 .53/.55(b) .57/.58 .53/.54 .56/.57(c) .57/.58 .55/.57 .57/.593(a) .36/.45 .33/.44 .39/.49(b) .35/.40 .36/.44 .39/.48(c) .37/.46 .36/.48 .40/.505(a) .24/.34 .23/.33 .26/.39(b) .24/.36 .25/.38 .26/.38(c) .24/.36 .21/.32 .28/.39Table 6: Improved TREE results on the SRA task test set.Table 5 presents the improved SVR results on theSRA task test set, which shows about 0.03 increasein all scores when compared with Table 3.
SVR be-comes the 2nd best system and 2nd best result in2-way evaluation and the 3rd best system from thetop based on 2-way and 3-way evaluation (5th resultoverall) on the uQ Beetle task.4LIX=AB + C100A , where A is the number of words, C iswords longer than 6 characters, B is words that start or end withany of ?.
?, ?
:?, ?!
?, ???
similar to (Hagstro?m, 2012).589We observe that decision tree regression (Hastieet al 2009) (TREE) generalizes to uQ and uD do-mains better than the RR or SVR models especiallyin the SciEntsBank corpus.
Table 6 presents TREEresults on the SRA SciEntsBank test set, whichshows significant increase in uQ and uD tasks whencompared with Table 5.5 ConclusionReferential translation machines provide a cleanand intuitive computational model for automaticallygrading student answers by measuring the acts oftranslation involved and achieve to be the 2nd bestsystem on some tasks in the SRA challenge.
RTMsmake quality and semantic similarity judgmentspossible based on the retrieval of relevant trainingdata as interpretants for reaching shared semantics.AcknowledgmentsThis work is supported in part by SFI (07/CE/I1142)as part of the Centre for Next Generation Locali-sation (www.cngl.ie) at Dublin City University andin part by the European Commission through theQTLaunchPad FP7 project (No: 296347).
We alsothank the SFI/HEA Irish Centre for High-End Com-puting (ICHEC) for the provision of computationalfacilities and support.ReferencesDaniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual simi-larity by combining multiple content similarity mea-sures.
In *SEM 2012: The First Joint Conferenceon Lexical and Computational Semantics ?
Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation (SemEval2012), pages 435?440, Montre?al, Canada, 7-8 June.Association for Computational Linguistics.Ergun Bic?ici and Josef van Genabith.
2013.
CNGL-CORE: Referential translation machines for measur-ing semantic similarity.
In *SEM 2013: The First JointConference on Lexical and Computational Semantics,Atlanta, Georgia, USA, 13-14 June.
Association forComputational Linguistics.Ergun Bic?ici and Deniz Yuret.
2011a.
Instance selec-tion for machine translation using feature decay al-gorithms.
In Proceedings of the Sixth Workshop onStatistical Machine Translation, pages 272?283, Edin-burgh, Scotland, July.
Association for ComputationalLinguistics.Ergun Bic?ici and Deniz Yuret.
2011b.
RegMT system formachine translation, system combination, and evalua-tion.
In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation, pages 323?329, Edin-burgh, Scotland, July.
Association for ComputationalLinguistics.Ergun Bic?ici, Declan Groves, and Josef van Genabith.2013.
Predicting sentence translation quality using ex-trinsic and language independent features.
MachineTranslation.Ergun Bic?ici.
2011.
The Regression Model of MachineTranslation.
Ph.D. thesis, Koc?
University.
Supervisor:Deniz Yuret.Ergun Bic?ici.
2008.
Consensus ontologies in sociallyinteracting multiagent systems.
Journal of Multiagentand Grid Systems.Carl Hugo Bjo?rnsson.
1968.
La?sbarhet.
Liber.Chris Bliss.
2012.
Comedy is transla-tion, February.
http://www.ted.com/talks/chris bliss comedy is translation.html.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montre?al, Canada, June.
Association for Compu-tational Linguistics.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, pages 138?145, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Myroslava O. Dzikovska, Rodney D. Nielsen, and ChrisBrew.
2012.
Towards effective tutorial feedbackfor explanation questions: A dataset and baselines.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 200?210, Montre?al, Canada, June.
Associationfor Computational Linguistics.Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.2013.
Semeval-2013 task 7: The joint student re-sponse analysis and 8th recognizing textual entailment590challenge.
In *SEM 2013: The First Joint Conferenceon Lexical and Computational Semantics, Atlanta,Georgia, USA, 13-14 June.
Association for Compu-tational Linguistics.Kenth Hagstro?m.
2012.
Swedish readability calcula-tor.
https://github.com/keha76/Swedish-Readability-Calculator.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.2009.
The Elements of Statistical Learning: DataMining, Inference and Prediction.
Springer-Verlag,2nd edition.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proceedings of the5th annual international conference on Systems docu-mentation, SIGDOC ?86, pages 24?26, New York, NY,USA.
ACM.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Yoav Seginer.
2007.
Learning Syntactic Structure.
Ph.D.thesis, Universiteit van Amsterdam.Alex J. Smola and Bernhard Scho?lkopf.
2004.
A tutorialon support vector regression.
Statistics and Comput-ing, 14(3):199?222, August.Wikipedia.
2013.
Lix.
http://en.wikipedia.org/wiki/LIX.591
