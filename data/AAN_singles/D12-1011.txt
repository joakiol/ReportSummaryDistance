Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 116?127, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLinking Named Entities to Any DatabaseAvirup Sil?Temple UniversityPhiladelphia, PAavi@temple.eduErnest Cronin?St.
Joseph?s UniversityPhiladelphia, PAernest.cronin@gmail.comPenghai NieSt.
Joseph?s UniversityPhiladelphia, PAnph87903@gmail.comYinfei YangSt.
Joseph?s UniversityPhiladelphia, PAyangyin7@gmail.comAna-Maria PopescuYahoo!
LabsSunnyvale, CAamp@yahoo-inc.comAlexander YatesTemple UniversityPhiladelphia, PAyates@temple.eduAbstractExisting techniques for disambiguating namedentities in text mostly focus on Wikipedia asa target catalog of entities.
Yet for manytypes of entities, such as restaurants andcult movies, relational databases exist thatcontain far more extensive information thanWikipedia.
This paper introduces a new task,called Open-Database Named-Entity Disam-biguation (Open-DB NED), in which a systemmust be able to resolve named entities to sym-bols in an arbitrary database, without requir-ing labeled data for each new database.
Weintroduce two techniques for Open-DB NED,one based on distant supervision and the otherbased on domain adaptation.
In experimentson two domains, one with poor coverage byWikipedia and the other with near-perfect cov-erage, our Open-DB NED strategies outper-form a state-of-the-art Wikipedia NED systemby over 25% in accuracy.1 IntroductionNamed-entity disambiguation (NED) is the task oflinking names mentioned in text with an establishedcatalog of entities (Bunescu and Pasca, 2006; Rati-nov et al2011).
It is a vital first step for se-mantic understanding of text, such as in groundedsemantic parsing (Kwiatkowski et al2011), aswell as for information retrieval tasks like personname search (Chen and Martin, 2007; Mann andYarowsky, 2003).NED requires a catalog of symbols, called refer-ents, to which named-entities will be resolved.
MostNED systems today use Wikipedia as the catalog ofreferents, but exclusive focus on Wikipedia as a tar-get for NED systems has significant drawbacks: de-spite its breadth, Wikipedia still does not contain allor even most real-world entities mentioned in text.As one example, it has poor coverage of entities thatare mostly important in a small geographical region,such as hotels and restaurants, which are widely dis-cussed on the Web.
57% of the named-entities inthe Text Analysis Conference?s (TAC) 2009 entitylinking task refer to an entity that does not appearin Wikipedia (McNamee et al2009).
Wikipedia isclearly a highly valuable resource, but it should notbe thought of as the only one.Instead of relying solely on Wikipedia, we pro-pose a novel approach to NED, which we refer toas Open-DB NED: the task is to resolve an en-tity to Wikipedia or to any relational database thatmeets mild conditions about the format of the data,described below.
Leveraging structured, relationaldata should allow systems to achieve strong accu-racy, as with domain-specific or database-specificNED techniques like Hoffart et al NED systemfor YAGO (Hoffart et al2011).
And because ofthe availability of huge numbers of databases onthe Web, many for specialized domains, a success-ful system for this task will cover entities that aWikipedia NED or database-specific system cannot.We investigate two complementary learningstrategies for Open-DB NED, both of which signifi-cantly relax the assumptions of traditional NED sys-tems.
The first strategy, a distant supervision ap-proach, uses the relational information in a givendatabase and a large corpus of unlabeled text tolearn a database-specific model.
The second strat-116egy, a domain adaptation approach, assumes a sin-gle source database that has accompanying labeleddata.
Classifiers in this setting must learn a modelthat transfers from the source database to any newdatabase, without requiring new training data for thenew database.
Experiments show that both strategiesoutperform a state-of-the-art Wikipedia NED sys-tem by wide margins without requiring any labeleddata from the test domain, highlighting the signifi-cant advantage of having domain-specific relationaldata.The next section contrasts Open-DB NED withprevious work.
Section 3 formalizes the task.
Sec-tions 4 and 5 present our distant supervision strategyand domain-adaptation strategy, respectively.
Sec-tion 6 introduces a technique that is a hybrid of thetwo learning strategies.
Section 7 describes our ex-periments, and Section 8 concludes.2 Previous WorkAs mentioned above, restricting the catalog of ref-erents to Wikipedia, as most recent NED systemsdo (Bunescu and Pasca, 2006; Mihalcea and Cso-mai, 2007; Fader et al2009; Han and Zhao, 2009;Kulkarni et al2009; Ratinov et al2011), can re-strict the coverage of the system.
Zhou et al2010)estimate that 23% of names in Yahoo!
news arti-cles have no referent in Wikipedia, and Cucerzan(2007) estimates the rate at 16% in MSNBC newsarticles.
There is reason to suspect that these esti-mates are on the low side, however, as news tends tocover popular entities, which are most likely to ap-pear in Wikipedia; the mentions in TAC?s 2009 en-tity linking task are drawn from both newswire andblogs, and have a far higher rate (57%) of missingWikipedia entries.
Lin et al2012) find that 33% ofmentions in a corpus of 500 million Web documentscannot be linked to Wikipedia.NED systems that are focused on specific do-mains (or verticals) greatly benefit from reposito-ries of domain-specific knowledge, only a subsetof which may be found in Wikipedia.
For exam-ple, Pantel and Fuxman (2011) use a query-clickgraph to resolve names in search engine queries to alarge product catalog from a commercial search en-gine, while Dalvi et al2009; 2012) focus on movieand restaurant databases.
Bellare and McCallum(2009) use the sequence information available in ci-tation text to link author, title, and venue names to apublication database.
Open-DB NED systems workon any database, so they can serve as baselines fordomain-specific NED tasks, as well as provide dis-ambiguation for domains where no domain-specificNED system exists.Numerous previous studies have considered dis-tant or weak supervision from a single relationaldatabase as an alternative to manual supervision forinformation extraction (Hoffmann et al2011; Weldet al2009; Bellare and McCallum, 2007; Bunescuand Mooney, 2007; Mintz et al2009; Riedel et al2010; Yao et al2010).
In contrast to these sys-tems, our distant supervision NED system providesa meta-algorithm for generating an NED system forany database and any entity type.Existing domain adaptation or transfer learningapproaches are inappropriate for the Open-DB NEDtask, either because they require labeled data in boththe source and target domains (Daume?
III et al2010; Ben-David et al2010), or because they lever-age some notion of distributional similarity betweenwords in the source and target domains (Blitzer etal., 2006; Huang and Yates, 2009), which does notapply to the database symbols across the two do-mains.
Instead, our domain adaptation techniqueuses domain-independent features of relational data,which apply regardless of the actual contents of thedatabase, as explained further below.3 The Open-DB NED Problem andAssumptions3.1 Problem FormulationA mention is an occurrence of a named-entityin a document.
Formally, a mention m =(d, start, end) is a triple consisting of a documentd, as well as a start and end position for the men-tion within the document.
We say that d is thecontext of m. A relational database is a 2-tuple(S,R).
Here, S is a set of symbols for constants,attributes, and relations in the database, and R ={r1, .
.
.
, rn} is a set of relation instances of the formri = {(c1,1, .
.
.
, c1,ki), .
.
.
, (cni,1, .
.
.
, cni,ki)},where each cj is taken from S, ki is the arity of re-lation ri and ni is the number of known instancesof ri.
We will write example database symbols in117movieid title year1 Next Door 19752 Next Door 20053 Next Door 20084 Next Door 20085 Next Door 2010?
?
?actorid name1 Nicole Kreux2 Richard Ryan3 Kristoffer Joner4 Lee Perkins5 Carla Valentine?
?acted_inmovie_id actor_id role5 1 Evelyn5 2 Bruce2 3 John1 4 Kid3 5 Elana?
?
?playerid name height position1 Carlos Lee 6?2?
LF2 Rob Bironas 6?0?
K3 Chris Johnson 6?3?
3B4 Chris Johnson 5?11?
RB5 Chris Johnson 6?1?
DB?
?
?teamid name1 San Diego Padres2 Houston Texans3 Tennessee Titans4 Oakland Raiders5 Houston Astros?
?plays_forplayer_id team_id4 35 23 51 52 3?
?Figure 1: Example movie database (above) and sportsdatabase (below) in BCNF.teletype, and mentions in ?quotations.?
For aparticular database DB, we refer to its componentsas DB.S and DB.R.
For a set of databases D, de-fine the set of referents as SD = (?DB?DDB.S)?
{OOD}, where OOD is a special symbol indicat-ing something that is ?out of database?, or not foundin any of the databases in D.Given a corpus C, a set of mentions M that oc-cur in C, and a set of databases D, the Open-DBNED task is to produce a function f : M ?
SD,which identifies an appropriate target symbol fromone of the databases in D, or determines that themention is OOD.
Note that this problem formula-tion assumes no labeled data.
This is significantlymore challenging than traditional NED settings, butallows the system to generalize easily to any newdatabase.
In the domain adaptation section below,we relax this condition somewhat, to allow labeleddata for a small number of initial databases; the sys-tem must then transfer what it learns from the la-beled domains to any new database.
Also note thatthe focus for this paper is disambiguation; we as-sume that the set of mentions are correctly demar-cated in the input text.
Previous systems, such asLex (Downey et al2007), have investigated the taskof finding correct named-entity boundaries in text.3.2 AssumptionsTo allow our systems to handle arbitrary databases,we need to make some assumptions about a standardformat for the data.
We will assume that databasesare provided in a particular form, called Boyce-CoddNormal Form (BCNF) (Silberschatz et al2010).A relational schema is said to be in BCNF whenall redundancy based on functional dependency hasbeen removed, although other types of redundancymay still exist.
Formally, a schema R is said tobe in BCNF with respect to a set of functional de-pendencies F if for every one of the dependencies(X ?
Y ) ?
F , either1.
Y ?
X , meaning this is a trivial functional de-pendency, or2.
X is a superkey, meaning that X is a set of at-tributes that together define a unique ID for therelation.In practice, this is a relatively safe assumption asdatabase designers often aim for even stricter normalforms.
For databases not in BCNF, such as tablesextracted from Web pages, standard algorithms ex-ist for converting them into BCNF, given appropri-ate functional dependencies, although there are setsof functional dependencies for which BCNF is notachievable.
Figure 1 shows two example databasesin BCNF.
We use these tables as examples through-out the paper.We will additionally assume that all attributes, in-cluding names and nicknames, of entities that arecovered by the database are treated as functional de-pendencies of the entity.
Again, in practice, thisis a fairly safe assumption as this is part of gooddatabase design, but if a database does not con-form to this, then there will be some entities in thedatabase that our algorithms cannot resolve to.
Thisassumption implies that it is enough to use the set ofsuperkeys for relations as the set of possible refer-ents; our algorithms make use of this fact.Finally, we will assume the existence of a func-tion ?
(s, t) which indicates whether the text t is avalid surface form of database symbol s. Our exper-iments in Section 7.3 explore several possible simpledefinitions for this function.4 A Distant Supervision Strategy forOpen-DB NEDOur first approach to the Open-DB NED problem re-lies on the fact that, while many mentions are indeedambiguous and difficult to resolve correctly, most118mentions have only a very small number of possi-ble referents in a given database.
?Chris Johnson?is the name of doubtless thousands of people, butfor articles that are reasonably well-aligned with oursports database, most of the time the name will referto just three different people.
Most sports names arein fact less ambiguous still.
Thus, taking a corpus ofunlabeled sports articles, we use the information inthe database to provide (uncertain) labels, and thentrain a log-linear model from this probabilistically-labeled data.This strategy requires a set of features for themodel.
Traditionally, such features would be hand-crafted for a particular domain and database.
As afirst step towards our Open-DB system, we presenta log-linear model for disambiguation, as well as asimple feature-generation algorithm that produces alarge set of useful features from a BCNF database.We then present a distant-supervision learning pro-cedure for this model.4.1 Disambiguation ModelLet SD be the set of possible referents.
We constructa vector of feature functions f(m, s) describing thedegree to which m and s ?
SD appear to matchone another.
The feature functions are described be-low.
The model includes a vector of weights w, oneweight per feature function, and sets the probabilityof entity s given m and w as:P (s|m,w) =exp (w ?
f(m, s))?s?
?SDexp (w ?
f(m, s?
))(1)4.2 Database-driven Feature GenerationFigure 2 shows our algorithm for automatically gen-erating feature functions fi(m, s) from a BCNFdatabase.
As mentioned above, we only need to con-sider resolving to database symbols s that are keys,or unique IDs, for some tuple in a database.
Foran entity in the database with key id, the featuregeneration algorithm generates two types of featurefunctions: attribute counts and similar entity counts.Each of these features measures the similarity be-tween the information stored in the database aboutthe entity id, and the information in the text in d sur-rounding mention m.An attribute count feature function fatti,j (m, id)for the jth attribute of relation ri counts how manyAlgorithm: Feature GenerationInput: DB, a database in BCNFOutput: F, a set of feature functionsInitialization: F?
?Attribute Count Feature Functions:For each relation ri ?
DB.RFor each j in {1, .
.
.
, ki}Define function fatti,j (m, id):count?
0Identify the tuple t ?
ri containing idval?
tjcount?
count +ContextMatches(val,m)return countF?
F ?
{fatti,j }Similar-Entity Count Feature Functions:For each relation ri ?
DB.RFor each j in {1, .
.
.
, ki}Define function fsimi,j (m, id):count?
0Identify the tuple t ?
ri containing idval?
tjIdentify the set of similar tuples T ?
:T ?
= {t?|t?
?
ri, t?j = val}For each tuple t?
?
T ?For each j?
?
{1, .
.
.
, ki}val?
?
t?jcount?
count +ContextMatches(val?,m)return countF?
F ?
{fsimi,j }Figure 2: Feature generation algorithm.
TheContextMatches(s,m) function counts how manytimes a string that matches database symbol s appearsin the context of m. In our implementation, we use allof d(m) as the context.
Matching between strings anddatabase symbols is discussed in Sec.
7.3.attributes of the entity id appear near m. For exam-ple, if id is 5 in the movie relation in Figure 1, thefeature function for attribute year would count howoften 2010 matches the text surrounding mentionm.
Defining precisely whether a database symbol?matches?
a word or phrase is a subtle issue; we ex-plore several possibilities in Section 7.3.
In addition119to attribute counts for attributes within a single rela-tion, we also use attributes from relations that havebeen inner-joined on primary key and foreign keypairs.
For example, for movies, we include attributessuch as director name, genre, and actor name.
Highvalues for these attribute count features indicate thatthe text around m closely matches the informationin the database about entity id, and therefore id is astrong candidate for the referent of m. We use thewhole document as the context for finding matches,although other variants are worth future investiga-tion.A similar entity count feature functionfsimi,j (m, id) for the jth attribute in relation ricounts how many entities similar to id are men-tioned in the neighborhood of m. As an example,consider a mention of ?Chris Johnson?, id = 3,and the similar entity feature for the positionattribute of the players relation in the sportsdatabase.
The feature function would first identifythat 3B is the position of the player with id = 3.
Itwould then identify all players that had the sameposition.
Finally, it would count how often anyattributes of this set of players appear near ?ChrisJohnson?.
Likewise, the similar entity feature forthe team id attribute would count how manyteammates of the player with id = 3 appear near?Chris Johnson?.
A high count for this teammatefeature is a strong clue that id is the correct referentfor m, while a high count for players of the sameposition is a weak but still valuable clue.4.3 Parameter Estimation via DistantSupervisionUsing string similarity, we can heuristically deter-mine that three IDs with name attribute ChrisJohnson are highly likely to be the correct targetfor a mention of ?Chris Johnson?.
Our distant su-pervision parameter estimation strategy is to moveas much probability mass as possible onto the setof realistic referents obtained via string similarity.Since our features rely on finding attributes and sim-ilar entities, the side effect of this strategy is thatmost of the probability mass for a particular mentionis moved onto the one target ID with high attributecount and similar entity count features, thus disam-biguating the entity.
Although the string-similarityheuristic is typically noisy, the strong information inthe database and the fact that many entity mentionsare typically not ambiguous allows the technique tolearn effectively from unlabeled text.Let ?
(m,DB) be a heuristic string-matchingfunction that returns a set of plausible ID values indatabaseDB for mentionm.
The objective functionfor this training procedure is a modified marginal loglikelihood (MLL) function that encourages probabil-ity mass to be placed on the heuristically-matchedtargets:MLL(M,w) =?m?Mlog?id??
(m,DB)P (id|m,w)This objective is smooth but non-convex.
We usea gradient-based optimization procedure that finds alocal maximum.
Our implementation uses an open-source version of the LBFG-S optimization tech-nique (Liu and Nocedal, 1989).
The gradient of ourobjective is given by?LL(M,w)?wi=?m?MEid??
(m,DB) [fi(m, id)]?Eid?DB.S [fi(m, id)]where the expectations are taken according toP (id|m,w).5 A Domain-Adaptation Strategy forOpen-DB NEDOur domain-adaptation strategy builds an Open-DBNED system by training it on labeled examples froman initial database or small set of initial databases.Unlike traditional NED, however, the purpose inOpen-DB NED is to resolve to any database.
Thusthe strategy must take care to build a model thatcan transfer what it has learned to a new database,without requiring additional labeled data for the newdatabase.At first, the problem seems intractable ?
justbecause a system can disambiguate between ?NextDoor?, the 2005 Norwegian film, and ?Next Door?,the 1975 short film by director Andrew Silver, thatseems to provide little benefit for disambiguating be-tween different athletes named ?Andre Smith.?
Thecrux of the problem lies in the fact that database-driven features are domain-specific.
Counting howmany times the director of a movie appears is highly120useful in the movie domain, but worthless in thesports domain.Our solution works by re-defining the problem insuch a way that we can define domain-independentand database-independent features.
For example,rather than counting how often the director ofa movie appears in the context around a moviemention, we create a domain-independent CountAtt(m, s) feature function that counts how often anyattribute of s appears in the context of m. Formovies, Count Att will add together counts for ap-pearances of a movie?s production year and IMDBrating, among other attributes.
In the sports domain,Count Att will add together counts for appearancesof a player?s height, position, salary, etc..
But in ei-ther domain, the feature is well-defined, and in eitherdomain, larger values of the feature indicate a bettermatch between m and s. Thus there is a hope fortraining a model with domain-independent featureslike Count Att on labeled data from one domain, saymovies, and producing a model that has high accu-racy on the sports domain.We first formalize the notion of a domain adap-tation NED model, and then describe our algorithmfor producing such a model.
We say that a domainconsists of a database DB as well as a distributionD(M), whereM is the space of mentions.
For in-stance, the movie domain might consist of the Inter-net Movie Database (IMDB) and a distribution thatplaces most probability mass on documents aboutmovies and Hollywood stars.
In domain adapta-tion, a system observes a set of training examples(m, s, g(m, s)), where instances m ?
M are drawnfrom a source domain?s distribution DS and refer-ents s are drawn from the source domain?s databaseDBS .
The labels g(m, s) are boolean values in-dicating a correct or incorrect match between themention and referent.
The system must then learna hypothesis for classifying examples (m, s) drawnfrom a target domain?s distributionDT and databaseDBT .
Note that for domain adaptation, we can-not use the more traditional problem formulation inwhich the referent s is a label (i.e., s = g(m)) for themention, since the set of possible referents changesfrom domain to domain, and therefore the output ofg would be completely different from one domain tothe next.Table 1 lists the domain-independent featuresDomain-Independent Feature FunctionsCount Att:?i,j fatti,j (m, s)Count Sim:?i,j fsimi,j (m, s)Count All: Count Att + Count SimCount Unique:?i,j{0 if fatti,j (m, s) = 0,1 if fatti,j (m, s) > 0.Count Num:?i,j|jis a numeric att.
fatti,j (m, s)Table 1: Primary feature functions for a domain adapta-tion approach to NED.
These features made the biggestdifference in our experiments, but we also tested varia-tions such as counting unique numeric attribute appear-ances, counting unique similar entities, counting relationname appearances, counting extended attributed appear-ances, and others.used in our domain adaptation model.
These fea-tures use the attribute counts and similar entitycounts from the distant supervision model as subrou-tines.
By aggregating over those domain-dependentfeature functions, the domain adaptation system ar-rives at feature functions that can be defined for anydatabase, rather than for a specific database.Note that there is a tradeoff between the do-main adaptation technique and the distant super-vision technique.
The domain adaptation modelhas access to labeled data, unlike the distant su-pervision model.
In addition, the domain adapta-tion model requires no text whatsoever from the tar-get domain, not even an unlabeled corpus, to setweights for the target domain.
Once trained, it isready for NED over any database that meets our as-sumptions, out of the box.
However, because themodel needs to be able to transfer to arbitrary newdomains, the domain adaptation model is restrictedto domain-independent features, which are ?coarser-grained.?
That is, the distant supervision model hasthe ability to place more weight on attributes likedirector rather than genre, or team rather than po-sition, if those attributes are more discriminative.The domain adaptation model cannot place differ-ent weights on the different attributes, since thoseweights would not transfer across databases.As with distant supervision, the domain adapta-tion strategy uses a log-linear model over these fea-ture functions.
We use standard techniques for train-ing the model using labeled data from the source do-121main: conditional log likelihood (CLL) as the objec-tive function, and LBFG-S for convex optimization.CLL(L,w) =?
(m,id,label)?LlogP (label|m, id,w)The training algorithm is guaranteed to converge tothe globally optimal parameter setting for this objec-tive function over the training data.
The manuallyannotated data contains only positive examples; togenerate negative examples, we use the same name-matching heuristic ?
(m,DB) to identify a set of po-tentially confusing bad matches.
On test data, weuse the trained model to choose the id for a given mwith the highest probability of being correct.6 A Hybrid ModelThe distant supervision and domain adaptationstrategies use two very different sources of evidencefor training a disambiguation classifier: the string-matching heuristic and unlabeled text from the targetdomain for the the distant supervision model, andaggregate features over labeled text from a separatedomain for domain adaptation.
This begs the ques-tion, do these sources of evidence complement oneanother?
To address this question, we design a Hy-brid model with features and training strategies fromboth distant supervision and domain adaptation.The training data consists of a set LS of labeledmentions from a source domain, a source databaseDBS , a set of unlabeled mentions MT from the tar-get domain, and the target-domain database DBT .The full feature set of the Hybrid model is the unionof the distant supervision feature functions for thetarget domain and the domain-independent domainadaptation feature functions.
Note that the distantsupervision feature functions are domain-specific,so they almost always will be uniformly zero on LS ,but the domain adaptation feature functions will beactivated on both LS and MT .
The combined train-ing objective for the Hybrid model is:LL(LS ,MT ,w) = CLL(LS ,w) +MLL(MT ,w)7 ExperimentsOur experiments compare our strategies for Open-DB NED against one another, as well as against aWikipedia NED system from previous work, on twodomains: sports and movies.7.1 DataFor the movie domain, we collected a set of156 cult movie titles from an online movie site(www.olivefilms.com).
For each movie title, we ex-ecuted a Web search using a commercial search en-gine, and collected the top five documents for eachtitle from the search engine?s results.
Nearly all top-five results included at least one mention of an en-tity not found in Wikipedia; overall, only 16% of thementions could be linked to Wikipedia.
After strip-ping javascript and html annotations, we removeddocuments with fewer than 50 words, leaving a to-tal of 770 documents.
We select one occurrence ofany of the 156 movie titles from each document asour set of mentions.
Many titles are ambiguous notjust among different movies with the same name, butalso among novels, plays, geographical entities, andassorted other types of entities.
To provide labels forthese mentions, we use both a movie database andWikipedia.
We downloaded the complete data dumpfrom the online Internet Movie Database (IMDB,www.imdb.com).
For our set of possible referents,we use the set of all key values in IMDB, and the setof all Wikipedia articles.
Annotators manually la-beled each mention using this set of referents.
Table2 shows summary statistics about this labeled data.For the sports domain, we downloaded all playerdata from Yahoo!, Inc.?s sports database for theyears 2011-2012 and two American sports leagues,the National Football League (NFL) and MajorLeague Baseball (MLB).
From the database, we ex-tracted ambiguous player names and team names,including names like ?Philadelphia?
which may re-fer to Philadelphia Eagles in the NFL data,Philadelphia Phillies in the MLB data, orthe city of Philadelphia itself (in both types ofdata).
We then collected 1300 Yahoo!
news arti-cles which include a mention that partially matchesat least one of these database symbols.
We manu-ally labeled a random sample of 564 mentions fromthis data, including 279 player name mentions and285 city name mentions.
Many player name andplace name mentions are ambiguous between thetwo sports leagues, as well as with teams or play-ers from other leagues.
In order to focus on thehardest cases, we specifically exclude mentions like?Philadelphia?
from the labeled data if any of their122domain |M | E|?
(m,DB)| OOD Wikimovies 770 2.6 13% 16%sports 549 4.5 0% 100%Table 2: Number of mentions, average number of refer-ents per mention, % of mentions that are OOD, and %of mentions that are in Wikipedia in our movie and sportsdata.unambiguous completions appears in the same arti-cle (that is, if either of the team names ?PhiladelphiaEagles?
or ?Philadelphia Phillies?
appears in thesame article, we exclude the ?Philadelphia?
men-tion).
As before, the set of possible referents in-cludes the symbol OOD, key values from the sportsdatabase, and Wikipedia articles, and a given men-tion may be labeled with both a sports entity and aWikipedia article, if appropriate.
All of our data isavailable from the last author?s website.7.2 Evaluation MetricWe report on a version of exact-match accuracy.
Thesystem chooses the most likely label s?
for each m.This is judged correct if s?
matches the correct labels exactly, or (in cases where both a Wikipedia and adatabase entity are considered correct) if one of thelabels matches s?
exactly.
This metric allows systemsto resolve against either reference, Wikipedia or an-other database, without requiring it to match both ifthe same entity appears in both references.7.3 Exact or Partial Matching?One important question in the design of our systemsis how to determine the ?match?
between databasesymbols and text.
This question comes into play intwo components of our systems: it affects the com-putation of feature functions that count how often amatch of some attribute is found in text, and it af-fects which set of heuristically-determined databaseentities are considered to be possible matches for agiven mention.We experiment with two different matchingstrategies between a symbol s and text t, exactmatching and partial matching.
Exact matching?exact(s, t) requires the sequence of characters in sto appear exactly (modulo character encoding) in t.For instance, the database value Chris JohnsonSystem AccuracyNo-Wikipedia Domain Adapt.
0.61DocSim-Wikipedia Domain Adapt.
0.69Table 3: Including a simple document-similarity featurefor comparing a mention?s context with a Wikipedia pageprovides an 8% improvement over ignoring Wikipedia in-formation.would match ?Chris Johnson?, but not ?C.
John-son?
or ?Johnson?
in text.
For partial matching,we used different tests for numeric and textual en-tities.
For numeric entities, ?partial matched s andt if the numeric value of one was within 10% ofthe other, so that 5312 would match ?5,000.?
Wemade no attempt to convert numeric phrases, suchas ?3.6 million?, into numeric values.
For textualentities, ?partial matched s and t if at least onetoken from each matched exactly.
Thus ChrisJohnson matches both ?Chris?
and ?C.
Johnson?.We found ?partial to be consistently superior forcomputing ?
(m,DB), since it has much better re-call for mentions like ?Philadelphia?.
On the otherhand, if we use ?partial for computing our models?feature functions, like the Count Att(m, s) in the do-main adaptation model, counts varied widely acrossdomains.
A simple version of the domain adapta-tion classifier (only the Count All and Count Uniquefeatures) trained on sports data and tested on moviesachieved an accuracy of 24% using ?partial, com-pared with 61% using ?exact.
For all remainingtests, we used ?exact for computing features, and?partial for computing ?
(m,DB).7.4 Incorporating Wikipedia referentsThus far, all of our features work on relational data,not Wikipedia.
In order to allow our systems to linkto Wikipedia, we create a single ?document simi-larity?
feature describing the similarity between thetext around a mention and the text appearing on aWikipedia page.
We build a vector space model ofboth the document containing the mention and theWikipedia page, remove stopwords, and use cosinesimilarity to compute this feature.To evaluate the effectiveness of this Wikipediafeature, we tested two versions of our domain adap-tation system, both trained on sports data and tested1230.130.4 0.430.54 0.650.71 0.72 0.7300.210.3310.54 0.63 0.620.6600.10.20.30.40.50.60.70.80.91AccuracyOpen-DB NED TestMovies SportsFigure 3: All three Open-DB NED strategies out-perform a state-of-the-art Wikipedia NED system by25% or more on sports and movies, and outperforma Wikipedia NED system with oracle information by14% or more on the movie data.
Differences betweenthe Modified Zhou Wikifier and the Open-DB strategiesare statistically significant (p < 0.01, Fisher?s exact test)on both domains.on the movies domain.
The first version involvesno Wikipedia information whatsoever, thus it has noreason to select a Wikipedia article over OOD.
Thesecond system includes the document similarity fea-ture.
Table 3 shows the results of these systems.
En-couragingly, our single document similarity featureproduces a significant improvement over the modelwithout Wikipedia information, so we use this fea-ture in all of our systems tested below.
More so-phisticated use of Wikipedia is certainly possible,and an important question for future work is howto combine Open-DB NED more seamlessly withWikipedia NED.7.5 Comparing Open-DB NED StrategiesFor each domain, we compare our domain-adaptation strategy, distant supervision, and hy-brid strategies.
The domain-adaptation model istrained on the labeled data for sports when testingon movies, and vice versa.
We use a movies test setof 180 mentions that is separate from the develop-ment data used for the above tests.
For the distantsupervision strategy, we use the entire collection oftexts from each domain as input (1300 articles forsports, 770 articles for movies), with the labels re-moved during training.We compare against a state-of-the-art WikipediaNED system used in production by a major Webcompany.
This system is a modified version of thesystem described by Zhou et al2010), where cer-tain features have been removed for efficiency.
Werefer to this as the Modified-Zhou Wikifier.
Thissystem uses a gradient-boosted decision tree andmultiple local and global features for computingthe similarity between a mention?s context and aWikipedia article.
We also test a hypothetical sys-tem, Oracle Wikifier, which is given no informationabout entities in IMDB, but is assumed to be ableto correctly resolve any mention that refers to anentity found in Wikipedia.
Thus, this system hasperfect accuracy on mentions that can be found inWikipedia, and accuracy similar to a baseline thatpredicts randomly on all mentions that fall outsideof Wikipedia1.
Oracle-Wikifier serves as an upperbound on systems that have no access to a domain-specific database.
In addition, we compare againsttwo standard baselines: a classifier that always pre-dicts OOD, and a classifier that chooses randomly.Finally, we compare against a system that trains thedomain adaptation model using distant supervision(?DA Trained with DS?
).Figure 3 shows our results.
All three Open-DBapproaches outperform the baseline techniques onthis test by wide margins, with the Hybrid model in-creasing by 30% or more over the random baseline.On the movie domain, the Hybrid model outper-forms the Oracle Wikifier by nearly 20%.
Encour-agingly, the Hybrid model consistently outperformsboth distant supervision and domain adaptation, sug-gesting that the two sources of evidence are partiallycomplementary.
Distant supervision performs betteron the movies test, whereas domain adaptation hasthe advantage on sports.
The differences among allthree Open-DB approaches is relatively small, com-pared with the difference between these approachesand Oracle Wikifier on the movie data.The domain adaptation system outperforms DATrained with DS on both domains, suggestingthat labeled data from a separate domain is bet-ter evidence for parameter estimates than unlabeleddata from the same domain.
The distant super-vision system also outperforms DA Trained with1Alternatively, one could make the oracle system predictOOD on all mentions that fall outside of Wikipedia.
Randompredictions perform better on our data.124DS on both domains, suggesting that the fine-grained, domain-specific features do in fact providemore helpful information than the coarser-grained,domain-independent features of the domain adapta-tion model.All of the Open-DB NED systems outperform theModified Zhou Wikifier on both data sets by a widemargin.
In fact the Modified Zhou Wikifier has sim-ilar results on both domains, despite the fact thatWikipedia has far greater coverage on sports thanmovies.
In part, the poor performance of the Modi-fied Zhou Wikifier reflects the difficult nature of thetask.
In previous experiments on an MSNBC newstest set it reached 85% accuracy, but a random clas-sifier there achieved 60% accuracy compared with21% on our sports data.
Another difficulty withthe Modified Zhou Wikifier is its strong preferencefor globally common entities.
It consistently clas-sifies mentions that are ambiguous between a cityand a team (like ?Chicago?
in ?Chicago sweeps theRed Sox?)
as cities when they should be resolvedto teams, in large part because Chicago is a morecommon referent in general text than either of thebaseball teams that play in that city.
In sports arti-cles, however, both meanings are common, and onlythe surrounding context can help determine the cor-rect referent.Besides wikifiers, NED systems may also becompared with dictionary-based word sense disam-biguation techniques like the Lesk algorithm2 (Lesk,1986).
The Lesk algorithm is ?open?
in the sensethat it works for arbitrary dictionaries, and it definesa vector space model of the dictionary definitionsthat may be likened to the attribute-value model inour representation of entities in the database.
Ourapproach, however, estimates parameters for a sta-tistical model from data, whereas the Lesk algorithmuses an equal weight for all attributes.
To make anempirical comparison, we created a variant of theLesk algorithm for relational data: we took the dis-ambiguation model from Eqn.
1, supplied all ofthe features from the distant supervision model, andmanually set w = 1.
This ?relational Lesk?
modelachieves an accuracy of 0.11 on movies, and 0.15on sports, significantly below the random baseline.Giving equal weight to noisy attributes like genre2We thank the reviewers for making this connection.and more discriminative attributes like directorsignificantly hurts the performance.For both the movie and sports domain, approx-imately 80% of the Hybrid model?s errors are be-cause of predicting database symbols, when the cor-rect referent is a Wikipedia page or OOD.
Thisnearly always occurs because some words in thecontext of a mention match an attribute of an in-correct database referent.
For instance, the crimegenre is an attribute for several movies, but it alsomatches in contexts surrounding book titles and nu-merous other entities.
In the movie domain, most ofthe remaining errors are incorrect OOD predictionsfor mentions that should resolve to the database, butthe article contains no attributes or similar entitiesto the database entity.
In the sports domain, manyof the remaining errors were due to predicting in-correct player referents.
Quite often, this was be-cause the document discusses a fantasy sports leagueor team, where players from different professionalsports teams are mixed together on a ?fantasy team?belonging to a fan of the sport.
Since players in thefantasy leagues have different teammates than theydo in the database, these articles consistently con-fuse our methods.8 Conclusion and Future WorkThis paper introduces the task of Open-DB NamedEntity Disambiguation, and presents two distinctstrategies for solving this task.
Experiments indicatethat a mixture of the two strategies significantly out-performs a state-of-the-art Wikipedia NED system,on a dataset where Wikipedia has good coverage andon another dataset where Wikipedia has poor cover-age.
The results indicate that there is a significantbenefit to leveraging other sources of knowledge inaddition to Wikipedia, and that it is possible to lever-age this knowledge without requiring labeled datafor each new source.
The initial success of theseOpen-DB NED approaches indicates that this task isa promising area for future research, including ex-citing extensions that link large numbers of domain-specific databases to text.AcknowledgmentsThis work was supported in part by a gift from Ya-hoo!, Inc.125ReferencesKedar Bellare and Andrew McCallum.
2007.
Learn-ing extractors from unlabeled text using relevant data-bases.
In Sixth International Workshop on InformationIntegration on the Web.Kedar Bellare and Andrew McCallum.
2009.
General-ized Expectation Criteria for Bootstrapping Extractorsusing Record-Text Alignment.
In Empirical Methodsin Natural Language Processing (EMNLP-09).Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2010.
A theory of learning from differentdomains.
Machine Learning, 79:151?175.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.Razvan Bunescu and Raymond Mooney.
2007.
Learningto extract relations from the web using minimal super-vision.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL-07).R.
Bunescu and M. Pasca.
2006.
Using encyclope-dic knowledge for named entity disambiguation.
InProceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06).Ying Chen and James Martin.
2007.
Towards Ro-bust Unsupervised Personal Name Disambiguation.
InEMNLP, pages 190?198.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on wikipedia data.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages708?716.Nilesh N. Dalvi, Ravi Kumar, Bo Pang, and AndrewTomkins.
2009.
Matching Reviews to Objects using aLanguage Model.
In EMNLP, pages 609?618.Nilesh N. Dalvi, Ravi Kumar, and Bo Pang.
2012.
Objectmatching in tweets with spatial models.
In WSDM,pages 43?52.Hal Daume?
III, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In Proceedings of the ACL Workshop onDomain Adaptation (DANLP).D.
Downey, M. Broadhead, and O. Etzioni.
2007.
Lo-cating complex named entities in web text.
In Procs.of the 20th International Joint Conference on ArtificialIntelligence (IJCAI 2007).Anthony Fader, Stephen Soderland, and Oren Etzioni.2009.
Scaling wikipedia-based named entity disam-biguation to arbitrary web text.
In Proceedings ofthe WikiAI 09 - IJCAI Workshop: User ContributedKnowledge and Artificial Intelligence: An EvolvingSynergy.Xianpei Han and Jun Zhao.
2009.
Named entity dis-ambiguation by leveraging Wikipedia semantic knowl-edge.
In Proceeding of the 18th ACM Conferenceon Information and Knowledge Management (CIKM),pages 215?224.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,Hagen Furstenau, Manfred Pinkal, Marc Spaniol,Bilyana Taneva, Stefan Thater, and Gerhard Weikum1.2011.
Robust Disambiguation of Named Entities inText.
In EMNLP, pages 782?792.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S. Weld.
2011.
Knowledge-Based Weak Supervision for Information Extraction ofOverlapping Relations.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL).Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervised se-quence labeling.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, andSoumen Chakrabarti.
2009.
Collective annotationof wikipedia entities in web text.
In Proceedings ofthe 15th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining (KDD), pages457?466.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,and Mark Steedman.
2011.
Lexical Generalizationin CCG Grammar Induction for Semantic Parsing.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).M.E.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of theSIGDOC Conference.Thomas Lin, Mausam, and Oren Etzioni.
2012.
Entitylinking at web scale.
In Knowledge Extraction Work-shop (AKBC-WEKEX), 2012.D.C.
Liu and J. Nocedal.
1989.
On the limited mem-ory method for large scale optimization.
Mathemati-cal Programming B, 45(3):503?528.G.S.
Mann and D. Yarowsky.
2003.
Unsupervised per-sonal name disambiguation.
In CoNLL.Paul McNamee, Mark Dredze, Adam Gerber, NikeshGarera, Tim Finin, James Mayfield, Christine Pi-atko, Delip Rao, David Yarowsky, and Markus Dreyer.2009.
HLTCOE Approaches to Knowledge Base Pop-ulation at TAC 2009.
In Text Analysis Conference.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
:Linking documents to encyclopedic knowledge.
In126Proceedings of the Sixteenth ACM Conference onInformation and Knowledge Management (CIKM),pages 233?242.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics (ACL-2009), pages 1003?1011.Patrick Pantel and Ariel Fuxman.
2011.
Jigs and Lures:Associating Web Queries with Structured Entities.
InACL.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguationto wikipedia.
In Proc.
of the Annual Meeting of theAssociation of Computational Linguistics (ACL).Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the Sixteenth Euro-pean Conference on Machine Learning (ECML-2010),pages 148?163.Avi Silberschatz, Henry F. Korth, and S. Sudarshan.2010.
Database System Concepts.
McGraw-Hill,sixth edition.Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2009.Using Wikipedia to Bootstrap Open Information Ex-traction.
In ACM SIGMOD Record.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extractionwithout labelled data.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2010), pages 1013?1023.Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, FlavianVasile, and Scott Gaffney.
2010.
Resolving surfaceforms to wikipedia topics.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (Coling), pages 1335?1343.127
