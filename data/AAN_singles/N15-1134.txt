Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1250?1255,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLR Parsing for LCFRSLaura Kallmeyer and Wolfgang MaierInstitute for Language and InformationUniversity of D?usseldorfD?usseldorf, Germany{kallmeyer,maierwo}@phil.hhu.deAbstractLR parsing is a popular parsing strategy forvariants of Context-Free Grammar (CFG).
Ithas also been used for mildly context-sensitiveformalisms, such as Tree-Adjoining Gram-mar.
In this paper, we present the first LR-style parsing algorithm for Linear Context-Free Rewriting Systems (LCFRS), a mildlycontext-sensitive extension of CFG which hasreceived considerable attention in the lastyears.1 IntroductionLR parsing is an incremental shift-reduce parsingstrategy in which the transitions between parserstates are guided by an automaton which is com-piled offline.
LR parsers were first introducedfor deterministic context-free languages (Knuth,1965) and later generalized to context-free lan-guages (Tomita, 1984) and tree-adjoining languages(Nederhof, 1998; Prolo, 2003).Linear Context-Free Rewriting System (LCFRS)(Vijay-Shanker et al, 1987) is an immediate ex-tension of CFG in which each non-terminal cancover more than one continuous span of the in-put string.
LCFRS and equivalent formalismshave been used for the modeling of discontinu-ous constituents (Maier and Lichte, 2011) and non-projective dependencies (Kuhlmann, 2013), as wellas for data-driven parsing of such structures (Maierand Kallmeyer, 2010; Kallmeyer and Maier, 2013;van Cranenburgh, 2012; Angelov and Ljungl?of,2014).
They have also been used for modelingnon-concatenative morphology (Botha and Blun-som, 2013), for grammar engineering (Ranta, 2011),and for modeling alignments in machine translation(S?gaard, 2008; Kaeshammer, 2013).
To our knowl-edge, so far, no LR strategy for LCFRS has beenpresented in the literature.
In this paper, we presentan LR-style parser for LCFRS.
It is based on the in-cremental parsing strategy implemented by ThreadAutomata (Villemonte de la Clergerie, 2002).The remainder of the article is structured as fol-lows.
In the following section, we introduce LCFRSand thread automata.
Section 3 presents the algo-rithm along an example.
In particular, section 3.2gives the algorithms for automaton and parse tableconstructions, and section 3.3 presents the parsingalgorithm.
Section 4 concludes the article.2 Preliminaries2.1 LCFRSIn this paper, we restrict ourselves to string rewritingLCFRS and omit the more general definition (Weir,1988).In LCFRS, a single non-terminal can span k ?
1continuous blocks of a string.
A CFG is simply aspecial case of an LCFRS in which k = 1. k iscalled the fan-out of the non-terminal.
We notateLCFRS with the syntax of Simple Range Concate-nation Grammars (SRCG) (Boullier, 1998), a for-malism equivalent to LCFRS.An LCFRS1(Vijay-Shanker et al, 1987; Seki etal., 1991) is a tuple G = (N,T, V, P, S) where N1Note that for purposes of exposition, we limit ourselves to?-free LCFRS.1250is a finite set of non-terminals with a function dim:N ?
N determining the fan-out of each A ?
N ; Tand V are disjoint finite sets of terminals and vari-ables; S ?
N is the start symbol with dim(S) = 1.P is a finite set of rewriting rules with rank m ?0.
All ?
?
P have the formA(?0, .
.
.
, ?dim(A)?1)?
A1(X(1)0, .
.
.
, X(1)dim(A1)?1)?
?
?Am(X(m)0, .
.
.
, X(m)dim(Am)?1)where A,A1, .
.
.
, Am?
N , X(l)j?
V for 1 ?l ?
m, 0 ?
j < dim(Ai) and ?i?
(V ?
T )+for0 ?
i < dim(A).
All ?iand X(l)jare called argu-ments (or sometimes components); the elements in?iare called argument elements.
A?is the set ofall argument elements of ?.
Variable occurrences inthe arguments of the non-terminals of ?
are orderedby a strict total order ?.
For all X1, X2?
V oc-curring in arguments of a non-terminal of ?, it holdsthat X1?
X2iff either X1precedes X2in an argu-ment of the non-terminal or the argument X1occursin precedes the argument X2occurs in.For all ?
?
P , every variable X occurring in ?occurs exactly once in the left-hand side (LHS) andexactly once in the right-hand side (RHS).
Further-more, if for two variables X1, X2?
V , it holds thatX1?
X2on the RHS, then also X1?
X2on theLHS.
The rank of G is the maximal rank of any ofits rules, its fan-out is the maximal fan-out of any ofits non-terminals.We use the following additional notation: For arule ?
?
P , lhs(?)
gives the LHS non-terminal;lhs(?, i) gives the ith argument of the LHS andlhs(?, i, j) its jth symbol; rhs(?, k) gives the kthRHS non-terminal; and rhs(?, k, l) gives the lthcomponent of the kth RHS element (starting with in-dex 0 in all four cases).
These function have value?whenever there is no such element.
Furthermore, inthe sense of dotted productions, we define for each?
?
P a set of symbols denoting computation pointsof ?, C?= {?i.j| 0 ?
i < dimA, 0 ?
j ?
|?i|}, aswell as the set C =??
?PC?.A non-terminal A ?
N can be instantiatedw.r.t.
an input string w1?
?
?w|w|and a rule ?
?
Pwith lhs(?)
= A.
An instantiation maps all argu-ment elements of ?
to spans ofw ((i?1, j)wdenotesthe span wi?
?
?wj, 1 ?
i ?
j ?
n).
All instantia-tions are given by a function ?
: A??
N?N where?
: S(xy)?
A(x, y) ?
: A(a, b)?
??
: A(ax, ya)?
A(x, y)Figure 1: LCFRS for {anaban|n ?
0}for all x, y ?
A?with x 6= y, ?
(x) = (i, j)wand?
(y) = (k, l)wit holds that i, k ?
0; j, l ?
|w|; ifx (y) is a terminal, then j = i + 1 (l = k + 1),otherwise j > i (k > l).
Iff x ?
y in ?, thenj ?
k. A derivation rewrites strings of instantiatednon-terminals, i.e., given an instantiated clause, theinstantiated LHS non-terminal may be replaced withthe sequence of instantiated RHS terminals.
The lan-guage of the grammar is the set of strings which canbe reduced to the empty word, starting with S in-stantiated to the input string.See figure 1 for a sample LCFRS.2.2 Thread AutomataThread automata (TA) (Villemonte de la Clergerie,2002) are a generic automaton model which can beparametrized to recognize different mildly context-sensitive languages.
The TA for LCFRS (LCFRS-TA) implements a prefix-valid top-down incremen-tal parsing strategy similar to the ones of Kallmeyerand Maier (2009) and Burden and Ljungl?of (2005).An LCFRS-TA for some LCFRS G =(N,T, V, P, S) works as follows.
The process-ing of a single rule is handled by a single threadwhich will traverse the LHS arguments of therule.
A thread is given by a pair p : X , wherep ?
{1, .
.
.
,m}?with m the rank of G is theaddress, and X ?
N ?
{ret} ?
C where ret /?
Nis the content of the thread.
An automaton stateis given by a tuple ?i, p, T ?
where T is a set ofthreads, the thread store, p is the address of theactive thread, and i ?
0 indicates that i tokens havebeen recognized.
We introduce a new start symbolS?/?
N that expands to S and use ?0, ?, {?
: S?}?
asstart state.The specific TA for a given LCFRSG = (N,T, V, P, S) can be defined as tuple?N?, T, S?, ret , ?,??
with N?= N ?
C ?
{S?, ret};?
is a function from C to {1, .
.
.
,m} ?
{?
}such that ?
(?k,i) = j if there is a l such thatlhs(?, k, i) = rhs(?, j ?
1, l), and ?
(?k,i) = ?
iflhs(?, k, i) ?
T ?
{?}
(intuitively, a ?
value j tellsus that the next symbol to process is a variable that1251Call: S??
[S?
]S ?0,0?
[?0,0]A ?0,1?
[?0,1]APredict: S ?
?0,0A?
?0,0A?
?0,0Scan: ?0,0a?
?0,1?1,1a?
?1,2?0,0a?
?0,1?1,0b?
?1,1Publish: ?0,2?
ret ?1,2?
ret ?1,1?
retSuspend: [?0,1]ret ?
?0,2[?1,0]ret ?
?1,1[?0,0]?0,2?
?0,1[?0,2] [?0,0]?0,1?
?0,1[?0,1] [?0,1]?0,2?
?0,2[?0,2] [?0,1]?0,1?
?0,2[?0,1]Resume: ?0,1[?0,2]?
[?0,1]?1,0?0,1[?0,1]?
[?0,1]?1,0?1,0[?0,2]?
[?1,0]?1,0?1,0[?0,1]?
[?1,0]?1,0Figure 2: TA transitions for the LCFRS from figure 1is an argument of the jth RHS non-terminal); and?
is a finite set of transitions.
Every transition hasthe form ?a?
?
with a ?
T ?
{?}
and they roughlyindicate that in the thread store, ?
can be replacedwith ?
while scanning a.
Square brackets in ?
and?
indicate parts that do not belong to the activethread.
This will be made more precise below.
?contains the following transitions (see figure 2):?
Call transitions start a new thread, either forthe start symbol or for a daughter non-terminal.They move down in the parse tree.S??
[S?
]S (initial call), ?k,i?
[?k,i]A if A =rhs(?, j ?
1) and lhs(?, k, i) = rhs(?, j ?
1, 0)where j = ?(?k,i).?
Predict transitions predict a new rule for a non-terminal A: A?
?0,0if A = lhs(?).?
Scan reads a LHS terminal while scanning thenext input symbol:?k,ilhs(?,k,i)?
?k,i+1if lhs(?, k, i) ?
T .?
Publish marks the completion of a production,i.e., its full recognition:?k,j?
ret if dim(lhs(?))
= k + 1 and j =|lhs(?, k)|.?
Suspend suspends a daughter thread and re-sumes the parent.
i.e., moves up in the parse tree.There are two cases:(i) The daughter is completely recognized:[?k,i]ret ?
?k,i+1if lhs(?, k, i) =rhs(?, ?
(?k,i)?1, dim(rhs(?(?k,i)?1))?1).
(ii) The daughter is not yet completely recog-nized, we have only finished one of itscomponents: [?k,i]?l,j?
?k,i+1[?l,j] ifdim(lhs(?))
> l + 1, |lhs(?, l)| = j,lhs(?, k, i) = rhs(?, ?
(?k,i) ?
1, l) andrhs(?, ?(?k,i)?
1) = lhs(?).?
Resume resumes an already present daughterthread, i.e., moves down into some daughter thathas already been partly recognized.
?k,i[?l,j] ?
[?k,i]?l+1,0if lhs(?, k, i) =rhs(?, ?(?k,i)?
1, l + 1), rhs(?, ?(?k,i)?
1) =lhs(?)
and ?l,j+1/?
C.This is not exactly the TA for LCFRS proposed inVillemonte de la Clergerie (2002) but rather the onefrom Kallmeyer (2010), which is close to the Earleyparser from Burden and Ljungl?of (2005).The set of configurations for a given inputw ?
T?is then defined by the deduction rules in figure 3 (theuse of set union S1?
S2in these rules assumes thatS1?
S2= ?).
The accepting state of the automatonfor some input w is ?|w|, 1, {?
: S?, 1 : ret}?.2.3 LR ParsingIn an LR parser, the parser actions are guided byan automaton, resp.
a parse table which is com-piled offline.
Consider the context-free case.
An LRparser for CFG is a guided shift-reduce parser, inwhich we first build the LR automaton.
Its states aresets of dotted productions closed under prediction,and its transitions correspond to having recognizeda part of the input, e.g., to moving the dot over aRHS element after having scanned a terminal or rec-ognized a non-terminal.
Given an automaton with nstates, we build the parse table with n rows.
Eachrow i, 0 ?
i < n, describes the possible parser ac-tions associated with the state qi, i.e., for each stateand each possible shift or reduce operation, it tellsus in which state to go after the operation.3 LR for LCFRS3.1 IntuitionThe states in the automaton are predict and resumeclosures of TA thread stores.
In order to keep themfinite, we allow the addresses to be regular expres-sions.
A configuration of the parser consists of a1252Initial configuration:?0, ?, {?
: S?
}?Initial call:?0, ?, {?
: S?}?
?0, 1, {?
: S?, 1 : S}?Further calls:?i, p,S ?
p : ?k,i?
?i, pj,S ?
p : ?k,i?
pj : A??k,i?
[?k,i]A ?
?,A ?
N, ?
(?k,i) = j + 1Predict:?i, p,S ?
p : A?
?i, p,S ?
p : ?0,0?A ?
N,A?
?1,0?
?Scan:?j, p,S ?
p : ?k,i?
?j + 1, p,S ?
p : ?k,i+1??k,iwj+1?
?k,i+1?
?Publish:?i, p,S ?
{p : ?k,i}?
?i, p,S ?
{p : ret}??k,j?
ret ?
?Suspend 1:?i, pj,S ?
{p : ?k,i, pj : ret}?
?i, p,S ?
{p : ?k,i+1}?
[?k,i]ret ?
?k,i+1?
?Suspend 2:?i, pj,S ?
{p : ?k,i, pj : ?l,m}?
?i, p,S ?
{p : ?k,i+1, pj : ?l,m}?[?k,i]?l,m?
?k,i+1[?l,m] ?
?Resume:?i, p,S ?
{p : ?k,i, p?
(?k,i) : ?l,j}?
?i, p?
(?k,i),S ?
{p : ?k,i, p?
(?k,i) : ?l+1,0}??k,i[?l,j]?
[?k,i]?l+1,0?
?Figure 3: Deduction rules for TA configurationsstack, a set of completed components and the re-maining input.
The completed components are ofthe form p : ?iwhere p is an address and ?ithe component of a rule.
The stack has the form?1x1?2.
.
.
xn?1?nwhere ?iis an address followedby a state and xi?
T ?
{Ak|A ?
N, 1 ?
k ?dim(A)}.Shift: Whenever we have p : q on top of the stackand an edge from q to q?labeled with the next inputsymbol and an address p?, we add the input symbolfollowed by pp?
: q?to the stack.Suspend: Whenever the top of the stack is p1: qsuch that there is a ?i?1,k?
q with k = |lhs(?, i ?1)| and i < dim(?
), we can suspend.
If i = 1,we add p1: ?ito the set of completed componentsand we remove |lhs(?, i)| terminals/component non-terminals and their preceding states from the stack.If i ?
1, we check whether there is a p2: ?i?1in theset of completed components such that the intersec-tion L(p1) ?
L(p2) is not empty.2We then removep2: ?i?1from the set of complete components andwe add p : ?ito it where p is a regular expressiondenoting L(p1) ?
L(p2).
Suppose the topmost stateon the stack is now p?
: q?.
We then have to followthe edge leading from q?to some q?
?labeled Ai: p?
?where A = lhs(?).
This means that we push Aifollowed by p?p??
: q?
?on the stack.2Note that the corresponding finite state automata can be de-terministic; in this case the intersection is quadratic in the sizeof the two automata.In LCFRS without left recursion in any of the components, theintersection is trivial since the regular expressions denote onlya single path each.Reduce: Whenever there is a ?i?1,kin our currentstate with k = |lhs(?, i ?
1)| and i = dim(?
), wecan reduce, which is like suspend except that noth-ing is added to the set of completed components.3.2 Automaton and parse table constructionThe states of the LR-automaton are sets of pairs p :X where p is a regular expression over {1, .
.
.
,m},m the rank of G, and X ?
C ?
{S?}.
They representpredict and resume closures.The predict/resume clo-sure q of some set q is described by the deductionrules in figure 4.
This closure is not always finite.?
: S?1 : ?0,0lhs(?)
= Sp : ?i,jpk : ?
?l,0lhs(?, i, j) = rhs(?, k ?
1, l),rhs(?, k) = lhs(??
)Figure 4: Predict/resume closureHowever, if it is not, we obtain a set of items thatcan be represented by a finite set of pairs r : ?i,jplus eventually ?
: S?such that r is a regular ex-pression denoting a set of possible addresses.
As anexample for such a case, see q3in figure 5.The reason why we can represent these closuresby finite sets using regular expressions for pathsis the following: There is a finite number of pos-sible elements ?i,j.
For each of these, the setof possible addresses it might be combined within a state that is the closure of {?
: X1, ?
:X2, .
.
.
, ?
: Xn} is generated by the CFG ?C?{S?}?
{Snew}, {1, .
.
.
,m}, P, Snew?
with Snew?
Xi?P for all 1 ?
i ?
n, X ?
Y k ?
P for all in-1253stancesp:Xpk:Yof deduction rules and ?i,j?
?.
Thisis a regular grammar, its string language can thus becharacterized by a regular expression.The construction of the set of states starts withq0= {?
: S?}.
For every state q, every non-terminal A and every 1 ?
i ?
dim(A), we defineread(q, Ai, p) = {?
: ?j,k+1| p : ?j,k?
q and thereis some l such that rhs(?, l) = A and lhs(?, j, k) =rhs(?, l, i?1)} and read(q, Ai, p) = read(q, Ai, p).Similarly, for every such q and every a ?
T , we de-fine read(q, a, p) = {?
: ?j,k+1| p : ?j,k?
q andlhs(?, j, k) = a} and read(q, a, p) = read(q, a, p).The set of states of our automaton is then the closureof {q0} under the application of the read-functions.The edges in our automaton correspond to read-transitions, where each edge is labeled with the cor-responding pair Ai, p or a, p respectively.
The au-tomaton we obtain for the grammar in figure 1 isshown in figure 5.
The number of possible states?
: S?, 1 : ?0,011 : ?0,0, 11 : ?0,0q0?
: ?0,1, ?
: ?0,11 : ?0,0, 1 : ?0,0q1?
: ?0,2q2?
: ?0,11+: ?1,0, 1+: ?1,0q3?
: ?1,1q4?
: ?1,2q5?
: ?1,1q6?
: ?0,2q7?
: S?
?q8a, 11a, 1A1, ?A1, 1A2, 1+b, 1+A2, ?a, ?S1, ?Figure 5: The automatonis necessarily finite since each state is the closureof some set containing only items with address ?.There are only finitely many such sets.In the parse table, our operations are s(p, q) forshifting some terminal a followed by the old ad-dress concatenated with p and state q and r(?, i)for reducing the ith component of rule ?.
Thetwo reduce operations can be distinguished by thecomponent indices.
Furthermore, the goto-part ofthe table tells where to go when traversing a com-ponent edge and which address to add then.
Theparse table can be read off the automaton as fol-lows: action(q, a) = s(p, q?)
iff read(q, a, p) = q?;action(q,?)
= r(?, i) iff there is some p : ?i,k?
qsuch that k = |lhs(?, i)|.
Concerning the gotopart of the table, we have goto(q, Ai) = ?p, q??
iffread(q, Ai, p) = q?.
Figure 6 shows the parse tablea b A1A2S10 s(11, 1) ?1, 3?
?
?, 8?1 s(1, 1) r(?, 1) ?
?, 2?2 r(?, 1)3 s(1+, 6) ?1+, 4?,?
?, 7?4 s(?, 5)5 r(?, 2)6 r(?, 2)7 r(?, 1)8 accFigure 6: The parse tablestack completed input operation?
:q0[ ] aaba initial state?
:q0a 11:q1[ ] aba shift a,11?
:q0a 11:q1a 111:q1[ ] ba shift a,1?
:q0a 11:q1A111:q2[111:?1] ba suspend ?0,1?
:q0A11:q3[111:?1,11:?1] ba suspend ?0,2?
:q0A11:q3b 11+:q6[111:?1,11:?1]a shift b,1+?
:q0A11:q3A211+:q4[11:?1] a reduce ?1,1?
:q0A11:q3A211+:q4a 11+:q5[11:?1] ?
shift a,??
:q0A11:q3A21:q4[ ] ?
reduce ?1,2?:q0S1?
:q8[ ] ?
reduce ?0,2Figure 7: Sample run with w = aabafor our example.3.3 ParsingWe run the automaton with ??
: q0, [ ], w?
and in-put w = aaba.
The trace is shown in figure 7.
Westart in q0, and shift two as, which leads to q1.
Wehave then fully recognized the first components of ?and ?
: We suspend them and keep them in the set ofcompleted components, which takes us to q3.
Shift-ing the b takes us to q6, from where we can reduce,which finally takes us to q4.
From there, we can shiftthe remaining a (to q5), with which we have fullyrecognized ?.
We can now reduce both ?
and withthat, ?, which takes us to the accepting state q8.4 ConclusionWe presented the first LR style algorithm for LCFRSparsing.
It offers a convenient factorization of pre-dict/resume operations.
We are currently exploringthe possibility to use it in data-driven parsing.AcknowledgmentsThe work presented in this paper was partly fundedby the German Research Foundation (DFG).
Wewish to thank three anonymous reviewers for theirvaluable comments.1254ReferencesKrasimir Angelov and Peter Ljungl?of.
2014.
Fast statis-tical parsing with parallel multiple context-free gram-mars.
In Proceedings of the 14th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 368?376, Gothenburg, Sweden.Jan A. Botha and Phil Blunsom.
2013.
Adaptor gram-mars for learning non-concatenative morphology.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages 345?356, Seattle, WA.Pierre Boullier.
1998.
Proposal for a Natural LanguageProcessing syntactic backbone.
Research Report3342, INRIA-Rocquencourt, Rocquencourt, France.H?akan Burden and Peter Ljungl?of.
2005.
Parsing linearcontext-free rewriting systems.
In Proceedings of theNinth International Workshop on Parsing Technology,pages 11?17, Vancouver, BC.Miriam Kaeshammer.
2013.
Synchronous linearcontext-free rewriting systems for machine translation.In Proceedings of the Seventh Workshop on Syntax, Se-mantics and Structure in Statistical Translation, pages68?77, Atlanta, GA.Laura Kallmeyer and Wolfgang Maier.
2009.
Anincremental Earley parser for simple range con-catenation grammar.
In Proceedings of the 11thInternational Conference on Parsing Technologies(IWPT?09), pages 61?64, Paris, France.Laura Kallmeyer and Wolfgang Maier.
2013.
Data-driven parsing using probabilistic linear context-free rewriting systems.
Computational Linguistics,39(1):87?119.Laura Kallmeyer.
2010.
Parsing beyond Context-FreeGrammar.
Springer, Heidelberg.Donald E. Knuth.
1965.
On the translation of languagesfrom left to right.
Information and Control, 8(6):607?639, July.Marco Kuhlmann.
2013.
Mildly non-projectivedependency grammar.
Computational Linguistics,39(2):355?387.Wolfgang Maier and Laura Kallmeyer.
2010.
Discon-tinuity and non-projectivity: Using mildly context-sensitive formalisms for data-driven parsing.
InProceedings of the Tenth International Workshop onTree Adjoining Grammar and Related Formalisms(TAG+10), pages 119?126, New Haven, CT.Wolfgang Maier and Timm Lichte.
2011.
Characteriz-ing discontinuity in constituent treebanks.
In FormalGrammar.
14th International Conference, FG 2009.Bordeaux, France, July 25-26, 2009.
Revised SelectedPapers, volume 5591 of Lecture Notes in Artificial In-telligence, pages 167?182, Berlin, Heidelberg, NewYork.
Springer-Verlag.Mark-Jan Nederhof.
1998.
An alternative LR algorithmfor TAGs.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguisticsand 17th International Conference on ComputationalLinguistics, volume 1, pages 946?952, Montreal, QC.Carlos A. Prolo.
2003.
LR Parsing for Tree Adjoin-ing Grammars and its Application to Corpus-basedNatural Language Parsing.
Ph.D. thesis, Departmentof Computer and Information Science, University ofPennsylvania, Philadelphia, PA.Aarne Ranta.
2011.
Grammatical Framework: Pro-gramming with Multilingual Grammars.
CSLI Pub-lications, Stanford.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, andTadao Kasami.
1991.
On multiple context-free gram-mars.
Theoretical Computer Science, 88(2):191?229.Anders S?gaard.
2008.
Range concatenation grammarsfor translation.
In The 22nd International Conferenceon Computational Linguistics (COLING), pages 103?106, Manchester, England.Masaru Tomita.
1984.
LR parsers for natural lan-guages.
In Proceedings of COLING 1984: The 10thInternational Conference on Computational Linguis-tics, pages 354?357, Stanford University.Andreas van Cranenburgh.
2012.
Efficient parsing withlinear context-free rewriting systems.
In Proceedingsof the 13th Conference of the European Chapter ofthe Association for Computational Linguistics, pages460?470, Avignon, France.K.
Vijay-Shanker, David Weir, and Aravind K. Joshi.1987.
Characterising structural descriptions used byvarious formalisms.
In Proceedings of the 25th AnnualMeeting of the Association for Computational Linguis-tics, pages 104?111, Stanford, CA.
?Eric Villemonte de la Clergerie.
2002.
Parsing mildlycontext-sensitive languages with thread automata.
InProceedings of COLING 2002: The 19th InternationalConference on Computational Linguistics, Taipei, Tai-wan.David Weir.
1988.
Characterizing Mildly Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Univer-sity of Pennsylviania, Philadelphia, PA.1255
