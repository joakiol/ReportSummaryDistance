DESCRIPTION OF THE LTG SYSTEM USED FOR MUC-7Andrei Mikheev, Claire Grover and Marc MoensHCRC Language Technology Group,University of Edinburgh,2 Buccleuch Place, Edinburgh EH8 9LW, UK.mikheev@harlequin.co.uk C.Grover@ed.ac.uk M.Moens@ed.ac.ukOVERVIEWThe basic building blocks in our muc system are reusable text handling tools which we have beendeveloping and using for a number of years at the Language Technology Group.
They are modular toolswith stream input/output; each tool does a very specic job, but can be combined with other tools ina unix pipeline.
Dierent combinations of the same tools can thus be used in a pipeline for completingdierent tasks.Our architecture imposes an additional constraint on the input/output streams: they should have acommon syntactic format.
For this common format we chose eXtensible Markup Language (xml).
xmlis an ocial, simplied version of Standard Generalised Markup Language (sgml), simplied to makeprocessing easier [3].
We were involved in the development of the xml standard, building on our expertisein the design of our own Normalised sgml (nsl) and nsl tool lt nsl [10], and our xml tool lt xml[11].
A detailed comparison of this sgml-oriented architecture with more traditional data-base orientedarchitectures can be found in [9].A tool in our architecture is thus a piece of software which uses an api for all its access to xml and sgmldata and performs a particular task: exploiting markup which has previously been added by other tools,removing markup, or adding new markup to the stream(s) without destroying the previously addedmarkup.
This approach allows us to remain entirely within the sgml paradigm for corpus markup whileallowing us to be very general in the design of our tools, each of which can be used for many purposes.Furthermore, because we can pipe data through processes, the unix operating system itself provides thenatural \glue" for integrating data-level applications.The sgml-handling api in our workbench is our lt nsl library [10] which can handle even the mostcomplex document structures (dtds).
It allows a tool to read, change or add attribute values andcharacter data to sgml elements and to address a particular element in an nsl or xml stream using aquery language called ltquery.The simplest way of conguring a tool is to specify in a query where the tool should apply its processing.The structure of an sgml text can be seen as a tree, as illustrated in Figure 1.
Elements in such a treecan be addressed in a way similar to unix le system pathnames.
For instance, DOC/TEXT/P[0] willgive all rst paragraphs under TEXT elements which are under DOC.
We can address an element by freelycombining partial descriptions, e.g.
its location in the tree, its attributes, character data in the elementand sub-elements contained in the element.
The queries can also contain wildcards.
For instance, thequery .
*/S will give all sentences anywhere in the document, at any level of embedding.Using the syntax of ltquery we can directly specify which parts of the stream we want to process andwhich part we want to skip, and we can tailor tool-specic resources for this kind of targeted processing.Now at Harlequin Ltd. (Edinburgh oce)1DATE TRAILERP PS S SPTEXTDOCID...SLUGSTORYID PREAMBLENWORDSDOCFigure 1: Partial SGML structure of a MUC article.For example, we have a programme called fsgmatch which can be used to tokenize input text accordingto rules specied in certain resource grammars.
It can be called with dierent resource grammars fordierent document parts.
Here is an example pipeline using fsgmatch:>> cat text | fsgmatch -q ".
*/DATE|NWORDS" date.gr| fsgmatch -q ".
*/PREAMBLE" preamb.gr| fsgmatch -q ".
*/TEXT/P[0]" P0.grIn this pipeline, fsgmatch takes the input text, and processes the material that has been marked up asDATE or NWORDS using a tokenisation grammar called date.gr; then it processes the material in PREAMBLEusing the tokenisation grammar preamb.gr; and then it processes the rst paragraph in the TEXT sectionusing the grammar P0.gr.This technique allows one to tailor resource grammars very precisely to particular parts of the text.
Forexample, the reason for applying P0.gr to the rst sentence of a news wire is that that sentence oftencontains unusual information which occurs nowhere else in the article and which is very useful for themuc task: in particular, if the sentence starts with capitalised words followed by &MD; the capitalisedwords indicate a location, e.g.
PASADENA, Calif. &MD;.We have used our tools in dierent language engineering tasks, such as information extraction in amedical domain [4], statistical text categorisation [2], collocation extraction for lexicography [1], etc.The tools include text annotation tools (a tokeniser, a lemmatiser, a tagger, etc.)
as well as tools forgathering statistics and general purpose utilities.
Combinations of these tools provide us with the meansto explore corpora and to do fast prototyping of text processing applications.
A detailed description ofthe tools, their interactions and application can be found in [4] and [5]; information can also be foundat our website, http://www.ltg.ed.ac.uk/software/.
This tool infrastructure was the starting pointfor our muc campaign.LTG TOOLS IN MUCAmongst the tools used in our muc system is an existing ltg tokeniser, called lttok.
Tokenisers takean input stream and divide it up into \words" or tokens, according to some agreed denition of what atoken is.
This is not just a matter of nding white spaces between characters|for example, \Tony BlairJr" could be treated as a single token.lttok is a tokeniser which looks at the characters in the input stream and bundles them into tokens.The input to lttok can be sgml-marked up text, and lttok can be directed to only process characterswithin certain sgml elements.
One muc-specic adjustment to the tokenisation rules was to treat a2hyphenated expression as separate units rather than a single unit, since some of the ne expressionsrequired this, e.g.
<TIMEX TYPE="DATE">first-quarter</TIMEX>-charge.Here is an example of the use of lttok.cat text | muc2xml| lttok -q ".
*/P" -mark W standard.grThe rst call in this pipeline is to muc2xml, a programme which takes the muc text and maps it intovalid xml.
lttok then uses a resource grammar, standard.gr, to tokenise all the text in the P elements.It marks the tokens using the sgml element W. The output from this pipeline would look as follows:... <W>said</W> <W>the</W> <W>director</W> <W>of</W> <W>Russian</W> <W>Bear</W><W>Ltd.</W> <W>He</W> <W>denied</W> <W>this.</W> <W>But</W> ...As the example shows, the tokeniser does not attempt to resolve whether a period is a full stop or partof an abbreviation.
Depending on the choice of resource le for lttok, a period will either always beattached to the preceding word (as in this example) or it will always be split o.This creates an ambiguity where a sentence-nal period is also part of an abbreviation, as in the rstsentence of our example.
To resolve this ambiguity we use a special program, ltstop, which appliesa maximum entropy model pre-trained on a corpus [8].
To use ltstop the user must specify whetherperiods in the input are attached to or split o from the preceding words; in our case, they were attachedto the words, and ltstop is used with the option -split.
With this option, ltstop will split the periodfrom regular words and create an end-of-sentence token <W C=".
">.</W>; or it will leave the period withthe word if it is an abbreviation; or, in the case of sentence-nal abbreviations, it will leave the periodwith the abbreviation and in addition create a virtual full stop <W C=".
"></W>Like the other ltg tools ltstop can be targeted at particular sgml elements.
In our example, we wantto target it at <W> elements within <P> elements|the output of lttok.
It can be used with dierentmaximum entropy models, trained on dierent types of corpora.For our example, the full pipeline looks as follows:cat text | muc2xml| lttok -q ".
*/P" -mark W standard.gr| ltstop -q ".
*/P/W" -split fs_model.meThis will generate the following output:<W>said</W> <W>the</W> <W>director</W> <W>of</W> <W>Russian</W><W>Bear</W><W>Ltd.</W> <W C=`.
'></W> <W>He</W> <W>denied</W> <W>this</W><W C=`.
'>.</W> <W>But</W>...Note how ltstop has \added" a nal stop to the rst sentence, making explicit that the period after\Ltd" has two distinct functions.Another standard ltg tool we used in our muc system was our part-of-speech tagger lt pos [7].
lt posis sgml-aware: it reads a stream of sgml elements specied by the query and applies a Hidden MarkovModeling technique with estimates drawn from a trigram maximum entropy model to assign the mostlikely part of speech tags.
An important feature of the tagger is an advanced module for handlingunknown words [6], which proved to be crucial for name spotting.Some muc-specic extensions were added at this point in the processing chain: for capitalised words, weadded information as to whether the word exists in lowercase in the lexicon (marked as L=l) or whetherit exists in lowercase elsewhere in the same document (marked as L=d).
We also developed a modelwhich assigns certain \semantic" tags which are particularly useful for muc processing.
For example,words ending in -yst and -ist (analyst, geologist) as well as words occurring in a special list of words3(spokesman, director) are recognised as professions and marked as such (S=PROF).
Adjectives endingin -an or -ese whose root form occurs in a list of locations (American/America, Japanese/Japan) aremarked as locative adjectives (S=LOC JJ).The output of this part of speech tagging could look as follows:<W C=VBD>said</W> <W C=DET>the</W> <W C=NN S=PROF>director</W> <W C=IN>of</W><W C=NNP S=LOC_JJ>Russian</W><W C=NNP L=l>Bear</W> <W>Ltd.</W> <W C=`.
'></W>We also used a number of other sgml-tools, such as sgdelmarkup which strips unwanted markup froma document, sgsed and sgtr, sgml-aware versions of the unix tools sed and tr.But the core tool in our muc system is fsgmatch.
fsgmatch is an sgml transducer.
It takes certaintypes of sgml elements and wraps them into larger sgml elements.
In addition, it is also possible touse fsgmatch for character-level tokenisation, but in this paper we will only describe its functionality atthe sgml level.fsgmatch can be called with dierent resource grammars, e.g.
one can develop a grammar for recognisingnames of organisations.
Like the other ltg tools, it is also possible to use fsgmatch in a very targetedway, telling it only to process sgml elements within certain other sgml elements, and to use a specicresource grammar for that purpose.Piping the previous text through fsgmatch with a resource grammar for company names would resultin the following:<W>said</W> <W>the</W> <W>director</W> <W>of</W> <ENAMEX TYPE="ORGANIZATION"><W>Russian</W> <W>Bear</W> <W>Ltd.</W> </ENAMEX> <W C=`.
'> </W>The combined functionality of lttok and fsgmatch gives system designers many degrees of freedom.Suppose you want to map character strings like \25th" or \3rd" into sgml entities.
You can do this at thecharacter level, using lttok, specifying that strings that match [0-9]+[ -]?
((st)|(nd)|(rd)|(th))should be wrapped into the sgml structure <W C=ORD>.
Or you can do it at the sgml level: if yourtokeniser had marked up numbers like \25" as <W C=NUM> then you can write a rule for fsgmatch sayingthat <W C=NUM> followed by a <W> element whose character data consist of th, nd, rd or st can bewrapped into an <W C=ORD> element.A transduction rule in fsgmatch can access and utilize any information stated in the element attributes,check sub-elements of an element, do lexicon lookup for character data of an element, etc.
For instance,a transduction rule can say: \if there are one or more W elements (i.e.
words) with attribute C (i.e.
partof speech tag) set to NNP (proper noun) followed by a W element with character data \Ltd.
", then wrapthis sequence into an ENAMEX element with attribute TYPE set to ORGANIZATION.Transduction rules can check left and right contexts, and they can access sub-elements of complexelements; for example, a rule can check whether the last W element under an NG element (i.e.
the headnoun of a noun group) is of a particular type, and then include the whole noun group into a higher levelconstruction.
Element contents can be looked up in a lexicon.
The lexicon lookup supports multi-wordentries and multiple rule matches are always resolved to the longest one.TIMEX, NUMEX, ENAMEXIn our muc system, timex and numex expressions are handled dierently from enamex expressions.
Thereason for this is that temporal and numeric expressions in English newspapers have a fairly structuredappearance which can be captured by means of grammar rules.
We developed grammars for the temporaland numeric expressions we needed to capture, and also compiled lists of temporal entities and currencies.The sgml transducer fsgmatch used these resources to wrap the appropriate strings with timex andnumex tags.4enamex expressions are more complex, and more context-dependent.
Lists of organisations and placenames, and grammars of person names, are useful resources, but need to be handled with care: contextwill determine whether Arthur Andersen is used as the name of a person or a company, whether Wash-ington is a location or a person, or whether Granada is the name of a company or a location.
At thesame time, once Granada has been used as the name of a company, the author of a newspaper articlewill not suddenly start using it to indicate a location without giving contextual clues that such a shift indenotation has taken place.
Because of this, we strongly believe that identication of supportive contextis more important for the identication of names of places, organisations and people than are lists orgrammars.
We do use such lists, but alter them dynamically: if anywhere in the text we have foundsucient context to decide that Granada is used as the name of an organisation, it is added to our list oforganisations for the further processing of that text.
When we start processing a new text, we don't makeany assumptions anymore about whether Granada is an organisation or place, until we nd supportivecontext for one or the other.To identify enamex elements we combine symbolic transduction of sgml elements with probabilisticpartial matching in 5 phases:1. sure-re rules2.
partial match 13. relaxed rules4.
partial match 25. title assignmentWe describe each in turn.ENAMEX: 1.
Sure-re RulesThe sure-re transduction rules used in the enamex task are very context oriented and they re onlywhen a possible candidate expression is surrounded by a suggestive context.
For example, \GerardKlauer" looks like a person name, but in the context \Gerard Klauer analyst" it is the name of anorganisation (as in \General Motors analyst").
Sure-re rules rely on known corporate designators(Ltd., Inc., etc.
), titles (Mr., Dr., Sen.), and denite contexts such as those in Figure 2.At this stage our muc system treats information from the lists as likely rather than denite and alwayschecks if the context is either suggestive or non-contradictive.
For example, a likely company name witha conjunction is left untagged at this stage if the company is not listed in a list of known companies: ina sentence like \this was good news for China International Trust and Investment Corp", it is not clearat this stage whether the text deals with one or two companies, and no markup is applied.Similarly, the system postpones the markup of unknown organizations whose name starts with a sentenceinitial common word, as in \Suspended Ceiling Contractors Ltd denied the charge".
Since the sentence-initial word has a capital letter, it could be an adjective modifying the company \Ceiling ContractorsLtd", or it could be part of the company name, \Suspended Ceiling Contractors Ltd".Names of possible locations found in our gazetteer of place names are marked as location only if theyappear with a context that is suggestive of location.
\Washington", for example, can just as easily bea surname or the name of an organization.
Only in a suggestive context, like \in the Wahington area",will it be marked up as location.ENAMEX: 2.
Partial Match 1After the sure-re symbolic transduction the system performs a probabilistic partial match of the en-tities identied in the document.
This is implemented as an interaction between two tools.
The rst5Context Rule Assign ExampleXxxx+ is a?
JJ* PROF PERS Yuri Gromov is a former directorPERSON-NAME is a?
JJ* REL PERS John White is beloved brotherXxxx+, a JJ* PROF, PERS White, a retired director,Xxxx+ ,?
whose REL PERS Nunberg, whose stepfatherXxxx+ himself PERS White himselfXxxx+, DD+, PERS White, 33,shares of Xxxx+ ORG shares of EaglePROF of/at/with Xxxx+ ORG director of Trinity Motorsin/at LOC LOC in WashingtonXxxx+ area LOC Beribidjan areaFigure 2: Examples of sure-re transduction material for enamex.
Xxxx+ is a sequence of capitalisedwords; DD is a digit; PROF is a profession (director, manager, analyst, etc.
); REL is a relative (sister,nephew, etc.
); JJ* is a sequence of zero or more adjectives; LOC is a known location; PERSON-NAMEis a valid person name recognized by a name grammar.tool collects all named entities already identied in the document.
It then generates all possible partialorders of the composing words preserving their order, and marks them if found elsewhere in the text.For instance, if at the rst stage the expression \Lockheed Martin Production" was tagged as organi-zation because it occurred in a context suggestive of organisations, then at the partial matching stageall instances of \Lockheed Martin Production", \Lockheed Martin", \Lockheed Production", \MartinProduction", \Lockheed" and \Martin" will be marked as possible organizations.
This markup, however,is not denite since some of these words (such as \Martin") could refer to a dierent entity.This annotated stream goes to a second tool, a pre-trained maximum entropy model.
It takes intoaccount contextual information for named entities, such as their position in the sentence, whether thesewords exist in lowercase and if they were used in lowercase in the document, etc.
These features arepassed to the model as attributes of the partially matched words.
If the model provides a positiveanswer for a partial match, the match is wrapped into a corresponding ENAMEX element.
Figure 3 givesan example of this....<W C=IN>of<W> <W C=NNP M='+'>Lockheed<W> <W C=NNP L=d M=ORGANIZATION>Production<W> <W C=IN>in<W>...Figure 3: Partially matched organization name \Lockheed Production".
The attribute M species that\Lockheed" is a part but not a terminal word of the partial match and that \Production" is the terminalword and the class of the match is ORGANIZATION.
This kind of markup allows us to pass relevantfeatures to the decision making module without premature commitment.ENAMEX: 3.
Rule RelaxationOnce this has been done, the system again applies the symbolic transduction rules.
But this time therules have much more relaxed contextual constraints and extensively use the information from alreadyexisting markup and lexicons.
For instance, the system will mark word sequences which look like personnames.
For this it uses a grammar of names: if the rst capitalised word occurs in a list of rst namesand the following word(s) are unknown capitalised words, then this string can be tagged as a PERSON.Here we are no longer concerned that a person name can refer to a company.
If the name grammarhad applied earlier in the process, it might erroneously have tagged \Philip Morris" as a PERSON insteadof an ORGANISATION.
However, at this point in the chain of enamex processing, that is not a problem6anymore: \Philip Morris" will by now already have been identied as an ORGANISATION by the sure-rerules or during partial matching.
If the author of the article had also been referring to the person\Philip Morris", s/he would have used explicit context to make this clear, and our muc system wouldhave detected this.
If there had been no supportive context so far for \Philip Morris" as organisationor person, then the name grammar at this stage will tag it as a likely person, and check if there issupportive context for that hypothesis.At this stage the system will also attempt to resolve the \and" conjunction problem noted above with\this was good news for China International Trust and Investment Corp".
The system checks if possibleparts of the conjunctions were used in the text on their own and thus are names of dierent organizations;if not, the system has no reason to assume that more than one company is being talked about.In a similar vein, the system resolves the attachment of sentence initial capitalised modiers, the problemalluded to above with the \Suspended Ceiling Contractors Ltd" example: if the modier was seen withthe organization name elsewhere in the text, with a capital letter and not at the start of a sentence,then the system has good evidence that the modier is part of the company name; if the modier doesnot occur anywhere else in the text with the company name, it is assumed not to be part of it.At this stage known organizations and locations from the lists available to the system are marked in thetext, again without checking the context in which they occur.ENAMEX: Partial Match 2At this point, the system has exhausted its resources (name grammar, list of locations, etc).
Thesystem then performs another partial match to annotate names like \White" when \James White" hadalready been recognised as a person, and to annotate company names like \Hughes" when \HughesCommunications Ltd." had already been identied as an organisation.
As in Partial Match 1, thisprocess of partial matching is again followed by a probabilistic assignment supported by the maximumentropy model.ENAMEX: Title AssignmentBecause titles of news wires are in capital letters, they provide little guidance for the recognition ofnames.
In the nal stage of enamex processing, entities in the title are marked up, by matching orpartially matching the entities found in the text, and checking against a maximum-entropymodel trainedon document titles.
For example, in \murdoch satellite explodes on take-off" \Murdoch" willbe tagged as a person because it partially matches \Rupert Murdoch" elsewhere in the text.ENAMEX: ConclusionThe table in Figure 4 shows the progress of the performance of the system through the ve stages.Stage ORGANIZATION PERSON LOCATIONSure-re Rules R: 42 P: 98 R: 40 P: 99 R: 36 P: 96Partial Match 1 R: 75 P: 98 R: 80 P: 99 R: 69 P: 93Relaxed Rules R: 83 P: 96 R: 90 P: 98 R: 86 P: 93Partial Match 2 R: 85 P: 96 R: 93 P: 97 R: 88 P: 93Title Assignment R: 91 P: 95 R: 95 P: 97 R: 95 P: 93Figure 4: Scores obtained by the system through dierent stages of the analysis.
R = recall; P = preci-sion.7As one would expect, the sure-re rules give very high precision (around 96-98%), but very low recall|inother words, it doesn't nd many enamex entities, but the ones it nds are correct.
Note that the sure-re rules do not use list information much; the high precision is achieved mainly through the detectionof supportive context for what are in essence unknown names of people, places and organisations.
Recallgoes up dramatically during Partial Match 1, when the knowledge obtained during the rst step (e.g.that this is a text about Washington the person rather than Washington the location) is propagatedfurther through the text, context permitting.
Subsequent phases of processing add gradually more andmore enamex entities (recall increases to around 90%), but on occasion introduce errors (resulting in aslight drop in precision).
Our nal score for ORGANISATION, PERSON and LOCATION is given in the bottomline of Figure 4.WALKTHROUGH EXAMPLES<ENAMEX TYPE="PERSON">MURDOCH</ENAMEX> SATELLITE FOR LATIN PROGRAMMINGEXPLODES ON TAKEOFFThe system correctly tags \Murdoch" as a PERSON, despite the fact that the title is all capitalised, andthere is little supportive context.
The reason for this is that elsewhere in the text there are sentenceslike \dealing a potential blow to Rupert Murdoch's ambitions", and the system correctly analysed\Rupert Murdoch" as a PERSON, on the basis of its grammar of names (see enamex: Relaxed Rules).During Partial Match 2, the partial orders of this name are generated and any occurrences of \Rupert"and \Murdoch" are tagged as PERSONs (e.g.
in the string \Murdoch-led venture"), context permitting.During the Title Assignment phase, \Murdoch" in the title is then also tagged as PERSON, since there isno context to suggest otherwise.<ENAMEX TYPE="PERSON">Llennel Evangelista</ENAMEX>, a spokesmanfor <ENAMEX TYPE="ORGANIZATION">Intelsat</ENAMEX>, a globalsatellite consortium ...\Llennel Evangelista" is correctly tagged as PERSON.
Our grammar of names would not have been ableto detect this, since it didn't have \Llennel" as a possible Christian name; this again illustrates thatit is dangerous to rely too much on resources like lists of Christian names, since these will never becomplete.
However, our muc system detected that \Lennel Evangelista" is a person at a much earlierstage: because of the sure-re rule that in clauses like \Xxxx, a JJ* PROFESSION for/of/in ORG", thestring of unknown, capitalized words Xxxx refers to a PERSON.
Using partial matching, \Evangelista" in\Evangelista said..." was also tagged as PERSON.\Intelsat" was correctly tagged as an ORGANISATION because of the context in which if appears: \Xxxx,a JJ* consortium/company/...".
During Partial Matching, other occurrences of \Intelsat" are markedas ORGANISATION, e.g.
in \Intelsat satellite".<ENAMEX TYPE="ORGANIZATION">Grupo Televisa</ENAMEX> and<ENAMEX TYPE="ORGANIZATION">Globo</ENAMEX> plan to offer...\Grupo Televisa" was correctly identied as an ORGANIZATION.
Elsewhere the same text mentions \Grupo Televisa SA, the Mexican broadcaster", which is recognised as an ORGANIZATION because it knowsthat \Xxxx SA/NV/Ltd..." are names of organisations.
Through partial matching, \Grupo Televisa"without the \SA" is also recognised as an ORGANIZATION.\Globo" is recognised as an ORGANIZATION because elsewhere in the text there is reasonably evidencethat \Globo" is the name of an organisation.
In addition, there is a conjunction rule which prefersconjunctions of like entities.8This conjunction rule also worked for the string \in U7ited States and Russia": \Russia" is in the list oflocations and in a context supportive of locations; because of the typo, \U7ited States" was not in thelist of locations.
But because of the conjunction rule, it is correctly tagged as a LOCATION nevertheless.EVALUATIONOur system achieved a combined Precision and Recall score of 93.39.
This was the highest score of theparticipating named entity recognition systems.
Here is a breakdown of our scores:POS ACT| COR INC MIS SPU NON| REC PRE------------------------+------------------------+---------SUBTASK SCORES | |enamex | |person 883 872| 842 24 17 6 4| 95 97organization 1854 1784|1692 10 152 82 31| 91 95location 1308 1326|1239 14 55 73 21| 95 93timex | |date 1201 1079|1063 3 135 13 61| 89 99time 191 156| 151 4 36 1 29| 79 97numex | |money 216 210| 204 0 12 6 11| 94 97percent 100 103| 100 0 0 3 0| 100 97------------------------+------------------------+---------P&R 2P&R P&2RF-MEASURES 93.39 94.51 92.29Figure 5: LTG Scores for the Named Entity Recognition task.In what follows, we will discuss our system performance in each of the Named Entity categories.
Ingeneral, our system performed very well in all categories.
But the reason our system outperformed othersystems was due to its performance in the category ORGANIZATION where it scored signicantly betterthan the next best system: 91 precision and 95 recall, whereas the next best system scored 87 precisionand 89 recall.
We attribute this to the fact that our system does not rely much on pre-established lists,but instead builds document-specic lists on they, looking for sure-re contexts to make decisions aboutnames of organisations, and on the use of partial orders of multi-word entities.
This pays o particularlyin the case of organisations, which are often multi-word expressions, containing many common words.ORGANIZATIONOne type of error occurred when a company such as \Granada Group Plc" was referred to just as\Granada", and this word is also a known location.
The location information tended to override the tagsresulting from partial matching, resulting in the wrong tag.
The reason for this is that these metonymicrelations do not always hold: if a text refers to an organisation called the \Pittsburgh Pirates", and itthen refers to \Pittsburgh", it is more likely that \Pittsburgh" is a reference to a location rather thananother reference to that organisation.
In the same vein, the system treats a reference to \Granada" asa location, even after reference has been made to the organisation \Granada Group Plc", in the absenceof clear contextual clues to the contrary.A second type of error resulted from wrongly resolving conjunctions in company names, as in <ORG>Smithand Ivanoff Inc.</ORG> As explained above, the system's strategy was to assume the conjunction9referred to a single organisation, unless its constituent parts occurred on its list of known companiesor occurred on their own elsewhere in the text.
In some cases, the absence of such information led tomistaggings, which are penalised quite heavily: you lose once in recall (since the system did not recognisethe name of the company) and twice in precision (since the system produced two spurious names).Many spurious taggings in ORGANIZATION were caused by the fact that artefacts like newpapers orTV channels have very similar contexts to ORGANIZATIONs, resulting in mistaggings.
For instance, in\editor of the Pacic Report", the string \Pacic Report" was wrongly tagged as an ORGANISATIONbecause of the otherwise very productive rule which says that Xxxx in \PROF of/at/with Xxxx" shouldbe tagged as an ORGANIZATION.The misses consisted mostly of short expressions mentioned just once in the text and without a suggestivecontext.
As a result, the system did not have enough information to tag these terms correctly.
Also,there were about 40 mentions of the Ariane 4 and 5 rockets, and according to the answer keys \Ariane"should have been tagged as organisation in each case, accounting for 40 of the 152 misses.PERSONThe PERSON category did not present too many diculties to our system.
The system handled a fewdicult cases well when an expression \sounded" like a person name but in fact was not, e.g.
\GerardKlauer" in \a Gerard Klauer analyst"|the example discussed above.One article was responsible for quite a few errors: in an article about Timothy Leary's death, \TimothyLeary" was twice and \Zachary Leary" seven times recognised as a PERSON; but 11 other mentions of\Leary" were wrongly tagged as ORGANIZATION.
The reason for this was the phrase \...family memberswith Leary when he died".
The system applied the rule PROFs of/for/with Xxxx+ ==> ORGANIZATION.
The word \members" was listed in the lexicon as a profession and this caused \Leary" to be wronglytagged as ORGANIZATION.
This accounts for 11 of the 24 incorrectly tagged PERSONs.Most of the 17 missing person names were one-word expressions mentioned just once in the text, andthe system did not have enough information to perform a classication.LOCATIONLOCATION was the most disappointing category for us.
Just one word (\Columbia") which was taggedas location but in fact was the name of a space-shuttle was responsible for 38 of the 73 spurious assign-ments.
The problem arose from sentences like \Columbia is to blast o from NASA's Kennedy SpaceCenter...", where we erroneously tagged \Columbia" as a location.
Interestingly, we correctly did nottag \Columbia" in the string \space shuttle Columbia"; this was correctly recognised by the system asan artefact.
In the Named Entity Recognition Task one does not have to mark up artefacts, but it isuseful to recognise them nevertheless: using the partial matching rule, the system now also knew that\Columbia" was the likely name of an artefact and should not be marked up.Unfortunately, the text also contained the expression \a satellite 13 miles from Columbia".
This contextis strongly suggestive of LOCATION.
That, and the fact that \Columbia" occurs in the list of placenames,overruled the evidence that it referred to an artefact.Out of the 55 misses, 30 were due to not assigning LOCATION tags to various heavenly bodies.TIMEXIn the TIMEX category we have relatively low recall.
Our failure to markup expressions was sometimesdue to underspecication in the guidelines and the training data; with the corrected answer keys ourrecall for times went up from 79 to 85.
Apart from this, we also failed to recognise expressions like \thesecond day of the shuttle's 10-day mission", \the scal year starting Oct. 1" , etc, which need to be10marked as timex expressions in their entirety.
And we did not group expressions like \from August 1993to July 1995" into one group but tagged them as two temporal expressions (which gives three errors).NUMEXIn the NUMEX category most of our errors came from the fact that we preferred simple constructions overmore complex groupings.
For instance, \between $300 million and $700 million" we didn't tag as a singlenumex expression, but instead tagged it asbetween <NUMEX TYPE="MONEY">$300 million</NUMEX> and <NUMEX TYPE="MONEY">$700 million</NUMEX>CONCLUSIONOne of the design features of our system which sets it apart from other systems is that it is designedfully within the sgml paradigm: the system is composed from several tools which are connected via apipeline with data encoded in sgml.
This allows the same tool to apply dierent strategies to dierentparts of the texts using dierent resources.
The tools do not convert from sgml into an internal formatand back, but operate at the sgml level.Our system does not rely heavily on lists or gazetteers but instead treats information from such lists as\likely" and concentrates on nding contexts in which such likely expressions are denite.
In fact, therst phase of the enamex analysis uses virtually no lists but still achieves substantial recall.The system is document centred.
This means that at each stage the system makes decisions according toa condence level that is specic to that processing stage, and drawing on information from other partsof the document.
The system is truly hybrid, applying symbolic rules and statistical partial matchingtechniques in an interleaved fashion.Unsurprisingly the major problem for the system were single capitalised words, mentioned just once ortwice in the text and without suggestive contexts.
In such a case the system could not apply contextualassignment, assignment by analogy or lexical lookup.At the time we participated in the muc competition, our system was not particularly fast|it operated atabout 8 words per second, taking around 3 hours to process the 100 articles.
This has now considerablyimproved.AcknowledgementsThe work reported in this paper was supported in part by grant GR/L21952 (Text Tokenisation Tool)from the UK Engineering and Physical Sciences Research Council.
For help during the system buildingthe authors wish to thank Colin Matheson of the LTG for writing a grammar for handling numericalexpressions and testing the system on the Wall Street Journal, and Steve Finch of Thomson Technologiesand Irina Nazarova of Edinburgh Parallel Computing Center for helping us build lexical resources for thesystem.
We would also like to acknowledge that this work was based on a long-standing collaborativerelationship with Steve Finch who was involved in the design of many of the tools which we later usedduring the muc system development.References[1] C. Brew and D. McKelvie 1996.
\Word Pair Extraction for Lexicography."
In Proceedings ofNeM-LaP'96, pp 44{55.
Ankara, Turkey.11[2] S. Finch and M. Moens 1995.
\SISTA nal report."
Available from: Language Technology Group,University of Edinburgh.
[3] E.R.
Harold.
1998.
\XML: Extensible Markup Language.
Structuring Complex Content for theWeb."
Foster City/Chicago/New York: IDG Books Worldwide.
[4] A. Mikheev and S. Finch 1995.
\Towards a Workbench for Acquisition of Domain Knowledge fromNatural Language."
In Proceedings of the 7th Conference of the European Chapter of the Associationfor Computational Linguistics (EACL'95), pp 194{201.
Dublin.
[5] A. Mikheev and S. Finch 1997.
\A Workbench for Finding Structure in Texts" In Proceedings ofthe Fifth Conference on Applied Natural Language Processing, pp 372{379.
Washington D.C.[6] A. Mikheev.
1997 \Automatic Rule Induction for Unknown Word Guessing."
In ComputationalLinguistics 23 (3), pp 405{423.
[7] A. Mikheev.
1997 \LT POS { the LTG part of speech tagger."
Language Technology Group,University of Edinburgh.
http://www.ltg.ed.ac.uk/software/pos[8] A. Mikheev.
1998 \Feature Lattices for Maximum Entropy Modelling" In Proceedings of the 36thConference of the Association for Computational Linguistics, pp 848{854.
Montreal, Quebec.
[9] D. McKelvie, C. Brew and H.S.
Thompson 1998.
\Using SGML as a Basis for Data-IntensiveNatural Language Processing."
In Computers and the Humanities 35, pp 367{388.
[10] H.S.
Thompson, D. McKelvie and S. Finch 1997.
\The Normalised SGML Li-brary LT NSL version 1.5."
Language Technology Group, University of Edinburgh.http://www.ltg.ed.ac.uk/software/nsl[11] H.S.
Thompson, C. Brew, D. McKelvie, A. Mikheev and R. Tobin 1998.
\The XMLLibrary LT XML version 1.0."
Language Technology Group, University of Edinburgh.http://www.ltg.ed.ac.uk/software/xml12
