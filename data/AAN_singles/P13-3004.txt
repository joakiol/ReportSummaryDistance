Proceedings of the ACL Student Research Workshop, pages 23?30,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDetecting Metaphor by Contextual AnalogyEirini FlorouDept of Linguistics, Faculty of PhilosophyUniversity of Athens, Greeceeirini.florou@gmail.comAbstractAs one of the most challenging issues inNLP, metaphor identification and its in-terpretation have seen many models andmethods proposed.
This paper presents astudy on metaphor identification based onthe semantic similarity between literal andnon literal meanings of words that can ap-pear at the same context.1 IntroductionA metaphor is a literary figure of speech that de-scribes a subject by asserting that it is, on somepoint of comparison, the same as another other-wise unrelated object.
Metaphor is a type of anal-ogy and is closely related to other rhetorical fig-ures of speech that achieve their effects via asso-ciation, comparison or resemblance including al-legory, hyperbole, and simile.
Rhetorical theo-rists and other scholars of language have discussednumerous dimensions of metaphors, though thesenomenclatures are by no means universal nor nec-essarily mutually exclusive.A very challenging task in linguistics is themetaphor identification and the its interpreta-tion.
Metaphor identification procedure (MIP)is a method for identifying metaphorically usedwords in discourse.
It can be used to recognizemetaphors in spoken and written language.
Theprocedure aims to determine the relationship ofa particular lexical unit in the discourse and rec-ognize its use in a particular context as possiblymetaphorical.
Since many words can be consid-ered metaphorical in different contexts, MIP re-quires a clear distinction between words that con-vey metaphorical meaning and those that do not,despite the fact that language generally differs inthe degrees of metaphoricity.In this paper we propose a method for identi-fying metaphorical usage in verbs.
Our methodis looking for semantic analogies in the contextof a verb by comparing it against prior known in-stances of literal and non-literal usage of the sameverb in different contexts.
After discussing themetaphor identication literature (Section 2), weproceed to present our research proposal (Section3) and to present and discuss our first experimentsbased on WordNet similarity measures (Section4).
Experiment results help us to draw conclu-sions and insights about analogical reasoning andmemory-based learning for this task and to outlinepromising research paths (Section 5).2 BackgroundAccording to Lakoff and Johnson (1980),metaphor is a productive phenomenon that oper-ates at the level of mental processes.
Metaphoris thus not merely a property of language, butrather a property of thought.
This view was sub-sequently acquired and extended by a multitudeof approaches (Grady, 1997; Narayanan, 1997;Fauconnier and Tuner, 2002; Feldman, 2006;Pinker, 2007) and the term conceptual metaphorwas adopted to describe it.In cognitive linguistics, conceptual metaphor, orcognitive metaphor, refers to the understanding ofan idea, or conceptual domain, in terms of another,for example, understanding quantity in terms ofdirectionality as in, for example, ?prices are ris-ing?.
A conceptual metaphor uses an idea andlinks it to another in order to better understandsomething.
It is generaly accepted that the concep-tual metaphor of viewing communication as a con-duit is a large theory explained with a metaphor.These metaphors are prevalent in communicationand everyone actually perceives and acts in accor-dance with the metaphors.2.1 Metaphor IdentificationAutomatic processing of metaphor can be clearlydivided into two subtasks: metaphor identifica-23tion (distinguishing between literal and metaphor-ical language in text) and metaphor interpreta-tion (identifying the intended literal meaning of ametaphorical expression).
Both of them have beenrepeatedly attempted in NLP.The most influential account of metaphor iden-tification is that of Wilks (1978).
According toWilks, metaphors represent a violation of selec-tional restrictions in a given context.
Consider anexample such as My car drinks gasoline; the verbdrink normally takes an animate subject and a liq-uid object.This approach was automated by Fass (1991)in his MET* system.
However, Fass himself in-dicated a problem with the method: it detectsany kind of non-literalness or anomaly in lan-guage (metaphors, metonymies and others), i.e.,it overgenerates with respect to metaphor.
Thetechniques MET* uses to differentiate betweenthose are mainly based on hand-coded knowledge,which implies a number of limitations.
First, lit-eralness is distinguished from non-literalness us-ing selectional preference violation as an indica-tor.
In the case that non-literalness is detected, therespective phrase is tested for being a metonymicrelation using hand-coded patterns.
If the systemfails to recognize metonymy, it proceeds to searchthe knowledge base for a relevant analogy in or-der to discriminate metaphorical relations fromanomalous ones.Berber Sardinha (2002) describes a collocation-based method for spotting metaphors in corpora.His procedure is based on the notion that twowords sharing collocations in a corpus may havebeen used metaphorically.
The first step was topick out a reasonable number of words that hadan initial likelihood of being part of metaphori-cal expressions.
First, words with marked fre-quency (in relation to a large general corpus ofPortuguese) were selected.
Then, their colloca-tions were scored for closeness in meaning usinga program called ?distance?
(Padwardhan et al2003), under the assumption that words involvedin metaphorical expressions tend to be denota-tionally unrelated.
This program accesses Word-Net in order to set the scores for each word pair.The scores had to be adapted in order for themto be useful for metaphor analysis.
Finally, thosewords that had an acceptable semantic distancescore were evaluated for their metaphoric poten-tial.
The results indicated that the procedure didpick up some major metaphors in the corpus, butit also captured metonyms.Another approach to finding metaphor in cor-pora is CorMet, presented by Mason (2004).
Itworks by searching corpora of different domainsfor verbs that are used in similar patterns.
Whenthe system spots different verbs with similar se-lectional preferences (i.e., with similar words insubject, object and complement positions), it con-siders them potential metaphors.CorMet requires specific domain corpora and alist of verbs for each domain.
The specific do-main corpora are compiled by searching the webfor domain-specific words.
These words are se-lected by the author, based on his previous knowl-edge of subject areas and are stemmed.
The mosttypical verbs for each specific corpus are identifiedthrough frequency markedness, by comparing thefrequencies of word stems in the domain corpuswith those of the BNC.
The resulting words have afrequency that is statistically higher in the domaincorpus than in the reference corpus.
These stemsare then classified according to part of speech byconsulting WordNet.Alternative approaches search for metaphorsof a specific domain defined a priori in a spe-cific type of discourse.
The method by Gedi-gian et al(2006) discriminates between literal andmetaphorical use.
They trained a maximum en-tropy classifier for this purpose.
They obtainedtheir data by extracting the lexical items whoseframes are related to MOTION and CURE fromFrameNet (Fillmore et al 2003).
Then, theysearched the PropBank Wall Street Journal corpus(Kingsbury and Palmer, 2002) for sentences con-taining such lexical items and annotated them withrespect to metaphoricity.Birke and Sarkar (2006) present a sentence clus-tering approach for non-literal language recog-nition implemented in the TroFi system (TropeFinder).
This idea originates from a similarity-based word sense disambiguation method devel-oped by Karov and Edelman (1998).
The methodemploys a set of seed sentences, where the sensesare annotated, computes similarity between thesentence containing the word to be disambiguatedand all of the seed sentences and selects the sensecorresponding to the annotation in the most simi-lar seed sentences.
Birke and Sarkar (2006) adaptthis algorithm to perform a two-way classification:literal vs. non-literal, and they do not clearly de-24fine the kinds of tropes they aim to discover.
Theyattain a performance of 53.8% in terms of f-score.Both Birke and Sarkar (2006) and Gedigianet al(2006) focus only on metaphors expressedby a verb.
As opposed to that the approach of Kr-ishnakumaran and Zhu (2007) deals with verbs,nouns and adjectives as parts of speech.
Theyuse hyponymy relation in WordNet and word bi-gram counts to predict metaphors at the sentencelevel.
Given an IS-A metaphor (e.g.
The world isa stage) they verify if the two nouns involved arein hyponymy relation in WordNet, and if this isnot the case then this sentence is tagged as con-taining a metaphor.
Along with this they con-sider expressions containing a verb or an adjec-tive used metaphorically.
Hereby they calculatebigram probabilities of verb-noun and adjective-noun pairs (including the hyponyms/hypernymsof the noun in question).
If the combinationis not observed in the data with sufficient fre-quency, the system tags the sentence containing itas metaphorical.
This idea is a modification of theselectional preference view of Wilks (1978).Berber Sardinha (2010) presents a computerprogram for identifying metaphor candidates,which is intended as a tool that can help re-searchers find words that are more likely to bemetaphor vehicles in a corpus.
As such, it may beused as a device for signalling those words that theresearcher might want to focus on first, becausethese have a higher probability of being metaphorsin their corpus, or conversely, it may indicate thosewords that are worth looking at because of theirapparent low probability of being metaphors.
Theprogram is restricted to finding one component oflinguistic metaphors and has been trained on busi-ness texts in Portuguese, and so it is restricted tothat kind of text.Shutova et al(2012) present an approach toautomatic metaphor identification in unrestrictedtext.
Starting from a small seed set of manuallyannotated metaphorical expressions, the system iscapable of harvesting a large number of metaphorsof similar syntactic structure from a corpus.
Theirmethod captures metaphoricity by means of verband noun clustering.
Their system starts froma seed set of metaphorical expressions exempli-fying a range of source-target domain mappings;performs unsupervised noun clustering in orderto harvest various target concepts associated withthe same source domain; by means of unsuper-vised verb clustering creates a source domain verblexicon; searches the BNC for metaphorical ex-pressions describing the target domain conceptsusing the verbs from the source domain lexicon.According to Shutova et al(2012), abstract con-cepts that are associated with the same source do-main are often related to each other on an intu-itive and rather structural level, but their mean-ings, however, are not necessarily synonymous oreven semantically close.
The consensus is thatthe lexical items exposing similar behavior in alarge body of text most likely have the same mean-ing.
They tested their system starting with a col-lection of metaphorical expressions representingverb-subject and verb-object constructions, wherethe verb is used metaphorically.
They evaluatedthe precision of metaphor identification with thehelp of human judges.
Shutova?s system employ-ing unsupervised methods for metaphor identifica-tion operates with precision of 0.79.For verb and noun clustering, they used the sub-categorization frame acquisition system by Preisset al(2007) and spectral clustering for both verbsand nouns.
They acquired selectional preferencedistributions for Verb-Subject and Verb-Object re-lations from the BNC parsed by RASP; adoptedResnik?s selectional preference measure; and ap-plied to a number of tasks in NLP including wordsense disambiguation (Resnik, 1997).3 Detecting the metaphor use of a wordby contextual analogyThe first task for metaphor processing is itsidentification in a text.
We have seen abovehow previous approaches either utilize hand-codedknowledge (Fass, 1991), (Krishnakumaran andZhu, 2007) or reduce the task to searching formetaphors of a specific domain defined a priori ina specific type of discourse (Gedigian et al 2006).By contrast, our research proposal is a methodthat relies on distributional similarity; the assump-tion is that the lexical items showing similar be-haviour in a large body of text most likely haverelated meanings.
Noun clustering, specifically,is central to our approach.
It is traditionally as-sumed that noun clusters produced using distribu-tional clustering contain concepts that are similarto each other.253.1 Word Sense Disambiguation andMetaphorOne of the major developments in metaphor re-search in the last several years has been the fo-cus on identifying and explicating metaphoric lan-guage in real discourse.
Most research in WordSense Disambiguation has concentrated on usingcontextual features, typically neighboring words,to help infer the correct sense of a target word.
Incontrast, we are going to discover the predominantsense of a word from raw text because the firstsense heuristic is so powerful and because man-ually sense-tagged data is not always available.In word sense disambiguation, the first or pre-dominant sense heuristic is used when informa-tion from the context is not sufficient to make amore informed choice.
We will need to use parseddata to find distributionally similar words (near-est neighbors) to the target word which will reflectthe different senses of the word and have associ-ated distributional similarity scores which couldbe used for ranking the senses according to preva-lence.The predominant sense for a target word is de-termined from a prevalence ranking of the possiblesenses for that word.
The senses will come froma predefined inventory which might be a dictio-nary or WordNet-like resource.
The ranking willbe derived using a distributional thesaurus auto-matically produced from a large corpus, and a se-mantic similarity measure will be defined over thesense inventory.
The distributional thesaurus willcontain a set of words that will be ?nearest neigh-bors?
Lin (1998) to the target word with respectto similarity of the way in which they will be dis-tributed.
The thesaurus will assign a distributionalsimilarity score to each neighbor word, indicatingits closeness to the target word.We assume that the number and distributionalsimilarity scores of neighbors pertaining to a givensense of a target word will reflect the prevalence ofthat sense in the corpus from which the thesauruswas derived.
This is because the more prevalentsenses of the word will appear more frequentlyand in more contexts than other, less prevalentsenses.
The neighbors of the target word relateto its senses, but are themselves word forms ratherthan senses.
The senses of the target word haveto be predefined in a sense inventory and we willneed to use a semantic similarity score which willbe defined over the sense inventory to relate theneighbors to the various senses of the target word.The measure for ranking the senses will use thesum total of the distributional similarity scores ofthe k nearest neighbors.
This total will be dividedbetween the senses of the target word by appor-tioning the distributional similarity of each neigh-bor to the senses.
The contribution of each neigh-bor will be measured in terms of its distributionalsimilarity score so that ?nearer?
neighbors countfor more.
The distributional similarity score ofeach neighbor will be divided between the vari-ous senses rather than attributing the neighbor toonly one sense.
This is done because neighborscan relate to more than one sense due to relation-ships such as systematic polysemy.
To sum up, wewill rank the senses of the target word by appor-tioning the distributional similarity scores of thetop k neighbors between the senses.
Each distri-butional similarity score (dss) will be weighted bya normalized semantic similarity score (sss) be-tween the sense and the neighbor.We chose to use the distributional similarityscore described by Lin (1998) because it is an un-parameterized measure which uses pointwise mu-tual information to weight features and which hasbeen shown Weeds et al(2004) to be highly com-petitive in making predictions of semantic similar-ity.
This measure is based on Lin?s information-theoretic similarity theorem (Lin, 1997) : The sim-ilarity between A and B is measured by the ratiobetween the amount of information needed to statethe commonality of A and B and the informationneeded to fully describe what A and B are.3.2 Similarity-based metaphorical usageestimationAfter the noun clustering and finding the predomi-nant sense of an ambiguous word, as the local con-text of this word can give important clues to whichof its senses was intended, the metaphor identifi-cation system will start from a small set of seedmetaphors (the seed metaphors are a model ex-tracted from metaphor-annotated and dependency-parsed sentences), to point out if a word is used lit-eraly or non literaly at the certain context.
For thepurposes of this work as context should be consid-ered the verb of the seed metaphors.
We are goingto take as seed metaphors the examples of Lakoff?sMaster Metaphor List (Lakoff et al 1991).Then, as we will have already find the k nearestneighbors for each noun and we will have created26clusters for nouns which can appear at the samecontext, we will be able to calculate their seman-tic similarity.
We then will use the WordNet sim-ilarity package Padwardhan et al(2003) in orderto measure the semantic similarity between eachmember of the cluster and the noun of the anno-tated metaphor.
The WordNet similarity packagesupports a range of WordNet similarity scores.
Wewill experiment using a lot of these in order tofind those which perform the best.
Each time, wewant to estimate if the similarity between the tar-get noun and the seed metaphor will be higher thanthe similarity between the target noun and anotherliteral word which could appear at the certain con-text.
Calculating the target word?s semantic sim-ilarity with the seed words (literal or non literal)we will be able to find out if the certain word hasa literal or metaphorical meaning at the concretecontext.By this way, starting from an already knownmetaphor, we will be able to identify other non lit-eral uses of words which may appear at the samecontext, estimating the similarity measure of thetarget word between the seed metaphor and an-other literal meaning of a word at the same con-text.
If the semantic similarity?s rate of the targetword (for instance the word ?assistance?
at the con-text of the verb ?give?)
and the annotated metaphor(like ?quidance?
at the certaincontext) is higherthat the rate of the target word and the seed wordwith the literal meaning (for example the word?apple?
at the same context) , then we will be ableto assume that the tartget word is used metaphori-cally, at the concrete context.4 First Experiments and ResultsIn order to evaluate our method we search for com-mon English verbs which can take either literalor non literal predicates.
As the most commonverbs (be, have and do) can function as verbs andauxiliary verbs, we didn?t use them for our ex-periments.
As a consequence, we chose commonfunction verbs which can take a direct object aspredicate.
More specifically, at our experimentswe concentrated on literal and non literal predi-cates of the verbs: break, catch, cut, draw, drop,find, get, hate, hear, hold, keep, kill, leave, listen,lose, love, make, pay, put, save, see, take, want.We used the VU Amsterdam Metaphor Corpus11Please see http://www.metaphorlab.vu.nl/en/research/funded_research/in order to extract data for our experiments.
Weused shallow heuristics to match verbs and directobjects, with manually checking and correctingthe result.
We have also used the British NationalCorpus (BNC), in order to take more samples,mostly literal.
In the case of he BNC, we wereable to extract the direct object from the depencyparses, but had manually controlled metaphoricalvs.
literal usage.
In all, we collected 124 instancesof literal usage and 275 instances of non-literal us-age involving 311 unique nouns.With this body of literal and non-literal con-texts, we tried every possible combination of oneliteral and one non-literal object for each verb asseed, and tested with the remaining words.
Themean results are collected in Table 1, where we seehow the LCS-based measures by Resnik (1997)and Wu and Palmer (1994) performed the best.One observation is that the differences betweenthe different measures although significant, theyare not as dramatic as to effect reversals in thedecision.
This is apparent in the simple votingresults (right-most column in Table 1) where allmeasures yield identical results.
Only when dif-ferences in the similarities accumulate before thecomparison between literal and non-literal contextis made (three left-most columns in Table 1), doesthe choice of similarity measure make a differ-ence.Another observation pertains to relaxing the de-pendency on WordNet so that method can be basedon similarities defined over more widely availablelexical resources.
In this respect, the low F-scoreby the adapted Lesk measure is not very encourag-ing, as variations of the Lesk measure could be de-fined over the glosses in digital dictionaries with-out explicit WordNet-style relations.
Combinedwith the high valuation of methods using the LCS,this leads us to conclude that the relative taxo-nomic position is a very important factor.Finally, and happily counter to our prior in-tuition, we would like to note the robustness ofthe method to the number of different senses testwords have: plotting the F-score against the num-ber of senses did not result in consistently de-teriorating results as the senses multiply (Fig-ure 1).2 If this had happened, we would have con-VU-Amsterdam-Metaphor-Corpus2Although some of the nouns in our collection have asmany as 33 senses, we have only plotted the data for up to 15senses; the data is too sparse to be reasonably usuable beyondthat point.27Table 1: F?=1 scores for all combinations of seven different similarity measures and five ways of derivinga single judgement on literal usage by testing all senses of a word against all senses of the seed words.Measure Maximum Average Sum Simple VotingMean Std dev Mean Std dev Mean Std dev Mean Std devAdapted Lesk 63.87 6.96 63.39 9.41 64.77 6.47 68.64 10.69Jiang et al(1997) 70.92 9.19 64.31 8.41 65.14 6.45 68.64 10.69Lin (1998) 71.35 10.70 70.39 10.02 70.07 9.47 68.64 10.69Path length 67.63 9.83 72.60 8.83 65.33 6.91 68.64 10.69Resnik (1993) 66.14 9.13 72.92 9.08 70.54 8.24 68.64 10.69Wu and Palmer (1994) 70.84 9.38 72.97 9.05 66.02 6.82 68.64 10.69Resnik graphPage 11 2 3 4 5 6 7 8 9 10 11 12 13 140102030405060708090100WUP graphPage 11 2 3 4 5 6 7 8 9 10 11 12 13 140102030405060708090100Resnik (1997) Wu and Palmer (1994)Figure 1: Plot of precision (dotted line, circles), recall (dotted line, triangles), and F?=1 score (solidline) versus the number of different senses for a word.
Also includes the frequency of each sense count(dashed line, squares).
For both measures, final judgement is made on average similarity of all senses.fronted a Catch-22 situation where disambiguationis needed in order to carry out metaphora iden-tification, a disambiguation task itself.
The waythings stand, our method can be successfully ap-plied to shallow NLP tasks or as a pre-processingand optimization step for WSD and parsing.5 ConclusionsIn this paper, we presented a mildly supervisedmethod for identifying metaphorical verb usage bytaking the local context into account.
This proce-dure is different from the majority of the previousworks in that it does not rely on any metaphor-specic hand-coded knowledge, but rather on pre-vious observed unambiguous usages of the verb.The method can operates on open domain textsand the memory needed for the seeds can be rela-tively easily collected by mining unannotated cor-pora.
Furthermore, our method differs as com-pares the meaning of nouns which appear at thesame context without associating them with con-cepts and then comparing the concepts.
We se-lected this procedure as words of the same abstractconcept maybe not appear at the same contextwhile words from different concepts could appearat the same context, especially when the certaincontext is metaphorical.
Although the system hasbeen tested only on verb-direct object metaphors,the described identi- cation method should be im-mediately applicable to a wider range of wordclasses, which is one of the future research direc-tions we will pursue.
Another promising researchdirection relates to our observation regarding theimportance of measuring similarities by consider-ing the relative taxonomic position of the two con-cepts; more specifically, we will experiment withclustering methods over unannotated corpora as away of producing the taxonomy over which wewill dene some Resnik-esque similarity measure.28ReferencesTony Berber Sardinha.
2002.
Metaphor in earlyapplied linguistics writing: A corpus-basedanalysis of lexis in dissertations.
In I Confer-ence on Metaphor in Language and Thought.Tony Berber Sardinha.
2010.
Creating and usingthe Corpus do Portugues and the frequency dic-tionary of portuguese.
Working with PortugueseCorpora.Julia Birke and Anoop Sarkar.
2006.
A clusteringapproach for the nearly unsupervised recogni-tion of nonliteral language.
In Proceedings ofEACL-06, pages 329?336.
Trento, Italy.Dan Fass.
1991. met*: a method for discrimi-nating metonymy and metaphor by computer.Computational Linguistics, 17(1).Gilles Fauconnier and Mark Tuner.
2002.
The WayWe Think: Conceptual Blending and the Mind?sHidden Complexities.
Basic Books.Jerome Feldman.
2006.
From Molecule toMetaphor: A Neutral Theory of Language.
TheMIT Press.Charles Fillmore, Christopher Johnson andMiriam Petruck.
2003.
Background toframenet.
International Journal of Lexicogra-phy, 16(3):235?250.Matt Gedigian, John Bryant, Srini Narayanan, andBranimir Ciric.
2006.
Catching metaphors.
InProceedings of the 3rd Workshop on ScalableNatural Language Understanding, pages 41?48.
New York.Joseph Edward Grady.
1997.
Foundations ofmeaning: primary metaphors and primaryscenes.
University Microfilms International.Yael Karov and Shimon Edelman.
1998.Similarity-based word sense disambigua-tion.
Computational Linguistics, 24(1):41?59.Paul Kingsbury and Martha Palmer.
2002.
Fromtreebank to propbank.
In Proceedings of LREC-2002, pages 1989?1993.
Gran Canaria, CanaryIslands, Spain.Saisuresh Krishnakumaran and Xiaojin Zhu.2007.
Hunting elusive metaphors using lex-ical resources.
In Proceedings of the Work-shop on Computational Approaches to Fig-urative Language, April, 2007, Rochester,New York, pages 13?20.
Association forComputational Linguistics, Rochester, NewYork.
URL http://www.aclweb.org/anthology/W/W07/W07-0103.George Lakoff, Jane Espenson, and AlanSchwartz.
1991.
The master metaphor list.University of California at Berkeley.George Lakoff and Mark Johnson.
1980.Metaphors We Live By.
University of ChicagoPress.Dekang Lin.
1997.
Using syntactic dependencyas local context to resolve word sense ambigu-ity.
In Proceedings of ACL-97, pages 64?71.Madrid, Spain.Dekang Lin.
1998.
An information-theoretic def-inition of similarity.
In Proceedings of the 15thInternational Conference on Machine Learn-ing (ICML-98).
Madison, WI, USA, July 1998.,page 296304.Zachary Mason.
2004.
Cormet: a computational,corpus-based conventional metaphor extractionsystem.
Computational Linguistics, 30(1):23?44.Srini Narayanan.
1997.
Knowledge-based Ac-tion Representations for Metaphor and Aspect(KARMA).
University of Californial.Siddharth Padwardhan, Satanjeev Banerjee, andTed Pedersen.
2003.
Using measures of seman-tic relatedness for word sense disambiguation.In Proceedings of the 4th International Confer-ence on Intelligent Text Processing and Compu-tational Linguistics (CICLing-03), Mexico City,pages 241?257.Stephen Pinker.
2007.
The Stuff of Thought: Lan-guage as a Window into Human Nature.
VikingAdult.Judita Preiss, Ted Briscoe, and Anna Korhonen.2007.
A system for large-scale acquisition ofverbal, nominal and adjectival subcategoriza-tion frames from corpora.
In Proceedings ofACL-07, volume 45, page 912.Philip Resnik.
1997.
Selectional preference andsense disambiguation.
In ACL SIGLEX Work-shop on Tagging Text with Lexical Semantics.Washington, D.C.Ekaterina Shutova, Simone Teufel and Anna Ko-rhonen.
2012.
Statistical metaphor processing.Computational Linguistics, 39(2).Julie Weeds, David Weir, and Diana McCarthy.2004.
Characterising measures of lexical dis-29tributional similarity.
In Proceedings of Col-ing 2004, pages 1015?1021.
COLING, Geneva,Switzerland.Yorick Wilks.
1978.
Making preferences more ac-tive.
Artificial Intelligence, 11(3).Zhibiao Wu and Martha Palmer.
1994.
Verb se-mantics and lexical selection.
In Proceedings ofthe 32nd Annual Meeting of the ACL (ACL-04),pages 133?138.30
