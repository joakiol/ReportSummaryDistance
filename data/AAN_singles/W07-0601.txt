Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 1?8,Prague, Czech Republic, June 2007 c?2007 Association for Computational LinguisticsA Linguistic Investigation into Unsupervised DOPRens BodSchool of Computer ScienceUniversity of St AndrewsILLC, University of Amsterdamrb@cs.st-and.ac.ukAbstractUnsupervised Data-Oriented Parsing models(U-DOP) represent a class of structurebootstrapping models that have achievedsome of the best unsupervised parsing resultsin the literature.
While U-DOP wasoriginally proposed as an engineeringapproach to language learning (Bod 2005,2006a), it turns out that the model has anumber of properties that may also be oflinguistic and cognitive interest.
In this paperwe will focus on the original U-DOP modelproposed in Bod (2005) which computes themost probable tree from among the shortestderivations of sentences.
We will show thatthis U-DOP model can learn both rule-basedand exemplar-based aspects of language,ranging from agreement and movementphenomena to discontiguous contructions,provided that productive units of arbitrarysize are allowed.
We argue that our resultssuggest a rapprochement between nativismand empiricism.1  IntroductionThis paper investigates a number of linguistic andcognitive aspects of the unsupervised data-orientedparsing framework, known as U-DOP (Bod 2005,2006a, 2007).
U-DOP is a generalization of the DOPmodel which was originally proposed for supervisedlanguage processing (Bod 1998).
DOP produces andanalyzes new sentences out of largest and mostprobable subtrees from previously analyzedsentences.
DOP maximizes what has been called the?structural analogy?
between a sentence and a corpusof previous sentence-structures (Bod 2006b).
WhileDOP has been successful in some areas, e.g.
inambiguity resolution, there is also a seriousshortcoming to the approach: it does not account forthe acquisition of initial structures.
That is, DOPassumes that the structures of previous linguisticexperiences are already given and stored in a corpus.As such, DOP can at best account for adult languageuse and has nothing to say about language acquisition.In Bod (2005, 2006a), DOP was extended tounsupervised parsing in a rather straightforward way.This new model, termed U-DOP, again starts with thenotion of tree.
But since in language learning we donot yet know which trees should be assigned to initialsentences, it is assumed that a language learner willinitially allow (implicitly) for all possible trees and letlinguistic experience decide which trees are actuallylearned.
That is, U-DOP generates a new sentence byreconstructing it out of the largest possible and mostfrequent subtrees from all possible (binary) trees ofprevious sentences.
This has resulted in state-of-the-art performance for English, German and Chinesecorpora (Bod 2007).Although we do not claim that U-DOP providesany near-to-complete theory of language acquisition,we intend to show in this paper that it can learn avariety of linguistic phenomena, some of which areexemplar-based, such as idiosyncratic constructions,others of which are typically viewed as rule-based,such as auxiliary fronting and subject-verb agreement.We argue that U-DOP can be seen as arapprochement between nativism and empiricism.
Inparticular, we argue that there is a fallacy in theargument that for syntactic facets to be learned theymust be either innate or in the input data: they can justas well emerge from an analogical process withoutever hearing the particular facet and without assumingthat it is hard-wired in the mind.In the following section, we will start byreviewing the original DOP framework in Bod(1998).
In section 3 we will show how DOP can be1generalized to language learning, resulting in U-DOP.Next, in section 4, we show that the approach canlearn idiosyncratic constructions.
In section 5 wediscuss how U-DOP can learn agreement phenomena,and in section 6 we extend our argument to auxiliarymovement.
We end with a conclusion.2  Review of ?supervised?
DOPIn its original version, DOP derives new sentences bycombining subtrees from previously derived sentences.One of the main motivations behind the DOPframework was to integrate rule-based and exemplar-based aspects of language processing (Bod 1998).
Asimple example may illustrate the approach.
Consideran extremely small corpus of only two phrase-structuretrees that are labeled by traditional categories, shown infigure 1.theNPPon rackPPtheNPdressNPVwantedVPNPsheStheNPPwith telescopePPtheNPsaw dogVPVVPNPsheSFigure 1.
An extremely small corpus of two treesA new sentence can be derived by combining subtreesfrom the trees in the corpus.
The combinationoperation between subtrees is called labelsubstitution, indicated as ?.
Starting out with thecorpus of figure 1, for instance, the sentence She sawthe dress with the telescope may be derived as shownin figure 2.theNPPwith telescopePPNPsawVPVVPNPsheStheNPdress=theNPPwith telescopePPtheNPsawVPVVPNPsheS?dressFigure 2.
Analyzing a new sentence by combining subtreesfrom figure 1We can also derive an alternative tree structure forthis sentence, namely by combining three (rather thantwo) subtrees from figure 1, as shown in figure 3.
Wewill write (t ?
u) ?
v  as t ?
u ?
v with the conventionthat ?
is left-associative.PPtheNPdressNPVVPNPsheSsawVtheNPPwith telescopePP =?
?PPtheNPdressNPVVPNPsheSsaw theNPPwith telescopeFigure 3.
A different derivation for the same sentenceDOP?s subtrees can be of arbitrary size: theycan range from context-free rewrite rules to entiresentence-analyses.
This makes the model sensitive tomulti-word units, idioms and other idiosyncraticconstructions, while still maintaining fullproductivity.
DOP is consonant with the view, asexpressed by certain usage-based and constructionistaccounts in linguistics, that any string of words canfunction as a construction (Croft 2001; Tomasello2003; Goldberg 2006; Bybee 2006).
In DOP suchconstructions are formalized as lexicalized subtrees,which form the productive units of a Stochastic Tree-Substitution Grammar or STSG.Note that an unlimited number of sentencescan be derived by combining subtrees from the corpusin figure 1.
However, virtually every sentencegenerated in this way is highly ambiguous, yieldingseveral syntactic analyses.
Yet, most of these analysesdo not correspond to the structure humans perceive.Initial DOP models proposed an exclusivelyprobabilistic metric to rank different candidates,where the ?best?
tree was computed from thefrequencies of subtrees in the corpus (see Bod 1998).While it is well known that the frequency of astructure is a very important factor in languagecomprehension and production (Jurafsky 2003), it isnot the only factor.
Discourse context, semantics andrecency also play an important role.
DOP canstraightforwardly take into account discourse andsemantic information if we have corpora with suchinformation from which we take our subtrees, and thenotion of recency can be incorporated by a frequency-adjustment function (Bod 1998).
There is, however,an important other factor which does not correspondto the notion of frequency: this is the simplicity of astructure (cf.
Frazier 1978; Chater 1999).
In Bod(2002), the simplest structure was formalized by theshortest derivation of a sentence consisting of thefewest subtrees from the corpus.
Note that the shortestderivation will include the largest possible subtreesfrom the corpus, thereby maximizing the structuraloverlap between a sentence and previous sentence-2structures.
Only in case the shortest derivation is notunique, the frequencies of the subtrees are used tobreak ties among the shortest derivations.
This DOPmodel assumes that language users maximize whathas been called the structural analogy between asentence and previous sentence-structures bycomputing the most probable tree with largeststructural overlaps between a sentence and a corpus.We will use this DOP  model from Bod (2002) as thebasis for our investigation of unsupervised DOP.We can illustrate DOP?s notion of structuralanalogy with the examples given in the figures above.DOP predicts that the tree structure in figure 2 ispreferred because it can be generated by just twosubtrees from the corpus.
Any other tree structure,such as in figure 3, would need at least three subtreesfrom the training set in figure 1.
Note that the treegenerated by the shortest derivation indeed tends to bestructurally more similar to the corpus (i.e.
having alarger overlap with one of the corpus trees) than thetree generated by the longer derivation.
Had werestricted the subtrees to smaller sizes -- for exampleto depth-1 subtrees, which makes DOP equivalent to a(probabilistic) context-free grammar -- the shortestderivation would not be able to distinguish betweenthe two trees in figures 2 and 3 as they would both begenerated by 9 rewrite rules.When the shortest derivation is not unique, weuse the subtree frequencies to break ties.
The ?besttree?
of a sentence is defined as the most probable treegenerated by a shortest derivation of the sentence,also referred to as ?MPSD?.
The probability of a treecan be computed from the relative frequencies of itssubtrees, and the definitions are standard forStochastic Tree-Substitution Grammars (STSGs), seee.g.
Manning and Sch?tze (1999) or Bod (2002).Interestingly, we will see that the exact computationof probabilities is not necessary for our arguments inthis paper.3  U-DOP: from sentences to structuresDOP can be generalized to language learning by usingthe same principle as before: language usersmaximize the structural analogy between a newsentence and previous sentence-structures bycomputing the most probable shortest derivation.However, in language learning we cannot assume thatthe correct phrase-structures of previously heardsentences are already known.
Bod (2005) thereforeproposed the following generalization of DOP, whichwe will simply refer to as U-DOP: if a languagelearner does not know which syntactic tree should beassigned to a sentence, s/he initially allows(implicitly) for all possible trees and let linguisticexperience decide which is the ?best?
tree bymaximizing structural analogy (i.e.
the MPSD).Although several alternative versions of U-DOP have been proposed (e.g.
Bod 2006a, 2007), wewill stick to the computation of the MPSD for thecurrent paper.
Due to its use of the shortest derivation,the model?s working can often be predicted withoutany probabilistic computations, which makes itespecially apt to investigate linguistic facets such asauxiliary fronting (see section 6).From a conceptual perspective we candistinguish three learning phases under U-DOP,which we shall discuss in more detail below.
(i) Assign all unlabeled binary trees to a set ofsentencesSuppose that a language learner hears the followingtwo (?Childes-like?)
sentences: watch the dog and thedog barks.
How could a rational learner figure out theappropriate tree structures for these sentences?
U-DOP conjectures that a learner does so by allowingany fragment of the heard sentences to form aproductive unit and to try to reconstruct thesesentences out of most probable shortest combinations.Consider the set of all unlabeled binary trees forthe sentences watch the dog and the dog barks givenin figure 4.
Each node in each tree is assigned thesame category label X, since we do not (yet) knowwhat label each phrase will receive.watch the dogXXXwatch the dogXthe dogXXbarksXXthe dog barksFigure 4.
The unlabeled binary tree set for watch the dogand the dog barksAlthough the number of possible binary trees for asentence grows exponentially with sentence length,these binary trees can be efficiently represented bymeans of a chart or tabular diagram.
By addingpointers between the nodes we obtain a structure3known as a shared parse forest (Billot and Lang1989).
(ii) Divide the binary trees into all subtreesFigure 5 exhaustively lists the subtrees that can beextracted from the trees in figure 4.
The first subtreein each row represents the whole sentence as a chunk,while the second and the third are ?proper?
subtrees.watch the dogXXdogXXwatch theXXwatch the dogXXwatchX the dogXthe dogXXbarksXXbarksthe dogXXXthe dog barksXXtheXdog barksFigure 5.
The subtree set for the binary trees in figure 4.Note that while most subtrees occur once, the subtree[the dog]X occurs twice.
There exist efficientalgorithms to convert all subtrees into a compactrepresentation (Goodman 2003) such that standardbest-first parsing algorithms can be applied to themodel (see Bod 2007).
(iii) Compute the ?best?
tree for each sentenceGiven the subtrees in figure 5, the language learnercan now induce analyses for a sentence such as thedog barks in various ways.
The phrase structure [the[dog barks]X]X can be produced by two differentderivations, either by selecting the large subtree thatspans the whole sentence or by combining twosmaller subtrees:XXthe dog barksorXXtheXdog barksoFigure 6.
Deriving the dog barks from figure 5Analogously, the competing phrase structure [[thedog]X barks]X  can also produced by two derivations:the dogXXbarksorXXbarksthe dogXoFigure 7.
Other derivations for the dog barksNote that the shortest derivation is not unique: thesentence the dog barks can be trivially parsed by anyof its fully spanning trees.
Such a situation does notusually occur when structures for new sentences arelearned, i.e.
when we induce structures for a held-outtest set  using all subtrees from all possible treesassigned to a training set.
For example, the shortestderivation for the new ?sentence?
watch dog barks isunique, given the set of subtrees in figure 5.
But in theexample above we need subtree frequencies to breakties, i.e.
U-DOP computes the most probable treefrom among the shortest derivations, the MPSD.
Theprobability of a tree is compositionally computedfrom the frequencies of its subtrees, in the same wayas in the supervised version of DOP (see Bod 1998,2002).
Since the subtree [the dog]X is the only subtreethat occurs more than once, we can predict that themost probable tree corresponds to the structure [[thedog]X barks]X in figure 7 where the dog is aconstituent.
This can also be shown formally, but aprecise computation is unnecessary for this example.4  Learning constructions by U-DOPFor the sake of simplicity, we have only consideredsubtrees without lexical labels in the previous section.Now, if we also add an (abstract) label to each wordin figure 4, then a possible subtree would the subtreein figure 9, which has a discontiguous yield watch Xdog, and which we will therefore refer to as a?discontiguous subtree?.Xwatch dogXX X XFigure 9.
A discontiguous subtreeThus lexical labels enlarge the space of dependenciescovered by our subtree set.
In order for U-DOP to4take into account both contiguous and non-contiguouspatterns, we will define the total tree-set of a sentenceas the set of all unlabeled trees that are unary at theword level and binary at all higher levels.Discontiguous subtrees, like in figure 9, areimportant for acquiring a variety of constructions asin (1)-(4):(1) Show me the nearest airport to Leipzig.
(2) BA carried more people than cargo in 2005.
(3) What is this scratch doing on the table?
(4) Don?t take him by surprise.These constructions have been discussed at variousplaces in the literature, and all of them arediscontiguous in that the constructions do not appearas contiguous word strings.
Instead the words areseparated by ?holes?
which are sometimes representedby dots as in more ?
than ?, or by variables as inWhat is X doing Y (cf.
Kay and Fillmore 1999).
Inorder to capture the syntactic structure ofdiscontiguous constructions we need a model thatallows for productive units that can be partiallylexicalized, such as subtrees.
For example, theconstruction more ... than ?
in (2) can be representedby a subtree as in figure 10.more thanXXX XXX XFigure 10.
Discontiguous subtree for more...than...U-DOP can learn the structure in figure 10 from a fewsentences only, using the mechanism described insection 3.
While we will go into the details of learningdiscontiguous subtrees in section 5, it is easy to seethat U-DOP will prefer the structure in figure 10 overa structure where e.g.
[X than] forms a constituent.First note that the substring more X can occur at theend of a sentence (in e.g.
Can I have more milk?
),whereas the substring X than cannot occur at the endof a sentence.
This means that [more X] will bepreferred as a constituent in [more X than X].
Thesame is the case for than X in e.g.
A is cheaper thanB.
Thus both [more X] and [than X] can appearseparately from the construction and will win out infrequency, which means that U-DOP will learn thestructure in figure 10 for the construction more ?than ?.Once it is learned, (supervised) DOP enforcesthe application of the subtree in figure 10 whenever anew form using the construction more ... than ... isperceived or produced because the particular subtreewill directly cover it and lead to the shortestderivation.5  Learning agreement by U-DOPDiscontiguous context is important not only forlearning constructions but also for learning varioussyntactic regularities.
Consider the following sentence(5):(5) Swimming in rivers is dangerousHow can U-DOP deal with the fact that humanlanguage learners will perceive an agreement relationbetween swimming and is, and not between rivers andis?
We will rephrase this question as follows: whichsentences must be perceived such that U-DOP canassign as the best structure for swimming in rivers isdangerous the tree 16(a) which attaches theconstituent is dangerous to swimming in rivers, andnot an incorrect tree like 16(b) which attaches isdangerous to rivers?
Note that tree (a) correctlyrepresents the dependency between swimming and isdangerous, while tree (b) misrepresents a dependencybetween rivers and is dangerous.swimming isXXXXXX Xin riversX Xdangerousswimming isXXXXXX Xin riversX Xdangerous(a)     (b)Figure 16.
Two possible trees for Swimming in rivers isdangerousIt turns out that we need to observe only oneadditional sentence to overrule tree (b), i.e.
sentence(6):(6) Swimming together is funThe word together can be attached either to swimmingor to is fun, as illustrated respectively by 17(a) and17(b) (of course, together can also be attached to isalone, and the resulting phrase together is to fun, butour argument still remains valid):5swimming isXXXXXX Xtogether funswimming isXXXXXXXtogether fun(a)             (b)Figure 17.
Two possible trees for Swimming together is funFirst note that there is a large common subtreebetween 16(a) and 17(a), as shown in figure 18.swimming isXXXXXX XFigure 18.
Common subtree in the trees 16(a) and 17(a)Next note that there is not such a large commonsubtree between 16(b) and 17(b).
Since the shortestderivation is not unique (as both trees can beproduced by directly using the largest tree from thebinary tree set), we must rely on the frequencies ofthe subtrees.
It is easy to see that the trees 16(a) and17(a) will overrule respectively 16(b) and 17(b),because 16(a) and 17(a) share the largest subtree.Although 16(b) and 17(b) also share subtrees, theycover a smaller part of the sentence than does thesubtree in figure 18.
Next note that for every commonsubtree between 16(a) and 17(a) there exists acorresponding common subtree between 16(b) and17(b) except for the common subtree in figure 18 (andone of its sub-subtrees by abstracting fromswimming).
Since the frequencies of all subtrees of atree contribute to its probability, if follows that figure18 will be part of the most probable tree, and thus16(a) and 17(a) will overrule respectively 16(b) and17(b).However, our argument is not yet complete: wehave not yet ruled out another possible analysis forswimming in rivers is dangerous where in riversforms a constituent together with is dangerous.Interestingly, it suffices to perceive a sentence like(7): He likes swimming in river.
The occurrence ofswimming in rivers at the end of this sentence willlead to a preference for 16(a) because it will get ahigher frequency as a group.
An implementation ofU-DOP confirmed our informal argument.We conclude that U-DOP only needs threesentences to learn the correct tree structure displayingthe dependency between the subject swimming andthe verb is, known otherwise as ?agreement?.
Oncewe have learned the correct structure for subject-verbagreement by the subtree in figure 18, (U-)DOPenforces agreement by the shortest derivation.It can also be shown that U-DOP still learns thecorrect agreement if sentences with incorrectagreement, like *Swimming in rivers are dangerous,are heard as long as the correct agreement has ahigher frequency than the incorrect agreement duringthe learning process.6  Learning ?movement?
by U-DOPWe now come to what is often assumed to be thegreatest challenge for models of language learning,and what Crain (1991) calls the ?parade case of aninnate constraint?
: the problem of auxiliarymovement, also known as auxiliary fronting orinversion.
Let?s start with the typical examples, whichare similar to those used in Crain (1991),MacWhinney (2005), Clark and Eyraud (2006) andmany others:(8) The man is hungryIf we turn sentence (8) into a (polar) interrogative, theauxiliary is is fronted, resulting in sentence (9).
(9) Is the man hungry?A language learner might derive from these twosentences that the first occurring auxiliary is fronted.However, when the sentence also contains a relativeclause with an auxiliary is, it should not be the firstoccurrence of is that is fronted but the one in the mainclause, as shown in sentences (11) and (12).
(11) The man who is eating is hungry(12) Is the man who is eating hungry?There is no reason that children should favor thecorrect auxiliary fronting.
Yet children do produce thecorrect sentences of the form (12) and rarely if ever ofthe form (13) even if they have not heard the correctform before (see Crain and Nakayama 1987).
(13) *Is the man who eating is hungry?How can we account for this phenomenon?According to the nativist view, sentences of the type6in (12) are so rare that children must have innatelyspecified knowledge that allows them to learn thisfacet of language without ever having seen it (Crainand Nakayama 1987).
On the other hand, it has beenclaimed that this type of sentence is not rare at all andcan thus be learned from experience (Pullum andScholz 2002).
We will not enter the controversy onthis issue, but believe that both viewpoints overlook avery important alternative possibility, namely thatauxiliary fronting needs neither be innate nor in theinput data to be learned, but may simply be anemergent property of the underlying model.How does (U-)DOP account for thisphenomenon?
We will show that the learning ofauxiliary fronting can proceed with only twosentences:(14) The man who is eating is hungry(15) Is the boy hungry?Note that these sentences do not contain an exampleof the fact that an auxiliary should be fronted from themain clause rather than from the relative clause.For reasons of space, we will have to skip theinduction of the tree structures for (14) and (15),which can be derived from a total of six sentencesusing similar reasoning as in section 5, and which aregiven in figure 20a,b (see Bod forthcoming, for moredetails and a demonstration that the induction of thesetwo tree structures is robust).isXXXXXXis eatingX XhungryXthe manX XXwhoXXXXXXthe boyX Xis hungry(a)       (b)Figure 20.
Tree structures for the man who is eating ishungry and is the boy hungry?
learned by U-DOPGiven the trees in figure 20, we can now easily showthat U-DOP?s shortest derivation produces the correctauxiliary fronting, without relying on any probabilitycalculations.
That is, in order to produce the correctinterrogative, Is the man who is eating hungry, weonly need to combine the following two subtrees fromthe acquired structures in figure 20, as shown infigure 21 (note that the first subtree is discontiguous):XXXXXis hungryXXiseatingX XXthemanX XXwhoXoFigure 21.
Producing the correct auxiliary fronting bycombining two subtrees from figure 20On the other hand, to produce the sentence withincorrect auxiliary fronting *Is the man who eating ishungry?
we need to combine many more subtreesfrom figure 20.
Clearly the derivation in figure 21 isthe shortest one and produces the correct sentence,thereby blocking the incorrect form.1Thus the phenomenon of auxiliary frontingneeds neither be innate nor in the input data to belearned.
By using the notion of shortest derivation,auxiliary fronting can be learned from just a couplesentences only.
Arguments about frequency and?poverty of the stimulus?
(cf.
Crain 1991;MacWhinney 2005) are therefore irrelevant ?provided that we allow our productive units to be ofarbitrary size.
(Moreover, learning may be furthereased once the syntactic categories have beeninduced.
Although we do not go into categoryinduction in the current paper, once unlabeledstructures have been found, category learning turnsout to be a relatively easy problem).Auxiliary fronting has been previously dealtwith in other probabilistic models of structurelearning.
Perfors et al (2006) show that Bayesianmodel selection can choose the right grammar forauxiliary fronting.
Yet, their problem is different inthat Perfors et al start from a set of given grammarsfrom which their selection model has to choose thecorrect one.
Our approach is more congenial to Clarkand Eyraud (2006) who show that by distributionalanalysis in the vein of Harris (1954) auxiliary frontingcan be correctly predicted.
However, different fromClark and Eyraud, we have shown that U-DOP canalso learn complex, discontiguous constructions.
Inorder to learn both rule-based phenomena likeauxiliary inversion and exemplar-based phenomenalike idiosyncratic constructions, we believe we need1We are implicitly assuming here an extension of DOPwhich computes the most probable shortest derivation givena certain meaning to be conveyed.
This semantic DOPmodel was worked out in Bonnema et al (1997) where themeaning of a sentence was represented by its logical form.7the richness of a probabilistic tree grammar ratherthan a probabilistic context-free grammar.7  ConclusionWe have shown that various syntactic phenomena canbe learned by a model that only assumes (1) thenotion of recursive tree structure, and (2) ananalogical matching algorithm which reconstructs anew sentence out of largest and most frequentfragments from previous sentences.
The majordifference between our model and othercomputational learning models (such as Klein andManning 2005 or Clark and Eyraud 2006) is that westart with trees.
But since we do not know which treesare correct, we initially allow for all of them and letanalogy decide.
Thus we assume that the languagefaculty (or ?Universal Grammar?)
has knowledgeabout the notion of tree structure but no more thanthat.
Although we do not claim that we havedeveloped any near-to-complete theory of alllanguage acquisition, our argument to use onlyrecursive structure as the core of language knowledgehas a surprising precursor.
Hauser, Chomksy andFitch (2002) claim that the core language facultycomprises just ?recursion?
and nothing else.
If onetakes this idea seriously, then U-DOP is probably thefirst fully computational model that instantiates it: U-DOP?s trees encode the ultimate notion of recursionwhere every label can be recursively substituted forany other label.
All else is analogy.ReferencesBillot, S. and B. Lang, 1989.
The Structure of SharedForests in Ambiguous Parsing.
Proceedings ACL 1989.Bod, R. 1998.
Beyond Grammar.
Stanford: CSLIPublications.Bod, R. 2002.
A Unified Model of Structural Organizationin Language and Music, Journal of Artificial IntelligenceResearch, 17, 289-308.Bod, R. 2005.
Combining Supervised and UnsupervisedNatural Language Processing.
The 16th Meeting ofComputational Linguistics in the Netherlands (CLIN2005).Bod, R. 2006a.
An All-Subtrees Approach to UnsupervisedParsing.
Proceedings ACL-COLING 2006, 865-872.Bod, R. 2006b.
Exemplar-Based Syntax: How to GetProductivity from Examples.
The Linguistic Review 23,291-320.Bod, 2007.
Is the End of Supervised Parsing in Sight?.Proceedings ACL 2007, Prague.Bod, forthcoming.
From Exemplar to Grammar: HowAnalogy Guides Language Acquisition.
In J. Blevins andJ.
Blevins (eds.)
Analogy in Grammar, OxfordUniversity Press.Bonnema, R., R. Bod and R. Scha, 1997.
A DOP Model forSemantic Interpretation.
Proceedings ACL/EACL 1997,Madrid, Spain, 159-167.Bybee, J.
2006.
From Usage to Grammar: The Mind?sResponse to Repetition.
Language 82(4), 711-733.Chater, N. 1999.
The Search for Simplicity: A FundamentalCognitive Principle?
The Quarterly Journal ofExperimental Psychology, 52A(2), 273-302.Clark, A. and R. Eyraud, 2006.
Learning AuxiliaryFronting with Grammatical Inference.
ProceedingsCONLL 2006, New York.Crain, S. 1991.
Language Acquisition in the Absence ofExperience.
Behavorial and Brain Sciences 14, 597-612.Crain, S. and M. Nakayama, 1987.
Structure Dependencein Grammar Formation.
Language 63, 522-543.Croft, B.
2001.
Radical Construction Grammar.
OxfordUniversity Press.Frazier, L. 1978.
On Comprehending Sentences: SyntacticParsing Strategies.
PhD.
Thesis, U. of Connecticut.Goldberg, A.
2006.
Constructions at Work: the nature ofgeneralization in language.
Oxford University Press.Goodman, J.
2003.
Efficient algorithms for the DOPmodel.
In R. Bod, R. Scha and K. Sima'an (eds.).
Data-Oriented Parsing, CSLI Publications, 125-146.Harris, Z.
1954.
Distributional Structure.
Word 10, 146-162.Hauser, M., N. Chomsky and T. Fitch, 2002.
The Facultyof Language: What Is It, Who Has It, and How Did ItEvolve?, Science 298, 1569-1579.Jurafsky, D. 2003.
Probabilistic Modeling inPsycholinguistics.
In Bod, R., J. Hay and S.
Jannedy(eds.
), Probabilistic Linguistics, The MIT Press, 39-96.Kay, P. and C. Fillmore 1999.
Grammatical constructionsand linguistic generalizations: the What's X doing Y?construction.
Language, 75, 1-33.Klein, D. and C. Manning 2005.
Natural language grammarinduction with a generative constituent-context model.Pattern Recognition 38, 1407-1419.MacWhinney, B.
2005.
Item-based Constructions and theLogical Problem.
Proceedings of the Second Workshopon Psychocomputational Models of Human LanguageAcquisition, Ann Arbor.Manning, C. and H. Sch?tze 1999.
Foundations ofStatistical Natural Language Processing.
The MIT Press.Perfors, A., Tenenbaum, J., Regier, T. 2006.
Poverty of theStimulus?
A rational approach.
Proceedings 28th AnnualConference of the Cognitive Science Society.
VancouverPullum, G. and B. Scholz 2002.
Empirical assessment ofstimulus poverty arguments.
The Linguistic Review 19,9-50.Tomasello, M. 2003.
Constructing a Language.
HarvardUniversity Press.8
