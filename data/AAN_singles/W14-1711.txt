Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 83?90,Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational LinguisticsFactored Statistical Machine Translation for Grammatical ErrorCorrectionYiming Wang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng, Yi LuNatural Language Processing & Portuguese-Chinese Machine Translation Laboratory,Department of Computer and Information Science,University of Macau, Macau S.A.R., China{wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo,lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.moAbstractThis paper describes our ongoing work ongrammatical error correction (GEC).
Focusingon all possible error types in a real-lifeenvironment, we propose a factored statisticalmachine translation (SMT) model for this task.We consider error correction as a series oflanguage translation problems guided byvarious linguistic information, as factors thatinfluence translation results.
Factors includedin our study are morphological information, i.e.word stem, prefix, suffix, and Part-of-Speech(PoS) information.
In addition, we alsoexperimented with different combinations oftranslation models (TM), phrase-based andfactor-based, trained on various datasets toboost the overall performance.
Empiricalresults show that the proposed model yields animprovement of 32.54% over a baselinephrase-based SMT model.
The systemparticipated in the CoNLL 2014 shared taskand achieved the 7thand 5thF0.5 scores1 on theofficial test set among the thirteenparticipating teams.1 IntroductionThe task of grammatical error detection andcorrection (GEC) is to make use ofcomputational methods to fix the mistakes in awritten text.
It is useful in two aspects.
For anon-native English learner it may help toimprove the grammatical quality of the writtentext.
For a native speaker the tool may help toremedy mistakes automatically.
Automatic1  These two rankings are based on gold-standard editswithout and with alternative answers, respectively.correction of grammatical errors is an activeresearch topic, aiming at improving the writingprocess with the help of artificial intelligenttechniques.
Second language learning is a usergroup of particular interest.Recently, Helping Our Own (HOO) andCoNLL held a number of shared tasks on thistopic (Dale et al., 2012, Ng et al., 2013, Ng et al.,2014).
Previous studies based on rules (Sidorovet al., 2013), data-driven methods (Berend et al.,2013, Yi et al., 2013) and hybrid methods (Putraand Szab?, 2013, Xing et al., 2013) have shownsubstantial gains for some frequent error typesover baseline methods.
Most proposed methodsshare the commonality that a sub-model is builtfor a specific type of error, on top of which astrategy is applied to combine a number of theseindividual models.
Also, detection and correctionare often split into two steps.
For example, Xinget al.
(2013) presented the UM-Checker for fiveerror types in the CoNLL 2013 shared task.
Thesystem implements a cascade of five individualdetection-and-correction models for differenttypes of error.
Given an input sentence, errors aredetected and corrected one-by-one by each sub-model at the level of its corresponding error type.The specifics of an error type are fullyconsidered in each sub-model, which is easier torealize for a single error type than for multipletypes in a single model.
In addition, dividing theerror detection and correction into two stepsalleviates the application of machine learningclassifiers.
However, an approach that considerserror types individually may have negativeeffects:?
This approach assumes independencebetween each error type.
It ignores theinteraction of neighboring errors.
Results(Xing et al., 2013) have shown that83consecutive errors of multiple types tend tohinder solving these errors individually.?
As the number of error types increases, thecomplexities of analyzing, designing, andimplementing the model increase, inparticular when combinatorial errors aretaken into account.?
Looking for an optimal model combinationbecomes complex.
A simple pipelineapproach would result in interference and thegeneration of new errors, and hence topropagating those errors to the subsequentprocesses.?
Separating the detection and correction tasksmay result in more errors.
For instance, oncea candidate is misidentified as an error, itwould be further revised and turned into anerror by the correction model.
In thisscenario the model risks losing precision.In the shared task of this year (Ng et la.,2014), two novelties are introduced: 1) all typesof errors present in an essay are to be detectedand corrected (i.e., there is no restriction on thefive error types of the 2013 shared task); 2) theofficial evaluation metric of this year adopts F0.5,weighting precision twice as much as recall.
Thisrequires us to explore an alternative universaljoint model that can tackle various kinds ofgrammatical errors as well as join the detectionand correction processes together.
Regardinggrammatical error correction as a process oftranslation has been shown to be effective (Ehsanand Faili, 2013, Mizumoto et al., 2011,Yoshimoto et al., 2013, Yuan and Felice, 2013).We treat the problematic sentences and goldensentences as pairs of source and target sentences.In SMT, a translation model is trained on aparallel corpus that consists of the sourcesentences (i.e.
sentences that may containgrammatical errors) and the targeted translations(i.e.
the grammatically well-formed sentences).The challenge is that we need a large amount ofthese parallel sentences for constructing such adata-driven SMT system.
Some researches(Brockett et al., 2006, Yuan and Felice, 2013)explore generating artificial errors to resolve thissparsity problem.
Other studies (Ehsan and Faili,2013, Yoshimoto et al., 2013, Yuan and Felice,2013) focus on using syntactic information (suchas PoS or tree structure) to enhance the SMTmodels.In this paper, we propose a factored SMTmodel by taking into account not only the surfaceinformation contained in the sentence, but alsomorphological and syntactic clues (i.e., wordstem, prefix, suffix and finer PoS information).To counter the sparsity problem we do not useartificial or manual approaches to enrich thetraining data.
Instead we apply factored andtransductive learning techniques to enhance themodel on a small dataset.
In addition, we alsoexperimented with different combinations oftranslation models (TM), phrase- and factor-based, that are trained on different datasets toboost the overall performance.
Empirical resultsshow that the proposed model yields animprovement of 32.54% over a baseline phrase-based SMT model.The remainder of this paper is organized asfollows: Section 2 describes our proposedmethods.
Section 3 reports on the design of ourexperiments.
We discuss the result, including theofficial shared task results, in Section 4,.
Wesummarize our conclusions in Section 5.2 MethodologyIn contrast with phrase-based translation models,factored models make use of additional linguisticclues to guide the system such that it generatestranslated sentences in which morphological andsyntactic constraints are met (Koehn and Hoang,2007).
The linguistic clues are taken as factors ina factored model; words are represented asvectors of factors rather than as a single token.This requires us to pre-process the training datato factorize all words.
In this study, we explorethe use of various types of morphologicalinformation and PoS as factors.
For each possiblefactor we build an individual translation model.The effectiveness of all factors is analyzed bycomparing the performance of the correspondingmodels on the grammatical error correction task.Furthermore, two approaches are proposed tocombine those models.
One adopts the modelcascading method based on transductive learning.The second approach relies on learning anddecoding multiple factors learning.
The details ofeach approach are discussed in the followingsub-sections.2.1 Data PreparationIn order to construct a SMT model, we convertthe training data into a parallel corpus where theproblematic sentences that ought to be correctedare regarded as source sentences, while thereference sentences are treated as thecorresponding target translations.
We discoveredthat a number of sentences is absent at the targetside due to incorrect annotations in the golden84data.
We removed these unparalleled sentencesfrom the data.
Secondly, the initialcapitalizations of sentences are converted to theirmost probable casing using the Moses truecaser2.URLs are quite common in the corpus, but theyare not useful for learning and even may causethe model to apply unnecessary correction on it.Thus, we mark all of the ULRs with XMLmarkups, signaling the SMT decoder not toanalyze an URL and output it as is.2.2 Model ConstructionIn this study we explore four different factors:prefix, suffix, stem, and PoS.
This linguisticinformation not only helps to capture the localconstraints of word morphologies and theinteraction of adjacent words, but also helps toprevent data sparsity caused by inflected wordvariants and insufficient training data.Word stem: Instead of lemmas, we preferword stemming as one of the factors, consideringthat stemming does not requires deepmorphological analysis and is easier to obtain.Second, during the whole error detection andcorrection process, stemming information is usedas auxiliary information in addition to theoriginal word form.
Third, for grammatical errorcorrection using word lemmas or word stems infactored translation model shows no significantdifference.
This is because we are translating textof the same language, and the translation of thisfactor, stem or lemma, is straightforwardlycaptured by the model.
Hence, we do not rely onthe word lemma.
In this work, we use theEnglish Porter stemmer (Porter, 1980) forgenerating word stems.Prefix: The second type of morphologicalinformation we explored is the word prefix.Although a prefix does not present strongevidence to be useful to the grammatical errorcorrection, we include it in our study in order tofully investigate all types of morphologicalinformation.
We believe the prefix can be animportant factor in the correction of initialcapitalization, e.g.
?In this era, engineeringdesigns??
should be changed to ?In this era,engineering designs??
In model construction,we take the first three letters of a word as itsprefix.
If the length of a word is less than three,we use the word as the prefix factor.Suffix: Suffix, one of the important factors,helps to capture the grammatical agreementsbetween predicates and arguments within a2 After decoding, we will de-truecase all these words.sentence.
Particularly the endings of plural nounsand inflected verb variants are useful for thedetection of agreement violations that shown upin word morphologies.
Similar to how werepresent the prefix, we are interested in the lastthree characters of a word.ExamplesSentencethis card contains biometric data toadd security and reduce the risk offalsificationOriginalPOSDT NN BVZ JJ NNS TO VB NNCC VB DT NN IN NNSpecificPOSDT NN VBZ JJ NNS TO_to VBNN CC VB DT_the NN IN_ofNNTable 1: Example of modified PoS.According to the description of factors, Figure1 illustrates the forms of various factorsextracted from a given example sentence.Surfaceconstantly combining ideas willresult in better solutions beingformulatedPrefix con com ide wil res in bet sol bei forSuffix tly ing eas ill ult in ter ons ing tedStemconstantli combin idea will result inbetter solut be formulSpecificPOSRB VBG NNS MD VB IN JJR NNSVBG VBNFigure 1: The factorized sentence.PoS: Part-of-Speech tags denote the morpho-syntactic category of a word.
The use of PoSsequences enables us to some extent to recovermissing determiners, articles, prepositions, aswell as the modal verb in a sentence.
Empiricalstudies (Yuan and Felice, 2013) havedemonstrated that the use of this information cangreatly improve the accuracy of the grammaticalerror correction.
To obtain the PoS, we adopt thePenn Treebank tag set (Marcus et al., 1993),which contains 45 PoS tags.
The Stanford parser(Klein and Manning, 2002) is used to extract thePoS information.
Inspired by Yuan and Felice(2013), who used preposition-specific tags to fixthe problem of being unable to distinguishbetween prepositions and obtained goodperformance, we create specific tags both fordeterminers (i.e., a, an, the) and prepositions.Table 1 provides an example of this modification,where prepositions, TO and IN, and determiner,85DT, are revised to TO_to, IN_of and DT_the,respectively.2.3 Model CombinationIn addition to the design of different factoredtranslation models, two model combinationstrategies are designed to treat grammatical errorcorrection problem as a series of translationprocesses, where an incorrect sentence istranslated into the correct one.
In bothapproaches we pipeline two translation models,and    .
In the first approach, we derivefour combinations of different models thattrained on different sources.?
In case I,andare both factoredmodels but trained on different factors, e.g.fortraining on ?surface + factori?
andon ?surface + factori?j?.
Both modelsuse the same training sentences, but differentfactors.?
In case II,is trained on sentences thatpaired with the output from the previousmodel,, and the golden correct sentences.We want to create a second model that canalso tackle the new errors introduced by thefirst model.?
In case III, similar to case II, the secondtranslation model,is replaced by aphrase-based translation model.?
In case IV, the quality of training data isconsidered vital to the construction of a goodtranslation model.
The present training datasetis not large enough.
To complement this, thesecond model,, is trained on an enlargeddata set, by combining the training data ofboth models, i.e.
the original parallel data(official incorrect and correct sentence pairs)and the supplementary parallel data(sentences output from the first model,,and the correct sentences).
Note that we donot de-duplicate sentences.In all cases, the testing process is carried outas follows.
The test set is translated by the firsttranslation model,.
The output from the firstmodel is then fed into the second translationmodel,.
The output of the second model isused as the final corrections.The second combination approach is to makeuse of multiple factors for model construction.The question is whether multiple factors whenused together may improve the correction results.In this setting we combine two factors togetherwith the word surface form to build a multi-factored translation model.
All pairs of factorsare used, e.g.
stem and PoS.
The decodingsequence is as follows: translate the input stemsinto target stems; translate the PoS; and generatethe surface form given the factors of stem andPoS.3 Experiment Setup3.1 DatasetWe pre-process the NUCLE corpus (Dahlmeieret al., 2013) as described in Section 2 for trainingdifferent translation models.
We use both theofficial golden sentences and additionalWMT2014 English monolingual data3 to train anin-domain and a general-domain language model(LM), respectively.
These language models arelinearly interpolated in the decoding phase.
Wealso randomly select a number of sentence pairsfrom the parallel corpus as a development set anda test set, disjoint from the training data.
Table 2summarizes the statistics of all the datasets.Corpus Sentences TokensParallelCorpus55,5031,124,521 /1,114,040AdditionalMonolingual85,254,788 2,033,096,800Dev.
Set 500 10,532 / 10,438Test Set 900 18,032 / 17,906Table 2: Statistics of used corpora.The experiments were carried out withMOSES 1.04 (Philipp Koehn et al., 2007).
Thetranslation and the re-ordering model utilizes the?grow-diag-final?
symmetrized word-to-wordalignments created with GIZA++5 (Och and Ney,2003) and the training scripts of MOSES.
A 5-gram LM was trained using the SRILM toolkit6(Stolcke et al., 2002), exploiting the improvedmodified Kneser-Ney smoothing (Kneser andNey, 1995), and quantizing both probabilitiesand back-off weights.
For the log-linear modeltraining, we take minimum-error-rate training(MERT) method as described in (Och, 2003).The result is evaluated by M2 Scorer (Dahlmeierand Ng, 2012) computing precision, recall andF0.5.3 http://www.statmt.org/wmt14/translation-task.html.4 http://www.statmt.org/moses/.5 http://code.google.com/p/giza-pp/.6 http://www.speech.sri.com/projects/srilm/.86In total, one baseline system, five individualsystems, and four combination systems areevaluated in this study.
The baseline system(Baseline) is trained on the words-only corpususing a phrase-based translation model.
For theindividual systems we adopt the factoredtranslation model that are trained respectively on1) surface and stem factors (Sys+stem), 2) surfaceand suffix factors (Sys+suf), 3) surface and prefixfactors (Sys+pref), 4) surface and PoS factors(Sys+PoS), and 5) surface and modified-PoSfactors (Sys+MPoS).
The combination systemsinclude: 1) the combination of ?factored +phrase-based?
and ?factored + factored?
formodels cascading; and 2) the factors of surface,stem and modified-PoS (Sys+stem+MPoS) arecombined for constructing a correction systembased on a multi-factor model.4 Results and DiscussionsWe report our results in terms of the precision,recall and F0.5 obtained by each of the individualmodels and combined models.4.1 Individual ModelTable 3 shows the absolute measures for thebaseline system, while the other individualmodels are listed with values relative to thebaseline.Model Precision  Recall  F0.5Baseline 25.58 3.53 11.37Sys+stem -14.84 +13.00 +0.18Sys+suf -14.57 +14.77 +0.60Sys+pref -15.74 +12.20 -0.77Sys+PoS -11.63 +9.79 +2.45Sys+MPoS -10.25 +10.60 +3.70Table 3: Performance of various models.The baseline system has the highest precisionscore but the lowest recall.
Nearly all individualmodels except Sys+pref show improvements in thecorrection result (F0.5) over the baseline.
Overall,Sys+MPoS achieves the best result for thegrammatical error correction task.
It shows asignificant improvement over the other modelsand outperforms the baseline model by 3.7 F0.5score.
The Sys+stem and Sys+suf models obtain animprovement of 0.18 and 0.60 in F0.5 scores,respectively, compared to the baseline.
Althoughthe differences are not significant, it confirms ourhypothesis that morphological clues do help toimprove error correction.
The F0.5 score ofSys+pref is the lowest among the models includingthe baseline, showing a drop of 0.77 in F0.5 scoreagainst the baseline.
One possible reason is thatfew errors (in the training corpus) involve wordprefixes.
Thus, the prefix does not seem to be asuitable factor for tackling the GEC problem.TypeSys+stem(%)Sys+suf(%)Sys+MPoS(%)ErrorNum.Vt 17.07 12.20 12.20 41ArtOrDet 37.65 36.47 29.41 85Nn 33.33 19.61 23.53 51Prep 10.26 10.26 12.82 39Wci 9.10 10.61 6.10 66Rloc- 15.20 13.92 10.13 79Table 4: The capacity of different models inhandling six frequent error types.We analyze the capacities of the models ondifferent types of errors.
Sys+PoS and Sys+MPoS arebuilt by using the PoS and modified PoS.
Both ofthem yield an improvement in F0.5 score.
Overall,Sys+MPoS produces more accurate results thanSys+pref.
Therefore, we specifically compare andevaluate the best three models, Sys+stem, Sys+sufand Sys+MPoS.
Table 4 presents evaluation scoresof these models for the six most frequent errortypes, which take up a large part of the trainingand test data.
Among them, Sys+stem displays apowerful capacity to handle determiner andnoun/number agreement errors, up to 37.65%and 33.33%.
Sys+suf shows the ability to correctdeterminer errors at 36.47%; Sys+MPoS yields asimilar performance to Sys+suf.
All threeindividual models exhibit a relatively highcapacity to handle determiner errors.
The likelyreason is that this mistake constitutes the largestportion in training data and test set, giving thelearning models many examples to capture thisproblem well.
In the case of preposition errors,Sys+MPoS demonstrates a better performance.
This,once again, confirms the result (Yuan and Felice,2013) that the modified PoS factor is effectivefor every preposition word.
For these six errortypes, the individual models show a weakcapacity to handle the word collocation or idiomerror category (Wci).
Although Sys+MPoSachieves the highest F0.5 score in the overallevaluation, it only achieves 6.10% in handlingthis error type.
The likely reason is that idiomsare not frequent in the training data, and also thatin most of the cases they contain out-of-vocabulary words never seen in training data.4.2 Model CombinationWe intend to further boost the overallperformance of the correction system by87combining the strengths of individual modelsthrough model combination, and compare againstthe baseline.
The systems compared here coverthree pipelined models and a multi-factoredmodel, as described earlier in Section 3.
Thecombined systems include: 1) CSyssuf+phrase: thecombination of Sys+suf and the baseline phrase-based translation model; 2) CSyssuf+suf: wecombine two similar factored models with suffixfactors, Sys+suf, which is trained on the samecorpus; and 3) TSyssuf+phrase: similar toCSyssuf+phrase, but the training data for the secondphrase-based model is augmented by adding theoutput sentences from the previous model (pairedwith the correct sentences).
Our intention is toenlarge the size of the training data.
Theevaluation results are presented in Table 5.Model Precision Recall F0.5Baseline 25.58 3.53 11.37CSyssuf+phrase -14.70 +14.61 +0.45CSyssuf+suf -15.04 +14.13 +0.09TSyssuf+phrase -14.76 +14.61 +0.40Sys+stem+MPoS -15.87 +11.72 -0.90Table 5: Evaluation results of combined models.In Table 5 we observe that Sys+stem+MPoS hurtsperformance and shows a drop of 0.9% in F0.5score.
Both the CSyssuf+phrase and CSyssuf+sufshow minor improvements over the baselinesystem.
Even when we enrich the training datafor the second model in TSyssuf+phrase, it cannothelp in boosting the overall performance of thesystem.
One of the problems we observe is that,with this combination structure, new incorrectsentences are introduced by the model at eachstep.
The errors are propagated and accumulatedto the final result.
Although CSyssuf+phrase andCSyssuf+suf produce a better F0.5 score over thebaseline, they are not as good as the individualmodels, Sys+PoS and Sys+MPoS, which are trainedon PoS and modified-PoS, respectively.4.3 The Official ResultAfter fully evaluating the designed individualmodels as well as the integrated ones, we adoptSys+MPoS as our designated system for thisgrammatical error correction task.
The officialtest set consists of 50 essays, and 2,203 errors.Table 6 shows the final result obtained by oursubmitted system.Table 7 details the correction rate of the fivemost frequent error types obtained by our system.The result suggests that the proposed system hasa better ability in handling the verb, article anddeterminer error than other error types.Criteria Result Alt.
ResultP 0.3127 0.4317R 0.1446 0.1972F0.5 0.2537 0.3488Table 6: The official correction results of oursubmitted system.Type Error Correct %Vt 203/201 21/22 10.34/10.94V0 57/54 9/9 15.79/16.67Vform 156/169 11/18 7.05/10.65ArtOrDet 569/656 84/131 14.76/19.97Nn 319/285 31/42 9.72/10.91Table 7: Detailed error information of evaluationsystem (with alternative result).5 ConclusionThis paper describes our proposed grammaticalerror detection and correction system based on afactored statistical machine translation approach.We have investigated the effectiveness of modelstrained with different linguistic informationsources, namely morphological clues andsyntactic PoS information.
In addition, we alsoexplore some ways to combine different modelsin the system to tackle the correction problem.The constructed models are compared against thebaseline model, a phrase-based translation model.Results show that PoS information is a veryeffective factor, and the model trained with thisfactor outperforms the others.
One difficulty ofthis year?s shared task is that participants have totackle all 28 types of errors, which is five timesmore than last year.
From the results, it isobvious there are still many rooms for improvingthe current system.AcknowledgementsThe authors are grateful to the Science andTechnology Development Fund of Macau andthe Research Committee of the University ofMacau for the funding support for their research,under the Reference nos.
MYRG076 (Y1-L2)-FST13-WF and MYRG070 (Y1-L2)-FST12-CS.The authors also wish to thank the anonymousreviewers for many helpful comments withspecial thanks to Antal van den Bosch for hisgenerous help on this manuscript.88ReferencesG?bor Berend, Veronika Vincze, Sina Zarriess,and Rich?rd Farkas.
2013.
LFG-basedFeatures for Noun Number and ArticleGrammatical Errors.
CoNLL-2013.Chris Brockett, William B. Dolan, and MichaelGamon.
2006.
Correcting ESL errors usingphrasal SMT techniques.
Proceedings of the21st International Conference onComputational Linguistics and the 44thannual meeting of the Association forComputational Linguistics pages 249?256.Daniel Dahlmeier, Hwee Tou Ng, and Siew MeiWu.
2013.
Building a Large AnnotatedCorpus of Learner English: The NUS Corpusof Learner English.
Proceedings of the EighthWorkshop on Innovative Use of NLP forBuilding Educational Applications.
pages 22-31.Robert Dale, Ilya Anisimoff, and GeorgeNarroway.
2012.
HOO 2012: A report on thepreposition and determiner error correctionshared task.
Proceedings of the SeventhWorkshop on Building EducationalApplications Using NLP pages 54?62.Nava Ehsan, and Heshaam Faili.
2013.Grammatical and context-sensitive errorcorrection using a statistical machinetranslation framework.
Software: Practice andExperience.
Wiley Online Library.D.
Klein, and C. D. Manning.
2002.
Fast exactinference with a factored model for naturallanguage parsing.
Advances in neuralinformation processing systems.Reinhard Kneser, and Hermann Ney.
1995.Improved backing-off for m-gram languagemodeling.
Acoustics, Speech, and SignalProcessing, 1995.
ICASSP-95., 1995International Conference on Vol.
1, pages181?184.P.
Koehn, and H. Hoang.
2007.
Factoredtranslation models.
Proceedings of the JointConference on Empirical Methods in NaturalLanguage Processing and ComputationalNatural Language Learning (EMNLP-CoNLL)Vol.
868, pages 876?876.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, et al.
2007.Moses: Open source toolkit for statisticalmachine translation.
Proceedings of the 45thAnnual Meeting of the ACL on InteractivePoster and Demonstration Sessions pages177?180.M.
P. Marcus, M. A. Marcinkiewicz, and B.Santorini.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.Computational linguistics.
MIT Press.Tomoya Mizumoto, Mamoru Komachi, MasaakiNagata, and Yuji Matsumoto.
2011.
MiningRevision Log of Language Learning SNS forAutomated Japanese Error Correction ofSecond Language Learners.
IJCNLP pages147?155.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe,Christian Hadiwinoto, Raymond HendySusanto, and Bryant Christopher.
2014.
Theconll-2014 shared task on grammatical errorcorrection.
Proceedings of CoNLL.
Baltimore,Maryland, USA.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu,Christian Hadiwinoto, and Joel Tetreault.2013.
The conll-2013 shared task ongrammatical error correction.
Proceedings ofCoNLL.Franz Josef Och.
2003.
Minimum Error RateTraining in Statistical Machine Translation,160?167.Franz Josef Och, and Hermann Ney.
2003.
Asystematic comparison of various statisticalalignment models.
Computational linguistics.MIT Press.Martin F. Porter.
1980.
An algorithm for suffixstripping.
Program: electronic library andinformation systems.
MCB UP Ltd.Desmond Darma Putra, and Lili Szab?.
2013.UdS at the CoNLL 2013 Shared Task.CoNLL-2013.Grigori Sidorov, Anubhav Gupta, Martin Tozer,Dolors Catala, Angels Catena, and SandrineFuentes.
2013.
Rule-based System forAutomatic Grammar Correction UsingSyntactic N-grams for English LanguageLearning (L2).
CoNLL-2013.Andreas Stolcke, and others.
2002.
SRILM-anextensible language modeling toolkit.INTERSPEECH.Junwen Xing, Longyue Wang, Derek F. Wong,Lidia S. Chao, and Xiaodong Zeng.
2013.UM-Checker: A Hybrid System for EnglishGrammatical Error Correction.
Proceedings ofthe Seventeenth Conference on ComputationalNatural Language Learning: Shared Task,34?42.
Sofia, Bulgaria: Association forComputational Linguistics.
Retrieved fromhttp://www.aclweb.org/anthology/W13-3605Bong-Jun Yi, Ho-Chang Lee, and Hae-ChangRim.
2013.
KUNLP Grammatical ErrorCorrection System For CoNLL-2013 Shared89Task.
CoNLL-2013.Ippei Yoshimoto, Tomoya Kose, KensukeMitsuzawa, Keisuke Sakaguchi, TomoyaMizumoto, Yuta Hayashibe, MamoruKomachi, et al.
2013.
NAIST at 2013 CoNLLgrammatical error correction shared task.CoNLL-2013.Zheng Yuan, and Mariano Felice.
2013.Constrained grammatical error correctionusing Statistical Machine Translation.CoNLL-2013.90
