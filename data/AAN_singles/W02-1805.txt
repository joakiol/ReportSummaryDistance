Categorical Ambiguity and Information ContentA Corpus-based Study of ChineseChu-Ren Huang, Ru-Yng ChangInstitute of Linguistics, Preparatory Office, Academia Sinica128 Sec.2 Academy Rd., Nangkang, Taipei, 115, Taiwan, R.O.C.churen@gate.sinica.edu.tw, ruyng@hp.iis.sinica.edu.tw1.
IntroductionAssignment of grammatical categories isthe fundamental step in natural languageprocessing.
And ambiguity resolution is one ofthe most challenging NLP tasks that is currentlystill beyond the power of machines.
When twoquestions are combined together, the problem ofresolution of categorical ambiguity is what acomputational linguistic system can doreasonably good, but yet still unable to mimic theexcellence of human beings.
This task is evenmore challenging in Chinese language processingbecause of the poverty of morphologicalinformation to mark categories and the lack ofconvention to mark word boundaries.
In thispaper, we try to investigate the nature ofcategorical ambiguity in Chinese based on SinicaCorpus.
The study differs crucially from previousstudies in that it directly measure informationcontent as the degree of ambiguity.
This methodnot only offers an alternative interpretation ofambiguity, it also allows a different measure ofsuccess of categorical disambiguation.
Instead ofprecision or recall, we can also measure by howmuch the information load has been reduced.This approach also allows us to identify whichare the most ambiguous words in terms ofinformation content.
The somewhat surprisingresult actually reinforces the Saussurian viewthat underlying the systemic linguistic structure,assignment of linguistic content for eachlinguistic symbol is arbitrary.2.
Previous WorkAssignment of grammatical categories ortagging is a well-tested NLP task that can bereliably preformed with stochastic methodologies(e.g.
Manning and Shutz 1999).
Depending onthe measurement method, over 95% precisioncan be achieved regularly.
But the questionremains as to why the last few percentages are sohard for machines and not a problem for humans.In addition, even though over 95% seems to begood scores intuitively, we still need to find outif they are indeed better than the na?ve baselineperformance.
Last but not the least, since naturallanguages are inherently and universallyambiguous, does this characteristic serve anycommunicative purpose and can a computationallinguistic model take advantage of the samecharacteristics.Since previous NLP work on categoricalassignment and ambiguity resolution achievedvery good results using only distributionalinformation, it seems natural to try to capture thenature of categorical ambiguity in terms ofdistributional information.
This is how thebaseline model was set in Meng and Ip (1999),among others.
Huang et al (2002), the mostextensive study on categorical ambiguity inChinese so far, also uses only distributionalinformation.Huang et al (2002) confirmed someexpected characteristics of ambiguity withconvincing quantitative and qualitative data fromthe one million word Sinica Corpus 2.0.
Theirgeneralizations include that categoricalambiguity correlates with frequency; that verbstend to be more ambiguous than nouns, and thatcertain categories (such as prepositions) areinherently more ambiguous.What is not totally unexpected, and yet runsagainst certain long-held assumptions is thedistribution of ambiguity.
It is found that only asmall fraction of all words (4.298%) are assignedmore than one category.
However, in terms ofactual use, these words make up 54.59% of thewhole corpus.
These two facts are consistentwith the frequency effect on ambiguity.
Aninteresting fact is that of all the words that canhave more than one category, 88.37% of theactual uses are in the default category.A significant fact regarding Chineselanguage processing can be derived from theabove data.
Presupposing lexical knowledge ofthe lexicon and the default category of each word,a na?ve baseline model for category assignmenttwo simple steps: First, if a word has only onecategory in the lexicon, assign that category tothe word.
Second, if a word has more than onecategory in the lexicon, assign the default (i.e.most frequently used) category to that word.Since step 1) is always correct and the precisionrate of step 2) depends on the percentage of useof the default category.
Huang et al (2002)estimated the expected precision of such a na?vemodel to be over 93.65%.Huang et al?s (2002) work, however, has itslimitation.
It takes categorical ambiguity as alexical attribute.
In other words, an attribute iseither + or -, and a certain word is eithercategorically ambiguous or not.
For Huang et al(2002), the degree of ambiguity is actually thedistribution of the attribute of being ambiguousamong a set of pre-defined (usually by frequencyranking) lexical items.
Strictly speaking, this dataonly shows the tendency of being categoricallyambiguous for the set members.
In other words,what has been shown is actually:Words with higher frequency are morelikely to be categorically ambiguous.The data has noting to say about whether alexical item or a set of lexical items are moreambiguous than others or not.A good example of the inadequacy ofHuang et al?s (2002) approach is theirmeasurement of the correlation between numberof potential categories and the likelihood ofdefault category to occur.No.
of Categories Freq.
(by type) Freq.
(by token)2 77.65%  91.21%3 77.71%  88.39%4  74.21%  89.50%5  73.83%  92.43%6  73.46%  86.09%7  68.51%  86.09%Total           77.36%99/48&Table 1.
Frequency of Default CategoryIn table one, the number seems to suggest thatnumber of possible categories of a word form isnot directly correlated with its degree ofambiguity, since its probability of being assignedthe default category is not predictable andremains roughly the same in average.
This issomewhat counter-intuitive in the sense that weexpect the more complex the informationstructure (i.e.
more possible categories), the lesslikely that it will be assigned a simple default.Since the methodology is to take distributionalinformation over a large corpus, it is most likelythe number shown in table 1 is distorted by thedominance of the most frequent words.Is there an alternative to pure distributionalmeasurement?
Recall that ambiguity is aboutinformation content.
Hence if the quantity ofinformation content is measured, there will be amore direct characterization of ambiguity.3.
Towards an Informational Descriptionof Categorical Ambiguity3.1.
Degree of Categorical AmbiguityIn this paper, we will adopt Shannon?sInformation Theory and measure categoricalambiguity by entropy.
We define the informationcontent of a sign as its entropy value.H = - (p0 log p0 + p1 log p1 + ?+pn log pn)When measuring categorical ambiguity, fora word with n potential categories, theinformation content of that word in terms ofgrammatical categories is the sum all the entropyof all its possible categories.
We will make thefurther assumption of that the degree ofambiguity of a word corresponds to the quantityof its information content.The above definition nicely reflects the intuitionthat the more predictable the category is, the lessambiguous it is.
That is, a word that can be 90%predicted by default is less ambiguous than aword that can only be predicted in 70% of thecontext.
And of course the least ambiguouswords are those with only one possible categoryand can be predicted all the time (it informationvalue is actually 0).3.2.
Degree of Ambiguity and Number ofPossible Categories RevisitedArmed with a new measurement of thedegree of ambiguity for each lexical item, we cannow take another look at the purported lack ofcorrelation between number of possiblecategories and degree of ambiguity.
Insteadhaving to choose between type of token as unitsof frequency counting, we can now calculate thedegree of categorical ambiguity for each lexicalform in terms of entropy.
The entropy of alllexical forms with the same numbers of possiblecategories can then be averaged.
The results isdiagramed below:Diagram 1.
Degree.
of Ambiguity vs.
Number of Categories?????????????????????????????
?
?
?
?
?
?
?
??
??
?
?Number of Possible CategoriesAverageDegreeof Amb.(Entropy)??
tags13 tagsIn the above diagram, we can clearly see thatwhether a 47 tags system or 13 tags system ischosen, the number of potential categoriescorrelates with the degree of ambiguity.
Thehigher number of potential categories a wordhas, the more ambiguous it is.
This correctlyreflects previous observational and theoreticalpredictions.3.3.
Frequency and Degree of AmbiguityOne of the important findings of Huang etal.
(2002) was that the likelihood to beambiguous indeed correlates with frequency.That is, a more frequently used word is morelikely to be categorically ambiguous.
However,we do not know that, of all the categoricallyambiguous words, whether their degree ofambiguity corresponds to frequency or not.In terms of the number of possiblecategories, more frequent words are more likelyto have larger number of categories.
Since wehave just showed in last session that largernumber of possible categories correlates withdegree of ambiguity.
This fact seems to favorthe prediction that more frequent words are alsomore ambiguous (i.e.
harder to predict theircategories.
)Common sense of empirical models,however, suggests that it is easier to predict thebehaviors of more familiar elements.Confidence of prediction corresponds toquantity of data.
A different manifestation ofthis feature is that there is a data sparsenessproblem but never a data abundance problem.
Inaddition, the high precision rate of categoricalassignment requires that most frequent words,which take up the majority of the corpus, beassigned correct category at a reasonableprecision rate.
These two facts seem to suggestthat the less frequent words may be harder topredict and hence more ambiguous..Diagram 2.
Frequency and ambiguity?
??????????????????
????
?????
?????
?????
?????
?????
?????
????Ambiguity? ? ??
???AG ?
? ? ??
???Diagram 2 plots the degree of ambiguity of eachambiguous word in terms of its frequency in theSinica Corpus (Chen et al 1996).
Not only doesthe distribution of the degree of ambiguity varywidely, the medium tendency line (thick blackline in the diagram) varies barely perceptiblyacross frequency ranges.
As suggested by thetwo competing tendencies discussed above, ourexhaustive study actually shows that there is nocorrelation between degree of ambiguity andfrequency.
This generalization can be shownwith even more clarity in Diagram 3.Diagram 3.
Degree of Ambiguity vs.
Frequency Ranking?????
???
???
???
???
????
???
?Frequency Ranking?? ??
??? ? ??
??In Diagram 3, entropy value of each word formis plotted against its frequency ranking.
Whenword forms share the same frequency, they aregiven the same ranking, and no ranking jumpswere given after multiple elements sharing thesame ranking.
Due to the sharing of rankings,the highest rank only goes to 1,000.
Diagram 3shows unequivocally that the range of degree ofambiguity remains the same across differentfrequency ranges.
That is, degree of ambiguitydoes not correlate to word frequency.4.
ConclusionIn this paper, we propose aninformation-based measure for ambiguity inChinese.
The measurement compliments themore familiar distributional data and allows usto investigate directly the categoricalinformation content of each lexical word.
Weshowed in this paper that degree of ambiguityindeed correlates with the number of possiblecategories of that word.
However, degree ofambiguity of a word does not correlates with itsfrequency, although its tendency to becategorically ambiguous is dependent onfrequency.The above findings have very importantimplications for theories and applications inlanguage processing.
In terms of representationof linguistic knowledge, it underlines thearbitrariness of the encoding of lexicalinformation, following Saussure.
In terms ofprocessing model and empirical prediction, itsuggests a model not unlike the theory ofunpredictability in physics.
Each word is like anelectron.
While the behavior of a group of wordscan be accurately predicted by stochastic model,the behavior of any single word is notpredictable.
In terms of linguistic theory, this isbecause there are too many rules that may applyto each lexical item at different time and ondifferent levels, hence we cannot predict exactlyhow these rules the results without knowsexactly which ones applied and in what order.This view is compatible with the LexicalDiffusion (Wang 1969) view on application oflinguistic rules.In NLP, this clearly predicts theperformance ceiling of stochastic approaches.As well as that the ceiling can be surpassed byhybriding with specific lexical heuristic rulescovering the ?hard?
cases for stochasticapproaches, as suggested in Huang et al (2002).References:Chen, Keh-jiann, Chu-Ren Huang, Li-ping Chang, andHui-Li Hsu.
1996.
Sinica Corpus: DesignMethodology for Balanced Corpora.
In.
B.-S. Parkand J.B. Kim.
Eds.
Proceeding of the 11th Pacific AsiaConference on Language, Information andComputation.
167-176.http//:www.sinica.edu.tw/SinicaCorpusChinese Knowledge Information Processing (CKIP) Group.1995.
An Introduction to Academia Sinica BalancedCorpus for Modern Mandarin Chinese.
CKIP TechnicalReport.
95-01.
Nankang: Academia SinicHuang, Chu-Ren, Chao-Ran Chen and Claude C.C.
Shen.2002.
Quantitative Criteria for Computational ChineseThe Nature of Categorical Ambiguity and ItsImplications for Language Processing: ACorpus-based Study of Mandarin Chinese.
MineharuNakayama (Ed.)
Sentence Processing in East AsianLanguages.
53-83.
Stanford: CSLI PublicationsManning Christopher D. and Hinrich Shutze.
1999.Foundations of Statistical Natural LanguageProcessing.
Cambridge: MIT Press.Meng, Helen and Chun Wah Ip.
1999.
An Analytical Studyof Transformational Tagging for Chinese Text.
InProceedings of ROCLING XII.
101-122.
Taipei:Association of Computational Linguistics and ChineseLanguage Processing.Schutz, Hinrich.
1997.
Ambiguity Resolution in LanguageLearning: Computational and Cognitive Models.
Stanford:CSLI Publications.Wang, S.-Y.
W. 1969.
Competing Changes as a Cause ofResidue.
Language.
45.9-25.
