Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059?1069,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsEfficient Non-parametric Estimation ofMultiple Embeddings per Word in Vector SpaceArvind Neelakantan*, Jeevan Shankar*, Alexandre Passos, Andrew McCallumDepartment of Computer ScienceUniversity of Massachusetts, AmherstAmherst, MA, 01003{arvind,jshankar,apassos,mccallum}@cs.umass.eduAbstractThere is rising interest in vector-spaceword embeddings and their use in NLP,especially given recent methods for theirfast estimation at very large scale.
Nearlyall this work, however, assumes a sin-gle vector per word type?ignoring poly-semy and thus jeopardizing their useful-ness for downstream tasks.
We presentan extension to the Skip-gram model thatefficiently learns multiple embeddings perword type.
It differs from recent relatedwork by jointly performing word sensediscrimination and embedding learning,by non-parametrically estimating the num-ber of senses per word type, and by its ef-ficiency and scalability.
We present newstate-of-the-art results in the word similar-ity in context task and demonstrate its scal-ability by training with one machine on acorpus of nearly 1 billion tokens in lessthan 6 hours.1 IntroductionRepresenting words by dense, real-valued vectorembeddings, also commonly called ?distributedrepresentations,?
helps address the curse of di-mensionality and improve generalization becausethey can place near each other words having sim-ilar semantic and syntactic roles.
This has beenshown dramatically in state-of-the-art results onlanguage modeling (Bengio et al, 2003; Mnih andHinton, 2007) as well as improvements in othernatural language processing tasks (Collobert andWeston, 2008; Turian et al, 2010).
Substantialbenefit arises when embeddings can be trained onlarge volumes of data.
Hence the recent consider-able interest in the CBOW and Skip-gram models*The first two authors contributed equally to this paper.of Mikolov et al (2013a); Mikolov et al (2013b)?relatively simple log-linear models that can betrained to produce high-quality word embeddingson the entirety of English Wikipedia text in lessthan half a day on one machine.There is rising enthusiasm for applying thesemodels to improve accuracy in natural languageprocessing, much like Brown clusters (Brown etal, 1992) have become common input featuresfor many tasks, such as named entity extraction(Miller et al, 2004; Ratinov and Roth, 2009) andparsing (Koo et al, 2008; T?ackstr?om et al, 2012).In comparison to Brown clusters, the vector em-beddings have the advantages of substantially bet-ter scalability in their training, and intriguing po-tential for their continuous and multi-dimensionalinterrelations.
In fact, Passos et al (2014) presentnew state-of-the-art results in CoNLL 2003 namedentity extraction by directly inputting continuousvector embeddings obtained by a version of Skip-gram that injects supervision with lexicons.
Sim-ilarly Bansal et al (2014) show results in depen-dency parsing using Skip-gram embeddings.
Theyhave also recently been applied to machine trans-lation (Zou et al, 2013; Mikolov et al, 2013c).A notable deficiency in this prior work is thateach word type (e.g.
the word string plant) hasonly one vector representation?polysemy andhononymy are ignored.
This results in the wordplant having an embedding that is approximatelythe average of its different contextual seman-tics relating to biology, placement, manufactur-ing and power generation.
In moderately high-dimensional spaces a vector can be relatively?close?
to multiple regions at a time, but this doesnot negate the unfortunate influence of the triangleinequality2here: words that are not synonyms butare synonymous with different senses of the sameword will be pulled together.
For example, pollenand refinery will be inappropriately pulled to a dis-2For distance d, d(a, c) ?
d(a, b) + d(b, c).1059tance not more than the sum of the distances plant?pollen and plant?refinery.
Fitting the constraints oflegitimate continuous gradations of semantics arechallenge enough without the additional encum-brance of these illegitimate triangle inequalities.Discovering embeddings for multiple senses perword type is the focus of work by Reisinger andMooney (2010a) and Huang et al (2012).
Theyboth pre-cluster the contexts of a word type?s to-kens into discriminated senses, use the clusters tore-label the corpus?
tokens according to sense, andthen learn embeddings for these re-labeled words.The second paper improves upon the first by em-ploying an earlier pass of non-discriminated em-bedding learning to obtain vectors used to rep-resent the contexts.
Note that by pre-clustering,these methods lose the opportunity to jointly learnthe sense-discriminated vectors and the cluster-ing.
Other weaknesses include their fixed num-ber of sense per word type, and the computationalexpense of the two-step process?the Huang etal (2012) method took one week of computationto learn multiple embeddings for a 6,000 subsetof the 30,000 vocabulary on a corpus containingclose to billion tokens.3This paper presents a new method for learn-ing vector-space embeddings for multiple sensesper word type, designed to provide several ad-vantages over previous approaches.
(1) Sense-discriminated vectors are learned jointly with theassignment of token contexts to senses; thus wecan use the emerging sense representation to moreaccurately perform the clustering.
(2) A non-parametric variant of our method automaticallydiscovers a varying number of senses per wordtype.
(3) Efficient online joint training makesit fast and scalable.
We refer to our method asMultiple-sense Skip-gram, or MSSG, and its non-parametric counterpart as NP-MSSG.Our method builds on the Skip-gram model(Mikolov et al, 2013a), but maintains multiplevectors per word type.
During online trainingwith a particular token, we use the average of itscontext words?
vectors to select the token?s sensethat is closest, and perform a gradient update onthat sense.
In the non-parametric version of ourmethod, we build on facility location (Meyerson,2001): a new cluster is created with probabilityproportional to the distance from the context to the3Personal communication with authors Eric H. Huang andRichard Socher.nearest sense.We present experimental results demonstratingthe benefits of our approach.
We show quali-tative improvements over single-sense Skip-gramand Huang et al (2012), comparing against wordneighbors from our parametric and non-parametricmethods.
We present quantitative results in threetasks.
On both the SCWS and WordSim353 datasets our methods surpass the previous state-of-the-art.
The Google Analogy task is not espe-cially well-suited for word-sense evaluation sinceits lack of context makes selecting the sense dif-ficult; however our method dramatically outper-forms Huang et al (2012) on this task.
Finallywe also demonstrate scalabilty, learning multiplesenses, training on nearly a billion tokens in lessthan 6 hours?a 27x improvement on Huang et al.2 Related WorkMuch prior work has focused on learning vectorrepresentations of words; here we will describeonly those most relevant to understanding this pa-per.
Our work is based on neural language mod-els, proposed by Bengio et al (2003), which extendthe traditional idea of n-gram language models byreplacing the conditional probability table with aneural network, representing each word token bya small vector instead of an indicator variable, andestimating the parameters of the neural networkand these vectors jointly.
Since the Bengio et al(2003) model is quite expensive to train, much re-search has focused on optimizing it.
Collobert andWeston (2008) replaces the max-likelihood char-acter of the model with a max-margin approach,where the network is encouraged to score the cor-rect n-grams higher than randomly chosen incor-rect n-grams.
Mnih and Hinton (2007) replacesthe global normalization of the Bengio model witha tree-structured probability distribution, and alsoconsiders multiple positions for each word in thetree.More relevantly, Mikolov et al (2013a) andMikolov et al (2013b) propose extremely com-putationally efficient log-linear neural languagemodels by removing the hidden layers of the neu-ral networks and training from larger context win-dows with very aggressive subsampling.
Thegoal of the models in Mikolov et al (2013a) andMikolov et al (2013b) is not so much obtain-ing a low-perplexity language model as learn-ing word representations which will be useful in1060downstream tasks.
Neural networks or log-linearmodels also do not appear to be necessary tolearn high-quality word embeddings, as Dhillonand Ungar (2011) estimate word vector repre-sentations using Canonical Correlation Analysis(CCA).Word vector representations or embeddingshave been used in various NLP tasks suchas named entity recognition (Neelakantan andCollins, 2014; Passos et al, 2014; Turian et al,2010), dependency parsing (Bansal et al, 2014),chunking (Turian et al, 2010; Dhillon and Ungar,2011), sentiment analysis (Maas et al, 2011), para-phrase detection (Socher et al, 2011) and learningrepresentations of paragraphs and documents (Leand Mikolov, 2014).
The word clusters obtainedfrom Brown clustering (Brown et al, 1992) havesimilarly been used as features in named entityrecognition (Miller et al, 2004; Ratinov and Roth,2009) and dependency parsing (Koo et al, 2008),among other tasks.There is considerably less prior work on learn-ing multiple vector representations for the sameword type.
Reisinger and Mooney (2010a) intro-duce a method for constructing multiple sparse,high-dimensional vector representations of words.Huang et al (2012) extends this approach incor-porating global document context to learn mul-tiple dense, low-dimensional embeddings by us-ing recursive neural networks.
Both the meth-ods perform word sense discrimination as a pre-processing step by clustering contexts for eachword type, making training more expensive.While methods such as those described in Dhillonand Ungar (2011) and Reddy et al (2011) usetoken-specific representations of words as partof the learning algorithm, the final outputs arestill one-to-one mappings between word types andword embeddings.3 Background: Skip-gram modelThe Skip-gram model learns word embeddingssuch that they are useful in predicting the sur-rounding words in a sentence.
In the Skip-grammodel, v(w) ?
Rdis the vector representation ofthe word w ?
W , where W is the words vocabu-lary and d is the embedding dimensionality.Given a pair of words (wt, c), the probabilitythat the word c is observed in the context of wordwtis given by,P (D = 1|v(wt), v(c)) =11 + e?v(wt)Tv(c)(1)The probability of not observing word c in the con-text of wtis given by,P (D = 0|v(wt), v(c)) =1?
P (D = 1|v(wt), v(c))Given a training set containing the sequence ofword types w1, w2, .
.
.
, wT, the word embeddingsare learned by maximizing the following objectivefunction:J(?)
=?
(wt,ct)?D+?c?ctlogP (D = 1|v(wt), v(c))+?(wt,c?t)?D??c?
?c?tlogP (D = 0|v(wt), v(c?
))where wtis the tthword in the training set, ctis the set of observed context words of word wtand c?tis the set of randomly sampled, noisy con-text words for the word wt.
D+consists ofthe set of all observed word-context pairs (wt, ct)(t = 1, 2 .
.
.
, T ).
D?consists of pairs (wt, c?t)(t = 1, 2 .
.
.
, T ) where c?tis the set of randomlysampled, noisy context words for the word wt.For each training word wt, the set of contextwords ct= {wt?Rt, .
.
.
, wt?1, wt+1, .
.
.
, wt+Rt}includesRtwords to the left and right of the givenword as shown in Figure 1.
Rtis the window sizeconsidered for the word wtuniformly randomlysampled from the set {1, 2, .
.
.
, N}, where N isthe maximum context window size.The set of noisy context words c?tfor the wordwtis constructed by randomly sampling S noisycontext words for each word in the context ct. Thenoisy context words are randomly sampled fromthe following distribution,P (w) =punigram(w)3/4Z(2)where punigram(w) is the unigram distribution ofthe words and Z is the normalization constant.4 Multi-Sense Skip-gram (MSSG) modelTo extend the Skip-gram model to learn multipleembeddings per word we follow previous work(Huang et al, 2012; Reisinger and Mooney, 2010a)1061WordVectorword wtv(wt+2)ContextVectorsv(wt+1)v(wt-1)v(wt-2)v(wt)Figure 1: Architecture of the Skip-gram modelwith window size Rt= 2.
Context ctof wordwtconsists of wt?1, wt?2, wt+1, wt+2.and let each sense of word have its own embed-ding, and induce the senses by clustering the em-beddings of the context words around each token.The vector representation of the context is the av-erage of its context words?
vectors.
For every wordtype, we maintain clusters of its contexts and thesense of a word token is predicted as the clusterthat is closest to its context representation.
Afterpredicting the sense of a word token, we performa gradient update on the embedding of that sense.The crucial difference from previous approachesis that word sense discrimination and learning em-beddings are performed jointly by predicting thesense of the word using the current parameter es-timates.In the MSSG model, each word w ?
W isassociated with a global vector vg(w) and eachsense of the word has an embedding (sense vec-tor) vs(w, k) (k = 1, 2, .
.
.
,K) and a context clus-ter with center ?
(w, k) (k = 1, 2, .
.
.
,K).
The Ksense vectors and the global vectors are of dimen-sion d and K is a hyperparameter.Consider the word wtand let ct={wt?Rt, .
.
.
, wt?1, wt+1, .
.
.
, wt+Rt} be theset of observed context words.
The vector repre-sentation of the context is defined as the averageof the global vector representation of the words inthe context.
Let vcontext(ct) =12?Rt?c?ctvg(c)be the vector representation of the context ct. Weuse the global vectors of the context words insteadof its sense vectors to avoid the computationalcomplexity associated with predicting the senseof the context words.
We predict st, the senseWord 6enseVectorsv(wt2)vJ(wt+2)ContextVectorsvJ(wt+1)vJ(wt-1)vJ(wt-2)$verDJe ContextVectorContext COXsterCentersv(wt1)v(wt)3redLcted6ense st?(wt1)vcontext(ct)?(wt2)?
(wt)ContextVectorsvJ(wt+2)vJ(wt+1)vJ(wt-1)vJ(wt-2)Figure 2: Architecture of Multi-Sense Skip-gram(MSSG) model with window size Rt= 2 andK = 3.
Context ctof word wtconsists ofwt?1, wt?2, wt+1, wt+2.
The sense is predicted byfinding the cluster center of the context that is clos-est to the average of the context vectors.of word wtwhen observed with context ctasthe context cluster membership of the vectorvcontext(ct) as shown in Figure 2.
More formally,st= argmaxk=1,2,...,Ksim(?
(wt, k), vcontext(ct)) (3)The hard cluster assignment is similar to the k-means algorithm.
The cluster center is the aver-age of the vector representations of all the contextswhich belong to that cluster.
For sim we use co-sine similarity in our experiments.Here, the probability that the word c is observedin the context of word wtgiven the sense of theword wtis,P (D = 1|st,vs(wt, 1), .
.
.
, vs(wt,K), vg(c))= P (D = 1|vs(wt, st), vg(c))=11 + e?vs(wt,st)Tvg(c)The probability of not observing word c in the con-text of wtgiven the sense of the word wtis,P (D = 0|st,vs(wt, 1), .
.
.
, vs(wt,K), vg(c))= P (D = 0|vs(wt, st), vg(c))= 1?
P (D = 1|vs(wt, st), vg(c))Given a training set containing the sequence ofword types w1, w2, ..., wT, the word embeddingsare learned by maximizing the following objective1062Algorithm 1 Training Algorithm of MSSG model1: Input: w1, w2, ..., wT, d, K, N .2: Initialize vs(w, k) and vg(w), ?w ?
W,k ?
{1, .
.
.
,K} randomly, ?
(w, k) ?w ?
W,k ?
{1, .
.
.
,K} to 0.3: for t = 1, 2, .
.
.
, T do4: Rt?
{1, .
.
.
, N}5: ct= {wt?Rt, .
.
.
, wt?1, wt+1, .
.
.
, wt+Rt}6: vcontext(ct) =12?Rt?c?ctvg(c)7: st= argmaxk=1,2,...,K{sim(?
(wt, k), vcontext(ct))}8: Update context cluster center ?
(wt, st)since context ctis added to context cluster stof word wt.9: c?t= Noisy Samples(ct)10: Gradient update on vs(wt, st), global vec-tors of words in ctand c?t.11: end for12: Output: vs(w, k), vg(w) and context clustercenters ?
(w, k), ?w ?W,k ?
{1, .
.
.
,K}function:J(?)
=?
(wt,ct)?D+?c?ctlogP (D = 1|vs(wt, st), vg(c))+?(wt,c?t)?D??c?
?c?tlogP (D = 0|vs(wt, st), vg(c?
))where wtis the tthword in the sequence, ctis theset of observed context words and c?tis the set ofnoisy context words for the word wt.
D+and D?are constructed in the same way as in the Skip-gram model.After predicting the sense of word wt, we up-date the embedding of the predicted sense forthe word wt(vs(wt, st)), the global vector of thewords in the context and the global vector of therandomly sampled, noisy context words.
The con-text cluster center of cluster stfor the word wt(?
(wt, st)) is updated since context ctis added tothe cluster st.5 Non-Parametric MSSG model(NP-MSSG)The MSSG model learns a fixed number of sensesper word type.
In this section, we describe anon-parametric version of MSSG, the NP-MSSGmodel, which learns varying number of senses perword type.
Our approach is closely related tothe online non-parametric clustering procedure de-scribed in Meyerson (2001).
We create a new clus-ter (sense) for a word type with probability propor-tional to the distance of its context to the nearestcluster (sense).Each wordw ?W is associated with sense vec-tors, context clusters and a global vector vg(w) asin the MSSG model.
The number of senses for aword is unknown and is learned during training.Initially, the words do not have sense vectors andcontext clusters.
We create the first sense vectorand context cluster for each word on its first occur-rence in the training data.
After creating the firstcontext cluster for a word, a new context clusterand a sense vector are created online during train-ing when the word is observed with a context werethe similarity between the vector representation ofthe context with every existing cluster center of theword is less than ?, where ?
is a hyperparameterof the model.Consider the word wtand let ct={wt?Rt, .
.
.
, wt?1, wt+1, .
.
.
, wt+Rt} be theset of observed context words.
The vector repre-sentation of the context is defined as the averageof the global vector representation of the words inthe context.
Let vcontext(ct) =12?Rt?c?ctvg(c)be the vector representation of the context ct. Letk(wt) be the number of context clusters or thenumber of senses currently associated with wordwt.
st, the sense of word wtwhen k(wt) > 0 isgiven byst=????
?k(wt) + 1, ifmaxk=1,2,...,k(wt){sim(?
(wt, k), vcontext(ct))} < ?kmax, otherwise(4)where ?
(wt, k) is the cluster center ofthe kthcluster of word wtand kmax=argmaxk=1,2,...,k(wt)sim(?
(wt, k), vcontext(ct)).The cluster center is the average of the vectorrepresentations of all the contexts which belong tothat cluster.
If st= k(wt) + 1, a new contextcluster and a new sense vector are created for theword wt.The NP-MSSG model and the MSSG modeldescribed previously differ only in the way wordsense discrimination is performed.
The objec-tive function and the probabilistic model associ-ated with observing a (word, context) pair giventhe sense of the word remain the same.1063Model Time (in hours)Huang et al 168MSSG 50d 1MSSG-300d 6NP-MSSG-50d 1.83NP-MSSG-300d 5Skip-gram-50d 0.33Skip-gram-300d 1.5Table 1: Training Time Results.
First five modelreported in the table are capable of learning mul-tiple embeddings for each word and Skip-gramis capable of learning only single embedding foreach word.6 ExperimentsTo evaluate our algorithms we train embeddingsusing the same corpus and vocabulary as used inHuang et al (2012), which is the April 2010 snap-shot of the Wikipedia corpus (Shaoul and West-bury, 2010).
It contains approximately 2 millionarticles and 990 million tokens.
In all our experi-ments we remove all the words with less than 20occurrences and use a maximum context window(N ) of length 5 (5 words before and after the wordoccurrence).
We fix the number of senses (K) tobe 3 for the MSSG model unless otherwise speci-fied.
Our hyperparameter values were selected bya small amount of manual exploration on a vali-dation set.
In NP-MSSG we set ?
to -0.5.
TheSkip-gram model, MSSG and NP-MSSG modelssample one noisy context word (S) for each of theobserved context words.
We train our models us-ing AdaGrad stochastic gradient decent (Duchi etal, 2011) with initial learning rate set to 0.025.Similarly to Huang et al (2012), we don?t use aregularization penalty.Below we describe qualitative results, display-ing the embeddings and the nearest neighbors ofeach word sense, and quantitative experiments intwo benchmark word similarity tasks.Table 1 shows time to train our models, com-pared with other models from previous work.
Allthese times are from single-machine implementa-tions running on similar-sized corpora.
We seethat our model shows significant improvement inthe training time over the model in Huang etal (2012), being within well within an order-of-magnitude of the training time for Skip-gram mod-els.APPLESkip-gram blackberry, macintosh, acorn, pear, plumMSSGpear, honey, pumpkin, potato, nutmicrosoft, activision, sony, retail, gamestopmacintosh, pc, ibm, iigs, chipsetsNP-MSSGapricot, blackberry, cabbage, blackberries, pearmicrosoft, ibm, wordperfect, amiga, trs-80FOXSkip-gram abc, nbc, soapnet, espn, kttvMSSGbeaver, wolf, moose, otter, swannbc, espn, cbs, ctv, pbsdexter, myers, sawyer, kelly, griffithNP-MSSGrabbit, squirrel, wolf, badger, stoatcbs,abc, nbc, wnyw, abc-tvNETSkip-gram profit, dividends, pegged, profits, netsMSSGsnap, sideline, ball, game-trying, scoringnegative, offset, constant, hence, potentialpre-tax, billion, revenue, annualized, us$NP-MSSGnegative, total, transfer, minimizes, looppre-tax, taxable, per, billion, us$, incomeball, yard, fouled, bounced, 50-yardwnet, tvontorio, cable, tv, tv-5ROCKSkip-gram glam, indie, punk, band, popMSSGrocks, basalt, boulders, sand, quartzitealternative, progressive, roll, indie, blues-rockrocks, pine, rocky, butte, deerNP-MSSGgranite, basalt, outcropping, rocks, quartzitealternative, indie, pop/rock, rock/metal, blues-rockRUNSkip-gram running, ran, runs, afoul, amokMSSGrunning, stretch, ran, pinch-hit, runsoperated , running, runs, operate, managedrunning, runs, operate, drivers, configureNP-MSSGtwo-run, walk-off, runs, three-runs, startsoperated, runs, serviced, links, walkrunning, operating, ran, go, configurere-election, reelection, re-elect, unseat, term-limitedhelmed, longest-running, mtv, promoted, producedTable 2: Nearest neighbors of each sense of eachword, by cosine similarity, for different algo-rithms.
Note that the different senses closely cor-respond to intuitions regarding the senses of thegiven word types.6.1 Nearest NeighborsTable 2 shows qualitatively the results of dis-covering multiple senses by presenting the near-est neighbors associated with various embeddings.The nearest neighbors of a word are computed bycomparing the cosine similarity between the em-bedding for each sense of the word and the contextembeddings of all other words in the vocabulary.Note that each of the discovered senses are indeedsemantically coherent, and that a reasonable num-ber of senses are created by the non-parametricmethod.
Table 3 shows the nearest neighbors ofthe word plant for Skip-gram, MSSG , NP-MSSGand Haung?s model (Huang et al, 2012).1064Skip-gramplants, flowering, weed, fungus, biomassMS-SGplants, tubers, soil, seed, biomassrefinery, reactor, coal-fired, factory, smelterasteraceae, fabaceae, arecaceae, lamiaceae, eri-caceaeNPMS-SGplants, seeds, pollen, fungal, fungusfactory, manufacturing, refinery, bottling, steelfabaceae, legume, asteraceae, apiaceae, floweringpower, coal-fired, hydro-power, hydroelectric, re-fineryHua-nget alinsect, capable, food, solanaceous, subsurfacerobust, belong, pitcher, comprises, eaglesfood, animal, catching, catch, ecology, flyseafood, equipment, oil, dairy, manufacturerfacility, expansion, corporation, camp, co.treatment, skin, mechanism, sugar, drugfacility, theater, platform, structure, storagenatural, blast, energy, hurl, powermatter, physical, certain, expression, agentsvine, mute, chalcedony, quandong, excreteTable 3: Nearest Neighbors of the word plantfor different models.
We see that the discoveredsenses in both our models are more semanticallycoherent than Huang et al (2012) and NP-MSSGis able to learn reasonable number of senses.6.2 Word SimilarityWe evaluate our embeddings on two relateddatasets: the WordSim-353 (Finkelstein et al,2001) dataset and the Contextual Word Similari-ties (SCWS) dataset Huang et al (2012).WordSim-353 is a standard dataset for evaluat-ing word vector representations.
It consists of alist of pairs of word types, the similarity of whichis rated in an integral scale from 1 to 10.
Pairsinclude both monosemic and polysemic words.These scores to each word pairs are given with-out any contextual information, which makes themtricky to interpret.To overcome this issue, Stanford?s ContextualWord Similarities (SCWS) dataset was developedby Huang et al (2012).
The dataset consists of2003 word pairs and their sentential contexts.
Itconsists of 1328 noun-noun pairs, 399 verb-verbpairs, 140 verb-noun, 97 adjective-adjective, 30noun-adjective, 9 verb-adjective, and 241 same-word pairs.
We evaluate and compare our embed-dings on both WordSim-353 and SCWS word sim-ilarity corpus.Since it is not trivial to deal with multiple em-beddings per word, we consider the following sim-ilarity measures between words w and w?giventheir respective contexts c and c?, where P (w, c, k)is the probability that w takes the kthsense giventhe context c, and d(vs(w, i), vs(w?, j)) is the sim-ilarity measure between the given embeddingsvs(w, i) and vs(w?, j).The avgSim metric,avgSim(w,w?
)=1K2K?i=1K?j=1d (vs(w, i), vs(w?, j)) ,computes the average similarity over all embed-dings for each word, ignoring information fromthe context.To address this, the avgSimC metric,avgSimC(w,w?)
=K?j=1K?i=1P (w, c, i)P (w?, c?, j)?
d (vs(w, i), vs(w?, j))weighs the similarity between each pair of sensesby how well does each sense fit the context athand.The globalSim metric uses each word?s globalcontext vector, ignoring the many senses:globalSim(w,w?)
= d (vg(w), vg(w?))
.Finally, localSim metric selects a single sensefor each word based independently on its contextand computes the similarity bylocalSim(w,w?)
= d (vs(w, k), vs(w?, k?))
,where k = argmaxiP (w, c, i) and k?=argmaxjP (w?, c?, j) and P (w, c, i) is the prob-ability that w takes the ithsense given context c.The probability of being in a cluster is calculatedas the inverse of the cosine distance to the clustercenter (Huang et al, 2012).We report the Spearman correlation between amodel?s similarity scores and the human judge-ments in the datasets.Table 5 shows the results on WordSim-353task.
C&W refers to the language model by Col-lobert and Weston (2008) and HLBL model is themethod described in Mnih and Hinton (2007).
OnWordSim-353 task, we see that our model per-forms significantly better than the previous neuralnetwork model for learning multi-representationsper word (Huang et al, 2012).
Among the meth-ods that learn low-dimensional and dense repre-sentations, our model performs slightly better thanSkip-gram.
Table 4 shows the results for theSCWS task.
In this task, when the words are1065Model globalSim avgSim avgSimC localSimTF-IDF 26.3 - - -Collobort & Weston-50d 57.0 - - -Skip-gram-50d 63.4 - - -Skip-gram-300d 65.2 - - -Pruned TF-IDF 62.5 60.4 60.5 -Huang et al-50d 58.6 62.8 65.7 26.1MSSG-50d 62.1 64.2 66.9 49.17MSSG-300d 65.3 67.2 69.3 57.26NP-MSSG-50d 62.3 64.0 66.1 50.27NP-MSSG-300d 65.5 67.3 69.1 59.80Table 4: Experimental results in the SCWS task.
The numbers are Spearmans correlation ?
?
100between each model?s similarity judgments and the human judgments, in context.
First three modelslearn only a single embedding per model and hence, avgSim, avgSimC and localSim are not reportedfor these models, as they?d be identical to globalSim.
Both our parametric and non-parametric modelsoutperform the baseline models, and our best model achieves a score of 69.3 in this task.
NP-MSSGachieves the best results when globalSim, avgSim and localSim similarity measures are used.
The bestresults according to each metric are in bold face.Model ??
100HLBL 33.2C&W 55.3Skip-gram-300d 70.4Huang et al-G 22.8Huang et al-M 64.2MSSG 50d-G 60.6MSSG 50d-M 63.2MSSG 300d-G 69.2MSSG 300d-M 70.9NP-MSSG 50d-G 61.5NP-MSSG 50d-M 62.4NP-MSSG 300d-G 69.1NP-MSSG 300d-M 68.6Pruned TF-IDF 73.4ESA 75Tiered TF-IDF 76.9Table 5: Results on the WordSim-353 dataset.The table shows the Spearmans correlation ?
be-tween the model?s similarities and human judg-ments.
G indicates the globalSim similarity mea-sure and M indicates avgSim measure.The bestresults among models that learn low-dimensionaland dense representations are in bold face.
PrunedTF-IDF (Reisinger and Mooney, 2010a), ESA(Gabrilovich and Markovitch, 2007) and TieredTF-IDF (Reisinger and Mooney, 2010b) constructspare, high-dimensional representations.Figure 3: The plot shows the distribution of num-ber of senses learned per word type in NP-MSSGmodelgiven with their context, our model achieves newstate-of-the-art results on SCWS as shown in theTable-4.
The previous state-of-art model (Huanget al, 2012) on this task achieves 65.7% usingthe avgSimC measure, while the MSSG modelachieves the best score of 69.3% on this task.
Theresults on the other metrics are similar.
For afixed embedding dimension, the model by Huanget al (2012) has more parameters than our modelsince it uses a hidden layer.
The results showthat our model performs better than Huang et al(2012) even when both the models use 50 dimen-sional vectors and the performance of our modelimproves as we increase the number of dimensionsto 300.We evaluate the models in a word analogy task1066(a) (b)Figure 4: Figures (a) and (b) show the effect of varying embedding dimensionality and number of sensesrespectively of the MSSG Model on the SCWS task.Model Task Sim ??
100Skip-gram WS-353 globalSim 70.4MSSG WS-353 globalSim 68.4MSSG WS-353 avgSim 71.2NP MSSG WS-353 globalSim 68.3NP MSSG WS-353 avgSim 69.66MSSG SCWS localSim 59.3MSSG SCWS globalSim 64.7MSSG SCWS avgSim 67.2MSSG SCWS avgSimC 69.2NP MSSG SCWS localSim 60.11NP MSSG SCWS globalSim 65.3NP MSSG SCWS avgSim 67NP MSSG SCWS avgSimC 68.6Table 6: Experiment results on WordSim-353 andSCWS Task.
Multiple Embeddings are learned fortop 30,000 most frequent words in the vocabulary.The embedding dimension size is 300 for all themodels for this task.
The number of senses forMSSG model is 3.introduced by Mikolov et al (2013a) where bothMSSG and NP-MSSG models achieve 64% accu-racy compared to 12% accuracy by Huang et al(2012).
Skip-gram which is the state-of-art modelfor this task achieves 67% accuracy.Figure 3 shows the distribution of number ofsenses learned per word type in the NP-MSSGmodel.
We learn the multiple embeddings for thesame set of approximately 6000 words that wereused in Huang et al (2012) for all our experimentsto ensure fair comparision.
These approximately6000 words were choosen by Huang et al.
mainlyfrom the top 30,00 frequent words in the vocab-ulary.
This selection was likely made to avoidthe noise of learning multiple senses for infre-quent words.
However, our method is robust tonoise, which can be seen by the good performanceof our model that learns multiple embeddings forthe top 30,000 most frequent words.
We foundthat even by learning multiple embeddings for thetop 30,000 most frequent words in the vocubu-lary, MSSG model still achieves state-of-art resulton SCWS task with an avgSimC score of 69.2 asshown in Table 6.7 ConclusionWe present an extension to the Skip-gram modelthat efficiently learns multiple embeddings perword type.
The model jointly performs wordsense discrimination and embedding learning, andnon-parametrically estimates the number of sensesper word type.
Our method achieves new state-of-the-art results in the word similarity in con-text task and learns multiple senses, training onclose to billion tokens in less than 6 hours.
Theglobal vectors, sense vectors and cluster centers ofour model and code for learning them are avail-able at https://people.cs.umass.edu/?arvind/emnlp2014wordvectors.
In fu-ture work we plan to use the multiple embeddingsper word type in downstream NLP tasks.1067AcknowledgmentsThis work was supported in part by the Centerfor Intelligent Information Retrieval and in part byDARPA under agreement number FA8750-13-2-0020.
The U.S. Government is authorized to re-produce and distribute reprints for Governmentalpurposes notwithstanding any copyright notationthereon.
Any opinions, findings and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthose of the sponsor.ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring Continuous Word Representationsfor Dependency Parsing.
Association for Computa-tional Linguistics (ACL).Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search (JMLR).Peter F. Brown, Peter V. Desouza, Robert L. Mercer,Vincent J. Della Pietra, and Jenifer C. Lai.
1992.Class-based N-gram models of natural languageComputational Linguistics.Ronan Collobert and Jason Weston.
2008.
A Uni-fied Architecture for Natural Language Process-ing: Deep Neural Networks with Multitask Learn-ing.
International Conference on Machine learning(ICML).Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.2011.
Multi-View Learning of Word Embeddings viaCCA.
Advances in Neural Information ProcessingSystems (NIPS).John Duchi, Elad Hazan, and Yoram Singer 2011.Adaptive sub- gradient methods for online learn-ing and stochastic optimization.
Journal of MachineLearning Research (JMLR).Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: the con-cept revisited.
International Conference on WorldWide Web (WWW).Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
International JointConference on Artificial Intelligence (IJCAI).Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving WordRepresentations via Global Context and MultipleWord Prototypes.
Association of ComputationalLinguistics (ACL).Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple Semi-supervised Dependency Parsing.Association for Computational Linguistics (ACL).Quoc V. Le and Tomas Mikolov.
2014 DistributedRepresentations of Sentences and Documents.
Inter-national Conference on Machine Learning (ICML)Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011 Learning Word Vectors for Sentiment AnalysisAssociation for Computational Linguistics (ACL)Adam Meyerson.
2001 IEEE Symposium on Foun-dations of Computer Science.
International Confer-ence on Machine Learning (ICML)Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient Estimation of WordRepresentations in Vector Space.
Workshop at In-ternational Conference on Learning Representations(ICLR).Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed Repre-sentations of Words and Phrases and their Composi-tionality.
Advances in Neural Information Process-ing Systems (NIPS).Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.2013c.
Exploiting Similarities among Languagesfor Machine Translation.
arXiv.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and dis-criminative training.
North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies (NAACL-HLT).Andriy Mnih and Geoffrey Hinton.
2007.
Threenew graphical models for statistical language mod-elling.
International Conference on Machine learn-ing (ICML).Arvind Neelakantan and Michael Collins.
2014.Learning Dictionaries for Named Entity Recogni-tion using Minimal Supervision.
European Chap-ter of the Association for Computational Linguistics(EACL).Alexandre Passos, Vineet Kumar, and Andrew McCal-lum.
2014.
Lexicon Infused Phrase Embeddings forNamed Entity Resolution.
Conference on NaturalLanguage Learning (CoNLL).Lev Ratinov and Dan Roth.
2009.
Design Chal-lenges and Misconceptions in Named Entity Recog-nition.
Conference on Natural Language Learning(CoNLL).Siva Reddy, Ioannis P. Klapaftis, and Diana McCarthy.2011.
Dynamic and Static Prototype Vectors for Se-mantic Composition.
International Joint Conferenceon Artificial Intelligence (IJCNLP).1068Joseph Reisinger and Raymond J. Mooney.
2010a.Multi-prototype vector-space models of word mean-ing.
North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies (NAACL-HLT)Joseph Reisinger and Raymond Mooney.
2010b.
Amixture model with sharing for lexical semantics.Empirical Methods in Natural Language Processing(EMNLP).Cyrus Shaoul and Chris Westbury.
2010.
The Westburylab wikipedia corpus.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011Dynamic Pooling and Unfolding Recursive Autoen-coders for Paraphrase Detection.
Advances in Neu-ral Information Processing Systems (NIPS).Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkor-eit.
2012.
Cross-lingual Word Clusters for DirectTransfer of Linguistic Structure.
North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word Representations: A Simple and GeneralMethod for Semi-Supervised Learning.
Associationfor Computational Linguistics (ACL).Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual Word Embed-dings for Phrase-Based Machine Translation.
Em-pirical Methods in Natural Language Processing.1069
