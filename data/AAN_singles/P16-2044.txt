Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 269?274,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTweet2Vec: Character-Based DistributedRepresentations for Social MediaBhuwan Dhingra1, Zhong Zhou2, Dylan Fitzpatrick1,2Michael Muehl1and William W. Cohen11School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA2Heinz College, Carnegie Mellon University, Pittsburgh, PA, USA{bdhingra,djfitzpa,mmuehl}@andrew.cmu.eduzhongzhou@cmu.edu wcohen@cs.cmu.eduAbstractText from social media provides a set ofchallenges that can cause traditional NLPapproaches to fail.
Informal language,spelling errors, abbreviations, and specialcharacters are all commonplace in theseposts, leading to a prohibitively large vo-cabulary size for word-level approaches.We propose a character compositionmodel, tweet2vec, which finds vector-space representations of whole tweets bylearning complex, non-local dependenciesin character sequences.
The proposedmodel outperforms a word-level baselineat predicting user-annotated hashtags as-sociated with the posts, doing significantlybetter when the input contains many out-of-vocabulary words or unusual charactersequences.
Our tweet2vec encoder is pub-licly available1.1 IntroductionWe understand from Zipf?s Law that in any nat-ural language corpus a majority of the vocabu-lary word types will either be absent or occur inlow frequency.
Estimating the statistical proper-ties of these rare word types is naturally a diffi-cult task.
This is analogous to the curse of di-mensionality when we deal with sequences of to-kens - most sequences will occur only once in thetraining data.
Neural network architectures over-come this problem by defining non-linear compo-sitional models over vector space representationsof tokens and hence assign non-zero probabilityeven to sequences not seen during training (Ben-gio et al, 2003; Kiros et al, 2015).
In this work,we explore a similar approach to learning dis-tributed representations of social media posts by1https://github.com/bdhingra/tweet2veccomposing them from their constituent characters,with the goal of generalizing to out-of-vocabularywords as well as sequences at test time.Traditional Neural Network Language Models(NNLMs) treat words as the basic units of lan-guage and assign independent vectors to eachword type.
To constrain memory requirements,the vocabulary size is fixed before-hand; therefore,rare and out-of-vocabulary words are all groupedtogether under a common type ?UNKNOWN?.This choice is motivated by the assumption of ar-bitrariness in language, which means that surfaceforms of words have little to do with their semanticroles.
Recently, (Ling et al, 2015) challenge thisassumption and present a bidirectional Long ShortTerm Memory (LSTM) (Hochreiter and Schmid-huber, 1997) for composing word vectors fromtheir constituent characters which can memorizethe arbitrary aspects of word orthography as wellas generalize to rare and out-of-vocabulary words.Encouraged by their findings, we extend theirapproach to a much larger unicode character set,and model long sequences of text as functionsof their constituent characters (including white-space).
We focus on social media posts fromthe website Twitter, which are an excellent test-ing ground for character based models due to thenoisy nature of text.
Heavy use of slang andabundant misspellings means that there are manyorthographically and semantically similar tokens,and special characters such as emojis are also im-mensely popular and carry useful semantic infor-mation.
In our moderately sized training datasetof 2 million tweets, there were about 0.92 mil-lion unique word types.
It would be expensiveto capture all these phenomena in a word basedmodel in terms of both the memory requirement(for the increased vocabulary) and the amount oftraining data required for effective learning.
Ad-ditional benefits of the character based approach269include language independence of the methods,and no requirement of NLP preprocessing such asword-segmentation.A crucial step in learning good text representa-tions is to choose an appropriate objective functionto optimize.
Unsupervised approaches attempt toreconstruct the original text from its latent rep-resentation (Mikolov et al, 2013; Bengio et al,2003).
Social media posts however, come withtheir own form of supervision annotated by mil-lions of users, in the form of hashtags which linkposts about the same topic together.
A natural as-sumption is that the posts with the same hashtagsshould have embeddings which are close to eachother.
Hence, we formulate our training objectiveto maximize cross-entropy loss at the task of pre-dicting hashtags for a post from its latent represen-tation.We propose a Bi-directional Gated RecurrentUnit (Bi-GRU) (Chung et al, 2014) neural net-work for learning tweet representations.
Treat-ing white-space as a special character itself, themodel does a forward and backward pass over theentire sequence, and the final GRU states are lin-early combined to get the tweet embedding.
Pos-terior probabilities over hashtags are computedby projecting this embedding to a softmax out-put layer.
Compared to a word-level baseline thismodel shows improved performance at predictinghashtags for a held-out set of posts.
Inspired byrecent work in learning vector space text represen-tations, we name our model tweet2vec.2 Related WorkUsing neural networks to learn distributed repre-sentations of words dates back to (Bengio et al,2003).
More recently, (Mikolov et al, 2013) re-leased word2vec - a collection of word vectorstrained using a recurrent neural network.
Theseword vectors are in widespread use in the NLPcommunity, and the original work has since beenextended to sentences (Kiros et al, 2015), doc-uments and paragraphs (Le and Mikolov, 2014),topics (Niu and Dai, 2015) and queries (Grbovicet al, 2015).
All these methods require storing anextremely large table of vectors for all word typesand cannot be easily generalized to unseen wordsat test time (Ling et al, 2015).
They also requirepreprocessing to find word boundaries which isnon-trivial for a social network domain like Twit-ter.In (Ling et al, 2015), the authors present acompositional character model based on bidirec-tional LSTMs as a potential solution to these prob-lems.
A major benefit of this approach is that largeword lookup tables can be compacted into char-acter lookup tables and the compositional modelscales to large data sets better than other state-of-the-art approaches.
While (Ling et al, 2015)generate word embeddings from character repre-sentations, we propose to generate vector repre-sentations of entire tweets from characters in ourtweet2vec model.Our work adds to the growing body of workshowing the applicability of character models for avariety of NLP tasks such as Named Entity Recog-nition (Santos and Guimar?es, 2015), POS tag-ging (Santos and Zadrozny, 2014), text classifica-tion (Zhang et al, 2015) and language modeling(Karpathy et al, 2015; Kim et al, 2015).Previously, (Luong et al, 2013) dealt withthe problem of estimating rare word representa-tions by building them from their constituent mor-phemes.
While they show improved performanceover word-based models, their approach requiresa morpheme parser for preprocessing which maynot perform well on noisy text like Twitter.
Alsothe space of all morphemes, though smaller thanthe space of all words, is still large enough thatmodelling all morphemes is impractical.Hashtag prediction for social media has beenaddressed earlier, for example in (Weston et al,2014; Godin et al, 2013).
(Weston et al, 2014)also use a neural architecture, but compose textembeddings from a lookup table of words.
Theyalso show that the learned embeddings can gener-alize to an unrelated task of document recommen-dation, justifying the use of hashtags as supervi-sion for learning text representations.3 Tweet2VecBi-GRU Encoder: Figure 1 shows our modelfor encoding tweets.
It uses a similar structure tothe C2W model in (Ling et al, 2015), with LSTMunits replaced with GRU units.The input to the network is defined by an al-phabet of characters C (this may include the en-tire unicode character set).
The input tweet is bro-ken into a stream of characters c1, c2, ...cmeachof which is represented by a 1-by-|C| encoding.These one-hot vectors are then projected to a char-acter space by multiplying with the matrix PC?270Figure 1: Tweet2Vec encoder for social media textR|C|?dc, where dcis the dimension of the char-acter vector space.
Let x1, x2, ...xmbe the se-quence of character vectors for the input tweet af-ter the lookup.
The encoder consists of a forward-GRU and a backward-GRU.
Both have the samearchitecture, except the backward-GRU processesthe sequence in reverse order.
Each of the GRUunits process these vectors sequentially, and start-ing with the initial state h0compute the sequenceh1, h2, ...hmas follows:rt= ?
(Wrxt+ Urht?1+ br),zt= ?
(Wzxt+ Uzht?1+ bz),?ht= tanh(Whxt+ Uh(rtht?1) + bh),ht= (1?
zt) ht?1+ zt?ht.Here rt, ztare called the reset and update gatesrespectively, and?htis the candidate output statewhich is converted to the actual output state ht.Wr,Wz,Whare dh?
dcmatrices and Ur, Uz, Uhare dh?
dhmatrices, where dhis the hidden statedimension of the GRU.
The final states hfmfromthe forward-GRU, and hb0from the backward GRUare combined using a fully-connected layer to thegive the final tweet embedding et:et= Wfhfm+Wbhb0(1)Here Wf,Wbare dt?
dhand b is dt?
1 biasterm, where dtis the dimension of the final tweetembedding.
In our experiments we set dt= dh.All parameters are learned using gradient descent.Softmax: Finally, the tweet embedding ispassed through a linear layer whose output is thesame size as the number of hashtags L in the dataset.
We use a softmax layer to compute the poste-rior hashtag probabilities:P (y = j|e) =exp(wTje+ bj)?Li=1exp(wTie+ bj).
(2)Objective Function: We optimize the cate-gorical cross-entropy loss between predicted andtrue hashtags:J =1BB?i=1L?j=1?ti,jlog(pi,j) + ????2.
(3)Here B is the batch size, L is the number ofclasses, pi,jis the predicted probability that the i-th tweet has hashtag j, and ti,j?
{0, 1} denotesthe ground truth of whether the j-th hashtag is inthe i-th tweet.
We use L2-regularization weightedby ?.4 Experiments and Results4.1 Word Level BaselineSince our objective is to compare character-basedand word-based approaches, we have also imple-mented a simple word-level encoder for tweets.The input tweet is first split into tokens alongwhite-spaces.
A more sophisticated tokenizer maybe used, but for a fair comparison we wanted tokeep language specific preprocessing to a min-imum.
The encoder is essentially the same astweet2vec, with the input as words instead of char-acters.
A lookup table stores word vectors for theV (20K here) most common words, and the restare grouped together under the ?UNK?
token.4.2 DataOur dataset consists of a large collection of globalposts from Twitter2between the dates of June 1,2013 to June 5, 2013.
Only English language posts(as detected by the lang field in Twitter API) andposts with at least one hashtag are retained.
Weremoved infrequent hashtags (< 500 posts) sincethey do not have enough data for good general-ization.
We also removed very frequent tags (>19K posts) which were almost always from auto-matically generated posts (ex: #androidgame)which are trivial to predict.
The final dataset con-tains 2 million tweets for training, 10K for valida-tion and 50K for testing, with a total of 2039 dis-tinct hashtags.
We use simple regex to preprocessthe post text and remove hashtags (since these areto be predicted) and HTML tags, and replace user-names and URLs with special tokens.
We also re-moved retweets and convert the text to lower-case.2https://twitter.com/271Tweets Word model baseline tweet2vecninety-one degrees.#initialsofsomeone..#nw #gameofthrones#summer #loveit#sunself-cooked scramble egg.
yum!!
!url#music #cheap#cute#yummy #food#foodporncan?t sleeeeeeep#gameofthrones#heartbreaker#tired #insomniaoklahoma!!!!!!!!!!!
champions!!!!
!#initialsofsomeone..#nw #lrt#wcws #sooners#ou7 % of battery .
iphones die too quick .#help #power#money #s#fml #apple #bbl#thestrugglei have the cutest nephew in the world !url#nephew #cute#family#socute #cute#puppyTable 1: Examples of top predictions from the models.
The correct hashtag(s) if detected are in bold.word tweet2vecdt, dh200 500Total Parameters 3.91M 3.90MTraining Time / Epoch 1528s 9649sTable 2: Model sizes and training time/epoch4.3 Implementation DetailsWord vectors and character vectors are both set tosize dL= 150 for their respective models.
Therewere 2829 unique characters in the training set andwe model each of these independently in a charac-ter look-up table.
Embedding sizes were chosensuch that each model had roughly the same num-ber of parameters (Table 2).
Training is performedusing mini-batch gradient descent with Nesterov?smomentum.
We use a batch size B = 64, initiallearning rate ?0= 0.01 and momentum parame-ter ?0= 0.9.
L2-regularization with ?
= 0.001was applied to all models.
Initial weights weredrawn from 0-mean gaussians with ?
= 0.1 andinitial biases were set to 0.
The hyperparame-ters were tuned one at a time keeping others fixed,and values with the lowest validation cost werechosen.
The resultant combination was used totrain the models until performance on validationset stopped increasing.
During training, the learn-ing rate is halved everytime the validation set pre-cision increases by less than 0.01 % from oneepoch to the next.
The models converge in about20 epochs.
Code for training both the models ispublicly available on github.4.4 ResultsWe test the character and word-level variants bypredicting hashtags for a held-out test set of posts.Since there may be more than one correct hashtagper post, we generate a ranked list of tags for eachModelPrecision@1Recall@10MeanRankFull test set (50K)word 24.1% 42.8% 133tweet2vec 28.4% 48.5% 104Rare words test set (2K)word 20.4% 37.2% 167tweet2vec 32.9% 51.3% 104Frequent words test set (2K)word 20.9% 41.3% 133tweet2vec 23.9% 44.2% 112Table 3: Hashtag prediction results.
Best numbersfor each test set are in bold.post from the output posteriors, and report aver-age precision@1, recall@10 and mean rank of thecorrect hashtags.
These are listed in Table 3.To see the performance of each model on postscontaining rare words (RW) and frequent words(FW) we selected two test sets each containing2,000 posts.
We populated these sets with postswhich had the maximum and minimum numberof out-of-vocabulary words respectively, wherevocabulary is defined by the 20K most frequentwords.
Overall, tweet2vec outperforms the wordmodel, doing significantly better on RW test setand comparably on FW set.
This improved perfor-mance comes at the cost of increased training time(see Table 2), since moving from words to charac-ters results in longer input sequences to the GRU.We also study the effect of model size on theperformance of these models.
For the word modelwe set vocabulary size V to 8K, 15K and 20K re-spectively.
For tweet2vec we set the GRU hiddenstate size to 300, 400 and 500 respectively.
Fig-ure 2 shows precision 1 of the two models as thenumber of parameters is increased, for each test272(a) Full Test Set (b) Rare Words Test Set (c) Frequent Words Test SetFigure 2: Precision @1 v Number of model parameters for word model and tweet2vec.Dataset # Hashtags word tweet2vecsmall 933 28.0% 33.1%medium 2039 24.1% 28.4%large 5114 20.1% 24.6%Table 4: Precision @1 as training data size andnumber of output labels is increased.
Note that thetest set is different for each setting.set described above.
There is not much variation inthe performance, and moreover tweet2vec alwaysoutperforms the word based model for the samenumber of parameters.Table 4 compares the models as complexity ofthe task is increased.
We created 3 datasets (small,medium and large) with an increasing number ofhashtags to be predicted.
This was done by vary-ing the lower threshold of the minimum numberof tags per post for it to be included in the dataset.Once again we observe that tweet2vec outperformsits word-based counterpart for each of the threesettings.Finally, table 1 shows some predictions fromthe word level model and tweet2vec.
We selectedthese to highlight some strengths of the characterbased approach - it is robust to word segmenta-tion errors and spelling mistakes, effectively inter-prets emojis and other special characters to makepredictions, and also performs comparably to theword-based approach for in-vocabulary tokens.5 ConclusionWe have presented tweet2vec - a character levelencoder for social media posts trained using super-vision from associated hashtags.
Our result showsthat tweet2vec outperforms the word based ap-proach, doing significantly better when the inputpost contains many rare words.
We have focusedonly on English language posts, but the charactermodel requires no language specific preprocessingand can be extended to other languages.
For fu-ture work, one natural extension would be to usea character-level decoder for predicting the hash-tags.
This will allow generation of hashtags notseen in the training dataset.
Also, it will be in-teresting to see how our tweet2vec embeddingscan be used in domains where there is a needfor semantic understanding of social media, suchas tracking infectious diseases (Signorini et al,2011).
Hence, we provide an off-the-shelf en-coder trained on medium dataset described aboveto compute vector-space representations of tweetsalong with our code on github.AcknowledgmentsWe would like to thank Alex Smola, Yun Fu,Hsiao-Yu Fish Tung, Ruslan Salakhutdinov, andBarnabas Poczos for useful discussions.
We wouldalso like to thank Juergen Pfeffer for providing ac-cess to the Twitter data, and the reviewers for theircomments.ReferencesYoshua Bengio, R?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arXiv preprint arXiv:1412.3555.Fr?deric Godin, Viktor Slavkovikj, Wesley De Neve,Benjamin Schrauwen, and Rik Van de Walle.
2013.Using topic models for twitter hashtag recommen-dation.
In Proceedings of the 22nd internationalconference on World Wide Web companion, pages593?596.
International World Wide Web Confer-ences Steering Committee.273Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavl-jevic, Fabrizio Silvestri, and Narayan Bhamidipati.2015.
Context-and content-aware embeddings forquery rewriting in sponsored search.
In Proceed-ings of the 38th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 383?392.
ACM.Sepp Hochreiter and J?rgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Andrej Karpathy, Justin Johnson, and Fei-Fei Li.
2015.Visualizing and understanding recurrent networks.arXiv preprint arXiv:1506.02078.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M Rush.
2015.
Character-aware neural lan-guage models.
arXiv preprint arXiv:1508.06615.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,Richard S Zemel, Antonio Torralba, Raquel Urta-sun, and Sanja Fidler.
2015.
Skip-thought vectors.arXiv preprint arXiv:1506.06726.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
arXivpreprint arXiv:1405.4053.Wang Ling, Tiago Lu?s, Lu?s Marujo, Ram?n Fernan-dez Astudillo, Silvio Amir, Chris Dyer, Alan WBlack, and Isabel Trancoso.
2015.
Finding functionin form: Compositional character models for openvocabulary word representation.
arXiv preprintarXiv:1508.02096.Thang Luong, Richard Socher, and Christopher DManning.
2013.
Better word representationswith recursive neural networks for morphology.
InCoNLL, pages 104?113.
Citeseer.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Li-Qiang Niu and Xin-Yu Dai.
2015.
Topic2vec:Learning distributed representations of topics.
arXivpreprint arXiv:1506.08422.Cicero Nogueira dos Santos and Victor Guimar?es.2015.
Boosting named entity recognition withneural character embeddings.
arXiv preprintarXiv:1505.05008.Cicero D Santos and Bianca Zadrozny.
2014.
Learningcharacter-level representations for part-of-speechtagging.
In Proceedings of the 31st InternationalConference on Machine Learning (ICML-14), pages1818?1826.Alessio Signorini, Alberto Maria Segre, and Philip MPolgreen.
2011.
The use of twitter to track lev-els of disease activity and public concern in the usduring the influenza a h1n1 pandemic.
PloS one,6(5):e19467.Jason Weston, Sumit Chopra, and Keith Adams.
2014.tagspace: Semantic embeddings from hashtags.In Proceedings of the 2014 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1822?1827.Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015.Character-level convolutional networks for text clas-sification.
In Advances in Neural Information Pro-cessing Systems, pages 649?657.274
