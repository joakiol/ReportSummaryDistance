Proceedings of the 12th Conference of the European Chapter of the ACL, pages 345?353,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsEnd-to-End Evaluation in Simultaneous TranslationOlivier Hamon1,2, Christian F?gen3, Djamel Mostefa1, Victoria Arranz1,Muntsin Kolss3, Alex Waibel3,4 and Khalid Choukri11Evaluations and Language Resources Distribution Agency (ELDA), Paris, France2 LIPN (UMR 7030) ?
Universit?
Paris 13 & CNRS, Villetaneuse, France3 Univerit?t Karlsruhe (TH), Germany4 Carnegie Mellon University, Pittsburgh, USA{hamon|mostefa|arranz|choukri}@elda.org,{fuegen|kolss|waibel}@ira.uka.deAbstractThis paper presents the end-to-end evalu-ation of an automatic simultaneous trans-lation system, built with state-of-the-artcomponents.
It shows whether, and forwhich situations, such a system might beadvantageous when compared to a humaninterpreter.
Using speeches in Englishtranslated into Spanish, we present theevaluation procedure and we discuss theresults both for the recognition and trans-lation components as well as for the over-all system.
Even if the translation processremains the Achilles?
heel of the system,the results show that the system can keepat least half of the information, becomingpotentially useful for final users.1 IntroductionAnyone speaking at least two different languagesknows that translation and especially simultaneousinterpretation are very challenging tasks.
A humantranslator has to cope with the special nature ofdifferent languages, comprising phenomena liketerminology, compound words, idioms, dialectterms or neologisms, unexplained acronyms or ab-breviations, proper names, as well as stylistic andpunctuation differences.
Further, translation or in-terpretation are not a word-by-word rendition ofwhat was said or written in a source language.
In-stead, the meaning and intention of a given sen-tence have to be reexpressed in a natural and fluentway in another language.Most professional full-time conference inter-preters work for international organizations likethe United Nations, the European Union, or theAfrican Union, whereas the world?s largest em-ployer of translators and interpreters is currentlythe European Commission.
In 2006, the EuropeanParliament spent about 300 million Euros, 30% ofits budget, on the interpretation and translation ofthe parliament speeches and EU documents.
Gen-erally, about 1.1 billion Euros are spent per yearon the translating and interpreting services withinthe European Union, which is around 1% of thetotal EU-Budget (Volker Steinbiss, 2006).This paper presents the end-to-end evaluationof an automatic simultaneous translation system,built with state-of-the-art components.
It showswhether, and in which cases, such a system mightbe advantageous compared to human interpreters.2 Challenges in Human InterpretationAccording to Al-Khanji et al (2000), researchersin the field of psychology, linguistics and interpre-tation seem to agree that simultaneous interpre-tation (SI) is a highly demanding cognitive taskinvolving a basic psycholinguistic process.
Thisprocess requires the interpreter to monitor, storeand retrieve the input of the source language ina continuous manner in order to produce the oralrendition of this input in the target language.
It isclear that this type of difficult linguistic and cog-nitive operation will force even professional in-terpreters to elaborate lexical or synthetic searchstrategies.Fatigue and stress have a negative effect on theinterpreter, leading to a decrease in simultaneousinterpretation quality.
In a study by Moser-Merceret al (1998), in which professional speakers wereasked to work until they could no longer provideacceptable quality, it was shown that (1) duringthe first 20 minutes the frequency of errors rosesteadily, (2) the interpreters, however, seemed tobe unaware of this decline in quality, (3) after 60minutes, all subjects made a total of 32.5 mean-ing errors, and (4) in the category of nonsense thenumber of errors almost doubled after 30 minuteson the task.Since the audience is only able to evaluate thesimultaneously interpreted discourse by its form,345the fluency of an interpretation is of utmost im-portance.
According to a study by Kopczynski(1994), fluency and style were third on a list ofpriorities (after content and terminology) of el-ements rated by speakers and attendees as con-tributing to quality.
Following the overview in(Yagi, 2000), an interpretation should be as natu-ral and as authentic as possible, which means thatartificial pauses in the middle of a sentence, hes-itations, and false-starts should be avoided, andtempo and intensity of the speaker?s voice shouldbe imitated.Another point to mention is the time span be-tween a source language chunk and its target lan-guage chunk, which is often referred to as ear-voice-span.
Following the summary in (Yagi,2000), the ear-voice-span is variable in durationdepending on some source and target languagevariables, like speech delivery rate, informationdensity, redundancy, word order, syntactic charac-teristics, etc.
Short delays are usually preferred forseveral reasons.
For example, the audience is irri-tated when the delay is too large and is soon askingwhether there is a problem with the interpretation.3 Automatic Simultaneous TranslationGiven the explanations above on human interpre-tation, one has to weigh two factors when consid-ering the use of simultaneous translation systems:translation quality and cost.The major disadvantage of an automatic systemcompared to human interpretation is its translationquality, as we will see in the following sections.Current state-of-the-art systems may reach satis-factory quality for people not understanding thelecturer at all, but are still worse than human inter-pretation.
Nevertheless, an automatic system mayhave considerable advantages.One such advantage is its considerable short-term memory: storing long sequences of words isnot a problem for a computer system.
Therefore,compensatory strategies are not necessary, regard-less of the speaking rate of the speaker.
However,depending on the system?s translation speed, la-tency may increase.
While it is possible for hu-mans to compress the length of an utterance with-out changing its meaning (summarization), it isstill a challenging task for automatic systems.Human simultaneous interpretation is quite ex-pensive, especially due to the fact that usually twointerpreters are necessary.
In addition, human in-terpreters require preparation time to become fa-miliar with the topic.
Moreover, simultaneous in-terpretation requires a soundproof booth with au-dio equipment, which adds an overall cost that isunacceptable for all but the most elaborate multi-lingual events.
On the other hand, a simultaneoustranslation system also needs time and effort forpreparation and adaptation towards the target ap-plication, language and domain.
However, onceadapted, it can be easily re-used in the same do-main, language, etc.
Another advantage is that thetranscript of a speech or lecture is produced forfree by using an automatic system in the sourceand target languages.3.1 The Simultaneous Translation SystemFigure 1 shows a schematic overview of the si-multaneous translation system developed at Uni-versit?t Karlsruhe (TH) (F?gen et al, 2006b).
Thespeech of the lecturer is recorded with the helpof a close-talk microphone and processed by thespeech recognition component (ASR).
The par-tial hypotheses produced by the ASR module arecollected in the resegmentation component, formerging and re-splitting at appropriate ?seman-tic?
boundaries.
The resegmented hypotheses arethen transferred to one or more machine transla-tion components (MT), at least one per languagepair.
Different output technologies may be usedfor presenting the translations to the audience.
Fora detailed description of the components as wellas the client-server framework used for connect-ing the components please refer to (F?gen et al,2006b; F?gen et al, 2006a; Kolss et al, 2006; F?-gen and Kolss, 2007; F?gen et al, 2001).3.2 End-to-End EvaluationThe evaluation in speech-to-speech translationjeopardises many concepts and implies a lot ofsubjectivity.
Three components are involved andan overall system may grow the difficulty of esti-mating the output quality.
However, two criteriaare mainly accepted in the community: measuringthe information preservation and determining howmuch of the translation is understandable.Several end-to-end evaluations in speech-to-speech translation have been carried out in the lastfew years, in projects such as JANUS (Gates etal., 1996), Verbmobil (N?bel, 1997) or TC-STAR(Hamon et al, 2007).
Those projects use themain criteria depicted above, and protocols differin terms of data preparation, rating, procedure, etc.346DictionarySourceHypothesis TranslatableSegmentModelSource BoundaryResegmen?tationRecognitionSpeechTranslationModel ModelTarget LanguageMachineTranslationModelSource AcousticModelSource LanguageOutputTranslatedTranslationVocabularyAudio StreamTextOutput(Subtitles)(Synthesis)SpokenFigure 1: Schematic overview and information flow of the simultaneous translation system.
The maincomponents of the system are represented by cornered boxes and the models used for theses componentsby ellipses.
The different output forms are represented by rounded boxes.To our opinion, to evaluate the performance of acomplete speech-to-speech translation system, weneed to compare the source speech used as input tothe translated output speech in the target language.To that aim, we reused a large part of the evalua-tion protocol from the TC-STAR project(Hamonet al, 2007).4 Evaluation TasksThe evaluation is carried out on the simultaneouslytranslated speech of a single speaker?s talks andlectures in the field of speech processing, given inEnglish, and translated into Spanish.4.1 Data usedTwo data sets were selected from the talks andlectures.
Each set contained three excerpts, nolonger than 6 minutes each and focusing on dif-ferent topics.
The former set deals with speechrecognition and the latter with the descriptions ofEuropean speech research projects, both from thesame speaker.
This represents around 7,200 En-glish words.
The excerpts were manually tran-scribed to produce the reference for the ASR eval-uation.
Then, these transcriptions were manuallytranslated into Spanish by two different transla-tors.
Two reference translations were thus avail-able for the spoken language translation (SLT)evaluation.
Finally, one human interpretation wasproduced from the excerpts as reference for theend-to-end evaluation.
It should be noted that forthe translation system, speech synthesis was usedto produce the spoken output.4.2 Evaluation ProtocolThe system is evaluated as a whole (black boxevaluation) and component by component (glassbox evaluation):ASR evaluation.
The ASR module is evaluatedby computing the Word Error Rate (WER) in caseinsensitive mode.SLT evaluation.
For the SLT evaluation, the au-tomatically translated text from the ASR output iscompared with two manual reference translationsby means of automatic and human metrics.
Twoautomatic metrics are used: BLEU (Papineni etal., 2001) and mWER (Niessen et al, 2000).For the human evaluation, each segment is eval-uated in relation to adequacy and fluency (Whiteand O?Connell, 1994).
For the evaluation of ad-equacy, the target segment is compared to a ref-erence segment.
For the evaluation of fluency,the quality of the language is evaluated.
The twotypes of evaluation are done independently, buteach evaluator did both evaluations (first that offluency, then that of adequacy) for a certain num-ber of segments.
For the evaluation of fluency,evaluators had to answer the question: ?Is the textwritten in good Spanish??.
For the evaluation ofadequacy, evaluators had to answer the question:?How much of the meaning expressed in the ref-erence translation is also expressed in the targettranslation?
?.For both evaluations, a five-point scale is pro-posed to the evaluators, where only extreme val-ues are explicitly defined.
Three evaluations arecarried out per segment, done by three differentevaluators, and segments are divided randomly,because evaluators must not recreate a ?story?347and thus be influenced by the context.
The totalnumber of judges was 10, with around 100 seg-ments per judge.
Furthermore, the same numberof judges was recruited for both categories: ex-perts, from the domain with a knowledge of thetechnology, and non-experts, without that knowl-edge.End-to-End evaluation.
The End-to-End eval-uation consists in comparing the speech in thesource language to the output speech in the tar-get language.
Two important aspects should betaken into account when assessing the quality ofa speech-to-speech system.First, the information preservation is measuredby using ?comprehension questionnaires?.
Ques-tions are created from the source texts (the En-glish excerpts), then questions and answers aretranslated into Spanish by professional translators.These questions are asked to human judges afterthey have listened to the output speech in the tar-get language (Spanish).
At a second stage, the an-swers are analysed: for each answer a Spanish val-idator gives a score according to a binary scale (theinformation is either correct or incorrect).
This al-lows us to measure the information preservation.Three types of questions are used in order to di-versify the difficulty of the questions and test thesystem at different levels: simple Factual (70%),yes/no (20%) and list (10%) questions.
For in-stance, questions were: What is the larynx respon-sible for?, Have all sites participating in CHILbuilt a CHIL room?, Which types of knowledgesources are used by the decoder?, respectively.The second important aspect of a speech-to-speech system is the quality of the speech output(hereafter quality evaluation).
For assessing thequality of the speech output one question is askedto the judges at the end of each comprehensionquestionnaire: ?Rate the overall quality of this au-dio sample?, and values go from 1 (?1: Very bad,unusable?)
to 5 (?It is very useful?).
Both auto-matic system and interpreter outputs were evalu-ated with the same methodology.Human judges are real users and native Span-ish speakers, experts and non-experts, but differentfrom those of the SLT evaluation.
Twenty judgeswere involved (12 excerpts, 10 evaluations per ex-cerpt and 6 evaluations per judge) and each judgeevaluated both automatic and human excerpts on a50/50 percent basis.5 Components Results5.1 Automatic Speech RecognitionThe ASR output has been evaluated using themanual transcriptions of the excerpts.
The overallWord Error Rate (WER) is 11.9%.
Table 1 showsthe WER level for each excerpt.Excerpts WER [%]L043-1 14.5L043-2 14.5L043-3 9.6T036-1 11.3T036-2 11.7T036-3 9.2Overall 11.9Table 1: Evaluation results for ASR.T036 excerpts seem to be easier to recognize au-tomatically than L043 ones, probably due to themore general language of the former.5.2 Machine Translation5.2.1 Human EvaluationEach segment within the human evaluation is eval-uated 4 times, each by a different judge.
This aimsat having a significant number of judgments andmeasuring the consistency of the human evalua-tions.
The consistency is measured by computingthe Cohen?s Kappa coefficient (Cohen, 1960).Results show a substantial agreement for flu-ency (kappa of 0.64) and a moderate agreementfor adequacy (0.52).The overall results of the hu-man evaluation are presented in Table 2.
Regard-ing both experts?
and non-experts?
details, agree-ment is very similar (0.30 and 0.28, respectively).All judges Experts Non expertsFluency 3.13 2.84 3.42Adequacy 3.26 3.21 3.31Table 2: Average rating of human evalua-tions [1<5].Both fluency and adequacy results are over themean.
They are lower for experts than for non-experts.
This may be due to the fact that expertsare more familiar with the domain and thereforemore demanding than non experts.
Regarding thedetailed evaluation per judge, scores are generallylower for non-experts than for experts.3485.2.2 Automatic EvaluationScores are computed using case-sensitive metrics.Table 3 shows the detailed results per excerpt.Excerpts BLEU [%] mWER [%]L043-1 25.62 58.46L043-2 22.60 62.47L043-3 28.73 62.64T036-1 34.46 55.13T036-2 29.41 59.91T036-3 35.17 50.77Overall 28.94 58.66Table 3: Automatic Evaluation results for SLT.Scores are rather low, with a mWER of 58.66%,meaning that more than half of the translation iscorrect.
According to the scoring, the T036 ex-cerpts seem to be easier to translate than the L043ones, the latter being of a more technical nature.6 End-to-End Results6.1 Evaluators AgreementIn this study, ten judges carried out the evaluationfor each excerpt.
In order to observe the inter-judges agreement, the global Fleiss?s Kappa co-efficient was computed, which allows to measurethe agreement between m judges with r criteria ofjudgment.
This coefficient shows a global agree-ment between all the judges, which goes beyondCohen?s Kappa coefficient.
However, a low co-efficient requires a more detailed analysis, for in-stance, by using Kappa for each pair of judges.Indeed, this allows to see how deviant judges arefrom the typical judge behaviour.
For m judges,n evaluations and r criteria, the global Kappa isdefined as follows:?
= 1 ?nm2 ?
?ni=1?rj=1 X2ijnm(m?
1) ?rj=1 Pj(1 ?
Pj)where:Pj =?ni=1 Xijnmand: Xij is the number of judgments for the ithevaluation and the jth criteria.Regarding quality evaluation (n = 6, m = 10,r = 5), Kappa values are low for both human in-terpreters (?
= 0.07) and the automatic system(?
= 0.01), meaning that judges agree poorly(Landis and Koch, 1977).
This is explained bythe extreme subjectivity of the evaluation and thesmall number of evaluated excerpts.
Looking ateach pair of judges and the Kappa coefficientsthemselves, there is no real agreement, since mostof the Kappa values are around zero.
However,some judge pairs show fair agreement, and someothers show moderate or substantial agreement.
Itis observed, though, that some criteria are not fre-quently selected by the judges, which limits thestatistical significance of the Kappa coefficient.The limitations are not the same for the com-prehension evaluation (n = 60, m = 10, r = 2),since the criteria are binary (i.e.
true or false).
Re-garding the evaluated excerpts, Kappa values are0.28 for the automatic system and 0.30 for the in-terpreter.
According to Landis and Koch (1977),those values mean that judges agree fairly.
Inorder to go further, the Kappa coefficients werecomputed for each pair of judges.
Results wereslightly better for the interpreter than for the au-tomatic system.
Most of them were between 0.20and 0.40, implying a fair agreement.
Some judgesagreed moderately.Furthermore, it was also observed that for the120 available questions, 20 had been answeredcorrectly by all the judges (16 for the interpreterevaluation and 4 for the automatic system one)and 6 had been answered wrongly by all judges (1for the former and 5 for the latter).
That shows atrend where the interpreter comprehension wouldbe easier than that of the automatic system, or atleast where the judgements are less questionable.6.2 Quality EvaluationTable 4 compares the quality evaluation results ofthe interpreter to those of the automatic system.Samples Interpreter Automatic systemL043-1 3.1 1.6L043-2 2.9 2.3L043-3 2.4 2.1T036-1 3.6 3.1T036-2 2.7 2.5T036-3 3.5 2.5Mean 3.03 2.35Table 4: Quality evaluation results for the inter-preter and the automatic system [1<5].As can be seen, with a mean score of 3.03 evenfor the interpreter, the excerpts were difficult tointerpret and translate.
This is particularly so for349L043, which is more technical than T036.
TheL043-3 excerpt is particularly technical, with for-mulae and algorithm descriptions, and even a com-plex description of the human articulatory system.In fact, L043 provides a typical presentation withan introduction, followed by a deeper descriptionof the topic.
This increasing complexity is re-flected on the quality scores of the three excerpts,going from 3.1 to 2.4.T036 is more fluent due to the less technical na-ture of the speech and the more general vocabu-lary used.
However, the T036-2 and T036-3 ex-cerpts get a lower quality score, due to the descrip-tion of data collections or institutions, and thus theuse of named entities.
The interpreter does notseem to be at ease with them and is mispronounc-ing some of them, such as ?Grenoble?
pronouncedlike in English instead of in Spanish.
The inter-preter seems to be influenced by the speaker, ascan also be seen in his use of the neologism ?el ce-nario?
(?the scenario?)
instead of ?el escenario?.Likewise, ?Karlsruhe?
is pronounced three timesdifferently, showing some inconsistency of the in-terpreter.The general trend in quality errors is similar tothose of previous evaluations: lengthening words(?seeee?ales?
), hesitations, pauses between syl-lables and catching breath (?caracter?s...ticas?
),careless mistakes (?probibilidad?
instead of ?prob-abilidad?
), self-correction of wrong interpreting(?reconocien-/reconocimiento?
), etc.An important issue concerns gender and num-ber agreement.
Those errors are explained bythe presence of morphological gender in Spanish,like in ?estos se?ales?
instead of ?estas se?ales?
(?these signals?)
together with the speaker?s speedof speech.
The speaker seems to start by defaultwith a masculine determiner (which has no gen-der in English), adjusting the gender afterward de-pending on the noun following.
A quick transla-tion may also be the cause for this kind of errors,like ?del se?al acustico?
(?of the acoustic signal?
)with a masculine determiner, a feminine substan-tive and ending in a masculine adjective.
Sometranslation errors are also present, for instance?computerizar?
instead of ?calcular?
(?compute?
).The errors made by the interpreter help to un-derstand how difficult oral translation is.
Thisshould be taken into account for the evaluation ofthe automatic system.The automatic system results, like those ofthe interpreter, are higher for T036 than for L043.However, scores are lower, especially for theL043-1 excerpt.
This seems to be due to thetype of lexicon used by the speaker for this ex-cerpt, more medical, since the speaker describesthe articulatory system.
Moreover, his descriptionis sometimes metaphorical and uses a rather col-loquial register.
Therefore, while the interpreterfinds it easier to deal with these excerpts (knownvocabulary among others) and L043-3 seems to bemore complicated (domain-specific, technical as-pect), the automatic system finds it more compli-cated with the former and less with the latter.
Inother words, the interpreter has to ?understand?what is said in L043-3, contrary to the automaticsystem, in order to translate.Scores are higher for the T036 excerpts.
In-deed, there is a high lexical repetition, a largenumber of named entities, and the quality of theexcerpt is very training-dependant.
However, thesystem runs into trouble to process foreign names,which are very often not understandable.
Differ-ences between T036-1 and the other T036 excerptsare mainly due to the change in topic.
While theformer deals with a general vocabulary (i.e.
de-scription of projects), the other two excerpts de-scribe the data collection, the evaluation metrics,etc., thus increasing the complexity of translation.Generally speaking, quality scores of the au-tomatic system are mainly due to the transla-tion component, and to a lesser extent to therecognition component.
Many English words arenot translated (?bush?, ?keyboards?, ?squeaking?,etc.
), and word ordering is not always correct.This is the case for the sentence ?how we solveit?, translated into ?c?mo nos resolvers lo?
insteadof ?c?mo lo resolvemos?.
Funnily enough, theproblems of gender (?maravillosos aplicaciones?- masc.
vs fem.)
and number (?pueden real-mente ser aplicado?
- plu.
vs sing.)
the in-terpreter has, are also found for the automaticsystem.
Moreover, the translation of compoundnouns often shows wrong word ordering, in partic-ular when they are long, i.e.
up to three words (e.g.
?reconocimiento de habla sistemas?
for ?speechrecognition system?
instead of ?sistemas de re-conocimiento de habla?
).Finally, some error combinations result in fullynon-understandable sentences, such as:?usted tramo se en emacs es squeakingruido y dries todos demencial?350where the following errors take place:?
tramo: this translation of ?stretch?
resultsfrom the choice of a substantive instead of averb, giving rise to two choices due to the lex-ical ambiguity: ?estiramiento?
and ?tramo?,which is more a linear distance than a stretchin that context;?
se: the pronoun ?it?
becomes the reflexive?se?
instead of the personal pronoun ?lo?;?
emacs: the recognition module transcribedthe couple of words ?it makes?
into ?emacs?,not translated by the translation module;?
squeaking: the word is not translated by thetranslation module;?
dries: again, two successive errors are made:the word ?drives?
is transcribed into ?dries?by the recognition module, which is then leftuntranslated.The TTS component also contributes to decreas-ing the output quality.
The prosody module finds ithard to make the sentences sound natural.
Pausesbetween words are not very frequent, but they donot sound natural (i.e.
like catching breath) andthey are not placed at specific points, as it wouldbe done by a human.
For instance, the prosodymodule does not link the noun and its determiner(e.g.
?otros aplicaciones?).
Finally, a not user-friendly aspect of the TTS component is the rep-etition of the same words always pronounced inthe same manner, what is quite disturbing for thelistener.6.3 Comprehension EvaluationTables 5 and 6 present the results of the compre-hension evaluation, for the interpreter and for theautomatic system, respectively.
They provide thefollowing information:identifiers of the excerpt: Source data are thesame for the interpreter and the automaticsystem, namely the English speech;subj.
E2E: The subjective results of the end-to-end evaluation are done by the same assessorswho did the quality evaluation.
This showsthe percentage of good answers;fair E2E: The objective verification of the an-swers.
The audio files are validated to checkwhether they contain the answers to the ques-tions or not (as the questions were createdfrom the English source).
This shows themaximum percentage of answers an evalua-tor managed to find from either the interpreter(speaker audio) or the automatic system out-put (TTS) in Spanish.
For instance, informa-tion in English could have been missed bythe interpreter because he/she felt that this in-formation was meaningless and could be dis-carded.
We consider those results as an ob-jective evaluation.SLT, ASR: Verification of the answers in eachcomponent of the end-to-end process.
In or-der to determine where the information forthe automatic system is lost, files from eachcomponent (recognised files for ASR, trans-lated files for SLT, and synthesised files forTTS in the ?fair E2E?
column) are checked.Excerpts subj.
E2E fair E2EL043-1 69 90L043-2 75 80L043-3 72 60T036-1 80 100T036-2 73 80T036-3 76 100Mean 74 85Table 5: Comprehension evaluation results for theinterpreter [%].Regarding Table 5, the interpreter loses 15%of the information (i.e.
15% of the answers wereincorrect or not present in the interpreter?s trans-lation) and judges correctly answered 74% of thequestions.
Five documents get above 80% of cor-rect results, while judges find almost above 70%of the answers for the six documents.Regarding the automatic system results (Table6), the information rate found by judges is justabove 50% since, by extension, more than half thequestions were correctly answered.
The lowestexcerpt, L043-1, gets a rate of 25%, the highest,T036-1, a rate of 76%, which is in agreement withthe observation for the quality evaluation.
Infor-mation loss can be found in each component, es-pecially for the SLT module (35% of the informa-tion is lost here).
It should be noticed that the TTSmodule made also errors which prevented judges351Excerpts subj.
E2E fair E2E SLT ASRL043-1 25 30 30 70L043-2 62 70 80 70L043-3 43 40 60 100T036-1 76 80 90 100T036-2 61 70 60 80T036-3 47 60 70 80Mean 52 58 65 83Table 6: Comprehension evaluation results for theautomatic system [%].from answering related questions.
Moreover, theASR module loses 17% of the information.
Thoseresults are certainly due to the specific vocabularyused in this experiment.So as to objectively compare the interpreter withthe automatic system, we selected the questionsfor which the answers were included in the inter-preter files (i.e.
those in the ?fair E2E?
columnof Table 5).
The goal was to compare the overallquality of the speech-to-speech translation to in-terpreters?
quality, without the noise factor of theinformation missing.
The assumption is that theinterpreter translates the ?important information?and skips the useless parts of the original speech.This experiment is to measure the level of this in-formation that is preserved by the automatic sys-tem.
So a new subset of results was obtained, onthe information kept by the interpreter.
The samestudy was repeated for the three components andthe results are shown in Tables 7 and 8.Excerpts subj.
E2E fair E2E SLT ASRL043-1 27 33 33 78L043-2 65 75 88 75L043-3 37 67 83 100T036-1 76 80 90 100T036-2 69 88 75 100T036-3 47 60 70 80Mean 53 60 70 80Table 7: Evaluation results for the automatic sys-tem restricted to the questions for which answerscan be found in the interpreter speech [%].Comparing the automatic system to the inter-preter, the automatic system keeps 40% of the in-formation where the interpreter translates the doc-uments correctly.
Those results confirm that ASRloses a lot of information (20%), while SLT loses10% further, and so does the TTS.
Judges are quiteclose to the objective validation and found most ofthe answers they could possibly do.Excerpts subj.
E2EL043-1 66L043-2 90L043-3 88T036-1 80T036-2 81T036-3 76Mean 80Table 8: Evaluation results for interpreter, re-stricted to the questions for which answers can befound in the interpreter speech [%].Subjective results for the restricted evaluationare similar to the previous results, on the full data(80% vs 74% of the information found by thejudges).
Performance is good for the interpreter:98% of the information correctly translated by theautomatic system is also correctly interpreted bythe human.
Although we can not compare theperformance of the restricted automatic system tothat of the restricted interpreter (since data sets ofquestions are different), it seems that of the inter-preter is better.
However, the loss due to subjectiveevaluation seems to be higher for the interpreterthan for the automatic system.7 ConclusionsRegarding the SLT evaluation, the results achievedwith the simultaneous translation system are stillrather low compared to the results achieved withoffline systems for translating European parlia-ment speeches in TC-STAR.
However, the offlinesystems had almost no latency constraints, andparliament speeches are much easier to recognizeand translate when compared to the more spon-taneous talks and lectures focused in this paper.This clearly shows the difficulty of the whole task.However, the human end-to-end evaluation of thesystem in which the system is compared with hu-man interpretation shows that the current transla-tion quality allows for understanding of at leasthalf of the content, and therefore, may be alreadyquite helpful for people not understanding the lan-guage of the lecturer at all.352ReferencesRajai Al-Khanji, Said El-Shiyab, and Riyadh Hussein.2000.
On the Use of Compensatory Strategies in Si-multaneous Interpretation.
Meta : Journal des tra-ducteurs, 45(3):544?557.Jacob Cohen.
1960.
A coefficient of agreement fornominal scales.
In Educational and PsychologicalMeasurement, volume 20, pages 37?46.Christian F?gen and Muntsin Kolss.
2007.
The influ-ence of utterance chunking on machine translationperformance.
In Proc.
of the European Conferenceon Speech Communication and Technology (INTER-SPEECH), Antwerp, Belgium, August.
ISCA.Christian F?gen, Martin Westphal, Mike Schneider,Tanja Schultz, and Alex Waibel.
2001.
LingWear:A Mobile Tourist Information System.
In Proc.
ofthe Human Language Technology Conf.
(HLT), SanDiego, California, March.
NIST.Christian F?gen, Shajith Ikbal, Florian Kraft, KenichiKumatani, Kornel Laskowski, John W. McDonough,Mari Ostendorf, Sebastian St?ker, and MatthiasW?lfel.
2006a.
The isl rt-06s speech-to-text system.In Steve Renals, Samy Bengio, and Jonathan Fiskus,editors, Machine Learning for Multimodal Interac-tion: Third International Workshop, MLMI 2006,Bethesda, MD, USA, volume 4299 of Lecture Notesin Computer Science, pages 407?418.
Springer Ver-lag Berlin/ Heidelberg.Christian F?gen, Muntsin Kolss, Matthias Paulik, andAlex Waibel.
2006b.
Open Domain Speech Trans-lation: From Seminars and Speeches to Lectures.In TC-Star Speech to Speech Translation Workshop,Barcelona, Spain, June.Donna Gates, Alon Lavie, Lori Levin, Alex.
Waibel,Marsal Gavalda, Laura Mayfield, and Monika Wosz-cyna.
1996.
End-to-end evaluation in janus: Aspeech-to-speech translation system.
In Proceed-ings of the 6th ECAI, Budapest.Olivier Hamon, Djamel Mostefa, and Khalid Choukri.2007.
End-to-end evaluation of a speech-to-speechtranslation system in tc-star.
In Proceedings of theMT Summit XI, Copenhagen, Denmark, September.Muntsin Kolss, Bing Zhao, Stephan Vogel, AshishVenugopal, and Ying Zhang.
2006.
The ISL Statis-tical Machine Translation System for the TC-STARSpring 2006 Evaluations.
In TC-Star Workshopon Speech-to-Speech Translation, Barcelona, Spain,December.Andrzej Kopczynski, 1994.
Bridging the Gap: Empiri-cal Research in Simultaneous Interpretation, chapterQuality in Conference Interpreting: Some PragmaticProblems, pages 87?100.
John Benjamins, Amster-dam/ Philadelphia.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.In Biometrics, Vol.
33, No.
1 (Mar., 1977), pp.
159-174.Barbara Moser-Mercer, Alexander Kunzli, and Ma-rina Korac.
1998.
Prolonged turns in interpreting:Effects on quality, physiological and psychologicalstress (pilot study).
Interpreting: International jour-nal of research and practice in interpreting, 3(1):47?64.Sonja Niessen, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
An evaluation tool for ma-chine translation: Fast evaluation for mt research.In Proceedings of the 2nd International Conferenceon Language Resources and Evaluation, Athens,Greece.Rita N?bel.
1997.
End-to-end Evaluation in Verb-mobil I.
In Proceedings of the MT Summit VI, SanDiego.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
Technical ReportRC22176 (W0109-022), Research Report, Com-puter Science IBM Research Division, T.J.WatsonResearch Center.Accipio Consulting Volker Steinbiss.
2006.Sprachtechnologien f?r Europa.
www.tc-star.org/pubblicazioni/D17_HLT_DE.pdf.John S. White and Theresa A. O?Connell.
1994.Evaluation in the arpa machine translation program:1993 methodology.
In HLT ?94: Proceedings of theworkshop on Human Language Technology, pages135?140, Morristown, NJ, USA.
Association forComputational Linguistics.Sane M. Yagi.
2000.
Studying Style in Simultane-ous Interpretation.
Meta : Journal des traducteurs,45(3):520?547.353
