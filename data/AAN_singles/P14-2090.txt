Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551?556,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsOptimizing Segmentation Strategies for Simultaneous Speech TranslationYusuke Oda Graham Neubig Sakriani Sakti Tomoki Toda Satoshi NakamuraGraduate School of Information ScienceNara Institute of Science and TechnologyTakayama, Ikoma, Nara 630-0192, Japan{oda.yusuke.on9, neubig, ssakti, tomoki, s-nakamura}@is.naist.jpAbstractIn this paper, we propose new algorithmsfor learning segmentation strategies for si-multaneous speech translation.
In contrastto previously proposed heuristic methods,our method finds a segmentation that di-rectly maximizes the performance of themachine translation system.
We describetwo methods based on greedy search anddynamic programming that search for theoptimal segmentation strategy.
An experi-mental evaluation finds that our algorithmis able to segment the input two to threetimes more frequently than conventionalmethods in terms of number of words,while maintaining the same score of auto-matic evaluation.11 IntroductionThe performance of speech translation systemshas greatly improved in the past several years,and these systems are starting to find wide use ina number of applications.
Simultaneous speechtranslation, which translates speech from thesource language into the target language in realtime, is one example of such an application.
Whentranslating dialogue, the length of each utterancewill usually be short, so the system can simplystart the translation process when it detects the endof an utterance.
However, in the case of lectures,for example, there is often no obvious boundarybetween utterances.
Thus, translation systems re-quire a method of deciding the timing at whichto start the translation process.
Using estimatedends of sentences as the timing with which to starttranslation, in the same way as a normal text trans-lation, is a straightforward solution to this problem(Matusov et al, 2006).
However, this approach1The implementation is available athttp://odaemon.com/docs/codes/greedyseg.html.impairs the simultaneity of translation because thesystem needs to wait too long until the appearanceof a estimated sentence boundary.
For this reason,segmentation strategies, which separate the inputat appropriate positions other than end of the sen-tence, have been studied.A number of segmentation strategies for simul-taneous speech translation have been proposed inrecent years.
F?ugen et al (2007) and Bangalore etal.
(2012) propose using prosodic pauses in speechrecognition to denote segmentation boundaries,but this method strongly depends on characteris-tics of the speech, such as the speed of speaking.There is also research on methods that depend onlinguistic or non-linguistic heuristics over recog-nized text (Rangarajan Sridhar et al, 2013), and itwas found that a method that predicts the locationof commas or periods achieves the highest perfor-mance.
Methods have also been proposed usingthe phrase table (Yarmohammadi et al, 2013) orthe right probability (RP) of phrases (Fujita et al,2013), which indicates whether a phrase reorder-ing occurs or not.However, each of the previously mentionedmethods decides the segmentation on the basisof heuristics, so the impact of each segmenta-tion strategy on translation performance is not di-rectly considered.
In addition, the mean numberof words in the translation unit, which strongly af-fects the delay of translation, cannot be directlycontrolled by these methods.2In this paper, we propose new segmentation al-gorithms that directly optimize translation perfor-mance given the mean number of words in thetranslation unit.
Our approaches find appropri-ate segmentation boundaries incrementally usinggreedy search and dynamic programming.
Eachboundary is selected to explicitly maximize trans-2The method using RP can decide relative frequency ofsegmentation by changing a parameter, but guessing thelength of a translation unit from this parameter is not trivial.551lation accuracy as measured by BLEU or anotherevaluation measure.We evaluate our methods on a speech transla-tion task, and we confirm that our approaches canachieve translation units two to three times as fine-grained as other methods, while maintaining thesame accuracy.2 Optimization FrameworkOur methods use the outputs of an existing ma-chine translation system to learn a segmentationstrategy.
We define F = {fj: 1 ?
j ?
N},E = {ej: 1 ?
j ?
N} as a parallel corpusof source and target language sentences used totrain the segmentation strategy.
N represents thenumber of sentences in the corpus.
In this work,we consider sub-sentential segmentation, wherethe input is already separated into sentences, andwe want to further segment these sentences intoshorter units.
In an actual speech translation sys-tem, these sentence boundaries can be estimatedautomatically using a method like the period es-timation mentioned in Rangarajan Sridhar et al(2013).
We also assume the machine translationsystem is defined by a function MT (f) that takesa string of source words f as an argument and re-turns the translation result?e.3We will introduce individual methods in the fol-lowing sections, but all follow the general frame-work shown below:1.
Decide the mean number of words ?
and themachine translation evaluation measure EVas parameters of algorithm.
We can use anautomatic evaluation measure such as BLEU(Papineni et al, 2002) as EV .
Then, we cal-culate the number of sub-sentential segmen-tation boundaries K that we will need to in-sert into F to achieve an average segmentlength ?
:K := max(0,?
?f?F |f |???N).
(1)2.
Define S as a set of positions in F in whichwe will insert segmentation boundaries.
Forexample, if we will segment the first sentenceafter the third word and the third sentence af-ter the fifth word, then S = {?1, 3?
, ?3, 5?
}.3In this work, we do not use the history of the languagemodel mentioned in Bangalore et al (2012).
Considering thisinformation improves the MT performance and we plan toinclude this in our approach in future work.Figure 1: Concatenated translation MT (f ,S).Based on this representation, choose K seg-mentation boundaries in F to make the setS?that maximizes an evaluation function ?as below:S?
:= arg maxS?{S?:|S?|=K}?
(S;F , E , EV,MT ).
(2)In this work, we define ?
as the sum of theevaluation measure for each parallel sentencepair ?fj,ej?:?
(S) :=N?j=1EV (MT (fj,S), ej), (3)where MT (f ,S) represents the concatena-tion of all partial translations {MT (f(n))}given the segments S as shown in Figure 1.Equation (3) indicates that we assume allparallel sentences to be independent of eachother, and the evaluation measure is calcu-lated for each sentence separately.
This lo-cality assumption eases efficient implementa-tion of our algorithm, and can be realized us-ing a sentence-level evaluation measure suchas BLEU+1 (Lin and Och, 2004).3.
Make a segmentation model MS?by treatingthe obtained segmentation boundaries S?aspositive labels, all other positions as negativelabels, and training a classifier to distinguishbetween them.
This classifier is used to de-tect segmentation boundaries at test time.Steps 1. and 3. of the above procedure are triv-ial.
In contrast, choosing a good segmentation ac-cording to Equation (2) is difficult and the focusof the rest of this paper.
In order to exactly solveEquation (2), we must perform brute-force searchover all possible segmentations unless we makesome assumptions about the relation between the?
yielded by different segmentations.
However,the number of possible segmentations is exponen-tially large, so brute-force search is obviously in-tractable.
In the following sections, we propose 2552I ate lunch but she leftSegments already selected at the k-th iteration?
= 0.5 ?
= 0.8(k+1)-th segment?
= 0.7Figure 2: Example of greedy search.Algorithm 1 Greedy segmentation searchS??
?for k = 1 to K doS??
S??
{arg maxs?S??(S??
{s})}end forreturn S?methods that approximately search for a solutionto Equation (2).2.1 Greedy SearchOur first approximation is a greedy algorithm thatselects segmentation boundaries one-by-one.
Inthis method, k already-selected boundaries are leftunchanged when deciding the (k+1)-th boundary.We find the unselected boundary that maximizes ?and add it to S:Sk+1= Sk?
{arg maxs?Sk?(Sk?
{s})}.
(4)Figure 2 shows an example of this process for asingle sentence, and Algorithm 1 shows the algo-rithm for calculating K boundaries.2.2 Greedy Search with Feature Groupingand Dynamic ProgrammingThe method described in the previous sectionfinds segments that achieve high translation per-formance for the training data.
However, becausethe translation system MT and evaluation mea-sureEV are both complex, the evaluation function?
includes a certain amount of noise.
As a result,the greedy algorithm that uses only ?
may find asegmentation that achieves high translation perfor-mance in the training data by chance.
However,these segmentations will not generalize, reducingthe performance for other data.We can assume that this problem can be solvedby selecting more consistent segmentations of thetraining data.
To achieve this, we introduce a con-straint that all positions that have similar charac-teristics must be selected at the same time.
Specif-ically, we first group all positions in the sourceI ate lunch but she leftPRP VBD NN CC PRP VBDI ate an apple and an orangePRP VBD DT NN CC DT NNWORD:POS:WORD:POS:GroupPRP+VBDGroupNN+CCGroupDT+NNFigure 3: Grouping segments by POS bigrams.sentences using features of the position, and intro-duce a constraint that all positions with identicalfeatures must be selected at the same time.
Figure3 shows an example of how this grouping workswhen we use the POS bigram surrounding eachpotential boundary as our feature set.By introducing this constraint, we can expectthat features which have good performance over-all will be selected, while features that have rela-tively bad performance will not be selected even ifgood performance is obtained when segmenting ata specific location.
In addition, because all posi-tions can be classified as either segmented or notby evaluating whether the corresponding feature isin the learned feature set or not, it is not necessaryto train an additional classifier for the segmenta-tion model when using this algorithm.
In otherwords, this constraint conducts a kind of featureselection for greedy search.In contrast to Algorithm 1, which only selectedone segmentation boundary at once, in our newsetting there are multiple positions selected at onetime.
Thus, we need to update our search algo-rithm to handle this setting.
To do so, we usedynamic programming (DP) together with greedysearch.
Algorithm 2 shows ourGreedy+DP searchalgorithm.
Here, c(?
;F) represents the numberof appearances of ?
in the set of source sentencesF , and S(F ,?)
represents the set of segments de-fined by both F and the set of features ?.The outer loop of the algorithm, like Greedy,iterates over all S of size 1 to K. The inner loopexamines all features that appear exactly j timesin F , and measures the effect of adding them tothe best segmentation with (k ?
j) boundaries.2.3 Regularization by Feature CountEven after we apply grouping by features, itis likely that noise will still remain in the lessfrequently-seen features.
To avoid this problem,we introduce regularization into the Greedy+DPalgorithm, with the evaluation function ?
rewrit-553Algorithm 2 Greedy+DP segmentation search?0?
?for k = 1 to K dofor j = 0 to k ?
1 do???
{?
: c(?
;F) = k ?
j ?
??
?j}?k,j?
?j?
{arg max?????
(S(F ,?j?
{?
}))}end for?k?
arg max??{?k,j:0?j<k}?
(S(F ,?
))end forreturn S(F ,?K)ten as below:??(?)
:= ?
(S(F ,?))?
?|?|.
(5)The coefficient ?
is the strength of the regulariza-tion with regards to the number of selected fea-tures.
A larger ?
will result in a larger penaltyagainst adding new features into the model.
Asa result, the Greedy+DP algorithm will value fre-quently appearing features.
Note that the methoddescribed in the previous section is equal to thecase of ?
= 0 in this section.2.4 Implementation DetailsOur Greedy and Greedy+DP search algorithmsare completely described in Algorithms 1 and 2.However, these algorithms require a large amountof computation and simple implementations ofthem are too slow to finish in realistic time.
Be-cause the heaviest parts of the algorithm are thecalculation of MT and EV , we can greatly im-prove efficiency by memoizing the results of thesefunctions, only recalculating on new input.3 Experiments3.1 Experimental SettingsWe evaluated the performance of our segmentationstrategies by applying them to English-Germanand English-Japanese TED speech translation datafrom WIT3 (Cettolo et al, 2012).
For English-German, we used the TED data and splits fromthe IWSLT2013 evaluation campaign (Cettolo etal., 2013), as well as 1M sentences selected fromthe out-of-domain training data using the methodof Duh et al (2013).
For English-Japanese, weused TED data and the dictionary entries and sen-tences from EIJIRO.4Table 1 shows summaries ofthe datasets we used.4http://eowp.alc.co.jp/info2/f -e Type#wordsf eEn-DeTrain MT 21.8M 20.3MTrain Seg.
424k 390kTest 27.6k 25.4kEn-JaTrain MT 13.7M 19.7MTrain Seg.
401k 550kTest 8.20k 11.9kTable 1: Size of MT training, segmentation train-ing and testing datasets.We use the Stanford POS Tagger (Toutanovaet al, 2003) to tokenize and POS tag Englishand German sentences, and KyTea (Neubig et al,2011) to tokenize Japanese sentences.
A phrase-based machine translation (PBMT) system learnedby Moses (Koehn et al, 2007) is used as the trans-lation system MT .
We use BLEU+1 as the eval-uation measure EV in the proposed method.
Theresults on the test data are evaluated by BLEU andRIBES (Isozaki et al, 2010), which is an evalu-ation measure more sensitive to global reorderingthan BLEU.We evaluated our algorithm and two conven-tional methods listed below:Greedy is our first method that uses simple greedysearch and a linear SVM (using surroundingword/POS 1, 2 and 3-grams as features) tolearn the segmentation model.Greedy+DP is the algorithm that introducesgrouping the positions in the source sentenceby POS bigrams.Punct-Predict is the method using predicted po-sitions of punctuation (Rangarajan Sridhar etal., 2013).RP is the method using right probability (Fujita etal., 2013).3.2 Results and DiscussionFigures 4 and 5 show the results of evaluation foreach segmentation strategy measured by BLEUand RIBES respectively.
The horizontal axis is themean number of words in the generated transla-tion units.
This value is proportional to the delayexperienced during simultaneous speech transla-tion (Rangarajan Sridhar et al, 2013) and thus asmaller value is desirable.RP, Greedy, and Greedy+DP methods havemultiple results in these graphs because thesemethods have a parameter that controls segmen-tation frequency.
We move this parameter fromno segmentation (sentence-based translation) to5540 5 10 15101214161820En-DeEn-Ja#words/segmentBLEUPunct-PredictRPGreedyGreedy+DPGreedy+DP(?=0.5)Figure 4: BLEU score of test set.0 5 10 154550556065707580En-DeEn-Ja#words/segmentRIBESPunct-PredictRPGreedyGreedy+DPGreedy+DP(?=0.5)Figure 5: RIBES score of test set.segmenting every possible boundary (word-basedtranslation) and evaluate the results.First, focusing on the Greedy method, we cansee that it underperforms the other methods.
Thisis a result of over-fitting as will be described indetail later.
In contrast, the proposed Greedy+DPmethod shows high performance compared to theother methods.
Especially, the result of BLEU onthe English-German and the RIBES on both lan-guage pairs show higher performance than RP atall speed settings.
Punct-Predict does not havean adjustable parameter, so we can only showone point.
We can see that Greedy+DP can be-gin translation about two to three times faster thanPunct-Predict while maintaining the same perfor-mance.Figure 6 shows the BLEU on the training data.From this figure, it is clear that Greedy achievesmuch higher performance than Greedy+DP.
Fromthis result, we can see that the Greedy algorithm ischoosing a segmentation that achieves high accu-racy on the training data but does not generalize tothe test data.
In contrast, the grouping constraint inthe Greedy+DP algorithm is effectively suppress-ing this overfitting.The mean number of words ?
can be decidedindependently from other information, but a con-figuration of ?
affects tradeoff relation betweentranslation accuracy and simultaneity.
For exam-ple, smaller ?
makes faster translation speed butit also makes less translation accuracy.
Basically,we should choose ?
by considering this tradeoff.4 Conclusion and Future WorkWe proposed new algorithms for learning a seg-mentation strategy in simultaneous speech trans-0 5 10 155101520253035En-DeEn-Ja#words/segmentBLEUGreedyGreedy+DPGreedy+DP(?=0.5)Figure 6: BLEU score of training set.lation.
Our algorithms directly optimize the per-formance of a machine translation system accord-ing to an evaluation measure, and are calculated bygreedy search and dynamic programming.
Exper-iments show our Greedy+DP method effectivelyseparates the source sentence into smaller unitswhile maintaining translation performance.With regards to future work, it has beennoted that translation performance can be im-proved by considering the previously translatedsegment when calculating LM probabilities (Ran-garajan Sridhar et al, 2013).
We would like to ex-pand our method to this framework, although in-corporation of context-sensitive translations is nottrivial.
In addition, theGreedy+DP algorithm usesonly one feature per a position in this paper.
Usinga variety of features is also possible, so we plan toexamine expansions of our algorithm to multipleoverlapping features in future work.AcknowledgementsPart of this work was supported by JSPS KAK-ENHI Grant Number 24240032.555ReferencesSrinivas Bangalore, Vivek Kumar Rangarajan Srid-har, Prakash Kolan, Ladan Golipour, and AuraJimenez.
2012.
Real-time incremental speech-to-speech translation of dialogs.
In Proc.
NAACL HLT,pages 437?445.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
Wit3: Web inventory of transcribed andtranslated talks.
In Proc.
EAMT, pages 261?268.Mauro Cettolo, Jan Niehues, Sebastian St?uker, LuisaBentivogli, and Marcello Federico.
2013.
Report onthe 10th iwslt evaluation campaign.
In Proc.
IWSLT.Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-jime Tsukada.
2013.
Adaptation data selection us-ing neural language models: Experiments in ma-chine translation.
In Proc.
ACL, pages 678?683.Christian F?ugen, Alex Waibel, and Muntsin Kolss.2007.
Simultaneous translation of lectures andspeeches.
Machine Translation, 21(4):209?252.Tomoki Fujita, Graham Neubig, Sakriani Sakti,Tomoki Toda, and Satoshi Nakamura.
2013.
Sim-ple, lexicalized choice of translation timing for si-multaneous speech translation.
In InterSpeech.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
Automaticevaluation of translation quality for distant languagepairs.
In Proc.
EMNLP, pages 944?952.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
ACL, pages 177?180.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: Amethod for evaluating automatic evaluation metricsfor machine translation.
In Proc.
COLING.Evgeny Matusov, Arne Mauser, and Hermann Ney.2006.
Automatic sentence segmentation and punc-tuation prediction for spoken language translation.In Proc.
IWSLT, pages 158?165.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptablejapanese morphological analysis.
In Proc.
NAACLHLT, pages 529?533.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In Proc.
ACL, pages311?318.Vivek Kumar Rangarajan Sridhar, John Chen, SrinivasBangalore, Andrej Ljolje, and Rathinavelu Chengal-varayan.
2013.
Segmentation strategies for stream-ing speech translation.
In Proc.
NAACL HLT, pages230?238.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proc.
NAACL, pages 173?180.Mahsa Yarmohammadi, Vivek Kumar Rangara-jan Sridhar, Srinivas Bangalore, and BaskaranSankaran.
2013.
Incremental segmentation anddecoding strategies for simultaneous translation.
InProc.
IJCNLP, pages 1032?1036.556
