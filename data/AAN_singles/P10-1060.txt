Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 585?594,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsGenerating Focused Topic-specific Sentiment LexiconsValentin Jijkoun Maarten de Rijke Wouter WeerkampISLA, University of Amsterdam, The Netherlandsjijkoun,derijke,w.weerkamp@uva.nlAbstractWe present a method for automaticallygenerating focused and accurate topic-specific subjectivity lexicons from a gen-eral purpose polarity lexicon that allowusers to pin-point subjective on-topic in-formation in a set of relevant documents.We motivate the need for such lexiconsin the field of media analysis, describea bootstrapping method for generating atopic-specific lexicon from a general pur-pose polarity lexicon, and evaluate thequality of the generated lexicons bothmanually and using a TREC Blog tracktest set for opinionated blog post retrieval.Although the generated lexicons can be anorder of magnitude more selective than thegeneral purpose lexicon, they maintain, oreven improve, the performance of an opin-ion retrieval system.1 IntroductionIn the area of media analysis, one of the keytasks is collecting detailed information about opin-ions and attitudes toward specific topics from var-ious sources, both offline (traditional newspapers,archives) and online (news sites, blogs, forums).Specifically, media analysis concerns the follow-ing system task: given a topic and list of docu-ments (discussing the topic), find all instances ofattitudes toward the topic (e.g., positive/negativesentiments, or, if the topic is an organization orperson, support/criticism of this entity).
For everysuch instance, one should identify the source ofthe sentiment, the polarity and, possibly, subtopicsthat this attitude relates to (e.g., specific targetsof criticism or support).
Subsequently, a (hu-man) media analyst must be able to aggregatethe extracted information by source, polarity orsubtopics, allowing him to build support/criticismnetworks etc.
(Altheide, 1996).
Recent advancesin language technology, especially in sentimentanalysis, promise to (partially) automate this task.Sentiment analysis is often considered in thecontext of the following two tasks:?
sentiment extraction: given a set of textualdocuments, identify phrases, clauses, sen-tences or entire documents that express atti-tudes, and determine the polarity of these at-titudes (Kim and Hovy, 2004); and?
sentiment retrieval: given a topic (and possi-bly, a list of documents relevant to the topic),identify documents that express attitudes to-ward this topic (Ounis et al, 2007).How can technology developed for sentimentanalysis be applied to media analysis?
In orderto use a sentiment extraction system for a mediaanalysis problem, a system would have to be ableto determine which of the extracted sentiments areactually relevant, i.e., it would not only have toidentify specific targets of all extracted sentiments,but also decide which of the targets are relevantfor the topic at hand.
This is a difficult task, asthe relation between a topic (e.g., a movie) andspecific targets of sentiments (e.g., acting or spe-cial effects in the movie) is not always straight-forward, in the face of ubiquitous complex lin-guistic phenomena such as referential expressions(?.
.
.
this beautifully shot documentary?)
or bridg-ing anaphora (?the director did an excellent jobs?
).In sentiment retrieval, on the other hand, thetopic is initially present in the task definition, butit is left to the user to identify sources and targetsof sentiments, as systems typically return a listof documents ranked by relevance and opinion-atedness.
To use a traditional sentiment retrievalsystem in media analysis, one would still have tomanually go through ranked lists of documents re-turned by the system.585To be able to support media analysis, we need tocombine the specificity of (phrase- or word-level)sentiment analysis with the topicality provided bysentiment retrieval.
Moreover, we should be ableto identify sources and specific targets of opinions.Another important issue in the media analysiscontext is evidence for a system?s decision.
If theoutput of a system is to be used to inform actions,the system should present evidence, e.g., high-lighting words or phrases that indicate a specificattitude.
Most modern approaches to sentimentanalysis, however, use various flavors of classifi-cation, where decisions (typically) come with con-fidence scores, but without explicit support.In order to move towards the requirements ofmedia analysis, in this paper we focus on two ofthe problems identified above: (1) pinpointing ev-idence for a system?s decisions about the presenceof sentiment in text, and (2) identifying specifictargets of sentiment.We address these problems by introducing aspecial type of lexical resource: a topic-specificsubjectivity lexicon that indicates specific relevanttargets for which sentiments may be expressed; fora given topic, such a lexicon consists of pairs (syn-tactic clue, target).
We present a method for au-tomatically generating a topic-specific lexicon fora given topic and query-biased set of documents.We evaluate the quality of the lexicon both manu-ally and in the setting of an opinionated blog postretrieval task.
We demonstrate that such a lexi-con is highly focused, allowing one to effectivelypinpoint evidence for sentiment, while being com-petetive with traditional subjectivity lexicons con-sisting of (a large number of) clue words.Unlike other methods for topic-specific senti-ment analysis, we do not expand a seed lexicon.Instead, we make an existing lexicon more fo-cused, so that it can be used to actually pin-pointsubjectivity in documents relevant to a given topic.2 Related WorkMuch work has been done in sentiment analy-sis.
We discuss related work in four parts: sen-timent analysis in general, domain- and target-specific sentiment analysis, product review miningand sentiment retrieval.2.1 Sentiment analysisSentiment analysis is often seen as two separatesteps for determining subjectivity and polarity.Most approaches first try to identify subjectiveunits (documents, sentences), and for each of thesedetermine whether it is positive or negative.
Kimand Hovy (2004) select candidate sentiment sen-tences and use word-based sentiment classifiersto classify unseen words into a negative or posi-tive class.
First, the lexicon is constructed fromWordNet: from several seed words, the structureof WordNet is used to expand this seed to a fulllexicon.
Next, this lexicon is used to measure thedistance between unseen words and words in thepositive and negative classes.
Based on word sen-timents, a decision is made at the sentence level.A similar approach is taken by Wilson et al(2005): a classifier is learnt that distinguishes be-tween polar and neutral sentences, based on a priorpolarity lexicon and an annotated corpus.
Amongthe features used are syntactic features.
After thisinitial step, the sentiment sentences are classifiedas negative or positive; again, a prior polarity lexi-con and syntactic features are used.
The authorslater explored the difference between prior andcontextual polarity (Wilson et al, 2009): wordsthat lose polarity in context, or whose polarity isreversed because of context.Riloff and Wiebe (2003) describe a bootstrap-ping method to learn subjective extraction pat-terns that match specific syntactic templates, usinga high-precision sentence-level subjectivity clas-sifier and a large unannotated corpus.
In ourmethod, we bootstrap from a subjectivity lexi-cion rather than a classifier, and perform a topic-specific analysis, learning indicators of subjectiv-ity toward a specific topic.2.2 Domain- and target-specific sentimentThe way authors express their attitudes varieswith the domain: An unpredictable movie can bepositive, but unpredictable politicians are usuallysomething negative.
Since it is unrealistic to con-struct sentiment lexicons, or manually annotatetext for learning, for every imaginable domain ortopic, automatic methods have been developed.Godbole et al (2007) aim at measuring over-all subjectivity or polarity towards a certain entity;they identify sentiments using domain-specificlexicons.
The lexicons are generated from man-ually selected seeds for a broad domain such asHealth or Business, following an approach simi-lar to (Kim and Hovy, 2004).
All named entitesin a sentence containing a clue from a lexicon are586considered targets of sentiment for counting.
Be-cause of the data volume, no expensive linguisticprocessing is performed.Choi et al (2009) advocate a joint topic-sentiment analysis.
They identify ?sentiment top-ics,?
noun phrases assumed to be linked to a sen-timent clue in the same expression.
They addresstwo tasks: identifying sentiment clues, and clas-sifying sentences into positive, negative, or neu-tral.
They start by selecting initial clues from Sen-tiWordNet, based on sentences with known polar-ity.
Next, the sentiment topics are identified, andbased on these sentiment topics and the current listof clues, new potential clues are extracted.
Theclues can be used to classifiy sentences.Fahrni and Klenner (2008) identify potentialtargets in a given domain, and create a target-specific polarity adjective lexicon.
To this end,they find targets using Wikipedia, and associatedadjectives.
Next, the target-specific polarity of ad-jectives is detemined using Hearst-like patterns.Kanayama and Nasukawa (2006) introduce po-lar atoms: minimal human-understandable syn-tactic structures that specify polarity of clauses.The goal is to learn new domain-specific polaratoms, but these are not target-specific.
Theyuse manually-created syntactic patterns to identifyatoms and coherency to determine polarity.In contrast to much of the work in the literature,we need to specialize subjectivity lexicons not fora domain and target, but for ?topics.
?2.3 Product features and opinionsMuch work has been carried out for the task ofmining product reviews, where the goal is to iden-tify features of specific products (such as picture,zoom, size, weight for digital cameras) and opin-ions about these specific features in user reviews.Liu et al (2005) describe a system that identifiessuch features via rules learned from a manuallyannotated corpus of reviews; opinions on featuresare extracted from the structure of reviews (whichexplicitly separate positive and negative opinions).Popescu and Etzioni (2005) present a methodthat identifies product features for using corpusstatistics, WordNet relations and morphologicalcues.
Opinions about the features are extracted us-ing a hand-crafted set of syntactic rules.Targets extracted in our method for a topic aresimilar to features extracted in review mining forproducts.
However, topics in our setting go be-yond concrete products, and the diversity and gen-erality of possible topics makes it difficult to ap-ply such supervised or thesaurus-based methods toidentify opinion targets.
Moreover, in our methodwe directly use associations between targets andopinions to extract both.2.4 Sentiment retrievalAt TREC, the Text REtrieval Conference, therehas been interest in a specific type of sentimentanalysis: opinion retrieval.
This interest materi-alized in 2006 (Ounis et al, 2007), with the opin-ionated blog post retrieval task.
Finding blog poststhat are not just about a topic, but also contain anopinion on the topic, proves to be a difficult task.Performance on the opinion-finding task is domi-nated by performance on the underlying documentretrieval task (the topical baseline).Opinion finding is often approached as a two-stage problem: (1) identify documents relevant tothe query, (2) identify opinions.
In stage (2) onecommonly uses either a binary classifier to distin-guish between opinionated and non-opinionateddocuments or applies reranking of the initial resultlist using some opinion score.
Opinion add-onsshow only slight improvements over relevance-only baselines.The best performing opinion finding system atTREC 2008 is a two-stage approach using rerank-ing in stage (2) (Lee et al, 2008).
The authorsuse SentiWordNet and a corpus-derived lexiconto construct an opinion score for each post in aninitial ranking of blog posts.
This opinion scoreis combined with the relevance score, and postsare reranked according to this new score.
We de-tail this approach in Section 6.
Later, the authorsuse domain-specific opinion indicators (Na et al,2009), like ?interesting story?
(movie review), and?light?
(notebook review).
This domain-specificlexicon is constructed using feedback-style learn-ing: retrieve an initial list of documents and usethe top documents as training data to learn an opin-ion lexicon.
Opinion scores per document are thencomputed as an average of opinion scores overall its words.
Results show slight improvements(+3%) on mean average precision.3 Generating Topic-Specific LexiconsIn this section we describe how we generate a lex-icon of subjectivity clues and targets for a giventopic and a list of relevant documents (e.g., re-587Extract allsyntactic contextsof clue wordsBackgroundcorpusTopic-independentsubjectivity lexiconRelevant docsTopicFor each clueword, select Dcontexts withhighest entropyList of syntactic clues:(clue word, syn.
context)Extract alloccurrencesendpoints ofsyntactic cluesExtract alloccurrencesendpoints ofsyntactic cluesPotential targets inbackground corpusPotential targets inrelevant doc.
listCompare frequenciesusing chi-square;select top T targetsList of T targetsFor each target,find syn.
clues itco-occurs withTopic-specific lexicon of tuples:(syntactic clue, target)Step 1Step 2Step 3Figure 1: Our method for learning a topic-dependent subjectivity lexicon.trieved by a search engine for the topic).
As an ad-ditional resource, we use a large background cor-pus of text documents of a similar style but withdiverse subjects; we assume that the relevant doc-uments are part of this corpus as well.
As the back-ground corpus, we used the set of documents fromthe assessment pools of TREC 2006?2008 opin-ion retrieval tasks (described in detail in section 4).We use the Stanford lexicalized parser1 to extractlabeled dependency triples (head, label, modifier).In the extracted triples, all words indicate their cat-egory (noun, adjective, verb, adverb, etc.)
and arenormalized to lemmas.Figure 1 provides an overview of our method;below we describe it in more detail.3.1 Step 1: Extracting syntactic contextsWe start with a general domain-independent priorpolarity lexicon of 8,821 clue words (Wilson et al,2005).
First, we identify syntactic contexts inwhich specific clue words can be used to express1http://nlp.stanford.edu/software/lex-parser.shtmlattitude: we try to find how a clue word can be syn-tactically linked to targets of sentiments.
We take asimple definition of the syntactic context: a singlelabeled directed dependency relation.
For everyclue word, we extract all syntactic contexts, i.e.,all dependencies, in which the word is involved(as head or as modifier) in the background corpus,along with their endpoints.
Table 1 shows exam-ples of clue words and contexts that indicate sen-timents.
For every clue, we only select those con-texts that exhibit a high entropy among the lemmasat the other endpoint of the dependencies.
E.g.,in our background corpus, the verb to like occurs97,179 times with a nominal subject and 52,904times with a direct object; however, the entropy oflemmas of the subjects is 4.33, compared to 9.56for the direct objects.
In other words, subjects oflike are more ?predictable.?
Indeed, the pronounI accounts for 50% of subjects, followed by you(14%), they (4%), we (4%) and people (2%).
Themost frequent objects of like are it (12%), what(4%), idea (2%), they (2%).
Thus, objects of tolike will be preferred by the method.Our entropy-driven selection of syntactic con-texts of a clue word is based on the following as-sumption:Assumption 1: In text, targets of sentimentsare more diverse than sources of sentimentsor other accompanying attributes such as lo-cation, time, manner, etc.
Therefore targetsexhibit higher entropy than other attributes.For every clue word, we select the top D syntac-tic contexts whose entropy is at least half of themaximum entropy for this clue.To summarize, at the end of Step 1 of ourmethod, we have extracted a list of pairs (clueword, syntactic context) such that for occurrencesof the clue word, the words at the endpoint of thesyntactic dependency are likely to be targets ofsentiments.
We call such a pair a syntactic clue.3.2 Step 2: Selecting potential targetsHere, we use the extracted syntantic clues to iden-tify words that are likely to serve as specific tar-gets for opinions about the topic in the relevantdocuments.
In this work we only consider individ-ual words as potential targets and leave exploringother options (e.g., NPs and VPs as targets) for fu-ture work.
In extracting targets, we rely on thefollowing assumption:588Clue word Syntactic context Target Exampleto like has direct object u2 I do still like U2 very muchto like has clausal complement criticize I don?t like to criticize our intelligence servicesto like has about-modifier olympics That?s what I like about Winter Olympicsterrible is adjectival modifier of idea it?s a terrible idea to recall judges for...terrible has nominal subject shirt And Neil, that shirt is terrible!terrible has clausal complement can It is terrible that a small group of extremists can .
.
.Table 1: Examples of subjective syntactic contexts of clue words (based on Stanford dependencies).Assumption 2: The list of relevant documentscontains a substantial number of documentson the topic which, moreover, contain senti-ments about the topic.We extract all endpoints of all occurrences of thesyntactic clues in the relevant documents, as wellas in the background corpus.
To identify potentialattitude targets in the relevant documents, we com-pare their frequency in the relevant documents tothe frequency in the background corpus using thestandard ?2 statistics.
This technique is based onthe following assumption:Assumption 3: Sentiment targets related tothe topic occur more often in subjective con-text in the set of relevant documents, thanin the background corpus.
In other words,while the background corpus contains senti-ments towards very diverse subjects, the rel-evant documents tend to express attitudes re-lated to the topic.For every potential target, we compute the ?2-score and select the top T highest scoring targets.As the result of Steps 1 and 2, as candidate tar-gets for a given topic, we only select words that oc-cur in subjective contexts, and that do so more of-ten than we would normally expect.
Table 2 showsexamples of extracted targets for three TREC top-ics (see below for a description of our experimen-tal data).3.3 Step 3: Generating topic-specific lexiconsIn the last step of the method, we combine cluesand targets.
For each target identified in Step 2,we take all syntactic clues extracted in Step 1 thatco-occur with the target in the relevant documents.The resulting list of triples (clue word, syntacticcontext, target) constitute the lexicon.
We conjec-ture that an occurrence of a lexicon entry in a textindicates, with reasonable confidence, a subjectiveattitude towards the target.Topic ?Relationship between Abramoff and Bush?abramoff lobbyist scandal fundraiser bush fund-raiser re-publican prosecutor tribe swirl corrupt corruption norquistdemocrat lobbying investigation scanlon reid lawmakerdealings presidentTopic ?MacBook Pro?macbook laptop powerbook connector mac processor note-book fw800 spec firewire imac pro machine apple power-books ibook ghz g4 ata binary keynote drive modemTopic: ?Super Bowl ads?ad bowl commercial fridge caveman xl endorsement adver-tising spot advertiser game super essential celebrity payoffmarketing publicity brand advertise watch viewer tv footballvenueTable 2: Examples of targets extracted at Step 2.4 Data and Experimental SetupWe consider two types of evaluation.
In the nextsection, we examine the quality of the lexiconswe generate.
In the section after that we evaluatelexicons quantitatively using the TREC Blog trackbenchmark.For extrinsic evaluation we apply our lexi-con generation method to a collection of doc-uments containing opinionated utterances: blogposts.
The Blogs06 collection (Macdonald andOunis, 2006) is a crawl of blog posts from 100,649blogs over a period of 11 weeks (06/12/2005?21/02/2006), with 3,215,171 posts in total.
Be-fore indexing the collection, we perform two pre-processing steps: (i) when extracting plain textfrom HTML, we only keep block-level elementslonger than 15 words (to remove boilerplate mate-rial), and (ii) we remove non-English posts usingTextCat2 for language detection.
This leaves uswith 2,574,356 posts with 506 words per post onaverage.
We index the collection using Indri,3 ver-sion 2.10.TREC 2006?2008 came with the task of opin-ionated blog post retrieval (Ounis et al, 2007).For each year a set of 50 topics was created, giv-2http://odur.let.rug.nl/?vannoord/TextCat/3http://www.lemurproject.org/indri/589ing us 150 topics in total.
Every topic comes witha set of relevance judgments: Given a topic, a blogpost can be either (i) nonrelevant, (ii) relevant, butnot opinionated, or (iii) relevant and opinionated.TREC topics consist of three fields (title, descrip-tion, and narrative), of which we only use the titlefield: a query of 1?3 keywords.We use standard TREC evaluation measures foropinion retrieval: MAP (mean average precision),R-precision (precision within the top R retrieveddocuments, where R is the number of known rel-evant documents in the collection), MRR (meanreciprocal rank), P@10 and P@100 (precisionwithin the top 10 and 100 retrieved documents).In the context of media analysis, recall-orientedmeasures such as MAP and R-precision are moremeaningful than the other, early precision-orientedmeasures.
Note that for the opinion retrieval taska document is considered relevant if it is on topicand contains opinions or sentiments towards thetopic.Throughout Section 6 below, we test for signif-icant differences using a two-tailed paired t-test,and report on significant differences for ?
= 0.01(N and H), and ?
= 0.05 (M and O).For the quantative experiments in Section 6 weneed a topical baseline: a set of blog posts po-tentially relevant to each topic.
For this, we usethe Indri retrieval engine, and apply the MarkovRandom Fields to model term dependencies in thequery (Metzler and Croft, 2005) to improve topi-cal retrieval.
We retrieve the top 1,000 posts foreach query.5 Qualitative Analysis of LexiconsLexicon size (the number of entries) and selectiv-ity (how often entries match in text) of the gen-erated lexicons vary depending on the parame-ters D and T introduced above.
The two right-most columns of Table 4 show the lexicon sizeand the average number of matches per topic.
Be-cause our topic-specific lexicons consist of triples(clue word, syntactic context, target), they actu-ally contain more words than topic-independentlexicons of the same size, but topic-specific en-tries are more selective, which makes the lexiconmore focused.
Table 3 compares the applicationof topic-independent and topic-specific lexicons toon-topic blog text.We manually performed an explorative erroranalysis on a small number of documents, anno-There are some tragic mo-ments like eggs freezing ,and predators snatching thefemales and little ones-youknow the whole NATUREthing ... but this movie isawesomeThere are some tragic mo-ments l ike eggs freezing ,and predators snatching thefemales and little ones-youknow the whole NATUREthing ... but this movie isawesomeSaturday was more errands,then spent the evening withDad and Stepmum, and fi-nallywas able to see Marchof the Penguins, whichwas wonderful.
ChristmasDay was lovely, surroundedby family, good food anddrink, and little L to playwith.Saturday was more errands,then spent the evening withDad and Stepmum, and fi-nally was able to see Marchof the Penguins, whichwas wonderful.
ChristmasDay was lovely, surroundedby family, good food anddrink, and little L to playwith.Table 3: Posts with highlighted targets (bold) andsubjectivity clues (blue) using topic-independent(left) and topic-specific (right) lexicons.tated using the smallest lexicon in Table 4 for thetopic ?March of the Pinguins.?
We assigned 186matches of lexicon entries in 30 documents intofour classes:?
REL: sentiment towards a relevant target;?
CONTEXT: sentiment towards a target thatis irrelevant to the topic due to context (e.g.,opinion about a target ?film?, but refering toa film different from the topic);?
IRREL: sentiment towards irrelevant target(e.g., ?game?
for a topic about a movie);?
NOSENT: no sentiment at allIn total only 8% of matches were manually clas-sified as REL, with 62% classified as NOSENT,23% as CONTEXT, and 6% as IRREL.
On theother hand, among documents assessed as opio-nionated by TREC assessors, only 13% did notcontain matches of the lexicon entries, comparedto 27% of non-opinionated documents, whichdoes indicate that our lexicon does attempt to sep-arate non-opinionated documents from opinion-ated.6 Quantitative Evaluation of LexiconsIn this section we assess the quality of the gen-erated topic-specific lexicons numerically and ex-trinsically.
To this end we deploy our lexicons tothe task of opinionated blog post retrieval (Ouniset al, 2007).
A commonly used approach to thistask works in two stages: (1) identify topically rel-evant blog posts, and (2) classify these posts asbeing opinionated or not.
In stage 2 the standard590approach is to rerank the results from stage 1, in-stead of doing actual binary classification.
We takethis approach, as it has shown good performancein the past TREC editions (Ounis et al, 2007) andis fairly straightforward to implement.
We also ex-plore another way of using the lexicon: as a sourcefor query expansion (i.e., adding new terms to theoriginal query) in Section 6.2.
For all experimentswe use the collection described in Section 4.Our experiments have two goals: to comparethe use of topic-independent and topic-specificlexicons for the opinionated post retrieval task,and to examine how different settings for the pa-rameters of the lexicon generation affect the em-pirical quality.6.1 Reranking using a lexiconTo rerank a list of posts retrieved for a given topic,we opt to use the method that showed best per-formance at TREC 2008.
The approach takenby Lee et al (2008) linearly combines a (top-ical) relevance score with an opinion score foreach post.
For the opinion score, terms from a(topic-independent) lexicon are matched againstthe post content, and weighted with the probabilityof term?s subjectivity.
Finally, the sum is normal-ized using the Okapi BM25 framework.
The finalopinion score Sop is computed as in Eq.
1:Sop(D) =Opinion(D) ?
(k1 + 1)Opinion(D) + k1 ?
(1 ?
b +b?|D|avgdl ), (1)where k1, and b are Okapi parameters (set to theirdefault values k1 = 2.0, and b = 0.75), |D| is thelength of document D, and avgdl is the averagedocument length in the collection.
The opinionscore Opinion(D) is calculated using Eq.
2:Opinion(D) =?w?OP (sub|w) ?
n(w,D), (2)where O is the set of terms in the sentiment lex-icon, P (sub|w) indicates the probability of termw being subjective, and n(w,D) is the number oftimes term w occurs in document D. The opinionscoring can weigh lexicon terms differently, usingP (sub|w); it normalizes scores to cancel out theeffect of varying document sizes.In our experiments we use the method de-scribed above, and plug in the MPQA polaritylexicon.4 We compare the results of using this4http://www.cs.pitt.edu/mpqa/topic-independent lexicon to the topic-dependentlexicons our method generates, which are alsoplugged into the reranking of Lee et al (2008).In addition to using Okapi BM25 for opinionscoring, we also consider a simpler method.
Aswe observed in Section 5, our topic-specific lexi-cons are more selective than the topic-independentlexicon, and a simple number of lexicon matchescan give a good indication of opinionatedness of adocument:Sop(D) = min(n(O,D), 10)/10, (3)where n(O,D) is the number of matches of theterm of sentiment lexicon O in document D.6.1.1 Results and observationsThere are several parameters that we can varywhen generating a topic-specific lexicon and whenusing it for reranking:D: the number of syntactic contexts per clueT : the number of extracted targetsSop(D): the opinion scoring function.?
: the weight of the opinion score in the linearcombination with the relevance score.Note that ?
does not affect the lexicon creation,but only how the lexicon is used in reranking.Since we want to assess the quality of lexicons,not in the opinionated retrieval performance assuch, we factor out ?
by selecting the best settingfor each lexicon (including the topic-independent)and each evaluation measure.In Table 4 we present the results of evaluationof several lexicons in the context of opinionatedblog post retrieval.First, we note that reranking using all lexi-cons in Table 4 significantly improves over therelevance-only baseline for all evaluation mea-sures.
When comparing topic-specific lexicons tothe topic-independent one, most of the differencesare not statistically significant, which is surpris-ing given the fact that most topic-specific lexiconswe evaluated are substantially smaller (see the tworightmost columns in the table).
The smallest lex-icon in Table 4 is seven times more selective thanthe general one, in terms of the number of lexiconmatches per document.The only evaluation measure where the topic-independent lexicon consistently outperformstopic-specific ones, is Mean Reciprocal Rank thatdepends on a single relevant opinionated docu-ment high in a ranking.
A possible explanation591Lexicon MAP R-prec MRR P@10 P@100 |lexicon| hits per docno reranking 0.2966 0.3556 0.6750 0.4820 0.3666 ?
?topic-independent 0.3182 0.3776 0.7714 0.5607 0.3980 8,221 36.17D T Sop3 50 count 0.3191 0.3769 0.7276O 0.5547 0.3963 2,327 5.023 100 count 0.3191 0.3777 0.7416 0.5573 0.3971 3,977 8.585 50 count 0.3178 0.3775 0.7246O 0.5560 0.3931 2,784 5.735 100 count 0.3178 0.3784 0.7316O 0.5513 0.3961 4,910 10.06all 50 count 0.3167 0.3753 0.7264O 0.5520 0.3957 4,505 9.34all 100 count 0.3146 0.3761 0.7283O 0.5347O 0.3955 8,217 16.72all 50 okapi 0.3129 0.3713 0.7247H 0.5333O 0.3833O 4,505 9.34all 100 okapi 0.3189 0.3755 0.7162H 0.5473 0.3921 8,217 16.72all 200 okapi 0.3229N 0.3803 0.7389 0.5547 0.3987 14,581 29.14Table 4: Evaluation of topic-specific lexicons applied to the opinion retrieval task, compared to the topic-independent lexicon.
The two rightmost columns show the number of lexicon entries (average per topic)and the number of matches of lexicon entries in blog posts (average for top 1,000 posts).is that the large general lexicon easily finds a few?obviously subjective?
posts (those with heavilyused subjective words), but is not better at detect-ing less obvious ones, as indicated by the recall-oriented MAP and R-precision.Interestingly, increasing the number of syntac-tic contexts considered for a clue word (parame-ter D) and the number of selected targets (param-eter T ) leads to substantially larger lexicons, butonly gives marginal improvements when lexiconsare used for opinion retrieval.
This shows that ourbootstrapping method is effective at filtering outnon-relevant sentiment targets and syntactic clues.The evaluation results also show that the choiceof opinion scoring function (Okapi or raw counts)depends on the lexicon size: for smaller, more fo-cused lexicons unnormalized counts are more ef-fective.
This also confirms our intuition that forsmall, focused lexicons simple presence of a sen-timent clue in text is a good indication of subjec-tivity, while for larger lexicons an overall subjec-tivity scoring of texts has to be used, which can behard to interpret for (media analysis) users.6.2 Query expansion with lexiconsIn this section we evaluate the quality of targetsextracted as part of the lexicons by using them forquery expansion.
Query expansion is a commonlyused technique in information retrieval, aimed atgetting a better representation of the user?s in-formation need by adding terms to the originalretrieval query; for user-generated content, se-lective query expansion has proved very benefi-cial (Weerkamp et al, 2009).
We hypothesize thatif our method manages to identify targets that cor-respond to issues, subtopics or features associatedRun MAP P@10 MRRTopical blog post retrievalBaseline 0.4086 0.7053 0.7984Rel.
models 0.4017O 0.6867 0.7383HSubj.
targets 0.4190M 0.7373M 0.8470MOpinion retrievalBaseline 0.2966 0.4820 0.6750Rel.
models 0.2841H 0.4467H 0.5479HSubj.
targets 0.3075 0.5227N 0.7196Table 5: Query expansion using relevance mod-els and topic-specific subjectivity targets.
Signifi-cance tested against the baseline.with the topic, the extracted targets should be goodcandidates for query expansion.
The experimentsdescribed below test this hypothesis.For every test topic, we select the 20 top-scoringtargets as expansion terms, and use Indri to re-turn 1,000 most relevant documents for the ex-panded query.
We evaluate the resulting rankingusing both topical retrieval and opinionated re-trieval measures.
For the sake of comparison, wealso implemented a well-known query expansionmethod based on Relevance Models (Lavrenkoand Croft, 2001): this method has been shown towork well in many settings.
Table 5 shows evalu-ation results for these two query expansion meth-ods, compared to the baseline retrieval run.The results show that on topical retrieval queryexpansion using targets significantly improves re-trieval performance, while using relevance mod-els actually hurts all evaluation measures.
Thefailure of the latter expansion method can be at-tributed to the relatively large amount of noisein user-generated content, such as boilerplate592material, timestamps of blog posts, commentsetc.
(Weerkamp and de Rijke, 2008).
Our methoduses full syntactic parsing of the retrieved doc-uments, which might substantially reduce theamount of noise since only (relatively) well-formed English sentences are used in lexicon gen-eration.For opinionated retrieval, target-based expan-sion also improves over the baseline, although thedifferences are only significant for P@10.
Theconsistent improvement for topical retrieval sug-gests that a topic-specific lexicon can be used bothfor query expansion (as described in this section)and for opinion reranking (as described in Sec-tion 6.1).
We leave this combination for futurework.7 Conclusions and Future WorkWe have described a bootstrapping method for de-riving a topic-specific lexicon from a general pur-pose polarity lexicon.
We have evaluated the qual-ity of generated lexicons both manually and usinga TREC Blog track test set for opinionated blogpost retrieval.
Although the generated lexiconscan be an order of magnitude more selective, theymaintain, or even improve, the performance of anopinion retrieval system.As to future work, we intend to combine ourmethod with known methods for topic-specificlexicon expansion (our method is rather concernedwith lexicon ?restriction?).
Existing sentence-or phrase-level (trained) sentiment classifiers canalso be used easily: when collecting/counting tar-gets we can weigh them by ?prior?
score providedby such classifiers.
We also want to look at morecomplex syntactic patterns: Choi et al (2009) re-port that many errors are due to exclusive use ofunigrams.
We would also like to extend poten-tial opinion targets to include multi-word phrases(NPs and VPs), in addition to individual words.Finally, we do not identify polarity yet: this canbe partially inherited from the initial lexicon andrefined automatically via bootstrapping.AcknowledgementsThis research was supported by the EuropeanUnion?s ICT Policy Support Programme as partof the Competitiveness and Innovation FrameworkProgramme, CIP ICT-PSP under grant agreementnr 250430, by the DuOMAn project carried outwithin the STEVIN programme which is fundedby the Dutch and Flemish Governments underproject nr STE-09-12, and by the Netherlands Or-ganisation for Scientific Research (NWO) underproject nrs 612.066.512, 612.061.814, 612.061.-815, 640.004.802.ReferencesAltheide, D. (1996).
Qualitative Media Analysis.
Sage.Choi, Y., Kim, Y., and Myaeng, S.-H. (2009).
Domain-specific sentiment analysis using contextual feature gen-eration.
In TSA ?09: Proceeding of the 1st internationalCIKM workshop on Topic-sentiment analysis for massopinion, pages 37?44, New York, NY, USA.
ACM.Fahrni, A. and Klenner, M. (2008).
Old Wine or WarmBeer: Target-Specific Sentiment Analysis of Adjectives.In Proc.of the Symposium on Affective Language in Hu-man and Machine, AISB 2008 Convention, 1st-2nd April2008.
University of Aberdeen, Aberdeen, Scotland, pages60 ?
63.Godbole, N., Srinivasaiah, M., and Skiena, S. (2007).
Large-scale sentiment analysis for news and blogs.
In Proceed-ings of the International Conference on Weblogs and So-cial Media (ICWSM).Kanayama, H. and Nasukawa, T. (2006).
Fully automatic lex-icon expansion for domain-oriented sentiment analysis.
InEMNLP ?06: Proceedings of the 2006 Conference on Em-pirical Methods in Natural Language Processing, pages355?363, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Kim, S. and Hovy, E. (2004).
Determining the sentiment ofopinions.
In Proceedings of COLING 2004.Lavrenko, V. and Croft, B.
(2001).
Relevance-based languagemodels.
In SIGIR ?01: Proceedings of the 24th annualinternational ACM SIGIR conference on research and de-velopment in information retrieval.Lee, Y., Na, S.-H., Kim, J., Nam, S.-H., Jung, H.-Y., and Lee,J.-H. (2008).
KLE at TREC 2008 Blog Track: Blog Postand Feed Retrieval.
In Proceedings of TREC 2008.Liu, B., Hu, M., and Cheng, J.
(2005).
Opinion observer: an-alyzing and comparing opinions on the web.
In Proceed-ings of the 14th international conference on World WideWeb.Macdonald, C. and Ounis, I.
(2006).
The TREC Blogs06collection: Creating and analysing a blog test collection.Technical Report TR-2006-224, Department of ComputerScience, University of Glasgow.Metzler, D. and Croft, W. B.
(2005).
A markov random feldmodel for term dependencies.
In SIGIR ?05: Proceed-ings of the 28th annual international ACM SIGIR con-ference on research and development in information re-trieval, pages 472?479, New York, NY, USA.
ACM Press.Na, S.-H., Lee, Y., Nam, S.-H., and Lee, J.-H. (2009).
Im-proving opinion retrieval based on query-specific senti-ment lexicon.
In ECIR ?09: Proceedings of the 31th Eu-ropean Conference on IR Research on Advances in In-formation Retrieval, pages 734?738, Berlin, Heidelberg.Springer-Verlag.Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., andSoboroff, I.
(2007).
Overview of the TREC 2006 blogtrack.
In The Fifteenth Text REtrieval Conference (TREC2006).
NIST.Popescu, A.-M. and Etzioni, O.
(2005).
Extracting prod-uct features and opinions from reviews.
In Proceedingsof Human Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Process-ing (HLT/EMNLP).Riloff, E. and Wiebe, J.
(2003).
Learning extraction patterns593for subjective expressions.
In Proceedings of the 2003Conference on Empirical methods in Natural LanguageProcessing (EMNLP).Weerkamp, W., Balog, K., and de Rijke, M. (2009).
A gener-ative blog post retrieval model that uses query expansionbased on external collections.
In Joint conference of the47th Annual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Conference onNatural Language Processing of the Asian Federation ofNatural Language Processing (ACL-ICNLP 2009), Singa-pore.Weerkamp, W. and de Rijke, M. (2008).
Credibility im-proves topical blog post retrieval.
In Proceedings of ACL-08: HLT, page 923931, Columbus, Ohio.
Associationfor Computational Linguistics, Association for Computa-tional Linguistics.Wilson, T., Wiebe, J., and Hoffmann, P. (2005).
Recognizingcontextual polarity in phrase-level sentiment analysis.
InHLT ?05: Proceedings of the conference on Human Lan-guage Technology and Empirical Methods in Natural Lan-guage Processing, pages 347?354, Morristown, NJ, USA.Association for Computational Linguistics.Wilson, T., Wiebe, J., and Hoffmann, P. (2009).
Recog-nizing contextual polarity: an exploration of features forphrase-level sentiment analysis.
Computational Linguis-tics, 35(3):399?433.594
