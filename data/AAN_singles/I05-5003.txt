Using Machine Translation Evaluation Techniques to DetermineSentence-level Semantic EquivalenceAndrew FinchATR Research Institute2-2-2 Hikaridai?Keihanna Science City?Kyoto 619-0288JAPANandrew.finch@atr.jpYoung-Sook HwangATR Research Institute2-2-2 Hikaridai?Keihanna Science City?Kyoto 619-0288JAPANyoungsook.hwang@atr.jpEiichiro SumitaATR Research Institute2-2-2 Hikaridai?Keihanna Science City?Kyoto 619-0288JAPANeiichiro.sumita@atr.jpAbstractThe task of machine translation (MT)evaluation is closely related to thetask of sentence-level semantic equiv-alence classification.
This paper in-vestigates the utility of applying stan-dard MT evaluation methods (BLEU,NIST, WER and PER) to building clas-sifiers to predict semantic equivalenceand entailment.
We also introduce anovel classification method based onPER which leverages part of speechinformation of the words contributingto the word matches and non-matchesin the sentence.
Our results showthat MT evaluation techniques are ableto produce useful features for para-phrase classification and to a lesser ex-tent entailment.
Our technique gives asubstantial improvement in paraphraseclassification accuracy over all of theother models used in the experiments.1 IntroductionAutomatic machine translation evaluation is ameans of scoring the output from a machine trans-lation system with respect to a small corpus ofreference translations.
The basic principle beingthat an output is a good translation if it is ?close?in some way to a member of a set of perfect trans-lations for the input sentence.
The closeness thatthese techniques are trying to capture is in essencethe notion of semantic equivalence.
Two sen-tences being semantically equivalent if they con-vey the same meaning.MT evaluation techniques have found appli-cation in the field of entailment recognition, aclose relative of semantic equivalence determina-tion that seeks methods for deciding whether theinformation provided by one sentence is includedin an another.
(Perez and Alfonseca, 2005) di-rectly applied the BLEU score to this task and(Kouylekov and Magnini, 2005) applied both aword and tree edit distance algorithm.
In this pa-per we evaluate these techniques or variants ofthem and other MT evaluation techniques on bothentailment and semantic equivalence determina-tion, to allow direct comparison to our results.When using a single reference sentence foreach candidate the task of deciding whether apair of sentences are paraphrases and the task ofMT evaluation are very similar.
Differences arisefrom the nature of the sentences being compared,that is MT output might not consist of grammat-ically correct sentences.
Moreover, MT evalu-ation scoring need not necessarily be computedon a sentence-by-sentence basis, but can be basedon statistics derived at the corpus level.
Finally,the process of MT evaluation is asymmetrical.That is, there is a distinction between the ref-erences and the candidate machine translations.Fortunately, the automatic MT evaluation tech-niques commonly in use do not make any ex-plicit attempt to score grammaticality, and (ex-cept BLEU) decompose naturally into their com-ponent scores at the sentence level.
(Blatz et al,2004) used a variant of the WER score and theNIST score at the sentence level to assign correct-17ness to translation candidates, by scoring themwith respect to a reference set.
These correctnesslabels were used as the ?ground truth?
for classi-fiers for the correctness of translation candidatesfor candidate sentence confidence estimation.
Wetoo adopt sentence level versions of these scoresand use them to classify paraphrase candidates.The motivation for these experiments is two-fold: firstly to determine how useful the featuresused by these MT evaluation techniques to se-mantic equivalence classifiers.
One would ex-pect that systems that perform well in one domainshould also perform well in the other.
After all,determining sentence level semantic equivalenceis ?part of the job?
of an MT evaluator.
Our sec-ond motivation is the conjecture that successfultechniques and strategies will be transferable be-tween the two tasks.2 MT Evaluation MethodsMT evaluation schemes score a set of MT sys-tem output segments (sentences in our case) S ={s1, s2, ..., sI} with respect to a set of referencesR corresponding to correct translations for theirrespective segments.
Since we classify sentencepairs, we only consider the case of using a singlereference for evaluation.
Thus the set of refer-ences is given by: R = {r1, r2, ..., rI}.2.1 WERWord error rate (WER) (Su et al, 1992) is a mea-sure of the number of edit operations required totransform one sentence into another, defined as:WER(si, ri) =I(si, ri) + D(si, ri) + S(si, ri)|ri|where I(si, ri), D(si, ri) and S(si, ri) are thenumber of insertions, deletions and substitutionsrespectively.2.2 PERPosition-independent word error rate (PER) (Till-mann et al, 1997) is similar to WER except thatword order is not taken into account, both sen-tences are treated as bags of words:PER(si, ri) =max[diff(si, ri), diff(ri, si)]|ri|where diff(si, ri) is the number of words ob-served only in si.2.3 BLEUThe BLEU score (Papineni et al, 2001) is basedon the geometric mean of n-gram precision.
Thescore is given by:BLEU = BP ?
exp[ N?n=11N ?
log(pn)]where N is the maximum n-gram size.The n-gram precision pn is given by:pn =?
?
count(ngram)i=1..I ngram?si?
?
countsys(ngram)i=1..I ngram?siwhere count(ngram) is the count of ngramfound in both si and ri and countsys(ngram) isthe count of ngram in si.The brevity penalty BP penalizes MT outputfor being shorter than the corresponding refer-ences and is given by:BP = exp[min[1?
LrefLsys, 1]]where Lsys is the number of words in the MToutput sentences and Lref is the number of wordsin the corresponding references.The BLEU brevity penalty is a single valuecomputed over the whole corpus rather than anaverage of sentence level penalties which wouldhave made its effect too severe.
For this reason,in our experiments we omit the brevity penaltyfrom the BLEU score.
Its effect is small since thereference sentences and system outputs are drawnfrom the same sample and have approximately thesame average length.We ran experiments for N = 1...4, these arereferred to as BLEU1 to BLEU4 respectively.2.4 NISTThe NIST score (Doddington, 2002) also usesn-gram precision, differing in that an arithmeticmean is used, weights are used to emphasize in-formative word sequences and a different brevitypenalty is used:NIST =N?n=1BP ??
info(ngram)all ngramthat co?occur?
1ngram?si18Sentence pair 1 (semantically equivalent):1.
Amrozi accused his brother, whom he called ?the witness?, of deliberately distorting his evidence.2.
Referring to him as only ?the witness?, Amrozi accused his brother of deliberately distorting his evidence.Sentence pair 2 (not semantically equivalent):1.
Yucaipa owned Dominick?s before selling the chain to Safeway in 1998 for $2.5 billion.2.
Yucaipa bought Dominick?s in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.Sentence pair 3 (semantically equivalent):1.
The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.2.
PG&E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday.Figure 1: Example sentences from the Microsoft Research Paraphrase Corpus (MSRP)info is defined to be:info(ngram) = log2[count((n?
1)gram)count(ngram)]where count(ngram) is the count of ngram =w1w2 .
.
.
wn in all the reference translations, and(n?
1)gram is w1w2 .
.
.
wn?1.For NIST the brevity penalty is computed on asegment-by-segment basis and is given by:BP = exp[?
log2min[LsysLref, 1]]where Lsys is the length of the MT systemoutput, Lref is the average number of words ina reference translation and ?
is chosen to makeBP = 0.5 when LsysLref =23 .We ran experiments for N = 1...5, these arereferred to as NIST1 to NIST5 respectively.
Weinclude the brevity penalty in the scores used forour experiments.2.5 Introducing Part of Speech InformationEarly experiments based on the PER score re-vealed that removing certain classes of functionwords from the edit distance calculation had apositive impact on classification performance.
In-stead of simply removing these words, we cre-ated a mechanism that would allow the classifierto learn for itself the usefulness of various classesof word.
For example, one would expect edits in-volving nouns or verbs to cost more than edits in-volving interjections or punctuation.
We used aPOS tagger for the UPENN tag set (Marcus et al,1994) to label all the data.
We then divided thetotal edit distance, into components, one for eachPOS tag which hold the amount of edit distancethat words bearing this POS tag contributed to thetotal edit distance.
The feature vector thereforehaving one element for each UPENN POS tag.Let W?
be the bag of words from si that haveno matches in ri and let W+ be the bag of wordsfrom si that have matches in ri.
The value of thefeature vector ~f?
corresponding to the contribu-tion to the PER from POS tag t is given by:f?t =?w?W?
count?t (w)|si|where count?t (w) is the number of times wordw occurs in W?
with tag t.The feature vector defined above characterizesthe nature of the words in the sentences that donot match.
However it might also be important toinclude information on the words in the sentencethat match.
To investigate this, we augment thefeature vector ~f?
with an analogous set of fea-tures ~f+ (again one for each UPENN POS tag)that represent the distribution over the tag set ofword unigram precision, given by:f+t =?w?W+ count+t (w)|si|where count+t (w) is the number of times wordw occurs in W+ with tag t.This technique is analogous to the NIST scorein that it allows the classifier to weight the impor-tance of matches, but differs in that this weight islearned rather than defined, and is with respect tothe word?s grammatical/semantic role rather thanas a function of rarity.
When both ~f+ and ~f?
are19MSRP PASCAL CD IE MT QA RC PP IRSentence1 length 21.6 27.8 24.0 27.4 36.7 31.5 27.9 24.0 24.6Sentence2 length 21.6 11.6 16.1 8.4 19.2 8.7 10.2 11.2 7.2Length difference ratio 0.14 0.54 0.32 0.66 0.46 0.68 0.60 0.46 0.66Edit distance 11.3 22.0 18.2 22.2 28.1 26.8 21.8 17.3 21.0Table 1: Corpus statistics (columns CD-IR are sub-tasks of PASCAL), ?length difference ratio?
isexplained in Section 3, ?edit distance?
is the average Levenstein distance between the sentences of thepairsused in combination the method differs again byutilizing information about the nature of both thematching words and the non-matching words.We will refer to the system based only on thefeature vector ~f?
as POS- , that based only on~f+ as POS+ and that based on both as POS.2.6 Dealing with SynonymsOften in paraphrases the semantic informationcarried by a word in one sentence is conveyed bya synonymous word in its paraphrase.
To coverthese cases we investigated the effect of allow-ing words to match with synonyms in the editdistance calculations.
Another pilot experimentwas run with a modified edit distance that al-lowed words in the sentences to match if theirsemantic distance was less than a specific thresh-old (chosen by visual inspection of the output ofthe system).
The semantic distance measure weused was that of (Jiang and Conrath, 1997) de-fined using the relationships between words in theWordNet database (Fellbaum, 1998).
A perfor-mance improvement of approximately 0.6% wasachieved on the semantic equivalence task usingthe strategy.3 Experimental DataTwo corpora were used for the experiments in thispaper: the Microsoft Research Paraphrase Corpus(MSRP) and the PASCAL Challenge?s entailmentrecognition corpus (PASCAL).
Corpus statisticsfor these corpora (after pre-processing) are pre-sented in Table 1.The MSRP corpus consists of 5801 sentencepairs drawn from a corpus of news articles fromthe internet.
The sentences were annotated by hu-man annotators with labels indicating whether ornot the two sentences are close enough in mean-ing to be close paraphrases.
Multiple annotatorswere used to annotate each sentence: two anno-tators labeled the data and a third resolved thecases where they disagreed.
The average inter-annotator agreement on this task was 83%, indi-cating the difficulty in defining the task and theambiguity of the labeling.
Approximately 67% ofthe sentences were judged to be paraphrases.
Thedata was divided randomly into 4076 training sen-tences and 1725 test sentences.
For full details ofhow the corpus was collected we refer the readerto the corpus documentation.
To give an idea ofthe nature of the data and the difficulty of the task,three sentences from the corpus are shown in Fig-ure 1.
The example sentences show the ambigu-ity inherent in this task.
The first sentence pairis clearly a pair of paraphrases.
The second pairof sentences share semantic information, but werejudged to be not semantically equivalent.
Thethird pair are not paraphrases, they are clearly de-scribing the movements of totally different stocks,but the sentences share sufficient semantic con-tent to be labeled equivalent.For the MSRP corpus we present results usingthe provided training and test sets to allow com-parison with our results.
To obtain more accuratefigures and to get an estimate of the confidenceintervals we also conducted experiments by 10-fold jackknifing over all the data.
The results fromeach fold were then averaged and 95% confidenceintervals were estimated for the means.The PASCAL data consists of 567 developmentsentences and 800 test sentences drawn from 7domains: comparable document (CD), informa-tion extraction (IE), machine translation (MT),question answering (QA), reading comprehension(RC), paraphrasing (PP) and information retrieval(IR).
A full description of this corpus is given in20the/DT cat/NN sat/VBD on/IN the/DT mat/NNthe/DT dog/NN sat/VBD on/IN the/DT mat/NNDT2/6NN1/6VBD1/6IN1/6DT0/6NN1/6VBD0/6IN0/6Matches Non-matchesSentence 1:Sentence 2:Feature Vector = (0.33, 0.16, 0.16, 0.16, 0, 0.16, 0, 0):Figure 2: Example of a POS feature vector.
The sentences are presented in word/TAG format, and thefeature vector is labeled with these POS tags (in the upper part of the squares)the corpus documentation 1.
The data differs fromthe MSRP corpus in that it is annotated for en-tailment rather than semantic equivalence.
Thisexplains the asymmetry in the sentence lengths,which is apparent even in the PP component ofthe corpus.
We do not present results for 10-foldjackknifing on the PASCAL data since the datawere too small in number for this type of analy-sis.In Table 1 ?Sentence 1?
refers to the first sen-tence of a sentence pair in the corpus, and ?Sen-tence 2?
the second.
The length distance ratio(LDR) is defined to be the average over the corpusof:LDR(si, ri) =||si| ?
|ri||max(|si|, |ri|)This measures the similarity of the lengths ofthe sentences in the pairs, it has the property ofbeing 0 when all sentence pairs have sentences ofthe same length and 1 when all sentence pairs dif-fer maximally in length.
For the PASCAL corpusthe LDR is around 0.5 for the corpus as a whole,corresponding to a large difference in the sentencelengths.
The CD component of the corpus beingconsiderably more consistent in terms of sentencelength.
The differences among the tasks in termsof edit distance are less clear-cut, with the PP taskhaving the lowest average edit distance despite itshigher LDR.
The MSRP corpus has an LDR ofonly 0.14.
The sentences pairs are more similar interms of their length and edit distance than thosein the PASCAL corpus.
We will argue later thatthis length similarity has a significant effect onthe performance and applicability of these tech-niques.1http://www.pascal-network.org/Challenges/RTE/4 Experimental Methodology4.1 TokenizationIn order that the sentences could be tagged withUPENN tags (Marcus et al, 1994), they were pre-processed by a tokenizer.
After tokenization theaverage MSRP sentence length was 21 words.4.2 StemmingStemming conflates morphologically relatedwords to the same root and has been shown tohave a beneficial effect on IR tasks (Krovetz,1993).
A pilot experiment showed that theperformance of a PER-based system degraded ifthe stemmed form of the word was used in placeof the surface form.
However, if the stemmer wasapplied only to words labeled by a POS taggeras verbs and nouns, a performance improvementof around 0.8% was observed on the semanticequivalence task.
Therefore, for the purposesof the experiments, the nouns and verbs in thesentences were all pre-processed by a stemmer.4.3 ClassificationWe used a support vector machine (SVM) clas-sifier (Vapnik, 1995) with radial basis functionkernels to classify the data.
The training sets forthe respective corpora were used for training, ex-cept in the jackknifing experiments.
Feature vec-tors (an example is given in Figure 2) were con-structed directly from the output of the MT evalu-ation systems, when used.
The vector has 2 parts,one due to matches and one due to non-matches.The sum of the elements corresponding to non-matches is equal to the PER.
We calculated thevectors for each sentence in the pair as both ref-erence and system output and averaged to get thevector for the pair.215 Results5.1 MSRP CorpusThe results for the jackknifing experiments areshown in Table 2 and the results using the pro-vided training and test sets are shown in Table 3.In the tables the rows labeled ?PER POS+?, re-fer to models built using feature vectors made bycombining both the PER and POS+ feature vec-tors.
The rows labeled POS refer to models builtfrom the combination of features from the POS+and POS- models.
The rows labeled ALL referto models built from combining all of the featuresused in these experiments.The results show that decomposing the PERedit distance score into components for each POStag is not able to better the classification perfor-mance of PER.
The accuracy (jackknifing) forPER alone was 71.25% and the accuracy for theanalogous technique which divides this informa-tion in contributions for each POS tag (POS-) was70.99%.
However, when the features from PERand POS- are combined there is an improvementin performance (to 72.71%) indicating that thecomponents for each POS tag are useful, but onlyin addition to the more primitive feature encod-ing the total edit distance.
Moreover, comparingthe results from POS-, POS+ and POS it is clearthat there lot to be gained by considering the con-tributions from both the matching words and thenon-matching words.
Using both together gives aclassification performance of 74.2% whereas us-ing either component in isolation can give a per-formance no better than 71.5%.The one of the worst performing systems wasthat based on the WER score.
However, it ispossible that the way the sentences were selectedhandicapped this system, since only sentencespairs with a word-based Levenshtein distance of 8or higher were included in the corpus.
Choosingsentence pairs with larger edit distances makeslarge structural differences more likely, and theediting effort needed to correct such structural dif-ferences may obscure the lexical comparison thatthis score relies upon.The results for the BLEU score were unex-pected because the performance degrades as theorder of n-gram considered increases.
This effectis much less apparent in the NIST scores wherethe performance degrades but to a lesser extent.Paraphrases exhibit variety in their grammaticalstructure and perhaps changes in word orderingcan explain this effect.
If so, the geometric meanemployed in the BLEU score would make the ef-fect of higher order n-grams considerably moredetrimental than with the arithmetic mean used inthe NIST score.5.2 PASCAL Challenge CorpusThe results for the PASCAL corpus are given inTable 4.
As expected our results are consistentwith those of (Perez and Alfonseca, 2005).
The5% overall gain in accuracy may be accountedfor by the stemming and synonym extensions toour technique and the fact that we used BLEU1.Our approach also differs by being symmetricalover source and reference sentences, however itis not clear whether this would improve perfor-mance.
The number of test examples for thesub-experiments for each task is low (50 to 150),therefore the results here are likely to be noisy,but it is apparent from our results that the CDtask is the most suitable for approaches based onword/n-gram matching.
Our POS technique per-formed well on overall and particularly well onthe CD andMT tasks, but the overall performanceimprovement relative to the other techniques isnot as clear-cut.
We believe this is due to difficul-ties arising from the asymmetrical nature of thedata, and we explore this in the next section.5.3 Sentence length similarityIn this experiment we investigate whether there isany advantage to be gained by using these tech-niques on corpora consisting of sentence pairs ofsimilar length.
Both the BLEU and NIST scoresuse some form of count of the total number ofn-grams in the denominator of their n-gram pre-cision formulae.
When the sentences differ inlength, the total number of n-grams is likely tobe large in relation to the number of matching n-grams since this is bounded by the number of n-grams in the shorter sentence.
This may result inan increase in the ?noise?
in the score due to vari-ations in sentence length similarity, degrading itseffectiveness.
To address the more general issueof whether sentence length similarity has an im-pact on the effectiveness of these techniques we22Accuracy Precision Recall F-measure?95% conf.
?95% conf.
?95% conf.
?95% conf.WER 68.80?0.90 69.89?1.08 94.20?0.99 80.22?0.69PER 71.25?1.03 72.05?1.23 93.58?0.59 81.39?0.72POS- 70.99?1.16 72.07?1.43 92.99?1.52 81.15?0.79PER POS- 72.71?1.34 73.99?1.47 91.67?0.53 81.86?0.97POS+ 71.56?0.99 72.51?1.20 93.02?1.50 81.46?0.74POS 74.18?0.94 75.52?1.16 91.13?0.59 82.58?0.76BLEU1 72.30?1.10 73.71?1.30 91.41?0.70 81.59?0.83BLEU2 70.26?1.37 71.55?1.46 92.65?0.66 80.72?0.95BLEU3 68.30?1.42 69.40?1.25 94.54?0.87 80.03?0.97BLEU4 67.64?1.22 68.46?1.13 96.18?0.67 79.97?0.86NIST1 71.78?1.44 73.95?1.55 89.65?1.06 81.02?1.04NIST2 71.64?1.12 73.64?1.43 90.13?0.25 81.03?0.81NIST3 71.59?1.17 72.94?1.36 91.82?0.39 81.28?0.87NIST4 71.56?1.17 72.82?1.35 92.08?0.38 81.30?0.87NIST5 71.52?1.14 72.75?1.33 92.18?0.45 81.30?0.85ALL 75.35?1.13 77.35?1.10 89.54?0.90 82.99?0.89Table 2: Experimental Results (10-fold Jackknifing)Accuracy Precision Recall F-measureWER 68.29 69.35 93.72 79.71PER 71.88 72.30 93.55 81.56POS- 70.96 72.09 91.89 80.79PER POS- 73.33 74.14 91.98 82.10POS+ 70.96 72.09 91.89 80.79POS 74.20 75.29 91.11 82.45BLEU1 73.22 74.17 91.63 81.98BLEU2 70.96 71.62 93.29 81.03BLEU3 68.93 69.45 95.12 80.28BLEU4 67.88 68.13 97.12 80.08NIST1 72.35 73.83 90.50 81.32NIST2 71.59 73.09 90.67 80.94NIST3 71.01 72.17 91.80 80.81NIST4 70.96 72.09 91.89 80.79NIST5 70.75 71.89 91.67 80.58ALL 74.96 76.58 89.80 82.66Table 3: Experimental Results (Microsoft?s Provided Train and Test Set)sorted the sentences pairs of the MSRP corpusaccording to the length difference ratio (LDR) de-fined in Section 3, and partitioned the sorted cor-pus into two: low and high LDR.We then selectedas many sentences as possible from the corpussuch that the training and test sets for each dataset (high and low LDR) contained the same num-ber positive and negative examples.
This gave twosets (high and low LDR) of 1008 training exam-ples and 438 test examples, all training and testdata consisiting of 50% positive and 50% nega-tive examples.
The results are shown in Table 5.The experimental results validate our concerns.
Inall of the cases the performance was higher onthe data with low LDR.
Moreover, the effect wasmost for the BLEU and NIST scores for which wehave an explanation of the cause.6 ConclusionWe have shown that it is possible to derive fea-tures that can be used to determine whether sim-ilar sentences are paraphrases of each other frommethods currently being used to automaticallyevaluate machine translation systems.
The ex-periments also show that using features that en-code the distribution over the POS tag set of bothmatching words and non-matching words can sig-nificantly enhance the performance of a PER-based system on this task.23Task BLEU1 NIST1 PER POS ALLCD 74.67 76.67 73.33 79.33 82.00IE 49.17 50.00 48.33 42.50 44.17IR 47.78 45.56 41.11 37.78 40.00MT 39.17 52.50 69.17 65.83 61.67PP 56.00 44.00 58.00 44.00 38.00QA 56.15 53.08 56.92 53.08 55.38RC 52.86 53.57 48.57 57.14 55.00ALL 54.50 55.63 57.37 56.75 56.75Table 4: Accurracy Results (PASCAL Train and PASCAL Test Set)BLEU1 NIST1 PER POS ALLLow LDR 76.71 77.85 72.15 75.80 76.48High LDR 68.49 70.09 69.63 72.83 73.52Table 5: Accuracy Results Length Similarity (MSRP)This research begs the important question ?Isthere any correlation between performance on thesemantic equivalence classification task and per-formance of the underlying evaluation techniqueon the task of MT evaluation??.
Intuitively atleast, there certainly should be.
If there is, it maybe possible to use the task of classifying sentencesfor semantic equivalence as a proxy for the com-plex and time-consuming task of evaluating eval-uation schemes by correlating automatic scoreswith human scores during the development pro-cess of MT evaluation techniques.
In future workwe look forward to addressing this question, aswell as incorporating new features into the mod-els to increase their potency.7 AcknowledgmentsThe research reported here was supported in partby a contract with the National Institute of Infor-mation and Communications Technology entitled?A study of speech dialogue translation technol-ogy based on a large corpus?.ReferencesJ.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,C.
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.2004.
Confidence estimation for machine transla-tion.
Technical report, Final report JHU / CLSP2003 Summer Workshop, Baltimore.G.
Doddington.
2002.
Automatic Evaluation ofMachine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the HLTConference, San Diego, California.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Jay J. Jiang and David W. Conrath.
1997.
Seman-tic similarity based on corpus statistics and lexicaltaxonomy.
CoRR, 9709008.Milen Kouylekov and Bernardo Magnini.
2005.Recognizing textual entailment with tree edit dis-tance algorithms.
In Proceedings PASCAL Chal-lenges Worshop on Recognising Textual Entailment,Southampton, UK.Robert Krovetz.
1993.
Viewing morphology as aninference process.
Technical Report UM-CS-1993-036, University of Mass-Amherst, April.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.Bleu: a Method for Automatic Evaluation of Ma-chine Translation.
IBM Research Report rc22176(w0109022), Thomas J. Watson Research Center.Diana Perez and Enrique Alfonseca.
2005.
Appli-cation of the bleu algroithm for recognising tex-tual entailments.
In Proceedings PASCAL Chal-lenges Worshop on Recognising Textual Entailment,Southampton, UK.K.Y.
Su, M.W.
Wu, and J.S.
Chang.
1992.
A newquantitative quality measure for machine transla-tion systems.
In Proceedings of COLING-92, pages433?439, Nantes, France.C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, andH.
Sawaf.
1997.
Accelerated dp based searchfor statistical translation.
In Proceedings ofEurospeech-97, pages 2667?2670, Rhodes, Greece.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc.,New York, NY, USA.24
