Natural Language Processing:What's Really Involved?Roger  Schank  and A lex KassYale Un ivers i ty  Depar tment  of Computer  Sc ienceBox 2158 Yale S ta t ion ,  New Haven,  CT  06520I n t roduct ionThe question before us is whether or not NLP has gone anywhere since the lastTINLAP.
The answer depends trongly on what we take NLP to be.
If we acceptthe common assumption that language processing can be isolated from the rest ofcognition, and that the goal of NLP research to study language as a formal system in-dependent ofsuch (admittedly difficult) issues as memory, learning, and explanation,then our conclusion is grim.
Not only has this approach failed to make significantprogress in the eight years since TINLAP2, it is permanently doomed to failure be-cause it misses the point of what language is all about.
It is only possible for researchon understanding atural language to make progress when the researchers ealize thatthe heart of NLU is the understanding process, not language per se.Language is so fascinating because it can be a vehicle for communicating suchfascinating content.
The proper focus of NLP research is on the content, and how itis extracted from language.
As we all know by now, this is a difficult problem becausemuch of what is communicated by natural anguage is not explicitly stated.
Usersof natural anguages rely on the fact that they can assume a tremendous amount ofshared knowledge to help resolve ambiguities, determine anaphoric reference, fill inellipsis, etc.
The two fundamental problems we must solve in order to get computersto approach uman levels of language underStanding are, first, to endow them withthe kind of knowledge bases that humans have and second, to program them to usethat knowledge to guide the understanding process.NLP researchers must, therefore, address uch questions as how we understandand represent the concepts that language can communicate, how we learn new con-cepts, and how we organize this knowledge in memory so that it will be available111to guide the understanding process in the future.
A science of NLP built aroundquestions uch as these can  make progress, and in fact, has been making progress inour lab and elsewhere.Our Evolv ing V iew of the Understand ing ProcessWe have been working on programs that understand natural anguage text for manyyears, now in our laboratory at Yale, and at the Stanford AI lab before that.
Butwithin that context, our focus has shifted considerably because our conception ofwhat it means to understand has changed drastically.
It is this evolution of ournotion of what constitutes understanding that represents the real progress we havemade.We started out working on Conceptual Dependency (CD) theory \[Schank 75\],which was a theory of language-free r presentation f the content of natural anguagesentences.
This led to the development of progra.m.~ that could map from language toCD (parsers) \[Riesbeck 75\] and back to language again, (generators) \[Goldman 75\].Understanding meant getting the mapping to CD right, as demonstrated by theability of the generator to produce a correct paraphrase of the original input.Of course, much of what someone who is paraphrasing or translating must under-stand from the input is inferred rather than stated, so we were motivated to developa theory of inference.
Our first theory of inference was quite simple.
We attachedvarious types of inferences to the types of CD forms.
Each time the parser produceda CD form, the associated inferences would be fired off, producing more CD formswhich would fire off more inferences, etc.
(See \[Rieger 75\]).So now our theory of understanding included a theory of inference, which was,in a sense, a theory of context, but the context didn't really drive the understand-ing process.
While reading a sentence in a text, our programs (unlike people) didnot develop expectations about what the following sentences would say.
The lack oftop-down guidance made the inference process to unconstrained; irrelevant inferencesoverwhelmed the relevant ones.
In response ot this problem we developed a theoryof scripts and plans as memory structures that would provide top-down expecta-tions to the parser.
Our view of understanding now broadened: rather than simplymapping sentences to an internal representation we were now interested in findinga larger memory structure into which the representation would fit.
Script-basedunderstanders developed in our lab included SAM \[Cullingford 78\] and FRUMP\[DeJong 77\].The success of the script-based approach led us to focus on memory issues.
Twointer-related issues not adequately addressed by script-applier projects uch as SAM112and FRUMP were how scripts are organized in memory and how they are learneddynamically.
Since a person cannot truly be said to understand if he doesn't learnfrom his experiences, it seems odd that so many NLP researchers seem comfortablewith the notion that language understanding should be studied independent of issuesof memory modification.
We began closing this gap by proposing an organizationalscheme (called Memory Organization Packets, or MOPs \[Schank 81\]) by which mem-ory structures could be interconnected.
We began to view understanding asa processof becoming reminded of the appropriate prior episodes and building new generaliza-tions that could account for both the old and the new.
The CYRUS \[Kolodner 80\]and IPP \[Lebowitz 80\] projects represented the first cuts at implementing this theory.At this point it should be clear that the trend in our research as been steadilyaway from viewing language understanding as an encapsulated, ata-driven processof mapping input strings to internal representations.
The more we worked withlanguage-understanding systems in our lab, the clearer it became how active andmemory-driven the language understanding process was.
The understander's id-iosyncratic set of prior experiences, and the way these are organized, have a powerfuleffect on the meaning of any input to that understander: Different people understandthe same thing differently.This insight led us naturally to our current work, which is on question-asking andexplanation.
Stories that don't relate to an$ previously understood experience arecryptic, while stories that contain only things that we expect are boring and teachus nothing.
The truly interesting stories - -  the ones we find ourselves wonderingover - -  are the ones which are close enough to things we understand to be able tointeract with our prior experiences, but which cannot quite be explained by any ofthe ready-made xplanations sitting in our memory.
The problem with programsthat relied completely on applying pre-established knowledge structures is that theyignored anomalies in the stories instead of realizing that the anomalies are the mostinteresting part.Creative thinking is required in order to develop explanations that can be usedto understand novel stories.
But we believe that this kind of creativity can be accom-plished by a fairly simple, mechanical process.
Understanding these stories involvesretrieving explanations that have worked in the past from memory, revising thoseexplanations to make them applicable to the current story, and finally storing themodifications back in memory so that the understander will have learned a new expla-nation by the time the story has been understood.
The creativity stems from findingan interesting, relevant explanation i  memory (creative indexing) and knowing howto modifiy the explanation to make it fit the problem at hand (creative tweaking).For example, suppose you read that Swale, a star 3-year-old racehorse, was founddead in his stall a few days after a big race.
If you are not an avid racing fan you areunlikely to have ready-made explanations for racehorse deaths.
However, if you thinki13creatively about the problem you may be reminded of any number of relevant casesyou know about and you can probably modify the associated explanations to makethem fit Swale.
For example, if you think of Swale as an athlete who died despitebeing in good condition you might be reminded of the jogger, Jim Fixx, who had ahidden heart defect which combined with the stress of jogging to kill him.
PerhapsSwale had a hidden defect, and perhaps running in races was for Swale what joggingwas for Fixx.
If you think of Swale as a star performer who died before his timeyou may be reminded of Janis Joplin or John Belushi.
While Swale probably wasn'ttaking recreational drugs, this explanation can lead to drug-related explanations thatare more plausible.
Perhaps Swale was poisoned by a competitor, or maybe his ownerwas giving him drugs to make him run faster and accidentally gave him too much.The two tricks to generating hypotheses in this way are, first, to characterise theproblem in a way that makes the old explanations come to mind, and second, totweak the old explanations into something that fits the new situation.A full presentation of our current work is clearly impossible in this paper (see\[Schank 86\] for a theoretical discussion and \[Kass 86\] and \[Leake and Owens 86\] forbrief discussions of a program built around these principles); the goal here is simplyto point out how our interest in natural anguage processing has led us naturally,and indeed inevitably, to develop theories of explanation and creativity.
Some maysay that we have strayed from the core issues of NLP, but our point is that these arethe core issues.
The drive to explain what might be "going on in the story guides theentire understanding process.ConclusionSo, our answer to the original question is that the study of NLP in terms o/ theoverall understanding process is making very good progress.
It's not so much that wehave developed solutions to all our problems, although we certainly have developedsome solutions.
The point is that we are starting to understand what the problemis, and this is much more important.AcknowledgementsThanks to Larry Birnbaum, David Leake, Chris Owens, and Ashwin Ram for theirhelpful comments.
Our work is supported in part by the Air Force Office of ScientificResearch under grant 85-0343.114References\[Cullingford 78\] Cullingford, R., Script Application: Computer Understanding ofNewspaper Stories, Ph.D. Thesis, Yale University, 1978.
Research Report#116.\[DeJong 77\] DeJong, G.F., Skimming newspaper stories by computer, Technical Re-port 104, Yale University Department of Computer Science, 1977.\[Goldman 75\] Goldman, N., Conceptual Generation, Conceptual Information Pro-cessing, North-Holland, Amsterdam, 1975.\[Kass 86\] Kass, A., Modifying Explanations toUnderstand Stories, Proceedings oftheEighth Annual Conference of the Cognitive Science Society, Cognitive ScienceSociety, Amherst, MA, August 1986.\[Kolodner 80\] Kolodner, J.L., Retrieval and Organizational Strategies in ConceptualMemory: AComputer Model, Ph.D. Thesis, Yale University, November 1980.\[Leake and Owens 86\] Leake, D. B. and Owens, C. C., Oragnizing Memory for Expla-nation, Proceedings of the Eighth Annual Conference of the Cognitive ScienceSociety, Cognitive Science Society, Lawrence Erlbaum Associates, 1986.\[Lebowitz 80\] Lebowitz, M., Generalization and Memory in an Integrated Under-standing System, Ph.D. Thesis, Yale University, October 1980.\[Rieger 75\] Rieger, C., Conceptual Memory and Inference, Conceptual InformationProcessing, North-Holland, Amsterdam, 1975.\[Riesbeck 75\] Riesbeck, C., Conceptual Analysis, Conceptual Information Process-ing, North-Holland, Amsterdam, 1975.\[Schank 75\] Schank, R.C., Fundamental Studies in Computer Science, Volume 3:Conceptual Information Processing, North-Holland, Amsterdam, 1975.\[Schank 81\] Schank, R.C., Reading and Understanding: Teaching from the Perspec-tive of Artificial Intelligence, Lawrence Erlbaum Associates, Hillsdale, N J,1981.\[Schank 86\] Schank, R.C., Explanation Patterns: Understanding Mechanically andCreatively, 1986.
Book in press.115
