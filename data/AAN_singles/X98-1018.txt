DYNAMIC DATA FUSIONTed DiamondEl izabeth D. LiddvTextWise  LLC2 -212  Center  for  Sc ience  and Techno logySyracuse ,  NY  13244liz@textwise.com, ted@textwise.comPhone: (315) 443-1989INTRODUCTIONInformation retrieval researchers have longappreciated the value of combining, or ffilsing,multiple retrieval systems' relevance scores for a setof documents to improve retrieval performance.However, it is only recently that researchers havebegun to consider adjusting the score fusion methodto the user's topic and initial results.
This studyexplores the value of fusing multiple retrievalsystems' scores in a manner that adjusts to: thesemantic and syntactic features of the user's naturallanguage query, the various systems' biases towardlong or short documents, and the extent to which thescores produced by the multiple systems arestatistically independent.PREVIOUS WORKThe ability to improve retrieval performance byusing multiple retrieval systems has beendocumented extensively (e.g,, \[1\], \[31, \[41).
It is onlyrecently, however, that researchers have turned theirattention to the possibility of adjusting the manner inwhich results are combined to the specific query athand.
Researchers have reported success in usinginitial relevance judgments to adjust the way in whichresults are combined \[3\], and have also reportedsuccess in using the joint distribution of relevancescores from multiple marchers (among other things)to predict when to combine the results of multiplesystems \[6\].
The purpose of the current research is toexplore the use of the joint distribution of relevancescores, semantic and syntactic features of queries.and the length of retrieved ocuments to predict howto combine the results of several retrieval systems.DEFINITIONS AND RESEARCH QUESTIONSWe define a q,ery as a natural languageexpression of a user's need.
For sonic query andsome collection of documents, it is possible for ahuman to attribute the relevance of the document othe query.
A retrieral system is a machine thataccepts a query and full texts of documents, andproduces, for each document, a relevance score forthe query-document pair.
A measure of theeffectiveness of a retrieval system for a query and acollection is precision, the proportion of the Ndocuments with the highest relevance scores that arerelevant (in our study, N is 5, 10, or 30).Using multiple retrieval systems producesmultiple retrieval scores for a query-document pair.A fitsion fimction accepts these scores as its inputs,and produces a single relevance score as its output forthe query-document pair.
A staticfiisionfimction hasonly the relevance scores for a single query-documentpair as its inputs.
A dynamic filsion fimction canhave more inputs.We are concerned with the following twoquestions:If we allow each query its own static fusionfunction, can we achieve higher precision than ifwe force all queries to have the same staticfusion function?.
If we can achieve higher precision by allowingeach query its own static fusion function, thenwhat inputs or tkmtures would enable us toconstruct a dynamic fusion function that adjuststo the query, the documents retrieved by theretrieval systems, and the distribution of scoresproduced by the retrieval systems'?THE DATAQueries, Documents, and RelevanceJudgmentsWe used 247 queries, including TREC I-6training queries, and queries developed bv businessanalysts for TcxtWisc's internal use.
We appticdthese queries to the TREC Wall Street Journalcollection fronl (1986-1992).
For the TREC queries.we used only TREC relevance judgments.
Relevance123judgments for the TextWise queries were initiallymade on a 5-point scale, which we mapped to thebinary judgments used by TREC.Several of the retrieval systems described belowused a document segmentation scheme to splitcompound documents into their components,resulting in a collection size of 222,525.
For thesesystems, retrieval scores were calculated separatelyfor the components of compound documents, andthen merged by taking the maximum componentscore, thus mapping back to the original documentspace of 173,252.Retrieval SystemsWe used five retrieval systems to generaterelevance scores for query-document pairs:Fuzzy Boolean (FB).
This system translates a queryinto a Boolean expression in which the terminals aresingle terms, compound nominals, and proper nouns;instantiates the terminals in the expression with thedocument's tfidfweights; and applies fuzzy Booleansemantics to resolve the instantiated expression into ascalar elevance score.Probabilistic (PRB).
This system applies a matchformula that sums term frequencies of query terms inthe document, weighted by terms' inverse documentfrequencies, and adjusts tor document length.
Weapplied this formula to a vocabulary of single terms.Subiect Field Code (SFC).
This system applies avector similarity metric to query and documentrepresentations in TextWise's Subject Field Codespace to obtain relevance scores.N-gram (NG3).
This system applies a vectorsimilarity metric to query and documentrepresentations obtained by counting the occurrencesof 3-letter sequences (after squeezing out blanks,newlines, and other non-alphabetic characters I.Latent Semantic Indexing (LSI).
This system obtainsquery and document representations by applying atranslation matrix to single terms (excludingcompound nominals and proper nouns).
We obtainedthe translation matrix by singular valuedecomposition of a matrix of (.
idf weights for singleterms from a 1/3 sample of the Wall Street Journal.We used a vector similarity metric to obtainrelevance scores.Query and Document RepresentationsWe used the following procedures to process thequeries and documents into tbrms that enabledapplication of matching formulae to producerelevance scores:Document Segmentation.
We used either theoriginal document segmentation from the TREC dataor a more aggressive segmentation that splitcompound ocuments into their components.Stop Word Removal.
For all but one retrievalsystem, we removed stopwords.Stemming.
For the various retrieval systems, weused the Xerox stemmer, the Stone stemmer, or weobtained word roots as a byproduct of constructingtrigrams.Phrase Reco,~nition.
For some retrieval systems, weused a set of part-of-speech-based rules to detect andaggregate sequences of tokens into compoundnominal phrases.Proper Nouns.
For some retrieval systems, wedetected proper nouns, and normalized multipleexpressions of the same proper noun entity to acanonical form.Term Weit~htin~.
In documents, weights representedthe frequency of terms in the document, conditionedby the number of documents in which the termsTable 1Features of Retrieval SystcmsFEATURE FB PRBDoc.
SegmentationStop Word RemovalStemmingPhrase RecognitionProper NounsTcrm WeightingDimension ReductionAggressiveYesXeroxYesYesq: i,!fNoneAggressiveYesMatch SemanticsXeroxNoYesq: it!tRetrieval SystemsAggressivcYesSFC NG3 LSIStandardStoneNoNone SFCFuzzv Boolean Probabilistic VectorNoTrigramNoAgeressiveYesNoneXeroxNoNo No Notf {f i~!f t t: idfLSIVector Vector124appeared ( tf id~.Dimension Reduction.
We used single words totranslate into weightings in a 900-dimensional featurespace using TextWise's Subject Field Coder (SFC),or into a 167-dimensional feature space using LatentSemantic Indexing (LSI).Table 1 summarizes the query representations,document representations, and matching semanticsused by the five matchers.Dynamic Fusion Function Input FeaturesIn addition to the five relevance score inputs tothe dynamic fusion function, we used the followinginputs:Query FeaturesSeveral items of information might be availableabout the query independently of any particularretrieval approach or its representation f the query,the documents, or their similarity:Query Length (QLEN).
The number of tokens in thenatural anguage query.Query Terms' Specificity (QTSP).
The averageinverse document frequency (IDF) of the quartile ofthe query's terms with the highest IDF's.Number of Proper Nouns (QNPN).Number of Compound Nominals (QNCN).Query Terms" Synonymy (QTSY).
Over all terms inthe query, the average of the number of words in thesvnset for the correct sense of the query term inWordNet.
WordNet is a semantic knowledge basethat distinguishes words by their senses, and groupsword:senses that are synonymous to each other intosynsets.Query Terms' Polyscmv (QTPL), Over all terms inquery, the average number of senses for the queryterm in WordNet,Document FeaturesThere is currently one document feature,instantiated separately for each query, for eachretrieval system S:Length of Top-Ranked Documents Retrieved bySystem (DLEN\[S\]).
This is the average of thenumber of tokens in the top 5 documents cored bysystem S.Score DistributionsThe following features are instantiated once foreach retrieval system S, for each query:Maximum Score Assigned by Approach (SMAX\[S\]).Variance o f  Scores Assigned by Approach(SVAR\[S1).The lbllowing input to the dynamic fusionfunction is instantiated once for each pair of retrievalsystems S~ and S,:Correlation of Ranks Assi,~ned to Documents by TwoApproaches (SCOR\[St, Sz\]..~ For documents rankedin the top 1,000 by any of the retrieval systems forthe query, the correlation of the documents' ranks insystems S~ and $2.RESEARCH QUESTION 1: OPPORTUNITYFOR IMPROVING RETRIEVALFor a sample of 50 queries from our 297, welbund, separately for each query, an optimal staticfusion function.
We then lound the single optimalstatic fusion function that gave the best precisionover all 50 queries.
Table 2 shows the precision forthe 50 queries using the 5 retrieval systems?
Table 2.Precision of Five Systems, Overall Static Fusion Functions.and Query-Specific Fusion Functions,When Training and Testing on Same Data lor Each QuerySingle Retrieval Systems Static Fusion FunctionsSingle Overall Query-SpecificPrec.
at FB SFC PROB NG3 LSI (vs. FB) (vs. overall)5 .3360 .0080 .2080 .1760 .1640 .3840 (+14%) .5960 (+55%7I0 .2680 .0060 .1800 .1560 .1440 .3280 (+22%7 .5040 (+54%)30 .2240 .
(7127 I .I 193 .14\[4 .1273 .2547 (+14%) .3427 (+35%)125separately, using a single overall static fusionfunction, and using 50 (possibly) different query-specific static functions.At first glance, our results suggest hat allowingquery-specific fusion functions substantiallyimproves retrieval.
For instance, by using query-specific static fusion functions, we achieved precisionat 5 of .5960, compared to .3840 when applying thesame static fusion function to all queries.
However,this comparison is overly optimistic, since it allowsquery-specific fusion functions to be trained andevaluated on exactly the same data, while forcing theoverall fusion function to be trained on a large set ofdata, but then evaluated on a small subset of thatdata.
To provide a more pessimistic omparison, wepartitioned the data for our 50 queries into equally-sized training and test sets.
We trained each query-specific fusion function on the query's training data,and evaluated it on the test data.
(Although our goalis to improve retrospective r trieval, this arrangementresembles the TREC routing scenario.)
Table 3shows a considerably weaker, but still appreciableimprovement due to using query-specific fusionfunctions.
For instance, we achieved precision at 5 of.4160 when allowing each query its own static fusionfunction, compared to .3400 when forcing all queriesto use the same function.dimensions are the relevance scores from the set ofmatchers.
We constructed a fused score for a testdocument by summing the relevance judgments forthe test document's K nearest training documents(where K was 5, 10, 15 or 20).
We tried weightingthe sums by an inverse function of the distancebetween the test document and the trainingdocument.
We also tried scaling the dimensions'contribution to the distance metric with a weightreflecting the corresponding matcher's precision.To our surprise, none of these experimentsproduced K-NN-based fusion functions thatperformed consistently better than a linear fusionfunction.
On closer inspection, it appears that at leastpart of the poor performance of K-NN as a fusionfunction can be attributed to instances in which theprobability distribution of relevance for the trainingdocuments for the query did not resemble theprobability distribution of relevance for all thedocuments in the query.
In this sort of situation, thelinear model appears to be more robust than K-NN.It may be that a more careful selection of the trainingset would result in more reasonable performancefrom K-NN-based fusion functions.For the linear fusion function, we found theoptimal vector of coefficients by selecting thecoefficients that produce the greatest precision at 5Table 3.Precision of Five Systems, Overall Static Fusion Functions,and Query-Specific Fusion Functions,When Training and Testing on Different Data for Each QuerySingle Retrieval Systems Static Fusion FunctionsSingle Overall(vs. FB).1187 .1313Prec.
at FB SFC PROB NG3 LSI5 .3360 .0040 .1960 .1920 .1600 .3400(+01%) .4160 (+22%)10 .2680 .0100 .1680 .1600 .1340 .3120(+16%) .3620(+16%)30 .1967 .0140Query-Speci tic(vs. overall).1253 .2230 (+13%) .2533 (+34%)We constrained our fusion functions to beweighted linear combinations of the five retrievalscores lk)r a query-document pair.
We considered thepossibility of more complex non-linear fusion modelsthrough exploration of K-Ncarest Neighbor (K-NN)classifiers.
(The use of K-NN tbr selecting a singleretrieval system has been documented in \[5\].
Bycontrast, we sought to use K-NN to fuse relevancescores.)
In this approach, training documents andtheir rclevance judgments populatcd a space whose(the proportion of the five top-ranked ocuments thatare relevantl.
To date, we have lk~und the optimalvector using an exhaustive search over the set ofvectors whose elements are non-negative, evenlydivisible by 0.1. and whose elements um to 1.0.
(We had tried using logistic regression to find thecoefficients, but the coefficients we found in thismanner yielded considerably lower precision thanthose we found using the exhaustive search method.
)126In sum, it appears that for our selection ofretrieval systems, there is a potential for improvingretrieval through query-specific fusion.One way to exploit this opportunity is to useinitially-retrieved documents to adjust the weights ofthe single overall static fusion function, as in \[3\].Although we tried several ways of updating fusionfunction coefficients with relevance feedback, wewere unable to exploit any of the apparent potentialto improve retrieval performance in this way.distribution of the retrieval systems' retrieval scoresfor the query enumerated above.
We are currentlyworking on building such a dynamic fusion function.Dynamic Fusion Function ArchitectureWe chose to implement he dynamic fusionfunction as a hybrid of a "mixture expert" and thestatic linear fusion models used in Research QuestionI.
The mixture expert attempts to predict the bestcoefficients to use for the linear fusion function.Figure I shows the relationship of the mixture expertQueryDocumentFigure 1Dynamic Fusion Functions+RelevanceMatchers  ScoresQuery Document"e atu re s Fe a ttt re sWeightsMixture ExpertScoreCorrelati~Feature:Fused ScoreRESEARCH QUESTION 2: THE DYNAMICFUSION FUNCTIONSo far, optimal fusion coefficients for a queryhave been determined using full knowledge of therelevance of the documents for the query.
In theretrospective retrieval setting, these relevancejudgments will not be available beforehand, and thuscannot be used to adjust the fusion model to thequery.
For the retrospective setting, we seek toconstruct a dynamic fusion function that can adjustthe way it fuses the five systems' relevance scores fora query-document pair using additional inputs.
Theseinputs include the Icatures of the query, features ofthe retrieved documents, and features of the jointto the linear fusion model and the individual retrievalsystems.Training and EvaluationWe use the remaining 197 queries lor training.For these queries, we have used all the documents tofind coefficient vectors for optimal linear staticfusion models.
These coefficient vectors constitutethe "target" outputs the mixture expert will be trainedto reproduce.We also fit a single linear static fusion function tothe 197 training queries, again using all the data fromthose queries.
The performance of this static fusionfunction on all of the documents for the 50 test127queries constitutes the baseline for the secondresearch question.
To answer this research question,we will compare the performance of the dynamicfusion function for the 50 test queries to this baseline.DISCUSSIONSo far, our results suggest hat, for our choice ofretrieval systems, there is an opportunity to improveretrieval performance by using dynamic fusionfunctions instead of using a single static fusionfunction \['or all queries.
One possible qualification tothese results is that limiting ourselves to a linear formfor the static fusion models may result in artificiallylow baseline retrieval for the single overall staticfunction.
The volatility of the K-NN technique in thecontext of our data made it difficult to say whether ornot a non-linear form for the fusion model isnecessary.Our preferred implementation of the mixtureexpert in the dynamic fusion function is a multilayerfeedforward neural network, with output nodescorresponding to the linear weights of the linearfusion function.
However, given that our real goal isto maximize precision, rather than to replicate theweights exactly, a straightforward application ofbackpropogation to train such a network to replicatethe target weights is inappropriate.
The optimallinear weights are likely to be on "plateaus" withrespect o precision, with little change in precision inresponse to large changes in linear weights.
We arecurrently investigating alternative ways of trainingthe mixture xpert in the dynamic fusion model.REFERENCES\[ I 1 Bartell, B., Cottrell, G.W., Belew, R.K.Automatic ombination of multiple ranked systems.Proceedings of the Seventeenth Annual InternationalACM-SIGIR Conterence on Research andDevelopment in Information Retrieval.
173-181,1994.\[2\] Belkin.
N., Kantor.
P., Fox, E., Shaw, J..Combining the evidence of multiple queryrepresentations forinlormation retrieval, lnlbrmationProcessing and Management 31(3), 431-448, 1995.\[3\] Fox, E., Shaw.
J.
Combination of muhiplcsearches.
In The Second Text Retrieval Conlerence(TREC-2), D. Harman (ed), NIST SpecialPublications 500-215.
Gaithersburg.
MD.
242-252,1994.\[4\] Hull.
D., Pedersen, J., Schuetze, H. Methodcombination for document filtering.
Proceedings ofthe 19th Annual Internation ACM SIGIR Conferenceon Research and Development in InformationRetrieval, Zurich, 279-288, 1996.\[5\] Savoy, J., Ndarugendawmo, M., Vrajitoru, D.Report on the TREC-4 experiment: combiningprobabilistic and vector-space schemes.
\[TREC-4WWW site\], 1996.\[6\] Vogt., C., Cottrell, G.W.
Predicting thePerformance of Linearly Combined IR Systems.Proceedings of the Twenty First Annual InternationalACM-SIGIR Conference on Research andDevelopment in Information Retrieval.
1998.128
