Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1215?1220,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsDistributed Representations of Words to GuideBootstrapped Entity ClassifiersSonal GuptaDepartment of Computer ScienceStanford Universitysonal@cs.stanford.eduChristopher D. ManningDepartment of Computer ScienceStanford Universitymanning@stanford.eduAbstractBootstrapped classifiers iteratively generalizefrom a few seed examples or prototypes toother examples of target labels.
However,sparseness of language and limited supervi-sion make the task difficult.
We address thisproblem by using distributed vector represen-tations of words to aid the generalization.
Weuse the word vectors to expand entity setsused for training classifiers in a bootstrappedpattern-based entity extraction system.
Ourexperiments show that the classifiers trainedwith the expanded sets perform better on en-tity extraction from four online forums, with30% F1improvement on one forum.
The re-sults suggest that distributed representationscan provide good directions for generalizationin a bootstrapping system.1 IntroductionBootstrapped or distantly-supervised learning is aform of semi-supervised learning, in which supervi-sion is provided by seed examples.
Supervised ma-chine learning systems, on the other hand, requirehand-labeling sufficient data to train a model, whichcan be costly and time consuming.
Bootstrappedinformation extraction (IE) has become even morepertinent with the ever-growing amount of data cou-pled with the emergence of open IE systems (Carl-son et al, 2010; Fader et al, 2011) and shared taskslike TAC-KBP.1Limited supervision provided in bootstrappedsystems, though an attractive quality, is also one of1http://www.nist.gov/tac/2014/KBPits main challenges.
When seed sets are small, noisy,or do not cover the label space, the bootstrappedclassifiers do not generalize well.We use a major guiding inspiration of deep learn-ing: we can learn a lot about syntactic and semanticsimilarities between words in an unsupervised fash-ion and capture this information in word vectors.This distributed representation can inform an induc-tive bias to generalize in a bootstrapping system.In this paper, we present a simple approach of us-ing the distributed vector representations of words toexpand training data for entity classifiers in a boot-strapped system (see Algorithm 1).
To improve thestep of learning an entity classifier, we first learnvector representation of entities using the continu-ous bag of words model (Mikolov et al, 2013a).
Wethen use kNN to expand the training set of the clas-sifier by adding unlabeled entities close to seed en-tities in the training set.
The key insight is to usethe word vector similarity indirectly by enhancingtraining data for the entity classifier.
We do not di-rectly label the unlabeled entities using the similar-ity between word vectors, which we show extractsmany noisy entities.
We show that classifiers trainedwith expanded sets of entities perform better on ex-tracting drug-and-treatment entities from four onlinehealth forums from MedHelp.22 Related WorkBootstrapping has many variants, such asself-training, co-training, and label propaga-tion.
Yarowsky?s style of self-training algo-2http://www.medhelp.org1215rithms (Yarowsky, 1995) have been shown to besuccessful at bootstrapping (Collins and Singer,1999).
Co-training (Blum and Mitchell, 1998) andits boostrapped adaptation (Collins and Singer,1999) require disjoint views of the features ofthe data.
Whitney and Sarkar (2012) proposeda modified Yarowsky algorithm that used labelpropagation on graphs, inspired by Subramanya etal.
(2010) algorithm that used a large labeled datafor domain adaptation.In this paper, we use the setting of bootstrappedpattern-based entity extraction (Riloff, 1996; Thelenand Riloff, 2002).
This can be viewed as a form ofthe Yarowsky algorithm, with pattern learning as anadditional step.
Pattern based approaches have beenwidely used for IE (Chiticariu et al, 2013; Fader etal., 2011; Etzioni et al, 2005).
Patterns are useful intwo ways: they are good features, and they identifypromising candidate entities.
Recently, Gupta andManning (2014) improved pattern scoring (Step 2in Algorithm 1) using predicted labels of unlabeledentities.
For entity scoring (Step 3), they used anaverage of feature values to predict the scores.
Weuse the same framework but focus on improving theentity classifiers.In most IE systems, including ours, word classesor word vectors are used as features in a classi-fier (Haghighi and Klein, 2006; Ratinov and Roth,2009).To the best of our knowledge, our work is the firstto use distributed representations of words to im-prove a bootstrapped system by expanding the train-ing set.3 BackgroundIn a bootstrapped pattern-based entity learning sys-tem, seed dictionaries and/or patterns provide weaksupervision to label data.
The system iterativelylearns new entities belonging to a specific label fromunlabeled text (Riloff, 1996; Collins and Singer,1999) using patterns, such as lexico-syntactic sur-face word patterns (Hearst, 1992) and dependencytree patterns (Yangarber et al, 2000).
We use lexico-syntactic surface word patterns to extract entitiesfrom unlabeled text starting with seed dictionariesfor multiple classes.
Algorithm 1 gives an overview.In this paper, we focus on improving the entity clas-sifier (Step 3) by expanding its training data usingdistributed vector representations of words.Algorithm 1 Bootstrapped Pattern-based Entity Ex-tractionGiven: Text D, labels L, seed entities El?l ?
Lwhile not-terminating-condition (e.g.
precision ishigh) dofor l ?
L do1.
Label D with El2.
Create patterns around labeled entities.Learn good patterns and use them to extractcandidate entities Cl.3.
Learn an entity classifier and classify Cl.Add new classified entities to El.Labeling known entities: The text is labeled usingthe label dictionaries, starting with the seed dictio-naries in the first iteration.Creating and Learning Patterns: Patterns are thencreated using the context around the labeled entitiesto create candidate patterns for label l. Candidatepatterns are scored using a pattern scoring measureand the top ones are added to the list of learned pat-terns for label l. In our experiments, we use a widelyused pattern scoring measure, RlogF (Riloff, 1996;Thelen and Riloff, 2002).
Top ranked patterns withscores above a certain threshold are used to extractcandidate entities Clfrom text.Learning entities: An entity classifier predicts thelabels of Cland adds the newly classified entitiesto label l?s dictionary, El.
We discard commonwords, negative entities, and those containing non-alphanumeric characters from the set.Entity Classifier We build a one-vs-all entity clas-sifier using logistic regression.
In each iteration,for label l, the entity classifier is trained by treat-ing l?s dictionary entities (seed and learned in pre-vious iterations) as positive and entities belongingto all other labels as negative.
To improve gener-alization, we also sample the unlabeled entities thatare not function words as negative.
To train with abalanced dataset, we randomly sub-sample the neg-atives such that the number of negative instances isequal to the number of positive instances.
The fea-tures for the entities are similar to Gupta and Man-ning (2014): edit distances from positive and nega-tive entities, relative frequency of the entity words1216in the seed dictionaries, word classes computed us-ing the Brown clustering algorithm (Brown et al,1992; Liang, 2005), and pattern TF-IDF score.
Thelast feature gives higher scores to entities that are ex-tracted by many learned patterns and have low fre-quency in the dataset.
In our experiments, we callthis classifier as NotExpanded.4 ApproachThe lack of labeled data to train a good entity classi-fier is one of the challenges in bootstrapped learning.We use distributed representations of words, in theform of word vectors, to guide the entity classifier byexpanding its training set.
As explained in the pre-vious section, we train a one-vs-all entity classifierin each iteration of the bootstrapped entity extrac-tion for each label.
We use unlabeled entities that aresimilar to the seed entities of the label as positive ex-amples, and use unlabeled entities that are similar toseed entities of other labels as negative examples.3To compute similarity of an unlabeled entity tothe positive entities, we find k most similar positiveentities, measured by cosine similarity between theword vectors, and average the scores.
Similarly, wecompute similarity of the unlabeled entity to the neg-ative entities.
If the entity?s positive similarity scoreis above a given threshold ?
and is higher than itsnegative similarity score, it is added to the trainingset with positive label.
We expand the negative enti-ties similarly.4An alternative to our approach is to directly la-bel the entities using the vector similarities.
Our ex-perimental results suggest that even though exploit-ing similarities between word vectors is useful forguiding the classifier by expanding the training set,it is not robust enough to use for labeling entitiesdirectly.
For example, for our development dataset,when ?
was set as 0.4, 16 out of 41 unlabeled entitiesthat were expanded into the training set as positive3We take the cautious approach of finding similar entitiesonly to the seed entities and not the learned entities.
The algo-rithm can be modified to find similar entities to learned entitiesas well.
Cautious approaches have been shown to be better forbootstrapped learning (Abney, 2004).4We tried expanding just the positive entities and just thenegative entities.
Their relative performance, though higherthan the baselines, varied between the datasets.
Thus, for con-ciseness, we present results only for expanding both positivesand negatives.entities were false positives.5Thus, labeling entitiessolely based on similarity scores resulted in lowerperformance.
A classifier, on the other hand, can useother sources of information as features to predict anentity?s label.We compute the distributed vector representationsusing the continuous bag-of-words model (Mikolovet al, 2013a; Mikolov et al, 2013b) implementedin the word2vec toolkit.6We train 200-dimensionalvector representations on a combined dataset of a2014 Wikipedia dump (1.6 billion tokens), a sam-ple of 50 million tweets from Twitter (200 mil-lion tokens), and an in-domain dataset of all Med-Help forums (400 million tokens).
We removedwords that occurred less than 20 times, resultingin a vocabulary of 89k words.
We call this datasetWiki+Twit+MedHelp.
We used the parameters sug-gested in Pennington et al (2014): negative sam-pling with 10 samples and a window size of 10.
Weran the model for 3 iterations.5 Experimental SetupWe present results on the same experimental setup,dataset, and seed lists as used in Gupta and Manning(2014).
The task is to extract drug-and-treatment(DT) entities in sentences from four forums on theMedHelp user health discussion website: 1.
Asthma,2.
Acne, 3.
Adult Type II Diabetes (called Dia-betes), and 4.
Ear Nose & Throat (called ENT).
ADT entity is defined as a pharmaceutical drug, or anytreatment or intervention mentioned that may help asymptom or a condition.
The output of all systemswere judged by the authors, following the guidelinesin (Gupta and Manning, 2014).
We used Asthma asthe development forum for parameter and thresholdtuning.
We used threshold ?
as 0.4 and use k (num-ber of nearest neighbors) as 2 when expanding theseed sets.We evaluate systems by their precision and recall.Precision is defined as the fraction of correct enti-ties among the entities extracted.
Similar to (Guptaand Manning, 2014), we present the precision andrecall curves for precision above 75% to comparesystems when they extract entities with reasonably5Increasing ?
extracted far fewer entities.
?
= 0.5 extractedonly 5 entities, all true positives, and ?
= 0.6 extracted none.6http://code.google.com/p/word2vec/1217Forum Expanded Expanded-M NotExpanded AverageAsthma 77.01 75.68 74.48 65.42Acne 73.84 75.41 71.65 65.05Diabetes 82.37 44.25 48.75 21.82ENT 80.66 80.04 77.02 59.50Table 1: Area under Precision-Recall curve for all the systems.
Expanded is our system when word vectorsare learned using the Wiki+Twit+MedHelp data and Expanded-M is when word vectors are learning usingthe MedHelp data.0.75 0.80.85 0.90.95 1  00.20.40.60.81PrecisionRecall (268.0 correct entities)ASTHMA ExpandedNotExpanded Average0.75 0.80.85 0.90.95 1  00.20.40.60.81PrecisionRecall (268.0 correct entities)ACNEExpandedNotExpanded Average0.75 0.80.85 0.90.95 1  00.20.40.60.81PrecisionRecall (268.0 correct entities)DIABETES Expanded NotExpanded Average0.75 0.80.85 0.90.95 1  00.20.40.60.81PrecisionRecall (268.0 correct entities)ENTExpandedNotExpanded AverageFigure 1: Precision vs. Recall curves of our system and the baselines for the four forums.high precision.
Recall is defined as the fraction ofcorrect entities among the total unique correct enti-ties pooled from all systems.7We calculate the areaunder the precision-recall curves (AUC-PR) to com-pare the systems.We call our system Expanded in the experiments.To compare the effects of word vectors learned us-ing different types of datasets, we also study our sys-tem when the word vectors are learned using just thein-domain MedHelp data, called Expanded-M. Wecompare against two baselines: NotExpanded as ex-plained in Section 3, and Average, in which we av-erage the feature values, similar to (Gupta and Man-7Note that calculating lower precisions or true recall is veryhard to compute.
Our dataset is unlabeled and manually label-ing all entities is expensive.
Pooling is a common evaluationstrategy in such situations (such as, TAC-KBP shared task).ning, 2014).6 Results and DiscussionTable 1 shows AUC-PR of various systems and Fig-ure 1 shows the precision-recall curves.
Our sys-tems Expanded and Expanded-M, which used simi-lar entities for training, improved the scores for allfour forums.
We believe the improvement for theDiabetes forum was much higher than other forumsbecause the baseline?s performance on the forum de-graded quickly in later iterations (see the figure), andimproving the classifier helped in adding more cor-rect entities.
Additionally, Diabetes DT entities aremore lifestyle-based and hence occur frequently inweb text, making the word vectors trained using theWiki+Twit+MedHelp dataset better suited.In three out of four forums, word vectors trained1218Positives NegativesAsthmapranayama, sterilizing,expectorants, inhalable,sanitizers, ayurvedicblock, yougurt, medcine,exertion, hate, virallyDiabetesquinoa, vinegars, vegat-ables, threadmill, pos-silbe, asanas, omegasnicely, chiropracter, ex-hales, paralytic, metabo-lize, fluffyTable 2: Examples of unlabeled entities that wereexpanded into the training sets.
Gray colored entitieswere judged by the authors as falsely labeled.using a large corpus perform better than thosetrained using the smaller in-domain corpus.
For theAcne forum, where brand name DT entities are morefrequent, the entities expanded by MedHelp vectorshad fewer false positives than those expanded byWiki+Twit+MedHelp.Table 2 shows some examples of unlabeled enti-ties that were included as positive/negative entitiesin the entity classifiers.
Even though some entitieswere included in the training data with wrong labels,overall the classifiers benefited from the expansion.7 ConclusionWe improve entity classifiers in bootstrapped en-tity extraction systems by enhancing the trainingset using unsupervised distributed representations ofwords.
The classifiers learned using the expandedseed sets extract entities with better F1score.
Thissupports our hypothesis that generalizing labels toentities that are similar according to unsupervisedmethods of word vector learning is effective in im-proving entity classifiers, notwithstanding that thelabel generalization is quite noisy.ReferencesS.
Abney.
2004.
Understanding the Yarowsky algorithm.Computational Linguistics, 30:365?395.A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Conference onLearning Theory (COLT).P.
F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, andR.
L. Mercer.
1992.
Class-based n-gram models ofnatural language.
Computational Linguistics, 18:467?479.A.
Carlson, J. Betteridge, R. C. Wang, Jr. E. R. Hruschka,and T. M. Mitchell.
2010.
Coupled semi-supervisedlearning for information extraction.
InWeb Search andData Mining (WSDM), pages 101?110.L.
Chiticariu, Y. Li, and F. R. Reiss.
2013.
Rule-basedinformation extraction is dead!
long live rule-basedinformation extraction systems!
In Empirical Meth-ods in Natural Language Processing (EMNLP), pages827?832.M.
Collins and Y.
Singer.
1999.
Unsupervised modelsfor named entity classification.
In Empirical Methodsin Natural Language Processing (EMNLP).O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: An experimental study.
Artificial Intelligence,165(1):91?134.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identifyingrelations for open information extraction.
In EmpiricalMethods in Natural Language Processing (EMNLP).S.
Gupta and C. D. Manning.
2014.
Improved patternlearning for bootstrapped entity extraction.
In Compu-tational Natural Language Learning (CoNLL).A.
Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In North American Associ-ation for Computational Linguistics (NAACL), pages320?327.M.
A. Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Interational Conference onComputational linguistics, pages 539?545.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013a.Efficient estimation of word representations in vectorspace.
Technical Report 1301.3781, arXiv.T.
Mikolov, I. Sutskever, K. Chen, G. Corrado, andJ.
Dean.
2013b.
Distributed representations of wordsand phrases and their compositionality.
In Advancesin Neural Information Processing Systems (NIPS).J.
Pennington, R. Socher, and C. D. Manning.
2014.Glove: Global vectors for word representation.
InEmpirical Methods in Natural Language Processing(EMNLP).L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Com-putational Natural Language Learning (CoNLL).E.
Riloff.
1996.
Automatically generating extractionpatterns from untagged text.
In Association for theAdvancement of Artificial Intelligence (AAAI), pages1044?1049.1219A.
Subramanya, S. Petrov, and F. Pereira.
2010.
Effi-cient graph-based semi-supervised learning of struc-tured tagging models.
In Empirical Methods in Natu-ral Language Processing (EMNLP).M.
Thelen and E. Riloff.
2002.
A bootstrapping methodfor learning semantic lexicons using extraction patterncontexts.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 214?221.M.
Whitney and A. Sarkar.
2012.
Bootstrapping viagraph propagation.
In Association for ComputationalLinguistics (ACL).R.
Yangarber, R. Grishman, and P. Tapanainen.
2000.Automatic acquisition of domain knowledge for in-formation extraction.
In International Conferenceon Computational Linguistics (COLING), pages 940?946.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Associationfor Computational Linguistics (ACL).1220
