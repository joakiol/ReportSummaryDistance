Proceedings of the Third Workshop on Statistical Machine Translation, pages 131?134,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEuropean Language Translation with Weighted Finite State Transducers:The CUED MT System for the 2008 ACL Workshop on SMTGraeme Blackwood, Adria` de Gispert, Jamie Brunning, William ByrneMachine Intelligence LaboratoryDepartment of Engineering, Cambridge UniversityTrumpington Street, Cambridge, CB2 1PZ, U.K.{gwb24|ad465|jjjb2|wjb31}@cam.ac.ukAbstractWe describe the Cambridge University En-gineering Department phrase-based statisti-cal machine translation system for Spanish-English and French-English translation in theACL 2008 Third Workshop on Statistical Ma-chine Translation Shared Task.
The CUEDsystem follows a generative model of trans-lation and is implemented by composition ofcomponent models realised as Weighted Fi-nite State Transducers, without the use of aspecial-purpose decoder.
Details of systemtuning for both Europarl and News translationtasks are provided.1 IntroductionThe Cambridge University Engineering Departmentstatistical machine translation system follows theTransducer Translation Model (Kumar and Byrne,2005; Kumar et al, 2006), a phrase-based generativemodel of translation that applies a series of transfor-mations specified by conditional probability distri-butions and encoded as Weighted Finite State Trans-ducers (Mohri et al, 2002).The main advantages of this approach are its mod-ularity, which facilitates the development and eval-uation of each component individually, and its im-plementation simplicity which allows us to focus onmodeling issues rather than complex decoding andsearch algorithms.
In addition, no special-purposedecoder is required since standard WFST operationscan be used to obtain the 1-best translation or a lat-tice of alternative hypotheses.
Finally, the systemarchitecture readily extends to speech translation, inwhich input ASR lattices can be translated in thesame way as for text (Mathias and Byrne, 2006).This paper reviews the first participation of CUEDin the ACL Workshop on Statistical Machine Trans-lation in 2008.
It is organised as follows.
Firstly,section 2 describes the system architecture and itsmain components.
Section 3 gives details of the de-velopment work conducted for this shared task andresults are reported and discussed in section 4.
Fi-nally, in section 5 we summarise our participation inthe task and outline directions for future work.2 The Transducer Translation ModelUnder the Transducer Translation Model, the gen-eration of a target language sentence tJ1 starts withthe generation of a source language sentence sI1 bythe source language model PG(sI1).
Next, the sourcelanguage sentence is segmented into phrases accord-ing to the unweighted uniform phrasal segmenta-tion model PW (uK1 ,K|sI1).
This source phrase se-quence generates a reordered target language phrasesequence according to the phrase translation and re-ordering model PR(xK1 |uK1 ).
Next, target languagephrases are inserted into this sequence according tothe insertion model P?
(vR1 |xK1 , uK1 ).
Finally, thesequence of reordered and inserted target languagephrases are transformed to word sequences tJ1 underthe target phrasal segmentation model P?
(tJ1 |vR1 ).These component distributions together form a jointdistribution over the source and target language sen-tences and their possible intermediate phrase se-quences as P (tJ1 , vR1 , xK1 , uK1 , sI1).In translation under the generative model, we startwith the target sentence tJ1 in the foreign language131and search for the best source sentence s?I1.
Encod-ing each distribution as a WFST leads to a model oftranslation as the series of compositionsL = G ?
W ?
R ?
?
??
?
T (1)in which T is an acceptor for the target languagesentence and L is the word lattice of translations ob-tained during decoding.
The most likely translations?I1 is the path in L with least cost.2.1 TTM Reordering ModelThe TTM reordering model associates a jump se-quence with each phrase pair.
For the experi-ments described in this paper, the jump sequenceis restricted such that only adjacent phrases can beswapped; this is the MJ1 reordering model of (Ku-mar and Byrne, 2005).
Although the reorderingprobability for each pair of phrases could be esti-mated from word-aligned parallel data, we here as-sume a uniform reordering probability p tuned as de-scribed in section 3.1.
Figure 1 shows how the MJ1reordering model for a pair of phrases x1 and x2 isimplemented as a WFST.0 1x : xx2 : x1x1 : x2p / b=+11 / b=?11?p / b=0Figure 1: The uniform MJ1 reordering transducer.3 System DevelopmentCUED participated in two of the WMT shared tasktracks: French?English and Spanish?English.
Forboth tracks, primary and contrast systems were sub-mitted.
The primary submission was restrictedto only the parallel and language model data dis-tributed for the shared task.
The contrast submissionincorporates large additional quantities of Englishmonolingual training text for building the second-pass language model described in section 3.2.Table 1 summarises the parallel training data, in-cluding the total number of sentences, total num-ber of words, and lower-cased vocabulary size.
TheSpanish and French parallel texts each contain ap-proximately 5% News Commentary data; the restis Europarl data.
Various single-reference develop-ment and test sets were provided for each of thetracks.
However, the 2008 evaluation included a newNews task, for which no corresponding developmentset was available.sentences words vocabFR 39.9M 124kEN1.33M 36.4M 106kES 38.2M 140kEN 1.30M 35.7M 106kTable 1: Parallel corpora statistics.All of the training and system tuning was per-formed using lower-cased data.
Word alignmentswere generated using GIZA++ (Och and Ney, 2003)over a stemmed version of the parallel text.
Stemsfor each language were obtained using the Snowballstemmer1.
After unioning the Viterbi alignments,the stems were replaced with their original words,and phrase-pairs of up to five foreign words in lengthwere extracted in the usual fashion (Koehn et al,2003).3.1 System TuningMinimum error training (Och, 2003) underBLEU (Papineni et al, 2001) was used to optimisethe feature weights of the decoder with respectto the dev2006 development set.
The followingfeatures are optimized:?
Language model scale factor?
Word and phrase insertion penalties?
Reordering scale factor?
Insertion scale factor?
Translation model scale factor: u-to-v?
Translation model scale factor: v-to-u?
Three phrase pair count featuresThe phrase-pair count features track whether eachphrase-pair occurred once, twice, or more than twice1Available at http://snowball.tartarus.org132in the parallel text (Bender et al, 2007).
All de-coding and minimum error training operations areperformed with WFSTs and implemented using theOpenFST libraries (Allauzen et al, 2007).3.2 English Language ModelsSeparate language models are used when translatingthe Europarl and News sets.
The models are esti-mated using SRILM (Stolcke, 2002) and convertedto WFSTs for use in TTM translation.
We use the of-fline approximation in which failure transitions arereplaced with epsilons (Allauzen et al, 2003).The Europarl language model is a Kneser-Ney (Kneser and Ney, 1995) smoothed default-cutoff 5-gram back-off language model estimatedover the concatenation of the Europarl and Newslanguage model training data.
The News languagemodel is created by optimising the interpolationweights of two component models with respect tothe News Commentary development sets since webelieve these more closely match the newstest2008domain.
The optimised interpolation weights were0.44 for the Europarl corpus and 0.56 for the muchsmaller News Commentary corpus.
For our contrastsubmission, we rescore the first-pass translation lat-tices with a large zero-cutoff stupid-backoff (Brantset al, 2007) language model estimated over approx-imately five billion words of newswire text.4 Results and DiscussionTable 2 reports lower-cased BLEU scores for theFrench?English and Spanish?English Europarland News translation tasks.
The NIST scores arealso provided in parentheses.
The row labelled?TTM+MET?
shows results obtained after TTMtranslation and minimum error training, i.e.
our pri-mary submission constrained to use only the datadistributed for the task.
The row labelled ?+5gram?shows translation results obtained after rescoringwith the large zero-cutoff 5-gram language modeldescribed in section 3.2.
Since this includes addi-tional language model data, it represents the CUEDcontrast submission.Translation quality for the ES?EN task isslightly higher than that of FR?EN.
For Europarltranslation, most of the additional English languagemodel training data incorporated into the 5-gramrescoring step is out-of-domain and so does not sub-stantially improve the scores.
Rescoring yields anaverage gain of just +0.5 BLEU points.Translation quality is significantly lower in bothlanguage pairs for the new news2008 set.
Two fac-tors may account for this.
The first is the changein domain and the fact that no training or devel-opment set was available for the News translationtask.
Secondly, the use of a much freer translationin the single News reference, which makes it dif-ficult to obtain a good BLEU score.
However, thesecond-pass 5-gram language model rescoring gainsare larger than those observed in the Europarl sets,with approximately +1.7 BLEU points for each lan-guage pair.
The additional in-domain newswire dataclearly helps to improve translation quality.Finally, we use a simple 3-gram casing modeltrained on the true-case workshop distributedlanguage model data, and apply the SRILMdisambig tool to restore true-case for our finalsubmissions.
With respect to the lower-cased scores,true-casing drops around 1.0 BLEU in the Europarltask, and around 1.7 BLEU in the News Commen-tary and News tasks.5 SummaryWe have reviewed the Cambridge University Engi-neering Department first participation in the work-shop on machine translation using a phrase-basedSMT system implemented with a simple WFST ar-chitecture.
Results are largely competitive with thestate-of-the-art in this task.Future work will examine whether further im-provements can be obtained by incorporating addi-tional features into MET, such as the word-to-wordModel 1 scores or phrasal segmentation models.
TheMJ1 reordering model could also be extended to al-low for longer-span phrase movement.
MinimumBayes Risk decoding, which has been applied suc-cessfully in other tasks, could also be included.The difference in the gains from 5-gram latticerescoring suggests that, particularly for Europarltranslation, it is important to ensure the languagemodel data is in-domain.
Some form of count mix-ing or alternative language model adaptation tech-niques may prove useful for unconstrained Europarltranslation.133Task dev2006 devtest2006 test2007 test2008 newstest2008FR?EN TTM+MET 31.92 (7.650) 32.51 (7.719) 32.94 (7.805) 32.83 (7.799) 19.58 (6.108)+5gram 32.51 (7.744) 32.96 (7.797) 33.33 (7.880) 33.03 (7.856) 21.22 (6.311)ES?EN TTM+MET 33.11 (7.799) 32.25 (7.649) 32.90 (7.766) 33.11 (7.859) 20.99 (6.308)+5gram 33.30 (7.835) 32.96 (7.740) 33.55 (7.857) 33.47 (7.893) 22.83 (6.513)Table 2: Translation results for the Europarl and News tasks for various dev sets and the 2008 test sets.AcknowledgementsThis work was supported in part under the GALEprogram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011-06-C-0022.ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In Proceedings of the 41st Meeting ofthe Association for Computational Linguistics, pages557?564.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFST: ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the 9th International Con-ference on Implementation and Application of Au-tomata, pages 11?23.
Springer.Oliver Bender, Evgeny Matusov, Stefan Hahn, SasaHasan, Shahram Khadivi, and Hermann Ney.
2007.The RWTH Arabic-to-English spoken language trans-lation system.
In Proceedings of the 2007 AutomaticSpeech Understanding Workshop, pages 396?401.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 858?867.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Acoustics, Speech, andSignal Processing, pages 181?184.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference for Computational Lin-guistics on Human Language Technology, pages 48?54, Morristown, NJ, USA.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 161?168.Shankar Kumar, Yonggang Deng, and William Byrne.2006.
A weighted finite state transducer transla-tion template model for statistical machine translation.Natural Language Engineering, 12(1):35?75.Lambert Mathias and William Byrne.
2006.
Statisticalphrase-based speech translation.
In 2006 IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing.Mehryar Mohri, Fernando Pereira, and Michael Riley.2002.
Weighted finite-state transducers in speechrecognition.
In Computer Speech and Language, vol-ume 16, pages 69?88.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Meeting of the Association for ComputationalLinguistics, pages 160?167, Morristown, NJ, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Meeting of the Association for ComputationalLinguistics, pages 311?318, Morristown, NJ, USA.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing.134
