Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 345?348,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCrowdsourcing the evaluation of a domain-adapted named entityrecognition systemAsad B. Sayeed, Timothy J. Meyer,Hieu C. Nguyen, Olivia BuzekDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742asayeed@cs.umd.edu,tmeyer1@umd.edu,{hcnguyen88,olivia.buzek}@gmail.comAmy WeinbergDepartment of LinguisticsUniversity of MarylandCollege Park, MD 20742weinberg@umiacs.umd.eduAbstractNamed entity recognition systems sometimeshave difficulty when applied to data from do-mains that do not closely match the trainingdata.
We first use a simple rule-based tech-nique for domain adaptation.
Data for robustvalidation of the technique is then generated,and we use crowdsourcing techniques to showthat this strategy produces reliable results evenon data not seen by the rule designers.
Weshow that it is possible to extract large im-provements on the target data rapidly at lowcost using these techniques.1 Introduction1.1 Named entities and errorsIn this work, we use crowdsourcing to generate eval-uation data to validate simple techniques designed toadapt a widely-used high-performing named entityrecognition system to new domains.
Specifically, weachieve a roughly 10% improvement in precision ontext from the information technology (IT) businesspress via post hoc rule-based error reduction.
Wefirst tested the system on a small set of data that weannotated ourselves.
Then we collected data fromAmazon Mechanical Turk in order to demonstratethat the gain is stable.
To our knowledge, there is noprevious work on crowdsourcing as a rapid meansof evaluating error mitigation in named entity rec-ognizer development.Named entity recognition (NER) is a well-knownproblem in NLP which feeds into many other re-lated tasks such as information retrieval (IR) andmachine translation (MT) and more recently socialnetwork discovery and opinion mining.
Generally,errors in the underlying NER technology correlatewith a steep price in performance in the NLP sys-tems further along a processing pipeline, as incor-rect entities propagate into incorrect translations orerroneous graphs of social networks.Not all errors carry the same price.
In some ap-plications, omitting a named entity has the conse-quence of reducing the availability of training data,but including an incorrectly identified piece of textas as a named entity has the consequence of pro-ducing misleading results.
Our application wouldbe opinion mining; an omitted entity may preventthe system from attributing an opinion to a source,but an incorrect entity reveals non-existent opinionsources.Machine learning is currently used extensively inbuilding NER systems.
One such system is BBN?sIdentifinder (Bikel et al, 1999).
The IdentiFinder al-gorithm, based on Hidden Markov Models, has beenshown to achieve F-measure scores above 90% whenthe training and testing data happen to be derivedfrom Wall Street Journal text produced in the 1990s.We use IdentiFinder 3.3 as a starting point for per-formance improvement in this paper.The use of machine learning in existing systemsrequires us to produce new and costly training dataif we want to adapt these systems directly to otherdomains.
Our post hoc error reduction strategy istherefore profoundly different: it relieves us of theburden of generating complete training examples.The data we generate are strictly corrections of theexisting system?s output.
Our thus cheaper evalua-tion is therefore primarily on improvements to pre-345cision, while minimizing damage to recall, unlikean evaluation based on retraining with new, fully-annotated text.1.2 CrowdsourcingCrowdsourcing is the use of the mass collabora-tion of Internet passers-by for large enterprises onthe World Wide Web such as Wikipedia and surveycompanies.
However, a generalized way to mon-etize the many small tasks that make up a largertask is relatively new.
Crowdsourcing platformslike Amazon Mechanical Turk have allowed someNLP researchers to acquire data for small amountsof money from large, unspecified groups of Internetusers (Snow et al, 2008; Callison-Burch, 2009).The use of crowdsourcing for an NLP annotationtask required careful definition of the specifics ofthe task.
The individuals who perform these taskshave no specific training, and they are trying to getthrough as many tasks as they can, so each task mustbe specified very simply and clearly.Part of our work was to define a named entityerror detection task simply enough that the resultswould be consistent across anonymous annotators.2 Methodology2.1 Process overviewThe overall process for running this experiment wasas follows (figure 1).Figure 1: Diagram of data pipeline.First, we performed an initial performance assess-ment of IdentiFinder on our domain.
We selected200 articles from an IT trade journal.
IdentiFinderwas used to tag persons and organizations in thesedocuments.
Domain experts (in this case, the au-thors of this paper) analyzed the entity tags pro-duced by the NER system and annotated the erro-neous tags.
We built an error reduction system basedon our error analysis.
We then ran the IdentiFinderoutput through the error reduction system and eval-uated its performance against our annotations.Next, we constructed an Amazon MechanicalTurk-based interface for na?
?ve web users or ?Turk-ers?
to annotate the IdentiFinder entities for errors.We measured the interannotator agreement betweenthe Turkers and the domain experts, and we evalu-ated the IdentiFinder output and the repaired outputagainst the expert-generated and Turker gold stan-dards.We selected a new batch of 800 articles and ranIdentiFinder and the filters on them, and we againran our Mechanical Turk application on the Iden-tiFinder output.
We measured the performance ofIdentiFinder and filtered output against the Turkerannotations.2.2 Performance evaluationPerformance is evaluated in terms of standard pre-cision and recall of entities.
If the system outputcontains a person or organization labelled correctlyas such, it considers this to be a hit.
If it contains aperson or organization that is mislabelled or other-wise incorrect in the gold standard annotation, it isa miss.
We compute the F-measure as the harmonicmean of precision and recall.As the IdentiFinder output is the baseline, and weignore missed entities, by definition the baseline re-call is 100%.3 Experiments and resultsHere we delve into further detail about the tech-niques we used and the results that they yielded.
Theresults are summarized in table 1.3.1 Baseline performance assessmentWe randomly selected 200 documents from Infor-mationWeek, a major weekly magazine in the ITbusiness press.
Running them through IdentiFinderproduces NIST ACE-standard XML entity markup.We focused on the ENAMEX tags of person and or-ganization type that IdentiFinder produces.After we annotated the ENAMEX tags for errors,we found that closer inspection of the errors in theIdentiFinder output allowed us to classify the major-ity of them into three major categories:346Annotator Collection System Precision Recall F-measureAuthors 200 document IdentiFinder only 0.74 1 0.85Authors 200 document Filtered 0.86 0.98 0.92MTurk 200 document IdentiFinder only 0.69 1 0.82MTurk 200 document Filtered 0.79 0.97 0.87MTurk 800 document IdentiFinder only 0.67 1 0.80MTurk 800 document Filtered 0.77 0.95 0.85Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.?
IdentiFinder tags words that are simply notnamed entities.?
IdentiFinder assigns the wrong category (per-son or organization) to an entity.?
IdentiFinder includes extraneous words in anotherwise correct entity.The second and third types of error are particu-larly challenging.
An example of the second type isthe following:Yahoo is a reasonably strong competitorto Google.
It gets about half as much on-line revenue and search traffic as Google,.
.
.Google is marked twice incorrectly as being a personrather than an organization.Finally, here is an example of the third error type:A San Diego bartender reported that BillGates danced the night away in his bar onNov.
11.IdentiFinder incorrectly marks ?danced?
as part of aperson tag.We were able to find the precision of IdentiFinderagainst our annotations: 0.74.
This is poorer than thereported performance of IdentiFinder on Wall StreetJournal text (Bikel et al, 1999).3.2 Domain-specific error reductionWe wrote a series of rule-based filters to removeinstances of the error types?of which there weremany subtypes?described in the previous sec-tion.
For instance, the third example above waseliminated via the use of a part-of-speech tagger;?danced?
was labelled as a verb, and entities withtagged verbs were removed.
In the second case,the mislabelling of Google as a person rather thanan organization is identified by looking at Identi-Finder?s majority labelling of Google throughout thecorpus?as an organization.
Simple rules about cap-italization allow instances like the first example tobe identified as errors.This step increases the precision of the systemoutput to 86%, while only sacrificing a tiny amountof recall.
We see that this 10% increase is main-tained even on the Mechanical Turk-generated an-notations.3.3 Mechanical Turk tasksThe basic unit of Mechanical Turk is the Human In-telligence Task (HIT).
Turkers select HITs presentedas web pages and perform the described task.
Data-collectors create HITs and pay Amazon to disbursesmall amounts of money to Turkers who completethem.We designed our Mechanical Turk process so thatevery HIT we create corresponds to an IdentiFinder-marked document.
Within its corresponding HIT,each document is broken up into paragraphs.
Fol-lowing every paragraph is a table whose rows con-sist of every person/organization ENAMEX discov-ered by IdentiFinder and whose columns consist ofone of the four categories: ?Person,?
?Organization,??Neither,?
and ?Don?t Know.?
Then for each entity,the user selects exactly one of the four options.Each HIT is assigned to three different Turkers.Every entity in that HIT is assigned a person or or-ganization ENAMEX tag if two of the three Turkersagreed it was one of those (majority vote); other-wise, it is marked as an invalid entity.We calculated the agreement between our annota-tions and those developed from the Turker majority347vote scheme.
This yields a Cohen?s ?
of 0.68.
Weconsidered this to be substantial agreement.After processing the same 200 document set fromour own annotation, we found that the precisionof IdentiFinder was 69%, but after error reduction,it increased to 79% with only a miniscule loss ofknown valid entities (recall).We then took another 800 documents from Infor-mationWeek and ran them through IdentiFinder.
Wedid not annotate these documents ourselves, but in-stead turned them over to Turkers.
IdentiFinder out-put alone has a 67% precision, but after error reduc-tion, it rises to 77%, and recall is still minimally af-fected.4 Discussion4.1 BenefitsIt appears that high-performing NER systems ex-hibit rather severe domain adaption problems.
Theperformance of IdentiFinder is quite low on the ITbusiness press.
However, a simple rule-based sys-tem was able to gain 10% improvement in precisionwith little recall sacrificed.
This is a particularly im-portant improvement in applications with low toler-ance for erroneous entities.However, rule-based systems built by experts areknown to be vulnerable to new data unseen by theexperts.
In order to apply this domain-specific errorreduction reliably, it has to be tested on data gatheredelsewhere.
We used crowdsourced data to show thatthe rule-based system was robust when confrontedwith data that the designers did not see.One danger in crowdsourcing is a potential lackof commitment on the part of the annotators, as theyattempt to get through tasks as quickly as possible.It turns out that in an NER context, we can design acrowdsourced task that yields relatively reliable re-sults across data sets by ensuring that for every datapoint, there were multiple annotators making onlysimple decisions about entity classification.This method also provides us with a source of eas-ily acquired supervised training data for testing moreadvanced techniques, if required.4.2 CostsIt took not more than an estimated two person weeksto complete this work.
This includes doing theexpert annotations, designing the Mechanical Turktasks, and building the domain-specific error reduc-tion rules.For each HIT, each annotator was paid 0.05 USD.For three annotators for 1000 documents, that is150.00 USD (plus additional small Amazon sur-charges and any taxes that apply).5 Conclusions and Future WorkThis work was done on a single publication in a sin-gle domain.
One future experiment would be to seewhether these results are reliable across other pub-lications in the domain.
Another set of experimentswould be to determine the optimum number of an-notators; we assumed three, but cross-domain resultsmay be more stable with more annotators.Retraining an NER system for a particular domaincan be expensive if new annotations must be gen-erated from scratch.
While there is work on usingadvanced machine learning techniques for domaintransfer (Guo et al, 2009), simply repairing the theerrors post hoc via a rule-based system can have alow cost for high gains.
This work shows a casewhere the results are reliable and the verificationsimple, in a context where reducing false positivesis a high priority.AcknowledgementsThis paper is based upon work supported by the Na-tional Science Foundation under Grant IIS-0729459.This research was also supported in part by NSFaward IIS-0838801.ReferencesDaniel M. Bikel, Richard Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
Mach.
Learn., 34(1-3).Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In EMNLP 2009, Singapore, August.Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,Xian Wu, and Zhong Su.
2009.
Domain adapta-tion with latent semantic association for named entityrecognition.
In NAACL 2009, Morristown, NJ, USA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In EMNLP 2008, Morristown, NJ, USA.348
