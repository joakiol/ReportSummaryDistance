The Complexity of Ranking Hypotheses inOptimality TheoryJason Riggle?University of ChicagoGiven a constraint set with k constraints in the framework of Optimality Theory (OT), what isits capacity as a classification scheme for linguistic data?
One useful measure of this capacityis the size of the largest data set of which each subset is consistent with a different grammarhypothesis.
This measure is known as the Vapnik-Chervonenkis dimension (VCD) and is astandard complexity measure for concept classes in computational learnability theory.
In thiswork, I use the three-valued logic of Elementary Ranking Conditions to show that the VCD ofOptimality Theory with k constraints is k?1.
Analysis of OT in terms of the VCD establishesthat the complexity of OT is a well-behaved function of k and that the ?hardness?
of learning inOT is linear in k for a variety of frameworks that employ probabilistic definitions of learnability.1.
IntroductionGiven a set CON of k constraints in the framework of Optimality Theory (OT; Princeand Smolensky 1993), what is the capacity of CON as a classification scheme for samplesof language data?
In OT, constraints are functions that map candidates to natural num-bers, where each candidate is a member of the (possibly infinite) set of possible deriva-tions of an input form i supplied by the candidate generating function GEN(i).
Thenumber that a constraint Ci assigns to a candidate indicates how many times thatcandidate violates Ci.
A grammar is a ranking of the constraints that imposes a totalordering on CON, RCON (or simply R when CON is clear from the context), and thelanguage that is generated by grammar R is the set of candidates that are optimalaccording toR as in Definition 1.Definition 1a.
Candidate a is more harmonic than candidate b according to R, written a b,if they share the same input and a is assigned fewer violations by the highest-ranked constraint that assigns different numbers of violations to a and b.b.
Candidate a is optimal according to rankingR iff no other candidate generatedby GEN is more harmonic than a.Because each of the k!
rankings of CON is a different grammar that generates apotentially unique language, one natural measure of the classificatory capacity of CON?
Department of Linguistics, University of Chicago, 1010 E. 59th St., Chicago, IL 60637.jriggle@uchicago.edu.
Many thanks to Alan Prince, Jeff Heinz, Greg Kobele, Colin Wilson, and twoanonymous Computational Linguistics reviewers for helpful comments and suggestions.
Any errors are, ofcourse, my own.Submission received: 22 December 2006; revised submission received: 9 October 2007; accepted forpublication: 17 December 2007.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 1is the upper bound of k!
languages in what Prince and Smolensky (1993, page 27) dubthe factorial typology of the constraint set.
Another complexity metric that is useful inanalyses of learnability (especially for non-finite concept classes) is the cardinality ofthe largest data set of which each subset corresponds to a different ranking hypothesis.The idea of measuring the complexity of a concept class (in the case at hand, a set ofgrammars) in this way comes from the work of Vapnik and Chervonenkis (1971) and isknown as the Vapnik-Chervonenkis dimension (VCD).
In OT, the VCD of a constraintset CON (i.e., the concept class consisting of languages generated by rankings of CON)is the size of the largest sample (set of candidates) that is shatterable as in Definition 2.Definition 2A sample S is shatterable by a constraint set CON iff, for every partitioning of S into twodisjoint sets T and F (including the null/whole partition), there is at least one rankingRCON that makes every s ?
T optimal but no s ?
F optimal.Vapnik and Chervonenkis?s definition of shatterability has interesting implicationsfor samples consisting of OT candidates.
For instance, each candidate in a shatterablesample S must be an input ?
output mapping for a unique input form because twocandidates a and b with the same input would either tie with identical sets of violationsor show harmonic inequality.
In the case of a tie, no ranking could realize a partitioningthat separates a and b and, in the case of harmonic inequality, no ranking could realizea partitioning in which a and b are simultaneously optimal.
More generally, the VCDplaces an upper bound on the number of distinct grammar hypotheses that can be real-ized over any sample of linguistic data consisting of OT candidates, and thus providesa ready measure of the complexity of the hypothesis space in Optimality Theory.The VCD of a concept class is obviously not independent of its size.
As Blumeret al (1989) point out, for any finite concept class C, the VCD is bounded at log2 |C|because it takes at least 2d hypotheses to associate a unique hypothesis with every sub-set of a sample of size d. Thus, because the number of grammars (hypotheses) overk constraints is finite?one grammar for each of the k!
rankings?the VCD of OT isbounded at log2 k!.
Or, put more simply, because log2 x!
?
x log2 x, this establishesk log2 k as an upper bound on the VCD of OT.
In this article, I will show how thestructure of the hypothesis space in Optimality Theory provides a tighter bound onthe VCD of OT than the bound established by the finitude of the hypothesis space.
Iwill improve upon the inherent bound of k log2 k by showing that the VCD of OT withk constraints is actually bounded at k ?
1 and thus grows linearly with the size of |CON|.The complexity measured by the VC dimension has a number of ramifications forlearning in Optimality Theory.
For instance, the VCD of a concept class places an abso-lute lower bound on the number of mistakes that any error-driven learning algorithmcan be guaranteed of achieving (Littlestone 1988).
This fact tells us that it may yetbe possible to improve upon the quadratic mistake bound of (k2 ?
k)/2 for RecursiveConstraint Demotion (Tesar and Smolensky 1993, 2000; Tesar 1995, 1997, 1998), thereigning mistake bound for any OT learning algorithm.
The VCD of a concept classalso provides a very general bound on the number of data samples that are required forlearning in probabilistic models of learning that will be discussed in Section 5.2.
Elementary Ranking ConditionsThe main result for the VC dimension of OT will be given in Section 4.
First, somesupporting results will be established showing that there is an upper bound of k ?
1 on48Riggle Ranking Hypotheses in OTshatterable sets of statements about constraint rankings that are expressed with Prince?s(2002) Elementary Ranking Conditions.If our sample space X consists of candidates, then any x ?
X can be describedin terms of the set of constraint rankings under which x is optimal.
Prince (2002)provides a scheme for encoding this kind of ranking information called an ElementaryRanking Condition (ERC).
In this section, I will review some formal properties of ERCsthat are relevant for establishing the VC dimension of OT.
Prince demonstrates manyformal properties of ERCs beyond those covered here and shows that ERCS are equiv-alent to the implication-negation fragment of the three-valued relevance logic RM3 (cf.Anderson and Belnap 1975).
This section will review properties of ERCs that are mostrelevant for the results at hand.
For formal proofs and a complete exposition of the logicof ERCs, see Prince (2002).For a constraint set CON containing k constraints, ERCs are k-length vectors thatuse the symbols L, e, and W to encode logical statements about rankings.
Each con-straint is assigned an arbitrary numeric index, and in each ERC ?, the ith coordinate ?irefers to the constraint with ith index Ci.
The meaning of an ERC is that at least oneconstraint whose corresponding coordinate contains a W outranks all of the constraintswhose coordinates contain L?s.
Thus, ?W, e, L, L?
means that C1 outranks both C3 andC4, while ?L, L, W, W?
means that either C3 or C4 outranks both C1 and C2.
ERCs canbe constructed by comparing candidates as in Definition 3.
Note that Ci(a) denotes thenumber of times candidate a violates the constraint with index i.Definition 3Given a constraint set CON with k constraints indexed {1 ... k} and two candidatesthat share the same input, the function ercCON(a, b) returns an ERC ?
= ?
?1, ...,?k?
thatdescribes the rankings under which a b.1erc(a, b) = ?
?1, ...,?k?where?????
?i = W if Ci(a) < Ci(b)?i = L if Ci(a) > Ci(b)?i = e if Ci(a) = Ci(b)The symbol W in ?i of erc(a, b) = ?
is a mnemonic for the fact that Ci favors a(the winner), whereas an L in coordinate i is a mnemonic for the fact that Ci favors b(the loser).
An e in ?i indicates that the candidates are equivalent according to Ci.Example 1input C1 C2 C3cand.
a * ** * erc(b, a) = ?L, W, W ?
= b a if C2 or C3 outranks C1cand.
b ** * erc(a, b) = ?W, L, L ?
= a b if C1 outranks C2 and C3(*?s indicate number of violations)Note the symmetry between erc(a, b) = ?W, L, L ?, which says that candidate a is moreharmonic than b under any ranking where C1 outranks both C2 and C3, and erc(b, a),1 The function ercCON(a, b), or simply erc(a, b) when CON is clear from context, is undefined for candidateinput ?
output mappings with different inputs because they cannot be meaningfully compared.49Computational Linguistics Volume 35, Number 1which says that b is more harmonic than a under any ranking where either C2 or C3outranksC1.
This symmetry reflects the fact that erc(a, b) and erc(b, a) encode antitheticalranking conditions.
The opposition between these ERCs follows straightforwardly fromthe fact that only one of the two candidates can be optimal under any given ranking.The illustrative tableaux presented with OT analyses can be turned into sets ofERCs by making pairwise comparisons between the violations for one designated (orobserved) winner and the violations for each other candidate.Example 2input C1 C2 C3cand.
a * ** winnercand.
b ** * erc(a, b) = ?W, L, e ?
= a b if C1 outranks C2cand.
c * ** erc(a, c) = ?e, L, W?
= a c if C3 outranks C2cand.
d * * * erc(a, d) = ?e, L, W?
= a d if C3 outranks C2cand.
e ** ** erc(a, e) = ?W, e, e ?
= a e under every rankingcand.
f *** * erc(a, f ) = ?L, W, W?
= a f if C2 or C3 outranks C1The comparison of candidate a with candidate e in Example 2, erc(a, e) = ?W, e, e?,yields an odd ranking condition that does not actually express a particular ranking (noconstraint has an L), but instead indicates that C1 favors candidate a and no constraintfavors candidate e. In this case, candidate e is said to be harmonically bounded bycandidate a because there can be no ranking under which e is more harmonic thana.
Conversely, if candidate e were designated the winner, then erc(e, a) = ?L, e, e?.
ThisERC also does not encode a specific ranking, but rather indicates that the mere existenceof candidate a as an alternative means that no ranking can make candidate e optimal.Like most OT tableaux, Example 2 is an illustration of how a handful of candidatesfare with respect to one another according to a particular set of constraints.
To knowwhich rankings (if any) make candidate a globally optimal, it would be necessary todefine the candidate generating function GEN in order to obtain a representation of theentire set of ERCs {erc(a, x) | x ?
GEN(input)} = ERCS(a).
This is not as daunting as itmight appear because, even though |GEN(input)|may be infinite, the fact that the num-ber of k-length ERCs is finite guarantees that each of the candidates in GEN(input) willmap to one of a finite number of ERC sets.
Furthermore, as Riggle (2004) demonstrates,the standard OT assumption of the universality of faithfulness constraints that pe-nalize changes to the input guarantees that all but finitely many of the members ofGEN(input) will be harmonically bounded.
Riggle also presents an algorithm for com-puting this finite set of contenders (i.e., candidates that are not harmonically bounded)that can be used in cases where GEN is restricted so that it is a rational function.2Regardless of how optimization is computed, what is relevant for the assessment ofthe VCD of OT is the definition of optimality.
Following Definition 1, a rankingRCON canbe seen as a function from candidates to True (if they are optimal) or False (if they are2 GEN is rational if it is representable as a finite state transducer.
Riggle?s (2004) CONTENDERS algorithm isan extension of Ellison?s (1994) application of Dijkstra?s (1959) ?shortest paths?
algorithm to optimizationin OT that operates over finite-state representations of GEN and EVAL.
Ellison showed that if harmony isused as the ?distance?
to be optimized, then optimal outputs can be efficiently found.
The CONTENDERSalgorithm follows a similar strategy but, instead of finding the shortest (i.e., most harmonic) path for oneranking, the algorithm finds all non-harmonically-bounded paths and thereby optimizes for all rankings.50Riggle Ranking Hypotheses in OTnot).
The entire ERC set for a candidate ERCS(a) describes exactly the rankings underwhich candidate a is a globally optimal candidate.The reduction of candidates to ERC sets makes it possible to use the logic of ERCsto reason about candidates.
Most of the time, the ERCs of interest are those that containat least one L and one W?what Prince calls nontrivial ERCs.
ERCs that contain W?sbut no L?s are generated when a candidate is compared with another candidate that itharmonically bounds, such as erc(a, e) = ?W, e, e ?
in Example 2.
This ERC reveals thatcandidate e cannot be optimal but yields no information about what rankings makecandidate a optimal.
Similarly, no ranking information can be gleaned from the all-eERC that results from comparing ?tied?
candidates that have the same violations.Finally, ERCs like erc(e, a) = ?L, e, e ?, with L?s but no W?s reveal nothing other than thefact that candidate e cannot be optimal under any ranking.The most relevant logical relation for ERCs is that of entailment.
The entailmentrelation among nontrivial ERCs is given in Definition 4 (Prince 2002, page 6, Proposi-tion 1.1).Definition 4For nontrivial ERCs ?
and ?, ?
?
?
iff each ?i ?
?
entails ?i ?
?
where L ?
e ?
W.Because nontrivial ERCs encode disjunctions of conjunctions (i.e., [C1 or ... Cn] outranks[C1?
and ... Cn?
]), entailments of the form ?
?
?
line up with the logical operations ofdisjunction introduction (whenever ?
has W where ?
has an L or an e) and conjunctionelimination (whenever ?
has an e where ?
has an L).Example 3?W, L, L, e?
?
?W, e, L, e?
i.e., If C1 outranks C2 and C3 then C1 outranks C3.
?W, e, L, e?
?
?W, e, L, W?
i.e., If C1 outranks C3 then C1 or C4 outranks C3.
?W, L, L, e?
?
?W, e, L, W?
i.e., If C1 outranks C2 and C3 then C1 or C4 outranks C3.In addition to revealing entailments among individual ranking conditions, the logicof ERCs makes it possible to derive new ranking conditions that are entailed by thecombination of other ERCs.
Prince (2002, page 8) provides a logical operation calledfusion that derives entailments from sets of ERCs.Definition 5The fusion of ERC set ?
is a single ERC ?
that is entailed by ?
where:?i = L if any ERC in ?
has an L in its ith coordinate,?i = e if every ERC in ?
has an e in its ith coordinate,?i = W otherwise.Every ERC entailed by ?
is entailed by the fusion of a subset of ?
(Prince 2002,page 14).
Thus, the operation of fusion can reveal nonobvious entailments among ERCs.Consider ?
= {?W,W, e, L?, ?L, W, W, e?, ?W, e, L, W?}.
The ERCs in ?
denote, respectively,?C1 orC2 outranks C4,?
?C2 orC3 outranks C1,?
and ?C1 orC4 outranksC3.?
The fusionof ?
is ?L, W, L, L?, which encodes the inference from ?
that C2 outranks C1, C3, and C4.The operation of fusion can also reveal inconsistencies in ERC sets.
Consider theset ?
= {?W, L, W?, ?L, W, W?, ?W,W, L?}.
Fusing ?
yields ?L, L, L?.
As with harmonicallybounded candidates, this ERC shows that no constraint ranking is consistent withthe statements in ?
(in fact, they are circular).
Prince refers to the class of ERCs with51Computational Linguistics Volume 35, Number 1L?s but no W?s as L+.
He shows that these ERCs arise from fusion if and only if thefused set contains incompatible ranking conditions.Definition 6An ERC set is consistent iff it has no subset that fuses to an ERC in L+ (Prince 2002,page 11).For any consistent ERC set there is a constraint ranking (often several) of whichall of its ERCs are true statements (Prince 2002, page 21).
The ERCs in an inconsistentset, on the other hand, can never all be true of a single ranking.
Inconsistency can arisefrom a single pair of candidates (e.g., ERCS(e) in Example 2 contains erc(e, a) = ?L, e, e?
).Inconsistency can also arise across multiple candidate comparisons (e.g., ERCS(d) inExample 2 contains erc(d, a) = ?e, W, L?
and erc(d, c) = ?e, L, W?).
This latter type of in-consistency, where several of the ERCs associated with a candidate fuse to L+, arisesfrom what Samek-Lodovici and Prince (1999) call collective harmonic bounding.Finally, it is possible for inconsistencies to arise when ERCs for several candidateswith distinct inputs are combined.
For example, if ERCS(x) = {?W, L, W?
}, ERCS(y) ={?L, W, W?
}, and ERCS(z) = {?W,W, L?}
then, even though x, y, and z may be candidatesfor distinct inputs (i.e., come from different tableaux), the union of their ERCs fusesto ?L, L, L?
?
L+ and thereby reveals that there is no ranking under which all threecandidates are simultaneously optimal.Inverting the W?s and L?s of an ERC produces its antithetical counterpart that is truewhenever the original ERC is false and vice versa.
This opposition can be exploited indescribing the range of consistent ERC sets.Definition 7The negation of ?
is ?where: ?i = W if ?i = L, ?i = L if ?i = W, and ?i = e if ?i = e.Provided that ?
is not all e?s, every ranking is described by either ?
or ?
but not both(Prince 2002, page 42).
In this way, ERC negation is just the standard notion of negationin three-valued logics.
The opposition between ?
and ?
makes a binary partition onthe space of rankings.
This is intuitively obvious for simple statements like ?W, L, e?
and?L, W, e?.
The opposition is a bit less intuitive for more complex conditions like ?W, L, L?and ?L, W, W?, but the fact that erc(a, b) is the antithesis of erc(b, a) makes it abundantlyclear (i.e., if a and b are not tied, then every ranking must prefer one or the other).
Theantithetical relationship between an ERC and its negation is reflected in the operationof fusion by the fact that fusing antithetical ERCs will always yield an ERC in L+.3.
The VCD of Elementary Ranking ConditionsBefore turning to the question of the VC dimension of the sample space in OT, it will behelpful to define shatterability purely in terms of ERCs and thereby to establish a boundon the VCD of sets of ERCs.
We will say that an ERC ?
is true of a given ranking R ifthe condition imposed by ?
is consistent with the linear ordering of the constraintsdefined byR.Definition 8An ERC set ?
over constraints CON is shatterable iff for every subset ?
?
?, there is arankingRCON of which all ERCs in ???
are true while all the ERCs in?
are false.52Riggle Ranking Hypotheses in OTFrom this definition of shatterability for sets of ERCs, it is immediately clear that onlynontrivial ERCs can occur in shatterable sets.Lemma 1Every ERC in a shatterable set must contain at least one L and one W.Proof : The ERCs of L+ cannot occur in a shatterable set because there is no rankingof which they are true.
Conversely, ERCs with no L?s cannot occur in shatterable setsbecause there is no ranking of which they are false.
With Definition 8 in hand, and having excluded the trivial ERCs from the picture,it will be possible to reduce shatterability for ERC sets to consistency under negation.First, a definition of negation for sets of ERCs.Definition 9A partial negation of ERC set ?
is obtained by negating every ERC in a subset?
?
?.For example: ?
= {?W, L, L?, ?e, W, L?}
has four partial negations: one per subset.{?
= ?W, L, L??
= ?e, W, L?}
{?
= ?L, W, W??
= ?e, W, L?}
{?
= ?W, L, L??
= ?e, L, W?}
{?
= ?L, W, W??
= ?e, L, W?
}Theorem 1An ERC set ?
is shatterable iff every partial negation of ?
is consistent.Proof : Suppose every partial negation of ?
is consistent.
Thus, for any partial negationin which ?
is the negated subset of ?
and ?
is the rest of ?, it is the case that thereis a ranking R of which all the ERCs of ?+ ?
are true.
Because a nontrivial ERC andits negation are never both true of the same ranking and trivial ERCs cannot occur inshatterable sets, the ERCs in ??
?
are false of ranking R while the ERCs of ?
are true.Because ?
was arbitrary, it is the case that for every subset of ?, there is a ranking ofwhich the ERCs in that subset are false while the rest are true, and thus consistencyunder partial negation is sufficient for shatterability.
If, on the other hand, there is apartial negation that is not consistent, then there is a subset of ?
such that if the ERCsin that subset are negated, the resulting?+ ?
is not consistent.
However, because thereis no ranking of which the members of an inconsistent ERC set are all true, ?
is notshatterable because there is no ranking of which the ERCs in ?
are truewhile the ERCs in??
?
are false.
Thus, consistency under partial negation is both necessary and sufficientfor shatterability.
Corollary 1Every subset of a shatterable ERC set is itself shatterable.Proof : Because each partial negation of a shatterable ERC set must, by definition, beconsistent and because every subset of a consistent set must also be consistent, it is thecase that every subset of a shatterable set is consistent under every partial negation andis thus shatterable.
Defining shatterability in terms of partial negation lines up with the commonsenseobservation that no set containing ?
and ?where ?
?
?
is shatterable because there canbe no ranking of which the former is true while the latter is false.
This is neatly capturedby the fact that if ?
?
?, no superset of {?,?}
can be shattered because fusing {?,?
}is guaranteed to yield an ERC in L+.
The requirement of consistency under partialnegation also shows why relatively weak conditions like ?W,W, L?
and ?W, L, W?
cannotco-occur in shatterable sets even though neither entails the other.
In this case, fusing the53Computational Linguistics Volume 35, Number 1negation of both ERCs yields ?L, L, L?
?
L+.
This follows transparently from the fact thateither the statement ?C1 or C2 outranks C3?
or the statement ?C1 or C3 outranks C2?
istrue of any ranking of three constraints.The definition of shatterability for ERC sets in terms of consistency under partialnegation makes it easy to demonstrate that for |CON| = k, there are shatterable ERCsets of size k ?
1.
Diagonal ERC sets provide a particularly simple example of a class ofshatterable ERC sets of this size.Definition 10ERC set ?
is diagonal if its members can be givenas a list L?
in which each nth ERC in the list has aW in its nth coordinate, an L in its n + 1th coordinate,and e?s everywhere else.E.g., ?
=???????
?W, L, e, e, e ??
e, W, L, e, e ??
e, e, W, L, e ??
e, e, e, W, L ???????
?Lemma 2Diagonal ERC sets are shatterable.Proof : Assume that ?
is a diagonal ERC set and ??
is an arbitrary subset of an arbitrarypartial negation of?.
If n is the number of ERCs in??
then, by the definition of diagonalERC sets, there must be at least n + 1 coordinates (columns) in ??
that are filled withL or W for some ERC in ??
(i.e., are not all-e columns).
Because each of the n ERCs hasonly one L, at most n columns contain L?s, thus the fusion of ??
contains at least one W.Because ??
was an arbitrary subset, no subset fuses to L+.
Because the partial negationwas arbitrary, every partial negation is consistent and thus ?
is shatterable.
From the shatterability of diagonal ERC sets (with k ?
1 members if |CON| = k), weobtain a lower bound of k ?
1 on the VCD of ERC sets.
Having established that thereare shatterable sets of k-length ERCs with k ?
1 members, what remains to be shown isthat no set larger than k ?
1 is shatterable.Definition 11Coordinate Ci is W-unique in ERC set ?
if ?
has a partial negation ??
such that in thefusion of ?
?, ?
= ?
?1, ...,?k?, the only coordinate that contains a W is ?i.Definition 12The minor ?
?,j of an ERC set ?
is a new set ??
in which ERC ?
has been removed andthe jth coordinate has been removed from the remaining ERCs.For example, if ?
=????????
: ?
L, L, W, e, W ??
: ?
e, e, e, L, W ??
: ?
e, e, W, L, e ??
: ?
L, W, e, e, e ???????
?then ?
?, 3 =????
: ?
L, L, e, W ??
: ?
e, e, L, W ??
: ?
L, W, e, e ???
?As illustrated in Definition 12, the term ?minor?
used here is analogous to thestandard notion of the minor of a matrix.
It is straightforward to show that everyshatterable ERC set contains shatterable minors that can be obtained by removing oneconstraint?s coordinate (column) and one ERC (row).Lemma 3Reduction Lemma.
?
If ?
is a shatterable ERC set, then it has a shatterable minor ?
?,j.54Riggle Ranking Hypotheses in OTProof : By Corollary 1, for any ?
?
?, ??
{?}
is shatterable.
In ??
{?}
there mustbe at least one coordinate Cj that is not W-unique.
If this were not the case and everycoordinate in ??
{?}
was W-unique, then one of the L?s in ?
would occlude the onlyW in a partial negation of ?, making it inconsistent contra the assumption that ?
isshatterable (?
must have at least one L by Lemma 1).
Because Cj is not W-unique, forevery partial negation of every subset of ??
{?
}, there is a coordinate other than Cjthat fuses to W. This being the case, shatterability is preserved if Cj is eliminated.
Thus,the minor ?
?,j is shatterable as required.
Theorem 2For k > 1, the largest shatterable set of k-length ERCs has k ?
1 members.Proof : If x is the size of the largest shatterable set of k-length ERCs and y is the size ofthe largest shatterable set of (k + 1)-length ERCs, then y is not greater than x + 1.
Thismust be so because if y ?
x + 2 then x could not be the size of the largest shatterableset of k-length ERCs because a set of (k + 1)-length ERCs would have a shatterableminor larger than x.
Because ?W, L?
and ?L, W?
are the only nontrivial ERCs for k = 2and because they are antithetical and thus cannot co-occur in a shatterable set, thelargest shatterable ERC set at k = 2 consists of a single ERC.
This base case establishesan upper bound of k ?
1 on the size of shatterable ERC sets and the diagonal ERC setsprovide a lower bound of k ?
1.
Together these bounds place the cardinality of thelargest shatterable set at exactly k ?
1.
Along with the diagonal ERC sets, there are many shatterable ERC sets with k ?
1members, but no shatterable sets with more than k ?
1 members.
What remains now isto connect this result for ERC sets back to the realm of candidates.4.
The VCD of Optimality TheoryThe question posed at the outset of this article was: for a constraint set CON with kconstraints thatmap candidates to natural numbers, what is the cardinality of the largestset of candidates S such that, for each subset T ?
S, there is at least one ranking RCONunder which every t in T is optimal, but no s in S ?
T is optimal?
Clearly, the answerto this question depends greatly on details of the constraints in CON.
However, if wereduce candidates to the ERC sets associated with them, it is possible to place an upperbound on the size of S without knowing anything about CON other than its size k.Recall that a candidate c is mapped to True by ranking RCON just in case every ERCin ERCS(c) is consistent withR.
Conversely, c is mapped to False byR if any of the ERCsin ERCS(c) is not consistent with R. This notion can be extended to sets of candidatesas follows.
If S is a set of candidates, then ERCS(S) is the union of ERCS(s) for all s ?
S.A sample S is accepted by rankingR just in case every?
in ERCS(S) is consistent withR.Conversely, S is rejected byR if any ?
in ERCS(S) is not consistent withR.
Furthermore,if ERCS(S) is consistent, then there must be at least one ranking that accepts S. In thiscase, we will refer to S as a consistent sample.
The concepts of partial negation andW-uniqueness also have analogs for candidate sets.Definition 13For a consistent sample S, a partial exclusion is a partial negation of ERCS(S) thatrejects some F ?
S by rendering ercs( f ) inconsistent for each f ?
F while preserving theconsistency of ercs(s) for every s ?
(S ?
F).55Computational Linguistics Volume 35, Number 1Definition 14Ci isw-unique in S if there is a partitioning of S into T and F under which Ci is the onlycoordinate that fuses to W in ERCS(T) for every partial exclusion that rejects F.The property of W-uniqueness in samples crucially contrasts with what one might callbeing semi-unique?the case where for at least one, but not all, of the partial exclusionsthat reject F, Ci is the only column that fuses to W in ERCS(T).Definition 15Given a constraint set CON and a sample S, the minor Sx,j is obtained by removingcandidate x from S and removing constraint Cj from CON.By extending partial negation, W-uniqueness, and the concept of minors to the realm ofsamples, it is straightforward to show that shatterable samples have shatterable minors.Lemma 4Shatterable samples have shatterable minors.Proof : Assume that S is a shatterable sample.
Because removing candidate x from S hasno effect on whether the remainder of S can be shattered, S ?
{x} is also shatterable.Sample S ?
{x} must have at least one coordinate that is not W-unique.
If this werenot the case, then, because ERCS(x) must contain at least one coordinate with an L(else there would be no way to reject {x}), the presence of x in S would place an L in aW-unique coordinate in S ?
{x}.
However, this would make it impossible to associate aranking with at least one partitioning of S into accepted and rejected subsets contra theassumption that S is shatterable.
If Cj is a coordinate in S ?
{x} that is not W-unique,then, for every partial exclusion that rejects F, under each partitioning of S ?
{x} intoT and F, there is at least one other coordinate Ci that fuses to W. Thus, Cj could beremoved from CON while preserving the shatterability of S ?
{x}.
Therefore, Sx,j is ashatterable minor as required.
The crucial piece of the proof in Section 3 that the VCD of ERC sets is k ?
1 wasthe illustration of a one-to-one relationship between k and the bound on shatterable setsby showing that removing an ERC from a shatterable set makes it possible to removea coordinate from the remaining ERCs while preserving shatterability.
If shatterableERC sets could be larger than k ?
1, then it would have been necessary to remove sev-eral ERCs before it was possible to safely remove a coordinate from the remainingERCs.
Because ERC sets and candidate samples both have shatterable minors, a similarstrategy will show that shatterable samples must also grow at a one-to-one rate with k.Theorem 3If |CON| = k, then the size of shatterable sample sets is bounded at k ?
1.Proof : If k = 2, a sample consisting of a single candidate can be shattered if ERCS(s) is{?W, L?}
or {?L, W?
}, but no larger sample can be shattered.
If there were such a sample,it would contain at least two candidates a and b and there would be a ranking underwhich both candidates were optimal, a ranking under which neither candidate wasoptimal, a ranking that made a but not b optimal, and another ranking that made bbut not a optimal.
This state of affairs requires at least four distinct rankings, which isimpossible with only two constraints.
Thus, it is established that, at k = 2, the largestshatterable sample set has at most one candidate.56Riggle Ranking Hypotheses in OTIf S is the largest shatterable sample for k constraints, then, at k + 1, the size ofthe largest shatterable sample is |S|+ 1.
If this were not the case, there would be ashatterable sample X such that |X| ?
|S|+ 2 for k + 1 constraints.
However, becauseshatterable samples have shatterable minors (Lemma 4), this would mean that therewas a shatterable sample of size |S|+ 1 for k constraints, contrary to the assumptionthat S was largest.
Given the base case that |S| = 1 when k = 2, the cardinality ofshatterable samples is thus bounded at k ?
1 as required.
The bound of k ?
1 defines the limiting case that is obtained when there can be can-didates in the sample space for any ERC set.
In actual practice, the specific details ofthe constraints in CON and the range of ways that they interact will determine whichelements of the powerset of the set of k-length ERCs are associated with candidates inthe sample space.
This means that the VC dimension of a specific constraint set CONcan be much lower than |CON| ?
1.
Nonetheless, the result that the VCD of OT can beat most |CON| ?
1 is propitious for the learnability of Optimality Theoretic grammars.5.
ConclusionsBounding the VC dimension of OT according to the number of constraints in CON es-tablishes a general property of the sets of ranking hypotheses that can be associatedwith sets of candidates.
This bound is independent of any assumptions about how theERC sets for candidates are computed, independent of any assumptions about howoptimizations are computed, and independent of any assumptions about the formalproperties of constraints other than that they map candidates to N.The linear growth of the VCD with |CON| = k provides a very general positivelearnability result for OT.
Blumer et al (1989), building on the learning model of Valiant(1984), define a concept class C as uniformly learnable if there is a learning algorithmA such that, for any error thresholdand confidence level ?, if A is given m trainingsamples randomly drawn according to a probability distribution ?
over the samplespace, then A has at least probability ?
of generating a hypothesis whose likelihoodof misclassifying any point in the sample space drawn randomly according to ?
is lessthan.
Blumer et al link the VC dimension to learnability by showing that conceptclasses are uniformly learnable if and only if they have a finite VCD.
Moreover, theyshow that upper bounds on m can be established for learning that depend only on theVC dimension of the concept class to be learned.
The bound on m according to d =VCDfrom Blumer et al is given in Equation (1).m ?
?4(d ln 12+ ln2?)?
(1)This is a worst-case bound that holds for the most adversarial probability distributionsover the sample space and the worst consistent learning algorithms (i.e., algorithmsthat are consistent in that they correctly classify all data in the training set, but worst-case in that they err maximally on all unobserved data).
Specific OT learning algorithmsthat have tighter bounds and non-worst-case probability distributions over samples willcertainly present a different picture.For a concrete example of OT learning, consider a version of Prince and Smolensky?s(1993) basic CV syllable theory in which candidates are mappings from {C, V}* to {C,V, .
}*, and for each input i ?
{C, V}*, the candidate set produced by GEN(i) represents57Computational Linguistics Volume 35, Number 1all ways of modifying i through deletion and insertion of ., C, and V.3 If CON contains (i)a constraint against deletion, (ii) a constraint against V insertion, (iii) a constraint againstC insertion, (iv) a constraint against syllables with codas, and (v) a constraint againstsyllables without onsets, then the range of possible rankings of these five constraintsallows for 120 different grammars which in turn define twelve different languages(i.e., twelve subsets of the sample space X = {C, V}*?
{C, V, .
}*).If learners are trained with positive evidence in the form of optimal input ?
outputmappings, then the probability distribution over the sample space can be characterizedin terms of the probability distribution over the input strings in {C, V}*.
Each optimalcandidate a = (i ?
o) provides information about the teacher?s ranking in the form ofERCS(a) = {erc(a, b)|b = (i ?
x) ?
GEN(i)}.
Riggle (2004) shows that, because the func-tions in this system are all rational (i.e., finite state representable), the set ERCS(a) canbe derived via an algorithm called CONTENDERS.
In this system, ERCS(a) can containfrom zero to twelve ERCs.
The zero-ERC cases arise for input strings that share thesame optimal output under all rankings (i.e., /CV/?[.CV.]).
The sets top out at twelvebecause there are never more than twelve contenders (i.e., non-harmonically-boundedcandidates) for any given input string.
The twelve ERC bound is a consequence of thefact the 120 rankings only realize twelve distinct languages.4As noted in Section 1, candidates with the same input cannot co-occur in shatterablesets.
Because of this, the bound on shatterable samples established in Section 4 carriestransparently over to the more general case where learners are trained with optimal(i, o) mappings and then tested with novel inputs.
Because the set of contenders isdetermined solely by GEN and CON (which the learner is presumed to have access to)if the learner can compute CONTENDERS(i), then testing on novel inputs reduces tohaving the learner select one optimal candidate from the set of contenders, whichin turn reduces to binary questions of harmonic inequality between pairs a and b inCONTENDERS(i), which in turn reduces to the question of which of erc(a, b) or erc(b, a) isconsistent with the ERCs gleaned from previous observations.This is merely one sketch of how the learning problem in OT can be formulatedso that the VC dimension can predict its success.
There are undoubtedly other possibleformulations.
Furthermore, as noted, real-world cases will often contain details that aremore relevant than the VC dimension in predicting learnability.
For instance, in syl-lable structure grammar just described, there are inputs for which the CONTENDERSalgorithm generates one candidate per language in the factorial typology.
In such acase, the ERC set for a single optimal candidate can serve as a ?global trigger?
that issufficient to uniquely identify the teacher?s language.
Further analysis with specificconstraints and OT learning algorithms like Recursive Constraint Demotion (Tesar 1995,1997, 1998; Tesar and Smolensky 1993, 2000), the Gradual Learning Algorithm (Boersma1997, 1998; Boersma and Hayes 2001), and the ERC-Union learner (Riggle 2004) willsurely yield further insights and a less abstract picture of learning in Optimality Theory.The VCD is an extremely robust metric that characterizes hardness inmany learningframeworks (Haussler, Kearns, and Schapire 1992) and is applicable without any as-sumptions other than that the learner is consistent.
Any learner that bases its hypotheseson the union of the ERCs associated with the data on which it is trained is guaranteed tobe consistent, and thus an extremely simple ERC-union learner can learn OT grammars3 C and V represent consonants and vowels respectively and ?.?
represents a syllable boundary marker.4 Riggle (2004) extends Prince and Smolensky?s nine-way factorial typology to twelve with a slightly looserGEN.
In this case, because log2 12 = 4, the ERC-based VCD bound is the same as that obtained by thefinitude of the typology.
Usually, however, we do not have the luxury of knowing the size of C.58Riggle Ranking Hypotheses in OTfrom random training texts whose size m is linear in k. This linear bound on the re-lationship between k and sample complexity is a nice tightening of the k log2 k boundthat follows from the finitude of k!
and contrasts starkly with pessimistic assessmentsof learnability suggested by the factorial relationship between k and the number ofpossible grammars.ReferencesAnderson, Alan R. and Nuel D. Belnap, Jr.1975.
Entailment - The Logic of Relevance andNecessity.
Princeton University Press.Blumer, Anselm, Andrzej Ehrenfeucht,David Haussler, and Manfred K.Warmuth.
1989.
Learnability and theVapnik-Chervonenkis dimension.
Journalof the ACM, 36(4):929?965.Boersma, Paul.
1997.
How we learn variation,optionality, and probability.
Proceedings ofthe Institute of Phonetic Sciences, 21:43?58.Boersma, Paul.
1998.
Functional Phonology:Formalizing the Interactions betweenArticulatory and Perceptual Drives.
Ph.D.thesis, The Hague.Boersma, Paul and Bruce Hayes.
2001.Empirical tests of the gradual learningalgorithm.
Linguistic Inquiry, 32:45?86.Dijkstra, Edsger.
W. 1959.
A note ontwo problems in connexion with graphs.Numerische Mathematik, 1:269?271.Ellison, T. Mark.
1994.
Phonologicalderivation in optimality theory.
InProceedings of the Fifteenth Conference onComputational Linguistics, pages 1007?1013,Kyoto, Japan.
doi:dx.doi.org/10.3115/991250.991312.Haussler, David, Michael Kearns, and RobertSchapire.
1992.
Bounds on the samplecomplexity of Bayesian learning usinginformation theory and the VC dimension.Technical Report UCSC-CRL-91-44.Littlestone, Nick.
1988.
Learning quicklywhen irrelevant attributes abound:A new linear-threshold algorithm.
MachineLearning, 2(4):285?318.Prince, Alan.
2002.
Entailed ranking arguments.ROA 500.
Available at http://roa.rutgers.edu.Prince, Alan and Paul Smolensky.
1993.Optimality Theory: Constraint Interaction inGenerative Grammar.
Blackwell, Malden,MA.Riggle, Jason.
2004.
Generation, Recognition,and Learning in Finite State OptimalityTheory.
Ph.D. thesis, University ofCalifornia, Los Angeles.Samek-Lodovici, Vieri and Alan Prince.
1999.Optima.
ROA 785.
Available athttp://roa.rutgers.edu.Tesar, Bruce.
1995.
Computational OptimalityTheory.
Ph.D. thesis, University ofColorado.Tesar, Bruce.
1997.
Multi-recursive constraintdemotion.
ROA 197.
Available athttp://roa.rutgers.edu.Tesar, Bruce.
1998.
Error-driven learning inOptimality Theory via the efficientcomputation of optimal forms.
In Is theBest Good Enough?
Optimality andCompetition in Syntax, ed.
Pilar Barbosa,Danny Fox, Paul Hagstran, Martha J.McGinnis, and David Pesetsky.
MIT Press,Cambridge, MA.Tesar, Bruce and Paul Smolensky.
1993.The learnability of optimality theory:An algorithm and some basic complexityresults.
Unpublished manuscript.Department of Computer Science& Institute of Cognitive Science,University of Colorado at Boulder.Tesar, Bruce and Paul Smolensky.
2000.Learnability in Optimality Theory.
MIT Press,Cambridge, MA.Valiant, Leslie G. 1984.
A theory of thelearnable.
Communications of the ACM,27(11):1134?1142.Vapnik, V. N. and A. Chervonenkis.
1971.On the uniform convergence of relativefrequencies of events to their probabilities.Theory of Probability and its Applicaions,16:264?280.59
