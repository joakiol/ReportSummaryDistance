Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 829?838,Honolulu, October 2008. c?2008 Association for Computational LinguisticsN -gram Weighting: Reducing Training Data Mismatchin Cross-Domain Language Model EstimationBo-June (Paul) Hsu, James GlassMIT Computer Science and Artificial Intelligence Laboratory32 Vassar Street, Cambridge, MA, 02139 USA{bohsu,glass}@csail.mit.eduAbstractIn domains with insufficient matched trainingdata, language models are often constructedby interpolating component models trainedfrom partially matched corpora.
Since the n-grams from such corpora may not be of equalrelevance to the target domain, we proposean n-gram weighting technique to adjust thecomponent n-gram probabilities based on fea-tures derived from readily available segmen-tation and metadata information for each cor-pus.
Using a log-linear combination of suchfeatures, the resulting model achieves up to a1.2% absolute word error rate reduction over alinearly interpolated baseline language modelon a lecture transcription task.1 IntroductionMany application domains in machine learning suf-fer from a dearth of matched training data.
However,partially matched data sets are often available inabundance.
Past attempts to utilize the mismatcheddata for training often result in models that exhibitbiases not observed in the target domain.
In thiswork, we will investigate the use of the often readilyavailable data segmentation and metadata attributesassociated with each corpus to reduce the effect ofsuch bias.
We will examine this approach in the con-text of language modeling for lecture transcription.Compared with other types of audio data, lecturespeech often exhibits a high degree of spontaneityand focuses on narrow topics with special termi-nologies (Glass et al, 2004).
While we may haveexisting transcripts from general lectures or writtentext on the precise topic, training data that matchesboth the style and topic of the target lecture is oftenscarce.
Thus, past research has investigated variousadaptation and interpolation techniques to make useof partially matched corpora (Bellegarda, 2004).Training corpora are often segmented into docu-ments with associated metadata, such as title, date,and speaker.
For lectures, if the data contains evena few lectures on linear algebra, conventional lan-guage modeling methods that lump the documentstogether will tend to assign disproportionately highprobability to frequent terms like vector and matrix.Can we utilize the segmentation and metadata infor-mation to reduce the biases resulting from trainingdata mismatch?In this work, we present such a technique wherewe weight each n-gram count in a standard n-gramlanguage model (LM) estimation procedure by a rel-evance factor computed via a log-linear combina-tion of n-gram features.
Utilizing features that cor-relate with the specificity of n-grams to subsets ofthe training documents, we effectively de-emphasizeout-of-domain n-grams.
By interpolating models,such as general lectures and course textbook, thatmatch the target domain in complementary ways,and optimizing the weighting and interpolation pa-rameters jointly, we allow each n-gram probabil-ity to be modeled by the most relevant interpolationcomponent.
Using a combination of features derivedfrom multiple partitions of the training documents,the resulting weighted n-gram model achieves upto a 1.2% absolute word error rate (WER) reduc-tion over a linearly interpolated baseline on a lecturetranscription task.8292 Related WorkTo reduce topic mismatch in LM estimation, we(2006) have previously assigned topic labels to eachword by applying HMM-LDA (Griffiths et al, 2005)to the training documents.
Using an ad hoc methodto reduce the effective counts of n-grams endingon topic words, we achieved better perplexity andWER than standard trigram LMs.
Intuitively, de-emphasizing such n-grams will lower the transitionprobability to out-of-domain topic words from thetraining data.
In this work, we further explore thisintuition with a principled feature-based model, in-tegrated with LM smoothing and estimation to allowsimultaneous optimization of all model parameters.As Gao and Lee (2000) observed, even purportedmatched training data may exhibit topic, style, ortemporal biases not present in the test set.
To ad-dress the mismatch, they partition the training doc-uments by their metadata attributes and compute ameasure of the likelihood that an n-gram will appearin a new partitioned segment.
By pruning n-gramswith generality probability below a given threshold,the resulting model achieves lower perplexity than acount-cutoff model of equal size.
Complementary toour work, this technique also utilizes segmentationand metadata information.
However, our model en-ables the simultaneous use of all metadata attributesby combining features derived from different parti-tions of the training documents.3 N -gram WeightingGiven a limited amount of training data, an n-gramappearing frequently in a single document may beassigned a disproportionately high probability.
Forexample, an LM trained from lecture transcriptstends to assign excessive probability to words fromobserved lecture topics due to insufficient coverageof the underlying document topics.
On the otherhand, excessive probabilities may also be assignedto n-grams appearing consistently across documentswith mismatched style, such as the course textbookin the written style.
Traditional n-gram smoothingtechniques do not address such issues of insufficienttopic coverage and style mismatch.One approach to addressing the above issues isto weight the counts of the n-grams according tothe concentration of their document distributions.Assigning higher weights to n-grams with evenlyspread distributions captures the style of a data set,as reflected across all documents.
On the other hand,emphasizing the n-grams concentrated within a fewdocuments focuses the model on the topics of theindividual documents.In theory, n-gram weighting can be applied to anysmoothing algorithm based on counts.
However,because many of these algorithms assume integercounts, we will apply the weighting factors to thesmoothed counts, instead.
For modified Kneser-Neysmoothing (Chen and Goodman, 1998), applying n-gram weighting yields:p(w|h) = ?(hw)c??
(hw)?w ?(hw)c?
(hw)+ ?(h)p(w|h?
)where p(w|h) is the probability of word w given his-tory h, c?
is the adjusted Kneser-Ney count, c??
is thediscounted count, ?
is the n-gram weighting factor,?
is the normalizing backoff weight, and h?
is thebackoff history.Although the weighting factor ?
can in general beany function of the n-gram, in this work, we willconsider a log-linear combination of n-gram fea-tures, or ?
(hw) = exp(?
(hw) ?
?
), where ?
(hw)is the feature vector for n-gram hw and ?
specifiesthe parameter vector to be learned.
To better fit thedata, we allow independent parameter vectors ?o foreach n-gram order o.
Note that with ?
(hw) = 1, themodel degenerates to the original modified Kneser-Ney formulation.
Furthermore, ?
only specifies therelative weighting among n-grams with a commonhistory h. Thus, scaling ?
(hw) by an arbitrary func-tion g(h) has no effect on the model.In isolation, n-gram weighting shifts probabilitymass from out-of-domain n-grams via backoff tothe uniform distribution to improve the generalityof the resulting model.
However, in combinationwith LM interpolation, it can also distribute prob-abilities to LM components that better model spe-cific n-grams.
For example, n-gram weighting cande-emphasize off-topic and off-style n-grams fromgeneral lectures and course textbook, respectively.Tuning the weighting and interpolation parametersjointly further allows the estimation of the n-gramprobabilities to utilize the best matching LM com-ponents.8303.1 FeaturesTo address the issue of sparsity in the documenttopic distribution, we can apply n-gram weight-ing with features that measure the concentration ofthe n-gram distribution across documents.
Simi-lar features can also be computed from documentspartitioned by their categorical metadata attributes,such as course and speaker for lecture transcripts.Whereas the features derived from the corpus docu-ments should correlate with the topic specificity ofthe n-grams, the same features computed from thespeaker partitions might correspond to the speakerspecificity.
By combining features from multiplepartitions of the training data to compute the weight-ing factors, n-gram weighting allows the resultingmodel to better generalize across categories.To guide the presentation of the n-gram featuresbelow, we will consider the following example parti-tion of the training corpus.
Words tagged by HMM-LDA as topic words appear in bold.A B A A C C A B A BB A A C C B A A B AA C B A A C A B B AOne way to estimate the specificity of an n-gramacross partitions is to measure the n-gram frequencyf , or the fraction of partitions containing an n-gram.For instance, f(A) = 3/3, f(C) = 2/3.
However,as the size of each partition increases, this ratio in-creases to 1, since most n-grams have a non-zeroprobability of appearing in each partition.
Thus,an alternative is to compute the normalized entropyof the n-gram distribution across the S partitions,or h = ?1logS?Ss=1 p(s) log p(s), where p(s) is thefraction of an n-gram appearing in partition s. Forexample, the normalized entropy of the unigram C ish(C) = ?1log 3 [26 log 26 + 46 log 46 +0] = .58.
N -gramsclustered in fewer partitions have lower entropy thanones that are more evenly spread out.Following (Hsu and Glass, 2006), we also con-sider features derived from the HMM-LDA wordtopic labels.1 Specifically, we compute the empir-ical probability t that the target word of the n-gram1HMM-LDA is performed using 20 states and 50 topics witha 3rd-order HMM.
Hyperparameters are sampled with a log-normal Metropolis proposal.
The model with the highest likeli-hood from among 10,000 iterations of Gibbs sampling is used.Feature oftheithinkkmeansthesunthisisaalotofbigoofemfRandom 0.03 0.32 0.33 0.19 0.53 0.24 0.37 0.80log(c) 9.29 8.09 3.47 5.86 6.82 7.16 3.09 4.92fdoc 1.00 0.93 0.00 0.18 0.92 0.76 0.00 0.04fcourse 1.00 1.00 0.06 0.56 0.94 0.94 0.06 0.06f speaker 0.83 0.70 0.00 0.06 0.41 0.55 0.01 0.00hdoc 0.96 0.84 0.00 0.56 0.93 0.85 0.00 0.34hcourse 0.75 0.61 0.00 0.55 0.78 0.65 0.00 0.00hspeaker 0.76 0.81 0.00 0.09 0.65 0.80 0.12 0.00tdoc 0.00 0.00 0.91 1.00 0.01 0.00 0.00 0.04tcourse 0.00 0.00 0.88 0.28 0.01 0.00 0.00 1.00tspeaker 0.00 0.00 0.94 0.92 0.01 0.00 0.09 0.99Table 1: A list of n-gram weighting features.
f : n-gramfrequency, h: normalized entropy, t: topic probability.is labeled as a topic word.
In the example corpus,t(C) = 3/6, t(A C) = 2/4.All of the above features can be computed for anypartitioning of the training data.
To better illustratethe differences, we compute the features on a set oflecture transcripts (see Section 4.1) partitioned bylecture (doc), course, and speaker.
Furthermore, weinclude the log of the n-gram counts c and randomvalues between 0 and 1 as baseline features.
Table 1lists all the features examined in this work and theirvalues on a select subset of n-grams.3.2 TrainingTo tune the n-gram weighting parameters ?, we ap-ply Powell?s method (Press et al, 2007) to numeri-cally minimize the development set perplexity (Hsuand Glass, 2008).
Although there is no guaranteeagainst converging to a local minimum when jointlytuning both the n-gram weighting and interpolationparameters, we have found that initializing the pa-rameters to zero generally yields good performance.4 Experiments4.1 SetupIn this work, we evaluate the perplexity and WER ofvarious trigram LMs trained with n-gram weightingon a lecture transcription task (Glass et al, 2007).The target data consists of 20 lectures from an in-troductory computer science course, from which wewithhold the first 10 lectures for the development831Dataset # Words # Sents # DocsTextbook 131,280 6,762 271Lectures 1,994,225 128,895 230Switchboard 3,162,544 262,744 4,876CS Dev 93,353 4,126 10CS Test 87,527 3,611 10Table 2: Summary of evaluation corpora.Perplexity WERModel Dev Test Dev TestFixKN(1) 174.7 196.7 34.9% 36.8%+ W(hdoc) 172.9 194.8 34.7% 36.7%FixKN(3) 168.6 189.3 34.9% 36.9%+ W(hdoc) 166.8 187.8 34.6% 36.6%FixKN(10) 167.5 187.6 35.0% 37.2%+ W(hdoc) 165.3 185.8 34.7% 36.8%KN(1) 169.7 190.4 35.0% 37.0%+ W(hdoc) 167.3 188.2 34.8% 36.7%KN(3) 163.4 183.1 35.0% 37.1%+ W(hdoc) 161.1 181.2 34.7% 36.8%KN(10) 162.3 181.8 35.1% 37.1%+ W(hdoc) 160.1 180.0 34.8% 36.8%Table 3: Performance of n-gram weighting with a varietyof Kneser-Ney settings.
FixKN(d): Kneser-Ney with dfixed discount parameters.
KN(d): FixKN(d) with tunedvalues.
W(feat): n-gram weighting with feat feature.set (CS Dev) and use the last 10 for the test set(CS Test).
For training, we will consider the coursetextbook with topic-specific vocabulary (Textbook),numerous high-fidelity transcripts from a variety ofgeneral seminars and lectures (Lectures), and theout-of-domain LDC Switchboard corpus of spon-taneous conversational speech (Switchboard) (God-frey and Holliman, 1993).
Table 2 summarizes allthe evaluation data.To compute the word error rate, we use a speaker-independent speech recognizer (Glass, 2003) with alarge-margin discriminative acoustic model (Chang,2008).
The lectures are pre-segmented into utter-ances via forced alignment against the referencetranscripts (Hazen, 2006).
Since all the models con-sidered in this work can be encoded as n-gram back-off models, they are applied directly during the firstrecognition pass instead of through a subsequent n-best rescoring step.Model Perplexity WERLectures 189.3 36.9%+ W(hdoc) 187.8 (-0.8%) 36.6%Textbook 326.1 43.1%+ W(hdoc) 317.5 (-2.6%) 43.1%LI(Lectures + Textbook) 141.6 33.7%+ W(hdoc) 136.6 (-3.5%) 32.7%Table 4: N -gram weighting with linear interpolation.4.2 SmoothingIn Table 3, we compare the performance of n-gramweighting with the hdoc document entropy featurefor various modified Kneser-Ney smoothing config-urations (Chen and Goodman, 1998) on the Lec-tures dataset.
Specifically, we considered varyingthe number of discount parameters per n-gram orderfrom 1 to 10.
The original and modified Kneser-Neysmoothing algorithms correspond to a setting of 1and 3, respectively.
Furthermore, we explored usingboth fixed parameter values estimated from n-gramcount statistics and tuned values that minimize thedevelopment set perplexity.In this task, while the test set perplexity tracksthe development set perplexity well, the WER corre-lates surprisingly poorly with the perplexity on boththe development and test sets.
Nevertheless, n-gramweighting consistently reduces the absolute test setWER by a statistically significant average of 0.3%,according to the Matched Pairs Sentence SegmentWord Error test (Pallet et al, 1990).
Given that weobtained the lowest development set WER with thefixed 3-parameter modified Kneser-Ney smoothing,all subsequent experiments are conducted using thissmoothing configuration.4.3 Linear InterpolationApplied to the Lectures dataset in isolation, n-gramweighting with the hdoc feature reduces the test setWER by 0.3% by de-emphasizing the probabilitycontributions from off-topic n-grams and shiftingtheir weights to the backoff distributions.
Ideallythough, such weights should be distributed to on-topic n-grams, perhaps from other LM components.In Table 4, we present the performance of apply-ing n-gram weighting to the Lectures and Textbookmodels individually versus in combination via linearinterpolation (LI), where we optimize the n-gram832Model Perplexity WERLI(Lectures + Textbook) 141.6 33.7%+ W(Random) 141.5 (-0.0%) 33.7%+ W(log(c)) 137.5 (-2.9%) 32.8%+ W(fdoc) 136.3 (-3.7%) 32.8%+ W(fcourse) 136.5 (-3.6%) 32.7%+ W(f speaker) 138.1 (-2.5%) 33.0%+ W(hdoc) 136.6 (-3.5%) 32.7%+ W(hcourse) F 136.1 (-3.9%) 32.7%+ W(hspeaker) 138.6 (-2.1%) 33.1%+ W(tdoc) 134.8 (-4.8%) 33.2%+ W(tcourse) 136.4 (-3.6%) 33.1%+ W(tspeaker) 136.4 (-3.7%) 33.2%Table 5: N -gram weighting with various features.weighting and interpolation parameters jointly.
Theinterpolated model with n-gram weighting achievesperplexity improvements roughly additive of the re-ductions obtained with the individual models.
How-ever, the 1.0% WER drop for the interpolated modelsignificantly exceeds the sum of the individual re-ductions.
Thus, as we will examine in more detailin Section 5.1, n-gram weighting allows probabili-ties to be shifted from less relevant n-grams in onecomponent to more specific n-grams in another.4.4 FeaturesWith n-gram weighting, we can model the weight-ing function ?
(hw) as a log-linear combination ofany n-gram features.
In Table 5, we show the effectvarious features have on the performance of linearlyinterpolating Lectures and Textbook.
As the docu-ments from the Lectures dataset is annotated withcourse and speaker metadata attributes, we includethe n-gram frequency f , entropy h, and topic proba-bility t features computed from the lectures groupedby the 16 unique courses and 299 unique speakers.2In terms of perplexity, the use of the Randomfeature has negligible impact on the test set per-formance, as expected.
On the other hand, thelog(c) count feature reduces the perplexity by nearly3%, as it correlates with the generality of the n-grams.
By using features that leverage the infor-mation from document segmentation and associated2Features that are not applicable to a particular corpus (e.g.hcourse for Textbook) are removed from the n-gram weightingcomputation for that component.
Thus, models with course andspeaker features have fewer tunable parameters than the others.metadata, we are generally able to achieve furtherperplexity reductions.
Overall, the frequency andentropy features perform roughly equally.
However,by considering information from the more sophisti-cated HMM-LDA topic model, the topic probabilityfeature tdoc achieves significantly lower perplexitythan any other feature in isolation.In terms of WER, the Random feature againshows no effect on the baseline WER of 33.7%.However, to our surprise, the use of the simplelog(c) feature achieves nearly the same WER im-provement as the best segmentation-based feature,whereas the more sophisticated features computedfrom HMM-LDA labels only obtain half of the re-duction even though they have the best perplexities.When comparing the performance of different n-gram weighting features on this data set, the per-plexity correlates poorly with the WER, on both thedevelopment and test sets.
Fortunately, the featuresthat yield the lowest perplexity and WER on the de-velopment set alo yield one of the lowest perplex-ities and WERs, respectively, on the test set.
Thus,during feature selection for speech recognition ap-plications, we should consider the development setWER.
Specifically, since the differences in WERare often statistically insignificant, we will select thefeature that minimizes the sum of the developmentset WER and log perplexity, or cross-entropy.3In Tables 5 and 6, we have underlined the per-plexities and WERs of the features with the lowestcorresponding development set values (not shown)and bolded the lowest test set values.
The featuresthat achieve the lowest combined cross-entropy andWER on the development set are starred.4.5 Feature CombinationUnlike most previous work, n-gram weighting en-ables a systematic integration of features computedfrom multiple document partitions.
In Table 6, wecompare the performance of various feature combi-nations.
We experiment with incrementally addingfeatures that yield the lowest combined developmentset cross-entropy and WER.
Overall, this metric ap-pears to better predict the test set WER than eitherthe development set perplexity or WER alone.3The choice of cross-entropy instead of perplexity is par-tially motivated by the linear correlation reported by (Chen andGoodman, 1998) between cross-entropy and WER.833Features Perplexity WERhcourse 136.1 32.7%+ log(c) 135.4 (-0.5%) 32.6%+ fdoc 135.1 (-0.7%) 32.6%+ hdoc 135.6 (-0.5%) 32.6%+ tdoc F 133.2 (-2.1%) 32.6%+ fcourse 136.0 (-0.1%) 32.6%+ tcourse 134.8 (-1.0%) 32.9%+ f speaker 136.0 (-0.1%) 32.6%+ hspeaker 136.1 (-0.0%) 32.8%+ tspeaker 134.7 (-1.0%) 32.7%hcourse + tdoc 133.2 32.6%+ log(c) 132.8 (-0.3%) 32.5%+ fdoc F 132.8 (-0.4%) 32.5%+ hdoc 133.0 (-0.2%) 32.5%+ fcourse 133.1 (-0.1%) 32.5%+ tcourse 133.0 (-0.1%) 32.6%+ f speaker 133.1 (-0.1%) 32.5%+ hspeaker 133.2 (-0.0%) 32.6%+ tspeaker 133.1 (-0.1%) 32.7%Table 6: N -gram weighting with feature combinations.Using the combined feature selection technique,we notice that the greedily selected features tend todiffer in the choice of document segmentation andfeature type, suggesting that n-gram weighting caneffectively integrate the information provided by thedocument metadata.
By combining features, we areable to further reduce the test set WER by a statis-tically significant (p < 0.001) 0.2% over the bestsingle feature model.4.6 Advanced InterpolationWhile n-gram weighting with all three features isable to reduce the test set WER by 1.2% over thelinear interpolation baseline, linear interpolation isnot a particularly effective interpolation technique.In Table 7, we compare the effectiveness of n-gramweighting in combination with better interpolationtechniques, such as count merging (CM) (Bacchi-ani et al, 2006) and generalized linear interpolation(GLI) (Hsu, 2007).
As expected, the use of moresophisticated interpolation techniques decreases theperplexity and WER reductions achieved by n-gramweighting by roughly half for a variety of featurecombinations.
However, all improvements remainstatistically significant.Model Perplexity WERLinear(L + T) 141.6 33.7%+ W(hcourse) 136.1 (-3.9%) 32.7%+ W(tdoc) 133.2 (-5.9%) 32.6%+ W(fdoc) 132.8 (-6.2%) 32.5%CM(L + T) 137.9 33.0%+ W(hcourse) 135.5 (-1.8%) 32.4%+ W(tdoc) 133.4 (-3.3%) 32.4%+ W(fdoc) 133.2 (-3.5%) 32.4%GLIlog(1+c?
)(L + T) 135.9 33.0%+ W(hcourse) 133.0 (-2.2%) 32.4%+ W(tdoc) 130.6 (-3.9%) 32.4%+ W(fdoc) 130.5 (-4.2%) 32.4%Table 7: Effect of interpolation technique.
L: Lectures, T:Textbook.Feature Parameter Valueshdoc ?L = [3.42, 1.46, 0.12]?T = [?0.45,?0.35,?0.73][?L, ?T] = [0.67, 0.33]tdoc ?L = [?2.33,?1.63,?1.19]?T = [1.05, 0.46, 0.12][?L, ?T] = [0.68, 0.32]Table 8: N -gram weighting parameter values.
?L, ?T:parameters for each order of the Lectures and Textbooktrigram models, ?L,?T: linear interpolation weights.Although the WER reductions from better inter-polation techniques are initially statistically signif-icant, as we add features to n-gram weighting, thedifferences among the interpolation methods shrinksignificantly.
With all three features combined, thetest set WER difference between linear interpolationand generalized linear interpolation loses its statisti-cal significance.
In fact, we can obtain statisticallythe same WER of 32.4% using the simpler model ofcount merging and n-gram weighting with hcourse.5 Analysis5.1 Weighting ParametersTo obtain further insight into how n-gram weightingimproves the resulting n-gram model, we present inTable 8 the optimized parameter values for the linearinterpolation model between Lectures and Textbookusing n-gram weighting with hdoc and tdoc features.Using ?
(hw) = exp(?
(hw) ?
?)
to model the n-gram weights, a positive value of ?i corresponds to834100 300 1000 3000 10000Development Set Size (Words)132134136138140142144PerplexityLICMGLILI+WCM+WGLI+WFigure 1: Test set perplexity vs. development set size.increasing the weights of the ith order n-grams withpositive feature values.For the hdoc normalized entropy feature, valuesclose to 1 correspond to n-grams that are evenly dis-tributed across the documents.
When interpolatingLectures and Textbook, we obtain consistently pos-itive values for the Lectures component, indicatinga de-emphasis on document-specific terms that areunlikely to be found in the target computer sciencedomain.
On the other hand, the values correspond-ing to the Textbook component are consistently neg-ative, suggesting a reduced weight for mismatchedstyle terms that appear uniformly across textbooksections.For tdoc, values close to 1 correspond to n-gramsending frequently on topic words with uneven dis-tribution across documents.
Thus, as expected, thesigns of the optimized parameter values are flipped.By de-emphasizing topic n-grams from off-topiccomponents and style n-grams from off-style com-ponents, n-gram weighting effectively improves theperformance of the resulting language model.5.2 Development Set SizeSo far, we have assumed the availability of a largedevelopment set for parameter tuning.
To obtaina sense of how n-gram weighting performs withsmaller development sets, we randomly select utter-ances from the full development set and plot the testset perplexity in Figure 1 as a function of the devel-opment set size for various modeling techniques.As expected, GLI outperforms both LI and CM.However, whereas LI and CM essentially convergein test set perplexity with only 100 words of devel-100 300 1000 3000 10000Development Set Size (Words)32.032.533.033.534.034.535.0WERLICMGLILI+WCM+WGLI+WFigure 2: Test set WER vs. development set size.opment data, it takes about 500 words before GLIconverges due to the increased number of parame-ters.
By adding n-gram weighting with the hcoursefeature, we see a significant drop in perplexity forall models at all development set sizes.
However,the performance does not fully converge until 3,000words of development set data.As shown in Figure 2, the test set WER behavesmore erratically, as the parameters are tuned to min-imize the development set perplexity.
Overall, n-gram weighting decreases the WER significantly,except when applied to GLI with less than 1000words of development data when the perplexity ofGLI has not itself converged.
In that range, CM withn-gram weighting performs the best.
However, withmore development data, GLI with n-gram weight-ing generally performs slightly better.
From theseresults, we conclude that although n-gram weight-ing increases the number of tuning parameters, theyare effective in improving the test set performanceeven with only 100 words of development set data.5.3 Training Set SizeTo characterize the effectiveness of n-gram weight-ing as a function of the training set size, we evalu-ate the performance of various interpolated modelswith increasing subsets of the Lectures corpus andthe full Textbook corpus.
Overall, every doubling ofthe number of training set documents decreases boththe test set perplexity and WER by approximately 7points and 0.8%, respectively.
To better compare re-sults, we plot the performance difference betweenvarious models and linear interpolation in Figures 3and 4.8352 4 8 16 32 64 128 230Training Set Size (Documents)-12-10-8-6-4-2024PerplexityDifferenceLICMGLILI+WCM+WGLI+WFigure 3: Test set perplexity vs. training set size.Interestingly, the peak gain obtained from n-gramweighting with the hdoc feature appears at around16 documents for all interpolation techniques.
Wesuspect that as the number of documents initiallyincreases, the estimation of the hdoc features im-proves, resulting in larger perplexity reduction fromn-gram weighting.
However, as the diversity of thetraining set documents increases beyond a certainthreshold, we experience less document-level spar-sity.
Thus, we see decreasing gain from n-gramweighting beyond 16 documents.For all interpolation techniques, even though theperplexity improvements from n-gram weightingdecrease with more documents, the WER reductionsactually increase.
N -gram weighting showed sta-tistically significant reductions for all configurationsexcept generalized linear interpolation with less than8 documents.
Although count merging with n-gramweighting has the lowest WER for most training setsizes, GLI ultimately achieves the best test set WERwith the full training set.5.4 Training CorporaIn Table 9, we compare the performance of n-gramweighting with different combination of trainingcorpora and interpolation techniques to determineits effectiveness across different training conditions.With the exception of interpolating Lectures andSwitchboard using count merging, all other modelcombinations yield statistically significant improve-ments with n-gram weighting using hcourse, tdoc,and fdoc features.The results suggest that n-gram weighting withthese features is most effective when interpolating2 4 8 16 32 64 128 230Training Set Size (Documents)-1.4-1.2-1.0-0.8-0.6-0.4-0.20.00.2WERDifferenceLICMGLILI+WCM+WGLI+WFigure 4: Test set WER vs. training set size.Model L + T L + S T + S L + T + SLI 33.7% 36.7% 36.4% 33.6%LI + W 32.5% 36.4% 35.7% 32.5%CM 33.0% 36.6% 35.5% 32.9%CM + W 32.4% 36.5% 35.4% 32.3%GLI 33.0% 36.6% 35.7% 32.8%GLI + W 32.4% 36.4% 35.3% 32.2%Table 9: Test set WER with various training corpus com-binations.
L: Lectures, T: Textbook, S: Switchboard, W:n-gram weighting.corpora that differ in how they match the target do-main.
Whereas the Textbook corpus is the only cor-pus with matching topic, both Lectures and Switch-board have a similar matching spoken conversa-tional style.
Thus, we see the least benefit fromn-gram weighting when interpolating Lectures andSwitchboard.
By combining Lectures, Textbook,and Switchboard using generalized linear interpola-tion with n-gram weighting using hcourse, tdoc, andfdoc features, we achieve our best test set WER of32.2% on the lecture transcription task, a full 1.5%over the initial linear interpolation baseline.6 Conclusion & Future WorkIn this work, we presented the n-gram weightingtechnique for adjusting the probabilities of n-gramsaccording to a set of features.
By utilizing featuresderived from the document segmentation and asso-ciated metadata inherent in many training corpora,we achieved up to a 1.2% and 0.6% WER reduc-tion over the linear interpolation and count mergingbaselines, respectively, using n-gram weighting ona lecture transcription task.836We examined the performance of various n-gramweighting features and generally found entropy-based features to offer the best predictive perfor-mance.
Although the topic probability featuresderived from HMM-LDA labels yield additionalimprovements when applied in combination withthe normalized entropy features, the computationalcost of performing HMM-LDA may not justify themarginal benefit in all scenarios.In situations where the document boundaries areunavailable or when finer segmentation is desired,automatic techniques for document segmentationmay be applied (Malioutov and Barzilay, 2006).Synthetic metadata information may also be ob-tained via clustering techniques (Steinbach et al,2000).
Although we have primarily focused on n-gram weighting features derived from segmentationinformation, it is also possible to consider other fea-tures that correlate with n-gram relevance.N -gram weighting and other approaches to cross-domain language modeling require a matched devel-opment set for model parameter tuning.
Thus, forfuture work, we plan to investigate the use of the ini-tial recognition hypotheses as the development set,as well as manually transcribing a subset of the testset utterances.As speech and natural language applications shifttowards novel domains with limited matched train-ing data, better techniques are needed to maximallyutilize the often abundant partially matched data.
Inthis work, we examined the effectiveness of the n-gram weighting technique for estimating languagemodels in these situations.
With similar investmentsin acoustic modeling and other areas of natural lan-guage processing, we look forward to an ever in-creasing diversity of practical speech and naturallanguage applications.Availability An implementation of the n-gramweighting algorithm is available in the MIT Lan-guage Modeling (MITLM) toolkit (Hsu and Glass,2008): http://www.sls.csail.mit.edu/mitlm/.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir constructive feedback.
This research is sup-ported in part by the T-Party Project, a joint researchprogram between MIT and Quanta Computer Inc.ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochasticgrammars.
Computer Speech & Language, 20(1):41?68.Jerome R. Bellegarda.
2004.
Statistical language modeladaptation: Review and perspectives.
Speech Commu-nication, 42(1):93?108.Hung-An Chang.
2008.
Large margin Gaussian mix-ture modeling for automatic speech recognition.
Mas-sachusetts Institute of Technology.
Masters Thesis.Stanley F. Chen and Joshua Goodman.
1998.
An empiri-cal study of smoothing techniques for language model-ing.
In Technical Report TR-10-98.
Computer ScienceGroup, Harvard University.Jianfeng Gao and Kai-Fu Lee.
2000.
Distribution-basedpruning of backoff language models.
In Proc.
Asso-ciation of Computational Linguistics, pages 579?588,Hong Kong, China.James Glass, Timothy J. Hazen, Lee Hetherington, andChao Wang.
2004.
Analysis and processing of lectureaudio data: Preliminary investigations.
In Proc.
HLT-NAACL Workshop on Interdisciplinary Approaches toSpeech Indexing and Retrieval, pages 9?12, Boston,MA, USA.James Glass, Timothy J. Hazen, Scott Cyphers, IgorMalioutov, David Huynh, and Regina Barzilay.
2007.Recent progress in the MIT spoken lecture process-ing project.
In Proc.
Interspeech, pages 2553?2556,Antwerp, Belgium.James Glass.
2003.
A probabilistic framework forsegment-based speech recognition.
Computer Speech& Language, 17(2-3):137?152.John J. Godfrey and Ed Holliman.
1993.
Switchboard-1transcripts.
Linguistic Data Consortium, Philadelphia,PA, USA.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In Advances in Neural Information ProcessingSystems 17, pages 537?544.
MIT Press, Cambridge,MA, USA.T.J.
Hazen.
2006.
Automatic alignment and error cor-rection of human generated transcripts for long speechrecordings.
In Proc.
Interspeech, Pittsburgh, PA,USA.Bo-June (Paul) Hsu and James Glass.
2006.
Style &topic language model adaptation using HMM-LDA.In Proc.
Empirical Methods in Natural Language Pro-cessing, pages 373?381, Sydney, Australia.Bo-June (Paul) Hsu and James Glass.
2008.
Iterativelanguage model estimation: Efficient data structure &algorithms.
In Proc.
Interspeech, Brisbane, Australia.837Bo-June (Paul) Hsu.
2007.
Generalized linear interpola-tion of language models.
In Proc.
Automatic SpeechRecognition and Understanding, pages 136?140, Ky-oto, Japan.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Proc.Association for Computational Linguistics, pages 25?32, Sydney, Australia.D.
Pallet, W. Fisher, and Fiscus.
1990.
Tools for the anal-ysis of benchmark speech recognition tests.
In Proc.ICASSP, Albuquerque, NM, USA.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2007.
Numerical Recipes.Cambridge University Press, 3rd edition.Michael Steinbach, George Karypis, and Vipin Kumar.2000.
A comparison of document clustering tech-niques.
Technical Report #00-034, University of Min-nesota.838
