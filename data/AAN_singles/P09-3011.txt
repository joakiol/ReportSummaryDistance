Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88?95,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPClustering Technique in Multi-Document Personal Name Disambigu-ationChen ChenKey Laboratory of Computa-tional Linguistics (PekingUniversity),Ministry of Education, Chinachenchen@pku.edu.cnHu JunfengKey Laboratory of Computa-tional Linguistics (PekingUniversity),Ministry of Education, Chinahujf@pku.edu.cnWang HoufengKey Laboratory of Computa-tional Linguistics (PekingUniversity),Ministry of Education, Chinawanghf@pku.edu.cnAbstractFocusing on multi-document personal namedisambiguation, this paper develops an agglo-merative clustering approach to resolving thisproblem.
We start from an analysis of point-wise mutual information between feature andthe ambiguous name, which brings about anovel weight computing method for feature inclustering.
Then a trade-off measure betweenwithin-cluster compactness and among-clusterseparation is proposed for stopping clustering.After that, we apply a labeling method to findrepresentative feature for each cluster.
Finally,experiments are conducted on word-basedclustering in Chinese dataset and the resultshows a good effect.1 IntroductionMulti-document named entity co-reference reso-lution is the process of determining whether anidentical name occurring in different texts refersto the same entity in the real world.
With the rap-id development of multi-document applicationslike multi-document summarization and informa-tion fusion, there is an increasing need for multi-document named entity co-reference resolution.This paper focuses on multi-document personalname disambiguation, which seeks to determineif the same name from different documents refersto the same person.This paper develops an agglomerative cluster-ing approach to resolving multi-document per-sonal name disambiguation.
In order to representtexts better, a novel weight computing methodfor clustering features is presented.
It is based onthe pointwise mutual information between theambiguous name and features.
This paper alsodevelops a trade-off point based cluster-stoppingmeasure and a labeling algorithm for each clus-ters.
Finally, experiments are conducted onword-based clustering in Chinese dataset.
Thedataset contains eleven different personal nameswith varying-sized datasets, and has 1669 texts inall.The rest of this paper is organized as follows:in Section 2 we review the related work; Section3 describes the framework; section 4 introducesour methodologies including feature weightcomputing with pointwise mutual information,cluster-stopping measure based on trade-offpoint, and cluster labeling algorithm.
These arethe main contribution of this paper; Section 5discusses our experimental result.
Finally, theconclusion and suggestions for further extensionof the work are given in Section 6.2 Related WorkDue to the varying ambiguity of personal namesin a corpus, existing approaches typically cast itas an unsupervised clustering problem based onvector space model.
The main difference amongthese approaches lies in the features, which areused to create a similarity space.
Bagga & Bald-win (1998) first performed within-document co-reference resolution, and then explored featuresin local context.
Mann & Yarowsky (2003) ex-tracted local biographical information as features.Al-Kamha and Embley (2004) clustered searchresults with feature set including attributes, linksand page similarities.
Chen and Martin (2007)explored the use of a range of syntactic and se-mantic features in unsupervised clustering ofdocuments.
Song (2007) learned the PLSA andLDA model as feature sets.
Ono et al (2008)used mixture features including co-occurrences88of named entities, key compound words, and top-ic information.
Previous works usually focus onfeature identification and feature selection.
Themethod to assign appropriate weight to each fea-ture has not been discussed widely.A major challenge in clustering analysis is de-termining the number of ?clusters?.
Therefore,clustering based approaches to this problem stillrequire estimating the number of clusters.
In Hie-rarchy clustering, it equates to determine thestopping step of clustering.
The measure to findthe ?knee?
in the criterion function curve is awell known cluster-stopping measure.
Pedersenand Kulkarni had studied this problem (Pedersenand Kulkarni, 2006).
They developed cluster-stopping measures named PK1, PK2, PK3, andpresented the Adapted Gap Statistics.After estimating the number of ?clusters?, weobtain the clustering result.
In order to label the?clusters?, the method that finding representativefeatures for each ?cluster?
is needed.
For example,the captain John Smith can be labeled as captain.Pedersen and Kulkarni (2006) selected the top Nnon-stopping word features from texts groupedin a cluster as label.3 FrameworkOn the assumption of ?one person per document?(i.e.
all mentions of an ambiguous personal namein one document refer to the same personal enti-ty), the task of disambiguating personal name intext set intends to partition the set into subsets,where each subset refer to one particular entity.Suppose the set of texts containing the ambi-guous name is denoted by D= {d1,d2,?,dn}, anddi (0<i<n+1) stands for one text.
The entitieswith the ambiguous name are denoted by a setE= {e1,e2,?,em}, where the number of entities ?m?is unknown.
The ambiguous name in each text diindicates only one entity ek.
The aim of the workis to map an ambiguous name appearing in eachtext to an entity.
Therefore, those texts indicatingthe same entity need to be clustered together.In determining whether a personal name refersto a specific entity, the personal information, so-cial network information and related topics playimportant roles,  all of which are expressed bywords in texts,.
Extracting words as features, thispaper applies an agglomerative clustering ap-proach to resolving name co-reference.
Theframework of our approach consists of the fol-lowing seven main steps:Step 1: Pre-process each text with Chineseword segmentation tool;Step 2: Extract words as features from theset of texts D;.Step 3: Represent texts d1,?,dn by featuresvectors;Step 4: Calculate similarity between texts;Step 5: Cluster the set D step by step untilonly one cluster exists;Step 6: Estimate the number of entities inaccordance with cluster-stoppingmeasure;Step 7: Assign each cluster a discriminatinglabel.This paper focuses on the Step 4, Step 6 andStep 7, i.e., feature weight computing method,clustering stopping measure and cluster labelingmethod.
They will be described in the next sec-tion in detail.Step1 and Step3 are simple, and there is nofurther description here.
In Step 2, we use co-occurrence words of the ambiguous name intexts as features.
In the process of agglomerativeclustering (see Step 5), each text is viewed as onecluster at first, and the most similar two clustersare merged together as a new cluster at eachround.
After replacing the former two clusterswith the new one, we use average linked methodto update similarity between clusters.4 Methodology4.1 Feature weightEach text is represented as a feature vector, andeach item of the vector represents the weightvalue for corresponding feature in the text.
Sinceour approach is completely unsupervised wecannot use supervised methods to selectsignificant features.
Since the weight of featurewill be adjusted well instead of feature selection,all words in set D are used as feature in ourapproach.The problem of computing feature weight isinvolved in both text clustering and text classifi-cation.
By comparing the supervised text classi-fication and unsupervised text clustering, we findthat the former one has a better performance ow-ing to the selection of features and the computingmethod of feature weight.
Firstly, in the applica-tion of supervised text classification, features canbe selected by many methods, such as, MutualInformation (MI) and Expected Cross Entropy(ECE) feature selection methods.
Secondly,model training methods, such as SVM model, aregenerally adopted by programs when to find the89optimal feature weight.
There is no training datafor unsupervised tasks, so above-mentioned me-thods are unsuitable for text clustering.In addition, we find that the text clustering forpersonal name disambiguation is different fromcommon text clustering.
System can easily judgewhether a text contains the ambiguous personalname or not.
Thus the whole collection of textscan be easily divided into two classes: texts  withor without the name.
As a result, we can easilycalculate the pointwise mutual informationbetween feature words and the personal name.To a certain extent, it represents the correlativedegree between feature words and the underlyingentity corresponding to the personal name.For these reasons, our feature weightcomputing method calculates the pointwisemutual information between personal name andfeature word.
And the value of pointwise mutualinformation will be used to expresse featureword?s weight by combining the feature?s tf (theabbreviation for term-frequency) in text and idf(the abbreviation for inverse document frequency)in dataset.
The formula of feature weight compu-ting proposed in this paper is as below, and it isneed both texts containing and not containing theambiguous personal name to form dataset D. Foreach tk in di that contains name, its mi_weight iscomputed as follow:))(||log()),MI(1log())),(log(1(),,_weight(mikkikiktdfDnametdttfdnamet?+?+=(1)And)()(||),(||/)()(||/),()()(),(),MI(2kkkkkkktdfnamedfDtnamedfDtdfnamedfDtnamedftpnameptnamepnamet?
?=?=?=(2)Where tk is a feature; name is the ambiguousname; di is the ith text in dataset; tf(tk,di)represents term frequency of feature tk in text di;df(tk), df(name) is the number of the texts con-taining tk or name in dataset D respectively;df(tk,name) is the number of texts containing bothtk and name; |D| is the number of all the texts.Formula (2) can be comprehended as: if wordtk occurs much more times in texts containing theambiguous name than in texts not containing thename, it must have some information about thename.A widely used approach for computing featureweight is tf*idf scheme as formula (3) (Saltonand Buckley.
1998), which only uses the textscontaining the ambiguous name.
We denote it byold_weight .
For each tk in di containing name,the old_weight is computed as follow:)),()(log())),(log(1(),,(old_weightnametdfnamedfdttfdnametkikik?+=               (3)The first term on the right side is tf, and thesecond term is idf.
If the idf scheme is computedin the whole dataset D for reducing noise, theweight computing formula can be expressed asfollow, and is denoted by imp_weight:))(|D|log())),(log(1(),_weight(impkikiktdfdttfdt?+= (4)Before clustering, the similarity between textsis computed by cosine value of the anglebetween vectors (such as dx, dy in formula (5)):yxyxyx ddddd,d ?
?=)cos(                               (5)Each item of the vector (i.e.
dx, dy) representsthe weight value for corresponding feature in thetext.4.2 Cluster-stopping measureThe process of clustering will produce n clusterresults, one for each step.
Independent ofclustering algorithm, the cluster stopping meas-ure should choose the cluster results which canrepresent the structure of data.A fundamental and difficult problem in clusteranalysis is to measure the structure of clusteringresult.
The geometric structure is a representativemethod.
It defines that a ?good?
clustering re-sults should make data points from one cluster?compact?, while data points from different clus-ter are ?separate?
as far as possible.
The indica-tors should quantify the ?compactness?
and ?se-paration?
for clusters, and combine both.
In thestudy of cluster stopping measures by Pedersenand Kulkarni (2006), the criterion functions de-fines text similarity based on cosine value of theangle between vectors.
Their cluster-stoppingmeasures focused on finding the ?knee?
of crite-rion function.Our cluster-stopping measure is also based onthe geometric structure of dataset.
The measureaims to find the trade-off point between within-cluster compactness and among-clusterseparation.
Both the within-cluster compactness(Internal critical function) and among-cluster90separation (External critical function) are definedby Euclidean distance.
The hybrid criticalfunction (Hybrid critical function) combinesinternal and external criterion functions.Suppose that the given dataset contains N ref-erences, which are denoted as: d1,d2,?,dN; thedata have been repeatedly clustered into k clus-ters, where k=N,?,1; and clusters are denoted asCr, r=1,?k; and the number of references ineach cluster is nr, so nr=|Cr|.
We introduce Incrf(Internal critical function), Excrf (Externalcritical function) and Hycrf (Hybrid criticalfunction) to measure it as follows.?
?= ?
?=kik1)Incrf(iyx Cd,d2yx dd                  (6)?
?
?= ?= ??
?=kikijj jinnk1 ,11)Excrf(jyix Cd,Cd2yx dd(7)))Excrf()(Incrf(M1)Hycrf( kkk +?=          (8)Where M=Incrf(1)=Excrf(N)Figure 1 Hycrf vs. t (N-k)Chen proved the existence of the minimumvalue between (0,1) in Hycrf(k) (see Chen et al2008).
The Hycrf value in a typical Hycrf(t)curve is shown as Figure 1, where t=N-k.Function Hycrf based on Incrf and Excrf isused as the Hybrid criterion function.
The Hycrfcurve will rise sharply after the minimum, indi-cating that the cluster of several optimal parti-tions?
subsets will lead to drastic drop in clusterquality.
Thus cluster partition can be determined.Using the attributes of the Hycrf(k) curve, we putforward a new cluster-stopping measure namedtrade-off point based cluster-stopping measure(TO_CSM).
)1Hycrf()Hycrf()1Hycrf(1)TO_CSM( +?+= kkkk(9)Trade-off point based cluster-stopping meas-ure (TO_CSM) selects the k value which max-imizes TO_CSM(k), and indicates the number ofcluster.
The first term on the right side of formu-la (9) is used to minimize the value of Hycrf(k),and the second one is used to find the ?knee?
ris-ing sharply.4.3 LabelingOnce the clusters are created, we label eachentity to represent the underlying entity withsome important information.
A label isrepresented as a list of feature words, whichsummarize the information about cluster?sunderlying entity.The algorithm is outlined as follows: afterclustering N references into m clusters, for eachcluster Ck in {C1, C2, ?, Cm}, we calculate thescore of each feature for Ck and choose featuresas the label of Ck whose scores rank top N. Inparticular, the score caculated in this paper isdifferent from Pedersen and Kulkarni?s (2006).We combine pointwise mutual informationcomputing method with term frequency in clusterto compute the score.The formula of feature scoring for labeling isshown as follows:))),(log(1(),(MI),MI(),Score( nameikikkikCttfCtnametCt+?
?=(10)The calculation of MI(tk,name) is shown asformula (2) in subsection 4.1. tf(tk,Ci) representsthe total occurrence frequency of feature tk incluster Ci .
The MIname(tk,Ci) is computed as for-mula (11):)()(||),(||/)()(||/),()()(),()C,(MI2ikikikikikikiknameCdftdfDCtdfDCdftdfDCtdfCptpCtpt?
?=?=?=(11)In formula (10), the weight of stopping wordscan be reduced by the first item.
The second itemcan increase the weight of words with high dis-tinguishing ability for a certain ambiguous name.The third item of formula (10) gives higherscores to features whose frequency are higher.00.511.51 8 15 22 29 36 43 50 57 64 71 78 85 92 99 106113120Hycrf(t)915 Experiment5.1 DataThe dataset is from WWW, and contains 1,669texts with eleven real ambiguous personal names.Such raw texts containing ambiguous names arecollected via search engine1, and most of themare news.
The eleven person-names are, "??
?Liu-Yi-si ?Lewis?
", "???
Liu-Shu-zhen ", "??
Li-Qiang", "??
Li-Na", "???
Li-Gui-ying", "???
Mi-xie-er ?Michelle?
", "?
?Ma-Li ?Mary?
", "???
Yue-han-xun ?John-son?
", "??
Wang-Tao", "??
Wang-Gang", "???
Chen-Zhi-qiang".
Names like ?Michelle?,?Johnson?
are transliterated from English to Chi-nese, while names like ?Liu ?Shu-zhen?, ?Chen-Zhi-qiang?
are original Chinese personal names.Some of these names only have a few persons,while others have more persons.Table 1 shows our data set.
?#text?
presentsthe number of texts with the personal name.?#per?
presents the number of entities with thepersonal name in text dataset.
?#max?
presentsthe maximum of texts for an entity with the per-sonal name, and ?#min?
presents the minimum.#text #per #max #minLewis 120 6 25 10Liu-Shu-zhen 149 15 28 3Li-Qiang 122 7 25 9Li-Na 149 5 39 21Li-Gui-ying 150 7 30 10Michelle 144 7 25 12Mary 127 7 35 10Johnson 279 19 26 1Wang-Gang 125 18 26 1Wang-Tao 182 10 38 5Chen-Zhi-qiang 122 4 52 13Table 1 Statistics of the test datasetWe first convert all the downloaded docu-ments into plain text format to facilitate the testprocess, and pre-process them by using the seg-mentation toolkit ICTCLAS2.In testing and evaluating, we adopt B-Cubeddefinition for Precision, Recall and F-Measureas indicators (Bagga, Amit and Baldwin.
1998).F-Measure is the harmonic mean of Precisionand Recall.The definitions are presented as below:1 April.20082 http://ictclas.org/?
?= Dd dprecisionNprecision 1              (12)?
?= Dd drecallNrecall 1                           (13)recallprecisionrecallprecisionmeasureF +??=?
2      (14)where precisiond is the precision for a text d.Suppose the text d is in subset A, precisiond isthe percentage of texts in A which indicates thesame entity as d. Recalld is the recall ratio for atext d. Recalld is the ratio of number of textswhich indicates the same entity as d in A to thatin corpus D. n = | D |, D refers to a collection oftexts containing a particular name (such as WangTao, e.g.
a set of 200 texts, n = 200).
Subset A isa set formed after clustering (text included inclass), and d refers to a certain text that contain-ing "Wang Tao".5.2 ResultAll the 1669 texts in the dataset are employedduring experiment.
Each personal name disam-biguation process only clusters the texts contain-ing the ambiguous name.
After pre-processing, inorder to verify the mi_weight method for featureweight computing, all the words in texts are usedas features.Using formula (1), (3) and (4) as featureweight computing formula, we can get the evalu-ation of cluster result shown as table 2.
In thisstep, cluster-stopping measure is not used.
In-stead, the highest F-measure during clustering ishighlighted to represent the efficiency of the fea-ture weight computing method.Further more, we carry out the experiment onthe trade-off point based cluster-stoppingmeasure, and compare its cluster result withhighest F-measure and cluster result determinedby cluster-stopping measure PK3 proposed byPedersen and Kulkarni?s.
Based on theexperiment in Table 2, a structure tree isconstructed in the clustering process.
Cluster-stopping measures are used to determine whereto stop cutting the dendrogram.
As shown inTable 3, the TO-CMS method predicts theoptimal results of four names in eleven, whilePK3 method predicts the optimal result of onename, which are marked in a bold type.92old_weight imp_weight mi_weight#pre #rec #F #pre #rec #F #pre #rec #FLewis 0.9488 0.8668.
0.9059 1 1 1 1 1 1Liu-Shu-zhen 0.8004 0.7381 0.7680 0.8409 0.8004 0.8201 0.9217 0.7940 0.8531Li-Qiang 0.8057 0.6886 0.7426 0.9412 0.7968 0.8630 0.8962 0.8208 0.8569Li-Na 0.9487 0.7719 0.8512 0.9870 0.8865 0.9340 0.9870 0.9870 0.9870Li-Gui-ying 0.8871 0.9124 0.8996 0.9879 0.8938 0.9385 0.9778 0.8813 0.9271Michelle 0.9769 0.7205 0.8293 0.9549 0.8146 0.8792 0.9672 0.9498 0.9584Mary 0.9520 0.6828 0.7953 1 0.9290 0.9632 1 0.9001 0.9474Johnson 0.9620 0.8120 0.8807 0.9573 0.8083 0.8765 0.9593 0.8595 0.9067Wang-Gang 0.8130 0.8171 0.8150 0.7804 0.9326 0.8498 0.8143 0.9185 0.8633Wang-Tao 1 0.9323 0.9650 0.9573 0.9485 0.9529 0.9897 0.9768 0.9832Chen-Zhi-qiang 0.9732 0.8401 0.9017 0.9891 0.9403 0.9641 0.9891 0.9564 0.9725Average 0.9153 0.7916 0.8504 0.9451 0.8864 0.9128 0.9548 0.9131 0.9323Table 2 comparison of feature weight computing method (highest F-measure)Optimal TO-CMS PK3#pre #rec #F #pre #rec #F #pre #rec #FLewis 1 1 1 1 1 1 0.8575 1 0.9233Liu-Shuzhen 0.9217 0.7940 0.8531 0.8466 0.8433 0.8450 0.5451 0.9503 0.6928Li-Qiang 0.8962 0.8208 0.8569 0.8962 0.8208 0.8569 0.7897 0.9335 0.8556Li-Na 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9016 0.9424Li-Gui-ying 0.9778 0.8813 0.9271 0.9778 0.8813 0.9271 0.8750 0.9427 0.9076Michelle 0.9672 0.9498 0.9584 0.9482 0.9498 0.9490 0.9672 0.9498 0.9584Mary 1 0.9001 0.9474 0.8545 0.9410 0.8957 0.8698 0.9410 0.9040Johnson 0.9593 0.8595 0.9067 0.9524 0.8648 0.9066 0.2423 0.9802 0.3885Wang-Gang 0.8143 0.9185 0.8633 0.9255 0.7102 0.8036 0.5198 0.9550 0.6732Wang-Tao 0.9897 0.9768 0.9832 0.8594 0.9767 0.9144 0.9700 0.9768 0.9734Chen-Zhi-qiang 0.9891 0.9564 0.9725 0.8498 1 0.9188 0.8499 1 0.9188Average 0.9548 0.9131 0.9323 0.9179 0.9068 0.9095 0.7703 0.9574 0.8307Table 3 comparison of cluster-stopping measures?
performancename Entity Created LabelsLewis Person-1 ???(Babbitt),???????
(Sinclair Lewis),?????
(Arrow smith),???
(Literature Prize),???(Dresser),????(Howells),?????
(Swedish Academy),???????
(Sherwood Anderson),???????
(Elmer  Gan Hartley),??(street),??(award),????????
(AmericanLiterature and Arts Association)Person-2 ????
(Bank of America),????
(Bank of America),??(bank),???(investors),???
(credit card),??
(Bank of China),??(Citibank),??
(mergers and acquisitions),??
(Construction Bank),???
(executive officer),???(banking),??(stock),?????
(Ken Lewis)Person-3 ??(Single),???(Liana),??(album),???(Liana),???????(LianaLewis),???(Liana),??(airborne),??(sales),???
(Music Awards),??????
(Maria Kelly),?(List),??
(debut)?Person-4 ??????
(Carl Lewis),??
(long jump),??(Carl),???(Owens),??
(track and field),???(Burrell),?????
(the U.S. Olympic Committee),??(sprint),???(Taylors),?????(Belgrade),??????
(Verde Exxon),???
(Exxon)93Person-5 ??(Tyson),??
(King of Boxer),??
(knock down),???(heavyweight),??
(Don King),??(boxing),??(belt),??(Boxing),?(fist),??(bout),??
(Ring),WBCPerson-6 ???(Daniel),?????
(Day Lewis),??(Blood),?????????
(DanielDay Lewis),??
(There Will Be Blood),??
(left crus),??
(movie king),???????
(New York Film Critics Circles),???
(the Gold Oscar statues),???
(Best Actor in a Leading Role),???(Oscar),????
(There Will BeBlood)Table 4  Labels for ?Lewis?
clustersOn the basis of text clustering result thatobtained from the Trade-off based cluster-stopping measure experiment in Table 3, we tryour labelling method mentioned in subsection 4.3.For each cluster, we choose 12 words withhighest score as its label.
The experiment resultdemonstrates that the created label is able torepresent the category.
Take name ????
Liu-Yi-si ?Lewis??
for example, the labeling resultshown as Table 4.5.3 DiscussionFrom the test result in table 2, we find that ourfeature weight computing method can improvethe Chinese personal name clustering disambigu-ation performance effectively.
For each personalname in test dataset, the performance is im-proved obviously.
The average value of optimalF-measures for eleven names rises from 85.04%to 91.28% by using the whole dataset D for cal-culated idf, and rises from 91.28% to 93.23% byusing mi_weight.
Therefore, in the application ofChinese text clustering with constraints, we cancompute pointwise mutual information betweenconstraints and feature, and it can be mergedwith feature weight value to improve the cluster-ing performance.We can see from table 3 that trade-off pointbased cluster-stopping measure (TO_CSM) per-forms much better than PK3.
According to theexperimental results, PK3 measure is not thatrobust.
The optimal number of clusters can bedetermined for certain data.
However, we foundthat it did not apply to all cases.
For example, itobtains the optimal estimation result for data?Michelle?, as for ?Liu Shuzhen?, ?Wang Gang?and ?Johnson?, the results are extremely bad.The better result is achieved by using TO_CSMmeasure, and the selected results are closer to theoptimal value.
The PK3 measure uses the meanand the standard deviation to deduce, and itsprocesses are more complicated than TO_CSM?s.Our cluster labeling method computes the fea-tures?
score with formula (10).
From the labelingresults sample shown in Table 4, we can see thatall of the labels are representative.
Most of themare person and organizations?
name, and the restare key compound words.
Therefore, when theclustering performance is good, the quality ofcluster labels created by our method is also good.6 Future WorkThis paper developed a clustering algorithm ofmulti-document personal name disambiguation,and put forward a novel feature weight compu-ting method for vector space model.
This methodcomputes weight with the pointwise mutual in-formation between the personal name and feature.We also study a hybrid criterion function basedon trade-off point and put forward the trade-offpoint cluster-stopping measure.
At last, we expe-riment on our score computing method for clus-ter labeling.Unsupervised personal name disambiguationtechniques can be extended to address the prob-lem of unsupervised Entity Resolution and unsu-pervised word sense discrimination.
We will at-tempt to apply the feature weight computing me-thod to these fields.One of the main directions of our future workwill be how to improve the performance of per-sonal name disambiguation.
Computing weightbased on a window around names may be helpful.Moreover, word-based text features haven?tsolved two difficult problems of natural languageproblems: Synonym and Polysemy, which se-riously affect the precision and efficiency ofclustering algorithms.
Text representation basedon concept and topic may solve the problem.AcknowledgmentsThis research is supported by National NaturalScience Foundation of Chinese (No.60675035)and Beijing Natural Science Foundation(No.4072012)94ReferencesAl-Kamha.
R. and D. W. Embley.
2004.
Groupingsearch-engine returned citations for person-namequeries.
In Proceedings of WIDM?04, 96-103,Washington, DC, USA.Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector spacemodel.
In Proceedings of 17th International Con-ference on Computational Linguistics, 79?85.Bagga, Amit and B. Baldwin.
1998.
Algorithms forscoring co-reference chains.
In Proceedings of theFirst International Conference on Language Re-sources and Evaluation Workshop on Linguisticco-reference.Chen Ying and James Martin.
2007.
Towards RobustUnsupervised Personal Name Disambiguation,EMNLP 2007.Chen Lifei, Jiang Qingshan, and Wang Shengrui.2008.
A Hierarchical Method for Determining theNumber of Clusters.
Journal of Software, 19(1).
[inChinese]Chung Heong Gooi and James Allan.
2004.
Cross-document co-reference on a large scale corpus.
In S.Dumais, D. Marcu, and S. Roukos, editors, HLT-NAACL 2004: Main Proceedings, 9?16, Boston,Massachusetts, USA, May 2 - May 7 2004.
Asso-ciation for Computational Linguistics.Gao Huixian.
Applied Multivariate Statistical Analy-sis.
Peking Univ.
Press.
2004.G.
Salton and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
InformationProcessing and Management,Kulkarni Anagha and Ted Pedersen.
2006.
How ManyDifferent ?John Smiths?, and Who are They?
InProceedings of the Student Abstract and PosterSession of the 21st National Conference on Artifi-cial Intelligence, Boston, Massachusetts.Mann G. and D. Yarowsky.
2003.
Unsupervised per-sonal name disambiguation.
In W. Daelemans andM.
Osborne, editors, Proceedings of CoNLL-2003,33?40, Edmonton, Canada.Niu Cheng, Wei Li, and Rohini K. Srihari.
2004.Weakly Supervised Learning for Cross-documentPerson Name Disambiguation Supported by Infor-mation Extraction.
In Proceedings of ACL 2004.Ono.
Shingo, Issei Sato, Minoru Yoshida, and HiroshiNakagawa2.
2008.
Person Name Disambiguationin Web Pages Using Social Network, CompoundWords and Latent Topics.
T. Washio et al (Eds.
):PAKDD 2008, LNAI 5012, 260?271.Song Yang, Jian Huang, Isaac G. Councill, Jia Li, andC.
Lee Giles.
2007.
Efficient Topic-based Unsu-pervised Name Disambiguation.
JCDL?07, June18?23, 2007, Vancouver, British Columbia, Cana-da.Ted Pedersen and Kulkarni Anagha.
2006.
AutomaticCluster Stopping with Criterion Functions and theGap Statistic.
In Proceedings of the DemonstrationSession of the Human Language Technology Con-ference and the Sixth Annual Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistic, New York City, NY.95
