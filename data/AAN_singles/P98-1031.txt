Named Entity Scoring for Speech InputJohn D. BurgerDavid PalmerLynette HirschmanThe MITRE Corporation202 Burlington RoadBedford, MA 01730, USAjohn @ mitre.orgpalmer@mitre.orglynette @ mitre.orgAbstractThis paper describes a new scoring algorithm thatsupports comparison of linguistically annotated datafrom noisy sources.
The new algorithm generalizesthe Message Understanding Conference (MUC)Named Entity scoring algorithm, using a compari-son based on explicit alignment of the underlyingtexts, followed by a scoring phase.
The scoringprocedure maps corresponding tagged regions andcompares these according to tag type and tag extent,allowing us to reproduce the MUC Named Entityscoring for identical underlying texts.
In addition,the new algorithm scores for content  (transcriptioncorrectness) of the tagged region, a useful distinc-tion when dealing with noisy data that may differfrom a reference transcription (e.g., speech recog-nizer output).
To illustrate the algorithm, we haveprepared a small test data set consisting of a carefultranscription of speech data and manual insertion ofSGML named entity annotation.
We report resultsfor this small test corpus on a variety of experi-ments involving automatic speech recognition andnamed entity tagging.1.
In t roduct ion :  The  Prob lemLinguistically annotated training and test corporaare playing an increasingly prominent role innatural language processing research.
The PennTREEBANK and the SUSANNE corpora(Marcus 93, Sampson 95) have provided corporafor part-of-speech taggers and syntacticprocessing.
The Message UnderstandingConferences (MUCs) and the Tipster programhave provided corpora for newswire dataannotated with named entities ~ in multiplelanguages (Merchant 96), as well as for higherlevel relations extracted from text.
The value ofthese corpora depends critically on the ability toevaluate hypothesized annotations against a goldstandard reference or key.To date, scoring algorithms such as the MUCNamed Entity scorer (Chinchor 95) haveassumed that the documents to be compareddiffer only in linguistic annotation, not in theunderlying text.
2 This has precludedapplicability to data derived from noisy sources.For example, if we want to compare named entity(NE) processing for a broadcast news source,created via automatic speech recognition and NEtagging, we need to compare it to data created bycareful human transcription and manual NEtagging..
But the underlying texts--therecogmzer output and the gold standardtranscription--differ, and the MUC algorithmcannot be used.
Example 1 shows the referencetranscription from a broadcast news source, andbelow it, the transcription produced by anautomatic speech recognition system.
Theexcerpt also includes reference and hypothesisNE annotation, in the form of SGML tags, where<P> tags indicate the name of a person, <L> thatof a location, and <O> an organization)We have developed a new scoring algorithm thatsupports comparison of linguistically annotateddata from noisy sources.
The new algorithmgeneralizes the MUC algorithm, using acomparison based on explicit alignment of theunderlying texts.
The scoring procedure thenmaps corresponding tagged regions andcompares these according to tag type and tagextent.
These correspond to the componentscurrently used by the MUC scoring algorithm.In addition, the new algorithm also compares thecontent of the tagged region, measuringcorrectness of the transcription within the region,when working with noisy data (e.g., recognizeroutput).2.
Scoring ProcedureThe scoring algorithm proceeds in five stages:1.
Preprocessing to prepare data for alignment2.
Alignment of lexemes in the reference andhypothesis files3.
Named entity mapping to determinecorresponding phrases in the reference andhypothesis files4.
Comparison of the mapped entities in termsof tag type, tag extent and tag content5.
Final computation of the scoret MUC "named entities" include person, organizationand location names, as well as numeric expressions.-'Indeed, the Tipster scoring and annotation algorithmsrequire, as part of the Tipster architecture, that theannotation preserve the underlying text including whitespace.
The MUC named entity scoring algorithm usescharacter offsets to compare the mark-up of two texts.3The SGML used in Tipster evaluations is actuallymore explicit than that used in this paper, e.g.,<ENAMEX TYPE=PERSON> rather than <P>.201ref: ATTHE <L> NEW YORK </L> DESK I 'M  <P>PHIL IPBOROFF</P> <L>hyp:ATTHE <L> NEWARK </L> BASK ON F ILM FORUMExample 1: Aligned and tagged text2.1 Stage 1: PreprocessingThe algorithm takes three files as input: thehuman-transcribed reference file with key NEphrases, the speech recognizer output, whichincludes coarse-grained timestamps used in thealignment process, and the recogizer outputtagged with NE mark-up.The first phase of the scoring algorithm involvesreformatting these input files to allow directcomparison of the raw text.
This is necessary be-cause the transcript file and the output of thespeech recognizer may contain information inaddition to the lexemes.
For example, for theBroadcast News corpus provided by the Lin-guistic Data Consortium, 4 the transcript filecontains, in addition to mixed-case text rep-resenting the words spoken, extensive SGMLand pseudo-SGML annotation including seg-ment timestamps, speaker identification, back-ground noise and music conditions, andcomments.
In the preprocessing phase, thisref: AT THE NEW YORK DESK I 'M PHIL IP  BOROFFhyp: AT THE NEWARK BASK ON F ILM FORUM MISSESMISS ISS IPP I  </L> REPUBL ICANMISSES THE REPUBL ICAN2.2 Stage 2: Lexeme AlignmentA key component of the scoring process is theactual alignment of individual lexemes in thereference and hypothesis documents.
This taskis similar to the alignment that is used to evaluateword error rates of speech recognizers: we matchlexemes in the hypothesis text with theircorresponding lexemes in the reference text.The standard alignment algorithm used for worderror evaluation is a component of the NISTSCLite scoring package used in the BroadcastNews evaluations (Garofolo 97).
For eachlexeme, it provides four possible classificationsof the alignment: correct, substitution, insertion,and deletion.
This classification has beensuccessful for evaluating word error.
However, itrestricts alignment to a one-to-one mappingbetween hypothesis and reference texts.
It isvery common for multiple lexemes in one text tocorrespond to a single lexeme in the other, inaddition to multiple-to-multiple correspon-MISS ISS IPP I  REPUBL ICANTHE REPUBL ICANref: AT THE NEW YORK DESK I'M PHILIP BOROFF MISSISSIPPI REPUBLICANhyp: At" THE N~-~/~U< BASK ON FILM FORUM MISSES THE REPUBLICANExample 2: SCLite alignment (top) vs. phonetic alignment (bottom)annotation and all punctuation is removed, andall remaining text is converted to upper-case.Each word in the reference text is then assignedan estimated timestamp based on the explicittimestamp of the larger parent segmentJGiven the sequence of all the timestamped wordsin each file, a coarse segmentation a d alignmentis performed to assist the lexeme alignment inStage 2.
This is done by identifying sequencesof three or more identical words in the referenceand hypothesis transcriptions, transforming thelong sequence into a set of shorter sequences,each with possible mismatches.
Lexemealignment is then performed on these shortsequences .64http://www.ldc.upenn.edu/5It should be possible to provide more accurate wordtimestamps by using a large-vocabulary recognizer toprovide a forced alignment on the clean transcription.6The sequence l ngth is dependent onthe word-eror rateof the recognizer ouput, but in general the averagesequence is 20-30 words long after this coarsesegmentation.dences.
For example, compare New York  andNewark  in Example 1.
Capturing thesealignment possibilities is especially important inevaluating NE performance, since the alignmentfacilitates phrase mapping and comparison oftagged regions.In the current implementation of our scoringalgorithm, the alignment is done using a pho-netic alignment algorithm (Fisher 93).
In directcomparison with the standard alignmentalgorithm in the SCLite package, we have foundthat the phonetic algorithm results in moreintuitive results.
This can be seen clearly inExample 2, which repeats the reference andhypothesis texts of the previous example.
Thetop alignment is that produced by the SCLitealgorithm; the bottom by the phonetic algorithm.Since this example contains several instances ofpotential named entities, it also illustrates theimpact of different alignment algorithms (andalignment errors) on phrase mapping andcomparison.
We will compare the effect of thetwo algorithms on the NE score in Section 3.202ref: INVEST ING * : .
* "-~ * ~,~ :, \[ TRADING JNITH CUBAhyp: INVEST ING IN  TRAIN ING i WOULD ,.
KEEP  OFF  " .A~..
LOT  !
OFref: ImrEsTING AND ,TmmIMG~ i WItH !
': ~" / i  * FROM OTTAWA rH~S, IShyp: INVESTING:.
IN  TRAIN ING WOULD KEEP  0FF  A i~'LOT .
; OF  WHAT THIS  ~ ISExample 3: Imperfect alignments (SCLite top, phonetic bottom)Even the phonetic algorithm makes alignmentmistakes.
This can be seen in Example 3, where,as before, SCLite's alignment is shown abovethat of the phonetic algorithm.
Once again, wejudge the latter to be a more intuituivealignment--nonetheless, OTTAWA would argu-ably align better with the three word sequenceLOT OF WHAT.
As we shall see, these potentialmisalignments are taken into account in thealgorithm's mapping and comparison phases.2.3 Stage 3: MappingThe result of the previous phase is a series ofalignments between the words in the referencetext and those in a recognizer's hypothesis.
Inboth of these texts there is named-entity (NE)markup.
The next phase is to map the referenceNEs to the hypothesis NEs.
The result of thiswill be corresponding pairs of reference andhypothesis phrases, which will be compared forcorrectness in Stage 4.Currently, the scorer uses a simple, greedymapping algorithm to find corresponding NEpairs.
Potential mapped pmrs are those thatoverlap--that is, if some word(s) in a hypothesisNE have been aligned with some word(s) in areference NE, the reference and hypothesis NEsmay be mapped to one another.
If more thanone potential mapping is possible, this iscurrently resolved in simple left-to-right fashion:the first potential mapping pair is chosen.
Amore sophisticated algorithm, such as that usedin the MUC scorer, will eventually be used thatattempts to optimize the pairings, in order to givethe best possible final score.In the general case, there will be reference NEsthat do not map to any hypothesis NE, and viceversa.
As we shall see below, the unmappedreference NEs are completely missing from thehypothesis, and thus will correspond to recallerrors.
Similarly, unmapped hypothesis NEs arecompletely spurious: theyprecision errors.2.4 Stage 4: ComparisonOnce the mapping phasereference-hypothesis NEs,compared for correctness.will be scored ashas found pairs ofthese pa~rs areAs indicated above,we compare along three independentcomponents: type, extent and content.
The firsttwo components correspond to MUC scoring andpreserve backward compatibility.
Thus ourFROM OTTAWA THIS  ISWHAT THIS  'i ISalgorithm can be used to generate MUC-style NEscores, given two texts that differ only inannotation.Type is the simplest of the three components: Ahypothesis type is correct only if it is the same asthe corresponding reference typer.
Thus, inExample 4, hypothesis 1 has an incorrect type,while hypothesis 2 is correct.Extent comparison makes further use of theinformation from the alignment phase.
Strictextent comparison requires the first word of thehypothesis NE to align with the first word of thereference NE, and similarly for the last word.Thus, in Example 4, hypotheses 1 and 2 arecorrect in extent, while hypotheses 3 and 4 arenot.
Note that in hypotheses 2 and 4 thealignment phase has indicated a split between thesingle reference word GINGRICH and the twohypothesis words GOOD RICH (that is, there is aone- to two-word alignment).
In contrast,hypothesis 3 shows the alignment produced bySCLite, which allows only one-to-one alignment.In this case, just as in Example 4, extent is judgedto be incorrect, since the final words of thereference and hypothesis NEs do not align.This strict extent comparison can be weakenedby adjusting an extent olerance.
This is definedas the degree to which the first and/or last wordof the hypothesis need not align exactly with thecorresponding word of the reference NE.
Forexample, if the extent tolerance is 1, thenhypotheses 3 and 4 would both be correct in theextent component.
The main reason for a non-zero tolerance is to allow for possiblediscrepancies in the lexeme alignment process--thus the tolerance only comes into play if thereare word errors adjacent to the boundary inquestion (either the beginning or end of the NE).Here, because both GOOD and RICH are errors,hypotheses 3, 4 and 6 are given the benefit of thedoubt when the extent tolerance is 1.
ForRef: <P> NEWT "G INGRiCH " </P>Hypl: <0> NEWT GOODRICH </0>Hyp2: <P> NEWT GOOD R ICH </P>Hyp3: <P> NEWT GOOD R ICH </P>Hyp4: <P> NEWT GOOD</P> R ICHHyp5: NEWT <P> GINGRICH " </P>Hyp6: NEW <P> .
G INGRICH </P>Example 4203hypothesis 5, however, extent is judged to beincorrect, no matter what the extent tolerance is,due to the lack of word errors adjacent to theboundaries of the entity.Content is the score component closest to thestandard measures of word error.
Using theword alignment information from the earlierphase, a region of intersection between thereference and the hypothesis text is computed,and there must be no word errors in this region.That is, each hypothesis word must align withexactly one reference word, and the two must beidentical.
The intuition behind using theintersection or overlap region is that otherwiseextent errors would be penalized twice.
Thus inhypothesis6, even though NEWT is in thereference NE, the substitution error (NEW) doesnot count with respect to content comparison,because only the region containing GINGRICHis examined.
Note that the extent tolerancedescribed above is not used to determine theregion of intersection.Table 1 shows the score results for each of thesescore components on all six of the hypotheses inExample 4.
The extent component is shown fortwo different hresholds, 0 and 1 (the latter beingthe default setting in our implementation).2.5 Stage 5: Final Computat ionAfter the mapped pairs are compared along allthree components, a final score is computed.
Weuse precision and recall, in order to distinguishbetween errors of commission (spuriousresponses) and those of omission (missingresponses).
For a particular pair of referenceand hypothesis NE compared in the previousphase, each component that is incorrect is asubstitution error, counting against both recalland precision, because a required referenceelement was missing, and a spurious hypothesiselement was present.Each of the reference NEs that was not mappedto a hypothesis NE in the mapping phase alsocontributes errors: one recall error for each scorecomponent missing from the hypothesis text.Similarly, an unmapped hypothesis NE iscompletely spurious, and thus contributes threeprecision errors: one for each of the scorecomponents.
Finally, we combine the precisionand recall scores into a balanced F -measure .This is a combination of precision and recall,such that F - -  2PR / (P  + R).
F-measure is asingle metric, a convenient way to comparesystems or texts along one dimension 7.7Because F-measure combines recall and precision, iteffectively counts substitution errors twice.
Makhoulet al (1998) have proposed an alternate slot error metric1 02 13 14 15 16 1Extent1Content (0) Extent (1)1l110l1 00 00 00 10 1Table 13.
Exper iments  and  Resu l tsTo validate our scoring algorithm, we developeda small test set consisting of the Broadcast Newsdevelopment test for the 1996 HUB4 evaluation(Garofolo 97).
The reference transcription(179,000 words) was manually annotated withNE information (6150 entities).
We thenperformed a number of scoring experiments ontwo sets of transcription/NE hypothesesgenerated automatically from the same speechdata.
The first data that we scored was the resultof a commonly available speech recognitionsystem, which was then automatically tagged forNE by our system Alembic (Aberdeen 95).
Thesecond set of data that was scored was madeavailabe to us by BBN, and was the result of theBYBLOS speech recognizer and IdentiFinder TMNE extractor (Bikel 97, Kubala 97, 98).
In bothcases, the NE taggers were run on the referencetranscription as well as the correspondingrecognizer's output.These data were scored using the original MUCscorer as well as our own scorer run in twomodes: the three-component mode describedabove, with an extent threshold of 1, and a"MUC mode", intended to be backward-compatible with the MUC scorer, s We show theresults in Table 2.First, we note that when the underlying texts areidentical, (columns A and I) our new scoringalgorithm in MUC mode produces the sameresult as the MUC scorer.
In normal mode, thescores for the reference text are, of course,higher, because there are no content errors.
Notsurprisingly, we note lower NE performance onrecognizer output.
Interestingly, for both theAlembic system (S+A) and the BBN systemthat counts ubstitution errors only once.SOur scorer is configurable in a variety of ways.
Inparticular, the extent and content components can becombined into a single component, which is judged tobe correct only if the individual extent and content arecorrect.
In this mode, and with the extent thresholddescribed above set to zero, the scorer effectivelyreplicates the MUC algorithm.204MetricWord correctnessMUC scorerMITRE scorer(MUC mode)MITRE scorerReference text \] Recognizer outputA I S+A B+I1.00 1.00 0.47 0.800.65 0.85 .
.
.
.
.0.65 0.85 0.40 0.71!0 .75  0.91 0.43 0.76Table 2(B+I), the degradation is less than we mightexpect: given the recognizer word error ratesshown, one might predict that the NEperformance on recognizer output would be nobetter than the NE performance on the referencetext times the word recognition rate.
One mightthus expect scores around 0.31 (i.e., 0.65x0.47)for the Alembic system and 0.68 (i.e.,0.85?0.80) for the BBN system.
However, NEperformance is well above these levels for bothsystems, in both scoring modes.We also wished to determine how sensitive theNE score was to the alignment phase.
Toexplore this, we compared the SCLite andphonetic alignment algorithms, run on the S+Adata, with increasing levels of extent tolerance, asshown in Table 3.
As we expected, the NE scoresconverged as the extent tolerance was relaxed.This suggests that in the case where a phoneticalignment algorithm is unavailable (as iscurrently the case for languages other thanEnglish), robust scoring results might still beachieved by relaxing the extent tolerance.4.
Conc lus ionWe have generalized the MUC text-based namedentity scoring procedure to handle non-identicalunderlying texts.
Our algorithm can also beused to score other kinds of non-embeddedSGML mark-up, e.g., part-of-speech, wordsegmentation or noun- and verb-group.
Despiteits generality, the algorithm is backward-compatible with the original MUC algorithm.The distinction made by the algorithm betweenextent and content allows speech understandingsystems to achieve a partial score on the basis ofidentifying a region as containing a name, evenif the recognizer is unable to correctly identifythe content words.
Encouraging this sort ofpartial correctness i  important because it allowsfor applications that might, for example, indexradio or video broadcasts using named entities,allowing a user to replay a particular region inorder to listen to the corresponding content.This flexibility also makes it possible to exploreinformation sources such as prosodics foridentifying regions of interest even when it mayExtent ~ SC-Lite PhoneticTolerance Alignment Alignment1 0.42 0.432 0.44 0.453 0.45 0.45Table 3be difficult to achieve a completely correcttranscript, e.g., due to novel words.AcknowledgementsOur thanks go to BBN/GTE for providingcomparative data for the experiments duscussedin Section 3, as well as fruitful discussion of theissues involved in speech understanding metrics.ReferencesJ.
Aberdeen, J. Burger, D. Day, L. Hirschman, P.Robinson , M. Vilain (1995).
"MITRE: Description ofthe Alembic System as Used for MUC-6", in Proceed-ings of the Sixth Message Understanding Conference.D.
Bikel, S. Miller, R. Schwartz, R. Weischedel(1997).
"NYMBLE: A High-Performance LearningName-finder", in Proceedings ofthe Fifth Conference onApplied Natural Language Processing.N.
Chinchor (1995).
"MUC-5 Evaluation Metrics", inProceedings of the Fifth Message Understanding Confer-ence.W.M.
Fisher, J.G.
Fiscus (1993).
"Better AlignmentProcedures for Speech Recognition Evaluation".ICASSP Vol.
II.J.
Garofolo, J. Fiscus, W. Fisher (1997) "Design andPreparation of the 1996 Hub-4 Broadcast News Bench-mark Test Corpora", in Proceedings of the 1997DARPA Speech Recognition Workshop.F.
Kubala, H. Jin, S. Matsoukas, L. Nguyen,R.
Schwartz, J. Makhoul (1997) "The 1996 BBNByblos Hub-4 Transcription System", in Proceedings ofthe 1997 DARPA Speech Recognition Workshop.F.
Kubala, R. Schwartz, R. Stone, R. Weischedel(1998) "Named Entity Extraction from Speech", inProceedings of the Broadcast News Transcription co~lUnderstanding Workshop.J.
Makhoul, F. Kubala, R. Schwartz (1998)"Performance Measures for Information Extraction".unpublished manuscript, BBN Technologies, GTE In-ternetworking.M.
Marcus, S. Santorini, M. Marcinkiewicz (1993)"Building a large annotated corpus of English: the PennTreebank", Computational Linguistics, 19(2).R.
Merchant, M. Okurowski (1996) "The MultilingualEntity Task (MET) Overview", in Proceedings ofTIPSTER Text Program (Phase I1).G.R.
Sampson (1995) English for the Computer, Ox-ford University Press.205
