Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272?282,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsIncrementally Tracking Reference in Human/Human DialogueUsing Linguistic and Extra-Linguistic InformationCasey KenningtonCITEC, Bielefeld UniversityUniversit?atsstra?e 25Bielefeld Germanyckennington@cit-ec.uni-bielefeld.deRyu IidaNICT UCRISeika-cho, Soraku-gunKyoto, Japanryu.iida@nict.go.jpTakenobu TokunagaTokyo Instituteof Technology?Ookayama, MeguroTokyo, Japantake@cs.titech.ac.jpDavid SchlangenBielefeld UniversityUniversit?atsstra?e 25Bielefeld Germanydavid.schlangen@uni-bielefeld.deAbstractA large part of human communication in-volves referring to entities in the world andoften these entities are objects that are visu-ally present for the interlocutors.
A systemthat aims to resolve such references needs totackle a complex task: objects and their vi-sual features need to be determined, the re-ferring expressions must be recognised, andextra-linguistic information such as eye gazeor pointing gestures need to be incorporated.Systems that can make use of such informa-tion sources exist, but have so far only beentested under very constrained settings, such asWOz interactions.
In this paper, we apply toa more complex domain a reference resolutionmodel that works incrementally (i.e., word byword), grounds words with visually presentproperties of objects (such as shape and size),and can incorporate extra-linguistic informa-tion.
We find that the model works well com-pared to previous work on the same data, de-spite using fewer features.
We conclude thatthe model shows potential for use in a real-time interactive dialogue system.1 IntroductionReferring to entities in the world via definite de-scriptions makes up a large part of human commu-nication (Poesio and Vieira, 1997).
In task-orientedsituations, these references are often to entities thatare visible in the shared environment.
This kind ofreference has attracted attention in recent computa-tional research, but the kinds of interactions stud-ied are often fairly restricted in controlled lab situ-ations (Tanenhaus and Spivey-Knowlton, 1995) orsimulated human/computer interactions, (Schlangenet al, 2009; Kousidis et al, 2013; Chai et al, 2014).In such task-oriented, co-located settings, interlocu-tors can make use of extra-linguistic cues such asgaze or pointing gestures.
Furthermore, listenersresolve references as they unfold, often identifyingthe referred entity before the end of the reference(Tanenhaus and Spivey-Knowlton, 1995; Spivey etal., 2002), however research in reference resolutionhas mostly focused on full, completed referring ex-pressions.In this paper we make a first move towards ad-dressing somewhat more complex domains.
We ap-ply a model of reference resolution, which has beentested in a simpler setup, on more natural data com-ing from a corpus of human/human interactions.
Themodel is incremental in that it does not wait un-til the end of an utterance to process, rather it up-dates its interpretation at each word increment.
Themodel can also incorporate other modalities, suchas gaze or pointing cues (deixis) incrementally.
Wealso model the saliency of the context, and show thatthe model can easily take such contextual informa-tion into account.
The model improves over previ-ous work on reference resolution applied to the samedata (Iida et al, 2010; Iida et al, 2011).The paper is structured as follows: in the follow-ing section we discuss related work on incrementalresolution of referring expressions.
We explain themodel that we use in Section 3 and the data we applyit to in Section 4.
We then describe the experimentsand the results and provide a discussion.2 Related WorkReference resolution (RR), which is the task of re-solving referring expressions (REs) to what they are272intended to refer to, has been well-studied in variousfields such as psychology (Isaacs and Clark, 1987;Tanenhaus and Spivey-Knowlton, 1995), linguistics(Pineda and Garza, 2000), as well as human/human(Iida et al, 2010) and human/machine interaction(Prasov and Chai, 2010; Siebert and Schlangen,2008; Schlangen et al, 2009).
In recent years,multi-modal corpora have emerged which provideRR with important contextual information: collect-ing dialogue between two humans (Tokunaga et al,2012; Spanger et al, 2012), between a human and a(simulated) dialogue system (Kousidis et al, 2013;Liu et al, 2013), with gaze, information about theshared environment, and in some cases deixis.It has been shown that incorporating gaze im-proves RR in a situated setting because speakersneed to look at and distinguish from distractors theobjects they are describing: this has been shownin a static scene on a computer screen (Prasov andChai, 2008), in human-human interactive puzzletasks (Iida et al, 2010; Iida et al, 2011), in webbrowsing (Hakkani-t?ur et al, 2014), and in a mov-ing car where speakers look at objects in their vicin-ity (Misu et al, 2014).
Incorporating pointing (deic-tic) gestures is also potentially useful in situated RR;as for example Matuszek et al (2014) have shownin work on resolving objects processed by computervision techniques.
Chen and Eugenio (2012) lookedinto reference in multi-modal settings, with focus onco-referential pronouns and pointing gestures.
How-ever, these approaches were applied in settings inwhich communication between the two interlocutorswas constrained, or the developed systems did notprocess incrementally.
Kehler (2000) presented ap-proach that focused more on interaction in a maptask, though the model was not incremental, nor didgrounding occur between language and world, as wedo here.Incremental RR has also been studied in a num-ber of papers, including a framework for fast in-cremental interpretation (Schuler et al, 2009), aBayesian filtering model approach that was sensi-tive to disfluencies (Schlangen et al, 2009), a modelthat used Markov Logic Networks to resolve objectson a screen (Kennington and Schlangen, 2013), amodel of RR and incremental feedback (Traum et al,2012), and an approach that used a semantic repre-sentation to refer to objects (Peldszus et al, 2012;Kennington et al, 2014).
However, the approachesreported there did not incorporate multi-modal in-formation, were too slow to work in real-time, wereevaluated on constrained data, or only focused on aspecific type of RR, ignoring pronouns or deixis.In this paper, we opted to use the model presentedin Kennington et al (2013), the simple incrementalupdate model (SIUM).
It has been tested extensivelyagainst data from a puzzle-playing human/computerinteraction domain (the PENTO data, (Kousidis etal., 2013)); it can incorporate multi-modal informa-tion, works in real-time, and can resolve definite,exophoric, and deictic references in a single frame-work, all of which makes it a potential candidate forworking in an interactive, multi-modal dialogue sys-tem.
The model is similar to the one proposed in Fu-nakoshi et al (2012), which could resolve descrip-tions, anaphora, and deixis in a unified manner, butthat model does not work incrementally.1The main contributions of this paper are the morethorough exposition of the model (in Section 3) andits application and evaluation on much less con-strained, more interactive (and hence realistic) datathan what it has previously been tested on (Section4).
Moreover, the data set used here is also from a ty-pologically very different language (Japanese) thanwhat the model has been previously tested on (Ger-man), and so the robustness of the model againstthese differences is also investigated.We will now describe the model, and that will befollowed by a description of the corpus we used.3 The Simple Incremental Update ModelFollowing Kennington et al (2013) and Kenningtonet al (2014), we model the task at hand as one ofrecovering I , the intention of the speaker makingthe RE, where I ranges over the possible alternatives(the objects in the domain).
This recovery proceedsincrementally (word by word), for RE of arbitrarylength.
That is, ifU denotes the current word, we areinterested in P (I|U), the current hypothesis about1It can be argued that any non-incremental model could bemade into an incremental one by applying that model at eachword (Khouzaimi et al, 2014), but we would argue that moremodeling effort is required in order for the model to work in aninteractive dialogue system, see (Schlangen and Skantze, 2009;Aist et al, 2007; Skantze and Schlangen, 2009; Skantze andHjalmarsson, 1991).273the intended referent, given the observed word.
Weassume the presence of an unobserved, latent vari-able R, which models properties of the candidateobjects such as colour or shape; explained furtherbelow), and so the computation formally is:P (I|U) =?r?RP (I, U,R)P (U)(1)Which, after making some independence assump-tions, can be factored into:P (I|U) =1P (U)P (I)?r?RP (U |R)P (R|I) (2)This is an update model in the usual sense that theposterior P (I|U) at one step becomes the prior P (I)at the next.
P (R|I) provides the link between theintentions (that is, objects) and the properties (e.g.,the colour and shape of each object), and P (U |R)the link between properties and (observed) words.Being incremental, this model is computed at eachword.
As properties play an important role in thismodel, they will now be explained.Properties The variable R models visual or ab-stract properties of entities (such as real-world ob-jects or linguistic entities) and their selection forverbalisation in the referring expression.
The sim-ple assumption made by the model is that only suchproperties can be selected for verbalisation whichthe candidate object actually has.
Hence, the start-ing point for the model is a representation of theworld and the current dialogue context in terms ofthe properties of the objects.
For this paper, thismeans properties belonging to objects in the sharedwork space.We will explain the properties we used in our im-plementation of this model (henceforth SIUM, i.e.,simple incremental update model), the motivationfor using them, and give an example of applying themodel in Section 5.4 The REX DataThe corpora presented in Iida et al (2011) andSpanger et al (2012) are a collection of hu-man/human interaction data where the participantscollaboratively solved Tangram puzzles.
In this set-ting, anaphoric references (i.e., pronoun referencesto entities in an earlier utterance, e.g., ?move it tothe left?)
and exophoric references via definite de-scriptions (i.e., references to real-world objects, e.g.,?that one?
or ?the big triangle?)
are common (notethat both refer in different ways to objects that arephysically present).
The corpus also records anadded modality: the gaze of the puzzle solver (SV)who gives the instructions and that of the operator(OP), who moves the tangram pieces.
The mousepointer controlled by the OP could also be consid-ered a modality, used as a kind of pointing gesturethat both participants can observe.
The goal of thetask was to arrange puzzle pieces on a board intoa specified shape (example in Figure 1), which wasonly known to SV and hidden from OP.
The lan-guage of the dialogues was Japanese.Figure 1: Example Tangram Board; the goal shape is theswan in the top left, the shared work area is the largeboard on the right, the mouse cursor and OP gaze (bluedot) are on object 5, the SV gaze is the red dot (gaze pointswere not seen by the participants).This environment provided frequent use of REsthat aimed to distinguish puzzle pieces (and piecegroups) from each other.
The following are someexample REs from the REX corpus:(1) a. chicchai sankakkeib.
small triangle(2) a. sono ichiban migi ni shippo ni natte irusankakkeib.
that most right tail becoming triangle?that right-most triangle that is the tail?Example (1) is a typical example of an RE as foundin the corpus.
Note that this at the same time consti-tutes the whole utterance, which hence can be classi-274fied as a non-sentential utterance (Schlangen, 2004).Its transliteration consists of 8 Japanese characters,which could be tokenized into two words.
Themore difficult RE shown in Example (2) requires themodel to learn how spatial placements map to cer-tain descriptions.
Moreover, Japanese is a head-finallanguage where comparative landmark pieces are ut-tered before the referent.
Also, because this was ahighly interactive setting, many exophoric pronounswere used, e.g., sore and sono, both meaning that.2Pronoun references like this made up around 32% ofthe utterances.Corpus annotations included (for both partici-pants) transcriptions of utterances, the object beinglooked at any given time, the object being pointedat or manipulated by the mouse, segmentation of theREs and the corresponding referred object or objects.The spatial layout of the board was recorded eachtime an object was manipulated.
Further details ofthe corpus can be found in (Iida et al, 2011).
Inorder to directly compare our work with previouswork, in our evaluations below we consider the sameannotated REs.
Iida et al (2011) applied a supportvector machine-based ranking algorithm (Joachims,2002) to the task of resolving REs in this corpus.They used a total of 36 binary features in the SVMclassifier, which predicted the referred object.
Theyfurther used a separate model for pronoun utterancesand non-pronoun utterances, allowing the classifierto learn patterns without confusing utterance types.More details on the results of these models are givenbelow.The SIU-model has previously been applied to twodatasets from the Pentomino domain (Kennington etal., 2013), where the speaker?s goal was to identifyone out of a set of tetris-like (but consisting of fiveinstead of four blocks) puzzle pieces.
However, inthese datasets, the references were ?one-shot?
andnot embedded in longer dialogues, as is the case inthe REX corpus.
A summary of differences betweenthe two tasks is summarised in Table 1.
ApplyingSIUM to data like that found in the REX corpus is anatural next step to test the abilities of the model asa RR component in a dialogue system.2To be precise, sono is a demonstrative adjective.PENTO REXlanguage German Japaneselanguage type SVO SOVphrase type head-initial head-finalavg utt length 7-8 4-5number of objects 15 7interactivity human-wizard human-humanrecorded gaze SV (speaker) SV, OP% of pronoun utts 0% 32%Table 1: Summary of differences between PENTO andREX tasks.5 ExperimentProcedure The procedure for this experiment is asfollows.
In order to compare our results directlywith those of Iida et al (2011), we provide ourmodel with the same training and evaluation data, ina 10-fold cross-validation of the 1192 REs from 27dialogues (the T2009-11 corpus in Tokunaga et al(2012)).
For development, we used a separate partof the REX corpus (N2009-11) that was structuredsimilarly to the one used in our evaluation.Task The task is RR.
At each increment, SIUM re-turns a distribution over all objects; the probabilityfor each object represents the strength of the beliefthat it is the referred one.
The argmax of the distri-bution is chosen as the hypothesised referred object.P(R|I) P (R|I) models the likelihood of selectinga property of a candidate object for verbalisation;this likelihood is assumed to be uniform for all theproperties that the candidate object has.3We derivethese properties from a representation of the scene;similar to how Iida et al (2011) computed featuresto present to their classifier: namely Ling (linguisticfeatures), TaskSp (task specific features), and Gaze(from SV only).
Some features were binary, otherssuch as shape and size had more values.
Table 2shows all the properties that were used here.
Eachwill now be explained.Ling Each object had a shape, size, and relativeposition to the other pieces.
We determined by hand3Uniformity in the likelihood of the properties isn?t an idealapproach as certain properties could be more likely to be se-lected than others; we leave a more principled approach to us-ing saliency to help determine the likelihood of the propertiesto future work.275Ling TaskSptri/squ/pgram most recent movesmall/med/big mouse pointedleft/mid/rightprev referred Gazetop/cen/bottom most gazed atreferred 5 gazed at in uttreferred 10 longest gazed atreferred 20 recent fixationTable 2: List of properties used for each source of infor-mation.the shape and size properties which remained staticthrough each dialogue.
The position properties werederived from the corpus logs.
For each object, thecentroid of each object was computed.
Then, thevertical and horizontal range for all of the objectswas calculated and then split into three even sec-tions in each dimension (see Figure 2).
An objectwith a centroid in the left-most section of the hor-izontal range received a left property, similarlymiddle and right properties were calculated forcorresponding objects.
For vertical placement, top,center and bottom properties were given to ob-jects in the respective vertical segments.
Figure 2shows an example segmentation.
Each object had avertical and a horizontal property at all times, how-ever, moving an object could result in a change ofone of these spatial properties as the dialogue pro-gressed.
As an example, compare Figure 1, whichis a snapshot of the interaction towards the begin-ning, and Figure 2, which shows a later stage of thegame board; spatial layout changes throughout thedialogue.These properties differ somewhat from the fea-tures for the Ling model presented in Iida et al(2011).
Three features that we did use as propertieshad to do with reference recency: the most recentlyreferred object received the referred X proper-ties, if an object was referred to in the past 5, 10, or20 seconds.TaskSp Iida et al (2011) used 14 task-specific fea-tures, three of which they found to be the most in-formative in their model.
Here, we will only usethe two most informative features as properties (thethird one, whether or not an object was being manip-ulated at the beginning of the RE, did not improveFigure 2: Tangram later in the dialogue; the notion ofright-ness and other spatial concepts changes throughoutthe dialogue (compare to Figure 1), the grids are added toshow which objects receive which horizontal and whichvertical properties.results in a held-out test): the object that was mostrecently moved received the most recent moveproperty and objects that have the mouse cursor overthem received the mouse pointed property (seeFigure 2; object 4 would receive both of these prop-erties, but only for the duration that the mouse wasactually over it).
Each of these properties can be ex-tracted directly from the corpus annotations.Gaze Similar to Iida et al (2011), we considergaze during a window of 1500ms before the onsetof the RE.
The object that was gazed at the longestduring that time received a longest gazed atproperty, the object which was fixated upon mostrecently during that interval before the RE onsetreceived a recent fixation property, and theobject which had the most fixations received themost gazed at property.
During a RE, an ob-ject received the gazed at in utt property if itis gazed at during the RE up until that point.
Theseproperties can be extracted directly from the corpusannotations.
Other gaze features are not really ac-cessible to an incremental model such as this, asgaze features extracted from gaze activity over theRE can only be computed when it is complete.
OurGaze properties are made up of these 4 properties,as opposed to the 14 features in Iida et al (2011).P(U|R) P (U |R) is the model that connects theproperty selected for verbalisation with a way of ver-balising it (a value for U ).
Instead of directly learn-ing this model from data, which would suffer fromdata sparseness, we trained a naive Bayes model276for P (R|U) (as, according to Bayes?
rule, P (U |R)is equal to P (R|U)P (U)1P (R), which, plugged ininto formula (2), cancels out1P (U); further assum-ing the P (R) is uniform, we can directly replaceP (U |R) with P (R|U) here).
On the language side(the variable U in the model), we used n-grams overJapanese characters (we attempted tokenisation ofthe REs into words, but found that using charactersworked just as well in the held-out set).P(I) The prior P (I) is the posterior of the previ-ously computed increment.
In the first increment,it can simply be set to a uniform distribution.
Here,we apply a more informative prior based on saliency.We learn a context model which is queried whenthe first word begins, taking information about thecontext immediately before the beginning of the REinto account, producing a distribution over objects,which becomes P (I) of the first increment in theRE.
The context model itself is a simple applicationof the SIUM, where instead of being a word, U isa token that represents saliency.
The context modelthus learns what properties are important to the pre-RE context and provides an up-to-date distributionover the objects as a RE begins.5.1 ExampleFigure 3: Example scene with two triangles and onesquare, 1 is being looked at by the SV, 3 was recentlymoved and the mouse pointer is still over it.We will now give a simple example of how themodel is applied to the REX data using a subset of theabove properties for the RE small triangle.
Table 3shows a simple normalised co-occurrence count ofhow many times properties were observed as be-longing to a referred object (the basis for P (U |R)).Figure 3 shows the current toy scene, and Table 4shows the properties that each object in the scenehas during the utterance.
Table 5 shows the full ap-plication of the model by summing over the proper-ties for the product P (U |R)P (R|I) and multiplyingby the prior P (I), the posterior of the previous step.Included in this example is how the initial prior iscomputed from the context model.property small triangle square ?context?small .87 .02 .4 .04big .01 .08 .02 .05triangle .04 .88 .01 .09square .04 .01 .9 .09left .06 .07 .06 .09center .04 .03 .04 .07right .04 .06 .05 .03most gazed .07 .09 .07 .6recent move .03 .1 .04 .56mouse pointed .08 .05 .06 .71Table 3: Applications of P (U |R), for some values of Uand R; we assume that this model is learned from data(rows are excerpted from a larger distribution over all thewords in the vocabulary)property 1 2 3small 0.25 0.33 0big 0 0 0.2triangle 0.25 0 0.2square 0 0.33 0left 0.25 0 0center 0 0 0.2right 0 0.33 0most gazed 0.25 0 0recent move 0 0 0.2mouse pointed 0 0 0.2Table 4: P (R|I), for our example domain.
The probabil-ity mass is distributed over the number of properties thata candidate object actually has.Before the RE even begins, the prior saliencyyields that 3 is the most likely object to be referred;it was the most salient in that it was the most re-cently moved object and the mouse pointer was stillover it.
However, initial prior information alone isnot enough to resolve the intended object; for thatthe RE is needed.
After the word small is uttered,1 is the most likely referred object.
After triangle,1 remains the highest in the distribution.
With theRE alone, in this case there would have been enoughinformation to infer that 1 was the referred object,but adding the prior information provided additionalevidence.277I U ?
P (U |R) ?
P (R|I) P (I|U)1 ?context?
.25(.04 + .09 + .09 + .6) .372 .33(.04 + .09 + .03) .0953 .2(.05 + .09 + .07 + .56 + .71) .5351 small .25(.87 + .04 + .06 + .07) .652 .33(.87 + .04 + .04) .23 .2(.01 + .04 + .04 + .03 + .08) .151 triangle .25(.02 + .88 + .07 + .09) .812 .33(.02 + .01 + .06) .0283 .2(.08 + .88 + .03 + .1 + .05) .162Table 5: Application of RE small triangle, where 1 is thereferred objectEvaluation Metrics We report results of our eval-uation in referential accuracy on utterances that wereannotated as referring to a single object (referencesto group objects is left for future work).
Going be-yond Iida et al (2011), our model computes a resolu-tion hypothesis incrementally; for the performanceof this aspect of the system we followed previouslyused metrics for evaluation (Schlangen et al, 2009;Kennington et al, 2013):first correct: how deep into the RE does the modelpredict the referent for the first time?first final: how deep into the RE does the model pre-dict the correct referent and keep that decision untilthe end?edit overhead: how often did the model unneces-sarily change its prediction (the only necessary pre-diction happens when it first makes a correct predic-tion)?We compare non-incremental results to three eval-uations performed in Iida et al (2011), namely whenLing is used alone, Ling+TaskSP used together, andLing+TaskSp+Gaze.
Furthermore, they show resultsof models where a separate part handled REs thatused pronouns, as well as a part that handled thenon-pronoun REs, and a combined model that han-dled both types of expressions.6 Results6.1 Reference ResolutionResults of our evaluation are shown in Figure 4.The SIUM model performs better than the combinedapproach of Iida et al (2011), and performs bet-ter than their separated model?when not includinggaze (there is a significant difference between SIUMand the separated models for Ling+TaskSp, though0?%20?%40?%60?%80?%Ling Ling+TaskSp Ling+TaskSp+Gaze74?%71.6?%63.4?%77?%71.5?%61.8?%72.5?%69.9?%62.7?%Iida2011-combined Iiida2011-separated SIUMFigure 4: Comparison of model accuracies; our SIUMapproach generally performs better over the combinedmodel presented in Iida et al, (2011)SIUM only got one more correct than the separatedmodel).
This is a welcome result, as it shows thatour very simple incremental model that uses a ba-sic classifier is comparable to a non-incremental ap-proach that uses a more complicated classifier.
Itfurther shows that the SIUM model is robust to usingTaskSp and Gaze features as properties, as long asthose features are available immediately before theRE begins, or during the RE.The best-performing approach is the Iida2011-separated model with gaze.
This is the case forseveral reasons: first, their models use features thatare not available to our incremental model (e.g.,their model uses 14 gaze features, some of whichwere based on the entire RE, ours only uses 4 prop-erties).
Second, and more importantly, separatedmodels means less feature confusion: in Iida et al(2011) (Section 5.2), the authors give a compari-son of the most informative features for each model;task and gaze features were prominent for the pro-noun model, whereas gaze and language featureswere prominent for the non-pronoun model.
Wealso tested SIUM under separated conditions to bet-ter compare with the approaches presented here.
Theseparated models, however, did not improve.
This,we assume, is because the model grounds languagewith properties (see Discussion below).
An interac-tive dialogue system might not have the luxury ofchoosing between two models at runtime.
We as-sume that a model that can sufficiently handle both2781-5 6-8 9-14first correct (% into RE) 35.47 22.34 14.8first final (% into RE) 69.0 49.85 48.0edit overhead (all lengths) 0.88%never correct (all lengths) 5.5%Table 6: Incremental results for SIUM, numbers represent% into RE.types of utterances is to be preferred to one thatdoesn?t.6.2 Incremental BehaviourTable 6 shows how our model fares using the incre-mental metrics described earlier.
(As this has notbeen done in Iida et al (2011), direct comparisonis not possible.)
For the evaluation, REs are binnedinto short, normal, and long (1-5, 6-8, 9-14 charac-ters, respectively, based on what the average num-bers of words in REs in this corpus is), to make rela-tive statements (?% into the utterance?)
comparable.Ideally, a system would make the first correct de-cision as early as possible without changing that de-cision.
The results in the table show a respectableincremental model; on average it picks the right ob-ject early, with some edit overhead (making unnec-essary changes in its prediction), finally fixing on afinal decision before the end of the RE with low editoverhead, meaning it rarely changes its mind onceit has made a decision.
In some cases, SIUM neverguessed the correct object, labeled never correct inthe table.
These incremental results are consistentwith previous work for the SIUM; overall, the modelis stable across the RE.6.3 DiscussionDespite being very simple, there is an important dif-ference that allows SIUM to improve over previouswork.
It learns to connect object properties selectedfor verbalisation to ways of verbalising them, andforms a stochastic expectation about which prop-erties might be selected for verbalisation (namely,those that are present).
This represents a type ofgrounding (Harnad, 1990; Roy, 2005).4In termsof the SIUM formalism, the link between object andwords is mediated by the properties the object has4Not to be confused with building common ground (Clark,1996) which is also referred to as grounding.Figure 5: Words that describe objects are linked viaa hand-coded compatibility function; links fromwords to multiple properties can exist, provided it iscoded.and by a stochastic process of associating wordswith properties.
Figure 6 visualises this: each wordhas a stochastic connection between each propertyand objects have a set of properties.
The propertynames are arbitrary as long as they are consistent.
Incontrast, previous work in RR (Iida et al, 2011; Chaiet al, 2014) used a hand-coded concept-labeled se-mantic representation and checked if aspects of theRE match that of a particular object.
If so, a bi-nary compatibility feature was set.
Figure 5 showsthis; words can only link to objects via hand-craftedrules (e.g., the word or FOL predicate and propertystring must match).
By the way SIUM uses proper-ties, it can also perform (exophoric) pronoun resolu-tion, deixis (the mouse pointer) and definite descrip-tions, in a single framework.
This is a nice featureof the model: adding additional modalities does notrequire model reformulation.Figure 6: Words that describe objects are linked via prop-erties stochastically: thicker lines between U and R rep-resent higher probabilities.
The lines between R and Idenote a property belonging to an object.
The cardinalityof U does not equal R.279Incorporating saliency information via a contextmodel is also a nice feature of the model.
Inthis paper, we computed the initial P (I) usinga context model instantiated by SIUM.
By con-sidering only this saliency information, the con-text model can predict the referred object in 41%of the cases.
It also learned which propertieswere important for saliency (that is, these arethe properties that the model would most likelyselect): recently fixated, most gaze at,longest gazed at, prev ref, as might be ex-pected.
In less than 2% of the cases, the con-text model referred to the correct object, but waswrongly ?overruled?
when processing the corre-sponding RE.There were shortcomings, however.
In previ-ous work, it was shown that SIUM performed wellwhen REs contained pronouns (see Kennington et al(2013), experiment 2).
However, in the current workwe observed that REs with pronouns were more dif-ficult for the model to resolve than the model pre-sented in Iida et al (2011).
We surmise that SIUMhad a difficult time grounding certain properties, asthe Japanese pronoun sore can be used anaphoricallyor demonstratively in this kind of context (i.e., some-times sore refers to previously-manipulated objects,or objects that are newly identified with a mousepointer over them); the model presented in Iida etal.
(2011) made more use of contextual informationwhen pronouns were used, particularly in the com-bined model which incorporated gaze information,as shown above.7 ConclusionThe SIUM is a model of RR that grounds languagewith the world, works incrementally, can incorpo-rate modalities such as gaze and deixis, and can re-solve multiple kinds of RRs in a single framework.This paper represents the natural next step in evalu-ating SIUM in a setting that was less constrained andmore interactive, with added knowledge that it canwork in more than one language.There is more to be tested for SIUM.
A commonform of RR happens collaboratively over multiple ut-terances (Clark and Wilkes-Gibbs, 1986; Heemanand Hirst, 1995), SIUM has only been tested on iso-lated REs.
Though SIUM required fewer features (re-alised as properties) than previous work, those prop-erties still need to be computed.
We leave for fu-ture work investigation of a version of the model thatcan ground language with raw(er) information fromthe world (e.g., vision information), eliminating theneed to determine properties.Acknowledgements Thanks to the anonymous re-viewers for their excellent comments and sugges-tions.ReferencesGregory Aist, James Allen, Ellen Campana, Car-los Gomez Gallo, Scott Stoness, and Mary Swift.2007.
Incremental understanding in human-computerdialogue and experimental evidence for advantagesover nonincremental methods.
In Pragmatics, vol-ume 1, pages 149?154, Trento, Italy.Joyce Y Chai, Lanbo She, Rui Fang, Spencer Ottarson,Cody Littley, Changsong Liu, and Kenneth Hanson.2014.
Collaborative effort towards common ground insituated human-robot dialogue.
In Proceedings of the2014 ACM/IEEE international conference on Human-robot interaction, pages 33?40, Bielefeld, Germany.Lin Chen and Barbara Di Eugenio.
2012.
Co-referencevia Pointing and Haptics in Multi-Modal Dialogues.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 523?527.
Association for Computational Lin-guistics.Herbert H Clark and Deanna Wilkes-Gibbs.
1986.
Re-ferring as a collaborative process.
Cognition, 22:1?39.Herbert H Clark.
1996.
Using Language.
CambridgeUniversity Press.Kotaro Funakoshi, Mikio Nakano, Takenobu Tokunaga,and Ryu Iida.
2012.
A Unified Probabilistic Approachto Referring Expressions.
In Proceedings of the 13thAnnual Meeting of the Special Interest Group on Dis-course and Dialogue, pages 237?246, Seoul, SouthKorea, July.
Association for Computational Linguis-tics.Dilek Hakkani-t?ur, Malcolm Slaney, Asli Celikyilmaz,and Larry Heck.
2014.
Eye Gaze for Spoken Lan-guage Understanding in Multi-Modal ConversationalInteractions.
In ICMI.Stevan Harnad.
1990.
The Symbol Grounding Problem.Physica D, 42:335?346.Peter a. Heeman and Graeme Hirst.
1995.
Collaboratingon Referring Expressions.
Computational Linguistics,21(3):32.280Ryu Iida, Shumpei Kobayashi, and Takenobu Tokunaga.2010.
Incorporating Extra-linguistic Information intoReference Resolution in Collaborative Task Dialogue.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistic, pages 1259?-1267, Uppsala, Sweden.Ryu Iida, Masaaki Yasuhara, and Takenobu Tokunaga.2011.
Multi-modal Reference Resolution in Situ-ated Dialogue by Integrating Linguistic and Extra-Linguistic Clues.
In Proceedings of the 5th Interna-tional Joint Conference on Natural Language Process-ing (IJCNLP 2011), pages 84?92.Ellen a. Isaacs and Herbert H. Clark.
1987.
Referencesin conversation between experts and novices.
Journalof Experimental Psychology: General, 116(1):26?37.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining - KDD ?02, pages133?142.Andrew Kehler.
2000.
Cognitive Status and Form ofReference in Multimodal Human-Computer Interac-tion.
In AAAI 00, The 15th Annual Conference of theAmerican Association for Artificial Intelligence, pages685?689.Casey Kennington and David Schlangen.
2013.
Situ-ated incremental natural language understanding usingMarkov Logic Networks.
Computer Speech & Lan-guage, pages ?.Casey Kennington, Spyros Kousidis, and DavidSchlangen.
2013.
Interpreting Situated DialogueUtterances: an Update Model that Uses Speech, Gaze,and Gesture Information.
In SIGdial 2013.Casey Kennington, Spyros Kousidis, and DavidSchlangen.
2014.
Situated Incremental Natu-ral Language Understanding using a Multimodal,Linguistically-driven Update Model.
In CoLing 2014.Hatim Khouzaimi, Romain Laroche, and FabriceLefevre.
2014.
An easy method to make dialogue sys-tems incremental.
In Proceedings of the 15th AnnualMeeting of the Special Interest Group on Discourseand Dialogue (SIGDIAL), pages 98?107, Philadelphia,PA, U.S.A. Association for Computational Linguistics.Spyros Kousidis, Casey Kennington, and DavidSchlangen.
2013.
Investigating speaker gaze andpointing behaviour in human-computer interactionwith the mint.tools collection.
In SIGdial 2013.Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai.2013.
Modeling Collaborative Referring for SituatedReferential Grounding.
In Proceedings of the SIG-DIAL 2013 Conference, pages 78?86, Metz, France,August.
Association for Computational Linguistics.Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, andDieter Fox.
2014.
Learning from Unscripted DeicticGesture and Language for Human-Robot Interactions.In Aaai.
AAAI Press.Teruhisa Misu, Antoine Raux, Ian Lane, and MoffettField.
2014.
Situated Language Understanding at25 Miles per Hour.
In SIGdial 2014, pages 22?31,Philadelphia, PA, U.S.A. Association for Computa-tional Linguistics.Andreas Peldszus, Okko Bu?, Timo Baumann, and DavidSchlangen.
2012.
Joint Satisfaction of Syntactic andPragmatic Constraints Improves Incremental SpokenLanguage Understanding.
In Proceedings of the 13thEACL, pages 514?523, Avignon, France, April.
Asso-ciation for Computational Linguistics.Luis Pineda and Gabriela Garza.
2000.
A Model forMultimodal Reference Resolution.
ComputationalLinguistics, 26:139?193.Massimo Poesio and Renata Vieira.
1997.
A Corpus-Based Investigation of Definite Description Use.Computational Linguistics, 24(2):47, June.Zahar Prasov and Joyce Y. Chai.
2008.
What?s in a gaze?In Proceedings of the 13th international conference onIntelligent user interfaces - IUI ?08, page 20.Zahar Prasov and Joyce Y Chai.
2010.
Fusing Eye Gazewith Speech Recognition Hypotheses to Resolve Ex-ophoric References in Situated Dialogue.
In Emnlp2010, number October, pages 471?481.Deb Roy.
2005.
Grounding words in perception and ac-tion: Computational insights.
Trends in Cognitive Sci-ences, 9(8):389?396, August.David Schlangen and Gabriel Skantze.
2009.
A gen-eral, abstract model of incremental dialogue process-ing.
Proceedings of the 12th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics on EACL 09, 2(1):710?718.David Schlangen, Timo Baumann, and Michaela At-terer.
2009.
Incremental Reference Resolution: TheTask, Metrics for Evaluation, and a Bayesian FilteringModel that is Sensitive to Disfluencies.
In Proceed-ings of the 10th SIGdial, number September, pages30?37, London, UK.
Association for ComputationalLinguistics.David Schlangen.
2004.
A Coherence-Based Approachto the Interpretation of Non-Sentential Utterances inDialogue.
Ph.D. thesis, University of Edinburgh.William Schuler, Stephen Wu, and Lane Schwartz.
2009.A Framework for Fast Incremental Interpretation dur-ing Speech Decoding.
Computational Linguistics,35(3):313?343.Alexander Siebert and David Schlangen.
2008.
A Sim-ple Method for Resolution of Definite Reference in aShared Visual Context.
In Proceedings of the 9th SIG-dial, number June, pages 84?87, Columbus, Ohio.
As-sociation for Computational Linguistics.281Gabriel Skantze and Anna Hjalmarsson.
1991.
TowardsIncremental Speech Production in Dialogue Systems.In Word Journal Of The International Linguistic Asso-ciation, pages 1?8, Tokyo, Japan, September.Gabriel Skantze and David Schlangen.
2009.
Incremen-tal dialogue processing in a micro-domain.
Proceed-ings of the 12th Conference of the European Chapterof the Association for Computational Linguistics onEACL 09, (April):745?753.Philipp Spanger, Masaaki Yasuhara, Ryu Iida, TakenobuTokunaga, Asuka Terai, and Naoko Kuriyama.
2012.REX-J: Japanese referring expression corpus of sit-uated dialogs.
Language Resources and Evaluation,46(3):461?491, December.Michael J Spivey, Michael K Tanenhaus, Kathleen MEberhard, and Julie C Sedivy.
2002.
Eye movementsand spoken language comprehension: Effects of visualcontext on syntactic ambiguity resolution.
CognitivePsychology, 45(4):447?481.Michael K Tanenhaus and Michael J Spivey-Knowlton.1995.
Integration of visual and linguistic informa-tion in spoken language comprehension.
Science,268:1632.Takenobu Tokunaga, Ryu Iida, Asuka Terai, and NaokoKuriyama.
2012.
The REX corpora : A collection ofmultimodal corpora of referring expressions in collab-orative problem solving dialogues.
In Proceedings ofthe Eigth International Conference on Language Re-sources and Evaluation (LREC 2012), pages 422?429.David Traum, David Devault, Jina Lee, Zhiyang Wang,and Stacy Marsella.
2012.
Incremental dialogue un-derstanding and feedback for multiparty, multimodalconversation.
In Lecture Notes in Computer Science(including subseries Lecture Notes in Artificial Intel-ligence and Lecture Notes in Bioinformatics), volume7502 LNAI, pages 275?288.
Springer.282
