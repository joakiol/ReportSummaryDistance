Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 672?679,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsUnsupervised Language Model Adaptation IncorporatingNamed Entity InformationFeifan Liu and Yang LiuDepartment of Computer ScienceThe University of Texas at Dallas, Richardson, TX, USA{ffliu,yangl}@hlt.utdallas.eduAbstractLanguage model (LM) adaptation is im-portant for both speech and languageprocessing.
It is often achieved by com-bining a generic LM with a topic-specificmodel that is more relevant to the targetdocument.
Unlike previous work on un-supervised LM adaptation, this paper in-vestigates how effectively using namedentity (NE) information, instead of con-sidering all the words, helps LM adapta-tion.
We evaluate two latent topic analysisapproaches in this paper, namely, cluster-ing and Latent Dirichlet Allocation(LDA).
In addition, a new dynamicallyadapted weighting scheme for topic mix-ture models is proposed based on LDAtopic analysis.
Our experimental resultsshow that the NE-driven LM adaptationframework outperforms the baseline ge-neric LM.
The best result is obtained us-ing the LDA-based approach byexpanding the named entities with syntac-tically filtered words, together with usinga large number of topics, which yields aperplexity reduction of 14.23% comparedto the baseline generic LM.1 IntroductionLanguage model (LM) adaptation plays an impor-tant role in speech recognition and many naturallanguage processing tasks, such as machine trans-lation and information retrieval.
Statistical N-gramLMs have been widely used; however, they captureonly local contextual information.
In addition, evenwith the increasing amount of LM training data,there is often a mismatch problem because of dif-ferences in domain, topics, or styles.
Adaptation ofLM, therefore, is very important in order to betterdeal with a variety of topics and styles.Many studies have been conducted for LM ad-aptation.
One method is supervised LM adaptation,where topic information is typically available and atopic specific LM is interpolated with the genericLM (Kneser and Steinbiss, 1993; Suzuki and Gao,2005).
In contrast, various unsupervised ap-proaches perform latent topic analysis for LM ad-aptation.
To identify implicit topics from theunlabeled corpus, one simple technique is to groupthe documents into topic clusters by assigning onlyone topic label to a document (Iyer and Ostendorf,1996).
Recently several other methods in the lineof latent semantic analysis have been proposed andused in LM adaptation, such as latent semanticanalysis (LSA) (Bellegarda, 2000), probabilisticlatent semantic analysis (PLSA) (Gildea and Hof-mann, 1999), and LDA (Blei et al, 2003).
Most ofthese existing approaches are based on the ?bag ofwords?
model to represent documents, where allthe words are treated equally and no relation orassociation between words is considered.Unlike prior work in LM adaptation, this paperinvestigates how to effectively leverage namedentity information for latent topic analysis.
Namedentities are very common in domains such asnewswire or broadcast news, and carry valuableinformation, which we hypothesize is topic indica-tive and useful for latent topic analysis.
We com-pare different latent topic generation approaches aswell as model adaptation methods, and propose anLDA based dynamic weighting method for thetopic mixture model.
Furthermore, we expand672named entities by incorporating other contentwords, in order to capture more topic information.Our experimental results show that the proposedmethod of incorporating named information in LMadaptation is effective.
In addition, we find that forthe LDA based adaptation scheme, adding morecontent words and increasing the number of topicscan further improve the performance significantly.The paper is organized as follows.
In Section 2we review some related work.
Section 3 describesin detail our unsupervised LM adaptation approachusing named entities.
Experimental results are pre-sented and discussed in Section 4.
Conclusion andfuture work appear in Section 5.2 Related WorkThere has been a lot of previous related work onLM adaptation.
Suzuki and Gao (2005) compareddifferent supervised LM adaptation approaches,and showed that three discriminative methods sig-nificantly outperform the maximum a posteriori(MAP) method.
For unsupervised LM adaptation,an earlier attempt is a cache-based model (Kuhnand Mori, 1990), developed based on the assump-tion that words appearing earlier in a document arelikely to appear again.
The cache concept has alsobeen used to increase the probability of unseen buttopically related words, for example, the trigger-based LM adaptation using the maximum entropyapproach (Rosenfeld, 1996).Latent topic analysis has recently been investi-gated extensively for language modeling.
Iyer andOstendorf (1996) used hard clustering to obtaintopic clusters for LM adaptation, where a singletopic is assigned to each document.
Bellegarda(2000) employed Latent Semantic Analysis (LSA)to map documents into implicit topic sub-spacesand demonstrated significant reduction in perplex-ity and word error rate (WER).
Its probabilisticextension, PLSA, is powerful for characterizingtopics and documents in a probabilistic space andhas been used in LM adaptation.
For example,Gildea and Hofmann (1999) reported noticeableperplexity reduction via a dynamic combination ofmany unigram topic models with a generic trigrammodel.
Proposed by Blei et al (2003), LatentDirichlet Allocation (LDA) loosens the constraintof the document-specific fixed weights by using aprior distribution and has quickly become one ofthe most popular probabilistic text modeling tech-niques.
LDA can overcome the drawbacks in thePLSA model, and has been shown to outperformPLSA in corpus perplexity and text classificationexperiments (Blei et al, 2003).
Tam and Schultz(2005) successfully applied the LDA model to un-supervised LM adaptation by interpolating thebackground LM with the dynamic unigram LMestimated by the LDA model.
Hsu and Glass (2006)investigated using hidden Markov model withLDA to allow for both topic and style adaptation.Mrva and Woodland (2006) achieved WER reduc-tion on broadcast conversation recognition usingan LDA based adaptation approach that effectivelycombined the LMs trained from corpora with dif-ferent styles: broadcast news and broadcast con-versation data.In this paper, we investigate unsupervised LMadaptation using clustering and LDA based topicanalysis.
Unlike the clustering based interpolationmethod as in (Iyer and Ostendorf, 1996), we ex-plore different distance measure methods for topicanalysis.
Different from the LDA based frameworkas in (Tam and Schultz, 2005), we propose a noveldynamic weighting scheme for the topic adaptedLM.
More importantly, the focus of our work is toinvestigate the role of named entity information inLM adaptation, which to our knowledge has notbeen explored.3 Unsupervised LM Adaptation Integrat-ing Named Entities (NEs)3.1 Overview of the NE-driven LM Adapta-tion FrameworkFigure 1 shows our unsupervised LM adaptationframework using NEs.
For training, we use the textcollection to train the generic word-based N-gramLM.
Then we apply named entity recognition(NER) and topic analysis to train multiple topicspecific N-gram LMs.
During testing, NER is per-formed on each test document, and then a dynami-cally adaptive LM based on the topic analysisresult is combined with the general LM.
Note thatin this figure, we evaluate the performance of LMadaptation using the perplexity measure.
We willevaluate this framework for N-best or lattice res-coring in speech recognition in the future.In our experiments, different topic analysismethods combined with different topic matchingand adaptive schemes result in several LM adapta-673tion paradigms, which are described below in de-tails.Training Text Test TextNER NERLatent TopicAnalysisComputePerplexityGeneric N-gramTrainingTopic ModelTrainingTopic MatchingTopic ModelAdaptationModelInterpolationFigure 1.
Framework of NE-driven LM adaptation.3.2 NE-based Clustering for LM AdaptationClustering is a simple unsupervised topic analysismethod.
We use NEs to construct feature vectorsfor the documents, rather than considering all thewords as in most previous work.
We use theCLUTO1 toolkit to perform clustering.
It finds apredefined number of clusters based on a specificcriterion, for which we chose the following func-tion:?
?= ?=Ki SuvkiuvsimSSS1 ,*21 ),(maxarg)( Lwhere K is the desired number of clusters, Si is theset of documents belonging to the ith cluster, v andu represent two documents, and sim(v, u) is thesimilarity between them.
We use the cosine dis-tance to measure the similarity between two docu-ments:||||||||),(uvuvuvsim rrrr?
?=                         (1)where vrand urare the feature vectors represent-ing the two documents respectively, in our experi-ments composed of NEs.
For clustering, theelements in every feature vector are scaled basedon their term frequency and inverse document fre-1 Available at http://glaros.dtc.umn.edu/gkhome/views/clutoquency, a concept widely used in information re-trieval.After clustering, we train an N-gram LM, calleda topic LM, for each cluster using the documents init.During testing, we identify the ?topic?
for thetest document, and interpolate the topic specificLM with the background LM, that is, if the testdocument belongs to the cluster S*, we can predicta word wk in the document given the word?s his-tory hk using the following equation:)|()1()|()|(* kkSTopickkGeneralkkhwphwphwp??+=??
(2)where ?
is the interpolation weight.We investigate two approaches to find the topicassignment S* for a given test document.
(A) cross-entropy measureFor a test document d=w1,w2,?,wn with a worddistribution pd(w) and a cluster S with a topic LMps(w), the cross entropy CE(d, S) can be computedas:?=?==niisidsd wpwpppHSdCE12 ))((log)(),(),(From the information theoretic perspective, thecluster with the lower cross entropy value is ex-pected to be more topically correlated to the testdocument.
For each test document, we compute thecross entropy values according to different clusters,and select the cluster S* that satisfies:),(minarg1*iKiSdCES?
?=(B) cosine similarityFor each cluster, its centroid can be obtained by:?== inkikii uncv11where uik is the vector for the kth document in the ithcluster, and ni is the number of documents in the ithcluster.
The distance between the test documentand a cluster can then be easily measured by thecosine similarity function as in Equation (1).
Ourgoal here is to find the cluster S* which the testdocument is closest to, that is,||||||||maxarg1*iiKi cvdcvdS ??=?
?rr674where dris the feature vector for the test document.3.3 NE-based LDA for LM AdaptationLDA model (Blei et al, 2003) has been introducedas a new, semantically consistent generative model,which overcomes overfitting and the problem ofgenerating new documents in PLSA.
It is a three-level hierarchical Bayesian model.
Based on theLDA model, a document d is generated as follows.?
Sample a vector of K topic mixture weights?
from a prior Dirichlet distribution withparameter ?
:?=?=Kkkkf11);( ?????
For each word w in d, pick a topic k from themultinomial distribution ?
.?
Pick a word w from the multinomial distri-bution kw,?
given the kth topic.For a document d=w1,w2,?wn, the LDA modelassigns it the following probability:?
??
????????
?== =??????
dfdp niKkkkwi);()(1 1We use the MATLAB topic Toolbox 1.3 (Grif-fiths et al, 2004) in the training set to obtain thedocument-topic matrix, DP, and the word-topicmatrix, WP.
Note that here ?words?
correspond tothe elements in the feature vector used to representthe document (e.g., NEs).
In the DP matrix, an en-try cik represents the counts of words in a documentdi that are from a topic zk (k=1,2,?,K).
In the WPmatrix, an entry fjk represents the frequency of aword wj generated from a topic zk (k=1,2,?,K)over the training set.For training, we assign a topic zi* to a documentdi such that ikKki cz ?
?=1* maxarg .
Based on the docu-ments belonging to the different topics, K topic N-gram LMs are trained.
This ?hard clustering?
strat-egy allows us to train an LM that accounts for allthe words rather than simply those NEs used inLDA analysis, as well as use higher order N-gramLMs, unlike the ?unigram?
based LDA in previouswork.For a test document d = w1,w2,?,wn that is gen-erated by multiple topics under the LDA assump-tion, we formulate a dynamically adapted topicmodel using the mixture of LMs from differenttopics:?=?
?=KikkzikkadaptLDA hwphwp i1)|()|( ?where )|( kkz hwp i  stands for the ith topic LM, and?i is the mixture weight.
Different from the idea ofdynamic topic adaptation in (Tam and Schultz,2005), we propose a new weighting scheme to cal-culate ?i that directly uses the two resulting matri-ces from LDA analysis during training:?==njjjkk dwpwzp1)|()|(??
?==== nqqjjKpjpjkjkwfreqwfreqdwpffwzp11)()()|(,)|(where freq(wj) is the frequency of a word wj in thedocument d. Other notations are consistent with theprevious definitions.Then we interpolate this adapted topic modelwith the generic LM, similar to Equation (2):)|()1()|()|(kkadaptLDAkkGeneralkkhwphwphwp??+=??
(3)4 Experiments4.1 Experimental Setup# of files # of words # of NEsTraining Data 23,985 7,345,644 590,656Test Data 2,661 831,283 65,867Table 1.
Statistics of our experimental data.The data set we used is the LDC Mandarin TDT4corpus, consisting of 337 broadcast news showswith transcriptions.
These files were split intosmall pieces, which we call documents here, ac-cording to the topic segmentation informationmarked in the LDC?s transcription.
In total, thereare 26,646 such documents in our data set.
Werandomly chose 2661 files as the test data (whichis balanced for different news sources).
The restwas used for topic analysis and also generic LMtraining.
Punctuation marks were used to deter-mine sentences in the transcriptions.
We used theNYU NE tagger (Ji and Grishman, 2005) to recog-nize four kinds of NEs: Person, Location, Organi-675zation, and Geo-political.
Table 1 shows the statis-tics of the data set in our experiments.We trained trigram LMs using the SRILM tool-kit (Stolcke, 2002).
A fixed weight (i.e., ?
inEquation (2) and (3)) was used for the entire testset when interpolating the generic LM with theadapted topic LM.
Perplexity was used to measurethe performance of different adapted LMs in ourexperiments.4.2 Latent Topic Analysis ResultsTopic # of  FilesTop 10 Descriptive Items(Translated from Chinese)1 3526U.S., Israel, Washington, Palestine,Bush, Clinton, Gore, Voice of Amer-ica, Mid-East, Republican Party2 3067Taiwan, Taipei, Mainland, TaipeiCity, Chinese People?s BroadcastingStation, Shuibian Chen,  the Execu-tive Yuan, the Legislative Yuan, De-mocratic Progressive Party,Nationalist Party3 4857Singapore, Japan, Hong Kong, Indo-nesia, Asia, Tokyo, Malaysia, Thai-land, World, China4 4495World, German, Landon, Russia,France, England, Xinhua NewsAgency, Europe, U.S., ItalyCluster-ingBased5 7586China, Beijing, Nation, China CentralTelevision Station, Xinhua NewsAgency, Shanghai, World, StateCouncil, Zemin Jiang, Beijing City1 5859China, Japan, Hong Kong, Beijing,Shanghai, World, Zemin Jiang, Ma-cao,  China Central Television Sta-tion, Africa2 3794U.S., Bush, World,  Gore,  SouthKorea, North Korea, Clinton, GeorgeWalker Bush, Asia, Thailand3 4640Singapore, Indonesia, Team, Israel,Europe, Germany, England, France,Palestine, Wahid4 4623Taiwan, Russia, Mainland, India,Taipei, Shuibian Chen, Philippine,Estrada, Communist Party of China,RUS.LDABased5 4729Xinhua News Agency, Nation, Bei-jing, World, Canada, Sydney, Brazil,Beijing City, Education Ministry,CubaTable 2.
Topic analysis results using clusteringand LDA (the number of documents and the top 10words (NEs) in each cluster).For latent topic analysis, we investigated two ap-proaches using named entities, i.e., clustering andLDA.
5 latent topics were used in both approaches.Table 2 illustrates the resulting topics using the top10 words in each topic.
We can see that the wordsin the same cluster share some similarity and thatthe words in different clusters seem to be ?topi-cally?
different.
Note that errors from automaticNE recognition may impact the clustering results.For example, ??/team?
in the table (in topic 3 inLDA results) is an error and is less discriminativefor topic analysis.Table 3 shows the perplexity of the test set us-ing the background LM (baseline) and each of thetopic LMs, from clustering and LDA respectively.We can see that for the entire test set, a topic LMgenerally performs much worse than the genericLM.
This is expected, since the size of a topic clus-ter is much smaller than that of the entire trainingset, and the test set may contain documents fromdifferent topics.
However, we found that when us-ing an optimal topic model (i.e., the topic LM thatyields the lowest perplexity among the 5 topicLMs), 23.45% of the documents in the test set havea lower perplexity value than that obtained fromthe generic LM.
This suggests that a topic modelcould benefit LM adaptation and motivates a dy-namic topic adaptation approach for different testdocuments.PerplexityBaseline 502.02CL-1 1054.36CL-2 1399.16CL-3 919.237CL-4 962.996CL-5 981.072LDA-1 1224.54LDA-2 1375.97LDA-3 1330.44LDA-4 1328.81LDA-5 1287.05Table 3.
Perplexity results using the baseline LMvs.
the single topic LMs.4.3 Clustering vs. LDA Based LM AdaptationIn this section, we compare three LM adaptationparadigms.
As we discussed in Section 3, two ofthem are clustering based topic analysis, but usingdifferent strategies to choose the optimal cluster;and the third one is based on LDA analysis that676uses a dynamic weighting scheme for adaptedtopic mixture model.Figure 2 shows the perplexity results using dif-ferent interpolation parameters with the generalLM.
5 topics were used in both clustering andLDA based approaches (as in Section 4.2).
?CL-CE?
means clustering based topic analysis viacross entropy criterion, ?CL-Cos?
represents clus-tering based topic analysis via cosine distance cri-terion, and ?LDA-MIX?
denotes LDA based topicmixture model, which uses 5 mixture topic LMs.4404504604704804905005105205305400.4 0.5 0.6 0.7 0.8?PerplexityBaseline CL-CE CL-Cos LDA-MIXFigure 2.
Perplexity using different LM adaptationapproaches and different interpolation weights?with the general LM.We observe that all three adaptation approachesoutperform the baseline when using a proper inter-polation weight.
?CL-CE?
yields the best perplex-ity of 469.75 when ?
is 0.5, a reduction of 6.46%against the baseline perplexity of 502.02.
For clus-tering based adaptation, between the two strategiesused to determine the topic for a test document,?CL-CE?
outperforms ?CL-Cos?.
This indicatesthat the cosine distance measure using only namesis less effective than cross entropy for LM adapta-tion.
In addition, cosine similarity does not matchperplexity as well as the CE-based distance meas-ure.
Similarly, for the LDA based approach, usingonly NEs may not be sufficient to find appropriateweights for the topic model.
This also explains thebigger interpolation weight for the general LM inCL-Cos and LDA-MIX than that in ?CL-CE?.For a fair comparison between the clusteringand LDA based LM adaptation approaches, wealso evaluated using the topic mixture model forthe clustering based approach and using only onetopic in the LDA based method.
For clusteringbased adaptation, we constructed topic mixturemodels using the weights obtained from a linearnormalization of the two distance measures pre-sented in Section 3.2.
In order to use only one topicmodel in LDA based adaptation, we chose thetopic cluster that has the largest weight in theadapted topic mixture model (as in Sec 3.3).
Table4 shows the perplexity for the three approaches(CL-Cos, CL-CE, and LDA) using the mixturetopic models versus a single topic LM.
We observesimilar trends as in Figure 2 when changing theinterpolation weight ?
with the generic LM; there-fore, in Table 4 we only present results for one op-timal interpolation weight.Single-Topic Mixture-TopicCL-Cos (?
=0.7) 498.01 497.86CL-CE (?
=0.5) 469.75 483.09LDA (?
=0.7) 488.96 489.14Table 4.
Perplexity results using the adapted topicmodel (single vs. mixture) for clustering and LDAbased approaches.We can see from Table 4 that using the mixturemodel in clustering based adaptation does not im-prove performance.
This may be attributed to howthe interpolation weights are calculated.
For ex-ample, only names are used in cosine distance,and the normalized distance may not be appropri-ate weights.
We also notice negligible differencewhen only using one topic in the LDA basedframework.
This might be because of the smallnumber of topics currently used.
Intuitively, usinga mixture model should yield better performance,since LDA itself is based on the assumption ofgenerating words from multiple topics.
We willinvestigate the impact of the number of topics onLM adaptation in Section 4.5.4.4 Effect of Different Feature Configura-tions on LM AdaptationWe suspect that using only named entities may notprovide enough information about the ?topics?
ofthe documents, therefore we investigate expandingthe feature vectors with other words.
Since gener-ally content words are more indicative of the topicof a document than function words, we used a POStagger (Hillard et al, 2006) to select words for la-tent topic analysis.
We kept words with three POSclasses: noun (NN, NR, NT), verb (VV), and modi-677fier (JJ), selected from the LDC POS set2.
This issimilar to the removal of stop words widely used ininformation retrieval.Figure 3 shows the perplexity results for threedifferent feature configurations, namely, all-words(w), names (n), and names plus syntactically fil-tered items (n+), for the CL-CE and LDA basedapproaches.
The LDA based LM adaptation para-digm supports our hypothesis.
Using named infor-mation instead of all the words seems to efficientlyeliminate redundant information and achieve betterperformance.
In addition, expanding named enti-ties with syntactically filtered items yields furtherimprovement.
For CL-CE, using named informa-tion achieves the best result among the three con-figurations.
This might be because that theclustering method is less powerful in analyzing theprincipal components as well as dealing with re-dundant information than the LDA model.4604654704754804854904955005050.4 0.5 0.6 0.7 0.8?PerplexityCL-CE(w) CL-CE(n) CL-CE(n+)LDA-MIX(w) LDA-MIX(n) LDA-MIX(n+)Figure 3.
Comparison of perplexity using differentfeature configurations.4.5 Impact of Predefined Topic Number onLM AdaptationLDA based topic analysis typically uses a largenumber of topics to capture the fine grained topicspace.
In this section, we evaluate the effect of thenumber of topics on LM adaptation.
For compari-son, we evaluate this for both LDA and CL-CE,similar to Section 4.3.
We use the ?n+?
featureconfiguration as in Section 4.4, that is, names plusPOS filtered items.
When using a single-topicadapted model in the LDA or CL-CE based ap-proach, finer-grained topic analysis (i.e., increasingthe number of topics) leads to worse performancemainly because of the smaller clusters for eachtopic; therefore, we only show results here using2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdfthe mixture topic adapted models.
Figure 4 showsthe perplexity results using different numbers oftopics.
The interpolation weight?
with the generalLM is 0.5 in all the experiments.
For the topic mix-ture LMs, we used a maximum of 9 mixtures (alimitation in the current SRILM toolkit) when thenumber of topics is greater than 9.We observe that as the number of topics in-creases, the perplexity reduces significantly forLDA.
When the number of topics is 50, theadapted LM using LDA achieves a perplexity re-duction of 11.35% compared to using 5 topics, and14.23% against the baseline generic LM.
Therefore,using finer-grained multiple topics in dynamic ad-aptation improves system performance.
When thenumber of topics increases further, e.g., to 100, theperformance degrades slightly.
This might be dueto the limitation of the number of the topic mix-tures used.
A similar trend is observable for theCL-CE approach, but the effect of the topic num-ber is much greater in LDA than CL-CE.435.2477.2430.6445.8485.7477.3471.8 467.2483.1 485.1400420440460480500n=5 n=10 n=20 n=50 n=100# of TopicsPerplexityLDA CL-CEFigure 4.
Perplexity results using different prede-fined numbers of topics for LDA and CL-CE.4.6 DiscussionAs we know, although there is an increasingamount of training data available for LM training,it is still only for limited domains and styles.
Creat-ing new training data for different domains is timeconsuming and labor intensive, therefore it is veryimportant to develop algorithms for LM adaptation.We investigate leveraging named entities in theLM adaptation task.
Though some errors of NERmay be introduced, our experimental results haveshown that exploring named information for topicanalysis is promising for LM adaptation.Furthermore, this framework may have otheradvantages.
For speech recognition, using NEs fortopic analysis can be less vulnerable to recognition678errors.
For instance, we may add a simple moduleto compute the similarity between two NEs basedon the word tokens or phonetics, and thus compen-sate the recognition errors inside NEs.
Whereas,word-based models, such as the traditional cacheLMs, may be more sensitive to recognition errorsthat are likely to have a negative impact on theprediction of the current word.
From this point ofview, our framework can potentially be more ro-bust in the speech processing task.
In addition, thenumber of NEs in a document is much smaller thanthat of the words, as shown in Table 1; hence, us-ing NEs can also reduce the computational com-plexity, in particular in topic analysis for training.5 Conclusion and Future WorkWe compared several unsupervised LM adaptationmethods leveraging named entities, and proposed anew dynamic weighting scheme for topic mixturemodel based on LDA topic analysis.
Experimentalresults have shown that the NE-driven LM adapta-tion approach outperforms using all the words, andyields perplexity reduction compared to the base-line generic LM.
In addition, we find that for theLDA based method, adding other content words,combined with an increased number of topics, canfurther improve the performance, achieving up to14.23% perplexity reduction compared to the base-line LM.The experiments in this paper combine modelsprimarily through simple linear interpolation.
Thusone direction of our future work is to develop algo-rithms to automatically learn appropriate interpola-tion weights.
In addition, our work in this paperhas only showed promising results in perplexityreduction.
We will investigate using this frame-work of LM adaptation for N-best or lattice rescor-ing in speech recognition.AcknowledgementsWe thank Mari Ostendorf, Mei-Yuh Hwang, andWen Wang for useful discussions, and Heng Ji forsharing the Mandarin named entity tagger.
Thiswork is supported by DARPA under Contract No.HR0011-06-C-0023.
Any opinions expressed inthis material are those of the authors and do notnecessarily reflect the views of DARPA.ReferencesJ.
Bellegarda.
2000.
Exploiting Latent Semantic Infor-mation in Statistical Language Modeling.
In IEEETransactions on Speech and Audio Processing.88(80):1279-1296.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent DirichletAllocation.
Journal of Machine Learning Research.3:993-1022.D.
Gildea and T. Hofmann.
1999.
Topic-Based Lan-guage Models using EM.
In Proc.
of Eurospeech.T.
Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum.2004.
Integrating Topics and Syntax.
Adv.
in NeuralInformation Processing Systems.
17:537-544.D.
Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-Tur, M. Harper, M. Ostendorf, and W. Wang.
2006.Impact of Automatic Comma Prediction onPOS/Name Tagging of Speech.
In Proc.
of the FirstWorkshop on Spoken Language Technology (SLT).P.
Hsu and J.
Glass.
2006.
Style & Topic LanguageModel Adaptation using HMM-LDA.
In Proc.
ofEMNLP, pp:373-381.R.
Iyer and M. Ostendorf.
1996.
Modeling Long Dis-tance Dependence in Language: Topic Mixtures vs.Dynamic Cache Models.
In Proc.
of ICSLP.H.
Ji and R. Grishman.
2005.
Improving NameTaggingby Reference Resolution and Relation Detection.
InProc.
of ACL.
pp: 411-418.R.
Kneser and V. Steinbiss.
1993.
On the Dynamic Ad-aptation of Stochastic language models.
In Proc.
ofICASSP, Vol 2, pp: 586-589.R.
Kuhn and R.D.
Mori.
1990.
A Cache-Based NaturalLanguage Model for Speech Recognition.
In IEEETransactions on Pattern Analysis and Machine Intel-ligence, 12: 570-583.D.
Mrva and P.C.
Woodland.
2006.
Unsupervised Lan-guage Model Adaptation for Mandarin BroadcastConversation Transcription.
In Proc.
ofINTERSPEECH, pp:2206-2209.R.
Rosenfeld.
1996.
A Maximum Entropy Approach toAdaptive Statistical Language Modeling.
Computer,Speech and Language, 10:187-228.A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit.
In Proc.
of ICSLP.H.
Suzuki and J. Gao.
2005.
A Comparative Study onLanguage Model Adaptation Techniques Using NewEvaluation Metrics, In Proc.
of HLT/EMNLP.Y.C.
Tam and T. Schultz.
2005.
Dynamic LanguageModel Adaptation Using Variational Bayes Inference.In Proc.
of INTERSPEECH, pp:5-8.679
