Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 250?255,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsCCG Supertagging with a Recurrent Neural NetworkWenduan XuUniversity of CambridgeComputer Laboratorywx217@cam.ac.ukMichael Auli*Facebook AI Researchmichaelauli@fb.comStephen ClarkUniversity of CambridgeComputer Laboratorysc609@cam.ac.ukAbstractRecent work on supertagging using a feed-forward neural network achieved signifi-cant improvements for CCG supertaggingand parsing (Lewis and Steedman, 2014).However, their architecture is limited toconsidering local contexts and does notnaturally model sequences of arbitrarylength.
In this paper, we show how di-rectly capturing sequence information us-ing a recurrent neural network leads to fur-ther accuracy improvements for both su-pertagging (up to 1.9%) and parsing (upto 1% F1), on CCGBank, Wikipedia andbiomedical text.1 IntroductionCombinatory Categorial Grammar (CCG; Steed-man, 2000) is a highly lexicalized formalism;the standard parsing model of Clark and Curran(2007) uses over 400 lexical categories (or su-pertags), compared to about 50 POS tags for typ-ical CFG parsers.
This makes accurate disam-biguation of lexical types much more challenging.However, the assignment of lexical categories canstill be solved reasonably well by treating it asa sequence tagging problem, often referred to assupertagging (Bangalore and Joshi, 1999).
Clarkand Curran (2004) show that high tagging accu-racy can be achieved by leaving some ambiguity tothe parser to resolve, but with enough of a reduc-tion in the number of tags assigned to each wordso that parsing efficiency is greatly increased.In addition to improving parsing efficiency, su-pertagging also has a large impact on parsing ac-curacy (Curran et al, 2006; Kummerfeld et al,2010), since the derivation space of the parseris determined by the supertagger, at both train-*All work was completed before the author joined Face-book.ing and test time.
Clark and Curran (2007) en-hanced supertagging using a so-called adaptivestrategy, such that additional categories are sup-plied to the parser only if a spanning analysis can-not be found.
This strategy is used in the defacto C&C parser (Curran et al, 2007), and thetwo-stage CCG parsing pipeline (supertagging andparsing) continues to be the choice for most re-cent CCG parsers (Zhang and Clark, 2011; Auliand Lopez, 2011; Xu et al, 2014).Despite the effectiveness of supertagging, themost widely used model for this task (Clark andCurran, 2007) has a number of drawbacks.
First,it relies too heavily on POS tags, which leadsto lower accuracy on out-of-domain data (Rimelland Clark, 2008).
Second, due to the sparse, in-dicator feature sets mainly based on raw wordsand POS tags, it shows pronounced performancedegradation in the presence of rare and unseenwords (Rimell and Clark, 2008; Lewis and Steed-man, 2014).
And third, in order to reduce com-putational requirements and feature sparsity, eachtagging decision is made without considering anypotentially useful contextual information beyond alocal context window.Lewis and Steedman (2014) introduced a feed-forward neural network to supertagging, and ad-dressed the first two problems mentioned above.However, their attempt to tackle the third prob-lem by pairing a conditional random field withtheir feed-forward tagger provided little accuracyimprovement and vastly increased computationalcomplexity, incurring a large efficiency penalty.We introduce a recurrent neural network-based(RNN) supertagging model to tackle all the aboveproblems, with an emphasis on the third one.RNNs are powerful models for sequential data,which can potentially capture long-term depen-dencies, based on an unbounded history of pre-vious words (?2); similar to Lewis and Steedman(2014) we only use distributed word representa-250tions (?2.2).
Our model is highly accurate, and byintegrating it with the C&C parser as its adaptivesupertagger, we obtain substantial accuracy im-provements, outperforming the feed-forward setupon both supertagging and parsing.2 Supertagging with a RNN2.1 ModelWe use an Elman recurrent neural network (El-man, 1990) which consists of an input layer xt,a hidden state (layer) htwith a recurrent connec-tion to the previous hidden state ht?1and an out-put layer yt.
The input layer is a vector represent-ing the surrounding context of the current wordat position t, whose supertag is being predicted.1The hidden state ht?1keeps a representation of allcontext history up to the current word.
The cur-rent hidden state htis computed using the currentinput xtand hidden state ht?1from the previousposition.
The output layer represents probabilityscores of all possible supertags, with the size ofthe output layer being equal to the size of the lexi-cal category set.The parameterization of the network consistsof three matrices which are learned during super-vised training.
Matrix U contains weights be-tween the input and hidden layers, V containsweights between the hidden and output layers, andW contains weights between the previous hiddenstate and the current hidden state.
The followingrecurrence2is used to compute the activations ofthe hidden state at word position t:ht= f(xtU+ ht?1W), (1)where f is a non-linear activation function; herewe use the sigmoid function f(z) =11+e?z.
Theoutput activations are calculated as:yt= g(htV), (2)where g is the softmax activation function g(zi) =ezi?jezjthat squeezes raw output activations into aprobability distribution.2.2 Word EmbeddingsOur RNN supertagger only uses continuous vec-tor representations for features and each feature1This is different from some RNN models (e.g., Mikolovet al (2010)) where the input is a one-hot vector.2We assume the input to any layer is a row vector unlessotherwise stated.type has an associated look-up table, which mapsa feature to its distributed representation.
In to-tal, three feature types are used.
The first type isword embeddings: given a sentence of N words,(w1, w2, .
.
.
, wN), the embedding feature of wt(for 1 ?
t ?
N ) is obtained by projecting it ontoa n-dimensional vector space through the look-uptable Lw?
R|w|?n, where |w| is the size of the vo-cabulary.
Algebraically, the projection operationis a simple vector-matrix product where a one-hotvector bj?
R1?|w|(with zeros everywhere exceptat the jth position) is multiplied with Lw:ewt= bjLw?
R1?n, (3)where j is the look-up index for wt.In addition, as in Lewis and Steedman (2014),for every word we also include its 2-character suf-fix and capitalization as features.
Two more look-up tables are used for these features.
Ls?
R|s|?mis the look-up table for suffix embeddings, where|s| is the suffix vocabulary size.
Lc?
R2?mis the look-up table for the capitalization embed-dings.
Lccontains only two embeddings, repre-senting whether or not a given word is capitalized.We extract features from a context window sur-rounding the current word to make a tagging de-cision.
Concretely, with a context window of sizek, bk/2c words either side of the target word areincluded.
For a word wt, its continuous featurerepresentation is:fwt= [ewt; swt; cwt], (4)where ewt?
R1?n, swt?
R1?mand cwt?R1?mare the output vectors from the three differ-ent look-up tables, and [ewt; swt; cwt] denotes theconcatenation of three vectors and hence fwt?R1?(n+2m).
At word position t, the input layer ofthe network xtis:xt= [fwt?bk/2c; .
.
.
fwt; .
.
.
; fwt+bk/2c], (5)where xt?
R1?k(n+2m)and the right-hand side isthe concatenation of all feature representations ina size k context window.We use pre-trained word embeddingsfrom Turian et al (2010) to initialize look-up table Lw, and we apply a set of wordpre-processing techniques at both training andtest time to reduce sparsity.
All words are firstlower-cased, and all numbers are collapsed intoa single digit ?0?.
If a lower-cased hyphenated251word does not have an entry in the pre-trainedword embeddings, we attempt to back-off to thesubstring after the last hyphen.
For compoundwords and numbers delimited by ?\/?, we attemptto back-off to the substring after the delimiter.After pre-processing, the Turian embeddings havea coverage of 94.25% on the training data; forout-of-vocabulary words, three separate randomlyinitialized embeddings are used for lower-casealphanumeric words, upper-case alphanumericwords, and non-alphanumeric symbols.
Forpadding at the start and end of a sentence, the ?un-known?
entry from the pre-trained embeddings isused.
Look-up tables Lsand Lcare also randomlyinitialized, and all look-up tables are modifiedduring supervised training using backpropagation.3 ExperimentsDatasets and Baseline.
We follow the standardsplits of CCGBank (Hockenmaier and Steedman,2007) for all experiments using sections 2-21 fortraining, section 00 for development and section23 as in-domain test set.
The Wikipedia corpusfrom Honnibal et al (2009) and the Bioinfer cor-pus (Pyysalo et al, 2007) are used as two out-of-domain test sets.
We compare supertaggingaccuracy with the MaxEnt C&C supertagger andthe neural network tagger of Lewis and Steed-man (2014) (henceforth NN), and we also evaluateparsing accuracy using these three supertaggers asa front-end to the C&C parser.
We use the same425 supertag set used in both C&C and NN.Hyperparameters and Training.
For Lw, weuse the scaled 50-dimensional Turian embeddings(n = 50 for Lw) as initialization.
We have ex-perimented during development with using 100-dimensional embeddings and found no improve-ments in the resulting model.
Out-of-vocabularyembedding values in Lwand all embedding valuesin Lsand Lcare initialized with a uniform distri-bution in the interval [?2.0, 2.0].
The embeddingdimension size m of Lsand Lcis set to 5.
Otherparameters of the network {U,V,W} are initial-ized with values drawn uniformly from the inter-val [?2.0, 2.0], and are then scaled by their corre-sponding input vector size.
We experimented withcontext window sizes of 3, 5, 7, 9 and 11 duringdevelopment and found a window size of 7 givesthe best performing model on the dev set.
We usea fixed learning rate of 0.0025 and a hidden statesize of 200.Model Accuracy TimeC&C (gold POS) 92.60 -C&C (auto POS) 91.50 0.57NN 91.10 21.00RNN 92.63 -RNN+dropout 93.07 2.02Table 1: 1-best tagging accuracy and speed com-parison on CCGBank Section 00 with a singleCPU core (1,913 sentences), tagging time in secs.To train the model, we optimize cross-entropyloss with stochastic gradient descent using mini-batched backpropagation through time (BPTT;Rumelhart et al, 1988; Mikolov, 2012); the mini-batch size for BPTT, again tuned on the dev set, isset to 9.Embedding Dropout Regularization.
Withoutany regularization, we found cross-entropy erroron the dev set started to increase while the error onthe training set was continuously driven to a verysmall value (Fig.
1a).
With the suspicion of over-fitting, we experimented with l1and l2regulariza-tion and learning rate decay but none of these tech-niques gave any noticeable improvements for ourmodel.
Following Legrand and Collobert (2014),we instead implemented word embedding dropoutas a regularization for all the look-up tables, sincethe capacity of our tagging model mainly comesfrom the look-up tables, as in their system.
Weobserved more stable learning and better general-ization of the trained model with dropout.
Similarto other forms of droput (Srivastava et al, 2014),we randomly drop units and their connections toother units at training time.
Concretely, we applya binary dropout mask to xt, with a dropout rateof 0.25, and at test time no mask is applied, butthe input to the network, xt, at each word positionis scaled by 0.75.
We experimented during devel-opment with different dropout rates, but found theabove choice to be optimal in our setting.3.1 Supertagging ResultsWe use the RNN model which gives the high-est 1-best supertagging accuracy on the dev setas the final model for all experiments.
Withoutany form of regularization, the best model was ob-tained at the 20th epoch, and it took 35 epochs forthe dropout model to peak (Fig.
1b).
We use thedropout model for all experiments and, unlike theC&C supertagger, no tag dictionaries are used.Table 1 shows 1-best supertagging accuracieson the dev set.
The accuracy of the C&C supertag-2525 67 89 1011 1213 1415 160  5  10  15  20  25  30  35  40  45  50cross-entropy errorepochsRNN + dropoutRNN(a) learning curve on dev set0.85 0.860.87 0.880.89 0.90.91 0.920.93 0.940  5  10  15  20  25  30  35  40  45  501-best accuracyepochsRNN + dropoutRNN(b) 1-best accuracy on dev set0.96 0.9650.97 0.9750.98 0.9850.99 0.99510  2  4  6  8  10  12  14  16multi-tagging accuracyambiguity levelRNN + dropoutRNNNNC&C(c) multi-tagging accuracy on dev setFigure 1: Learning curve and 1-best tagging accuracy of the RNN model on CCGBank Section 00.
Plot(c) shows ambiguity vs. multi-tagging accuracy for all supertaggers (auto POS).0.965 0.970.975 0.980.985 0.990.995 10  10  20  30  40  50  60  70  80  90multi-tagging accuracyambiguity levelRNN + dropoutNNC&C(a) CCBBank Section 230.940.950.960.970.980.9910  20  40  60  80  100 120 140 160 180multi-tagging accuracyambiguity levelRNN + dropoutNNC&C(b) wikipedia0.93 0.940.95 0.960.97 0.980.99 10  20  40  60  80  100  120  140multi-tagging accuracyambiguity levelRNN + dropoutNNC&C(c) bio-GENIAFigure 2: Multi-tagging accuracy for all supertagging models on CCGBank Section 23, Wikipedia andBio-GENIA data (auto POS).RNN NN C&C (auto pos) C&C (gold pos)?
WORD SENT amb.
WORD SENT amb.
WORD SENT amb.
WORD SENT amb.0.075 97.33 66.07 1.27 96.83 61.27 1.34 96.34 60.27 1.27 97.34 67.43 1.270.030 98.12 74.39 1.46 97.81 70.83 1.58 97.05 65.50 1.43 97.92 72.87 1.430.010 98.71 81.70 1.84 98.54 79.25 2.06 97.63 70.52 1.72 98.37 77.73 1.720.005 99.01 84.79 2.22 98.84 83.38 2.55 97.86 72.24 1.98 98.52 79.25 1.980.001 99.41 90.54 3.90 99.29 89.07 4.72 98.25 80.24 3.57 99.17 87.19 3.00Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&C ?
levelson CCGBank Section 00.Model Section 23 Wiki BioC&C (gold POS) 93.32 88.80 91.85C&C (auto POS) 92.02 88.80 89.08NN 91.57 89.00 88.16RNN 93.00 90.00 88.27Table 3: 1-best tagging accuracy compari-son on CCGBank Section 23 (2,407 sentences),Wikipedia (200 sentences) and Bio-GENIA (1,000sentences).ger drops about 1% with automatically assignedPOS tags, while our RNN model gives higher ac-curacy (+0.47%) than the C&C supertagger withgold POS tags.
All timing values are obtained ona single Intel i7-4790k core, and all implementa-tions are in C++ except NN which is implementedusing Torch and Java, and therefore we believe theefficiency of NN could be vastly improved with animplementation with a lower-level language.Table 2 compares different supertagging mod-els for multi-tagging accuracy at the default ?levels used by the C&C parser on the dev set.The ?
parameter determines the average numberof supertags assigned to each word (ambiguity)by a supertagger when integrated with the parser;categories whose probabilities are not within ?times the probability of the 1-best category arepruned.
At the first ?
level (0.075), the three su-pertagging models give very close ambiguity lev-els, but our RNN model clearly outperforms NNand C&C (auto POS) in both word (WORD) andsentence (SENT) level accuracies, giving similarword-level accuracy as C&C (gold POS).
For other?
levels (except ?
= 0.001), the RNN model givescomparable ambiguity levels to the C&C modelwhich uses a tagdict, while being much more ac-curate than both the other two models.253LP LR LF SENT CAT cov.C&C (normal) 85.18 82.53 83.83 31.42 92.39 100C&C (hybrid) 86.07 82.77 84.39 32.62 92.57 100C&C (normal + RNN) 86.74 84.58 85.65 34.13 93.60 100C&C (hybrid + RNN) 87.73 84.83 86.25 34.97 93.84 100C&C (normal) 85.18 84.32 84.75 31.73 92.83 99.01 (C&C cov)C&C (hybrid) 86.07 84.49 85.28 32.93 93.02 99.06 (C&C cov)C&C (normal + RNN) 86.81 86.01 86.41 34.37 93.80 99.01 (C&C cov)C&C (hybrid + RNN) 87.77 86.25 87.00 35.20 94.04 99.06 (C&C cov)C&C (normal + RNN) 86.74 86.15 86.45 34.33 93.81 99.42C&C (hybrid + RNN) 87.73 86.41 87.06 35.17 94.05 99.42Table 4: Parsing development results on CCGBank Section 00 (auto POS).CCGBank Section 23 Wikipedia BioinferLP LR LF cov.
LP LR LF cov.
LP LR LF cov.C&C 86.24 84.85 85.54 99.42 81.58 80.08 80.83 99.50 77.78 76.07 76.91 95.40C&C (+ NN) 86.71 85.56 86.13 99.92 82.65 81.36 82.00 100 79.77 78.62 79.19 97.40C&C (+ RNN) 87.68 86.47 87.07 99.96 83.22 81.78 82.49 100 80.10 78.21 79.14 97.80C&C 86.24 84.17 85.19 100 81.58 79.48 80.52 100 77.78 71.44 74.47 100C&C (+ NN) 86.71 85.40 86.05 100 - - - - 79.77 75.35 77.50 100C&C (+ RNN) 87.68 86.41 87.04 100 - - - - 80.10 75.52 77.74 100Table 5: Parsing test results on all three domains (auto POS).
We evaluate on all sentences (100%coverage) as well as on only those sentences that returned spanning analyses (% cov.).
RNN and NNboth have 100% coverage on the Wikipedia data.Fig.
1c compares multi-tagging accuracies of allthe models on the dev set.
For all models, the same?
levels are used (ranging from 0.075 to 10?4,and all C&C default values are included).
TheRNN model consistently outperforms other mod-els across different ambiguity levels.Table 3 shows 1-best accuracies of all modelson the test data sets (Bio-GENIA gold-standardCCG lexical category data from Rimell and Clark(2008) are used, since no gold categories are avail-able in the Bioinfer data).
With gold-standard POStags, the C&C model outperforms both the NN andRNN models on CCGBank and Bio-GENIA; withauto POS, the accuracy of the C&C model dropssignificantly, due to its high reliance on POS tags.Fig.
2 shows multi-tagging accuracies on alltest data (using ?
levels ranging from 0.075 to10?6, and all C&C default values are included).On CCGBank, the RNN model has a clear accu-racy advantage, while on the other two data sets,the accuracies given by the NN model are closerto the RNN model at some ambiguity levels, rep-resenting these data sets are still more challengingthan CCGBank.
However, both the NN and RNNmodels are more robust than the C&C model on thetwo out-of-domain data sets.3.2 Parsing ResultsWe integrate our supertagging model into the C&Cparser, at both training and test time, using all de-fault parser settings; C&C hybrid model is used forCCGBank and Wikipedia; the normal-form modelis used for the Bioinfer data, in line with Lewis andSteedman (2014) and Rimell and Clark (2008).Parsing development results are shown in Table 4;for out-of-domain data sets, no separate develop-ment experiments were done.
Final results areshown in Table 5, and we substantially improveparsing accuracies on CCGBank and Wikipedia.The accuracy of our model on CCGBank repre-sents a F1 score improvement of 1.53%/1.85%over the C&C baseline, which is comparable tothe best known accuracy reported in Auli andLopez (2011).
However, our RNN-supertagging-based model is conceptually much simpler, withno change to the parsing model required at all.4 ConclusionWe presented a RNN-based model for CCG su-pertagging, which brings significant accuracy im-provements for supertagging and parsing, on bothin- and out-of-domain data sets.
Our supertaggeris fast and well-suited for large scale processing.AcknowledgementsThe first author acknowledges the Carnegie Trustfor the Universities of Scotland and the CambridgeTrusts for providing funding.
SC is supportedby ERC Starting Grant DisCoTex (306920) andEPSRC grant EP/I037512/1.
We also thank theanonymous reviewers for their helpful comments.254ReferencesMichael Auli and Adam Lopez.
2011.
Training alog-linear parser with loss functions via softmax-margin.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 333?343.
Association for Computational Lin-guistics.Srinivas Bangalore and Aravind K Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational linguistics, 25(2):237?265.Stephen Clark and James R Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of the 20th international confer-ence on Computational Linguistics, page 282.
Asso-ciation for Computational Linguistics.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.James R Curran, Stephen Clark, and David Vadas.2006.
Multi-tagging for lexicalized-grammar pars-ing.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for Computa-tional Linguistics, pages 697?704.
Association forComputational Linguistics.James R Curran, Stephen Clark, and Johan Bos.
2007.Linguistically motivated large-scale NLP with C&Cand Boxer.
In Proceedings of the 45th Annual Meet-ing of the ACL on Interactive Poster and Demonstra-tion Sessions, pages 33?36.
Association for Compu-tational Linguistics.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Matthew Honnibal, Joel Nothman, and James R Cur-ran.
2009.
Evaluating a statistical CCG parser onwikipedia.
In Proceedings of the 2009 Workshop onThe People?s Web Meets NLP: Collaboratively Con-structed Semantic Resources, pages 38?41.
Associ-ation for Computational Linguistics.Jonathan K Kummerfeld, Jessika Roesner, Tim Daw-born, James Haggerty, James R Curran, and StephenClark.
2010.
Faster parsing by supertagger adap-tation.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 345?355.
Association for ComputationalLinguistics.Jo?el Legrand and Ronan Collobert.
2014.
Joint RNN-based greedy parsing and word composition.
arXivpreprint arXiv:1412.7028.Mike Lewis and Mark Steedman.
2014.
ImprovedCCG parsing with semi-supervised supertagging.Transactions of the Association for ComputationalLinguistics, 2:327?338.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH 2010, 11th Annual Conference of theInternational Speech Communication Association,Makuhari, Chiba, Japan, September 26-30, 2010,pages 1045?1048.Tom?a?s Mikolov.
2012.
Statistical Language ModelsBased on Neural Networks.
Ph.D. thesis, Brno Uni-versity of Technology.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBj?orne, Jorma Boberg, Jouni J?arvinen, and TapioSalakoski.
2007.
Bioinfer: a corpus for informationextraction in the biomedical domain.
BMC bioinfor-matics, 8(1):50.Laura Rimell and Stephen Clark.
2008.
Adapt-ing a lexicalized-grammar parser to contrasting do-mains.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 475?484.
Association for Computational Lin-guistics.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1988.
Learning representations by back-propagating errors.
Cognitive modeling, 5.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, Mass.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th annual meeting of the association for compu-tational linguistics, pages 384?394.
Association forComputational Linguistics.Wenduan Xu, Stephen Clark, and Yue Zhang.
2014.Shift-reduce CCG parsing with a dependencymodel.
In Proceedings of the 2014 ACL Conference.Yue Zhang and Stephen Clark.
2011.
Shift-reduceCCG parsing.
In Proc.
ACL 2011, pages 683?692,Portland, OR.255
