The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 257?262,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsA Naive Bayes classifier for automatic correction of prepositionand determiner errors in ESL textGerard Lynch, Erwan Moreau and Carl VogelCentre for Next Generation LocalisationIntegrated Language Technology GroupSchool of Computer Science and StatisticsTrinity College Dublin, Irelandgplynch,moreaue,vogel@scss.tcd.ieAbstractThis is the report for the CNGL ILT team en-try to the HOO 2012 shared task.
A Naive-Bayes-based classifier was used in the taskwhich involved error detection and correctionin ESL exam scripts.
The features we use in-clude n-grams of words and POS tags togetherwith features based on the external Google N-Grams corpus.
Our system placed 11th outof 14 teams for the detection and recognitiontasks and 11th out of 13 teams for the correc-tion task based on F-score for both prepositionand determiner errors.1 IntroductionThe HOO 2012 shared task seeks to apply compu-tational methods to the correction of certain typesof errors in non-native English texts.
The previousyear?s task, (Dale and Kilgarriff, 2011), focused ona larger scale of errors and a corpus of academic ar-ticles.
This year?s task focuses on six error types in acorpus of non-native speaker text.
The scope of theerrors is as follows:1Error Code Description ExampleRT Replace Preposition When I arrived at LondonMT Missing preposition I gave it JohnUT Unnecessary preposition I told to John thatRD Replace determiner Have the nice dayMD Missing determiner I have carUD Unnecessary determiner There was a lot of the trafficTable 1: Error types for HOO 2012 Shared TaskIn Section 2, we give a brief summary of the datafor the shared task and in Section 3 we explain the1http://correcttext.org/hoo2012/errortypes.html last verified, May 10, 2012individual steps in the system.
Section 4 details thedifferent configurations for each of the runs submit-ted and finally, Section 5 presents the results.2 Training dataThe training data for this shared task has been pro-vided by Cambridge University Press and consists ofscripts from students sitting the Cambridge ESOLFirst Certificate in English (FCE) exams.
The top-ics of the texts are comparable as they have beendrawn from two consecutive exam years.
The data isprovided in XML format and contains 1000 originalexam scripts, together with a standoff file containingedits of the type described in Section 1 above, alsoin XML format.
These edits consist of offset infor-mation, edit type information and before and aftertext for correction.
The results for the shared taskwere presented in this format.The test data consists of 100 exam scripts drawnfrom a new corpus of exam scripts.Some extra metadata is present in the source files,including information about the student?s mothertongue and the age-range of the student, however themother tongue data is not present in the test set.3 ApproachThe approach we have chosen for this task involvesthe use of supervised machine-learning algorithmsin a four-part classification task.3.1 Overview of the systemThe first part of the task involves identification ofedits in the training data, perhaps the most challeng-257ing given the large imbalance of edits vs non-editsin the data.The next step concerns classification of edits intothe six types described above, and the final taskinvolves correction of edits, replacing or addingprepositions and determiners, and possibly in somecases removal of same.There is a fourth step involved which reassessesthe classification and correction based on some sim-ple heuristics, using POS tags of the head word ofeach instance.
If the headword is not a prepositionand the system has marked a replace preposition er-ror at that position, this error will be removed fromthe system.
Likewise when the headword is not adeterminer and a replace determiner error has beenmarked.
If the replacement suggested is the sameas the original text (in some cases this occurs), theedit is also removed.
Another case for removal inthis fashion includes an error type involving a miss-ing determiner error where the head word is neithera noun or an adjective.
In some cases the systemreported and corrected an error suggesting the sametext as was originally there, i.e no change.
Thesecases are also removed from the end result.3.2 ClassificationWe utilise the freely available Weka machine learn-ing toolkit (Hall et al, 2009), and the algorithm usedfor classification in each step is Naive Bayes.3.2.1 Representing the dataWe represent each word in the training data as avector of features.
There are 39 basic features usedin the detection process, and 42 in the classificationand training step.
The first 7 features contain in-formation which is not used for classification but isused to create the edit structures, such as start offset,end offset, native language, age group and sourcefilename and part information.
These features in-clude the current word plus the four preceding andfollowing words, POS and spell-checked versions ofeach, together with bigrams of the two following andtwo preceding words with spell-checked and POSversions for these.
Information on speaker age andnative language is also included although native lan-guage information is not present in the test set.3.2.2 Additional processingAll tokens have been lower-cased and punctuationhas been removed.
POS information for each tokenhas been added.
The open-source POS tagger fromthe OpenNLP tools package (OpenNLP, 2012) hasbeen used to this end.
Spell correction facility hasbeen provided using the basic spellchecker in theLucene information retrieval API(Gospodnetic andHatcher, 2005) and the top match string as providedby this spell correcting software is used in additionto each feature.
The basic maximum entropy modelfor English is used for the POS tagger.We had also planned to include features basedon the Google Books n-gram corpus, (Michel et al,2011) which is freely available on the web, but un-fortunately did not get to include them in the ver-sion submitted due to errors which were found in thescripts for generating the features late in the process.Nevertheless, we describe these features in Section3.3 and present some cross-validation results fromthe training data for the detection step in Section 5.1.3.3 Google N-grams Features3.3.1 MotivationThe Google Books N-Grams2 is a collection ofdatasets which consist of all the sequences of words(n-grams) extracted from millions of books (Michelet al, 2011).
The ?English Million?
dataset containsmore more than 500 millions distinct n-grams3, fromsize 1 to 5. for every n-gram, its frequency, pagefrequency (number of pages containing it) and bookfrequency (number of books containing it) are pro-vided.In this Shared Task, we aim to use the Google N-grams as a reference corpus to help detecting theerrors in the input.
The intuition is the following:if an error occurs, comparing the frequency of theinput n-grams against the frequency of other possi-bilities in the Google N-grams data might provideuseful indication on the location/type of the error.For example, given the input ?I had to go in a li-brary?, The Google N-grams contain only 36,716occurrences of the trigram ?go in a?, but 244,098occurrences of ?go to a?, which indicates that thelatter is more likely.2http://books.google.com/ngrams/datasets3The least frequent n-grams were discarded.258However there are several difficulties in usingsuch a dataset:?
Technical limitations.
Extracting informationfrom the dataset can take a lot of time becauseof the size of the data, thus the range of ap-proaches is restricted by efficiency constraints.?
Quality of the data.
The Google N-grams wereextracted automatically using OCR, whichmeans that the dataset can contain errors or un-expected data (for example, the English datasetcontains a significant number of non-Englishwords).This is why the Google N-grams must be usedcautiously, and only as an indication among others.3.3.2 MethodOur goal is to add features extracted from theGoogle N-grams dataset to the features describedabove, and feed the supervised classification processwith these.
Before computing the features, a list Lof ?target expressions?
is extracted from the train-ing data, which contains all the words or sequencesof words (determiners and prepositions) which oc-cur in a correction.
Then, given an input sentenceA1 .
.
.
Am and a position n in this sentence, twotypes of information are extracted from the Googledata:?
Specific indications of whether an error existsat this position:1.
No change: the frequency of the input se-quence An?1An and An?1AnAn+1 ;2.
Unnecessary word(s): the frequency of thesequence An?1An+1 if A ?
L;3.
Missing word(s): the frequency of the se-quence XAn (resp.
An?1XAn for tri-grams) for any target expression X ?
L;4.
Replacement: if A ?
L, the frequency ofXAn+1 (resp.
An?1XAn+1 for trigrams)for any target expression X ?
L;?
Generic indications taking the context into ac-count: for length N from 1 to 5 in a windowAn?4 .
.
.
An+4, 16 combinations are computedbased only on the fact the n-grams appear in theGoogle data; for example, one of these combi-nations is the normalized sum for the 4 5-gramsin this window of 0 or 1 (the n-gram occurs ordoes not).Additionally, several variants are considered:?
bigrams or trigrams for ?specific?
features;?
binary values for ?specific?
features: 1 if then-gram appears, 0 otherwise;?
keep only the ?generic?
features and the firstthree features.4 Run configurationsTen runs were submitted to the organisers based ondifferent configurations.
Modification of the datawas carried out using both instance reduction andfeature selection techniques.
The system facilitatedthe use of different training data for each of the threemain classification steps.4.1 Least frequent words filterBefore classification, the data is preprocessed by re-placing all the least frequent words with a defaultvalue (actually treated as missing values by the clas-sifier).
This is intended to help the classifier focuson the most relevant indications and to prevent over-specification of the classification model.4.2 Instance reduction filters4.2.1 POSTrigrams filterThe POS trigrams filter works as follows: duringthe training stage, the sequences of POS tags for thewords current-1.current.current+1 are extracted foreach instance, together with its corresponding class.Every POS trigram is then associated with the fol-lowing ratio:Frequency of true instancesFrequency of false instancesThen, when predicting the class, the filter is appliedbefore running the classifier: the sequences of tri-grams are extracted for each instance, and are com-pared against the corresponding ratio observed dur-ing the training stage; the instance is filtered out ifthe ratio is lower than some threshold N%.
In Table259Run Detection Classification Correction0 R1 Normal Normal1 R20 Normal Normal2 Full F12 Normal3 R10 Normal Normal4 R30 Normal Normal5 F12 F12 Normal6 R4new Normal Normal7 R4 + F12 F12 Normal8 R4 Normal Normal9 R2 Normal NormalTable 2: Run configurations2, the label RN refers to the percentage (N) used ascut-off in the experiments.This filter is intended to reduce the impact of thefact that the classes are strongly unbalanced.
It per-mits discarding a high number of false instances,while removing only a small number of true in-stances.
However, as a side effect, it can cause theclassifier to miss some clues which were in the dis-carded instances.4.2.2 CurrentPlusOrMinusOne filterThe current plusorminus one filter works as fol-lows: A list of all current.current+1 word bigramsis made from the error instances in the training data,along with all current-1.current bigrams.
The non-error instances in the training data are then filteredbased on whether an instance contains an occur-rence of any current.current+1 or current-1.currentbigram in the list.4.3 Feature selection filters4.3.1 F12During preliminary experiments, selecting a sub-set of 12 features produced classification accuracygains in the detection and classification steps of theprocess using ten-fold cross validation on the train-ing set.
These twelve features were: current, cur-rent+1.current+2, current-1.current-2, currentSC,currentPOS, current-1, current-2, current+1, cur-rent+2, current+1SC, and current-1SC.
The SCpostfix refers to the spell-corrected token, with POSreferring to the part-of-speech tag.
The F12 config-uration filter removes all other features except these.5 ResultsTable 3 displays the results for both preposition anddeterminer errors which were obtained by the sys-tem on the preliminary test set before teams sub-mitted their revisions.
Table 4 refers to the resultsobtained by the system after the revised errors wereremoved/edited.Task Rank Run Precision Recall F-ScoreDetection 11 9 5.33 25.61 8.82Recognition 11 9 4.18 20.09 6.92Correction 11 9 2.66 12.8 4.41Table 3: Overall results on original data: TCTask Rank Run Precision Recall F-ScoreDetection 11 8 6.56 26.0 10.48Recognition 11 8 4.91 19.45 7.84Correction 11 8 3.09 12.26 4.94Table 4: Overall results on revised data: TC5.1 Some detailed results (detection)The results reported here were obtained on the train-ing data only, using 5-fold cross-validation, and onlyfor the detection task.
We have studied various set-tings for the parameters; figure 1 shows a globaloverview of the performance depending on severalparameters (we show only a few different values inorder to keep the graph readable).The results show that the Google features con-tribute positively to the performance, but onlyslightly: the F1 score is 0.6% better on average.
Thisoverview also hides the fact that some combinationsof values work better together; for instance, contraryto the fact that not filtering the POS trigrams per-Run3 Recall Precision FDetection 9.05 7.42 8.15Correction 4.19 3.44 3.78Recognition 9.05 7.42 8.15Run8 Recall Precision FDetection 22.51 5.44 8.76Correction 11.25 2.72 4.38Recognition 22.51 5.44 8.76Run9 Recall Precision FDetection 25.61 5.33 8.82Correction 12.80 2.66 4.41Recognition 20.09 4.18 6.92Table 5: Top results on original test data260Figure 1: Average F-score depending on several parameters.10111213141516mean off120501005001000POS?trigrams.0POS?trigrams.1POS?trigrams.10POS?trigrams.3 2?3?binary2?binary3 i rnonewindow0window2window4factor(minFreq) filter googleFeatures attributesforms better on average, the best performances areobtained when filtering, as shown in figure 2.Figure 2: F-score (%) w.r.t POS trigrams filter threshold.Parameters: window 2, Google features with bigrams andtrigrams.0 2 4 6 8 1005101520filter thresholdf1 scoremin.
frequency 20min.
frequency 50min.
frequency 100min.
frequency 500min.
frequency 1000?
Minimum frequency4 (preprocessing, see 4.1).4Remark: the values used as ?minimum frequencies?
re-ported in this paper can seem unusually high.
This is due tothe fact that, for technical reasons, the thresholds were appliedglobally to the data after it had been formatted as individual in-stances, each instance containing a context window of 9 words.As a consequence a threshold of N means that a given wordmust occur at least N/9 times in the original input data.As shown in Figure 2, using a high thresholdhelps the classifier build a better model.?
POS trigrams filter (see 4.2.1.)
Even if not fil-tering at all performs better on average, the bestcases are obtained with a low threshold.
Addi-tionally, this parameter can be used to balancebetween recall and precision (when one wantsto favor one or the other).?
Size of the context window.
Results can showimportant differences depending on the sizeof the window, but no best configuration wasfound in general for this parameter.?
Google features (see 3.3.2.)
The Google fea-tures help slightly in general, and are used inthe best cases that we have obtained.
How-ever there is no significantly better approachbetween using the original frequencies, simpli-fying these to binary values, or even not usingthe list of target expressions.6 ConclusionsThe task of automated error correction is a difficultone, with the best-performing systems managing ap-prox.
40 % F-score for the detection, recognitionand correction (Dale et al, 2012).
There are severalareas where our system?s performance might be im-proved.
The spellcheck dictionary which was used261was a general one and this resulted in many spellingcorrections which were out of context.
A more tai-lored dictionary employing contextual awareness in-formation could be beneficial for the preprocessingstep.Multi-word corrections were not supported by thesystem due to how the instances were constructedand these cases were simply ignored, to the detri-ment of the results.In the basic feature set, the majority of featureswere based on word unigrams, however more n-gram features could improve results as these werefound to perform well during classification.There were many different ways to exploit theGoogle N-Grams features and it may be the casethat better combinations of features can be found foreach of the classification steps.Finally, very little time was spent tuning thedatasets for the classification and correction step asopposed to the detection phase, this is another part ofthe system where fine-tuning parameters could im-prove performance.AcknowledgmentsThis material is based upon works supported bythe Science Foundation Ireland under Grant No.
[SFI07/CE/I 1142.
].ReferencesRobert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th European Workshop on Natural Lan-guage Generation, Dublin, Ireland.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A Report on the Preposition andDeterminer Error Correction Shared Task.
In Pro-ceedings of the Seventh Workshop on Innovative Useof NLP for Building Educational Applications, Mon-treal, Canada.O.
Gospodnetic and E. Hatcher.
2005.
Lucene.
Man-ning.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The WEKA data min-ing software: an update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.J.B.
Michel, Y.K.
Shen, A.P.
Aiden, A. Veres, M.K.Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,J.
Orwant, et al 2011.
Quantitative analysis ofculture using millions of digitized books.
Science,331(6014):176.OpenNLP.
2012.
Website: http://opennlp.
apache.
org.262
