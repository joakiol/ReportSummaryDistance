Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 165?168,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPHierarchical Multi-Class Text Categorizationwith Global Margin MaximizationXipeng QiuSchool of Computer ScienceFudan Universityxpqiu@fudan.edu.cnWenjun GaoSchool of Computer ScienceFudan Universitywjgao616@gmail.comXuanjing HuangSchool of Computer ScienceFudan Universityxjhuang@fudan.edu.cnAbstractText categorization is a crucial and well-proven method for organizing the collec-tion of large scale documents.
In this pa-per, we propose a hierarchical multi-classtext categorization method with globalmargin maximization.
We not only max-imize the margins among leaf categories,but also maximize the margins amongtheir ancestors.
Experiments show that theperformance of our algorithm is competi-tive with the recently proposed hierarchi-cal multi-class classification algorithms.1 IntroductionIn the past serval years, hierarchical text catego-rization has become an active research topic indatabase area (Koller and Sahami, 1997; Weigendet al, 1999) and machine learning area (Rousu etal., 2006; Cai and Hofmann, 2007).Hierarchical categorization methods can be di-vided in two types: local and global approaches(Wang et al, 1999; Sun and Lim, 2001).
A lo-cal approach usually proceeds in a top-down fash-ion, which firstly picks the most relevant cate-gories of the top level and then recursively makingthe choice among the low-level categories.
Theglobal approach builds only one classifier to dis-criminate all categories in a hierarchy.
Due that theglobal hierarchical categorization can avoid thedrawbacks about those high-level irrecoverable er-ror, it is more popular in the machine learning do-main.The essential idea behind global approach isthat the close classes(nodes) have some commonunderlying factors.
Especially, the descendantclasses can share the characteristics of the ances-tor classes, which is similar with multi-task learn-ing(Caruana, 1997).
A key problem for global hi-erarchical categorization is how to combine theseunderlying factors.In this paper, we propose an method for hierar-chical multi-class text categorization with globalmargin maximization.
We emphasize that it is im-portant to separate all the nodes of the correct pathin the class hierarchy from their sibling node, thenwe incorporate such information into the formula-tion of hierarchical support vector machine.The rest of the paper is organized as follows.Section 2 describes the basic model of multi-classhierarchical categorization with maximizing mar-gin.
Then we propose our improved versions insection 3.
Section 4 gives the experimental analy-sis.
Section 5 concludes the paper.2 Hierarchical Multi-Class TextCategorizationMulticlass SVM can be generalized to the problemof hierarchical categorization (Cai and Hofmann,2007), which has more than two categories in mostof the case.
Denote Yias the multilabels of xiand?Yithe multilabels set not in Yi.
The separationmargin of w, with respect to xi, can be approxi-mated as:?i(w) = miny?Yi,?y??Yi??(xi,y)?
?(xi,?y),w?
(1)The loss function can be accommodated tomulti-class SVM to scale the penalties for marginviolations proportional to the loss.
This is moti-vated by the fact that margin violations involvingan incorrect class with high loss should be penal-ized more severely.
So the cost-sensitive hierar-chical multiclass formulation takes takes the fol-lowing form:minw,?12||w||2+ Cn?i=1?i(2)s.t.?w,??i(y,?y)??1?
?il(y,?y), (?i,y?Yi,?y??Yi)?i?
0(?i)165where ?
?i(y,?y) = ?
(xi,y) ?
?
(xi,?y),l(y,?y) > 0 and ?
(x,y) is the joint feature of in-put x and output y, which can be represented as:?
(x,y) = ?(y)?
?
(x) (3)where ?
is the tensor product.
?
(y) is the featurerepresentation of y.Thus, we can classify a document x to label y?
:y?= arg maxyF (w,?
(x,y)) (4)where F (?)
is a map function.There are different kinds of loss functionsl(y,?y).One is thezero-one loss, l0/1(y,u) = [y 6= u].Another is specially designed for the hierarchyis tree loss(Dekel et al, 2004).
Tree loss is definedas the length of the path between two multilabelswith positive microlabels,ltr= |path(i : yi= 1, j : uj= 1)| (5)(Rousu et al, 2006) proposed a simplified ver-sion of lH, namely l?H:l?H=?jcj[yj6= uj&ypa(j) = upa(j)], (6)that penalizes a mistake in a child only if the labelof the parent was correct.
There are some differentchoices for setting cj.
One naive idea is to usea uniform weighting (cj= 1).
Another possiblechoice is to divide the loss among the sibling:croot= 1, cj= cParent(j)/(|Sib(j)|+ 1) (7)Another possible choice is to scale the loss by theproportion of the hierarchy that is in the subtreeT (j) rooted by j:cj= |T (j)|/|T (root)| (8)Using these scaling weights, the derived losses arereferred as l?uni,l?siband l?subrespectively.3 Hierarchical Multi-Class TextCategorization with Global MarginMaximizationIn previous literature (Cai and Hofmann, 2004;Tsochantaridis et al, 2005), they focused on sep-arating the correct path from those incorrect path.Inspired by the example in Figure 1, we emphasizeit is also important to separate the ancestor node inthe correct path from their sibling node.The vector w can be decomposed in to the setof wifor each node (category) in the hierarchy.
InFigure 1, the example hierarchy has 7 nodes and 4of them are leaf nodes.
The category is encodeas an integer, 1, .
.
.
, 7.
Suppose that the train-ing pattern x belongs to category 4.
Both w inthe Figure 1a and Figure 1b can successfully clas-sify x into category 4, since F (w,?
(x,y4)) =?1,2,4?wi,x?
is the maximal among all the possi-ble discriminate functions.
So both learned param-eter w is acceptable in current hierarchical supportvector machine.Here we claim the w in Figure 1b is better than thew in Figure 1a.
Since we notice in Figure 1a, thediscriminate function ?w2,x?
is smaller than thediscriminate function ?w3,x?.
The discriminatefunction ?wi,x?
measures the similarity of x tocategory i.
The larger the discriminate function is,the more similar x is to category i.
Since category2 is in the path from the root to the correct cate-gory and category 3 is not, intuitively, x should becloser to category 2 than category 3.
But the dis-criminate function in Figure 1a is contradictive tothis assumption.
But such information is reflectedcorrectly in Figure 1b.
So we conclude w in Fig.1b is superior to w in 1a.Here we propose a novel formulation to incor-porate such information.
Denote Aias the mul-tilabel in Yithat corresponds to the nonleaf cate-gories and Sib(z) denotes the sibling nodes of z,that is the set of nodes that have the same parentwith z, except z itself.
Implementing the aboveidea, we can get the following formulation:minw,?,?12?w?2+ C1?i?i+ C2?i?i(9)s.t.
?w, ??i(y,?y)?
?
1?
?il(y,?y), (?i,y ?
Yi?y ?
?Yi)?w, ??i(z,?z)?
?
1?
?il(z,?z), (?i,z ?
A(i)?z ?
Sib(z))?i?
0(?i)?i?
0(?i)It arrives at the following Lagrangian:L(w, ?1, ..., ?n, ?1, ..., ?n)=12?w?2+ C1Xi?i+ C2Xi?i?XiXy?Yi?y?
?Yi?iy?y(?w, ??i(y,?y)?
?
1 +?il(y,?y))16612 34 5 6 710,1  xw3,2  xw 5,3  xw5,4  xw 1,7  xw1,5  xw 2,6  xw12 34 5 6 710,1  xw7,2  xw 3,3  xw5,4  xw 1,7  xw1,5  xw 2,6  xwa) b)Figure 1: Two different discriminant function in a hierarchy?XiXz?Ai?z?Sib(z)?iz?z(?w, ??i(z,?z)?
?
1 +?il(z,?z))?Xici?i?Xidi?i(10)The dual QP becomesmax??(?)
=?i?y?Yi?y??Yi?iy?y+?i?z?Ai?z?Sib(z)?iz?z?12?i,j?y?Yi?y??Yi?r?Yj?r??Yj?1i,j,y,?y,r,?r(11)?12?i,j?z?Ai?z?Sib(z)?k?Aj?k?Sib(k)?2i,j,z,?z,k,?k,s.t.?iy?y?
0, (12)?jz?z?
0, (13)?y?Yi?y??Yi?iy?yl(y,?y)?
C1, (14)?z?Ai?z?Sib(z)?iz?zl(z,?z)?
C2, (15)where ?1i,j,y,?y,r,?r=?iy?y?jr?r??
?i(y,?y), ??j(r,?r)?
and ?2i,j,z,?z,k,?k=?iz?z?jk?k??
?i(z,?z), ?
?j(k,?k)?.3.1 Optimization AlgorithmThe derived QP can be very large, since the num-ber of ?
and ?
variables is up to O(n?2N), wheren is number of training pattern and N is the num-ber of nodes in the hierarchy.
But two propertiesof the dual problem can be exploited to design amuch more efficient optimization.First, the constraints in the dual problem Eq.
11- Eq.
15 factorize over the instance index for both?-variables and ?-variables.
The constraints inEq.
14 do not couple ?-variables and ?-variablestogether.
Further, dual variables ?iy?yand ?jy?
?y?belonging to different training instances i and j donot join in a same constraints.
This inspired anoptimization procedure which iteratively performssubspace optimization over all dual variables ?iy?ybelonging to the same training instance.
This willin general reduced to a much smaller QP, sinceit freezes all ?jy?ywith j 6= i and ?-variables attheir current values.
This strategy can be appliedin solving ?-variables.Secondly, the number of active constraints at thesolution is expected to be relatively small, sinceonly a small fraction of categories?y ?
?Yi( or?y ?
Sib(y) when y ?
Ai) will typically fail toachieve the required margin.
The expected sparse-ness of the variable for the dual problem can beexploited by employing a variable selection strat-egy.
Equivalently, this corresponds to a cuttingplane algorithm for the primal QP.
Intuitively, wewill identify the most violated margin constraintwith index (i,y,?y) and then add the correspond-ing variable to the optimization problem.
Thismeans that we start with extremely sparse prob-lems and only successively increase the number ofvariables in the active set.
This general approachto deal with large linear or quadratic optimizationproblems is also known as column selection.
Inpractice, it is often not necessary to optimize untilfinal convergence, which adds to the attractivenessof this approach.We have used the LOQO optimization package(Vanderbei, 1999) in our experiments.4 ExperimentWe evaluate our proposed model on the section Din the WIPO-alpha collection1, which consists ofthe 1372 training and 358 testing document.
The1World Intellectual Property Organization (WIPO)167Table 1: Prediction losses (%) obtained on WIPO.The values per column is calculated with the dif-ferent loss function.XXXXXXXTrainTestl0/1l?ltrlunilsiblsubHSVM 48.6 188.8 94.4 97.2 5.4 7.5l0/1HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4HSVM 49.7 187.7 93.9 99.4 5.0 7.1l?HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9HM3 70.9 167.0 - 89.1 5.0 7.0HSVM 49.4 186.0 93.0 98.9 5.0 7.5ltrHSVM-S 48.9 181.4 90.2 97.8 4.9 7.1HSVM 47.2 181.0 90.5 94.4 5.0 7.0l?uniHSVM-S 46.9 179.3 88.7 91.9 4.9 6.9HM3 70.1 172.1 - 88.8 5.2 7.4HSVM 49.4 184.9 92.5 98.9 4.8 7.4l?sibHSVM-S 48.9 170.2 91.6 90.8 4.7 7.4HM3 64.8 172.9 - 92.7 4.8 7.1HSVM 50.6 189.9 95.0 101.1 5.2 7.5l?subHSVM-S 47.2 169.4 85.2 89.4 4.3 6.6HM3 65.0 170.9 - 91.9 4.8 7.2number of nodes in the hierarchy is 188, with max-imum depth 3.We compared the performance of our proposedmethod HSVM-S with two algorithms: HSVM(Caiand Hofmann, 2007) and HM3(Rousu et al, 2006).4.1 Effect of Different Loss FunctionWe compare the methods based on different lossfunctions, l0/1, l?, ltr, lu?ni, ls?iband ls?ub.
The per-formances for three algorithms can be seen in Ta-ble 1.
Those empty cells, denoted by ?-?, are notavailable in (Rousu et al, 2006).As expected, l0/1is inferior to other hierarchi-cal losses by getting poorest performance in all thetesting losses, since it can not take into account thehierarchical information between categories.
Theresults suggests that training with a hierarchicallosses function, like ls?ibor lu?ni, would lead to abetter reduced l0/1on the test set as well as interms of the hierarchical loss.
In Table 1, we canalso point out that when training with the samehierarchical loss, the performance of HSVM-S isbetter than HSVM under the measure of most hier-archical losses, since HSVM-S includes more hier-archical information,the relationship between thesibling categories, than HSVM which only separatethe leave categories.5 ConclusionIn this paper we present a hierarchical multi-classdocument categorization, which focus on maxi-mize the margin of the classes at the differentlevels in the class hierarchy.
In future work, weplan to extend the proposed hierarchical learningmethod to the case where the hierarchy is a DAGinstead of tree and scale up the method further.AcknowledgmentsThis work was (partially) funded by ChineseNSF 60673038, Doctoral Fund of Ministry ofEducation of China 200802460066, and Shang-hai Science and Technology Development Funds08511500302.ReferencesL.
Cai and T Hofmann.
2004.
Hierarchical docu-ment categorization with support vector machines.In Proceedings of the ACM Conference on Informa-tion and Knowledge Management.L.
Cai and T. Hofmann.
2007.
Exploiting known tax-onomies in learning overlapping concepts.
In Pro-ceedings of International Joint Conferences on Arti-ficial Intelligence.R.
Caruana.
1997.
Multi-task learning.
MachineLearning, 28(1):41?75.Ofer Dekel, Joseph Keshet, and Yoram Singer.
2004.Large margin hierarchical classification.
In Pro-ceedings of the 21 st International Conference onMachine Learning.D.
Koller and M Sahami.
1997.
Hierarchically classi-fying documents using very few words.
In Proceed-ings of the International Conference on MachineLearning (ICML).Juho Rousu, Craig Saunders, Sandor Szedmak, andJohn Shawe-Taylor.
2006.
Kernel-based learningof hierarchical multilabel classification models.
InJournal of Machine Learning Research.A.
Sun and E.-P Lim.
2001.
Hierarchical text classi-fication and evaluation.
In Proceedings of the IEEEInternational Conference on Data Mining (ICDM).Ioannis Tsochantaridis, Thorsten Joachims, ThomasHofmann, and Yasemin Altun.
2005.
Large mar-gin methods for structured and interdependent out-put variables.
In Journal of Machine Learning.R.
J. Vanderbei.
1999.
Loqo: An interior point codefor quadratic programming.
In Optimization Meth-ods and Software.K.
Wang, S. Zhou, and S Liew.
1999.
Building hier-archical classifiers using class proximities.
In Pro-ceedings of the International Conference on VeryLarge Data Bases (VLDB).A.
Weigend, E. Wiener, and J Pedersen.
1999.
Exploit-ing hierarchy in text categorization.
In InformationRetrieval.168
