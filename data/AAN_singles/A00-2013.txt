Morphological Tagging: Data vs. DictionariesJ an  Haji~.
*Depar tment  of Computer  ScienceJohns  Hopkins  Univers i tyBal t imore,  MD 21218hajic@cs.jhu.eduAbst ractPart of Speech tagging for English seems to havereached the the human levels of error, but full mor-phological tagging for inflectionally rich languages,such as Romanian, Czech, or Hungarian, is still anopen problem, and the results are far from beingsatisfactory.
This paper presents results obtainedby using a universalized exponential feature-basedmodel for five such languages.
It focuses on the datasparseness issue, which is especially severe for suchlanguages (the more so that there are no extensiveannotated data for those languages).
In conclusion,we argue strongly that the use of an independentmorphological dictionary is the preferred choice tomore annotated data under such circumstances.1 Full Morpho log ica l  Tagg ingEnglish Part of Speech (POS) tagging has beenwidely described in the recent past, starting withthe (Church, 1988) paper, followed by numerousothers using various methods: neural networks (Ju-lian Benello and Anderson, 1989), HMM tagging(Merialdo, 1992), decision trees (Schmid, 1994),transformation-based error-driven learning (Brill,1995), and max imum entropy (Ratnaparkhi, 1996),to select just a few.
However different the methodswere, English dominated in these tests.Unfortunately, English is a morphologically "im-poverished" language: there are no complicatedagreement relations, word order variation is mini-real, and the morphological categories are either ex-tremely simple (-s for plural of nouns, for example),or (almost) nonexistent (cases expressed by inflec-tion, for example) - with not too many exceptionsand irregularities.
Therefore the number of tags se-lected for an English tagset is not that large (40-75in the typical case).
Also, the average ambiguityis low (2.32 tags per token on the manually tagged* The work described herein has been started and largelydone within author's home institution, the Institute of For-mal and Applied Linguistics, Charles University, Prague, CZ,within the project VS96151 of the Ministry of Educationof the Czech Republic and partially also under the grant405/96/K214 ofthe Grant Agency of the Czech Republic.Wall Street Journal part in the Penn Treebank, forexample).Highly inflective and agglutinative languages aredifferent.
Obviously we can limit the number of tagsto the major part-of-speech classes, plus some (likethe Xerox Language Tools (Chanod, 1997) for suchlanguages do), and in fact achieve similar perfor-mance, but that limits the usefulness of the resultsthus obtained for further analysis.
These languages,obviously, do not use the rich inflection just for theamusement (or embarrassment) of their speakers (orNLP researchers): the inflectional categories carryimportant information which ought to be known ata later time (e.g., during parsing).
Thus one wantsnot only to tell apart verbs from nouns, but alsonominative from genitive, masculine animate frominanimate, singular from plural - all of them beingoften ambiguous one way or the other.The average tagset, as found even in a moderatecorpus, contains between 500 and 1,000 distinct ags- whereas the size of the set of possible and plausibletags can reach 3,000 to 5,000.
Obviously, any of thestatistical methods used for English (even if fullysupervised) clash with (or, fall through) the datasparseness problem (see below Table 1 for details).There have been attempts to solve this problemfor some of the highly inflectional European lan-guages ((Daelemans et al, 1996), (Erjavec et al,1999), (Tufts, 1999), and also our own in (Haji~and Hladk~, 1997), (Haji~ and Hladk~, 1998), seealso below), but so far no method nor a tagger hasbeen evaluated against a larger number of those lan-guages in a similar setting, to allow for a side-by-side comparison of the difficulty (or ease) of fullmorphological tagging of those languages.
Thanksto the Multext-East project (V6ronis, 1996a), thereare now five annotated corpora available (which aremanually fully morphologically tagged) to performsuch experiments.2 The  Languages  Used  and  TheTra in ing  DataWe use the Multext-East-annotated version of theOrwell's 1984 novel in Czech, Estonian, Hungarian,94Romanian and Slovene I.
The annotation uses a sin-gle SGML-based formal scheme, and even commonguidelines for tagset design and annotation, nev-ertheless the tagsets differ substantially since thelanguages differ as well: Romanian is a French-likeromance language, Hungarian is agglutinative, andthe other languages are more or less inflectional-type languages 2.
The annotated data contains about100k tokens (including punctuation) for each lan-guage; out of those, the first 20k tokens has beenused for testing, the rest for training.
We have alsoextended the tag identifiers by appending a stringof hyphens ('-') to suit the exponential tagger whichexpects the tags to be of equal length; the mappingwas 1:1 for all tags in all languages, since the "long"tags are in fact the Multext-East standard.From the tagging point of view, the language char-acteristics displayed in Table 1 are the most rele-vant 3 .3 The  Methodo logyThe main tagger used for the comparison experimentis the probabilistic exponential-model-based, error-driven learner we described in detail in (Haji~ andHladk~, 1998).
Modifications had to be made, how-ever, to make it more universal across languages.3.1 Structure of  the ModelThe model described in (Haji~ and Hladk~, 1998)is a general exponential (specifically, a log-linear)model (such as the one used for Maximum Entropy-based models):pAc(ylx ) = exp(~\]in_1AJi(y, x))z(x) (1)where fi(y,x) is a binary-valued feature of theevent value being predicted and its context, A~ isa weight of the feature fi, and Z(x) is the naturalnormalization factor.
This model is then essentiallyreduced to Naive Bayes by the approximation of theIThere are more languages involved in the Multext-Eastproject, but only these five languages have been really care-fully tagged; English is unfortunately tagged using Eric Brill'stagger trained in unsupervised mode, leaving multiple outputat almost every ambiguous token, and Bulgarian is totallyunusable since it has been tagged automatically with onlya baseline tagger.
The English results reported below thuscome from the Penn Treebank data, from which we have usedroughly 100,000 words to match the training data sizes for theremaining languages.
For Czech, Hungarian, and Slovene weuse later versions of the annotated data (than those found onthe Multext-East CD) which we obtained directly from theauthors of the annotations after the Multext-CD had beenpublished, since the new data contain rather substantial im-provements over the originally published data.2For detailed account of the lexical characteristics of theselanguages, see (Vdronis, 1996b).3We have included English here for comparison purposes,since these characteristics are independent of the annotation.Ai parameters, which is done because there are mil-lions of possible features in the pool and thus the fullentropy maximization is prohibitively expensive, ifwe want to select a small number of features insteadof keeping them all.The tags are predicted separately for each mor-phological category (such as POS, NUMBER,CASE, DEGREE OF  COMPARISON,  etc.).
Themodel makes an extensive use of so-called "ambigu-ity classes" (ACs).
An ambiguity class is a set ofvalues (such as genitive and accusative) of a singlecategory (such as CASE) which arises for some wordforms as a result of morphological analysis.
For un-ambiguous word forms (unambiguous from the pointof view of a certain category), the ambiguity class setcontains only a single value; for ambiguous forms,there are 2 or more values in the AC.
For example,let's suppose we use part-of-speech (POS), numberand tense as morphological categories for English;then the word form "borrowed" is 2-way ambiguousin POS ({V, J} for verb and adjective, respectively),unambiguous in number (linguistic arguments apart,number is typically regarded "not applicable" to ad-jectives as well as to almost all forms of verbs inEnglish), and 3-way ambiguous in tense ({P,N,-)for past tense, past participle, and "not applicable"in the adjective form).The predictions of the models are always condi-tioned on the ambiguity class of the category (POS,NUMBER, ...) in question.
In other words, there isa separate model for each category and an ambigu-ity class from that category.
Naturally, there is nomodel for unambiguous ACs classes.
However, eventhough the ambiguity classes bring very valuable in-formation about the word form being tagged and areliable information about the context (since theyare fixed during tagging), using ACs causes also anunwelcome ffect of partitioning the already scarcedata and also effectively ignores tatistics of the un-ambiguous cases.The context of features uses the neighboring words(original word forms) and ambiguity classes on sub-tags, where their relative position in text might beeither fixed (0, -1, +1) or "variable" using a value ofthe POS subtag as the "stop here" criterion, up to4 text positions (words) apart.3.2 General Subtag FeaturesThe original model uses the ambiguity classes notonly for conditioning on context in features, but alsofor the individual models based on category and anAC.More general features have been introduced,which do not depend on the ambiguity class of thesubtag being predicted any more.
This allows tolearn also from unambiguous tokens.
However, thetraining time is increased ramatically by doing sosince all events in the training data have to be taken95LanguageEnglish 4CzechEstonianHungarianRomanianSloveneTable 1: Training data in numbersTraining Size Tagset Size Ambiguous Tokens99903870718138310299210458394457139970476401486103338.65%45.97%40.24%21.58%40.00%38.01%into consideration, as opposed to the case of trainingthe small AC-based model, when only those trainingevents which contain the particular AC are used.3.3 Var iab le  D is tance  Cond i t ionThe "stop" criterion for finding the appropriate rel-ative position was originally based on hard codedchoices suitable for the Czech language only, and ofcourse it depended on the tagset as well.
This depen-dency has been removed by selecting the appropriateconditions automatically when building the pool ofpossible features at the initialization phase 5 (usingthe relative frequency of the POS ambiguity classes,and a threshold to cut off less frequent categories tolimit the size of the feature pool).3.4 Weight  Var ia t ionEven though the full computation of the appropriatefeature weight is still prohibitive (the more so whenthe general features are added), the learner is nowallowed to vary the weights (in several discrete steps)during feature selection, as a (somewhat crude) at-tempt to depart from the Naive Bayes simplificationto the approximation ofthe "correct" Maximum En-tropy estimation.3.5 Hand l ing  Unknown WordsIn order to compare the effects of (not) using an in-dependent dictionary, we have added an unknownword handling module to the code.
6 It extractsthe prefix and suffix frequency information (and thecombination thereof) from the training data.
Then,for each of the combinations, it selects the most fre-quent set of tags seen in the training data and storesit for later use.
When tagging, the data is first pipedthrough a "guesser" which assigns to the unknownwords such a set of possible tags which is stored withthe longest matching prefix/suffix combination.5Also, the use of variable-distance context may be switchedoff entirely.6Originally, the code relied exclusively on the use of suchan independent dictionary.
Since the coverage of the Czechdictionary we have used is extensive, we have been simplyignoring the unknown word problem altogether in the past.4 The  Resu l ts4.1 Repor t ing  Er ror  Rate :  Words  vs .TokensSince "best-only" tagging has been carried out, theerror rate (i.e, 100 - accuracy in %) measure hasbeen used throughout as the only evaluation crite-rion.
However, since some results reported previ-ously were apparently obtained using only the "real"words as the total for accuracy evaluation, whereasin other experiments every token counts (includingpunctuation 7, for example), we have computed bothand report them separately s.4.2 Ava i lab i l i ty  o f  Dic t ionary  In fo rmat ionWe use two methods to obtain the set of possibletags for any given word form (i.e., to analyze it mor-phologically).
Both methods include handling un-known words.
First, we use only information whichmay be obtained automatically from the manuallyannotated corpus (we call this method automatic).This is the way the Maximum Entropy tagger (Rat-naparkhi, 1996) runs if one uses the binary versionfrom the website (see the comparison in Section 5).However, it is not unreasonable to assume that alarger independent dictionary exists which can helpto obtain a list of possible tags for each word formin test data.
This is what we have at our disposalfor the languages in question, since the developmentof such a dictionary was part of the Multext-Eastproject.
We can thus assume a dictionary info isavailable for unknown words in the test data, i.e.,even though there is no statistics available for them(since they did not appear in the training data), allpossible tags for (almost 9) every test token are avail-able.
This method is referred to as independent inthe following text.We have also used a third method of obtain-ing a dictionary information (called mized), namely,by using only the words from the training data,rAnd sometimes a separate token for sentence boundarySTable 1 has been computed using all tokens.
In fact, thelanguages differ significantly in the proportion of punctuation:from about 18% (English) to 30% (Estonian).9Depending on the quality of the independent dictionary.Of course, the tagsets must match, which could be a problemper se.
Here it is simple, since the dictionaries have beendeveloped using the same tagsets as the tagged ata.96but complementing the information about them ob-tained from the training data by including all otherpossible tags for such words.
Therefore the net resultis that during testing, we have only training wordsat our disposal, but with a complete dictionary in-formation (as if coming from a full morphologicaldictionary) 1?.The results on the full training data set are sum-marized in Table 2.The baseline error rate is computed as follows.First of all, we use the independent dictionary forobtaining the possible tags for each word.
Then weextract only the lexical information from the currentposition 11 and counts used for smoothing (which isbased on the ambiguity classes only and it does notuse lexical information).
The system is then trainednormally, which means it uses the lexical informationonly if the AC-based smoothing 12 alone does notwork.
This baseline method is thus very close to theusual baseline method of using simple conditionaldistribution of tags given words.The message of Table 2 seems to be obvious; butbefore we jump to conclusions, let's present anotherset of experiments.In view of the recent interest in dealing with"small languages", and with regard to the questionsof cost-effectiveness of using "human" resources (i.e.annotation vs. rule-writing vs. tools developmentetc.
), we have also performed experiments with re-duced training data size (but with an enriched fea-ture pool - by lowering thresholds, adding more ofthe "general features" as described above, etc.
- asallowed by reasonable time/space constraints).
13These results are summarized in Table 3 (usingonly the dictionary derived from the training data),Table 4 (using words from training data with mor-phological information complemented from a dictio-nary) and Table 5 (using the "independent" dictio-nary).
In all cases, we again count only true words(no punctuation).
Accordingly, the major POS er-ror rate is reported, too (12 POS tags to be dis-tinguished only: Noun, Verb, Adjective .
.
.
.
; see Ta-bles 6, 7, and 8).1?This arrangement removes the "closed vocabulary" phe-nomenon from the test data, since for the Multext-East data,we did not have a truly independent vocabulary available.11Words from the training data which are not singletons(freq > 1) are used.
Surprisingly enough, it would not hurtto use them too.
We believe it is due to the smoothing methodused.
Even though this is valid only for the baseline xperi-ment, we have observed ingeneral that this form of exponen-tial model (with error-driven training, that is) is remarkablyresistant to overtrainlng.12Using ACs linearly interpolated with global unigram sub-tag distribution and finally the uniform distribution.13By reasonable we mean less than a day of CPU for train-ing.Table 9: Exponential w/feature selection vs. Max-imum Entropy tagger (Words-only Error Rate, nodictionary)Language TaggerExp.English 9.18%Czech 18.83%Estonian 13.95%Hungarian 8.16%Romanian 7.76%Slovene 16.26%MaxEnt6.38%17.77%14.92%8.55%7.66%17.44%4.3 Tagger  Compar i sonThe work (Erjavec et al, 1999) consistently com-pares several taggers (HMM, Brill's Transformation-based Tagger, Ratnaparkhi's Maximum Entropytagger, and the Daelemans et al's Memory-basedTagger) on Slovene.
We have chosen the MaximumEntropy tagger (Ratnaparkhi, 1996) for a compari-son with our universal tagger, since it achieved (bya small margin) the best overall result on Sloveneas reported there (86.360% on all tokens) of tag-gers available to us (MBT, the best overall, was notfreely available to us at the time of writing).
Wehave trained and tested the Maximum Entropy Tag-ger on exactly the same data, using the off-the-shelf(java binary only) version.The results are compared in Table 9.Since we want to show how a tagger accuracy isinfluenced by the amount of training data available,we have run a series of experiments comparing theresults of the exponential tagger to the maximumentropy tagger when there is only a limited amountof data available.
The results are summarized inTable 10.
Since the public version of the MaxEnttagger cannot be modified to take advantage of nei-ther the mixed nor the independent dictionary, wehave compared it only to the automatic dictionaryversion of the exponential tagger.
To save space,the results are tabulated only for the training datasizes of 2000, 5000 and 20000 words.
Again, onlythe "true" word error rate is reported.As the tables show, for the languages we tested,the exponential, feature-based tagger we adaptedfrom (Haji~ and Hladk~, 1998) achieves imilar re-sults as the Maximum Entropy tagger 14 15.
(usingexactly the same (full) training data; the "score"is 3:3, with the MaxEnt tagger being substantiallybetter on English; probably the development lan-14Otherwise the acknowledged leader in English tagging15The only substantial difference we noticed was in taggingspeed.
The runtime speed of the MaxEnt tagger is lower, onlyabout 10 words per second vs. almost 500 words per second;it should be noted however that we are comparing MaxEnt'sjava bytecode and C.97Table 2: Results (Error rate, ER) on full training data, only true words counted (no punctuation)Dictionary:LanguageEnglishCzechEstonianHungarianRomanianSloveneAutomaticBaseline Pull11.42% 9.18%23.02% 18.83%16.12% 13.95%8.35% 8.16%10.87% 7.76%20.53% 16.26%MixedBaseline Full11.40% 7.91%22.61% 14.78%16.19% 12.98%8.31% 8.00%10.81% 7.34%20.01% 13.29%IndependentBaseline Full7.07% 3.58%19.40% 9.59%9.94% 5.34%3.55% 2.58%7.49% 3.35%17.29% 9.00%Table 3: Error rate on reduced training data, dictionary: automaticLanguageEnglishCzechEstonianHungarianRomanianSloveneI00036.20%48.22%48.14%39.68%40.61%45.84%200029.36%42.95%42.10%32.21%35.02%39.58%Training500023.47%36.54%32.44%23.94%25.06%33.12%data size1000018.27%30.97%26.81%18.04%19.26%28.60%2000014.46%27.08%21.51%13.92%15.16%24.50%Full9.18%18.83%13.95%8.16%7.76%16.26%Table 4: Error rate on reduced training data, dictionary: mixedLanguageEnglishCzechEstonianHungarianRomanianSloveneTraining data sizeI00036.15%48.97%48.24%39.87%42.85%46.74%200029.58%41.93%42.79%32.71%35.70%39.88%500022.93%34.37%32.98%23.63%25.46%32.00%1000017.70%28.10%26.60%17.98%19.23%26.20%2000014.00%23.31%21.02%13.82%14.81%21.73%Full7.91%14.78%12.98%8.00%7.34%13.29%Table 5: ErrorLanguageEnglishCzechEstonianHungarianRomanianSlovenerate on reduced training data, dictionary: "independent"100010.29%22.51%13.11%6.84%13.11%24.63%20007.64%18.07%11.95%5.35%9.47%19.17%Training data size5000 100005.53% 4.54%17.33% 15.10%10.70% 9.29%4.29% 4.07%7.81% 6.18%16.17% 14.12%20000 Pull3.83% 3.58%12.62% 9.59%8.10% 5.34%3.48% 2.58%5.07% 3.35%12.62% 9.00%guage bias shows herein).
However, when the train-ing data size goes down, the advantage of predictingthe single morphological categories separately favorsthe exponential tagger (with the notable and sub-stantial exception of English).
The less data, thelarger the difference (Tab 10).16On the other hand, the Exponential tagger has been de-veloped on Czech originally and it lost on this language.
Itshould be noted that the original version of the exponen-tial tagger did contain more Czech-specific eatures, and thusmight in fact do better.The resulting accuracy (of both taggers) is stillunsatisfactory not only from the point of view ofresults obtained on English, but also from the prac-tical point of view: approx.
85% accuracy (Czech,Slovene) typically means that about five out of six10-word sentences contain at least one error in it.That is bad news e.g.
for parsing projects involvingtagging as a preliminary step.98Table 6: POS Error rate on reduced training data, dictionary: automaticLanguageEnglishCzechEstonianHungarianRomanianSlovene100026.77%24.32%35.81%30.54%31.33%27.16%Training data size2000 500020.82% 16.11%20.20% 13.46%30.52% 23.02%24.99% 18.09%27.59% 19.24%23.15% 17.01%1000011.86%9.70%18.26%13.15%14.51%12.89%20000 Full9.48% 5.64%7.22% 3.72%14.31% 8.46%10.29% 5.81%11.25% 5.21%9.74% 5.61%Table 7: POS Error rate on reduced training data, dictionary: mixedLanguageEnglishCzechEstonianHungarianRomanianSlovene100026.69%24.32%36.48%30.28%33.56%27.58%Training data size200021.09%20.61%31.76%25.25%28.34%23.30%5OOO15.82%13.47%23.55%17.59%20.03%16.85%1000011.53%10.19%18.21%12.89%14.52%12.59%200009.08%7.37%14.32%10.15%11.03%9.88%Fu l l4.94%3.76%8.2O%5.64%5.04%5.12%Table 8: POS Error rate on reduced training data, dictionary: "independent"Language Training data size1000 2000 5000 10000 20000 Full-English 6.42% 5.36% 3.63% 3.02% 2.53% 2.43%Czech 3.21% 2.85% 2.17% 2.01% 1.65% 1.12%Estonian 6.71% 6.32% 5.27% 4.31% 3.77% 2.36%Hungarian 5.35% 4.42% 3.39% 3.18% 2.75% 2.04%Romanian 9.51% 6.54% 5.36% 4.00% 3.18% 1.89%Slovene 6.10% 5.19% 4.04% 3.59% 3.25% 2.08%5 Conc lus ions5.1 The Differences Among LanguagesThe following discussion abstracts from the tagsetdesign, relying on the fact that the Multext-Eastproject has been driven by common tagset guidelinesto an unprecedented xtent, given the very differentlanguages involved.
At the same time, we acknowl-edge that even so, their design for the individuallanguages might have influenced the results.
Also,the quality of the annotation is an important factor;we believe though that the later data we obtainedfor the experiments described here are within therange of usual human error and do not suffer fromnegligence 1~.First of all, it is clear that these languages differsubstantially just by looking at the simple training17Specifically, we are sure that the post-release Czech,Slovene and Hungarian data we are using are without anno-tation defects beyond the usual occasional nnotation error,as they have been double checked, and we also believe thatthe other two languages are reasonably clean.
Bulgarian, al-though present on the CD, is unfortunately unusable since ithas not been manually annotated; for English, see above.data statistics, where the number of unique tags seenin a relatively small collection of about 100k tokens ishigh - from 401 (Hungarian) to 1033 (Slovene); com-pare that to English with only 139 tags.
However, itis interesting to see that the average per-token ambi-guity is much more narrowly distributed, and in factEnglish ranks 3rd (after Hungarian and Slovene),Czech being the last with almost every other tokenambiguous on average.
This ambiguity does not cor-respond with the results obtained: Slovene, beingthe second least ambiguous, is the second most dif-ficult to tag.
Only Czech behaves consistently bytailing the pack in both cases.5.2 Comparison to Previous ResultsAny comparison is necessarily difficult due to differ-ent evaluation methodologies, ven within the "best-only", accuracy-based reporting.
Nevertheless, wewill try.For Romanian, Tufts in his recent work (Tufts,1999) reports 98.5% accuracy (i.e.
1.5% error rate)on Romanian, using the classifier combination ap-proach advocated by e.g.
(Brill and Wu, 1998).
His99Table 10: Error rate comparison on reduced training data, automatic dictionaryLanguageEnglishCzechEstonianHungarianRomanianSloveneTraining data size2000ME Exp26'03% 29.36%50.77% 42.95%51.08% 42.10%41.12% 32.21%42.88% 35.02%49.46% 39.58%5000ME Exp17.70% 23.47%41.95% 36.54%40.09% 32.44%30.68% 23.94%30.07% 25.06%39.34% 33.12%20000ME Exp9.61% 14.46%28.16% 27.08%25.50% 21.51%17.27% 13.92%16.67% 15.16%27.77% 24.50%results are well above the 3.29% error rate achievedhere (with even a larger tagset of 1391 vs. 486 here),but the paper does not say how this number has beencomputed (training data size, the all-token/words-only question) thus making any conclusions difficultto make.
He also argues that his method is languageindependent but no results are mentioned for otherlanguages.For Czech, previous work achieved similar results(6.20% on newspaper text using the all-tokens-basederror rate computation, on 160,000 training tokens;vs. 7.04% here on approx, half that amount of train-ing data; same handling of unknown words).
This isin line with the expectations, since the same method-ology (tagging as well as evaluation) has been used,except he features used in that work were specifi-cally tuned to Czech.The most detailed account of Slovene (Erjavec etal., 1999) reports various results, which might notbe directly comparable because it is unclear whetherthey use the all-tokens-based or words-only compu-tation of the error rate.
They report 6.421% errorrate on the full tagset on known words, and 13.583%on all words (tokens?)
including unknown words(the exponential tagger we used achieved 13.82% onall tokens, 16.26% on words only).
They use almostthe same data (Orwell's 1984, but leaving out theAppendices) ls.
They also report that the originalCzech-specific exponential tagger used as a basis forthe work reported here achieved 7.28% error rate onSlovene on full tags on the same data, which meansthat by the changes to the exponential tagger aimedat its language independence w  introduced in Sec-tion 3, we have not achieved any improvement (onSlovene) of the exp.
tagger (the error rate stayed at7.26% - using all-tokens-based valuation umbers,dictionary available; but the data was not exactlythe same, presumably).5.3  D ic t ionary  vs .
T ra in ing  DataThis is, according to our opinion, the most interest-ing result of the experiments described so far.
As18Their tag count is lower (1021) than here (1033), butthat's not really relevant.
They do not report he averageambiguity or a similar measure.100already Table 2 clearly suggests, even the baselinetagging results obtained with the help of an indepen-dent dictionary are comparable (if not better) thanthe fully-trained tagger on 100k words, but withoutthe dictionary information.
The situation is evenclearer when comparing the POS-only results: herethe "independent" dictionary results are better byfar, with almost no training data needed.Looking at the characteristics of the languages, itis apparent that the inflections cause the problem:the coverage of a previously unseen text is inferior tothe usual coverage of English or another analyticallanguage.
Therefore, unless we can come up witha really clever way of learning rules for dealing withpreviously unseen words, it is clearly strongly prefer-able to work on a morphological dictionary 19, ratherthan to try to annotate more data.6 Future  WorkWe would like to compare more taggers using stillother methodologies, especially the MBT tagger,which achieved the best results on Slovene but whichwas not available to us at the time of writing thispaper.
Obviously, we would also like to use the clas-sifter combination method on them, to confirm thereally surprisingly good results on Romanian andtest it on the other languages as well.We would also like to enrich the best taggers avail-able today (such as the Maximum Entropy tagger)by using the dictionary information available andcompare the results with the exponential feature-based tagger we have been using in the experimentshere.For Czech and Slovene, the results are still far be-low what one would like to see (in absolute terms).
Itseems that the key lies in the initial feature set defi-nition - including statistical tagset clustering, whichmight potentially lead to more reliable estimates ofcertain parameters while using still the same size oftraining data.19Not necessarily manually - apparently, even a partiallysupervised method would be of tremendous help.7 AcknowledgementsThe author wishes to thank many Multext-East par-ticipants for their efforts to improve the originaldata, especially to Niki Petkevi~, Tomaz Erjavec,Heiki-Jaan Ka~lep and G?bor Pr6sz6ky, and for pro-viding the final versions of the annotated ata forthe experiments.
Any errors and mistakes are solelyto be blamed on the author, not the annotators, ofcourse.Re ferencesEric Brill and Jun Wu.
1998.
Classifier combinationfor improved lexical disambiguation.
In Proceed-ings of ACL/COLING'g8, pages 191-195, Mon-treal, Canada.
ACL/ICCL.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural anguage processing: A casestudy in part-of-speech tagging.
ComputationalLinguistics, 21:543-565.Jean-Pierre Chanod.
1997.
Current developmentsfor Central & Eastern European languages.
InProceedings of EU Project meeting TELRI I, Ro-mania.Kenneth W. Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestricted text.In Proceedings of the Second Conference on Ap-plied Natural Language Processing, pages 136-143,Austin, Texas.
ACL.Walter Daelemans, Jakub Zavrel, Peter Berck, andSteven Gillis.
1996.
MBT: A memory-based partof speech tagger generator.
In Proceedings ofWVLC 4, pages 14-27.
ACL.Tomaz Erjavec, Saso Dzeroski, and Jakub Zavrd.1999.
Morphosyntactic Tagging of Slovene: Eval-uating PoS Taggers and Tagsets.
Technical Re-port IJS-DP 8018, Dept.
for Intelligent Systems,Jozef Stefan Institute, Ljubljana, Slovenia, April2nd.Jan Haji~ and Barbora Hladk~t.
1997.
Tagging of in-flective languages: a comparison.
In Proceedingsof ANLP'gT, pages 136-143, Washington, DC.ACL.Jan Haji~ and Barbora Hladk& 1998.
Tagginginflective languages: Prediction of morphologi-cal categories for a rich, structured tagset.
InProceedings of A CL/COLING'98, pages 483-490,Montreal, Canada.
ACL/ICCL.Andrew W. Mackie Julian Benello and James A. An-derson.
1989.
Syntactic ategory disambiguationwith neural networks.
Computer Speech and Lan-guage, 3:203-217.Bernard Merialdo.
1992.
Tagging text with aprobabilistic model.
Computational Linguistics,20(2):155-171.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof EMNLP 1, pages 133-142.
ACL.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of In-ternational Con\]erence on New Methods in Lan-guage Processing, pages 44-49, Manchester, Eng-land.Dan Tufts.
1999.
Tiered tagging and combined lan-guage models classifiers.
In Proceedings of Text,Speech and Dialogue'99, Mari~nskd LLzn~, CzechRepublic, Sept. 15-18.Jean Vdronis.
1996a.
Multext-East(Copernicus 106).
http://www.lpl.univ-aix.fr/projects/multext-east.Jean Vdronis.
1996b.
Multext-East language-specific resources (Copernicus 106).http://www.lpl.univ-aix.fr/projects/multext-east/MTE2.html.101
