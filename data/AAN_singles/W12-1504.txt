INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 12?16,Utica, May 2012. c?2012 Association for Computational LinguisticsReferring in Installments: A Corpus Study of Spoken Object Referencesin an Interactive Virtual EnvironmentKristina Striegnitz?, Hendrik Buschmeier?and Stefan Kopp?
?Computer Science Department, Union College, Schenectady, NYstriegnk@union.edu?Sociable Agents Group ?
CITEC, Bielefeld University, Germany{hbuschme,skopp}@techfak.uni-bielefeld.deAbstractCommonly, the result of referring expressiongeneration algorithms is a single noun phrase.In interactive settings with a shared workspace,however, human dialog partners often split re-ferring expressions into installments that adaptto changes in the context and to actions of theirpartners.
We present a corpus of human?humaninteractions in the GIVE-2 setting in which in-structions are spoken.
A first study of objectdescriptions in this corpus shows that refer-ences in installments are quite common in thisscenario and suggests that contextual factorspartly determine their use.
We discuss whatnew challenges this creates for NLG systems.1 IntroductionReferring expression generation is classically consid-ered to be the problem of producing a single nounphrase that uniquely identifies a referent (Krahmerand van Deemter, 2012).
This approach is well suitedfor non-interactive, static contexts, but recently, therehas been increased interest in generation for situateddialog (Stoia, 2007; Striegnitz et al, 2011).Most human language use takes place in dynamicsituations, and psycholinguistic research on human?human dialog has proposed that the production ofreferring expressions should rather be seen as a pro-cess that not only depends on the context and thechoices of the speaker, but also on the reactions ofthe addressee.
Thus the result is often not a singlenoun phrase but a sequence of installments (Clarkand Wilkes-Gibbs, 1986), consisting of multiple utter-ances which may be interleaved with feedback fromthe addressee.
In a setting where the dialog partnershave access to a common workspace, they, further-more, carefully monitor each other?s non-linugisticactions, which often replace verbal feedback (Clarkand Krych, 2004; Gergle et al, 2004).
The followingexample from our data illustrates this.
A is instructingB to press a particular button.
(1) A: the blue buttonB: [moves and then hesitates]A: the one you see on your rightB: [starts moving again]A: press that oneWhile computational models of this behavior are stillscarce, some first steps have been taken.
Stoia (2007)studies instruction giving in a virtual environmentand finds that references to target objects are oftennot made when they first become visible.
Instead in-teraction partners are navigated to a spot from wherean easier description is possible.
Garoufi and Koller(2010) develop a planning-based approach of this be-havior.
But once their system decides to generate areferring expression, it is delivered in one unit.Thompson (2009), on the other hand, proposes agame-theoretic model to predict how noun phrasesare split up into installments.
While Thompson didnot specify how the necessary parameters to calculatethe utility of an utterance are derived from the contextand did not implement the model, it provides a goodtheoretical basis for an implementation.The GIVE Challenge is a recent shared task on sit-uated generation (Striegnitz et al, 2011).
In the GIVEscenario a human user goes on a treasure hunt in avirtual environment.
He or she has to press a series ofbuttons that unlock doors and open a safe.
The chal-lenge for the NLG systems is to generate instructionsin real-time to guide the user to the goal.
The instruc-tions are presented to the user as written text, which12means that there is less opportunity for interleavinglanguage and actions than with spoken instructions.While some systems generate sentence fragments incertain situations (e.g., not this one when the useris moving towards the wrong button), instructionsare generally produced as complete sentences andreplaced with a new full sentence when the contextchanges (a strategy which would not work for spokeninstructions).
Nevertheless, timing issues are a causefor errors that is cited by several teams who devel-oped systems for the GIVE challenge, and generatingappropriate feedback has been an important concernfor almost all teams (see the system descriptions in(Belz et al, 2011)).
Unfortunately, no systematic er-ror analysis has been done for the interactions fromthe GIVE challenges.
Anecdotally, however, not re-acting to signs of confusion in the user?s actions atall or reacting too late seem to be common causes forproblems.
Furthermore, we have found that the strat-egy of replacing instructions with complete sentencesto account for a change in context can lead to con-fusion because it seems unclear to the user whetherthis new instruction is a correction or an elaboration.In this paper we report on a study of the com-municative behavior of human dyads in the GIVEenvironment where instead of written text instructiongivers use unrestricted spoken language to direct in-struction followers through the world.
We find thatoften multiple installments are used to identify a ref-erent and that the instruction givers are highly respon-sive to context changes and the instruction followers?actions.
Our goal is to inform the development of ageneration system that generates object descriptionsin installments while taking into account the actionsof its interaction partner.2 A corpus of spoken instructions in avirtual environmentData collection method The setup of this studywas similar to the one used to collect the GIVE-2corpus of typed instructions (Gargett et al, 2010).Instruction followers (IFs) used the standard GIVE-2client to interact with the virtual environment.
In-struction givers (IGs) could observe the followers?position and actions in the world using an interactivemap, and they were also provided with the same 3Dview into the scene that the IFs saw on their screen.Differently from the normal GIVE-2 scenario, theIGs did not type their instructions but gave spokeninstructions, which were audio recorded as well asstreamed to the IFs over the network.
A log of the IFs?position, orientation and actions that was updated ev-ery 200ms was recorded in a database.Participants were recruited in pairs on BielefeldUniversity?s campus and received a compensationof six euros each.
They were randomly assignedto the roles of IG and IF and were seated and in-structed separately.
To become familiar with the task,they switched roles in a first, shorter training world.These interactions were later used to devise and testthe annotation schemes.
They then played two dif-ferent worlds in their assigned roles.
After the firstround, they received a questionnaire assessing thequality of the interaction; after the second round, theycompleted the Santa Barbara sense of direction test(Hegarty et al, 2006) and answered some questionsabout themselves.Annotations The recorded instructions of the IGswere transcribed and segmented into utterances (byidentifying speech pauses longer than 300ms) usingPraat (Boersma and Weenink, 2011).
We then createdvideos showing the IGs?
map view as well as the IFs?scene view and aligned the audio and transcriptionswith them.
The data was further annotated by the firsttwo authors using ELAN (Wittenburg et al, 2006).Most importantly for this paper, we classified ut-terances into the following types:(i) move (MV) ?
instruction to turn or to move(ii) manipulate (MNP) ?
instruction to manipulate an object(e.g., press a button)(iii) reference (REF) ?
utterance referring to an object(iv) stop ?
instruction to stop moving(v) warning ?
telling the user to not do something(vi) acknowledgment (ACK) ?
affirmative feedback(vii) communication management (CM) ?
indicating that theIG is planning (e.g., uhmm, just a moment, sooo etc.
)(viii) negative acknowledgment ?
indicating a mistake on theplayer?s part(ix) other ?
anything elseA few utterances which contained both move andpress instructions were further split, but in generalwe picked the label that fit best (using the above listas a precedence order to make a decision if two labelsfit equally well).
The inter-annotator agreement forutterance types was ?
= 0.89 (Cohen?s kappa), which13is considered to be very good.
Since the categorieswere of quite different sizes (cf.
Table 1), which mayskew the ?
statistic, we also calculated the kappa percategory.
It was satisfactory for all ?interesting?
cate-gories.
The agreement for category REF was ?
= 0.77and the agreement for other was ?
= 0.58.
The kappavalues for all other categories were 0.84 or greater.We reviewed all cases with differing annotations andreached a consensus, which is the basis for all resultspresented in this paper.
Furthermore, we collapsedthe labels warning, negative acknowledgment andother which only occurred rarely.To support a later more in depth analysis, we alsoannotated what types of properties are used in objectdescriptions, the givenness status of information ininstructions, and whether an utterance is giving pos-itive or negative feedback on a user action (even ifnot explicitly labeled as (negative) acknowledgment).Finally, information about the IF?s movements andactions in the world as well as the visible context wasautomatically calculated from the GIVE log files andintegrated into the annotation.Collected data We collected interactions betweeneight pairs.
Due to failures of the network connectionand some initial problems with the GIVE software,only four pairs were recorded completely, so thatwe currently have data from eight interactions withfour different IGs.
We are in the process of collect-ing additional data in order to achieve a corpus sizethat will allow for a more detailed statistical analy-sis.
Furthermore, we are collecting data in Englishto be able to make comparisons with the existingcorpus of written instructions in the GIVE world andto make the data more easily accessible to a wideraudience.
The corpus will be made freely availableat http://purl.org/net/sgive-corpus.Participants were between 20 and 30 years old andall of them are native German speakers.
Two of theIGs are male and two female; three of the IFs arefemale.
The mean length of the interactions is 5.24minutes (SD= 1.86), and the IGs on average use 325words (SD = 91).Table 1 gives an overview of the kinds of ut-terances used by the IGs.
While the general pic-ture is similar for all speakers, there are statisti-cally significant differences between the frequen-cies with which different IGs use the utterance typesTable 1: Overall frequency of utterance types.utterance type count %MV 334 46.58MNP 66 9.21REF 65 9.07stop 38 5.30ACK 92 12.83CM 97 13.53other 25 3.49Table 2: Transitional probabilities for utterance types.MVMNPREFstopACKCMotherIF pressMV .53 .08 .06 .06 .15 .08 .03 .00MNP .02 .03 .09 .02 .02 .02 .02 .80REF .00 .33 .19 .02 .14 .00 .02 .30stop .47 .03 .18 .03 .03 .16 .11 .00ACK .64 .08 .09 .03 .01 .10 .00 .05CM .53 .05 .10 .08 .01 .18 .05 .00other .44 .04 .12 .12 .08 .16 .00 .04IF press .21 .01 .00 .01 .36 .36 .04 .00(?2 = 78.82, p ?
0.001).
We did not find a signifi-cant differences (in terms of the utterance types used)between the two worlds that we used or between thetwo rounds that each pair played.3 How instruction givers describe objectsWe now examine how interaction partners establishwhat the next target button is.
Overall, there are 76utterance sequences in the data that identify a targetbutton and lead to the IF pressing that button.
Wediscuss a selection of seven representative examples.
(2) IG: und dann dr?ckst du den ganz rechten Knopf denblauen (and then you press the rightmost button theblue one; MNP)IF: [goes across the room and does it]In (2) the IG generates a referring expression iden-tifying the target and integrates it into an object ma-nipulation instruction.
In our data, 55% of the tar-get buttons (42 out of 76) get identified in this way(which fits into the traditional view of referring ex-pression generation).
In all other cases a sequence ofat least two, and in 14% of the cases more than two,utterances is used.The transitional probabilities between utterancetypes shown in Table 2 suggest what some commonpatterns may be.
For example, even though moveinstructions are so prevalent in our data, they areuncommon after reference or manipulate utterances.14Instead, two thirds of the reference utterances arefollowed by object manipulation instruction, anotherreference or an acknowledgement.
In the remainingcases, IFs press a button in response to the reference.
(3) IG: vor dir der blaue Knopf (in front of you the blue button;REF)IF: [moves across the room toward the button]IG: drauf dr?cken (press it; MNP)(4) IG: und auf der rechten Seite sind zwei rote Kn?pfe (andon the right are two red buttons; REF)IF: [turns and starts moving towards the buttons]IG: und den linken davon dr?ckst du (and you press the leftone; MNP)In (3) and (4) a first reference utterance is followedby a separate object manipulation utterance.
Whilein (3) the first reference uniquely identifies the target,in (4) the first utterance simply directs the player?sattention to a group of buttons.
The second utterancethen picks out the target.
(5) IG: dreh dich nach links etwas (turn left a little; MV)IF: [turns left] there are two red buttons in front of him(and some other red buttons to his right)IG: so, da siehst du zwei rote Schalter (so now you see twored buttons; REF)IF: [moves towards buttons]IG: und den rechten davon dr?ckst du (and you press theright one; MNP)IF: [moves closer, but more towards the left one]IG: rechts (right; REF)Stoia (2007) observed that IGs use move instruc-tions to focus the IF?s attention on a particular area.This is also common in our data.
For instance in (5),the IF is asked to turn to directly face the group ofbuttons containing the target.
(5) also shows how IGsmonitor their partners?
actions and respond to them.The IF is moving towards the wrong button causingthe IG to repeat part of the previous description.
(6) IG: den blauen Schalter (the blue button; REF)IF: [moves and then stops]IG: den du rechts siehst (the one you see on your right;REF)IF: [starts moving again]IG: den dr?cken (press that one; MNP)Similarly, in (6) the IG produces an elaborationwhen the IF stops moving towards the target, indicat-ing her confusion.
(7) IG: und jetzt rechts an der (and now to the right on the;REF)IF: [turns right, is facing the wall with the target button]IG: ja .
.
.
genau .
.
.
an der Wand den blauen Knopf (yes.
.
.
right .
.
.
on the wall the blue button; ACK, REF)IF: [moves towards button]IG: einmal dr?cken (press once; MNP)In (7) the IG inserts affirmative feedback whenthe IF reacts correctly to a portion of his utterance.As can be seen in Table 2, reference utterances arerelatively often followed by affirmative feedback.
(8) IF: [enters room, stops, looks around, ends up looking atthe target]IG: ja genau den gr?nen Knopf neben der Lampe dr?cken(yes right, press the green button next to the lamp;MNP)IGs can also take advantage of IF actions that arenot in direct response to an utterance.
This happensin (8).
The IF enters a new room and looks around.When she looks towards the target, the IG seizes theopportunity and produces affirmative feedback.4 Conclusions and future workWe have described a corpus of spoken instructions inthe GIVE scenario which we are currently buildingand which we will make available once it is com-pleted.
This corpus differs from other corpora of task-oriented dialog (specifically, the MapTask corpus(Anderson et al, 1991), the TRAINS corpus (Hee-man and Allen, 1995), the Monroe corpus (Stent,2000)) in that the IG could observe the IF?s actionsin real-time.
This led to interactions in which in-structions are given in installments and linguistic andnon-linguistic actions are interleaved.This poses interesting new questions for NLG sys-tems, which we have illustrated by discussing thepatterns of utterance sequences that IGs and IFs usein our corpus to agree on the objects that need tobe manipulated.
In line with results from psycholin-guistics, we found that the information necessary toestablish a reference is often expressed in multipleinstallments and that IGs carefully monitor how theirpartners react to their instructions and quickly re-spond by giving feedback, repeating information orelaborating on previous utterance when necessary.The NLG system thus needs to be able to de-cide when a complete identifying description canbe given in one utterance and when a description ininstallments is more effective.
Stoia (2007) as wellas Garoufi and Koller (2010) have addressed thisquestion, but their approaches only make a choice be-tween generating an instruction to move or a uniquelyidentifying referring expression.
They do not con-sider cases in which another type of utterance, forinstance, one that refers to a group of objects or gives15an initial ambiguous description, is used to draw theattention of the IF to a particular area and they do notgenerate referring expressions in installments.The system, furthermore, needs to be able to in-terpret the IF?s actions and decide when to insert anacknowledgment, elaboration or correction.
It thenhas to decide how to formulate this feedback.
Theaddressee, e.g., needs to be able to distinguish elabo-rations from corrections.
If the feedback was insertedin the middle of a sentence, if finally has to decidewhether this sentence should be completed and howthe remainder may have to be adapted.Once we have finished the corpus collection, weplan to use it to study and address the questions dis-cussed above.
We are planning on building on thework by Stoia (2007) on using machine learning tech-niques to develop a model that takes into account var-ious contextual factors and on the work by Thompson(2009) on generating references in installments.
Theset-up under which the corpus was collected, further-more, lends itself well to Wizard-of-Oz studies to testthe effectiveness of different interactive strategies fordescribing objects.Acknowledgments This research was supportedby the Deutsche Forschungsgemeinschaft (DFG) inthe Center of Excellence in ?Cognitive InteractionTechnology?
(CITEC) and by the Skidmore UnionNetwork which was funded through an ADVANCEgrant from the National Science Foundation.ReferencesAnne H. Anderson, Miles Bader, Ellen Gurman Bard, Eliz-abeth Boyle, Gwyneth Doherty, Simon Garrod, StephenIsard, Jacqueline Kowtko, Jan McAllister, Jim Miller,Catherine Sotillo, Henry S. Thompson, and ReginaWeinert.
1991.
The HCRC map task corpus.
Lan-guage and Speech, 34:351?366.Anja Belz, Albert Gatt, Alexander Koller, and KristinaStriegnitz, editors.
2011.
Proceedings of the Genera-tion Challenges Session at the 13th European Workshopon Natural Language Generation, Nancy, France.Paul Boersma and David Weenink.
2011.
Praat: doingphonetics by computer.
Computer program.
RetrievedMay 2011, from http://www.praat.org/.Herbert H. Clark and Meredyth A. Krych.
2004.
Speakingwhile monitoring addressees for understanding.
Jour-nal of Memory and Language, 50:62?81.Herbert H Clark and Deanna Wilkes-Gibbs.
1986.
Refer-ring as a collaborative process.
Cognition, 22:1?39.Andrew Gargett, Konstantina Garoufi, Alexander Koller,and Kristina Striegnitz.
2010.
The GIVE-2 corpusof giving instructions in virtual environments.
In Pro-ceedings of the Seventh International Conference onLanguage Resources and Evaluation (LREC?10), pages2401?2406, Valletta, Malta.Konstantina Garoufi and Alexander Koller.
2010.
Au-tomated planning for situated natural language gener-ation.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1573?1582, Uppsala, Sweden.Darren Gergle, Robert E. Kraut, and Susan R. Fussell.2004.
Action as language in a shared visual space.
InProceedings of the 2004 ACM Conference on ComputerSupported Cooperative Work, pages 487?496, Chicago,IL.Peter A. Heeman and James Allen.
1995.
The Trains 93dialogues.
Technical Report Trains 94-2, Computer Sci-ence Department, University of Rochester, Rochester,NY.Mary Hegarty, Daniel R. Montello, Anthony E. Richard-son, Toru Ishikawa, and Kristin Lovelace.
2006.
Spa-tial abilities at different scales: Individual differences inaptitude-test performance and spatial-layout learning.Intelligence, 34:151?176.Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A survey.Computational Linguistics, 38:173?218.Amanda Stent.
2000.
The Monroe corpus.
TechnicalReport 728/TN 99-2, Computer Science Department,University of Rochester, Rochester, NY.Laura Stoia.
2007.
Noun Phrase Generation for SituatedDialogs.
Ph.D. thesis, Graduate School of The OhioState University, Columbus, OH.Kristina Striegnitz, Alexandre Denis, Andrew Gargett,Konstantina Garoufi, Alexander Koller, and Mari?t The-une.
2011.
Report on the second second challenge ongenerating instructions in virtual environments (GIVE-2.5).
In Proceedings of the Generation ChallengesSession at the 13th European Workshop on NaturalLanguage Generation, pages 270?279, Nancy, France.Will Thompson.
2009.
A Game-Theoretic Model ofGrounding for Referential Communication Tasks.
Ph.D.thesis, Northwestern University, Evanston, IL.Peter Wittenburg, Hennie Brugman, Albert Russel, AlexKlassmann, and Han Sloetjes.
2006.
ELAN: A pro-fessional framework for multimodality research.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC), pages1556?1559, Genoa, Italy.16
