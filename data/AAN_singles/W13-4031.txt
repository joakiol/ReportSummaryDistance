Proceedings of the SIGDIAL 2013 Conference, pages 183?192,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsMultimodality and Dialogue Act Classification in the RoboHelper ProjectLin ChenDepartment of Computer ScienceUniversity of Illinois at Chicago851 S Morgan ST, Chicago, IL 60607lchen43@uic.eduBarbara Di EugenioDepartment of Computer ScienceUniversity of Illinois at Chicago851 S Morgan ST, Chicago, IL 60607bdieugen@uic.eduAbstractWe describe the annotation of a multi-modal corpus that includes pointing ges-tures and haptic actions (force exchanges).Haptic actions are rarely analyzed as full-fledged components of dialogue, but ourdata shows haptic actions are used to ad-vance the state of the interaction.
We re-port our experiments on recognizing Di-alogue Acts in both offline and onlinemodes.
Our results show that multimodalfeatures and the dialogue game aid in DAclassification.1 IntroductionWhen people collaborate on physical or virtualtasks that involve manipulation of objects, dia-logues become rich in gestures of different kinds;the actions themselves that collaborators engagein also perform a communicative function.
Col-laborators gesture while speaking, e.g.
saying?Try there??
while pointing to a faraway location;they perform actions to reply to their partner?s ut-terances, e.g.
opening a cabinet to comply with?please check cabinet number two?.
Conversely,they use utterances to reply to their partner?s ges-tures and actions, e.g.
saying ?not there, try theother one?
after their partner opens a cabinet.
Ges-tures and actions are an important part of such di-alogues; while the role of pointing gestures hasbeen explored, the role that haptic actions (forceexchanges) play in an interaction has not.In this paper, we present our corpus of multi-modal dialogues in a home care setting: a helperis helping an elderly person perform activities ofdaily living (ADLs) such as preparing dinner.
Weinvestigate how to apply Dialogue Act (DA) clas-sification to these multimodal dialogues.
Manychallenges arise.
First, an utterance may not di-rectly follow a spoken utterance, but a gesture or ahaptic action.
Likewise, the next move is not nec-essarily an utterance, it can be a gesture (pointingor haptics) only, or a multimodal utterance.
Third,when people use gestures and actions togetherwith utterances, the utterances become shorter,hence the textual context that has been used to ad-vantage in many previous models is impoverished.Our contributions concern: exploring the dialoguefunctions of what we call Haptic-Ostensive (H-O)actions (Foster et al 2008), namely haptics ac-tions that often perform a referential function; ex-perimenting with both offline and online DA clas-sification, whereas most previous work only fo-cuses on offline classification (Stolcke et al 2000;Hastie et al 2002; Di Eugenio et al 2010a); high-lighting the role played by multimodal featuresand dialogue structure (in the form of dialoguegames) as concerns DA classification.Our work is part of the RoboHelper project (DiEugenio et al 2010b) whose ultimate goal is todeploy robotic assistants for the elderly so thatthey can safely remain living in their home.
Themodels we derive from our experiments are thebuilding blocks of a multimodal information-statebased dialogue manager, whose architecture isshown in Figure 1.
The dialogue manager per-forms reference resolution, specifically resolvingthird person pronouns and deictics in utterances;classifies utterances to DAs; infers the dialoguegames for utterances; updates the dialogue state,and finally decides what the next step is in the in-teraction.
We have discussed our approach to mul-timodal reference resolution in (Chen et al 2011;Chen and Di Eugenio, 2012).
In this paper, we fo-cus on the Dialogue Act classification component.We will also touch on Dialogue Game inference.Our collaborators are developing the speech pro-cessing, vision and haptic recognition components(Franzini and Ben-Arie, 2012; Ma and Ben-Arie,2012; Javaid and Z?efran, 2012), that, when inte-grated with the dialogue manager we are building,183Figure 1: System Architecturewill make the interface situated in and able to dealwith a real environment.After discussing related work in Section 2, wepresent our multimodal corpus and the multidi-mensional annotation scheme we devised in Sec-tion 3.
In Section 4 we discuss all the features weused to build machine learning models to classifyDAs.
Sections 5 is devoted to our experiments andthe results we obtained.
We conclude and discussfuture work in Section 6.2 Related WorkDue to its importance in dialogue research, DAclassification has been the focus of a large bodyof research (Stolcke et al 2000; Sridhar etal., 2009; Di Eugenio et al 2010a; Boyeret al 2011).
Some of this work has beenmade possible by several available corpora taggedwith DAs, including HCRC Map Task (Ander-son et al 1991), CallHome (Levin et al 1998),Switchboard (Graff et al 1998), ICSI MeetingRecorder (MRDA) (Shriberg et al 2004), and theAMI multimodal corpus (Carletta, 2007).Researchers have applied various approachesto this task.
Initially only simple textual fea-tures were used, e.g.
n-grams were used tomodel the constraints for DA sequences in anHMM model (Stolcke et al 2000).
Zimmermannet al(2006) investigated the joint segmentationand classification of DAs using prosodic features.Sridhar et al(2009) showed that prosodic cuescan improve DA classification for a Maximum En-tropy based model.
Di Eugenio et al(2010a)extended Latent Semantic Analysis with linguis-tic features, including dialogue game information.Boyer et al(2011) integrates facial expressionsto significantly improve the recognition of severalDAs, whereas Ha et al(2012) shows that auto-matically recognized postural features may help todisambiguate DAs.It should be pointed out that most of this workfocuses on offline DA classification ?
namely, DAclassification is performed on the corpus usingthe gold-standard classification for the previousDA(s).
Since some sort of history of previousDAs is used by all systems, using online classi-fication for the previous DAs will unavoidably im-pact performance (Sridhar et al 2009; Kim et al2012).
Additionally, for models such as HMMsand CRF that approach the problem as sequencelabeling, online processing means that only a par-tial sequence is available.3 The ELDERLY-AT-HOME CorpusThis work is based on the ELDERLY-AT-HOMEcorpus, a multimodal corpus in the domain of el-derly care (Chen and Di Eugenio, 2012).
Thecorpus contains 20 human-human dialogues.
Ineach dialogue, a helper (HEL) and an elderlyperson (ELD) perform Activities of Daily Liv-ing (ADL) (Krapp, 2002), such as getting up fromchairs, finding pots, cooking pasta.
The settingis a fully equipped studio apartment used forteaching and research in a partner university (seeFigure 2).
The corpus contains 482 minutes ofrecorded videos, which comprise 301 minutes ofwhat we call effective video, obtained by eliminat-ing irrelevant content such as explanations of thetasks and interruptions by the person who accom-panied the elderly subject (who is not playing thepart of the helper).
This 301 minutes contain 4782spoken turns.
The corpus includes video and au-dio data in .avi and .wav format, haptics data col-lected via instrumented gloves in .csv format, andthe transcribed utterances in xml format.The Find subcorpus of our corpus comprisesonly Find tasks, where subjects look for and re-trieve various kitchen objects such as pots, silver-ware, pasta, etc.
from various locations in theapartment.
We define a Find task as a continuoustime span during which the two subjects are col-laborating on finding objects.
Find tasks naturallyarise while performing an ADL such as preparingdinner.
Figure 3 shows a Find task example.184Figure 2: Data Collection ExperimentFigure 3: Find Task Example3.1 AnnotationWe devised a multidimensional annotation schemesince we are interested in investigating the roleplayed in the interaction by modalities differentfrom speech.
Our annotation scheme comprisesthree main components: the multimodal event an-notation, which includes annotating for pointinggestures, haptic-ostensive actions, their features,and their relationships to utterances; the dialogueact annotation; and the referential expression an-notations already described in (Chen et al 2011;Chen and Di Eugenio, 2012).3.1.1 Multimodal Event AnnotationTo study the roles played by different sorts of mul-timodal actions, and how they contribute to theflow of the dialogue, pointing gestures, Haptic-Ostensive (H-O) actions, and the relations amongthem have been annotated on the Find subcorpus.The Find subcorpus contains 137 Find tasks, col-lected from the dialogues of 19 pairs of subjectsfrom the larger corpus.
1 The multimodal annota-1One pair of subjects was excluded, because ELD ap-peared confused.
Our goal was to recruit elderly subjects withtion tool Anvil (Kipp, 2001) was used to transcribeall the utterances, and to annotate for all categoriesdescribed in this paper.
Each annotation categoryis an annotation group in Anvil.
For each subject,one track is defined for each annotation group, fora total of 4 tracks per subject in Anvil.Pointing gestures are used naturally when peo-ple refer to a far away object.
We define a pointinggesture as a hand gesture without physical contactwith the target.
Our definition of pointing gesturedoes not include head or other body part move-ments used to indicate targets.
Our corpus in-cludes very few occurrences of those; additionally,our collaborators in the RoboHelper project focuson recognizing hand gestures.
We have identifiedtwo types of pointing gestures.
The first is, point-ing gestures with an identifiable target, which isusually indicated by a short time stable hand point-ing.
The other type is without a fixed target.
Itusually happens when the subject points to severaltargets in a short time, or the subject just points toa large space area.For a pointing gesture, we mark two attributes:the time span and the target.
The time span ofa pointing gesture starts when the subject initi-ates the hand movement, ends when the subjectstarts to draw the hand back.
We have devised aReferring Index System (Chen and Di Eugenio,2012) to mark the different types of targets: sin-gle identifiable target, multiple identifiable targetsand unidentifiable target.During Find tasks, subjects need to physicallyinteract with the objects, e.g.
they need to opencabinets to get plates, to put a pot on the stove etc.Those physical contact actions often perform a re-ferring function as well, either adding new enti-ties to the discourse model, or referring to an al-ready established referent.
For example, in Fig-ure 3, the action [Touch(Hel,Drawer1)] that ac-companies Utt4 disambiguates This by referring toDrawer1, tantamount to a pointing gesture; con-versely, the action [Takeout(HEL,spoon1)] associ-ated with Utt8 establishes a referent for spoon1.Following (Foster et al 2008), we label Haptic-Ostensive (H-O) those actions that involve physi-cal contact with an object, and that can at the sametime perform a referring function.
Note that targetobjects here exclude the partner?s body parts, aswhen HEL helps ELD get up from a chair.No existing work that we know of identifiesintact cognitive functions, but this subject was an exception.185types of H-O actions.
Hence, we had to define ourown categories, based on the following two princi-ples: (1) The H-O types must be grounded in ourdata, namely, the definitions are empirically based:these H-O actions are frequently observed in thecorpus.
(2) They are within the scope of what ourcollaborators can recognize from the haptic sig-nals.
The five H-O action types we defined are:?
Touch: when the subject only touches thetargets, no immediate further actions are per-formed?
MANIP-HOLD: when the subject takes outor picks up an object and holds it stably for ashort period of time?
MANIP-NO-HOLD: when the subject takesout or picks up an object, but without explic-itly showing it to the other subject?
Open: starts when the subject has physicalcontact with the handle of the fridge, a cabi-net or a drawer, and starts to pull; ends whenthe physical contact is off?
Close: when the subject has physical con-tact with the handle of the fridge, a cabinetor a drawer, and starts to push; ends when thephysical contact is offFor H-O action annotation, three attributes aremarked: time span, target and action type.
The?Target?
attribute is similar to the ?Target?
at-tribute in pointing gesture annotation.
Since H-O actions are more accurate than pointing ges-tures (Foster et al 2008), the targets are all iden-tifiable.Table 1 provides distributions of the length inseconds for different types of events in the Findcorpus.
Table 2 shows the counts of differentevents divided by type of participant.
From thesetwo tables, it is apparent that:?
Pointing gestures and H-O actions were fre-quently used: their total corresponds to 61%of the number of utterances?
Utterances are short: only 1.7?, and 4.2words on average?
ELD performed 66% of pointing gestures,and HEL 97.5% of H-O actionsMultimodal Event Relation Annotation.Pointing gestures and H-O actions can accompanyan utterance, e.g.
see move 2 in Figure 3: HELUtterances Pointing H-O Actions Total2555?
571?
1088?
4377?Table 1: Find Subcorpus: Length in secondsELD HEL TotalUtterances 756 760 1516Words 3612 2981 6593Pointing 219 113 332H-O Actions 15 582 597Table 2: Find Subcorpus: Countsasks ?Down there?
while pointing to a drawer;or can be used independently, e.g.
see move 6in Figure 3: HEL does not utter any words, butopens the drawer after ELD confirms that isthe right drawer with ?Uh-huh?.
In the lattercase, HEL used an action to respond to ELD.Pointing gestures and H-O actions are followedby utterances as well, e.g.
move 11 in Figure 3:after HEL opens a drawer, ELD says ?Yes, thereit is?.To understand how pointing gestures and H-Oactions participate in the dialogues and how theyinteract with utterances, we further annotated therelationship between utterances, pointing gesturesand H-O actions.
Just using timespans is notsufficient.
It is not necessarily the case that utter-ance U is associated with gesture / H-O action Gif their timespans overlap.
This type of annotationis purely local: the fact that turns 2-5 in Figure 3confirm which drawer to open, would be capturedat the dialogue game level.First, we assign to each utterance, pointing ges-ture and H-O action a unique event index, so thatwe can refer to these events with their indices.
Forpointing gestures and H-O actions, we define twomore attributes: ?associates?
and ?follows?.
If apointing gesture or H-O action is associated withan utterance, the ?associates?
value will be the in-dex of that utterance; by default, the ?associates?value is empty.
If a pointing gesture or H-O ac-tion independently follows an utterance, the ?fol-lows?
value will be that utterance?s index.
E.g.,for move 6 in Figure 3, we mark the H-O action?Open?
with ?follows [5]?.For utterances, we only mark the ?follows?
at-tribute.
If an utterance directly follows a point-ing gesture or H-O action, we use the index of thepointing gesture or H-O action as the ?follows?value.
By default, the ?follows?
attribute of an ut-terance is empty.
It means that an utterance fol-186lows its immediate previous utterance.We define a move as any combination of relatedutterances, pointing gestures and H-O actions, per-formed by the same subject.
On the basis of theevent relation annotations, we can compute the di-alogue?s move flow using the following algorithm.1.
Order all the utterances in a Find task sessionby the utterance start time2.
Until all the utterances are processed, foreach unprocessed utterance ui:(a) If ui follows a pointing gesture or H-Oaction, that pointing gesture or H-O ac-tion forms a new move mk; add mk tothe sequence before ui(b) Find all the pointing gestures and H-O actions labelled as associates of ui.These events form the movemi togetherwith ui(c) Recursively find the events which fol-low the last generated move, togetherwith all their associated events to formanother moveThis algorithm computes 1791 moves, as shown inTable 3.
More than 90% of pointing gestures areused with utterances.
Only 377 out of 596 H-O ac-tions are included in themoves, mostly because theH-O action ?Close?
frequently follows an ?Open?action (these cases are not detected by the algo-rithm, because they don?t advance the dialogue).ELD HEL TotalUtterances 545 507 1052Pointing 9 11 20H-O 5 213 218Utterance&Pointing 209 100 309Utterance&H-O 2 153 155Total 770 984 1754Table 3: Moves Statistics in Find Corpus3.1.2 Dialogue Act AnnotationSince the Find corpus is task-oriented in nature,we built on the dialogue act inventory of HCRCMapTask, a well-known task oriented corpus (An-derson et al 1991).
The MapTask tag set con-tains 11 moves:2 instruct, explain, check, align,query-w, query-yn; acknowledge, reply-y, reply-n,reply-w, clarify.
However, this inventory of DAsdoes not cover utterances that are used to respond2A twelfth move, Ready, does not appear in our corpus.to gestures and actions, such as Utt.11 in Figure 3.The semantics of the reply-{y/n/w} tags does notcover these situations.
Hence, we devised threemore tags, which apply only to statements that fol-low a move composed exclusively of a gesture oran action (in the sense of ?follow?
just discussed):?
state-y: a statement which conveys ?yes?,such as Utt.11 in Figure 3.?
state-n: a statement which conveys ?no?, e.g.if Utt.11 had been Wait, try the third drawer.?
state: still a statement , but not conveying ac-ceptance or rejection, e.g.
So we got the soup.Hence, the DAs in {state-y, state-n, state} areused to tag responses to actions, and the DAsin {reply-y, reply-n, reply-w} are used to tag re-sponses to utterances.
Table 4 shows the distribu-tion of DAs by subject.Dialogue Act ELD HEL Total RatioInstruct 295 19 314 20.7%Acknowledge 22 186 208 13.7%Reply-y 179 3 182 12.0%Check 1 155 156 10.3%Query-yn 23 133 156 10.3%Query-w 3 144 147 9.7%Reply-w 132 4 136 9.0%State-y 40 36 76 5.0%State-n 16 50 66 4.4%Reply-n 27 9 36 2.4%State 7 15 22 1.5%Explain 10 4 14 0.9%Align 1 2 3 0.3%Total 756 760 1516 100%Table 4: Dialogue Act Counts in Find CorpusIntercoder Agreement.
In order to verify thereliability of our annotations, we double coded15% of the data for pointing gestures, H-O actionsand DAs.
These are the dialogues from 3 pairs ofsubjects, and contain 22 Find tasks.
Because thepointing gestures and H-O actions are time spanbased, when we calculate agreement, we use anoverlap based approach.
If the two annotationsfrom the two coders overlap by more than 50% ofthe event length, and the other attributes are thesame, we count this as a match.
We used ?
tomeasure the reliability of the annotation (Cohen,1960).
We obtained reasonable values: for point-ing gestures, ?=0.751, for H-O actions, ?=0.703,and for DAs, ?=0.789.1874 Experimental SetupWe ran experiments classifying the DA tag for thecurrent utterance.
We employ supervised learn-ing approaches, specifically: Conditional RandomField (CRF) (Lafferty et al 2001), Maximum En-tropy (MaxEnt), Naive Bayes (NB), and DecisionTree (DT).
These algorithms are widely used forDA classification (Sridhar et al 2009; Ivanovic,2008; Ha et al 2012; Kim et al 2012).
Weused Mallet (McCallum, 2002) to build CRF mod-els.
MaxEnt models were built using the Max-Ent 3 package from the Apache OpenNLP pack-age.
Naive Bayes and Decision Tree models werebuilt with theWeka (Hall et al 2009) package (fordecision trees, we used the J48 implementation).All the results we will show below were obtainedusing 10 fold cross validation.4.1 FeaturesAmong our goals were not only to obtain effec-tive classifiers, but also to investigate which kindof features are most effective for our tasks.
Asa consequence, beyond textual features and dia-logue history features, we experimented with mul-timodal features extracted from other modalities,utterance features, and automatically inferred dia-logue game features.Textual features (TX) are the most widely usedfeatures for DA classification (Stolcke et al 2000;Bangalore et al 2008; Sridhar et al 2009; Di Eu-genio et al 2010a; Kim et al 2010; Boyer et al2011; Ha et al 2012; Kim et al 2012).
The tex-tual features we use include lexical, syntactic, andheuristic features.?
Lexical features: Unigrams of the words andpart-of-speech tags in the current utterance.The words used in the features are processedusing the morphology tool from the Stanfordparser (De Marneffe and Manning, 2008).?
Syntactic features: The top node and itsfirst two child nodes from the sentence parsetree.
If an utterance contains multiple sen-tences, we use the last sentence.
Sentencesare parsed using the Stanford parser.?
Number of sentences and number of words inthe utterance.
We use Apache OpenNLP li-brary 4 to detect sentences and tokenize them.3http://maxent.sourceforge.net4http://opennlp.apache.org/?
Heuristic features: whether an utterance con-tains WH words (e.g.
what, where), whetheran utterance contains yes/no words (e.g.
yes,no, yeah, nope).Utterance features (UT) are extracted fromthe current utterance?s meta information.
Previ-ous research showed that utterance meta informa-tion such as the utterance speaker can help classifyDAs (Ivanovic, 2008; Kim et al 2010).?
The actor of the utterance?
The time length of the utterance?
The distance of the current utterance from thebeginning of the dialogueThe pointing gesture feature (PT) indicateswhether the actor of the current utterance ui ismaking a pointing gesture G, i.e., whether G isassociated with ui, and hence, part of move mi.Haptic-Ostensive features (H-O) indicatewhether the actor of the current utterance ui is per-forming any H-O action G i.e., whether G is asso-ciated with ui, and hence, part of move mi; andthe type of that action, if yes.Location features (LO) include the locationsof the two actors, whether they are in the samelocation, whether the actor of the current utter-ance changes the location during the utterance.Since we do not have precise measurement of sub-jects?
locations, we annotate approximate loca-tions by dividing the apartment into four large ar-eas: kitchen, table, lounge and bed.The dialogue game feature (DG) models hi-erarchical dialogue structure.
Some previous re-search on DA classification has shown that hier-archical dialogue structure encoded via the no-tion of conversational games (Carlson, 1983) sig-nificantly improves DA classification (Hastie etal., 2002; Sridhar et al 2009; Di Eugenio et al2010a).
In MapTask, a game is defined as a se-quence of moves starting with an initiation (in-struct, explain, check, align, query-yn, query-w)and encompassing all utterances up until the pur-pose of the game has been fulfilled, or abandoned.In the Find corpus, dialogue games have not beenannotated.
In order to use the DG feature, we usea just-in-time approach to infer dialogue games.For each dialogue, we maintain a stack for dia-logue games.
When an utterance is classified asan initiating DA tag, we assume the dialogue has188entered a new dialogue game, and push the DA la-bel as the dialog game to the top of the stack.
TheDG feature value is the top element of the stack.The dialogue game feature is always inferred atrun time during classification process, just beforean utterance is being processed.
Hence, when weclassify the DA for the current utterance ui, theDG value that we use is the closest preceding ini-tiating DA.Dialogue history features (DH) model whathappened before the current utterance (Sridhar etal., 2009; Di Eugenio et al 2010a).
We encode:?
The previous move?s actor?
Whether the previous move has the same ac-tor as the current move?
The type of the previous move; if it is an ut-terance, its DA tag; if it is an H-O action, thetype of H-O action5 DA Classification ExperimentsWe ran the DA classification experiments withthree goals.
First, we wanted to assess the ef-fectiveness of different types of features, espe-cially, the effectiveness of gesture, H-O action, lo-cation and dialogue game features.
Second, wewanted to compare the performances of differentmachine learning algorithms on such a multimodaldialogue dataset.
Third, we wanted to investigatethe performances of different algorithms in the on-line and offline experiment settings.
The DA clas-sification task could be treated as a sequence label-ing problem (Stolcke et al 2000).
However, dif-ferent from other sequence labeling problems suchas part-of-speech tagging, a dialogue system can-not wait until the whole dialogue ends to classifythe current DA.
A dialogue system needs onlineDA classification models to classify the DAs whena new utterance is processed by the system.
Thereare two differences between online and offline DAclassification modes.
First, when we generate thedialogue history and dialogue game features, weuse the previously classified DA tag results for on-line mode, while we use the gold-standard DA tagsfor offline mode.
Second, MaxEnt (using beamsearch) and CRF evaluate and classify all the ut-terances in a dialogue at the same time in offlinemode; however in online mode, MaxEnt and CRFcan only work on the partial sequence up to theutterance to classify.
Whereas this may sound ob-vious, it explains why the performance of theseclassifiers may be even more negatively affectedin online mode with respect to their offline perfor-mance, as compared to other classifiers.
We willsee that indeed this will happen for CRF, but notfor MaxEnt.To evaluate feature effectiveness, we group thefeatures into seven groups: textual features (TX),utterance features (UT), pointing gesture fea-ture (PT), H-O action features (H-O), locationfeatures (LO), dialogue game feature (DG), dia-logue history features (DH).
Then we generate allthe combinations of feature groups to run exper-iments.
For each classification algorithm, we ran10-fold cross-validation experiments, for each fea-ture group combination, in both online and offlinemode.
It would be impossible to report all our re-sults.
Similarly to (Ha et al 2012), we report ourresults with single feature groups and incremen-tal feature group combinations, as shown in Ta-ble 5.
Whereas all combinations were tried, theomitted results do not shed any additional light onthe problem.
The majority baseline, which al-ways assigns the most frequent tag to every utter-ance, has an accuracy of 20.3%.The CRF offline model performs best, whichconfirms the results of (Kim et al 2010; Kimet al 2012).
This is due to the strong correla-tion between dialogue history features (DH) andthe states of the CRF.
In online mode, when thereis noise in the previous DA tags, the CRF?s per-formance drops significantly (p?.005, using ?2).A significant drop in performance from offline toonline mode also happens to NB (p?.005) andDT (p<.025).
MaxEnt performs very stably, thebest online model performs only .015 worse thanthe best offline model.
The best MaxEnt offlinemodel beats the other algorithms?
best models ex-cept CRF, while the MaxEnt online model outper-forms all the other algorithms?
online models.
Ourresults thus demonstrate that MaxEnt works bestfor online DA classification on our data.As concerns features, for online models, textualfeatures (TX) are the most predictive as a featuretype used by itself.
When we add pointing ges-ture (PT), H-O features (H-O) and location fea-tures (LO) together to textual features, we noticea significant performance improvement for mostmodels (except CRF models).
For MaxEnt, whichgives the best results for online models, none ofthe gesture, H-O action and location features alonesignificantly improve the results, but all three to-189Features CRF MaxEnt NB DTOffline Online Offline Online Offline Online Offline Online1.
TX (Textual) .654 .641 .630 .630 .449 .453 .450 .4502.
UT (Utterance) .506 .376 .353 .353 .417 .417 .392 .3923.
PT (Pointing) .225 .155 .210 .210 .212 .212 .212 .2124.
H-O (Haptic-Ostensive) .187 .147 .237 .237 .243 .243 .212 .2125.
LO (Location) .259 .176 .264 .264 .259 .259 .265 .2656.
DG (Dialogue Game) .737 .136 .305 .189 .212 .212 .212 .2127.
DH (Dialogue History) .895 .119 .480 .302 .478 .284 .471 .2948.
TX+PT .654 .651 .639 .639 .453 .453 .450 .4509.
TX+PT+H-O .670 .649 .637 .637 .456 .456 .449 .44910.
TX+PT+H-O+LO .648 .645 .657?
.657?
.523?
.523?
.536?
.536?11.
TX+PT+H-O+LO+UT .668 .612 .685 .685 .563 .563 .568 .56812.
TX+PT+H-O+LO+UT+DG .770??
.528 .722??
.709??
.566 .591??
.576 .607??13.
TX+PT+H-O+LO+UT+DG+DH .847?
.475 .757?
.742?
.635?
.606 .671?
.627Table 5: Dialogue Act Classification Accuracy: * indicates significant improvement after adding PT+H-O+LO to TX (cf.
lines 1 and 10); ** indicates significant improvement after adding DG to TX+PT+H-O+LO+UT (cf.
lines 11 and 12); ?indicates significant improvement after adding DH to TX+PT+H-O+LO+UT+DG (cf.
lines 12 and 13); bold font indicates the feature group set giving best performancefor each column.gether do.
This confirms the finding of (Ha et al2012) that non-verbal features help DA classifica-tion.
To assess which feature is the most importantamong those three non-verbal features, we exam-ined the experiment results with a leave-one-outstrategy, that is for each classifier in offline andonline modes, we leave one of the gesture, H-Oand location features out from the full experimentfeature set (TX+PT+H-O+LO+UT+DG+DH).
Nosignificant difference was discovered.When the dialogue game features (DG) areadded to the models, performance increases sig-nificantly for CRF offline model (p<.005), Max-Ent offline (p<.005) and online (p<.05) mod-els, NB online model (p<.05) and DT onlinemodel (p<.005).
It confirms previous findings, in-cluding by our group (Di Eugenio et al 2010a),that dialogue game features (DG) play a very im-portant role in DA classification, even via the sim-ple approximation we used.
When the dialoguehistory features (DH) are added to the models,performance increased significantly for all the of-fline models and the MaxEnt online model, withp<.005.
This confirms previous findings that dia-logue history helps with DA classification.6 Conclusions and Future WorkIn this paper we described our multimodal cor-pus which is annotated with multimodal informa-tion (pointing gestures and H-O actions) and dia-logue acts.
Our corpus analysis shows that peo-ple actively use pointing gestures and H-O actionsalongside utterances in dialogues.
The function ofH-O actions in dialogue had hardly been studiedbefore.
Our experiments show that MaxEnt per-forms best for the online DA classification task.Multimodal and dialogue game features both im-prove DA classification.Short-term future work includes manual anno-tation for dialogue games, in the hope that moreaccurate dialogue game features may further im-prove DA classification.
Longer term future workincludes prediction of the specific next move ?
thespecific DA and/or the specific gesture, pointing orH-O action.
We have now developed some of thebuilding blocks of an information-state based mul-timodal dialogue manager.
The major aspects westill need to address are defining the information-state for the Find task, and developing rules to up-date the information-state with multimodal infor-mation, the classified DAs, and the co-referenceresolution models we already built (Chen et al2011; Chen and Di Eugenio, 2012).
Once theinformation-state component is in place, we canexpect better and more detailed predictions.AcknowledgmentsThis work is supported by award IIS 0905593from the National Science Foundation.
Thanksto the other members of the RoboHelper project,for their many contributions, especially to the datacollection effort.190ReferencesAnne H. Anderson, Miles Bader, Ellen Gurman Bard,Elizabeth Boyle, Gwyneth Doherty, Simon Gar-rod, Stephen Isard, Jacqueline Kowtko, Jan McAl-lister, Jim Miller, Catherine Sotillo, and Henry S.1991.
The HCRC Map Task corpus.
Language andSpeech, 34(4):351.Srinivas Bangalore, Giuseppe Di Fabbrizio, andAmanda Stent.
2008.
Learning the structure oftask-driven human?human dialogs.
IEEE Transac-tions on Audio, Speech, and Language Processing,16(7):1249?1259.K.E.
Boyer, J.F.
Grafsgaard, E.Y.
Ha, R. Phillips, andJ.C.
Lester.
2011.
An affect-enriched dialogue actclassification model for task-oriented dialogue.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 1190?1199.Association for Computational Linguistics.Jean Carletta.
2007.
Unleashing the killer corpus:experiences in creating the multi-everything AMImeeting corpus.
Language Resources and Evalua-tion, 41(2):181?190.Lauri Carlson.
1983.
Dialogue games: An approach todiscourse analysis.
D. Reidel Publishing Company.Lin Chen and Barbara Di Eugenio.
2012.
Co-referencevia pointing and haptics in multi-modal dialogues.In The 2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies.
The As-sociation for Computational Linguistics.Lin Chen, Anruo Wang, and Barbara Di Eugenio.2011.
Improving pronominal and deictic co-reference resolution with multi-modal features.
InProceedings of SIGdial 2011, the 12th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, pages 307?311, Portland, Oregon, June.Association for Computational Linguistics.J.
Cohen.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and psychological measure-ment, 20(1):37?46.Marie-Catherine De Marneffe and Christopher D.Manning.
2008.
The Stanford typed dependenciesrepresentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.
Association for Com-putational Linguistics.Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin.2010a.
Dialogue act classification, higher order di-alogue structure, and instance-based learning.
Dia-logue & Discourse, 1(2):1?24.Barbara Di Eugenio, Milos?
Z?efran, Jezekiel Ben-Arie, Mark Foreman, Lin Chen, Simone Franzini,Shankaranand Jagadeesan, Maria Javaid, and KaiMa.
2010b.
Towards Effective Communicationwith Robotic Assistants for the Elderly: IntegratingSpeech, Vision and Haptics.
In Dialog with Robots,AAAI 2010 Fall Symposium, Arlington, VA, USA,November.M.E.
Foster, E.G.
Bard, M. Guhe, R.L.
Hill, J. Ober-lander, and A. Knoll.
2008.
The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue.
In Proceedings of the3rd ACM/IEEE International Conference on HumanRobot Interaction, pages 295?302.
ACM.Simone Franzini and Jezekiel Ben-Arie.
2012.
Speechrecognition by indexing and sequencing.
Interna-tional Journal of Computer Information Systems andIndustrial Management Applications, 4:358?365.David Graff, Alexandra Canavan, and George Zip-perlen.
1998.
Switchboard-2 Phase I.Eun Young Ha, Joseph F. Grafsgaard, ChristopherMitchell, Kristy Elizabeth Boyer, and James C.Lester.
2012.
Combining verbal and nonverbalfeatures to overcome the ?information gap?
in task-oriented dialogue.
In Proceedings of SIGdial 2012,the 13th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, pages 247?256,Seoul, South Korea, July.
Association for Computa-tional Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An up-date.
SIGKDD Explorations, 11(1).Helen Wright Hastie, Massimo Poesio, and Stephen Is-ard.
2002.
Automatically predicting dialogue struc-ture using prosodic features.
Speech Communica-tion, 36(1?2):63?79.Edward Ivanovic.
2008.
Automatic instant messagingdialogue using statistical models and dialogue acts.Master?s thesis, University of Melbourne.Maria Javaid and Milos?
Z?efran.
2012.
Interpretingcommunication through physical interaction duringcollaborative manipulation.
Draft, October.Su Nam Kim, Lawrence Cavedon, and Timothy Bald-win.
2010.
Classifying dialogue acts in one-on-onelive chats.
In Proceedings of EMNLP 2010, the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 862?871.
Association for Com-putational Linguistics.Su Nam Kim, Lawrence Cavedon, and Timothy Bald-win.
2012.
Classifying dialogue acts in multi-partylive chats.
In Proceedings of the 26th Pacific AsiaConference on Language, Information, and Compu-tation, pages 463?472, Bali, Indonesia, November.Faculty of Computer Science, Universitas Indone-sia.Michael Kipp.
2001.
Anvil-a generic annotation toolfor multimodal dialogue.
In Proceedings of the7th European Conference on Speech Communica-tion and Technology, pages 1367?1370.191Kristine M. Krapp.
2002.
The Gale Encyclopedia ofNursing & Allied Health.
Gale Group, Inc. ChapterActivities of Daily Living Evaluation.John D. Lafferty, Andrew McCallum, and Fer-nando C.N.
Pereira.
2001.
Conditional RandomFields: Probabilistic Models for Segmenting and La-beling Sequence Data.
In Proceedings of the Eigh-teenth International Conference on Machine Learn-ing, pages 282?289.
Morgan Kaufmann PublishersInc.L.
Levin, A. Thyme?-Gobbel, A. Lavie, K. Ries, andK.
Zechner.
1998.
A discourse coding scheme forconversational Spanish.
In Fifth International Con-ference on Spoken Language Processing.K.
Ma and J. Ben-Arie.
2012.
Multi-view multi-class object detection via exemplar compounding.In IEEE-IAPR 21st International Conference onPattern Recognition (ICPR 2012), Tsukuba, Japan,November.Andrew Kachites McCallum.
2002.
MALLET: A Ma-chine Learning for Language Toolkit.E.
Shriberg, R. Dhillon, S.V.
Bhagat, J. Ang, andH.
Carvey.
2004.
The ICSI Meeting RecorderDialog Act (MRDA) Corpus.
In Proceedings of5th SIGdial Workshop on Discourse and Dialogue,pages 97?100, Cambridge, MA, April 30-May 1.V.K.R.
Sridhar, S. Bangalore, and S. Narayanan.
2009.Combining lexical, syntactic and prosodic cues forimproved online dialog act tagging.
ComputerSpeech & Language, 23(4):407?422.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,and M. Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational linguistics, 26(3):339?373.Matthias Zimmermann, Andreas Stolcke, and Eliza-beth Shriberg.
2006.
Joint segmentation and clas-sification of dialog acts in multiparty meetings.
InICASSP 2006, the IEEE International Conferenceon Acoustics, Speech and Signal Processing, vol-ume 1.
IEEE.192
