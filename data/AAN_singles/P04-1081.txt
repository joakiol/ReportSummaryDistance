A Kernel PCA Method for Superior Word Sense DisambiguationDekai WU1 Weifeng SU Marine CARPUATdekai@cs.ust.hk weifeng@cs.ust.hk marine@cs.ust.hkHuman Language Technology CenterHKUSTDepartment of Computer ScienceUniversity of Science and TechnologyClear Water Bay, Hong KongAbstractWe introduce a new method for disambiguatingword senses that exploits a nonlinear Kernel Prin-cipal Component Analysis (KPCA) technique toachieve accuracy superior to the best published indi-vidual models.
We present empirical results demon-strating significantly better accuracy compared tothe state-of-the-art achieved by either na?
?ve Bayesor maximum entropy models, on Senseval-2 data.We also contrast against another type of kernelmethod, the support vector machine (SVM) model,and show that our KPCA-based model outperformsthe SVM-based model.
It is hoped that these highlyencouraging first results on KPCA for natural lan-guage processing tasks will inspire further develop-ment of these directions.1 IntroductionAchieving higher precision in supervised wordsense disambiguation (WSD) tasks without resort-ing to ad hoc voting or similar ensemble techniqueshas become somewhat daunting in recent years,given the challenging benchmarks set by na?
?veBayes models (e.g., Mooney (1996), Chodorow etal.
(1999), Pedersen (2001), Yarowsky and Flo-rian (2002)) as well as maximum entropy models(e.g., Dang and Palmer (2002), Klein and Man-ning (2002)).
A good foundation for comparativestudies has been established by the Senseval dataand evaluations; of particular relevance here arethe lexical sample tasks from Senseval-1 (Kilgarriffand Rosenzweig, 1999) and Senseval-2 (Kilgarriff,2001).We therefore chose this problem to introducean efficient and accurate new word sense disam-biguation approach that exploits a nonlinear KernelPCA technique to make predictions implicitly basedon generalizations over feature combinations.
The1The author would like to thank the Hong Kong Re-search Grants Council (RGC) for supporting this researchin part through grants RGC6083/99E, RGC6256/00E, andDAG03/04.EG09.technique is applicable whenever vector represen-tations of a disambiguation task can be generated;thus many properties of our technique can be ex-pected to be highly attractive from the standpoint ofnatural language processing in general.In the following sections, we first analyze the po-tential of nonlinear principal components with re-spect to the task of disambiguating word senses.Based on this, we describe a full model for WSDbuilt on KPCA.
We then discuss experimental re-sults confirming that this model outperforms state-of-the-art published models for Senseval-relatedlexical sample tasks as represented by (1) na?
?veBayes models, as well as (2) maximum entropymodels.
We then consider whether other kernelmethods?in particular, the popular SVM model?are equally competitive, and discover experimen-tally that KPCA achieves higher accuracy than theSVM model.2 Nonlinear principal components andWSDThe Kernel Principal Component Analysis tech-nique, or KPCA, is a nonlinear kernel methodfor extraction of nonlinear principal componentsfrom vector sets in which, conceptually, the n-dimensional input vectors are nonlinearly mappedfrom their original space Rn to a high-dimensionalfeature space F where linear PCA is performed,yielding a transform by which the input vectorscan be mapped nonlinearly to a new set of vectors(Scho?lkopf et al, 1998).A major advantage of KPCA is that, unlike othercommon analysis techniques, as with other kernelmethods it inherently takes combinations of pre-dictive features into account when optimizing di-mensionality reduction.
For natural language prob-lems in general, of course, it is widely recognizedthat significant accuracy gains can often be achievedby generalizing over relevant feature combinations(e.g., Kudo and Matsumoto (2003)).
Another ad-vantage of KPCA for the WSD task is that thedimensionality of the input data is generally veryTable 1: Two of the Senseval-2 sense classes for the target word ?art?, from WordNet 1.7 (Fellbaum 1998).Class Sense1 the creation of beautiful or significant things2 a superior skilllarge, a condition where kernel methods excel.Nonlinear principal components (Diamantarasand Kung, 1996) may be defined as follows.
Sup-pose we are given a training set of M pairs (xt, ct)where the observed vectors xt ?
Rn in an n-dimensional input space X represent the context ofthe target word being disambiguated, and the cor-rect class ct represents the sense of the word, fort = 1, ..,M .
Suppose ?
is a nonlinear mappingfrom the input space Rn to the feature space F .Without loss of generality we assume the M vec-tors are centered vectors in the feature space, i.e.,?Mt=1 ?
(xt) = 0; uncentered vectors can easilybe converted to centered vectors (Scho?lkopf et al,1998).
We wish to diagonalize the covariance ma-trix in F :C = 1MM?j=1?
(xj) ?T (xj) (1)To do this requires solving the equation ?v = Cvfor eigenvalues ?
?
0 and eigenvectors v ?
F .
Be-causeCv = 1MM?j=1(?
(xj) ?
v)?
(xj) (2)we can derive the following two useful results.
First,?
(?
(xt) ?
v) = ?
(xt) ?
Cv (3)for t = 1, ..,M .
Second, there exist ?i for i =1, ...,M such thatv =M?i=1?i?
(xi) (4)Combining (1), (3), and (4), we obtainM?M?i=1?i (?
(xt) ?
?
(xi ))=M?i=1?i(?
(xt) ?M?j=1?
(xj)) (?
(xj) ?
?
(xi ))for t = 1, ..,M .
Let K?
be the M ?
M matrix suchthatK?ij = ?
(xi) ?
?
(xj) (5)and let ?
?1 ?
?
?2 ?
.
.
.
?
?
?M denote the eigenval-ues of K?
and ?
?1 ,..., ?
?M denote the correspondingcomplete set of normalized eigenvectors, such that??t(?
?t ?
?
?t) = 1 when ?
?t > 0.
Then the lth nonlinearprincipal component of any test vector xt is definedasylt =M?i=1?
?li (?
(xi) ?
?
(xt )) (6)where ?
?li is the lth element of ?
?l .To illustrate the potential of nonlinear principalcomponents for WSD, consider a simplified disam-biguation example for the ambiguous target word?art?, with the two senses shown in Table 1.
Assumea training corpus of the eight sentences as shownin Table 2, adapted from Senseval-2 English lexicalsample corpus.
For each sentence, we show the fea-ture set associated with that occurrence of ?art?
andthe correct sense class.
These eight occurrences of?art?
can be transformed to a binary vector represen-tation containing one dimension for each feature, asshown in Table 3.Extracting nonlinear principal components forthe vectors in this simple corpus results in nonlineargeneralization, reflecting an implicit considerationof combinations of features.
Table 3 shows the firstthree dimensions of the principal component vectorsobtained by transforming each of the eight trainingvectors xt into (a) principal component vectors ztusing the linear transform obtained via PCA, and(b) nonlinear principal component vectors yt usingthe nonlinear transform obtained via KPCA as de-scribed below.Similarly, for the test vector x9, Table 4 shows thefirst three dimensions of the principal componentvectors obtained by transforming it into (a) a princi-pal component vector z9 using the linear PCA trans-form obtained from training, and (b) a nonlinearprincipal component vector y9 using the nonlinearKPCA transform obtained obtained from training.The vector similarities in the KPCA-transformedspace can be quite different from those in the PCA-transformed space.
This causes the KPCA-basedmodel to be able to make the correct class pre-diction, whereas the PCA-based model makes theTable 2: A tiny corpus for the target word ?art?, adapted from the Senseval-2 English lexical sample corpus(Kilgarriff 2001), together with a tiny example set of features.
The training and testing examples can berepresented as a set of binary vectors: each row shows the correct class c for an observed vector x of fivedimensions.TRAINING design/N media/N the/DT entertainment/N world/N Classx1 He studies art in London.
1x2 Punch?s weekly guide tothe world of the arts,entertainment, media andmore.1 1 1 1x3 All such studies have in-fluenced every form of art,design, and entertainmentin some way.1 1 1x4 Among the techni-cal arts cultivated insome continental schoolsthat began to affectEngland soon after theNorman Conquest werethose of measurementand calculation.1 2x5 The Art of Love.
1 2x6 Indeed, the art of doc-toring does contribute tobetter health results anddiscourages unwarrantedmalpractice litigation.1 2x7 Countless books andclasses teach the art ofasserting oneself.1 2x8 Pop art is an example.
1TESTINGx9 In the world of de-sign arts particularly, thisled to appointments madefor political rather thanacademic reasons.1 1 1 1wrong class prediction.What permits KPCA to apply stronger general-ization biases is its implicit consideration of com-binations of feature information in the data dis-tribution from the high-dimensional training vec-tors.
In this simplified illustrative example, thereare just five input dimensions; the effect is strongerin more realistic high dimensional vector spaces.Since the KPCA transform is computed from unsu-pervised training vector data, and extracts general-izations that are subsequently utilized during super-vised classification, it is quite possible to combinelarge amounts of unsupervised data with reasonablesmaller amounts of supervised data.It can be instructive to attempt to interpret thisexample graphically, as follows, even though theinterpretation in three dimensions is severely limit-ing.
Figure 1(a) depicts the eight original observedtraining vectors xt in the first three of the five di-mensions; note that among these eight vectors, therehappen to be only four unique points when restrict-ing our view to these three dimensions.
Ordinarylinear PCA can be straightforwardly seen as pro-jecting the original points onto the principal axis,Table 3: The original observed training vectors (showing only the first three dimensions) and their first threeprincipal components as transformed via PCA and KPCA.Observed vectors PCA-transformed vectors KPCA-transformed vectors Classt (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) ct1 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 12 (0, 1, 1) (1.675, -1.132, 0.1049) (1.149, 0.02934, 0.322) 13 (1, 0, 0) (-0.367, 1.697, -0.2391) (0.8209, 0.7722, -0.2015) 14 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 25 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 26 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 27 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 28 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1Table 4: Testing vector (showing only the first three dimensions) and its first three principal componentsas transformed via the trained PCA and KPCA parameters.
The PCA-based and KPCA-based sense classpredictions disagree.ObservedvectorsPCA-transformed vectors KPCA-transformed vec-torsPredictedClassCorrectClasst (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) c?t ct9 (1, 0, 1) (-0.3671, -0.5658, -0.2392) 2 19 (1, 0, 1) (4e-06, 8e-07, 1.111e-18) 1 1as can be seen for the case of the first principal axisin Figure 1(b).
Note that in this space, the sense 2instances are surrounded by sense 1 instances.
Wecan traverse each of the projections onto the prin-cipal axis in linear order, simply by visiting each ofthe first principal components z1t along the principleaxis in order of their values, i.e., such thatz11 ?
z18 ?
z14 ?
z15 ?
z16 ?
z17 ?
z12 ?
z13 ?
z19It is significantly more difficult to visualizethe nonlinear principal components case, however.Note that in general, there may not exist any prin-cipal axis in X , since an inverse mapping from Fmay not exist.
If we attempt to follow the same pro-cedure to traverse each of the projections onto thefirst principal axis as in the case of linear PCA, byconsidering each of the first principal componentsy1t in order of their value, i.e., such thaty14 ?
y15 ?
y16 ?
y17 ?
y19 ?
y11 ?
y18 ?
y13 ?
y12then we must arbitrarily select a ?quasi-projection?direction for each y1t since there is no actual prin-cipal axis toward which to project.
This results in a?quasi-axis?
roughly as shown in Figure 1(c) which,though not precisely accurate, provides some ideaas to how the nonlinear generalization capability al-lows the data points to be grouped by principal com-ponents reflecting nonlinear patterns in the data dis-tribution, in ways that linear PCA cannot do.
Notethat in this space, the sense 1 instances are alreadybetter separated from sense 2 data points.
More-over, unlike linear PCA, there may be up to M ofthe ?quasi-axes?, which may number far more thanfive.
Such effects can become pronounced in thehigh dimensional spaces are actually used for realword sense disambiguation tasks.3 A KPCA-based WSD modelTo extract nonlinear principal components effi-ciently, note that in both Equations (5) and (6) theexplicit form of ?
(xi) is required only in the formof (?
(xi) ??
(xj)), i.e., the dot product of vectors inF .
This means that we can calculate the nonlinearprincipal components by substituting a kernel func-tion k(xi, xj) for (?
(xi) ?
?
(xj )) in Equations (5)and (6) without knowing the mapping ?
explicitly;instead, the mapping ?
is implicitly defined by thekernel function.
It is always possible to constructa mapping into a space where k acts as a dot prod-uct so long as k is a continuous kernel of a positiveintegral operator (Scho?lkopf et al, 1998).the/DT4, 5, 6, 71, 832design/Nmedia/N(a)9the/DT4, 5, 6, 71, 832design/Nmedia/N(b)9the/DT4, 5, 6, 71, 832design/Nmedia/N(c)9first principalaxis: training example with sense class 1: training example with sense class 2: test example with unknown sense class: test example with predicted sensefirst principal?quasi-axis?class 2 (correct sense class=1): test example with predicted senseclass 1 (correct sense class=1)Figure 1: Original vectors, PCA projections, andKPCA ?quasi-projections?
(see text).Table 5: Experimental results showing that theKPCA-based model performs significantly betterthan na?
?ve Bayes and maximum entropy models.Significance intervals are computed via bootstrapresampling.WSD Model Accuracy Sig.
Int.na?
?ve Bayes 63.3% +/-0.91%maximum entropy 63.8% +/-0.79%KPCA-based model 65.8% +/-0.79%Thus we train the KPCA model using the follow-ing algorithm:1.
Compute an M ?
M matrix K?
such thatK?ij = k(xi, xj) (7)2.
Compute the eigenvalues and eigenvectors ofmatrix K?
and normalize the eigenvectors.
Let?
?1 ?
?
?2 ?
.
.
.
?
?
?M denote the eigenvaluesand ?
?1,..., ?
?M denote the corresponding com-plete set of normalized eigenvectors.To obtain the sense predictions for test instances,we need only transform the corresponding vectorsusing the trained KPCA model and classify the re-sultant vectors using nearest neighbors.
For a giventest instance vector x, its lth nonlinear principalcomponent isylt =M?i=1?
?lik(xi, xt) (8)where ?
?li is the ith element of ?
?l.For our disambiguation experiments we employ apolynomial kernel function of the form k(xi, xj) =(xi ?
xj)d, although other kernel functions such asgaussians could be used as well.
Note that the de-generate case of d = 1 yields the dot product kernelk(xi, xj) = (xi?xj) which covers linear PCA as aspecial case, which may explain why KPCA alwaysoutperforms PCA.4 Experiments4.1 KPCA versus na?
?ve Bayes and maximumentropy modelsWe established two baseline models to representthe state-of-the-art for individual WSD models: (1)na?
?ve Bayes, and (2) maximum entropy models.The na?
?ve Bayes model was found to be the mostaccurate classifier in a comparative study using asubset of Senseval-2 English lexical sample databy Yarowsky and Florian (2002).
However, themaximum entropy (Jaynes, 1978) was found toyield higher accuracy than na?
?ve Bayes in a sub-sequent comparison by Klein and Manning (2002),who used a different subset of either Senseval-1 orSenseval-2 English lexical sample data.
To controlfor data variation, we built and tuned models of bothkinds.
Note that our objective in these experimentsis to understand the performance and characteristicsof KPCA relative to other individual methods.
Itis not our objective here to compare against votingor other ensemble methods which, though known tobe useful in practice (e.g., Yarowsky et al (2001)),would not add to our understanding.To compare as evenly as possible, we em-ployed features approximating those of the ?feature-enhanced na?
?ve Bayes model?
of Yarowsky and Flo-rian (2002), which included position-sensitive, syn-tactic, and local collocational features.
The mod-els in the comparative study by Klein and Man-ning (2002) did not include such features, and so,again for consistency of comparison, we experi-mentally verified that our maximum entropy model(a) consistently yielded higher scores than whenthe features were not used, and (b) consistentlyyielded higher scores than na?
?ve Bayes using thesame features, in agreement with Klein and Man-ning (2002).
We also verified the maximum en-tropy results against several different implementa-tions, using various smoothing criteria, to ensurethat the comparison was even.Evaluation was done on the Senseval 2 Englishlexical sample task.
It includes 73 target words,among which nouns, adjectives, adverbs and verbs.For each word, training and test instances taggedwith WordNet senses are provided.
There are an av-erage of 7.8 senses per target word type.
On average109 training instances per target word are available.Note that we used the set of sense classes from Sen-seval?s ?fine-grained?
rather than ?coarse-grained?classification task.The KPCA-based model achieves the highest ac-curacy, as shown in Table 5, followed by the max-imum entropy model, with na?
?ve Bayes doing thepoorest.
Bear in mind that all of these models aresignificantly more accurate than any of the other re-ported models on Senseval.
?Accuracy?
here refersto both precision and recall since disambiguation ofall target words in the test set is attempted.
Resultsare statistically significant at the 0.10 level, usingbootstrap resampling (Efron and Tibshirani, 1993);moreover, we consistently witnessed the same levelof accuracy gains from the KPCA-based model overTable 6: Experimental results comparing theKPCA-based model versus the SVM model.WSD Model Accuracy Sig.
Int.SVM-based model 65.2% +/-1.00%KPCA-based model 65.8% +/-0.79%many variations of the experiments.4.2 KPCA versus SVM modelsSupport vector machines (e.g., Vapnik (1995),Joachims (1998)) are a different kind of ker-nel method that, unlike KPCA methods, have al-ready gained high popularity for NLP applications(e.g., Takamura and Matsumoto (2001), Isozaki andKazawa (2002), Mayfield et al (2003)) includingthe word sense disambiguation task (e.g., Cabezaset al (2001)).
Given that SVM and KPCA are bothkernel methods, we are frequently asked whetherSVM-based WSD could achieve similar results.To explore this question, we trained and tunedan SVM model, providing the same rich set of fea-tures and also varying the feature representations tooptimize for SVM biases.
As shown in Table 6,the highest-achieving SVM model is also able toobtain higher accuracies than the na?
?ve Bayes andmaximum entropy models.
However, in all our ex-periments the KPCA-based model consistently out-performs the SVM model (though the margin fallswithin the statistical significance interval as com-puted by bootstrap resampling for this single exper-iment).
The difference in KPCA and SVM perfor-mance is not surprising given that, aside from theuse of kernels, the two models share little structuralresemblance.4.3 Running timesTraining and testing times for the various model im-plementations are given in Table 7, as reported bythe Unix time command.
Implementations of allmodels are in C++, but the level of optimization isnot controlled.
For example, no attempt was madeto reduce the training time for na?
?ve Bayes, or to re-duce the testing time for the KPCA-based model.Nevertheless, we can note that in the operatingrange of the Senseval lexical sample task, the run-ning times of the KPCA-based model are roughlywithin the same order of magnitude as for na?
?veBayes or maximum entropy.
On the other hand,training is much faster than the alternative kernelmethod based on SVMs.
However, the KPCA-based model?s times could be expected to sufferin situations where significantly larger amounts ofTable 7: Comparison of training and testing times for the different WSD model implementations.WSD Model Training time [CPU sec] Testing time [CPU sec]na?
?ve Bayes 103.41 16.84maximum entropy 104.62 59.02SVM-based model 5024.34 16.21KPCA-based model 216.50 128.51training data are available.5 ConclusionThis work represents, to the best of our knowl-edge, the first application of Kernel PCA to atrue natural language processing task.
We haveshown that a KPCA-based model can significantlyoutperform state-of-the-art results from both na?
?veBayes as well as maximum entropy models, forsupervised word sense disambiguation.
The factthat our KPCA-based model outperforms the SVM-based model indicates that kernel methods otherthan SVMs deserve more attention.
Given the theo-retical advantages of KPCA, it is our hope that thiswork will encourage broader recognition, and fur-ther exploration, of the potential of KPCA modelingwithin NLP research.Given the positive results, we plan next to com-bine large amounts of unsupervised data with rea-sonable smaller amounts of supervised data such asthe Senseval lexical sample.
Earlier we mentionedthat one of the promising advantages of KPCA isthat it computes the transform purely from unsuper-vised training vector data.
We can thus make use ofthe vast amounts of cheap unannotated data to aug-ment the model presented in this paper.ReferencesClara Cabezas, Philip Resnik, and Jessica Stevens.Supervised sense tagging using support vectormachines.
In Proceedings of Senseval-2, Sec-ond International Workshop on Evaluating WordSense Disambiguation Systems, pages 59?62,Toulouse, France, July 2001.
SIGLEX, Associ-ation for Computational Linguistics.Martin Chodorow, Claudia Leacock, and George A.Miller.
A topical/local classifier for word senseidentification.
Computers and the Humanities,34(1-2):115?120, 1999.
Special issue on SEN-SEVAL.Hoa Trang Dang and Martha Palmer.
Combiningcontextual features for word sense disambigua-tion.
In Proceedings of the SIGLEX/SENSEVALWorkshop on Word Sense Disambiguation: Re-cent Successes and Future Directions, pages 88?94, Philadelphia, July 2002.
SIGLEX, Associa-tion for Computational Linguistics.Konstantinos I. Diamantaras and Sun Yuan Kung.Principal Component Neural Networks.
Wiley,New York, 1996.Bradley Efron and Robert J. Tibshirani.
An Intro-duction to the Bootstrap.
Chapman and Hall,1993.Hideki Isozaki and Hideto Kazawa.
Efficient sup-port vector classifiers for named entity recogni-tion.
In Proceedings of COLING-2002, pages390?396, Taipei, 2002.E.T.
Jaynes.
Where do we Stand on Maximum En-tropy?
MIT Press, Cambridge MA, 1978.Thorsten Joachims.
Text categorization with sup-port vector machines: Learning with many rel-evant features.
In Proceedings of ECML-98,10th European Conference on Machine Learning,pages 137?142, 1998.Adam Kilgarriff and Joseph Rosenzweig.
Frame-work and results for English Senseval.
Comput-ers and the Humanities, 34(1):15?48, 1999.
Spe-cial issue on SENSEVAL.Adam Kilgarriff.
English lexical sample task de-scription.
In Proceedings of Senseval-2, Sec-ond International Workshop on Evaluating WordSense Disambiguation Systems, pages 17?20,Toulouse, France, July 2001.
SIGLEX, Associ-ation for Computational Linguistics.Dan Klein and Christopher D. Manning.
Con-ditional structure versus conditional estimationin NLP models.
In Proceedings of EMNLP-2002, Conference on Empirical Methods in Nat-ural Language Processing, pages 9?16, Philadel-phia, July 2002.
SIGDAT, Association for Com-putational Linguistics.Taku Kudo and Yuji Matsumoto.
Fast methodsfor kernel-based text analysis.
In Proceedings ofthe 41set Annual Meeting of the Asoociation forComputational Linguistics, pages 24?31, 2003.James Mayfield, Paul McNamee, and Christine Pi-atko.
Named entity recognition using hundreds ofthousands of features.
In Walter Daelemans andMiles Osborne, editors, Proceedings of CoNLL-2003, pages 184?187, Edmonton, Canada, 2003.Raymond J. Mooney.
Comparative experiments ondisambiguating word senses: An illustration ofthe role of bias in machine learning.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing, Philadelphia, May1996.
SIGDAT, Association for ComputationalLinguistics.Ted Pedersen.
Machine learning with lexical fea-tures: The Duluth approach to SENSEVAL-2.In Proceedings of Senseval-2, Second Interna-tional Workshop on Evaluating Word Sense Dis-ambiguation Systems, pages 139?142, Toulouse,France, July 2001.
SIGLEX, Association forComputational Linguistics.Bernhard Scho?lkopf, Alexander Smola, and Klaus-Rober Mu?ller.
Nonlinear component analysis as akernel eigenvalue problem.
Neural Computation,10(5), 1998.Hiroya Takamura and Yuji Matsumoto.
Featurespace restructuring for SVMs with application totext categorization.
In Proceedings of EMNLP-2001, Conference on Empirical Methods in Nat-ural Language Processing, pages 51?57, 2001.Vladimir N. Vapnik.
The Nature of StatisticalLearning Theory.
Springer-Verlag, New York,1995.David Yarowsky and Radu Florian.
Evaluat-ing sense disambiguation across diverse param-eter spaces.
Natural Language Engineering,8(4):293?310, 2002.David Yarowsky, Silviu Cucerzan, Radu Florian,Charles Schafer, and Richard Wicentowski.
TheJohns Hopkins SENSEVAL2 system descrip-tions.
In Proceedings of Senseval-2, Sec-ond International Workshop on Evaluating WordSense Disambiguation Systems, pages 163?166,Toulouse, France, July 2001.
SIGLEX, Associa-tion for Computational Linguistics.
