The Automatic Acquisition of Frequencies of VerbSubcategorization Frames from Tagged CorporaAkira Ushioda, David A. Evans, Ted Gibson, Alex WaibelComputational Linguistics ProgramCarnegie Mellon UniversityPittsburgh, PA 15P13-3890aushioda @icl.
cmu.
eduAbst rac tWe describe a mechanism for automatically acquiring verb subcategorization framesand their frequencies in a large corpus.
A tagged corpus is first partially parsed toidentify noun phrases and then a finear grammar is used to estimate the appropri-ate subcategorization frame for each verb token in the corpus.
In an experimentinvolving the identification of six fixed subcategorization frames, our current systemshowed more than 80% accuracy.
In addition, a new statistical approach substan-tially improves the accuracy of the frequency estimation.1 IntroductionWhen we construct a grammar, there is always a trade-off between the coverage of thegrammar and the ambiguity of the grammar.
If we hope to develop an efficient high-coverage parser for unrestricted texts, we must have some means of dealing with thecombinatorial explosion of syntactic ambiguities.
While a general probabilistic optimiza-tion technique such as the Inside-Outside algorithm (\[Baker, 1979\], \[Lauri and Young,1990\], \[Jelinek el ai., 1990\], \[Carroll and Charniak, 1992\]) can be used to reduce ambi-guity by providing estimates on the applicability of the context-free rules in a grammar(for example), the algorithm does not take advantage of lexical information, includingsuch information as verb subcategorization frame preferences.
Discovering or acquiringlexically-sensitive linguistic structures from large corpora may offer an essential comple-mentary approach.Verb subcategorization (verb-subcat) frames represent one of the most important ele-ments of grammatical/lexical knowledge for efficient and reliable parsing.
At this stage inthe computational-linguistic exploration of corpora, dictionaries are still probably more re-liable than automatic acquisition systems as a source of subcategorization (subcat) framesfor verbs.
The Oxford Advanced Learners Dictionary (OALD) \[Hornby, 1989\], for exam-ple, uses 32 verb patterns to describe a usage of each verb for each meaning of the verb.However, dictionaries do not provide quantitative information such as how often eachverb is used with each of the possible subcat frames.
Since dictionaries are repositories,primarily, of what is possible, not what is most likely, they tend to contain informationabout rare usage \[de Marken, 1992\].
But without information about the frequencies of thesubcat frames we find in dictionaries, we face the prospect of having to treat each frameas equiprobable in parsing.
This can lead to serious inefficiency.
We also know that thefrequency of subcat frames can vary by domain; frames that are very rare in one domaincan be quite common in another.
If we could automatically determine the frequenciesof subcat frames for domains, we would be able to tailor parsing with domain-specific95heuristics.
Indeed, it would be desirable to have a subcat dictionary for each possibledomain.This paper describes a mechanism for automatically acquiring subcat frames and theirfrequencies based on a tagged corpus.
The method utilizes a tagged corpus because (i)we don't have to deal with a lexical ambiguity (ii) tagged corpora in various domainsare becoming readily available and (iii) simple and robust tagging techniques using suchcorpora recently have been developed (\[Church, 1988\], \[Brill, 1992\]).Brent reports a method for automatically acquiring subcat frames but without fre-quency measurements (\[Brent and Berwick, 1991\], \[Brent, 1991\]).
His approach is tocount occurrences of those unambiguous verb phrases that contain no noun phrases otherthan pronouns or proper nouns.
By thus restricting the "features" that trigger identifi-cation of a verb phrase, he avoids possible errors due to syntactic ambiguity.
Althoughthe rate of false positives is very low in his system, his syntactic features are so selectivethat most verb tokens fail to satisfy them.
(For example, verbs that occurred fewer than20 times in the corpus tend to have no co-occurrences with the features.)
Therefore hisapproach is not useful in determining verb-subcat frame frequencies.To measure frequencies, we need, ideally, to identify a subcat frame for each verb tokenin the corpus.
This, in turn, requires a full parse of the corpus.
Since manually parsedcorpora are rare and typically small, and since automatically parsed corpora containmany errors (given current parsing technologies), an alternative source of useful linguisticstructure is needed.
We have elected to use partially parsed sentences automaticallyderived from a lexically-tagged corpus.
The partial parse contains information aboutminimal noun phrases (without PP attachment or clausal complements).
While suchderived information about linguistic structure is less accurate and complete than thatavailable in certified, hand-parsed corpora, the approach promises to generalize and toyield large sample sizes.
In particular, we can use partially parsed corpora to measureverb-subcat frame frequencies.2 MethodThe procedure to find verh-subcat frequencies, automatically, is as follows.
(1) Make a list of verbs out of the tagged corpus.
(2) For each verb on the list (the "target verb"),(2.1) Tokenize each sentence containing the target verb in the following way:All the noun phrases except pronouns are tokenized as "n" by a noun phraseparser and all the rest of the words are also tokenized following the schmemain Table 1.
For example, the sentence "The corresponding mental-state verbsdo not follow \[target verb\] these rules in a straightforward way" is transformedto a sequence of tokens "bnvaknpne'.
(2.2) Apply a set of subcat extraction rules to the tokenized sentences.
These rulesare written as regular expressions and they are obtained through the examina-tion of occurrences of a small sample of verbs in a training text.Note that in the actual implementation f the procedure, all of the redundant oper-ations are eliminated.
Our NP parser also uses a finite-state grammar.
It is designed96b: sentence initial makerk: target verbi: p ronounn: noun phrasev: finite verbu: participial verbd: base form verbp: prepositione: sentence final makert: "to"m: moda lw: re lat ive  pronouna: adverbx: punctuat ionc: complementizer "that"s: the  restTable 1: List of Symbols/Categoriesespecially to support identification of verb-subcat frames.
One of its special features isthat it detects time-adjuncts such as "yesterday", "two months ago", or "the followingday", and eliminates them in the tokenization process.
For example, the sentence "He toldthe reporters the following day that..." is tokenized to "bivnc..." instead of "bivnnc...".3 Exper iment  on Wall Street Journal CorpusWe used the above method in experiments involving a tagged corpus of Wall Street Journal(WSJ) articles, provided by the Penn Treebank project.
Our experiment was limited intwo senses.
First, we treated all prepositional phrases as adjuncts.
(It is generally difficultto distinguish complement and adjunct PPs.)
Second, we measured the frequencies ofonly six fixed subcat frames for verbs in non-participle form.
(This does not representan essential shortcoming in the method; we only need to have additional subcat frameextraction rules to accommodate participles.
)We extracted two sets of tagged sentences from the WSJ corpus, each representing 3-MBytes and approximately 300,000 words of text.
One set was used as a training corpus,the other as a test corpus.
Table 2 gives the list of verb-subcat frame extraction rulesobtained (via examination) for four verbs "expect", "reflect", "tell", and "give", as theyoccurred in the training corpus.
Sample sentences that can be captured by each set ofrules are attached to the list.
Table 3 shows the result of the hand comparison of theautomat!cally identified verb-subcat frames for "give" and "expect" in the test corpus.The tabular columns give actual frequencies for each verb-subcat frame based on man-ual review and the tabular rowsgive the frequencies as determined automatically by thesystem.
The count of each cell (\[i, j\]) gives the number of occurrences of the verb thatare assigned the i-th subcat frame by the system and assigned the j-th frame by manualreview.
The frame/column labeled "REST" represents all other subcat frames, encom-passing such subcat frames as those involving wh-clauses, verb-particle combinations ( uchas "give up"), and no complements.Despite the simplicity of the rules, the frequencies for subcat frames determined underautomatic processing are very close to the real distributions.
Most of the errors areattributable to errors in the noun phrase parser.
For example, 10 out of the 13 errorsin the \[NP,NP+NP\] cell under "give" are due to noun phrase parsing errors such as themisidentification of a N-N sequence (e.g., *"give \[NP government officials rights\] againstthe press" vs. "give \[NP government officials\] \[NP rights\] against he press").97Notes:NP: noun phraseFrame1.
NP+NP2.
NP+CL3.
NPT INF4.
CL5.
NP6.
INFRulek ( i ln )nk ( i ln (pn)* )ck ( i ln ) ( i ln )a* (mlv )k ( i ln (pn)* ) ta*dk?k( i ln )a* (mlv )k ( i ln ) / \ [ 'mvd\ ]#pw( i ln (pn)* )a*m?a*k / \ [ ' t \ ]k ta*dCL: that-c lause with and without the complementizer "that"INF: "to" + infinitivex* matches a sequence of any number  of x's including zero xx?
is either x or empty(x ly)  matches either x or y\ [ ' xyz \ ]  matches any token except x, y, and zIx(sequence) matches (sequence) that  is not directly preceded by xx /y  matches x if x is immediately followed by ySample Sentences:Frame 1.Frame 2.Frame 3.~rame 4.Frame 5.Frame 6.
"...gives current management  enough t ime to work on...""...tel_.._l the people in the hall that..." ; "...tol__.d h im the man would...""...expected the impact from the restructur ing to make..."".
.
.thlnk that..
."
; ".
.
.thought the company eventually responded...""...sa__.E the man..
."
; "...which the president of the company wanted..."but not"...sa__~ him swim..."; "...(hotel) in which he stayed..."; "...(gift) which he expected to get...""...expects to gain..."Table 2: Set of Subcategorization Frame Extraction Rules98NP-t-NPNP+CLOutput NP+INFof  NPSystem CLINFRESTTota l"G ive"Rea l  Occur rencesNP+NP NP+CL NP+INF  NP CL INF REST52 0 0 0 0 0 0l 0 0 0 0 0 02 0 0 0 0 0 013 0 0 27 0 0 00 0 0 0 0 0 00 0 0 0 0 0 01 0 0 4 0 0 969 0 0 31 0 0 9Tota l5212400014109NP+NPNP+CLOutput  NP+INFo f  NPSystem CLINFRESTTota l"Expect"Rea l  Occur rencesNP+NP NP+CL NP+INF  NP CL INF REST0 0 0 0 0 0 00 0 0 0 0 0 00 0 55 1 0 0 00 0 4 28 0 0 00 0 0 0 8 0 00 0 0 0 0 40 00 0 1 6 0 0 7Tota l005632840140 0 60 35 8 40 7 150Tab le  3: Subcategor i za t ion  F rame Frequenciesacquire end likebuild expand needclose fail producecomment file proveconsider follow reachcontinue get receivedesign help reducedevelop hold seeelect let signspendtotaltryusewantworkTable 4: Verbs Tested99THIS  PAGE INTENTIONALLY  LEFT  BLANKI00Numbero fVerbs10- -6 - -  i3- -<SI l IV I5-10  10-15  15-20  20-25  25-30  30-35  34--40 40-45Error Rate (070)Figure 1: Distribution of ErrorsTo measure the total accuracy of the system, we randomly chose 33 verbs from the300 most frequent verbs in the test corpus (given in Table 4), automatically estimatedthe subcat frames for each occurrence of these verbs in the test corpus, and compared theresults to manually determined subcat frames.The overall results are quite promising.
The total number of occurrences of the 33verbs in the test corpus (excluding participle forms) is 2,242.
Of these, 1,933 were assignedcorrect subcat frames by the system.
(The 'correct'-assignment counts always appear inthe diagonal cells in a comparison table such as in Table 3.)
This indicates an overallaccuracy for the method of 86%.If we exclude the subcat frame "REST" from our statistics, the total number of oc-currences of the 33 verbs in one of the six subcat frames is 1,565.
Of these, 1,311 wereassigned correct subcat frames by the system.
This represents 83% accuracy.For 30 of the 33 verbs, both the first and the second (if any) most frequent subcatframes as determined by the system were correct.
For all of the verbs except one ("need"),the most frequent frame was correct.Figure 1 is a histogram showing the number of verbs within each error-rate zone.In computing the error rate, we divide the total 'off-diagonal'-cell counts, excluding thecounts in the "REST"  column, by the total cell counts, again excluding the "REST" col-umn margin.
Thus, the off-diagonal cell counts in the "REST" row, representing instanceswhere one of the six actual subcat frames was misidentified as "REST", are counted aserrors.
This formula, in general, gives higher error rates than would result from simplydividing the off-diagonal cell counts by the total cell counts.Overall, the most frequent source of errors, again, was errors in noun phrase boundarydetection.
The second most frequent source was misidentification of infinitival 'purpose'clauses, as in "he used a crowbar to open the door".
"To open the door" is a 'purpose'adjunct modifying either the verb phrase "used a crowbar" or the main clause "he used acrowbar".
But such adjuncts are incorrectly judged to be complements of their main verbsi01by the subcat frame extraction rules in Table 2.
In formulating the rules, we assumed thata 'purpose' adjunct appears effectively randomly and much less frequently than infinitivalcomplements.
This is true for our corpus in general; but some verbs, such as "use" and"need", appear elatively frequently with 'purpose' infinitivals.
In addition to errors fromparsing and 'purpose' infinitives, we observed several other, less frequent ypes of errors.These, too, pattern with specific verbs and do not occur randomly across verbs.4 Statistical AnalysisFor most of the verbs in the experiment, our method provides a good measure of subcatframe frequencies.
However, some of the verbs seem to appear in syntactic structures thatcannot be captured by our inventory of subcat frames.
For example, "need" is frequentlyused in relative clauses without relative pronouns, as in "the last thing they need".
Sincethis kind of relative clauses cannot be captured by the rules in Table 2, each occurrenceof these relative clause causes an error in measurement.
I  is likely that there are manyother classes of verbs with distinctive syntactic preferences.
If we try to add rules for eachsuch class, it will become increasingly difficult to write rules that affect only the targetclass and to eliminate undesirable rule interactions.In the following sections, we describe a statistical method which, based on a set oftraining samples, enables the system to learn patterns of errors and substantially increasethe accuracy of estimated verb-suhcat frequencies.4.1 Genera l  SchemeThe method described in Section 2 is wholly deterministic; it depends only on one setof subcat extraction rules which serve as filters.
Instead of treating the system outputfor each verb token as an estimated subcat frame, we can think of the output as onefeature associated with the occurrence of the verb.
This single feature can be combined,statistically, with other features in the corpus to yield more accurate characterizationsof verb contexts and more accurate subcat-frame frequency estimates.
If the other fea-tures are capturable via regular-expression rules, they can also be automatically detectedin the manner described in the Section 2.
For example, main verbs in relative clauseswithout relative pronouns may have a higher probability of having the feature "nnk", i.e.,"(NP)(NP)(VERB)".More formally, let Y be a response variable taking as its value a subeat frame.
LetX1, X2,..., XN be explanatory variables.
Each Xi is associated with a feature xpressedby one or a set of regular expressions.
If a feature is expressed by one regular expression(R), the value of the feature is 1 if the occurrence of the verb matches R and 0 otherwise.If the feature is expressed by a set of regular expressions, its value is the label of theregular expression that the occurrence of the verb matches.
The set of regular expressionsin Table 2 can therefore be considered to characterize one explanatory variable whosevalue ranges from (NP+NP) to (REST).Now, we assume that a training corpus is available in which all verb tokens are givenalong with their subcat frames.
By running our system on the training corpus, we canautomatically generate a (N + 1)-dimensional contingency table.
Table 3 is an exampleof a 2-dimensional contingency table with X = <OUTPUT OF SYSTEM> and Y = <REALOCCURRENCES>.
Using loglinear models \[Agresti, 1990\], we can derive fitted values of102each cell in the (N + 1)-dimensional contingency table.
In the case of a saturated model,in which all kinds of interaction of variables up to (N + 1)-way interactions are included,the raw cell counts are the Maximum Likelihood solution.
The fitted values are then usedto estimate the subcat frame frequencies of a new corpus as follows.First, the system is run on the new corpus to obtain an N-dimensional contingencytable.
This table is considered to be an X1 - X2 .
.
.
.
.
XN-marginal table.
What weare aiming at is the Y margins that represent the real subcat frame frequencies of thenew corpus.
Assuming that the training corpus and the new corpus are homogeneous(e.g., reflecting similar sub-domains or samples of a common domain), we estimate the Ymargins using Bayes theorem on the fitted values of the training corpus as follows:E(Y  = k IX1 - X2 .
.
.
.
.
XS~ marginal table of the new corpus)= ~.
.
.Z J~, i~ .
.
.
iN+P(Y=k lX1  = i l ,X~=i2 , ' " ,XN=iN)i l i2 iN= ~'"~JV~, i , .
.
.
i ,~+ P(X I= i l 'X2=i~' ' ' "XN=iN IY=k)  P (Y=k)i l i2 iN= EZ ,N?Ek, J~ili~...iNk' il 12 iNwhere ~ii l i~.. .
i .
+ is the cell count of the X1 - X2 .
.
.
.
.
XN marginal table of the newcorpus obtained as the system output, and .h411i2...iN~ is the fitted value of the (N + 1)-dimensional contingency table of the training corpus based on a particular loglinear model.4 .2 Lex ica l  Heur i s t i csThe simplest application of the above method is to use a 2-way contingency table, as inTable 3.
There are two possibilities to explore in constructing a 2-way contingency table.One is to sum up the cell counts of all the verbs in the training corpus and produce asingle (large) general table.
The other is to construct a table for each verb.
Obviouslythe former approach is preferable if it works.
Unfortunately, such a table is typically toogeneral to be useful; the estimated frequencies based on it are less accurate than rawsystem output.
This is because the sources of errors, viz., the distribution of off-diagonalcell counts of 2-way contingency tables, differ considerably from verb to verb.
The latterapproach is problematic if we have to make such a table for each domain.
However, if wehave a training corpus in one domain, and if the heuristics for each verb extracted fromthe training corpus are also applicable to other domains, the approach may work.To test the latter possibility, we constructed a contingency table for the verb fromthe test corpus described in the Section 3 that was most problematic (least accuratelyestimated) among the 33 verbs--"need".
Note that we are using the test corpus describedin the Section 3 as a training corpus here, because we already know both the measuredfrequency and the hand-judged frequency of "need" which are necessary to construct acontingency table.
The total occurrence of this verb was 75.
To smooth the table, 0.1 isadded to all the cell counts.
As new test corpora, we extracted another 300,000 words oftagged text from the WSJ corpus (labeled "W3") and also three sets of 300,000 words oftagged text from the Brown corpus (labeled "BI", "B2", and "B3"), as retagged under the103W3MeasuredBy HandEst imatedNP+NP NP+CL NP+INF  NP  CL INF  REST2.4 0.0 10.6 44.7 1.2 31.8 9.40.0 0.0 0.0 69.4 0.0 30.6 0.00.0 0.0 0.0 66.3 0.0 30.1 3.6Tot~ Occur rences :85B1MeasuredBy HandEst imatedNP+NP NP+CL NP+INF  NP  CL INF  REST1.8 0.9 7.9 38.6 1.8 14.9 34.20.0 0.0 0.0 72.8 0.0 15.8 11.40.0 0.0 0.0 76.6 0.0 14.4 9.1Tot~ Occur rences : l l4B2MeasuredBy HandEst imatedNP+NP NP+CL NP+INF  NP  CL INF  REST0.0 1.4 8.7 40.6 1.4 17.4 30.40.0 0.0 0.0 73.9 0.0 18.8 7.20.0 0.0 0.0 76.1 0.0 16.4 7.5Tota lOccur rences :69B3MeasuredBy HandEst imatedNP+NP NP+CL NP+INF  NP  CL INF  REST3.3 0.0 1.7 30.0 3.3 31.7 30.00.0 0.0 0.0 60.0 0.0 28.3 11.70.0 0.0 0.0 61.4 0.0 29.8 8.8Tota lOccur rences :60Table 5: Statistical Estimation (Unit = %) for the Verb "Need"Penn Treebank tagset.
All the training and test corpora were reviewed--and judged- -byhand.Table 5 gives the frequency distributions based on the system output, hand judge-ment, and statistical analysis.
(As before, we take the hand judgement to be the goldstandard, the actual frequency of a particular frame.)
After the Y margins are statisti-cally estimated, the least estimated Y values less than 1.0 are truncated to 0.
(These areconsidered to have appeared ue to the smoothing.
)In all of the test corpora, the method gives very accurate frequency distribution es-timates.
Big gaps between the automatically-measured an manually-determined fre-quencies of "NP" and "REST" are shown to be substantially reduced through the use ofstatistical estimation.
This result is especially encouraging because tile heuristics obtainedin one domain are shown to be applicable to a considerably different domain.
Further-more, by combining more feature sets and making use of multi-dimensional analysis, wecan expect to obtain more accurate stimations.1045 Conc lus ion  and Future D i rec t ionWe have demonstrated that by combining syntactic and statistical analysis, the frequenciesof verb-subcat frames can be estimated with high accuracy.
Although the present systemmeasures the frequencies of only six subcat frames, the method is general enough to beextended to many more frames.
The traditional application of regular expressions asrules for deterministic processing has self-evident limitations ince a linear grammar isnot powerful enough to capture general linguistic phenomena.
The statistical method wepropose uses regular expressions as filters for detecting specific features of the occurrencesof verbs and employs multi-dimensional analysis of the features based on loglinear modelsand Bayes Theorem.We expect that by identifying other useful syntactic features we can further improvethe accuracy of the frequency estimation.
Such features can be regarded as characterizingthe syntactic ontext of the verbs, quite broadly.
The features need not be linked to alocal verb context.
For example, a regular expression such as "w\[ 'vex\]*k" can be usedto find cases where the target verb is preceded by a relative pronoun such that there isno other finite verb or punctuation or sentence final period between the relative pronounand the target verb.If the syntactic structure of a sentence can be predicted using only syntactic and lexicalknowledge, we can hope to estimate the subcat frame of each occurrence of a verb usingthe context expressed by a set of features.
We thus can aim to extend and refine thismethod for use with general probabilistic parsing of unrestricted text.6 AcknowledgementsWe thank Teddy Seidenfeid, Jeremy York, and Alex Franz for their comments and dis-cussions with us.
We remain, of course, solely responsible for any errors or inadequaciesin the paper.Re ferences\[Agresti, 1990\] A. Agresti.
Categorical Data Analysis.
New York, NY: John Wiley andSons, 1990.\[Baker, 1979\] J. Baker.
"Trainable Grammars for Speech Recognition".
In D.II.
Klattand J.J. Wolf (eds.
), Speech Communication Papers for the 97th Meeting of lhe AcousticSociety of America, 1979, pp.
547-550.\[Brent, 1'991\] M.R.
Brent.
"Automatic Acquisition of Subcategorization Frames fromUntagged Text".
Proceedings ofthe 29th Annual Meeting of the ACL, 1991.\[Brent and Berwick, 1991\] M.R.
Brent and R.C.
Berwick.
"Automatic Acquisition of Sub-categorization Frames from Tagged Text".
In Proceedings of the DARPA Speech andNatural Language Workshop, Morgan Kaufmann, 1991.\[Brill, 1992\] E. Brill.
"A Simple Rule-Based Part of Speech Tagger".
In Proceedings ofthe DARPA Speech and Natural Language Workshop, Morgan Kaufmann, 1992.105\[Carroll and Charniak, 1992\] G. Carroll and E. Charniak.
"Learning Probabilistic De-pendency Grammars from Labelled Text".
In Working Notes of the Symposium onProbabilistic Approaches to Natural Language, AAAI Fall Symposium Series, 1992.\[Church, 1988\] K.W.
Church.
"A Stochastic Parts Program and Noun Phrase Parserfor Unrestricted Text".
In Proceedings of the Second Conference on Applied NaturalLanguage Processing, 1988.\[de Marken, 1992\] C.G.
de Mareken.
"Parsing the LOB Corpus".
In Proceedings of thegSth Annual Meeting of the ACL, 1990, pp.
243-251.\[Hornby, 1989\] A.S. Hornby, (ed.).
Oxford Advanced Learner's Dictionary of CurrentEnglish.
Oxford, UK: Oxford University Press, 1989.\[Jelinek et al, 1990\] F. Jelinek, L.D.
Lafferty, and R.L.
Mercer.
Basic Method of Proba-bilistic Context Free Grammars.
Technical Report RC 16374 (72684), IBM, YorktownHeights, NY 10598, 1990.\[Lauri and Young, 1990\] K. Lari and S.J.
Young.
"The Estimation of Stochastic Context-Free Grammars Using the Inside-Outside Algorithm".
Computer Speech and Language,4, 1990, pp.
35-56.106
