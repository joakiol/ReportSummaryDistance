Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 33?40,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsA Probabilistic Search for the Best Solution Among Partially CompletedCandidatesFilip Ginter, Aleksandr Mylla?ri, and Tapio SalakoskiTurku Centre for Computer Science (TUCS) andDepartment of Information TechnologyUniversity of TurkuLemminka?isenkatu 14 A20520 Turku, Finlandfirst.last@it.utu.fiAbstractWe consider the problem of identifyingamong many candidates a single best so-lution which jointly maximizes severaldomain-specific target functions.
Assum-ing that the candidate solutions can begenerated incrementally, we model the er-ror in prediction due to the incomplete-ness of partial solutions as a normallydistributed random variable.
Using thismodel, we derive a probabilistic search al-gorithm that aims at finding the best solu-tion without the necessity to complete andrank all candidate solutions.
We do not as-sume a Viterbi-type decoding, allowing awider range of target functions.We evaluate the proposed algorithm on theproblem of best parse identification, com-bining simple heuristic with more com-plex machine-learning based target func-tions.
We show that the search algorithmis capable of identifying candidates with avery high score without completing a sig-nificant proportion of the candidate solu-tions.1 BackgroundMost of the current NLP systems assume a pipelinearchitecture, where each level of analysis is imple-mented as a module that produces a single, locallyoptimal solution that is passed to the next module inthe pipeline.
There has recently been an increasedinterest in the application of joint inference, whichidentifies a solution that is globally optimal through-out the system and avoids some of the problems ofthe pipeline architecture, such as error propagation.We assume, at least conceptually, a division ofthe joint inference problem into two subproblems:that of finding a set of solutions that are structurallycompatible with each of the modules, and that of se-lecting the globally best of these structurally correctsolutions.
Many of the modules define a target func-tion that scores the solutions by some domain cri-teria based on local knowledge.
The globally bestsolution maximizes some combination of the targetfunctions, for example a sum.For illustration, consider a system comprising oftwo modules: a POS tagger and a parser.
The POStagger generates a set of tag sequences that are com-patible with the sentence text.
Further, it may im-plement a target function, based, for instance, ontag n-grams, that scores these sequences accordingto POS-centric criteria.
The parser produces a setof candidate parses and typically also implements atarget function that scores the parses based on theirstructural and lexical features.
Each parse that iscompatible with both the POS tagger and the parseris structurally correct.
The best solution may be de-fined, for instance, as such a solution that maximizesthe sum of the scores of the POS- and parser-centrictarget functions.In practice, the set of structurally correct solu-tions may be computed, for example, through theintersection or composition of finite-state automataas in the formalism of finite-state intersection gram-mars (Koskenniemi, 1990).
Finding the best so-33lution may be implemented as a best-path searchthrough Viterbi decoding, given a target functionthat satisfies the Viterbi condition.Most of the recent approaches to NLP tasks likeparse re-ranking make, however, use of feature-based representations and machine-learning inducedtarget functions, which do not allow efficient searchstrategies that are guaranteed to find the global op-timum.
In general case, all structurally correct so-lutions have to be generated and scored by the tar-get functions in order to guarantee that the globallyoptimal solution is found.
Further, each of the vari-ous problems in natural language processing is typ-ically approached with a different class of models,ranging from n-gram statistics to complex regressorsand classifiers such as the support vector machines.These different approaches need to be combined inorder to find the globally optimal solution.
There-fore, in our study we aim to develop a search strat-egy that allows to combine a wider range of targetfunctions.An alternative approach is that of propagating nbest solutions through the pipeline system, whereeach step re-ranks the solutions by local criteria(Ji et al, 2005).
Incorporating a wide range offeatures representing information from all levels ofanalysis into a single master classifier is other com-monly used method (Kambhatla, 2004; Zelenko etal., 2004).In this paper, we assume the possibility of gen-erating the structurally correct solutions incremen-tally, through a sequence of partially completed so-lutions.
We then derive a probabilistic search algo-rithm that attempts to identify the globally best solu-tion, without fully completing all structurally correctsolutions.
Further, we do not impose strong restric-tions, such as the Viterbi assumption, on the targetfunctions.To a certain extent, this approach is related to theproblem of cost-sensitive learning, where obtaininga feature value is associated with a cost and theobjective is to minimize the cost of training dataacquisition and the cost of instance classification(Melville et al, 2004).
However, the crucial dif-ference is that we do not assume the possibility toinfluence when advancing a partial solution, whichfeature will be obtained next.2 MethodLet us consider a system in which there are N so-lutions s1, .
.
.
, sN ?
S to a problem and M tar-get functions f1, .
.
.
, fM , where fk : S ?
R, thatassign a score to each of the solutions.
The scorefk(si) expresses the extent to which the solutionsi satisfies the criterion implemented by the targetfunction fk.
The overall score of a solution sif(si) =M?k=1fk(si) (1)is the sum of the scores given by the individual targetfunctions.
The objective is to identify s?, the bestamong the N possible solutions, that maximizes theoverall score:s?
= arg maxsif(si) .
(2)Suppose that the solutions are generated in-crementally so that each solution si can bereached through a sequence of F partial solutionssi,1, si,2, .
.
.
, si,F , where si,F = si.
Let furtheru : S ?
(0, 1] be a measure of a degree of com-pletion for a particular solution.
For a complete so-lution si, u(si) = 1, and for a partial solution si,n,u(si) < 1.
For instance, when assigning POS tagsto the words of a sentence, the degree of completioncould be defined as the number of words assignedwith a POS tag so far, divided by the total number ofwords in the sentence.The score of a partial solution si,n is, to a certainextent, a prediction of the score of the correspond-ing complete solution si.
Intuitively, the accuracy ofthis prediction depends on the degree of completion.The score of a partial solution with a high degreeof completion is generally closer to the final score,compared to a partial solution with a low degree ofcompletion.Let?k(si,n) = fk(si) ?
fk(si,n) (3)be the difference between the scores of si and si,n.That is, ?k(si,n) is the error in score caused by the in-completeness of the partial solution si,n.
As the so-lutions are generated incrementally, the exact valueof ?k(si,n) is not known at the moment of generatingsi,n because the solution si has not been completed34yet.
However, we can model the error based on theknowledge of si,n.
We assume that, for a given si,n,the error ?k(si,n) is a random variable distributed ac-cording to a probability distribution with a densityfunction ?k, denoted as?k(si,n) ?
?k(?
; si,n) .
(4)The partial solution si,n is a parameter to the distri-bution and, in theory, each partial solution gives riseto a different distribution of the same general shape.We assume that the error ?
(si,n) is distributedaround a mean value and for a ?reasonably behav-ing?
target function, the probability of a small erroris higher than the probability of a large error.
Ideally,the target function will not exhibit any systematic er-ror, and the mean value would thus be zero1.
For in-stance, a positive mean error indicates a systematicbias toward underestimating the score.
The meanerror should approach 0 as the degree of completionincreases and the error of a complete solution is al-ways 0.
We have further argued that the reliabilityof the prediction grows with the degree of comple-tion.
That is, the error of a partial solution with ahigh degree of completion should exhibit a smallervariance, compared to that of a largely incompletesolution.
The variance of the error for a completesolution is always 0.Knowing the distribution ?k of the error ?k, thedensity of the distribution dk(f ; si,n) of the finalscore fk(si) is obtained by shifting the density ofthe error ?k(si,n) by fk(si,n), that is,fk(si) ?
dk(f ; si,n) , (5)wheredk(f ; si,n) = ?k(f ?
fk(si,n) ; si,n) .
(6)So far, we have discussed the case of a single tar-get function fk.
Let us now consider the generalcase of M target functions.
Knowing the final scoredensity dk for the individual target functions fk, it isnow necessary to find the density of the overall scoref(si).
By Equation 1, it is distributed as the sum1We will see in our evaluation experiments that this is notthe case, and the target functions may exhibit a systematic biasin the error ?.?
(si,n)d(f ; si,n)?2(si,n)??
(si,n)f (si,n)0Sys.
bias in error ?Figure 1: The probability density d(f ; si,n) of thedistribution of the final score f(si), given a partialsolution si,n.
The density is assumed normally dis-tributed, with mean ?
(si,n) and variance ?2(si,n).With probability 1 ?
?, the final score is less than?
(si,n).of the random variables f1(si) , .
.
.
, fM (si).
There-fore, assuming independence, its density is the con-volution of the densities of these variables, that is,given si,n,d(f ; si,n) = (d1 ?
.
.
.
?
dM )(f ; si,n) , (7)andf(si) ?
d(f ; si,n) .
(8)We have assumed the independence of the targetfunction scores.
Further, we will make the assump-tion that d takes the form of the normal distribution,which is convolution-closed, a property necessaryfor efficient calculation by Equation 7.
We thus haved(f ; si,n) = n(f ; ?
(si,n) , ?2(si,n)), (9)where n is the normal density function.
While itis unlikely that independence and normality holdstrictly, it is a commonly used approximation, nec-essary for an analytical solution of (7).
The notionsintroduced so far are illustrated in Figure 1.2.1 The search algorithmWe will now apply the model introduced in the pre-vious section to derive a probabilistic search algo-rithm.35Let us consider two partial solutions si,n and sj,mwith the objective of deciding which one of them is?more promising?, that is, more likely to lead to acomplete solution with a higher score.
The condi-tion of ?more promising?
can be defined in severalways.
For instance, once again assuming indepen-dence, it is possible to directly compute the proba-bility P (f(si) < f(sj)):P (f(si) < f(sj))= P (f(si) ?
f(sj) < 0)=?
0??
(dsi,n ?
(?dsj,m))(f) df ,(10)where dsi,n refers to the function d(f ; si,n).
Sinced is the convolution-closed normal density, Equa-tion 10 can be directly computed using the normalcumulative distribution.
The disadvantage of thisdefinition is that the cumulative distribution needsto be evaluated separately for each pair of partialsolutions.
Therefore, we assume an alternate defi-nition of ?more promising?
in which the cumulativedistribution is evaluated only once for each partialsolution.Let ?
?
[0, 1] be a probability value and ?
(si,n)be the score such that P (f(si) > ?
(si,n)) = ?.
Thevalue of ?
(si,n) can easily be computed from the in-verse cumulative distribution function correspond-ing to the density function d(f ; si,n).
The interpre-tation of ?
(si,n) is that with probability of 1 ?
?,the partial solution si,n, once completed, will leadto a score smaller than ?(si,n).
The constant ?
isa parameter, set to an appropriate small value.
SeeFigure 1 for illustration.We will refer to ?
(si,n) as the maximal expectedscore of si,n.
Of the two partial solutions, we con-sider as ?more promising?
the one, whose maximalexpected score is higher.
As illustrated in Figure 2,it is possible for a partial solution si,n to be morepromising even though its score f(si,n) is lower thanthat of some other partial solution sj,m.Further, given a complete solution si and a partialsolution sj,m, a related question is whether sj,m is apromising solution, that is, whether it is likely thatadvancing it will lead to a score higher than f(si).Using the notion of maximal expected score, we saythat a solution is promising if ?
(sj,m) > f(si).With the definitions introduced so far, we aref (si,n) f (sj,m) ?(si,n)?
(sj,m)d(f ; si,n)d(f ; sj,m)Figure 2: Although the score of si,n is lower thanthe score of sj,m, the partial solution si,n is morepromising, since ?
(si,n) > ?(sj,m).
Note that forthe sake of simplicity, a zero systematic bias of theerror ?
is assumed, that is, the densities are centeredaround the partial solution scores.now able to perform two basic operations: comparetwo partial solutions, deciding which one of them ismore promising, and compare a partial solution withsome complete solution, deciding whether the par-tial solution is still promising or can be disregarded.These two basic operations are sufficient to devisethe following search algorithm.?
Maintain a priority queue of partial solutions,ordered by their maximal expected score.?
In each step, remove from the queue the par-tial solution with the highest maximal expectedscore, advance it, and enqueue any resultingpartial solutions.?
Iterate while the maximal expected score of themost promising partial solution remains higherthan the score of the best complete solution dis-covered so far.The parameter ?
primarily affects how early thealgorithm stops, however, it influences the order inwhich the solutions are considered as well.
Low val-ues of ?
result in higher maximal expected scores36and therefore partial solutions need to be advancedto a higher degree of completion before they can bedisregarded as unpromising.While there are no particular theoretical restric-tions on the target functions, there is an importantpractical consideration.
Since the target function isevaluated every time a partial solution si,n is ad-vanced into si,n+1, being able to use the informa-tion about si,n to efficiently compute fk(si,n+1) isnecessary.The algorithm is to a large extent related to the A?search algorithm, which maintains a priority queueof partial solutions, ordered according to a scoreg(x) + h(x), where g(x) is the score of x and h(x)is a heuristic overestimate2 of the final score of thegoal reached from x.
Here, the maximal expectedscore of a partial solution is an overestimate withthe probability of 1??
and can be viewed as a prob-abilistic counterpart of the A?
heuristic componenth(x).
Note that A?
only guarantees to find the bestsolution if h(x) never underestimates, which is notthe case here.2.2 Estimation of ?k(si,n) and ?2k(si,n)So far, we have assumed that for each partial so-lution si,n and each target function fk, the density?k(?
; si,n) is defined as a normal density specifiedby the mean ?k(si,n) and variance ?2k(si,n).
Thisdensity models the error ?k(si,n) that arises due tothe incompleteness of si,n.
The parameters ?k(si,n)and ?2k(si,n) are, in theory, different for each si,n andreflect the behavior of the target function fk as wellas the degree of completion and possibly other at-tributes of si,n.
It is thus necessary to estimate thesetwo parameters from data.Let us, for each target function fk, consider atraining set of observations Tk ?
S ?
R. Eachtraining observation tj =(sj,nj , ?k(sj,nj))?
Tkcorresponds to a solution sj,nj with a known error?k(sj,nj)= fk(sj) ?
fk(sj,nj).Before we introduce the method to estimate thedensity ?k(?
; si,n) for a particular si,n, we discussdata normalization.
The overall score f(si,n) is de-fined as the sum of the scores assigned by the in-dividual target functions fk.
Naturally, it is desir-2In the usual application of A?
to shortest-path search, h(x)is a heuristic underestimate since the objective is to minimizethe score.able that these scores are of comparable magnitudes.Therefore, we normalize the target functions usingthe z-normalizationz(x) = x ?
mean(x)stdev(x) .
(11)Each target function fk is normalized separately,based on the data in the training set Tk.
Throughoutour experiments, the values of the target function arealways z-normalized.Let us now consider the estimation of the mean?k(si,n) and variance ?2k(si,n) that define the den-sity ?k(?
; si,n).
Naturally, it is not possible to es-timate the distribution parameters for each solutionsi,n separately.
Instead, we approximate the parame-ters based on two most salient characteristics of eachsolution: the degree of completion u(si,n) and thescore fk(si,n).
Thus,?k(si,n) ?
?k(u(si,n) , fk(si,n)) (12)?2k(si,n) ?
?2k(u(si,n) , fk(si,n)) .
(13)Let us assume the following notation: ui = u(si,n),fi = fk(si,n), uj = u(sj,nj), fj = fk(sj,nj), and?j = ?k(sj,nj).
The estimate is obtained from Tkusing kernel smoothing (Silverman, 1986):?k(ui, fi) =?tj?T ?jK?tj?T K(14)and?2k(ui, fi) =?tj?T (?j ?
?k(ui, fi))2 K?tj?T K, (15)where K stands for the kernel value Kui,fi(uj , fj).The kernel K is the product of two Gaussians, cen-tered at ui and fi, respectively.Kui,fi(uj , fj)= n(uj ; ui, ?2u)?
n(fj ; fi, ?2f), (16)where n(x; ?, ?2)is the normal density function.The variances ?2u and ?2f control the degree ofsmoothing along the u and f axes, respectively.High variance results in stronger smoothing, com-pared to low variance.
In our evaluation, we set the370.2 0.6 1.0?4?201?A0.2 0.6 1.0?4?201?A20.2 0.6 1.0?1135?B0.2 0.6 1.0?1135?B2Figure 3: Mean and variance of the error ?
(si,n).By (12) and (13), the error is approximated as afunction of the degree of completion u(si,n) and thescore fk(si,n).
The degree of completion is on thehorizontal and the score on the vertical axis.
Theestimates (?A, ?2A) and (?B, ?2B) correspond to theRLSC regressor and average link length target func-tions, respectively.variance such that ?u and ?f equal to 10% of the dis-tance from min(uj) to max(uj) and from min(fj)to max(fj), respectively.The kernel-smoothed estimates of ?
and ?2 fortwo of the four target functions used in the evalua-tion experiments are illustrated in Figure 3.
Whileboth estimates demonstrate the decrease both inmean and variance for u approaching 0, the tar-get functions generally exhibit a different behav-ior.
Note that the values are clearly dependent onboth the score and the degree of completion, indi-cating that the degree of completion alone is not suf-ficiently representative of the partial solutions.
Ide-ally, the values of both the mean and variance shouldbe strictly 0 for u = 1, however, due to the effect ofsmoothing, they remain non-zero.3 EvaluationWe test the proposed search algorithm on the prob-lem of dependency parsing.
We have previously de-veloped a finite-state implementation (Ginter et al,2006) of the Link Grammar (LG) parser (Sleator andTemperley, 1991) which generates the parse throughthe intersection of several finite-state automata.
Theresulting automaton encodes all candidate parses.The parses are then generated from left to right, pro-ceeding through the automaton from the initial tothe final state.
A partial parse is a sequence of nwords from the beginning of the sentence, togetherwith string encoding of their dependencies.
Advanc-ing a partial parse corresponds to appending to it thenext word.
The degree of completion is then definedas the number of words currently generated in theparse, divided by the total number of words in thesentence.To evaluate the ability of the proposed method tocombine diverse criteria in the search, we use fourtarget functions: a complex state-of-the-art parse re-ranker based on a regularized least-squares (RLSC)regressor (Tsivtsivadze et al, 2005), and three mea-sures inspired by the simple heuristics applied bythe LG parser.
The criteria are the average length ofa dependency, the average level of nesting of a de-pendency, and the average number of dependencieslinking a word.
The RLSC regressor, on the otherhand, employs complex features and word n-gramstatistics.The dataset consists of 200 sentences ran-domly selected from the BioInfer corpus ofdependency-parsed sentences extracted from ab-stracts of biomedical research articles (Pyysalo etal., 2006).
For each sentence, we have randomlyselected a maximum of 100 parses.
For sentenceswith less than 100 parses, all parses were selected.The average number of parses per sentence is 62.Further, we perform 5 ?
2 cross-validation, that is,in each of five replications, we divide the data ran-domly to two sets of 100 sentences and use one set toestimate the probability distributions and the otherset to measure the performance of the search algo-rithm.
The RLSC regressor is trained once, using adifferent set of sentences from the BioInfer corpus.The results presented here are averaged over the 10folds.
As a comparative baseline, we use a simple38greedy search algorithm that always advances thepartial solution with the highest score until all so-lutions have been generated.3.1 ResultsFor each sentence s with parses S {s1, .
.
.
, sN}, letSC ?
S be the subset of parses fully completed be-fore the algorithm stops and SN = S \ SC the sub-set of parses not fully completed.
Let further TC bethe number of iterations taken before the algorithmstops, and T be the total number of steps needed togenerate all parses in S .
Thus, |S| is the size of thesearch space measured in the number of parses, andT is the size of the search space measured in thenumber of steps.
For a single parse si, rank(si) isthe number of parses in S with a score higher thanf(si) plus 1.
Thus, the rank of all solutions withthe maximal score is 1.
Finally, ord(si) correspondsto the order in which the parses were completed bythe algorithm (disregarding the stopping criterion).For example, if the parses were completed in theorder s3, s8, s1, then ord(s3) = 1, ord(s8) = 2,and ord(s1) = 3.
While two solutions have thesame rank if their scores are equal, no two solutionshave the same order.
The best completed solutions?C ?
SC is the solution with the highest rank in SCand the lowest order among solutions with the samerank.
The best solution s?
is the solution with rank 1and the lowest order among solutions with rank 1.
Ifs?
?
SC , then s?C = s?
and the objective of the algo-rithm to find the best solution was fulfilled.
We usethe following measures of performance: rank(s?C),ord(s?
), |SC ||S| , andTCT .
The most important criteriaare rank(s?C) which measures how good the bestfound solution is, and TCT which measures the pro-portion of steps actually taken by the algorithm ofthe total number of steps needed to complete all thecandidate solutions.
Further, ord(s?
), the numberof parses completed before the global optimum wasreached regardless the stopping criterion, is indica-tive about the ability of the search to reach the globaloptimum early among the completed parses.
Notethat all measures except for ord(s?)
equal to 1 forthe baseline greedy search, since it lacks a stoppingcriterion.The average performance values for four settingsof the parameter ?
are presented in Table 1.
Clearly,?
rank(s?C) ord(s?)
|SC ||S|TCT0.01 1.6 8.8 0.78 0.940.05 2.8 11.2 0.62 0.850.10 4.0 12.2 0.53 0.790.20 6.0 13.5 0.41 0.73Base 1.0 28.7 1.00 1.00Table 1: Average results over all sentences.the algorithm behaves as expected with respect tothe parameter ?.
While with the strictest setting?
= 0.01, 94% of the search space is explored, withthe least strict setting of ?
= 0.2, 73% is explored,thus pruning one quarter of the search space.
Theproportion of completed parses is generally consid-erably lower than the proportion of explored searchspace.
This indicates that the parses are generallyadvanced to a significant level of completion, butthen ruled out.
The behavior of the algorithm isthus closer to a breadth-first, rather than depth-firstsearch.
We also notice that the average rank of thebest completed solution is very low, indicating thatalthough the algorithm does not necessarily identifythe best solution, it generally identifies a very goodsolution.
In addition, the order of the best solution islow as well, suggesting that generally good solutionsare identified before low-score solutions.
Further,compared to the baseline, the globally optimal solu-tion is reached earlier among the completed parses,although this does not imply that it is reached earlierin the number of steps.
Apart from the overall aver-ages, we also consider the performance with respectto the number of alternative parses for each sentence(Table 2).
Here we see that even with the least strictsetting, the search finds a reasonably good solutionwhile being able to reduce the search space to 48%.4 Conclusions and future workWe have considered the problem of identifying aglobally optimal solution among a set of candidatesolutions, jointly optimizing several target functionsthat implement domain criteria.
Assuming the solu-tions are generated incrementally, we have deriveda probabilistic search algorithm that aims to identifythe globally optimal solution without completing allof the candidate solutions.
The algorithm is based ona model of the error in prediction caused by the in-39?
= 0.01 ?
= 0.2 Base|S| # rank(s?C) ord(s?)
|SC ||S|TCT rank(s?C) ord(s?
)|SC ||S|TCT ord(s?
)1-10 40 1.0 1.6 1.00 1.00 1.2 1.8 0.84 0.95 2.8511-20 18 1.1 4.4 0.88 0.97 2.8 7.0 0.54 0.79 9.8221-30 8 1.0 2.9 1.00 1.00 1.0 2.4 0.80 0.98 14.7531-40 9 1.2 7.8 0.79 0.95 2.6 10.8 0.48 0.74 20.6741-50 6 1.0 4.4 0.80 0.89 4.9 9.8 0.28 0.61 18.0751-60 3 1.0 2.3 0.64 0.88 7.1 5.9 0.30 0.59 38.6761-70 5 1.1 26.9 0.86 0.99 3.4 23.2 0.22 0.68 32.6071-80 3 1.0 8.7 0.78 0.98 9.2 19.6 0.30 0.71 49.6781-90 6 2.5 8.2 0.61 0.94 9.3 16.6 0.24 0.76 47.6791-100 102 5.2 20.9 0.50 0.81 18.9 38.2 0.15 0.48 52.69Table 2: Average results with respect to the number of alternative parses.
The column # contains the numberof sentences in the dataset which have the given number of parses.completeness of a partial solution.
Using the model,the order in which partial solutions are explored isdefined, as well as a stopping criterion for the algo-rithm.We have performed an evaluation using best parseidentification as the model problem.
The results in-dicate that the method is capable of combining sim-ple heuristic criteria with a complex regressor, iden-tifying solutions with a very low average rank.The crucial component of the method is the modelof the error ?.
Improving the accuracy of the modelmay potentially further improve the performance ofthe algorithm, allowing a more accurate stoppingcriterion and better order in which the parses arecompleted.
We have assumed independence be-tween the scores assigned by the target functions.
Asa future work, a multivariate model will be consid-ered that takes into account the mutual dependenciesof the target functions.ReferencesFilip Ginter, Sampo Pyysalo, Jorma Boberg, and TapioSalakoski.
2006.
Regular approximation of LinkGrammar.
Manuscript under review.Heng Ji, David Westbrook, and Ralph Grishman.
2005.Using semantic relations to refine coreference deci-sions.
In Proceedings of Human Language Technol-ogy Conference and Conference on Empirical Methodsin Natural Language Processing (HLT/EMNLP?05),Vancouver, Canada, pages 17?24.
ACL.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy modelsfor information extraction.
In The Companion Vol-ume to the Proceedings of 42st Annual Meeting of theAssociation for Computational Linguistics (ACL?04),Barcelona, Spain, pages 178?181.
ACL.Kimmo Koskenniemi.
1990.
Finite-state parsing anddisambiguation.
In Proceedings of the 13th In-ternational Conference on Computational Linguis-tics (COLING?90), Helsinki, Finland, pages 229?232.ACL.Prem Melville, Maytal Saar-Tsechansky, Foster Provost,and Raymond Mooney.
2004.
Active feature-valueacquisition for classifier induction.
In Proceedingsof the Fourth IEEE International Conference on DataMining (ICDM?04), pages 483?486.
IEEE ComputerSociety.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Ja?rvinen, and TapioSalakoski.
2006.
Bio Information Extraction Re-source: A corpus for information extraction in thebiomedical domain.
Manuscript under review.Bernard W. Silverman.
1986.
Density Estimation forStatistics and Data Analysis.
Chapman & Hall.Daniel D. Sleator and Davy Temperley.
1991.
Pars-ing English with a link grammar.
Technical ReportCMU-CS-91-196, Department of Computer Science,Carnegie Mellon University, Pittsburgh, PA.Evgeni Tsivtsivadze, Tapio Pahikkala, Sampo Pyysalo,Jorma Boberg, Aleksandr Mylla?ri, and TapioSalakoski.
2005.
Regularized least-squares for parseranking.
In Proceedings of the 6th InternationalSymposium on Intelligent Data Analysis (IDA?05),Madrid, Spain, pages 464?474.
Springer, Heidelberg.Dmitry Zelenko, Chinatsu Aone, and Jason Tibbets.2004.
Binary integer programming for information ex-traction.
In Proceedings of the ACE Evaluation Meet-ing.40
