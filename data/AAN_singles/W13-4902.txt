Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 12?21,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsLithuanian Dependency Parsing with Rich Morphological FeaturesJurgita Kapoc?iu?te?-Dzikiene?Kaunas University of TechnologyK.
Donelaic?io 73LT-44249 Kaunas, Lithuaniajurgita.k.dz@gmail.comJoakim NivreUppsala UniversityBox 635SE-75126 Uppsala, Swedenjoakim.nivre@lingfil.uu.seAlgis Krupavic?iusKaunas University of TechnologyK.
Donelaic?io 73LT-44249 Kaunas, Lithuaniapvai@ktu.ltAbstractWe present the first statistical dependencyparsing results for Lithuanian, a morpholog-ically rich language in the Baltic branch ofthe Indo-European family.
Using a greedytransition-based parser, we obtain a labeled at-tachment score of 74.7 with gold morphologyand 68.1 with predicted morphology (77.8 and72.8 unlabeled).
We investigate the usefulnessof different features and find that rich morpho-logical features improve parsing accuracy sig-nificantly, by 7.5 percentage points with goldfeatures and 5.6 points with predicted features.As expected, CASE is the single most impor-tant morphological feature, but virtually allavailable features bring some improvement,especially under the gold condition.1 IntroductionDuring the last decades, we have seen a tremendousincrease in the number of syntactic parsers avail-able for different languages, often enabled by thedevelopment of syntactically annotated corpora, ortreebanks.
The added linguistic diversity has high-lighted the fact that typological differences betweenlanguages lead to new challenges, both in parsingtechnology and treebank annotation.
In particu-lar, it has been observed repeatedly that richly in-flected languages, which often also exhibit relativelyfree word order, usually obtain lower parsing accu-racy, especially compared to English (Buchholz andMarsi, 2006; Nivre et al 2007).
This has led toa special interest in parsing methods for such lan-guages (Tsarfaty et al 2010; Tsarfaty et al 2013).In this paper, we contribute to the growing pool ofempirical evidence by presenting the first statisticaldependency parsing results for Lithuanian, a mor-phologically rich Baltic language characterized asone of the most archaic living Indo-European lan-guages (Gimbutas, 1963).Using the newly developed Lithuanian Treebank,we train and evaluate a greedy transition-basedparser and in particular investigate the impact of richmorphological features on parsing accuracy.
Ourexperiments show that virtually all morphologicalfeatures can be beneficial when parsing Lithuanian,which contrasts with many previous studies thathave mainly found a positive impact for isolated fea-tures such as CASE (Eryigit et al 2008).
Using allavailable features, we achieve a labeled attachmentscore of 74.7 with gold morphology (including part-of-speech tags and lemmas) and 68.1 with predictedmorphology.
The corresponding unlabeled attach-ment scores are 77.8 and 72.8, respectively.2 The Lithuanian TreebankThe Lithuanian Treebank was developed by the Cen-ter of Computational Linguistics, Vytautas MagnusUniversity.1 The annotated texts are taken fromthe newspaper domain and thus represent normativeLithuanian language.
The treebank contains 1,566sentences and 24,265 tokens: 19,625 words (9,848distinct) plus 4,640 punctuation marks (12 distinct).Word tokens in the Lithuanian Treebank are mor-1The treebank creation was one of the tasks of the projectInternet Resources: Annotated Corpus of the Lithuanian Lan-guage and Tools of Annotation, implemented in 2007-2008 andfunded by the Lithuanian Science and Studies Foundation.12SBJ OBJ MODIF PRED ATTR DEP ROOT TOTALAbbreviation 6 457 22 485Acronym 31 2 33Adjectival participle 1 28 84 12 125Adjective 1 63 1,104 157 75 1,400Adverbial participle 37 28 3 68Adverb 1,134 193 29 1,356Conjunction 5 1,171 93 1,269Infinitive 6 372 9 139 21 547Interjection 3 6 9Noun 775 1,097 1,314 1,712 1,415 217 6,530Numeral 1 22 158 72 6 259Participle 1 150 430 285 197 1,063Particle 27 78 1 216 36 358Preposition 253 168 630 35 1,086Pronoun 258 170 104 558 424 21 1,535Proper noun 15 1 22 20 1,307 60 1,425Roman number 25 3 28Verb 205 1,844 2,049TOTAL 1,057 1,533 2,856 663 3,992 6,842 2,682 19,625Table 1: Cooccurrence statistics on dependencies (columns) and PoS tags (rows) in the Lithuanian Treebank.phologically and syntactically annotated as follows:?
Syntactic dependencies: 7 different categorieslisted in Table 1 (columns).?
Part-of-Speech (PoS) tags: 18 different cate-gories listed in Table 1 (rows).
These tags sim-ply determine PoS but do not incorporate anyadditional morphological information.?
Morphological features: 12 different categorieslisted with possible values in Table 2.
Thenumber of morphological features assigned to aword varies from 0 (for particles, conjunctions,etc.)
to 9.2?
Lemmas: base form of word, lowercase exceptfor proper names.The syntactic annotation scheme only distin-guishes 5 basic grammatical relations (SBJ, OBJ,PRED, ATTR, MODIF) plus an additional under-specified relation (DEP) for other dependencies be-tween words and a special relation (ROOT) for2For example, the participle esanti (existent) is describedby 8 feature values: CASE: Nominative, GENDER: Feminine,NUMBER: Singular, TENSE: Present, VOICE: Active, RE-FLEX: Non-reflexive, PRONOM: Non-pronominal, ASPECT:Positive.words attached to an (implicit) artificial root node.The dependency structure always forms a tree orig-inating from the root node, but there may be morethan one token attached to the root node.
This hap-pens when a sentence contains several clauses whichdo not share any constituents.
Table 1 gives statis-tics on the different dependency relations and theirdistribution over different PoS tags.Examples of syntactically annotated sentences arepresented in Figure 1 and Figure 2.
All dependencyrelations are represented by arrows pointing fromthe head to the dependent, the labels above indicatethe dependency type.3 For example, as we can seein Figure 1, nerizikuoja (does not risk) is the head ofKas (Who) and this dependency relation has the SBJlabel.
The sentence in Figure 1 contains two clauses(separated by a comma) both containing SBJ depen-dency relations.
The sentence in Figure 2 containsthe main clause Bet s?tai pro medi?
praslinko nedideliss?es?e?lis and the subordinate clause kuriame se?de?jauin which the subject is expressed implicitly (a pro-noun as?
(I) can be inferred from the singular 1st per-son inflection of the verb se?de?jau (sat)).
In Lithua-nian sentences, the subject is very often omitted, andeven the verb can be expressed implicitly.
For exam-3ROOT dependencies are not shown explicitly.13Category Values Frequency Compatible PoS TagsCASE Nominative 3,421 Adjective, Noun, Numeral, Participle, Pronoun, Proper nounGenitive 4,204Dative 445Accusative 1,995Instrumental 795Locative 849Vocative 10GENDER Masculine 7,074 Adjective, Adverbial participle, Noun, Numeral, Participle,Feminine 4,482 Pronoun, Proper nounNeuter 283Appellative 1NUMBER Singular 8,822 Adjective, Adverbial participle, Noun, Numeral, Participle,Plural 4,624 Pronoun, Proper noun, VerbDual 3TENSE Present 1,307 Adjectival participle, Participle, VerbPast occasion 1,352Past 311Past iterative 31Future 123MOOD Indicative 1,950 VerbSubjunctive 87Imperative 12PERSON 1st 281 Verb2nd 413rd 1,727VOICE Active 456 ParticiplePassive 594Gerundive 13REFLEX Reflexive 526 Adjectival participle, Adverbial participle, Infinitive, Noun,Non-reflexive 3,486 Participle, VerbDEGREE Positive 1,712 Adjective, Adverb, Numeral, ParticipleComparative 1,712Superior 1Superlative 94TYPE Cardinal 145 NumeralOrdinal 105Multiple 9PRONOM Pronominal 247 Adjective, Participle, Pronoun, NumeralNon-pronominal 3,056ASPECT Positive 6,206 Adjectival participle, Adjective, Adverbial participle, Adverb,Negative 422 Infinitive, Noun, Participle, Particle, Preposition, VerbTable 2: Morphological categories in the Lithuanian Treebank: possible values, frequencies and compatible PoS tags.ple, in the sentence Jis geras z?mogus (He is a goodman), the copula verb yra (is) is omitted.The possible values of different morphologicalcategories are presented with descriptive statisticsin Table 2.
Given that word order in Lithuaniansentences is relatively free, morphological informa-tion is important to determine dependency relations.For example, an adjective modifying a noun hasto agree in GENDER, NUMBER and CASE, as ingraz?us miestas (beautiful city), where both the ad-jective and the noun are in masculine singular nom-inative.
Verbs agree with their subject in NUMBERand PERSON, as in ju?s vaz?iuojate (you are going) insecond person plural.
Finally, the CASE of a noun14Figure 1: Annotated sentence from the Lithuanian Treebank, consisting of two independent main clauses.
Translation:Who does not risk, that does not drink champagne but does not cry tearfully either.Figure 2: Annotated sentence from the Lithuanian Treebank, consisting of a main clause and a subordinate clause.Translation: But here through the tree in which I sat passed a small shadow.or pronoun is an important indicator of the syntac-tic relation to the verb, such that nominative CASEalmost always implies a SBJ relation.
However, thetransparency of morphological information is lim-ited by syncretism in CASE, NUMBER and GEN-DER.
Thus, the form mamos (mother(s)) can be ei-ther plural nominative or singular genitive; the formmokytojas (teacher(s)) can be either masculine sin-gular nominative or feminine plural accusative.3 Parsing FrameworkWe use the open-source system MaltParser (Nivreet al 2006a) for our parsing experiments with theLithuanian Treebank.
MaltParser is a transition-based dependency parser that performs parsing asgreedy search through a transition system, guidedby a history-based classifier for predicting the nexttransition (Nivre, 2008).
Although more accuratedependency parsers exist these days, MaltParser ap-peared suitable for our experiments for a number ofreasons.
First of all, greedy transition-based parsershave been shown to perform well with relativelysmall amounts of training data (Nivre et al 2006b).Secondly, MaltParser implements a number of dif-ferent transition systems and classifiers that can beexplored and also supports user-defined input for-mats and feature specifications in a flexible way.
Fi-nally, MaltParser has already been applied to a widerange of languages, to which the results can be com-pared.
In particular, MaltParser was used to obtainthe only published dependency parsing results forLatvian, the language most closely related to Lithua-nian (Pretkalnin.
a and Rituma, 2013).In our experiments, we use the latest release ofMaltParser (Version 1.7.2).4 After preliminary ex-periments, we decided to use the arc-eager transitionsystem (Nivre, 2003) with pseudo-projective pars-ing to recover non-projective dependencies (Nivreand Nilsson, 2005) and the LIBLINEAR learningpackage with multiclass SVMs (Fan et al 2008).Table 3 lists the options that were explored in thepreliminary experiments.
We first tested all possiblecombinations of learning method and parsing algo-rithms and then performed a greedy sequential tun-ing of the options related to covered roots, pseudo-projective parsing, and all combinations of allow-root and allow-reduce.In order to use MaltParser on the Lithuanian Tree-bank, we first converted the data to the CoNLL-Xformat,5 treating all morphological feature bundles4Available at http://maltparser.org.5See http://ilk.uvt.nl/conll/#dataformat.15Option ValueLearning method (-l) liblinearParsing algorithm (-a) nivreeagerCovered roots (-pcr) headPseudo-projective parsing (-pp) head+pathAllow root (-nr) trueAllow reduce (-ne) trueTable 3: List of MaltParser options explored in prelimi-nary experiments with best values used in all subsequentexperiments.as a single string and putting it into the FEATS col-umn, which means that there will be one booleanfeature for each unique set of features.
However,in order to study the influence of each individualmorphological feature, we also prepared an appro-priate format where every morphological feature hadits own (atom-valued) column (called CASE, GEN-DER, NUMBER, etc.
), which means that there willbe one boolean feature for each unique feature value,as specified in Table 2.
In the following, we will re-fer to these two versions as Set-FEATS and Atom-FEATS, respectively.
Another choice we had tomake was how to treat punctuation, which is not in-tegrated into the dependency structure in the Lithua-nian Treebank.
To avoid creating spurious non-projective dependencies by attaching them to theroot node, we simply attached all punctuation marksto an adjacent word.6 Therefore, we also excludepunctuation in all evaluation scores.We use five-fold cross-validation on the entiretreebank in all our experiments.
This means thatthe final accuracy estimates obtained after tuningfeatures and other parameters may be overly opti-mistic (in the absence of a held-out test set), butgiven the very limited amount of data available thisseemed like the most reasonable approach.
Weperform experiments under two conditions.
In theGold condition, the input to the parser contains PoStags, lemmas and morphological features taken fromthe manually annotated treebank.
In the Predictedcondition, we instead use input annotations pro-duced by the morphological analyser and lemma-tizer Lemuoklis (Zinkevic?ius, 2000; Daudaravic?iuset al 2007), which also solves morphological dis-6This is automatically handled by the covered roots optionin MaltParser; see Table 3.Category AccuracyPOSTAG 88.1LEMMA 91.1Set-FEATS 78.6Atom-FEATSCASE 87.2GENDER 88.3NUMBER 86.2TENSE 94.1MOOD 95.9PERSON 95.8VOICE 90.2REFLEX 93.3DEGREE 90.3TYPE 80.7PRONOM 89.3ASPECT 93.5Table 4: Accuracy of the morphological analyzer andlemmatizer used in the Predicted condition.ambiguation problems at the sentence level.
Table 4shows the accuracy of this system for the output cat-egories that are relevant both in the Set-FEATS andAtom-FEATS format.4 Parsing Experiments and ResultsIn our first set of experiments, we tuned two featuremodels in the Gold condition:?
Baseline: Starting from the default featuremodel in MaltParser, we used backward andforward feature selection to tune a featuremodel using only features over the FORM,LEMMA, POSTAG and DEPREL fields in theCoNLL-X format (that is, no morphologicalfeatures).
Only one feature was explored ata time, starting with FORM and going on toLEMMA, POSTAG, DEPREL, and conjunc-tions of POSTAG and DEPREL features.
Thebest templates for each feature type were re-tained when moving on to the next feature.?
Baseline+FEATS: Starting from the Baselinemodel, we used forward feature selection totune a feature model that additionally containsfeatures over the FEATS field in the Set-FEATS16Figure 3: The feature models Baseline and Baseline+FEATS.
Rows represent address functions, columns representattribute functions.
Gray cells represent single features, dotted lines connecting cell pairs or lines connecting celltriplets represent conjoined features.
The Baseline model contains only features that do not involve the FEATS column.version, optionally conjoined with POSTAGfeatures.The features included in these two models aredepicted schematically in Figure 3.
The Base-line+FEATS model includes all features, while theBaseline model includes all features except thosethat refer to the FEATS field.
In the Gold condi-tion, the Baseline model achieves a labeled attach-ment score (LAS) of 67.19 and an unlabeled attach-ment score (UAS) of 73.96, while Baseline+FEATSgets 74.20 LAS and 77.40 UAS.
In the Predictedcondition, the corresponding results are 62.47/70.30for Baseline and 68.05/72.78 for Baseline+FEATS.Thus, with the addition of morphological features(all of them together) the Baseline+FEATS modelexceeds the Baseline by 7.01 percentage points forLAS and 3.44 for UAS in the Gold condition and by5.58 percentage points for LAS and 2.48 for UAS inthe Predicted condition.
To determine whether thedifferences are statistically significant we performedMcNemar?s test (McNemar, 1947) with one degreeof freedom.
The test showed the differences in LASand UAS between Baseline and Baseline+FEATSfor both the Gold and Predicted conditions to be sta-tistically significant with p << 0.05.In our second set of experiments, we started fromthe Baseline model and incrementally added mor-phological features in the Atom-FEATS format, onemorphological category at a time, using the samefive feature templates (three single and two con-joined) as for FEATS in the Baseline+FEATS model(see Figure 3).
The order of explored morpho-logical features was random, but only features thatincreased parsing accuracy when added were re-tained when adding the next morphological feature.The LAS results of these experiments are summa-rized in Figure 4 (reporting results in the Gold con-dition) and Figure 5 (in the Predicted condition).We do not present UAS results because they showthe same trend as the LAS metric although shiftedupwards.
In the Gold condition, the best featuremodel is Baseline + CASE + GENDER + NUM-BER + TENSE + DEGREE + VOICE + PERSON+ TYPE, which achieves 74.66 LAS and 77.84UAS and exceeds the Baseline by 7.47 percentagepoints for LAS and 3.88 for UAS (MOOD, RE-FLEX, PRONOM and ASPECT made no improve-ments or even degraded the performance).
In thePredicted condition, the best feature model remainsBaseline+FEATS, but using the Atom-FEATS ver-sion the best results are achieved with Baseline +CASE + GENDER + TENSE + VOICE + PERSON+ REFLEX, which exceeds the Baseline by 5.36 per-centage points for LAS and 2.55 for UAS (NUM-BER, MOOD, DEGREE, REFLEX, PRONOM andASPECT made no improvements or even degraded17the performance).
All these differences are statis-tically significant.
By contrast, the differences be-tween the best models with Atom-FEATS and Set-FEATS are not statistically significant for any metricor condition (with p values in the range 0.35?0.87).5 DiscussionFirst of all, we may conclude that the Baselinefeature model (without morphological information)does not perform very well for a morphologicallyrich language like Lithuanian (see Figure 4 and Fig-ure 5), despite giving high accuracy for morpholog-ically impoverished languages like English.
How-ever, it is likely that the accuracy of the Baselinemodel would be a bit higher for the Lithuanian Tree-bank if PoS tags incorporated some morphologicalinformation as they do, for example, in the EnglishPenn Treebank (Marcus et al 1993).It thus seems that basic PoS tags as well as lem-mas are too general to be beneficial enough forLithuanian.
The simple morphemic word formcould be more useful (even despite the fact thatLithuanian is syncretic language), but the treebankis currently too small, making the data too sparse tocreate a robust model.7 Thus, the effective way ofdealing with unseen words is by incorporating mor-phological information.In the Predicted condition, we always see a dropin accuracy compared to the Gold condition, al-though our case is not exceptional.
For example, theBaseline model has a drop in LAS of 4.72 percent-age points from Gold to Predicted, but this gap couldpossibly be narrowed by retuning the feature modelfor the Predicted condition instead of simply reusingthe model tuned for the Gold condition.
We alsotried training the model on gold annotations for pars-ing predicted annotations, but these produced evenworse results, confirming that it is better to makethe training condition resemble the parsing condi-tion.
Despite noisy information, morphological fea-tures are still very beneficial compared to not usingthem at all (see Figure 5).
Our findings thus agreewith what has been found for Arabic by Marton etal.
(2013) but seem to contradict the results obtained7We tried to reduce data sparseness a little bit by changingall words into lowercase, but the drop in accuracy revealed thatorthographic information is also important for parsing.for Hebrew by Goldberg and Elhadad (2010).As we can see from both curves in Figure 4 andFigure 5, the top contributors are CASE, VOICE,and TENSE, but the CASE feature gives the biggestcontribution to accuracy.
It boosts LAS by 6.51points in the Gold condition and almost 5 points inthe Predicted condition, whereas the contribution ofall the other morphological features is less than 1point (and not statistically significant).
In a con-trol experiment we reversed the order in which mor-phological features are added (presented in Figure 4and Figure 5), adding CASE at the very end.
Inthis case, the addition of all features except case re-sulted in a statistically significant improvement inthe Gold condition (p = 0.001) but not in the Pre-dicted condition (p = 0.24).
However, the contribu-tion of CASE was by far the most important again?
increasing LAS by 5.55 points in the Gold condi-tion and by 4.68 points in the Predicted condition.To further investigate the selection of morphologi-cal features, we also performed a greedy selectionexperiment.
During this experiment CASE was se-lected first, again proving it to be the most influentialfeature.
It was followed by VOICE, MOOD, NUM-BER and DEGREE in the Gold condition and byGENDER, TENSE, PERSON and TYPE in the Pre-dicted condition.
Overall, however, greedy selectiongave worse results than random selection, achieving74.42 LAS and 77.60 UAS in the Gold condition and67.83 LAS and 72.80 UAS in the Predicted condi-tion.To find that CASE is the most important featureis not surprising, as CASE has been shown to bethe most helpful feature for many languages (at leastin the Gold condition).
But whereas few other fea-tures have been shown to help for other languages,in our case the majority of features (8 out of 12 inthe Gold condition) are beneficial for Lithuanian.The so-called agreement features (GENDER, NUM-BER and PERSON) are beneficial for Lithuanian(at least in the Gold condition) as well as for Ara-bic (Marton et al 2013), but not such languages asHindi (Ambati et al 2010) and Hebrew (Goldbergand Elhadad, 2010).
In the Predicted condition, theirpositive impact is marginal at best, possibly becauseNUMBER is very poorly predicted by the morpho-18Figure 4: The contribution of individual morphological features in the Gold condition.
The x axis represents featuremodels incorporating different attributes; the y axis represents LAS.
The horizontal line at 74.20 represents the LASof Baseline+FEATS.Figure 5: The contribution of individual morphological features in the Predicted condition.
The x axis representsfeature models incorporating different attributes; the y axis represents LAS.
The horizontal line at 68.05 represents theLAS of Baseline+FEATS.19logical analyzer.8It is also worth noting that morphological fea-tures have less influence on UAS than LAS, as thegain in UAS over the Baseline is 3-4 percentagepoints lower compared to LAS.
This means thatmorphology is more important for selecting the typeof dependency than for choosing the syntactic head.More precisely, adding morphology improves bothrecall and precision for the labels SBJ and OBJ,which is probably due primarily to the CASE fea-ture.Despite the positive effect of morphological infor-mation, the best LAS achieved is only 74.66 in theGold condition and 68.05 in the Predicted condition.An error analysis shows that 38.0% of all LAS er-rors have an incorrect syntactic head, 12.5% have anincorrect dependency label, and 49.5% have both in-correct.
The most commonly occurring problem isthe ambiguity between DEP and ROOT dependen-cies.For example, in the sentence atsidu?re?
Vokietijoje,lanke?
paskaitas (he got to Germany, attended lec-tures) lanke?
(attended) is the dependent of atsidu?re?
(got), because it is the consecutive action performedby the same subject (the subject is expressed implic-itly and can be identified according the appropriateverb form).
But in the sentence buvo puiku ir mums,ir jam patiko (it was great for us and he enjoyed it)patiko (enjoyed) is not a dependent of buvo (was)but of the root node, because the sentence containstwo separate clauses with their subjects and verbs.9Other common ambiguities are among differenttypes of labels that are expressed by the same mor-phological categories and depends on the context(and the meaning) of the sentence, for example, inthe phrase uz?z?elti augalais (to green with plants),augalais (plants) is a dependent of uz?z?elti (to green)with the OBJ label; in uz?siimti projektais (to en-gage in projects) projektais (projects) is a dependentof uz?siimti (to engage) with the MODIF label; andin pavadinti vardais (to name with names) vardais(names) is a dependent on pavadinti (to name) with8The accuracy is only 86.2%, the lowest of all features.9This type of ambiguity is somewhat artificial, since it arisesfrom the choice to not annotate relations between completeclauses in the Lithuanian Treebank.
We expect that parsingaccuracy would be improved if all interclausal relations wereannotated explicitly.DEP label.
The choice of dependency label in thesecases depends on the semantic role of the modifier,corresponding to the question what in the first case,the question how in the second case, and yet a dif-ferent relation in the third case.
In all these casesmorphology does not help to determine the particu-lar label of the dependency relation.Finally, we note that the results obtained forLithuanian are in the same range as those reportedfor Latvian, another Baltic language.
Using Malt-Parser in 10-fold cross-validation on a data set of2,500 sentences, Pretkalnin.
a and Rituma (2013)achieve an unlabeled attachment score of 74.6 inthe Gold condition and 72.2 in the Predicted condi-tions, to be compared with 77.8 and 72.8 in our ex-periments.
It should be remembered, however, thatthe results are not directly comparable due to differ-ences in annotation schemes.6 ConclusionIn this paper we have presented the first statisti-cal dependency parsing results for Lithuanian.
Us-ing the transition-based system MaltParser, we havedemonstrated experimentally that the role of mor-phology is very important for the Lithuanian lan-guage.
The addition of morphological informationresulted in a gain in attachment scores of 7.5 points(labeled) and 3.9 points (unlabeled) with manuallyvalidated morphology (the Gold condition) and of5.6 points (labeled) and 2.5 points (unlabeled) withautomatically predicted morphology (the Predictedcondition).
In the Gold condition, we achieved thebest results by adding each morphological featureseparately (using the Atom-FEATS representation),but in the Predicted condition adding all features to-gether (using the Set-FEATS representation turnedout to be better).
The most important morphologicalfeature is CASE, followed by VOICE and TENSE.Future work includes a more detailed error anal-ysis for the different models, which could throwfurther light on the impact of different features.
Itcould also be worthwhile to experiment with dif-ferent feature templates for different morphologi-cal categories.
For example, for agreement fea-tures it seems important to conjoin the values of twowords that are candidates for a dependency, whilethis might not be necessary for features like CASE.20However, in order to get a major improvement inparsing accuracy, we probably need larger amountsof syntactically annotated data as well as more con-sistent annotations of interclausal relations.AcknowledgmentsThis research is funded by European Union Struc-tural Funds Project ?Postdoctoral Fellowship Im-plementation in Lithuania?
(No.
VP1-3.1-S?MM-01)and was initiated when the first author was visitingthe Department of Linguistics and Philology at Up-psala University, Sweden.ReferencesBharat Ram Ambati, Samar Husain, Joakim Nivre, andRajeev Sangal.
2010.
On the role of morphosyntacticfeatures in hindi dependency parsing.
In Proceedingsof the NAACL HLT 2010 First Workshop on StatisticalParsing of Morphologically-Rich Languages, SPMRL?10, pages 94?102, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL), pages 149?164.Vidas Daudaravic?ius, Erika Rimkute?, and Andrius Utka.2007.
Morphological annotation of the Lithuaniancorpus.
In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing: Informa-tion Extraction and Enabling Technologies (ACL?07),pages 94?99.Gu?lsen Eryigit, Joakim Nivre, and Kemal Oflazer.
2008.Dependency parsing of Turkish.
Computational Lin-guistics, 34.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for large lin-ear classification.
Journal of Machine Learning Re-search, 9:1871?1874.Marija Gimbutas.
1963.
The Balts.
Thames and Hudson.Yoav Goldberg and Michael Elhadad.
2010.
Easy firstdependency parsing of modern hebrew.
In Proceed-ings of the NAACL HLT 2010 First Workshop on Sta-tistical Parsing of Morphologically-Rich Languages,SPMRL ?10, pages 103?107, Stroudsburg, PA, USA.Association for Computational Linguistics.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.Yuval Marton, Nizar Habash, and Owen Rambow.
2013.Dependency parsing of modern standard arabic withlexical and inflectional features.
Computational Lin-guistics, 39(1):161?194, March.Quinn Michael McNemar.
1947.
Note on the samplingerror of the difference between correlated proportionsor percentages.
Psychometrika, 12(2):153?157.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 99?106.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006a.Maltparser: A data-driven parser-generator for depen-dency parsing.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC), pages 2216?2219.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen Eryig?it,and Svetoslav Marinov.
2006b.
Labeled pseudo-projective dependency parsing with support vector ma-chines.
In Proceedings of the 10th Conference onComputational Natural Language Learning (CoNLL),pages 221?225.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared Task ofEMNLP-CoNLL 2007, pages 915?932.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT), pages 149?160.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34:513?553.Lauma Pretkalnin.
a and Laura Rituma.
2013.
Statisticalsyntactic parsing for Latvian.
In Proceedings of the19th Nordic Conference of Computational Linguistics(NODALIDA 2013), pages 279?289.Reut Tsarfaty, Djame?
Seddah, Yoav Goldberg, San-dra Kuebler, Yannick Versley, Marie Candito, Jen-nifer Foster, Ines Rehbein, and Lamia Tounsi.
2010.Statistical parsing of morphologically rich languages(spmrl) what, how and whither.
In Proceedings of theNAACL HLT 2010 First Workshop on Statistical Pars-ing of Morphologically-Rich Languages, pages 1?12.Reut Tsarfaty, Djame?
Seddah, Sandra Ku?bler, and JoakimNivre.
2013.
Parsing morphologicall rich languages:Introduction to the special issue.
Computational Lin-guistics, 39:15?22.Vytautas Zinkevic?ius.
2000.
Lemuoklis ?
morfologineianalizei [Morphological analysis with Lemuoklis].Gudaitis, L.
(ed.)
Darbai ir dienos, 24:246?273.
(inLithuanian).21
