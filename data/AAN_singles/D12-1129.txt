Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1411?1422, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA New Minimally-Supervised Frameworkfor Domain Word Sense DisambiguationStefano Faralli and Roberto NavigliDipartimento di InformaticaSapienza Universita` di Roma{faralli,navigli}@di.uniroma1.itAbstractWe present a new minimally-supervisedframework for performing domain-drivenWord Sense Disambiguation (WSD).
Glos-saries for several domains are iteratively ac-quired from the Web by means of a boot-strapping technique.
The acquired glosses arethen used as the sense inventory for fully-unsupervised domain WSD.
Our experiments,on new and gold-standard datasets, show thatour wide-coverage framework enables high-performance results on dozens of domains ata coarse and fine-grained level.1 IntroductionDomain information pervades most of the text weread every day.
If we just think of the Web, the vastmajority of its textual content is domain oriented.A case in point is Wikipedia, which provides ency-clopedic coverage for a huge number of knowledgedomains (Medelyan et al 2009), but most blogs,Web sites and newspapers also provide a great dealof information focused on specific areas of knowl-edge.
When it comes to automatic text understand-ing, then, it is crucial to take into account the domainspecificity of a piece of text, so as to perform a fo-cused and as-precise-as-possible analysis which, inits turn, can enable domain-aware applications suchas question answering and information extraction.Domain knowledge also has the potential to improveopen-text applications such as summarization (Cey-lan et al 2010) and machine translation (Foster etal., 2010).Research in Word Sense Disambiguation (Nav-igli, 2009, WSD), the task aimed at the automaticlabeling of text with word senses, has been ori-ented towards domain text understanding for sev-eral years now.
Many approaches have been devised,including the identification of domain-specific pre-dominant senses (McCarthy et al 2007; Lapata andKeller, 2007), the development of domain resources(Magnini and Cavaglia`, 2000; Magnini et al 2002),their application to WSD (Gliozzo et al 2004), andthe effective use of link analysis algorithms such asPersonalized PageRank (Agirre et al 2009; Nav-igli et al 2011).
More recently, semi-supervised ap-proaches to domain WSD have been proposed whichaim at decreasing the amount of supervision neededto carry out the task (Khapra et al 2010).High-performance domain WSD, however, hasbeen hampered by the widespread use of a general-purpose sense inventory, i.e., WordNet (Miller etal., 1990; Fellbaum, 1998).
Unfortunately WordNetdoes not contain many specialized terms, makingit difficult to use it in work on arbitrary special-ized domains.
While Wikipedia has recently beenconsidered a valid alternative (Mihalcea, 2007), itis mainly focused on covering named entities and,strictly speaking, does not contain a formal wide-coverage sense inventory (not even in disambigua-tion pages, which are often incomplete, especiallyin the lexicographic sense).In this paper we provide three main contributions:?
We tackle the above issues by introducinga new framework based on the minimally-supervised acquisition of specialized glossariesfor dozens of domains.1411?
In turn, we use the acquired domain glossariesas a sense inventory for domain WSD.
As a re-sult, we redefine the domain WSD task as oneof picking out the most appropriate gloss (fine-grained setting) or domain (coarse-grained set-ting) from a multi-domain glossary.?
We show that our framework represents a con-siderable departure from the common usageof a general-purpose sense inventory such asWordNet, in that, thanks to the wide cov-erage of domain meanings, it enables high-performance unsupervised WSD on many do-mains in the range of 69-80% F1.Furthermore, our approach can be customized toany set of domains of interest, and new senses, i.e.,glosses, can be added at any time (either manually orautomatically) to the multi-domain sense inventory.2 Related WorkDomain WSD has been the focus of much interestin the last few years.
An important research direc-tion identifies distributionally similar neighbors inraw text as cues for determining the predominantsense of a target word by means of a semantic simi-larity measure (McCarthy et al 2004; Koeling et al2005; McCarthy et al 2007).
Other distributionalmethods include the use of a word-category cooccur-rence matrix, where categories are coarse senses ob-tained from an existing thesaurus (Mohammad andHirst, 2006), and synonym-based word occurrencecounts (Lapata and Keller, 2007).
Domain-informedmethods have also been proposed which make use ofdomain labels as cues for disambiguation purposes(Gliozzo et al 2004).Domain-driven approaches have been shown toobtain the best performance among the unsupervisedalternatives (Strapparava et al 2004), especiallywhen domain kernels are coupled with a syntag-matic one (Gliozzo et al 2005).
However, their per-formance is typically lower than supervised systems.On the other hand, supervised systems fall shortof carrying out high-performance WSD within do-mains, the main reason being the need for retrainingon each new specific knowledge domain.
Nonethe-less, the knowledge acquisition bottleneck can berelieved by means of domain adaptation (Chan andNg, 2006; Chan and Ng, 2007; Agirre and de La-calle, 2009) or by effectively injecting a general-purpose corpus into a smaller domain-specific train-ing set (Khapra et al 2010).However, as mentioned above, most work ondomain WSD uses WordNet as a sense inven-tory.
But even if WordNet senses have been en-riched with topically-distinctive words and concepts(Agirre and de Lacalle, 2004; Cuadros and Rigau,2008), manually-developed domain labels (Magniniet al 2002), and disambiguated semantic relations(Navigli, 2005), the main obstacle of being stuckwith an open-ended fine-grained sense inventory re-mains.
Recent results on the SPORTS and FINANCEgold standard dataset (Koeling et al 2005) showthat domain WSD can achieve accuracy in the 50-60% ballpark when a state-of-the-art algorithm suchas Personalized PageRank is paired with a distribu-tional approach (Agirre et al 2009) or with seman-tic model vectors acquired for many domains (Nav-igli et al 2011).In this paper, we take domain WSD to the nextlevel by proposing a new framework based onthe minimally-supervised acquisition of large do-main sense inventories thanks to which high per-formance can be attained on virtually any domainusing unsupervised algorithms.
Glossary acquisi-tion approaches in the literature are mostly fo-cused on pattern-based definition extraction (Fujiiand Ishikawa, 2000; Hovy et al 2003; Fahmi andBouma, 2006, among others) and lattice-based su-pervised models (Navigli and Velardi, 2010) start-ing from an initial terminology, while we jointlybootstrap the lexicon and the definitions for sev-eral domains with minimal supervision and withoutthe requirement of domain-specific corpora.
To doso, we adapt bootstrapping techniques (Brin, 1998;Agichtein and Gravano, 2000; Pasca et al 2006) tothe novel task of domain glossary acquisition fromthe Web.Approaches to domain sense modeling have al-ready been proposed which go beyond the WordNetsense inventory (Duan and Yates, 2010).
Distinc-tive collocations are extracted from corpora and usedas features to bootstrap a supervised WSD system.Experiments in the biomedical domain show goodperformance, however only in-domain ambiguity isaddressed.
In contrast, our approach tackles cross-1412Figure 1: The bootstrapping process for glossary acquisition.domain ambiguity, by working with virtually any setof domains and minimizing the requirements by har-vesting domain terms and definitions from the Web,bootstrapped using a small number of seeds.The existing approach closest to ours is that ofHuang and Riloff (2010), who devised a bootstrap-ping approach to induce semantic class taggers fromdomain text.
The semantic classes are associatedwith arbitrary NPs and must be established before-hand.
Our objective, instead, is to perform domaindisambiguation at the word level.
To do this, we re-define the domain WSD problem as one of selectingthe most suitable gloss from those available in ourfull-fledged multi-domain glossary.3 A Minimally-Supervised Framework forDomain WSDIn this section we present our new framework forperforming domain WSD.
The framework consistsof two phases: glossary bootstrapping (Section 3.1)and domain WSD (Section 3.2).3.1 Phase 1: Bootstrapping Domain GlossariesThe objective of the first phase is to acquire a multi-domain glossary from the Web with minimal super-vision.
We initially select a set D of domains ofinterest.
For each individual domain d ?
D we startwith an empty set of HTML patterns Pd (i.e., Pd :=?
), used for gloss harvesting.
During this phase weiteratively populate the pattern set by means of sixsteps, described in the next six subsections and de-picted in Figure 1.
The final output of this phase willbe a glossary Gd consisting of domain terms andtheir automatically-harvested glosses.3.1.1 Step 1: Initial seed selectionFirst, given the domain d, we manuallypick out K hypernymy relation seeds Sd ={(t1, h1), .
.
.
, (tK , hK)}, where the pair (ti, hi)contains a domain term ti and its generalization hi(e.g., (firewall, security system)).
The only con-straint we impose is that the selected relations mustbe distinctive for the domain d of interest.
The cho-sen hypernymy relations have to be as topical andrepresentative as possible for the given domain (e.g.,(compiler, computer program) is an appropriate pairfor computer science, while (byte, unit of measure-ment) is not, as it might cause the extraction of sev-eral glossaries of various units and measures).
Notethat this is the only human intervention in the entireglossary acquisition process.We now set the iteration counter k to 1 and startthe first iteration of the process (steps 2-5).
Aftereach iteration k, we keep track of the set of glossesGkd, acquired during iteration k.3.1.2 Step 2: Seed queriesFor each seed pair (ti, hi), we submit the follow-ing three queries to a Web search engine: ?ti?
?hi?glossary1, ?ti?
?hi?
definition, ?ti?
?hi?dictionary and collect the 64 top-ranking resultsfor each query2.
Each resulting page is a candidateglossary for the domain d identified by our relationseeds Sd.3.1.3 Step 3: Pattern and glossary extractionWe initialize the glossary for iteration k as fol-lows: Gkd := ?.
Next, from each resulting page,we harvest all the text snippets s starting withti and ending with hi (e.g., firewall</b> -- a<i>security system), i.e., s = ti .
.
.
hi.
For eachsuch text snippet s, we perform five substeps:a) extraction of the term/gloss separator: we1In what follows, we use the typewriter font for key-words and term/gloss separators.2We use the Google AJAX API, which returns 64 results.1413Term Gloss Hypernym # seeds Gloss scoredynamicpacket filterA firewall facility that monitors the state of connections and uses thisinformation to determine which network packets to allow through the firewallfirewall 2 0.75peripheral Hardware that extends the capabilities of the computer, such as a printer,modem, or scanner.hardware 1 0.83die An integrated circuit chip cut from a finished wafer.
integrated circuit 1 0.75constructor a method used to help create a new object and initialise its data method 0 1.00schema In database terminology, a schema is the organization of the tables, the fields ineach table, and the relationships between fields and tables.database 0 0.78Table 1: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms are underlined).start from ti and move right until we extractthe longest sequence pM of HTML tags andnon-alphanumeric characters, which we call theterm/gloss separator, between ti and the glossarydefinition (e.g., ?</b> --?
between ?firewall?and ?a?
in the above example);b) gloss extraction: we expand the snippet s tothe right of hi in search of the entire gloss ofti, i.e., until we reach a non-formatting tag el-ement (e.g., <span>, <p>, <div>), while ig-noring formatting elements such as <b>, <i>and <a> which are typically included within adefinition sentence.
As a result, we obtain thesequence ti pM glosss(ti) pR, where glosss(ti)is our gloss for seed term ti in snippet s (whichincludes hi by construction) and pR is the non-formatting HTML tag element to the right ofthe extracted gloss.
For example, we extend theabove definition for firewall to: ?a <i>securitysystem</i> for protecting against illegal entryto a local area network.
?.c) pattern instance extraction: we extract the fol-lowing pattern instance:pL ti pM glosss(ti) pR,where pL and pR are, respectively, the left bound-ary of ti and the right boundary of glosss(ti), andpM is the term/gloss separator extracted at step3(a).
The two boundaries pL and pR are obtainedby extracting the longest sequence of HTMLtags and non-alphanumeric characters obtainedwhen moving to the left of ti and the right ofglosss(ti), respectively3.
For the above exam-ple, we extract the following pattern instance:3The minimum and maximum length of both pL and pR areset to 4 and 50 characters, respectively, as a result of a tuningphase described in Section 4.1.pL = ?<p><b>?, ti = ?firewall?, pM = ?</b>--?, glosss(ti) = ?a <i>security system</i>for protecting against illegal entry to a local areanetwork.
?, pR =?</p>?.d) pattern extraction: we generalize the above pat-tern instance to the following pattern:pL ?
pM ?
pR,i.e., we replace ti and glosss(ti) with *.
In theabove example, we obtain the following pattern:<p><b> ?
</b> -- ?
</p>.Finally, we add the generalized pattern to the setof patterns Pd, i.e., we set Pd := Pd ?
{pL ?pM ?
pR}.
We also add the first sentence ofthe retrieved definition glosss(ti) to our glossaryGkd, i.e., Gkd := Gkd ?
{(ti, first(glosss(ti)))},where first(g) returns the first sentence of glossg.e) pattern matching: we look for additional pairsof terms/glosses in the Web page containing thesnippet s by matching the page against the gen-eralized pattern pL ?
pM ?
pR.
We then addto Gkd the new (term, gloss) pairs matching thegeneralized pattern.As a result of this step, we obtain a glossary Gkdfor the terms discovered at iteration k.3.1.4 Step 4: Gloss ranking and filteringImportantly not all the extracted definitions per-tain to the domain of interest.
In order to rank bydomain pertinence the glosses obtained at iterationk, we define the terminology T k?11 of the termsaccumulated up until iteration k ?
1 as follows:T k?11 :=?k?1i=1 Ti, where T i := {t : ?
(t, g) ?
Gid}.1414Gloss DomainMeasures undertaken to return a degraded ecosystem?s functions and values, including its hydrology, plant and.
.
.
BIOLOGYThe renewing or repairing of a natural system so that its functions and qualities are comparable to its original.
.
.
GEOGRAPHYThe reign of Charles II in England.
ROYALTYA goal of criminal sentencing that attempts to make the victim ?whole again.?
LAWThe process and work of improving the degraded quality of the sound or image in terms of video and audio preservation.
MEDIAA process used by radio astronomers to eliminate the smoothing effect observed in radio maps that is caused by.
.
.
PHYSICSTable 2: Examples of glosses harvested for the term restoration.For the base step k = 1, we define T 01 := T1, i.e.,we use the first-iteration terminology itself.
To rankthe glosses, we first transform each acquired glossg to its bag-of-words representation Bag(g), whichcontains all the single- and multi-word expressionsin g. We then score each gloss g by the ratio of do-main terms found in its bag of words:score(g) =|Bag(g) ?
T k?11 ||Bag(g)|.
(1)In Table 1 we show some glosses in the computerscience domain (second column, domain terms areunderlined) together with their score (last column).Next, we use a threshold ?
(tuned on a held-out do-main, described in Section 4.1) to remove from Gkdthose glosses g whose score(g) < ?.3.1.5 Step 5: Seed selection for next iterationWe now aim at selecting the new set of hyper-nymy relation seeds to be used to start the next it-eration.
We perform three substeps:a) Hypernym extraction: for each newly-acquiredterm/gloss pair (t, g) ?
Gkd, we automatically ex-tract a candidate hypernym h from the textualgloss g. To do this we use a simple unsupervisedheuristic which just selects the first term in thegloss.
More sophisticated, supervised approachescould have been used for hypernym extractionfrom glosses (Navigli and Velardi, 2010).
How-ever, note that, for the purposes of our glossaryextraction task, it is not crucial to extract ac-curate hypernyms, but rather to harvest terms hwhich are very likely to occur in the glosses of t.We show an example of hypernym extraction forsome terms in Table 1 (we report the term in col-umn 1, the gloss in column 2 and the hypernymsextracted by our hypernym extraction techniquein column 3).b) (Term, Hypernym)-ranking: we sort all theglosses in Gkd by the number of seed terms foundin each gloss.
In the case of ties (i.e., glosses withthe same number of seed terms), we further sortthe glosses by the score shown in Formula 1.
Weshow the number of seed terms and the scoresfor some glosses in Table 1 (columns 4 and 5,respectively), where seed terms are in bold anddomain terms (i.e., in T k?11 ) are underlined.c) New seed selection: as new seeds we select the(term, hypernym) pairs corresponding to the Ktop-ranking glosses.If k equals the maximum number of iterations, westop.
Else, we increment the iteration counter (i.e.,k := k + 1) and jump to step (2) of our glossarybootstrapping algorithm after replacing Sd with thenew set of seeds.The output of the glossary bootstrapping phase isa domain glossary Gd :=?i=1,...,maxGid, wheremax is the total number of iterations.3.1.6 Step 6: Increasing CoverageGiven the nature of Web domain glossaries onecan rarely find terms and definitions for generalterms (e.g., jurisprudence for the LAW domain).
Inorder to cover this gap, we apply domain filtering(see Section 3.1.4) to all the glosses contained in ageneral-purpose dictionary (we use WordNet).
Wethen add the surviving term/gloss pairs to Gd.3.2 Phase 2: Domain WSDNow that we have acquired a glossary for each do-main in our set D, we can create a multi-domainglossary G := {((t, g), d) : d ?
D, (t, g) ?
Gd}.Our glossary G is thus a set of term/gloss pairsfor many domains.
Note that one pair might indi-vidually belong to more than one domain, as glos-sary bootstrapping is performed separately for eachdomain.
In Table 2 we show an example of the1415glosses acquired for the term restoration.
We ob-serve that 5 out of 6 senses are not available in Word-Net (namely: the BIOLOGY, GEOGRAPHY, LAW, ME-DIA and PHYSICS senses).
Many of them are domain-specific meanings for the general concept of ?theact of restoring?, with the BIOLOGY and GEOGRA-PHY senses being very similar.
However, this is aperfectly acceptable phenomenon as any of the twosenses, i.e., glosses, would be equally valid whendisambiguating a domain text dealing with ecosys-tem restoration.3.2.1 Gloss-driven WSDWe redefine the task of domain WSD as one ofselecting the most suitable gloss, if one exists, foran input term t. For instance, consider the sentence:?He performed the restoration of heavily corruptedimages?.
An appropriate option for this occurrencewould be the MEDIA sense of restoration in Table 2.Our gloss-driven WSD paradigm has the desir-able property of automatically providing two levelsof sense granularity: a domain, coarse-grained level,similar in spirit to Word Domain Disambiguation(Sanfilippo et al 2006), in which the sense inven-tory of a term t is just the set of domains for which tis covered (e.g., BIOLOGY, GEOGRAPHY, ROYALTY, LAW,MEDIA, PHYSICS in the example of Table 2), and afine-grained level, which requires the selection ofthe gloss which best describes the sense denotedby the given word occurrence.
A second desirableproperty of our gloss-driven WSD paradigm is thatit relies on a flexible framework, which allows forthe bootstrapping of new domain glossaries or theexpansion of existing ones.
However, while thesetwo properties ?
i.e., double level of granularity dis-tinctions and flexibility ?
are naturally inherent inthe gloss-driven paradigm, the same cannot be saidfor mainstream open-text WSD in which general-purpose static dictionaries are typically used.In order to evaluate our framework for domainWSD, we propose two fully unsupervised algo-rithms for gloss-driven domain WSD.
Ideally, highperformance could be obtained using state-of-the-artsupervised WSD systems.
However, in order to trainsuch systems, a wide-coverage sense-labeled corpusshould be available for each domain, a heavy taskwhich we leave to future work.
Instead, our objec-tive is to show that high-performance domain WSDcan be enabled with little effort by our framework.3.2.2 Algorithm 1: WSD with PersonalizedPageRankDomain Glossaries as Graphs For each domaind ?
D, we create an undirected graph Nd =(Vd, Ed) as follows: Vd is the set of concepts identi-fied by term/gloss pairs in the domain glossary Gd,i.e., Vd := Gd; Ed is the set of edges between pairsof concepts, where an edge {(t, g), (t?, g?)}
exists ifand only if t?
is such that t?
6= t and t?
occurs in thebag of words of the gloss g of t. In other words, t isconnected to all the domain senses of words used inits definition g.Graph-based WSD Given an input text, for eachdomain d ?
D, we produce its bag of domain con-tent words Cd = {w1, w2, .
.
.
, wn} by perform-ing tokenization, lemmatization and compoundingbased on the lexicon of domain d. Then, given atarget word t, we use Cd \ {t} as the context to dis-ambiguate t within the domain d. In order to carryout domain WSD, i.e., to pick out the most suit-able sense of t across domains, we apply a state-of-the-art graph-based algorithm, namely PersonalizedPageRank (Haveliwala, 2002, PPR), to each domaingraph Nd.
PPR is a variant of the popular PageRankalgorithm (Brin and Page, 1998) in which the damp-ing probability mass is concentrated on a selectednumber of graph nodes, instead of being uniformlydistributed across all nodes.
Specifically, followingAgirre and Soroa (2009) we concentrate the proba-bility mass on the nodes (t?, g?)
?
Vd for which theterm t?
is a context word, i.e., t?
?
Cd.
Next, for eachdomain d ?
D, we run PPR for a given number ofiterations and obtain as output a probability distribu-tion PPVd over the graph nodes.
Finally, we selectthe most suitable gloss of t as follows:SensePPR(t) = argmaxg:?d?D,(t,g)?VdPPVd(t, g) (2)where PPVd(t, g) is the PPR probability for theterm/gloss pair (t, g) and SensePPR(t) contains thebest interpretation of t across all the domains D.3.2.3 Algorithm 2: PPR Boosted with DomainDistribution InformationThe words in a given text do not typically dealwith a single domain.
Instead, they touch different1416ART BIOLOGY BUSINESS CHEMISTRY COMPUTING EDUCATION ENGINEERING ENVIRONMENT FOOD & DRINK GEOGRAPHYGEOLOGY HEALTH HISTORY LANGUAGE LAW LITERATURE MATHS MEDIA METEOROLOGY MUSICPHILOSOPHY PHYSICS POLITICS PSYCHOLOGY RELIGION ROYALTY SPORTS TOURISM VIDEOGAMES WARFARETable 3: List of the 30 domains used in our experiments.COMPUTING FOOD ENVIRONMENT BUSINESSchip circuit timbale dish sewage waste eurobond bonddestructor method brioche bread acid rain rain asset play stockcompiler program macaroni pasta ecosystem system income stock securityhtml language pizza dish air monitoring sampling financial intermediary institutionfirewall security system ice cream dessert global warming temperature derivative financial productremote lan access process pasteurized milk milk fermentation decomposition arbitrage pricing theory economic theoryrelational database tabular database salted butter butter attainment area area banker?s draft bill of exchangeadmin console user interface prosecco wine fugitive dust matter working capital cashTable 4: Hypernymy relation seeds used to bootstrap glossary acquisition in four of the 30 domains.areas of knowledge which are intertwined with eachother within the discourse.
For example, a text deal-ing with VIDEOGAMES will often concern domainssuch as BUSINESS, COMPUTING, SPORTS, etc.
Given aninput text, we can capture its relevance for each do-main by calculating the following domain score:?d =|Cd|?d?
?D |Cd?
|(3)where, as above, Cd is the set of content words fromthe input text which are covered by domain d. Wethus propose a second algorithm which synergisti-cally combines the spreading effect of PPR with thedomain distribution information.
The best sense fora given term t is calculated as follows:SenseDomPPR(t) = argmaxg:?d?D,(t,g)?Vd?dPPVd(t, g)(4)that is, we select as the most suitable gloss for t theone which maximizes the product of its domain rel-evance score by its domain PPVd value.
Note thatthe same gloss can occur in multiple domains andthat it might obtain different scores depending on thedomain.
Again, since the approach is gloss-driven,we do not see this as a problem, but rather as a natu-ral characteristic of our framework.4 Experimental Setup4.1 DomainsWe selected 30 domains starting from the Wikipediafeatured articles4.
We show the domain labels in Ta-4http://en.wikipedia.org/wiki/Wikipedia:Featured articlesTable 5: Statistics on the multi-domain acquired glossary.From the Web From WordNet From both TotalTerms 74,295 83,904 18,313 176,512Glosses 153,920 68,731 596 223,247ble 3 (some labels have been conveniently short-ened, e.g., PHYSICS should read PHYSICS & ASTRON-OMY).
We manually identified 8 hypernym/hyponymseeds for each domain, totalizing 240 seeds.
Weused two criteria for selecting a seed: i) it covers aseparate segment of the domain, and ii) it has to bespecialized enough to avoid ambiguity.
We show theseeds used in four of our domains in Table 4.
Webootstrapped our glossary acquisition technique (cf.Section 3.1) on each domain and performed 5 itera-tions.
For increasing the coverage of domain termswe used WordNet glosses (see Section 3.1.6).
As aresult, we obtained 30 domain glossaries.
We alsokept aside a 31st domain, namely FASHION, whichwe employed for tuning the minimum and maximumlength of both pL and pR in Section 3.1.3 and thethreshold ?
used to filter out non-domain glosses inSection 3.1.4.In Table 5 we show the statistics for the ac-quired multi-domain glossary by distinguishingWeb-derived and WordNet terms and glosses.4.2 Sense InventoryOur sense inventory is given by the 30-domainglossary obtained as a result of our glossary boot-strapping phase.
Overall we collected 176,512 and223,247 distinct terms and glosses, respectively,with an important contribution from both the Web1417and WordNet (see Table 5).
The average num-ber of glosses per term in our inventory is 1.9 (3.6glosses on polysemous terms).
However, note thata monosemous word in our domain sense inventorydoes not necessarily make disambiguation easier,as i) we might have missed other domain-specificsenses, ii) an uncovered, non-domain sense might fita word occurrence (in this case, the domain WSDalgorithms might be (wrongly) biased towards re-turning the only possible choice if a non-zero dis-ambiguation score is calculated for it).In order to determine the suitability of our multi-domain sense inventory, we compared it with thelatest version of WordNet Domains (Magnini et al2002, WND 3.2), a well-known resource whichprovides domain labels for almost 65,000 nomi-nal WordNet synsets (we removed all the synsetstagged with the FACTOTUM label, which indicates nodomain specificity).
Since WND uses about 160finer-grained domain labels, we manually mappedthem to our 30 labels when possible (e.g.
SOCCERand SWIMMING were mapped to SPORTS), totalizing62,100 domain-labeled synsets.We calculated the coverage of our sense inventoryagainst WND at the synset and the sense level, foreach non-FACTOTUM synset.
Given a WordNet synsetS, let d =?s?S ds be the union of the domains dsprovided for each synonym s ?
S by our sense in-ventory (ds = ?
if not present), and let d?
be the do-main labels assigned to S by WND.
A synset is cov-ered if d and d?
intersect.
At the sense level, instead,we consider a synonym s ?
S to be covered if ds andd?
intersect.
Our synset and sense coverage is 65.9%(40,969/62,100) and 63.7% (71,950/112,875), re-spectively.
We also calculated an extra-coverage of203.2% (229,384/112,875), that is the fraction of do-main senses which are not available in WND, butwe are able to provide in our sense inventory (seee.g.
the example in Table 2) over the total number ofsenses in WND.
While coverage and extra-coverageprovide a good indicator of the completeness of oursense inventory, we need to calculate its precision todetermine its correctness.
To do so, we randomlysampled 500 domain glosses of terms for which noWordNet sense was tagged with the same domain inWND.
A manual validation of this sample resultedin an 87.0% (435/500) estimate of the precision ofour sense inventory.4.3 DatasetsA dataset for 30 domains We used the Giga-word corpus (Graff and Cieri, 2003) to extract a 6-paragraph text snippet for each of the 30 domains.As a result, we obtained a domain dataset made upof 180 paragraphs to which we applied tokeniza-tion, lemmatization and compounding, totaling 1432domain content words overall (47.7 content wordsper domain on average).
The average polysemy ofthe words in the dataset was of 9.7 glosses and 4.4domains per word.
Each content word was manu-ally tagged with the most suitable glosses from ourmulti-domain glossary (3.9 glosses, i.e., senses perword were assigned on average).
The annotationtask was performed by two annotators with adjudi-cation.Sports and Finance We also experimented withthe gold standard produced by Koeling et al(2005).The dataset covers two domains: SPORTS and FI-NANCE.
The dataset comprises 41 ambiguous words(with an average polysemy of 6.7 senses), manyof which express different meanings in the two do-mains.
In each domain, and for each word, around100 sentences were sense-annotated with WordNet.Environment Finally, we also carried out an ex-periment on the ENVIRONMENT dataset from theSemeval-2010 domain WSD task (Agirre et al2010).
The dataset includes 1,398 content words (ofwhich 1,032 content nouns) tagged with WordNetsenses.4.4 SystemsWe applied the two algorithms proposed in Section3.2, namely vanilla PPR and domain-boosted PPR.For both versions of PPR we employed UKB, areadily-available implementation of PPR for WSD5,successfully experimented by Agirre and Soroa(2009) and Agirre et al(2009).4.5 BaselinesRandom baseline We compared our algorithmswith the random baseline, which associates a ran-dom gloss among those available for each word oc-currence according to a uniform distribution.5http://ixa2.si.ehu.es/ukb/1418Predominant domain We also compared our al-gorithms with a predominant sense baseline whichassigns to each word occurrence the domain labelwith the highest domain score ?d among those avail-able for the word (cf.
Formula 3).
Note that this isa strong baseline, because it aims at identifying thedomain covered by the majority of terms in the inputtext, however it can disambiguate only at a coarse-grained level, i.e., at the domain level.5 Experimental Results30 domains We ran our WSD systems and thebaselines on our 30-domain dataset, on a sentence-by-sentence basis.
We calculated results at the twolevels of granularity enabled by our WSD frame-work: a coarse-grained setting where systems out-put the most appropriate domain label for each worditem to be disambiguated; a fine-grained settingwhere systems are required to output the most suit-able gloss for the input word.
The results are shownin Table 6.
Domain PPR outperforms Vanilla PPRby some points in precision, recall and F1 in both thecoarse-grained and the fine-grained setting, achiev-ing an F1 around 80% and 69%, respectively (dif-ferences in recall performance are statistically sig-nificant using a ?2 test).
The predominant domainbaseline, available only in the coarse-grained set-ting, lags behind Domain PPR by more than 3 pointsin precision and 2 in recall.
While these differencesare not statistically significant, the variance acrossdomains is much higher, thus suggesting lower reli-ability of the method.These results were obtained in a fully unsuper-vised setting in which no structured knowledge wasprovided, unlike previous applications of PPR toWSD (Agirre et al 2009; Agirre and Soroa, 2009)which relied on the underlying WordNet graph, amanually created resource.
Furthermore, our graphcontains ?noisy?
semantic relations, as we connecteach gloss to all the senses of its gloss words (cf.Section 3.2.2).
Finally, we note that the resultsshown in Table 6 could never have been obtainedwith WordNet.
In fact, drawing on our domain map-ping, we calculated that the correct domain sense isnot in WordNet for about 68% of the words in thedataset.
Instead, the results in Table 6 show that ourframework enables high-performance unsupervisedCoarse-grained Fine-grainedP R F1 P R F1Vanilla PPR 76.7 74.3?
75.5 66.1 64.1?
65.1Domain PPR 81.2 78.7?
79.9 69.7 67.6?
68.6Predom.
domain 77.9 76.8 77.3 - - -Random baseline 42.5 42.5 42.5 44.1 44.1 44.1Table 6: Performance results on the 30-domain dataset(?
differences between the two systems are statisticallysignificant using a ?2 test, p < 0.05).WSD thanks to the wide coverage of domain mean-ings.As regards the random baseline, this performs42.5% and 44.1% in the two settings.
Despite thehigher polysemy of glosses (9.7 glosses vs. 4.4 do-mains per word in the dataset), the performance ishigher in the fine-grained setting because often thereis more than one gloss covering the same meaning ofa domain word.Sports, Finance and Environment For theSPORTS, FINANCE and ENVIRONMENT datasets (cf.
Sec-tion 4.3) we did not have gloss-based sense annota-tions, so we could not perform a fine-grained evalu-ation.
Therefore, we first studied the different sys-tems at a coarse level on the basis of the domain dis-tribution of the senses returned for the word itemsin the dataset.
We show the 3 most frequent domainlabels for each system and each dataset in Figure 2.The figure seems to confirm our results showing Do-main PPR as being more robust than its Vanilla ver-sion.
Next, to get a more accurate evaluation, werandomly sampled 200 sentences from each datasetand manually validated the coarse-grained senses,i.e., domain assignments, output by each system onthis set of sentences.
We remark that several wordsin the datasets did not pertain to the domain of inter-est.
For instance, will and share do not have anysports sense in WordNet, while the same appliesto half and chip for the business domain.
Table 7shows the results of our validation, where a domainoutput by a system was considered correct if a suit-able gloss existed for that domain in our inventory.The results show that our framework enablescoarse-grained recall in the 70-80% ballpark evenon difficult gold standard datasets for which fine-grained recall with WordNet struggles to surpass the50-60% range.
For instance, the best performance1419Vanilla PPR Domain PPR Pred.
dom.
Vanilla PPR Domain PPR Pred.
dom.
Vanilla PPR Domain PPR Pred.
dom.FINANCE SPORTS ENVIRONMENTFigure 2: Frequency of the most common domain labels returned by our 3 systems on standard domain datasets.FINANCE SPORTS ENVIRONMENTP R F1 P R F1 P R F1Vanilla PPR 57.8 56.5 57.1 65.5 63.2 64.3 81.5 77.9 79.7Domain PPR 77.8 76.1 76.9 72.1 71.3 71.7 83.1 79.4 81.2Predom.
domain 80.0 78.3 79.1 72.6 70.1 71.3 72.7 70.6 71.6Table 7: Coarse-grained performance results on gold-standard domain datasets.on the ENVIRONMENT dataset was around 60% re-call (Kulkarni et al 2010) using a semi-supervisedWSD system, trained on the domain.
Similarly, boththe FINANCE and SPORTS datasets are notoriously dif-ficult gold standards on which state-of-the-art recallusing WordNet is lower than 60% (Navigli et al2011).Interestingly, the predominant domain baselineshows a bias towards BUSINESS, thus performing beston the FINANCE dataset.
This is because of the largenumber of terms covered in our domain glossary,and consequently the high overlap with cue wordsin context.
On the other two domains, we observeperformance in line with our 30-domain experiment.6 ConclusionWe have here presented a new framework for do-main Word Sense Disambiguation.
We depart fromthe use of general-purpose sense inventories likeWordNet and propose a bootstrapping approach tothe acquisition of sense inventories for virtually anydomain.
While we selected 30 domains for thisstudy, nothing would prevent us from using a smalleror larger set of these domains, or a set of completelydifferent domains.Our work provides three main contributions:i) we propose a new, flexible approach to glossarybootstrapping which harvests hundreds of thou-sands of term/gloss pairs; the resulting multi-domain glossary is shown to have wide cov-erage across domains and to include a largeamount of terms not available in WordNet;ii) we propose a novel framework for fully-unsupervised domain WSD which uses themulti-domain glossary as our sense inventory;iii) we show that high performance can be achievedby means of simple, unsupervised WSD algo-rithms (around 80% and 69% in a coarse- andfine-grained setting, respectively).Note that our aim here has not been to determinewhich system performs best, but rather to show thata reliable, full-fledged framework for domain WSDcan be set up with minimal supervision.
Addition-ally, our framework can be applied to any languageof interest, provided enough glossaries are availableonline, by simply translating the keywords used forour queries.The multi-domain glossary (and sense inven-tory) together with the seeds used for bootstrappingare available from http://lcl.uniroma1.it/dwsd.AcknowledgmentsThe authors gratefully acknowl-edge the support of the ERC Start-ing Grant MultiJEDI No.
259234.1420ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:extracting relations from large plain-text collections.In Proceedings of the fifth ACM conference on Digi-tal Libraries (DL 2000), pages 85?94, San Antonio,Texas, United States.Eneko Agirre and Oier Lopez de Lacalle.
2004.
Pub-licly available topic signatures for all WordNet nom-inal senses.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation,LREC 2004, pages 1123?1126, Lisbon, Portugal.Eneko Agirre and Oier Lopez de Lacalle.
2009.
Su-pervised domain adaption for WSD.
In Proceedingsof the 12th Conference of the European Chapter ofthe Association for Computational Linguistics, EACL2009, pages 42?50, Athens, Greece.Eneko Agirre and Aitor Soroa.
2009.
Personaliz-ing PageRank for Word Sense Disambiguation.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, EACL 2009, pages 33?41, Athens, Greece.Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.2009.
Knowledge-based WSD on specific domains:performing better than generic supervised WSD.
InProceedings of the 21st International Joint Conferenceon Artificial Intelligence (IJCAI), pages 1501?1506,Pasadena, California.Eneko Agirre, Oier Lo?pez de Lacalle, Christiane Fell-baum, Shu-Kai Hsieh, Maurizio Tesconi, MonicaMonachini, Piek Vossen, and Roxanne Segers.
2010.Semeval-2010 task 17: All-words word sense disam-biguation on a specific domain.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 75?80, Uppsala, Sweden.Sergey Brin and Michael Page.
1998.
Anatomy of alarge-scale hypertextual web search engine.
In Pro-ceedings of the 7th Conference on World Wide Web,WWW 2007, pages 107?117, Brisbane, Australia.Sergey Brin.
1998.
Extracting patterns and relationsfrom the world wide web.
In Proceedings of the In-ternational Workshop on The World Wide Web andDatabases (WebDB 1998), pages 172?183, London,UK.Hakan Ceylan, Rada Mihalcea, Umut O?zertem, ElenaLloret, and Manuel Palomar.
2010.
Quantifying thelimits and success of extractive summarization sys-tems across domains.
In Human Language Technolo-gies: The 2010 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 903?911, Los Angeles, California.Yee Seng Chan and Hwee Tou Ng.
2006.
Estimatingclass priors in domain adaptation for word sense dis-ambiguation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, ACL 2006, pages 89?96, Sydney, Aus-tralia.Yee Seng Chan and Hwee Tou Ng.
2007.
Domain adap-tation with active learning for word sense disambigua-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, ACL 2007,pages 49?56, Prague, Czech Republic.Montse Cuadros and German Rigau.
2008.
KnowNet:building a large net of knowledge from the Web.In Proceedings of the 22nd International Conferenceon Computational Linguistics, COLING 2008, pages161?168, Manchester, U.K.Weisi Duan and Alexander Yates.
2010.
Extractingglosses to disambiguate word senses.
In Proceedingsof Human Language Technologies: Conference of theNorth American Chapter of the Association of Com-putational Linguistics, NAACL 2010, pages 627?635,Los Angeles, California, USA.Ismail Fahmi and Gosse Bouma.
2006.
Learning to iden-tify definitions using syntactic features.
In Proceed-ings of the EACL 2006 workshop on Learning Struc-tured Information in Natural Language Applications,pages 64?71, Trento, Italy.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in statistical machine translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP 2010), pages 451?459, Cambridge, Massachusetts.Atsushi Fujii and Tetsuya Ishikawa.
2000.
Utilizingthe world wide web as an encyclopedia: extractingterm descriptions from semi-structured texts.
In Pro-ceedings of the 38th Annual Meeting on Associationfor Computational Linguistics, ACL 2000, pages 488?495, Hong Kong.Alfio Gliozzo, Carlo Strapparava, and Ido Dagan.
2004.Unsupervised and supervised exploitation of semanticdomains in lexical disambiguation.
Computer Speechand Language, 18(3):275?299.Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.2005.
Domain kernels for word sense disambiguation.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, ACL 2005, pages403?410, Ann Arbor, Michigan.David Graff and Christopher Cieri.
2003.
English Giga-word, LDC2003T05.
In Linguistic Data Consortium,Philadelphia.Taher H. Haveliwala.
2002.
Topic-sensitive PageRank.In Proceedings of 11th International Conference onWorld Wide Web, WWW 2002, pages 517?526, Hon-olulu, Hawaii.1421Eduard Hovy, Andrew Philpot, Judith Klavans, UlrichGermann, and Peter T. Davis.
2003.
Extending meta-data definitions by automatically extracting and orga-nizing glossary definitions.
In Proceedings of the 2003Annual National Conference on Digital GovernmentResearch, pages 1?6, Boston, MA.Ruihong Huang and Ellen Riloff.
2010.
Inducingdomain-specific semantic class taggers from (almost)nothing.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, ACL2010, pages 275?285, Uppsala, Sweden.Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, andPushpak Bhattacharyya.
2010.
All words domainadapted WSD: Finding a middle ground between su-pervision and unsupervision.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1532?1541, Sweden.Rob Koeling, Diana McCarthy, and John Carroll.
2005.Domain-specific sense distributions and predominantsense acquisition.
In Proceedings of the Human Lan-guage Technology Conference and the 2005 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 419?426, Vancouver, B.C., Canada.Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, andPushpak Bhattacharyya.
2010.
CFILT: Resource con-scious approaches for all-words domain specific WSD.In Proceedings of the 5th International Workshop onSemantic Evaluation (Semeval-2010), pages 421?426,Stroudsburg, PA, USA.Mirella Lapata and Frank Keller.
2007.
An informationretrieval approach to sense ranking.
In Proceedings ofHuman Language Technologies 2007: The Conferenceof the North American Chapter of the Association forComputational Linguistics, HLT-NAACL 2007, pages348?355, Rochester, USA.Bernardo Magnini and Gabriela Cavaglia`.
2000.
In-tegrating subject field codes into WordNet.
In Pro-ceedings of the 2nd Conference on Language Re-sources and Evaluation, LREC 2000, pages 1413?1418, Athens, Greece.Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,and Alfio Gliozzo.
2002.
The role of domain informa-tion in word sense disambiguation.
Natural LanguageEngineering, 8:359?373.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant senses in un-tagged text.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,ACL 2004, pages 280?287, Barcelona, Spain.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,33(4):553?590.Olena Medelyan, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning fromWikipedia.
Int.
J. Hum.-Comput.
Stud., 67(9):716?754.Rada Mihalcea.
2007.
Using Wikipedia for automaticWord Sense Disambiguation.
In Proceedings of Hu-man Language Technologies 2007: The Conferenceof the North American Chapter of the Association forComputational Linguistics, HLT-NAACL, pages 196?203, Rochester, N.Y.George A. Miller, R.T. Beckwith, Christiane D. Fell-baum, D. Gross, and K. Miller.
1990.
WordNet: anonline lexical database.
International Journal of Lexi-cography, 3(4):235?244.Saif Mohammad and Graeme Hirst.
2006.
Determiningword sense dominance using a thesaurus.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics,EACL 2006, pages 121?128, Trento, Italy.Roberto Navigli and Paola Velardi.
2010.
LearningWord-Class Lattices for definition and hypernym ex-traction.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, ACL2010, pages 1318?1327, Uppsala, Sweden.Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier de La-calle, and Eneko Agirre.
2011.
Two birds with onestone: Learning semantic models for text categoriza-tion and Word Sense Disambiguation.
In Proceed-ings of the 20th ACM Conference on Information andKnowledge Management, CIKM 2011, pages 2317?2320, Glasgow, UK.Roberto Navigli.
2005.
Semi-automatic extension oflarge-scale linguistic knowledge bases.
In Proceed-ings of the 18th Internationa Florida AI Research Sym-posium Conference (FLAIRS), 15?17 May 2005, pages548?553, Clearwater Beach, Florida.Roberto Navigli.
2009.
Word Sense Disambiguation: Asurvey.
ACM Computing Surveys, 41(2):1?69.Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits, and Alpa Jain.
2006.
Organizing and searchingthe world wide web of facts - step one: the one-millionfact extraction challenge.
In Proceedings of the 21stNational Conference on Artificial intelligence (AAAI2006), pages 1400?1405, Boston, MA.Antonio Sanfilippo, Stephen Tratz, and Michelle Gre-gory.
2006.
Word domain disambiguation via wordsense disambiguation.
In Proceedings of the Hu-man Language Technology Conference of the NAACL,Companion Volume: Short Papers, NAACL 2006,pages 141?144, New York, USA.Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano.2004.
Pattern abstraction and term similarity forWord Sense Disambiguation: IRST at Senseval-3.In Proceedings of the 3rd International Workshop onthe Evaluation of Systems for the Semantic Analy-sis of Text (SENSEVAL-3), pages 229?234, Barcelona,Spain.1422
