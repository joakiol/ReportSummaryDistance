Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1?9,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPMulti-word expressions in textual inference: Much ado about nothing?Marie-Catherine de MarneffeLinguistics DepartmentStanford UniversityStanford, CAmcdm@stanford.eduSebastian Pad?oInstitut f?ur MaschinelleSprachverarbeitungStuttgart University, Germanypado@ims.uni-stuttgart.deChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CAmanning@stanford.eduAbstractMulti-word expressions (MWE) have seen much at-tention from the NLP community.
In this paper, weinvestigate their impact on the recognition of tex-tual entailment (RTE).
Using the manual MicrosoftResearch annotations, we first manually count andclassify MWEs in RTE data.
We find few, mostof which are arguably unlikely to cause processingproblems.
We then consider the impact of MWEs ona current RTE system.
We are unable to confirm thatentailment recognition suffers from wrongly alignedMWEs.
In addition, MWE alignment is difficultto improve, since MWEs are poorly represented instate-of-the-art paraphrase resources, the only avail-able sources for multi-word similarities.
We con-clude that RTE should concentrate on other phe-nomena impacting entailment, and that paraphraseknowledge is best understood as capturing generallexico-syntactic variation.1 IntroductionMulti-word expressions (MWEs) can be defined as?idiosyncratic interpretations that cross word bound-aries?, such as traffic light or kick the bucket.
Calleda ?pain in the neck for NLP?, they have receivedconsiderable attention in recent years and it hasbeen suggested that proper treatment could makea significant difference in various NLP tasks (Saget al, 2002).
The importance attributed to them isalso reflected in a number of workshops (Bond etal., 2003; Tanaka et al, 2004; Moir?on et al, 2006;Gr?egoire et al, 2007).
However, there are few de-tailed breakdowns of the benefits that improvedMWE handling provides to applications.This paper investigates the impact of MWEson the ?recognition of textual entailment?
(RTE)task (Dagan et al, 2006).
Our analysis ties in withthe pivotal question of what types of knowledgeare beneficial for RTE.
A number of papers havesuggested that paraphrase knowledge plays a veryimportant role (Bar-Haim et al, 2005; Marsi et al,2007; Dinu and Wang, 2009).
For example, Bar-Haim et al (2005) conclude: ?Our analysis alsoshows that paraphrases stand out as a dominantcontributor to the entailment task.
?The term ?paraphrase?
is however often con-strued broadly.
In Bar-Haim et al (2005), it refersto the ability of relating lexico-syntactic reformula-tions such as diathesis alternations, passivizations,or symmetrical predicates (X lent his BMW to Y/Yborrowed X?s BMW).
If ?paraphrase?
simply refersto the use of a language?s lexical and syntacticpossibilities to express equivalent meaning in dif-ferent ways, then paraphrases are certainly impor-tant to RTE.
But such a claim means little morethan that RTE can profit from good understand-ing of syntax and semantics.
However, given theabovementioned interest in MWEs, there is anotherpossibility: does success in RTE involve properhandling of MWEs, such as knowing that take apass on is equivalent to aren?t purchasing, or kickedthe bucket to died?
This seems not too far-fetched:Knowledge about MWEs is under-represented inexisting semantic resources like WordNet or dis-tributional thesauri, but should be present in para-phrase resources, which provide similarity judg-ments between phrase pairs, including MWEs.The goal of our study is to investigate the meritsof this second, more precise, hypothesis, measur-ing the impact of MWE processing on RTE.
Inthe absence of a universally accepted definitionof MWEs, we define MWEs in the RTE settingas multi-word alignments, i.e., words that partici-pate in more than one word alignment link betweenpremise and hypothesis:(1)PRE: He died.HYP: He kicked the bucket.The exclusion of MWEs that do not lead to multi-word alignments (i.e., which can be aligned wordby word) is not a significant loss, since these casesare unlikely to cause significant problems for RTE.In addition, an alignment-based approach has theadvantage of generality: Almost all existing RTEmodels align the linguistic material of the premise1and hypothesis and base at least part of their de-cision on properties of this alignment (Burchardtet al, 2007; Hickl and Bensley, 2007; Iftene andBalahur-Dobrescu, 2007; Zanzotto et al, 2007).We proceed in three steps.
First, we analyzethe Microsoft Research (MSR) manual word align-ments (Brockett, 2007) for the RTE2 dataset (Bar-Haim et al, 2006), shedding light on the rela-tionship between alignments and multi-word ex-pressions.
We provide frequency estimates anda coarse-grained classification scheme for multi-word expressions on textual entailment data.
Next,we analyze two widely used types of paraphraseresources with respect to their modeling of MWEs.Finally, we investigate the impact of MWEs andtheir handling on practical entailment recognition.2 Multi-Word Expressions in AlignmentAlmost all textual entailment recognition modelsincorporate an alignment procedure that establishescorrespondences between the premise and the hy-pothesis.
The computation of word alignmentsis usually phrased as an optimization task.
Thesearch space is based on lexical similarities, butusually extended with structural biases in order toobtain alignments with desirable properties, suchas the contiguous alignment of adjacent words, orthe mapping of different source words on to differ-ent target words.
One prominent constraint of theIBM word alignment models (Brown et al, 1993)is functional alignment, that is each target wordis mapped onto at most one source word.
Othermodels produce only one-to-one alignments, whereboth alignment directions must be functional.MWEs that involve many-to-many or one-to-many alignments like Ex.
(1) present a problemfor such constrained word alignment models.
Afunctional alignment model can still handle caseslike Ex.
(1) correctly in one direction (from bottomto top), but not in the other one.
One-to-one align-ments manage neither.
Various workarounds havebeen proposed in the MT literature, such as comput-ing word alignments in both directions and formingthe union or intersection.
Even if an alignment istechnically within the search space, accurate knowl-edge about plausible phrasal matches is necessaryfor it to be assigned a high score and thus identified.3 MWEs in the RTE2 DatasetIn the first part of our study, we estimate the extentto which the inability of aligners to model one-to-CARDINALITYM-to-M 1-to-MDECOM- yes (1) (3)POSABLE?
no (2) (4)OTHER (5), (6), (7)Table 1: MWEs categories and definition criteria(M-to-M: many-to-many; 1-to-M: one-to-many).many and many-to-many correspondences is anissue.
To do so, we use the Microsoft Researchmanual alignments for the RTE2 data.
To date, theMSR data constitutes the only gold standard align-ment corpus publicly available.
Since annotatorswere not constrained to use one-to-one alignments,we assume that the MSR alignments contain multi-word alignments where appropriate.From the MSR data, we extract all multi-wordalignments that fall outside the scope of ?func-tional?
alignments, i.e., alignments of the form?many-to-many?
or ?one-to-many?
(in the directionhypothesis-premise).
We annotate them accordingto the categories defined below.
The MSR datadistinguishes between SURE and POSSIBLE align-ments.
We only take the SURE alignments intoaccount.
While this might mean missing somemulti-word alignments, we found many ?possible?links to be motivated by the desire to obtain a high-coverage alignment, as Ex.
2 shows:(2)PRE: ECB spokeswoman, Regina Schueller, ...HYP: Regina Schueller ...Here, the hypothesis words ?Regina Schueller?
areindividually ?sure?-aligned to the premise words?Regina Schueller?
(solid lines), but are also both?possible?-linked to ?ECB spokeswoman?
(dashedlines).
This ?possible?
alignment can be motivatedon syntactic or referential grounds, but does notindicate a correspondence in meaning (as opposedto reference).3.1 Analysis of Multi-Word ExpressionsTable 1 shows the seven categories we define todistinguish the different types of multi-word align-ments.
We use two main complementary criteriafor our annotation.
The first one is the cardinalityof the alignment: does it involve phrases properon both sides (many-to-many), or just on one side(one-to-many)?
The second one is decomposabil-ity: is it possible to create one or more one-to-onealignments that capture the main semantic contribu-tion of the multi-word alignment?
Our motivation2for introducing this criterion is that even alignersthat are unable to recover the complete MWE havea chance to identify the links crucial for entailmentif the MWE is decomposable (categories (1) and(3)).
This is not possible for the more difficultnon-decomposable categories (2) and (4).
The re-maining categories, (5) to (7), involve auxiliaries,multiple mentions, and named entities, which arenot MWEs in the narrow sense.
We will henceforthuse the term ?true MWEs?
to refer to categories(1)?
(4), as opposed to (5)?
(7).The criteria we use for MWE categorization aredifferent from the ones adopted by Sag et al (2002).Sag et al?s goal is to classify constructions by theirrange of admissible variation, and thus relies heav-ily on syntactic variability.
Since we are more inter-ested in semantic properties, we base our classes onalignment patterns, complemented by semantic de-composability judgments (which reflect the severityof treating MWEs like compositional phrases).
Asmentioned in Section 1, our method misses MWEsaligned with one-to-one links; however, the use ofa one-to-one link by the annotation can be seen asevidence for decomposability.A.
Multiple words on both sides(1) Compositional phrases (CP):Each word in the left phrase can be aligned to oneword in the right phrase, e.g., capital punishment?
death penalty for which capital can be alignedto death and punishment to penalty.
(2) Non-compositional phrases (NCP):There is no simple way to align words between thetwo phrases, such as in poorly represented?
veryfew or illegally entered?
broke into.B.
One word to multiple words(3) Headed multi-word expressions (MWEH):A single word can be aligned with one token ofan MWE: e.g., vote?
cast ballots where ballotscarries enough of the semantics of vote.
(4) Non-headed MWEs (MWENH):The MWE as a whole is necessary to capture themeaning of the single word, which doesn?t alignwell to any individual word of the MWE: e.g., ferry?
passenger vessel.
(5) Multiple mentions (MENTION):These alignments link one word to multiple occur-rences of the same or related word(s) in the text,e.g., military?
forces ... Marines, antibiotics?Status Category RTE2 dev RTE2 testdecomp.
CP 5 0MWEH 40 31non- NCP 6 0decomp.
MWENH 30 29Subtotal: True MWEs 81 60other MENTION 26 48PART 82 54AUX 0 2Total: All MWEs 189 164Table 2: Frequencies of sentences with differentmulti-word alignment categories in MSR data.antibiotics ...
drug.
(6) Parts of named entities (PART):Each element of a named entity is aligned to thewhole named entity: e.g., Shukla?
Nidhi Shukla.This includes the use of acronyms or abbreviationson one side and their spelled-out forms on the otherside, such as U.S.?
United States.
(7) Auxiliaries (AUX):The last category involves the presence of an auxil-iary: e.g., were?
are being.Initially, one of the authors used these categoriesto analyze the complete RTE2 MSR data (dev andtest sets).
The most difficult distinction to drawwas, not surprisingly, the decision between decom-posable multi-word alignments (categories (1) and(3)) and non-decomposable ones (categories (2)and (4)).
To ascertain that a reliable distinctioncan be made, another author did an independentsecond analysis of the instances from categories(1) through (4).
We found moderate inter-annotatoragreement (?
= 0.60), indicating that not all, butmost annotation decisions are uncontroversial.3.2 Distribution of Multi-Word ExpressionsTable 2 shows the distribution in the MSR dataof all alignment categories.
Our evaluation willconcentrate on the ?true MWE?
categories (1) to(4): CP, NCP, MWEH and MWENH.11The OTHER categories (5) to (7) can generally be dealtwith during pre- or post-processing: Auxiliary-verb combi-nations (cat.
7) are usually ?headed?
so that it is sufficient toalign the main verb; multiple occurrences of words referringto the same entity (cat.
5) is an anaphor resolution problem;and named-entity matches (cat.
6) are best solved by using anamed entity recognizer to collapse NEs into a single token.3In RTE2 dev and test, we find only 81 and 60true MWEs, respectively.
Out of the 1600 sentencepairs in the two datasets, 8.2% involve true MWEs(73 in RTE2 dev and 58 in RTE2 test).
On the levelof word alignments, the ratio is even smaller: only1.2% of all SURE alignments involve true MWEs.Furthermore, more than half of them are decom-posable (MWEH/CP).
Some examples from thiscategory are (?heads?
marked in boldface):sue?
file lawsuits againstdiseases?
liver cancerBarbie?
Barbie dollgot?
was awarded withworks?
executive directormilitary?
naval forcesIn particular when light verbs are involved (filelawsuits) or when modification adds just minormeaning aspects (executive director), we argue thatit is sufficient to align the left-hand expression tothe ?head?
in order to decide entailment.Consider, in contrast, these examples from thenon-decomposable categories (MWENH/NCP):politician?
presidential candidatekilled?
lost their livesshipwreck?
sunken shipever?
in its historywidow?
late husbandsexes?
men and womenThese cases span a broad range of linguistic rela-tions from pure associations (widow/late husband)to collective expressions (sexes/men and women).Arguably, in these cases aligning the left-hand wordto any single word on the right can seriously throwoff an entailment recognition system.
However,they are fairly rare, occurring only in 65 out of1600 sentences.3.3 Conclusions from the MSR AnalysisOur analysis has found that 8% of the sentencesin the MSR dataset involve true MWEs.
At theword level, the fraction of true MWEs of all SUREalignment links is just over 1%.Of course, if errors in the alignment of theseMWEs had a high probability to lead to entailmentrecognition errors, MWEs would still constitute amajor factor in determining entailment.
However,we have argued that about half of the true MWEsare decomposable, that is, the part of the alignmentthat is crucial for entailment can be recovered witha one-to-one alignment link that can be identifiedeven by very limited alignment models.This leaves considerably less than 1% of all wordalignments (or ?4% of sentence pairs) where im-perfect MWE alignments are able at all to exert anegative influence on entailment.
However, this isjust an upper bound ?
their impact is by no meansguaranteed.
Thus, our conclusion from the annota-tion study is that we do not expect MWEs to play alarge role in actual entailment recognition.4 MWEs in Paraphrase ResourcesBefore we come to actual experiments on the au-tomatic recognition of MWEs in a practical RTEsystem, we need to consider the prerequisites forthis task.
As mentioned in Section 2, if an RTEsystem is to establish multi-word alignments, it re-quires a knowledge source that provides accuratesemantic similarity judgments for ?many-to-many?alignments (capital punishment ?
death penalty)as well as for ?one-to-many?
alignments (vote ?cast ballots).
Such similarities are not present instandard lexical resources like WordNet or DekangLin?s thesaurus (Lin, 1998).The best class of candidate resources to providewide-coverage of multi-word similarities seems tobe paraphrase resources.
In this section, we ex-amine to what extent two of the most widely usedparaphrase resource types provide supporting ev-idence for the true MWEs in the MSR data.
Wedeliberately use corpus-derived, noisy resources,since we are interested in the real-world (ratherthan idealized) prospects for accurate MWE align-ment.Dependency-based paraphrases.
Lin and Pan-tel (2002)?s DIRT model collects lexicalized de-pendency paths with two slots at either end.
Pathswith similar distributions over slot fillers count asparaphrases, with the quality measured by a mutualinformation-based similarity over the slot fillers.The outcome of their study is the DIRT databasewhich lists paraphrases for around 230,000 depen-dency paths, extracted from about 1 GB of mis-cellaneous newswire text.
We converted the DIRTparaphrases2into a resource of semantic similari-ties between raw text phrases.
We used a heuristicmapping from dependency relations to word or-der, and obtained similarity ratings by rescaling theDIRT paraphrase ratings, which are based on a mu-tual information-based measure of filler similarity,onto the range [0,1].2We thank Patrick Pantel for granting us access to DIRT.4Parallel corpora-based paraphrases.
An alter-native approach to paraphrase acquisition was pro-posed by Bannard and Callison-Burch (2005).
Itexploits the variance inherent in translation to ex-tract paraphrases from bilingual parallel corpora.Concretely, it observes translational relationshipsbetween a source and a target language and pairsup source language phrases with other source lan-guage phrases that translate into the same targetlanguage phrases.
We applied this method tothe large Chinese-English GALE MT evaluationP3/P3.5 corpus (?2 GB text per language, mostlynewswire).
The large number of translations makesit impractical to store all observed paraphrases.
Wetherefore filtered the list of paraphrases against theraw text of the RTE corpora, acquiring the 10 bestparaphrases for around 100,000 two- and three-word phrases.
The MLE conditional probabilitieswere scaled onto [0,1] for each target.Analysis.
We checked the two resources for thepresence of the true MWEs identified in the MSRdata.
We found that overall 34% of the MWEs ap-pear in these resources, with more decomposableMWEs (MWEH/CP) than non-decomposable ones(MWENH/NCP) (42.1% vs. 24.6%).
However, wefind that almost all of the MWEs that are coveredby the paraphrase resources are assigned very lowscores, while erroneous paraphrases (expressionswith clearly different meanings) have higher scores.This is illustrated in Table 3 for the case of poorlyrepresented, which is aligned to very few in oneRTE2 sentence.
This paraphrase is on the list, butwith a lower similarity than unsuitable paraphrasessuch as representatives or good.
This problem iswidespread.
Other examples of low-scoring para-phrases are: another step?
measures, quarantine?
in isolation, punitive measures ?
sanctions,held a position?
served as, or inability?
couldnot.The noise in the rankings means that any align-ment algorithm faces a dilemma: either it uses ahigh threshold and misses valid MWE alignments,or it lowers its threshold and risks constructingincorrect alignments.5 Impact of MWEs on PracticalEntailment RecognitionThis section provides the final step in our study: anevaluation of the impact of MWEs on entailmentrecognition in a current RTE system, and of thebenefits of explicit MWE alignment.
While thepoorly representedrepresented 0.42poorly 0.07rarely 0.06good 0.05representatives 0.04very few 0.04well 0.02representative 0.01Table 3: Paraphrases of ?poorly represented?
withscores (semantic similarities).results of this experiment are not guaranteed totransfer to other RTE system architectures, or tofuture, improved paraphrase resources, it providesa current snapshot of the practical impact of MWEhandling.5.1 The Stanford RTE SystemWe base our experiments on the Stanford RTE sys-tem which uses a staged architecture (MacCartneyet al, 2006).
After the linguistic analysis whichproduces dependency graphs for premise and hy-pothesis, the alignment stage creates links betweenthe nodes of the two dependency trees.
In the infer-ence stage, the system produces roughly 70 featuresfor the aligned premise-hypothesis pair, almost allof which are implementations of ?small linguistictheories?
whose activation indicates lexical, syn-tactic and semantic matches and mismatches ofdifferent types.
The entailment decision is com-puted using a logistic regression on these features.The Stanford system supports the use of dif-ferent aligners without touching the rest of thepipeline.
We compare two aligners: a one-to-onealigner, which cannot construct MWE alignments(UNIQ), and a many-to-many aligner (MANLI)(MacCartney et al, 2008), which can.
Both align-ers use around 10 large-coverage lexical resourcesof semantic similarities, both manually compiledresources (such as WordNet and NomBank) andautomatically induced resources (such as DekangLin?s distributional thesaurus or InfoMap).UNIQ: A one-to-one aligner.
UNIQ constructsan alignment between dependency graphs as thehighest-scoring mapping from each word in thehypothesis to one word in the premise, or to null.Mappings are scored by summing the alignmentscores of all individual word pairs (provided by thelexical resources), plus edge alignment scores that5use the syntactic structure of premise and hypoth-esis to introduce a bias for syntactic parallelism.The large number of possible alignments (expo-nential in the number of hypothesis words) makesexhaustive search intractable.
Instead, UNIQ uses astochastic search based on Gibbs sampling, a well-known Markov Chain Monte Carlo technique (seede Marneffe et al (2007) for details).Since it does not support many-to-many align-ments, the UNIQ aligner cannot make use of themulti-word information present in the paraphraseresources.
To be able to capture some commonMWEs, the Stanford RTE system was originallydesigned with a facility to concatenate MWEspresent in WordNet into a single token (mostlyparticle verbs and collocations, e.g., treat as orforeign minister).
However, we discovered thatWordNet collapsing always has a negative effect.Inspection of the constructed alignments suggeststhat the lexical resources that inform the alignmentprocess do not provide scores for most collapsedtokens (such as wait for), and precision suffers.MANLI: A phrase-to-phrase aligner.
MANLIaims at finding an optimal alignment betweenphrases, defined as contiguous spans of one or mul-tiple words.
MANLI characterizes alignments asedit scripts, sets of edits (substitutions, deletions,and insertions) over phrases.
The quality of anedit script is the sum of the quality of the individ-ual edit steps.
Individual edits are scored using afeature-based scoring function that takes edit typeand size into consideration.3The score for substi-tution edits also includes a lexical similarity scoresimilar to UNIQ, plus potential knowledge aboutthe semantic relatedness of multi-word phrases notexpressible in UNIQ.
Substitution edits also usecontextual features, including a distortion scoreand a matching-neighbors feature.4Due to thedependence between alignment and segmentationdecisions, MANLI uses a simulated annealing strat-egy to traverse the resulting large search space.Even though MANLI is our current best candi-date at recovering MWE alignments, it currentlyhas an important architectural limitation: it workson textual phrases rather than dependency tree frag-ments, and therefore misses all MWEs that are notcontiguous (e.g., due to inserted articles or adver-3Positive weights for all operation types ensure thatMANLI prefers small over large edits where appropriate.4An adaptation of the averaged perceptron algorithm(Collins, 2002) is used to tune the model parameters.micro-avgP R F1UNIQ w/o para 80.4 80.8 80.6MANLI w/o para 77.0 85.5 81.0w/ para 76.7 85.4 80.8Table 4: Evaluation of aligners and resourcesagainst the manual MSR RTE2 test annotations.bials).
This accounts for roughly 9% of the MWEsin RTE2 data.
Other work on RTE has targetedspecifically this observation and has described para-phrases on a dependency level (Marsi et al, 2007;Dinu and Wang, 2009).Setup.
To set the parameters of the two models(i.e., the weights for different lexical resources forUNIQ, and the weights for the edit operation forMANLI), we use the RTE2 development data.
Test-ing takes place on the RTE2 test and RTE4 datasets.For MANLI, we performed this procedure twice,with the paraphrase resources described in Sec-tion 4 once deactivated and once activated.
Weevaluated the output of the Stanford RTE systemboth on the word alignment level, and on the entail-ment decision level.5.2 Evaluation of Alignment AccuracyThe results for evaluating the MANLI and UNIQalignments against the manual alignment links inthe MSR RTE2 test set are given in Table 4.
Wepresent micro-averaged numbers, where each align-ment link counts equally (i.e., longer problems havea larger impact).
The overall difference is not large,but MANLI produces a slightly better alignment.The ability of MANLI to construct many-to-many alignments is reflected in a different positionon the precision/recall curve: the MANLI aligneris less precise than UNIQ, but has a higher recall.Examples for UNIQ and MANLI alignments areshown in Figures 1 and 2.
A comparison of thealignments shows the pattern to be expected fromTable 4: MANLI has a higher recall, but containsoccasional questionable links, such as at President?
President in Figure 1.However, the many-to-many alignments thatMANLI produces do not correspond well to theMWE alignments.
The overall impact of the para-phrase resources is very small, and their additionactually hurts MANLI?s performance slightly.
Amore detailed analysis revealed two contrary trends.On the one hand, the paraphrase resources provide6Aligner w/o para w/ paraUNIQ 63.8 ?MANLI 60.6 60.6Table 5: Entailment recognition accuracy of theStanford system on RTE2 test (two-way task).Aligner w/o para w/ para TAC systemUNIQ 63.3 ?
61.4MANLI 59.0 57.9 57.0Table 6: Entailment recognition accuracy of theStanford system on RTE4 (two-way task).beneficial information, maybe surprisingly, in theform of broad distributional similarities for singlewords that were not available from the standard lex-ical resources (e.g., the alignment ?the company?sletter??
?the company?s certificate?
).On the other hand, MANLI captures not one ofthe true MWEs identified in the MSR data.
It onlyfinds two many-to-many alignments which belongto the CP category: aimed criticism ?
has criti-cised, European currency ?
euro currency.
Wesee this as the practical consequences of our ob-servation from Section 4: The scores in currentparaphrase resources are too noisy to support accu-rate MWE recognition (cf.
Table 3).5.3 Evaluation of Entailment RecognitionWe finally evaluated the performance of the Stan-ford system using UNIQ and MANLI alignmentson the entailment task.
We consider two datasets:RTE2 test, the alignment evaluation dataset, andthe most recent RTE4 dataset, where current num-bers for the Stanford system are available from lastyear?s Text Analysis Conference (TAC).A reasonable conjecture would be that betteralignments translate into better entailment recog-nition.
However, as the results in Tables 5 and 6show, this is not the case.
Overall, UNIQ outper-forms MANLI by several percent accuracy despiteMANLI?s better alignments.
This ?baseline?
differ-ence should not be overinterpreted, since it may besetup-specific: the features computed in the infer-ence stage of the Stanford system were developedmainly with the UNIQ aligner in mind.
A more sig-nificant result is that the integration of paraphraseknowledge in MANLI has no effect on RTE2 test,and even decreases performance on RTE4.The general picture that we observe is thatthere is only a loose coupling between alignmentsand the entailment decision: individual align-ments seldom matter.
This is shown, for exam-ple, by the alignments in Figures 1 and 2.
Eventhough MANLI provides a better overall alignment,UNIQ?s alignment is ?good enough?
for entailmentpurposes.
In Figure 1, the two words UNIQ leavesunaligned are a preposition (at) and a light verb(aimed), both of which are not critical to determinewhether or not the premise entails the hypothesis.This interpretation is supported by another analy-sis, where we tested whether entailments involvingat least one true MWE are more difficult to rec-ognize.
We computed the entailment accuracy forall applicable RTE2 test pairs (7%, 58 sentences).The accuracy on this subset is 62% for the MANLImodel without paraphrases, 64% for the MANLImodel with paraphrases, and 74% for UNIQ.
Thedifferences from the numbers in Table 5 are notsignificant due to the small size of the MWE sam-ple, but we observe that the accuracy on the MWEsubset tends to be higher than on the whole set(rather than lower).
Futhermore, even though we fi-nally see a small beneficial effect of paraphrases onthe MANLI aligner, the UNIQ aligner, which com-pletely ignores MWEs, still performs substantiallybetter.Our conclusion is that wrong entailment deci-sions rarely hinge on wrongly aligned MWEs, atleast with a probabilistic architecture like the Stan-ford system.
Consequently, it suffices to recoverthe most crucial alignment links to predict entail-ment, and the benefits associated with the use ofa more restricted alignment formulation, like theone-to-one alignment formulation of UNIQ, out-weighs those of more powerful alignment models,like MANLI?s phrasal alignments.6 ConclusionsWe have investigated the influence of multi-wordexpressions on the task of recognizing textual en-tailment.
In contrast to the widely held view thatproper treatment of MWEs could bring about a sub-stantial improvement in NLP tasks, we found thatthe importance of MWEs in RTE is rather small.Among the MWEs that we identified in the align-ments, more than half can be captured by one-to-one alignments, and should not pose problems forentailment recognition.Furthermore, we found that the remainingMWEs are rather difficult to model faithfully.
TheMSR MWEs are poorly represented in state-of-the-7FormerSouthAfricanPresidentaimedcriticismatPresidentBushFormerSouthAfricanPresidenthas criticisedPresidentGeorgeBush NULL FormerSouthAfricanPresidentaimedcriticismatPresidentBushFormerSouthAfricanPresidenthas criticisedPresidentGeorgeBush NULLFigure 1: UNIQ (left) and MANLI (right) alignments for problem 483 in RTE2 test.
The rows representthe hypothesis words, and the columns the premise words.FormeSoruetrheueAfuricauminPosoahedeuiaBeBsGgBPderdGeSor mguhnhecaumiaN mgffeudeuiaBeBsGgBPAaiaPeUL??AaiaPe??
?FormeSoruetrheueAfuricauminPosoahedeuiaBeBsGgBPderdGeSor mguhnhecaumiaN mgffeudeuiaBeBsGgBPAaiaPeUL??AaiaPe??
?Figure 2: UNIQ (left) and MANLI (right) alignments for problem 1 in RTE2 test.art lexical resources, and when they are present,scoring issues arise.
Consequently, at least inthe Stanford system, the integration of paraphraseknowledge to enable MWE recognition has madealmost no difference either in terms of alignmentaccuracy nor in entailment accuracy.
Furthermore,it is not the case that entailment recognition accu-racy is worse for sentences with ?true?
MWEs.
Insum, we find that even though capturing and repre-senting MWEs is an interesting problem in itself,MWEs do not seem to be such a pain in the neck ?at least not for textual entailment.Our results may seem to contradict the resultsof many previous RTE studies such as (Bar-Haimet al, 2005) which found paraphrases to make animportant contribution.
However, the beneficial ef-fect of paraphrases found in these studies refers notto an alignment task, but to the ability of relatinglexico-syntactic reformulations such as diathesisalternations or symmetrical predicates (buy/sell).In the Stanford system, this kind of knowledgeis already present in the features of the inferencestage.
Our results should therefore rather be seenas a clarification of the complementary nature ofthe paraphrase and MWE issues.In our opinion, there is much more potentialfor improvement from better estimates of semanticsimilarity.
This is true for phrasal similarity, as ournegative results for multi-word paraphrases show,but also on the single-word level.
The 2% gainin accuracy for the Stanford system here over thereported TAC RTE4 results stems merely from ef-forts to clean up and rescale the lexical resourcesused by the system, and outweighs the effect ofMWEs.
One possible direction of research is con-ditioning semantic similarity on context: Most cur-rent lexical resources characterize similarity at thelemma level, but true similarities of word or phrasepairs are strongly context-dependent: obtain andbe awarded are much better matches in the contextof a degree than in the context of data.AcknowledgmentsWe thank Bill MacCartney for his help with theMANLI aligner, and Michel Galley for the parallelcorpus-based paraphrase resource.
This paper isbased on work funded in part by DARPA throughIBM.
The content does not necessarily reflect theviews of the U.S. Government, and no official en-dorsement should be inferred.8ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics, pages 597?604, AnnArbor, MI.Roy Bar-Haim, Idan Szpecktor, and Oren Glickman.2005.
Definition and analysis of intermediate entail-ment levels.
In Proceedings of the ACL Workshop onEmpirical Modeling of Semantic Equivalence andEntailment, pages 55?60, Ann Arbor, MI.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor.
2006.
The second PASCAL recognisingtextual entailment challenge.
In Proceedings of theSecond PASCAL Challenges Workshop on Recognis-ing Textual Entailment, Venice, Italy.Francis Bond, Anna Korhonen, Diana McCarthy, andAline Villavicencio, editors.
2003.
Proceedings ofthe ACL 2003 workshop on multiword expressions:Analysis, acquisition and treatment.Chris Brockett.
2007.
Aligning the RTE 2006 corpus.Technical Report MSR-TR-2007-77, Microsoft Re-search.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matic of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Aljoscha Burchardt, Nils Reiter, Stefan Thater, andAnette Frank.
2007.
A semantic approach to tex-tual entailment: System evaluation and task analy-sis.
In Proceedings of the ACL-PASCAL Workshopon Textual Entailment and Paraphrasing, pages 10?15, Prague, Czech Republic.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entail-ment challenge.
In J. Quinonero-Candela, I. Da-gan, B. Magnini, and F. d?Alch Buc, editors, Ma-chine Learning Challenges.
Lecture Notes in Com-puter Science, Vol.
3944, pages 177?190.
Springer.Marie-Catherine de Marneffe, Trond Grenager, BillMacCartney, Daniel Cer, Daniel Ramage, Chlo?eKiddon, and Christopher D. Manning.
2007.
Align-ing semantic graphs for textual inference and ma-chine reading.
In Proceedings of the AAAI SpringSymposium.Georgiana Dinu and Rui Wang.
2009.
Inference rulesand their application to recognizing textual entail-ment.
In Proceedings of the 12th Conference of theEuropean Chapter of the ACL (EACL 2009), pages211?219, Athens, Greece.Nicole Gr?egoire, Stefan Evert, and Su Nam Kim, edi-tors.
2007.
Proceedings of the ACL workshop: Abroader perspective on multiword expressions.Andrew Hickl and Jeremy Bensley.
2007.
A discoursecommitment-based framework for recognizing tex-tual entailment.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing,pages 171?176, Prague, Czech Republic.Adrian Iftene and Alexandra Balahur-Dobrescu.
2007.Hypothesis transformation and semantic variabilityrules used in recognizing textual entailment.
InProceedings of the ACL-PASCAL Workshop on Tex-tual Entailment and Paraphrasing, pages 125?130,Prague, Czech Republic.Dekang Lin and Patrick Pantel.
2002.
Discovery ofinference rules for question answering.
Journal ofNatural Language Engineering, 7(4):343?360.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the joint An-nual Meeting of the Association for ComputationalLinguistics and International Conference on Com-putational Linguistics, pages 768?774, Montr?eal,Canada.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Man-ning.
2006.
Learning to recognize features of validtextual entailments.
In Proceedings of NAACL.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment modelfor natural language inference.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, Honolulu, Hawaii.Erwin Marsi, Emiel Krahmer, and Wauter Bosma.2007.
Dependency-based paraphrasing for recogniz-ing textual entailment.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 83?88, Prague, Czech Republic.Begona Villada Moir?on, Aline Villavicencio, DianaMcCarthy, Stefan Evert, and Suzanne Stevenson, ed-itors.
2006.
Proceedings of the ACL Workshop onMultiword Expressions: Identifying and ExploitingUnderlying Properties.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multi-wordexpressions: a pain in the neck for NLP.
In Proceed-ings of CICLing.Takaaki Tanaka, Aline Villavicencio, Francis Bond,and Anna Korhonen, editors.
2004.
Proceedings ofthe second ACL workshop on multiword expressions:Integrating processing.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2007.
Shallow semantic infast textual entailment rule learners.
In Proceed-ings of the ACL-PASCAL Workshop on Textual En-tailment and Paraphrasing, pages 72?77, Prague,Czech Republic.9
