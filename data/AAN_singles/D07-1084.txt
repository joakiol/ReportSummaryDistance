Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
801?809, Prague, June 2007. c?2007 Association for Computational LinguisticsFinding Good Sequential Model Structuresusing Output TransformationsEdward LoperDepartment of Computer & Information ScienceUniversity of Pennsylvania3330 Walnut StreetPhiladelphia, PA 19104edloper@cis.upenn.eduAbstractIn Sequential Viterbi Models, such asHMMs, MEMMs, and Linear Chain CRFs,the type of patterns over output sequencesthat can be learned by the model depend di-rectly on the model?s structure: any patternthat spans more output tags than are coveredby the models?
order will be very difficultto learn.
However, increasing a model?s or-der can lead to an increase in the number ofmodel parameters, making the model moresusceptible to sparse data problems.This paper shows how the notion of outputtransformation can be used to explore a va-riety of alternative model structures.
Us-ing output transformations, we can selec-tively increase the amount of contextual in-formation available for some conditions, butnot for others, thus allowing us to capturelonger-distance consistencies while avoid-ing unnecessary increases to the model?s pa-rameter space.
The appropriate output trans-formation for a given task can be selected byapplying a hill-climbing approach to held-out data.
On the NP Chunking task, ourhill-climbing system finds a model structurethat outperforms both first-order and second-order models with the same input feature set.1 Sequence PredictionA sequence prediction task is a task whose input isa sequence and whose output is a corresponding se-quence.
Examples of sequence prediction tasks in-clude part-of-speech tagging, where a sequence ofwords is mapped to a sequence of part-of-speechtags; and IOB noun phrase chunking, where a se-quence of words is mapped to a sequence of labels,I, O, and B, indicating whether each word is inside achunk, outside a chunk, or at the boundary betweentwo chunks, respectively.In sequence prediction tasks, we are interested infinding the most likely output sequence for a giveninput.
In order to be considered likely, an outputvalue must be consistent with the input value, but itmust also be internally consistent.
For example, inpart-of-speech tagging, the sequence ?preposition-verb?
is highly unlikely; so we should reject an out-put value that contains that sequence, even if theindividual tags are good candidates for describingtheir respective words.2 Sequential Viterbi ModelsThis intuition is captured in many sequence learningmodels, including HiddenMarkovModels (HMMs),Maximum Entropy Markov Models (MEMMs), andLinear Chain Conditional Random Fields (LC-CRFs), by including terms corresponding to piecesof output structure in their scoring functions.
(Shaand Pereira, 2003; Sutton andMcCallum, 2006; Mc-Callum et al, 2000; Alpaydin, 2004)Each of these Sequential Viterbi Models definesa set of scoring functions that evaluate fixed-sizepieces of the output sequence based on fixed-sizepieces of the input sequence.1 The overall score for1For HMMs and MEMMs, the local scores are negative logprobabilities.
For LC-CRFs, the local scores do not have anydirect probabilistic interpretation.801(a)(b)(c)(d)Figure 1: Common Model Structures.
(a) Simplefirst order.
(b) Extended first order.
(c) Simple sec-ond order.
(d) Extended second order.an output value is then computed by summing thescores for all its fixed-size pieces.
Sequence predic-tion models can differ from one another along twodimensions:1.
Model Structure: The set of output pieces andinput pieces for which local scoring functionsare defined.2.
Model Type: The set of parametrized equa-tions used to define those local scoring func-tions, and the procedures used to determinetheir parameters.In this paper, we focus on model structure.
In par-ticular, we are interested in finding a suitable modelstructure for a given task and training corpus.2.1 Common Model StructuresThe model structure used by classical HMMs is the?simple first order?
structure.
This model structuredefines two local scoring functions.
The first scoringfunction evaluates an output value in the context ofthe corresponding input value; and the second scor-ing function evaluates adjacent pairs of output val-ues.
Simple LC-CRFs often extend this structure byadding a third local scoring function, which evalu-ates adjacent pairs of output values in the context ofthe input value corresponding to one of those out-puts.
These model structures are illustrated in Fig-ure 1.Because these first order structures include scor-ing functions for adjacent pairs of output items,they can identify and reject output values that con-tain improbable subsequences of length two.
Forexample, in part-of-speech tagging, the sequence?preposition-verb?
is highly unlikely; and suchmodels will easily learn to reject outputs contain-ing that sequence.
However, it is much more dif-ficult for first order models to identify improbablesubsequences of length three or more.
For example,in English texts, the sequence ?verb-noun-verb?
ismuch less likely than one would predict based juston the subsequences ?verb-noun?
and ?noun-verb.
?But first order models are incapable of learning thatfact.Thus, in order to improve performance, it is of-ten necessary to include scoring functions that spanover larger sequences.
In the ?simple second order?model structure, the local scoring function for adja-cent pairs of output values is replaced with a scoringfunction for each triple of consecutive output values.In extended versions of this structure typically usedby LC-CRFs, scoring functions are also added thatcombine output value triples with an input value.These model structures are illustrated in Figure 1.Similarly, third order and and fourth order modelscan be used to further increase the span over whichscoring functions are defined.Moving to higher order model structures increasesthe distance over which the model can check con-sistency.
However, it also increases the number ofparameters the model must learn, making the modelmore susceptible to sparse data problems.
Thus, theusefulness of a model structure for a given task willdepend on the types of constraints that are importantfor the task itself, and on the size and diversity of thetraining corpus.3 Searching for Good Model StructuresWe can therefore use simple search methods to lookfor a suitable model structure for a given task andtraining corpus.
In particular, we have performedseveral experiments using hill-climbing methods tosearch for an appropriate model structure for a giventask.
In order to apply hill-climbing methods, weneed to define:1.
The search space.
I.e., concrete representationsfor the set of model structures we will consider.2.
A set of operations for moving through thatsearch space.8023.
An evaluation metric.In Section 4, we will define the search space us-ing transformations on output values.
This will al-low us to consider a wide variety of model struc-tures without needing to make any direct modifica-tions to the underlying sequence modelling systems.Output value transformations will be concretely rep-resented using Finite State Transducers (FSTs).
InSection 5, we will define the set of operations formoving through the search space as modification op-erations on FSTs.
For the evaluation metric, we sim-ply train and test the model, using a given modelstructure, on held-out data.4 Representing Model Structure withReversible Output TransformationsThe common model structures described in Sec-tion 2.1 differ from one another in that they exam-ine varying sizes of ?windows?
on the output struc-ture.
Rather than varying the size of the window, wecan achieve the same effect by fixing the windowsize, but transforming the output values.
For exam-ple, consider the effects of transforming the outputvalues by replacing individual output tags with pairsof adjacent output tags:y1, y2, .
.
.
, yt ?
?START, y1?, ?y1, y2?, ?y2, y3?, .
.
.
, ?yt?1, yt?E.g.
:I O O I I B I ?OI IO OO OI II IB BITraining a first order model based on these trans-formed values is equivalent to training a second or-der model based on the original values, since in eachcase the local scoring functions will be based onpairs of adjacent output tags.
Similarly, transform-ing the output values by replacing individual outputtags with triples of adjacent output tags is equivalentto training a third order model based on the originaloutput values.Of course, when we apply a model trained on thistype of transformed output to new inputs, it will gen-erate transformed output values.
Thus, the transfor-mation must be reversible, so that we can map theoutput of the model back to an un-transformed out-put value.This transformational approach has the advantagethat we can explore different model structures us-ing off-the-shelf learners, without modifying them.In particular, we can apply the transformation corre-sponding to a given model structure to the trainingcorpus, and then train the off-the-shelf learner basedon that transformed corpus.
To predict the value fora new input, we simply apply the learned model togenerate a corresponding transformed output value,and then use the inverse transformation to map thatvalue back to an un-transformed value.Output encoding transformations can be used torepresent a large class of model structures, includingcommonly used structures (first order, second order,etc) as well as a number of ?hybrid?
structures thatuse different window sizes depending on the contentof the output tags.Output encoding transformations can also be usedto represent a wide variety of other model struc-tures.
For example, there has been some debateabout the relative merits of different output encod-ings for the chunking task (Tjong Kim Sang andVeenstra, 1999; Tjong Kim Sang, 2000; Shen andSarkar, 2005).
These encodings differ in whetherthey define special tags for the beginning of chunks,for the ends of chunks, and for boundaries betweenchunks.
The output transformation procedure de-scribed here is capable of capturing all of the outputencodings used for chunking.
Thus, this transforma-tional method provides a unified framework for con-sidering both the type of information that should beencoded by individual tags (i.e., the encoding) andthe distance over which that information should beevaluated (i.e., the order of the model).
Under thisframework, we can use simple search procedures tofind an appropriate transformation for a given task.4.1 Representing Transformations as FSTsFinite State Transducers (FSTs) provide a naturalformalism for representing output transformations.FSTs are powerful enough to capture different or-ders of model structure, including hybrid orders; andto capture different output encodings, such as theones considered in (Shen and Sarkar, 2005).
FSTsare efficient, so they add very little overhead.
Fi-nally, there exist standard algorithms for inverting803O:OI:?I:IB:EO:IOO:OI:?I:IB:EO:EOIOE1IOE2I:IB:BIOB1O:OO:OI:BI:IB:BO:OIOB2IOBESO:OI:?I:BB:BO:SOB:BI:IO:SOFigure 2: FSTs for Five Common Chunk Encod-ings.
Each transducer takes an IOB1-encoded stringfor a given output value, and generates the corre-sponding string for the same output value, using anew encoding.
Note that the IOB1 FST is an iden-tity transducer; and note that the transducers thatmake use of the E tag must use -output edges todelay the decision of which tag should be used untilenough information is available.and determinizing FSTs.
24.1.1 Necessary Properties forOutput-Transformation FSTsIn order for an FST to be used to transform outputvalues, it must have the following three properties:1.
The FST?s inverse should be deterministic.3Otherwise, we will be unable to convertthe model?s (transformed) output into an un-transformed output value.2.
The FST should recognize exactly the set ofvalid output values.
If it does not recognizesome valid output value, then it won?t be ableto transform that value.
If it recognizes someinvalid output value, then there exists an trans-formed output value that would map back to aninvalid output value.3.
The FST should not modify the length of theoutput sequence.
Otherwise, it will not be pos-2Note that we are not attempting to learn a transducerthat generates the output values from input values, as is donein e.g.
(Oncina et al, 1993) and (Stolcke and Omohundro,1993).
Rather, we we are interested in finding a transducer fromone output encoding to another output encoding that will bemore amenable to learning by the underlying Sequential ViterbiModel.3Or at least determinizable.sible to align the output values with input val-ues when running the model.In addition, it seems desirable for the FST to havethe following two properties:4.
The FST should be deterministic.
Otherwise, asingle training example?s output could be en-coded in multiple ways, which would maketraining the individual base decision classifiersdifficult.5.
The FST should generate every output string.Otherwise, there would be some possible sys-tem output that we are unable to map back toan un-transformed output.Unfortunately, these two properties, when taken to-gether with the first three, are problematic.
To seewhy, assume an FST with an output alphabet ofsize k. Property (5) requires that all possible out-put strings be generated, and property (1) requiresthat no string is generated for two input strings,so the number of strings generated for an inputof length n must be exactly kn.
But the numberof possible chunkings for an input of length n is3n ?
3n?1 ?
3n?2; and there is no integer k suchthat kn = 3n ?
3n?1 ?
3n?2.4We must therefore relax at least one of these twoproperties.
Relaxing the property 4 (deterministicFSTs) will make training harder; and relaxing theproperty 5 (complete FSTs) will make testing harder.In the experiments presented here, we chose to relaxthe second property.4.1.2 Inverting the TransformationRecall that the motivation behind property 5 isthat we need a way to map any output generatedby the machine learning system back to an un-transformed output value.As an alternative to requiring that the FST gener-ate every output string, we can define an extendedinversion function, that includes the inverted FST,but also generates output values for transformed val-ues that are not generated by the FST.
In particular,4To see why the number of possible chunkings is 3n ?3n?1 ?
3n?2, consider the IOB1 encoding: it generates allchunkings, and is valid for any of the 3n strings except thosethat start with B (of which there are 3n?1) and those that in-clude the sequence OB (of which there are 3n?2).804in cases where the transformed value is not gener-ated by the FST, we can assume that one or moreof the transformed tags was chosen incorrectly; andmake the minimal set of changes to those tags thatresults in a string that is generated by the FST.
Thus,we can compute the optimal un-transformed outputvalue corresponding to each transformed output us-ing the following procedure:1.
Invert the original FST.
I.e., replace each arc?S ?
Q[?
: ?]?
with an arc ?S ?
Q[?
: ?]?.2.
Normalize the FST such that each arc has ex-actly one input symbol.3.
Convert the FST to a weighted FST by as-signing a weight of zero to all arcs.
Thisweighted FST uses non-negative real-valuedweights, and the weight of a path is the sumof the weights of all edges in that path.4.
For each arc ?S ?
Q[x : ?
]?, and each y 6= x,add a new arc ?S ?
Q[y : ?]?
with a weightone.5.
Determinize the resulting FST, using a vari-ant of the algorithm presented in (Mohri,1997).
This determinization algorithm willprune paths that have non-optimal weights.In cases where determinization algorithm hasnot completed by the time it creates 10,000states, the candidate FST is assumed to be non-determinizable, and the original FST is rejectedas a candidate.The resulting FST will accept all sequences oftransformed tags, and will generate for each trans-formed tag the un-transformed output value that isgenerated with the fewest number of ?repairs?
madeto the transformed tags.5 FST Modification OperationsIn order to search the space of output-transformingFSTs, we must define a set of modification oper-ations, that generate a new FST from a previousFST.
In order to support a hill-climbing searchstrategy, these modification operations should makesmall incremental changes to the FSTs.
The selec-tion of appropriate modification operations is impor-tant, since it will significantly impact the efficiencyof the search process.
In this section, I describe theset of FST modification operations that are used forthe experiments described in this paper.
These oper-ations were chosen based our intuitions about whatmodifications would support efficient hill-climbingsearch.
In future experiments, we plan to examinealternative modification operations.5.1 New Output TagThe new output tag operation replaces an arc ?S ?Q[?
: ?x?]?
with an arc ?S ?
Q[?
: ?y?
]?, wherey is a new output tag that is not used anywhere elsein the transducer.
When a single output tag appearson multiple arcs, this operation effectively splits thattag in two.
For example, when applied to the identitytransducer for the IOB1 encoding shown in Figure 2,this operation can be used to distinguish O tags thatfollow other O tags from O tags that follow I or Btags ?
effectively increasing the order of the modelstructure for just O tags.5.2 Specialize Output Tag5The specialize output tag operation is similar to thenew output tag operation, but rather than replacingthe output tag with a new tag, we ?subdivide?
thetag.
When the model is trained, features will be in-cluded for both the subdivided tag and the original(undivided) tag.5.3 Loop UnrollingThe loop unrolling operation acts on a single self-loop arc e at a state S, and makes the followingchanges to the FST:1.
Create a new state S?.2.
For each outgoing arc e1 = ?S ?
Q[?
: ?]?
6=e, add add an arc e2 = ?S?
?
Q[?
: ?]?.
Notethat if e1 was a self-loop arc (i.e., S = Q), thene2 will point from S?
to S.3.
Change the destination of loop arc e from S toS?.By itself, the loop unrolling operation just mod-ifies the structure of the FST, but does not change5This operation requires the use of a model where featuresare defined over (input,output) pairs, such as MEMMs or LC-CRFs.805the actual transduction performed by the FST.
It istherefore always immediately followed by applyingthe new output tag operation or the specialize outputtag operation to the loop arc e.5.4 Copy Tag ForwardThe copy tag forward operation splits an existingstate in two, directing all incoming edges that gen-erate a designated output tag to one copy, and allremaining incoming edges to the other copy.
Theoutgoing edges of these two states are then distin-guished from one another, using either the specializeoutput tag operation (if available) or the new outputtag operation.This modification operation creates separateedges for different output histories, effectively in-creasing the ?window size?
of tags that pass throughthe state.5.5 Copy State ForwardThe copy state forward operation is similar to thecopy tag forward operation; but rather than redirect-ing incoming edges based on what output tags theygenerate, it redirects incoming edges based on whatstate they originate from.
This modification opera-tion allows the FST to encode information about thehistory of states in the transformational FST as partof the model structure.5.6 Copy Feature ForwardThe copy feature forward operation is similar to thecopy tag forward operation; but rather than redirect-ing incoming edges based on what output tags theygenerate, it redirects incoming edges based on a fea-ture of the current input value.
This modification op-eration allows the transformation to subdivide out-put tags based on features of the input value.6 Hill Climbing SystemHaving defined a search space, a set of transforma-tions to explore that space, and an evaluation met-ric, we can use a hill-climbing system to search fora good model structure.
This approach starts witha simple initial FST, and makes incremental localchanges to that FST until a locally optimal FST isfound.
In order to help avoid sub-optimal local max-ima, we use a fixed-size beam search.
To increasethe search speed, we used a 12-machine cluster toevaluate candidate FSTs in parallel.
The hill climb-ing system iteratively performs the following proce-dure:1.
Initialize candidates to be the singleton setcontaining the identity transducer.2.
Repeat ...(a) Generate a new FST, by applying a ran-dom modification operation to a randomlyselected member of the candidatesset.
(b) Evaluate the new FSTs, and test its perfor-mance on the held-out data set.
(This isdone in parallel.
)(c) Once the FST has been evaluated, add it tothe candidates set.
(d) Sort the candidates set by their scoreon the held-out data, and discard all butthe 10 highest-scoring candidates.... until no improvement is made for twentyconsecutive iterations.3.
Return the candidate FST with the highestscore.7 Noun Phrase Chunking ExperimentIn order to test this approach to finding a good modelstructure, we applied our hill-climbing system to thetask of noun phrase chunking.
The base systemwas a Linear Chain CRF, implemented using Mal-let (McCallum, 2002).
The set of features used arelisted in Figure 1.
Training and testing were per-formed using the noun phrase chunking corpus de-scribed in Ramshaw & Marcus (1995) (Ramshawand Marcus, 1995).
A randomly selected 10% of theoriginal training corpus was used as held-out data,to provide feedback to the hill-climbing system.7.1 NP Chunking Experiment: ResultsOver 100 iterations, the hill-climbing system in-creased chunking performance on the held-out datafrom a F-score of 94.93 to an F-score of 95.32.This increase was reflected in an improvement onthe test data from an F-score of 92.48 to an F-score806Feature Descriptionyi The current output tag.yi, wi+n A tuple of the current output tag andthe i + nth word, ?2 ?
n ?
2.yi, wi, wi?1 A tuple of the current output tag, thecurrent word, and the previous word.yi, wi, wi+1 A tuple of the current output tag, thecurrent word, and the next word.yi, ti+n A tuple of the current output tag andthe part of speech tag of the i + nthword, ?2 ?
n ?
2.yi, ti+n,ti+n+1A tuple of the current output tag andthe two consecutive part of speechtags starting at word i + n, ?2 ?n ?
1.yi+n?1, ti+n,ti+n+1A tuple of the current output tag, andthree consecutive part of speech tagscentered on word i+n,?1 ?
n ?
1.Table 1: Feature Set for the CRF NP Chunker.
yiis the ith output tag; wi is the ith word; and ti is thepart-of-speech tag for the ith word.System F1 (Held-out) F1 (Test)Baseline (first order) 94.93 92.48Second order 95.14 92.63Learned structure 95.32 92.80Table 2: Results for NP Chunking Experiment.of 92.80.6 As a point of comparison, a simple sec-ond order model achieves an intermediate F-score of92.63 on the test data.
Thus, the model learned bythe hill-climbing system outperforms both the sim-ple first-order model and the simple second-ordermodel.Figure 3 shows how the scores of FSTs on held-out data changed as the hill-climbing system ran.Figure 4 shows the search tree explored by the hill-climbing system.6The reason that held-out scores are significantly higher thantest scores is that held-out data was taken from the same sec-tions of the original corpus as the training data; but test data wastaken from new sections.
Thus, there was more lexical overlapbetween the training data and the held-out data than betweenthe training data and the testing data....Figure 3: Performance on Heldout Data for NPChunking Experiment.
In this graph, each pointcorresponds to a single transducer generated by thehill-climbing system.
The height of each trans-ducer?s point indicates its score on held-out data.The line indicates the highest score that has beenachieved on the held-out data by any transducer.Figure 4: Hill Climbing Search Tree for NPChunking Experiment This tree shows the ?an-cestry?
of each transducer tried by the hill climb-ing system.
Lighter colors indicate higher scoreson the held-out data.
After one hundred iterations,the five highest scoring transducers were fst047,fst058, fst083, fst102, and fst089.807S0S1S2O:O1O:O1O:O1B:B1B:B2I:I1I+<t[-2;-1]=NNP;,>:I2I-<t[-2;-1]=NNP;,>:I3I+<t[-2;-1]=NNP;,>:I2I-<t[-2;-1]=NNP;,>:I3Figure 5: Final FST.
The highest-scoring FST gen-erated by the hill-climbing algorithm, after a run of100 iterations.
For a discussion of this transducer,see Section 7.1.1.7.1.1 NP Chunking Experiment: The SelectedTransformationFigure 5 shows the FST for the best output trans-formation found after 100 iterations of the hill-climbing algorithm.
Inspection of this FST revealsthat it transforms the original set of three tags (I , O,and B) to six new tags: I1, I2, I3, O, B1, and B2.The first three of these tags are used at the begin-ning of a chunk: I1 is used if the preceding tag wasO; B1 is used if the preceding tag was B; and B2is used if the preceding tag was I .
This is similar toa second order model, in that it records informationabout both the current tag and the previous tag.The next tag, O, is used for all words outside ofchunks.
Thus, the hill-climbing system found thatincreasing the window size used for O chunks doesnot help to learn any useful constraints with neigh-boring tags.Finally, two tags are used for words that are insidea chunk, but not at the beginning of the chunk: I2and I3.
The choice of which tag should be used de-pends on the input feature that tests whether the cur-rent word is a comma, and the previous word was aproper noun (NNP).
At first, this might seem like anodd feature to distinguish.
But note that in the WallStreet Journal, it is quite common for proper nounsto include internal commas; but for other nouns, it isfairly uncommon.
By dividing the I tag in two basedon this feature, the model can use separate distribu-tions for these two cases.
Thus, the model avoidsconflating two contexts that are significantly differ-ent from one another for the task at hand.8 DiscussionSequential Viterbi Models are capable of learning tomodel the probability of local patterns on the out-put structure.
But the distance that these patternscan span is limited by the model?s structure.
Thisdistance can be lengthened by moving to higher or-der model structures, but only at the expense of anincrease in the number of model parameters, alongwith the data sparsity issues that can arise from thatincrease.
Therefore, it makes sense to be more selec-tive about how we extend the model structure.
Usingreversible output transformations, it is possible todefine model structures that extend the reach of themodel only where necessary.
And as we have shownhere, it is possible to find a suitable output transfor-mation for a given task by using simple search pro-cedures.9 AcknowledgementsWe gratefully acknowledge the support of theNational Science Foundation Grant NSF-0415923,Word Sense Disambiguation, the DTO-AQUAINTNBCHC040036 grant under the University ofIllinois subcontract to University of Penn-sylvania 2003-07911-01, NSF-ITR-0325646:Domain-Independent Semantic Interpretation, andDefense Advanced Research Projects Agency(DARPA/IPTO) under the GALE program,DARPA/CMO Contract No.
HR0011-06-C-0022.ReferencesEthem Alpaydin, 2004.
Introduction to Machine Learn-ing, chapter 7.
The MIT Press.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov modelsfor information extraction and segmentation.
In Proc.17th International Conf.
on Machine Learning, pages591?598.
Morgan Kaufmann, San Francisco, CA.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.808Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Linguis-tics, 23(2):269?311.Jose?
Oncina, Pedro Garc?
?a, and Enrique Vidal.
1993.Learning subsequential transducers for pattern recog-nition tasks.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 15:448?458, May.Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In DavidYarowsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora, pages82?94, Somerset, New Jersey.
Association for Compu-tational Linguistics.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedings ofHLT-NAACL, pages 134?141.Hong Shen and Anoop Sarkar.
2005.
Voting betweenmultiple data representations for text chunking.
In Ad-vances in Artificial Intelligence: 18th Conference ofthe Canadian Society for Computational Studies of In-telligence, May.Andreas Stolcke and Stephen Omohundro.
1993.
HiddenMarkov Model induction by Bayesian model merging.In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors,Advances in Neural Information Processing Systems 5.Morgan Kaufman, San Mateo, Ca.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.
MITPress.
To appear.Erik Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In Proceedings of EACL?99,Bergen.
Association for Computational Linguistics.Erik Tjong Kim Sang.
2000.
Noun phrase recognitionby system combination.
In Proceedings of BNAIC,Tilburg, The Netherlands.809
