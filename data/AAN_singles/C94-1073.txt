A Corpus-Based Learning Technique for Building ASelf-Extensible ParserRey-Long Liu and Von-Wun SooDepartment ofComputer ScienceNational Tsing-Hua UniversityHsinChu, Taiwan, 1LO.C.e-mail: sooQ<s.nthu.edu.twAbstractI Iuman intervention and/or training corpora taggedwith various kinds of information were often assumedin many natural language acquisition models.
Thisassumption is a major source of inconsistencies, er-rors, and inefficiency in learning.
In this paper, weexplore the extent to which a parser may extend it-self without relying on extra input from the outsideworld.
A learning technique called SEP is proposedand attached to the parser.
The input to SEP is rawsentences, while the output is the knowledge that ismissing in the parser.
Since parsers and raw sentencesare commonly available and no human intervention isneeded in learning, SEP could make fully automaticlarge-scale acquisition more feasible.Keywords :  fully automatic natural language acqui-sition, self-extensible parser, corpus-based learning1 In t roduct ionIt is commonly believed in many psycholinguisticsstudies \[Pinker, 1984; Wexler & Culicover, 1980\] thatextra input (in addition to raw sentences) is necessaryfor human language learners.
Most existing compu-tational natural language acquisition models also as-sumed various kinds of the extra input (e.g.
semanticassociations \[Siskind, 1990; Webster & Marcus, 1989;Zernik, 1987\] and syntactic structures \[Berwick, 1985;Liu & Soo, 1992a\] of input sentences) and humanintervention (e.g.
information interactively given bythe trainer \[tang & Ilirschman, 1988; Velardi et al,1991\]) during learning.
The preparation of tile ex-tra input and human intervention may often causeinconsistencies, errors, and inefficiency in learning.
Itis often a bottle neck in scaling up natural languageprocessing systems.Therefore, simple syntactic heuristics had beenused to collect the extra input \[Brent, 1993; Sekine,1992\].
However, the information that may be collect-ed by the simple heuristics is limited.
More sophisti-cated processors uch as taggers and parsers had alsobeen tested in collecting the extra input \[Webster &Marcus, 1989; Zernik, 1989\].
ttowever, since learningwas based on the input that may be successfully andunambiguously analyzed by the processors, upgrad-ing tile performance of the processors became a newbottle neck of upgrading the performance of learning.Furthermore, since the heuristics and proccssors areseparated from the learning component, he learningcomponent cannot know what knowledge is actuallymissing in the heuristics and processors.
The learn-ing component might learn the knowledge that theprocessors already have.In this paper, we investigate the robust ways of ac-quiring parsing knowledge without requiring extra in-put and truman intervention.
The input to the systemis raw sentences, while the output is the knowledgethat is missing before learning.
The parser takes anactive role in collecting extra information for learn-ing.
Thus a parser could be self-extensible in thesense that it couhl automatically acquire what it ac-tually lacks without relying on extra information fromthe outside world.
The parsing capability is improvedthrough learning, and in turn, the learning capabilityis improved ue to the improved parsing capability.To achieve that, learning should be triggered whenthe parser fails to analyze input sentences.
In thatcase, however, there might be a large number of hy-potheses to fix the failures.
For example, suppose aparser finds an unknown word when parsing a sen-fence.
In that sentence, tile unknown word may havemany possible syntactic and semantic behaviors (e.g.parts-of-speech and argument structures) which maylead to many different sentence structures.
Therefore,the major challenge of the study is the generation andverification of the hypotheses for missing knowledge.We thus propose a learning technique called SEPthat is attached to the parser.
SEP is triggered whentile parser fails in parsing raw sentences.
In the nexttwo sections we describe SEP's two modules: the hy-pothesis generation module and the hypothesis veri-fication module.
For each training sentence, the twomodules are triggered sequentially.
Experimental re-sults are then presented in section 4.4412 Generat ion  o f  hypothesesSEP generates hypotheses based on the partial resultsof failed parsing, universal inguistic constraints, andthe parser's existing knowledge.2.1 Collecting partial results of pars-ingAs the parscr's knowledge is incomplete for parsing aninput sentence, the bottom-up chart parsing strategyis suitable in collecting partial results of parsing.
Thefailures of parsing are due to the lack of some knowl-edge pieces for grouping the partial results into thetarget constituent (e.g.
a major sentence Smaj).As an example, consider the sentence "taking exer-cises is good for your health".
A parser with a com-plete knowledge base for parsing the sentence mayconstruct a constituent Smaj after deriving all rele-vant constituents such as noun phrases (NPs) and ver-b phrases (VPs).
The grouping of constituents shouldbe based on both syntactic and semantic onstraints.As the parser does not have adequate knowledge forparsing the sentence, failures will occur.
For example,parsing will fail if "take" is an unknown word for theparser.
There might be a large number of hypothe-ses to fix the failure.
For example, "take" may be anoun or a verb.
The learner may even hypothesizethat Smaj may be constructed by matching the se-quence "taking NP VP".
That is, without the help ofadditional information, a huge number of ridiculoushypotheses might be generated.2.2 Generating hypotheses based onuniversal constraints and the pars-er's knowledgeThere are universal inguistic constraints that mayrestrict the forms of missing knowledge.
The X-Bartheory, for example, postulates that any maximal pro-jection (constituent) sbould be composed of at mostthree components: a specifier, an argument struc-ture of the lexical head (i.e.
X-bar), and a modifier\[Chomsky, 1981\].
The set of possible subcategoriza-tion frames of lexical heads had also been set-up inmany studies \[Gazdar, 1985\].
Based on these studies,SEP allows a constituent to have at most three com-ponents.
This constraint (called Three-ComponentsConstrttint ) is incorporated into SEP's hypothesisgeneration process which is composed of two phas-es: the top-down phase and the bottom-up phase.The bottom-up hase is triggered after the top-downphase is completed.2.2.1 The  top-down phaseIn the top-down phase, SEP uses the parser's existingknowledge to perform top-down prediction of missingknowledge.
This phase is for the case in which theparser has knowledge for constructing a constituent(e.g.
Smaj may be constructed by an NP followed bya VP) excepts the knowledge for constructing all theindividual components (e.g.
the NP and the VP) ofthe constituent.The input sentence:Taking exercises is good for your health.Constituents already constructed:NP(2-2), VP(3-4), NP(6-7), PP(5-7), VP(3-7)The top-down phase:Pass 1: Sinai( i -7) :- NP(1-2), VP(3-7).pass 2: Ne0-2) :- VPO-2).Pass 3: VP(1-2) :- ?
?The bottom-up hase:Step 1: Completed constituents: NP(2-2)Step 2: Hypothesis: VP(1-2) :- verb(i-i), NP(2-2)Step a: Checking the Three-Components ConstraintStep 4: If valid, return the hypothesisFig.1.
An example trace of SgP's hypothesis generatimlAs an example, consider "taking exercises is goodfor your health".
Suppose "take" is an unknownword.
Thus parsing fails, and SEP is triggered.
Thereasoning process is illustrated in Fig.1.
In Fig.l, thenumbers denote the starting positions and ending po-sitions of constituents, and the constituents that can-not be constructed in parsing are marked in the bold-face form.
In the top-down phase, SEP first searchesfor the parser's knowledge pieces for constructing thetop level goal constituent Sinai.
Suppose one of theknowledge pieces says that Smaj may be constructedby an NP followed by a VP (Pass 1 in Fig.l).
Sin-ce the VP may be instantiated by "is good for yourhealth", SEP expects there should be an NP from po-sition 1 to position 2.
Thus SEP retrieves all knowl-edge pieces for constructing NPs.
Suppose that oneof them says that a predicate NP may be constructedfrom a VP (Pass 2 in Fig.l).
Thus, SEP attempts toretrieve VP rules.
Ilowew.
'r, since "take" is unknown,no knowledge may be retrieved for constructing theVP (Pass 3 in Fig.l).
The top-down phase thus stopsand the bottom-ul) phase i,~ triggered.2.2.2 The bot tom-up  phaseIn the bottom-up hase, SEP uses the partial resultsof parsing to perform bottom-up prediction of miss-ing knowledge.
Thin phase is for the case in whichthe parser has knowledge for constructing all the in-dividual components (e.g.
an NP and a VP) of aconstituent (e.g.
Sinai) excepts the knowledge forgrouping these components (e.g.
Smaj may be con-structed by the NP followed by the VP).For the above example, SEP has hypothesized thatthere is a VP from position 1 to position 2.
In thebottom-up phase, SEP first observes the partial re-442sults of parsing from position 1 to position 2.
S-ince only the NP "exercises" is constructed in thisrange (Step 1 in Fig.l), the NP is the only possibleargument in the VP.
Thus the hypothesis "VP(1-2):- verb(l- l) ,  NP(2-2)" is generated (Step 2 in Fig.l).Then the Three-Components Constraint is checked(Step 3 in Fig.l).
Since the hypothesis atisfies theconstraint, it may be returned as a hypothetic knowl-edge piece (Step 4 in Fig.l).
If the hypothesis is con-firmed (see section 3), SEP acquires both a categoryand an argument structure of "take".Top-down search(/br conclu,vion part) }?
C3 :- Clp, Cn.TBottom-up search(for condinon parOFig.2.
The top-down phase and the bottom-up haseiv summary, the top-down phase and the bottom-up phase of SEP are complementary to each other.in the top-down phase, SEP hypothesizes the conclu-sion parts of the missing rules, while in the bottom-upphase, SEP hypothesizes the condition parts of themissing rules.
A schematic view of the two phases isillustrated in Fig.2.
The top-down phase starts fromthe constituents not constructed (marked with bold-face circles) to the constituents already constructed.The bottom-up hase starts from the constituents al-ready constructed to the constituents not construct-ed.
The two phases meet at the possible failure pointsof parsing.
A failure point indicates that there is amissing rule in the parser.It should be noted that, in generating hylmthc:ms ,SEP might need to consider several reasoning treessuch as the one in Fig.2.
Tt, is is because there ntightbe several rules for constructing a constituent (e.g.Smaj).
Each rule indicates a path of reasoning, antithus leads to a new reasoning tree.If the parser has only one missing rule for parsinga sentence, the top-down phase and the bottom-upphase will be able to meet at the corresponding fail-ure point.
This is because in that case SEP will haveenough rules (including grammar or lexical rules) toperform top-down traversal and enough constituents(already constructed in parsing) to perform bottom-up traversal.
On the other hand, if the parser hasmore titan one missing rules for parsing a sentence,the top-down phase will stops at the points which thebottom-up hase cannot reach.
IV'or example, if bothCp and Cn in Fig.2 cannot be constructed in parsing,the two phrases cannot meet.
In that case, tt,e ambi-guity space may be too large to resolve.
That is, wemay have:The  Cornp lete-VChen-One-Miss lng Theorem:Supoose the parser lacks only one rule R (either alexical nile or a grammar ule) to completely parse asentence.
Then R will be included in the set of hy-potheses generated by SEP. Ttmt is, if the input sen-tertce is "not difficult" for the parser to learn knowl~edge from it, the missing rule will be generated bySEP.Therefore, in each step of finding an existing rule toperform top-down traversal, SEP selects a rule onlywhen all but one of the components in the conditionpart of the rule are constructed in parsing.
If no suchrules may be found, the top-down phase stops and thebottom-up hase is triggered.
If the input sentence isnot too difficult for the learner, the bottom-up haseand the top-down phase can meet at a failure point,and thus hypotheses of missing knowledge may begenerated.
If the parser has only one missing rule forparsing the input sentence, the missing rule will bein the hypothesis et generated.
This from-simple-to-difficult learning sequence is assumed in many learn-ing models (and human learners as well).
Since rawsentences are commonly available, SEP may easily getsuitable training sentences.3 Veri f icat ion of  hypothesesSEP's hypothesis generation module might generateseveral hypotheses whose validities hould be verifiedbefore assimilated into the parser's knowledge base.The algorithm of the hypothesis verification modnleis outlined in Fig.3.Algorithm: SEP's hypothesis verificationlnlmt: Sets of hypothem~s generatedOntput: A hypothesis of the target missing ruleBegin:For m~ch ypothesis .
'let lI genen~ted for It sentenceIf there is only one hypothesis in II, return the hy-pothesis; (Step 1)Otherwise, increment the frequency of each hypoth-esis h in it by one; (Step 2)Return the hypothesis with the highest frequency;(Step a)End.Fig.3.
'File algorithm of SEWs hypothesis verificationSEWs hytmthesis verification module makes decisionbased on the hypothesis ets generated for trainingsentences (one hypothesis et per training sentence).If there is only one hypothesis in any one of the hy-pothesis ets, SEP returns the hypothesis as the tar-443get missing knowledge (Step 1).
Since only one hy-pothesis in each hypothesis et may be confirmed (i.e.only one rule is missing), other hypotheses in the hy-pothesis ets may be excluded.If more than one hypotheses are generated for atraining sentence, the frequency of each of the hy-potheses is updated (Step 2).
After considering allthe hypothesis ets, SEP returns the hypothesis withthe highest frequency of occurrence (Step 3).
It isobvious that, a hypothesis with a higher frequency ofbeing generated is more likely to be the target missingknowledge.As a hypothetic knowledge piece is confirmed, itshould be annotated with critical syntactic and se-mantic features \[Liu & Soo, 1992a, 1992b\].
A knowl-edge piece without suitable feature annotation willbe too general and thus useless.
For example, sup-pose the learner acquires a knowledge piece for con-structing a predicate NP from a VP (e.g.
"takingexercises").
It must annotate the NP with the fea-ture "NUM=singular ' ;  otherwise the ungramrnaticalsentence "taking exercises are  good for your health"will be accepted as well.
The annotation is based onuniversal linguistic principles such as the universalfeature instantiation principles in generalized phrasestructure grammar (GPSG \[Gazdar, 1985\]).
For ex-ample, the feature "NUM=singular" is annotated byobserving the fact that the verb "is" needs a singularexternal argument.4 ExperimentIn the experiment, SEWs hypothesis generation andverification modules were evaluated.
We used a pars-er whose knowledge base included 2513 lexicon en-tries, 22 grammar ules, and 20 morphological rules.4 .1  Eva luat ion  o f  the  hypothes is  gen-e ra t ion  modu leTo compare the performance of SEP with a learnerthat is provided with extra input, a set of sentencesthat had been tested in previous experiments \[Liu &Soo, 1993a, 1992a, 1992b\] was entered to SEP. Thedifference was that, only raw sentences were enteredto SEP.
There were 165 sentences (about 1700 words)in the corpus.
These sentences were majorly extract-ed from two articles of an English textbook.Among the 165 sentences, 80 sentences were suc-cessfully parsed by the parser, and hence SEP wasnot triggered for them.
SEP was triggered for acquir-ing the missing knowledge for parsing the other 85sentences.
There were totally 202 hypotheses gener-ated.
Thus, on average SEP produced 2.38 (202/85)hypotheses per input sentence that cannot be parsedby the parser.
Furthermore, among the 85 sentencesthat triggered learning, SEP successfully generatedhypotheses from 55 sentences.
That is, the 55 sen-tences were not too difficult for the parser.
From thispoint of view, SEP produced 3.67 (202/55) hypothe-ses per missing rule.
Therefore, SEP needs to collectmore evidences in order to determine a target missingrule among 3.67 hypothetic knowledge pieces.
(1) AP(3-11) :- NP(3-5), S(6-11).
(2) NP(3-11):- NP(3-5), S(6-11).
(3) VP(2-11) :-is(2-2), NP(3-5), S(6-11).
(4) NP(1-6):- S(1-5), NP(6-6).
(5) s(1-~1) :- s(1-5), s(~-n).
(6) Sml~j(1-11) :- S(1-5), S(6-11).Fig.4.
An example of the hypotheses generated by SEPAs an example, consider the sentence "Lead is a softmetal that serves rnany purposes in home" in the cor-pus.
The parser had a missing rule for constructingNPs with relative clauses.
Six hypotheses were gen-erated by SEP They are illustrated in Fig.4 (for theillustrative purpose, syntactic and semantic featuresare omitted).
IIypothesis (1) and (2) were generat-ed based on the existing argument structures of "is";SEP thought that if an AP (Adjective Phrase) oran NP may be constructed from position 3 to po-sition 11, parsing may become successful, lIypothesis(3) was generated for learning a new argument struc-ture of"is".
Ilypothesis (4) was generated when SEPhypothesized "serve" as the main verb of the sen-tence, tlypothesis (5) and (6) were generated sinceSEP thought the parser might need to know a newsentence structure.
Among tim six hypotheses, thetarget missing rule is hypothesis (2), which is quitelikely to have the highest frequency of being generat-ed in learning on a large corpus of sentences.4 .2  Eva luat ion  o f  the  hypothes is  ver i -f i ca t ion  modu leIn the experiment, we evaluated SEWs performancein hypothesis verification.
Training sentences wereextracted from the DJ corpus (Wall Street Journalarticles).
The size of the corpus was about 32 megabytes.
Since SEP only assmned raw sentences asinput, other kinds of information (e.g.
the part-of-speech information) in the DJ corpus were not con-sidered in the experiment.For verifying the hypotheses generated for parsingthe above sentencc "Lead is a soft metal that servesmany purposes in home" (Fig.4), 1000 sentences con-taining "that" were extracted from 19200 sentences(71294 words) of the DJ corpus.
The 1000 sentenceswere fed into tile parser.
As described above, sincethe knowledge for the NPs with relative clauses wasmissing, parsing failed and SEP was triggered for eachsentence.
In many cases, SEP could not generate anyhypotheses, ince there were many unknown words for444the parser.
As hypotheses could be generated, SEPupdated the frequencies of tile hypotheses (Step 2 inFig.3).
As learning proceeded to the sentence:... SEC, an agency that  covets i ts independence,  ...SEP generated only one hypothesis "NP :- NP, S".This was because, "an agency ..." was unambigu-ously segmented by commas and expected to be anoun phrase.
Since only one hypothesis was generat-ed, tl, e hypothesis was returned as the target missingknowledge (Step 1 in Fig.a).
~l'hat is, SEP concludedthat hypothesis (2) in Fig.4 was needed (and miss-ing in tile parser) for parsing both sentences.
There-fore, although the sentences that are too difficult forthe parser are skipped, SEP may still tind suitablesentences to learn since raw sentences are commonlyavailable.The current version of SEP may be extended in thefollowing two ways:?
Acquisition of movement constructions: Movementconstructions cannot he learned using SEP. For exam-ple, the movement construction i  the sentence "IIerecomes the dog" cannot be learned, since it cannot bedetected if only raw sentences are entered to the learn-er \[Liu& Soo, 1992b\].?
The use of more universal inguistic constraints:More suitable universal inguistic constraints may beused to both reduce the number and promote thequality of the hypotheses generated.
For example,hypothesis (1) in Fig.4 may be filtered out by consult-ing the fact that the head component of a constituentshould be included in the condition parts of the rulesfor constructing the constituent.
Since neither NPnor S may be the head of AP, the hypothesis may bediscarded.
As another example, according to X-Bartheory, each constituent is composed of a specifier,an argument structure, and a moditier.
The possi-ble syntactic categories (e.g.
NP, PP, VP, and AP)of the components of each kind of constituents havebeen identified in previous linguistics tudies (e.g.
adeterminer may be the specifier of art NP).
It is obvi-ous that, if SEP generates hypotheses hy consideringthe universal constraints, the quality of the generatedhypotheses may be promoted.5 Re la ted  workPrevious natural language acquisition models couhtbe characterized as interactive acquisition \[tang &tIirschman, 1988; Liu & Soo, I993a, 1993b; Velardi ctal., 1991\], corpus-based acquisition \[Brent, 1993; 3a-cobs & Zernik, 1988; Zernik, 1989\], dictionary-basedacquisition \[Montemagni & Vanderwende, 1992; San-filippo & Pozanski, 1992\], statistics-based acquisition\[Smadja, 1991; Sekine et al 1992\], and connectionist-based acquisition \[Paisal & Kwasny, 1990; McClelland& Kawamoto, 1986\].Our motivation in the study is to provide the parserwith the capability of extending itself without usingextra input and intervention fi'om tile outside worht.From this point of view, interactive acquisition andconnectionist-based acquisition will have difficultiesin resolving tile problems of inconsistencies, errors,and inefficiency of learning, since tttey required theinformation encoded by the trainer.SEP could be characterized as corpus-based acqui-sition.
'\]'he point here is that S~P only assumesraw sentences as the input.
From the point of view,corpus-based acquisition and statistics-based acqui-sition that require pre-processed ata will have di fficulties in getting adequate extra input, since rawsentences are nmch more commonly available thanpre-processed data.SEP collects observations from the parser whichmay grow through learning.
From this point ofview, collecting information using simple but non-extensible heuristics \[Brent, 1.99:1\] might miss manyopportunities of learning (due to the inadeqnacy ofthe collected information), although raw sentences areassumed as the major input in that study as well.In addition to grammar and lexical informationfrom the parser, other types of useful informationslay include contextual, conceptual, and association-al semantic information \[SNkind, 1990;//aeons & Zer-I, ik, 1988; Wcbster & Marcus, 1989; Zernik, 1987\], al-though they are much more difliculL to collect in prac-tice (especially when the parser is incomplete and tileinput sentences are noisy in many real-world applica-tions).
'1'his paper explores the feasibility of allow-tug a parser (either preliminary or sophisticated) toextend itself with available raw sentences and infor-mation from itself.
Noise-tolerant learning is imple-mented by allowing the learner to acquire knowledgehased on a large number (rather than one) of obser-vations.
The practical learning method may makeeftk:ien\[;, fully-antomatic, and large-scale acquisitionntore approachable.Parsers (or taggers) had been used in many pre-vious models as wel\].
They were pre-processors oflearning \[Zernik & Jacobs, 199(I; Pustcjovsky et al,1199;t; Montemagni & Vanderwende, 19921 Sanfilippo& Pozanski, 1992; Zernik, 1989\] or post-processors oflearning \[Smadja, \]991\].
In those models, the proces-sors were assumed to be complete and thus separatedfrom the h;arning components.
Learning was basedon the "success"  of syntactic processing.
'1'o extendthe capability of the processors, learning should betriggered when the processors fail to analyze inputsentences.
This is the reason why SEP atternpts tolearn knowledge when the parser fails in parsing rawsentences.
For those models assuming syntactic infor-mation its their necessary input, SEP may be attachedto them to wake their processors more self-extensible.44.5We are currently integrating SEP with a frameworkof syntactic and semantic knowledge acquisition \[Liu& Soo, 1992a; Liu & Son, 1993b\].6 Conc lus ionBuilding necessary knowledge bases is a major bottleneck of designing a practical parser.
Previous tudieson the problem had proposed many learning methodsto reduce the difficulty.
However, during learningmost of them still need extra information or humanintervention, which are the major sources of incon-sistencies, errors, and inefficiency in learning.
Thispaper is thus dedicated to the fully automatic nat-ural language learning in which only raw sentencesare entered to the system.
We study how a parsermay extend itself by observing its own experiencesof parsing.
The proposed learning technique SEP istriggered when the parser fails in parsing raw sen-tences.
It is shown that hypotheses for missing knowl-edge may be generated based on the parser's existingknowledge, universal linguistic onstraints, and par-tial results of the failed parsing.
Those hypotheticknowledge pieces that are likely to facilitate success-ful parsing may then be extracted as the new parsingknowledge.
Thus the parser may acquire the knowl-edge that it actually lacks in parsing.
As more parsersbecome available, SEP may be attached to them inorder to enhance their knowledge fully automatically.Acknowledgement This research is supportedin part by NSC (National Science Council of R.@.C.
)under the grant NSC83-0408-E-007-008.Re ferenceBerwick R. C. (1985).
The Acquisition of Syntac-tic Knowledge~ The MIT Press, Cambridgep Mas-sachusetts, London, England.Brent M. R. (1993).
From Grammar to Lczicon: Un-supervised Learning of Lexical Syntaz~ ComputationalLinguistics, Vol.
9, No.
2, pp.
243-262.Chomsky N. (1981).
Lectures on Government andBinding, Forts Publications- Dordrecht.Faisal K. A. and Kwasny S. C. (1990).
Design of aHybrid Deterministic Parser, Proc.
of COLING.Gazdar G., Klein E., Pullum G. K., and Sag I.
A.(1985).
Generalized Phrase Structure Grammar, Har-vard University Press, Cambridge, MA.Jacobs P. and Zernik U.
(1988).
Acquiring LexicalKnowledge from Tezt: A Case Study, Proc.
of AAAI.Lang F.-M. and Hirsehman L. (1988).
ImprovedPortability and Parsing through Interactive Acquisi-tion of Semantic Information, Proc.
of the secondconference on Applied NLP, pp.
49-57.Liu R.-L. and Soo V.-W. (1993a).
Parsing-DrivenGeneralization for Natural Language Acquisition, In-ternational Journal of Pattern Recognition and Arti-ficial Intelligence, Voi.
7, No.
3.Liu R.-L and Soo V.-W (1993b).
An Empirical Studyon Thematic Knowledge Acquisition Based on Syn-tactic Clues and Heuristics, Proc.
of the ACL-93.Liu R.-L. and Soo V.-W. (1992a).
Augmenting andEfficiently Utilizing Domain Theory in Ezplanation-Based Natural Language Acquisition, Proc.
of the 9thInternational Machine Learning Conference.Liu R.-L and Soo V.-W. (1992b).
Acquisitionof Unbounded Dependency Using Explanation-BasedLearning, Proc.
of ROCLING V.McClelland J. L. and Kawamoto A. II.
(1986).
Mech-anisms of Sentence Processing: Assigning Roles toConstituents of Sentences, in Parallel DistributedProcessing, Vol.
2, pp.
272-325.Montemagni S. and Vanderwende L. (1992).
Struc-tural Patterns vs.
String Patterns for ExtractingSemantic Information from Dictionary, Proc.
ofCOLING-92, pp.
546-552.Pinker S. (1984).
Language Learnability and Lan-guage Development, The IIarvard University Press,Cambridge, Massachusetts, London, England.Pustejovsky J., Berger S, and Anick P. (1993).
LexicaISemantic Techniques for Corpus Analysis, Computa-tional Linguistics, Vol.
9, No.
2, pp.
331-358.Sanfilippo A. and Pozanski V. (1992).
The Acquisi-tion of Lexical Knowledge from Combined Machine-Readable Dictionary Sources, Proc.
of the Third Con-ference on Applied NLP, pp.
80-87.Sekine S., Carroll J. J., Ananiadou S., and Tsujii 3".(1992).
Automatic Learning for Semantic Colloca-tion, Proc.
of the Third Conference on Applied NLP.Siskind J. M. (1990).
Acquiring Core Meanings ofWords, Represented as Jackendoff-style ConceptualStructures, from Correlated Streams of Linguistic andNon-linguistic Input, Proc.
of the ACL-90.Smadja F. A.
(1991).
From N-Grams to Collocations:An Evaluation of EXTRACT, Proc.
of the 29th an-nual meeting of the ACL, pp.
279-284.Vclardi P., Pazlenza M. T., and Fasolo M. (1991).How to Encode Semantic Knowledge: A Method forMeaning Representation and Computer-Aided Acqui-sition, Computational Linguistics, Vol.
17, No.
2.Webster M. and Marcus M. (1989).
Automatic Acqui-sition of the Le~ical Semantics of Verbs from SentenceIS.ames~ Proc.
of the ACL-89, pp.
177-184.Wexler K. and Culicovcr P. W. (1980).
Formal Prin-ciples of Language Acquisition, The MIT Press, Cam-bridge, Massachusetts, London, England.Zernik U.
(1987).
Learning ldioms - With and With-out Explanation, Proc.
of IJCAI, pp.
133-136.Zernik U.
(1989).
Lexicon Acquisition: Learning fromCorpus by Capitalizing on Lexical Categories, Proc.of IJCAI, pp.
1556-1562.Zernlk U. and .\]acobs P. (1990).
Tagging for Learning:Collecting Thematic Relation from Corpus, Proc.
ofCOLING, pp.
34-39.446
