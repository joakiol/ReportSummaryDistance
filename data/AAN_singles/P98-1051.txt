Error Driven Word Sense DisambiguationLuca  D in i  and V i t to r io  Di  Tomaso  F r4d4r ique  SegondCELI  Xerox Research Centre Europe{dini, dit omaso}@celi, sns.
it Frederique.
Segond?xrce.
xerox, comAbst rac tIn this paper we describe a method for per-forming word sense disambiguation (WSD).
Themethod relies on unsupervised learning and ex-ploits functional relations among words as pro-duced by a shallow parser.
By exploiting an er-ror driven rule learning algorithm (Brill 1997),the system is able to produce rules for WSD,which can be optionally edited by humans in or-der to increase the performance of the system.1 In t roduct ionAlthough automatic word sense disambiguation(WSD) remains a much more difficult task thanpart of speech (POS) disambiguation, resourcesand automatic systems are starting to appear.Some of these systems are even mature nough tobe evaluated.
This paper presents an overviewof a system for English WSD which will be eval-uated ill the context of the SENSEVAL project 1.We report on performing automatic WSD us-ing a specially-adapted version of Brill's er-ror driven unsupervised learning program (Brill,1997), originally developed for POS tagging.
Inour experiment, like ill Resnik (1997), we usedboth functional and semantic information in or-der to improve the learning capabilities of thesystem.
Indeed, by having access to a syntacticand functional sketch of sentences, and by beingable to stipulate which relations are importantfor sentence meaning, we overcame some of thetraditional problems found in continuous bigrammodels, such as the occurrence of interpolatedclauses and passive constructions.Consider, for example, temporal expressionslike Tuesday in The stock market Tuesday stageda technical recovery.
Such expressions are quitefrequent in newspaper text, often appearing near1 http://www.itri.bton.ac.uk/events/sensevalverbs.
Without any functional information, thesemantic rules produced by the algorithm willstipulate a strong semantic relation between thesemantic lass of words like Tuesday and the se-mantic lass of verbs like stage.
On the contrary,if we use information from a shallow parser, weknow that Tuesday is an adverbial expression,probably part of the verb phrase, and that thereally important relation to learn is the one be-tween the subject and the verb.In the following sections we describe (i) the re-sources we used (Penn Tree Bank, 45 upper levelWordNet tags); (ii) the experiment we ran usingrule induction techniques on functional relations(functional relation extraction, tag merging, cor-pus preparation and learning); (iii) the evalu-ation we performed on the semantically hand-tagged part of the Brown corpus and, finally, wesketch out the general architecture we are in theprocess of implementing.2 The  ResourcesWe decided to take advantage of the syntacticstructures already contained in the Penn TreeBank (PTB) (Mitchell et al, 1995) in orderto build a large set of functional relation pairs(much as in Resnik (1997)).
These relations arethen used to learn how to perform semantic dis-ambiguation.
To distinguish word meanings weuse the top 45 semantic tags included in Word-Net (Miller, 1990).
The non-supervised Brill al-gorithm is used to learn and then to apply se-mantic disambiguation rules.
The semanticallyhand-tagged Brown Corpus is used to evaluatethe performance of automatically acquired rules.2.1 Obta in ing  Funct iona l  S t ruc tures .We consider as crucial for semantic dis-ambiguation the following functional rela-tions: SUB J/VERB, VERB/OBJ, VERB/PREP/PREP-320OBJ, NOUN/PREP/PREP-OBJ.In order to extract them, we parsed thePTB structures using Zebu (Laubusch, 1994),a LARLR(1) parser implemented in LISP.
Theparser scans the trees, collecting informationabout relevant functional relations and writingthem out in an explicit format.
For instance, thefragment you do something to the economy, af-ter some intermediate steps which are describedin Dini et al (1998a) and Dini et al (1998b), istransformed into:HASOBJ do somethingHASSBJ do youPREPMOD do TO economy2.2 Add ing  Lexical Semant ics.The WordNet team has developed a generalsemantic tagging scheme where every set ofsynonymous enses, synsets, is tagged withone of 45 tags as in WordNet version 1.5.
Weuse these tags to label all the content wordscontained in extracted functional relations.
Weassociate ach word with all its possible sensesordered in a canonical way.
The semanticallytagged version of the sample sentence givenabove is:HASOBJ do/sga~iv*_8o?ia l .mogion_?rea*ion_body something/~opHASSBJ  do/aga~iv*.socia l_mo'~ion_?reat ion_body y u /p*rsonPREPMOD do/s~at ive_soc ia l .mog ion .c raat ion_body  TOecono lT ly  / group_?
ogn i g i on_at g r ibuge _act2.3 P repar ing  the input.As a result of adding lexical semantics weget a triple <functional relation, wordi/tagsetl,wordj/tagsetj>, but in its current formulation,the unsupervised learning algorithm is only ableto learn relations holding among bigrams.
Thus,it can learn either relations between a func-tional relation name (e.g.
"HASOBJ') and atagset or between tagsets, without consideringthe relation between them.
In both cases wereport a loss of information which is fatal forthe learning of proper rules for semantic dis-ambiguation.
There is an intuitive solution tothis problem: most of the relations we are in-terested in are diadic in nature.
For example,adjectival modification is a relation holding be-tween two heads (MOD(hl,h2)).
Also relationsconcerning verbal arguments can be split, in aneo-davidsonian perspective, into more atomicrelations uch as "SUBJ (h 1,h2)" "OBJ (h 1,h2)".These relations can be translated into a "bi-gram format" by assuming that the relation it-self is incorporated among the properties of theinvolved words (e.g.
wl/IS-OBJ w2/ IS -HEAD) .Learnable properties of words are standardly ex-pressed through tags.
Thus, we can merge func-tional and semantic tags into a single tag (e.g.wl / IS -OBJ  w2/ IS -HEAD + wi/2_3 w2/4 ~ w l / IS -OBJ2._IS-OBJ3 w2/ IS -HEAD4) .
The learner ac-qu i res  constraints which relate functional andsemantic information, as planned in this exper-iment.
We obtain the following format whereevery line of the input text represents what welabel an FS-pair (Functional Semantic pair):d_i HASOBJ something/gAsosJ- I  u/42_41_38..36..29do /  HASSBJ you/HAS~_BJ-'/ 42_4 I_38_36-29where relations labelled with -I are just inverserelations (e.g.
HAS-SUBJ -I  - IS-SUB J-OF).Functional relation involving modificationthrough prepositional phrases is ternary as itinvolves the preposition, the governing headand the governed head.
Crucially, however, onlysubstantive heads receive semantic tags, whichallows us to condense the preposition form inthe FS tags as well.
The representation f themodification structure of the phrase do to theeconomy becomes:do/  MOD-TO economy/MOD-TO-i:~2_41_38_36_29 14_9_7_43 Unsuperv ised  Learn ing  for  WSDSufficiently large texts should contain good cuesto learn rules for WSD in terms of selectionalpreferences.
2 The crucial assumption in usingfunctional relations for WSD is that, when com-positionality holds, selectional preferences canbe checked through an intersection operationbetween the semantic features of the syntacti-cally related lexical items.
By looking at func-tional relations that contain at least one non-ambiguously tagged word, we can learn evidencefor disambiguating ambiguous words appearingin the same context.
So, if we know that in thesentence John went to Milan the word Milan is~By selectional preferences we mean both the selectionof semantic features of a dependent given a certain headand its inverse (i.e.
selection of a head's emantic featuresby a dependent constituent).321unambiguously tagged as place, we learn thatin a structure GO to X, where GO is a verb ofthe same semantic lass as the word go and Xis a word containing place among its possiblesenses, then X is disambiguated asplace.The Brill algorithm 3 is based on rule patternswhich describe rules that can be learned, as wellas on a lexicon where words are associated withambiguity classes.
The learning algorithm is re-cursively applied to an ambiguously tagged cor-pus, producing a set of rules.
The set of learn-able rules includes the rules for which there iscorpus evidence in terms of unambiguous config-urations.
In other words, the learning algorithmextensively relies on bigrams where one of thewords is unambiguously tagged.
The preferredrules, the ones with the highest score, are thosethat best minimize the entropy of the untaggedcorpus.
For instance, a rule which resolves am-biguity for 1000 oceurences ofa given ambiguityclass is preferred to one which resolves the sameambiguity only 100 times.Consider the following rule pattern: ChangetagSet (X1 ,.~.?
...X~) into tag -?i if the left con-text is associated with the tagSet (1~, Y2 ... lm).This pattern generates rules such as: 4bil8_b?4 b i i8  LEFT b42_b32 1209.64which is paraphrased as: I f  a noun is ambigu-ous between person and act and it appears asthe subject of a verb which is ambiguous be-tu, een s ta t ive  and communication, then dis-ambiguate it as person.
This instantiation re-lies on the fact that the untagged corpus con-tains a significant number of cases where a noununambiguously tagged as person appears as sub-ject.
of a verb ambiguous between s~catiw andcommunication.
The rule is then applied to thecorpus in order to further reduce its ambiguity,and the new corpus is passed again as an inputto the learner, and the next most preferred ruleis learned.Three different scoring methods have beenused 5 as criteria to select the best rule.
Theyare referred to in the program documentation,ZFor the sake of clarity, we just present here the gen-eral lines of Brill's algorithm.
For a detailed version ofthe algorithm see Brill's original paper (Brill, 1997).4Letters are abbreviation for functional relation andnumbers are abbreviations for semantic tags.5The search space of the algorithm is parametrisedsetting two different hresholds governing the possibilityand in Dini et al (1998a), as "paper", "origi-nal" and "goodlog".
Here we will describe only"original" and "goodlog", because "paper" dif-fers from "original" only for some implementa-tion details.In the method called "original", at every it-eration step the best scored disambiguation ruleis learned, and the score of a rule is computed,according to Brill, in the following way: assumethat Change the tag of a word from ~ to Y incontext C is  arule (Y E ~).
Call R the tag Zwhich maximizes the following function (whereZ ranges over all the tags in ~ except Y, freq(Y)is the number of occurences of words unambigu-ously tagged with Y,  freq(Z) is the number ofoccurences of words unambiguously tagged withZ, and incontext( Z, C) is the number of timesa word unambiguously tagged with Z occurs incontext C):freq(Y)*incontext( Z,C) R = argmaxz  \ ] req(Z)The score assigned to the rule would then be:S:  incontext(Y, C) - freq(Y)*incontext(R,C)f req(R)In short, a good transformation from ~ to Yis one for which alternative tags in ~ have eithervery low frequency in the corpus or they seldomappear in context C. At every iteration cycle,the algorithm simply computes the best scoringtransformation.The method "goodlog" uses a probabilisticmeasure which minimizes the effects of tag fre-quenc, adopting this is the formula for giving ascore to the rule that selects the best tag Y ina context C (Y and Z belong to the ambiguoustagset):S ,~ .
i, t incontext (YC)  * \ ] req(Z)  ~= argrnaxy(~)aos( logt  \ ] req(Y)  incontext (Z ,C)  " )The differences in results between the differentscoring methods are reported and commented onin section 4 in table 1.4 Eva luat ionFor the evaluation we used as test corpus the sub-set of the Brown corpus manually tagged withthe 45 top-level WordNet tags.
We started withthe Penn Tree Bank representation a d wentthrough all the necessary steps to build FS-pairsfor a tag or a word to appear in a rule: i) the minimalfrequency of a tag; ii) the minimal frequency of a wordin the corpus.
We set the first parameter to 400 (thatis, we asked the learner to consider only the 400 mostfrequent TagSets) and we ignored the second one (that iswe asked the learner to consider all words in the corpus).322used by the applier.
These FS pairs were thenlabelled according to the manual codification andused as a standard for evaluation.
We also pro-duced, from the same source, a randomly taggedcorpus for measuring the improvements of oursystem with respect o random choice.The results of comparing the randomly taggedcorpus and the corpus tagged by our systemusing the methods "original" and "goodlog"are shown in table 1.
As usual, Precision isI II Precision I RecMI I F-measure Adjusted IRandom 0.45 0.44 0.44 0.28500 Goodlog 0.97 0.25 0.40 0.91"500 Original 0.78 0.30 0.44 0.50Table 1: Precision and recall figuresthe number of correctly tagged words dividedby the total number of tagged words; Recallis the number of correctly tagged words di-vided by the number of words in the test cor-pus (about 40000).
F-measure is (2*Preci-sion*Recall)/(Precison+Recall).
Thecolumn la-belled "Adjusted" reports the Precision takinginto account non-ambiguous words.
The ad-justed precision is computed in the followingway: (Correct - unambiguous words) / ((Cor-rect + Uncorrect) - unambiguous words).
Onan absolute basis, our results improve on those ofResnik (1997).
who used an information-theorymodel of selectional strength preference ratherthan an error-driven learning algorithm.
In-deed, if we compare the "Adjusted" measurewe obtained with a set of about 500 rules (50%precision), with the average reported by Resnik(1997) (41?~ precision), we obtain an advantageof 10 points, which, for a task suchas WSD, isnoteworthy.
For comparison with other experi-ments, refer to Resnik (1997).It is interesting to compare the figures pro-vided by "'goodlog" and "original".
Since "good-log" smooths the influence of absolute tag fre-quency, the learned rules achieve much higherprecision, even though they are less efficient interms of the number of words they can disam-biguate.
This is due to the fact that the most fre-quent words also tend to be the most ambiguousones, thus the ones for which the task of WSD ismost difficult (cf.
Dini et al (1998a)).5 Towards  SENSEVALAs mentioned above, the present system willbe adopted in the context of the SENSEVALproject, where we will adopt the Xerox Incre-mental Finite State Parser, which is completelybased on finite state technology.
Thus, in thepresent pilot experiment, we are only interestedin relations which could reasonably be capturedby a shallow parser, and complex informativerelations present in the Penn Tree Bank aresimply disregarded uring the parsing step de-scribed in section 2.1.
Also, structures whichare traditionally difficult to parse through FiniteState Automata, such as incidental and paren-thetic clauses or coordinate structures, are dis-carded from the learning corpus.
This mighthave caused a slight decrease in the performanceof the system.Some additional decrease might have beencaused by noise introduced by incorrect assign-ment of senses in context during the learningphase (see Schuetze t al.
(1995)).
In particu-lar, the system has to face the problem of senseassignment to named entities uch as person orindustry names.
Since we didn't use any textpreprocessor, we simply made the assumptionthat any word having no semantic tag in Word-Net, and which is not a pronoun, is assignedthe label human.
This assumption is certainlyquestionable and we adopted it only as a work-ing hypothesis.
In the following rounds of thisexperiment we will plug in a module for namedentity recognition in order to improve the per-formance of the system.Another issue that will be tackled in the SEN-SEVAL project concerns word independence.
Inthis experiment we duplicated lexical heads whenthey were in a functional relation with differentitems.
This permitted an easy adaptation to theinput specification of the Brill learner, but it hasdrawbacks both in the learning and the applica-tion phase.
During the learning phase the in-ability to capture the identity of the same lexicalhead subtracts evidence for the learning of newrules.
For instance, assume that at an iterationcycle n the algorithm has learned that verbal in-formation is enough to disambiguate the wordcat as animal in the wild cat mewed.
Since theFS-pairs cat/mew and wild/cat are autonomous,at cycle n + 1 the learner will have no evidenceto learn that the adjective wild tends to associate323with nouns of type animal.
On the contrary, cat,as appearing in wild cat, will still be ambiguous.The consequences of assuming independenceof lexical heads are even worse in the rule ap-plication phase.
First, certain words are disam-biguated only in some of the instances in whichthey appear, thus producing a decrease in termsof recall.
Second, there might be a case wherethe same word is tagged differently according tothe relations into which it enters, thus causinga decrease in terms of precision.
Both problemswill be overcome by the new Java-based versionsof the Brill learner and applier which have beendeveloped at CELI.When considering the particular WSD task, itis evident hat the information conveyed by ad-jectives and pre-nominal modifiers is at least asimportant as that conveyed by verbs, and it isstatistically more prominent.
In the corpus ob-tained from parsing the PTB, approximatelyof FS-pairs are represented by pre-nominal mod-ification (roughly analogous to the subject-verbFS-pairs and more frequent than the object-verbpairs, which amount o 1 of the whole corpus).But adjectives receive very poor lexical-semanticinformation from WordNet.
This forced us to ex-clude them both fl'om the training and test cor-pora.
This situation will again improve in theSENSEVAL experiment with the adoption of adifferent semantic lexicon.6 Conc lus ionWe presented a WSD system with reasonableresults as well as suggestions for improving it.We will implement these improvements in thecontext of the SENSEVAL experiment and weplan to extend the system to other languages,with special attention to French and Italian.
6 In-deed, the availability of lexical resources provid-ing a word sense classification with roughly thesame granularity of the 45 top classes of Wordnetmakes our method applicable also to languagesfor which no sense tagged corpora has been pro-duced.
In the long run, these extensions willlead, we hope, to better systems for foreign lan-guage understanding and machine translation.Acknowledgements  We are grateful to KenBeesley, Andrea Bolioli, Gregory Grefenstette,6The system will be used in the MIETTA project(LE4-8343) for enhancing the performance of the infor-mation extraction and information retrieval module.David Hull, Hinrich Schuetze and Annie Zaenenfor their comments and discussion on earlier ver-sions of this paper.
Our gratitude also goes toVincent Nainemoutou and Herve Poirier for pro-viding us with technical support.
Any remainingerrors are our own fault.ReferencesE.
Brill and P. Resnik.
1994.
A rule-based ap-proach to prepositional phrase attachment dis-ambiguation.
In Proceedings of COLING.E.
Brill.
1997.
Unsupervised learning of dis-ambiguation rules for part of speech tagging.In Natural Language Processing Using VeryLarge Corpora.
Kluwer Academic Press.Dini, L., V. Di Tomaso, F. Segond 1998.
ErrorDriven Unsupervised Semantic Disambigua-tion.
In Proceedings of TANLPS ECML-98.Chemnitz, Germany.Dini, L., V. Di Tomaso, F. Segond 1998.
WordSense Disambiguation with Functional Rela-tion.
In Proceedings of LREC-98.
Granada,Spain.J.
Laubusch.
1994.
Zebu: A tool for specifyingreversible LARL(1) parsers.G.
Miller.
1990.
Wordnet: An on-line lexicaldatabase.
Int.
Journal of Lexicography.M.
Mitchell, B. Santorini, and M.A.Marcinkiewicz.
1995.
Building a large anno-tated corpus of English : the Penn Treebank.Computational Linguistics, (19) :313-330.P.
Resnik and D. Yarowsky.
1997.
A perspec-tive on word sense disambiguation methodsand their evaluation.
In Proceedings of ACLSIGLEX Workshop on Tagging Text withLexical Semantics: Why, What, and How?,Washington, D.C., USA.P.
Resnik.
1997.
Selectional preference andsense disambiguation.
In Proceedings of ACLSIGLEX Workshop on Tagging Text withLexical Semantics: Why, What, and How?,Washington, D.C., USA.H.
Schuetze, , and J. Pedersen.
1995.
Informa-tion retrieval based on word senses.
In Pro-ceedings 4th Annual Symposium on DocumentAnalysis and Information Retrieval, Las Ve-gas, USA.D.
Yarowsky.
1995.
Unsupervised word sensedisambiguation method rivaling supervisedmethods.
In Proceedings of the A CL.324
