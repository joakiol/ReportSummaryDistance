Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712?724,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUsing Syntactic and Semantic Structural Kernels forClassifying Definition Questions in Jeopardy!Alessandro Moschitti?
Jennifer Chu-Carroll?
Siddharth Patwardhan?James Fan?
Giuseppe Riccardi?
?Department of Information Engineering and Computer ScienceUniversity of Trento, 38123 Povo (TN), Italy{moschitti,riccardi}@disi.unitn.it?IBM T.J. Watson Research Center P.O.
Box 704, Yorktown Heights, NY 10598, U.S.A.{jencc,siddharth,fanj}@us.ibm.comAbstractThe last decade has seen many interesting ap-plications of Question Answering (QA) tech-nology.
The Jeopardy!
quiz show is certainlyone of the most fascinating, from the view-points of both its broad domain and the com-plexity of its language.
In this paper, we studykernel methods applied to syntactic/semanticstructures for accurate classification of Jeop-ardy!
definition questions.
Our extensive em-pirical analysis shows that our classificationmodels largely improve on classifiers based onword-language models.
Such classifiers arealso used in the state-of-the-art QA pipelineconstituting Watson, the IBM Jeopardy!
sys-tem.
Our experiments measuring their impacton Watson show enhancements in QA accu-racy and a consequent increase in the amountof money earned in game-based evaluation.1 IntroductionQuestion Answering (QA) is an important researcharea of Information Retrieval applications, which re-quires the use of core NLP capabilities, such as syn-tactic and semantic processing for a more effectiveuser experience.
While the development of mostexisting QA systems are driven by organized eval-uation efforts such as TREC (Voorhees and Dang,2006), CLEF (Giampiccolo et al, 2007), and NT-CIR (Sasaki et al, 2007), there exist efforts thatleverage data from popular quiz shows, such as WhoWants to be a Millionaire (Clarke et al, 2001; Lamet al, 2003) and Jeopardy!
(Ferrucci et al, 2010), todemonstrate the generality of the technology.Jeopardy!
is a popular quiz show in the US whichhas been on the air for 27 years.
In each game, threecontestants compete for the opportunity to answer60 questions in 12 categories of 5 questions each.Jeopardy!
questions cover an incredibly broad do-main, from science, literature, history, to popularculture.
We are drawn to Jeopardy!
as a test bedfor open-domain QA technology due to its broad do-main, complex language, as well as the emphasis onaccuracy, confidence, and speed during game play.While the vast majority of Jeopardy!
questionsare factoid questions, we find several other typesof questions in the Jeopardy!
data, which can ben-efit from specialized processing in the QA system.The additional processing in these questions com-plements that of the factoid questions to achieve im-proved overall QA performance.
Among the varioustypes of questions handled by the system are defini-tion questions shown in the examples below:(1) GON TOMORROW: It can be the basketbelow a hot-air balloon or a flat-bottomedboat used on a canal (answer: gondola);(2) I LOVE YOU, ?MIN?
: Overbearing (an-swer: domineering);(3) INVEST: From the Latin for ?year?, it?san investment or retirement fund that paysout yearly (answer: an annuity)where the upper case text indicates the Jeop-ardy!
category for each question1.Several characteristics of this class of questionswarrant special processing: first, the clue (question)1A Jeopardy!
category indicates a theme is common amongits 5 questions.712often aligns well with dictionary entries, makingdictionary resources potentially effective.
Second,these clues often do not indicate an answer type,which is an important feature for identifying cor-rect answers in factoid questions (in the examplesabove, only (3) provided an answer type, ?fund?
).Third, definition questions are typically shorter inlength than the average factoid question.
These dif-ferences, namely the shorter clue length and the lackof answer types, make the use of a specialized ma-chine learning model potentially promising for im-proving the overall system accuracy.
The first stepfor handling definitions is, of course, the automaticseparation of definitions from other question types,which is not a simple task in the Jeopardy!
domain.For instance, consider the following example whichis a variation of (3) above:(4) INVEST: From the Latin for ?year?,an annuity is an investment or retirementfund that pays out this often (answer:yearly)Even though the clue is nearly identical to (3), theclue does not provide a definition for the answeryearly, although at first glance we may have beenmisled.
The source of complexity is given by the factthat Jeopardy!
clues are not phrased in interrogativeform as questions typically are.
This complicates thedesign of definition classifiers since we cannot di-rectly use either typical structural patterns that char-acterize definition/description questions, or previousapproaches, e.g.
(Ahn et al, 2004; Kaisser and Web-ber, 2007; Blunsom et al, 2006).
Given the com-plexity and the novelty of the task, we found it use-ful to exploit the kernel methods technology.
Thishas shown state-of-the-art performance in QuestionClassification (QC), e.g.
(Zhang and Lee, 2003;Suzuki et al, 2003; Moschitti et al, 2007) and itis very well suited for engineering feature represen-tations for novel tasks.In this paper, we apply SVMs and kernel meth-ods to syntactic/semantic structures for modelingaccurate classification of Jeopardy!
definition ques-tions.
For this purpose, we use several levels of lin-guistic information: word and POS tag sequences,dependency, constituency and predicate argumentstructures and we combined them using state-of-the-art structural kernels, e.g.
(Collins and Duffy,2002; Shawe-Taylor and Cristianini, 2004; Mos-chitti, 2006).
The extensive empirical analysis ofseveral advanced models shows that our best model,which combines different kernels, improves the F1of our baseline model by 67% relative, from 40.37to 67.48.
Surprisingly, with respect to previous find-ings on standard QC, e.g.
(Zhang and Lee, 2003;Moschitti, 2006), the Syntactic Tree Kernel (Collinsand Duffy, 2002) is not effective whereas the ex-ploitation of partial tree patterns proves to be es-sential.
This is due to the different nature of Jeop-ardy!
questions, which are not expressed in the usualinterrogative form.To demonstrate the benefit of our question clas-sifier, we integrated it into our Watson by couplingit with search and candidate generation against spe-cialized dictionary resources.
We show that in end-to-end evaluations, Watson with kernel-based defi-nition classification and specialized definition ques-tion processing achieves statistically significant im-provement compared to our baseline systems.In the reminder of this paper, Section 2 describesWatson by focusing on the problem of definitionquestion classification, Section 3 describes our mod-els for such classifiers, Section 4 presents our exper-iments on QC, whereas Section 5 shows the final im-pact on Watson.
Finally, Section 6 discusses relatedwork and Section 7 derives the conclusions.2 Watson: The IBM Jeopardy!
SystemThis section gives a quick overview of Watson andthe problem of classification of definition questions,which is the focus of this paper.2.1 OverviewWatson is a massively parallel probabilisticevidence-based architecture for QA (Ferrucci etal., 2010).
It consists of several major stages forunderlying sub-tasks, including analysis of thequestion, retrieval of relevant content, scoring andranking of candidate answers, as depicted in Figure1.
In the rest of this section, we provide an overviewof Watson, focusing on the task of answeringdefinitional questions.Question Analysis: The first stage of the pipeline,it applies several analytic components to identifykey characteristics of the question (such as answer713Figure 1: Overview of Watsontype, question classes, etc.)
used by later stages ofthe Watson pipeline.
Various general purpose NLPcomponents, such as a parser and named entity de-tector, are combined with task-specific modules forthis analysis.The task-specific analytics include several QCcomponents, which determine if the question be-longs to one or more broad ?question classes?.These question classes can influence later stages ofthe Watson pipeline.
For instance, a question de-tected as an abbreviation question can invoke spe-cialized candidate generators to produce possible ex-pansions of the abbreviated term in the clue.
Simi-larly, the question classes can impact the methodsfor answer scoring and the machine learning mod-els used for ranking candidate answers.
The focusof this paper is on the definition class, which is de-scribed in the next section.Hypothesis Generation: Following question anal-ysis, the Watson pipeline searches its document col-lection for relevant documents and passages that arelikely to contain the correct answer to the question.This stage of the pipeline generates search queriesbased on question analysis results, and obtains aranked list of documents and passages most relevantto the search queries.
A variety of candidate gen-eration techniques are then applied to the retrievedresults to produce a set of candidate answers.Information obtained from question analysis canbe used to influence the search and candidate gener-ation processes.
The question classes detected dur-ing question analysis can focus the search towardsspecific subsets of the corpus.
Similarly, during can-didate generation, strategies used to generate the setof candidate answers are selected based on the de-tected question classes.Hypothesis and Evidence Scoring: A wide varietyof answer scorers are then used to gather evidencesupporting each candidate answer as the correct an-swer to the given question.
The scorers include bothcontext dependent as well as context independentscorers, relying on various structured and unstruc-tured resources for their supporting evidence.Candidate Ranking: Finally, machine learningmodels are used to weigh the gathered evidence andrank the candidate answers.
The models generate aranked list of answers each with an associated con-fidence.
The system can also choose to refrain fromanswering a question if it has low confidence in allcandidates.
This stage of the pipeline employs sev-eral machine learning models specially trained tohandle various types of questions.
These models aretrained using selected feature sets based on questionclasses and candidate answers are ?routed?
to theappropriate model according to the question classesdetected during question analysis.2.2 Answering Definition QuestionsAmong the many question classes that Watson iden-tifies and leverages for special processing, of partic-ular interest for this paper is the class we refer toas definition questions.
These are questions whoseclue texts contain one or more definitions of the cor-rect answer.
For instance, in example (3), the mainclause in the question corresponds to a dictionarydefinition of the correct answer (annuity).
Lookingup this definition in dictionary resources could en-able us to answer this question correctly and withhigh confidence.
This suggests that special process-714ing of such definition questions could allow us tohone in on the correct answer through processes dif-ferent from those used for other types of questions.This paper explores strategies for definition ques-tion processing to improve overall question answer-ing performance.
A key challenge we have to ad-dress is that of accurate recognition of such ques-tions.
Given an input question the Watson questionanalysis stage uses a definition question recognizerto detect this specific class of questions.
We exploreseveral approaches for recognition, including a rulebased approach and a variety of statistical models.Questions that are recognized as definition ques-tions invoke search processes targeted towardsdictionary-like sources in our system.
We use a va-riety of such sources, such as standard English dic-tionaries, Wiktionary, WordNet, etc.
After gather-ing supporting evidence for candidate answers ex-tracted from these sources, our system routes thecandidates to definition-specific candidate rankingmodels, which have been trained with selected fea-ture sets.The following sections present a description andevaluation of our approach for identifying and an-swering definition questions.3 Kernel Models for QuestionClassificationPrevious work (Zhang and Lee, 2003; Suzuki et al,2003; Blunsom et al, 2006; Moschitti et al, 2007)as shown that syntactic structures are essential forQC.
Given the novelty of both the domain and thetype of our classification items, we rely on kernelmethods to study and design effective representa-tions.
Indeed, these are excellent tools for auto-matic feature engineering, especially for unknowntasks and domains.
Our approach consists of usingSVMs and kernels for structured data applied to sev-eral types of structural lexical, syntactic and shallowsemantic information.3.1 Tree and Sequence KernelsKernel functions are implicit scalar products be-tween data examples (i.e.
questions in our case)in the very high dimensional space of substructures,where each of the latter is a component of the im-plicit vectors associated with the examples.ROOTSBARQWHADVPWRBWhenSVPVBNhitPPINbyNPNNSelectrons,,NPDTaNNphosphorVPVBZgivesPRPRPoffNPNPJJelectromagneticNNenergyPPINinNPDTthisNNform1Figure 2: Constituency TreePASSPA1phosphorA0electronPRhitPPRenergyA2electromag.A1phosphorPA1energyPRgiveAM-TMPhitA0phosphorROOTVBZOBJNNNMODINPMODNNformNMODDTthisinenergyNMODJJelectromag.PRTRPoffgivesSBJNNphosphorNMODDTaP,TMPVBNLGSINPMODNNSelectronsbyhitTMPWRBwhen1Figure 3: Dependency Treenegative mistake STK, ok PTKNPADJPJJconceitedCCorJJarrogantNPNNmeaningNNadjectiveNN5-letterNNfowlpositive mistake STK, ok PTKNPVPPPNPNNfieldVBGplayingDTaINonVBNusedNPNNgrassJJgreenJJartificialNPVPPPNPNNcanalDTaINonVBNusedNPNNboatJJflat-bottomedDTaPASSPA1phosphorA0electronPRhitPPRenergyAM-MNRelectromag.A1phosphorPA1energyPRgiveAM-TMPhitA0phosphor1Figure 4: A tree encoding a Predicate Argument Structure SetAlthough several kernels for structured data havebeen developed (see Section 6), the main distinc-tions in terms of feature spaces is given by the fol-lowing three different kernels:?
Sequence Kernels (SK); we implemented thediscontinuous string kernels described in (Shawe-Taylor and Cristianini, 2004).
This allows for rep-resenting a string of symbols in terms of its possi-ble substrings with gaps, i.e.
an arbitrary number ofsymbols can be skipped during the generation of asubstring.
The symbols we used in the sequential de-scriptions of questions are words and part-of-speechtags (in two separate sequences).
Consequently, allpossible multiwords with gaps are features of the im-plicitly generated vector space.715?
Syntactic Tree Kernel (STK) (Collins and Duffy,2002) applied to constituency parse trees.
This gen-erates all possible tree fragments as features withthe conditions that sibling nodes from the originaltrees cannot be separated.
In other words, substruc-tures are composed by atomic building blocks cor-responding to nodes along with all their direct chil-dren.
These, in case of a syntactic parse tree, arecomplete production rules of the associated parsergrammar2.?
Partial Tree Kernel (PTK) (Moschitti, 2006) ap-plied to both constituency and dependency parsetrees.
This generates all possible tree fragments, asabove, but sibling nodes can be separated (so theycan be part of different tree fragments).
In otherwords, a fragment is any possible tree path, fromwhose nodes other tree paths can depart.
Conse-quently, an extremely rich feature space is gener-ated.
Of course, PTK subsumes STK but sometimesthe latter provides more effective solutions as thenumber of irrelevant features is smaller as well.When applied to sequences and tree structures, thekernels discussed above produce many differentkinds of features.
Therefore, the design of appro-priate syntactic/semantic structures determines therepresentational power of the kernels.
Hereafter, weshow the models we used.3.2 Syntactic Semantic StructuresWe applied the above kernels to different structures.These can be divided in sequences of words (WS)and part of speech tags (PS) and different kinds oftrees.
For example, given the non-definition Jeop-ardy!
question:(5) GENERAL SCIENCE: When hit by elec-trons, a phosphor gives off electromag-netic energy in this form.
(answer: lightor photons),we use the following sequences:WS: [when][hit][by][electrons][,][a][phosphor][gives][off][electromagnetic][energy][in][this][form]PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in][dt][nn]Additionally, we use constituency trees (CTs), see2From here the name syntactic tree kernelsFigure 2 and dependency structures converted intothe dependency trees (DTs), e.g.
shown in Figure3.
Note that, the POS-tags are central nodes, thegrammatical relation label is added as a fathernode and all the relations with the other nodes aredescribed by means of the connecting edges.
Wordsare considered additional children of the POS-tagnodes (in this case the connecting edge just servesto add a lexical feature to the target POS-tag node).Finally, we also use predicate argument structuresgenerated by verbal and nominal relations accord-ing to PropBank (Palmer et al, 2005) and NomBank(Meyers et al, 2004).
Given the target sentence, theset of its predicates are extracted and converted intoa forest, then a fake root node, PAS, is used to con-nect these trees.
For example, Figure 4 illustrates aPredicate Argument Structures Set (PASS) encodingtwo relations, give and hit, as well as the nominaliza-tion energy along with all their arguments.4 Experiments on Definition QuestionClassificationIn these experiments, we study the role of kerneltechnology for the design of accurate classificationof definition questions.
We build several classifiersbased on SVMs and kernel methods.
Each classi-fier uses advanced syntactic/semantic structural fea-tures and their combination.
We carry out an exten-sive comparison in terms of F1 between the differentmodels on the Jeopardy!
datasets.4.1 Experimental SetupCorpus: the data for our QC experiments consistsof a randomly selected set of 33 Jeopardy!
games3.These questions were manually annotated based onwhether or not they are considered definitional.
Thisresulted in 306 definition and 4964 non-definitionclues.
Each test set is stored in a separate file con-sisting of one line per question, which contains tab-separated clue information and the Jeopardy!
cate-gory, e.g.
INVEST in example (4).Tools: for SVM learning, we used the SVMLight-TK software4, which includes structural kernels inSVMLight (Joachims, 1999)5.
For generating con-3Past Jeopardy!
games can be downloaded fromhttp://www.j-archive.com.4Available at http://dit.unitn.it/?moschitt5http://svmlight.joachims.org716stituency trees, we used the Charniak parser (Char-niak, 2000).
We also used the syntactic?semanticparser by Johansson and Nugues (2008) to gener-ate dependency trees (Mel?c?uk, 1988) and predicateargument trees according to the PropBank (Palmeret al, 2005) and NomBank (Meyers et al, 2004)frameworks.Baseline Model: the first model that we used as abaseline is a rule-based classifier (RBC).
The RBCleverages a set of rules that matches against lexicaland syntactic information in the clue to make a bi-nary decision on whether or not the clue is consid-ered definitional.
The rule set was manually devel-oped by a human expert, and consists of rules thatattempt to identify roughly 70 different constructsin the clues.
For instance, one of the rules matchesthe parse tree structure for ?It?s X or Y?, which willidentify example (1) as a definition question.Kernel Models: we apply the kernels describedin Section 3 to the structures extracted from Jeop-ardy!
clues.
In particular, we design the followingmodels: BOW, i.e.
linear kernel on bag-of-wordsfrom the clues; WSK, PSK and CSK, i.e.
SK appliedto the word and POS-tag sequences from the clues,and the word sequence taken from the question cat-egories, respectively; STK-CT, i.e.
STK applied toCTs of the clue; PTK-CT and PTK-DT, i.e.
PTKapplied to CTs and DTs of the clues, respectively;PASS, i.e.
PTK applied to the Predicate ArgumentStructure Set extracted from the clues; and RBC, i.e.a linear kernel applied to the vector only constitutedby the 1/0 output of RBC.Learning Setting: there is no particular parameteri-zation.
Since there is an imbalance between positiveand negative examples, we used a Precision/Recalltrade-off parameter in SVM-Light-TK equal to 5.6Measures: the performance is measured with Pre-cision, Recall and F1-measure.
We estimated themby means of Leave-One-Out7 (LOO) on the questionset.4.2 Results and DiscussionTable 1 shows the performance obtained using dif-ferent kernels (feature spaces) with SVMs.
We note6We have selected 5 as a reasonable value, which kept bal-anced Precision and Recall on a validation set.7LOO applied to a corpus ofN instances consists in trainingon N ?
1 examples and testing on the single held-out example.This process is repeated for all instances.Kernel Space Prec.
Rec.
F1RBC 28.27 70.59 40.38BOW 47.67 46.73 47.20WSK 47.11 50.65 48.82STK-CT 50.51 32.35 39.44PTK-CT 47.84 57.84 52.37PTK-DT 44.81 57.84 50.50PASS 33.50 21.90 26.49PSK 39.88 45.10 42.33CSK 39.07 77.12 51.86Table 1: Kernel performance using leave-one-out cross-validation.that: first, RBC has good Recall but poor Precision.This is interesting since, on one hand, these resultsvalidate the complexity of the task: in order to cap-ture the large variability of the positive examples,the rules developed by a skilled human designer areunable to be sufficiently precise to limit the recog-nition to those examples.
On the other hand, RBC,being a rather different approach from SVMs, can besuccessfully exploited in a joint model with them.Second, BOW yields better F1 than RBC but itdoes not generalize well since its F1 is still low.When n-grams are also added to the model bymeans of WSK, the F1 improves by about 1.5 ab-solute points.
As already shown in (Zhang and Lee,2003; Moschitti et al, 2007), syntactic structures areneeded to improve generalization.Third, surprisingly with respect to previous work,STK applied to CT8 provides accuracy lower thanBOW, about 8 absolute points.
The reason is due tothe different nature of the Jeopardy!
questions: largesyntactic variability reduces the probability of find-ing general and well formed patterns, i.e.
structuresgenerated by entire production rules.
This suggeststhat PTK, which can capture patterns derived frompartial production rules, can be more effective.
In-deed, PTK-CT achieves the highest F1, outperform-ing WSK also when used with a different syntacticparadigm, i.e.
PTK-DT.Next, PSK and PASS provide a lower accuracybut they may be useful in kernel combinations asthey can complement the information captured bythe other models.
Interestingly, CSK alone is rathereffective for classifying definition questions.
We be-8Applying it to DT does not make much sense as alreadypointed out in (Moschitti, 2006).717?Figure 5: Similarity according to PTK and STKlieve this is because definition questions are some-times clustered into categories such as 4-LETTERWORDS or BEGINS WITH ?B?.Moreover, we carried out qualitative error analy-sis on the PTK and STK outcome, which supportedour initial hypothesis.
Let us consider the bottomtree in Figure 5 in the training set.
The top tree isa test example correctly classified by PTK but in-correctly classified by STK.
The dashed line in thetop tree contains the largest subtree matched by PTK(against the bottom tree), whereas the dashed line inthe bottom tree indicates the largest subtree matchedby STK (against the top tree).
As the figure shows,PTK can exploit a larger number of partial patterns.Finally, the above points suggest that differentkernels produce complementary information.
It isthus promising to experiment with their combina-tions.
The joint models can be simply built bysumming kernel functions together.
The results areshown in Table 2.
We note that: (i) CSK comple-ments the WSK information, achieving a substan-tially better result, i.e.
62.95; (ii) PTK-CT+CSKperforms even better than WSK+CSK (as PTK out-performs WSK); and (iii) adding RBC improvesfurther on the above combinations, i.e.
68.11 and67.32, respectively.
This evidently demonstratesthat RBC captures complementary information.
Fi-nally, more complex kernels, especially the overallkernel summation, do not seem to improve the per-Kernel Space Prec.
Rec.
F1WSK+CSK 70.00 57.19 62.95PTK-CT+CSK 69.43 60.13 64.45PTK-CT+WSK+CSK 68.59 62.09 65.18CSK+RBC 47.80 74.51 58.23PTK-CT+CSK+RBC 59.33 74.84 65.79BOW+CSK+RBC 60.65 73.53 66.47PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56WSK+CSK+RBC 69.26 66.99 68.11ALL 61.42 67.65 64.38Table 2: Performance of Kernel Combinations usingleave-one-out cross-validation.formance.
This is also confirmed by the PASS re-sults derived in (Moschitti et al, 2007) on TRECQC.5 Experiments on the Jeopardy SystemSince the kernel-based classifiers perform substan-tially better than RBC, we incorporate the PTK-CT+WSK+CSK model9 into Watson for definitionclassification and evaluated the QA performanceagainst two baseline systems.
For the end-to-end ex-periments, we used Watson?s English Slot Grammarparser (McCord, 1980) to generate the constituencytrees.
The component level evaluation shows thatwe achieved comparable performance as previouslydiscussed with ESG.5.1 Experimental SetupWe integrated the classifier into the question analy-sis module, and incorporated additional componentsto search against dictionary resources and extractcandidate answers from these search results when aquestion is classified as definitional.
In the final ma-chine learning models, a separate model is trainedfor definition questions to enable scoring tailored tothe specific characteristics of those questions.Based on our manually annotated gold standard,less than 10% of Jeopardy!
questions are classifiedas definition questions.
Due to their relatively lowfrequency we conduct two types of evaluations.
Thefirst is definition-only evaluation, in which we applyour definition question classifier to identify a large9Since we aim to compare a purely statistical approach tothe rule-based approach, we did not experiment with the modelthat uses RBC as a feature in our end-to-end experiments.718set of definition questions and evaluate the end-to-end system?s performance on this large set of ques-tions.
These results enable us to draw statisticallysignificant conclusions about our approach to ad-dressing definition questions.The second type of evaluation is game-basedevaluation, which assesses the impact of our defi-nition question processing on Watson performancewhile preserving the natural distribution of thesequestion types in Jeopardy!
data.
Game-based eval-uations situate the system?s performance on defini-tion questions relative to other types of questions,and enable us to gauge the component?s contribu-tions in a game-based setting.For both evaluation settings, three configurationsof Watson are used as follows:?
the NoDef system, in which Watson is config-ured without definition classification and pro-cessing, thereby treating all definition ques-tions as regular factoid questions;?
the StatDef system, which leverages the sta-tistical classifier and subsequent definition spe-cific search and candidate generation compo-nents as described above; and?
the RuleDef system, in which Watson adoptsRBC and employs the same additional defini-tion search and candidate generation compo-nents as the StatDef system.For the definition-only evaluation, we selected allquestions recognized as definitional by the statisticalclassifier from roughly 1000 unseen games (60000questions), resulting in a test set of 1606 questions.Due to the size of the initial set, it is impractical tomanually create a gold standard for measuring Pre-cision and Recall of the classifier.
Instead, we com-pare the StatDef system against the NoDef on these1606 questions using two metrics: accuracy, definedas the percentage of questions correctly answered,and p@70, the system?s Precision when answeringonly the top 70% most confident questions.
P@70 isan important metric in Jeopardy!
game play as wellas in real world applications where the system mayrefrain from answering a question when it is not con-fident about any of its answers.
Since RBC identifiessignificantly more definition questions, we startedNoDef StatDef NoDef RuleDef# Questions 1606 1606 1875 1875Accuracy 63.76% 65.57% 56.64% 57.51%P@70 82.22% 84.53% 72.73% 74.87%Table 3: Definition-Only Evaluation Resultswith an initial set of roughly 300 games, from whichthe RBC identified 1875 questions as definitional.We compared the RuleDef system?s performance onthese questions against the NoDef baseline using theaccuracy and p@70 metrics.For the game-based evaluation, we randomly se-lected 66 unseen Jeopardy!
games, consisting of3546 questions after excluding audio/visual ques-tions.10 We contrast the StatDef system perfor-mance against that of NoDef and RuleDef alongseveral dimensions: accuracy and p@70, describedabove, as well as earnings, the average amount ofmoney earned for each game.5.2 Definition-Only EvaluationFor the definition-only evaluation, we compared theStatDef system against the NoDef system on a set of1606 questions that the StatDef system classified asdefinitional.
The results are shown in the first twocolumns in Table 3.
To contrast the gain obtainedby the StatDef system against that achieved by theRuleDef system, we ran the RuleDef system overthe 1875 questions identified as definitional by therule-based classifier.
We contrast the RuleDef sys-tem performance with that of the NoDef system, asshown in the last two columns in Table 3.Our results show that based on both evaluationmetrics, StatDef improved upon the NoDef baselinemore than RuleDef improved on the same baselinesystem.
Furthermore, for the accuracy metric whereall samples are paired and independent, the differ-ence in performance between the StatDef and NoDefsystems is statistically significant at p<0.05, whilethat between the RuleDef and NoDef systems is not.5.3 Game-Based EvaluationThe game-based evaluation was carried out on 66unseen games (roughly 3500 questions).
Of these10Audio/visual questions are those accompanied by either animage or an audio clip.
The text portions of these questions areoften insufficient for identifying the correct answers.719# Def Q?s Accuracy P@70 EarningsNoDef 0 69.71% 86.79% $24,818RuleDef 480 69.23% 86.31% $24,397StatDef 131 69.85% 87.19% $25,109Table 4: Game-Based Evaluation Resultsquestions, the StatDef system classified 131 of themas definitional while the RuleDef system identified480 definition questions.
Both systems were com-pared against the NoDef system using the accuracy,p@70, and earnings metric computed over all ques-tions, as shown in Table 4.Our results show that even though in thedefinition-only evaluation both the RuleDef andStatDef systems outperformed the NoDef baseline,in our game-based evaluation, the RuleDef systemperformed worse than the NoDef baseline.
The low-ered performance is due to the fact that the Preci-sion of the RBC is much lower than that of the sta-tistical classifier, and the special definition process-ing applied to questions that are erroneously clas-sified as definitional was harmful.
Our evaluationof this false positive set showed that its accuracydropped by 6% compared to the NoDef system.
Onthe other hand, the StatDef system outperformed thetwo other systems, and its accuracy improvementupon the RuleDef system is statistically significantat p<0.05.6 Related WorkOur paper studies the use of advanced representa-tion for QC in the Jeopardy!
domain.
As previouslymentioned Jeopardy!
questions are stated as affir-mative sentences, which are different from the typ-ical QA questions.
For the design of our models,we have carefully taken into account previous work.This shows that semantics and syntax are essentialto retrieve precise answers, e.g (Hickl et al, 2006;Voorhees, 2004; Small et al, 2004).We focus on definition questions, which typicallyrequire more complex processing than factoid ques-tions (Blair-Goldensohn et al, 2004; Chen et al,2006; Shen and Lapata, 2007; Bilotti et al, 2007;Moschitti et al, 2007; Surdeanu et al, 2008; Echi-habi and Marcu, 2003).
For example, language mod-els were applied to definitional QA in (Cui et al,2005) to learn soft pattern models based on bigrams.Other related work, such as (Sasaki, 2005; Suzukiet al, 2002), was also very tied to bag-of-wordsfeatures.
Predicate argument structures have beenmainly used for reranking (Shen and Lapata, 2007;Bilotti et al, 2007; Moschitti et al, 2007; Surdeanuet al, 2008).Our work and methods are similar to (Zhang andLee, 2003; Moschitti et al, 2007), which achievedthe state-of-the-art in QC by applying SVMs alongwith STK-CT.
The results were derived by experi-menting with a TREC dataset11(Li and Roth, 2002),reaching an accuracy of 91.8%.
However, such datarefers to typical instances from QA, whose syntacticpatterns can be easily generalized by STK.
In con-trast, we have shown that STK-CT is not effectivefor our domain, as it presents very innovative ele-ments: questions in affirmative and highly variableformat.
Thus, we employed new methods such asPTK, dependency structures, multiple sequence ker-nels including category information and many com-binations.Regarding the use of Kernel Methods, there isa considerably large body of work in Natural Lan-guage Processing, e.g.
regarding syntactic parsing(Collins and Duffy, 2002; Kudo et al, 2005; Shenet al, 2003; Kudo and Matsumoto, 2003; Titov andHenderson, 2006; Toutanova et al, 2004), namedentity recognition and chunking (Cumby and Roth,2003; Daume?
III and Marcu, 2004), relation extrac-tion (Zelenko et al, 2002; Culotta and Sorensen,2004; Bunescu and Mooney, 2005; Zhang et al,2005; Bunescu, 2007; Nguyen et al, 2009a), textcategorization (Cancedda et al, 2003), word sensedisambiguation (Gliozzo et al, 2005) and seman-tic role labeling (SRL), e.g.
(Kazama and Torisawa,2005; Che et al, 2006a; Moschitti et al, 2008).However, ours is the first study on the use of sev-eral combinations of kernels applied to several struc-tures on very complex data from the Jeopardy!
do-main.7 Final Remarks and ConclusionIn this paper we have experimented with advancedstructural kernels applied to several kinds of syntac-tic/semantic linguistic structures for the classifica-tion of questions in a new application domain, i.e.Jeopardy!.
Our findings are summarized hereafter:11Available at http://cogcomp.cs.illinois.edu/Data/QA/QC/720First, it should be noted that basic kernels, suchas STK, PTK and SK, when applied to new repre-sentations, i.e.
syntactic/semantic structures, con-stitute new kernels.
Thus structural representationsplay a major role and, from this perspective, our pa-per makes a significant contribution.Second, the experimental results show that thehigher variability of Jeopardy!
questions prevents usfrom achieving generalization with typical syntacticpatterns even if they are derived by powerful meth-ods such as STK.
In contrast, partial patterns, suchas those provided by PTK applied to constituency(or dependency) trees, prove to be effective.In particular, STK has been considered as the bestkernel for exploiting syntactic information in con-stituency trees, e.g.
it is state-of-the-art in: QC(Zhang and Lee, 2003; Moschitti et al, 2007; Mos-chitti, 2008); SRL, (Moschitti et al, 2008; Mos-chitti et al, 2005; Che et al, 2006b); pronominalcoreference resolution (Yang et al, 2006; Versleyet al, 2008) and Relation Extraction (Zhang et al,2006; Nguyen et al, 2009b).
We showed that, inthe complex domain of Jeopardy!, STK surprisinglyprovides low accuracy whereas PTK is rather ef-fective and greatly outperforms STK.
We have alsoprovided an explanation of such behavior by meansof error analysis: in contrast with traditional ques-tion classification, which focuses on basic syntacticpatterns (e.g.
?what?, ?where?, ?who?
and ?how?
).Figure 5 shows that PTK captures partial patternsthat are important for more complex questions likethose in Jeopardy!Third, we derived other interesting findings forNLP related to this novel domain, e.g.
: (i) the im-pact of dependency trees is similar to the one ofconstituency trees.
(ii) A simple computational rep-resentation of shallow semantics, i.e.
PASS (Mos-chitti, 2008), does not work in Jeopardy!.
(iii) Se-quence kernels on category cues, i.e., higher level oflexical semantics, improve question classification.
(iv) RBC jointly used with statistical approaches ishelpful to tackle the Jeopardy!
complexity.Next, our kernel models improve up to 20 abso-lute percent points over n-grams based approaches,reaching a significant accuracy of about 70%.
Wat-son, exploiting such a classifier, improved previ-ous versions using RBC and no definition classifica-tion both in definition-only evaluations and in game-based evaluations.Finally, we point out that:?
Jeopardy!
has a variety of different special ques-tion types that are handled differently.
We focus onkernel methods for definition question for two rea-sons.
First, their recognition relies heavily on parsestructures and is therefore more amenable to the ap-proach proposed in the paper than the recognitionof other question types.
Second, definition is by farthe most frequent special question type in Jeopardy!
;therefore, we can obtain sufficient data for trainingand testing.?
We were unable to address the whole QC prob-lem using a statistical model due to the lack of suffi-cient training data for most special question classes.Furthermore, we focused only on the definition clas-sification and its impact on system performance dueto space reasons.?
Our RBC has a rather imbalanced trade-off be-tween Precision and Recall.
This may not be thebest operating point, but the optimal point is diffi-cult to obtain empirically for an RBC, which is astrong motivation of the work in this paper.
We ex-perimented with tuning the trade-off between Preci-sion and Recall with the RBC, but since RBC useshand-crafted rules and does not have a parameter forthat, ultimately the statistical approach proved moreeffective.In future work, we plan to extend the current re-search by investigating models capable of exploit-ing predicate argument structures for question clas-sification and answer reranking.
The use of syntac-tic/semantic kernels is a promising research direc-tion (Basili et al, 2005; Bloehdorn and Moschitti,2007a; Bloehdorn and Moschitti, 2007b).
In thisperspective kernel learning is a very interesting re-search line, considering the complexity of represen-tation and classification problems in which our ker-nels operate.AcknowledgementsThis work has been supported by the IBM?s OpenCollaboration Research (OCR) awards program.
Weare deeply in debt with Richard Johansson, who pro-duced the earlier syntactic/semantic representationsof the Jeopardy!
questions from the text format.721ReferencesKisuh Ahn, Johan Bos, Stephen Clark, James R. Cur-ran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B.Smillie, and Bonnie Webber.
2004.
Question an-swering with qed and wee at trec-2004.
In E. M.Voorhees and L. P. Buckland, editors, The ThirteenthText REtrieval Conference, TREC 2004, pages 595?599, Gaitersburg, MD.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005.
Effective use of WordNet semanticsvia kernel-based learning.
In Proceedings of CoNLL-2005, pages 1?8, Ann Arbor, Michigan.
Associationfor Computational Linguistics.M.
Bilotti, P. Ogilvie, J. Callan, and E. Nyberg.
2007.Structured retrieval for question answering.
In Pro-ceedings of ACM SIGIR.S.
Blair-Goldensohn, K. R. McKeown, and A. H.Schlaikjer.
2004.
Answering definitional questions:A hybrid approach.
In M. Maybury, editor, Proceed-ings of AAAI 2004.
AAAI Press.Stephan Bloehdorn and Alessandro Moschitti.
2007a.Combined syntactic and semantic kernels for text clas-sification.
In Proceedings of ECIR 2007, Rome, Italy.Stephan Bloehdorn and Alessandro Moschitti.
2007b.Structure and semantics for expressive text kernels.
InIn Proceedings of CIKM ?07.Phil Blunsom, Krystle Kocik, and James R. Curran.2006.
Question classification with log-linear models.In SIGIR ?06: Proceedings of the 29th annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 615?616, NewYork, NY, USA.
ACM.Razvan Bunescu and Raymond Mooney.
2005.
A short-est path dependency kernel for relation extraction.
InProceedings of HLT and EMNLP, pages 724?731,Vancouver, British Columbia, Canada, October.Razvan C. Bunescu.
2007.
Learning to extract relationsfrom the web using minimal supervision.
In Proceed-ings of ACL.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence kernels.Journal of Machine Learning Research, 3:1059?1082.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of NAACL.Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.2006a.
A hybrid convolution tree kernel for seman-tic role labeling.
In Proceedings of the COLING/ACL2006 Main Conference Poster Sessions, pages 73?80, Sydney, Australia, July.
Association for Compu-tational Linguistics.Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.2006b.
A hybrid convolution tree kernel for semanticrole labeling.
In Proceedings of the COLING/ACL onMain conference poster sessions, COLING-ACL ?06,pages 73?80, Stroudsburg, PA, USA.
Association forComputational Linguistics.Y.
Chen, M. Zhou, and S. Wang.
2006.
Reranking an-swers from definitional QA using language models.
InProceedings of ACL.Charles Clarke, Gordon Cormack, and Thomas Lynam.2001.
Exploiting redundancy in question answering.In Proceedings of the 24th SIGIR Conference, pages358?365.Michael Collins and Nigel Duffy.
2002.
New Rank-ing Algorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
In Pro-ceedings of ACL?02.H.
Cui, M. Kan, and T. Chua.
2005.
Generic soft patternmodels for definitional QA.
In Proceedings of SIGIR,Salvador, Brazil.
ACM.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofACL, pages 423?429, Barcelona, Spain, July.Chad Cumby and Dan Roth.
2003.
Kernel Methods forRelational Learning.
In Proceedings of ICML 2003.Hal Daume?
III and Daniel Marcu.
2004.
Np bracketingby maximum entropy tagging and SVM reranking.
InProceedings of EMNLP?04.A.
Echihabi and D. Marcu.
2003.
A noisy-channel ap-proach to question answering.
In Proceedings of ACL.David Ferrucci, Eric Brown, Jennifer Chu-Carroll, JamesFan, David Gondek, Aditya Kalyanpur, Adam Lally,J.
William Murdock, Eric Nyberg, John Prager, NicoSchlaefer, and Chris Welty.
2010.
Building watson:An overview of the deepqa project.
AI Magazine,31(3).Danilo Giampiccolo, Pamela Froner, Anselmo Pen?as,Christelle Ayache, Dan Cristea, Valentin Jijkoun,Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, andRichard Suteliffe.
2007.
Overview of the CLEF 2007multilingual question anwering track.
In Proceedingsof the Cross Language Evaluation Forum.Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.2005.
Domain kernels for word sense disambiguation.In Proceedings of ACL?05, pages 403?410.A.
Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, andB.
Rink.
2006.
Question answering with lcc chaucerat trec 2006.
In Proceedings of TREC.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
Advances in Kernel Methods ?
Sup-port Vector Learning, 13.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In CoNLL 2008:Proceedings of the Twelfth Conference on NaturalLanguage Learning, pages 183?187, Manchester,United Kingdom.722Michael Kaisser and Bonnie Webber.
2007.
Questionanswering based on semantic roles.
In Proceedings ofthe Workshop on Deep Linguistic Processing, DeepLP?07, pages 41?48, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jun?ichi Kazama and Kentaro Torisawa.
2005.
Speedingup training with tree kernels for node relation labeling.In Proceedings of HLT-EMNLP?05.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL?03.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree features.In Proceedings of ACL?05.Shyong Lam, David Pennock, Dan Cosley, and SteveLawrence.
2003.
1 billion pages = 1 milllion dollars?mining the web to pay ?who wants to be a millionaire?In Proceedings of the 19th Conference on Uncertaintyin AI.X.
Li and D. Roth.
2002.
Learning question classifiers.In Proceedings of ACL.Michael C. McCord.
1980.
Slot grammars.
Computa-tional Linguistics.Igor A. Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University Press of New York, Albany.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
The NomBank project: An interimreport.
In HLT-NAACL 2004 Workshop: Frontiersin Corpus Annotation, pages 24?31, Boston, UnitedStates.Alessandro Moschitti, Bonaventura Coppola, Ana-MariaGiuglea, and Roberto Basili.
2005.
Hierarchical se-mantic role labeling.
In CoNLL 2005 shared task.Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,and Suresh Manandhar.
2007.
Exploiting syntacticand shallow semantic kernels for question/answer clas-sification.
In Proceedings of ACL?07.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InProceedings of ECML?06, pages 318?329.Alessandro Moschitti.
2008.
Kernel methods, syntax andsemantics for relational text categorization.
In Pro-ceeding of CIKM ?08, NY, USA.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009a.
Convolution kernels onconstituent, dependency and sequential structures forrelation extraction.
In Proceedings of EMNLP, pages1378?1387, Singapore, August.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009b.
Convolution kernels onconstituent, dependency and sequential structures forrelation extraction.
In EMNLP ?09: Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 1378?1387, Morristown,NJ, USA.
Association for Computational Linguistics.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of seman-tic roles.
Computational Linguistics, 31(1):71?105.Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, andHsin-Hsi Chen.
2007.
Overview of the NTCIR-6cross-lingual question answering (CLQA) task.
InProceedings of the 6th NTCIR Workshop on Evalua-tion of Information Access Technologies.Y.
Sasaki.
2005.
Question answering as question-biasedterm extraction: A new approach toward multilingualqa.
In Proceedings of ACL, pages 215?222.John Shawe-Taylor and Nello Cristianini.
2004.
LaTeXUser?s Guide and Document Reference Manual.
Ker-nel Methods for Pattern Analysis, Cambridge Univer-sity Press.D.
Shen and M. Lapata.
2007.
Using semantic rolesto improve question answering.
In Proceedings ofEMNLP-CoNLL.L.
Shen, A. Sarkar, and A. Joshi.
2003.
Using LTAGBased Features in Parse Reranking.
In Proceedings ofEMNLP, Sapporo, Japan.S.
Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin,N.
Shimizu, P. Kantor, D. Kelly, and N. Wacholder.2004.
Hitiqa: Towards analytical question answering.In Proceedings of COLING.M.
Surdeanu, M. Ciaramita, and H. Zaragoza.
2008.Learning to rank answers on large online QA collec-tions.
In Proceedings of ACL-HLT, Columbus, Ohio.J.
Suzuki, Y. Sasaki, and E. Maeda.
2002.
Svm an-swer selection for open-domain question answering.In Proceedings of Coling, pages 974?980.Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and EisakuMaeda.
2003.
Question classification using hdag ker-nel.
In Proceedings of the ACL 2003 Workshop onMultilingual Summarization and Question Answering,pages 61?68, Sapporo, Japan, July.
Association forComputational Linguistics.Ivan Titov and James Henderson.
2006.
Porting statisti-cal parsers with data-defined kernels.
In Proceedingsof CoNLL-X.Kristina Toutanova, Penka Markova, and ChristopherManning.
2004.
The Leaf Path Projection View ofParse Trees: Exploring String Kernels for HPSG ParseSelection.
In Proceedings of EMNLP 2004.Yannick Versley, Alessandro Moschitti, Massimo Poe-sio, and Xiaofeng Yang.
2008.
Coreference sys-tems based on kernels methods.
In The 22nd Interna-tional Conference on Computational Linguistics (Col-ing?08), Manchester, England.723Ellen M. Voorhees and Hoa Trang Dang.
2006.Overview of the TREC 2005 question answering track.In Proceedings of the TREC 2005 Conference.E.
M. Voorhees.
2004.
Overview of the trec 2004 ques-tion answering track.
In Proceedings of TREC 2004.Xiaofeng Yang, Jian Su, and Chewlim Tan.
2006.Kernel-based pronoun resolution with structured syn-tactic knowledge.
In Proc.
COLING-ACL 06.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2002.
Kernel methods for relationextraction.
In Proceedings of EMNLP-ACL, pages181?201.Dell Zhang and Wee Sun Lee.
2003.
Question classifica-tion using support vector machines.
In Proceedings ofthe 26th annual international ACM SIGIR conferenceon Research and development in informaion retrieval,pages 26?32.
ACM Press.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, andChew Lim Tan.
2005.
Discovering relations betweennamed entities from a large raw corpus using treesimilarity-based clustering.
In Proceedings of IJC-NLP?2005, Lecture Notes in Computer Science (LNCS3651), pages 378?389, Jeju Island, South Korea.Min Zhang, Jie Zhang, and Jian Su.
2006.
Explor-ing Syntactic Features for Relation Extraction using aConvolution tree kernel.
In Proceedings of NAACL.724
