Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 657?660,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproving Data Driven Dependency Parsing using Clausal InformationPhani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, Rajeev SangalLanguage Technologies Research Centre, IIIT-Hyderabad, India.phani.gadde@research.iiit.ac.in, karan_jindal@students.iiit.ac.in,{samar,dipti,sangal}@mail.iiit.ac.inAbstractThe paper describes a data driven dependencyparsing approach which uses clausal informa-tion of a sentence to improve the parser per-formance.
The clausal information is addedautomatically during the parsing process.
Wedemonstrate the experiments on Hindi, a lan-guage with relatively rich case marking sys-tem and free-word-order.
All the experimentsare done using a modified version ofMSTParser.
We did all the experiments on theICON 2009 parsing contest data.
We achievedan improvement of 0.87% and 0.77% in unla-beled attachment and labeled attachment accu-racies respectively over the baseline parsingaccuracies.1 IntroductionLinguistic analysis of morphologically rich free-word-order languages (MoRFWO) using depen-dency framework have been argued to be moreeffective (Shieber, 1985; Mel?
?uk, 1988, Bharati etal., 1993).
Not surprisingly, most parsers for suchlanguages are dependency based (Nivre et al,2007a; Bharati et al, 2008a; Hall et al, 2007).
Inspite of availability of annotated treebanks, state-of-the-art parsers for MoRFWO have not reachedthe performance obtained for English.
Some of thereasons stated for the low performance are smalltreebank size, complex linguistic phenomenon,long-distance dependencies, and non-projectivestructures (Nivre et al, 2007a, 2007b; Bharati etal., 2008a).Several approaches have been tried to handle thesedifficulties in such languages.
For Hindi, Bharati etal.
(2008a) and Ambati et al (2009) used semanticfeatures in parsing to reduce the negative impact ofunavailable syntactic features and showed that useof minimal semantics can help in identifying cer-tain core dependency labels.
Various attempts haveproved to simplify the structure by dividing thesentence into suitable linguistic units (Attardi andDell?Orletta 2008; Bharati et al, 1993, 2008b,2009; Husain et al, 2009).
These approaches han-dle complex structures by breaking the parsingprocess into several steps.
Attardi and Dell'Orletta(2008) used chunk information as a feature toMaltParser (Nivre et al, 2007a) for parsing Eng-lish.
Bharati et al, 1993 used the notion of localword groups, while Bharati et al, 2009 and Husainet al, 2009 used clauses.In this paper, we describe a data driven depen-dency parsing approach which uses clausal infor-mation of a sentence to improve the parserperformance.
Previous attempts at data drivenparsing for Hindi have failed to exploit this featureexplicitly.
The clausal information is added auto-matically during the parsing process.
We demon-strate the experiments on Hindi1.
All theexperiments are done using a modified version ofMSTParser (McDonald et al, 2005a and the refer-ences therein) (henceforth MST) on the ICON2009 parsing contest2 (Husain, 2009) data.
Weachieved an improvement of 0.87% and 0.77% inunlabeled attachment and labeled attachment accu-racies respectively over the baseline parsing accu-racies.1 Hindi is a verb final language with free word order and a richcase marking system.
It is an official language of India and isspoken by ~800 million people.2 http://www.icon2009.in/contests.html6572 Why Clausal Information?Traditionally, a clause is defined as a group ofwords having a subject and a predicate.
Clauseboundary identification is the process of dividingthe given sentence into a set of clauses.
It can beseen as a partial parsing step after chunking, inwhich one tries to divide the sentence into mea-ningful units.
It is evident that most of the depen-dents of words in a clause appear inside the sameclause; in other words the dependencies of thewords in a clause are mostly localized within theclause boundary.In the dependency parsing task, a parser has todisambiguate between several words in the sen-tence to find the parent/child of a particular word.This work is to see whether the clause boundaryinformation can help the parser to reduce thesearch space when it is trying to find the correctparent/child for a word.
The search space of theparser can be reduced by a large extent if we solvea relatively small problem of identifying the claus-es.
Interestingly, it has been shown recently thatmost of the non-projective cases in Hindi are inter-clausal (Mannem et al, 2009).
Identifying clausalboundaries, therefore, should prove to be helpful inparsing non-projective structures.
The same holdstrue for many long-distance dependencies.3 Experimental Setup3.1 DatasetThe experiments reported in this paper have beendone on Hindi; the data was released as part of theICON 2009 parsing contest (Husain, 2009).
Thesentences used for this contest are subset of theHyderabad Dependency Treebank (HyDT) devel-oped for Hindi (Begum et al, 2008).
The depen-dency relations in the treebank are syntactico-semantic.
The dependency tagset in the annotationscheme has around 28 relations.
The dependencytrees in the treebank show relations between chunkheads.
Note, therefore, that the experiments andresults described in this paper are based on parsetrees that have chunk head as nodes.The data provided in the task contained morpho-logical features along with the lemma, POS tag,and coarse POS tag, for each word.
These are sixmorphological features namely category, gender,number, person, vibhakti3 or TAM4 markers of thenode3.2 Clause Boundary IdentifierWe used the Stage15 parser of Husain et al (2009),to provide the clause boundary information that isthen incorporated as features during the actualparsing process.
The Stage1 parser uses MST toidentify just the intra-clausal relations.
To achievethis, Husain et al, introduce a special dummy nodenamed _ROOT_ which becomes the head of thesentence.
All the clauses are connected to thisdummy node with a dummy relation.
In effect theStage1 parser gives only intra-clausal relations.
Inthe current work, we used MaltParser6 (Nivre et al,2007b) (henceforth Malt) to do this task.
This isbecause Malt performs better than MST in case ofintra-clausal relations, which are mostly short dis-tance dependencies.
We use the same algorithmand feature setting of Bharati et al, (2008a) to trainthe Stage1 parser.Since the above tool parses clauses, thereforealong with the clause boundary information wealso know the root of the clausal sub-tree.
Severalexperiments were done to identify the most optim-al set of clausal features available from the partialparse.
The best results are obtained when theclause boundary information, along with the headinformation i.e.
head node of a clause, is given as afeature to each node.We trained the Stage1 parser by converting thetreebank data into the stage1 format, following thesteps that were given in Husain et al (2009).
Thisconversion depends on the definition of the clause.We experimented with different definitions ofclause in order to tune the tool to give the optimalclause boundary and head information required forparsing.
For the results reported in this paper, aclause is a sequence of words, with a single verb,unless the verb is a child of another verb.3 Vibhakti is a generic term for preposition, post-position andsuffix.4TAM: Tense, Aspect and Modality.5Stage1 handles intra-clausal dependency relations.
Theserelations generally correspond to the argument structure of theverb, noun-noun genitive relation, infinitive-noun relation,adjective-noun, adverb-verb relations, etc.6 Malt version 1.2658Precision RecallClause Boundary 84.83% 91.23%Head Information 92.42% 99.40%Table 1.
Accuracies of the features being usedTable 1 gives the accuracy of the clausal informa-tion being used as features in parsing.
It is clearfrom Table1 that the tool being used doesn?t havevery high clause boundary identification perfor-mance; nevertheless, the performance is sufficientenough to make an improvement in parsing expe-riments.
On the other hand, the head of the clause(or, the root head in the clausal sub-tree) is identi-fied efficiently.
All the above experiments for pa-rameter tuning were done on the development dataof the ICON 2009 parsing contest.3.3 ParserWe used MSTParser7 for the actual parsing step.MST uses Chu-Liu-Edmonds Maximum SpanningTree Algorithm for non-projective parsing andEisner's algorithm for projective parsing (Eisner,1996).
It uses online large margin learning as thelearning algorithm (McDonald et al, 2005b).We modified MST so that it uses the clauseboundary.
Unlike the normal features that MSTuses, the clause boundary features span acrossmany words..4 Experiments and ResultsWe experimented with different combinations ofthe information provided in the data (as mentionedin 3.1).
Vibhakti and TAM fields gave better re-sults than others.
This is consistent with the bestprevious settings for Hindi parsing (Bharati et al,2008a, Ambati et al, 2009).
We used the resultsobtained using this setting as our baseline (F1).We first experimented by giving only the clauseinclusion (boundary) information to each node(F2).
This feature should help the parser reduce itssearch space during parsing decisions.
Then, weprovided only the head and non-head information(whether that node is the head of the clause or not)(F3).
The head or non-head information helps inhandling complex sentences that have more than7 MST version 0.4bone clause and each verb in the sentence has itsown argument structure.
We achieved the best per-formance by using both as features (F4) during theparsing process.LA (%) UA (%) L (%)F1 73.62 91.00 76.04F2 72.66 91.00 74.74F3 73.88 91.35 75.78F4 74.39 91.87 76.21Table 2.
Parsing accuracies with different featuresTable 2 gives the results for all the settings.
It isinteresting to note that the boundary information(F1) alone does not cross the baseline; howeverthis feature is reliable enough to give the best per-formance when combined with F3.5 ObservationsWe see from the above results (F4 in Table 2) thatthere is a rise of 0.87% in UA (unlabeledattachment) and 0.77% in LA (labeled attachment)over previous best (F1).
This shows the positiveeffect of using the clausal information during theparsing process.We analyzed the performance of both the pars-ers in handling the long distance dependencies andnon-projective dependencies.
We found that thenon-projective arcs handled by F4 have a precisionand recall of 41.1% and 50% respectively for UA,compared to 30.5% and 39.2% for the same arcsduring F1.Figure 1.
Distance statsFigure 1 compares the accuracies of the depen-dencies at various distances.
It is clear that the ef-fect of clausal information become more659pronounced as the distance increases.
This meansF4 does help the parser in effectively handling longdistance dependencies as well.6 Conclusion and Future WorkThe results show that there is a significantimprovement in the parsing accuracy when theclausal information is being used.The clausal information is presently being usedonly as attachment features in MST.
Experimentscan be done in future, to find out if there is a labelbias to the clause boundary, which also helps inreducing the search space for specific labels.
Im-proving the feature set for the labeled parse alsoimproves the unlabeled attachment accuracy, asMST does attachments and labels in a single step,and the labels of processed nodes will also be tak-en in features.We can see from Table1 that the precision of theclause boundary is 84.83%.
Using a tool, targetedat getting just the clausal information, instead ofusing a parser can improve the accuracy of theclausal information, which helps improving pars-ing.ReferencesB.
R. Ambati, P. Gadde, and K. Jindal.
2009.
Experi-ments in Indian Language Dependency Parsing.
InProceedings of the ICON09 NLP Tools Contest: In-dian Language Dependency Parsing, pp 32-37.B.
R. Ambati, P. Gade and C. GSK.
2009.
Effect ofMi-nimal Semantics on Dependency Parsing.
In the Pro-ceedings of RANLP 2009 Student ResearchWorkshop.G.
Attardi and F. Dell?Orletta.
Chunking and Depen-dency Parsing.
LREC Workshop on Partial Parsing:Between Chunking and Deep Parsing.
Marrakech,Morocco.
2008.R.
Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, andR.
Sangal.
2008.
Dependency annotation scheme forIndian languages.
In Proceedings of IJCNLP-2008.A.
Bharati and R. Sangal.
1993.
Parsing Free Word Or-der Languages in the Paninian Framework.
Proceed-ings of ACL:93.A.
Bharati, S. Husain, B. Ambati, S. Jain, D. Sharmaand R. Sangal.
2008a.
Two Semantic features makeall the difference in Parsing accuracy.
In Proceed-ings.
of International Conference on Natural Lan-guage Processing-2008.A.
Bharati, S. Husain, D. Sharma, and R. Sangal.
2008b.A two stage constraint based dependency parser forfree word order languages.
In Proceedings.
ofCOLIPS International Conference on Asian Lan-guage Processing.
Thailand.
2008.A.
Bharati, S. Husain, D. M. Sharma and R. Sangal.Two stage constraint based hybrid approach to freeword order language dependency parsing.
In the Pro-ceedings of the 11th International Conference onParsing Technologies (IWPT09).
Paris.
2009.J.
Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M.Nilsson,M.
Saers.2007.
Single Malt or Blended?
AStudy in Multilingual Parser Optimization.In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007.S.
Husain.
2009.
Dependency Parsers for Indian Lan-guages.
In Proceedings of ICON09 NLP Tools Con-test:Indian Language Dependency Parsing.Hyderabad, India.
2009.S.
Husain, P. Gadde, B. Ambati, D. M. Sharma and Ra-jeev Sangal.
2009.
A modular cascaded approach tocomplete parsing.
In the Proceedings of COLIPS In-ternational Conference on Asian LanguageProcessing.
Singapore.
2009.P.
Mannem and H. Chaudhry.2009.
Insights into Non-projectivity in Hindi.
In ACL-IJCNLP Student paperworkshop.
2009.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005a.
Non-projective dependency parsing usingspanning tree algorithms.
In the Proceedings ofHLT/EMNLP, pp.
523?530.R.
McDonald, K. Crammer, and F. Pereira.
2005b.
On-line large-margin training of dependency parsers.
Inthe Proceedings of ACL 2005. pp.
91?98.I.
A. Mel'Cuk.
1988.
Dependency Syntax: Theory andPractice, State University Press of New York.J.
Nivre, J.
Hall, S. Kubler, R. McDonald, J. Nilsson, S.Riedel and D. Yuret.
2007a.
The CoNLL 2007Shared Task on Dependency Parsing.
In Proceedingsof the CoNLL Shared Task Session of EMNLP-CoNLL 2007.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit, S.K?bler, S. Marinov and E Marsi.
2007b.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2), 95-135.S.
M. Shieber.
1985.
Evidence against the context-freeness of natural language.
In Linguistics and Phi-losophy, p. 8, 334?343.660
