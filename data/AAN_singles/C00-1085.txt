Est imat ion of Stochast ic Attr ibute-Value Grammars using anInformative SampleMi les  Osborneosborne~let .
rug .n lR i jksun ivers i te i t  Gron ingen ,  The  Nether lands*Abst rac tWe argue that some of the computational complexityassociated with estimation of stochastic attribute-value grammars can be reduced by training upon aninformative subset of the full training set.
Resultsusing the parsed Wall Street Journal corpus showthat in some circumstances, it is possible to obtainbetter estimation results using an informative sam-ple than when training upon all the available ma-terial.
Further experimentation demonstrates thatwith unlexicalised models, a Gaussian prior can re-duce overfitting.
However, when models are lexi-ealised and contain overlapping features, overfittingdoes not seem to be a problem, and a Gmlssian priormakes minimal difference to performance.
Our ap-proach is applicable for situal;ions when there arean infeasibly large mnnber of parses in the trainingset, or else for when recovery of these parses fl'oma packed representation is itself comi)utationally ex-pensive.1 I n t roduct ionAbney showed that attribute-value grammars can-not be modelled adequately using statistical tech-niques which assume that statistical dependenciesare accidental (Ablmy, 1997).
Instead of using amodel class that assumed independence, Abney sug-gested using Random Fields Models (RFMs) tbrattribute-value grmnmars.
RFMs deal with thegraphical structure of a parse.
Because they do notmake independence assumptions about the stochas-tic generation process that might have producedsome parse, they are able to model correctly depen-dencies that exist within parses.When estimating standardly-formulated RFMs, itis necessary to sum over all parses licensed by thegrammar.
For many broad coverage natural lan-guage grammars, this might involve summing overan exponential number of parses.
This would makethe task eomtmtationally intractable.
Almey, fol-lowing the lead of Lafferty et al suggested a Monte* Current address: osborne@eogsei.ed.ae.uk, University ofEdinburgh, Division of Informaties, 2 Bueeleuch Place, EII89LW, Scotland.Carlo simulation as a way of reducing the computa-tional burden associated with RFM estimation (Laf-ferty et al, 1997).
However, Johnson ct al consid-ered the form of sampling used in this sinmlation(Metropolis-Hastings) intractable (Johnson et M.,1999).
Instead, they proposed an Mternative strat-egy that redefined the estimation task.
It was arguedthat this redefinition made estimation eomtmtation-Mly simple enough that a Monte Carlo simulationwas unnecessary.
They presented results obtainedusing a small unlexicalised model trained on a mod-est corlms.Unfortunately, Johnson et alassumed it was possi-ble to retrieve all parses licensed by a grmnmar whenparsing a given training set.
For us, this was notthe case.
In our experiments with a manually writ-ten broad coverage Definite Clause Grammar (DCG)(Briscoe and Carroll, 1996), we were only able to re-cover M1 parses for Wall Street .Journal sentencesthat were at most 13 tokens long within acceptabletime and space bounds on comtmtation.
When weused an incremental Minilnum Description Length(MDL) based learner to extend the coverage of ourmmmally written gralnular (froul roughly 6()~ toaround 90% of the parsed Wall Street .Jouriml), thesituation became worse.
Sentence ambiguity consid-erably increased.
We were then only able to recoverall parses for Wall Street Journal sentences that wereat most 6 tokens long (Osborne, 1999).We can however, and usually in polynomial time,recover up to 30 parses for sentences up to 30 tokenslong when we use a probabilistic unpacking mecha-nism (Carroll and Briscoe, 1992).
(Longer sentencesthan 30 tokens can be parsed, but the nmnber ofparses we can recover for them drops off rapidly).
1However, 30 is far less tlmn the maximum numberl"vVe made an attempt o determine the maximum num-ber of parses our grammar might assign to sentences.
Ona 450MIIz Ultra Spare 80 with 2 G'b of real memory, witha limit of at most 1000 parses per sentence, and allowingno more than 100 CPU seconds per sentence, we found thatsentence ambiguity increased exponentially with respect tosentence l ngth.
Sentences with 30 tokens had an estimatedaverage of 866 parses (standard eviation 290.4).
Withoutthe limit of 1000 parses per sentence, it seems likely that thisaverage would incrc, ase.586of parses per sentence o111' grammar mighl, assign toWall Stl"ec't Journal sent;enees.
Any training set wehave a(:eess to will therefore be l|eeessarily limite(lin size.We therefore need an estimation strategy thattakes seriously the issue of extracting the 1)esl, per-refinance fl 'om a limited size training Met.
A limitedsize tra.ining sol; means one ereate(l )y retrieving atmost n t)arses per Ment(mee.
Although we (:annot re-cover all t)ossil)le i )arses~ we (lo }lave a choice as towhich llarses estimation should 1)e based Ul)On.Our ai)proach to the prol)lem of making I{FM es-timation feasible, ibr our highly amt)iguous I)CG isto seek ol\]|; an ivformativc samt)le and train ui)onthat.
We (lo not redefine the estimation task in anon-s l ;a l~t la rd  w;~y, 1101' (lo we llSe a ~{o\] l te Car lo  s i ln -ulation.We (:all a salul)lc informative if it 1)oth leads tothe select;ion of a 111ollol that does not mldertit oroverfit, and also is typical of t'utm'e samples, l)esl)itc() l ie's intuitions, an infornmtive saml)le might be aprol)er subset of the fifll training set.
This meansthat estinlation using the int'ornmtiv(; sample mightyield 1)etter esults than estimation using all of thel;rainhlg Met;.The ):(;st of this 1)aper is as tbllows, l,'irstly weintroduce RFMs.
Then we show how they nlay beesl;imated and how an infbrmative saml)le might 1)eidentified.
Nexl;, we give details of the, a(;tribute-vahle gramnlar we use, all(t show \]lOW we ~o at)ot l tmodelling it.
We then i)resent two sets of experi-mel)ts.
The first set is small scale, and art!
de.signedto show the existent;e of ;m inti)rmative sample.
Thesecond ski of CXl)erilll(;llI, S al 'e larger in scale, an(1build upon the COml)utational savil|gS we are al)leto achieve using a probabilistic Unl)acking strategy.They show how large me(Ms (two orders of magni-tude larger than those reported by Johnson ctal)can 1)e estimated using the l)arsed Wall Street .lour-hal eort)us.
Overlitting is shown to take place.
Theyalso show how this overfitting can be (partially) re-duced by using a Gaussian prior.
Finally, we endwith  SOllle COllllllelltS Oil Ollr WOl.k.2 Random F ie ld  Mode lsHere we show how attribute-wflue grammars may bemodelle(1 using RFMs.
Although our commentary isin terms of RFMs and grammars, it should t)e ol)-vious that RFM technology can be applied to otherestimation see.narios.Let G be an attribute-value grammar, D the setof sentences within the string-set defined lly L(G)and ~ the union of the set of parses assigne(1 toeach sentence in D by the gramnmr G. A RandomField Model, M, cons is t  of  two  cora l )o r ients :  a se t  o ffeatures, F and a set; of 'wei.qhts, A.l?eatures are the basle building blocks of RFMs.They enable the system designer to spccit)~ {;lie keyasl)ects of what it; takes to ditferentiate one 1)arsefrom a11other parse.
Each feature is a t'lmetion froma 1)arse to an integer.
Her(.
', the integer value as-sociated with a feature is interpreted as the nmn-ber of times a feature 'matches' (is 'active') witha parse.
Note features hould not be confllsed withfeatures as found in feature-value t)undles (these willbe called atl;ril)utes instead).
\]Peatures are usuallymanually selected by the sysl;ein designer.The other component of a RFM, A, is a set ofweights, hffornmlly, weights tell its how ti;atures areto be used when nlodellillg parses.
For exanlple, anactive feature with a large weight might indicate thatsome parse had a higtl prolmlfility.
Each weight Ai isassociated with a thatm'e fi.
Weights arc' real-valuednmnl)ers an(l ~:H'O autonmtically deternfined 113: an es-timation process (for example using hnproved Itera-tire Scaling (LaflL'rty et al, 1997)).
One of the nicel)roI)erties of Rl.i'Ms is that 1111o likelihood fiuw?ionof a RFM is strictly concave.
This means 1;hat here~/t"e. 11o h)cal lllillillla~ and so  wc can  be, l)e sure  thatsealing will result in estinmtion of a 11.1,'54 that isglol)ally ot)timal.The (unnormalised) total weight of a i)arse :c,'(J(:r), is a flulction of the.
k feaLures that are 'active'on a 1)arse:k.,/,(.
;) = (.))
(1)i=lThe prol)ability of a parse, P(x I M), is simplythe result of norm~dising the total weight associatedwith that parse:J'(:,, IM)  = 12)z -- (a)yGf~The inl;erpretation of this I)robability depends uponthe apt)lication of tile RFM.
Here, we use parse prol)-abilities to rettect preferences for parses.When using RFMs for parse selection, we sin>ply select the parse that ma.ximises ~/;(:1:).
In thesecircumstances, there is 11o need to nornlalise (com-pute Z).
Also, when comtmting ,/~(:c) for comi)etingparses, there is no built-in bias towards shorter (orlonger) derivations, and so no need to normalise withrespect o deriw~tion length/2The reason there is no need to normalisc with respect toderivation length is that features can have positive o1" nega-tive weights.
The weight of a parse will ttlcrcforc not alwaysmonotonical ly increase with respect to the re,tuber of activeti~atm'cs.5873 RFM Est imat ion  and  Se lec t ion  o fthe  In fo rmat ive  SampleWe now sketch how RFMs may be estimated andthen outline how we seek out an informa.tive smnple.We use hnproved Iterative Scaling (IIS) to esti-mate RFMs.
In outline, the IIS algorithm is as fol-lows:1.
Start with a reference distribution H,, a set offeatures F and a set of weights A.
Let M bethe RFM defined using F and A.2.
Initialise all weights to zero.
This makes tileinitial model uniform.3.
Compute the expectation of each feature w.r.tR.4.
For each feature fi(a) Find a weight ~; that equates the expecta-tion of fi w.r.t/?, and the expectation of fiw.r.t M.(b) Ileplace the old value of ki with 21.5.
If the model has converged to/?, output M.6.
Otherwise, go to step 4Tile key step here is 4a, computing the expectations?
of features w.r.t the RFM.
This involves calculatingthe probability of a parse, which, as we saw fronlequation 2, requires a summation over all parses inft.We seek out an informative sample ~l (fh C ~)as follows:I.
Pick out from ~ a sample of size n.2.
Estimate a model using that smnple and evalu-ate it.3.
If the model just estimated shows signs of over-fitting (with respect o an unseen held-out dataset), halt and output the inodel.4.
Otherwise, increase n and go back to step 1.Our approach is motivated by tile following (par-tially related) observations:?
Because we use a non-Imrmnetric model classand select an instance of it in terlns of somesample (section 5 gives details), a stochasticcomplexity argument tells us that an overly sim-ple model (resulting from a small sample) islikely to underfit.
Likewise, an overly complexmodel (resulting from a large sample) is likelyto overfit.
An informative samI)le will thereforerelate to a model that does not under or overfit.?
On average, an informative sample will be %yp-ical' of future samples.
For many reaMife situ-ations, this set is likely to be small relative tothe size of the full training set.We incorporate the first observation through oursearch mechanism.
Because we start with small sam-pies and gradually increase their size, we remainwithin the donmin of etliciently recoverable samples.The second observation is (largely) incorporatedin the way we pick samples.
The experimental sec-tion of this paper goes into the relevant details.Note our approach is heuristic: we cmmot affordto evahmte all 21~1 possible training sets.
The actualsize of the informative sample fit will depend bothtile Ill)On the model class used and the maximumsentence length we can de~,l with.
We would ex-pect: richer, lexicalised models to exhibit overfittingwith slnaller samples than would be the case withunlexicalised models.
We would expect he size ofan informative sample to increase as the maxilnumsentence length increased.There are similarities between our approach andwith estimation using MDL (Rissanen, 1989).
How-ever, our implementation does not explicitly attemptto minimise code lengths.
Also, there are similari-ties with importance sampling approaches to RFMestimation (such as (Chen and ll,osenfeld, 1999a)).However, such attempts do not miifinfise under oroverfitting.4 The GrammarThe grammar we model with I/andom Fields, (calledthe Ta 9 Sequence Grammar (Briseoe and Carroll,1996), or TSG for short) was developed with regardto coverage, and when compiled consists of 455 Def-inite Clause Grammar (DCG) rules.
It does notparse sequences of words directly, but instead as-signs derivations to sequences of part-of-speech tags(using the CLAWS2 tagset.
The grammar is rela-tively shallow, (for exmnple, it does not fltlly anal-yse unbounded ependencies) but it does make anattelnpt o deal with coilunou constructions, uch asdates or names, commonly found in corpora, but oflittle, theoretical interest.
Furthermore, it integratesinto the syntax a text gramma.r, grouping utterancesinto units that reduce the overall ambiguity.5 Mode l l ing  the  GrammarModelling the TSG with respect o the parsed WallStreet .\]ournal consists of two steps: creation of afeature set and definition of the reference distribu-tion.Our feature set is created by parsing sentences inthe training set (~br), and using earl, parse to ill-stantiate templates.
Each template defines a familyof features.
At present, the templates we use aresomewhat ad-hoc.
However, they are motivated bythe observations that linguistically-stipulated units(DCG rules) are informative, trod that ninny DCGapl)lications in preferred parses can be predicted us-ing lexical information.588AP/al:unimlie{h.'dIA 1/aI) p i :unimpededunimI)eded PP/t) I :by\[P1/ iml  :byby N1/n:trafli{:Itrafl\]{:Figure 1: TSG Parse FragumntThe first template creates features that count{;lie numl)er of tinms a. DUG instantiationis i)resentwithin ,2 1}arse.
a For examt}le , Sul)p{}s{~ we 1)arse{tthe Wall Street Journal AP:1 unimpeded 1}y t:ra{licA parse tree generated by TSG nfight lie as shownin figure 1.
Here, to s:~ve on Sl}ace, wc have labdledeach interior node in the parse tree with TSG rulenames, and not attribut(;-valu(~ bun(lies.
Further-more, we have mmota.t('xl each node with the lmadw(}rd of tim l}hrase in question.
Within ollr gl;aill-mar, heads arc (usually) ext)lMtly marke{t. This1110;/,118 W(~ do  l\]ot \]l;~v{~ to Ill&k(~ ;lily g l lossos w}lcllidentit\[ying the head of a. local tree.
With head in-foi'mtd;io\]b we are alo/e to lexicalise models.
\Ve haa;esuppressed taggillg information.For {'.xamp\]e, a \]hature (h'Jin(;d using this t(;nlplat{;might (:O1111t tho, nu inber  ()f t imes  th(!
we saw:AP/atIA1/at/1)1111 a 1)arse.
Such features r(~coi'd sore( 2(if the contextof the rule a.t}p\]i(:ation, i  that rule al}t}Iication8 thatdiffer ii1 terms of how attributes are bound will 1}emodelled by (litlhrent hatures.Our se{'ond total}late creates features that al'{'~ par-tially lexicalised.
I~br each lo{:al tree (of depth one)that has a \]?P daughter, we create a feature thatcounts the lmmber of times that h)cal tree, de(:oratedwith the head-woM of the I ' l ' ,  was seen in a. parse.An cxmnple of such ;1 lexicMised feature would 1}e:A1/apt}lIPI)/til:l)y3Note, all (}111" fo.al;/ll'es Slll)i)r(?ss ;tlly t{!l'nlillals thgtl, al)i}em'in a h}caI 1,Fe(!.
Lexical informaI;ioll is in{:luded when we decideto lexicalise features.These featm'cs are designed to model PP attach-ments that can be resolved using the head of thePP.The thh'd mid tinM template creates featuros thatare again partiMly lexicalised.
This time, we createlocal trees of det}th one that are, decorated with thehead word.
For example, here is one such feature:AP/al :mfimpededIA1/applNote the second and third templates result in fea-tures that overlap with features resulting fl'om at)-i}\]icati(ms of the first template.We create the reference distribution 1~ (an associ-ation of t)r{}l}al)i\]ities with TSG parses of sentences,such that the t}robabilities reflect 1}a.rse i)references)using the following process:1.
Extra{;t some sami}le f~T (using the al)l)roachmentioned in sc(:tion 3).2.
For each sentence in tim sample, for each l)arseof that sent;encc', {:Olnl)ute the '(lista.ncc' be-tween the TSG 1}mse and the WSJ refereuceparse.
\]1\] our at)t)roach, dista.nce is cM{:lfla.tc(1in tcl7111s o f  a weighted Slltll o f  crossing rates, re-call and 1}recision.
Mininlising it maximises ourdefinition of parse plausibility.
4 However, thereis nothing inherently crucial about this decision.Auy othc'r objective flmction (thaJ; can l)c ret)-r(~sent('.
(l as an CXl}Oncntial distribution) couh\]1)e used instead.3.
Normalise the distan('es, uch that for some ,sen-tcn(:e, tim sum of tim distances of all rt~cov-O,l.
'od ~\[?SG t)al"S(~S \]\['(/1" that  soii|;(!ilCO, is a COllSt?tilta.cross all sento.nces.
Nornmlising in this man-ner ensures that each sentence is cquil)robal}le0"emcmber that \]{FM probabilities are in termsof I}a.rse lir{'.fl~r{'.nces, and not probability of oc-{:llrr{HIee ill 8{}111{~ (;orl)llS).4.
Map the norinalised distances into 1}robabili-ties.
If d(p) is the normalised {listance of TSGl/;}~l"Se p, then associate with parse 1) the refer-(race probability given by the maximum likeli-hood estimator:rl(1,) (4)Our approach therefore gives t}artial cl'e(lit (a 11oil-zero reference l)robability) to a.ll parses in ~z.
/2, isthcreibr(; not as discontimlous as the equivalent dis-trit)ution used by Johnson at al.
We therefl)re do notneed to use simulated annea.ling o1' other numericallyintensive techniques to cstiinate models.4Ore' distanc(~ mo.l;ric is the same one used I}y llekto{m(ltektoen, 19.97)5896 Exper imentsHere we present wo sets of experiments.
The firstset demonstrate he existence of an informative sam-ple.
It also shows some of the characteristics of threesmnpling strategies.
The second set of experimentsis larger in scale, and show RFMs (both lexicalisedand unlexicalised) estimated using sentences up to30 tokens long.
Also, the effects of a Gaussian priorare demonstrated asa way of (partially) dealing withoverfitting.6.1 Test ing the Var ious Sampl ingStrategiesIn order to see how various sizes of sample related toestimation accuracy and whether we could achievesimilar levels of performm~ce without recovering allpossible parses, we ran the following experiments.We used a model consisting of features that weredefined using all three templates.
We also threwaway all features that occurred less than two times inthe training set.
We randomly split; the Wall StreetJournal into disjoint training, held-out and testingsets.
All sentences in the training and held-out setswere at most 14 tokens long.
Sentences ill the test-tug set, were at most 30 tokens long.
There were6626 sentences in the training set, 98 sentences inthe held-out set and 441 sentences in tile testing set.Sentences in the held-out set had on average 12.6parses, whilst sentences in the testing-set had on av-erage 60.6 parses per sentence.The held-out set was used to decide which modelperformed best.
Actual performmme of the modelsshould be judged with rest)ect o the testing set.Evaluation was in terIns of exact match: tbr eachsentence in the test set, we awarded ourselves at)oint if the RFM ranked highest he same parse thatwas ranked highest using the reference probabilities.When evahmting with respect to the held-out set,we recovered all parses for sentences in the held-outset.
When evaluating with respect o the testing-set,we recovered at most 100 parses per sentence.For each run, we ran IIS for the same numberof iterations (20).
In each case, we evaluated theRFM after each other iteration and recorded the bestclassification pertbrmance.
This step was designedto avoid overfitting distorting our results.Figure 2 shows the results we obtained with pos-sible ways of picking 'typical' samples.
The firstcolumn shows the maxinmm number of parses persentences that we retrieved in each sample.The second column shows the size of the sample(in parses).The other cohmms give classification accuracy re-sults (a percentage) with respect o the testing set.In parentheses, we give performance with respect; tothe held-out set.The column marked Rand shows the performanceMax parses Size1 66262 123313 170265 2487810 39581100 1196941000 246686oo 267400Rand SCFG Ref25.2 (51.7) 23.3 (59.0) 23.4 (50.0)37.9 (63.0) 40.4 (60.3) 40.4 (60.0)43.2 (65.5) 43.7 (63.8) 43.7 (63.8)43.7 (70.2) 45.8 (69.5) 45.8 (69.5)47.4 (72.0) 47.0 (70.0) 46.9 (70.0)45.0 (68.7) 45.0 (68.0) 45.0 (68.0)44.4 (67.4) 43.0 (67.0) 43.0 (67.0)43.0 (66.0) 43.0 (66.0) 43.0 (66.0)Figure 2: Results with various sampling strategiesof runs that used a sample that contained parseswhich were randomly and uniformly selected out ofthe set, of all possible parses.
The classification ac-curacy results for this sampler are averaged over 10runs.The column marked SCFG shows the results ob-tained when using a salnple that contained 1)arsesthat were retrieved using the probabilistic unI)ackingstrategy.
This did not involve retrieving all possibleparses for each sentence in the training set,.
Sincethere is no random component, he results arc fl'om asingle run.
Here, parses were ranked using a stochas-tic context free backbone approximation ofTSG.
Pa-rameters were estimated using simple counting.FinMly, the eohunn marked Ref shows the re-sults ol)tained when USillg a sample that containedthe overall n-best parses per sentence, as defined interms of the reference distril)ution.As a baseline, a nlodel containing randomly as-signed weights produced a classification accuracy of45% on the held-out sentences.
These results wereaveraged over 10 runs.As can be seen, increasing the sainple size pro-duces better results (for ca& smnl)ling strategy).Around a smnple size of 40k parses, overfitting startsto manifest, and perIbrmance bottoms-out.
One ofthese is therefore our inforinative sample.
Note thatthe best smnple (40k parses) is less than 20% of thetotal possible training set.The ditference between the various samplers ismarginal, with a slight preference for Rand.
How-ever the fact that SUFG sampling seems to do ahnostas well as Rand sampling, and fllrthermore does notrequire unpacking all parses, makes it the samplingstrategy of choice.SCFG sampling is biased in the sense that thesample produced using it will tend to concentratearound those parses that are all close to the best,parses.
Rand smnpling is unbiased, and, aparth'om the practical problems of having to recover allparses, nfight in some circumstances be better thanSCFG sampling.
At the time of writing this paper,it was unclear whether we could combine SCFG withRand sampling -sample parses from the flfll distribu-590lion without unpacking all parses.
We suspect hatfor i)robabilistic unt)acking to be efficient, it nmst\]:ely upon some non-uniform distribution.
Unpack-ing randomly and uniformly would probably resultin a large loss in computational e iiciency.6.2 Larger  Scale Eva luat ionHere we show results using a larger salnl)le and test-ing set.
We also show the effects of lexicalisation,overtitting, and overfitting avoidance using a Gaus-sian prior.
Strictly speaking this section could havebeen omitted fl'om the paper.
However, if one viewsestimation using an informative sami)le as overfit-ling avoi(lance, then  estimation using a Gaussianl)rior Call be seen as another, complementary takeon the problem.The experimental setup was as follows.
We rall-domly split the Wall St, reel: Journal corpus into atraining set and a testing set.
Both sets containedsentence.s t;hat were at most 30 tokens hmg.
Whencreating the set of parses used to estimate Ii.FMs, weused the SCFG approach, and retained the top 25parses per sentence.
Within the training set (arisingDora 16, 200 sentences), there were 405,020 parses.The testing set consisted of 466 sentences, with anaverage of 60.6 parses per sentence.When evahmtillg, we retrieved at lllOSt 100 lmrscsper sentence in the tes t ing  set and  scored them usingour reference distribution.
As lmfore, we awardedourselves a i)oinl; if the most probable testing parse(in terms of the I/.MF) coincided with the most t)rol)-able parse (in terms of the reference distribution).
Inall eases, we ran IIS tbr 100 iterations.For the tirst experiment, we used just the firsttelnp\]at('.
(features that rc'la.t(;d to DC(I insl;antia-tions) to create model l; the second experiment uso.dthe first and second teml)lat(~s (additional t'eatm'o.srelating to PP attachment) o create model 2.
Thelinal experiment used all three templat('~s (additionalfea,tl lres that were head-lexicalised) to create model3.The three mo(lels contained 39,230, 65,568 and278, 127 featm:es respectively,As a baseline, a model containing randomly as-signed weights achieved a 22% classification accu-racy.
These results were averaged over 10 runs.
Fig-ure 3 shows the classification accuracy using models1, 2 and 3.As can 1)e seen, the larger scale exl)erimentalresults were better than those achieved using thesmaller samples (mentioned in section 6.1).
The rea-Sell for this was because we used longer sentc,11ces.The.
informative sainple derivable Kern such a train-ing set was likely to be larger (more representative of54525Oo>,~, 48o<464442I I i .
I ,  .
A .
.
I i .
.
.
I .
.~'\ .
, / ' , - - -  " ' .
.
. '"
.
.
.
.
..model1 .
.
.
.
.
.
.
."
f f  ~ " ~  ~L model2 .
.
.
.
.
./ .
.
.
.
, '  ~ model3 .
.
.
.
_,, ,,,,- .
.
.
.
.
.
.
.
.
.
~_ \ / .
.~  \10 20 30 40 50 60 70 80 90 100IterationsFigure 3: Classification Accuracy tbr Three ModelsEstinmted using Basic IIS56 - - ~  r r \] l 1 l 1 7545250o~~: 4a464442model1 -- - -model2 .....0 10 20 30 40 50 60 70 80 90 1 O0IterationsFigure .l: Classification Accuracy for .\[hre(.
ModelsEstinmted using a Gmlssian Prior and IISthe population) than the informative sample deriv-al)led from a training set using shorter, less syntat'-tically (Xmll)lex senten(:es.
With the unle.xicalisedmodel, we see (:lear signs of overfitting.
Model 2overfits even more so.
For reasons that are unclear,we see that the larger model 3 does not ai)pem: toexhibit overtitting.We next used the Gaussian Prior method ofChen and Rosenfeld to reduce overfitting (Chenand Rosenfeld, 1999b).
This involved integratinga Gaussian prior (with a zero mean) into Ills andsearching for the model that maximised the, prod-uct of the likelihood and prior prolmbilities.
For theexperiments reported here, we used a single wlri-ante over the entire model (better results might beachievable if multiple variances were used, i)erhapswith one variance per telnl)late type).
The aetllalvalue of the variance was t'cmnd by trial-and-error.Itowever, optimisation using a held-out set is easyto achieve,.591We repeated the large-scale xperiment, but thistime using a Gaussian prior.
Figure 4 shows theclassification accuracy of the models when using aGmlssian Prior.When we used a Gaussian prior, we fotmd that allmodels showed signs of imt)rovenmnt (allbeit withvarying degrees): performance ither increased, orelse did not decrease with respect to the munberof iterations, Still, model 2 continued to underper-form.
Model 3 seemed most resistent o the prior.It theretbre appears that a Gaussian prior is mostuseful for unlexicalised models, and that for mod-els built from complex, overlapping features, otherforms of smoothing must be used instead.7 CommentsWe argued that RFM estimation tbr broad-coverageattribute-valued grammars could be made eompu-tationally tractable by training upon an inforlna-tive sample.
Our small-scale xperiments suggestedthat using those parses that could be etliciently un-packed (SCFG sampling) was ahnost as effective assampling from all possible parses (R~and salnplillg).Also, we saw that models should not be both builtand also estimated using all possible parses.
Betterresults can be obtained when models m'e built andtrained using an intbrmative san@e.Given the relationshi I) between sample size andmodel complexity, we see that when there is a dan-ger of overfitting, one should build models on the ba-sis of all informative set.
Itowever, this leaves openthe possil)ility of training such a model upon a su-1)erset of the, informative set;.
Although we ha.re nottested this scenario, we believe that this would leadto t)etter esults ttlan those achieved here.The larger scale experiments showed that I{FMscan be estimated using relatively long sentences.They also showed that a simple Gaussian prior couldreduce the etfects of overfitting.
However, they alsoshowed that excessive overfitting probably requiredan alternative smoothing approach.The smaller and larger experiments can be bothviewed as (complementary) ways of dealing withoverfitting.
We conjecture that of the two ap-proaches, the informative smnple al)proach is prefer-able as it deals with overfitting directly: overfittingresults fi'om fitting to complex a model with too lit-tle data.Our ongoing research will concentrate uponstronger ways of dealing with overfitting in lexi-calised RFMs.
One line we are pursuing is to com-bine a compression-based prior with an exponentialmodel.
This blends MDL with Maximum Entropy.We are also looking at alternative template sets.For example, we would probably benefit fi'om usingtemplates that capture more of the syntactic ontextof a rule instantiation.AcknowledgmentsWe would like to tliank Rob Malouf, Domfla NieGearailt and tim anonymous reviewers for com-ments.
This work was supported by tile TMRProject Lcar'nin9 Computational Grammars.ReferencesSt, even P. Atmey.
1997.
Stochastic Attribute-Value Grmmnm:s. Computational Linguistics,23(4):597- 618, December.Miles Osborne 19?9.
DCG induction using MDLand Parsed Corpora.
In James Cussens, editor,Lcarnin9 Langua9 c in Logic, pages 63-71, Bled,Slovenia, June.Ted Briscoe and John Carroll.
1.996.
AutolnaticExtraction of Subcategorization from Corpora.In Proceedings of the 5 th Conference on AppliedNLP, p~ges 356-363, Washington, DC.John Carroll and Ted Briscoe.
1992.
Probabilis-tic Normalisation and Unpacking of Paclmd ParseForests for Unification-lmse, d Grmnmars.
Ill Pro-cccdi,n9 s of the AAAI  Fall Symposi'u,m on P~vb-abilistic AppTvach, es to Natural Language , pages33-38, Cambridge, MA.Stanley Chen and Honald l{osenfeld.
1999a.
Effi-cient Sampling and Feature Selection in WholeSentence Maxinmin Entrol)y Language Models.
InICA SSP '99.Stanley F. Chen and Ronald Rosenfeld.
1999b.A Gaussian Prior for Smoothing Maxinmm \]211-tropy Models.
Technical Rel)ort CMU-CS-99-108,Carnegie Mellon University.Eirik Hektoen.
1997.
Probabilistic Parse Select;ionBased on Semantic Cooet:l lr l 'el lees.
\]ill Pl'og('.cd-ings oJ" th, e 5th l'ntc, r'national Wo~wkh, op on ParsingTcch, nolo.qics, Cambridge, Massach'usctts, 1)ages113 122.Marl< Johnson, Stuart Geman, Stephen Cannon,Zhiyi Chi, and Stephan Riezler.
1999.
Esl, inmtorsfor Stochastic "Unification-based" (~rammars.
In37 th Annual Meeting of the ACL,J.
Latferty, S. Della Pietra, and V. Della Pietra.1997.
Inducing Features of Random Fields.
1EEETransactions on Pattern Analysis and Mach, incIntclligcncc, 19(4):380 393, April.Jorma Rissanen.
1989.
Stochastic Complezity inStatistical i'nquiry.
Series in Computer Science -Volmne 15.
World Scientific.592
