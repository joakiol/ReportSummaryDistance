Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26?35,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Neural Network Approach to Selectional Preference AcquisitionTim Van de CruysIRIT & CNRSToulouse, Francetim.vandecruys@irit.frAbstractThis paper investigates the use of neuralnetworks for the acquisition of selectionalpreferences.
Inspired by recent advancesof neural network models for NLP applica-tions, we propose a neural network modelthat learns to discriminate between felici-tous and infelicitous arguments for a par-ticular predicate.
The model is entirely un-supervised ?
preferences are learned fromunannotated corpus data.
We propose twoneural network architectures: one that han-dles standard two-way selectional prefer-ences and one that is able to deal withmulti-way selectional preferences.
Themodel?s performance is evaluated on apseudo-disambiguation task, on which itis shown to achieve state of the art perfor-mance.1 IntroductionPredicates often have a semantically motivated pref-erence for particular arguments.
Compare for ex-ample the sentences in (1) and (2).
(1) The vocalist sings a ballad.
(2) The exception sings a tomato.Most language users would have no problems ac-cepting the first sentence as well-formed: a vocalistcan be expected to sing, and a ballad is somethingthat can be sung.
The same language users, how-ever, would likely consider the second sentence tobe ill-formed: an exception is not supposed to sing,nor is a tomato something that is typically sung.Within the field of natural language processing,this inclination of predicates to select for particulararguments is known as selectional preference.The automatic acquisition of selectional prefer-ences has been a popular research subject withinthe field of natural language processing.
An auto-matically acquired selectional preference resourceis a versatile tool for numerous NLP applications,such as semantic role labeling (Gildea and Jurafsky,2002), word sense disambiguation (McCarthy andCarroll, 2003), and metaphor processing (Shutovaet al., 2013).Models for selectional preference need to ade-quately deal with the consequences of Zipf?s law:language is inherently sparse, and the majority oflanguage utterances occur very infrequently.
Asa consequence, models that are based on corpusdata need to properly generalize beyond the mereco-occurrence frequencies of sparse corpus data,taking into account the semantic similarity of bothpredicates and arguments.
Researchers have comeup with various approaches to this generalizationstep.
Earlier approaches to selectional preferenceacquisition mostly rely on hand-crafted resourcessuch as WordNet (Resnik, 1996; Li and Abe, 1998;Clark and Weir, 2001), while later approaches tendto take advantage of unsupervised learning machin-ery, such as latent variable models (Rooth et al.,1999;?O S?eaghdha, 2010) and distributional simi-larity metrics (Erk, 2007; Pad?o et al., 2007).This paper investigates the use of neural net-works for the acquisition of selectional preferences.Inspired by recent advances of neural network mod-els for NLP applications (Collobert and Weston,2008; Mikolov et al., 2013), we propose a neuralnetwork model that learns to discriminate betweenfelicitous and infelicitous arguments for a particu-lar predicate.
The model is entirely unsupervised ?preferences are learned from unannotated corpusdata.
Positive training instances are constructedfrom attested corpus data, while negative instancesare constructed from randomly corrupted instances.We propose two neural network architectures: onethat handles standard two-way selectional prefer-ences and one that is able to deal with multi-wayselectional preferences, where the interaction be-26tween multiple verb arguments is taken into ac-count.
The model?s performance is evaluated on apseudo-disambiguation task, on which it is shownto achieve state of the art performance.The contributions of this paper are twofold.
Firstof all, we apply and evaluate a neural network ap-proach to the problem of standard (two-way) se-lectional preference acquisition.
Selectional pref-erence acquisition using neural networks has notyet been explored in the literature.
Secondly, wepropose a novel network architecture and trainingobjective for the acquisition of multi-way selec-tional preferences, where the interaction betweena verb and its various arguments is captured at thesame time.The remainder of this paper is as follows.
Sec-tion 2 first discusses related work with respect to se-lectional preference acquisition and neural networkmodeling.
Section 3 describes our neural networkarchitecture and its training procedure.
Section 4evaluates the model?s performance, comparing itto other existing models for selectional preferenceacquisition.
Finally, section 5 concludes and indi-cates a number of avenues for future work.2 Related Work2.1 Selectional preferencesOne of the first approaches to the automatic induc-tion of selectional preferences from corpora wasthe one by Resnik (1996).
Resnik (1996) relieson WordNet synsets in order to generate gener-alized noun clusters.
The selectional preferencestrength of a specific verb v in a particular relationis calculated by computing the Kullback-Leiblerdivergence between the cluster distribution of theverb and the prior cluster distribution.SR(v)=?cp(c|v) logp(c|v)p(c)(1)where c stands for a noun cluster, and R stands for agiven predicate-argument relation.
The selectionalassociation of a particular noun cluster is then thecontribution of that cluster to the verb?s preferencestrength.AR(v,c)=p(c|v) logp(c|v)p(c)SR(v)(2)The model?s generalization relies entirely on Word-Net, and there is no generalization among the verbs.Other researchers have equally relied on Word-Net in order to generalize over arguments.
Li andAbe (1998) use the principle of Minimum Descrip-tion Length in order to find a suitable generalizationlevel within the lexical WordNet hierarchy.
A sameintuition is used by Clark and Weir (2001), but theyuse hypothesis testing instead to find the appro-priate level of generalization.
A recent approachthat makes use of WordNet (in combination withBayesian modeling) is the one by?O S?eaghdha andKorhonen (2012).Most researchers, however, acknowledge theshortcomings of hand-crafted resources, and fo-cus on the acquisition of selectional preferencesfrom corpus data.
Rooth et al.
(1999) propose anExpectation-Maximization (EM) clustering algo-rithm for selectional preference acquisition basedon a probabilistic latent variable model.
The ideais that both predicate v and argument o are gen-erated from a latent variable c, where the latentvariables represent clusters of tight verb-argumentinteractions.p(v,o) =?c?Cp(c,v,o) =?c?Cp(c)p(v|c)p(o|c) (3)The use of latent variables allows the model togeneralize to predicate-argument tuples that havenot been seen during training.
The latent variabledistribution ?
and the probabilities of predicatesand argument given the latent variables ?
are au-tomatically induced from data using EM.
We willcompare against their model for evaluation pur-poses.Erk (2007) and Erk et al.
(2010) describe amethod that uses corpus-driven distributional simi-larity metrics for the induction of selectional pref-erences.
The key idea is that a predicate-argumenttuple (v,o) is felicitous if the predicate v appearsin the training corpus with arguments o?that aresimilar to o, i.e.S(v,o) =?o??Ovwt(v,o?)Z(v)?
sim(o,o?)
(4)where Ovrepresents the set of arguments that havebeen attested with predicate v, wt(?)
represents anappropriate weighting function (such as the fre-quency of the (v,o?)
tuple), and Z is a normaliza-tion factor.
We equally compare to their model forevaluation purposes.Bergsma et al.
(2008) present a discriminativeapproach to selectional preference acquisition.
Pos-itive examples are taken from observed predicate-27argument pairs, while negative examples are con-structed from unobserved combinations.
An SVMclassifier is used to distinguish the positive from thenegative instances.
The training procedure used intheir model is based on an intuition that is similarto ours, although it is implemented using differenttechniques.A number of researchers presented models thatare based on the framework of topic modeling.
?OS?eaghdha (2010) describes three models for selec-tional preference induction based on Latent Dirich-let Allocation, which model the selectional pref-erence of a predicate and a single argument.
Rit-ter et al.
(2010) equally present a selectional pref-erence model based on topic modeling, but theytackle multi-way selectional preferences (of transi-tive predicates, which take two arguments) instead.Finally, in previous work (Van de Cruys, 2009)we presented a model for multi-way selectionalpreference induction based on tensor factorization.Three-way co-occurrences of subjects, verbs, andobjects are represented as a three-way tensor (thegeneralization of a matrix), and a latent factoriza-tion model is applied in order to generalize tounseen instances.
We will compare our neuralnetwork based-approach for multi-way selectionalpreference acquisition to this tensor-based factor-ization model.2.2 Neural networksIn the last few years, neural networks have becomeincreasingly popular in NLP applications.
In partic-ular, neural language models (Bengio et al., 2003;Mnih and Hinton, 2007; Collobert and Weston,2008) have demonstrated impressive performanceat the task of language modeling.
By incorporatingdistributed representations for words that modeltheir similarity, neural language models are ableto overcome the problem of data sparseness thatstandard n-gram models are confronted with.
Alsorelated to our work is the approach by Tsubaki etal.
(2013), who successfully use a neural networkto model co-compositionality.Our model for selectional preference acquisitionuses a network architecture that is similar to theabovementioned models.
Its training objective isalso similar to the ranking-loss training objectiveproposed by Collobert and Weston (2008), but wepresent a novel, modified version in order to dealwith multi-way selectional preferences.3 Methodology3.1 Neural network architectureOur model computes the score for a predicate iand an argument j as follows.
First, the selectionalpreference tuple (i, j) is represented as the concate-nation of the vectors viand oj, i.e.x= [vi,oj] (5)Vectors viand ojare extracted from two embeddingmatrices, V ?
RN?I(the predicate matrix, where Irepresents the number of elements in the predicatevocabulary) and O ?
RN?J(the argument matrix,where J represents the number of elements in theargument vocabulary).
N is a parameter setting ofthe model, representing the vector size of the em-beddings.
Matrices V and O will be automaticallylearned during training.Vector x then serves as input vector to our neuralnetwork.
We use a feed-forward neural networkarchitecture with one hidden layer:a1= f (W1x+b1) (6)y = W2a1(7)where x ?
R2Nis our input vector, a1?
RHrepre-sents the activation of the hidden layer with H hid-den nodes, W1?
RH?2Nand W2?
R1?Hrespec-tively represent the first and second layer weights,b1represents the first layer?s bias, f (?)
representsthe element-wise activation function tanh, and y isour final selectional preference score.
The left-handpicture of figure 1 gives a graphical representationof our standard neural network architecture.3.2 Training the networkA proper estimation of a neural network?s param-eters requires a large amount of training data.
Tobe able to use non-annotated corpus data for train-ing, we use the method proposed by Collobert andWeston (2008).
The authors present a method fortraining a neural network language model from un-labeled data by corrupting actual attested n-gramswith a random word.
They then define a ranking-type cost function, which allows the network tolearn to discriminate between good and bad wordsequences.
We adopt the same method for our se-lectional preference model as follows.Let (i, j) be our proper, attested predicate-argument tuple.
The goal of our model is to dis-criminate the correct tuple (i, j) from other, non-attested tuples (i, j?
), in which the correct predicate28V iO jj W2W1a1x yV iO jk W2W1a1xyS jjFigure 1: Neural network architectures for selectional preference acquisition.
The left-hand picture showsthe architecture for two-way selectional preferences, the right-hand picture shows the architecture forthree-way selectional preferences.
In both cases, vector x is constructed from the appropriate predicateand argument vectors from the embedding matrices, and fed forward through the network to yield apreference score y.j has been replaced with a random predicate j?.
Werequire the score for the correct tuple to be largerthan the score for the corrupt tuple by a marginof one.
For one tuple (i, j), this corresponds tominimizing the objective function in (8)?j?
?Jmax(0,1?g[(i, j)]+g[(i, j?)])
(8)where J represents the predicate vocabulary, andg[?]
represents our neural network scoring functionpresented in the previous section.In line with Collobert and Weston (2008), thegradient of the objective function is sampled byrandomly picking one corrupt argument j?from theargument vocabulary for each attested predicate-argument tuple (i, j).
The derivative of the costwith respect to the model?s parameters (weight ma-trices W1and W2, bias vector b1, and embeddingmatrices V and O) is computed, and the appropriateparameters are updated through backpropagation.3.3 Multi-way selectional preferencesThe model presented in the previous section isonly able to deal with two-way selectional pref-erences.
In this section, we present an extension ofthe model that is able to handle multi-way selec-tional preferences.11We exemplify the model using three-way selectional pref-erences for transitive predicates, but the model can be straight-forwardly generalized to other multi-way selectional prefer-ences.In order to model the selectional preference of atransitive verb for its subject and direct object, westart out in a similar fashion to the two-way case.Instead of having only one embedding matrix, wenow have two embedding matrices S ?
RN?JandO?RN?K, representing the two different argumentslots of a transitive predicate.
Our input vector cannow be represented asx= (vi,sj,ok) (9)Note that x ?
R3Nand W1?
RH?3N.
The rest ofour neural network architecture stays exactly thesame.
The right-hand picture of figure 1 presents agraphical representation.For the multi-way case, we present an adaptedversion of the training objective.
Given an attestedsubject-verb-object tuple (i, j,k), the goal of ournetwork is now to discriminate this correct tuplefrom other, corrupted tuples (i, j,k?
), (i, j?,k) and(i, j?,k?
), where the correct arguments have beenreplaced by random subjects j?and random objectsk?.
Note that we do not only want the networkto learn the infelicity of tuples in which both thesubject and object slot are corrupted; we also wantour network to learn the infelicity of tuples in whicheither the subject or object slot is corrupt, while theother slot contains the correct, attested argument.This leads us to the objective function representedin (10).29?k?
?Kmax(0,1?g[(i, j,k)]+g[(i, j,k?)])+?j?
?Jmax(0,1?g[(i, j,k)]+g[(i, j?,k)])+?j??Jk?
?Kmax(0,1?g[(i, j,k)]+g[(i, j?,k?)])
(10)As in the two-way case, the gradient of the objec-tive function is sampled by randomly picking onecorrupted subject j?and one corrupted object k?foreach tuple (i, j,k).
All of the model?s parametersare again updated through backpropagation.4 Evaluation4.1 Implementational detailsWe evaluate our neural network approach to se-lectional preference acquisition using verb-objecttuples for the two-way model, and subject-verb-object tuples for the multi-way model.Our model has been applied to English, using theUKWaC corpus (Baroni et al., 2009), which coversabout 2 billion words of web text.
The corpushas been part of speech tagged and lemmatizedwith Stanford Part-Of-Speech Tagger (Toutanovaet al., 2003), and parsed with MaltParser (Nivreet al., 2006), so that dependency tuples could beextracted.For the two-way model, we select all verbs andobjects that appear within a predicate-argument re-lation with a frequency of at least 50.
This givesus a total of about 7K verbs and 30K objects.
Forthe multi-way model, we select the 2K most fre-quent verbs, together with the 10K most frequentsubjects and the 10K most frequent objects (thatappear within a transitive frame).All words are converted to lowercase.
We usethe lemmatized forms, and only keep those formsthat contain alphabetic characters.
Furthermore,we require each tuple to appear at least three timesin the corpus.We set N, the size of our embedding matrices, to50, and H, the number of units in the hidden layer,to 100.
Following Huang et al.
(2012), we usemini-batch L-BFGS (Liu and Nocedal, 1989) with1000 pairs of good and corrupt tuples per batch fortraining, and train for 10 epochs.4.2 Evaluation Setup4.2.1 TaskOur models are quantitatively evaluated using apseudo-disambiguation task (Rooth et al., 1999),which bears some resemblance to our training pro-cedure.
The task provides an adequate test of thegeneralization capabilities of our models.
For thetwo-way case, the task is to judge which object (oor o?)
is more likely for a particular verb v, where(v,o) is a tuple attested in the corpus, and o?is a di-rect object randomly drawn from the object vocab-ulary.
The tuple is considered correct if the modelprefers the attested tuple (v,o) over (v,o?).
For thethree-way case, the task is to judge which subject(s or s?)
and direct object (o or o?)
are more likelyfor a particular verb v, where (v,s,o) is the attestedtuple, and s?and o?are a random subject and objectdrawn from their respective vocabularies.
The tu-ple is considered correct if the model prefers theattested tuple (v,s,o) over the alternatives (v,s,o?
),(v,s?,o), and (v,s?,o?).
Tables 1 and 2 respectivelyshow a number of examples from the two-way andthree-way pseudo-disambiguation task.v o o?perform play geometrybuy wine renaissanceread introduction peanutTable 1: Pseudo-disambiguation examples for two-way verb-object tuplesv s o s?o?win team game diversity eggpublish government document grid priestdevelop company software breakfast landlordTable 2: Pseudo-disambiguation examples forthree-way subject-verb-object tuplesThe models are evaluated using 10-fold crossvalidation.
All tuples from our corpus are randomlydivided into 10 equal parts.
Next, for each fold, 9parts are used for training, and the remaining partis used for testing.
In order to properly test thegeneralization capability of our models, we makesure that all instances of a particular tuple appear inone part only.
This way, we make sure that tuplesused for testing are never seen during training.For the two-way model, our corpus consists ofabout 70M tuple instances (1.9M types), so in each30fold, about 63M tuple instances are used for train-ing and about 7M (190K types) are used for testing.For the three-way model, our corpus consists ofabout 5,5M tuple instances (750K types), so ineach fold, about 5M tuples are used for trainingand about 500K (75K types) are used for testing.Note that our training procedure is instance-based,while our evaluation is type-based: during training,the neural network sees a tuple as many times as itappears in the training set, while for testing eachindividual tuple is only evaluated once.4.2.2 Comparison modelsWe compare our neural network model to a numberof other models for selectional preference acquisi-tion.For the two-way case, we compare our modelto the EM-based clustering technique presentedby Rooth et al.
(1999),2and to Erk et al.
?s (2010)similarity-based model.
For Rooth et al.
?s model,we set the number of latent factors to 50.
Us-ing a larger number of latent factors does not in-crease performance.
For Erk et al.
?s model, wecreate a dependency-based similarity model fromthe UKWaC corpus using our 30K direct objectsas instances and 100K dependency relations asfeatures.
The resulting matrix is weighted usingpointwise mutual information (Church and Hanks,1990).
Similarity values are computed using cosine.Furthermore, we use a sampling procedure in thetesting phase: we sample 5000 predicate-argumentpairs for each fold, as testing Erk et al.
?s model onthe complete test sets proved prohibitively expen-sive.For the three-way case, we compare our modelto the tensor factorization model we developed inprevious work (Van de Cruys, 2009).
We set thenumber of latent factors to 300.34.3 Results4.3.1 Two-way modelTable 3 compares the results of our neural networkarchitecture for two-way selectional preferences tothe results of Rooth et al.
?s (1999) model and Erket al.
?s (2010) model.2Our own implementation of Rooth et al.
?s (1999) al-gorithm is based on non-negative matrix factorization (Leeand Seung, 2000).
Non-negative matrix factorization withKullback-Leibler divergence has been shown to minimize thesame objective function as EM (Li and Ding, 2006).3The best scoring model presented by Van de Cruys (2009)also uses 300 latent factors; using more factors does not im-prove the results.model accuracy (???
)Rooth et al.
(1999) .720 ?
.002Erk et al.
(2010) .887 ?
.0042-way neural network .880 ?
.001Table 3: Comparison of model results for two-wayselectional preference acquisition ?
mean accuracyand standard deviations of 10-fold cross-validationresultsThe results indicate that our neural network ap-proach outperforms Rooth et al.
?s (1999) methodby a large margin (16%).
Clearly, the neural net-work architecture is able to model selectional pref-erences more profoundly than Rooth et al.
?s latentvariable approach.
The difference between themodels is highly statistically significant (pairedt-test, p < .01), as the standard deviations alreadyindicate.Erk et al.
?s model reaches a slightly better scorethan our model, and this result is also statisticallysignificant (paired t-test, p < .01).
However, Erk etal.
?s model does not provide full coverage, whereasthe other two models are able to compute scoresfor all pairs in the test set.
In addition, Erk et al.
?smodel is much more expensive to compute.
Ourmodel computes selectional preference scores forthe test set in a matter of seconds, whereas forErk et al.
?s model, we ended up sampling fromthe test set, as computing preference values for thecomplete test set proved prohibitively expensive.4.3.2 Three-way modelTable 4 compares the results of our neural networkarchitecture for three-way selectional preferenceacquisition to the results of the tensor-based factor-ization method (Van de Cruys, 2009).model accuracy (???
)Van de Cruys (2009) .874 ?
.0013-way neural network .889 ?
.001Table 4: Comparison of model results for three-wayselectional preference acquisition ?
mean accuracyand standard deviations of 10-fold cross-validationresultsThe results indicate that the neural network ap-proach slightly outperforms the tensor-based factor-ization method.
Again the model difference is sta-31tistically significant (paired t-test, p< 0.01).
Usingour adapted training objective, the neural networkis clearly able to learn a rich model of three-wayselectional preferences, reaching state of the artperformance.4.4 ExamplesWe conclude our results section by briefly present-ing a number of examples that illustrate the kindof semantics present in our models.
Similar to neu-ral language models, the predicate and argumentembedding matrices of our neural network con-tain distributed word representations, that capturethe similarity of predicates and arguments to otherwords.Tables 5 and 6 contain a number of nearest neigh-bour similarity examples for predicate and argu-ments from our two-way neural network model.The nearest neighbours were calculated using stan-dard cosine similarity.DRINK PROGRAM INTERVIEW FLOODSIP RECOMPILE RECRUIT INUNDATEBREW UNDELETE PERSUADE RAVAGEMINCE CODE INSTRUCT SUBMERGEFRY IMPORT PESTER COLONIZETable 5: Nearest neighbours of 4 verbs, calculatedusing the distributed word representations of em-bedding matrix V from our two-way neural net-work modelTable 5 indicates that the network is effectivelyable to capture a semantics for verbs.
The firstcolumn ?
verbs similar to DRINK ?
all have to dowith food consumption.
The second column con-tains verbs related to computer programming.
Thethird column is related to human communication;and the fourth column seems to illustrate the net-work?s comprehension of FLOOD having to do withinvasion and water.PAPER RASPBERRY SECRETARY DESIGNERBOOK COURGETTE PRESIDENT PLANNERJOURNAL LATTE MANAGER PAINTERARTICLE LEMONADE POLICE SPECIALISTCODE OATMEAL EDITOR SPEAKERTable 6: Nearest neighbours of 4 direct objects, cal-culated using the distributed word representationsof embedding matrix O from our two way neuralnetwork modelSimilarly, table 6 shows the network?s ability tocapture the meaning of nouns that appear as directobjects to the verbs.
Column one contains thingsthat can be read.
Column two contains things thatcan be consumed.
Column three seems to hint atsupervising professions, while column four seemsto capture creative professions.A similar kind of semantics is present in the em-bedding matrices of the three-way neural networkmodel.
Tables 7, 8, and 9 again illustrate this usingword similarity calculations.SEARCH DIMINISH CONFIGURE PROSECUTECLICK LESSEN AUTOMATE CRITICISEBROWSE DISTORT SCROLL URGESCROLL HEIGHTEN PROGRAM DEPLOREUPLOAD DEGRADE INSTALL CONDEMNTable 7: Nearest neighbours of 4 verbs, calculatedusing the distributed word representations of em-bedding matrix V from our three-way neural net-work modelTable 7 shows the network?s verb semantics forthe three-way case.
The first column is related tointernet usage, the second column contains verbsof scalar change, column three is again related tocomputer usage, and column four seems to capture?mending?
verbs.FLOWER COLLEGE PRESIDENT SONGFISH UNIVERSITY BUSH FILMBIRD INSTITUTE BLAIR ALBUMSUN DEPARTMENT MP PLAYTREE CENTRE CHAIRMAN MUSICTable 8: Nearest neighbours of 4 subjects, calcu-lated using the distributed word representations ofembedding matrix S from our three way neuralnetwork modelTable 8 illustrates the semantics for the subjectslot of our three-way model.
The first column cap-tures nature terms, the second column containsuniversity-related terms, the third column containspoliticians/government terms, and the fourth col-umn contains art expressions.Finally, table 9 demonstrates the semantics ofour three-way model?s object slot.
Column onegenerally contains housing terms, column two con-tains various locations, column three contains din-ing occasions, and column four contains textualexpressions.32WALL PARK LUNCH THESISFLOOR STUDIO DINNER QUESTIONNAIRECEILING VILLAGE MEAL DISSERTATIONROOF HALL BUFFET PERIODICALMETRE MUSEUM BREAKFAST DISCOURSETable 9: Nearest neighbours of 4 direct objects, cal-culated using the distributed word representationsof embedding matrix O from our three way neuralnetwork modelNote that the embeddings for the subject andthe object slot is different, although they mostlycontain the same words.
This allows the model tocapture specific semantic characteristics for wordsgiven their argument position.
Virus, for example,is in subject position more similar to active wordslike animal, whereas in object position, it is moresimilar to passive words like cell, device.
Similarly,mouse in subject position tends to be similar towords like animal, rat whereas in object position itis similar to words like web, browser.These examples, although anecdotal, illustratethat our neural network model is able to capture arich semantics for predicates and arguments, whichsubsequently allows the network to make accuratepredictions with regard to selectional preference.5 Conclusion and future workIn this paper, we presented a neural network ap-proach to the acquisition of selectional preferences.Inspired by recent work on neural language models,we proposed a neural network model that learnsto discriminate between felicitous and infelicitousarguments for a particular predicate.
The model isentirely unsupervised, as preferences are learnedfrom unannotated corpus data.
Positive traininginstances are constructed from attested corpus data,while negative instances are constructed from ran-domly corrupted instances.
Using designated net-work architectures, we are able to handle stan-dard two-way selectional preferences as well asmulti-way selectional preferences.
A quantitativeevaluation on a pseudo-disambiguation task showsthat our models achieve state of the art perfor-mance.
The results for our two-way neural networkare on a par with Erk et al.
?s (2010) similarity-based approach, while our three-way neural net-work slightly outperforms the tensor-based factor-ization model (Van de Cruys, 2009) for multi-wayselectional preference induction.We conclude with a number of issues for futurework.
First of all, we would like to investigate howour neural network approach might be improved byincorporating information from other sources.
Inparticular, we think of initializing our embeddingmatrices with distributed representations that comefrom a large-scale neural language model (Mikolovet al., 2013).
We also want to further investigatethe advantages and disadvantages of having dif-ferent embedding matrices for different argumentpositions in our multi-way neural network.
In ourresults section, we demonstrated that such an ap-proach allows for more flexibility, but it also addsa certain level of redundancy.
We want to inves-tigate the benefit of our approach, compared to amodel that shares the distributed word representa-tion among different argument positions.
Finally,we want to investigate more advanced neural net-work architectures for the acquisition of selectionalpreferences.
In particular, neural tensor networks(Yu et al., 2013) have recently demonstrated im-pressive results in related fields like speech recogni-tion, and might provide the necessary machinery tomodel multi-way selectional preferences in a moreprofound way.ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preferencefrom unlabeled text.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 59?68.
Association for Computa-tional Linguistics.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information & lexicogra-phy.
Computational Linguistics, 16(1):22?29.Stephen Clark and David Weir.
2001.
Class-basedprobability estimation using a semantic hierarchy.In Proceedings of the second meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Language technologies, pages95?102.
Association for Computational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deep33neural networks with multitask learning.
In Pro-ceedings of the 25th international conference on Ma-chine learning, pages 160?167.
ACM.Katrin Erk, Sebastian Pad?o, and Ulrike Pad?o.
2010.
Aflexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics,36(4):723?763.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 216?223, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational linguis-tics, 28(3):245?288.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Annual Meeting of the Associationfor Computational Linguistics (ACL).Daniel D. Lee and H. Sebastian Seung.
2000.
Al-gorithms for non-negative matrix factorization.
InAdvances in Neural Information Processing Systems13, pages 556?562.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the MDL principle.Computational linguistics, 24(2):217?244.Tao Li and Chris Ding.
2006.
The relationships amongvarious nonnegative matrix factorization methodsfor clustering.
In Data Mining, 2006.
ICDM?06.Sixth International Conference on, pages 362?371.IEEE.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical programming, 45(1-3):503?528.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Compu-tational Linguistics, 29(4):639?654.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In ICLR 2013.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th international conferenceon Machine learning, pages 641?648.
ACM.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of LREC-2006,pages 2216?2219.Diarmuid?O S?eaghdha and Anna Korhonen.
2012.Modelling selectional preferences in a lexical hier-archy.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics-Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, pages170?179.
Association for Computational Linguis-tics.Diarmuid?O S?eaghdha.
2010.
Latent variable mod-els of selectional preference.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 435?444.
Association forComputational Linguistics.Sebastian Pad?o, Ulrike Pad?o, and Katrin Erk.
2007.Flexible, corpus-based modelling of human plausi-bility judgements.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 400?409,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, 61:127?159, November.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent dirichlet allocation method for selectional pref-erences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 424?434, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via em-based clustering.
InProceedings of the 37th annual meeting of the As-sociation for Computational Linguistics on Compu-tational Linguistics, pages 104?111.
Association forComputational Linguistics.Ekaterina Shutova, Simone Teufel, and Anna Korho-nen.
2013.
Statistical metaphor processing.
Compu-tational Linguistics, 39(2):301?353.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT-NAACL 2003, pages 252?259.Masashi Tsubaki, Kevin Duh, Masashi Shimbo, andYuji Matsumoto.
2013.
Modeling and learning se-mantic co-compositionality through prototype pro-jections and neural networks.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 130?140, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Tim Van de Cruys.
2009.
A non-negative tensor fac-torization model for selectional preference induction.34In Proceedings of the Workshop on GeometricalModels of Natural Language Semantics, pages 83?90, Athens, Greece, March.
Association for Compu-tational Linguistics.Dong Yu, Li Deng, and Frank Seide.
2013.
Thedeep tensor neural network with applications tolarge vocabulary speech recognition.
IEEE Transac-tions on Audio, Speech, and Language Processing,21(2):388?396.35
