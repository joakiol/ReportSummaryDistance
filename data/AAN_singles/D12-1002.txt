Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 12?23, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsRegularized Interlingual Projections:Evaluation on Multilingual TransliterationJagadeesh JagarlamudiUniversity of MarylandCollege Park, USA, 20742jags@umiacs.umd.eduHal Daum?
IIIUniversity of MarylandCollege Park, USA, 20742hal@umiacs.umd.eduAbstractIn this paper, we address the problem of build-ing a multilingual transliteration system usingan interlingual representation.
Our approachuses international phonetic alphabet (IPA) tolearn the interlingual representation and thusallows us to use any word and its IPA repre-sentation as a training example.
Thus, our ap-proach requires only monolingual resources: aphoneme dictionary that lists words and theirIPA representations.1 By adding a phonemedictionary of a new language, we can readilybuild a transliteration system into any of theexisting previous languages, without the ex-pense of all-pairs data or computation.
Wealso propose a regularization framework forlearning the interlingual representation, whichaccounts for language specific phonemic vari-ability, and thus it can find better mappingsbetween languages.
Experimental results onthe name transliteration task in five diverselanguages show a maximum improvement of29% accuracy and an average improvement of17% accuracy compared to a state-of-the-artbaseline system.1 IntroductionBecause of the wide usage of English, many natu-ral language processing (NLP) tasks have bilingualresources from English into other languages.
For ex-ample, significantly larger parallel texts are available1It is arguable that getting words and their IPA representa-tion require knowledge about both words and IPA symbols, butit still is specific to one language and, in this sense, we refer toit as a monolingual resource.between English and other languages.
Similarly,bilingual dictionaries and transliteration data sets aremore accessible from a language into English thaninto a different language.
This situation has causedthe NLP community to develop approaches whichuse a resource rich language (Q say English) as pivotto build resources/applications between a new lan-guage pair P and R. Previous studies in machinetranslation (Utiyama and Isahara, 2007; Paul andSumita, 2011), transliteration (Khapra et al2010),and dictionary mining (Saralegi et al2011) showthat these bridge language approaches perform com-petitively with approaches that use resources be-tween P and R. In this paper, we propose a regular-ization framework for bridge language approachesand show its effectiveness for name transliterationtask.
The key idea of our approach is that it accountsfor language specific variation in the bridge lan-guage resources (i.e.
between P ?
Q and Q?
R)and aims to minimize this variation as much as pos-sible.
Though our technique is general, for claritywe describe it in the context of named entity (NE)transliteration.Named entity (NE) transliteration involvestransliterating a name in one language into anotherlanguage and is shown to be crucial for machinetranslation (MT) (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002; Hermjakob et al2008; Li et al2009) and cross-lingual informationretrieval (CLIR) (AbdulJaleel and Larkey, 2003;Mandl and Womser-Hacker, 2005; Udupa et al2009).
There exists a large body of literature intransliteration, especially in the bilingual setting,well summarized by Ravi and Knight (2009).
We12English BulgarianWord IPA Word IPAbashful /?b?
?f?l/ ?????
/?
?ib?m/tuesday /?tu?zde?/ ???
/luk/craft /k?
?ft/ ???
/kak/book /b?k/ ?????
/m?
?zej/head /h?d/ ?????
/sp?
?k?/Table 1: Example phoneme dictionaries in English andBulgarian.
The English translations for the Bulgarianwords are switch, onion, how, museum, and spekle.summarize the approaches that are most relevant tous in Sec.
5.
In this paper, we operate in the contextof transliteration mining (Klementiev and Roth,2006; Sproat et al2006) where we assume that weare given a source language name and a list of targetlanguage candidate transliterations and the task is toidentify the correct transliteration.Given a set of l languages, we address the prob-lem of building a transliteration system betweenevery pair of languages.
A straight forward su-pervised learning approach would require trainingdata of name pairs between every pair of languages(Knight and Graehl, 1998) or a set of commonnames transliterated from every language into apivot language.
Though it is relatively easy to ob-tain names transliterated into a pivot language (suchas English), it is unlikely that such data sets containthe same names.
Bridge language approaches over-come the need for common names and build translit-eration systems for resource poor languages (Khapraet al2010).
However, such approaches still requiretraining data consisting of bilingual name translit-erations (orthographic name-to-name mappings).
Inthis paper, we relax the need for name translitera-tions by using international phonetic alphabet (IPA)in a manner akin to a ?bridge language.
?2 IPA for TransliterationWe assume that we have a list of words and theirIPA representations in each of the l languages.
Thewords in different languages need not have any rela-tionship to each other.
Table 1 shows few words andtheir IPA representations in English and Bulgarianlanguages.
We refer to the set of (word, IPA) pairsas phoneme dictionary in this paper.
Notice that thecommon symbols in the IPA sequences indicate avague phonetic correspondence between the charac-ter sequences of English and Bulgarian.
For exam-ple, both the words ?bashful?
and ???????
have thesymbol ???
in their IPA sequences which indicate apossible mapping between the character sequences?sh?
and ??
?.The use of IPA as the bridge language offers mul-tiple advantages.
As shown in Table 1, it allows usto include any (word, IPA) pair in the training dataand thus it relaxes the need for name pairs as thetraining data.
Since we only need a phoneme dic-tionary in each language, our approach does not re-quire any bilingual resources to build the transliter-ation system.
Moreover, since our training data cancontain any word (not only the NEs), it is easier toobtain such a resource, for e.g.
the phoneme dic-tionaries obtained from Wiktionary contain at least2000 words in 21 languages and we will see in Sec.
6that we can build a decent transliteration systemwith2000 words.2 Finally, unlike other transliteration ap-proaches, by simply adding a phoneme dictionary of(l+1)st language we can readily get a transliterationsystem into any of the existing l languages and thusavoid the need for all-pairs data or computation.Using IPA as the bridge language poses somenew challenges such as the language specific phone-mic inventory.
For example, Mandarin doesn?thave /v/, so it is frequently substituted with /w/ or/f/.
Similarly, !X??
(Southern Khoisan, spoken inBotswana) has 122 consonants, mostly consistingof a large inventory of different word-initial clicksounds (Haspelmath et al2005), many of whichdo not exist in any other documented languages.Besides this language specific phonemic inventory,names have different IPA representations in differ-ent languages.
For example, as shown in Table 2,the IPA sequences for ?China?
in English and Dutchhave common IPA symbols but the English IPA se-quence has additional symbols.
Moreover, a namecan have multiple pronunciations with in a language,e.g.
?France?
has two different IPA sequences in En-glish (Table 2).In order to handle this phonemic diversity, ourmethod explicitly models language-specific variabil-ity and attempts to minimize this phonemic variabil-2In our experiments, we consider languages with small(2000) and big (>30K) phoneme dictionaries.13Word IPA sequenceChina /?t?a?.n?/ (En), /?
?ina/ (Du), /?
?i?na?/ (De)America /?
?m?r?k?/ (En), /a?me.ri.ka/ (Ro)France /?f??
?ns/ (En), /?f?
?nts/ (En), /f??
?s/ (Fr)Table 2: IPA sequences of few words in different lan-guages indicated using language codes in the parenthesis(?En?
for English, ?Du?
for Dutch, ?De?
for German, ?Ro?for Romanian, and ?Fr?
for French).ity as much as possible.
At a high level, our ap-proach uses the phoneme dictionaries of each lan-guage to learn mapping functions into an interlin-gual representation (also referred as common sub-space).
Subsequently, given a pair of languages, aquery name in one of the languages and a list ofcandidate transliterations in the other language, weuse the mapping functions of those two language toidentify the correct name transliteration.
The map-ping functions explicitly model the language specificvariability and thus account for fine grained differ-ences.
Our experimental results on four languagepairs from two different language families show amaximum improvement of 29% accuracy and an av-erage improvement of 17% accuracy compared toa state-of-the-art baseline approach.
An importantadvantage of our approach is that, it extends eas-ily to more than two languages and in fact addingphoneme dictionary from a different, but related,language improves the accuracies of a given lan-guage pair.
Our main contributions are: 1) build-ing a transliteration system using (word, IPA) pairsand hence using only monolingual resources and 2)proposing a regularization framework which is moregeneral and applies to other bridge language applica-tions such as lexicon mining (Mann and Yarowsky,2001).3 Low Dimensional ProjectionsOur approach is inspired by the Canonical Correla-tion Analysis (CCA) (Hotelling, 1936) and its appli-cation to transliteration mining (Udupa and Khapra,2010).First, we convert the phoneme dictionary of eachlanguage into feature vectors, i.e.
we convert eachword into a feature vector of n-gram character se-quences and similarly, we also, convert the IPArepresentations into feature vectors of n-gram IPAsymbol sequences.
For example, if we use uni-gram and bigram sequences as features, then thefeature vectors of ?head?
and its IPA sequence`h?d' are given by {h, e, a, d,#h, he, ea, ad, d$}and {h,?, d,#h, h?,?d, d$}.
For brevity, we referto the spaces of n-gram character and IPA symbolsequences as character and phonemic spaces respec-tively.
The character space is specific to each lan-guage while the phonemic space is shared across allthe languages.
Since we use IPA as bridge, eventhough two languages share orthography (e.g.
En-glish and French) it is irrelevant for our approach.Then, for each language, we find mappings(Aiand Ui)from the character and phonemic spacesinto a common k-dimensional subspace such that thecorrect transliterations lie closer to each other in thissubspace.
Before moving into the details of our ap-proach, we will describe the notation and then givean overview of the process by which our approachfinds the transliteration.3.1 NotationLet x(m)i ?
Rdi and p(m)i ?
Rc be the feature vec-tors of the mth word and its IPA sequence in theith(1 ?
?
?
l)language, where di is the size (i.e.
no.
offeatures) of the character space of the language andc is the size of the common phonemic space.
LetXi (di?ni) and Pi (c?ni) denote the ith languagedata matrices with x(m)i and p(m)i m = 1 ?
?
?ni as thecolumns respectively.
We consistently use subscriptto indicate the language and superscript to indicatethe index of an example point.3.2 Method OverviewDuring the training stage, for each language, we findmappings (or projection directions) Ai ?
R(di?k)and Ui ?
R(c?k) from the character and phonemicspaces into a k-dimensional subspace (or an interlin-gual representation) such that a name gets mappedto the same k-dimensional vector irrespective of thelanguage.
That is, given a name xi it gets mappedto the vector ATi xi and similarly its IPA sequencepi gets mapped to UTi pi.
During the testing stage,given a name xi in the source (ith) language, we findits transliteration in the target (jth) language xj bysolving the following decoding problem:arg minxjL(xi, xj)(1)14Figure 1: A single name (Gandhi) is shown in all the in-put feature spaces.
The alignment between the characterand phonemic space is indicated with double dimensionalarrows.
Bridge-CCA uses a single mapping function Ufrom the phonemic space into the common subspace (the2-dimensional green space at the top), where as our ap-proach uses two mapping functions U1 and U2, one foreach language, to map the IPA sequences into the com-mon subspace.where L(xi, xj)is given byminp?Rc?ATi xi ?
UTi p?2 + ?ATj xj ?
UTj p?2 (2)This formulation uses the source language mappings(Ai and Ui) to find the IPA sequence p that is clos-est to the source name and then uses it, along withthe target language mappings (Aj and Uj), to iden-tify the correct transliteration from a list of candidatetransliterations.At a high level, existing bridge language ap-proaches such as Bridge-CCA (Khapra et al2010)assume that Ui ?
Uj thus ignoring the languagespecific variation.
To understand its implicationconsider the example shown in Fig.
1.
The mid-dle portion of the Fig.
shows the name Gandhi(represented as point) in the character spaces ofEnglish and Hindi, three-dimensional spaces, andits IPA sequences in the phonemic space (the two-dimensional space in the middle).
Notice that, be-cause of the phonemic variation, the same name isrepresented by two distinct points in the commonphonemic space.3 Now, since Bridge-CCA uses asingle mapping function for both the IPA sequences,it fails to map these two distinct points into a com-mon point in the interlingual subspace.Our new formulation, as explained above, relaxesthis hard constraint and learns different mappingfunctions (Ui and Uj) and hence our approach canpotentially map both the distinct IPA sequences intoa single point.
As a result our approach success-fully handles the language specific phonemic vari-ation.
At the same time we constrain the projec-tion directions such that they behave similarly forthe phonemic sounds that are observed in majorityof the languages.
In the example shown in Fig.
1,our model (called Regularized Projections) finds twodifferent mapping functions U1 and U2, one for eachlanguage, from the phonemic space into the com-mon two-dimensional space at the bottom.3.3 Regularized ProjectionsIn this section we first formulate the problem of find-ing the mapping functions (Ai and Ui) of each lan-guage as an optimization problem.
In the followingsection (Sec.
4), we develop a method for solving theoptimization problem and also derive closed formsolution for the prediction problem given in Eq.
1.For simplicity, we describe our approach in terms ofsingle projection vectors, ai ?
Rdi and ui ?
Rc,rather than full matrices, but the generalization istrivial.Inspired by the Canonical Correlation Analysis(CCA) (Hotelling, 1936), we find projection direc-tions in the character and phonemic spaces of eachlanguage such that, after projection, a word is closerto its aligned IPA sequence.
To understand this, as-sume that we have a name (say ?Barack Obama?)
inall the languages4 and its feature vectors are givenby xi and pi i = 1 ?
?
?
l in the character and phone-3In reality, as explained in the previous section, the phone-mic variation that is commonly observed is that different fea-tures are triggered for different languages.
But for visualizationpurpose, we showed the IPA sequences as if they differ in thefeature values.4Our model does not require same names in different lan-guages; this is used only for easier understanding.15mic spaces respectively.
Then, we might try to findprojection directions ai in each language and u inthe common phonemic space such that:arg minai,ul?i=1(?xi, ai?
?
?pi,u?
)2(3)where ?
?, ??
denotes the dot product between twovectors.
This model assumes that the projection di-rection u is same for the phonemic space of all thelanguages.
This is a hard constraint and does nothandle the language specific variability as discussedin the previous section.
We model the languagespecificity by relaxing this hard constraint.In our model, intuitively, the parameters corre-sponding to the phonemic sounds that occur in ma-jority of the languages are shared across the lan-guages while the parameters of the language spe-cific sounds are modeled per each language.
Thisis achieved by modeling the projection directions ofthe ith language phonemic space ui ?
u + ri.
Thevector u ?
Rc is common to the phonemic spacesof all the languages and thus handles sounds thatare observed in multiple languages while ri ?
Rc,the residual vector, is specific to each language andaccounts for the language specific phonemic varia-tions.
Then the new formulation is given by:arg minai,u,ril?i=1?
?xi, ai?
?
?pi,u + ri?
?2 + ?
?pi, ri?2where ?
is the residual parameter.
The first term ofthis summation ensures that a word and its IPA se-quence gets mapped to closer points in the subspacewhile the second term forces the residual vectors tobe as small as possible.
By enforcing the residualvectors to be small, this formulation encourages thesounds that occur in majority of the languages to beaccounted by u and the sounds that are specific to thegiven language by ri.
The final optimization prob-lem is obtained by summing these terms over all theexamples and all the languages and is given by:minai,u,ril?i=1( ||XTi ai ?
P Ti (u + ri)||2ni+ ?
?P Ti ri?2)(4)s.t.l?i=11ni?XTi ai?2 = 1 andl?i=11ni?P Ti u?2 = 1The constraints of the above optimization problemavoid the trivial solution of setting all the vectors tozero and are referred to as length constraints.4 Model OptimizationIn this section, we derive the solutions for the opti-mization problems presented in the previous section.4.1 Training the ModelWe follow the standard procedure of forming the La-grangian and setting its derivative to zero.
The La-grangian L of the optimization problem in Eq.
4 isgiven by:L =?i1ni||XTi ai?P Ti (u+ri)||2+?
?i?P Ti ri?2+?
(?i1ni?XTi ai?2?1)+?
(?i1ni?P Ti u?2?1)where ?
and ?
are Lagrangian multipliers corre-sponding to the length constraints.
Differentiating Lwith respect to ai, ri and u and setting the derivativesto zero yields the following equations, respectively:(1 + ?
)XiXTi ai ?XiP Ti ri = XiP Ti u?PiXTi ai + (1 + ?ni)PiP Ti ri = ?PiP Ti u?i1ni(PiXTi ai?PiP Ti ri)= (1+?
)?i1niPiP Ti uWe can rewrite these equations in matrix form, asshown in Eqs.
5 and 6, since the solution becomesclear in this form.
For brevity, let Ei = (1 +?
)XiXTi , Fi = ?XiP Ti and Gi = (1 + ?ni)PiP Ti .Then, u can be solved for using the generalizedeigenvalue problem shown in Eq.
7.
This step in-volves computing an inverse of a (di+c) matrix andan eigenvalue problem of size c which can be ex-pensive since solving each of these problems involvecubic time.
This can be reduced further into a prob-lem of smaller size by using inverse of a partitionedmatrix as shown in Eq.
8.
This identity reduces thematrix inverse computation from a problem of sizedi + c into two smaller problems of size di and ceach.
This reduces the time complexity considerablysince the inverse computation is cubic in the size ofthe matrix.16[(1 + ?
)XiXTi ?XiP Ti?PiXTi (1 + ?ni)PiP Ti] [airi]=[XiP Ti?PiP Ti]u (5)?i1ni[PiXTi ?PiP Ti][airi]= (1 + ?
)?i1niPiP Ti u (6)?i1ni[?F Ti?Gi1+?ni][Ei FiF Ti Gi]?1 [ ?Fi?Gi1+?ni]u = (1 + ?
)?i1niPiP Ti u (7)If Mi =(Ei ?
FiG?1i FTi)?1, then[Ei FiF Ti Gi]?1=[Mi ?MiFiG?1i?G?1i F Ti Mi G?1i + G?1i F Ti MiFiG?1i](8)Substituting Eq.
8 into Eq.
7 and further simplify-ing results in the following eigenvalue problem forsolving u:?iGi + (?ni)2F Ti MiFini(1 + ?ni)2u = (1 + ?
)?iPiP Tiniuwhere Mi =(Ei ?
FiG?1i F Ti)?1.
Notice that thetermEi = (1+?
)XiXTi depends on the Lagrangianmultiplier ?.
Because of this, we cannot solve forboth the parameters and the Lagrangian multipliersat the same time.
One possible approach is to do analternate optimization over the parameters and La-grangian multipliers, but in this paper we fix ?
andsolve for u.
The value of ?
denotes the correlationand its maximum value is 1.
In practice, we oftenobserve that the top few correlations take the valueof 1.
Based on this observation we fix the value of ?to 1 (Sec.
6).Subsequently, we use u to solve for ai and ri asfollows:ai = ?
?niMiFi1 + ?niu (9)ri =?niG?1i F Ti MiFi ?
I1 + ?niu (10)In order to increase the stability of the system weregularizeGi andEi by adding ?I .
We use the top keigenvectors u and their corresponding ai and ri vec-tors as columns and form the mappingsU ,Ai andRirespectively.
These mappings are used in predictingthe transliteration of a name in one language intoany other language, which will be described in thefollowing section.4.2 Transliteration Mining (Prediction)During the testing phase, given a source name anda list of candidate transliterations, we solve the de-coding problem shown in Eq.
1 to find the appropri-ate target language transliteration.
Formally, givena word xi in ith language we find its transliterationinto jth language xj , by solving the optimizationproblem shown in Eq.
1, where Ui = U + Ri andUj = U + Rj .
Similar to the previous case, theclosed form solution can be found by computing thefirst derivative with respect to the unknown phonemesequence and the target language transliteration andsetting it to zero.
First, the IPA sequence p?
thatminimizes L(xi, xj)is given by:p?
= C?1ij(UiATi xi + UjATj xj)(11)whereCij = UiUTi +UjUTj .
We substitute this backin Eq.
2 and then solve for xj , the best transliterationin the jth language, as:Aj(I?UTj C?1ij Uj)ATj xj = AjUTj C?1ij UiATi xi (12)Since Ui and Uj are not full rank matrices, to in-crease the numerical stability of the prediction step,we useCij = UiUTi +UjUTj +0.001 I where I is anidentity matrix.
Notice that this solution doesn?t de-pend on the p?
and hence we don?t need to computeit explicitly.5 Related WorkThere is a large body of the literature in named entitytransliteration, so we will describe only the most rel-evant ones to our approach.
In transliteration, gener-ative approaches aim to generate the target language17transliteration of a given source name (Knight andGraehl, 1998; Jung et al2000; Haizhou et al2004;Al-Onaizan and Knight, 2002) while discriminativeapproaches assume a list of target language names,obtained from other sources, and try to identify thecorrect transliteration (Klementiev and Roth, 2006;Sproat et al2006).
The effectiveness of the dis-criminative approaches depend on the list of targetlanguage candidates.
Sproat et al2006) report anoracle accuracy of 85%, but it depends on the sourceof the candidate transliterations.
Nevertheless, allthese approaches require either bilingual name pairsor phoneme sequences to learn to transliterate be-tween two languages.
Thus, if we want to builda transliteration system between every pair of lan-guages in a given set of languages then these ap-proaches need resources between every pair of lan-guages which can be prohibitive.Bridge language approaches propose an alterna-tive and use a resource rich language such as Englishas common language (Khapra et al2010) but theystill need bilingual resources.
Moreover Bridge-CCA (Khapra et al2010) uses a single mappingfunction for the phonemic space of all the languagesand thus it can not handle language specific variabil-ity.
In the original setting, authors use English as thepivot and since the feature space of English is fixed,irrespective of the target language, this may not be aserious concern but it becomes crucial when we useIPA as the bridge language.Approaches that map words in different languagesinto the common phonemic space have also beenwell studied.
But most of these approaches use lan-guage specific resources such as CMU pronuncia-tion dictionary (Gao et al2004) or a carefully con-structed cost matrices for addition, substitution, anddeletion of phonemes between a pair of languages(Tao et al2006; Yoon et al2007).
Variants ofsoundex algorithm (Odel and Russel, 1918) such asKodex (Kang and Choi, 2000) use hand constructedconsonant to soundex code tables for name translit-eration.
Similar to our approach these variants onlyrequire soundex mappings of a new language tobuild transliteration system, but our model does notrequire explicit mapping between n-gram charactersand the IPA symbols instead it learns them auto-matically using phoneme dictionaries.
Alternativelyunsupervised approaches have also been explored(Ravi and Knight, 2009), but their accuracies arefairly low compared to the supervised and weeklysupervised approaches.6 ExperimentsOur experiments are designed to evaluate the follow-ing three aspects of our model, and of our approachto transliteration in general:IPA as bridge: Unlike other phonemic based ap-proaches (Sec.
5), we do not explicitly model thephoneme modifications between pairs of languages.Moreover, the phoneme dictionary in each languageis crawled from Wiktionary (Sec.
6.1), which islikely to be noisy.
So, the first aspect we wantto evaluate is the effectiveness of using IPA as thebridge language.
Here, we also compare our methodwith other bridge language approaches and establishthe importance of modeling language specific vari-ance.Multilinguality: In our method, simply adding aphoneme dictionary of a new language allows us toextend our transliteration system into any of the ex-isting languages.
We evaluate the effect of data froma different, but related, languages on a transliterationsystem between a given pair.Complementarity: Using IPA as bridge languageallows us to build transliteration system into re-source poor languages.
But we also want to eval-uate whether such an approach can help improvinga transliteration system trained directly on bilingualname-pairs.6.1 Data SetsWe use data sets from five languages in order to eval-uate the effectiveness of our approach.
The phonemedictionaries (list of words and their IPA represen-tations as shown in Table 1) are obtained fromWiktionary.
The Wiktionary dump downloaded inOctober 2011 has at least 2000 (word, IPA) pairsin 21 languages which also includes some resourcepoor languages (e.g.
Armenian, Taiwanese, Turkish,etc.
).In principle, our method allows us to buildtransliteration system into any of these languagepairs without any additional information.
But, in thispaper, we use English (En), Bulgarian (Bg), Rus-sian (Ru), French (Fr), and Romanian (Ro) for eval-18En.
Bg.
Ru.
Fr.
Ro.Train 31K 36K 1141 36K 5211Dev.
?
1264 2000 2717 430Test ?
1264 2000 2717 431# Features 5000 3998 2900 5000 3465Table 3: Statistics of different data sets.
Trainingdata is monolingual phoneme dictionaries while develop-ment/test sets are bilingual name pairs between Englishand the respective language.uation purposes, as they suffice to showcase all thethree aspects mentioned in the previous section.
Ta-ble 3 shows the sizes of phoneme dictionaries usedfor training the models.
The phoneme dictionar-ies of English, Bulgarian, and Russian contain morethan 30K (word,IPA) pairs while the remaining twolanguages have smaller phoneme dictionaries.
Thedevelopment and test sets between English and theremaining language pairs are obtained from geon-ames data base.5 These are geographic locationnames from different countries written in multiplelanguages.6.2 Experimental SetupWe convert the phoneme dictionaries of each lan-guage into feature vectors.
We use unigram andbigram features in the phonemic space and uni-gram, bigram and trigram features in the characterspace.
An example for feature generation is shownin Sec.
3.
After converting the data into feature vec-tors, we retain the most frequent 5000 features.
Weonly keep the frequent 5000 features since we ob-served, elsewhere, that including infrequent featuresleads CCA based methods to learn projection direc-tions with perfect correlations which are not effec-tive for downstream applications.
The last row ofTable 3 shows the number of features in the char-acter space of each of the languages.
The phone-mic space is common to all the languages and has3777 features.
Though the phonemic features arecommon to all the languages, as discussed in Sec.
2,only a subset of features will be observed in a givenlanguage.
For example, in our data sets, of the total3777 common phonetic features only 3312, 882, and1009 features are observed in English, Bulgarian,5http://www.geonames.org/0.60.650.70.750.80.850.90  10  20  30  40  50  60  70  80  90  100lambdaValid.
Acc.Valid.
MRRFigure 2: Performance of transliteration system withresidual parameter ?
on English-Bulgarian developmentdata set.and Russian languages respectively.
This indicatesthe diversity in the phonemic inventory of differentlanguages.We compare our approach against Bridge-CCA, astate-of-the-art bridge language transliteration sys-tem which is known to perform competitively withother discriminative approaches (Khapra et al2010).
We use the phoneme dictionaries in each lan-guage to train our approach, as well as the baselinesystem.
The projection directions learnt during thetraining are used to find the transliteration for a testname as described in Sec.
4.2.
We report the perfor-mance in terms of the accuracy (exact match) of thetop ranked transliteration and the mean reciprocalrank (MRR) of the correct transliteration.
We findtransliterations in both the directions (i.e.
target lan-guage transliterations given a source name and viceversa) and report average accuracies.
The regular-ization parameter (? )
and the size of the interlingualrepresentation (k) in both our approach and Bridge-CCA are tuned on the development set.6.3 Description of ResultsIn this section we report experimental results on thethree aspects mentioned above.6.3.1 IPA as BridgeFig.
2 shows the performance of our system withthe residual parameter ?
(in Eq.
4) on the develop-19En-Bg En-Ru En-Fr En-RoAcc.
MRR Acc.
MRR Acc.
MRR Acc.
MRR1 Bridge-CCA 68.83 77.22 44.50 53.22 41.55 52.89 71.69 79.592 Ours (cosine) 67.68 76.52 45.07 53.63 42.45 53.06 74.13 81.283 Ours (Eq.
12) 83.70 88.32 63.47 73.01 70.68 78.13 77.38 84.224 Ours (cosine + Multi.)
68.91 77.44 49.15 57.20 42.55 53.02 77.49 84.045 Ours (Eq.
12 + Multi.)
84.45 88.43 66.70 75.85 71.09 78.43 77.49 84.04Table 4: Results of our approach and the baseline system on the test set.
The second block shows the results when ourapproach is trained only on phoneme dictionaries of the language pair, the third block shows results when we includeother language data as well.ment data set.
When ?
is small, the model does notattempt to constrain the projection directions Ui?sand hence they tend to map names to completelyunrelated vectors.
As we increase the residual pa-rameter, it forces the residual vectors (Ri) to besmaller and thus the subspaces identified for eachlanguage are closely tied together.
Thus, it modelsthe commonalities across languages and also the lan-guage specific variability.
Based on the performancecurves on the development data, we fix ?
= 50 in therest of the experiments.Table 4 shows the results of Bridge-CCA and ourapproach on the four language pairs.
We report theresults of our approach with the decoding proposedin Sec.
4.2 and a simple cosine similarity measurein the common-subspace, i.e.
cos(ATi xi, ATj xj).Comparison of the accuracies in rows 1, 2 and 3,shows that simply using cosine similarity performsalmost same as the Bridge-CCA approach.
How-ever, using the decoding suggested in Eq.
12 givessignificant improvements.
To understand why thecosine angle between ATi xi and ATj xj is not the ap-propriate measure, assume that the vectors xi andxj are feature vectors of same name in two lan-guages and let p be its true IPA representation.
Then,since our model learns projection directions suchthat ATi xi ?
UTi p,cos(ATi xi, ATj xj) = cos((U+Ri)Tp, (U+Rj)Tp)The additional residual matrices Ri and Rj makethe cosine measure inappropriate.
At the same time,our model forces the residual matrices to be smalland this is probably the reason why it performscompetitively with the Bridge-CCA.
On the otherhand, our decoding method, as shown in Eq.
1, in-tegrates over the best possible phoneme sequenceand thus yields significant improvements.
In the restof the paper, we report results with the decodingin Eq.
12 unless specified explicitly.
Our approachachieves a maximum improvement of 29.13% ac-curacy over Bridge-CCA in English-French and onan average it achieves 17.17% and 15.19% improve-ment in accuracy and MRR respectively.
Notice thateven though our Russian phoneme dictionary hasonly 1141 (word, IPA) pairs, our approach is ableto achieve an accuracy of 63.47% and an MRR of73% indicating that the correct name transliterationis, on an average, at rank 1 or 2.6.3.2 MultilingualityThe fourth and fifth rows of Table 4 also show themultilingual results.
In particular, we train our sys-tem on data from the three languages En, Bg, andRu and test it on En-Bg and En-Ru test sets.
Simi-larly, we train a different system on data from En, Frand Ro and evaluate it on En-Fr and En-Ro test sets.We split the languages based on the language family,Russian and Bulgarian are Slavonic languages whileFrench and Romanian are Romance languages, andexpect that languages in same family have similarpronunciations.
Comparing the performance of oursystem with and without the multilingual data set, itis clear that having data from other languages helpsimprove the accuracy.6.3.3 ComplementarityIn the final experiment, we want to comparethe performance of our approach, which uses onlymonolingual resources, with a transliteration systemtrained using bilingual name pairs.
We train a CCAbased transliteration system (Udupa and Khapra,20En-Bg En-FrAcc.
MRR Acc.
MRRCCA 95.57 96.76 95.82 96.67Ours+CCA 95.69 96.90 96.14 96.90?
Err 2.7% 4.2% 7.5% 6.8%Ours+CCA(t) 95.80 96.95 96.34 97.04?
Err 5.4% 5.8% 12.3% 11.3%Table 5: Comparison with a system trained on bilingualname pairs.
The (t) in the third row indicates parame-ters are tuned for test set.
We also show the percentageerror reduction achieved by a linear combination of ourapproach and CCA.2010) on a training data of 3792 and 8151 locationname pairs.
Notice that the training and test data forthis system are from the same domain and thus it hasan additional advantage over our approach, which istrained on whatever happens to be on Wiktionary.The second row of Table 5 shows the results ofCCA on English-Bulgarian and English-French lan-guage pairs.
CCA achieves high accuracies eventhough the training data is relatively small, mostlikely because of the domain match between train-ing and test data sets.
As another baseline, we triedusing Google machine translation API to transliter-ate the English names of the En-Bg test set.
Wehoped that since these are names, the translation en-gine would simply transliterate them and return theresult.
Of the output, we observed that about 500names are passed through the MT system unchangedand so we ignore them.
On the remaining names,it achieved an accuracy of 76.15% and the averagestring edit distance of the returned transliteration tothe true transliteration is about 3.74.
These accura-cies are not directly comparable to the results shownin Table 5 because, presumably, it is a transliterationgeneration system unlike CCA which is a transliter-ation mining approach.
For lack fair comparison, wedon?t report the accuracies of the Google transliter-ation output in Table 5.Table 5 also shows the results of our system whencombined with the CCA approach.
For a given En-glish word, we score the candidate transliterationsusing our approach and then linearly combine theirscores with the scores assigned by CCA.
We per-form a line search between [0, 1] for the appropriateweight combination.
The third and fourth rows ofTable 5 show the results of the linear combinationwhen the weight is tuned for the development andtest sets respectively.
The improvements, thoughnot significant, are encouraging and suggest that asophisticated way of combining these different sys-tems may yield significant improvements.
This ex-periment shows that a transliteration system trainedon word and IPA representations can actually aug-ment a system trained on bilingual name pairs lead-ing to an improved performance.7 ConclusionIn this paper we proposed a regularization techniquefor the bridge language approaches and showedits effectiveness on the name transliteration task.Our approach learns interlingual representation us-ing only monolingual resources and hence can beused to build transliteration system between re-source poor languages.
We show that, by account-ing the language specific phonemic variation, wecan get a significant improvements.
Our experimen-tal results suggest that a transliteration system builtusing IPA data can also help improve the accuracyof a transliteration system trained on bilingual namepairs.Thought we used IPA as a bridge language thereare other viable options.
For example, as shownin Khapra et al2010) we can use English as thebridge language.
Since name transliteration prob-lem is being studied for a considerable time, manyresources already exist between English and otherlanguages.
So, one can argue the appropriateness ofIPA as bridge language compared to, say, English.While this is an important question, in this paper,we are primarily interested in showing the impor-tance of handling language specific phenomenon inthe bridge language approaches.
In future, we wouldlike to study the appropriateness of IPA vs. Englishas the bridge language and also the generalizabilityof our technique to other scenarios.AcknowledgementsThis work is partially funded by NSF grant IIS-1153487 and the BOLT program of the DefenseAdvanced Research Projects Agency, Contract No.HR0011-12-C-0015.21ReferencesNasreen AbdulJaleel and Leah S. Larkey.
2003.
Statis-tical transliteration for english-arabic cross languageinformation retrieval.
In Proceedings of the twelfth in-ternational conference on Information and knowledgemanagement, CIKM ?03, pages 139?146, New York,NY, USA.
ACM.Yaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in arabic text.
In Proceed-ings of the ACL-02 workshop on Computational ap-proaches to semitic languages, SEMITIC ?02, pages1?13, Stroudsburg, PA, USA.
ACL.Wei Gao, Kam fai Wong, and Wai Lam.
2004.
Phoneme-based transliteration of foreign names for OOV prob-lem.
In Proceedings of the 1st International Joint Con-ference on Natural Language Processing (IJCNLP),pages 374?381.Li Haizhou, Zhang Min, and Su Jian.
2004.
A jointsource-channel model for machine transliteration.
InProceedings of the 42nd Annual Meeting on Associa-tion for Computational Linguistics, ACL ?04, Strouds-burg, PA, USA.
ACL.Martin Haspelmath, Matthew Dryer, David Gil, andBernard Comrie, editors.
2005.
The World Atlas ofLanguage Structures.
Oxford University Press.Ulf Hermjakob, Kevin Knight, and Hal Daum?
III.
2008.Name translation in statistical machine translation -learning when to transliterate.
In Proceedings of ACL-08: HLT, pages 389?397, Columbus, Ohio, June.ACL.Harold Hotelling.
1936.
Relation between two sets ofvariables.
Biometrica, 28:322?377.Sung Young Jung, SungLim Hong, and Eunok Paek.2000.
An english to korean transliteration model ofextended markov window.
In Proceedings of the 18thconference on Computational linguistics - Volume 1,COLING ?00, pages 383?389, Stroudsburg, PA, USA.ACL.Byung-Ju Kang and Key-Sun Choi.
2000.
Two ap-proaches for the resolution of word mismatch prob-lem caused by english words and foreign words in ko-rean information retrieval.
In Proceedings of the 5thinternational workshop on on Information retrievalwith Asian languages, IRAL ?00, pages 133?140, NewYork, NY, USA.
ACM.Mitesh M. Khapra, Raghavendra Udupa, A. Kumaran,and Pushpak Bhattacharyya.
2010.
Pr + rq ?
pq:Transliteration mining using bridge language.
In Pro-ceedings of the Twenty-Fourth AAAI Conference onArtificial Intelligence, AAAI 2010, Atlanta, Georgia,USA, July.
AAAI Press.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th Annual Meeting ofthe Association for Computational Linguistics, ACL-44, pages 817?824, Stroudsburg, PA, USA.
ACL.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4):599?612.Haizhou Li, A. Kumaran, Vladimir Pervouchine, andMin Zhang.
2009.
Report of news 2009 machinetransliteration shared task.
In Proceedings of the 2009Named Entities Workshop: Shared Task on Transliter-ation, NEWS ?09, pages 1?18, Stroudsburg, PA, USA.ACL.Thomas Mandl and Christa Womser-Hacker.
2005.
Theeffect of named entities on effectiveness in cross-language information retrieval evaluation.
In Proceed-ings of the 2005 ACM symposium on Applied comput-ing, SAC ?05, pages 1059?1064, New York, NY, USA.ACM.Gideon S. Mann and David Yarowsky.
2001.
Multipathtranslation lexicon induction via bridge languages.
InProceedings of the 2nd meeting of the North AmericanChapter of the Association for Computational Linguis-tics on Language technologies, NAACL ?01, pages 1?8, Stroudsburg, PA, USA.
ACL.M.
K. Odel and R. C. Russel.
1918.
U.s. patent numbers,1,261,167 (1918) and 1,435,663(1922).Michael Paul and Eiichiro Sumita.
2011.
Translationquality indicators for pivot-based statistical mt.
InProceedings of 5th International Joint Conference onNatural Language Processing, pages 811?818, ChiangMai, Thailand, November.
AFNLP.Sujith Ravi and Kevin Knight.
2009.
Learning phonememappings for transliteration without parallel data.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 37?45, Boulder, Colorado, June.
ACL.Xabier Saralegi, Iker Manterola, and I?aki San Vicente.2011.
Analyzing methods for improving precision ofpivot based bilingual dictionaries.
In Proceedings ofthe 2011 Conference on Empirical Methods in Natu-ral Language Processing, pages 846?856, Edinburgh,Scotland, UK., July.
ACL.Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006.Named entity transliteration with comparable corpora.In Proceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,ACL-44, pages 73?80, Stroudsburg, PA, USA.
ACL.Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat,and ChengXiang Zhai.
2006.
Unsupervised named22entity transliteration using temporal and phonetic cor-relation.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?06, pages 250?257, Stroudsburg, PA, USA.ACL.Raghavendra Udupa and Mitesh M. Khapra.
2010.Transliteration equivalence using canonical correlationanalysis.
In ECIR?10, pages 75?86.Raghavendra Udupa, Saravanan K, Anton Bakalov, andAbhijit Bhole.
2009.
"they are out there, if you knowwhere to look": Mining transliterations of oov queryterms for cross-language information retrieval.
In Pro-ceedings of the 31th European Conference on IR Re-search on Advances in Information Retrieval, ECIR?09, pages 437?448, Berlin, Heidelberg.
Springer-Verlag.Masao Utiyama and Hitoshi Isahara.
2007.
A com-parison of pivot methods for phrase-based statisti-cal machine translation.
In Proceedings of HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 484?491, Rochester, New York, April.ACL.Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.2007.
Multilingual transliteration using feature basedphonetic method.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 112?119, Prague, Czech Republic, June.ACL.23
