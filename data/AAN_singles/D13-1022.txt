Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 210?221,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOptimal Beam Search for Machine TranslationAlexander M. Rush Yin-Wen ChangMIT CSAIL,Cambridge, MA 02139, USA{srush, yinwen}@csail.mit.eduMichael CollinsDepartment of Computer Science,Columbia University,New York, NY 10027, USAmcollins@cs.columbia.eduAbstractBeam search is a fast and empirically effectivemethod for translation decoding, but it lacksformal guarantees about search error.
We de-velop a new decoding algorithm that combinesthe speed of beam search with the optimal cer-tificate property of Lagrangian relaxation, andapply it to phrase- and syntax-based transla-tion decoding.
The new method is efficient,utilizes standard MT algorithms, and returnsan exact solution on the majority of transla-tion examples in our test data.
The algorithmis 3.5 times faster than an optimized incremen-tal constraint-based decoder for phrase-basedtranslation and 4 times faster for syntax-basedtranslation.1 IntroductionBeam search (Koehn et al 2003) and cube prun-ing (Chiang, 2007) have become the de facto decod-ing algorithms for phrase- and syntax-based trans-lation.
The algorithms are central to large-scalemachine translation systems due to their efficiencyand tendency to produce high-quality translations(Koehn, 2004; Koehn et al 2007; Dyer et al 2010).However despite practical effectiveness, neither al-gorithm provides any bound on possible decodingerror.In this work we present a variant of beam searchdecoding for phrase- and syntax-based translation.The motivation is to exploit the effectiveness and ef-ficiency of beam search, but still maintain formalguarantees.
The algorithm has the following bene-fits:?
In theory, it can provide a certificate of optimal-ity; in practice, we show that it produces opti-mal hypotheses, with certificates of optimality,on the vast majority of examples.?
It utilizes well-studied algorithms and extendsoff-the-shelf beam search decoders.?
Empirically it is very fast, results show that it is3.5 times faster than an optimized incrementalconstraint-based solver.While our focus is on fast decoding for machinetranslation, the algorithm we present can be appliedto a variety of dynamic programming-based decod-ing problems.
The method only relies on having aconstrained beam search algorithm and a fast uncon-strained search algorithm.
Similar algorithms existfor many NLP tasks.We begin in Section 2 by describing constrainedhypergraph search and showing how it generalizestranslation decoding.
Section 3 introduces a variantof beam search that is, in theory, able to producea certificate of optimality.
Section 4 shows how toimprove the effectiveness of beam search by usingweights derived from Lagrangian relaxation.
Sec-tion 5 puts everything together to derive a fast beamsearch algorithm that is often optimal in practice.Experiments compare the new algorithm withseveral variants of beam search, cube pruning, A?search, and relaxation-based decoders on two trans-lation tasks.
The optimal beam search algorithm isable to find exact solutions with certificates of opti-mality on 99% of translation examples, significantlymore than other baselines.
Additionally the optimal210beam search algorithm is much faster than other ex-act methods.2 BackgroundThe focus of this work is decoding for statistical ma-chine translation.
Given a source sentence, the goalis to find the target sentence that maximizes a com-bination of translation model and language modelscores.
In order to analyze this decoding problem,we first abstract away from the specifics of transla-tion into a general form, known as a hypergraph.
Inthis section, we describe the hypergraph formalismand its relation to machine translation.2.1 NotationThroughout the paper, scalars and vectors are writ-ten in lowercase, matrices are written in uppercase,and sets are written in script-case, e.g.
X .
All vec-tors are assumed to be column vectors.
The function?
(j) yields an indicator vector with ?
(j)j = 1 and?
(j)i = 0 for all i 6= j.2.2 Hypergraphs and SearchA directed hypergraph is a pair (V, E) where V ={1 .
.
.
|V|} is a set of vertices, and E is a set of di-rected hyperedges.
Each hyperedge e ?
E is a tuple?
?v2, .
.
.
, v|v|?, v1?where vi ?
V for i ?
{1 .
.
.
|v|}.The head of the hyperedge is h(e) = v1.
The tailof the hyperedge is the ordered sequence t(e) =?v2, .
.
.
, v|v|?.
The size of the tail |t(e)| may varyacross different hyperedges, but |t(e)| ?
1 for alledges and is bounded by a constant.
A directedgraph is a directed hypergraph with |t(e)| = 1 forall edges e ?
E .Each vertex v ?
V is either a non-terminal or aterminal in the hypergraph.
The set of non-terminalsis N = {v ?
V : h(e) = v for some e ?
E}.
Con-versely, the set of terminals is defined as T = V\N .All directed hypergraphs used in this work areacyclic: informally this implies that no hyperpath (asdefined below) contains the same vertex more thanonce (see Martin et al(1990) for a full definition).Acyclicity implies a partial topological ordering ofthe vertices.
We also assume there is a distinguishedroot vertex 1 where for all e ?
E , 1 6?
t(e).Next we define a hyperpath as x ?
{0, 1}|E| wherex(e) = 1 if hyperedge e is used in the hyperpath,procedure BESTPATHSCORE(?, ?
)pi[v]?
0 for all v ?
Tfor e ?
E in topological order do?
?v2, .
.
.
, v|v|?, v1?
?
es?
?
(e) +|v|?i=2pi[vi]if s > pi[v1] then pi[v1]?
sreturn pi[1] + ?Figure 1: Dynamic programming algorithm for uncon-strained hypergraph search.
Note that this version onlyreturns the highest score: maxx?X ?>x+ ?
.
The optimalhyperpath can be found by including back-pointers.x(e) = 0 otherwise.
The set of valid hyperpaths isdefined asX =??????????
?x :?e?E:h(e)=1x(e) = 1,?e:h(e)=vx(e) =?e:v?t(e)x(e) ?
v ?
N \ {1}??????????
?The first problem we consider is unconstrained hy-pergraph search.
Let ?
?
R|E| be the weight vectorfor the hypergraph and let ?
?
R be a weight offset.1The unconstrained search problem is to findmaxx?X?e?E?
(e)x(e) + ?
= maxx?X?>x+ ?This maximization can be computed for anyweights and directed acyclic hypergraph in timeO(|E|) using dynamic programming.
Figure 1shows this algorithm which is simply a version ofthe CKY algorithm.Next consider a variant of this problem: con-strained hypergraph search.
Constraints will be nec-essary for both phrase- and syntax-based decoding.In phrase-based models, the constraints will ensurethat each source word is translated exactly once.
Insyntax-based models, the constraints will be used tointersect a translation forest with a language model.In the constrained hypergraph problem, hyper-paths must fulfill additional linear hyperedge con-straints.
Define the set of constrained hyperpaths asX ?
= {x ?
X : Ax = b}1The purpose of the offset will be clear in later sections.
Forthis section, the value of ?
can be taken as 0.211where we have a constraint matrix A ?
R|b|?|E|and vector b ?
R|b| encoding |b| constraints.The optimal constrained hyperpath is x?
=arg maxx?X ?
?>x+ ?
.Note that the constrained hypergraph search prob-lem may be NP-Hard.
Crucially this is true evenwhen the corresponding unconstrained search prob-lem is solvable in polynomial time.
For instance,phrase-based decoding is known to be NP-Hard(Knight, 1999), but we will see that it can be ex-pressed as a polynomial-sized hypergraph with con-straints.Example: Phrase-Based Machine TranslationConsider translating a source sentencew1 .
.
.
w|w| toa target sentence in a language with vocabulary ?.
Asimple phrase-based translation model consists of atuple (P, ?, ?)
with?
P; a set of pairs (q, r) where q1 .
.
.
q|q| is a se-quence of source-language words and r1 .
.
.
r|r|is a sequence of target-language words drawnfrom the target vocabulary ?.?
?
: R|P|; parameters for the translation modelmapping each pair in P to a real-valued score.?
?
: R|??
?|; parameters of the language modelmapping a bigram of target-language words toa real-valued score.The translation decoding problem is to find thebest derivation for a given source sentence.
Aderivation consists of a sequence of phrases p =p1 .
.
.
pn.
Define a phrase as a tuple (q, r, j, k)consisting of a span in the source sentence q =wj .
.
.
wk and a sequence of target words r1 .
.
.
r|r|,with (q, r) ?
P .
We say the source words wj .
.
.
wkare translated to r.The score of a derivation, f(p), is the sum of thetranslation score of each phrase plus the languagemodel score of the target sentencef(p) =n?i=1?
(q(pi), r(pi)) +|u|+1?i=0?
(ui?1, ui)where u is the sequence of words in ?
formedby concatenating the phrases r(p1) .
.
.
r(pn), withboundary cases u0 = <s> and u|u|+1 = </s>.Crucially for a derivation to be valid it must sat-isfy an additional condition: it must translate everysource word exactly once.
The decoding problemfor phrase-based translation is to find the highest-scoring derivation satisfying this property.We can represent this decoding problem as a con-strained hypergraph using the construction of Changand Collins (2011).
The hypergraph weights en-code the translation and language model scores, andits structure ensures that the count of source wordstranslated is |w|, i.e.
the length of the source sen-tence.
Each vertex will remember the precedingtarget-language word and the count of source wordstranslated so far.The hypergraph, which for this problem is also adirected graph, takes the following form.?
Vertices v ?
V are labeled (c, u) where c ?
{1 .
.
.
|w|} is the count of source words trans-lated and u ?
?
is the last target-language wordproduced by a partial hypothesis at this vertex.Additionally there is an initial terminal vertexlabeled (0,<s>).?
There is a hyperedge e ?
E with head (c?, u?
)and tail ?
(c, u)?
if there is a valid correspondingphrase (q, r, j, k) such that c?
= c + |q| andu?
= r|r|, i.e.
c?
is the count of words translatedand u?
is the last word of target phrase r. Wecall this phrase p(e).The weight of this hyperedge, ?
(e), is the trans-lation model score of the pair plus its languagemodel score?
(e) = ?
(q, r)+??|r|?i=2?
(ri?1, ri)??+?
(u, r1)?
To handle the end boundary, there are hyper-edges with head 1 and tail ?
(|w|, u)?
for allu ?
?.
The weight of these edges is the cost ofthe stop bigram following u, i.e.
?
(u,</s>).While any valid derivation corresponds to a hy-perpath in this graph, a hyperpath may not corre-spond to a valid derivation.
For instance, a hyper-path may translate some source words more thanonce or not at all.212Figure 2: Hypergraph for translating the sentence w = les1 pauvres2 sont3 demunis4 with set of pairs P ={(les, the), (pauvres, poor), (sont demunis, don?t have any money)}.
Hyperedges are color-codedby source words translated: orange for les1, green for pauvres2, and red for sont3 demunis4.
The dotted linesshow an invalid hyperpath x that has signature Ax = ?0, 0, 2, 2?
6= ?1, 1, 1, 1?
.We handle this problem by adding additional con-straints.
For all source words i ?
{1 .
.
.
|w|}, define?
as the set of hyperedges that translate wi?
(i) = {e ?
E : j(p(e)) ?
i ?
k(p(e))}Next define |w| constraints enforcing that each wordin the source sentence is translated exactly once?e??
(i)x(e) = 1 ?
i ?
{1 .
.
.
|w|}These linear constraints can be represented witha matrix A ?
{0, 1}|w|?|E| where the rows corre-spond to source indices and the columns correspondto edges.
We call the product Ax the signature,where in this case (Ax)i is the number of times wordi has been translated.
The full set of constrained hy-perpaths is X ?
= {x ?
X : Ax = 1 }, and the bestderivation under this phrase-based translation modelhas score maxx?X ?
?>x+ ?
.Figure 2.2 shows an example hypergraphwith constraints for translating the sentence lespauvres sont demunis into English usinga simple set of phrases.
Even in this small exam-ple, many of the possible hyperpaths violate theconstraints and correspond to invalid derivations.Example: Syntax-Based Machine TranslationSyntax-based machine translation with a languagemodel can also be expressed as a constrained hyper-graph problem.
For the sake of space, we omit thedefinition.
See Rush and Collins (2011) for an in-depth description of the constraint matrix used forsyntax-based translation.3 A Variant of Beam SearchThis section describes a variant of the beamsearch algorithm for finding the highest-scoring con-strained hyperpath.
The algorithm uses three maintechniques: (1) dynamic programming with ad-ditional signature information to satisfy the con-straints, (2) beam pruning where some, possibly op-timal, hypotheses are discarded, and (3) branch-and-bound-style application of upper and lower boundsto discard provably non-optimal hypotheses.Any solution returned by the algorithm will be avalid constrained hyperpath and a member of X ?.Additionally the algorithm returns a certificate flagopt that, if true, indicates that no beam pruningwas used, implying the solution returned is opti-mal.
Generally it will be hard to produce a certificateeven by reducing the amount of beam pruning; how-ever in the next section we will introduce a methodbased on Lagrangian relaxation to tighten the upperbounds.
These bounds will help eliminate most so-lutions before they trigger pruning.3.1 AlgorithmFigure 3 shows the complete beam search algorithm.At its core it is a dynamic programming algorithmfilling in the chart pi.
The beam search chart indexeshypotheses by vertex v ?
V as well as a signaturesig ?
R|b| where |b| is the number of constraints.
Anew hypothesis is constructed from each hyperedgeand all possible signatures of tail nodes.
We definethe function SIGS to take the tail of an edge and re-213turn the set of possible signature combinationsSIGS(v2, .
.
.
v|v|) =|v|?i=2{sig : pi[vi, sig] 6= ??
}where the product is the Cartesian product over sets.Line 8 loops over this entire set.2 For hypothesis x,the algorithm ensures that its signature sig is equalto Ax.
This property is updated on line 9.The signature provides proof that a hypothesis isstill valid.
Let the function CHECK(sig) return trueif the hypothesis can still fulfill the constraints.
Forexample, in phrase-based decoding, we will defineCHECK(sig) = (sig ?
1); this ensures that eachword has been translated 0 or 1 times.
This check isapplied on line 11.Unfortunately maintaining all signatures is inef-ficient.
For example we will see that in phrase-based decoding the signature is a bit-string recordingwhich source words have been translated; the num-ber of possible bit-strings is exponential in the lengthof the sentence.
The algorithm includes two meth-ods for removing hypotheses, bounding and prun-ing.Bounding allows us to discard provably non-optimal solutions.
The algorithm takes as argumentsa lower bound on the optimal score lb ?
?>x?
+ ?
,and computes upper bounds on the outside scorefor all vertices v: ubs[v], i.e.
an overestimate ofthe score for completing the hyperpath from v. Ifa hypothesis has score s, it can only be optimal ifs+ ubs[v] ?
lb.
This bound check is performed online 11.Pruning removes weak partial solutions based onproblem-specific checks.
The algorithm invokes theblack-box function, PRUNE, on line 13, passing ita pruning parameter ?
and a vertex-signature pair.The parameter ?
controls a threshold for pruning.For instance for phrase-based translation, it specifiesa hard-limit on the number of hypotheses to retain.The function returns true if it prunes from the chart.Note that pruning may remove optimal hypotheses,so we set the certificate flag opt to false if the chartis modified.2For simplicity we write this loop over the entire set.
Inpractice it is important to use data structures to optimize look-up.
See Tillmann (2006) and Huang and Chiang (2005).1: procedure BEAMSEARCH(?, ?, lb, ?
)2: ubs?
OUTSIDE(?, ?
)3: opt?
true4: pi[v, sig]?
??
for all v ?
V, sig ?
R|b|5: pi[v, 0]?
0 for all v ?
T6: for e ?
E in topological order do7: ?
?v2, .
.
.
, v|v|?, v1?
?
e8: for sig(2) .
.
.
sig(|v|) ?
SIGS(v2, .
.
.
, v|v|) do9: sig ?
A?
(e) +|v|?i=2sig(i)10: s?
?
(e) +|v|?i=2pi[vi, sig(i)]11: if?
?s > pi[v1, sig] ?CHECK(sig) ?s+ ubs[v1] ?
lb??
then12: pi[v1, sig]?
s13: if PRUNE(pi, v1, sig, ?)
then opt?
false14: lb?
?
pi[1, c] + ?15: return lb?, optInput:????
(V, E , ?, ?)
hypergraph with weights(A, b) matrix and vector for constraintslb ?
R lower bound?
a pruning parameterOutput:[lb?
resulting lower bound scoreopt certificate of optimalityFigure 3: A variant of the beam search algorithm.
Usesdynamic programming to produce a lower bound on theoptimal constrained solution and, possibly, a certificate ofoptimality.
Function OUTSIDE computes upper boundson outside scores.
Function SIGS enumerates all possi-ble tail signatures.
Function CHECK identifies signaturesthat do not violate constraints.
Bounds lb and ubs areused to remove provably non-optimal solutions.
Func-tion PRUNE, taking parameter ?, returns true if it pruneshypotheses from pi that could be optimal.This variant on beam search satisfies the follow-ing two properties (recall x?
is the optimal con-strained solution)Property 3.1 (Primal Feasibility).
The returnedscore lb?
lower bounds the optimal constrainedscore, that is lb?
?
?>x?
+ ?
.Property 3.2 (Dual Certificate).
If beam search re-turns with opt = true, then the returned score isoptimal, i.e.
lb?
= ?>x?
+ ?
.An immediate consequence of Property 3.1 is thatthe output of beam search, lb?, can be used as the in-put lb for future runs of the algorithm.
Furthermore,214procedure PRUNE(pi, v, sig, ?
)C ?
{(v?, sig?)
: ||sig?||1 = ||sig||1,pi[v?, sig?]
6= ??
}D ?
C \mBEST(?, C, pi)pi[v?, sig?]?
??
for all v?, sig?
?
Dif D = ?
then return trueelse return falseInput:[(v, sig) the last hypothesis added to the chart?
?
Z # of hypotheses to retainOutput: true, if pi is modifiedFigure 4: Pruning function for phrase-based translation.Set C contains all hypotheses with ||sig||1 source wordstranslated.
The function prunes all but the top-?
scoringhypotheses in this set.if we loosen the amount of beam pruning by adjust-ing the pruning parameter ?
we can produce tighterlower bounds and discard more hypotheses.
We canthen iteratively apply this idea with a sequence ofparameters ?1 .
.
.
?K producing lower bounds lb(1)through lb(K).
We return to this idea in Section 5.Example: Phrase-based Beam Search.
Recallthat the constraints for phrase-based translation con-sist of a binary matrix A ?
{0, 1}|w|?|E| and vec-tor b = 1.
The value sigi is therefore the num-ber of times source word i has been translated inthe hypothesis.
We define the predicate CHECK asCHECK(sig) = (sig ?
1) in order to remove hy-potheses that already translate a source word morethan once, and are therefore invalid.
For this reason,phrase-based signatures are called bit-strings.A common beam pruning strategy is to grouptogether items into a set C and retain a (possiblycomplete) subset.
An example phrase-based beampruner is given in Figure 4.
It groups togetherhypotheses based on ||sigi||1, i.e.
the number ofsource words translated, and applies a hard pruningfilter that retains only the ?
highest-scoring items(v, sig) ?
C based on pi[v, sig].3.2 Computing Upper BoundsDefine the setO(v, x) to contain all outside edges ofvertex v in hyperpath x (informally, hyperedges thatdo not have v as an ancestor).
For all v ?
V , we setthe upper bounds, ubs, to be the best unconstrainedoutside scoreubs[v] = maxx?X :v?x?e?O(v,x)?
(e) + ?This upper bound can be efficiently computed forall vertices using the standard outside dynamic pro-gramming algorithm.
We will refer to this algorithmas OUTSIDE(?, ?
).Unfortunately, as we will see, these upper boundsare often quite loose.
The issue is that unconstrainedoutside paths are able to violate the constraints with-out being penalized, and therefore greatly overesti-mate the score.4 Finding Tighter Bounds withLagrangian RelaxationBeam search produces a certificate only if beampruning is never used.
In the case of phrase-basedtranslation, the certificate is dependent on all groupsC having ?
or less hypotheses.
The only way to en-sure this is to bound out enough hypotheses to avoidpruning.
The effectiveness of the bounding inequal-ity, s + ubs[v] < lb, in removing hypotheses is di-rectly dependent on the tightness of the bounds.In this section we propose using Lagrangian re-laxation to improve these bounds.
We first give abrief overview of the method and then apply it tocomputing bounds.
Our experiments show that thisapproach is very effective at finding certificates.4.1 AlgorithmIn Lagrangian relaxation, instead of solving the con-strained search problem, we relax the constraintsand solve an unconstrained hypergraph problemwith modified weights.
Recall the constrained hy-pergraph problem: maxx?X :Ax=b?>x + ?
.
The La-grangian dual of this optimization problem isL(?)
= maxx?X?>x+ ?
?
?>(Ax?
b)=(maxx?X(?
?A>?
)>x)+ ?
+ ?>b= maxx?X?
?>x+ ?
?where ?
?
R|b| is a vector of dual variables anddefine ??
= ?
?
A>?
and ?
?
= ?
+ ?>b.
Thismaximization is over X , so for any value of ?, L(?
)can be calculated as BestPathScore(?
?, ?
?
).Note that for all valid constrained hyperpaths x ?X ?
the termAx?b equals 0, which implies that thesehyperpaths have the same score under the modifiedweights as under the original weights, ?>x + ?
=??>x+?
?.
This leads to the following two properties,215procedure LRROUND(?k, ?)x?
arg maxx?X?>x+ ?
?
?>(Ax?
b)??
?
??
?k(Ax?
b)opt?
Ax = bub?
?>x+ ?return ?
?,ub, optprocedure LAGRANGIANRELAXATION(?)?
(0) ?
0for k in 1 .
.
.K do?
(k),ub, opt?
LRROUND(?k, ?
(k?1))if opt then return ?
(k),ub, optreturn ?
(K),ub, optInput: ?1 .
.
.
?K sequence of subgradient ratesOutput:???
final dual vectorub upper bound on optimal constrained solutionopt certificate of optimalityFigure 5: Lagrangian relaxation algorithm.
The algo-rithm repeatedly calls LRROUND to compute the subgra-dient, update the dual vector, and check for a certificate.where x ?
X is the hyperpath computed within themax,Property 4.1 (Dual Feasibility).
The valueL(?)
up-per bounds the optimal solution, that is L(?)
??>x?
+ ?Property 4.2 (Primal Certificate).
If the hyperpathx is a member of X ?, i.e.
Ax = b, then L(?)
=?>x?
+ ?
.Property 4.1 states that L(?)
always producessome upper bound; however, to help beam search,we want as tight a bound as possible: min?
L(?
).The Lagrangian relaxation algorithm, shown inFigure 5, uses subgradient descent to find this min-imum.
The subgradient of L(?)
is Ax ?
b wherex is the argmax of the modified objective x =arg maxx?X ?
?>x + ?
?.
Subgradient descent itera-tively solves unconstrained hypergraph search prob-lems to compute these subgradients and updates ?.See Rush and Collins (2012) for an extensive discus-sion of this style of optimization in natural languageprocessing.Example: Phrase-based Relaxation.
For phrase-based translation, we expand out the Lagrangian toL(?)
= maxx?X?>x+ ?
?
?>(Ax?
b) =maxx?X?e?E???(e)?k(p(e))?i=j(p(e))?i?
?x(e) + ?
+|s|?i=1?iThe weight of each edge ?
(e) is modified by thedual variables ?i for each source word translated bythe edge, i.e.
if (q, r, j, k) = p(e), then the scoreis modified by?ki=j ?i.
A solution under theseweights may use source words multiple times or notat all.
However if the solution uses each source wordexactly once (Ax = 1), then we have a certificateand the solution is optimal.4.2 Utilizing Upper Bounds in Beam SearchFor many problems, it may not be possible to satisfyProperty 4.2 by running the subgradient algorithmalone.
Yet even for these problems, applying sub-gradient descent will produce an improved estimateof the upper bound, min?
L(?
).To utilize these improved bounds, we simply re-place the weights in beam search and the outside al-gorithm with the modified weights from Lagrangianrelaxation, ??
and ?
?.
Since the result of beam searchmust be a valid constrained hyperpath x ?
X ?, andfor all x ?
X ?, ?>x + ?
= ?
?>x + ?
?, this sub-stitution does not alter the necessary properties ofthe algorithm; i.e.
if the algorithm returns with optequal to true, then the solution is optimal.Additionally the computation of upper boundsnow becomesubs[v] = maxx?X :v?x?e?O(v,x)??
(e) + ?
?These outside paths may still violate constraints, butthe modified weights now include penalty terms todiscourage common violations.5 Optimal Beam SearchThe optimality of the beam search algorithm is de-pendent on the tightness of the upper and lowerbounds.
We can produce better lower bounds byvarying the pruning parameter ?
; we can producebetter upper bounds by running Lagrangian relax-ation.
In this section we combine these two ideasand present a complete optimal beam search algo-rithm.Our general strategy will be to use Lagrangianrelaxation to compute modified weights and to usebeam search over these modified weights to attemptto find an optimal solution.
One simple method fordoing this, shown at the top of Figure 6, is to run216in stages.
The algorithm first runs Lagrangian relax-ation to compute the best ?
vector.
The algorithmthen iteratively runs beam search using the parame-ter sequence ?k.
These parameters allow the algo-rithm to loosen the amount of beam pruning.
Forexample in phrase based pruning, we would raisethe number of hypotheses stored per group until nobeam pruning occurs.A clear disadvantage of the staged approach isthat it needs to wait until Lagrangian relaxation iscompleted before even running beam search.
Of-ten beam search will be able to quickly find an opti-mal solution even with good but non-optimal ?.
Inother cases, beam search may still improve the lowerbound lb.This motivates the alternating algorithm OPT-BEAM shown Figure 6.
In each round, the algo-rithm alternates between computing subgradients totighten ubs and running beam search to maximizelb.
In early rounds we set ?
for aggressive beampruning, and as the upper bounds get tighter, weloosen pruning to try to get a certificate.
If at anypoint either a primal or dual certificate is found, thealgorithm returns the optimal solution.6 Related WorkApproximate methods based on beam search andcube-pruning have been widely studied for phrase-based (Koehn et al 2003; Tillmann and Ney, 2003;Tillmann, 2006) and syntax-based translation mod-els (Chiang, 2007; Huang and Chiang, 2007; Watan-abe et al 2006; Huang and Mi, 2010).There is a line of work proposing exact algorithmsfor machine translation decoding.
Exact decodersare often slow in practice, but help quantify the er-rors made by other methods.
Exact algorithms pro-posed for IBM model 4 include ILP (Germann et al2001), cutting plane (Riedel and Clarke, 2009), andmulti-pass A* search (Och et al 2001).
Zaslavskiyet al(2009) formulate phrase-based decoding as atraveling salesman problem (TSP) and use a TSPdecoder.
Exact decoding algorithms based on finitestate transducers (FST) (Iglesias et al 2009) havebeen studied on phrase-based models with limitedreordering (Kumar and Byrne, 2005).
Exact decod-ing based on FST is also feasible for certain hier-archical grammars (de Gispert et al 2010).
Changprocedure OPTBEAMSTAGED(?, ?
)?,ub, opt?LAGRANGIANRELAXATION(?
)if opt then return ub??
?
?
?A>??
?
?
?
+ ?>blb(0) ?
?
?for k in 1 .
.
.K dolb(k), opt?
BEAMSEARCH(?
?, ?
?, lb(k?1), ?k)if opt then return lb(k)return maxk?
{1...K} lb(k)procedure OPTBEAM(?, ?)?
(0) ?
0lb(0) ?
?
?for k in 1 .
.
.K do?
(k),ub(k), opt?
LRROUND(?k, ?
(k?1))if opt then return ub(k)??
?
?
?A>?(k)?
?
?
?
+ ?
(k)>blb(k), opt?
BEAMSEARCH(?
?, ?
?, lb(k?1), ?k)if opt then return lb(k)return maxk?
{1...K} lb(k)Input:[?1 .
.
.
?K sequence of subgradient rates?1 .
.
.
?K sequence of pruning parametersOutput: optimal constrained score or lower boundFigure 6: Two versions of optimal beam search: stagedand alternating.
Staged runs Lagrangian relaxation tofind the optimal ?, uses ?
to compute upper bounds, andthen repeatedly runs beam search with pruning sequence?1 .
.
.
?k.
Alternating switches between running a roundof Lagrangian relaxation and a round of beam search withthe updated ?.
If either produces a certificate it returns theresult.and Collins (2011) and Rush and Collins (2011) de-velop Lagrangian relaxation-based approaches forexact machine translation.Apart from translation decoding, this paper isclosely related to work on column generation forNLP.
Riedel et al(2012) and Belanger et al(2012)relate column generation to beam search and pro-duce exact solutions for parsing and tagging prob-lems.
The latter work also gives conditions for whenbeam search-style decoding is optimal.7 ResultsTo evaluate the effectiveness of optimal beam searchfor translation decoding, we implemented decodersfor phrase- and syntax-based models.
In this sec-tion we compare the speed and optimality of these217decoders to several baseline methods.7.1 Setup and ImplementationFor phrase-based translation we used a German-to-English data set taken from Europarl (Koehn, 2005).We tested on 1,824 sentences of length at most 50words.
For experiments the phrase-based systemsuses a trigram language model and includes standarddistortion penalties.
Additionally the unconstrainedhypergraph includes further derivation informationsimilar to the graph described in Chang and Collins(2011).For syntax-based translation we used a Chinese-to-English data set.
The model and hypergraphscome from the work of Huang and Mi (2010).
Wetested on 691 sentences from the newswire portionof the 2008 NIST MT evaluation test set.
For ex-periments, the syntax-based model uses a trigramlanguage model.
The translation model is tree-to-string syntax-based model with a standard context-free translation forest.
The constraint matrix Ais based on the constraints described by Rush andCollins (2011).Our decoders use a two-pass architecture.
Thefirst pass sets up the hypergraph in memory, and thesecond pass runs search.
When possible the base-lines share optimized construction and search code.The performance of optimal beam search is de-pendent on the sequences ?
and ?.
For the step-size ?
we used a variant of Polyak?s rule (Polyak,1987; Boyd and Mutapcic, 2007), substituting theunknown optimal score for the last computed lowerbound: ?k ?
ub(k)?lb(k)||Ax(k)?b||22.
We adjust the order ofthe pruning parameter ?
based on a function ?
ofthe current gap: ?k ?
10?
(ub(k)?lb(k)).Previous work on these data sets has shown thatexact algorithms do not result in a significant in-crease in translation accuracy.
We focus on the effi-ciency and model score of the algorithms.7.2 Baseline MethodsThe experiments compare optimal beam search(OPTBEAM) to several different decoding meth-ods.
For both systems we compare to: BEAM, thebeam search decoder from Figure 3 using the orig-inal weights ?
and ?
, and ?
?
{100, 1000}; LR-TIGHT, Lagrangian relaxation followed by incre-Figure 7: Two graphs from phrase-based decoding.Graph (a) shows the duality gap distribution for 1,824sentences after 0, 5, and 10 rounds of LR.
Graph (b)shows the % of certificates found for sentences with dif-fering gap sizes and beam search parameters ?.
Dualitygap is defined as, ub - (?>x?
+ ?
).mental tightening constraints, which is a reimple-mentation of Chang and Collins (2011) and Rushand Collins (2011).For phrase-based translation we compare with:MOSES-GC, the standard Moses beam search de-coder with ?
?
{100, 1000} (Koehn et al 2007);MOSES, a version of Moses without gap constraintsmore similar to BEAM (see Chang and Collins(2011)); ASTAR, an implementation of A?
searchusing original outside scores, i.e.
OUTSIDE(?, ?
),and capped at 20,000,000 queue pops.For syntax-based translation we compare with:ILP, a general-purpose integer linear program-ming solver (Gurobi Optimization, 2013) andCUBEPRUNING, an approximate decoding methodsimilar to beam search (Chiang, 2007), tested with?
?
{100, 1000}.7.3 ExperimentsTable 1 shows the main results.
For phrase-basedtranslation, OPTBEAM decodes the optimal trans-lation with certificate in 99% of sentences with anaverage time of 17.27 seconds per sentence.
This21811-20 (558) 21-30 (566) 31-40 (347) 41-50 (168) all (1824)Phrase-Based time cert exact time cert exact time cert exact time cert exact time cert exactBEAM (100) 2.33 19.5 38.0 8.37 1.6 7.2 24.12 0.3 1.4 71.35 0.0 0.0 14.50 15.3 23.2BEAM (1000) 2.33 37.8 66.3 8.42 3.4 18.9 21.60 0.6 3.2 53.99 0.6 1.2 12.44 22.6 36.9BEAM (100000) 3.34 83.9 96.2 18.53 22.4 60.4 46.65 2.0 18.1 83.53 1.2 6.5 23.39 43.2 62.4MOSES (100) 0.18 0.0 81.0 0.36 0.0 45.6 0.53 0.0 14.1 0.74 0.0 6.0 0.34 0.0 52.3MOSES (1000) 2.29 0.0 97.8 4.39 0.0 78.8 6.52 0.0 43.5 9.00 0.0 19.6 4.20 0.0 74.6ASTAR (cap) 11.11 99.3 99.3 91.39 53.9 53.9 122.67 7.8 7.8 139.61 1.2 1.2 67.99 58.8 58.8LR-TIGHT 4.20 100.0 100.0 23.25 100.0 100.0 88.16 99.7 99.7 377.9 97.0 97.0 60.11 99.7 99.7OPTBEAM 2.85 100.0 100.0 10.33 100.0 100.0 28.29 100.0 100.0 84.34 97.0 97.0 17.27 99.7 99.7ChangCollins 10.90 100.0 100.0 57.20 100.0 100.0 203.4 99.7 99.7 679.9 97.0 97.0 120.9 99.7 99.7MOSES-GC (100) 0.14 0.0 89.4 0.27 0.0 84.1 0.41 0.0 75.8 0.58 0.0 78.6 0.26 0.0 84.9MOSES-GC (1000) 1.33 0.0 89.4 2.62 0.0 84.3 4.15 0.0 75.8 6.19 0.0 79.2 2.61 0.0 85.011-20 (192) 21-30 (159) 31-40 (136) 41-100 (123) all (691)Syntax-Based time cert exact time cert exact time cert exact time cert exact time cert exactBEAM (100) 0.40 4.7 75.9 0.40 0.0 66.0 0.75 0.0 43.4 1.66 0.0 25.8 0.68 5.72 58.7BEAM (1000) 0.78 16.9 79.4 2.65 0.6 67.1 6.20 0.0 47.5 15.5 0.0 36.4 4.16 12.5 65.5CUBE (100) 0.08 0.0 77.6 0.16 0.0 66.7 0.23 0.0 43.9 0.41 0.0 26.3 0.19 0.0 59.0CUBE (1000) 1.76 0.0 91.7 4.06 0.0 95.0 5.71 0.0 82.9 10.69 0.0 60.9 4.66 0.0 85.0LR-TIGHT 0.37 100.0 100.0 1.76 100.0 100.0 4.79 100.0 100.0 30.85 94.5 94.5 7.25 99.0 99.0OPTBEAM 0.23 100.0 100.0 0.50 100.0 100.0 1.42 100.0 100.0 7.14 93.6 93.6 1.75 98.8 98.8ILP 9.15 100.0 100.0 32.35 100.0 100.0 49.6 100.0 100.0 108.6 100.0 100.0 40.1 100.0 100.0Table 1: Experimental results for translation experiments.
Column time is the mean time per sentence in seconds,cert is the percentage of sentences solved with a certificate of optimality, exact is the percentage of sentences solvedexactly, i.e.
?>x+ ?
= ?>x?
+ ?
.
Results are grouped by sentence length (group 1-10 is omitted for space).is seven times faster than the decoder of Chang andCollins (2011) and 3.5 times faster then our reim-plementation, LR-TIGHT.
ASTAR performs poorly,taking lots of time on difficult sentences.
BEAM runsquickly, but rarely finds an exact solution.
MOSESwithout gap constraints is also fast, but less exactthan OPTBEAM and unable to produce certificates.For syntax-based translation.
OPTBEAM finds acertificate on 98.8% of solutions with an averagetime of 1.75 seconds per sentence, and is four timesfaster than LR-TIGHT.
CUBE (100) is an orderof magnitude faster, but is rarely exact on longersentences.
CUBE (1000) finds more exact solu-tions, but is comparable in speed to optimal beamsearch.
BEAM performs better than in the phrase-based model, but is not much faster than OPTBEAM.Figure 7.2 shows the relationship between beamsearch optimality and duality gap.
Graph (a) showshow a handful of LR rounds can significantly tightenthe upper bound score of many sentences.
Graph (b)shows how beam search is more likely to find opti-mal solutions with tighter bounds.
BEAM effectivelyuses 0 rounds of LR, which may explain why it findsso few optimal solutions compared to OPTBEAM.Table 2 breaks down the time spent in each partof the algorithm.
For both methods, beam search hasthe most time variance and uses more time on longersentences.
For phrase-based sentences, Lagrangianrelaxation is fast, and hypergraph construction dom-?
30 allmean median mean medianHypergraph 56.6% 69.8% 59.6% 69.6%PB Lag.
Relaxation 10.0% 5.5% 9.4% 7.6%Beam Search 33.4% 24.6% 30.9% 22.8%Hypergraph 0.5% 1.6% 0.8% 2.4%SB Lag.
Relaxation 15.0% 35.2% 17.3% 41.4%Beam Search 84.4% 63.1% 81.9 % 56.1%Table 2: Distribution of time within optimal beam search,including: hypergraph construction, Lagrangian relax-ation, and beam search.
Mean is the percentage of totaltime.
Median is the distribution over the median valuesfor each row.inates.
If not for this cost, OPTBEAM might be com-parable in speed to MOSES (1000).8 ConclusionIn this work we develop an optimal variant of beamsearch and apply it to machine translation decod-ing.
The algorithm uses beam search to produceconstrained solutions and bounds from Lagrangianrelaxation to eliminate non-optimal solutions.
Re-sults show that this method can efficiently find exactsolutions for two important styles of machine trans-lation.Acknowledgments Alexander Rush, Yin-WenChang and Michael Collins were all supported byNSF grant IIS-1161814.
Alexander Rush was partiallysupported by an NSF Graduate Research Fellowship.219ReferencesDavid Belanger, Alexandre Passos, Sebastian Riedel, andAndrew McCallum.
2012.
Map inference in chainsusing column generation.
In NIPS, pages 1853?1861.Stephen Boyd and Almir Mutapcic.
2007.
Subgradientmethods.Yin-Wen Chang and Michael Collins.
2011.
Exact de-coding of phrase-based translation models through la-grangian relaxation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 26?37.
Association for Computational Lin-guistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
computational linguistics, 33(2):201?228.Adria de Gispert, Gonzalo Iglesias, Graeme Blackwood,Eduardo R. Banga, and William Byrne.
2010.
Hierar-chical Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars.
In Com-putational linguistics, volume 36, pages 505?533.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathenWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vlad Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Proceed-ings of the 39th Annual Meeting on Association forComputational Linguistics, ACL ?01, pages 228?235.Inc.
Gurobi Optimization.
2013.
Gurobi optimizer refer-ence manual.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, pages 53?64.
As-sociation for Computational Linguistics.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 273?283, Cambridge,MA, October.
Association for Computational Linguis-tics.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In Proceedings ofthe 12th Conference of the European Chapter of theACL (EACL 2009), pages 380?388, Athens, Greece,March.
Association for Computational Linguistics.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4):607?615.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology, NAACL ?03,pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
Machine translation: From real users to research,pages 115?124.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of Human Language Technology Con-ference and Conference on Empirical Methods in Nat-ural Language Processing, pages 161?168, Vancou-ver, British Columbia, Canada, October.
Associationfor Computational Linguistics.R.
Kipp Martin, Rardin L. Rardin, and Brian A. Camp-bell.
1990.
Polyhedral characterization of dis-crete dynamic programming.
Operations research,38(1):127?138.Franz Josef Och, Nicola Ueffing, and Hermann Ney.2001.
An efficient A* search algorithm for statisti-cal machine translation.
In Proceedings of the work-shop on Data-driven methods in machine translation -Volume 14, DMMT ?01, pages 1?8, Stroudsburg, PA,USA.
Association for Computational Linguistics.Boris Polyak.
1987.
Introduction to Optimization.
Opti-mization Software, Inc.Sebastian Riedel and James Clarke.
2009.
Revisitingoptimal decoding for machine translation IBM model4.
In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, Companion Volume: Short Papers, pages 5?8.
As-sociation for Computational Linguistics.Sebastian Riedel, David Smith, and Andrew McCallum.2012.
Parse, price and cut: delayed column and rowgeneration for graph based parsers.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and Computational220Natural Language Learning, pages 732?743.
Associa-tion for Computational Linguistics.Alexander M Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through la-grangian relaxation.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, vol-ume 1, pages 72?82.Alexander M Rush and Michael Collins.
2012.
A tutorialon dual decomposition and lagrangian relaxation forinference in natural language processing.
Journal ofArtificial Intelligence Research, 45:305?362.Christoph Tillmann and Hermann Ney.
2003.
Word re-ordering and a dynamic programming beam search al-gorithm for statistical machine translation.
Computa-tional Linguistics, 29(1):97?133.Christoph Tillmann.
2006.
Efficient dynamic pro-gramming search algorithms for phrase-based SMT.In Proceedings of the Workshop on ComputationallyHard Problems and Joint Inference in Speech and Lan-guage Processing, CHSLP ?06, pages 9?16.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the Association forComputational Linguistics, ACL-44, pages 777?784,Morristown, NJ, USA.
Association for ComputationalLinguistics.Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-cedda.
2009.
Phrase-based statistical machine transla-tion as a traveling salesman problem.
In Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume1 - Volume 1, ACL ?09, pages 333?341, Stroudsburg,PA, USA.
Association for Computational Linguistics.221
