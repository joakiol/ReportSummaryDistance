Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
190?198, Prague, June 2007. c?2007 Association for Computational LinguisticsTowards Robust Unsupervised Personal Name DisambiguationYing ChenCenter for Spoken Language ResearchUniversity of Colorado at Boulderyc@colorado.eduJames MartinDepartment of Computer ScienceUniversity of Colorado at BoulderJames.Martin@colorado.eduAbstractThe increasing use of large open-domaindocument sources is exacerbating theproblem of ambiguity in named entities.This paper explores the use of a range ofsyntactic and semantic features in unsu-pervised clustering of documents that re-sult from ad hoc queries containing names.From these experiments, we find that theuse of robust syntactic and semantic fea-tures can significantly improve the state ofthe art for disambiguation performance forpersonal names for both Chinese and Eng-lish.1 IntroductionAn ever-increasing number of question answering,summarization and information extraction systemsare coming to rely on heterogeneous sets ofdocuments returned by open-domain search en-gines from collections over which applicationdevelopers have no control.
A frequent specialcase of these applications involves queriescontaining named entities of various sorts andreceives as a result a large set of possibly relevantdocuments upon which further deeper processingis focused.
Not surprisingly, many, if not most, ofthe returned documents will be irrelevant to thegoals of the application because of the massiveambiguity associated with the query names ofpeople, places and organizations in large opencollections.
Without some means of separatingdocuments that contain mentions of distinctentities, most of these applications will produceincorrect results.
The work presented here, there-fore, addresses the problem of automaticallyproblem of automatically separating sets of newsdocuments generated by queries containing per-sonal names into coherent partitions.The approach we present here combines unsu-pervised clustering methods with robust syntacticand semantic processing to automatically clusterreturned news documents (and thereby entities)into homogeneous sets.
This work follows on thework of Bagga & Baldwin (1998), Mann &Yarowsky (2003), Niu et al (2004), Li et al(2004), Pedersen et al (2005), and Malin (2005).The results described here advance this workthrough the use of syntactic and semantic featuresthat can be robustly extracted from the kind ofarbitrary news texts typically returned from open-domain sources.The specific contributions reported here fallinto two general areas related to robustness.
In thefirst, we explore the use of features extracted fromsyntactic and semantic processing at a level that isrobust to changes in genre and language.
In par-ticular, we seek to go beyond the kind of bag oflocal words features employed in earlier systems(Bagga & Baldwin, 1998; Gooi & Allan, 2004;Pedersen et al, 2005) that did not attempt to ex-ploit deep semantic features that are difficult toextract, and to go beyond the kind of biographicalinformation (Mann & Yarowsky, 2003) that isunlikely to occur with great frequency (such asplace of birth, or family relationships) in many ofthe documents returned by typical search engines.The second contribution involves the applicationof these techniques to both English and Chinesenews collections.
As we?ll see, the methods areeffective with both, but error analyses reveal in-teresting differences between the two languages.190The paper is organized as follows.
Section 2addresses related work and compares our workwith that of others.
Section 3 introduces our newphrase-based features along two dimensions: fromsyntax to semantics; and from local sentential con-texts to document-level contexts.
Section 4 firstdescribes our datasets and then analyzes the per-formances of our system for both English andChinese.
Finally, we draw some conclusions.2 Previous workPersonal name disambiguation is a difficult prob-lem that has received less attention than those top-ics that can be addressed via supervised learningsystems.
Most previous work (Bagga & Baldwin,1998; Mann & Yarowsky, 2003; Li et al, 2004;Gooi & Allan, 2004;  Malin, 2005; Pedersen et al,2005; Byung-Won On and Dongwon Lee, 2007)employed unsupervised methods because no largeannotated corpus is available and because of thevariety of the data distributions for different am-biguous personal names.Since it is common for a single document tocontain one or more mentions of the ambiguouspersonal name of interest, there is a need to definethe object to be disambiguated (the ambiguousobject).
In Bagga & Baldwin (1998), Mann &Yarowsky (2003) and Gooi & Allan (2004), anambiguous object refers to a single entity with theambiguous personal name in a given document.The underlying assumption for this definition is?one person per document?
(all mentions of theambiguous personal name in one document referto the same personal entity in reality).
In Niu et al(2004) and Pedersen et al (2005), an ambiguousobject is defined as a mention of the ambiguouspersonal name in a corpus.The first definition of the ambiguous object(document-level object) can include much infor-mation derived from that document, so that it canbe represented by rich features.
The later defini-tion of the ambiguous object (mention-level object)can simplify the detection of the ambiguous object,but because of the limited coverage, it usually canuse only local context (the text around the men-tion of the ambiguous personal name) and mightmiss some document-level information.
The kindof name disambiguation based on mention-levelobjects really solves ?within-document name am-biguity?
and ?cross-document name ambiguity?simultaneously, and often has a higher perform-ance than the kind based on document-level ob-jects because two mentions of the ambiguous per-sonal name in a document are very likely to referto the same personal entity.
From our news corpus,we also found that mentions of the ambiguouspersonal name of interest in a news article rarelyrefer to multiple entities, so our system will focuson the name disambiguation for document-levelobjects.In general, there are two types of informationusually used in name disambiguation (Malin,2005): personal information and relational infor-mation (explicit and implicit).
Personal informa-tion gives biographical information about the am-biguous object, and relational information speci-fies explicit or implicit relations between the am-biguous object and other entities, such as a mem-bership relation between ?John Smith?
and ?LaborParty.?
Usually, explicit relational information canbe extracted from local context, and implicit rela-tional information is far away from the mentionsof the ambiguous object.
A hard case of name dis-ambiguation often needs implicit relational infor-mation that provides a background for the am-biguous object.
For example, if two news articlesin consideration report an event happening in?Labor Party,?
this implicit relational informationbetween ?John Smith?
and ?Labor Party?
can givea hint for name disambiguation if no personal orexplicit relational information is available.Bagga & Baldwin (1998), Mann & Yarowsky(2003), Gooi & Allan (2004), Niu et al (2004),and Pedersen et al (2005) explore features in localcontext.
Bagga & Baldwin (1998), Gooi & Allan(2004), and Pedersen et al (2005) use local tokenfeatures; Mann & Yarowsky (2003) extract localbiographical information; Niu et al (2004) use co-occurring Named Entity (NE) phrases and NErelationships in local context.
Most of these localcontextual features are personal information orexplicit relational information.Li et al (2004) and Malin (2005) considernamed-entity disambiguation as a graph problem,and try to capture information related to the am-biguous object beyond local context, even implicitrelational information.
Li et al (2004) use the EMalgorithm to learn the global probability distribu-tion among documents, entities, and representativementions, and Malin (2005) constructs a socialnetwork graph to learn a similarity matrix.191In this paper, we also explore both personal andrelational information beyond local context.
Butwe achieve it with a different approach: extractingthese types of information by means of syntacticand semantic processing.
We not only extract lo-cal NE phrases as in Niu et al (2004), but also useour entity co-reference system to extract accurateand representative NE phrases occurring in adocument which may have a relation to the am-biguous object.
At the same time, syntactic phraseinformation sometimes can overcome the imper-fection of our NE system and therefore makes ourdisambiguation system more robust.3 Overall MethodologyOur approach follows a common architecture fornamed-entity disambiguation: the detection ofambiguous objects, feature extraction and repre-sentation, similarity matrix learning, and finallyclustering.In our approach, all documents are preproc-essed with a syntactic phrase chunker (Hacioglu,2004) and the EXERT1 system (Hacioglu et al2005; Chen & Hacioglu, 2006), a named-entitydetection and co-reference resolution system thatwas developed for the ACE2 project.
A syntacticphrase chunker segments a sentence into a se-quence of base phrases.
A base phrase is a syntac-tic-level phrase that does not overlap another basephrase.
Given a document, the EXERT systemfirst detects all mentions of entities occurring inthat document (named-entity detection) and thenresolves the different mentions of an entity intoone group that uniquely represents the entity(within-document co-reference resolution).
TheACE 2005 task can detect seven types of namedentities: person, organization, geo-political entity,location, facility, vehicle, and weapon; each typeof named entity can occur in a document with anyof three distinct formats: name, nominal construc-tion, and pronoun.
The F score of the syntacticphrase chunker, which is trained and tested on thePenn TreeBank, is 94.5, and the performances ofthe EXERT system are 82.9 (ACE value fornamed-entity detection) and 68.5 (ACE value forwithin-document co-reference resolution).1 http://sds.colorado.edu/EXERT2 http://projects.ldc.upenn.edu/ace/3.1 The detection of ambiguous objectsIn our approach, we assume that the ambiguouspersonal name has already been determined by theapplication.
Moreover, we adopt the policy of?one person per document?
as in Bagga &Baldwin (1998), and define an ambiguous objectas a set of target entities given by the EXERTsystem.
A target entity is an entity that has amention of the ambiguous personal name.
Giventhe definition of an ambiguous object, we define alocal sentence (or local context) as a sentence thatcontains a mention of any target entity.3.2 Feature extraction and representationSince considerable personal and relational infor-mation related to the ambiguous object resides inthe noun phrases in the document, such as the per-son?s job and the person?s location, we attempt tocapture this noun phrase information along twodimensions: from syntax to semantics, and fromlocal contexts to document-level contexts.Base noun phrase feature: To keep this featurefocused, we extract only noun phrases occurringin the local sentences and the summarized sen-tences (the headline + the first sentence of thedocument) of the document.
The local sentencesusually include personal or explicit relational in-formation about the ambiguous object, and thesummarized sentences of a news document usu-ally give a short summary of the whole news story.With the syntactic phrase chunker, we developtwo base noun phrase models: (i) Contextual basenoun phrases (Contextual bnp), the base nounphrases in the local sentences; (ii) Summarizedbase noun phrases (Summarized bnp), the basenoun phrases in the local sentences and the sum-marized sentences.
A base noun phrase of interestserves as an element in the feature vector.Named-Entity feature: Given the EXERT sys-tem, a direct and simple way to extract some se-mantic information is to use the named entitiesdetected in the document.
Based on their relation-ship to the ambiguous personal name, the namedentities identified in a text can be divided intothree categories:(i) Target entity: an entity that has a mentionof the ambiguous personal name.
Target entitiesoften include some personal information about theambiguous object, such as the title, position, andso on.192(ii) Local entity: an entity other than a targetentity that has a mention occurring in any localsentence.
Local entities often include entities thatare closely related to the ambiguous object, suchas employer, location and co-workers.
(iii) Non-local entity: an entity that is not ei-ther the local entity or the target entity.
Non-localentities are often implicitly related to the ambigu-ous object and provide background informationfor the ambiguous object.To assess how important these entities are tonamed-entity disambiguation, we create two kindsof entity models: (i) Contextual entities: the enti-ties in the feature vector are target entities andlocal entities; (ii) Document entities: the entitiesin the feature vector include all entities in thedocument including target entities, local entitiesand non-local entities.Since a given entity can be represented bymany mentions in a document, we choose a singlerepresentative mention to represent each entity.The representative mention is selected accordingto the following ordered preference list: longestNAME mention, longest NOMINAL mention.
Arepresentative mention phrase serves as an ele-ment in a feature vector.Although the mentions of contextual entities of-ten overlap with contextual base noun phrases, therepresentative mention of a contextual entity oftengoes beyond local sentences, and is usually thefirst or longest mention of that contextual entity.Compared to contextual base noun phrases, therepresentative mention of a contextual entity oftenincludes more detail and accurate informationabout the entity.
On the other hand, the contextualbase noun phrase feature detects all noun phrasesoccurring in local sentences that are not limited tothe seven types of named entities discovered bythe EXERT system.
Compared to the contextualentity feature, the contextual base noun phraseEntity spaceText spaceFeature SpaceContextual base noun phrases?
feature vector: < Hope Mills police Capt.
John Smith16,what16, he16, the statements16, no criminal violation16, what17, the individuals17, no directthreat17, Smith17, He and Thomas18, they18, Collins18, his bill18>Summarized base noun phrases?
feature vector: < Hope Mills police Capt.
John Smith16,what16, he16, the statements16, no criminal violation16, what17, the individuals17, no directthreat17, Smith17, He and Thomas18, they18, Collins18, his bill18, Collins1, restaurant1, HOPEMILLS2, Commissioner Tonzie Collins2, a town restaurant2, an alleged run-in2, two work-ers2, Feb. 212>Contextual entities?
feature vector: < Hope Mills police Capt.
John Smith16, Jenny Tho-mas4, Commissioner Tonzie Collins2, He and Thomas4, the individuals17>Document entities?
feature vector: < Hope Mills police Capt.
John Smith 16, Jenny Tho-mas4, Commissioner Tonzie Collins2, He and Thomas4, the individuals17, Andy?sCheesesteaks4, HOPE MILLS 2, two workers2, the Village Shopping Center 4, Hope MillsRoad 4 >Target entity:     < Hope Mills police Capt.
John Smith16, he16, Smith17, He18>Local entity:       < Thomas18, Jenny Thomas4, manager4>,< Collins18, his18, Collins1, Commissioner Tonzie Collins 2>, ?
?Non-local entity: < restaurant1, a town restaurant2, there2, Andy?s Cheesesteaks4>, ??
(Headline & S1) Collins banned from restaurant(S2) HOPE MILLS ?
Commissioner Tonzie Collins has been banned from a town restau-rant after an alleged run-in with two workers there Feb. 21.
??
(S4) ?In all fairness, that is not a representation of the town,?
said Jenny Thomas, managerat Andy?s Cheesesteaks in the Village Shopping Center on Hope Mills Road.
??
(S16) Hope Mills police Capt.
John Smith said based on what he read in the statements,no criminal violation was committed.
(S17) ?Based on what the individuals involved said, there was no direct threat,?
Smith said.
(S18) He and Thomas said they don?t think Collins intentionally left without paying hisbill.
?
?Figure 1: A Sample of Feature Extraction193feature is more general and can sometimes over-come errors propagated from the named-entitysystem.To make this more concrete, the feature vectorsfor a document containing ?John Smith?
are high-lighted in Figure 1.
The superscript number foreach phrase refers to the sentence where thephrase is located, and we assume that the syntacticphrase chunker and the EXERT system work per-fectly.3.3 Similarity matrix learningGiven a pair of feature vectors consisting ofphrase-based features, we need to choose a simi-larity scheme to calculate the similarity.
Becauseof the word-space delimiter in English, the featurevector for an English document comprises phrases,whereas that for a Chinese document comprisestokens.
There are a number of similarity schemesfor learning a similarity matrix from token-basedfeature vectors, but there are few schemes forphrase-based feature vectors.Cohen et al (2003) compared various similarityschemes for the task of matching English entitynames and concluded that the hybrid scheme theycall SoftTFIDF performs best.
SoftTFIDF is a to-ken-based similarity scheme that combines a stan-dard TF-IDF weighting scheme with the Jaro-Winkler distance function.
Since Chinese featurevectors are token-based, we can directly useSoftTFIDF to learn the similarity matrix.
However,English feature vectors are phrase-based, so weneed to run SoftTFIDF iteratively and call it ?two-level SoftTFIDF.?
First, the standard SoftTFIDFis used to calculate the similarity between phrasesin the pair of feature vectors; in the second phase,we reformulate the standard SoftTFIDF to calcu-late the similarity for the pair of feature vectors.First, we introduce the standard SoftTFIDF.
Ina pair of feature vectors S and T, S = (s1, ?
, sn )and T = (t1, ?
, tm).
Here, si (i = 1?n) and tj (j =1?m) are substrings (tokens).
Let CLOSE(?
; S;T)be the set of substrings w?S such that there issome v?T satisfying dist(w; v) > ?.
The Jaro-Winkler distance function (Winkler, 1999) isdist(;).
For w?
CLOSE(?
; S;T), let D(w; T) =);(max vwdistTv?
.
Then the standard SoftTFIDFis computed as)D( )V( )V()( SoftTFIDF);;(w, Tw, Tw, SS,TTSCLOSEw??=?
?
?
)(IDF log  1)  (TF log  )(V' ww,Sw, S ?+=?
?= S w, Sw, Sw, Sw2)( V)(  V  )( Vwhere TFw,S is the frequency of substrings w in S,and IDFw is the inverse of the fraction of docu-ments in the corpus that contain w. In computingthe similarity for the English phrase-based featurevectors, in the second step of ?two-levelSoftTFIDF,?
the substring w is a phrase and dist isthe standard SoftTFIDF.So far, we have developed several feature mod-els and learned the corresponding similarity ma-trices, but clustering usually needs only oneunique similarity matrix.
Since a feature may havedifferent effects for the disambiguation dependingon the ambiguous personal name in consideration,to achieve the best disambiguation ability, eachpersonal name may need its own weightingscheme to combine the given similarity matrices.However, learning that kind of weighting schemeis very difficult, so in this paper, we simply com-bine the similarity matrices, assigning equalweight to each one.3.4 ClusteringAlthough clustering is a well-studied area, a re-maining research problem is to determine the op-timal parameter setting during clustering, such asthe number of clusters or the stop-threshold, aproblem that is important for real tasks and that isnot at all trivial.Since the focus of this paper is only on featuredevelopment, we simply employ a clusteringmethod that can reflect the quality of the similar-ity matrix for clustering.
Here, we choose ag-glomerative clustering with a single linkage.
Sinceeach personal name may need a different parame-ter setting, to test the importance of the parametersetting for clustering, we use two kinds of stop-thresholds for agglomerative clustering in our ex-periments: first, to find the optimal stop-thresholdfor any ambiguous personal name and for eachfeature model, we run agglomerative clusteringwith all possible stop-thresholds, and choose theone that has the best performance as the optimal194stop-threshold; second, we use a fixed stop-threshold acquired from development data.4 Performance4.1 DataTo capture the real data distribution, we use twosets of naturally occurring data: Bagga?s corpusand the Boulder Name corpus, which is a newscorpus locally acquired from a web search.Bagga?s corpus is a document collection for theEnglish personal name ?John Smith?
that wasused by Bagga & Baldwin (1998).
There are 256articles that match the ?/John.*?Smith/?
regularexpression in 1996 and 1997 editions of the NewYork Times, and 94 distinct ?John Smith?
personalentities are mentioned.
Of these, 83 ?John Smiths?are mentioned in only one article (singleton clus-ters containing only one object), and 11 other?John Smiths?
are mentioned several times in theremaining 173 articles (non-singleton clusterscontaining more than one object).
For the task ofcross-document co-reference, Bagga & Baldwin(1998) chose 24 articles from 83 singleton clusters,and 173 other articles in 11 non-singleton clustersto create the final test data set ?
Bagga?s corpus.We collected the Boulder Name corpus by firstselecting four highly ambiguous personal nameseach in English and Chinese.
For each personalname, we retrieved the first non-duplicated 100news articles from Google (Chinese) or Googlenews (English).
There are four data sets for Eng-lish personal names and four data sets for Chinesepersonal names: James Jones, John Smith, Mi-chael Johnson, Robert Smith, and Li Gang, Li Hai,Liu Bo, Zhang Yong.Compared to Bagga?s corpus, which is limitedto the New York Times, the documents in theBoulder Name corpus were collected from differ-ent sources, and hence are more heterogeneousand noisy.
This variety in the Boulder Name cor-pus reflects the distribution of the real data andmakes named-entity disambiguation harder.For each ambiguous personal name in both cor-pora, the gold standard clusters have a long-taileddistribution - a high percentage of singleton clus-ters plus a few non-singleton clusters.
For exam-ple, in the 111 documents containing ?JohnSmith?
in the Boulder Name corpus, 53 ?JohnSmith?
personal entities are mentioned.
Of them,37 ?John Smiths?
are mentioned only once.
Thelong-tailed distribution brings some trouble toclustering, since in many clustering algorithms asingleton cluster is considered as a noisy point andtherefore is ignored.4.2 Corpus performanceBecause of the long tail of the data set, we designa baseline using one cluster per document.
Toevaluate our disambiguation system, we choosethe B-cubed scoring method that was used byBagga & Baldwin (1998).In order to compare our work to that of others,we re-implement the model used by Bagga &Baldwin (1998).
First, extracting all local sen-tences produces a summary about the given am-biguous object.
Then, the object is represented bythe tokens in its summary in the format of a vector,and the tokens in the feature vector are in theirmorphological root form and are filtered by astop-word dictionary.
Finally, the similarity matrixis learned by the TF-IDF method.Because both ?two-level SoftTFIDF?
and ag-glomerative clustering require a parameter setting,for each language, we reserve two ambiguous per-sonal names from the Boulder Name corpus asdevelopment data (John Smith, Michael Johnson,Li Gang, Zhang Yong), and the other data are re-served as test data: Bagga?s corpus and the otherpersonal names in the Boulder Name corpus(Robert Smith, James Jones, Li Hai, Liu Bo).For any ambiguous personal name and for eachfeature model, we find the optimal stop-thresholdfor agglomerative clustering, and show the corre-sponding performances in Table 1, Table 2 andTable 3.
However, for the most robust featuremodel, Bagga + summarized bnp + document en-tities, we learn the fixed stop-threshold for ag-glomerative clustering from the development data(0.089 for English data and 0.078 for Chinesedata), and show the corresponding performancesin Table 4.4.2.1  Performance on Bagga?s corpusTable 1 shows the performance of each featuremodel for Bagga?s corpus with the optimal stop-threshold.
The metric here is the B-cubed F score(precision/recall).Because of the difference between Bagga?s re-sources and ours (different versions of the named-entity system and different dictionaries of themorphological root and the stop-words), our best195B-cubed F score for Bagga?s model is 80.3?
4.3percent lower than the best performance reportedby Bagga & Baldwin (1998): 84.6.From Table 1, we found that the syntactic fea-tures (contextual bnp and summarized bnp) andsemantic features (contextual entities and docu-ment entities) consistently improve the perform-ances, and all performances outperform the bestresult reported by Bagga & Baldwin (1998): 84.6Model B-cubed performanceGold standard cluster # 35Baseline 30.17 (100.00/17.78)Bagga 80.32 (94.77/69.70)Bagga + contextual bnp   89.16 (89.18/89.13)Bagga + summarized bnp 89.59 (92.60/86.78)Bagga + summarized bnp + contextual entities 89.60 (87.16/92.18)Bagga + summarized bnp + document entities 92.02 (93.10/90.97)Table 1:  Performances for Bagga?s corpus with the optimal stop-thresholdNameModelJohn Smith(dev)Michael Johnson(dev)Robert Smith(test)James Jones(test)AverageperformanceGold standard cluster # 53 52 65 24Baseline 64.63 (111) 67.97 (101) 78.79 (100) 37.50 (104) 62.22Bagga 82.63 (75) 89.07 (66) 91.56 (73) 86.42 (24) 87.42Bagga + contextual bnp   85.18 (62) 89.13 (65) 92.35 (74) 86.45 (22) 88.28Bagga + summarized bnp 85.97 (66) 91.08 (51) 93.17 (70) 90.11 (33) 90.08Bagga + summarized bnp+ contextual entities85.44 (70) 94.24 (55) 91.94 (73) 96.66 (24) 92.07Bagga + summarized bnp+ document entities91.94 (61) 92.55 (51) 93.48 (67) 97.10 (28) 93.77Table 2: Performances for the English Boulder Name corpus with the optimal stop-thresholdNameModelLi Gang(dev)Zhang Yong(dev)Li Hai(test)Liu Bo(test)AverageperformanceGold standard cluster # 57 63 57 45Baseline 72.61 (100) 76.83 (101) 74.03 (97) 62.07 (100) 71.39Bagga  96.21 (57) 96.43 (64) 94.51 (64) 91.66 (49) 94.70Bagga + contextual bnp   97.57 (57) 96.38 (66) 94.53 (64) 93.21 (51) 95.42Bagga + summarized bnp 98.50 (56) 96.17 (61) 95.38 (62) 93.21 (51) 95.81Bagga + summarized bnp+ contextual entities99.50 (58) 95.49 (63) 96.75 (58) 91.05 (52) 95.70Bagga + summarized bnp+ document entities99.50 (56) 94.57 (70) 98.57 (59) 97.02 (48) 97.41Table 3: Performances for the Chinese Boulder Name corpus with the optimal stop-thresholdEnglish Name John Smith(dev)Michael Johnson(dev)Robert Smith(test)James Jones(test)AverageperformanceBagga + summarized bnp+ document entities91.31(91.94)90.57(92.55)86.71(93.48)96.64(97.10)91.31(93.77)Chinese Name Li Gang(dev)Zhang Yong(dev)Li Hai(test)Liu Bo(test)AverageperformanceBagga + summarized bnp+ document entities99.06(99.50)94.56(94.56)98.25(98.57)89.18(97.02)95.26(97.41)Table 4: Performances for the Boulder Name corpus with the fixed stop-threshold1964.2.2 Performance on the Boulder Name cor-pusTable 2 and Table 3 show the performance of eachfeature model with the optimal stop-threshold forthe English and Chinese Boulder Name corpora,respectively.
The metric is the B-cubed F scoreand the number in brackets is the correspondingcluster number.
Since the same feature model hasdifferent contributions for different ambiguouspersonal names, we list the average performancesfor all ambiguous names in the last column in bothtables.Comparing Table 2 and Table 3, we find thatBagga?s model has different performances for theEnglish and Chinese corpora.
That means thatcontextual tokens have different contributions inthe two languages.
There are three apparentcauses for this phenomenon.
The first concernsthe frequency of pronouns in English vs. pro-dropin Chinese.
The typical usage of pronouns in Eng-lish requires an accurate pronoun co-referenceresolution that is very important for the local sen-tence extraction in Bagga?s model.
In the BoulderName corpus, we found that ambiguous personalnames occur in Chinese much more frequentlythan in English.
For example, the string ?Liu Bo?occurs 876 times in the ?Liu Bo?
data, but thestring ?John Smith?
occurs only 161 times in the?John Smith?
data.
The repetition of ambiguouspersonal names in Chinese reduces the burden onpronoun co-reference resolution and hence cap-tures local information more accurately.The second factor is the fact that tokens inBagga?s model for Chinese are words, but a Chi-nese word is a unit bigger than an English word,and may contain more knowledge.
For example,?the White House?
has three words in English,and a word in Chinese.
Since Chinese named-entity detection can be considered a sub-problemof Chinese word segmentation, a word in Chinesecan catch partial information about named entities.Finally, compared to Chinese news stories,English news stories are more likely to mentionpersons marginal to the story, and less likely togive the complete identifying information aboutthem in local context.
Those phenomena requiremore background information or implicit rela-tional information to improve English named-entity disambiguation.From Table 2 and Table 3, we see that the aver-age performance of all ambiguous personal namesis increased (from 87.42 to 93.77 for English andfrom 94.70 to 97.41 for Chinese) by incorporatingmore information: contextual bnp (contextual basenoun phrases), summarized bnp (summarized basenoun phrases), contextual entities, and documententities.
This indicates that the phrase-based fea-tures, the syntactic and semantic noun phrases, arevery useful for disambiguation.From Table 2 and Table 3, we also see that thephrase-based features can improve the averageperformance, but not always for all ambiguouspersonal names.
For example, the feature model?Bagga + summarized bnp + contextual entities?hurts the performance for ?Robert Smith.?
As wementioned above, the Boulder Name corpus isheterogeneous, so each feature does not make thesame contribution to the disambiguation for anyambiguous personal name.
What we need to do isto find a feature model that is robust for all am-biguous personal names.In Table 4, we choose the last feature model?Bagga + summarized bnp + document entities?asthe final feature model, learn the fixed stop-threshold for clustering from the developmentdata, and show the corresponding performances asB-cubed F scores.
The performances in italics arethe performances with the optimal stop-threshold.From Table 4, we find that, with the exception of?Robert Smith?
and ?Liu Bo,?
the performancesfor other ambiguous personal names with thefixed threshold are close to the corresponding bestperformances.5 ConclusionThis work has explored the problem of personalnamed-entity disambiguation for news corpora.Our experiments extend token-based informationto include noun phrase-based information alongtwo dimensions: from syntax to semantics, andfrom local sentential contexts to document-levelcontexts.
From these experiments, we find thatrich and broad information improves the disam-biguation performance considerably for both Eng-lish and Chinese.
In the future, we will continue toexplore additional semantic features that can berobustly extracted, including features derivedfrom semantic relations and semantic role labels,and try to extend our work from news articles to197web pages that include more noisy information.Finally, we have focused here primarily on featuredevelopment and not on clustering.
We believethat the skewed long-tailed distribution that char-acterizes this data requires the use of clusteringalgorithms tailored to this distribution.
In particu-lar, the large number of singleton clusters is anissue that confounds the standard clustering meth-ods we have been employing.ReferencesA.
Bagga and B. Baldwin.
1998.
Entity?based Cross?document Co?referencing Using the Vector SpaceModel.
In 17th COLING.Y.
Chen and K. Hacioglu.
2006.
Exploration ofCoreference Resolution: The ACE Entity Detectionand Recognition Task.
In 9th International Confer-ence on TEXT, SPEECH and DIALOGUE.W.
Cohen, P. Ravikumar, S. Fienberg.
2003.
A Com-parison of String Metrics for Name-Matching Tasks.In IJCAI-03 II-Web Workshop.C.
H. Gooi and J. Allan.
2004.
Cross-DocumentCoreference on a Large Scale Corpus.
In NAACLK.
Hacioglu, B. Douglas and Y. Chen.
2005.
Detectionof Entity Mentions Occurring in English and Chi-nese Text.
Computational Linguistics.K.
Hacioglu.
2004.
A Lightweight Semantic ChunkingModel Based On Tagging.
In HLT/NAACL.X.
Li, P. Morie, and D. Roth.
2004.
Robust Reading:Identification and Tracing of Ambiguous Names.
InProc.
of  NAACL, pp.
17?24.B.
Malin.
2005.
Unsupervised Name Disambiguationvia Social Network Similarity.
SIAM.G.
Mann and D. Yarowsky.
2003.
Unsupervised Per-sonal Name Disambiguation.
In Proc.
of CoNLL-2003, Edmonton, Canada.C.
Niu, W. Li, and R. K. Srihari.
2004.
Weakly Super-vised Learning for Cross-document Person NameDisambiguation Supported by Information Extrac-tion.
In ACLB.
On and D. Lee.
2007.
Scalable Name Disambigua-tion using Multi-Level Graph Partition.
SIAM.T.
Pedersen, A. Purandare and A. Kulkarni.
2005.Name Discrimination by Clustering Similar Con-texts.
In Proc.
of the Sixth International Conferenceon Intelligent Text Processing and ComputationalLinguistics, pages 226-237.
Mexico City, Mexico.T.
Pedersen and A. Kulkarni.
2007.
Unsupervised Dis-crimination of Person Names in Web Contexts.
InProc.
of the Eighth International Conference on In-telligent Text Processing and Computational Lin-guistics.W.
E. Winkler.
1999.
The state of record linkage andcurrent research  problems.
Statistics of Income Di-vision, Internal Revenue Service Publication R99/04.A.
Yates and O. Etzioni.
2007.
Unsupervised Resolu-tion of Objects and Relations on the Web.
InNAACL.198
