Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 41?50,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingNamed Entity Recognition for TeluguP Srikanth and Kavi Narayana MurthyDepartment of Computer and Information Sciences,University of Hyderabad,Hyderabad, 500 046,email: patilsrik@yahoo.co.in, knmuh@yahoo.comAbstractThis paper is about Named Entity Recogni-tion (NER) for Telugu.
Not much work hasbeen done in NER for Indian languages ingeneral and Telugu in particular.
Adequateannotated corpora are not yet available inTelugu.
We recognize that named entitiesare usually nouns.
In this paper we there-fore start with our experiments in buildinga CRF (Conditional Random Fields) basedNoun Tagger.
Trained on a manually taggeddata of 13,425 words and tested on a testdata set of 6,223 words, this Noun Taggerhas given an F-Measure of about 92%.
Wethen develop a rule based NER system forTelugu.
Our focus is mainly on identify-ing person, place and organization names.A manually checked Named Entity taggedcorpus of 72,157 words has been developedusing this rule based tagger through boot-strapping.
We have then developed a CRFbased NER system for Telugu and testedit on several data sets from the Eenaaduand Andhra Prabha newspaper corpora de-veloped by us here.
Good performance hasbeen obtained using the majority tag con-cept.
We have obtained overall F-measuresbetween 80% and 97% in various experi-ments.Keywords: Noun Tagger, NER for Telugu, CRF,Majority Tag.1 IntroductionNER involves the identification of named entitiessuch as person names, location names, names oforganizations, monetary expressions, dates, numer-ical expressions etc.
In the taxonomy of Compu-tational Linguistics, NER falls within the categoryof Information Extraction which deals with the ex-traction of specific information from given docu-ments.
NER emerged as one of the sub-tasks of theDARPA-sponsored Message Understanding Confer-ence (MUCs).
The task has important significance inthe Internet search engines and is an important taskin many of the Language Engineering applicationssuch as Machine Translation, Question-Answeringsystems, Indexing for Information Retrieval and Au-tomatic Summarization.2 Approaches to NERThere has been a considerable amount of work onNER in English (Isozaki and Kazawa, 2002; Zhangand Johnson, 2003; Petasis et al, 2001; Mikheevet al, 1999).
Much of the previous work on namefinding is based on one of the following approaches:(1) hand-crafted or automatically acquired rules orfinite state patterns (2) look up from large name listsor other specialized resources (3) data driven ap-proaches exploiting the statistical properties of thelanguage (statistical models).The earliest work in named-entity recognition in-volved hand-crafted rules based on pattern matching(Appelt et al, 1993).
For instance, a sequence ofcapitalized words ending in ?Inc.?
is typically thename of an organization in the US, so one could im-plement a rule to that effect.
Another example ofsuch a rule is: Title Capitalized word ?
Title Per-son name.
Developing and maintaining rules anddictionaries is a costly affair and adaptation to dif-ferent domains is difficult.41In the second approach, the NER system recog-nizes only the named entities stored in its lists, alsocalled gazetteers.
This approach is simple, fast, lan-guage independent and easy to re-target - just re-create the lists.
However, named entities are toonumerous and are constantly evolving.
Even whennamed entities are listed in the dictionaries, it is notalways easy to decide their senses.
There can besemantic ambiguities.
For example, ?Washington?refers to both person name as well as place name.Statistical models have proved to be quite ef-fective.
Such models typically treat named-entityrecognition as a sequence tagging problem, whereeach word is tagged with its entity type if it is partof an entity.
Machine learning techniques are rela-tively independent of language and domain and noexpert knowledge is needed.
There has been a lotof work on NER for English employing the machinelearning techniques, using both supervised learningand unsupervised learning.
Unsupervised learningapproaches do not require labelled training data -training requires only very few seed lists and largeunannotated corpora (Collins and Singer, 1999).
Su-pervised approaches can achieve good performancewhen large amounts of high quality training data isavailable.
Statistical methods such as HMM (Bikelet al, 1997; Zhou and Su, 2001), Decision treemodel (Baluja et al, 2000; Isozaki, 2001), and con-ditional random fields (McCallum, 2003) have beenused.
Generative models such as Hidden MarkovModels (Bikel et al, 1997; Zhou and Su, 2001) haveshown excellent performance on the Message Un-derstanding Conference (MUC) data-set (Chinchor,1997).
However, developing large scale, high qual-ity training data is itself a costly affair.3 NER for Indian languagesNLP research around the world has taken giant leapsin the last decade with the advent of effective ma-chine learning algorithms and the creation of largeannotated corpora for various languages.
However,annotated corpora and other lexical resources havestarted appearing only very recently in India.
Notmuch work has been done in NER in Indian lan-guages in general and Telugu in particular.
Here weinclude a brief survey.In (Eqbal, 2006), a supervised learning systembased on pattern directed shallow parsing has beenused to identify the named entities in a Bengali cor-pus.
Here the training corpus is initially taggedagainst different seed data sets and a lexical con-textual pattern is generated for each tag.
The entiretraining corpus is shallow parsed to identify the oc-currence of these initial seed patterns.
In a positionwhere the seed pattern matches wholly or in part,the system predicts the boundary of a named entityand further patterns are generated through bootstrap-ping.
Patterns that occur in the entire training corpusabove a certain threshold frequency are consideredas the final set of patterns learned from the trainingcorpus.In (Li and McCallum, 2003), the authors haveused conditional random fields with feature induc-tion to the Hindi NER task.
The authors have iden-tified those feature conjunctions that will signifi-cantly improve the performance.
Features consid-ered here include word features, character n-grams(n = 2,3,4), word prefix and suffix (length - 2,3,4)and 24 gazetteers.4 NER for TeluguTelugu, a language of the Dravidian family, is spo-ken mainly in southern part of India and ranks sec-ond among Indian languages in terms of number ofspeakers.
Telugu is a highly inflectional and agglu-tinating language providing one of the richest andmost challenging sets of linguistic and statistical fea-tures resulting in long and complex word forms (Ku-mar et al, June 2007).
Each word in Telugu is in-flected for a very large number of word forms.
Tel-ugu is primarily a suffixing Language - an inflectedword starts with a root and may have several suffixesadded to the right.
Suffixation is not a simple con-catenation and morphology of the language is verycomplex.
Telugu is also a free word order Language.Telugu, like other Indian languages, is a resourcepoor language - annotated corpora, name dictionar-ies, good morphological analyzers, POS taggers etc.are not yet available in the required measure.
Al-though Indian languages have a very old and richliterary history, technological developments are ofrecent origin.
Web sources for name lists are avail-able in English, but such lists are not available inTelugu forcing the use of transliteration.42In English and many other languages, named en-tities are signalled by capitalization.
Indian scriptsdo not show upper-case - lower-case distinction.The concept of capitalization does not exist.
Manynames are also common nouns.
Indian names arealso more diverse i.e there are lot of variations fora given named entity.
For example ?telugude:s?aM?is written as Ti.Di.pi, TiDipi, te.de.pa:, de:s?aM etc.Developing NER systems is thus both challengingand rewarding.
In the next section we describe ourwork on NER for Telugu.5 Experiments and Results5.1 CorpusIn this work we have used part of the LERC-UoHTelugu corpus, developed at the Language Engineer-ing Research Centre at the Department of Computerand Information Sciences, University of Hyderabad.LERC-UoH corpus includes a wide variety of booksand articles, and adds up to nearly 40 Million words.Here we have used only a part of this corpus includ-ing news articles from two of the popular newspa-pers in the region.
The Andhra Prabha (AP) cor-pus consists of 1.3 Million words, out of which thereare approximately 200,000 unique word forms.
TheEenaadu (EE) corpus consists of 26 Million wordsin all.5.2 Evaluation MetricsWe use two standard measures, Precision, Recall.Here precision (P) measures the number of correctNEs in the answer file (Machine tagged data ) overthe total number of NEs in the answer file and re-call (R) measures the number of correct NEs in theanswer file over the total number of NEs in the keyfile (gold standard).
F-measure (F) is the harmonicmean of precision and recall: F = (?2+1)PR?2R+P when?2 = 1.
The current NER system does not handlemulti-word expressions - only individual words arerecognized.
Partial matches are also considered ascorrect in our analyses here.
Nested entities are notyet handled.5.3 Noun IdentificationNamed entities are generally nouns and it is there-fore useful to build a noun identifier.
Nouns canbe recognized by eliminating verbs, adjectives andclosed class words.
We have built a CRF based bi-nary classifier for noun identification.
Training dataof 13,425 words has been developed manually byannotating each word as noun or not-noun.
Next wehave extracted the following features for each wordof annotated corpus:?
Morphological features: Morphological an-alyzer developed at University of Hyderabadover the last many years has been used to ob-tain the root word and the POS category for thegiven word.
A morphological analyzer is use-ful in two ways.
Firstly, it helps us to recog-nize inflected forms (which will not be listed inthe dictionary) as not named entities.
Secondly,word forms not recognized by morphology arelikely to be named entities.?
Length: This is a binary feature whose valueis 1 if length of the given word is less than orequal 3 characters, otherwise 0.
This is basedon the observation that very short words arerarely nouns.?
Stop words: A stop word list including func-tion words has been collected from exist-ing bi-lingual dictionaries.
Bi-lingual dic-tionaries used for our experiments include CP Brown?s English-Telugu dictionary (Brown,1997), Telugu-Hindi dictionary developed atUniversity of Hyderabad and the Telugu-English dictionary developed by V Rao Ve-muri.
We have also extracted high frequencywords from our corpora.
Initially words whichhave occurred 1000 times or more were se-lected, hand filtered and added to the stop wordlist.
Then, words which have occurred 500 to1000 times were looked at, hand filtered andadded to the stop word list.
The list now has1731 words.
If the given word belongs to thislist, the feature value is 1 otherwise 0.?
Affixes: Here, we use the terms prefix/suffixto mean any sequence of first/last few charac-ters of a word, not necessarily a linguisticallymeaningful morpheme.
The use of prefix andsuffix information is very useful for highly in-flected languages.
Here we calculate suffixesof length from 4 characters down to 1 char-acter and prefixes of length from 7 characters43down to 1 character.
Thus the total number ofprefix/suffix features are 11.
For example, forthe word ?virigiMdi?
(broke), the suffixes are?iMdi, Mdi, di, i?
and the prefixes are ?virigiM,virigi, virig, viri, vir, vi, v?.
The feature valuesare not defined (ND) in the following cases:?
If length of a word is less than or equal to3 characters, all the affix values are ND.?
If length of a word is from 4 to 6 charac-ters, initial prefixes will be ND.?
If the word contains special symbols ordigits, both the suffix and prefix values areND.?
Position: This is a binary feature, whose valueis 1 if the given word occurs at the end of thesentence, otherwise 0.
Telugu is a verb finallanguage and this feature is therefore signifi-cant.?
POS: A single dictionary file is compiled fromthe existing bi-lingual dictionaries.
This file in-cludes the head word and its Part of Speech.
Ifa given word is available in this file, then itsPOS tag is taken as feature otherwise featurevalue is 0.?
Orthographic information This is a binaryfeature whose value is 1 if a given word con-tains digits or special symbols otherwise thefeature value is 0.?
Suffixes A list of linguistic suffixes of verbs,adjectives and adverbs were compiled from(Murthy and J.P.L.Gywnn, 1985) to recognizenot-nouns in a given sentence.
This featurevalue is 1 if the suffix of the given word be-longs to this list, otherwise it is 0.A feature vector consisting of the above featuresis extracted for each word in the annotated corpus.Now we have training data in the form of (Wi, Ti),where Wi is the ith word and its feature vector, andTi is its tag - NOUN or NOT-NOUN.
The featuretemplate used for training CRF is shown in Table-1,where wi is the current word, wi?1 is previousword, wi?2 is previous to previous word, wi+1 isnext word and wi+2 is next to next word.wi?2wi?1wiwi+1wi+2combination of wi?1, wicombination of wi, wi+1feature vector of wimorph tags of wi?2, wi?1, wi, wi+1 and wi+2output tag of current and previous word (ti,ti?1)Table 1: Feature Template used for Training CRFbased Noun TaggerThe inputs for training CRF consists of the train-ing data and the feature template.
The model learnedduring training is used for testing.
Apart from thebasic features described above, we have also experi-mented by including varying amounts of contextualinformation in the form of neighbouring words andtheir morph features.
Let us define:?
F1: [(wi), feature vector of wi, ti, ti?1].?
F2 : [wi?1, wi+1, (wi?1, wi), (wi, wi+1) andthe morph tags of wi?1 and wi+1].?
F3 : [wi?2, wi+2, morph tags of wi?2 andwi+2]The CRF trained with the basic template F1,which consists of the current word, the feature vec-tor of the current word and the output tag of the pre-vious word as the features, was tested on a test dataof 6,223 words and an F-measure of 91.95% wasobtained.
Next, we trained the CRF by taking thecombination of F1 and F2.
We also trained usingcombination of F1, F2 and F3.
The performancesof all 3 combinations are shown in Table-2.
It maybe seen that performance of the system is reducingas we increase the number of neighbouring words asfeatures.
Adding contextual features does not help.5.4 Heuristic based NER systemNouns which have already been identified in thenoun identification phase are now checked fornamed entities.
In this work, our main focusis on identifying person, place and organizationnames.
Indian place names and person names often44Feature combinations Precision Recall F-measureF1 91.64 92.28 91.95F1+F2 91.46 92.28 91.86F1+F2+F3 91.17 91.99 91.57Table 2: Performance of the CRF based Noun tagger with different feature combinationshave some suffix or prefix clues.
For example?na:yuDu?
is a person suffix clue for identifying?ra:ma:na:yuDu?
as a person entity and ?ba:d?
is alocation suffix clue for identifying ?haidara:ba:d?,?adila:ba:d?
etc as place entities.
We have manuallyprepared a list of such suffixes for both persons andlocations as also a list of prefixes for person names.List of organization names is also prepared manu-ally.
We have also prepared a gazetteer consistingof location names and a gazetteer of person namecontexts since context lists are also very useful inidentifying person names.
For example, it has beenobserved that whenever a context word such as?maMtri?
appears, a person name is likely to follow.Regular expressions are used to identify personentities like ?en.rame:S?
and organization entitieswhich are in acronym form such as ?Ti.Di.pi?,?bi.je.pi?
etc.
Initially one file of the corpus istagged using these seed lists and patterns.
Thenwe manually check and tag the unidentified namedentities.
These new named entities are also addedto the corresponding gazetteers and the relevantcontexts are added to their corresponding lists.Some new rules are also observed during manualtagging of unidentified names.
Here is an exampleof a rule:?if word[i] is NOUN and word[i-1] belongs tothe person context list then word[i] is person name?.Currently the gazetteers include 1346 locationnames, 221 organization names, and small lists ofprefixes, suffixes and other contextual cues that sig-nal the presence of named entities, their types, ortheir beginning or ending.
Using these lists andrules, we then tag another file from the remain-ing corpus.
This process of semi-automatic taggingis continued for several iterations.
This way wehave developed a named entity annotated databaseof 72,157 words, including 6,268 named entities(1,852 place names, 3,201 person names and 1,215organization names).5.4.1 Issues in Heuristic NERThere are ambiguities.
For example, ?ko:Tla?
isa person first name in ?ko:Tla vijaybha:skar?
andit is also a common word that exists in a phrasesuch as ?padi ko:Tla rupa:yalu?
(10 crore rupees).There also exists ambiguity between a person entityand place entity.
For example, ?siMha:calaM?
and?raMga:reDDi?
are both person names as well asplace names.
There are also some problems whilematching prefixes and suffixes of named entities.For example ?na:Du?
is a useful suffix for match-ing place names and the same suffix occurs withtime entities such as ?so:mava:raMna:Du?.
Prefixeslike ?ra:j?
can be used for identifying person enti-ties such as ?ra:jkiran?, ?ra:jgo:pa:l?,?ra:js?e:khar?etc.
but the same prefix also occurs with commonwords like ?ra:jaki:ya:lu?.
Thus these heuristics arenot fool proof.
We give below the results of our ex-periments using our heuristic based NER system forTelugu.5.4.2 Experiment 1Here, we have presented the performance of theheuristic-based NER system over two test data sets(AP-1 and AP-2).
These test data sets are from theAP corpus.
Total number of words (NoW) and num-ber of named entities in the test data sets AP-1 andAP-2 are given in Table-3.
Performance of the sys-tem is measured in terms of F-measure.
The rec-ognized named entity must be of the correct type(person, place or organization) for it to be countedas correct.
A confusion matrix is also given.
Thenotation used is as follows: PER - person; LOC -location; ORG - organization; NN - not-name.
Theresults are depicted in Tables 4, 5 and 6.45AP-1 AP-2PER LOC ORG PER LOC ORGP (%) 83.44 97.5 97.40 60.57 87.93 87.5R (%) 84.84 96.29 87.20 72.83 86.56 77.77F (%) 84.13 96.89 92.01 66.13 87.23 82.34Table 4: Performance of Heuristic based NER SystemAP Corpus PER LOC ORG NoWAP-1 296 81 86 3,537AP-2 173 321 63 7,032Table 3: Number of Entities in Test Data SetsActual/Obtained PER LOC ORG NNPER 285 0 0 12LOC 0 81 0 0ORG 6 0 75 5NN 63 3 3 3004Table 5: Confusion Matrix for the Heuristic basedSystem on AP-1Actual/Obtained PER LOC ORG NNPER 126 0 0 47LOC 2 277 0 41ORG 0 0 49 14NN 80 38 7 6351Table 6: Confusion matrix of heuristic based systemon AP-25.5 CRF based NER systemNow that we have developed a substantial amount oftraining data, we have also attempted supervised ma-chine learning techniques for NER.
In particular, wehave used CRFs.
For the CRF based NER system,the following features are extracted for each wordof the labelled training data built using the heuristicbased NER system.?
Class Suffixes/Prefixes This includes the fol-lowing three features:?
Location suffix: If the given word containsa location suffix, feature value is 1 other-wise 0.?
Person suffix: If the given word contains aperson suffix, feature value is 1 otherwiseit is 0.?
Person prefix: If the given word contains aperson prefix, feature value is 1 otherwiseit is 0.?
Gazetteers Five different gazetteers have beenused.
If the word belongs to the person firstname list, feature value is 1 else if the word be-longs to person middle name list, feature valueis 2 else if the word belongs to person last namelist, feature value is 3 else if the word belongsto location list, feature value is 4 else if theword belongs to organization list, feature valueis 5 else feature value is 0.?
Context If the word belongs to person contextlist, feature value is 1 else if the word belongsto location context list, feature value is 2 elseif the word belongs to organization context list,feature value is 3 else the feature value is 0.?
Regular Expression This includes two fea-tures as follows:?
REP: This is regular expression used toidentify person names.
The feature valueis 1 if the given word matches./([a-zA-Z:?]{1,3})\.([a-zA-Z:?]{1,3})?\.?([a-zA-Z:?]{1,3})?\.?[a-zA-Z:??]{4,}/?
REO: This is regular expression usedto identify organization names men-tioned in acronym format like ?bi.je.pi?,?e.ai.Di.eM.ke?.
etc.
This feature value is1, if the given word matches/(.{1,3})\.(.{1,3})\.(.{1,3})\.(.{1,3})?\.?(.{1,3})?\.?/)/46?
Noun tagger Noun tagger output is also usedas a feature value.?
Orthographic Information, Affixes, Mor-phological feature, Position feature, Lengthare directly extracted from ?Noun Identifica-tion?
process.The training data used for training CRFs consistsof words, the corresponding feature vectors and thecorresponding name tags.
We have used ?CRF++:Yet another CRF toolkit?
(Taku, ) for our experi-ments.
Models are built based on training data andthe feature template.
Results are given in the nextsubsection.
These models are used to tag the testdata.
The feature template used in these experimentsis as follows:wi?3wi?2wi?1wiwi+1wi+2combination of wi?1, wicombination of wi, wi+1feature vector of wimorph tags of wi?2, wi?1, wi, wi+1 and wi+2output tag of the previous word ti?1context information of the neighbour wordsTable 7: Feature Template used for Training CRF5.5.1 Experiment 2In this experiment, we took 19,912 words oftraining data (TR-1) and trained the CRF enginewith different feature combinations of the featuretemplate.
Details of the training data (TR-1 ?TR-2 ?
TR-3) and test data sets used in theseexperiments are given in Tables 8 and 9.
Here theexperiments are performed by varying the numberof neighbouring words in the feature template.
Inthe first case, feature template consists of currentword (wi), feature vector of the current word, twoneighbours of the current word (wi?1, wi+1), morphtags of the neighbour words, context informationof the neighbour words, combination of currentword and its neighbours and the output tag of theprevious word.
A model is built by training the CRFengine using this template.
The model built is usedin testing data sets (AP-1 and AP-2).
Similarly,we repeated the same experiment by considering4 and 6 neighbouring words of the current wordin the feature template.
The results are shown inTable-9 with varying number of neighbour wordsrepresented as window-size.
It is observed that thereis not much improvement in the performance ofthe system by including more of the neighbouringwords as features.Performance of the system without takinggazetteer features is shown in Table-11.
We seethat the performance of the system reduces when wehave not considered morph features and Noun taggeroutput in the feature template as can be seen fromTable-12.Finally, we have tested the performance of thesystem on two new test data sets (EE-1 and EE-2)from the EE corpus with varying amounts of trainingdata.
Total number of words (NoW) and the numberof named entities in the test data sets EE-1 and EE-2are depicted in Table-8.
Performance of the systemin terms of F-measure is shown in table 13.EE Corpus PER LOC ORG NoWEE-1 321 177 235 6,411EE-2 325 144 187 5221Table 8: Number of Entities in Test Data SetsAP corpus PER LOC ORG NoWTR-1 804 433 175 19,912TR-2 1372 832 388 34,116TR-3 2555 1511 793 60,525Table 9: Number of Entities in Training Data SetsGazetteers have a major role in performance whilemorph is adding a bit.
F-Measures of 74% to 93%AP-1 AP-2 EE-1 EE-2PER 93.76 79.36 70.91 69.84LOC 96.81 89.78 81.84 70.91ORG 80.27 91.66 71.73 80.75Table 12: Performance of the CRF based NER Sys-tem without Morph and Noun Tagger Features47Win- AP-1 AP-2Size PER LOC ORG PER LOC ORG2 99.62 100 98.41 90.07 93.55 98.21P 4 99.62 100 96.96 89.36 93.53 98.216 99.62 100 96.96 90.71 93.55 98.212 89.86 93.82 72.09 72.15 85.98 87.30R 4 89.86 93.82 74.41 71.59 85.66 87.306 89.52 93.82 74.41 72.15 85.98 87.302 94.49 96.81 83.22 80.12 89.61 92.43F 4 94.49 96.81 84.21 79.49 89.43 92.436 94.30 96.81 84.21 80.37 89.61 92.43Table 10: Performance of CRF based NER system with different window sizesAP-1 AP-2PER LOC ORG PER LOC ORGP 90.86 97.95 97.91 89.05 96.88 96.15R 57.09 59.25 54.65 69.31 67.91 79.36F 70.12 73.84 70.14 77.95 79.85 86.95Table 11: Performance of the CRF based NER system without GazetteersTest Data CLASS TR-1 TR-2 TR-3PER 75.14 79.70 81.58EE-1 LOC 81.84 80.66 81.45ORG 76.76 78.46 79.89PER 69.98 74.47 79.70EE-2 LOC 70.91 70.96 71.2ORG 82.13 82.82 83.69Table 13: Performance of CRF based NER systemwith varying amounts of Training Data on EE TestDatahave been obtained.
Effect of training corpus sizehas been checked by using 19,912 words, 34,116words and 60,525 words training corpora built fromthe AP newspaper corpus.
Test data was from EEnewspaper.
It is clearly seen that larger the trainingdata, better is the performance.
See table 13.5.5.2 Experiment 3: Majority Tag as anAdditional FeatureThere are some names like ?kRSNa:?, which canrefer to either person name, place name or a rivername depending up on the context in which they areused.
Hence, if the majority tag is incorporated asa feature, a classifier can be trained to take into ac-count the context in which the named entity is used,as well as frequency information.
In this experiment,we have used an unlabelled data set as an additionalresource from the EE news corpus.
The unlabelleddata set consists of 11,789 words.Initially, a supervised classifier h1 is trained onthe labelled data (TR-3) of 60,525 words.
Then thisclassifier labels the unlabelled data set (U) (11,789words) and produces a machine tagged data set U ?.Although our NER system is not so robust, usefulinformation can still be gathered as we shall see be-low.Next, a majority tag list (L) is produced by ex-tracting the list of named entities with their associ-ated majority tags from the machine tagged data setU ?.
The process of extracting majority tag list (L) issimple: We first identify possible name classes as-signed for the named entities in U ?
and we assignthe class that has occurred most frequently.
Next, inorder to recover unidentified named entities (inflec-tions of named entities already identified), we com-pare the root words of those words whose class is as-signed neither to person, place or organization withthe named entities already identified.
If there is anymatch with any of the named entities, the tag of theidentified named entity is assigned to the unidenti-48EE Without Majority Tag With Majority TagCorpus PER LOC ORG PER LOC ORGP 96.99 98.4 99.36 97.02 98.38 98.78R 70.40 69.49 66.80 71.02 68.92 68.93F 81.58 81.45 79.89 82.01 81.06 81.20Table 14: Performance of CRF based NER using Maj-tag on EE-1EE Without Majority Tag With Majority TagCorpus PER LOC ORG PER LOC ORGP 98.18 83.96 98.55 98.22 84.11 97.88R 67.07 61.80 72.72 68 62.5 74.31F 79.70 71.2 83.69 80.36 71.71 84.49Table 15: Performance of CRF based NER using Maj-tag on EE-2fied named entity.
L thus consists of (NE, Maj-tag)pairs, where Maj-tag is the name class that occursmost frequently for the named entity (NE) in the ma-chine tagged data set U ?.Now, we add this Maj-tag as an additional featureto labelled data (TR-3): if a word in labelled datamatches with a named entity in the majority tag list(L), then the corresponding Maj-tag (name class) isassigned as a feature value to that word in the la-belled data.
Finally, a classifier h2 is trained on thelabelled data (TR-3).
We use this classifier (h2) totag the test data sets (EE-1 and EE-2).
It can beobserved from tables 14 and 15 that including themajority tag feature improves the performance a bit.6 ConclusionsNot much work has been done in NER in Teluguand other Indian languages so far.
In this paper, wehave reported our work on Named Entity Recogni-tion for Telugu.
We have developed a CRF basednoun tagger, whose output is used as one of thefeature for the CRF based NER system.
We havealso described how we have developed a substantialtraining data using a heuristic based system throughboot-strapping.
The CRF based system performsbetter when compared with the initial heuristic basedsystem.
We have also shown that performance ofthe system can be improved by adding gazetteers asfeatures.
Morphological analyser has shown a smallcontribution to the performance of the system.
Itis also observed that there is some increase in per-formance of the system by using majority tag con-cept.
We have obtained F-measures between 80%and 97% in various experiments.
It may be observedthat we have not used any POS tagger or parser orannotated corpora tagged with POS or syntactic in-formation.
Once adequate POS taggers and chun-kers are developed, we may be able to do better.
Thecurrent work is limited to recognizing single wordNEs.
We plan to consider multi-token named enti-ties and nested structures in our future work.ReferencesD.
Appelt, J. Hobbs, J.
Bear, D. Israel, M. Kameyama,A.Kehler, D. Martin, K.Meyers, and M. Tyson.
1993.SRI international FASTUS system: MUC-6 test resultsand analysis.Shumeet Baluja, Vibhu O. Mittal, and Rahul Suk-thankar.
2000.
Applying Machine Learning forHigh-Performance Named-Entity Extraction.
Compu-tational Intelligence, 16(4):586?596.Daniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance learning name-finder.
In Proceedings ofthe fifth conference on Applied natural language pro-cessing, pages 194?201, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Charles Philip Brown.
1997.
Telugu-English dictionary.New Delhi Asian Educational Services.Nancy Chinchor.
1997.
MUC-7 Named Entity Task Def-inition (version 3.0).
In Proceedings of the 7th Mes-sage Understanding Conference (MUC-7).49M.
Collins and Y.
Singer.
1999.
Unsupervised mod-els for named entity classification.
Proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Corpora.Asif Eqbal.
2006.
Named Entity Recognition for Ben-gali.
Satellite Workshop on Language, Artificial Intel-ligence and Computer Science for Natural LanguageApplications (LAICS-NLP), Department of ComputerEngineering Faculty of Engineering Kasetsart Univer-sity, Bangkok, Thailand.Hideki Isozaki and Hideto Kazawa.
2002.
Efficient sup-port vector classifiers for named entity recognition.In Proceedings of the 19th international conferenceon Computational linguistics, pages 1?7, Morristown,NJ, USA.
Association for Computational Linguistics.Hideki Isozaki.
2001.
Japanese named entity recogni-tion based on a simple rule generator and decision treelearning.
In ACL ?01: Proceedings of the 39th An-nual Meeting on Association for Computational Lin-guistics, pages 314?321, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.G.
Bharadwaja Kumar, Kavi Narayana Murthy, andB.B.Chaudhari.
June 2007.
Statistical Analysis ofTelugu Text Corpora.
IJDL,Vol 36, No 2, pages 71?99.Wei Li and Andrew McCallum.
2003.
Rapid develop-ment of Hindi named entity recognition using con-ditional random fields and feature induction.
ACMTransactions on Asian Language Information Process-ing (TALIP), 2(3):290?294.McCallum.
2003.
Early results for Named Entity Recog-nition with Conditional Random Fields, feature induc-tion and web-enhanced lexicons.
In Proceedings ofthe seventh conference on Natural language learningat HLT-NAACL 2003, pages 188?191, Morristown, NJ,USA.
Association for Computational Linguistics.Andrei Mikheev, Marc Moens, and Claire Grover.
1999.Named Entity Recognition without gazetteers.
In Pro-ceedings of the ninth conference on European chap-ter of the Association for Computational Linguistics,pages 1?8, Morristown, NJ, USA.
Association forComputational Linguistics.Bh.Krishna Murthy and J.P.L.Gywnn.
1985.
A Grammarof Modern Telugu.
Oxford University Press, Delhi.Georgios Petasis, Frantz Vichot, Francis Wolinski, Geor-gios Paliouras, Vangelis Karkaletsis, and Constan-tine D. Spyropoulos.
2001.
Using machine learningto maintain rule-based named-entity recognition andclassification systems.
In ACL ?01: Proceedings ofthe 39th Annual Meeting on Association for Compu-tational Linguistics, pages 426?433, Morristown, NJ,USA.
Association for Computational Linguistics.Taku.
http://crfpp.sourceforge.net/.Tong Zhang and David Johnson.
2003.
A Robust RiskMinimization based Named Entity Recognition sys-tem.
In Proceedings of the seventh conference on Nat-ural language learning at HLT-NAACL 2003, pages204?207, Morristown, NJ, USA.
Association for Com-putational Linguistics.GuoDong Zhou and Jian Su.
2001.
Named EntityRecognition using an HMM-based chunk tagger.
InACL ?02: Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages473?480, Morristown, NJ, USA.
Association for Com-putational Linguistics.50
