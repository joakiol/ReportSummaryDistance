Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389?398,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCoreference Semantics from Web FeaturesMohit Bansal and Dan KleinComputer Science DivisionUniversity of California, Berkeley{mbansal,klein}@cs.berkeley.eduAbstractTo address semantic ambiguities in corefer-ence resolution, we use Web n-gram featuresthat capture a range of world knowledge in adiffuse but robust way.
Specifically, we ex-ploit short-distance cues to hypernymy, se-mantic compatibility, and semantic context, aswell as general lexical co-occurrence.
Whenadded to a state-of-the-art coreference base-line, our Web features give significant gains onmultiple datasets (ACE 2004 and ACE 2005)and metrics (MUC and B3), resulting in thebest results reported to date for the end-to-endtask of coreference resolution.1 IntroductionMany of the most difficult ambiguities in corefer-ence resolution are semantic in nature.
For instance,consider the following example:When Obama met Jobs, the president dis-cussed the economy, technology, and educa-tion.
His election campaign is expected to [...]For resolving coreference in this example, a sys-tem would benefit from the world knowledge thatObama is the president.
Also, to resolve the pro-noun his to the correct antecedent Obama, we canuse the knowledge that Obama has an election cam-paign while Jobs does not.
Such ambiguities aredifficult to resolve on purely syntactic or configu-rational grounds.There have been multiple previous systems thatincorporate some form of world knowledge in coref-erence resolution tasks.
Most work (Poesio etal., 2004; Markert and Nissim, 2005; Yang etal., 2005; Bergsma and Lin, 2006) addresses spe-cial cases and subtasks such as bridging anaphora,other anaphora, definite NP reference, and pronounresolution, computing semantic compatibility viaWeb-hits and counts from large corpora.
Thereis also work on end-to-end coreference resolutionthat uses large noun-similarity lists (Daume?
III andMarcu, 2005) or structured knowledge bases such asWikipedia (Yang and Su, 2007; Haghighi and Klein,2009; Kobdani et al, 2011) and YAGO (Rahmanand Ng, 2011).
However, such structured knowledgebases are of limited scope, and, while Haghighi andKlein (2010) self-acquires knowledge about corefer-ence, it does so only via reference constructions andon a limited scale.In this paper, we look to the Web for broader ifshallower sources of semantics.
In order to harnessthe information on the Web without presupposinga deep understanding of all Web text, we insteadturn to a diverse collection of Web n-gram counts(Brants and Franz, 2006) which, in aggregate, con-tain diffuse and indirect, but often robust, cues toreference.
For example, we can collect the co-occurrence statistics of an anaphor with various can-didate antecedents to judge relative surface affinities(i.e., (Obama, president) versus (Jobs, president)).We can also count co-occurrence statistics of com-peting antecedents when placed in the context of ananaphoric pronoun (i.e., Obama?s election campaignversus Jobs?
election campaign).All of our features begin with a pair of head-words from candidate mention pairs and computestatistics derived from various potentially informa-tive queries?
counts.
We explore five major cat-egories of semantically informative Web features,based on (1) general lexical affinities (via genericco-occurrence statistics), (2) lexical relations (viaHearst-style hypernymy patterns), (3) similarity ofentity-based context (e.g., common values of y for389which h is a y is attested), (4) matches of distribu-tional soft cluster ids, and (5) attested substitutionsof candidate antecedents in the context of a pronom-inal anaphor.We first describe a strong baseline consisting ofthe mention-pair model of the Reconcile system(Stoyanov et al, 2009; Stoyanov et al, 2010) us-ing a decision tree (DT) as its pairwise classifier.
Tothis baseline system, we add our suite of featuresin turn, each class of features providing substantialgains.
Altogether, our final system produces the bestnumbers reported to date on end-to-end coreferenceresolution (with automatically detected system men-tions) on multiple data sets (ACE 2004 and ACE2005) and metrics (MUC and B3), achieving signif-icant improvements over the Reconcile DT baselineand over the state-of-the-art results of Haghighi andKlein (2010).2 Baseline SystemBefore describing our semantic Web features, wefirst describe our baseline.
The core inference andfeatures come from the Reconcile package (Stoy-anov et al, 2009; Stoyanov et al, 2010), with modi-fications described below.
Our baseline differs mostsubstantially from Stoyanov et al (2009) in using adecision tree classifier rather than an averaged linearperceptron.2.1 ReconcileReconcile is one of the best implementations of themention-pair model (Soon et al, 2001) of coref-erence resolution.
The mention-pair model relieson a pairwise function to determine whether or nottwo mentions are coreferent.
Pairwise predictionsare then consolidated by transitive closure (or someother clustering method) to form the final set ofcoreference clusters (chains).
While our Web fea-tures could be adapted to entity-mention systems,their current form was most directly applicable tothe mention-pair approach, making Reconcile a par-ticularly well-suited platform for this investigation.The Reconcile system provides baseline features,learning mechanisms, and resolution procedures thatalready achieve near state-of-the-art results on mul-tiple popular datasets using multiple standard met-rics.
It includes over 80 core features that exploitvarious automatically generated annotations such asnamed entity tags, syntactic parses, and WordNetclasses, inspired by Soon et al (2001), Ng andCardie (2002), and Bengtson and Roth (2008).
TheReconcile system also facilitates standardized em-pirical evaluation to past work.1In this paper, we develop a suite of simple seman-tic Web features based on pairs of mention head-words which stack with the default Reconcile fea-tures to surpass past state-of-the-art results.2.2 Decision Tree ClassifierAmong the various learning algorithms that Recon-cile supports, we chose the decision tree classifier,available in Weka (Hall et al, 2009) as J48, an opensource Java implementation of the C4.5 algorithm ofQuinlan (1993).The C4.5 algorithm builds decision trees by incre-mentally maximizing information gain.
The train-ing data is a set of already classified samples, whereeach sample is a vector of attributes or features.
Ateach node of the tree, C4.5 splits the data on anattribute that most effectively splits its set of sam-ples into more ordered subsets, and then recurses onthese smaller subsets.
The decision tree can then beused to classify a new sample by following a pathfrom the root downward based on the attribute val-ues of the sample.We find the decision tree classifier to work betterthan the default averaged perceptron (used by Stoy-anov et al (2009)), on multiple datasets using multi-ple metrics (see Section 4.3).
Many advantages havebeen claimed for decision tree classifiers, includinginterpretability and robustness.
However, we sus-pect that the aspect most relevant to our case is thatdecision trees can capture non-linear interactions be-tween features.
For example, recency is very im-portant for pronoun reference but much less so fornominal reference.3 Semantics via Web FeaturesOur Web features for coreference resolution are sim-ple and capture a range of diffuse world knowledge.Given a mention pair, we use the head finder in Rec-oncile to find the lexical heads of both mentions (for1We use the default configuration settings of Reconcile(Stoyanov et al, 2010) unless mentioned otherwise.390example, the head of the Palestinian territories is theword territories).
Next, we take each headword pair(h1, h2) and compute various Web-count functionson it that can signal whether or not this mention pairis coreferent.As the source of Web information, we use theGoogle n-grams corpus (Brants and Franz, 2006)which contains English n-grams (n = 1 to 5) andtheir Web frequency counts, derived from nearly 1trillion word tokens and 95 billion sentences.
Be-cause we have many queries that must be run againstthis corpus, we apply the trie-based hashing algo-rithm of Bansal and Klein (2011) to efficiently an-swer all of them in one pass over it.
The featuresthat require word clusters (Section 3.4) use the out-put of Lin et al (2010).2We describe our five types of features in turn.
Thefirst four types are most intuitive for mention pairswhere both members are non-pronominal, but, asidefrom the general co-occurrence group, helped for allmention pair types.
The fifth feature group appliesonly to pairs in which the anaphor is a pronoun butthe antecedent is a non-pronoun.
Related work foreach feature category is discussed inline.3.1 General co-occurrenceThese features capture co-occurrence statistics ofthe two headwords, i.e., how often h1 and h2 areseen adjacent or nearly adjacent on the Web.
Thiscount can be a useful coreference signal because,in general, mentions referring to the same entitywill co-occur more frequently (in large corpora) thanthose that do not.
Using the n-grams corpus (for n= 1 to 5), we collect co-occurrence Web-counts byallowing a varying number of wildcards between h1and h2 in the query.
The co-occurrence value is:bin(log10(c12c1 ?
c2))2These clusters are derived form the V2 Google n-gramscorpus.
The V2 corpus itself is not publicly available, butthe clusters are available at http://www.clsp.jhu.edu/?sbergsma/PhrasalClusterswherec12 = count(?h1 ?
h2?
)+ count(?h1 ?
?
h2?
)+ count(?h1 ?
?
?
h2?
),c1 = count(?h1?
), andc2 = count(?h2?
).We normalize the overall co-occurrence count of theheadword pair c12 by the unigram counts of the indi-vidual headwords c1 and c2, so that high-frequencyheadwords do not unfairly get a high feature value(this is similar to computing scaled mutual infor-mation MI (Church and Hanks, 1989)).3 This nor-malized value is quantized by taking its log10 andbinning.
The actual feature that fires is an indica-tor of which quantized bin the query produced.
Asa real example from our development set, the co-occurrence count c12 for the headword pair (leader,president) is 11383, while it is only 95 for the head-word pair (voter, president); after normalization andlog10, the values are -10.9 and -12.0, respectively.These kinds of general Web co-occurrence statis-tics have been used previously for other supervisedNLP tasks such as spelling correction and syntac-tic parsing (Bergsma et al, 2010; Bansal and Klein,2011).
In coreference, similar word-associationscores were used by Kobdani et al (2011), but fromWikipedia and for self-training.3.2 Hearst co-occurrenceThese features capture templated co-occurrence ofthe two headwords h1 and h2 in the Web-corpus.Here, we only collect statistics of the headwords co-occurring with a generalized Hearst pattern (Hearst,1992) in between.
Hearst patterns capture variouslexical semantic relations between items.
For ex-ample, seeing X is a Y or X and other Y indicateshypernymy and also tends to cue coreference.
Thespecific patterns we use are:?
h1 {is | are | was | were} {a | an | the}?
h2?
h1 {and | or} {other | the other | another} h2?
h1 other than {a | an | the}?
h23We also tried adding count(?h1 h2?)
to c12 but thisdecreases performance, perhaps because truly adjacent occur-rences are often not grammatical.391?
h1 such as {a | an | the}?
h2?
h1 , including {a | an | the}?
h2?
h1 , especially {a | an | the}?
h2?
h1 of {the| all}?
h2For this feature, we again use a quantized nor-malized count as in Section 3.1, but c12 here is re-stricted to n-grams where one of the above patternsoccurs in between the headwords.
We did not al-low wildcards in between the headwords and theHearst-patterns because this introduced a significantamount of noise.
Also, we do not constrain the or-der of h1 and h2 because these patterns can holdfor either direction of coreference.4 As a real ex-ample from our development set, the c12 count forthe headword pair (leader, president) is 752, whilefor (voter, president), it is 0.Hypernymic semantic compatibility for corefer-ence is intuitive and has been explored in varyingforms by previous work.
Poesio et al (2004) andMarkert and Nissim (2005) employ a subset of ourHearst patterns and Web-hits for the subtasks ofbridging anaphora, other-anaphora, and definite NPresolution.
Others (Haghighi and Klein, 2009; Rah-man and Ng, 2011; Daume?
III and Marcu, 2005)use similar relations to extract compatibility statis-tics from Wikipedia, YAGO, and noun-similaritylists.
Yang and Su (2007) use Wikipedia to auto-matically extract semantic patterns, which are thenused as features in a learning setup.
Instead of ex-tracting patterns from the training data, we use allthe above patterns, which helps us generalize to newdatasets for end-to-end coreference resolution (seeSection 4.3).3.3 Entity-based contextFor each headword h, we first collect context seedsy using the patternh {is | are | was | were} {a | an | the}?
ytaking seeds y in order of decreasing Web count.The corresponding ordered seed list Y = {y} givesus useful information about the headword?s entitytype.
For example, for h = president, the top4Two minor variants not listed above are h1 including h2and h1 especially h2.30 seeds (and their parts of speech) include impor-tant cues such as president is elected (verb), pres-ident is authorized (verb), president is responsible(adjective), president is the chief (adjective), presi-dent is above (preposition), and president is the head(noun).Matches in the seed lists of two headwords canbe a strong signal that they are coreferent.
For ex-ample, in the top 30 seed lists for the headwordpair (leader, president), we get matches includingelected, responsible, and expected.
To capture thiseffect, we create a feature that indicates whetherthere is a match in the top k seeds of the two head-words (where k is a hyperparameter to tune).We create another feature that indicates whetherthe dominant parts of speech in the seed listsmatches for the headword pair.
We first collect thePOS tags (using length 2 character prefixes to indi-cate coarse parts of speech) of the seeds matched inthe top k?
seed lists of the two headwords, wherek?
is another hyperparameter to tune.
If the domi-nant tags match and are in a small list of importanttags ({JJ, NN, RB, VB}), we fire an indicator featurespecifying the matched tag, otherwise we fire a no-match indicator.
To obtain POS tags for the seeds,we use a unigram-based POS tagger trained on theWSJ treebank training set.3.4 Cluster informationThe distributional hypothesis of Harris (1954) saysthat words that occur in similar contexts tend to havea similar linguistic behavior.
Here, we design fea-tures with the idea that this hypothesis extends toreference: mentions occurring in similar contextsin large document sets such as the Web tend to becompatible for coreference.
Instead of collecting thecontexts of each mention and creating sparse fea-tures from them, we use Web-scale distributionalclustering to summarize compatibility.Specifically, we begin with the phrase-based clus-ters from Lin et al (2010), which were created us-ing the Google n-grams V2 corpus.
These clusterscome from distributional K-Means clustering (withK = 1000) on phrases, using the n-gram context asfeatures.
The cluster data contains almost 10 mil-lion phrases and their soft cluster memberships.
Upto twenty cluster ids with the highest centroid sim-ilarities are included for each phrase in this dataset392(Lin et al, 2010).Our cluster-based features assume that if theheadwords of the two mentions have matches intheir cluster id lists, then they are more compatiblefor coreference.
We check the match of not just thetop 1 cluster ids, but also farther down in the 20 sizedlists because, as discussed in Lin and Wu (2009),the soft cluster assignments often reveal differentsenses of a word.
However, we also assume thathigher-ranked matches tend to imply closer mean-ings.
To this end, we fire a feature indicating thevalue bin(i+j), where i and j are the earliest matchpositions in the cluster id lists of h1 and h2.
Binninghere means that match positions in a close rangegenerally trigger the same feature.Recent previous work has used clustering infor-mation to improve the performance of supervisedNLP tasks such as NER and dependency parsing(Koo et al, 2008; Lin and Wu, 2009).
However, incoreference, the only related work to our knowledgeis from Daume?
III and Marcu (2005), who use wordclass features derived from a Web-scale corpus via aprocess described in Ravichandran et al (2005).3.5 Pronoun contextOur last feature category specifically addresses pro-noun reference, for cases when the anaphoric men-tion NP2 (and hence its headword h2) is a pronoun,while the candidate antecedent mention NP1 (andhence its headword h1) is not.
For such a head-word pair (h1, h2), the idea is to substitute the non-pronoun h1 into h2?s position and see whether theresult is attested on the Web.If the anaphoric pronominal mention is h2 and itssentential context is l?
l h2 r r?, then the substitutedphrase will be l?
l h1 r r?.5 High Web counts of sub-stituted phrases tend to indicate semantic compati-bility.
Perhaps unsurprisingly for English, only theright context was useful in this capacity.
We chosethe following three context types, based on perfor-mance on a development set:5Possessive pronouns are replaced with an additional apos-trophe, i.e., h1 ?s.
We also use features (see R1Gap) that allowwildcards (?)
in between the headword and the context whencollecting Web-counts, in order to allow for determiners andother filler words.?
h1 r (R1)?
h1 r r?
(R2)?
h1 ?
r (R1Gap)As an example of the R1Gap feature, if theanaphor h2 + context is his victory and one candidateantecedent h1 is Bush, then we compute the normal-ized valuecount(?Bush ?s ?
victory?)count(?
?
?s ?
victory?)count(?Bush?
)In general, we computecount(?h1 ?s ?
r?)count(?
?
?s ?
r?)count(?h1?
)The final feature value is again a normalized countconverted to log10 and then binned.6 We have threeseparate features for the R1, R2, and R1Gap contexttypes.
We tune a separate bin-size hyperparameterfor each of these three features.These pronoun resolution features are similar toselectional preference work by Yang et al (2005)and Bergsma and Lin (2006), who compute seman-tic compatibility for pronouns in specific syntacticrelationships such as possessive-noun, subject-verb,etc.
In our case, we directly use the general contextof any pronominal anaphor to find its most compat-ible antecedent.Note that all our above features are designed to benon-sparse by firing indicators of the quantized Webstatistics and not the lexical- or class-based identitiesof the mention pair.
This keeps the total number offeatures small, which is important for the relativelysmall datasets used for coreference resolution.
Wego from around 100 features in the Reconcile base-line to around 165 features after adding all our Webfeatures.6Normalization helps us with two kinds of balancing.
First,we divide by the count of the antecedent so that when choos-ing the best antecedent for a fixed anaphor, we are not biasedtowards more frequently occurring antecedents.
Second, we di-vide by the count of the context so that across anaphora, ananaphor with rarer context does not get smaller values (for all itscandidate antecedents) than another anaphor with a more com-mon context.393Dataset docs dev test ment chnACE04 128 63/27 90/38 3037 1332ACE05 81 40/17 57/24 1991 775ACE05-ALL 599 337/145 482/117 9217 3050Table 1: Dataset characteristics ?
docs: the total number of doc-uments; dev: the train/test split during development; test: thetrain/test split during testing; ment: the number of gold men-tions in the test split; chn: the number of coreference chains inthe test split.4 Experiments4.1 DataWe show results on three popular and comparativelylarger coreference resolution data sets ?
the ACE04,ACE05, and ACE05-ALL datasets from the ACEProgram (NIST, 2004).
In ACE04 and ACE05, wehave only the newswire portion (of the original ACE2004 and 2005 training sets) and use the standardtrain/test splits reported in Stoyanov et al (2009)and Haghighi and Klein (2010).
In ACE05-ALL,we have the full ACE 2005 training set and use thestandard train/test splits reported in Rahman and Ng(2009) and Haghighi and Klein (2010).
Note thatmost previous work does not report (or need) a stan-dard development set; hence, for tuning our fea-tures and its hyper-parameters, we randomly splitthe original training data into a training and devel-opment set with a 70/30 ratio (and then use the fulloriginal training set during testing).
Details of thecorpora are shown in Table 1.7Details of the Web-scale corpora used for extract-ing features are discussed in Section 3.4.2 Evaluation MetricsWe evaluated our work on both MUC (Vilain et al,1995) and B3 (Bagga and Baldwin, 1998).
Bothscorers are available in the Reconcile infrastruc-ture.8 MUC measures how many predicted clustersneed to be merged to cover the true gold clusters.B3 computes precision and recall for each mentionby computing the intersection of its predicted andgold cluster and dividing by the size of the predicted7Note that the development set is used only for ACE04, be-cause for ACE05, and ACE05-ALL, we directly test using thefeatures tuned on ACE04.8Note that B3 has two versions which handle twinless (spu-rious) mentions in different ways (see Stoyanov et al (2009) fordetails).
We use the B3All version, unless mentioned otherwise.MUC B3Feature P R F1 P R F1AvgPerc 69.0 63.1 65.9 82.2 69.9 75.5DecTree 80.9 61.0 69.5 89.5 69.0 77.9+ Co-occ 79.8 62.1 69.8 88.7 69.8 78.1+ Hearst 80.0 62.3 70.0 89.1 70.1 78.5+ Entity 79.4 63.2 70.4 88.1 70.9 78.6+ Cluster 79.5 63.6 70.7 87.9 71.2 78.6+ Pronoun 79.9 64.3 71.3 88.0 71.6 79.0Table 2: Incremental results for the Web features on the ACE04development set.
AvgPerc is the averaged perceptron baseline,DecTree is the decision tree baseline, and the +Feature rowsshow the effect of adding a particular feature incrementally (notin isolation) to the DecTree baseline.
The feature categoriescorrespond to those described in Section 3.and gold cluster, respectively.
It is well known(Recasens and Hovy, 2010; Ng, 2010; Kobdani etal., 2011) that MUC is biased towards large clus-ters (chains) whereas B3 is biased towards singletonclusters.
Therefore, for a more balanced evaluation,we show improvements on both metrics simultane-ously.4.3 ResultsWe start with the Reconcile baseline but employ thedecision tree (DT) classifier, because it has signifi-cantly better performance than the default averagedperceptron classifier used in Stoyanov et al (2009).9Table 2 compares the baseline perceptron results tothe DT results and then shows the incremental addi-tion of the Web features to the DT baseline (on theACE04 development set).The DT classifier, in general, is precision-biased.The Web features somewhat balance this by increas-ing the recall and decreasing precision to a lesser ex-tent, improving overall F1.
Each feature type incre-mentally increases both MUC and B3 F1-measures,showing that they are not taking advantage of anybias of either metric.
The incremental improve-ments also show that each Web feature type bringsin some additional benefit over the information al-ready present in the Reconcile baseline, which in-cludes alias, animacy, named entity, and WordNet9Moreover, a DT classifier takes roughly the same amount oftime and memory as a perceptron on our ACE04 developmentexperiments.
It is, however, slower and more memory-intensive(?3x) on the bigger ACE05-ALL dataset.394MUC B3System P R F1 P R F1ACE04-TEST-RESULTSStoyanov et al (2009) - - 62.0 - - 76.5Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2Haghighi and Klein (2010) 67.4 66.6 67.0 81.2 73.3 77.0This Work: Perceptron Baseline 65.5 61.9 63.7 84.1 70.9 77.0This Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0This Work: ?
of DT+Web over DT (p < 0.05) 1.7 (p < 0.005) 1.3ACE05-TEST-RESULTSStoyanov et al (2009) - - 67.4 - - 73.7Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9This Work: DT Baseline 79.6 59.7 68.2 89.4 64.2 74.7This Work: DT + Web Features 75.0 64.7 69.5 81.1 70.8 75.6This Work: ?
of DT+Web over DT (p < 0.12) 1.3 (p < 0.1) 0.9ACE05-ALL-TEST-RESULTSRahman and Ng (2009) 75.4 64.1 69.3 54.4 70.5 61.4Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6Haghighi and Klein (2010) 77.0 66.9 71.6 55.4 74.8 63.8This Work: Perceptron Baseline 68.9 60.4 64.4 80.6 60.5 69.1This Work: DT Baseline 78.0 60.4 68.1 85.1 60.4 70.6This Work: DT + Web Features 77.6 64.0 70.2 80.7 65.9 72.5This Work: ?
of DT+Web over DT (p < 0.001) 2.1 (p < 0.001) 1.9Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets.
All systems reported here use automaticallyextracted system mentions.
B3 here is the B3All version of Stoyanov et al (2009).
We also report statistical significance of theimprovements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993).
Theperceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05-ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used intheir paper (p.c.).
Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all andonly the singleton twinless system mentions, so it is neither B3All nor B3None).
For completeness, our (untuned) B3None results(DT + Web) on the ACE05-ALL dataset are P=69.9|R=65.9|F1=67.8.class / sense information.10Table 3 shows our primary test results on theACE04, ACE05, and ACE05-ALL datasets, for theMUC and B3 metrics.
All systems reported use au-tomatically detected mentions.
We report our re-sults (the 3 rows marked ?This Work?)
on the percep-tron baseline, the DT baseline, and the Web featuresadded to the DT baseline.
We also report statisticalsignificance of the improvements from the Web fea-10We also initially experimented with smaller datasets(MUC6 and MUC7) and an averaged perceptron baseline, andwe did see similar improvements, arguing that these features areuseful independently of the learning algorithm and dataset.tures on the DT baseline.11 For significance testing,we use the bootstrap test (Noreen, 1989; Efron andTibshirani, 1993).Our main comparison is against Haghighi andKlein (2010), a mostly-unsupervised generative ap-proach that models latent entity types, which gen-erate specific entities that in turn render individualmentions.
They learn on large datasets including11All improvements are significant, except on the smallACE05 dataset with the MUC metric (where it is weak, atp < 0.12).
However, on the larger version of this dataset,ACE05-ALL, we get improvements which are both larger andmore significant (at p < 0.001).395Wikipedia, and their results are state-of-the-art incoreference resolution.
We outperform their systemon most datasets and metrics (except on ACE05-ALL for the MUC metric).
The other systems wecompare to and outperform are the perceptron-basedReconcile system of Stoyanov et al (2009), thestrong deterministic system of Haghighi and Klein(2009), and the cluster-ranking model of Rahmanand Ng (2009).We develop our features and tune their hyper-parameter values on the ACE04 development set andthen use these on the ACE04 test set.12 On theACE05 and ACE05-ALL datasets, we directly trans-fer our Web features and their hyper-parameter val-ues from the ACE04 dev-set, without any retuning.The test improvements we get on all the datasets (seeTable 3) suggest that our features are generally use-ful across datasets and metrics.135 AnalysisIn this section, we briefly discuss errors (in the DTbaseline) corrected by our Web features, and ana-lyze the decision tree classifier built during training(based on the ACE04 development experiments).To study error correction, we begin with the men-tion pairs that are coreferent according to the gold-standard annotation (after matching the system men-tions to the gold ones).
We consider the pairs that arewrongly predicted to be non-coreferent by the base-line DT system but correctly predicted to be corefer-ent when we add our Web features.
Some examplesof such pairs include:Iran ; the countrythe EPA ; the agencyathletic director ; MulcahyDemocrat Al Gore ; the vice president12Note that for the ACE04 dataset only, we use the ?SmartIn-stanceGenerator?
(SIG) filter of Reconcile that uses only a fil-tered set of mention-pairs (based on distance and other proper-ties of the pair) instead of the ?AllPairs?
(AP) setting that usesall pairs of mentions, and makes training and tuning very slow.13For the ACE05 and ACE05-ALL datasets, we revert to the?AllPairs?
(AP) setting of Reconcile because this gives us base-lines competitive with Haghighi and Klein (2010).
Since we didnot need to retune on these datasets, training and tuning speedwere not a bottleneck.
Moreover, the improvements from ourWeb features are similar even when tried over the SIG baseline;hence, the filter choice doesn?t affect the performance gain fromthe Web features.Barry Bonds ; the best baseball playerVojislav Kostunica ; the pro-democracy leaderits closest rival ; the German magazine Das MotorradOne of those difficult-to-dislodge judges ; John MarshallThese pairs are cases where our featureson Hearst-style co-occurrence and entity-basedcontext-match are informative and help discriminatein favor of the correct antecedents.
One advan-tage of using Web-based features is that the Webhas a surprising amount of information on even rareentities such as proper names.
Our features alsocorrect coreference for various cases of pronominalanaphora, but these corrections are harder to conveyout of context.Next, we analyze the decision tree built aftertraining the classifier (with all our Web features in-cluded).
Around 30% of the decision nodes (bothnon-terminals and leaves) correspond to Web fea-tures, and the average error in classification at theWeb-feature leaves is only around 2.5%, suggest-ing that our features are strongly discriminative forpairwise coreference decisions.
Some of the mostdiscriminative nodes correspond to the general co-occurrence feature for most (binned) log-count val-ues, the Hearst-style co-occurrence feature for itszero-count value, the cluster-match feature for itszero-match value, and the R2 pronoun context fea-ture for certain (binned) log-count values.6 ConclusionWe have presented a collection of simple Web-countfeatures for coreference resolution that capture arange of world knowledge via statistics of generallexical co-occurrence, hypernymy, semantic com-patibility, and semantic context.
When added to astrong decision tree baseline, these features give sig-nificant improvements and achieve the best resultsreported to date, across multiple datasets and met-rics.AcknowledgmentsWe would like to thank Nathan Gilbert, Adam Pauls,and the anonymous reviewers for their helpful sug-gestions.
This research is supported by Qualcommvia an Innovation Fellowship to the first authorand by BBN under DARPA contract HR0011-12-C-0014.396ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of MUC-7and LREC Workshop.Mohit Bansal and Dan Klein.
2011.
Web-scale featuresfor full-scale parsing.
In Proceedings of ACL.Eric Bengtson and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
In Pro-ceedings of EMNLP.Shane Bergsma and Dekang Lin.
2006.
Bootstrap-ping path-based pronoun resolution.
In Proceedingsof COLING-ACL.Shane Bergsma, Emily Pitler, and Dekang Lin.
2010.Creating robust supervised classifiers via web-scale n-gram data.
In Proceedings of ACL.Thorsten Brants and Alex Franz.
2006.
The Google Web1T 5-gram corpus version 1.1.
LDC2006T13.Kenneth Ward Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information, and lexicogra-phy.
In Proceedings of ACL.Hal Daume?
III and Daniel Marcu.
2005.
A large-scaleexploration of effective global features for a joint en-tity detection and tracking model.
In Proceedings ofEMNLP.B.
Efron and R. Tibshirani.
1993.
An introduction to thebootstrap.
Chapman & Hall CRC.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.
InProceedings of EMNLP.Aria Haghighi and Dan Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In Proceed-ings of NAACL-HLT.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Zellig Harris.
1954.
Distributional structure.
Word,10(23):146162.Marti Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of COLING.Hamidreza Kobdani, Hinrich Schutze, MichaelSchiehlen, and Hans Kamp.
2011.
Bootstrap-ping coreference resolution using word associations.In Proceedings of ACL.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of ACL.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scalen-grams.
In Proceedings of LREC.Katja Markert and Malvina Nissim.
2005.
Comparingknowledge sources for nominal anaphora resolution.Computational Linguistics, 31(3):367?402.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of ACL.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings ofACL.NIST.
2004.
The ACE evaluation plan.
In NIST.E.W.
Noreen.
1989.
Computer intensive methods forhypothesis testing: An introduction.
Wiley, New York.Massimo Poesio, Rahul Mehta, Axel Maroudas, andJanet Hitzeman.
2004.
Learning to resolve bridgingreferences.
In Proceedings of ACL.J.
R. Quinlan.
1993.
C4.5: Programs for machine learn-ing.
Morgan Kaufmann Publishers Inc., San Fran-cisco, CA, USA.Altaf Rahman and Vincent Ng.
2009.
Supervised modelsfor coreference resolution.
In Proceedings of EMNLP.Altaf Rahman and Vincent Ng.
2011.
Coreference reso-lution with world knowledge.
In Proceedings of ACL.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and NLP: Using local-ity sensitive hash functions for high speed noun clus-tering.
In Proceedings of ACL.Marta Recasens and Eduard Hovy.
2010.
Corefer-ence resolution across corpora: Languages, codingschemes, and preprocessing information.
In Proceed-ings of ACL.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: Making sense of the state-of-the-art.In Proceedings of ACL/IJCNLP.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Rec-oncile: A coreference resolution research platform.
InTechnical report, Cornell University.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof MUC-6.Xiaofeng Yang and Jian Su.
2007.
Coreference resolu-tion using semantic relatedness information from auto-matically discovered patterns.
In Proceedings of ACL.397Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2005.
Im-proving pronoun resolution using statistics-based se-mantic compatibility information.
In Proceedings ofACL.398
