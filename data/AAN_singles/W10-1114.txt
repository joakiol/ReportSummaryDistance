Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 90?95,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExtracting Formulaic and Free Text Clinical Research Articles Metadatausing Conditional Random FieldsSein Lin1 Jun-Ping Ng1 Shreyasee Pradhan2Jatin Shah2 Ricardo Pietrobon2 Min-Yen Kan11Department of Computer Science, National University of Singaporejustin@seinlin.com, {junping,kanmy}@comp.nus.edu.sg2Duke-NUS Graduate Medical School Singapore{shreyasee.pradhan,jashstar}@gmail.com, rpietro@duke.eduAbstractWe explore the use of conditional randomfields (CRFs) to automatically extract impor-tant metadata from clinical research articles.These metadata fields include formulaic meta-data about the authors, extracted from the titlepage, as well as free text fields concerning thestudy?s critical parameters, such as longitudi-nal variables and medical intervention meth-ods, extracted from the body text of the arti-cle.
Extracting such information can help bothreaders conduct deep semantic search of arti-cles and policy makers and sociologists trackmacro level trends in research.
Preliminary re-sults show an acceptable level of performancefor formulaic metadata and a high precisionfor those found in the free text.1 IntroductionThe increasing number of clinical research articlespublished each year is a double-edged sword.
Asof 2009, PubMed indexed over 19 million citations,over which 700,000 were added over the previousyear1.
While the research results further our knowl-edge and competency in the field, the volume of in-formation poses a challenge to researchers who needto stay up to speed.
Even within a single clinicalresearch area, there can be hundreds of new clini-cal research results per year.
Policy makers, whoneed to decide which clinical research proposals tofund and fast-track, and which proposals could tagonto existing research and cost share, have equallydaunting information synthesis issues that have bothmonetary and public health implications (Johnstonet al, 2006).1http://www.nlm.nih.gov/bsd/bsd_key.htmlSystematic reviews ?
secondary publications thatcompile evidence and best practices from primaryresearch results ?
partially address these concerns,but can take years before their final publication, dueto liability and administrative overheads.
In manyfast-paced fields of clinical practice, such guidelinescan be outdated by the time of publication.
Re-searchers and policy makers alike still need effectivetools to help them search, digest and organize theirknowledge of the primary literature.One avenue that researchers have turned to is theuse of automated information extraction (IE).
Wedistinguish between two distinct uses of InformationExtraction: 1) extracting regular, formulaic fields(e.g., author names, their institutional affiliation andemail addresses), and 2) extracting free text descrip-tions of key study parameters (e.g., longitudinal vari-ables, observation time periods, databases utilized).Extracting such formulaic fields helps policymakers determine returns on health-care investments(Kwan et al, 2007), as well as researchers in largescale sociological studies understand macroscopictrends in clinical research authorship and topic shiftsover time (Cappell and Davis, 2008; Lin et al,2008).
But due to the wide variety of publicationvenues for clinical research, even performing theseemingly simple task of author name extractionturns out to be difficult, and published studies thusfar have relied on manual analysis and extraction.Proposals to extract values of key study parame-ters may have more profound effects.
Deeper char-acterization of research artifacts will enable moresemantically-oriented searches of the clinical lit-erature.
Further programmatic access allows andencourages data sharing of raw clinical trial re-sults, databases and cohorts (Piwowar and Chap-90man, 2008) that may result in cost sharing acrosson-going studies, saving funds for other deservingclinical trials.On one hand, the medical community has beenproactive in using natural language processing(NLP) and information extraction technology in an-alyzing their own literature.
Many approaches tometadata extraction have used regular expressionsor baseline supervised machine learning classifiers.However, these techniques are not considered state-of-the-art.On the other hand, much of the work from theNLP community applied to biomedical research hasbeen on in-depth relationship extraction, such asthe identification of gene pathways and protein-protein interaction (PPI).
While certainly difficultand worthwhile problems to solve, there is room forcontribution even at the basic IE level, to retrieveboth regular and free form metadata fields.We address this need in this paper.
We apply alinear-chain Conditional Random Field (CRF) (Laf-ferty et al, 2001) as our methodology for extractingmetadata fields.
CRFs are a sequence labeling modelthat has shown good performance over a large num-ber of information extraction tasks.
We conduct ex-periments using basic token features to assess theirefficacy for metadata extraction.
While preliminary,our results indicate that CRFs are suitable for iden-tifying formulaic metadata, but may need additionaldeeper, natural language processing features to iden-tify free text fields.2 Related WorkMany researchers have recognized the utility of theapplication of IE on biomedical text.
These workshave focused mainly on the application of well-known machine learning algorithms to tag impor-tant biomedical entities such as genes and proteinswithin biomedical articles.
(Tanabe and Wilbur,2002) uses a Na?
?ve Bayes classifier, while (Zhou etal., 2004) uses use a Hidden Markov Model (HMM).The Conditional Random Field (CRF) learningmodel combines the strengths of two well knownmethods: the Hidden Markov Model (HMM), a se-quence labeling methodology, and the MaximumEntropy Model, a classification methodology.
Itmodels the probability of a class label y for a giventoken, directly from the observable stream of tokensx in direct (discriminative) manner, rather than asa by-product as in generative methods such as inHMMs.
A CRF can model arbitrary dependenciesbetween observation and class variables, but mostcommonly, a simple linear chain sequence is used(which connects adjacent class variables to eachother and to their corresponding observation vari-able), making them topologically similar to HMMs.Since their inception in 2001, (linear chain) CRFshave been applied extensively to many areas, includ-ing the biomedical field.
CRFs have been used forthe processing and extraction of important medicalentities and their relationships among each other.
(He and Kayaalp, 2008) reports on the suitability ofCRFs to find biological entities, combining basic or-thographic token features with features derived fromsemantic lexicons such as UMLS, ABGene, Sem-Rep and MetaMap.
In a related vein, CRFs havebeen applied to gene map and relationship identifi-cation as well (Bundschus et al, 2008; Talreja et al,2004).In a different domain, digital library practition-ers have also studied how to extract formulaic meta-data to enable more comprehensive article index-ing.
To extract author and title information, systemshave used both the Support Vector Machine (SVM)(Han et al, 2003) and CRFs (Peng and McCallum,2004; Councill et al, 2008).
These works have beenapplied largely to the computer science communityand have not yet been extensively tested on biomed-ical and clinical research articles.Our work differs from the above by making useof CRFs to extract fields in clinical text.
Similarlexical-based features are employed, however in ad-dition to regular author metadata, we also attempt toextract domain-specific fields from the body text ofthe article.3 MethodExternal to the scope of the research presented here,our wider project goal focuses on constructing aknowledge base of clinical researchers, databases,instruments and expertise in the Asia-Pacific region.Dataset.
In this pilot study, we created a goldstandard dataset consisting of freely-available ar-ticles available from PubMedCentral.
These arti-91cles focused on health services research in the Asia-Pacific region.
In particular, we selected open-access full-text literature documenting oncologicaland cardio-vascular studies in the region, over athree year period from 2005 to 2008.By constructing an appropriate staged query withPubMed, we obtained an initial listing of 260 arti-cles.
From an initial analysis, we determined thata significant portion (?1/3) of the retrieved full-text were not primary research, but reviews, casestudies, editorials or descriptions.
After eliminatingthese, the remaining 185 articles were earmarked tobe manually tagged by clinicians affiliated with theproject.
Since the resulting corpus compiles arti-cles across different journals and other publicationvenues, their presentation of even the formulaic au-thor metadata varied.The clinicians were given rich text (RTF) versionsof the original HTML documents retrieved fromPubMed.
They identified and extracted only the sec-tions of the articles that had pertinent data classes totag.
This process excluded most introductory, dis-cussion and result sections, preserving the sectionsthat described the study and results at a high level(e.g., Demographics and Methods).After an initial training session, each clinicianused a word processor to manually insert openingand closing XML tags for the tagset for a particularsubsection of the 185-article corpus.
Due to the highcost of clinician time, we chose to emphasize cover-age, rather than have the clinicians multiply annotatethe same articles.
As a result, we could not calculateannotation agreement, but feel that the repeatabilityof the annotation was addressed by the initial train-ing.
At the time of writing, 93 articles have beencompletely tagged and sectioned, with the remainderin progress.
The average length of the documents isabout 1300 words.
Once the dataset has been com-pleted, we plan to release the annotated data offsetsto the public, to encourage comparative evaluation.The clinicians annotated the following FormulaicAuthor Metadata (3 classes):?
Author (Au): The names of the authors of thestudy;?
E-mail (Em): The email addresses of the corre-sponding authors of the study;?
Institution (In): The names of the institutionsthat the authors are from.Such metadata can be used to build an author ci-tation network for macro trend analysis.
Note thatthis data is obtained from the article?s title page it-self, and not from any references to source articles,which have been the target of previous studies onCRF-based information extraction (Peng and Mc-Callum, 2004; Councill et al, 2008).
The cliniciansalso annotated the following Key Study Parameters(10 classes):?
Age Group (Ag): The age range of the subjectsof the study (e.g., 45 and 80 years, 21-79years);?
Data Analysis Name (Da): The name of themethod or software used in the analysis of datacollected for the study (e.g., proportional haz-ards survival models, SAS package);?
Data Collection Method (Dc): The data collec-tion methods for the study (e.g., medicalrecords, review of medical records and linkageto computerized discharge abstracts);?
Database Name (Dn): The name of any biomed-ical databases used or mentioned in the study(e.g., Queensland Cancer Registry, NationalDeath Index, population-based registry);?
Data Type (Dt): The type of data involved in thestudy (e.g., Cohort study, retrospectively);?
Geographical Area (Ga): The names of the ge-ographical area in which an experiment takesplace or the subjects are from (e.g., Pune,Switzerland);?
Intervention (Iv): The name of medical interven-tion used in the study (e.g., surgery, radiother-apy, chemotherapy, radio-frequency ablation);?
Longitudinal Variables (Lv): Data collectedover the observation period (e.g., subjects);?
Number of Observations (No): The number ofcases or subjects observed in the study (e.g.,158 Indigenous, 84 patients);92?
Time Period (Tp): The duration of an experi-ment or observation in the study (e.g., 1997?2002, between January 1988 and June 2006).As can be seen from the examples, the taggingguidelines loosely define the criteria for tagging.
Forsome classes, clinicians tagged entire noun phrasesor clauses, and for others, only numeric values andmodifiers were tagged.
This variability arises fromthe difficulty in tagging these free text fields.Features.
The CRF model requires a set of bi-nary features to serve as a representation of the text.A simple baseline is to use the presence/absence ofparticular tokens as features.
The CRF software im-plementation we utilized is CRF++2, which com-piles the binary features automatically from a con-text window centered the current labeling probleminstance.We first preprocess an input article from its RTFrepresentation and convert it into plain text.
Thisis a lossy transformation that discards font informa-tion and corrupts mathematical symbols that couldbe helpful in the detection task.
We take hyphenatedtoken forms (e.g., 2006-2007) and convert them intoindividual tokens.
The plain text is processed to notethe specific locations of the XML tags for the learn-ing process.
The bulk of the words in each articlewere not tagged by clinicians, and for these words,we assigned a Not Applicable (NA) tag.
We listthe simple inventory of feature types that we use forclassification.?
Vocabulary: Individual word tokens are stemmedwith Porter?s stemming algorithm (Porter,1980) and down-cased to collapse orthographicvariants.
Each word token is then used as anindividual feature.
This feature alone was usedto compile baseline performance as discussedlater in evaluation.?
Lexical: Lists of keywords were compiled to lendadditional weight for specific classes.
In par-ticular, we compiled lists of months, commonnames, cue words that signaled observations,institution names and data analyses methods.For example, a list of common given and sur-name names is useful for the Au field; while a2http://crfpp.sourceforge.netlist of months and their abbreviated forms helpto identify Tp.
Each list constitutes a differentfeature.
As an example, in the case of humannames, the names Alice and Auburn are on thelist.
If a word token corresponds to any of thewords in the list, the corresponding feature isturned on (e.g., isApersonName).?
Position: If a word token is within the first 15lines of an article, this feature is turned on.
Thisspecifically caters to limit the scope of the for-mulaic author metadata fields, to match themonly at the beginning of the article.?
Email: We create a specific feature for email ad-dresses that is turned on when a particular wordtoken is matched by a handwritten regular ex-pression.?
Numeric: For some free text classes, such as Ag,No and Tp, the tagged text often contains nu-meric data.
This can be present in both numericand word form (e.g., 23 versus.
twenty-three).We turn this feature on for a token solely con-taining digits or numeric word forms.?
Orthographic: Orthographic features, such asthe capitalization of a word token are useful tohelp identify proper nouns and names.
If thereare capital letters within a word token, this fea-ture is turned on.4 EvaluationTo ascertain the efficacy of our proposed solution,three-fold cross validation (CV) was first performedon a dataset comprising the 93 articles which havebeen completely annotated.Baseline.
For the purpose of comparison, we cre-ated a baseline system that utilizes the same CRF++toolkit but uses only the vocabulary feature typewith a five-word window (two previous tokens, thetarget token to be classified, and two subsequent to-kens).
The performance of this baseline system isshown in Table 1, where the standard classificationperformance measures of precision, recall and F1 aregiven.
Count measures the number of word tokensthat are predicted as belonging to the stated field.Discussion.
We see that the overwhelming major-ity of tokens are not tagged (belonging to class NA).93The skewness of the dataset is not uncommon for IEtasks.The baseline results show weak performanceacross the board.
Clearly, significant feature engi-neering could help boost performance.
Of particularsurprise was the relatively weak performance on theformulaic metadata.
From our manual analysis, itwas clear that the wide range and variety of tokenspresent in names and institutions barred the systemfrom achieving good performance on these classes.Comparative studies in citation and reference pars-ing usually peg classification performance of theseclasses at the 95% and above level.Without suitable customization, detection of thekey study parameters was also not possible.
Onlyrelatively common fields could be captured by theCRF, and when captured were more precise butlacked enough data to build a model with any ac-ceptable level of recall.Table 2 illustrates the improved results obtainedby running CRF++ with all of the described featureson the same dataset.
The same five-word windowsize is used for the vocabulary feature.
As seen, sig-nificant improvements over the baseline are obtainedfor all except four fields ?
Da, Dc, Iv, and Lv.These four fields were the classes with the most vari-ability in annotation.
For example, the data collec-tion methodologies (Dc) and interventions (Iv) areoften captured as long sentence fragments and hardto model with individual word cues.The largest improvements occurred for the classesof age groups Ag and time periods Tp, both of whichbenefited from the addition of the numeric featurewhich boosted recognition performance.5 Future WorkThe work presented here is ongoing, and based onour current results, we are planning to re-examinethe quality of the annotations and refine our anno-tation guideline and scheme.
We discovered caseswhere the CRF tagger correctly annotated key studyparameters which the annotators had missed or mis-keyed.
Drawing on lessons from the initial anno-tation exercise, a more comprehensive guideline isplanned which will provide concise instructions withaccompanying annotation examples.We also plan to enrich the feature set.
The currentField Prec.
Recall F1 CountFormulaic Author MetadataAu 84.6 74.3 79.1 1818Em 93.4 92.2 92.8 151In 80.5 69.5 74.6 3906Macro Avg.
86.2 78.7 82.3Key Study ParametersAg 29.0 40.4 33.8 334Da 61.0 39.0 47.6 708Dc 8.3 3.2 4.6 48Dn 35.9 15.1 21.2 92Dt 52.8 26.8 35.5 36Ga 7.3 4.5 5.6 41Iv 4.6 1.4 2.1 22Lv 15.4 20.0 17.4 13No 14.4 5.8 8.3 125Tp 73.6 55.8 63.5 261Macro Avg.
30.2 21.2 24.0NA 97.1 98.5 97.8 119998Table 1: Baseline aggregated results over 93 tagged arti-cles under 3 fold cross validation.Field P. Recall F1 CountFormulaic Author MetadataAu 89.0 85.3 87.1 7312Em 100.0 97.3 98.6 154In 91.3 78.0 84.1 4515Macro Avg.
93.4 86.6 89.9Key Study ParametersAg 64.3 35.4 45.7 240Da 79.3 37.2 50.6 2296Dc 20.0 1.6 2.9 125Dn 42.5 10.5 16.8 219Dt 70.0 19.7 30.7 71Ga 43.7 10.4 16.8 62Iv 40.0 2.7 5.1 73Lv 0.0 0.0 0.0 10No 43.4 10.7 17.1 308Tp 82.7 69.4 75.5 344Macro Avg.
48.5 19.7 26.1NA 97.5 99.3 98.4 120430Table 2: Aggregated results using the full feature set un-der 3 fold cross validation.94set employed is still simplistic and serves as a de-velopmental platform for furthering our feature en-gineering process.
For example, the vocabulary, po-sition and word lists features can be further modifiedto capture more fined-grained information.Once we exhaust the development of basic fea-tures, our future work will attempt to harness deeper,semantic features, making use of part-of-speechtags, grammar parses, and named entity recognitionfor example.
The incorporation of these features willlikely be useful in improving the performance of theCRF learner.
We also plan to use both clinical re-search and general medical ontologies (e.g., UMLS)to gain additional insight on individual terms thathave special domain-specific meanings.6 ConclusionWe have developed a CRF-based information ex-traction system that targets two different types ofmetadata present in clinical articles.
Our work inprogress demonstrates that formulaic author meta-data can be effectively extracted using the CRFmethodology.
By further performing feature engi-neering, we were able to extract key study parame-ters with a moderate level of success.
Our post eval-uation analysis indicates that more careful attentionto annotation and feature engineering will be neces-sary to garner acceptable performance of such im-portant clinical study parameters.AcknowledgmentsWe like to express our gratitude to the reviewerswhose insightful comments and pointers to addi-tional relevant studies have helped improve the pa-per.ReferencesM.
Bundschus, M. Dejori, M. Stetter, V. Tresp, and H.P.Kriegel.
2008.
Extraction of semantic biomedicalrelations from text using conditional random fields.BMC bioinformatics, 9(1):207.Mitchell S. Cappell and Michael Davis.
2008.
A signif-icant decline in the american domination of researchin gastroenterology with increasing globalization from1980 to 2005: An analysis of american authorshipamong 8,251 articles.
The American Journal of Gas-troenterology, 103:1065?1074.Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.ParsCit: An open-source CRF reference string parsingpackage.
In Proceedings of the Language Resourcesand Evaluation Conference (LREC 08), Marrakesh,Morrocco.Hui Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang, andEd Fox.
2003.
Automatic document meta-data extrac-tion using support vector machines.
In Proceedings ofJoint Conference on Digital Libraries.Ying He and Mehmet Kayaalp.
2008.
Biological entityrecognition with conditional random fields.
In Pro-ceedings of the Annual Symposium of the AmericanMedical Informatics Association (AMIA), pages 293?297.S.C.
Johnston, J.D.
Rootenberg, S Katrak, Wade S.Smith, and Jacob S Elkins.
2006.
Effect of a us na-tional institutes of health programme of clinical trialson public health and costs.
Lancet, 367:13191327.Patrick Kwan, Janice Johnston, Anne Fung, Doris SYChong, Richard Collins, and Su Lo.
2007.
A system-atic evaluation of payback of publicly funded healthand health services research in hong kong.
BMCHealth Services Research, 7(1):121.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
InProceedings of the International Conference on Ma-chine Learning, pages 282?289.JM Lin, JW Bohland, P Andrews, Burns GA, CB Allen,and PP Mitra.
2008.
An analysis of the abstracts pre-sented at the annual meetings of the society for neuro-science from 2001 to 2006.
PLoS ONE, 3(e2052).F.
Peng and A. McCallum.
2004.
Accurate informationextraction from research papers using conditional ran-dom fields.
In Proceedings of Human Language Tech-nology Conference and North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL), pages 329?336.Heather A. Piwowar and Wendy W. Chapman.
2008.Identifying data sharing in biomedical literature.
InProceedings of the Annual Symposium of the Ameri-can Medical Informatics Association (AMIA).M.F.
Porter.
1980.
An Algorithm For Suffix Stripping.14(3):130?137.R.
Talreja, A. Schein, S. Winters, and L. Ungar.
2004.GeneTaggerCRF: An entity tagger for recognizinggene names in text.
Technical report, Univ.
of Penn-sylvania.L.
Tanabe and W.J.
Wilbur.
2002.
Tagging gene andprotein names in biomedical text.
Bioinformatics,18(8):1124.G.
Zhou, J. Zhang, J. Su, D. Shen, and C. Tan.
2004.Recognizing names in biomedical texts: a machinelearning approach.
Bioinformatics, 20(7):1178?1190.95
