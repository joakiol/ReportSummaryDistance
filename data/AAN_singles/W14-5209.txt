Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 77?82,Dublin, Ireland, August 23rd 2014.Quo Vadis UIMA?Thilo G?otzIBM Germany R&DJ?orn KottmannSandstone SAAlexander LangIBM Germany R&DAbstractIn this position paper, we will examine the current state of UIMA from the perspective of a textanalytics practitioner, and propose an evolution of the architecture that overcomes some of thecurrent shortcomings.1 IntroductionUIMA (Unstructured Information Management Architecture, (UIMA, 2014)) became an Apache opensource project in 2006.
By that time, it had already seen about 5 years of development inside IBM.
Thatis, it was already quite mature when it came to Apache.
The core of UIMA has seen relatively littledevelopment since then, but the world around it has moved on.
Thus, we should ask ourselves: how doesUIMA currently fit into the software landscape, and what could be directions for future development toensure that the fit remains good?At its core, UIMA is a component framework that facilitates the combination of various text analyticscomponents.
Components are combined into analysis chains.
Simple chains are relatively easy to build,more complex workflows are difficult.UIMA components define and publish their own data models.
The actual data is handled by theframework, and created and queried via APIs.
Methods to serialize the data, e.g., for network transport,are also provided by the framework.
The data access and component APIs exist for Java and C/C++, sothat it is possible to combine Java and native components in a single analysis chain.Over the years, the UIMA eco system has grown to include scale out frameworks that sit on top of thecore and provide single or multinode scale out for UIMA analysis.2 How does UIMA integrate with other frameworks?The short answer is: it doesn?t, and that is where we see the biggest chance of evolution for the future.UIMA is a framework itself, and does not integrate well with other frameworks.
It has its own componentarchitecture, its own idea of logging, its own idea of controlling file system access, etc.This was fine at the time UIMA was built as we didn?t have generic component frameworks as wehave them today (actually they did exist, but they were not successful).
It is still fine today if you justwant to run a few files from disk, or simply don?t need another framework.However, nowadays general purpose (component) frameworks are in widespread use.
On the onehand, we have enterprise frameworks such as J2EE (J2EE, 2014) and Spring (Spring, 2014) that hostclassic n-tier applications with some kind of http frontend.
On the other, we have big data scale outframeworks such as Apache Hadoop (data at rest, (Hadoop, 2014)) or Apache Storm (data in motion,streaming, (Storm, 2014)).
If you are building a non-trivial application, chances are good that you willmake use of the many services such frameworks offer and base you application on one of them.For example, imagine you want to scale out your analytics with Apache Hadoop.
As long as you runyour entire pipeline in a single mapper, everything is fine.
There may be a few details that need to beironed out, but in general, this kind of scale out scenario works pretty well.
However, suppose you wantThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
License details: http://creativecommons.org/licenses/by/4.0/77to run part of your analytics in a mapper, and another in a reducer, because you need to do aggregationof some kind.
So you need to serialize the results of your first analysis step, and pass them on to thethe second one.
While this can be done with quite a bit of work and deep UIMA skills, it is much morecomplicated than it ought to be.When we consider data-in-motion scenarios that are handled by Apache Storm or similar frameworks,this issue becomes even more pressing.
There the individual units of processing are usually very small,and the framework handles the workflow.
This is completely incompatible with the UIMA approachwhere UIMA needs to know about the entire processing pipeline to be able to instantiate the individualcomponents.
If every annotator works by itself and doesn?t know where its next data package will comefrom, coordinating the data transfer between all those components becomes a huge effort.It should be noted that even if you are willing to stay completely within the UIMA framework, thereare things you might reasonably want to do that are difficult to impossible.
For example, you can nottake two analysis chains that somebody else has configured and combine them into one.
Suppose youobtained a tokenizer and POS tagger from one source (so an aggregate analysis engine in UIMA speak),and some other chain that builds on that from somewhere else.
You can?t just combine the two, becauseit isn?t possible to make an aggregate of aggregates.
You need to take everything apart and then manuallyreassemble the whole thing to make one big analysis chain.
Generally, this is possible, but it can be verydifficult for something that should be really simple to do.Over the years, UIMA has accumulated its own set of scale out frameworks.
While this is ok if allyou want to do happens in UIMA, that is rarely the case for most industrial applications.
Instead, theframework is chosen for the kinds of data that are expected, or for how it fits into the rest of the softwarelandscape.
So a UIMA specific scale out framework extension is at best a niche solution.Another area of business computing where UIMA plays no role at all today is mobile.
UIMA is justtoo fat to run on a mobile device.
That is not to say that there isn?t a place for something like UIMA onmobile devices, but it would need to be much more lightweight than what we have today.
We will makesome suggestions in a later section how that could be achieved.On a positive note, uimaFIT (Ogren and Bethard, 2009) is a move in the right direction.
It allowsyou to configure your analysis engine from code, avoiding a lot of the top-level XML configuration thatmakes dealing with UIMA pipelines so cumbersome.
However, UIMAFit is a layer on top of the baseUIMA framework, and while it addresses some concerns, it does not do away with the underlying issues.3 The data modelThe UIMA data model is very strict in the sense that it is statically typed.
That is, when an analysischain is instantiated, all information about the kind of data that can be created in the chain needs to beknown ahead of time.
This approach has certain advantages, which is why it was chosen at the time.Specifically, it allows for a very compact encoding of the data in memory, and very fast access to thedata.On the other hand, such a strict data model is quite cumbersome to work with, particularly whenyou need to combine analysis components that may come from very different sources.
For example, acommon scenario is that you are using a tokenizer from some source, and want to add information tothe tokens (e.g., POS tags or some kind of dictionary lookup information).
You can?t do this unless youmodify the type system for the entire chain, even though it is only relevant from the time you actuallyadd the information.What we are facing is a classic trade-off between performance (memory and CPU) and developerconvenience.
While we still need efficient processing, given the size of the data sets we are looking atnowadays, we would still like to argue that developer time is a more expensive resource than machinepower.
Thus, it is time to move away from the static type system for data, and move towards a dynamicmodel that is a lot easier to handle from the perspective of the developer.It should be noted that some of the constraints on data were realized by UIMA developers very earlyon, basically as soon as people started running analyses in more than one process (or on more than onenode).
Options were added on top of UIMA to make communication between loosely coupled compo-78nents easier.
Again however, these were added on top the base UIMA framework and are essentially justused by UIMA?s own scale out frameworks.
That is, they are quite difficult to use by end users who wantto use a different framework for scale out.Another point that is made difficult by a static type system is the caching of analysis results in (rela-tional or other) databases.
This is a point that comes up again and again on the UIMA users?
mailing list.Actually, it is not so much the writing out as the reading in of previous results when your data modelhas changed in some minor way that doesn?t even affect what you want to do with the data.
Here also, adynamic approach to data modelling would go a long way towards simplifying this process.Finally, the need for the JCas (a system where you can offline create Java classes for your data struc-tures) would go away entirely.4 Analysis component reuseThe main raison d?
?etre for UIMA is analysis engine reuse.
Why then do we often find it so difficult toreuse existing components?The main obstacle for an implementor who wants to make an analysis engine (AE) reusable is that thetarget type system is not known to and varies between users.
The best a reusable AE can do is to allowusers to map the input and output types as part of the configuration.
Sadly, UIMA doesn?t support thisuse case explicitly.
There are no types for configuration values which are intended to contain type systemvalues such as UIMA type or feature names.Certain type systems are too complex for a type mapping and sometimes an AE is programmed againsta specific type system.
To integrate an existing AE in such a scenario is only possible with an adapter infront and behind the AE in order to convert the input and output types dynamically.
The adapters mustbe implemented as an AE and they increase the complexity of the UIMA pipeline.An implementor who updates an AE can only update the jar file used to distribute it to the users.
An AEconsists of two parts, the implementing classes and a XML Descriptor file.
The descriptor file containsboth the schema for the configuration and the user defined configuration values.
Users typically copythis file and change the configuration values.
An implementor can?t update the users?
XML descriptor toa newer version.
A user will have to manually merge the changes of the customized file and the updateddescriptor.Even though it is possible to build reusable UIMA AEs, users repeatedly prefer integrating commonopen source NLP components themselves rather than dealing with the existing UIMA integrations (e.g.,Apache OpenNLP).
This is quite sad given that UIMA set out to make this sort of manual integrationeffort redundant.5 Generic analysis enginesAnother advantage of having a dynamic type system that can be added to at runtime would be to makegeneric analysis engines easier to develop and more convenient to use.
By a generic annotator, we meanan annotator that is configured by the end user to perform a certain task.
This could be for examplea generic dictionary matcher, or a regular expression matcher as they already exists on Apache as adownloadable components.So what is the issue then?
Today in UIMA, the type system is defined ahead of time.
So it is notpossible to write, e.g., a regex matcher that creates a new kind of annotation, unless that annotation isalso added in the type system.
So you can either require users of such generic annotators to always changethings in two locations at the same time: the type system specification, and the regex configuration.
Weall know what kind of chaos that leads to.
Or you can have the generic annotator produce a genericannotation, such as a RegexAnnotation.
The results range from the awkward (because results needto be transferred to the real target annotation) to the unusable (because the fixed generic annotations arenot flexible enough to represent the intended results).If on the other hand the type system was dynamic, generic annotators could create new data types atruntime from an end user specification, with no extra effort.796 Design point: a layered architectureIn this section, we will propose a design that should overcome the shortcomings of UIMA that we haveidentified.
While there are many specifics to be worked out, there are sufficient details to form the basisof an ongoing discussion.
We?ll begin with an overview of the design we envision, and then drill downinto some details.1.
Off-line, serialized data layer (JSON and/or XML)2.
In-memory data access and creation APIs3.
Data (de)serialization4.
Component instantiation (with data source and sink)5.
Workflow6.
Application layer (logging, disk access etc.
)This is to be read from top to bottom as layers that only depend on the previous layers and are fullyfunctional by themselves (without lower layers).
We?ll look at some aspects of this design proposalbelow.6.1 Serialized data definitionAs the basic layer of UIMA, we propose a non-programmatic, off-line format for UIMA data.
Indepen-dently of what that format actually is, it is the basic level at which software components can interoperate.This is what goes over the wire/air, what is written to storage.
So it is important that there is one well-defined serial format that everybody understands.We follow the OASIS UIMA standard (Ferrucci et al., 2009) in this approach.
Of course the formatchosen in the standard (XMI) is very complicated.
The argument given at the time was that XMI is itselfa standard and that it would thereby be easy to achieve integration with other tools using this standard.If any such integration has ever materialized, we don?t know about it.
It seems much more important touse a format that is just powerful enough to do what needs to be done, and which is at the same time assimple as possible.A likely candidate for such a format is JSON (ECMA, 2013).
It is a simple, human readable formatwith very widespread use and good tooling support.
There is also built-in support for many of theconstructs we are used to in UIMA.
Special support would have to be defined for annotations, and forreferences to build true graph structures (JSON just defines trees, like XML).
There is no reason not tohave an XML format as well, if that is desired, and tools that translate between the two.16.2 In-memory data accessBy this we essentially mean what is now the UIMA Common Analysis System (CAS, (G?otz and Suhre,2004)).
Apart from the changes for a dynamic type system, which might not be so large, our layeringapproach requires that a CAS should be independent of the rest of the framework, which is not the casetoday.
However, if we give up on the idea of a global static type system, we are most of the way there.If we take the approach from the last section seriously (i.e., that the serialized data format is the basisof our architecture), then the in-memory APIs need to map pretty directly to the external data definition.6.3 Data serialization and deserializationThere is not much to say about this point.
Data that was defined in terms of the in-memory API needs tobe serialized to the external format, and vice versa.
A more relaxed approach to the data model shouldmake things a lot easier on this front.1Two reviewers have independently remarked that if that is the case, we might as well stick with XMI, and have a translationto/from JSON.
At a certain level, that is correct.
However, that would require anybody who wants to deliver a compliant serviceto read/write the verbose and complicated XMI format.
That seems counterproductive as the whole idea is to make the use ofUIMA at lot easier than it is today.806.4 Type System mapping and adaptersA type system mapping specifies which input and output types an analysis engine should use.
A dynamictype system allows an analysis engine to create or extend the specified output types.
Analysis enginesare typically chained together in a pipeline, where the outputs of the previous analysis engines are theinputs of the following analysis engines.
Just by defining the type mappings, the analysis engines will beable to dynamically create the necessary types.We see the type system mapping as a useful tool for analysis engines when there is a common under-standing in the community on how to encode the information in types.
If there is a disagreement on howto model the data, the analysis engine should use an adapter to translate between the two type systemworlds.
With a dynamic type system the analysis engine can create input and output types in the flavorthe implementor prefers and adapters can create the output types the user prefers.6.5 The upper layersOur proposal de-emphasizes the upper layers of the architecture, simply because we expect that many, ifnot most, applications will run on some sort of component framework anyway.
What we have defined upto now is sufficient to run the kinds of scenarios that we described in the earlier sections.We also believe that the configuration information that currently must be defined in the UIMA XMLcomponent descriptors is much better left to specific code of the individual components (note that oneof the goals of uimaFIT is just that).
If a component has a few settings you can adjust, then let there beAPIs for those settings.
It is so much easier to do this in your programming IDE than to fiddle with somepoorly documented options in XML.Nonetheless, for analysis workflows where the entire flow is running in a single process, it is stilluseful to have standard APIs that facilitate component instantiation, definition of analysis chains orsimple workflows, and finally an application layer that can do some sort of simple document I/O.
This isstill useful for debugging, tooling and to build demo applications.7 ConclusionAs we hope to have illustrated, UIMA is faced with a number of challenges.
The UIMA frameworkdoesn?t integrate well with other framworks.
Although UIMA can be used together with other frame-works, the integration is much more complicated than it should be.
The statically typed data model istroublesome to work with.
The type system is specified ahead of time and can?t be modified during run-time.
Type systems are often defined on a per-project basis and are incompatible with each other.
Due tothese limitations the reuse of UIMA analysis components across projects is often avoided.For similar reasons the development of generic annotators is very limited.
A generic annotator can?tcreate new types during runtime and must either put a heavy burden on the end user, or return limitedresults.To overcome these challenges, we have proposed the following changes:?
Move to a dynamic data model?
Standardize on an easy-to-use data interchange format (replace XMI by JSON, for example)?
Create a stratified architecture that supports framework integrationWe have spent relatively more time talking about the perceived UIMA shortcomings than about ourproposed solution.
Why is that?
The entire approach hinges on the decision to move to a dynamic datamodel.
Once this step is taken, only then do certain architectural options become available.
It is thecrucial step, and execution of the rest of the design would not be very difficult in our opinion.Our point of departure was the fact that UIMA analysis is difficult to integrate with other frameworks.However, we have seen that many current issues are also addressed by this evolution.
We hope this paperwill spawn a discussion among UIMA users and developers both, so we can make sure UIMA staysabreast of technical developments and continues to be relevant to the text analytics and computationallinguistics communities at large.81ReferencesECMA.
2013.
The JSON data interchange format.
ECMA-404 (RFC 4627).
Technical report, ECMA Interna-tional.
http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf.David Ferrucci, Adam Lally, Karin Verspoor, and Eric Nyberg.
2009.
Unstructured Information ManagementArchitecture (UIMA) Version 1.0, OASIS Standard.
Technical report, OASIS.Thilo G?otz and Oliver Suhre.
2004.
Design and implementation of the UIMA Common Analysis System.
IBMSyst.
J., 43(3):476?489, July.Hadoop.
2014.
Apache Hadoop.
http://hadoop.apache.org/.J2EE.
2014.
Java Enterprise Edition.
http://www.oracle.com/technetwork/java/javaee/overview/.Philip Ogren and Steven Bethard.
2009.
Building test suites for UIMA components.
In Proceedings of theWorkshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing (SETQA-NLP 2009), pages 1?4, Boulder, Colorado, June.
Association for Computational Linguistics.Spring.
2014.
Spring Framework.
http://projects.spring.io/spring-framework/.Storm.
2014.
Apache Storm.
http://storm.incubator.apache.org/.UIMA.
2014.
Apache UIMA.
http://uima.apache.org/.82
