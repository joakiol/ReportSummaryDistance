Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604?611,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsError Detection for Statistical MachineTranslation Using Linguistic FeaturesDeyi Xiong, Min Zhang, Haizhou LiHuman Language TechnologyInstitute for Infocomm Research1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.
{dyxiong, mzhang, hli}@i2r.a-star.edu.sgAbstractAutomatic error detection is desired inthe post-processing to improve machinetranslation quality.
The previous work islargely based on confidence estimation us-ing system-based features, such as wordposterior probabilities calculated from N -best lists or word lattices.
We propose toincorporate two groups of linguistic fea-tures, which convey information from out-side machine translation systems, into er-ror detection: lexical and syntactic fea-tures.
We use a maximum entropy clas-sifier to predict translation errors by inte-grating word posterior probability featureand linguistic features.
The experimen-tal results show that 1) linguistic featuresalone outperform word posterior probabil-ity based confidence estimation in errordetection; and 2) linguistic features canfurther provide complementary informa-tion when combined with word confidencescores, which collectively reduce the clas-sification error rate by 18.52% and im-prove the F measure by 16.37%.1 IntroductionTranslation hypotheses generated by a statisticalmachine translation (SMT) system always containboth correct parts (e.g.
words, n-grams, phrasesmatched with reference translations) and incor-rect parts.
Automatically distinguishing incorrectparts from correct parts is therefore very desir-able not only for post-editing and interactive ma-chine translation (Ueffing and Ney, 2007) but alsofor SMT itself: either by rescoring hypotheses inthe N -best list using the probability of correct-ness calculated for each hypothesis (Zens and Ney,2006) or by generating new hypotheses using N -best lists from one SMT system or multiple sys-tems (Akibay et al, 2004; Jayaraman and Lavie,2005).In this paper we restrict the ?parts?
to words.That is, we detect errors at the word level for SMT.A common approach to SMT error detection at theword level is calculating the confidence at which aword is correct.
The majority of word confidenceestimation methods follows three steps:1) Calculate features that express the correct-ness of words either based on SMT model(e.g.
translation/language model) or based onSMT system output (e.g.
N -best lists, wordlattices) (Blatz et al, 2003; Ueffing and Ney,2007).2) Combine these features together with a clas-sification model such as multi-layer percep-tron (Blatz et al, 2003), Naive Bayes (Blatzet al, 2003; Sanchis et al, 2007), or log-linear model (Ueffing and Ney, 2007).3) Divide words into two groups (correct trans-lations and errors) by using a classificationthreshold optimized on a development set.Sometimes the step 2) is not necessary if only oneeffective feature is used (Ueffing and Ney, 2007);and sometimes the step 2) and 3) can be mergedinto a single step if we directly output predictingresults from binary classifiers instead of makingthresholding decision.Various features from different SMT modelsand system outputs are investigated (Blatz et al,2003; Ueffing and Ney, 2007; Sanchis et al, 2007;Raybaud et al, 2009).
Experimental results showthat they are useful for error detection.
However,it is not adequate to just use these features as dis-cussed in (Shi and Zhou, 2005) because the infor-mation that they carry is either from the inner com-ponents of SMT systems or from system outputs.To some extent, it has already been considered bySMT systems.
Hence finding external information604sources from outside SMT systems is desired forerror detection.Linguistic knowledge is exactly such a goodchoice as an external information source.
It has al-ready been proven effective in error detection forspeech recognition (Shi and Zhou, 2005).
How-ever, it is not widely used in SMT error detection.The reason is probably that people have yet to findeffective linguistic features that outperform non-linguistic features such as word posterior proba-bility features (Blatz et al, 2003; Raybaud et al,2009).
In this paper, we would like to show aneffective use of linguistic features in SMT errordetection.We integrate two sets of linguistic features intoa maximum entropy (MaxEnt) model and developaMaxEnt-based binary classifier to predict the cat-egory (correct or incorrect) for each word in agenerated target sentence.
Our experimental re-sults show that linguistic features substantially im-prove error detection and even outperform wordposterior probability features.
Further, they canproduce additional improvements when combinedwith word posterior probability features.The rest of the paper is organized as follows.
InSection 2, we review the previous work on word-level confidence estimation which is used for errordetection.
In Section 3, we introduce our linguisticfeatures as well as the word posterior probabilityfeature.
In Section 4, we elaborate our MaxEnt-based error detection model which combine lin-guistic features and word posterior probability fea-ture together.
In Section 5, we describe the SMTsystem which we use to generate translation hy-potheses.
We report our experimental results inSection 6 and conclude in Section 7.2 Related WorkIn this section, we present an overview of confi-dence estimation (CE) for machine translation atthe word level.
As we are only interested in errordetection, we focus on work that uses confidenceestimation approaches to detect translation errors.Of course, confidence estimation is not limited tothe application of error detection, it can also beused in other scenarios, such as translation predic-tion in an interactive environment (Grandrabur andFoster, 2003) .In a JHU workshop, Blatz et al (2003) investi-gate using neural networks and a naive Bayes clas-sifier to combine various confidence features forconfidence estimation at the word level as well asat the sentence level.
The features they use forword level CE include word posterior probabil-ities estimated from N -best lists, features basedon SMT models, semantic features extracted fromWordNet as well as simple syntactic features, i.e.parentheses and quotation mark check.
Among allthese features, the word posterior probability is themost effective feature, which is much better thanlinguistic features such as semantic features, ac-cording to their final results.Ueffing and Ney (2007) exhaustively explorevarious word-level confidence measures to labeleach word in a generated translation hypothe-sis as correct or incorrect.
All their measuresare based on word posterior probabilities, whichare estimated from 1) system output, such asword lattices or N -best lists and 2) word orphrase translation table.
Their experimental re-sults show that word posterior probabilities di-rectly estimated from phrase translation table arebetter than those from system output except for theChinese-English language pair.Sanchis et al (2007) adopt a smoothed naiveBayes model to combine different word posteriorprobability based confidence features which areestimated from N -best lists, similar to (Ueffingand Ney, 2007).Raybaud et al (2009) study several confi-dence features based on mutual information be-tween words and n-gram and backward n-gramlanguage model for word-level and sentence-levelCE.
They also explore linguistic features using in-formation from syntactic category, tense, genderand so on.
Unfortunately, such linguistic featuresneither improve performance at the word level norat the sentence level.Our work departs from the previous work in twomajor respects.?
We exploit various linguistic features andshow that they are able to produce larger im-provements than widely used system-relatedfeatures such as word posterior probabilities.This is in contrast to some previous work.
Yetanother advantage of using linguistic featuresis that they are system-independent, whichtherefore can be used across different sys-tems.?
We treat error detection as a complete bi-nary classification problem.
Hence we di-605rectly output prediction results from our dis-criminatively trained classifier without opti-mizing a classification threshold on a distinctdevelopment set beforehand.1 Most previousapproaches make decisions based on a pre-tuned classification threshold ?
as followsclass ={correct, ?
(correct, ?)
> ?incorrect, otherwisewhere ?
is a classifier or a confidence mea-sure and ?
is the parameter set of ?.
The per-formance of these approaches is strongly de-pendent on the classification threshold.3 FeaturesWe explore two sets of linguistic features for eachword in a machine generated translation hypoth-esis.
The first set of linguistic features are sim-ple lexical features.
The second set of linguisticfeatures are syntactic features which are extractedfrom link grammar parse.
To compare with thepreviously widely used features, we also investi-gate features based on word posterior probabili-ties.3.1 Lexical FeaturesWe use the following lexical features.?
wd: word itself?
pos: part-of-speech tag from a tagger trainedon WSJ corpus.
2For each word, we look at previous nwords/tags and next n words/tags.
They togetherform a word/tag sequence pattern.
The basic ideaof using these features is that words in rare pat-terns are more likely to be incorrect than wordsin frequently occurring patterns.
To some extent,these two features have similar function to a tar-get language model or pos-based target languagemodel.3.2 Syntactic FeaturesHigh-level linguistic knowledge such as syntac-tic information about a word is a very natural andpromising indicator to decide whether this word issyntactically correct or not.
Words occurring in an1This does not mean we do not need a development set.We do validate our feature selection and other experimentalsettings on the development set.2Available via http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/ungrammatical part of a target sentence are proneto be incorrect.
The challenge of using syntac-tic knowledge for error detection is that machine-generated hypotheses are rarely fully grammati-cal.
They are mixed with grammatical and un-grammatical parts, which hence are not friendlyto traditional parsers trained on grammatical sen-tences because ungrammatical parts of a machine-generated sentence could lead to a parsing failure.To overcome this challenge, we select the LinkGrammar (LG) parser 3 as our syntactic parser togenerate syntactic features.
The LG parser pro-duces a set of labeled links which connect pairs ofwords with a link grammar (Sleator and Temper-ley, 1993).The main reason why we choose the LG parseris that it provides a robustness feature: null-linkscheme.
The null-link scheme allows the parser toparse a sentence even when the parser can not fullyinterpret the entire sentence (e.g.
including un-grammatical parts).
When the parser fail to parsethe entire sentence, it ignores one word each timeuntil it finds linkages for remaining words.
Afterparsing, those ignored words are not connected toany other words.
We call them null-linked words.Our hypothesis is that null-linked words areprone to be syntactically incorrect.
We hencestraightforwardly define a syntactic feature for aword w according to its links as followslink(w) ={yes, w has linksno, otherwiseIn Figure 1 we show an example of a generatedtranslation hypothesis with its link parse.
Herelinks are denoted with dotted lines which are an-notated with link types (e.g., Jp, Op).
Bracketedwords, namely ?,?
and ?including?, are null-linkedwords.3.3 Word Posterior Probability FeaturesOur word posterior probability is calculated onN -best list, which is first proposed by (Ueffing et al,2003) and widely used in (Blatz et al, 2003; Ueff-ing and Ney, 2007; Sanchis et al, 2007).Given a source sentence f , let {en}N1 be theN -best list generated by an SMT system, and let ein isthe i-th word in en.
The major work of calculatingword posterior probabilities is to find the Leven-shtein alignment (Levenshtein, 1966) between thebest hypothesis e1 and its competing hypothesis3Available at http://www.link.cs.cmu.edu/link/606Figure 1: An example of Link Grammar parsing results.en in the N -best list {en}N1 .
We denote the align-ment between them as ?
(e1, en).
The word in thehypothesis en which ei1 is Levenshtein aligned tois denoted as ?i(e1, en).The word posterior probability of ei1 is then cal-culated by summing up the probabilities over allhypotheses containing ei1 in a position which isLevenshtein aligned to ei1.pwpp(ei1) =?en: ?i(e1,en)=ei1p(en)?N1 p(en)To use the word posterior probability in our er-ror detection model, we need to make it discrete.We introduce a feature for a word w based on itsword posterior probability as followsdwpp(w) = ?
?log(pwpp(w))/df?where df is the discrete factor which can be set to1, 0.1, 0.01 and so on.
??
??
is a rounding oper-ator which takes the largest integer that does notexceed ?log(pwpp(w))/df .
We optimize the dis-crete factor on our development set and find theoptimal value is 1.
Therefore a feature ?dwpp =2?
represents that the logarithm of the word poste-rior probability is between -3 and -2;4 Error Detection with a MaximumEntropy ModelAs mentioned before, we consider error detec-tion as a binary classification task.
To formal-ize this task, we use a feature vector ?
to rep-resent a word w in question, and a binary vari-able c to indicate whether this word is correct ornot.
In the feature vector, we look at 2 wordsbefore and 2 words after the current word posi-tion (w?2, w?1, w, w1, w2).
We collect features{wd, pos, link, dwpp} for each word among thesewords and combine them into the feature vector?
for w. As such, we want the feature vector tocapture the contextual environment, e.g., pos se-quence pattern, syntactic pattern, where the wordw occurs.For classification, we employ the maximumentropy model (Berger et al, 1996) to predictwhether a word w is correct or incorrect given itsfeature vector ?.p(c|?)
= exp(?i ?ifi(c, ?))?c?
exp(?i ?ifi(c?, ?
))where fi is a binary model feature defined on cand the feature vector ?.
?i is the weight of fi.Table 1 shows some examples of our binary modelfeatures.In order to learn the model feature weights ?
forprobability estimation, we need a training set ofm samples {?i, ci}m1 .
The challenge of collect-ing training instances is that the correctness of aword in a generated translation hypothesis is notintuitively clear (Ueffing and Ney, 2007).
We willdescribe the method to determine the correctnessof a word in Section 6.1, which is broadly adoptedin previous work.We tune our model feature weights using anoff-the-shelf MaxEnt toolkit (Zhang, 2004).
Toavoid overfitting, we optimize the Gaussian prioron the development set.
During test, if the proba-bility p(correct|?)
is larger than p(incorrect|?
)according the trained MaxEnt model, the word islabeled as correct otherwise incorrect.5 SMT SystemTo obtain machine-generated translation hypothe-ses for our error detection, we use a state-of-the-artphrase-based machine translation system MOSES(Koehn et al, 2003; Koehn et al, 2007).
Thetranslation task is on the official NIST Chinese-to-English evaluation data.
The training data con-sists of more than 4 million pairs of sentences (in-cluding 101.93MChinese words and 112.78M En-glish words) from LDC distributed corpora.
Table2 shows the corpora that we use for the translationtask.We build a four-gram language model using theSRILM toolkit (Stolcke, 2002), which is trained607Feature Examplewd f(c, ?)
={1, ?.w.wd = ?.
?, c = correct0, otherwisepos f(c, ?)
={1, ?.w2.pos = ?NN?, c = incorrect0, otherwiselink f(c, ?)
={1, ?.w.link = no, c = incorrect0, otherwisedwpp f(c, ?)
={1, ?.w?2.dwpp = 2, c = correct0, otherwiseTable 1: Examples of model features.LDC ID DescriptionLDC2004E12 United NationsLDC2004T08 Hong Kong NewsLDC2005T10 Sinorama MagazineLDC2003E14 FBISLDC2002E18 Xinhua News V1 betaLDC2005T06 Chinese News TranslationLDC2003E07 Chinese TreebankLDC2004T07 Multiple Translation ChineseTable 2: Training corpora for the translation task.on Xinhua section of the English Gigaword cor-pus (181.1M words).
For minimum error rate tun-ing (Och, 2003), we use NIST MT-02 as the de-velopment set for the translation task.
In orderto calculate word posterior probabilities, we gen-erate 10,000 best lists for NIST MT-02/03/05 re-spectively.
The performance, in terms of BLEU(Papineni et al, 2002) score, is shown in Table 4.6 ExperimentsWe conducted our experiments at several levels.Starting with MaxEnt models with single linguis-tic feature or word posterior probability based fea-ture, we incorporated additional features incre-mentally by combining features together.
In do-ing so, we would like the experimental results notonly to display the effectiveness of linguistic fea-tures for error detection but also to identify the ad-ditional contribution of each feature to the task.6.1 Data CorpusFor the error detection task, we use the best trans-lation hypotheses of NIST MT-02/05/03 generatedby MOSES as our training, development, and testcorpus respectively.
The statistics about these cor-pora is shown in Table 3.
Each translation hypoth-esis has four reference translations.Corpus Sentences WordsTraining MT-02 878 24,225Development MT-05 1082 31,321Test MT-03 919 25,619Table 3: Corpus statistics (number of sentencesand words) for the error detection task.To obtain the linkage information, we run theLG parser on all translation hypotheses.
We findthat the LG parser can not fully parse 560 sen-tences (63.8%) in the training set (MT-02), 731sentences (67.6%) in the development set (MT-05)and 660 sentences (71.8%) in the test set (MT-03).For these sentences, the LG parser will use the thenull-link scheme to generate null-linked words.To determine the true class of a word in a gen-erated translation hypothesis, we follow (Blatz etal., 2003) to use the word error rate (WER).
Wetag a word as correct if it is aligned to itself inthe Levenshtein alignment between the hypothesisand the nearest reference translation that has min-imum edit distance to the hypothesis among fourreference translations.
Figure 2 shows the Lev-enshtein alignment between a machine-generatedhypothesis and its nearest reference translation.The ?Class?
row shows the label of each word ac-cording to the alignment, where ?c?
and ?i?
repre-sent correct and incorrect respectively.There are several other metrics to tag singlewords in a translation hypothesis as correct or in-correct, such as PER where a word is tagged ascorrect if it occurs in one of reference translationswith the same number of occurrences, Setwhich isa less strict variant of PER, ignoring the number ofoccurrences per word.
In Figure 2, the two words?last year?
in the hypothesis will be tagged as cor-rect if we use the PER or Set metric since they donot consider the occurring positions of words.
Our608ChinaUnicomnetprofitroseup38%lastyearChinaUnicomnetprofitroseup38%lastyearHypothesisReferenceChina/cUnicom/cnet/cprofit/crose/cup/c38%/clast/iyear/iClassFigure 2: Tagging a word as correct/incorrect according to the Levenshtein alignment.Corpus BLEU (%) RCW (%)MT-02 33.24 47.76MT-05 32.03 47.85MT-03 32.86 47.57Table 4: Case-insensitive BLEU score and ratioof correct words (RCW) on the training, develop-ment and test corpus.metric corresponds to the m-WER used in (Ueff-ing and Ney, 2007), which is stricter than PER andSet.
It is also stricter than normal WER metricwhich compares each hypothesis to all references,rather than the nearest reference.Table 4 shows the case-insensitive BLEU scoreand the percentage of words that are labeled as cor-rect according to the method described above onthe training, development and test corpus.6.2 Evaluation MetricsTo evaluate the overall performance of the errordetection, we use the commonly used metric, clas-sification error rate (CER) to evaluate our classi-fiers.
CER is defined as the percentage of wordsthat are wrongly tagged as followsCER = # of wrongly tagged wordsTotal # of wordsThe baseline CER is determined by assumingthe most frequent class for all words.
Since the ra-tio of correct words in both the development andtest set is lower than 50%, the most frequent classis ?incorrect?.
Hence the baseline CER in our ex-periments is equal to the ratio of correct words asthese words are wrongly tagged as incorrect.We also use precision and recall on errors toevaluate the performance of error detection.
Letng be the number of words of which the true classis incorrect, nt be the number of words which aretagged as incorrect by classifiers, and nm be thenumber of words tagged as incorrect that are in-deed translation errors.
The precision Pre is thepercentage of words correctly tagged as transla-tion errors.Pre = nmntThe recall Rec is the proportion of actual transla-tion errors that are found by classifiers.Rec = nmngF measure, the trade-off between precision and re-call, is also used.F = 2 ?
Pre?RecPre+Rec6.3 Experimental ResultsTable 5 shows the performance of our experimentson the error detection task.
To compare with pre-vious work using word posterior probabilities forconfidence estimation, we carried out experimentsusing wpp estimated from N -best lists with theclassification threshold ?
, which was optimized onour development set to minimize CER.
A relativeimprovement of 9.27% is achieved over the base-line CER, which reconfirms the effectiveness ofword posterior probabilities for error detection.We conducted three groups of experiments us-ing the MaxEnt based error detection model withvarious feature combinations.?
The first group of experiments uses singlefeature, such as dwpp, pos.
We find themost effective feature is pos, which achievesa 16.12% relative improvement over the base-line CER and 7.55% relative improvementover the CER of word posterior probabil-ity thresholding.
Using discrete word pos-terior probabilities as features in the Max-Ent based error detection model is marginallybetter than word posterior probability thresh-olding in terms of CER, but obtains a 13.79%relative improvement in F measure.
The syn-tactic feature link also improves the error de-tection in terms of CER and particularly re-call.609Combination Features CER (%) Pre (%) Rec (%) F (%)Baseline - 47.57 - - -Thresholding wpp - 43.16 58.98 58.07 58.52MaxEnt (dwpp) 44 43.07 56.12 81.86 66.59MaxEnt (wd) 19,164 41.57 58.25 73.11 64.84MaxEnt (pos) 199 39.90 58.88 79.23 67.55MaxEnt (link) 19 44.31 54.72 89.72 67.98MaxEnt (wd+ pos) 19,363 39.43 59.36 78.60 67.64MaxEnt (wd+ pos+ link) 19,382 39.79 58.74 80.97 68.08MaxEnt (dwpp+ wd) 19,208 41.04 57.18 83.75 67.96MaxEnt (dwpp+ wd+ pos) 19,407 38.88 59.87 78.38 67.88MaxEnt (dwpp+ wd+ pos+ link) 19,426 38.76 59.89 78.94 68.10Table 5: Performance of the error detection task.?
The second group of experiments concernswith the combination of linguistic featureswithout word posterior probability feature.The combination of lexical features improvesboth CER and precision over single lexicalfeature (wd, pos).
The addition of syntacticfeature link marginally undermines CER butimproves recall by a lot.?
The last group of experiments concerns aboutthe additional contribution of linguistic fea-tures to error detection with word posteriorprobability.
We added linguistic features in-crementally into the feature pool.
The bestperformance was achieved by using all fea-tures, which has a relative of improvement of18.52% over the baseline CER.The first two groups of experiments show thatlinguistic features, individually (except for link)or by combination, are able to produce much betterperformance than word posterior probability fea-tures in both CER and F measure.
The best com-bination of linguistic features achieves a relativeimprovement of 8.64% and 15.58% in CER andF measure respectively over word posterior prob-ability thresholding.The Table 5 also reveals how linguistic fea-tures improve error detection.
The lexical features(pos, wd) improve precision when they are used.This suggests that lexical features can help the sys-tem find errors more accurately.
Syntactic features(link), on the other hand, improve recall wheneverthey are used, which indicates that they can helpthe system find more errors.We also show the number of features in eachcombination in Table 5.
Except for the wd feature,0200400600800100038.638.839.039.239.439.639.840.040.240.440.6CER(%)Number of Training SentencesFigure 3: CER vs. the number of training sen-tences.the pos has the largest number of features, 199,which is a small set of features.
This suggests thatour error detection model can be learned from arather small training set.Figure 3 shows CERs for the feature combina-tion MaxEnt (dwpp + wd + pos + link) whenthe number of training sentences is enlarged incre-mentally.
CERs drop significantly when the num-ber of training sentences is increased from 100 to500.
After 500 sentences are used, CERs changemarginally and tend to converge.7 Conclusions and Future WorkIn this paper, we have presented a maximum en-tropy based approach to automatically detect er-rors in translation hypotheses generated by SMT610systems.
We incorporate two sets of linguisticfeatures together with word posterior probabilitybased features into error detection.Our experiments validate that linguistic featuresare very useful for error detection: 1) they bythemselves achieve a higher improvement in termsof both CER and F measure than word posteriorprobability features; 2) the performance is furtherimproved when they are combined with word pos-terior probability features.The extracted linguistic features are quite com-pact, which can be learned from a small train-ing set.
Furthermore, The learned linguistic fea-tures are system-independent.
Therefore our ap-proach can be used for other machine translationsystems, such as rule-based or example-based sys-tem, which generally do not produce N -best lists.Future work in this direction involve detect-ing particular error types such as incorrect po-sitions, inappropriate/unnecessary words (Elliott,2006) and automatically correcting errors.ReferencesYasuhiro Akibay, Eiichiro Sumitay, Hiromi Nakaiway,Seiichi Yamamotoy, and Hiroshi G. Okunoz.
2004.Using a Mixture of N-best Lists from Multiple MTSystems in Rank-sum-based Confidence Measurefor MT Outputs.
In Proceedings of COLING.Adam L. Berger, Stephen A. Della Pietra andVincentJ.
Della Pietra.
1996.
A Maximum Entropy Ap-proach to Natural Language Processing.
Computa-tional Linguistics, 22(1): 39-71.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, Nicola Ueffing.
2003.
Confidence estima-tion for machine translation.
final report, jhu/clspsummer workshop.Debra Elliott.
2006 Corpus-based Machine Transla-tion Evaluation via Automated Error Detection inOutput Texts.
Phd Thesis, University of Leeds.Simona Gandrabur and George Foster.
2003.
Confi-dence Estimation for Translation Prediction.
In Pro-ceedings of HLT-NAACL.S.
Jayaraman and A. Lavie.
2005.
Multi-engine Ma-chine Translation Guided by Explicit Word Match-ing.
In Proceedings of EAMT.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Pro-ceedings of HLT-NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstrantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL, Demonstration Session.V.
I. Levenshtein.
1966.
Binary Codes Capable of Cor-recting Deletions, Insertions and Reversals.
SovietPhysics Doklady, Feb.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof ACL 2003.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
2002.
BLEU: aMethod for AutomaticallyEvaluation of Machine Translation.
In Proceedingsof ACL 2002.Sylvain Raybaud, Caroline Lavecchia, David Langlois,Kamel Sma??li.
2009.
Word- and Sentence-levelConfidence Measures for Machine Translation.
InProceedings of EAMT 2009.Alberto Sanchis, Alfons Juan and Enrique Vidal.
2007.Estimation of Confidence Measures for MachineTranslation.
In Procedings of Machine TranslationSummit XI.Daniel Sleator and Davy Temperley.
1993.
Parsing En-glish with a Link Grammar.
In Proceedings of ThirdInternational Workshop on Parsing Technologies.Yongmei Shi and Lina Zhou.
2005.
Error Detec-tion Using Linguistic Features.
In Proceedings ofHLT/EMNLP 2005.Andreas Stolcke.
2002.
SRILM - an Extensible Lan-guage Modeling Toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing,volume 2, pages 901-904.Nicola Ueffing, Klaus Macherey, and Hermann Ney.2003.
Confidence Measures for Statistical MachineTranslation.
In Proceedings.
of MT Summit IX.Nicola Ueffing and Hermann Ney.
2007.
Word-Level Confidence Estimation for Machine Transla-tion.
Computational Linguistics, 33(1):9-40.Richard Zens and Hermann Ney.
2006.
N-gram Pos-terior Probabilities for Statistical Machine Transla-tion.
In HLT/NAACL: Proceedings of the Workshopon Statistical Machine Translation.Le Zhang.
2004.
Maximum Entropy Model-ing Tooklkit for Python and C++.
Available athttp://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.611
