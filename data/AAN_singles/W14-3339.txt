Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313?321,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsReferential Translation Machines for Predicting Translation QualityErgun Bic?iciCentre for Next Generation LocalisationSchool of ComputingDublin City University, Dublin, Ireland.ergun.bicici@computing.dcu.ieAndy WayCentre for Next Generation LocalisationSchool of ComputingDublin City University, Dublin, Ireland.away@computing.dcu.ieAbstractWe use referential translation machines(RTM) for quality estimation of translationoutputs.
RTMs are a computational modelfor identifying the translation acts betweenany two data sets with respect to interpre-tants selected in the same domain, whichare effective when making monolingualand bilingual similarity judgments.
RTMsachieve top performance in automatic, ac-curate, and language independent predic-tion of sentence-level and word-level sta-tistical machine translation (SMT) qual-ity.
RTMs remove the need to access anySMT system specific information or priorknowledge of the training data or modelsused when generating the translations andachieve the top performance in WMT13quality estimation task (QET13).
We im-prove our RTM models with the ParallelFDA5 instance selection model, with ad-ditional features for predicting the trans-lation performance, and with improvedlearning models.
We develop RTM mod-els for each WMT14 QET (QET14) sub-task, obtain improvements over QET13 re-sults, and rank 1st in all of the tasks andsubtasks of QET14.1 IntroductionWe use referential translation machines (RTM) forquality estimation of translation outputs, which isa computational model for identifying the acts oftranslation for translating between any given twodata sets with respect to a reference corpus se-lected in the same domain.
RTMs reduce our de-pendence on any task dependent resource.
Predic-tion of translation quality is important because theexpected translation performance can help in esti-mating the effort required for correcting the trans-lations during post-editing by human translators.Bicici et al.
(2013) develop the Machine Trans-lation Performance Predictor (MTPP), a state-of-the-art, language independent, and SMT systemextrinsic machine translation performance predic-tor, which can predict translation quality by look-ing at the test source sentences and becomes the2nd overall after also looking at the translationoutputs as well in QET12 (Callison-Burch et al.,2012).
RTMs achieve the top performance inQET13 (Bojar et al., 2013), ranking 1st or 2nd inall of the subtasks.
RTMs rank 1st in all of thetasks and subtasks of QET14 (Bojar et al., 2014).Referential translation models (Section 2)present an accurate and language independent so-lution for predicting the performance of naturallanguage tasks such as the quality estimation oftranslation.
We improve our RTM models (Bic?ici,2013) by:?
using a parameterized, fast implementationof FDA, FDA5, and our Parallel FDA5 in-stance selection model (Bic?ici et al., 2014),?
better modeling of the language in whichsimilarity judgments are made with improvedoptimization and selection of the LM data,?
increased feature set for also modeling thestructural properties of sentences,?
extended learning models.2 Referential Translation Machine(RTM)Referential translation machines provide a compu-tational model for quality and semantic similarityjudgments in monolingual and bilingual settingsusing retrieval of relevant training data (Bic?ici,2011; Bic?ici and Yuret, 2014) as interpretants forreaching shared semantics (Bic?ici, 2008).
RTMsachieve top performance when predicting the qual-ity of translations in QET14 and QET13 (Bic?ici,3132013), top performance when predicting mono-lingual cross-level semantic similarity (Jurgenset al., 2014), good performance when evaluat-ing the semantic relatedness of sentences andtheir entailment (Marelli et al., 2014), and alanguage independent solution and good perfor-mance when judging the semantic similarity ofsentences (Agirre et al., 2014; Bic?ici and Way,2014).RTM is a computational model for identifyingthe acts of translation for translating between anygiven two data sets with respect to a referencecorpus selected in the same domain.
An RTMmodel is based on the selection of interpretants,data close to both the training set and the test set,which allow shared semantics by providing con-text for similarity judgments.
In semiotics, an in-terpretant I interprets the signs used to refer to thereal objects (Bic?ici, 2008).
Each RTM model isa data translation model between the instances inthe training set and the test set.
We use the ParallelFDA5 (Feature Decay Algorithms) instance selec-tion model for selecting the interpretants (Bic?iciet al., 2014; Bic?ici and Yuret, 2014) this year,which allows efficient parameterization, optimiza-tion, and implementation of FDA, and build anMTPP model (Section 2.1).
We view that acts oftranslation are ubiquitously used during commu-nication:Every act of communication is an act oftranslation (Bliss, 2012).Given a training set train, a test set test, andsome corpus C, preferably in the same domain asthe training and test sets, the RTM steps are:1.
FDA5(train,test, C)?
I2.
MTPP(I,train)?
Ftrain3.
MTPP(I,test)?
Ftest4.
learn(M,Ftrain)?M5.
predict(M,Ftest)?
q?Step 1 selects the interpretants, I, relevant to boththe training and test data.
Steps 2 and 3 use Ito map train and test to a new space wheresimilarities between translation acts can be derivedmore easily.
Step 4 trains a learning modelM overthe training features, Ftrain, and Step 5 obtainsthe predictions.
RTM relies on the representative-ness of I as a medium for building data translationmodels between train and test.Our encouraging results in QET provides agreater understanding of the acts of translation weubiquitously use and how they can be used to pre-dict the performance of translation and judging thesemantic similarity between text.
RTM and MTPPmodels are not data or language specific and theirmodeling power and good performance are appli-cable in different domains and tasks.2.1 The Machine Translation PerformancePredictor (MTPP)MTPP (Bic?ici et al., 2013) is a state-of-the-art andtop performing machine translation performancepredictor, which uses machine learning modelsover features measuring how well the test setmatches the training set to predict the quality ofa translation without using a reference translation.2.2 MTPP Features for Translation ActsMTPP measures the coverage of individual testsentence features found in the training set andderives indicators of the closeness of test sen-tences to the available training data, the difficultyof translating the sentence, and the presence ofacts of translation for data transformation.
Fea-ture functions use statistics involving the trainingset and the test sentences to determine their close-ness.
Since they are language independent, MTPPallows quality estimation to be performed extrin-sically.
MTPP uses n-gram features defined overtext or common cover link (CCL) (Seginer, 2007)structures as the basic units of information overwhich similarity calculations are made.
Unsuper-vised parsing with CCL extracts links from basewords to head words, representing the grammati-cal information instantiated in the training and testdata.We extend the MTPP model we used lastyear (Bic?ici, 2013) in its learning module and thefeatures included.
Categories for the features (Sfor source, T for target) used are listed belowwhere the number of features are given in bracketsfor S and T, {#S, #T}, and the detailed descriptionsfor some of the features are presented in (Bic?ici etal., 2013).
The number of features for each taskdiffers since we perform an initial feature selectionstep on the tree structural features (Section 2.3).The number of features are in the range 337?437.?
Coverage {56, 54}: Measures the degree towhich the test features are found in the train-ing set for both S ({56}) and T ({54}).?
Perplexity {45, 45}: Measures the fluency ofthe sentences according to language models314(LM).
We use both forward ({30}) and back-ward ({15}) LM features for S and T.?
TreeF {0, 10-110}: 10 base features and upto 100 selected features of T among parse treestructures (Section 2.3).?
Retrieval Closeness {16, 12}: Measures thedegree to which sentences close to the test setare found in the selected training set, I, usingFDA (Bic?ici and Yuret, 2011a) and BLEU,F1(Bic?ici, 2011), dice, and tf-idf cosine sim-ilarity metrics.?
IBM2 Alignment Features {0, 22}: Calcu-lates the sum of the entropy of the dis-tribution of alignment probabilities for S(?s?S?p log p for p = p(t|s) where s andt are tokens) and T, their average for S andT, the number of entries with p ?
0.2 andp ?
0.01, the entropy of the word align-ment between S and T and its average, andword alignment log probability and its valuein terms of bits per word.
We also com-pute word alignment percentage as in (Ca-margo de Souza et al., 2013) and potentialBLEU, F1, WER, PER scores for S and T.?
IBM1 Translation Probability {4, 12}: Cal-culates the translation probability of testsentences using the selected training set,I (Brown et al., 1993).?
Feature Vector Similarity {8, 8}: Calculatessimilarities between vector representations.?
Entropy {2, 8}: Calculates the distributionalsimilarity of test sentences to the training setover top N retrieved sentences (Bic?ici et al.,2013).?
Length {6, 3}: Calculates the number ofwords and characters for S and T and theiraverage token lengths and their ratios.?
Diversity {3, 3}: Measures the diversity ofco-occurring features in the training set.?
Synthetic Translation Performance {3, 3}:Calculates translation scores achievable ac-cording to the n-gram coverage.?
Character n-grams {5}: Calculates cosinebetween character n-grams (for n=2,3,4,5,6)obtained for S and T (B?ar et al., 2012).?
Minimum Bayes Retrieval Risk {0, 4}: Cal-culates the translation probability for thetranslation having the minimum Bayes riskamong the retrieved training instances.?
Sentence Translation Performance {0, 3}:Calculates translation scores obtained ac-cording to q(T,R) using BLEU (Papineniet al., 2002), NIST (Doddington, 2002), orF1(Bic?ici and Yuret, 2011b) for q.?
LIX {1, 1}: Calculates the LIX readabilityscore (Wikipedia, 2013; Bj?ornsson, 1968) forS and T.1For Task 1.1, we have additionally used com-parative BLEU, NIST, and F1scores as additionalfeatures, which are obtained by comparing thetranslations with each other and averaging the re-sult (Bic?ici, 2011).2.3 Bracketing Tree Structural FeaturesWe use the parse tree outputs obtained by CCLto derive features based on the bracketing struc-ture.
We derive 5 statistics based on the geometricproperties of the parse trees: number of bracketsused (numB), depth (depthB), average depth (avgdepthB), number of brackets on the right branchesover the number of brackets on the left (R/L)2, av-erage right to left branching over all internal treenodes (avg R/L).
The ratio of the number of rightto left branches shows the degree to which the sen-tence is right branching or not.
Additionally, wecapture the different types of branching presentin a given parse tree identified by the number ofnodes in each of its children.Table 1 depicts the parsing output obtained byCCL for the following sentence from WSJ233:Many fund managers argue that now ?s the timeto buy .We use Tregex (Levy and Andrew, 2006) for vi-sualizing the output parse trees presented on theleft.
The bracketing structure statistics and fea-tures are given on the right hand side.
The rootnode of each tree structural feature represents thenumber of times that feature is present in the pars-ing output of a document.3 RTM in the Quality Estimation TaskWe participate in all of the four challenges of thequality estimation task (QET) (Bojar et al., 2014),which include English to Spanish (en-es), Span-ish to English (es-en), English to German (en-de), and German to English (de-en) translation di-rections.
There are two main categories of chal-lenges: sentence-level prediction (Task 1.
*) and1LIX=AB+ C100A, where A is the number of words, C iswords longer than 6 characters, B is words that start or endwith any of ?.
?, ?
:?, ?!
?, ???
similar to (Hagstr?om, 2012).2For nodes with uneven number of children, the nodes inthe odd child contribute to the right branches.3Wall Street Journal (WSJ) corpus section 23, distributedwith Penn Treebank version 3 (Marcus et al., 1993).315CCLnumB depthB avg depthB R/L avg R/L24.0 9.0 0.375 2.1429 3.40121 111 1311 211 812 1013 113 415 117 15Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).word-level prediction (Task 2).
Task 1.1 is aboutpredicting post-editing effort (PEE), Task 1.2 isabout predicting HTER (human-targeted transla-tion edit rate) (Snover et al., 2006) scores of trans-lations, Task 1.3 is about predicting post-editingtime (PET), and Task 2 is about binary, ternary, ormulti-class classification of word-level quality.For each task, we develop individual RTM mod-els using the parallel corpora and the LM corporadistributed by the translation task (WMT14) (Bo-jar et al., 2014) and the LM corpora provided byLDC for English (Parker et al., 2011) and Span-ish (?Angelo Mendonc?a, 2011)4.
The parallel cor-pora contain 4.5M sentences for de-en with 110Mwords for de and 116M words for en and 15.1Msentences for en-es with 412M words for en and462M words for es.
We do not use any resourcesprovided by QET including data, software, orbaseline features.
Instance selection for the train-ing set and the language model (LM) corpus ishandled by parallel FDA5 (Bic?ici et al., 2014),whose parameters are optimized for each transla-tion task.
LM are trained using SRILM (Stolcke,2002).
We tokenize and true-case all of the cor-pora.
The true-caser is trained on all of the avail-able training corpus using Moses (Koehn et al.,2007).
Table 2 lists the number of sentences inthe training and test sets for each task.For each task or subtask, we select 375 thousand(K) training instances from the available paralleltraining corpora as interpretants for the individualRTM models using parallel FDA5.
We add theselected training set to the 3 million (M) sentencesselected from the available monolingual corporafor each LM corpus.
The statistics of the trainingdata selected by and used as interpretants in the4English Gigaword 5th, Spanish Gigaword 3rd edition.Task Train TestTask 1.1 (en-es) 3816 600Task 1.1 (es-en) 1050 450Task 1.1 (en-de) 1400 600Task 1.1 (de-en) 1050 450Task 1.2 (en-es) 896 208Task 1.3 (en-es) 650 208Task 2 (en-es) 1957 382Task 2 (es-en) 900 150Task 2 (en-de) 715 150Task 2 (de-en) 350 100Table 2: Number of sentences in different tasks.RTM models is given in Table 3.
The details ofinstance selection with parallel FDA5 are providedin (Bic?ici et al., 2014).Task S TTask 1.1 (en-es) 6.2 6.9Task 1.1 (es-en) 7.9 7.4Task 1.1 (en-de) 6.1 6Task 1.1 (de-en) 6.9 6.4Task 1.2 (en-es) 6.1 6.7Task 1.3 (en-es) 6.2 6.8Task 2 (en-es) 6.2 6.8Task 2 (es-en) 7.5 7Task 2 (en-de) 5.9 5.9Task 2 (de-en) 6.3 6.8Table 3: Number of words in I (in millions) se-lected for each task (S for source, T for target).3.1 Learning Models and Optimization:We use ridge regression (RR), support vector re-gression (SVR) with RBF (radial basis functions)kernel (Smola and Sch?olkopf, 2004), and ex-316Task Translation Model r RMSE MAE RAETask1.1es-en FS-RR 0.3512 0.6394 0.5319 0.9114es-en PLS-RR 0.3579 0.6746 0.5488 0.9405en-de PLS-TREE 0.2922 0.7496 0.6223 0.9404en-de TREE 0.2845 0.7485 0.6241 0.9431en-es TREE 0.4485 0.619 0.45 0.9271en-es PLS-TREE 0.4354 0.6213 0.4723 0.973de-en RR 0.3415 0.7475 0.6245 0.9653de-en PLS-RR 0.3561 0.7711 0.6236 0.9639Task1.2en-es SVR 0.4769 0.203 0.1378 0.8443en-es TREE 0.4708 0.2031 0.1372 0.8407Task1.3en-es SVR 0.6974 21543 14866 0.6613en-es RR 0.6991 21226 15325 0.6817Table 4: Training performance of the top 2 individual RTM models prepared for different tasks.tremely randomized trees (TREE) (Geurts et al.,2006) as the learning models.
TREE is an en-semble learning method over randomized decisiontrees.
These models learn a regression functionusing the features to estimate a numerical targetvalue.
We also use these learning models aftera feature subset selection with recursive featureelimination (RFE) (Guyon et al., 2002) or a di-mensionality reduction and mapping step usingpartial least squares (PLS) (Specia et al., 2009),both of which are described in (Bic?ici et al., 2013).We optimize the learning parameters, the num-ber of features to select, the number of dimen-sions used for PLS, and the parameters for paral-lel FDA5.
More detailed descriptions of the opti-mization processes are given in (Bic?ici et al., 2013;Bic?ici et al., 2014).
We optimize the learning pa-rameters by selecting ?
close to the standard de-viation of the noise in the training set (Bic?ici,2013) since the optimal value for ?
is shown tohave linear dependence to the noise level for dif-ferent noise models (Smola et al., 1998).
We selectthe top 2 systems according to their performanceon the training set.
For Task 2, we use both GlobalLinear Models (GLM) (Collins, 2002) and GLMwith dynamic learning (GLMd) we developed lastyear (Bic?ici, 2013).
GLM relies on Viterbi de-coding, perceptron learning, and flexible featuredefinitions.
GLMd extends the GLM frameworkby parallel perceptron training (McDonald et al.,2010) and dynamic learning with adaptive weightupdates in the perceptron learning algorithm:w = w + ?
(?
(xi, yi)?
?
(xi,?y)) , (1)where ?
returns a global representation for in-stance i and the weights are updated by ?, whichdynamically decays the amount of the change dur-ing weight updates at later stages and preventslarge fluctuations with updates.3.2 Training ResultsWe use mean absolute error (MAE), relativeabsolute error (RAE), root mean squared error(RMSE), and correlation (r) to evaluate (Bic?ici,2013).
DeltaAvg (Callison-Burch et al., 2012) cal-culates the average quality difference between thetop n ?
1 quartiles and the overall quality for thetest set.
Table 4 provides the training results.3.3 Test ResultsTask 1.1: Predicting the Post-Editing Effort forSentence Translations: Task 1.1 is about pre-dicting post-editing effort (PEE) and their rank-ing.
The results on the test set are given in Ta-ble 5 where QuEst (Shah et al., 2013) SVR liststhe baseline system results.
Rank lists the overallranking in the task out of about 10 submissions.We obtain the rankings by sorting according to thepredicted scores and randomly assigning ranks incase of ties.
RTMs with SVR PLS learning is ableto achieve the top rank in this task.Task 1.2: Predicting HTER of Sentence Trans-lations Task 1.2 is about predicting HTER(human-targeted translation edit rate) (Snover etal., 2006), where case insensitive translation editrate (TER) scores obtained by TERp (Snover etal., 2009) and their ranking.
We derive featuresover sentences that are true-cased.
The results onthe test set are given in Table 6 where the ranks areout of about 11 submissions.
We are also able toachieve the top ranking in this task.317Ranking Translations DeltaAvg r Ranken-esTREE 0.26 -0.41 1PLS-TREE 0.26 -0.38 2QuEst SVR 0.14 -0.22es-enPLS-RR 0.20 -0.35 2FS-RR 0.19 -0.36 3QuEst SVR 0.12 -0.21en-deTREE 0.39 -0.54 1PLS-TREE 0.33 -0.42 2QuEst SVR 0.23 -0.34de-enRR 0.38 -0.51 1PLS-RR 0.35 -0.45 2QuEst SVR 0.21 -0.25Scoring Translations MAE RMSE Ranken-esTREE 0.49 0.61 1PLS-TREE 0.49 0.61 2QuEst SVR 0.52 0.66es-enFS-RR 0.53 0.64 1PLS-RR 0.55 0.71 2QuEst SVR 0.57 0.68en-deTREE 0.58 0.68 1PLS-TREE 0.60 0.71 2QuEst SVR 0.64 0.76de-enRR 0.55 0.67 1PLS-RR 0.57 0.74 2QuEst SVR 0.65 0.78Table 5: RTM-DCU Task1.1 results on the test setand baseline results.Ranking Translations DeltaAvg r Ranken-esSVR 9.31 0.53 1TREE 8.57 0.48 2QuEst SVR 5.08 0.31Scoring Translations MAE RMSE Ranken-esSVR 13.40 16.69 2TREE 14.03 17.48 4QuEst SVR 15.23 19.48Table 6: RTM-DCU Task1.2 results on the test setand baseline results.Task 1.3: Predicting Post-Editing Time for Sen-tence Translations Task 1.3 involves the predic-tion of the post-editing time (PET) for a translatorto post-edit the MT output.
The results on the testset are given in Table 7 where the ranks are out ofabout 10 submissions.
RTMs become the top in allmetrics with RR and SVR learning models.Task 2: Prediction of Word-level TranslationQuality Task 2 is about binary, ternary, or multi-class classification of word-level quality.
We de-velop individual RTM models for each subtask anduse the GLM and GLMd learning models (Bic?ici,2013), for predicting the quality at the word-level.The features used are similar to last year?s (Bic?ici,2013) and broadly categorized as CCL links, wordcontext based on surrounding words, word align-ments, word lengths, word locations, word pre-fixes and suffixes, and word forms (i.e.
capital,Ranking Translations DeltaAvg r Ranken-esRR 17.02 0.68 1SVR 16.60 0.67 2QuEst SVR 14.71 0.57Scoring Translations MAE RMSE Ranken-esSVR 16.77 26.17 1RR 17.50 25.97 7QuEst SVR 21.49 34.28Table 7: RTM-DCU Task1.3 results on the test setand baseline results.contains digit or punctuation).The results on the test set are given in Table 8where the ranks are out of about 8 submissions.RTMs with GLM or GLMd learning becomes thetop this task as well.ModelBinary Ternary Multi-classwF1Rank wF1Rank wF1Ranken-esGLM 0.351 6 0.299 5 0.268 1GLMd 0.329 7 0.266 6 0.032 7es-enGLM 0.269 2 0.220 2 0.087 1GLMd 0.291 1 0.239 1 0.082 2en-deGLM 0.453 1 0.211 2 0.150 1GLMd 0.369 2 0.219 1 0.125 2en-esGLM 0.261 1 0.083 2 0.024 2GLMd 0.230 2 0.086 1 0.031 1Table 8: RTM-DCU Task 2 results on the test set.wF1is the average weighted F1score.3.4 RTMs Across Tasks and YearsWe compare the difficulty of tasks according to theRAE levels achieved.
RAE measures the error rel-ative to the error when predicting the actual mean.A high RAE is an indicator that the task is hard.
InTable 9, we list the test results including the RAEobtained for different tasks and subtasks includingRTM results at QET13 (Bic?ici, 2013).
The bestresults are obtained for Task 1.3, which shows thatwe can only reduce the error with respect to know-ing and predicting the mean by about 28%.4 ConclusionReferential translation machines achieve top per-formance in automatic, accurate, and language in-dependent prediction of sentence-level and word-level statistical machine translation (SMT) qual-ity.
RTMs remove the need to access any SMTsystem specific information or prior knowledge ofthe training data or models used when generatingthe translations.318Task Translation Model r RMSE MAE RAETask1.1es-en FS-RR 0.3285 0.6373 0.5308 0.9es-en PLS-RR 0.3105 0.7124 0.5549 0.9409en-de PLS-TREE 0.4427 0.7091 0.6028 0.8883en-de TREE 0.5256 0.6788 0.5838 0.8602en-es TREE 0.4087 0.6114 0.4938 1.0983en-es PLS-TREE 0.4163 0.6084 0.4852 1.0794de-en RR 0.5399 0.6735 0.5513 0.8204de-en PLS-RR 0.4878 0.737 0.567 0.8437Task1.2en-es SVR 0.5499 0.1669 0.134 0.8532en-es TREE 0.5175 0.1748 0.1403 0.8931Task1.3en-es SVR 0.6336 26174 16770 0.7223en-es RR 0.6359 25966 17496 0.7536QET13 Task1.1 en-esPLS-SVR 0.5596 0.1683 0.1326 0.8849SVR 0.5082 0.1728 0.1385 0.924QET13 Task1.3 en-esPLS-SVR 0.6752 86.62 49.62 0.6919SVR 0.6682 90.36 49.21 0.6862Table 9: Test performance of the top 2 individual RTM models prepared for different tasks and RTMresults from QET13 on similar tasks (Bic?ici, 2013).AcknowledgmentsThis work is supported in part by SFI(07/CE/I1142) as part of the CNGL Centrefor Global Intelligent Content (www.cngl.org)at Dublin City University and in part by theEuropean Commission through the QTLaunchPadFP7 project (No: 296347).
We also thank theSFI/HEA Irish Centre for High-End Computing(ICHEC) for the provision of computationalfacilities and support.ReferencesEneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
SemEval-2014 Task 10: Multilingualsemantic textual similarity.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, August.Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In *SEM 2012: The First JointConference on Lexical and Computational Seman-tics ?
Volume 1: Proceedings of the main conferenceand the shared task, and Volume 2: Proceedings ofthe Sixth International Workshop on Semantic Eval-uation (SemEval 2012), pages 435?440, Montr?eal,Canada, 7-8 June.
Association for ComputationalLinguistics.Ergun Bic?ici and Andy Way.
2014.
RTM-DCU: Ref-erential translation machines for semantic similarity.In SemEval-2014: Semantic Evaluation Exercises- International Workshop on Semantic Evaluation,Dublin, Ireland, 23-24 August.Ergun Bic?ici and Deniz Yuret.
2011a.
Instance se-lection for machine translation using feature decayalgorithms.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 272?283,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Ergun Bic?ici and Deniz Yuret.
2011b.
RegMT systemfor machine translation, system combination, andevaluation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 323?329,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Ergun Bic?ici and Deniz Yuret.
2014.
Optimizing in-stance selection for statistical machine translationwith feature decay algorithms.
IEEE/ACM Transac-tions On Audio, Speech, and Language Processing(TASLP).Ergun Bic?ici, Declan Groves, and Josef van Genabith.2013.
Predicting sentence translation quality usingextrinsic and language independent features.
Ma-chine Translation, 27:171?192, December.Ergun Bic?ici, Qun Liu, and Andy Way.
2014.
Par-allel FDA5 for fast deployment of accurate statisti-cal machine translation systems.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, Baltimore, USA, June.
Association for Compu-tational Linguistics.319Ergun Bic?ici.
2011.
The Regression Model of MachineTranslation.
Ph.D. thesis, Koc?
University.
Supervi-sor: Deniz Yuret.Ergun Bic?ici.
2013.
Referential translation machinesfor quality estimation.
In Proceedings of the EighthWorkshop on Statistical Machine Translation, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.Ergun Bic?ici.
2008.
Consensus ontologies in sociallyinteracting multiagent systems.
Journal of Multia-gent and Grid Systems.Carl Hugo Bj?ornsson.
1968.
L?asbarhet.
Liber.Chris Bliss.
2012.
Comedy is transla-tion, February.
http://www.ted.com/talks/chris bliss comedy is translation.html.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Proc.
ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 1?44, Sofia, Bulgaria, August.
Associ-ation for Computational Linguistics.Ond?rej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, and Lucia Specia.2014.
Findings of the 2014 workshop on statisti-cal machine translation.
In Proc.
of the Ninth Work-shop on Statistical Machine Translation, Balrimore,USA, June.
Association for Computational Linguis-tics.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2):263?311, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proc.
of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montr?eal, Canada, June.
Association for Com-putational Linguistics.Jos?e Guilherme Camargo de Souza, Christian Buck,Marco Turchi, and Matteo Negri.
2013.
FBK-UEdin participation to the WMT13 quality estima-tion shared task.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, pages 352?358, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing - Volume 10, EMNLP?02, pages 1?8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology Research, pages 138?145, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Pierre Geurts, Damien Ernst, and Louis Wehenkel.2006.
Extremely randomized trees.
Machine Learn-ing, 63(1):3?42.Isabelle Guyon, Jason Weston, Stephen Barnhill, andVladimir Vapnik.
2002.
Gene selection for cancerclassification using support vector machines.
Ma-chine Learning, 46(1-3):389?422.Kenth Hagstr?om.
2012.
Swedish readabil-ity calculator.
https://github.com/keha76/Swedish-Readability-Calculator.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
SemEval-2014 Task 3:Cross-level semantic similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014), Dublin, Ireland, August.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL, pages 177?180, Prague, Czech Republic, June.Roger Levy and Galen Andrew.
2006.
Tregex andTsurgeon: tools for querying and manipulating treedata structures.
In Proceedings of the fifth interna-tional conference on Language Resources and Eval-uation.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the penn treebank.
Comput.Linguist., 19(2):313?330, June.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zam-parelli.
2014.
SemEval-2014 Task 1: Evaluation ofcompositional distributional semantic models on fullsentences through semantic relatedness and textualentailment.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland, August.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 456?464, Los Angeles, California,June.
Association for Computational Linguistics.320Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Proc.of 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword fifth edi-tion, Linguistic Data Consortium.Yoav Seginer.
2007.
Learning Syntactic Structure.Ph.D.
thesis, Universiteit van Amsterdam.Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, andLucia Specia.
2013.
Quest - design, implementationand extensions of a framework for machine transla-tion quality estimation.
Prague Bull.
Math.
Linguis-tics, 100:19?30.Alex J. Smola and Bernhard Sch?olkopf.
2004.
A tu-torial on support vector regression.
Statistics andComputing, 14(3):199?222, August.A.
J. Smola, N. Murata, B. Sch?olkopf, and K.-R.M?uller.
1998.
Asymptotically optimal choice of?-loss for support vector machines.
In L. Niklas-son, M. Boden, and T. Ziemke, editors, Proceedingsof the International Conference on Artificial NeuralNetworks, Perspectives in Neural Computing, pages105?110, Berlin.
Springer.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of Association for MachineTranslation in the Americas,.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, adequacy,or hter?
: exploring different human judgmentswith a tunable mt metric.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, StatMT ?09, pages 259?268, Stroudsburg, PA,USA.
Association for Computational Linguistics.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Estimat-ing the sentence-level quality of machine translationsystems.
In Proceedings of the 13th Annual Con-ference of the European Association for MachineTranslation (EAMT), pages 28?35, Barcelona, May.EAMT.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proc.
Intl.
Conf.
on SpokenLanguage Processing, pages 901?904.Wikipedia.
2013.
Lix.http://en.wikipedia.org/wiki/LIX.David Graff Denise DiPersio?Angelo Mendonc?a,Daniel Jaquette.
2011.
Spanish Gigaword third edi-tion, Linguistic Data Consortium.321
