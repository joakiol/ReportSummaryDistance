Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86?92,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsNarrative Schema as World Knowledge for Coreference ResolutionJoseph IrwinNara Institute ofScience and TechnologyNara Prefecture, Japanjoseph-i@is.naist.jpMamoru KomachiNara Institute ofScience and TechnologyNara Prefecture, Japankomachi@is.naist.jpYuji MatsumotoNara Institute ofScience and TechnologyNara Prefecture, Japanmatsu@is.naist.jpAbstractIn this paper we describe the system withwhich we participated in the CoNLL-2011Shared Task on modelling coreference.
Oursystem is based on a cluster-ranking modelproposed by Rahman and Ng (2009), withnovel semantic features based on recent re-search on narrative event schema (Chambersand Jurafsky, 2009).
We demonstrate someimprovements over the baseline when usingschema information, although the effect var-ied between the metrics used.
We also explorethe impact of various features on our system?sperformance.1 IntroductionCoreference resolution is a problem for automateddocument understanding.
We say two segments ofa natural-language document corefer when they re-fer to the same real-world entity.
The segments ofa document which refer to an entity are called men-tions.
In coreference resolution tasks, mentions areusually restricted to noun phrases.The goal of the CoNLL-2011 Shared Task (Prad-han et al, 2011) is to model unrestricted coreferenceusing the OntoNotes corpus.
The OntoNotes cor-pus is annotated with several layers of syntactic andsemantic information, making it a rich resource forinvestigating coreference resolution (Pradhan et al,2007).We participated in both the ?open?
and ?closed?tracks.
The ?closed?
track requires systems to onlyuse the provided data, while the ?open?
track al-lows use of external data.
We created a baselinesystem based on the cluster-ranking model proposedby Rahman and Ng (2009).
We then experimentedwith adding novel semantic features derived fromco-referring predicate-argument chains.
These nar-rative schema were developed by Chambers and Ju-rafsky (2009).
They are described in more detail ina later section.2 Related WorkSupervised machine-learning approaches to corefer-ence resolution have been researched for almost twodecades.
Recently, the state of the art seems to bemoving away from the early mention-pair classifica-tion model toward entity-based models.
Ng (2010)provides an excellent overview of the history and re-cent developments within the field.Both entity-mention and mention-pair models areformulated as binary classification problems; how-ever, ranking may be a more natural approach tocoreference resolution (Ng, 2010; Rahman and Ng,2009).
Rahman and Ng (2009) in particular pro-pose the cluster-ranking model which we used in ourbaseline.
In another approach, Daume?
and Marcu(2005) apply their Learning as Search Optimizationframework to coreference resolution, and show goodresults.Feature selection is important for good perfor-mance in coreference resolution.
Ng (2010) dis-cusses commonly used features, and analyses ofthe contribution of various features can be found in(Daume?
and Marcu, 2005; Rahman and Ng, 2011;Ponzetto and Strube, 2006b).
Surprisingly, Rahmanand Ng (2011) demonstrated that a system using al-most exclusively lexical features could outperform86systems which used more traditional sets of features.Although string features have a large effect onperformance, it is recognized that the use of seman-tic information is important for further improvement(Ng, 2010; Ponzetto and Strube, 2006a; Ponzettoand Strube, 2006b; Haghighi and Klein, 2010).
Theuse of predicate-argument structure has been ex-plored by Ponzetto and Strube (2006b; 2006a).3 Narrative Schema for CoreferenceNarrative schema are extracted from large-scale cor-pora using coreference information to identify pred-icates whose arguments often corefer.
Similaritymeasures are used to build up schema consistingof one or more event chains ?
chains of typically-coreferring predicate arguments (Chambers and Ju-rafsky, 2009).
Each chain corresponds to a role inthe schema.A role defines a class of participants in theschema.
Conceptually, if a schema is present in adocument, than each role in the schema correspondsto an entity in the document.
An example schema isshown with some typical participants in Figure 1.
Inthis paper the temporal order of events in the schemais not considered.prohibitrequireallowbarviolatesubj.
obj.law, bill, rule,amendmentcompany, mi-crosoft, govern-ment, banksFigure 1: An example narrative schema with two roles.Narrative schema are similar to the script con-cept put forth by Schank and Abelson (1977).
Likescripts, narrative schema can capture complex struc-tured information about events described in naturallanguage documents (Schank and Abelson, 1977;Abelson, 1981; Chambers and Jurafsky, 2009).We hypothesize that narrative schema can be agood source of information for making coreferencedecisions.
One reason they could be useful is thatthey can directly capture the fact that arguments ofcertain predicates are relatively more likely to referto the same entity.
In fact, they can capture globalinformation about verbs ranging over the entire doc-ument, which we expect may lead to greater accu-racy when combined with the incremental clusteringalgorithm we employ.Additionally, the information that two predicatesoften share arguments yields semantic informationabout the argument words themselves.
For exam-ple, if the subjects of the verbs eat and drink oftencorefer, we may be able to infer that words whichoccur in the subject position of these verbs sharesome property (e.g., animacy).
This last conjec-ture is somewhat validated by Ponzetto and Strube(2006b), who reported that including predicate-argument pairs as features improved the perfor-mance of a coreference resolver.4 System Description4.1 OverviewWe built a coreference resolution system based onthe cluster-ranking algorithm proposed by Rahmanand Ng (2009).
During document processing main-tains a list of clusters of coreferring mentions whichare created iteratively.
Our system uses a determin-istic mention-detection algorithm that extracts can-didate NPs from a document.
We process the men-tions in order of appearance in the document.
Foreach mention a ranking query is created, with fea-tures generated from the clusters created so far.
Ineach query we include a null-cluster instance, to al-low joint learning of discourse-new detection, fol-lowing (Rahman and Ng, 2009).For training, each mention is assigned to its cor-rect cluster according to the coreference annota-tion.
The resulting queries are used to train aclassification-based ranker.In testing, the ranking model thus learned is usedto rank the clusters in each query as it is created;the active mention is assigned to the cluster with thehighest rank.A data-flow diagram for our system is shown inFigure 2.87DocumentMentionExtractionFeatureExtractionEntitiesNarrativeSchemaDatabaseClusterRankingFigure 2: System execution flow4.2 Cluster-ranking ModelOur baseline system uses a cluster-ranking modelproposed by Rahman and Ng (2009; 2011).
In thismodel, clusters are iteratively constructed after con-sidering each active mention in a document in order.During training, features are created between the ac-tive mention and each cluster created so far.
A rankis assigned such that the cluster which is coreferentto the active mention has the highest value, and eachnon-coreferent cluster is assigned the same, lowerrank (The exact values are irrelevant to learning aranking; for the experiments in this paper we usedthe values 2 and 1).
In this way it is possible tolearn to preferentially rank correct clustering deci-sions higher.For classification, instances are constructed ex-actly the same way as for training, except that foreach active mention, a query must be constructedand ranked by the classifier in order to proceed withthe clustering.
After the query for each active men-tion has been ranked, the mention is assigned to thecluster with the highest ranking, and the algorithmproceeds to the next mention.4.3 NotationIn the following sections, mk is the active mentioncurrently being considered, mj is a candidate an-tecedent mention, and cj is the cluster to which itbelongs.
Most of the features used in our system ac-tually apply to a pair of mentions (i.e., mk and mj)or to a single mention (either mk or mj).
To cre-ate a training or test instance using mk and cj , thefeatures which apply to mj are converted to cluster-level features by a procedure described in 4.6.4.4 Joint Anaphoric Mention DetectionWe follow Rahman and Ng (2009) in jointly learn-ing to detect anaphoric mentions along with resolv-ing coreference relations.
For each active mentionmk, an instance for a ?null?
cluster is also created,with rank 2 if the mention is not coreferent withany preceding mention, or rank 1 if it has an an-tecedent.
This allows the ranker the option of mak-ing mk discourse-new.
To create this instance, onlythe features which involve just mk are used.4.5 FeaturesThe features used in our system are shown in Table1.
For the NE features we directly use the types fromthe OntoNotes annotation.
14.6 Making Cluster-Level FeaturesEach feature which applies to mj must be convertedto a cluster-level feature.
We follow the proce-dure described in (Rahman and Ng, 2009).
Thisprocedure uses binary features whose values corre-spond to being logically true or false.
Multi-valuedfeatures are first converted into equivalent sets ofbinary-valued features.
For each binary-valued fea-ture, four corresponding cluster-level features arecreated, whose values are determined by four logical1The set of types is: PERSON, NORP, FACILITY, ORGA-NIZATION, GPE, LOCATION, PRODUCT, EVENT, WORK,LAW, LANGUAGE, DATE, TIME, PERCENT, MONEY,QUANTITY, ORDINAL, CARDINAL88Features involving mj onlySUBJECT Y if mj is the grammatical subject of a verb; N otherwise*NE_TYPE1 the NE label for mj if there is one else NONEFeatures involving mk onlyDEFINITE Y if the first word of mk is the; N otherwiseDEMONSTRATIVE Y if the first word of mk is one of this, that, these, or those; N otherwiseDEF_DEM_NA Y if neither DEFINITE nor DEMONSTRATIVE is Y; N otherwisePRONOUN2 Y if mk is a personal pronoun; N otherwisePROTYPE2 nominative case of mk if mk is a pronoun or NA if it is not (e.g., HE if mk is him)NE_TYPE2 the NE label for mk if there is oneFeatures involving both mj and mkDISTANCE how many sentences separate mj and mk; the values are A) same sentence, B) previous sentence,and C) two sentences ago or moreHEAD_MATCH Y if the head words are the same; N otherwisePRONOUN_MATCH if either of mj and mk is not a pronoun, NA; if the nominative case of mj and mk is the same, C; Iotherwise*NE_TYPE?
the concatenation of the NE labels of mj and mk (if either or both are not labelled NEs, the featureis created using NONE as the corresponding label)SCHEMA_PAIR_MATCH Y if mj and mk appear in the same role in a schema, and N if they do notFeatures involving cj and mkSCHEMA_CLUSTER_MATCH a cluster-level feature between mk and cj (details in Section 4.7)Table 1: Features implemented in our coreference resolver.
Binary-valued features have values of YES or NO.
Multi-valued features are converted into equivalent sets of binary-valued features before being used to create the cluster-levelfeatures used by the ranker.predicates: NONE, MOST-FALSE, MOST-TRUE,and ALL.To be precise, a feature F may be thought of as afunction taking mj as a parameter, e.g., F (mj).
Tosimplify notation, features which apply to the pairmj ,mk take mk as an implicit parameter.
The log-ical predicates then compare the two counts n =|{mj | F (mj) = true}| and C = |cj |.
The re-sulting features are shown in Table 2.NONE F TRUE iff n = 0MOST-FALSE F TRUE iff n < C2MOST-TRUE F TRUE iff C2 ?
n < CALL F TRUE iff n = CTable 2: Cluster-level features created from binary-valued feature FThe two features marked with * are treateddifferently.
For each value of NE_TYPE1 andNE_TYPE?, a new cluster-level feature is cre-ated whose value is the number of times that fea-ture/value appeared in the cluster (i.e., if there weretwo PERSON NEs in a cluster then the featureNE_TYPE1_PERSON would have the value 2).4.7 SCHEMA_CLUSTER_MATCHThe SCHEMA_CLUSTER_MATCH feature is ac-tually three features, which are calculated over anentire candidate antecedent cluster cj .
First a list iscreated of all of the schema roles which the men-tions in cj participate in, and sorted in decreasingorder according to how many mentions in cj par-ticipate in each.
Then, the value of the featureSCHEMA_CLUSTER_MATCHn is Y if mentionmk also participates in the nth schema role in thelist, for n = 1, 2, 3.
If it does not, or if the corre-sponding nth schema role has fewer than two partic-ipants in cj , the value of this feature is N.4.8 Implementation DetailsOur system was implemented in Python, in order tomake use of the NLTK library2.
For the ranker weused SVMrank, an efficient implementation for train-ing ranking SVMs (Joachims, 2006) 3.2http://www.nltk.org/3http://svmlight.joachims.org/89R P F1MUC 12.45% 50.60% 19.98CLOSED B3 35.07% 89.90% 50.46CEAF 45.84% 17.38% 25.21Overall score: 31.88MUC 18.56% 51.01% 27.21OPEN B3 38.97% 85.57% 53.55CEAF 43.33% 19.36% 26.76Overall score: 35.84Table 3: Official system results5 Experiments and Results5.1 CoNLL System SubmissionWe submitted two results to the CoNLL-2011Shared Task.
In the ?closed?
track we submitted theresults of our baseline system without the schemafeatures, trained on all documents in both the train-ing and development portions of the OntoNotes cor-pus.We also submitted a result in the ?open?
track:a version of our system with the schema featuresadded.
Due to issues with the implementation of thissecond version, however, we were only able to sub-mit results from a model trained on just the WSJ por-tion of the training dataset.
For the schema features,we used a database of narrative schema released byChambers and Jurafsky (2010) ?
specifically the listof schemas of size 12.
4The official system scores for our system arelisted in Table 3.
We can attribute some of the lowperformance of our system to features which are toonoisy, and to having not enough features comparedto the large size of the dataset.
It is likely that thesetwo factors adversely impact the ability of the SVMto learn effectively.
In fact, the features which we in-troduced partially to provide more features to learnwith, the NE features, had the worst impact on per-formance according to later analysis.
Because of aproblem with our implementation, we were unableto get an accurate idea of our system?s performanceuntil after the submission deadline.4Available at http://cs.stanford.edu/people/nc/schemas/R P F1MUC 12.77% 57.66% 20.91Baseline B3 35.1% 91.05% 50.67CEAF 47.80% 17.29% 25.40MUC 12.78% 54.84% 20.73+SCHEMA B3 35.75% 90.39% 51.24CEAF 46.62% 17.43% 25.38Table 4: Schema features evaluated on the developmentset.
Training used the entire training dataset.5.2 Using Narrative Schema as WorldKnowledge for Coreference ResolutionWe conducted an evaluation of the baseline withoutschema features against a model with both schemafeatures added.
The results are shown in Table 4.The results were mixed, with B3 going up andMUC and CEAF falling slightly.
Cross-validationusing just the development set showed a more posi-tive picture, however, with both MUC and B3 scoresincreasing more than 1 point (p = 0.06 and p <0.01, respectively), and CEAF increasing about 0.5points as well (although this was not significant atp > 0.1).
5One problem with the schema features that wehad anticipated was that they may have a problemwith sparseness.
We had originally intended to ex-tract schema using the coreference annotation inOntoNotes, predicting that this would help alleviatethe problem; however, due to time constraints wewere unable to complete this effort.5.3 Feature AnalysisWe conducted a feature ablation analysis on ourbaseline system to better understand the contribu-tion of each feature to overall performance.
Theresults are shown in Table 5.
We removed fea-tures in blocks of related features; -HEAD removesHEAD MATCH; -DIST removes the DISTANCEfeature; -SUBJ is the baseline system without SUB-JECT; -PRO is the baseline system without PRO-NOUN2, PROTYPE2, and PRONOUN MATCH;-DEF DEM removes DEFINITE, DEMONSTRA-TIVE, and DEF DEM NA; and -NE removes thenamed entity features.5All significance tests were performed with a two-tailed t-test.90MUC 12.77% 57.66% 20.91Baseline B3 35.1% 91.05% 50.67CEAF 47.80% 17.29% 25.40R P F1 ?F1MUC 0.00% 33.33% 0.01 -20.90-HEAD B3 26.27% 99.98% 41.61 -9.06CEAF 52.88% 13.89% 22.00 -3.40MUC 0.39% 60.86% 0.79 -20.12-DIST B3 26.59% 99.72% 41.99 -8.68CEAF 52.76% 13.99% 22.11 -3.29MUC 12.47% 47.69% 19.78 -1.13-SUBJ B3 36.54% 87.80% 51.61 0.94CEAF 43.75% 17.22% 24.72 -0.68MUC 18.36% 55.98% 27.65 6.74-PRO B3 37.45% 85.78% 52.14 1.47CEAF 47.86% 19.19% 27.40 2.00MUC 18.90% 51.72% 27.68 6.77-DEF_DEM B3 41.65% 86.11% 56.14 5.47CEAF 46.39% 21.61% 29.48 4.08MUC 22.76% 49.5% 31.18 10.27-NE B3 46.78% 84.92% 60.33 9.66CEAF 45.65% 25.19% 32.46 7.06Table 5: Effect of each feature on performance.The fact that for three of the features, removingthe feature actually improved performance is trou-bling.
Possibly these features were too noisy; weneed to improve the baseline features for future ex-periments.6 ConclusionsSemantic information is necessary for many tasks innatural language processing.
Most often this infor-mation is used in the form of relationships betweenwords ?
for example, how semantically similar twowords are, or which nouns are the objects of a verb.However, it is likely that humans make use of muchhigher-level information than the similarity betweentwo concepts when processing language (Abelson,1981).
We attempted to take advantage of recent de-velopments in automatically aquiring just this sortof information, and demonstrated the possibility ofmaking use of it in NLP tasks such as coreference.However, we need to improve both the implementa-tion and data for this approach to be practical.For future work, we intend to investigate avenuesfor improving the aquisition and use of the narra-tive schema information, and also compare narra-tive schema with other types of semantic informa-tion in coreference resolution.
Because coreferenceinformation is central to the extraction of narrativeschema, the joint learning of coreference resolutionand narrative schema is another area we would liketo explore.ReferencesRobert P. Abelson.
1981.
Psychological status of thescript concept.
American Psychologist, 36(7):715?729.Nathanael Chambers and Dan Jurafsky.
2009.
Unsuper-vised Learning of Narrative Schemas and their Partic-ipants.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 602?610, Suntec, Singapore.Nathanael Chambers and Dan Jurafsky.
2010.
Adatabase of narrative schemas.
In Proceedings of theSeventh International Conference on Language Re-sources and Evaluation (LREC 2010), Malta.Hal Daume?
and Daniel Marcu.
2005.
A large-scale ex-ploration of effective global features for a joint en-tity detection and tracking model.
In Proceedings ofthe Conference on Human Language Technology andEmpirical Methods in Natural Language Processing -HLT ?05, pages 97?104, Morristown, NJ, USA.Aria Haghighi and Dan Klein.
2010.
Coreference reso-lution in a modular, entity-centered model.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 385?393.Thorsten Joachims.
2006.
Training linear SVMs in lin-ear time.
In Proceedings of the 12th ACM SIGKDD In-ternational Conference on Knowledge Discovery andData Mining KDD 06, pages 217?226.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: the first fifteen years.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, pages 1396?1411.Simone Paolo Ponzetto and Michael Strube.
2006a.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceedingsof the main conference on Human Language Technol-ogy Conference of the North American Chapter of theAssociation of Computational Linguistics, pages 192?199.Simone Paolo Ponzetto and Michael Strube.
2006b.
Se-mantic role labeling for coreference resolution.
InProceedings of the Eleventh Conference of the Euro-pean Chapter of the Association for Computational91Linguistics - EACL ?06, pages 143?146, Morristown,NJ, USA.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted Coreference: Identifying Entities and Eventsin OntoNotes.
In International Conference on Seman-tic Computing (ICSC 2007), pages 446?453.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling Unre-stricted Coreference in OntoNotes.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning (CoNLL 2011), Portland, Oregon.Altaf Rahman and Vincent Ng.
2009.
Supervised Mod-els for Coreference Resolution.
In Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing, pages 968?977, Singapore.Altaf Rahman and Vincent Ng.
2011.
Narrowingthe Modeling Gap: A Cluster-Ranking Approach toCoreference Resolution.
Journal of Artificial Intelli-gence Research, 40:469?521.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,plans, goals and understanding: An inquiry into hu-man knowledge structures.
Lawrence Erlbaum, Ox-ford, England.92
