Example-based Rescoring of Statistical Machine Translation OutputMichael Paul?
?, Eiichiro Sumita?
?ATR Spoken Language Translation LabsKeihanna Science City619-0288 Kyoto, Japan{ Michael.Paul , Eiichiro.Sumita , Seiichi.Yamamoto }@atr.jpand Seiichi Yamamoto??
?Kobe UniversityGraduate School of Science and Technology657-8501 Kobe, JapanAbstractConventional statistical machine translation(SMT) approaches might not be able to finda good translation due to problems in its sta-tistical models (due to data sparseness dur-ing the estimation of the model parameters) aswell as search errors during the decoding pro-cess.
This paper1 presents an example-basedrescoring method that validates SMT transla-tion candidates and judges whether the selecteddecoder output is good or not.
Given sucha validation filter, defective translations canbe rejected.
The experiments show a dras-tic improvement in the overall system perfor-mance compared to translation selection meth-ods based on statistical scores only.1 IntroductionThe statistical machine translation framework (SMT) for-mulates the problem of translating a sentence from asource language S into a target language T as the maxi-mization problem of the conditional probability:TM?LM = argmaxT p(S|T ) ?
p(T ), (1)where p(S|T ) is called a translation model (TM ), rep-resenting the generation probability from T into S, p(T )is called a language model (LM ) and represents the like-lihood of the target language (Brown et al, 1993).
TheTM and LM probabilities are trained automatically froma parallel text corpus (parameter estimation).
They rep-resent the general translation knowledge used to map asequence of words from the source language into the tar-get language.
During the translation process (decoding) astatistical score based on the probabilities of the transla-tion and the language models is assigned to each transla-tion candidate and the one with the highest TM?LM scoreis selected as the translation output.However, the system might not be able to find a goodtranslation due to parameter estimation problems of thestatistical models (due to data sparseness during the es-timation of the model probabilities) and search errors1The research reported here was supported in part by a con-tract with the Telecommunications Advancement Organizationof Japan entitled, ?A study of speech dialogue translation tech-nology based on a large corpus?.during the translation process.
Moreover, conventionalSMT approaches use words as the translation unit.
There-fore, the optimization is carried out locally generating thetranslation word-by-word.In the framework of example-based machine transla-tion (EBMT), however, a parallel text corpus is used di-rectly to obtain the translation (Nagao, 1984).
Given aninput sentence, translation examples from the corpus thatare best matched to the input are retrieved and adjustedto obtain the translation.
Thus the translation unit usedin EBMT approaches is a complete sentence, providing alarger context for the generation of an appropriate transla-tion.
However, this approach requires appropriate trans-lation examples to achieve an accurate translation.A combination of statistical and example-based MTapproaches shows some promising perspectives for over-coming the shortcomes of each approach.
In this paper,we propose an example-based rescoring method (EBRS)for selecting translation candidates generated by a statis-tical decoder, as illustrated in Figure 1.TranslationExanplesTM LMParallel Text CorpusTranslationCandidatesDecoderSeed"Input"SMTEBRS"Output""Output"(of conventialmethod)(of proposedmethod)RescoreEditDistanceFigure 1: OutlineIt retrieves translation examples that are similar to theinput from a parallel text corpus (cf.
Section 2).
Thetarget parts of these examples (seed) paired with the in-put form the input of a statistical decoder (cf.
Section 3).The statistical scores of each generated translation candi-date are rescored using information about how much theseed sentence is modified during decoding.
It measuresthe distance between the word sequences of the decoderoutput and its seed sentence based on the costs of edit dis-tance operations (cf.
Section 4).
We combine the distancemeasure with the statistical scores of the SMT engine, re-sulting in a reliability measure to identify modeling prob-lems in statistically optimized translation candidates andto reject inappropriate solutions (cf.
Section 5).2 Translation Example RetrievalTranslation examples consist of pairs of pre-translatedsentences, either by humans (high quality) or automati-cally using MT systems (reduced quality).
A collectionof translation examples can be used directly to obtain atranslation of a given input sentence.
The similarity ofthe input to the source part of the translation examplesenables us to identify translation candidates that might beclose to the actual translation.A common approach to measure the distance betweensequences of words is the edit distance criteria (Wagner,1974).
The distance is defined as the sum of the costsof insertion (INS), deletion (DEL), and substitution (SUB)operations required to map one word sequence into theother.
The edit distance can be calculated by a standarddynamic programming technique.ED(s1,s2) = |INS| + |DEL|+ |SUB|An extension of the edit-distance-based retrievalmethod is presented in (Watanabe and Sumita, 2003).
Itincorporates the tf?idf criteria as seen in the informationretrieval framework by treating each translation exampleas a document.
For each word of the input, its term fre-quency tfi,j is combined with its document frequency dfiinto a single weight wi,j , which is used to select the mostrelevant ones out of N documents (= example targets).Another possibility for obtaining translation examplesis simply to utilize available (off-the-shelf) MT systemsby pairing the input sentence with the obtained MT out-put.
However, the quality of those translation examplesmight be much lower than manually created translations.3 Statistical Decoding(Germann et al, 2001) presents a greedy approach tosearch for the translation that is most likely according topreviously learned statitistical models.
An extension ofthis approach that can take advantage of translation ex-amples provided for a given input sentence is proposed in(Watanabe and Sumita, 2003).
Instead of decoding andgenerating an output string word-by-word as is done inthe basic concept, this greedy approach slightly modifiesthe target part of the translation examples so that the pairbecomes the actual translation.The advantage of the example-based approach is thatthe search for a good translation starts from the retrievedtranslation example, not a guessed translation resultingin fewer search errors.
However, since it uses the samegreedy search algorithm as the basic method, search er-rors cannot be avoided completely.
Furthermore, the pa-rameter estimation problem still remains.The experiment discussed in Section 5.1 indeed showsa large degradation in the system performance whenthe greedy decoder is applied to already perfect transla-tions, indicating that the decoder may modify translationswrongly based on its statistical models (IBM model 4).4 Example-based RescoringTherefore we have to validate the quality of translationcandidates selected by the decoder and judge whetherproblems in the SMT models or search errors resulted inan inaccurate translation or not.Our approach extends the example-based concept of(Watanabe and Sumita, 2003).
It compares the decoderoutput with the seed sentence, i.e., the target part of thetranslation example that forms the input of the decoder.Given a translation example whose source part is quitesimilar to the input, we can assume that the fewer themodifications that are necessary to alter the correspond-ing example target to the translation candidate during de-coding, the less likely it is that there will be a problem inthe statistical models.The decision on translation quality is based on the editdistance criteria, as introduced in Section 2.
For eachtranslation candidate, we measure the edit distance be-tween the word sequence of the decoder output and theseed sentence.
The proposed method rescores the transla-tion candidates of the SMT decoder by combining the sta-tistical probabilities of the translation and language mod-els with the example-based translation quality hypothesisand selects the translation candidate with the highest re-vised score as the translation output.The rescoring function rescore has to be designed insuch a way that almost unaltered translation candidateswith good translation and language model scores are pre-ferred over those with the highest statistical scores thatrequired lots of modifications to the seed sentence.For the experiments described below we defined twodifferent rescoring functions.
First, the edit distance ofthe seed sentence sd and the decoder output d is used asa weight to decrease the statistical scores.
The larger theedit distance score, the smaller the revised score of therespective translation candidate.
The scaling factor scaledepends on the utilized corpus and can be optimized on adevelopment set reserved for parameter tuning.TM?LM?EDW(d) = TM?LM(d)exp( scale ?
ED(sd,d) ) (2)The second rescoring function assigns a probability toeach decoder output that combines the exponential of thesum of log probabilities of TM and LM and the scalednegative ED scores of all translation candidates TC asfollows.TM?LM?EDP(d) = (3)exp(log TM(d)+log LM(d)?scale ?
ED(sd,d))?
(stc,tc)?T Cexp(log TM(tc)+log LM(tc)?scale ?
ED(stc,tc))5 EvaluationThe evaluation of our approach is carried out using a col-lection of Japanese sentences and their English transla-tions that are commonly found in phrasebooks for touristsgoing abroad (Takezawa et al, 2002).
The Basic TravelExpression Corpus (BTEC) contains 157K sentence pairsand the average lengths in words of Japanese and En-glish sentences are 7.7 and 5.5, respectively.
The corpuswas split randomly into three parts for training (155K),parameter tuning (10K), and evaluation (10K) purposes.The experiments described below were carried out on 510sentences selected randomly as the test set.For the evaluation, we used the following automaticscoring measures and human assessment.?
Word Error Rate (WER), which penalizes the edit dis-tance against reference translations (Su et al, 1992)?
BLEU: the geometric mean of n-gram precision forthe translation results found in reference translations(Papineni et al, 2002)?
Translation Accuracy (ACC): subjective evaluationranks ranging from A to D (A: perfect, B: fair, C:acceptable and D: nonsense), judged blindly by anative speaker (Sumita et al, 1999)In contrast to WER, higher BLEU and ACC scores indicatebetter translations.
For the automatic scoring measureswe utilized up to 16 human reference translations.5.1 Downgrading Effects During DecodingIn order to get an idea about how much degradation isto be expected in the translation candidates modified bythe statistical decoder, we conducted an experiment us-ing the reference translations of the test set as the input ofthe example-based decoder.
These seed sentences are al-ready accurate translations, thus simulating the ?optimal?translation example retrieval case resulting in an upperboundary of the statistical decoder performance.Table 1: Downgrading Effects During Decodingscoring automatic subjective (ACC)scheme WER BLEU A A+B A+B+C gainTM?LM 0.255 0.744 0.660 0.790 0.854 ?TM?LM?EDP 0.179 0.814 0.745 0.854 0.898 0.044TM?LM?EDW 0.010 0.984 0.903 0.968 0.982 0.128The results summarized in Table 1 show a largedegradation (WER=25.5%, BLEU=0.744) in the refer-ence translations when modified by the statistical decoder(TM?LM).
Only 66.0% of the decoder output are still per-fect and 14.6% even result in unacceptable translations.The rescoring function TM?LM?EDP enables us to recoversome of the decoder problems gaining 4.4% in accuracycompared to the statistical decoder.
The best perfor-mance is achieved by the weight-based rescoring func-tion TM?LM?EDW.
However, around 10% of the selectedtranslations are not yet perfect.5.2 Baseline ComparisonIn the second experiment, we used two types of retrievalmethods (tf?idf-based, MT -based), as introduced in Sec-tion 2, and compared the results with the baseline sys-tem TM?LM, i.e., the example-based decoding approachof (Watanabe and Sumita, 2003) using the tf?idf criteriafor the retrieval of translation examples and only the sta-tistical scores for the selection of the translation.For the MT-based retrieval method we used eight ma-chine translation systems for Japanese-to-English.
Threeof them were in-house EBMT systems which differ in thetranslation unit (sentence-based vs. phrase-based).
Theywere trained on the same corpus as the statistical decoder.The remaining five systems were (off-the-shelf) general-purpose translation engines with quite different levels ofperformance (cf.
Table 2).Table 2: MT System PerformanceMT1 MT2 MT3 MT4 MT5 MT6 MT7 MT8WER 0.320 0.408 0.419 0.580 0.584 0.588 0.600 0.646BLEU 0.604 0.489 0.424 0.222 0.252 0.237 0.205 0.200The results of our experiments are summarized inTable 3.
The baseline system TM?LM seems to workbest when used in combination with the tf?idf-based re-trieval method, achieving around 80% translation accu-racy.
Moderate improvements of around 2% can be seenwhen the proposed rescoring functions are used togetherwith the seed sentences obtained for the baseline system.However, the largest gain in performance is achievedwhen the decoder is applied to the output of multiple ma-chine translation systems and the translation is selectedusing the weight-based rescoring function.Table 3: Baseline Comparisontf?idf-based automatic subjective (ACC)retrieval WER BLEU A A+B A+B+C gainTM?LM 0.313 0.655 0.629 0.743 0.808 ?TM?LM?EDP 0.297 0.668 0.668 0.766 0.823 0.015TM?LM?EDW 0.289 0.639 0.676 0.749 0.815 0.007MT -based automatic subjective (ACC)retrieval WER BLEU A A+B A+B+C gainTM?LM 0.338 0.630 0.627 0.731 0.796 -0.012TM?LM?EDP 0.292 0.673 0.719 0.811 0.854 0.046TM?LM?EDW 0.272 0.661 0.809 0.890 0.927 0.119Table 4 compares the evaluation results of the baselineand the TM?LM?EDW system.
67.5% of the translationsare assigned to the same rank, out of which 29.2% of thetranslations are identical.
TM?LM?EDW achieves highergrades for 27% of the sentences, whereas 5.5% of thebaseline system translations are better.
In total, the trans-lation accuracy improved by 11.9% to 92.7%.
Examplesof differing translation ratings are given in Table 5.One of the reasons for the improved performance isTable 4: Change in Translation AccuracyTM?LM?EDWA B C D ?A 0.592 0.012 0.015 0.010 0.629TM?LM B 0.080 0.024 0.004 0.006 0.114C 0.035 0.012 0.010 0.008 0.065D 0.102 0.033 0.008 0.049 0.192?
0.809 0.081 0.037 0.073Table 5: Translation Examplesinput: Zutsuu ga shimasu asupirin wa arimasu kaTM?LM [D] aspirin do i have a headacheTM?LM?EDW [A] i have a headache do you have any aspirininput: kore wa nani de dekiteimasu kaTM?LM [C] what is this madeTM?LM?EDW [A] what is this made ofinput: nanjikan no okure ni narimasu kaTM?LM [B] how many hours are we behind scheduleTM?LM?EDW [A] how many hours are we delayedinput: watashi wa waruku arimasenTM?LM [A] it ?s not my faultTM?LM?EDW [B] I ?m not badinput: omedetou onnanoko ga umareta sou desu neTM?LM [A] i hear you had a baby girl congratulationsTM?LM?EDW [C] congratulations i heard you were born a boyor a girlinput: ima me o akete mo ii desu kaTM?LM [A] is it all right to open my eyes nowTM?LM?EDW [D] do you mind opening the eyethat the seed sentences obtained by the tf?idf-based re-trieval method are not translations of the input sentence.Moreover, the translations of the MT-based retrievalmethod cover a large variation of expressions due to dif-ferent MT output styles, whereby the reduced quality ofthese seed sentences seems to be successfully compen-sated by the statistical models.
In contrast, the translationexamples retrieved by the tf?idf-based method are quitesimilar to each other.
Thus, local optimization might re-sult in the same decoder output.In addition, the statistical decoder has the tendency toselect shorter translations (4.8 words/sentence for TM?LMand 5.5 words/sentence for TM?LM?EDW, which might in-dicate some problems in the utilized translation models aswell as the language model.
(Watanabe and Sumita, 2003) try to overcome theseproblems by skipping the decoding process of seedsentences whose tf?idf-score indicates an exact matchand output the obtained seed sentence instead.
How-ever, this shortcut method (WER=0.295, BLEU=0.641,ACC=0.898) is out-performed by the proposed rescor-ing method by 2.9% in translation accuracy, because ourmethod takes advantage of translations successfully mod-ified by the decoder and is able to identify and rejectwrongly modified ones.Moreover, the rescoring function is language-independent and thus can be easily applied to otherlanguage-pairs as well.6 ConclusionIn this paper, we proposed an example-based method forselecting translation candidates generated by a statisticaldecoder.
It utilizes translation examples that are similarto the source sentence as the input and validates the de-coder output against its seed sentences in order to iden-tify defective translations.
The revised scoring schemeachieved a translation accuracy of 92.7%, an improve-ment of 11.9% over the baseline system.So far, we treated the statistical decoder as a black-box.
However, further investigations will have to sepa-rate modeling errors and search errors during decodingand compare our findings to advanced statistical model-ing approaches (phrase-based) and other search strate-gies.
Future work will also focus on the integration of theproposed rescoring formula in the decoding process.ReferencesP.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguis-tics, 19(2):263?311.U.
Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-mada.
2001.
Fast decoding and optimal decoding formachine translation.
In Proc.
of ACL 2001, Toulouse,France.M.
Nagao.
1984.
A Framework of a Mechanical Trans-lation between Japanese and English by Analogy Prin-ciple.
A. Elithorn and R. Banerji (eds), Artificial andHuman Intelligence, Amsterdam, North-Holland.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of the 40th ACL, pages 311?318,Philadelphia, USA.K.
Su, M. Wu, and J. Chang.
1992.
A new quantita-tive quality measure for machine translation systems.In Proc.
of the 14th COLING, pages 433?439, Nantes,France.E.
Sumita, S. Yamada, K. Yamamoto, M. Paul, H. Kash-ioka, K. Ishikawa, and S. Shirai.
1999.
Solutionsto problems inherent in spoken-language translation:The ATR-MATRIX approach.
In Proc.
of the MachineTranslation Summit VII, pages 229?235, Singapore.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel conversa-tions in the real world.
In Proc.
of the 3rd LREC, pages147?152, Las Palmas, Spain.R.W.
Wagner.
1974.
The string-to-string correctionproblem.
Journal of the ACM, 21(1):169?173.T.
Watanabe and E. Sumita.
2003.
Example-based de-coding for statistical machine translation.
In Proc.
ofthe Machine Translation Summit IX, pages 410?417,New Orleans, USA.
