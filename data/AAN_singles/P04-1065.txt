FSA: An Efficient and Flexible C++ Toolkit for Finite State AutomataUsing On-Demand ComputationStephan Kanthak and Hermann NeyLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen ?
University of Technology52056 Aachen, Germany{kanthak,ney}@informatik.rwth-aachen.deAbstractIn this paper we present the RWTH FSA toolkit ?
anefficient implementation of algorithms for creatingand manipulating weighted finite-state automata.The toolkit has been designed using the principleof on-demand computation and offers a large rangeof widely used algorithms.
To prove the superiorefficiency of the toolkit, we compare the implemen-tation to that of other publically available toolkits.We also show that on-demand computations help toreduce memory requirements significantly withoutany loss in speed.
To increase its flexibility, theRWTH FSA toolkit supports high-level interfacesto the programming language Python as well as acommand-line tool for interactive manipulation ofFSAs.
Furthermore, we show how to utilize thetoolkit to rapidly build a fast and accurate statisti-cal machine translation system.
Future extensibilityof the toolkit is ensured as it will be publically avail-able as open source software.1 IntroductionFinite-state automata (FSA) methods proved to el-egantly solve many difficult problems in the fieldof natural language processing.
Among the mostrecent ones are full and lazy compilation of thesearch network for speech recognition (Mohri etal., 2000a), integrated speech translation (Vidal,1997; Bangalore and Riccardi, 2000), speech sum-marization (Hori et al, 2003), language modelling(Allauzen et al, 2003) and parameter estimationthrough EM (Eisner, 2001) to mention only a few.From this list of different applications it is clear thatthere is a high demand for generic tools to createand manipulate FSAs.In the past, a number of toolkits have been pub-lished, all with different design principles.
Here, wegive a short overview of toolkits that offer an almostcomplete set of algorithms:?
The FSM LibraryTM from AT&T (Mohri etal., 2000b) is judged the most efficient im-plementation, offers various semirings, on-demand computation and many algorithms, butis available only in binary form with a propri-etary, non commercial license.?
FSA6.1 from (van Noord, 2000) is imple-mented in Prolog.
It is licensed under the termsof the (GPL, 1991).?
The WFST toolkit from (Adant, 2000) is builton top of the Automaton Standard TemplateLibrary (LeMaout, 1998) and uses C++ tem-plate mechanisms for efficiency and flexibil-ity, but lacks on-demand computation.
Alsolicensed under the terms of the (GPL, 1991).This paper describes a highly efficient new im-plementation of a finite-state automata toolkit thatuses on-demand computation.
Currently, it isbeing used at the Lehrstuhl fu?r Informatik VI,RWTH Aachen in different speech recognitionand translation research applications.
The toolkitwill be available under an open source license(GPL, 1991) and can be obtained from our websitehttp://www-i6.informatik.rwth-aachen.de.The remaining part of the paper is organizedas follows: Section 2 will give a short introduc-tion to the theory of finite-state automata to re-call part of the terminology and notation.
We willalso give a short explanation of composition whichwe use as an exemplary object of study in the fol-lowing sections.
In Section 2.3 we will discussthe locality of algorithms defined on finite-state au-tomata.
This forms the basis for implementationsusing on-demand computations.
Then the RWTHFSA toolkit implementation is detailed in Section3.
In Section 4.1 we will compare the efficiency ofdifferent toolkits.
As a showcase for the flexibilitywe show how to use the toolkit to build a statisticalmachine translation system in Section 4.2.
We con-clude the paper with a short summary in Section 5and discuss some possible future extensions in Sec-tion 6.2 Finite-State Automata2.1 Weighted Finite-State TransducerThe basic theory of weighted finite-state automatahas been reviewed in numerous papers (Mohri,1997; Allauzen et al, 2003).
We will introduce thenotation briefly.A semiring (K,?,?, 0, 1) is a structure with aset K and two binary operations ?
and ?
suchthat (K,?, 0) is a commutative monoid, (K,?, 1)is a monoid and ?
distributes over ?
and 0 ?x = x ?
0 = 0 for any x ?
K. We willalso associate the term weights with the elementsof a semiring.
Semirings that are frequently usedin speech recognition are the positive real semir-ing (IR?{??,+?
},?log,+,+?, 0) with a?logb = ?log(e?a + e?b) and the tropical semiring(IR?{??,+?
},min,+,+?, 0) representing thewell-known sum and maximum weighted path cri-teria.A weighted finite-state transducer (Q,?
?{?},?
?
{?
},K,E, i, F, ?, ?)
is a structure with aset Q of states1, an alphabet ?
of input symbols,an alphabet ?
of output symbols, a weight semir-ing K (we assume it k-closed here for some algo-rithms as described in (Mohri and Riley, 2001)), aset E ?
Q ?
(?
?
{?})
?
(?
?
{?})
?K ?
Q ofarcs, a single initial state i with weight ?
and a set offinal states F weighted by the function ?
: F ?
K.To simplify the notation we will also denote withQT and ET the set of states and arcs of a trans-ducer T. A weighted finite-state acceptor is simplya weighted finite-state transducer without the outputalphabet.2.2 CompositionAs we will refer to this example throughout the pa-per we shortly review the composition algorithmhere.
Let T1 : ?????
?
K and T2 : ?????
?
Kbe two transducers defined over the same semiringK.
Their composition T1 ?
T2 realizes the functionT : ?????
?
K and the theory has been describedin detail in (Pereira and Riley, 1996).For simplification purposes, let us assume that theinput automata are ?-free and S = (Q1?Q2,?,?, empty) is a stack of state tuples of T1 and T2 withpush, pop and empty test operations.
A non lazyversion of composition is shown in Figure 1.Composition of automata containing ?
labels ismore complex and can be solved by using an in-termediate filter transducer that also has been de-scribed in (Pereira and Riley, 1996).1we do not restrict this to be a finite set as most algorithmsof the lazy implementation presented in this paper also supporta virtually infinite setT = T1 ?
T2 :i = (i1, i2)S ?
(i1, i2)while not S empty(s1, s2) ?
SQT = QT ?
(s1, s2)foreach (s1, i1, o1, w1, t1) ?
ET1foreach (s2, i2, o2, w2, t2) ?
ET2 with o1 = i2ET = ET ?
((s1, s2), i1, o2, w1 ?
w2, (t1, t2))if (t1, t2) 6?
QT then S ?
(t1, t2)Figure 1: Simplified version of composition (as-sumes ?-free input transducers).What we can see from the pseudo-code above isthat composition uses tuples of states of the two in-put transducers to describe states of the target trans-ducer.
Other operations defined on weighted finite-state automata use different abstract states.
Forexample transducer determinization (Mohri, 1997)uses a set of pairs of states and weights.
However,it is more convenient to use integers as state indicesfor an implementation.
Therefore algorithms usu-ally maintain a mapping from abstract states to in-teger state indices.
This mapping has linear mem-ory requirements of O(|QT |) which is quite attrac-tive, but that depends on the structure of the abstractstates.
Especially in case of determinization wherethe size of an abstract state may vary, the complex-ity is no longer linear in general.2.3 Local AlgorithmsMohri and colleagues pointed out (Mohri et al,2000b) that a special class of transducer algorithmscan be computed on demand.
We will give a moredetailed analysis here.
We focus on algorithms thatproduce a single transducer and refer to them as al-gorithmic transducers.Definition: Let ?
be the input configuration ofan algorithm A(?)
that outputs a single finite-statetransducer T. Additionally, let M : S ?
QT bea one-to-one mapping from the set of abstract statedescriptions S that A generates onto the set of statesof T .
We call A local iff for all states s ?
QT Acan generate a state s of T and all outgoing arcs(s, i, o, w, s?)
?
ET , depending only on its abstractstate M?1(s) and the input configuration ?.With the preceding definition it is quite easy toprove the following lemma:Lemma: An algorithm A that has the local prop-erty can be built on demand starting with the ini-tial state iTA of its associated algorithmic transducerTA.Proof: For the proof it is sufficient to show thatwe can generate and therefore reach all states of TA.Let S be a stack of states of TA that we still haveto process.
Due to the one-to-one mapping M wecan map each state of TA back to an abstract stateof A.
By definition the abstract state is sufficient togenerate the complete state and its outgoing arcs.We then push those target states of all outgoing arcsonto the stack S that have not yet been processed.As TA is finite the traversal ends after all states ofTA as been processed exactly once.
2Algorithmic transducers that can be computedon-demand are also called lazy or virtual transduc-ers.
Note, that due to the local property the set ofstates does not necessarily be finite anymore.3 The ToolkitThe current implementation is the second version ofthis toolkit.
For the first version ?
which was calledFSM ?
we opted for using C++ templates to gain ef-ficiency, but algorithms were not lazy.
It turned outthat the implementation was fast, but many opera-tions wasted a lot of memory as their resulting trans-ducer had been fully expanded in memory.
How-ever, we plan to also make this initial version publi-cally available.The design principles of the second version of thetoolkit, which we will call FSA, are:?
decoupling of data structures and algorithms,?
on-demand computation for increased memoryefficiency,?
low computational costs,?
an abstract interface to alphabets to supportlazy mappings from strings to indices for arclabels,?
an abstract interface to semirings (should be k-closed for at least some algorithms),?
implementation in C++, as it is fast, ubiquitousand well-known by many other researchers,?
easy to use interfaces.3.1 The C++ Library ImplementationWe use the lemma from Section 2.3 to specify aninterface for lazy algorithmic transducers directly.The code written in pseudo-C++ is given in Figure2.
Note that all lazy algorithmic transducers are de-rived from the class Automaton.The lazy interface also has disadvantages.
Thevirtual access to the data structure might slow com-putations down, and obtaining global informationabout the automaton becomes more complicated.For example the size of an automaton can only beclass Automaton {public:struct Arc {StateId target();Weight weight();LabelId input();LabelId output();};struct State {StateId id();Weight weight();ConstArcIterator arcsBegin();ConstArcIterator arcsEnd();};virtual R<Alphabet> inputAlphabet();virtual R<Alphabet> outputAlphabet();virtual StateId initialState();virtual R<State> getState(StateId);};Figure 2: Pseudo-C++ code fragment for the ab-stract datatype of transducers.
Note that R<T>refers to a smart pointer of T.computed by traversing it.
Therefore central al-gorithms of the RWTH FSA toolkit are the depth-first search (DFS) and the computation of stronglyconnected components (SCC).
Efficient versions ofthese algorithms are described in (Mehlhorn, 1984)and (Cormen et al, 1990).It is very costly to store arbitrary types as arc la-bels within the arcs itself.
Therefore the RWTHFSA toolkit offers alphabets that define mappingsbetween strings and label indices.
Alphabets areimplemented using the abstract interface shown inFigure 4.
With alphabets arcs only need to storethe abstract label indices.
The interface for alpha-bets is defined using a single constant: for each la-bel index an alphabet reports it must ensure to al-ways deliver the same symbol on request throughgetSymbol().class Alphabet {public:virtual LabelId begin();virtual LabelId end();virtual LabelId next(LabelId);virtual string getSymbol(LabelId);};Figure 4: Pseudo-C++ code fragment for the ab-stract datatype of alphabets.3.2 AlgorithmsThe current implementation of the toolkit offers awide range of well-known algorithms defined onweighted finite-state transducers:?
basic operationssort (by input labels, output labels or by to-compose(T1, T2) = simple-compose( cache(sort-output(map-output(T1, AT2,I))),cache(sort-input(T2)))Figure 3: Optimized composition where AT2,I denotes the input alphabet of T2.
Six algorithmic transducersare used to gain maximum efficiency.
Mapping of arc labels is necessary as symbol indices may differbetween alphabets.tal arc), map-input and -output labels sym-bolically (as the user expects that two alpha-bets match symbolically, but their mappingto label indices may differ), cache (helps toreduce computations with lazy implementa-tions), topologically-sort states?
rational operationsproject-input, project-output, transpose (alsoknown as reversal: calculates an equivalent au-tomaton with the adjacency matrix being trans-posed), union, concat, invert?
classical graph operationsdepth-first search (DFS), single-source short-est path (SSSP), connect (only keep accessi-ble and coaccessible state), strongly connectedcomponents (SCCs)?
operations on relations of setscompose (filtered), intersect, complement?
equivalence transformationsdeterminize, minimize, remove-epsilons?
search algorithmsbest, n-best?
weight/probability-based algorithmsprune (based on forward/backward state po-tentials), posterior, push (push weights towardinitial/final states), failure (given an accep-tor/transducer defined over the tropical semir-ing converts ?-transitions to failure transitions)?
diagnostic operationscount (counts states, final states, different arctypes, SCCs, alphabet sizes, .
.
.)?
input/output operationssupported input and/or output formats are:AT&T (currently, ASCII only), binary (fast,uses fixed byte-order), XML (slower, any en-coding, fully portable), memory-mapped(also on-demand), dot (AT&T graphviz)We will discuss some details and refer to the pub-lication of the algorithms briefly.
Most of the basicoperations have a straigthforward implementation.As arc labels are integers in the implementationand their meaning is bound to an appropriate sym-bolic alphabet, there is the need for symbolic map-ping between different alphabets.
Therefore thetoolkit provides the lazy map-input and map-outputtransducers, which map the input and output arc in-dices of an automaton to be compatible with the in-dices of another given alphabet.The implementations of all classical graph algo-rithms are based on the descriptions of (Mehlhorn,1984) and (Cormen et al, 1990) and (Mohri and Ri-ley, 2001) for SSSP.
The general graph algorithmsDFS and SCC are helpful in the realisation of manyother operations, examples are: transpose, connectand count.
However, counting the number of statesof an automaton or the number of symbols of an al-phabet is not well-defined in case of an infinite setof states or symbols.SSSP and transpose are the only two algorithmswithout a lazy implementation.
The result of SSSPis a list of state potentials (see also (Mohri and Ri-ley, 2001)).
And a lazy implementation for trans-pose would be possible if the data structures providelists of both successor and predecessor arcs at eachstate.
This needs either more memory or more com-putations and increases the size of the abstract inter-face for the lazy algorithms, so as a compromise weomitted this.The implementations of compose (Pereira andRiley, 1996), determinize (Mohri, 1997), minimize(Mohri, 1997) and remove-epsilons (Mohri, 2001)use more refined methods to gain efficiency.
Alluse at least the lazy cache transducer as they re-fer to states of the input transducer(s) more thanonce.
With respect to the number of lazy trans-ducers involved in computing the result, composehas the most complicated implementation.
Giventhe implementations for the algorithmic transduc-ers cache, map-output, sort-input, sort-output andsimple-compose that assumes arc labels to be com-patible and sorted in order to perform matching asfast as possible, the final implementation of com-pose in the RWTH FSA toolkit is given in figure 3.So, the current implementation of compose uses 6algorithmic transducers in addition to the two inputautomata.
Determinize additionally uses lazy cacheand sort-input transducers.The search algorithms best and n-best are basedon (Mohri and Riley, 2002), push is based on (Mohriand Riley, 2001) and failure mainly uses ideas from(Allauzen et al, 2003).
The algorithms posteriorand prune compute arc posterior probabilities andprune arcs with respect to them.
We believe theyare standard algorithms defined on probabilistic net-works and they were simply ported to the frame-work of weighted finite-state automata.Finally, the RWTH FSA toolkit can be looselyinterfaced to the AT&T FSM LibraryTM throughits ASCII-based input/output format.
In addition,a new XML-based file format primarly designed asbeing human readable and a fast binary file formatare also supported.
All file formats support optionalon-the-fly compression using gzip.3.3 High-Level InterfacesIn addition to the C++ library level interface thetoolkit also offers two high-level interfaces: aPython interface, and an interactive command-lineinterface.The Python interface has been built using theSWIG interface generator (Beazley et al, 1996)and enables rapid development of larger applica-tions without lengthy compilation of C++ code.
Thecommand-line interface comes handy for quicklyapplying various combinations of algorithms totransducers without writing any line of code at all.As the Python interface is mainly identical to theC++ interface we will only give a short impressionof how to use the command-line interface.The command-line interface is a single exe-cutable and uses a stack-based execution model(postfix notation) for the application of operations.This is different from the pipe model that AT&Tcommand-line tools use.
The disadvantage of us-ing pipes is that automata must be serialized andget fully expanded by the next executable in chain.However, an advantage of multiple executables isthat memory does not get fragmented through theinteraction of different algorithms.With the command-line interface, operations areapplied to the topmost transducers of the stack andthe results are pushed back onto the stack again.
Forexample,> fsa A B compose determinize draw -reads A and B from files, calculates the determinizedcomposition and writes the resulting automaton tothe terminal in dot format (which may be piped todot directly).
As you can see from the examplessome operations like write or draw take additionalarguments that must follow the name of the opera-tion.
Although this does not follow the strict postfixdesign, we found it more convenient as these param-eters are not automata.4 Experimental Results4.1 Comparison of ToolkitsA crucial aspect of an FSA toolkit is its computa-tional and memory efficiency.
In this section we willcompare the efficiency of four different implemen-tations of weighted-finite state toolkits, namely:?
RWTH FSA,?
RWTH FSM (predecessor of RWTH FSA),?
AT&T FSM LibraryTM 4.0 (Mohri et al,2000b),?
WFST (Adant, 2000).We opted to not evaluate the FSA6.1 from (vanNoord, 2000) as we found that it is not easy to in-stall and it seemed to be significantly slower thanany of the other implementations.
RWTH FSA andthe AT&T FSM LibraryTM use on-demand com-putations whereas FSM and WFST do not.
As thealgorithmic code between RWTH FSA and its pre-decessor RWTH FSM has not changed much ex-cept for the interface of lazy transducers, we canalso compare lazy versus non lazy implementation.Nevertheless, this direct comparison is also possiblewith RWTH FSA as it provides a static storage classtransducer and a traversing deep copy operation.Table 1 summarizes the tasks used for the eval-uation of efficiency together with the sizes of theresulting transducers.
The exact meaning of the dif-ferent transducers is out of scope of this compari-son.
We simply focus on measuring the efficiency ofthe algorithms.
Experiment 1 is the full expansionof the static part of a speech recognition search net-work.
Experiment 2 deals with a translation prob-lem and splits words of a ?bilanguage?
into singlewords.
The meaning of the transducers used forExperiment 2 will be described in detail in Section4.2.
Experiment 3 is similar to Experiment 1 exceptfor that the grammar transducer is exchanged witha translation transducer and the result represents thestatic network for a speech-to-text translation sys-tem.Table 1: Tasks used for measuring the efficiency ofthe toolkits.
Sizes are given for the resulting trans-ducers (VM = Verbmobil).Experiment states arcs1 VM, HCL ?G 12,203,420 37,174,6842 VM, C1 ?A ?
C2 341,614 832,2253 Eutrans, HCL ?
T 1,201,718 3,572,601All experiments were performed on a PC with a1.2GHz AMD Athlon processor and 2 GB of mem-ory using Linux as operating system.
Table 2 sum-marizes the peak memory usage of the differenttoolkit implementations for the given tasks and Ta-ble 3 shows the CPU usage accordingly.As can be seen from Tables 2 and 3 for all giventasks the RWTH FSA toolkit uses less memory andcomputational power than any of the other toolk-its.
However, it is unclear to the authors why theAT&T LibraryTM is a factor of 1800 slower for ex-periment 2.
The numbers also do not change muchafter additionally connecting the composition result(as in RWTH FSA compose does not connect theresult by default): memory usage rises to 62 MBand execution time increases to 9.7 seconds.
How-ever, a detailed analysis for the RWTH FSA toolkithas shown that the composition task of experiment2 makes intense use of the lazy cache transducerdue to the loop character of the two transducers C1and C2.It can also be seen from the two tables thatthe lazy implementation RWTH FSA uses signif-icantly less memory than the non lazy implemen-tation RWTH FSM and less than half of the CPUtime.
One explanation for this is the poor mem-ory management of RWTH FSM as all interme-diate results need to be fully expanded in mem-ory.
In contrast, due to its lazy transducer inter-face, RWTH FSA may allocate memory for a stateonly once and reuse it for all subsequent calls to thegetState() method.Table 2: Comparison of peak memory usage in MB(?
aborted due to exceeded memory limits).Exp.
FSA FSM AT&T WFST1 360 1700 1500 > 1850?2 59 310 69 > 1850?3 48 230 176 550Table 3: Comparison of CPU time in seconds in-cluding I/O using a 1.2GHz AMD Athlon proces-sor (?
exceeded memory limits: given time indicatespoint of abortion).Exp.
FSA FSM AT&T WFST1 105 203 515 > 40?2 6.5 182 11760 > 64?3 6.6 21 28 38404.2 Statistical Machine TranslationStatistical machine translation may be viewed asa weighted language transduction problem (Vidal,1997).
Therefore it is fairly easy to build a machinetranslation system with the use of weighted finite-state transducers.Let fJ1 and eIi be two sentences from a sourceand target language respectively.
Also assume thatwe have word level alignments A of all sentencesfrom a bilingual training corpus.
We denote withepJp1 the segmentation of a target sentence eI1 intophrases such that fJ1 and epJp1 can be aligned mono-toneously.
This segmentation can be directly calcu-lated from the alignments A.
Then we can formu-late the problem of finding the best translation e?I1 ofa source sentence as follows:e?I1 = argmaxeI1Pr(fJ1 , eI1)?
argmaxA,epJp1Pr(fJ1 , epJp1 )= argmaxA,epJp1?fj :j=1..JPr(fj , epj |f j?11 , epj?1p1 )?
argmaxA,epJp1?fj :j=1..JPr(fj , epj |f j?1j?n, epj?1pj?n)The last line suggests to solve the translationproblem by estimating a language model on a bi-language (see also (Bangalore and Riccardi, 2000;Casacuberta et al, 2001)).
An example of sentencesfrom this bilanguage is given in Figure 5 for thetranslation task Vermobil (German ?
English).
Fortechnical reasons, ?-labels are represented by a $symbol.
Note, that due to the fixed segmentationgiven by the alignments, phrases in the target lan-guage are moved to the last source word of an align-ment block.So, given an appropriate alignment which canbe obtained by means of the pubically availableGIZA++ toolkit (Och and Ney, 2000), the approachis very easy in practice:1.
Transform the training corpus with a givenalignment into the corresponding bilingual cor-pus2.
Train a language model on the bilingual corpus3.
Build an acceptor A from the language modelThe symbols of the resulting acceptor are still a mix-ture of words from the source language and phrasesfrom the target language.
So, we additionally usetwo simple transducers to split these bilingual words(C1 maps source words fj to bilingual words thatstart with fj and C2 maps bilingual words with thetarget sequence epj to the sequences of target wordsthe phrase was made of):4.
Split the bilingual phrases of A into singlewords:T = C1 ?A ?
C2Then the translation problem from above can berewritten using finite-state terminology:dann|$ melde|$ ich|I_am_calling mich|$ noch|$ einmal|once_more .|.11U|eleven Uhr|o?clock ist|is hervorragend|excellent .|.ich|I bin|have da|$ relativ|quite_a_lot_of frei|free_days_then .|.Figure 5: Example corpus for the bilanguage (Verbmobil, German ?
English).Table 4: Translation results for different tasks compared to similar systems using the alignment template(AT) approach (Tests were performed on a 1.2GHz AMD Athlon).Task System Translation WER PER 100-BLEU Memory Time/Sentence[%] [%] [MB] [ms]Eutrans FSA Spanish ?
English 8.12 7.64 10.7 6-8 20AT 8.25 - - - -FUB FSA Italian ?
English 27.0 21.5 37.7 3-5 22AT 23.7 18.1 36.0 - -Verbmobil FSA German ?
English 48.3 41.6 69.8 65-90 460AT 40.5 30.1 62.2 - -PF-Star FSA Italian ?
English 39.8 34.1 58.4 12-15 35AT 36.8 29.1 54.3 - -e?
= project-output(best(f ?
T ))Translation results using this approach are summa-rized in Table 4 and are being compared with resultsobtained using the alignment template approach(Och and Ney, 2000).
Results for both approacheswere obtaining using the same training corpus align-ments.
Detailed task descriptions for Eutrans/FUBand Verbmobil can be found in (Casacuberta et al,2001) and (Zens et al, 2002) respectively.
We usethe usual definitions for word error rate (WER), po-sition independent word error rate (PER) and BLEUstatistics here.For the simpler tasks Eutrans, FUB and PF-Star,the WER, PER and the inverted BLEU statisticsare close for both approaches.
On the German-to-English Verbmobil task the FSA approach suffersfrom long distance reorderings (captured throughthe fixed training corpus segmentation), which is notvery surprising.Although we do not have comparable numbers ofthe memory usage and the translation times for thealignment template approach, resource usage of thefinite-state approach is quite remarkable as we onlyuse generic methods from the RWTH FSA toolkitand full search (i.e.
we do not prune the searchspace).
However, informal tests have shown thatthe finite-state approach uses much less memoryand computations than the current implementationof the alignment template approach.Two additional advantages of finite-state methodsfor translation in general are: the input to the searchalgorithm may also be a word lattice and it is easyto combine speech recognition with translation inorder to do speech-to-speech translation.5 SummaryIn this paper we have given a characterization of al-gorithms that produce a single finite-state automa-ton and bear an on-demand implementation.
Forthis purpose we formally introduced the local prop-erty of such an algorithm.We have described the efficient implementationof a finite-state toolkit that uses the principle oflazy algorithmic transducers for almost all algo-rithms.
Among several publically available toolkits,the RWTH FSA toolkit presented here turned out tobe the most efficient one, as several tests showed.Additionally, with lazy algorithmic transducers wehave reduced the memory requirements and even in-creased the speed significantly compared to a nonlazy implementation.We have also shown that a finite-state automatatoolkit supports rapid solutions to problems fromthe field of natural language processing such as sta-tistical machine translation.
Despite the genericityof the methods, statistical machine translation canbe done very efficiently.6 Shortcomings and Future ExtensionsThere is still room to improve the RWTH FSAtoolkit.
For example, the current implementationof determinization is not as general as described in(Allauzen and Mohri, 2003).
In case of ambiguousinput the algorithm still produces an infinite trans-ducer.
At the moment this can be solved in manycases by adding disambiguation symbols to the in-put transducer manually.As the implementation model is based on virtualC++ methods for all types of objects in use (semir-ings, alphabets, transducers and algorithmic trans-ducers) it should also be fairly easy to add supportfor dynamically loadable objects to the toolkit.Other semirings like the expectation semiring de-scribed in (Eisner, 2001) are supported but not yetimplemented.7 AcknowledgmentThe authors would like to thank Andre Altmann forhis help with the translation experiments.ReferencesAlfred V. Aho and Jeffrey D. Ullman, 1972, The The-ory of Parsing, Translation and Compiling, volume 1,Prentice-Hall, Englewood Cliffs, NJ, 1972.Arnaud Adant, 2000, WFST: A Finite-State Template Li-brary in C++, http://membres.lycos.fr/adant/tfe/.Cyril Allauzen, Mehryar Mohri, and Brian Roark, 2003,Generalized Algorithms for Constructing StatisticalLanguage Models, In Proc.
of the 41st Meeting of theAssociation for Computational Linguistics, Sapporo,Japan, July 2003.Cyril Allauzen and Mehryar Mohri, 2003, General-ized Optimization Algorithm for Speech RecognitionTransducers, In Proc.
of the IEEE Int.
Conf.
onAcoustics, Speech, and Signal Processing, pp.
, HongKong, China, April 2003.Srinivas Bangalore and Giuseppe Riccardi, 2000,Stochastic Finite-State models for Spoken LanguageMachine Translation, In Proc.
of the Workshop onEmbedded Machine Translation Systems, pp.
52?59,2000.David Beazley, William Fulton, Matthias Ko?ppe, LyleJohnson, Richard Palmer, 1996, SWIG - SimplifiedWrapper and Interface Generator, Electronic Docu-ment, http://www.swig.org, February 1996.F.
Casacuberta, D. Llorens, C. Martinez, S. Molau, F.Nevado, H. Ney, M. Pasto, D. Pico, A. Sanchis, E. Vi-dal and J.M.
Vilar, 2001, Speech-to-Speech Transla-tion based on Finite-State Transducer, In Proc.
IEEEInt.
Conf.
on Acoustics, Speech and Signal Process-ing, pp.
613-616, Salt Lake City, Utah, May 2001.Thomas H. Cormen, Charles E. Leiserson and Ronald L.Rivest, 1990, Introductions to Algorithms, The MITPress, Cambridge, MA, 1990.Jason Eisner, 2001, Expectation Semirings: FlexibleEM for Finite-State Transducers, In Proc.
of theESSLLI Workshop on Finite-State Methods in NLP(FSMNLP), Helsinki, August 2001.Free Software Foundation, 1991, GNU GeneralPublic License, Version 2, Electronic Document,http://www.gnu.org/copyleft/gpl.html, June 1991.Takaaki Hori, Chiori Hori and Yasuhiro Minami, 2003,Speech Summarization using Weighted Finite-StateTransducers, In Proc.
of the European Conf.
onSpeech Communication and Technology, Geneva,Switzerland, September 2003.Vincent Le Maout, 1998, ASTL: Automaton Stan-dard Template Library, http://www-igm.univ-mlv.fr/?lemaout/.Kurt Mehlhorn, 1984, Data Structures and Efficient Al-gorithms, Chapter 4, Springer Verlag, EATCS Mono-graphs, 1984, also available from http://www.mpi-sb.mpg.de/m?ehlhorn/DatAlgbooks.html.Mehryar Mohri, 1997, Finite-State Transducers in Lan-guage and Speech Processing, Computational Lin-guistics, 23:2, 1997.Mehryar Mohri, Fernando C.N.
Pereira, and MichaelRiley, 2000, Weighted Finite-State Transducers inSpeech Recognition, In Proc.
of the ISCA Tutorial andResearch Workshop, Automatic Speech Recognition:Challenges for the new Millenium (ASR2000), Paris,France, September 2000.Mehryar Mohri, Fernando C.N.
Pereira, and Michael Ri-ley, 2000, The Design Principles of a Weighted Finite-State Transducer Library, Theoretical Computer Sci-ence, 231:17-32, January 2000.Mehryar Mohri and Michael Riley, 2000, A WeightPushing Algorithm for Large Vocabulary SpeechRecognition, In Proc.
of the European Conf.
onSpeech Communication and Technology, pp.
1603?1606, A?alborg, Denmark, September 2001.Mehryar Mohri, 2001, Generic Epsilon-Removal Algo-rithm for Weighted Automata, In Sheng Yu and An-drei Paun, editor, 5th Int.
Conf., CIAA 2000, LondonOntario, Canada.
volume 2088 of Lecture Notes inComputer Science, pages 230-242.
Springer-Verlag,Berlin-NY, 2001.Mehryar Mohri and Michael Riley, 2002, An EfficientAlgorithm for the N-Best-Strings Problem, In Proc.of the Int.
Conf.
on Spoken Language Processing, pp.1313?1316, Denver, Colorado, September 2002.Franz J. Och and Hermann Ney, 2000, Improved Sta-tistical Alignment Models, In Proc.
of the 38th An-nual Meeting of the Association for ComputationalLinguistics, pp.
440-447, Hongkong, China, October2000.Fernando C.N.
Pereira and Michael Riley, 1996, SpeechRecognition by Composition of Weighted FiniteAutomata, Available from http://xxx.lanl.gov/cmp-lg/9603001, Computation and Language, 1996.Gertjan van Noord, 2000, FSA6 Reference Manual,http://odur.let.rug.nl/v?annoord/Fsa/.Enrique Vidal, 1997, Finite-State Speech-to-SpeechTranslation, In Proc.
of the IEEE Int.
Conf.
on Acous-tics, Speech and Signal Processing, pp.
111?114, Mu-nich, Germany, 1997.Richard Zens, Franz J. Och and H. Ney, 2002, Phrase-Based Statistical Machine Translation, In: M. Jarke,J.
Koehler, G. Lakemeyer (Eds.)
: KI - 2002: Ad-vances in artificial intelligence.
25.
Annual GermanConference on AI, KI 2002, Vol.
LNAI 2479, pp.
18-32, Springer Verlag, September 2002.
