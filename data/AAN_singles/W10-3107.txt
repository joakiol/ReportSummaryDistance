Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 41?45,Uppsala, July 2010.Levels of Certainty in Knowledge-Intensive Corpora:An Initial Annotation StudyAron HenrikssonDSV/KTH-Stockholm UniversitySwedenaronhen@dsv.su.seSumithra VelupillaiDSV/KTH-Stockholm UniversitySwedensumithra@dsv.su.seAbstractIn this initial annotation study, we sug-gest an appropriate approach for determin-ing the level of certainty in text, includingclassification into multiple levels of cer-tainty, types of statement and indicators ofamplified certainty.
A primary evaluation,based on pairwise inter-annotator agree-ment (IAA) using F1-score, is performedon a small corpus comprising documentsfrom the World Bank.
While IAA resultsare low, the analysis will allow further re-finement of the created guidelines.1 IntroductionDespite ongoing efforts to codify knowledge, it isoften communicated in an informal manner.
Inour choice of words and expressions, we implicitlyor explicitly judge the certainty of the knowledgewe wish to convey.
This fact makes it possibleto gauge the reliability of knowledge based on thesubjective perspective of the author.As knowledge is often difficult to ascertain, itseems reasonable to regard knowledge on a contin-uum of varying degrees of certainty, as opposed toa binary (mis)conception.
This corresponds to thenotion of epistemic modality: the degree of confi-dence in, or commitment to, the truth of proposi-tions (Hyland, 1998).
Hedging is a means of af-fecting epistemic modality by qualifying proposi-tions, realized through tentative words and expres-sions such as possibly and tends to.A holistic perspective on certainty?in whichnot only speculation is considered, but also signsof increased certainty?requires a classificationinto various levels.
Applying such an approachto knowledge-intensive corpora, it may in duecourse be possible to evaluate unstructured, infor-mal knowledge.
This would not least be valuableto organizational knowledge management prac-tices, where it could provide a rough indicator ofreliability in internal knowledge audits.2 Related ResearchThe hedging concept was first introduced byLakoff (1973) but has only really come into thespotlight in more recent years.
Studies havemainly taken place in the biomedical domain, withHyland?s (1998) influential work investigating thephenomenon in scientific research articles.
Spec-ulative keywords and negations, along with theirlinguistic scopes, are annotated in the BioScopecorpus by Vincze et al (2008), which contains alarge collection of medical and biological text (sci-entific articles and abstracts, as well as radiologyreports).
After several iterations of refining theirguidelines, they report IAA values ranging from77.6 to 92.37 F1-score for speculative keywords(62.5 and 95.54 F1-score for full scope).
This cor-pus is freely available and has been used for train-ing and evaluation of automatic classifiers, see e.g.Morante and Daelemans (2009).
One of the mainfindings is that hedge cues are highly domain-dependent.
Automatic identification of other pri-vate states, including opinions, represents a sim-ilar task, see e.g.
Wiebe et al (2005).
Diab etal.
(2009) study annotation of committed and non-committed belief and show that automatic taggingof such classes is feasible.
A different annotationapproach is proposed by Rubin et al (2006), inwhich certainty in newspaper articles is catego-rized along four dimensions: level, perspective, fo-cus and time.
Similarly, five dimensions are usedin Wilbur et al (2006) for the creation of an an-notated corpus of biomedical text: focus, polarity,certainty, evidence and directionality.3 MethodBased on previous approaches and an extensive lit-erature review, we propose a set of guidelines that41(1) incorporates some new features and (2) shiftsthe perspective to suit knowledge-intensive cor-pora, e.g.
comprising organizational knowledgedocuments.
Besides categorization into levels ofcertainty, this approach distinguishes between twotypes of statement and underscores the need totake into account words and expressions that addcertainty to a proposition.A small corpus of 10 World Bank documents?a publicly available resource known as Viewpoints(The World Bank Group, 2010)?is subsequentlyannotated in two sets by different annotators.
Thecorpus is from a slightly different domain to thosepreviously targeted and represents an adequate al-ternative to knowledge documents internal to anorganization by fulfilling the criterion of knowl-edge intensity.
The process is carried out in aProte?ge?
plugin: Knowtator (Ogren, 2006).
Pair-wise IAA, measured as F1-score, is calculated toevaluate the feasibility of the approach.Statements are annotated at the clause level, assentences often contain subparts subject to differ-ent levels of certainty.
These are not predefinedand the span of classes is determined by the an-notator.
Furthermore, a distinction is made be-tween different types of statement: statements thatgive an account of something, typically a reportof past events, and statements that express con-crete knowledge claims.
The rationale behind thisdistinction is that text comprises statements thatmake more or less claims of constituting knowl-edge.
Thus, knowledge claims?often less preva-lent than accounts?should be given more weightin the overall assessment, as the application liesin automatically evaluating the reliability of infor-mal knowledge.
Assuming the view of knowledgeand certainty as continuous, it is necessary to dis-cretize that into a number of intervals, albeit morethan two.
Hence, accounts and claims are cate-gorized according to four levels of certainty: verycertain, quite certain, quite uncertain and very un-certain.
In addition to the statement classes, fourindicators make up the total of twelve.
We in-troduce certainty amplifiers, which have receivedlittle attention in previous work.
These are lin-guistic features that add certainty to a statement,e.g.
words like definitely and expressions likewithout a shadow of a doubt.
Hedging indica-tors, on the other hand, have gained much atten-tion recently and signify uncertainty.
The sourcehedge class is applicable to instances where thesource of epistemic judgement is stated explicitly,yet only when it provides a hedging function (e.g.some say).
Modality strengtheners are featuresthat strengthen the effect of epistemic modalitywhen used in conjunction with other (un)certaintyindicators?but alone do not signify any polarityorientation?and may be in the form of vagueness(e.g.
<could be> around that number) or quantitygradations (e.g.
very <sure>).4 ResultsThe corpus contains a total of 772 sentences,which are annotated twice: set #1 by one anno-tator and set #2 by five annotators, annotating twodocuments each.
The statistics in Table 1 showa discrepancy over the two sets in the number ofclassified statements, which is likely due to diffi-culties in determining the scope of clauses.
Thereare likewise significant differences in the propor-tion between accounts and claims, as had been an-ticipated.Accounts ClaimsSet #1 Set #2 Set #1 Set #2726 574 395 393Table 1: Frequencies of accounts and claims.Despite the problem of discriminating between ac-counts and claims, they seem to be susceptible tovarying levels of certainty.
The average distribu-tion of certainty for account statements is depictedin Figure 1.
As expected, an overwhelming ma-jority (87%) of such statements are quite certain,merely relating past events and established facts.Figure 1: Average distribution of certainty in ac-count statements.By comparison, knowledge claims are morecommonly hedged (23%), although the majorityis still quite certain.
Interestingly, claims are alsoexpressed with added confidence more often thanaccounts?around one in every ten claims.42Figure 2: Average distribution of certainty inknowledge claims.As expected, the most common indicator is ofhedging.
Common cues include may, can, might,could, indicate(s), generally and typically.
Manyof these cues are also among the most common inthe biomedical sub-corpus of BioScope (Vincze etal., 2008).
It is interesting to note the fairly com-mon phenomenon of certainty amplifiers.
Theseare especially interesting, as they have not beenstudied much before, although Wiebe et al (2005)incorporate intensity ratings in their annotationscheme.
There is agreement on words like clearly,strongly and especially.Indicator Set #1 Set #2Certainty amplifier 61 29Hedging indicator 151 133Source hedge 0 40Modality strengthener 9 122Table 2: Frequency of indicatorsTo evaluate the approach, we calculate IAA bypairwise F1-score, considering set #1 as the goldstandard, i.e.
as correctly classified, in relation towhich the other subsets are evaluated.
We do thisfor exact matches and partial matches1.
For exactmatches in a single document, the F1-score val-ues range from an extremely low 0.09 to a some-what higher?although still poor?0.52, yieldingan overall average of 0.28.
These results clearlyreflect the difficulty of the task, although one hasto keep in mind the impact of the discrepancy inthe number of annotations.
This is partly reflectedin the higher overall average for partial matches:0.41.Certainty amplifiers and hedging indicatorshave F1-scores that range up to 0.53 and 0.55 re-spectively (ditto for partial matches) in a singledocument.
Over the entire corpus, however, the1Partial matches are calculated on a character level whileexact matches are calculated on a token level.averages come down to 0.27 for certainty ampli-fiers (0.30 for partial matches) and 0.33 for hedg-ing indicators (0.35 for partial matches).Given the poor results, we want to find outwhether the main difficulty is presented by havingto judge certainty according to four levels of cer-tainty, or whether it lies in having to distinguishbetween types of statement.
We therefore general-ize the eight statement-related classes into a singledivision between accounts and claims.
Naturally,the agreement is higher than for any single class,with 0.44 for the former and 0.41 for the latter.A substantial increase is seen in partial matches,with 0.70 for accounts and 0.55 for claims.
Theresults are, however, sufficiently low to concludethat there were real difficulties in distinguishingbetween the two.Statement Type Exact F1Partial F1Account 0.44 0.70Claim 0.41 0.55Table 3: Pairwise IAA per statement type, F1-scores for exact and partial matches.We subsequently generalize the eight classes intofour, according to their level of certainty alone.The results are again low: quite certain yieldsthe highest agreement at 0.47 (0.76 for partialmatches), followed by quite uncertain at 0.24(0.35 for partial matches).
These numbers suggestthat this part of the task is likewise difficult.
Therise in F1-scores for partial matches is noteworthy,as it highlights the problem of different interpreta-tions of clause spans.Certainty Level Exact F1Partial F1Very certain 0.15 0.15Quite certain 0.47 0.76Quite uncertain 0.24 0.35Very uncertain 0.08 0.08Table 4: Pairwise IAA per certainty level, F1-scores for exact and partial matches5 DiscussionIn the guidelines, it is suggested that the level ofcertainty can typically be gauged by identifyingthe number of indicators.
There is, however, a se-rious drawback to this approach.
Hedging indica-tors, in particular, are inherently uncertain to dif-ferent degrees.
Consider the words possibly and43probably.
According to the guidelines, a singleoccurrence of either of these hedging indicatorswould normally render a statement quite uncer-tain.
Giving freer hands to the annotator mightbe a way to evade this problem; however, it is notlikely to lead to any more consistent annotations.Kilicoglu and Bergler (2008) address this by as-signing weights to hedging cues.A constantly recurring bone of contention ispresented by the relationship between certaintyand precision.
One of the hardest judgements tomake is whether imprecision, or vagueness, is asign of uncertainty.
Consider the following exam-ple from the corpus:Cape Verde had virtually no private sec-tor.Clearly, this statement would be more certain if ithad said: Cape Verde had no private sector.
How-ever, virtually no could be substituted with, say,a very small, in which case the statement wouldsurely not be deemed uncertain.
Perhaps precisionis a dimension of knowledge that should be ana-lyzed in conjunction with certainty, but be anno-tated separately.6 ConclusionThere are, of course, a number of ways one cango about annotating the level of certainty froma knowledge perspective.
Some modifications tothe approach described here are essential?whichthe low IAA values are testament to?while oth-ers may be worth exploring.
Below is a selectionof five key changes to the approach that may leadto improved results:1.
Explicate statement types.
Although thereseems to be a useful difference between thetwo types, the distinction needs to be furtherexplicated in the guidelines.2.
Focus on indicators.
It is clear that indicatorscannot be judged in an identical fashion onlybecause they have been identified as signify-ing either certainty or uncertainty.
It is notsimply the number of occurrences of indica-tors that determines the level of certainty butrather how strong those indicators are.
A pos-sible solution is to classify indicators accord-ing to the level of certainty they affect.3.
Discard rare classes.
Very rare phenomenathat do not have a significant impact on theoverall assessment can be sacrificed withoutaffecting the results negatively, which mayalso make the task a little less difficult.4.
Clarify guidelines.
A more general remedyis to clarify further the guidelines, includinginstructions on how to determine the scope ofclauses; alternatively, predefine them.5.
Instruct annotators.
Exposing annotatorsto the task would surely result in increasedagreement, in particular if they agree be-forehand on the distinctions described in theguidelines.
At the same time, you do notwant to steer the process too much.
Perhapsthe task is inherently difficult to define in de-tail.
Studies on how to exploit subjective an-notations might be interesting to explore, seee.g.
Reidsma and op den Akker (2008).In the attempt to gauge the reliability of knowl-edge, incorporating multiple levels of certaintybecomes necessary, as does indicators of in-creased certainty.
Given the similar rates ofagreement on hedging indicators and certaintyamplifiers (0.33 and 0.27 respectively; 0.30 and0.35 for partial matches), the latter class seemto be confirmed.
It is an existing and impor-tant phenomenon, although?like hedging indica-tors?difficult to judge.
Moreover, a differentia-tion between types of statement is important dueto their?to different degrees?varying claims ofconstituting knowledge.
An automatic classifierbuilt on such an approach could be employed withsignificant benefit to organizations actively man-aging their collective knowledge.
The advantageof being aware of the reliability of knowledge areconceivably manifold: it could, for instance, be(1) provided as an attribute to end-users brows-ing documents, (2) used as metadata by searchengines, (3) used in knowledge audits and knowl-edge gap analyses, enabling organizations to learnwhen knowledge in a particular area needs to beconsolidated.
It is, of course, also applicable in amore general information extraction sense: infor-mation that is extracted from text needs to have acertainty indicator attached to it.A dimension other than certainty that has a clearimpact on knowledge is precision.
It would be in-teresting to evaluate the reliability of knowledgebased on a combination of certainty and precision.The annotated World Bank corpus will be madeavailable for further research on the Web.44ReferencesMona T. Diab, Lori Levin, Teruko Mitamura, OwenRambow, Vinodkumar Prabhakaram, and WeiweiGuo.
2009.
Committed belief annotation and tag-ging.
In Proceedings of the Third Linguistic Annota-tion Workshop, ACL-IJCNLP, pages 68?73, Suntec,Singapore, August.
ACL and AFNLP.Ken Hyland.
1998.
Hedging in Scientific Research Ar-ticles.
John Benjamins Publishing Company, Ams-terdam/Philadelphia.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing speculative language in biomedical research ar-ticles: a linguistically motivated perspective.
BMCBioinformatics, 9.George Lakoff.
1973.
Hedges: A study in meaningcriteria and the logic of fuzzy concepts.
Journal ofPhilosophical Logic, 2:458?508.Roser Morante and Walter Daelemans.
2009.
Learningthe scope of hedge cues in biomedical texts.
In Pro-ceedings of the Workshop on BioNLP, pages 28?36,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Philip V. Ogren.
2006.
Knowtator: a prote?ge?
plug-infor annotated corpus construction.
In Proceedings ofthe 2006 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology, pages 273?275, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Dennis Reidsma and Rieks op den Akker.
2008.
Ex-ploiting ?subjective?
annotations.
In HumanJudge?08: Proceedings of the Workshop on Human Judge-ments in Computational Linguistics, pages 8?16,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Victoria L. Rubin, Elizabeth D. Liddy, and NorikoKando.
2006.
Certainty identification in texts: Cat-egorization model and manual tagging results.
InComputing Affect and Attitutde in Text: Theory andApplications.
Springer.The World Bank Group.
2010.
Documents& Reports.
http://go.worldbank.org/3BU2Z3YZ40, Accessed May 13, 2010.Veronika Vincze, Gyo?rgy Szaarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The bio-scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39:165?210.J.
W. Wilbur, A. Rzhetsky, and H. Shatkay.
2006.
Newdirections in biomedical text annotation: definitions,guidelines and corpus construction.
BMC Bioinfor-matics, 7:356+, July.45
