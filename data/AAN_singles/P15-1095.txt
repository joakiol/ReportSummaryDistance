Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 982?991,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsRobust Subgraph Generation Improves Abstract Meaning RepresentationParsingKeenon WerlingStanford Universitykeenon@stanford.eduGabor AngeliStanford Universityangeli@stanford.eduChristopher D. ManningStanford Universitymanning@stanford.eduAbstractThe Abstract Meaning Representation(AMR) is a representation for open-domain rich semantics, with potential usein fields like event extraction and machinetranslation.
Node generation, typicallydone using a simple dictionary lookup, iscurrently an important limiting factor inAMR parsing.
We propose a small setof actions that derive AMR subgraphs bytransformations on spans of text, whichallows for more robust learning of thisstage.
Our set of construction actionsgeneralize better than the previous ap-proach, and can be learned with a sim-ple classifier.
We improve on the previ-ous state-of-the-art result for AMR pars-ing, boosting end-to-end performance by3 F1on both the LDC2013E117 andLDC2014T12 datasets.1 IntroductionThe Abstract Meaning Representation (AMR)(Banarescu et al, 2013) is a rich, graph-based lan-guage for expressing semantics over a broad do-main.
The formalism is backed by a large data-labeling effort, and it holds promise for enabling anew breed of natural language applications rang-ing from semantically aware MT to rich broad-domain QA over text-based knowledge bases.
Fig-ure 1 shows an example AMR for ?he gleefully ranto his dog Rover,?
and we give a brief introductionto AMR in Section 2.
This paper focuses on AMRparsing, the task of mapping a natural languagesentence into an AMR graph.We follow previous work (Flanigan et al, 2014)in dividing AMR parsing into two steps.
Thefirst step is concept identification, which generatesAMR nodes from text, and which we?ll refer to asNER++ (Section 3.1).
The second step is relationFigure 1: The AMR graph for He gleefully ran tohis dog Rover.
We show that improving the gen-eration of low level subgraphs (e.g., Rover gener-ating name:op1???
?Rover?)
significantly improvesend-to-end performance.identification, which adds arcs to link these nodesinto a fully connected AMR graph, which we?llcall SRL++ (Section 3.2).We observe that SRL++ is not the hard part ofAMR parsing; rather, much of the difficulty inAMR is generating high accuracy concept sub-graphs from the NER++ component.
For example,when the existing AMR parser JAMR (Flaniganet al, 2014) is given a gold NER++ output, andmust only perform SRL++ over given subgraphsit scores 80 F1?
nearly the inter-annotator agree-ment of 83 F1, and far higher than its end to endaccuracy of 59 F1.SRL++ within AMR is relatively easy given aperfect NER++ output, because so much pressureis put on the output of NER++ to carry meaningfulinformation.
For example, there?s a strong type-check feature for the existence and type of any arcjust by looking at its end-points, and syntactic de-pendency features are very informative for remov-ing any remaining ambiguity.
If a system is con-982Figure 2: A graphical explanation of our method.
We represent the derivation process for He gleefullyran to his dog Rover.
First the tokens in the sentence are labeled with derivation actions, then theseactions are used to generate AMR subgraphs, which are then stitched together to form a coherent whole.sidering how to link the node run-01 in Figure 1,the verb-sense frame for ?run-01?
leaves very littleuncertainty for what we could assign as an ARG0arc.
It must be a noun, which leaves either he ordog, and this is easily decided in favor of he bylooking for an nsubj arc in the dependency parse.The primary contribution of this paper is a novelapproach to the NER++ task, illustrated in Fig-ure 2.
We notice that the subgraphs aligned to lexi-cal items can often be generated from a small set ofgenerative actions which generalize across tokens.For example, most verbs generate an AMR nodecorresponding to the verb sense of the appropri-ate PropBank frame ?
e.g., run generates run-01in Figure 1.
This allows us to frame the NER++task as the task of classifying one of a small num-ber of actions for each token, rather than choosinga specific AMR subgraph for every token in thesentence.Our approach to the end-to-end AMR parsingtask is therefore as follows: we define an actionspace for generating AMR concepts, and createa classifier for classifying lexical items into oneof these actions (Section 4).
This classifier istrained from automatically generated alignmentsbetween the gold AMR trees and their associatedsentences (Section 5), using an objective which fa-vors alignment mistakes which are least harmful tothe NER++ component.
Finally, the concept sub-graphs are combined into a coherent AMR parseusing the maximum spanning connected subgraphalgorithm of Flanigan et al (2014).We show that our approach provides a largeboost to recall over previous approaches, and thatend to end performance is improved from 59 to62 smatch (an F1measure of correct AMR arcs;see Cai and Knight (2013)) when incorporated intothe SRL++ parser of Flanigan et al (2014).
Whenevaluating the performance of our action classifierin isolation, we obtain an action classification ac-curacy of 84.1%.2 The AMR FormalismAMR is a language for expressing semantics asa rooted, directed, and potentially cyclic graph,where nodes represent concepts and arcs are re-lationships between concepts.
AMR is basedon neo-Davidsonian semantics, (Davidson, 1967;Parsons, 1990).
The nodes (concepts) in an AMRgraph do not have to be explicitly grounded in thesource sentence, and while such an alignment isoften generated to train AMR parsers, it is not pro-vided in the training corpora.
The semantics ofnodes can represent lexical items (e.g., dog), sensetagged lexical items (e.g., run-01), type markers(e.g., date-entity), and a host of other phenomena.The edges (relationships) in AMR describe oneof a number of semantic relationships betweenconcepts.
The most salient of these is seman-tic role labels, such as the ARG0 and destinationarcs in Figure 2.
However, often these arcs define983Figure 3: AMR representation of the word sailor,which is notable for breaking the word up intoa self-contained multi-node unit unpacking thederivational morphology of the word.a semantics more akin to syntactic dependencies(e.g., mod standing in for adjective and adverbialmodification), or take on domain-specific mean-ing (e.g., the month, day, and year arcs of a date-entity).To introduce AMR and its notation in more de-tail, we?ll unpack the translation of the sentence?he gleefully ran to his dog Rover.?
We show inFigure 1 the interpretation of this sentence as anAMR graph.The root node of the graph is labeled run-01,corresponding to the PropBank (Palmer et al,2005) definition of the verb ran.
run-01 has anoutgoing ARG0 arc to a node he, with the usualPropBank semantics.
The outgoing mod edgefrom run-01 to glee takes a general purpose se-mantics corresponding to adjective, adverbial, orother modification of the governor by the depen-dent.
We note that run-01 has a destination arc todog.
The label for destination is taken from a finiteset of special arc sense tags similar to the prepo-sition senses found in (Srikumar, 2013).
The lastportion of the figure parses dog to a node whichserves as a type marker similar to named entitytypes, and Rover into the larger subgraph indicat-ing a concept with name ?Rover.
?2.1 AMR SubgraphsThe mapping from tokens of a sentence to AMRnodes is not one-to-one.
A single token or spanof tokens can generate a subgraph of AMR con-sisting of multiple nodes.
These subgraphs canlogically be considered the expression of a singleconcept, and are useful to treat as such (e.g., seeSection 3.1).Many of these multi-node subgraphs capturestructured data such as time expressions, as in Fig-Figure 4: AMR representation of the span Jan-uary 1, 2008, an example of how AMR can rep-resent structured data by creating additional nodessuch as date-entity to signify the presence of spe-cial structure.ure 4.
In this example, a date-entity node is cre-ated to signify that this cluster of nodes is part ofa structured sub-component representing a date,where the nodes and arcs within the componenthave specific semantics.
This illustrates a broaderrecurring pattern in AMR: an artificial node may,based on its title, have expected children with spe-cial semantics.
A particularly salient example ofthis pattern is the name node (see ?Rover?
in Fig-ure 1) which signifies that all outgoing arcs withlabel op comprise the tokens of a name object.The ability to decouple the meaning representa-tion of a lexical item from its surface form allowsfor rich semantic interpretations of certain con-cepts in a sentence.
For example, the token sailoris represented in Figure 3 by a concept graph rep-resenting a person who performs the action sail-01.
Whereas often the AMR node aligned to aspan of text is a straightforward function of thetext, these cases remain difficult to capture in aprincipled way beyond memorizing mappings be-tween tokens and subgraphs.3 Task DecompositionTo the best of our knowledge, the JAMR parseris the only published end-to-end AMR parser atthe time of publication.
An important insight inJAMR is that AMR parsing can be broken intotwo distinct tasks: (1) NER++ (concept identifi-cation): the task of interpreting what entities arebeing referred to in the text, realized by gener-ating the best AMR subgraphs for a given set oftokens, and (2) SRL++ (relation identification):the task of discovering what relationships exist be-tween entities, realized by taking the disjoint sub-graphs generated by NER++ and creating a fully-connected graph.
We describe both tasks in more984detail below.3.1 NER++Much of the difficulty of parsing to AMR lies ingenerating local subgraphs representing the mean-ing of token spans.
For instance, the formalismimplicitly demands rich notions of NER, lemma-tization, word sense disambiguation, number nor-malization, and temporal parsing; among others.To illustrate, a correct parse of the sentence in Fig-ure 2 requires lemmatization (gleefully ?
glee),word sense tagging (run?
run-01), and open do-main NER (i.e., Rover), Furthermore, many of thegenerated subgraphs (e.g., sailor in Figure 3) haverich semantics beyond those produced by standardNLP systems.Formally, NER++ is the task of generating adisjoint set of subgraphs representing the mean-ings of localized spans of words in the sentence.For NER++, JAMR uses a simple Viterbi sequencemodel to directly generate AMR-subgraphs frommemorized mappings of text spans to subgraphs.This paper?s main contribution, presented in Sec-tion 4, is to make use of generative actions to gen-erate these subgraphs, rather than appealing to amemorized mapping.3.2 SRL++The second stage of the AMR decomposition con-sists of generating a coherent graph from the set ofdisjoint subgraphs produced by NER++.
WhereasNER++ produces subgraphs whose arcs encodedomain-specific semantics (e.g., month), the arcsin SRL++ tend to have generally applicable se-mantics.
For example, the many arcs encode con-ventional semantic roles (e.g., ARG0 and desti-nation in Figure 2), or a notion akin to syntac-tic dependencies (e.g., mod and poss in Figure 2).For SRL++, JAMR uses a variation of the max-imum spanning connected graph algorithm aug-mented by dual decomposition to impose linguis-tically motivated constraints on a maximum likeli-hood objective.4 A Novel NER++ MethodThe training sets currently available for AMR arenot large.
To illustrate, 38% of the words in theLDC2014E113 dev set are unseen during trainingtime.
With training sets this small, memorization-based approaches are extremely brittle.
We re-move much of the necessity to memorize map-pings in NER++ by partitioning the AMR sub-graph search space in terms of the actions neededto derive a node from its aligned token.
At testtime we do a sequence labeling of input tokenswith these actions, and then deterministically de-rive the AMR subgraphs from spans of tokensby applying the transformation decreed by theiractions.
We explain in Section 4.1 how exactlywe manage this partition, and in Section 4.3 howwe create training data from existing resources tosetup and train an action-type classifier.4.1 Derivation actionsWe partition the AMR subgraph space into a set of9 actions, each corresponding to an action that willbe taken by the NER++ system if a token receivesthis classification.IDENTITY This action handles the commoncase that the title of the node corresponding to atoken is identical to the source token.
To executethe action, we take the lowercased version of thetoken to be the title of the corresponding node.NONE This action corresponds to ignoring thistoken, in the case that the node should not align toany corresponding AMR fragment.VERB This action captures the verb-sense dis-ambiguation feature of AMR.
To execute on a to-ken, we find the most similar verb in PropBankbased on Jaro-Winkler distance, and adopt its mostfrequent sense.
This serves as a reasonable base-line for word sense disambiguation, although ofcourse accuracy would be expected to improve ifa sophisticated system were incorporated.VALUE This action interprets a token by its in-teger value.
The AMR representation is sensitiveto the difference between a node with a title of 5(the integer value) and ?5?
or ?five?
?
the stringvalue.
This is a rare action, but is nonetheless dis-tinct from any of the other classes.
We execute thisaction by extracting an integer value with a regexbased number normalizer, and using the result asthe title of the generated node.LEMMA AMR often performs stemming andpart-of-speech transformations on the source to-ken in generating a node.
For example, we getglee from gleefully.
We capture this by a LEMMAaction, which is executed by using the lemma ofthe source token as the generated node title.
Notethat this does not capture all lemmatizations, as985there are often discrepancies between the lemmagenerated by the lemmatizer and the correct AMRlemma.NAME AMR often references names with aspecial structured data type: the name construc-tion.
For example, Rover in Figure 1.
We cancapture this phenomenon on unseen names by at-taching a created name node to the top of a span.PERSON A variant of the NAME action, thisaction produces a subgraph identical to the NAMEaction, but adds a node person as a parent.
Thisis, in effect, a name node with an implicit entitytype of person.
Due to discrepancies between theoutput of our named entity tagger and the richerAMR named entity ontology, we only apply thistag to the person named entity tag.DATE The most frequent of the structured datatype in the data, after name, is the date-entity con-struction (for an example see Figure 4).
We de-terministically take the output of SUTime (Changand Manning, 2012) and convert it into the date-entity AMR representation.DICT This class serves as a back-off for theother classes, implementing an approach similarto Flanigan et al (2014).
In particular, we mem-orize a simple mapping from spans of text (suchas sailor) to their corresponding most frequentlyaligned AMR subgraphs in the training data (i.e.,the graph in Figure 3).
See Section 5 for detailson the alignment process.
At test time we can do alookup in this dictionary for any element that getslabeled with a DICT action.
If an entry is notfound in the mapping, we back off to the secondmost probable class proposed by the classifier.It is worth observing at this point that our ac-tions derive much of their power from the similar-ity between English words and their AMR coun-terparts; creating an analogue of these actions forother languages remains an open problem.4.2 Action ReliabilityIn many cases, multiple actions could yield thesame subgraph when applied to a node.
In thissection we introduce a method for resolving thisambiguity based on comparing the reliability withwhich actions generate the correct subgraph, anddiscuss implications.Even given a perfect action classification fora token, certain action executions can introduceFigure 5: Reliability of each action.
The top roware actions which are deterministic; the secondrow occasionally produce errors.
DICT is the leastpreferred action, with a relatively high error rate.errors.
Some of our actions are entirely deter-ministic in their conversion from the word to theAMR subgraph (e.g., IDENTITY), but others areprone to making mistakes in this conversion (e.g.,VERB, DICT).
We define the notion of action re-liability as the probability of deriving the correctnode from a span of tokens, conditioned on hav-ing chosen the correct action.To provide a concrete example, our dictionarylookup classifier predicts the correct AMR sub-graph 67% of the time on the dev set.
We thereforedefine the reliability of the DICT action as 0.67.In contrast to DICT, correctly labeling a node asIDENTITY, NAME, PERSON, and NONE haveaction reliability of 1.0, since there is no ambigu-ity in the node generation once one of those ac-tions have been selected, and we are guaranteed togenerate the correct node given the correct action.We can therefore construct a hierarchy of reli-ability (Figure 5) ?
all else being equal, we pre-fer to generate actions from higher in the hierar-chy, as they are more likely to produce the cor-rect subgraph.
This hierarchy is useful in resolv-ing ambiguity throughout our system.
During thecreation of training data for our classifier (Sec-tion 4.3) from our aligner, when two actions couldboth generate the aligned AMR node we prefer themore reliable one.
In turn, in our aligner we biasalignments towards those which generating morereliable action sequences as training data (see Sec-tion 5).The primary benefit of this action-basedNER++ approach is that we can reduce the us-age of low reliability actions, like DICT.
Theapproach taken in Flanigan et al (2014) can be986Action # Tokens % TotalNONE 41538 36.2DICT 30027 26.1IDENTITY 19034 16.6VERB 11739 10.2LEMMA 5029 4.5NAME 4537 3.9DATE 1418 1.1PERSON 1336 1.1VALUE 122 0.1Table 1: Distribution of action types in theproxy section of the newswire section of theLDC2014T12 dataset, generated from automati-cally aligned data.Input token; word embeddingLeft+right token / bigramToken length indicatorToken starts with ?non?POS; Left+right POS / bigramDependency parent token / POSIncoming dependency arcBag of outgoing dependency arcsNumber of outgoing dependency arcsMax Jaro-Winkler to any lemma in PropBankOutput tag of the VERB action if appliedOutput tag of the DICT action if appliedNER; Left+right NER / bigramCapitalizationIncoming prep * or appos + parent has NERToken is pronounToken is part of a coref chainToken pronoun and part of a coref chainTable 2: The features for the NER++ maxent clas-sifier.thought of as equivalent to classifying every tokenas the DICT action.We analyze the empirical distribution of actionsin our automatically aligned corpus in Table 1.The cumulative frequency of the non-DICT ac-tions is striking: we can generate 74% of the to-kens with high reliability (p ?
0.9) actions.
In thislight, it is unsurprising that our results demonstratea large gain in recall on the test set.4.3 Training the Action ClassifierGiven a set of AMR training data, in the form of(graph, sentence) pairs, we first induce alignmentsfrom the graph nodes to the sentence (see Sec-tion 5).
Formally, for every node niin the AMRgraph, alignment gives us some token sj(at thejth index in the sentence) that we believe gener-ated the node ni.Then, for each action type, we can ask whetheror not that action type is able to take token sjandcorrectly generate ni.
For concreteness, imaginethe token sjis running, and the node nihas thetitle run-01.
The two action types we find that areable to correctly generate this node are DICT andVERB.
We choose the most reliable action typeof those available (see Figure 5) to generate theobserved node ?
in this case, VERB.In cases where an AMR subgraph is generatedfrom multiple tokens, we assign the action label toeach token which generates the subgraph.
Each ofthese tokens are added to the training set; at testtime, we collapse sequences of adjacent identicalaction labels, and apply the action once to the re-sulting token span.Inducing the most reliable action (according tothe alignments) for every token in the training cor-pus provides a supervised training set for our ac-tion classifier, with some noise introduced by theautomatically generated alignments.
We then traina simple maxent classifier1to make action deci-sions at each node.
At test time, the classifier takesas input a pair ?i, S?, where i is the index of the to-ken in the input sentence, and S is a sequence to-kens representing the source sentence.
It then usesthe features in Table 2 to predict the actions to takeat that node.5 Automatic Alignment of Training DataAMR training data is in the form of bi-text, wherewe are given a set of (sentence, graph) pairs, withno explicit alignments between them.
We wouldlike to induce a mapping from each node in theAMR graph to the token it represents.
It is per-fectly possible for multiple nodes to align to thesame token ?
this is the case with sailors, for in-stance.It is not possible, within our framework, to rep-resent a single node being sourced from multi-ple tokens.
Note that a subgraph can consist ofmany individual nodes; in cases where a subgraphshould align to multiple tokens, we generate analignment from the subgraph?s nodes to the associ-ated tokens in the sentence.
It is empirically veryrare for a subgraph to have more nodes than thetoken span it should align to.There have been two previous attempts at pro-ducing automatic AMR alignments.
The first was1A sequence model was tried and showed no improve-ment over a simple maxent classifier.987published as a component of JAMR, and used arule-based approach to perform alignments.
Thiswas shown to work well on the sample of 100hand-labeled sentences used to develop the sys-tem.
Pourdamghani et al (2014) approached thealignment problem in the framework of the IBMalignment models.
They rendered AMR graphs astext, and then used traditional machine translationalignment techniques to generate an alignment.We propose a novel alignment method, sinceour decomposition of the AMR node generationprocess into a set of actions provides an additionalobjective for the aligner to optimize, in addition tothe accuracy of the alignment itself.
We would liketo produce the most reliable sequence of actionsfor the NER++ model to train from, where reliableis taken in the sense defined in Section 4.2.
To givean example, a sequence of all DICT actions couldgenerate any AMR graph, but is very low reliabil-ity.
A sequence of all IDENTITY actions couldonly generate one set of nodes, but does it withabsolute certainty.We formulate this objective as a Boolean LPproblem.
Let Q be a matrix in {0, 1}|N|?|S|ofBoolean constrained variables, where N are thenodes in an AMR graph, and S are the tokens inthe sentence.
The meaning of Qi,j= 1 can beinterpreted as node nihaving being aligned to to-ken sj.
Furthermore, let V be a matrix T|N|?|S|,where T is the set of NER++ actions from Sec-tion 4.
Each matrix element Vi,jis assigned themost reliable action which would generate nodenifrom token sj.
We would like to maximize theprobability of the actions collectively generating aperfect set of nodes.
This can be formulated lin-early by maximizing the log-likelihood of the ac-tions.
Let the function REL(l) be the reliability ofaction l (probability of generating intended node).Our objective can then be formulated as follows:maxQ?i,jQi,j[log(REL(Vi,j)) + ?Ei,j] (1)s.t.
?jQi,j= 1 ?i (2)Qk,j+Ql,j?
1 ?k, l, j; nk= nl(3)where E is the Jaro-Winkler similarity between thetitle of the node i and the token j, ?
is a hyper-parameter (set to 0.8 in our experiments), and theoperator = denotes that two nodes in the AMRgraph are both not adjacent and do not have thesame title.The constraint (2), combined with the binaryconstraint on Q, ensures that every node in thegraph is aligned to exactly one token in the sourcesentence.
The constraint (3) ensures that only ad-jacent nodes or nodes that share a title can refer tothe same token.The objective value penalizes alignments whichmap to the unreliable DICT tag, while rewardingalignments with high overlap between the title ofthe node and the token.
Note that most incorrectalignments fall into the DICT class by default, asno other action could generate the correct AMRsubgraph.
Therefore, if there exists an alignmentthat would consume the token using another ac-tion, the optimization prefers that alignment.
TheJaro-Winkler similarity term, in turn, serves asa tie-breaker between equally (un)reliable align-ments.There are many packages which can solvethis Boolean LP efficiently.
We used Gurobi(Gurobi Optimization, 2015).
Given a matrix Qthat maximizes our objective, we can decode oursolved alignment as follows: for each i, align nito the j s.t.
Qi,j= 1.
By our constraints, exactlyone such j must exist.6 Related WorkPrior work in AMR and related formalisms in-clude Jones et al (2012), and Flanigan et al(2014).
Jones et al (2012), motivated by appli-cations in Machine Translation, proposed a graph-ical semantic meaning representation that predatesAMR, but is intimately related.
They proposea hyper-edge replacement grammar (HRG) ap-proach to parsing into and out of this graphicalsemantic form.
Flanigan et al (2014) forms thebasis of the approach of this paper.
Their systemintroduces the two-stage approach we use: theyimplement a rule-based alignment to learn a map-ping from tokens to subgraphs, and train a vari-ant of a maximum spanning tree parser adapted tographs and with additional constraints for their re-lation identifications (SRL++) component.
Wanget al (2015) uses a transition based algorithmto transform dependency trees into AMR parses.They achieve 64/62/63 P/R/F1with contributionsroughly orthogonal to our own.
Their transforma-tion action set could be easily augmented by therobust subgraph generation we propose here, al-though we leave this to future work.Beyond the connection of our work with Flani-988gan et al (2014), we note that the NER++ com-ponent of AMR encapsulates a number of lex-ical NLP tasks.
These include named entityrecognition (Nadeau and Sekine, 2007; Finkel etal., 2005), word sense disambiguation (Yarowsky,1995; Banerjee and Pedersen, 2002), lemmatiza-tion, and a number of more domain specific tasks.For example, a full understanding of AMR re-quires normalizing temporal expressions (Verha-gen et al, 2010; Str?otgen and Gertz, 2010; Changand Manning, 2012).In turn, the SRL++ facet of AMR takes manyinsights from semantic role labeling (Gildea andJurafsky, 2002; Punyakanok et al, 2004; Sriku-mar, 2013; Das et al, 2014) to capture the rela-tions between verbs and their arguments.
In addi-tion, many of the arcs in AMR have nearly syntac-tic interpretations (e.g., mod for adjective/adverbmodification, op for compound noun expressions).These are similar to representations used in syn-tactic dependency parsing (de Marneffe and Man-ning, 2008; McDonald et al, 2005; Buchholz andMarsi, 2006).More generally, parsing to a semantic represen-tation is has been explored in depth for when therepresentation is a logical form (Kate et al, 2005;Zettlemoyer and Collins, 2005; Liang et al, 2011).Recent work has applied semantic parsing tech-niques to representations beyond lambda calculusexpressions.
For example, work by Berant et al(2014) parses text into a formal representation ofa biological process.
Hosseini et al (2014) solvesalgebraic word problems by parsing them into astructured meaning representation.
In contrast tothese approaches, AMR attempts to capture opendomain semantics over arbitrary text.Interlingua (Mitamura et al, 1991; Carbonell etal., 1999; Levin et al, 1998) are an important in-spiration for decoupling the semantics of the AMRlanguage from the surface form of the text beingparsed; although, AMR has a self-admitted En-glish bias.7 ResultsWe present improvements in end-to-end AMRparsing on two datasets using our NER++ compo-nent.
Action type classifier accuracy on an auto-matically aligned corpus and alignment accuracyon a small hand-labeled corpus are also reported.Dataset System P R F12014T12JAMR 67.1 53.2 59.3Our System 66.6 58.3 62.22013E117JAMR 66.9 52.9 59.1Our System 65.9 59.0 62.3Table 3: Results on two AMR datasets for JAMRand our NER++ embedded in the JAMR SRL++component.
Note that recall is consistently higheracross both datasets, with only a small loss in pre-cision.7.1 End-to-end AMR ParsingWe evaluate our NER++ component in the contextof end-to-end AMR parsing on two corpora: thenewswire section of LDC2014T12 and the splitgiven in Flanigan et al (2014) of LDC2013E117,both consisting primarily of newswire.
We com-pare two systems: the JAMR parser (Flaniganet al, 2014),2and the JAMR SRL++ componentwith our NER++ approach.AMR parsing accuracy is measured with a met-ric called smatch (Cai and Knight, 2013), whichstands for ?s(emantic) match.?
The metric is the F1of a best-match between triples implied by the tar-get graph, and triples in the parsed graph ?
that is,the set of (parent, edge, child) triples in the graph.Our results are given in Table 3.
We reportmuch higher recall numbers on both datasets, withonly small (?
1 point) loss in precision.
Thisis natural considering our approach.
A betterNER++ system allows for more correct AMR sub-graphs to be generated ?
improving recall ?
butdoes not in itself necessarily improve the accuracyof the SRL++ system it is integrated in.7.2 Component AccuracyWe evaluate our aligner on a small set of 100 hand-labeled alignments, and evaluate our NER++ clas-sifier on automatically generated alignments overthe whole corpus,On a hand-annotated dataset of 100 AMRparses from the LDC2014T12 corpus,3our alignerachieves an accuracy of 83.2.
This is a measure-ment of the percentage of AMR nodes that arealigned to the correct token in their source sen-tence.
Note that this is a different metric than the2Available at https://github.com/jflanigan/jamr.3Our dataset is publicly available at http://nlp.stanford.edu/projects/amr989precision/recall of prior work on alignments, andis based on both a different alignment dataset andsubtly different alignment annotation scheme.
Inparticular, we require that every AMR node alignsto some token in the sentence, which forces thesystem to always align nodes, even when unsure.A standard semantics and annotation guideline forAMR alignment is left for future work; our accu-racy should be considered only an informal metric.We find our informativeness-based alignmentobjective slightly improves end-to-end perfor-mance when compared to the rule-based approachof (Flanigan et al, 2014), improving F1by roughly1 point (64/59/61 P/R/F1to 65/59/62 P/R/F1).On the automatic alignments over theLDC2014T12 corpus, our action classifierachieved a test accuracy of 0.841.
The classifier?smost common class of mistakes are incorrectDICT classifications.
It is reassuring that some ofthese errors can be recovered from by the na?
?vedictionary lookup finding the correct mapping.The DICT action lookup table achieved an ac-curacy of 0.67.
This is particularly impressivegiven that our model moves many of the difficultsemantic tasks onto the DICT tag, and that thislookup does not make use of any learning beyonda simple count of observed span to subgraph map-pings.8 ConclusionWe address a key challenge in AMR parsing: thetask of generating subgraphs from lexical itemsin the sentence.
We show that a simple classi-fier over actions which generate these subgraphsimproves end-to-end recall for AMR parsing withonly a small drop in precision, leading to an over-all gain in F1.
A clear direction of future work isimproving the coverage of the defined actions.
Forexample, a richer lemmatizer could shift the bur-den of lemmatizing unknown words into the AMRlemma semantics and away from the dictionarylookup component.
We hope our decompositionprovides a useful framework to guide future workin NER++ and AMR in general.AcknowledgmentsWe thank the anonymous reviewers for theirthoughtful feedback.
Stanford University grate-fully acknowledges the support of the Defense Ad-vanced Research Projects Agency (DARPA) DeepExploration and Filtering of Text (DEFT) Programunder Air Force Research Laboratory (AFRL)contract no.
FA8750-13-2-0040.
Any opinions,findings, and conclusion or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the view of the DARPA,AFRL, or the US government.ReferencesLaura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.
Proc.
Linguistic Annotation Work-shop.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted Lesk algorithm for word sense disambigua-tion using wordnet.
In Computational linguisticsand intelligent text processing.Jonathan Berant, Vivek Srikumar, Pei-Chun Chen,Brad Huang, Christopher D Manning, Abby Van-der Linden, Brittany Harding, and Peter Clark.2014.
Modeling biological processes for readingcomprehension.
In Proc.
EMNLP.Sabine Buchholz and Erwin Marsi.
2006.
CONLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, pages 149?164.Association for Computational Linguistics.Shu Cai and Kevin Knight.
2013.
Smatch: an evalua-tion metric for semantic feature structures.
In ACL(2), pages 748?752.Jaime G Carbonell, Teruko Mitamura, and Eric H Ny-berg.
1999.
The KANT perspective: A critiqueof pure transfer (and pure interlingua, pure statis-tics,...).Angel Chang and Chris Manning.
2012.
SUTIME: alibrary for recognizing and normalizing time expres-sions.
In Language Resources and Evaluation.Dipanjan Das, Desai Chen, Andr?e FT Martins, NathanSchneider, and Noah A Smith.
2014.
Frame-semantic parsing.
Computational Linguistics,40(1):9?56.Donald Davidson.
1967.
The logical form of actionsentences.
In Nicholas Rescher, editor, The Logicof Decision and Action, pages 81?120.
University ofPittsburgh Press, Pittsburgh, PA.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation.990Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In ACL.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,Chris Dyer, and Noah A Smith.
2014.
A discrim-inative graph-based parser for the abstract meaningrepresentation.
In ACL.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational linguis-tics, 28(3):245?288.Inc.
Gurobi Optimization.
2015.
Gurobi optimizer ref-erence manual.Mohammad Javad Hosseini, Hannaneh Hajishirzi,Oren Etzioni, and Nate Kushman.
2014.
Learningto solve arithmetic word problems with verb catego-rization.
In EMNLP.Bevan Jones, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, and Kevin Knight.
2012.Semantics-based machine translation with hyper-edge replacement grammars.
In COLING, pages1359?1376.Rohit J. Kate, Yuk Wah Wong, and Raymond J.Mooney.
2005.
Learning to transform natural toformal languages.
In AAAI, Pittsburgh, PA.Lori S Levin, Donna Gates, Alon Lavie, and AlexWaibel.
1998.
An interlingua based on domainactions for machine translation of task-oriented di-alogues.
In ICSLP, volume 98, pages 1155?1158.P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InACL.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In ACL, Morristown, NJ, USA.Teruko Mitamura, Eric H Nyberg, and Jaime G Car-bonell.
1991.
An efficient interlingua translationsystem for multi-lingual document production.
Pro-ceedings of Machine Translation Summit III.David Nadeau and Satoshi Sekine.
2007.
A sur-vey of named entity recognition and classification.Lingvisticae Investigationes, 30(1):3?26.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational linguistics,31(1):71?106.Terence Parsons.
1990.
Events in the Semantics of En-glish: A study in subatomic semantics.
MIT Press,Cambridge, MA.Nima Pourdamghani, Yang Gao, Ulf Hermjakob, andKevin Knight.
2014.
Aligning english strings withabstract meaning representation graphs.
In EMNLP.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic role labeling via integerlinear programming inference.
In Proceedings ofthe 20th international conference on ComputationalLinguistics, page 1346.
Association for Computa-tional Linguistics.Vivek Srikumar.
2013.
The semantics of role label-ing.
Ph.D. thesis, University of Illinois at Urbana-Champaign.Jannik Str?otgen and Michael Gertz.
2010.
Heideltime:High quality rule-based extraction and normaliza-tion of temporal expressions.
In Proceedings of the5th International Workshop on Semantic Evaluation,Sem-Eval.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:TempEval-2.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, Uppsala,Sweden.Chuan Wang, Nianwen Xue, and Sameer Pradhan.2015.
A transition-based algorithm for amr parsing.In NAACL-HLT.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In ACL.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In UAI.
AUAI Press.991
