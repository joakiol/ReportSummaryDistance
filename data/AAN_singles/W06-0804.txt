Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 25?32,Sydney, July 2006. c?2006 Association for Computational LinguisticsHow to Find Better Index Terms Through CitationsAnna RitchieUniversity of CambridgeComputer Laboratory15 J J Thompson AvenueCambridge, CB3 0FD, U.K.ar283@cl.cam.ac.ukSimone TeufelUniversity of CambridgeComputer Laboratory15 J J Thompson AvenueCambridge, CB3 0FD, U.K.sht25@cl.cam.ac.ukStephen RobertsonMicrosoft Research LtdRoger Needham House7 J J Thomson AvenueCambridge, CB3 0FB, U.K.ser@microsoft.comAbstractWe consider the question of how informa-tion from the textual context of citationsin scientific papers could improve index-ing of the cited papers.
We first present ex-amples which show that the context shouldin principle provide better and new indexterms.
We then discuss linguistic phenom-ena around citations and which type ofprocessing would improve the automaticdetermination of the right context.
Wepresent a case study, studying the effectof combining the existing index terms ofa paper with additional terms from papersciting that paper in our corpus.
Finally, wediscuss the need for experimentation forthe practical validation of our claim.1 IntroductionInformation Retrieval (IR) is an established fieldand, today, the ?conventional?
IR task is embodiedby web searching.
IR is mostly term-based, re-lying on the words within documents to describethem and, thence, try to determine which docu-ments are relevant to a given user query.
There aretheoretically motivated and experimentally vali-dated techniques that have become standard in thefield.
An example is the Okapi model; a prob-abilistic function for term weighting and docu-ment ranking (Spa?rck Jones, Walker & Robertson2000).
IR techniques using such statistical mod-els almost always outperform more linguisticallybased ones.
So, as statistical models are developedand refined, it begs the question ?Can Computa-tional Linguistics improve Information Retrieval?
?Our particular research involves IR on scien-tific papers.
There are definite parallels betweenthe web and scientific literature, such as hyper-links between webpages alongside citation linksbetween papers.
However, there are also funda-mental differences, like the greater variability ofwebpages and the independent quality control ofacademic texts through the peer review process.The analogy between hyperlinks and citations it-self is not perfect: whereas the number of hyper-links varies greatly from webpage to webpage, thenumber of citations in papers is more constrained,due to the combination of strict page limits, theneed to cite to show awareness of other work andthe need to conserve space by including only themost relevant citations.
Thus, while some aspectsof web-based techniques will carry across to thecurrent research domain, others will probably not.We are interested in investigating which lessonslearned from web IR can successfully be appliedto this slightly different domain.2 Index Terms Through Link StructureWe aim to improve automatic indexing of scien-tific papers by finding additional index terms out-side of the documents themselves.
In particular,we believe that good index terms can be found byfollowing the link structure between documents.2.1 HyperlinksThere is a wealth of literature on exploiting linkstructure between web documents for IR, includ-ing the ?sharing?
of index terms between hyper-linked pages.
Bharat & Mihaila (2001), for in-stance, propagate title and header terms to thepointed-to page, while Marchiori (1997) recur-sively augments the textual content of a page withall the text of the pages it points to.Research has particularly concentrated on an-chor text as a good place to find index terms, i.e.,25the text enclosed in the ?a?
tags of the HTMLdocument.
It is a well-documented problem thatwebpages are often poorly self-descriptive (e.g.,Brin & Page 1998, Kleinberg 1999).
For in-stance, www.google.com does not contain thephrase search engine.
Anchor text, on the otherhand, is often a higher-level description of thepointed-to page.
Davison (2000) provides a gooddiscussion of just how well anchor text does thisand provides experimental results to back thisclaim.
Thus, beginning with McBryan (1994),there is a trend of propagating anchor text alongits hyperlink to associate it with the linked page,as well as that in which it is found.
Google, forexample, includes anchor text as index terms forthe linked page (Brin & Page 1998).Extending beyond anchor text, Chakrabartiet al (1998) look for topic terms in a windowof text around hyperlinks and weight that link ac-cordingly, in the framework of a link structure al-gorithm, HITS (Kleinberg 1999).2.2 CitationsThe anchor text phenomenon is also observed withcitations: they are introduced purposefully along-side some descriptive reference to the cited doc-ument.
Thus, this text should contain good in-dex terms for the cited document.
In the fol-lowing sections, we motivate the use of referenceterms as index terms for cited documents, firstly,with some citation examples and, secondly, by dis-cussing previous work.Examples: Reference Terms as Index TermsFigure 1 shows some citations that exemplifywhy reference terms should be good index termsfor the cited document.
(1) is an example of a ci-tation with intuitively good index terms (those un-derlined) for the cited paper around it; a searcherlooking for papers about a learning system, partic-ularly one that uses theory renement and/or onethat learns non-recursive NP and VP structuresmight be interested in the paper, as might thosesearching for information about ALLiS.The fact that an author has chosen those partic-ular terms in referring to the paper means that theyreflect what that author feels is important about thepaper.
It is reasonable, then, that other researchersinterested in the same things would find the citedpaper useful and could plausibly use such termsas query terms.
It is true that the cited paper maywell contain these terms, and they may even beimportant, prominent terms, but this is not neces-sarily the case.
There are numerous situations inwhich the terms in the document are not the bestindicators of what is important in it.
Firstly, whatis important in a paper in terms of what it is knownand cited for is not always the same as what isimportant in it in terms of subject matter or fo-cus.
Secondly, what are considered to be the im-portant contributions of a paper may change overtime.
Thirdly, the terminology used to describe theimportant contributions may be different from thatused in the paper or may change over time.
(2) exemplifies this special case, where a paperis referred to using terms that are not in the paperitself: the cited paper is the standard reference forthe HITS algorithm yet the name HITS was onlyattributed to the algorithm after the paper was writ-ten and it doesn?t contain the term at all1.The last two examples show how citing au-thors can provide higher level descriptions of thecited paper, e.g., good overview and comparison.These meta-descriptors are less likely to appearin the papers themselves as prominent terms yet,again, could plausibly be used as query terms fora searcher.Reference Directed IndexingThese examples (and many more) suggest thattext used in reference to papers can provide use-ful index terms, just as anchor text does for web-pages.
Bradshaw & Hammond (2002) even go sofar as to argue that reference is more valuable asa source of index terms than the document?s owncontent.
Bradshaw?s theory is that, when citing,authors describe a document in terms similar to asearcher?s query for the information it contains.However, there is no anchor text, per se, in pa-pers, i.e., there are no HTML tags to delimit thetext associated with a citation, unlike in webpages.The question is raised, therefore, of what is theanchor text equivalent for formal citations.
Brad-shaw (2003) extracts NPs from a fixed window ofaround one hundred words around the citation anduses these as the basis of his Reference-DirectedIndexing (RDI).Bradshaw evaluates RDI by, first, indexing doc-uments provided by Citeseer (Lawrence, Bol-lacker & Giles 1999).
A set of 32 queries was cre-ated by randomly selecting keyword phrases from1There is a poetic irony in this: Kleinberg?s paper notesthe analagous problem of poorly self-descriptive webpages.26(1) ALLiS (Architecture for Learning Linguistic Structures) is a learning system which usestheory renement in order to learn non-recursive NP and VP structures (Dejean, 2000).
(2) Such estimation is simplied from HITS algorithm (Kleinberg, 1998).
(3) As two examples, (Rabiner, 1989) and (Charniak et al, 1993) givegood overviews of the techniques and equations used for Markov models and part-of-speech tagging,but they are not very explicit in the details that are needed for their application.
(4) For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).Figure 1: Citations Motivating Reference Index Terms24 documents in the collection with an author-written keywords section.
Document relevancewas determined by judging whether it addressedthe same topic as the topic in the query sourcepaper that is identified by the query keywords.Thus, the performance of RDI was compared tothat of a standard vector-space model implementa-tion (TF*IDF term weighting and cosine similarityretrieval), with RDI achieving better precision attop 10 documents (0.484 compared to 0.318, sta-tistically significant at 99.5% confidence).Citing StatementsIn a considerably earlier study, closer to ourown project, O?Connor (1982) motivated the useof words from citing statements as additionalterms to augment an existing document represen-tation.
Though O?Connor did not have machine-readable documents, procedures for ?automatic?recognition of citing statements were developedand manually carried out on a collection of chem-istry journal articles.Proceeding from the sentence in which a ci-tation is found, a set of hand-crafted, mostlysentence-based rules were applied to select theparts of the citing paper that conveyed informa-tion about the cited paper.
For instance, the citingsentence, S, was always selected.
If S contained aconnector (a keyword, e.g., this, similarly, former)in its first twelve words, its predecessor, S?1, wasalso selected etc.
The majority of rules selectedsentences from the text; others selected titles andwords from tables, figures and captions.The selected statements (minus stop words)were added to an existing representation for thecited documents, comprising human index termsand title and abstract terms, and a small-scale re-trieval experiment was performed.
A 20% in-crease in recall was found using the citing state-ments in addition to the existing index terms,though in a follow-up study on biomedical papers,the increase was only 4%2 (O?Connor 1983).O?Connor concludes that citing statements canaid retrieval but notes the inherent difficulty inidentifying them.
Some of the selection rules wereonly semi-automatic (e.g., required human identi-fication of an article as a review) and most reliedon knowledge of sentence boundaries, which is anon-trivial problem in itself.
In all sentence-basedcases, sentences were either selected in their en-tirety or not at all and O?Connor notes this as asource of falsely assigned terms.3 Complex Citation ContextsThere is evidence, therefore, that good index termsfor scholarly documents can be found in the doc-uments that cite them.
Identifying which termsaround a citation really refer to it, however, is non-trivial.
In this section, we discuss some exam-ples of citations where this is the case and proposepotential ways in which computational linguisticstechniques may be useful in more accurately lo-cating those reference terms.
We take as our theo-retical baseline all terms in a fixed window arounda citation.3.1 Examples: Finding Reference TermsThe first two examples in Figure 2 illustrate howthe amount of text that refers to a citation can vary.Sometimes, only two or three terms will refer to acitation, as is often the case in enumerations suchas (5).
On the other hand, (6) shows a citationwhere much of the following section refers to thecited work.
When a paper is heavily based on pre-vious work, for example, extensive text may be af-forded to describing that work in detail.
Thus, thiscontext could contribute dozens of legitimate in-dex terms.
A fixed size window around a citation2O?Connor attributes this to a lower average number ofciting papers in the biomedical domain.27(5) Similar advances have been made in machine translation (Frederking and Nirenburg, 1994),speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al, 1998).
(6) Brown et al (1993) proposed a series of statistical models of the translation process.IBM translation models try to model the translation probability ... which describes the relationshipbetween a source language sentence ... and a target language sentence ... .
Instatistical alignment models ... a ?hidden?
alignment ... is introduced, which describes a mappingfrom a target position ... to a source position ... .
The relationship between the translation modeland the alignment model is given by: ...(7) The results of disambiguation strategies reported for pseudo-words and the like are consistentlyabove 95% overall accuracy, far higher than those reported fordisambiguating three or more senses of polysemous words (Wilks et al 1993; Leacock, Towell,and Voorhees 1993).
(8) This paper concentrates on the use of zero, pronominal, and nominal anaphora in Chinesegenerated text.
We are not concerned with lexical anaphora (Tutin and Kittredge 1992) where theanaphor and its antecedent share meaning components, while the anaphor belongs to an openlexical class.
(9) Previous work on the generation of referring expressions focused onproducing minimal distinguishing descriptions (Dale and Haddock 1991; Dale 1992; Reiter andDale 1992) or descriptions customized for different levels of hearers (Reiter 1990).
Since we arenot concerned with the generation of descriptions for different levels of users, we look only at theformer group of work, which aims at generating descriptions for a subsequent reference todistinguish it from the set of entities with which it might be confused.
(10) Ferro et al (1999) and Buchholz et al (1999) both describe learning systems to nd GRs.
Theformer (TR) uses transformation-based error-driven learning (Brill and Resnik, 1994) and thelatter (MB) uses memory-based learning (Daelemans et al, 1999).Figure 2: Citations Motivating Computational Linguisticswould not capture all the terms referring to it andonly those.In list examples such as (5), where multiple ci-tations are in close proximity, almost any windowsize would result in overlapping windows and interms being attributed to the wrong citation(s), aswell as the right one.
In such examples, the pres-ence of other citations indicates a change in refer-ence term ?ownership?.
The same is often true ofsentence boundaries, as they often signal a changein topic.
Citations frequently occur at the start ofsentences, as in (6), where a different approach isintroduced.
Similarly, a citation at the end of asentence, as in (7), often indicates the completionof the current topic.
In both cases, the sentenceboundary (c.f.
topic change) is also the boundaryof the reference text.
The same arguments increas-ingly apply to paragraph and section boundaries.
(8) is another example where the reference textdoes not extend beyond the citation sentence,though the citation is not at a sentence boundary.Instead, the topic contrast is indicated by a linguic-tic cue, i.e., the negation in We are not.
This il-lustrates another phenomenon of citations: in con-trasting their work with others?, researchers oftenexplicitly state what their paper is not about.
Intu-itively, not only are these terms better descriptorsof the cited rather than citing paper, they mighteven raise the question of whether one should goas far as excluding selected terms during index-ing of the citing paper.
We are not advocating thishere, though, and note that, in practice, such termswould not have much impact on the document: wewould expect them to have low term frequenciesin comparison to the important terms in that doc-ument and in comparison to their frequencies inother documents where they are important.
(9) is another example of this negation effect(We are not concerned with...).
Along with (10),it also shows how complex the mapping betweenreference terms and citations can be.
Firstly, ref-erence terms may belong to more than one cita-28tion.
For instance, in (10), describe learning sys-tems to nd GRs refers to both Ferro et al (1999)and Buchholz et al (1999).
Here, the presence ofa second citation does not end the domain of thefirst?s reference text, indicated by the use of bothand the conjunction between the citations.
Simi-larly, transformation-based error-driven learningalso refers to two citations but, in this case, theyare on opposite sides of the reference text, i.e.,Ferro et al (1999) and (Brill and Resnik, 1994).Moreover, there is an intervening citation that itdoes not refer to, i.e., Buchholz et al (1999).
Thesame is true of memory-based learning.4 Case StudyIn this section, we study the effect of adding ci-tation index terms to one document: The Mathe-matics of Statistical Machine Translation: Param-eter Estimation from the Computational Linguis-tics journal3 .
Our experimental setting is a corpusof ?9000 papers in the ACL Anthology4 , a digitalarchive of computational linguistics research pa-pers.
We found 24 citations to the paper in 10 otherAnthology papers (that we knew to have citationsto this paper through an unrelated study).
As asimulation of ideal processing, we then manuallyextracted the terms from those around those cita-tions that specifically referred to the paper, hence-forth ideal reference terms.
Next, we extractedall terms from a fixed window of ?50 terms oneither side (equivalent to Bradshaw (2003)?s win-dow size), henceforth xed reference terms.
Fi-nally, we calculated various term statistics, includ-ing IDF values across the corpus.
All terms weredecapitalized.
We now attempt to draw a ?termprofile?
of the document, both before and afterthose reference terms are added to the document,and discuss the implications for IR.4.1 Index Term AnalysisTable 1 gives the top twenty ideal reference termsranked by their TF*IDF values in the original doc-ument.
Note that we observe the effects on therelative rankings of the ideal reference terms only,since it is these hand-picked terms that we con-sider to be important descriptors for the documentand whose statistics will be most affected by theinclusion of reference terms.
To give an indicationof their importance relative to other terms in the3http://www.aclweb.org/anthology/J93-2003.pdf4http://www.aclweb.org/anthology/RankIdeal Doc TF*IDF Term1 1 351.73 french2 2 246.52 alignments3 3 238.39 fertility4 4 212.20 alignment5 5 203.28 cept6 8 158.45 probabilities7 9 150.74 translation8 12 106.11 model9 17 79.47 probability10 18 78.37 models11 19 78.02 english12 21 76.23 parameters13 24 71.77 connected14 28 62.48 words15 32 57.57 em13 35 54.88 iterations14 45 45.00 statistical15 54 38.25 training16 69 32.93 word17 74 31.31 pairs18 81 29.29 machine19 83 28.53 empty20 130 19.72 seriesTable 1: Ideal Reference Term Ranking byTF*IDFdocument, however, the second column in Table 1gives the absolute rankings of these terms in theoriginal document.
These numbers confirm thatour ideal reference terms are, in fact, relatively im-portant in the document; indeed, the top five termsin the document are all ideal reference terms.
Fur-ther down the ranking, the ideal reference termsbecome more ?diluted?
with terms not picked fromour 24 citations.
An inspection revealed that manyof these terms were French words from exampletranslations, since the paper deals with machinetranslation between English and French.
Thus,they were bad index terms, for our purposes.Hence, we observed the effect of adding, first,the ideal reference terms then, separately, the fixedreference terms to the document, summarized inTables 2 to 5.
Tables 2 and 3 show the terms withthe largest differences in positions as a result ofadding the ideal and fixed reference terms respec-tively.For instance, ibm?s TF*IDF value more thandoubled.
The term ibm appears only six times inthe document (and not even from the main textbut from authors?
institutions and one bibliogra-phy item) yet one of its major contributions isthe machine translation models it introduced, nowstandardly referred to as ?the IBM models?.
Con-29TF*IDF IdealTerm ?
Doc+ideal Rank ?ibm 24.24 37.46 28?
20generative 4.44 11.10 38?
33source 5.35 6.42 65?
44decoders 6.41 6.41 ?
45corruption 6.02 6.02 ?
46expectation 2.97 5.94 51?
47relationship 2.96 5.92 52?
48story 2.94 5.88 53?
49noisy-channel 5.75 5.75 ?52extract 1.51 7.54 41?
38Table 2: Term Ranking Changes (Ideal)TF*IDF IdealTerm ?
Doc+fixed Rank ?ibm 48.48 61.70 28?
18target 19.64 19.64 ?
26source 14.99 16.06 65?
32phrase-based 14.77 14.77 ?
36trained 14.64 19.52 43?
27approaches 11.03 11.03 ?
41parallel 9.72 17.81 34?
29generative 8.88 15.54 38?
33train 8.21 8.21 ?
45channel 6.94 6.94 ?
55expectation 5.93 8.90 51?
44learn 5.93 7.77 60?
47Table 3: Term Ranking Changes (Fixed)sequently, ?IBM?
was contained in many citationcontexts in citing papers, leading to an ideal ref-erence term frequency of 11 for ibm.
As a result,ibm is boosted eight places to rank 20.
This exem-plifies how reference terms can better describe adocument, in terms of what searchers might plau-sibly look for (c.f.
Example 2).There were twenty terms that do not occurin the document itself but are nevertheless usedby citing authors to describe it, shown in Ta-bles 4 and 5.
Many of these have high IDF val-ues, indicating their distinctiveness in the corpus,e.g., decoders (6.41), corruption (6.02) and noisy-channel (5.75).
This, combined with the fact thatciting authors use these terms in describing thepaper, means that these terms are intuitively highquality descriptors of the paper.
Without the refer-ence index terms, however, the paper would scorezero for these terms as query terms.Many more fixed reference terms were foundper citation than ideal ones.
This can introducenoise.
In general, the TF*IDF values of ideal ref-erence terms can only be further boosted by in-cluding more terms and a comparison of Tables 2Term TF*IDFdecoders 6.41corruption 6.02noisy-channel 5.75attainable 5.45target 5.24source-language 4.99phrase-based 4.92target-language 4.82application-specific 4.40train 4.10intermediate 4.01channel 3.47approaches 3.01combinations 1.70style 2.12add 1.32major 1.16due 0.83considered 0.81developed 0.78Table 4: New Non-zero TF*IDF Terms (Ideal)with 3 (or 4 with 5) shows that this is sometimesthe case, e.g, ibm occurred a further eleven timesin the fixed reference terms, doubling its increasein TF*IDF.
However, instances of those terms thatonly occurred in the fixed reference terms did not,in fact, refer to the citation of the paper, by defi-nition of the ideal reference terms.
For instance,one such extra occurrence of ibm is from a sen-tence following the citation that describes the ex-act model used in the current work:(11) According to the IBM models (Brown et al,1993), the statistical word alignment modelcan be generally represented as in Equation(1) ...
In this paper, we use a simplied IBMmodel 4 (Al-Onaizan et al, 1999), which ...Here, the second occurrence refers to (Al-Onaizan et al, 1999) but, by its proximity to thecitation to our example paper (Brown et al, 1993),is picked up by the fixed window.
Since the termwas arguably not directly intended to describe ourpaper, then, a different term might equally havebeen used; one that was inappropriate as an in-dex term.
Table 6 lists the fixed reference termsthat were not also in the ideal reference terms; al-most 400 in total.
The vast majority of these occurvery infrequently which suggests that they shouldnot greatly affect the term profile of the document.However, the argument for adding good, high IDFreference terms that are not in the document itself30Term TF*IDFtarget 19.64phrase-based 14.77approaches 11.03train 8.21channel 6.94decoders 6.41corruption 6.02noisy-channel 5.75attainable 5.45source-language 4.99target-language 4.82application-specific 4.40intermediate 4.01combinations 3.40style 2.12considered 1.62major 1.16due 0.83developed 0.78Table 5: New Non-zero TF*IDF Terms (Fixed)conversely applies to adding bad ones: an ?incor-rect?
reference term added to the document willhave its TF*IDF pushed off the zero mark, givingit the potential to score against inappropriate queryterms.
If such a term is distinctive (i.e., has a highIDF), the effect may be significant.
The term giza,for example, has an IDF of 6.34 and is the nameof a particular tool that is not mentioned in ourexample paper.
However, since the tool is usedto train IBM models, the two papers in the exam-ple above are often cited by the same papers andin close proximity.
This increases the chances ofsuch terms being picked up as reference terms forthe wrong citation by a fixed window, heighteningthe adverse effect on its term profile.5 Discussion and ConclusionsIt is not too hard to find examples of citations thatshow a fixed window size is suboptimal for findingterms used in reference to cited papers.
In extract-ing the ideal reference terms from only 24 cita-tions for our case study, we saw just how difficultit is to decide which terms refer to which citations.We, the authors, came across examples where itwas ambiguous how many citations certain termsreferred to, ones where knowledge of the cited pa-pers was required to interpret the scope of the cita-tion and ones where we simply did not agree.
Thisis a highly complex indexing task; one which hu-mans have difficulty with, one for which we expectlow human agreement and, therefore, the type thatcomputational linguistics struggles to achieve highperformance on.
We agree with O?Connor (1982)that it is hard.
We make no claims that computa-tional linguistics will provide a full solution.Nevertheless, our examples suggest that evensimple computational linguistics techniquesshould help to more accurately locate referenceterms.
While it may be impossible to automati-cally pick out each specific piece of text that doesrefer to a given citation, there is much scope forimprovement over a fixed window.
The examplesin Section 2 suggest that altering the size of thewindow that is applied would be a good first step.Some form of text segmentation, whether it befull-blown discourse analysis or simple sentenceboundary detection, may be useful in determiningwhere the extent of the reference text is.While the case study presented here highlightsseveral interesting effects of using terms fromaround citations as additional index terms for thecited paper, it cannot answer questions about howsuccessful a practical method based on these ob-servations would be, over a using simple fixedwindow, for example.
In order for any real im-provement in IR, the term profile of a documentwould have to be significantly altered by the refer-ence terms.
Enough terms, in particular repeatedterms, would have to be successfully found via ci-tations for such a quantitative improvement.
It isnot clear that computational linguistic techniqueswill improve over the statistical effects of redun-dant data.We are thus in the last stages of setting up alarger experiment that will shed more light on thisquestion.
The experimental setup requires datawhere there are a significant number of citationsto a number of test documents and a significantnumber of reference set terms.
We have recentlypresented a test collection of scientific research pa-pers (Ritchie, Teufel & Robertson 2006), whichwe intend to use for this experiment.ReferencesBharat, K. & Mihaila, G. A.
(2001), When experts agree:using non-affiliated experts to rank popular topics, in?Tenth International World Wide Web Conference?,pp.
597?602.Bradshaw, S. (2003), Reference directed indexing: Re-deeming relevance for subject search in citation in-dexes., in ?ECDL?, pp.
499?510.Bradshaw, S. & Hammond, K. (2002), Automatically in-dexing documents: Content vs. reference, in ?Intel-ligent User Interfaces?.31TF # Terms Terms13 1 asr8 4 caption, closed, section, methods7 2 method, sentences6 4 describes, example, languages, system5 6 corpus, dictionary, heuristic, large, paper, results4 17 account, aligned, confidence, dependency, details, during, equation, generally, given, manual, measures,order, probabilistic, proposed, shown, simplified, systems, word-aligned3 29 according, algorithm, applications, build, case, choosing, chunk, current, described, employed, equiv-alence, experiments, introduced, introduction, length, links, number, obtain, obtained, performance,performing, problem, produced, related, show, sum, true, types, work2 64 adaptation, akin, approximate, bitext, calculated, called, categories, certain, chunks, common, consider,consists, domain-specific, error, estimation, experimental, extracted, families, feature, features, found,functions, generated, generic, giza, good, high, improve, information, input, iraq, knowledge, large-scale, lexicon, linked, log-linear, maximum, measure, notion, omitted, original, output, parameter, pick,position, practice, presents, quality, rate, represented, researchers, rock, role, sinhalese, takes, tamil,text-to-text, toolkit, transcripts, transcriptions, translations, version, word-based, word-to-word1 252 access, accuracy, achieve, achieving, actual, addition, address, adopted, advance, advantages, align-ing, amalgam, annotated, applied, apply, applying, approximated, association, asymmetric, augmented,availability, available, average, back-off, base, baum-welch, begin, bitexts, bunetsu, candidate, can-didates, cat, central, chinese, choose, chunk-based, class, closely, collecting, combination, compare,compared, compares, computed, concludes, consequently, contributed, convention, corpora, correspon-dence, corrupts, cost, counts, coverage, crucial, currently, decades, decoding, defines, denote, dependent,depending, determine, dictionaries, direct, directions, disadvantages, distinction, dominated, dynamic,efforts, english-chinese, english-spanish, enumerate, eojeol, eq, equations, errors, evaluation, excellent,expansion, explicitly, extracts, failed, fairly, final, finally, fit, flat-start, followed, form, formalisms, for-mulation, generation, gis, give, grouped, hallucination, halogen, handle, heuristic-based, hidden, highly,hill-climbing, hmm-based, hypothesis, ideal, identified, identify, identity, immediate, implemented, im-proved, improves, incorporate, increase, influence, initial, initialize, inspired, interchanging, introduces,investigations, involve, kate, kind, learning, learns, letter, letters, lexical, likelihood, link, list, longer,lowercase, main, make, makes, mapping, maximal, maximizes, means, modeling, modified, names,needed, nitrogen, nodes, occupy, omitting, optimal, outperform, overcome, parse, parser, part, part-of-speech, path, performed, play, plays, popular, pos, positions, power, precision, probable, produce, pro-gramming, promising, real-valued, reason, recall, recent, recently, recognition, recursion, recursively, re-duction, reductions, refine, relative, relying, renormalization, representation, require, requires, research,restricting, reveal, sample, sampling, satisfactory, segments, semantic, sequences, setting, shortcom-ings, showed, significant, significantly, similarity, similarly, simple, simplicity, situation, space, speech,spelling, state-of-the-art, step, strategies, string, strong, studies, summaries, summarization, supervised,syntactic, tags, task-specific, technique, techniques, technologies, terms, testing, threshold, translation-related, transliteration, tree, trees, trellis, type, underlying, unrealistic, unsupervised, uppercase, value,viterbi, wanted, ways, well-formedness, well-founded, widely, widespread, works, written, wtop, yas-met, years, yieldsTable 6: Term Frequencies of ?Noisy?
Reference Index TermsBrin, S. & Page, L. (1998), ?The anatomy of a large-scale hypertextual Web search engine?, ComputerNetworks and ISDN Systems 30(1?7), 107?117.Chakrabarti, S., Dom, B., Gibson, D., Kleinberg, J.,Raghavan, P. & Rajagopalan, S. (1998), Automaticresource list compilation by analyzing hyperlinkstructure and associated text, in ?Seventh Interna-tional World Wide Web Conference?.Davison, B. D. (2000), Topical locality in the web,in ?Research and Development in Information Re-trieval (SIGIR)?, pp.
272?279.Kleinberg, J. M. (1999), ?Authoritative sources in ahyperlinked environment?, Journal of the ACM46(5), 604?632.Lawrence, S., Bollacker, K. & Giles, C. L. (1999), In-dexing and retrieval of scientific literature, in ?Con-ference on Information and Knowledge Manage-ment (CIKM)?, pp.
139?146.Marchiori, M. (1997), ?The quest for correct informa-tion on the Web: Hyper search engines?, ComputerNetworks and ISDN Systems 29(8?13), 1225?1236.McBryan, O.
(1994), GENVL and WWWW: Tools fortaming the web, in ?First International World WideWeb Conference?.O?Connor, J.
(1982), ?Citing statements: Computerrecognition and use to improve retrieval?, Informa-tion Processing and Management 18(3), 125?131.O?Connor, J.
(1983), ?Biomedical citing statements:Computer recognition and use to aid full-text re-trieval?, Information Processing and Management19, 361?368.Ritchie, A., Teufel, S. & Robertson, S. (2006), Creatinga test collection for citation-based IR experiments,in ?HLT-NAACL?.Spa?rck Jones, K., Walker, S. & Robertson, S. E. (2000),?A probabilistic model of information retrieval: de-velopment and comparative experiments - parts 1& 2.?, Information Processing and Management36(6), 779?840.32
