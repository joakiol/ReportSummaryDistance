Proceedings of the 43rd Annual Meeting of the ACL, pages 491?498,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSimple Algorithms for Complex Relation Extractionwith Applications to Biomedical IERyan McDonald1 Fernando Pereira1 Seth Kulick21CIS and 2IRCS, University of Pennsylvania, Philadelphia, PA{ryantm,pereira}@cis.upenn.edu, skulick@linc.cis.upenn.eduScott Winters Yang Jin Pete WhiteDivision of Oncology, Children?s Hospital of Pennsylvania, Philadelphia, PA{winters,jin,white}@genome.chop.eduAbstractA complex relation is any n-ary relationin which some of the arguments may bebe unspecified.
We present here a simpletwo-stage method for extracting complexrelations between named entities in text.The first stage creates a graph from pairsof entities that are likely to be related, andthe second stage scores maximal cliquesin that graph as potential complex relationinstances.
We evaluate the new methodagainst a standard baseline for extractinggenomic variation relations from biomed-ical text.1 IntroductionMost research on text information extraction (IE)has focused on accurate tagging of named entities.Successful early named-entity taggers were basedon finite-state generative models (Bikel et al, 1999).More recently, discriminatively-trained models havebeen shown to be more accurate than generativemodels (McCallum et al, 2000; Lafferty et al, 2001;Kudo and Matsumoto, 2001).
Both kinds of mod-els have been developed for tagging entities suchas people, places and organizations in news mate-rial.
However, the rapid development of bioinfor-matics has recently generated interest on the extrac-tion of biological entities such as genes (Collier etal., 2000) and genomic variations (McDonald et al,2004b) from biomedical literature.The next logical step for IE is to begin to developmethods for extracting meaningful relations involv-ing named entities.
Such relations would be ex-tremely useful in applications like question answer-ing, automatic database generation, and intelligentdocument searching and indexing.
Though not aswell studied as entity extraction, relation extractionhas still seen a significant amount of work.
We dis-cuss some previous approaches at greater length inSection 2.Most relation extraction systems focus on the spe-cific problem of extracting binary relations, suchas the employee of relation or protein-protein in-teraction relation.
Very little work has been donein recognizing and extracting more complex rela-tions.
We define a complex relation as any n-aryrelation among n typed entities.
The relation isdefined by the schema (t1, .
.
.
, tn) where ti ?
Tare entity types.
An instance (or tuple) in the rela-tion is a list of entities (e1, .
.
.
, en) such that eithertype(ei) = ti, or ei =?
indicating that the ith ele-ment of the tuple is missing.For example, assume that the entity typesare T = {person, job, company} and we areinterested in the ternary relation with schema(person, job, company) that relates a personto their job at a particular company.
Forthe sentence ?John Smith is the CEO at Inc.Corp.
?, the system would ideally extract the tu-ple (John Smith, CEO, Inc. Corp.).
However, forthe sentence ?Everyday John Smith goes to hisoffice at Inc.
Corp.?, the system would extract(John Smith,?, Inc. Corp.), since there is no men-tion of a job title.
Hence, the goal of complex re-lation extraction is to identify all instances of therelation of interest in some piece of text, including491incomplete instances.We present here several simple methods for ex-tracting complex relations.
All the methods start byrecognized pairs of entity mentions, that is, binaryrelation instances, that appear to be arguments of therelation of interest.
Those pairs can be seen as theedges of a graph with entity mentions as nodes.
Thealgorithms then try to reconstruct complex relationsby making tuples from selected maximal cliques inthe graph.
The methods are general and can be ap-plied to any complex relation fitting the above def-inition.
We also assume throughout the paper thatthe entities and their type are known a priori in thetext.
This is a fair assumption given the current highstandard of state-of-the-art named-entity extractors.A primary advantage of factoring complex rela-tions into binary relations is that it allows the use ofstandard classification algorithms to decide whetherparticular pairs of entity mentions are related.
In ad-dition, the factoring makes training data less sparseand reduces the computational cost of extraction.We will discuss these benefits further in Section 4.We evaluated the methods on a large set of anno-tated biomedical documents to extract relations re-lated to genomic variations, demonstrating a consid-erable improvement over a reasonable baseline.2 Previous workA representative approach to relation extraction isthe system of Zelenko et al (2003), which attemptsto identify binary relations in news text.
In thatsystem, each pair of entity mentions of the correcttypes in a sentence is classified as to whether it isa positive instance of the relation.
Consider the bi-nary relation employee of and the sentence ?JohnSmith, not Jane Smith, works at IBM?.
The pair(John Smith, IBM) is a positive instance, while thepair (Jane Smith, IBM) is a negative instance.
In-stances are represented by a pair of entities and theirposition in a shallow parse tree for the containingsentence.
Classification is done by a support-vectorclassifier with a specialized kernel for that shallowparse representation.This approach ?
enumerating all possible en-tity pairs and classifying each as positive or nega-tive ?
is the standard method in relation extraction.The main differences among systems are the choiceof trainable classifier and the representation for in-stances.For binary relations, this approach is quitetractable: if the relation schema is (t1, t2), the num-ber of potential instances is O(|t1| |t2|), where |t| isthe number of entity mentions of type t in the textunder consideration.One interesting system that does not belong tothe above class is that of Miller et al (2000), whotake the view that relation extraction is just a formof probabilistic parsing where parse trees are aug-mented to identify all relations.
Once this augmen-tation is made, any standard parser can be trainedand then run on new sentences to extract new re-lations.
Miller et al show such an approach canyield good results.
However, it can be argued thatthis method will encounter problems when consid-ering anything but binary relations.
Complex re-lations would require a large amount of tree aug-mentation and most likely result in extremely sparseprobability estimates.
Furthermore, by integratingrelation extraction with parsing, the system cannotconsider long-range dependencies due to the localparsing constraints of current probabilistic parsers.The higher the arity of a relation, the more likelyit is that entities will be spread out within a pieceof text, making long range dependencies especiallyimportant.Roth and Yih (2004) present a model in which en-tity types and relations are classified jointly using aset of global constraints over locally trained classi-fiers.
This joint classification is shown to improveaccuracy of both the entities and relations returnedby the system.
However, the system is based on con-straints for binary relations only.Recently, there has also been many results fromthe biomedical IE community.
Rosario and Hearst(2004) compare both generative and discriminativemodels for extracting seven relationships betweentreatments and diseases.
Though their models arevery flexible, they assume at most one relation persentence, ruling out cases where entities participatein multiple relations, which is a common occurrencein our data.
McDonald et al (2004a) use a rule-based parser combined with a rule-based relationidentifier to extract generic binary relations betweenbiological entities.
As in predicate-argument extrac-tion (Gildea and Jurafsky, 2002), each relation is492always associated with a verb in the sentence thatspecifies the relation type.
Though this system isvery general, it is limited by the fact that the designignores relations not expressed by a verb, as the em-ployee of relation in?John Smith, CEO of Inc. Corp.,announced he will resign?.Most relation extraction systems work primarilyon a sentential level and never consider relations thatcross sentences or paragraphs.
Since current datasets typically only annotate intra-sentence relations,this has not yet proven to be a problem.3 Definitions3.1 Complex RelationsRecall that a complex n-ary relation is specified bya schema (t1, .
.
.
, tn) where ti ?
T are entity types.Instances of the relation are tuples (e1, .
.
.
, en)where either type(ei) = ti, or ei =?
(missing ar-gument).
The only restriction this definition placeson a relation is that the arity must be known.
As wediscuss it further in Section 6, this is not required byour methods but is assumed here for simplicity.
Wealso assume that the system works on a single rela-tion type at a time, although the methods describedhere are easily generalizable to systems that can ex-tract many relations at once.3.2 Graphs and CliquesAn undirected graph G = (V,E) is specified by aset of vertices V and a set of edges E, with eachedge an unordered pair (u, v) of vertices.
G?
=(V ?, E?)
is a subgraph of G if V ?
?
V and E?
={(u, v) : u, v ?
V ?, (u, v) ?
E}.
A clique C of G isa subgraph of G in which there is an edge betweenevery pair of vertices.
A maximal clique of G is aclique C = (VC , EC) such that there is no otherclique C ?
= (VC?
, EC?)
such that VC ?
VC?
.4 MethodsWe describe now a simple method for extractingcomplex relations.
This method works by first fac-toring all complex relations into a set of binary re-lations.
A classifier is then trained in the standardmanner to recognize all pairs of related entities.
Fi-nally a graph is constructed from the output of thisclassifier and the complex relations are determinedfrom the cliques of this graph.a.
All possiblerelation instances(John, CEO, Inc.
Corp.)(John,?, Inc. Corp.)(John, CEO, Biz.
Corp.)(John,?, Biz.
Corp.)(John, CEO,?
)(Jane, CEO, Inc.
Corp.)(Jane,?, Inc. Corp.)(Jane, CEO, Biz.
Corp.)(Jane,?, Biz.
Corp.)(Jane, CEO,?
)(?, CEO, Inc.
Corp.)(?, CEO, Biz.
Corp.)b.
All possiblebinary relations(John, CEO)(John, Inc. Corp.)(John, Biz.
Corp.)(CEO, Inc. Corp.)(CEO, Biz.
Corp.)(Jane, CEO)(Jane, Inc. Corp.)(Jane, Biz.
Corp.)Figure 1: Relation factorization of the sentence:John and Jane are CEOs at Inc. Corp. and Biz.Corp.
respectively.4.1 Classifying Binary RelationsConsider again the motivating example of the(person, job, company) relation and the sentence?John and Jane are CEOs at Inc. Corp. and Biz.Corp.
respectively?.
This sentence contains twopeople, one job title and two companies.One possible method for extracting the rela-tion of interest would be to first consider all 12possible tuples shown in Figure 1a.
Using allthese tuples, it should then be possible to traina classifier to distinguish valid instances such as(John, CEO, Inc. Corp.) from invalid ones such as(Jane, CEO, Inc. Corp.).
This is analogous to theapproach taken by Zelenko et al (2003) for binaryrelations.There are problems with this approach.
Computa-tionally, for an n-ary relation, the number of possi-ble instances is O(|t1| |t2| ?
?
?
|tn|).
Conservatively,letting m be the smallest |ti|, the run time is O(mn),exponential in the arity of the relation.
The secondproblem is how to manage incomplete but correctinstances such as (John,?, Inc. Corp.) when train-ing the classifier.
If this instance is marked as neg-ative, then the model might incorrectly disfavor fea-tures that correlate John to Inc. Corp.. However,if this instance is labeled positive, then the modelmay tend to prefer the shorter and more compact in-complete relations since they will be abundant in thepositive training examples.
We could always ignoreinstances of this form, but then the data would beheavily skewed towards negative instances.493Instead of trying to classify all possible relationinstances, in this work we first classify pairs of en-tities as being related or not.
Then, as discussed inSection 4.2, we reconstruct the larger complex rela-tions from a set of binary relation instances.Factoring relations into a set of binary decisionshas several advantages.
The set of possible pairs ismuch smaller then the set of all possible complexrelation instances.
This can be seen in Figure 1b,which only considers pairs that are consistent withthe relation definition.
More generally, the num-ber of pairs to classify is O((?i |ti|)2) , which isfar better than the exponentially many full relationinstances.
There is also no ambiguity when label-ing pairs as positive or negative when constructingthe training data.
Finally, we can rely on previouswork on classification for binary relation extractionto identify pairs of related entities.To train a classifier to identify pairs of relatedentities, we must first create the set of all positiveand negative pairs in the data.
The positive in-stances are all pairs that occur together in a validtuple.
For the example sentence in Figure 1, theseinclude the pairs (John, CEO), (John, Inc. Corp.),(CEO, Inc. Corp.), (CEO, Biz.
Corp.), (Jane, CEO)and (Jane, Biz.
Corp.).
To gather negative in-stances, we extract all pairs that never occur to-gether in a valid relation.
From the same exam-ple these would be the pairs (John, Biz.
Corp.) and(Jane, Inc. Corp.).This leads to a large set of positive and negativebinary relation instances.
At this point we could em-ploy any binary relation classifier and learn to iden-tify new instances of related pairs of entities.
Weuse a standard maximum entropy classifier (Bergeret al, 1996) implemented as part of MALLET (Mc-Callum, 2002).
The model is trained using the fea-tures listed in Table 1.This is a very simple binary classification model.No deep syntactic structure such as parse trees isused.
All features are basically over the words sepa-rating two entities and their part-of-speech tags.
Ofcourse, it would be possible to use more syntacticinformation if available in a manner similar to thatof Zelenko et al (2003).
However, the primary pur-pose of our experiments was not to create a betterbinary relation extractor, but to see if complex re-lations could be extracted through binary factoriza-Feature Setentity type of e1 and e2words in e1 and e2word bigrams in e1 and e2POS of e1 and e2words between e1 and e2word bigrams between e1 and e2POS between e1 and e2distance between e1 and e2concatenations of above featuresTable 1: Feature set for maximum entropy binaryrelation classifier.
e1 and e2 are entities.a.
Relation graph GJohn JaneCEOInc.
Corp. Biz.
Corp.b.
Tuples from G(John, CEO,?
)(John,?, Inc.
Corp.)(John,?, Biz.
Corp.)(Jane, CEO,?
)(?, CEO, Inc.
Corp.)(?, CEO, Biz.
Corp.)(John, CEO, Inc. Corp.)(John, CEO, Biz.
Corp.)Figure 2: Example of a relation graph and tuplesfrom all the cliques in the graph.tion followed by reconstruction.
In Section 5.2 wepresent an empirical evaluation of the binary relationclassifier.4.2 Reconstructing Complex Relations4.2.1 Maximal CliquesHaving identified all pairs of related entities in thetext, the next stage is to reconstruct the complex re-lations from these pairs.
Let G = (V,E) be an undi-rected graph where the vertices V are entity men-tions in the text and the edges E represent binaryrelations between entities.
We reconstruct the com-plex relation instances by finding maximal cliquesin the graphs.The simplest approach is to create the graphso that two entities in the graph have an edgeif the binary classifier believes they are related.For example, consider the binary factoriza-tion in Figure 1 and imagine the classifieridentified the following pairs as being related:(John, CEO), (John, Inc. Corp.), (John, Biz.
Corp.),(CEO, Inc. Corp.), (CEO, Biz.
Corp.) and(Jane, CEO).
The resulting graph can be seenin Figure 2a.Looking at this graph, one solution to construct-494ing complex relations would be to consider all thecliques in the graph that are consistent with the def-inition of the relation.
This is equivalent to havingthe system return only relations in which the binaryclassifier believes that all of the entities involved arepairwise related.
All the cliques in the example areshown in Figure 2b.
We add ?
fields to the tuples tobe consistent with the relation definition.This could lead to a set of overlappingcliques, for instance (John, CEO, Inc. Corp.) and(John, CEO,?).
Instead of having the system re-turn all cliques, our system just returns the maximalcliques, that is, those cliques that are not subsets ofother cliques.
Hence, for the example under con-sideration in Figure 2, the system would return theone correct relation, (John, CEO, Inc. Corp.), andtwo incorrect relations, (John, CEO, Biz.
Corp.) and(Jane, CEO,?).
The second is incorrect since itdoes not specify the company slot of the relationeven though that information is present in the text.It is possible to find degenerate sentences in whichperfect binary classification followed by maximalclique reconstruction will lead to errors.
One suchsentence is, ?John is C.E.O.
and C.F.O.
of Inc. Corp.and Biz.
Corp. respectively and Jane vice-versa?.However, we expect such sentences to be rare; infact, they never occur in our data.The real problem with this approach is that an ar-bitrary graph can have exponentially many cliques,negating any efficiency advantage over enumeratingall n-tuples of entities.
Fortunately, there are algo-rithms for finding all maximal cliques that are effi-cient in practice.
We use the algorithm of Bron andKerbosch (1973).
This is a well known branch andbound algorithm that has been shown to empiricallyrun linearly in the number of maximal cliques in thegraph.
In our experiments, this algorithm found allmaximal cliques in a matter of seconds.4.2.2 Probabilistic CliquesThe above approach has a major shortcom-ing in that it assumes the output of the bi-nary classifier to be absolutely correct.
Forinstance, the classifier may have thought withprobability 0.49, 0.99 and 0.99 that the fol-lowing pairs were related: (Jane, Biz.
Corp.),(CEO, Biz.
Corp.) and (Jane, CEO) respectively.The maximal clique method would not produce thetuple (Jane, CEO, Biz.
Corp.) since it never consid-ers the edge between Jane and Biz.
Corp. However,given the probability of the edges, we would almostcertainly want this tuple returned.What we would really like to model is a beliefthat on average a clique represents a valid relationinstance.
To do this we use the complete graphG = (V,E) with edges between all pairs of entitymentions.
We then assign weight w(e) to edge eequal to the probability that the two entities in e arerelated, according to the classifier.
We define theweight of a clique w(C) as the mean weight of theedges in the clique.
Since edge weights representprobabilities (or ratios), we use the geometric meanw(C) =???e?ECw(e)?
?1/|EC |We decide that a clique C represents a valid tuple ifw(C) ?
0.5.
Hence, the system finds all maximalcliques as before, but considers only those wherew(C) ?
0.5, and it may select a non-maximal cliqueif the weight of all larger cliques falls below thethreshold.
The cutoff of 0.5 is not arbitrary, since itensures that the average probability of a clique rep-resenting a relation instance is at least as large asthe average probability of it not representing a rela-tion instance.
We ran experiments with varying lev-els of this threshold and found that, roughly, lowerthresholds result in higher precision at the expenseof recall since the system returns fewer but largertuples.
Optimum results were obtained for a cutoffof approximately 0.4, but we report results only forw(C) ?
0.5.The major problem with this approach is thatthere will always be exponentially many cliquessince the graph is fully connected.
However, in ourexperiments we pruned all edges that would forceany containing clique C to have w(C) < 0.5.
Thistypically made the graphs very sparse.Another problem with this approach is the as-sumption that the binary relation classifier outputsprobabilities.
For maximum entropy and other prob-abilistic frameworks this is not an issue.
However,many classifiers, such as SVMs, output scores ordistances.
It is possible to transform the scores fromthose models through a sigmoid to yield probabili-495ties, but there is no guarantee that those probabilityvalues will be well calibrated.5 Experiments5.1 Problem Description and DataWe test these methods on the task of extracting ge-nomic variation events from biomedical text (Mc-Donald et al, 2004b).
Briefly, we define a varia-tion event as an acquired genomic aberration: a spe-cific, one-time alteration at the genomic level anddescribed at the nucleic acid level, amino acid levelor both.
Each variation event is identified by the re-lationship between a type of variation, its location,and the corresponding state change from an initial-state to an altered-state.
This can be formalized asthe following complex schema(var-type, location, initial-state, altered-state)A simple example is the sentence?At codons 12 and 61, the occurrence ofpoint mutations from G/A to T/G were observed?which gives rise to the tuples(point mutation, codon 12, G, T)(point mutation, codon 61, A, G)Our data set consists of 447 abstracts selectedfrom MEDLINE as being relevant to populating adatabase with facts of the form: gene X with vari-ation event Y is associated with malignancy Z. Ab-stracts were randomly chosen from a larger corpusidentified as containing variation mentions pertain-ing to cancer.The current data consists of 4691 sentences thathave been annotated with 4773 entities and 1218 re-lations.
Of the 1218 relations, 760 have two ?
ar-guments, 283 have one ?
argument, and 175 haveno ?
arguments.
Thus, 38% of the relations taggedin this data cannot be handled using binary relationclassification alone.
In addition, 4% of the relationsannotated in this data are non-sentential.
Our sys-tem currently only produces sentential relations andis therefore bounded by a maximum recall of 96%.Finally, we use gold standard entities in our exper-iments.
This way we can evaluate the performanceof the relation extraction system isolated from anykind of pipelined entity extraction errors.
Entities inthis domain can be found with fairly high accuracy(McDonald et al, 2004b).It is important to note that just the presence of twoentity types does not entail a relation between them.In fact, 56% of entity pairs are not related, due eitherto explicit disqualification in the text (e.g.
?...
thelack of G to T transversion ...?)
or ambiguities thatarise from multiple entities of the same type.5.2 ResultsBecause the data contains only 1218 examples of re-lations we performed 10-fold cross-validation testsfor all results.
We compared three systems:?
MC: Uses the maximum entropy binary classi-fier coupled with the maximal clique complexrelation reconstructor.?
PC: Same as above, except it uses the proba-bilistic clique complex relation reconstructor.?
NE: A maximum entropy classifier that naivelyenumerates all possible relation instances asdescribed in Section 4.1.In training system NE, all incomplete but correctinstances were marked as positive since we foundthis had the best performance.
We used the samepairwise entity features in the binary classifier ofthe above two systems.
However, we also addedhigher order versions of the pairwise features.
Forthis system we only take maximal relations,that is,if (John, CEO, Inc. Corp.) and (John,?, Inc. Corp.)are both labeled positive, the system would only re-turn the former.Table 2 contains the results of the maximum en-tropy binary relation classifier (used in systems MCand PC).
The 1218 annotated complex relations pro-duced 2577 unique binary pairs of related entities.We can see that the maximum entropy classifier per-forms reasonably well, although performance maybe affected by the lack of rich syntactic features,which have been shown to help performance (Milleret al, 2000; Zelenko et al, 2003).Table 3 compares the three systems on the realproblem of extracting complex relations.
An ex-tracted complex relation is considered correct if andonly if all the entities in the relation are correct.There is no partial credit.
All training and cliquefinding algorithms took under 5 minutes for the en-tire data set.
Naive enumeration took approximately26 minutes to train.496ACT PRD COR2577 2722 2101Prec Rec F-Meas0.7719 0.8153 0.7930Table 2: Binary relation classification results for themaximum entropy classifier.
ACT: actual number ofrelated pairs, PRD: predicted number of related pairsand COR: correctly identified related pairs.System Prec Rec F-MeasNE 0.4588 0.6995 0.5541MC 0.5812 0.7315 0.6480PC 0.6303 0.7726 0.6942Table 3: Full relation classification results.
For arelation to be classified correctly, all the entities inthe relation must be correctly identified.First we observe that the maximal clique methodcombined with maximum entropy (system MC) re-duces the relative error rate over naively enumer-ating and classifying all instances (system NE) by21%.
This result is very positive.
The system basedon binary factorization not only is more efficientthen naively enumerating all instances, but signifi-cantly outperforms it as well.
The main reason naiveenumeration does so poorly is that all correct butincomplete instances are marked as positive.
Thus,even slight correlations between partially correct en-tities would be enough to classify an instance as cor-rect, which results in relatively good recall but poorprecision.
We tried training only with correct andcomplete positive instances, but the result was a sys-tem that only returned few relations since negativeinstances overwhelmed the training set.
With fur-ther tuning, it may be possible to improve the per-formance of this system.
However, we use it only asa baseline and to demonstrate that binary factoriza-tion is a feasible and accurate method for extractingcomplex relations.Furthermore, we see that using probabilisticcliques (system PC) provides another large im-provement, a relative error reduction of 13%over using maximal cliques and 31% reductionover enumeration.
Table 4 shows the breakdownof relations returned by type.
There are threetypes of relations, 2-ary, 3-ary and 4-ary, eachwith 2, 1 and 0 ?
arguments respectively, e.g.System 2-ary 3-ary 4-aryNE 760:1097:600 283:619:192 175:141:60MC 760:1025:601 283:412:206 175:95:84PC 760:870:590 283:429:223 175:194:128Table 4: Breakdown of true positive relations bytype that were returned by each system.
Each cellcontains three numbers, Actual:Predicted:Correct,which represents for each arity the actual, predictedand correct number of relations for each system.
(point mutation, codon 12,?,?)
is a 2-ary relation.Clearly the probabilistic clique method is muchmore likely to find larger non-binary relations, veri-fying the motivation that there are some low proba-bility edges that can still contribute to larger cliques.6 Conclusions and Future WorkWe presented a method for complex relation extrac-tion, the core of which was to factorize complex re-lations into sets of binary relations, learn to identifybinary relations and then reconstruct the complex re-lations by finding maximal cliques in graphs thatrepresent relations between pairs of entities.
Theprimary advantage of this method is that it allowsfor the use of almost any binary relation classifier,which have been well studied and are often accu-rate.
We showed that such a method can be suc-cessful with an empirical evaluation on a large setof biomedical data annotated with genomic varia-tion relations.
In fact, this approach is both signifi-cantly quicker and more accurate then enumeratingand classifying all possible instances.
We believethis work provides a good starting point for contin-ued research in this area.A distinction may be made between the factoredsystem presented here and one that attempts to clas-sify complex relations without factorization.
Thisis related to the distinction between methods thatlearn local classifiers that are combined with globalconstraints after training and methods that incorpo-rate the global constraints into the learning process.McCallum and Wellner (2003) showed that learningbinary co-reference relations globally improves per-formance over learning relations in isolation.
How-ever, their model relied on the transitive property in-herent in the co-reference relation.
Our system canbe seen as an instance of a local learner.
Punyakanok497et al (2004) argued that local learning actually out-performs global learning in cases when local deci-sions can easily be learnt by the classifier.
Hence, itis reasonable to assume that our binary factorizationmethod will perform well when binary relations canbe learnt with high accuracy.As for future work, there are many things that weplan to look at.
The binary relation classifier we em-ploy is quite simplistic and most likely can be im-proved by using features over a deeper representa-tion of the data such as parse trees.
Other more pow-erful binary classifiers should be tried such as thosebased on tree kernels (Zelenko et al, 2003).
We alsoplan on running these algorithms on more data setsto test if the algorithms empirically generalize to dif-ferent domains.Perhaps the most interesting open problem is howto learn the complex reconstruction phase.
One pos-sibility is recent work on supervised clustering.
Let-ting the edge probabilities in the graphs represent adistance in some space, it may be possible to learnhow to cluster vertices into relational groups.
How-ever, since a vertex/entity can participate in one ormore relation, any clustering algorithm would be re-quired to produce non-disjoint clusters.We mentioned earlier that the only restriction ofour complex relation definition is that the arity ofthe relation must be known in advance.
It turns outthat the algorithms we described can actually handledynamic arity relations.
All that is required is toremove the constraint that maximal cliques must beconsistent with the structure of the relation.
Thisrepresents another advantage of binary factorizationover enumeration, since it would be infeasible toenumerate all possible instances for dynamic arityrelations.AcknowledgmentsThe authors would like to thank Mark Liberman,Mark Mandel and Eric Pancoast for useful discus-sion, suggestions and technical support.
This workwas supported in part by NSF grant ITR 0205448.ReferencesA.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.1996.
A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, 22(1).D.M.
Bikel, R. Schwartz, and R.M.
Weischedel.
1999.An algorithm that learns what?s in a name.
MachineLearning Journal Special Issue on Natural LanguageLearning, 34(1/3):221?231.C.
Bron and J. Kerbosch.
1973.
Algorithm 457: findingall cliques of an undirected graph.
Communications ofthe ACM, 16(9):575?577.N.
Collier, C. Nobata, and J. Tsujii.
2000.
Extractingthe names of genes and gene products with a hiddenMarkov model.
In Proc.
COLING.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proc.
NAACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ICML.A.
McCallum and B. Wellner.
2003.
Toward condi-tional models of identity uncertainty with applicationto proper noun coreference.
In IJCAI Workshop on In-formation Integration on the Web.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum entropy Markov models for information extrac-tion and segmentation.
In Proc.
ICML.A.
K. McCallum.
2002.
MALLET: A machine learningfor language toolkit.D.M.
McDonald, H. Chen, H. Su, and B.B.
Marshall.2004a.
Extracting gene pathway relations using a hy-brid grammar: the Arizona Relation Parser.
Bioinfor-matics, 20(18):3370?78.R.T.
McDonald, R.S.
Winters, M. Mandel, Y. Jin, P.S.White, and F. Pereira.
2004b.
An entity tagger forrecognizing acquired genomic variations in cancer lit-erature.
Bioinformatics, 20(17):3249?3251.S.
Miller, H. Fox, L.A. Ramshaw, and R.M.
Weischedel.2000.
A novel use of statistical parsing to extract in-formation from text.
In Proc.
NAACL.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Learning via inference over structurally constrainedoutput.
In Workshop on Learning Structured with Out-put, NIPS.Barbara Rosario and Marti A. Hearst.
2004.
Classifyingsemantic relations in bioscience texts.
In ACL.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InProc.
CoNLL.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernelmethods for relation extraction.
JMLR.498
