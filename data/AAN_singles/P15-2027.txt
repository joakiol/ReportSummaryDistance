Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 162?167,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEfficient Learning for Undirected Topic ModelsJiatao Gu and Victor O.K.
LiDepartment of Electrical and Electronic EngineeringThe University of Hong Kong{jiataogu, vli}@eee.hku.hkAbstractReplicated Softmax model, a well-knownundirected topic model, is powerful in ex-tracting semantic representations of docu-ments.
Traditional learning strategies suchas Contrastive Divergence are very inef-ficient.
This paper provides a novel esti-mator to speed up the learning based onNoise Contrastive Estimate, extended fordocuments of variant lengths and weightedinputs.
Experiments on two benchmarksshow that the new estimator achieves greatlearning efficiency and high accuracy ondocument retrieval and classification.1 IntroductionTopic models are powerful probabilistic graphicalapproaches to analyze document semantics in dif-ferent applications such as document categoriza-tion and information retrieval.
They are mainlyconstructed by directed structure like pLSA (Hof-mann, 2000) and LDA (Blei et al, 2003).
Accom-panied by the vast developments in deep learn-ing, several undirected topic models, such as(Salakhutdinov and Hinton, 2009; Srivastava etal., 2013), have recently been reported to achievegreat improvements in efficiency and accuracy.Replicated Softmax model (RSM) (Hinton andSalakhutdinov, 2009), a kind of typical undirectedtopic model, is composed of a family of RestrictedBoltzmann Machines (RBMs).
Commonly, RSMis learned like standard RBMs using approximatemethods like Contrastive Divergence (CD).
How-ever, CD is not really designed for RSM.
Differentfrom RBMs with binary input, RSM adopts soft-max units to represent words, resulting in great in-efficiency with sampling inside CD, especially fora large vocabulary.
Yet, NLP systems usually re-quire vocabulary sizes of tens to hundreds of thou-sands, thus seriously limiting its application.Dealing with the large vocabulary size of the in-puts is a serious problem in deep-learning-basedNLP systems.
Bengio et al (2003) pointed thisproblem out when normalizing the softmax proba-bility in the neural language model (NNLM), andMorin and Bengio (2005) solved it based on a hi-erarchical binary tree.
A similar architecture wasused in word representations like (Mnih and Hin-ton, 2009; Mikolov et al, 2013a).
Directed treestructures cannot be applied to undirected mod-els like RSM, but stochastic approaches can workwell.
For instance, Dahl et al (2012) found thatseveral Metropolis Hastings sampling (MH) ap-proaches approximate the softmax distribution inCD well, although MH requires additional com-plexity in computation.
Hyv?arinen (2007) pro-posed Ratio Matching (RM) to train unnormal-ized models, and Dauphin and Bengio (2013)added stochastic approaches in RM to accommo-date high-dimensional inputs.
Recently, a new es-timator Noise Contrastive Estimate (NCE) (Gut-mann and Hyv?arinen, 2010) is proposed for un-normalized models, and shows great efficiency inlearning word representations such as in (Mnihand Teh, 2012; Mikolov et al, 2013b).In this paper, we propose an efficient learningstrategy for RSM named ?-NCE, applying NCE asthe basic estimator.
Different from most related ef-forts that use NCE for predicting single word, ourmethod extends NCE to generate noise for doc-uments in variant lengths.
It also enables RSM touse weighted inputs to improve the modelling abil-ity.
As RSM is usually used as the first layer inmany deeper undirected models like Deep Boltz-mann Machines (Srivastava et al, 2013), ?-NCEcan be readily extended to learn them efficiently.2 Replicated Softmax ModelRSM is a typical undirected topic model, which isbased on bag-of-words (BoW) to represent docu-ments.
In general, it consists of a series of RBMs,162each of which contains variant softmax visibleunits but the same binary hidden units.Suppose K is the vocabulary size.
For a docu-ment with D words, if the ithword in the docu-ment equals the kthword of the dictionary, a vec-tor vi?
{0, 1}Kis assigned, only with the kthelement vik= 1.
An RBM is formed by assign-ing a hidden state h ?
{0, 1}Hto this documentV = {v1, ...,vD}, where the energy function is:E?
(V ,h) = ?hTWv?
?
bTv?
?D ?
aTh (1)where ?
= {W , b,a} are parameters shared byall the RBMs, and v?
=?Di=1viis commonly re-ferred to as the word count vector of a document.The probability for the document V is given by:P?
(V ) =1ZDe?F?
(V ), ZD=?Ve?F?
(V )F?
(V ) = log?he?E?
(V ,h)(2)where F?
(V ) is the ?free energy?, which can beanalytically integrated easily, and ZDis the ?par-tition function?
for normalization, only associatedwith the document length D. As the hidden stateand document are conditionally independent, theconditional distributions are derived:P?
(vik = 1|h) =exp(WTkh+ bk)?Kk=1exp(WTkh+ bk)(3)P?
(hj = 1|V ) = ?
(Wj v?
+D ?
aj) (4)where ?
(x) =11+e?x.
Equation (3) is the soft-max units describing the multinomial distributionof the words, and Equation (4) serves as an effi-cient inference from words to semantic meanings,where we adopt the probabilities of each hiddenunit ?activated?
as the topic features.2.1 Learning Strategies for RSMRSM is naturally learned by minimizing the nega-tive log-likelihood function (ML) as follows:L(?)
= ?EV ?Pdata [logP?
(V )] (5)However, the gradient is intractable for the combi-natorial normalization term ZD.
Common strate-gies to overcome this intractability are MCMC-based approaches such as Contrastive Divergence(CD) (Hinton, 2002) and Persistent CD (PCD)(Tieleman, 2008), both of which require repeatingGibbs steps of h(i)?
P?
(h|V(i)) and V(i+1)?P?
(V |h(i)) to generate model samples to approx-imate the gradient.
Typically, the performance andconsistency improve when more steps are adopted.Notwithstanding, even one Gibbs step is time con-suming for RSM, since the multinomial samplingnormally requires linear time computations.
The?alias method?
(Kronmal and Peterson Jr, 1979)speeds up multinomial sampling to constant timewhile linear time is required for processing the dis-tribution.
Since P?
(V |h) changes at every itera-tion in CD, such methods cannot be used.3 Efficient Learning for RSMUnlike (Dahl et al, 2012) that retains CD, weadopted NCE as the basic learning strategy.
Con-sidering RSM is designed for documents, we fur-ther modified NCE with two novel heuristics,developing the approach ?Partial Noise UniformContrastive Estimate?
(or ?-NCE for short).3.1 Noise Contrastive EstimateNoise Contrastive Estimate (NCE), similar to CD,is another estimator for training models with in-tractable partition functions.
NCE solves the in-tractability through treating the partition functionZDas an additional parameter ZcDadded to ?,which makes the likelihood computable.
Yet, themodel cannot be trained through ML as the likeli-hood tends to be arbitrarily large by setting ZcDtohuge numbers.
Instead, NCE learns the model in aproxy classification problem with noise samples.Given a document collection (data) {Vd}Td, andanother collection (noise) {Vn}Tnwith Tn= kTd,NCE distinguishes these (1+k)Tddocuments sim-ply based on Bayes?
Theorem, where we assumeddata samples matched by our model, indicatingP? '
Pdata, and noise samples generated from anartificial distribution Pn.
Parameters are learnedby minimizing the cross-entropy function:J(?)
= ?EVd?P?
[log ?k(X(Vd))]?kEVn?Pn[log ?k?1(?X(Vn))](6)and the gradient is derived as follows,???J(?)
=EVd?P?
[?k?1(?X)??X(Vd)]?kEVn?Pn[?k(X)?
?X(Vn)](7)where ?k(x) =11+ke?x, and the ?log-ratio?
is:X(V ) = log [P?
(V )/Pn(V )] (8)J(?)
can be optimized efficiently with stochasticgradient descent (SGD).
Gutmann and Hyv?arinen(2010) showed that the NCE gradient??J(?)
willreach the ML gradient when k ?
?.
In practice,a larger k tends to train the model better.1633.2 Partial Noise SamplingDifferent from (Mnih and Teh, 2012), which gen-erates noise per word, RSM requires the estimatorto sample the noise at the document level.
An in-tuitive approach is to sample from the empiricaldistribution p?
forD times, where the log probabil-ity is computed: logPn(V ) =?v?V[vTlog p?
].For a fixed k, Gutmann and Hyv?arinen (2010)suggested choosing the noise close to the data fora sufficient learning result, indicating full noisemight not be satisfactory.
We proposed an alter-native ?Partial Noise Sampling (PNS)?
to gener-ate noise by replacing part of the data with sam-pled words.
See Algorithm 1, where we fixed theAlgorithm 1 Partial Noise Sampling1: Initialize: k, ?
?
(0, 1)2: for each Vd= {v}D?
{Vd}Tddo3: Set: Dr= d?
?De4: Draw: Vr= {vr}Dr?
V uniformly5: for j = 1, ..., k do6: Draw: V(j)n= {v(j)n}D?Dr?
p?7: V(j)n= V(j)n?
Vr8: end for9: Bind: (Vd,Vr), (V(1)n,Vr), ..., (V(k)n,Vr)10: end forproportion of remaining words at ?, named ?noiselevel?
of PNS.
However, traversing all the condi-tions to guess the remaining words requiresO(D!)computations.
To avoid this, we simply bound theremaining words with the data and noise in ad-vance and the noise logPn(V ) is derived readily:logP?
(Vr) +?v?V \Vr[vTlog p?
](9)where the remaining words Vrare still assumedto be described by RSM with a smaller documentlength.
In this way, it also strengthens the robust-ness of RSM towards incomplete data.Sampling the noise normally requires additionalcomputational load.
Fortunately, since p?
is fixed,sampling is efficient using the ?alias method?.
Italso allows storing the noise for subsequent use,yielding much faster computation than CD.3.3 Uniform Contrastive EstimateWhen we initially implemented NCE for RSM,we found the document lengths terribly biased thelog-ratio, resulting in bad parameters.
Therefore?Uniform Contrastive Estimate (UCE)?
was pro-posed to accommodate variant document lengthsby adding the uniform assumption:?X(V ) = D?1log [P?
(V )/Pn(V )] (10)where UCE adopts the uniform probabilitiesD?P?andD?Pnfor classification to average the mod-elling ability at word-level.
Note that D is notnecessarily an integer in UCE, and allows choos-ing a real-valued weights on the document such asidf -weighting (Salton and McGill, 1983).
Typi-cally, it is defined as a weighting vector w, wherewk= logTd|V ?
{Vd}:vik=1,vi?V | is multiplied to thekthword in the dictionary.
Thus for a weighted in-put Vwand corresponding length Dw, we derive:?X(Vw) = Dw?1log [P?
(Vw)/Pn(Vw)] (11)where logPn(Vw) =?vw?V w[vwTlog p?].
Aspecific ZcDwwill be assigned to P?
(Vw).Combining PNS and UCE yields a new estima-tor for RSM, which we simply call ?-NCE1.4 Experiments4.1 Datasets and Details of LearningWe evaluated the new estimator to train RSMs ontwo text datasets: 20 Newsgroups and IMDB.The 20 Newsgroups2dataset is a collection ofthe Usenet posts, which contains 11,345 trainingand 7,531 testing instances.
Both the training andtesting sets are labeled into 20 classes.
Removingstop words as well as stemming were performed.The IMDB dataset3is a benchmark for senti-ment analysis, which consists of 100,000 moviereviews taken from IMDB.
The dataset is dividedinto 75,000 training instances (1/3 labeled and2/3 unlabeled) and 25,000 testing instances.
Twotypes of labels, positive and negative, are given toshow sentiment.
Following (Maas et al, 2011), nostop words are removed from this dataset.For each dataset, we randomly selected 10% ofthe training set for validation, and the idf -weightvector is computed in advance.
In addition, replac-ing the word count?v by dlog (1 +?v)e slightly im-proved the modelling performance for all models.We implemented ?-NCE according to the pa-rameter settings in (Hinton, 2010) using SGD inminibatches of size 128 and an initialized learningrate of 0.1.
The number of hidden units was fixed1?
comes from the noise level in PNS, but UCE is alsothe vital part of this estimator, which is absorbed in ?-NCE.2Available at http://qwone.com/?jason/20Newsgroups3Available at http://ai.stanford.edu/?amaas/data/sentiment164at 128 for all models.
Although learning the parti-tion function ZcDseparately for every length D isnearly impossible, as in (Mnih and Teh, 2012) wealso surprisingly found freezing ZcDas a constantfunction of D without updating never harmed butactually enhanced the performance.
It is proba-bly because the large number of free parametersin RSM are forced to learn better when ZcDis aconstant.
In practise, we set this constant functionas ZcD= 2H?(?kebk)D.
It can readily extend tolearn RSM for real-valued weighted length Dw.We also implemented CD with the same set-tings.
All the experiments were run on a singleGPU GTX970 using the library Theano (Bergstraet al, 2010).
To make the comparison fair, both?-NCE and CD share the same implementation.4.2 Evaluation of EfficiencyTo evaluate the efficiency in learning, we usedthe most frequent words as dictionaries with sizesranging from 100 to 20, 000 for both datasets, andtest the computation time both for CD of vari-ant Gibbs steps and ?-NCE of variant noise sam-ple sizes.
The comparison of the mean runningFigure 1: Comparison of running timetime per minibatch is clearly shown in Figure 1,which is averaged on both datasets.
Typically,?-NCE achieves 10 to 500 times speed-up com-pared to CD.
Although both CD and ?-NCE runslower when the input dimension increases, CDtends to take much more time due to the multino-mial sampling at each iteration, especially whenmore Gibbs steps are used.
In contrast, runningtime stays reasonable in ?-NCE even if a largernoise size or a larger dimension is applied.4.3 Evaluation of PerformanceOne direct measure to evaluate the modelling per-formance is to assess RSM as a generative modelto estimate the log-probability per word as per-plexity.
However, as ?-NCE learns RSM by dis-tinguishing the data and noise from their respec-tive features, parameters are trained more like afeature extractor than a generative model.
It is notfair to use perplexity to evaluate the performance.For this reason, we evaluated the modelling per-formance with some indirect measures.Figure 2: Precision-Recall curves for the retrievaltask on the 20 Newsgroups dataset using RSMs.For 20 Newsgroups, we trained RSMs on thetraining set, and reported the results on docu-ment retrieval and document classification.
Forretrieval, we treated the testing set as queries, andretrieved documents with the same labels in thetraining set by cosine-similarity.
Precision-recall(P-R) curves and mean average precision (MAP)are two metrics we used for evaluation.
For clas-sification, we trained a softmax regression on thetraining set, and checked the accuracy on the test-ing set.
We use this dataset to show the modellingability of RSM with different estimators.For IMDB, the whole training set is used forlearning RSMs, and an L2-regularized logistic re-gression is trained on the labeled training set.
Theerror rate of sentiment classification on the testingset is reported, compared with several BoW-basedbaselines.
We use this dataset to show the generalmodelling ability of RSM compared with others.We trained both ?-NCE and CD, and naturallyNCE (without UCE) at a fixed vocabulary size(2000 for 20 Newsgroups, and 5000 for IMDB).Posteriors of the hidden units were used as topicfeatures.
For ?-NCE , we fixed noise level at 0.5for 20 Newsgroups and 0.3 for IMDB.
In compar-ison, we trained CD from 1 up to 5 Gibbs steps.Figure 2 and Table 1 show that a larger noisesize in ?-NCE achieves better modelling perfor-165(a) MAP for document retrieval (b) Document classification accuracy (c) Sentiment classification accuracyFigure 3: Tracking the modelling performance with variant ?
using ?-NCE to learn RSMs.
CD is alsoreported as the baseline.
(a) (b) are performed on 20 Newsgroups, and (c) is performed on IMDB.mance, and ?-NCE greatly outperforms CD on re-trieval tasks especially around large recall values.The classification results of ?-NCE is also compa-rable or slightly better than CD.
Simultaneously,it is gratifying to find that the idf -weighting in-puts achieve the best results both in retrieval andclassification tasks, as idf -weighting is known toextract information better than word count.
In ad-dition, naturally NCE performs poorly comparedto others in Figure 2, indicating variant documentlengths actually bias the learning greatly.CD?-NCEk=1 k=5 k=25 k=25 (idf)64.1% 61.8% 63.6% 64.8% 65.6%Table 1: Comparison of classification accuracy onthe 20 Newsgroups dataset using RSMs.Models AccuracyBag of Words (BoW) (Maas and Ng, 2010) 86.75%LDA (Maas et al, 2011) 67.42%LSA (Maas et al, 2011) 83.96%Maas et al (2011)?s ?full?
model 87.44%WRRBM (Dahl et al, 2012) 87.42%RSM:CD 86.22%RSM:?-NCE-5 87.09%RSM:?-NCE-5 (idf) 87.81%Table 2: The performance of sentiment classifica-tion accuracy on the IMDB dataset using RSMscompared to other BoW-based approaches.On the other hand, Table 2 shows the perfor-mance of RSM in sentiment classification, wheremodel combinations reported in previous effortsare not considered.
It is clear that ?-NCE learnsRSM better than CD, and outperforms BoW andother BoW-based models4such as LDA.
The idf -4Accurately, WRRBM uses ?bag of n-grams?
assumption.weighting inputs also achieve the best perfor-mance.
Note that RSM is also based on BoW, in-dicating ?-NCE has arguably reached the limits oflearning BoW-based models.
In future work, RSMcan be extended to more powerful undirected topicmodels, by considering more syntactic informa-tion such as word-order or dependency relation-ship in representation.
?-NCE can be used to learnthem efficiently and achieve better performance.4.4 Choice of Noise Level-?In order to decide the best noise level (?)
for PNS,we learned RSMs using ?-NCE with differentnoise levels for both word count and idf -weightinginputs on the two datasets.
Figure 3 shows that?-NCE learning with partial noise (?
> 0) out-performs full noise (?
= 0) in most situations,and achieves better results than CD in retrieval andclassification on both datasets.
However, learningtends to become extremely difficult if the noisebecomes too close to the data, and this explainswhy the performance drops rapidly when ?
?
1.Furthermore, curves in Figure 3 also imply thechoice of ?
might be problem-dependent, withlarger sets like IMDB requiring relatively smaller?.
Nonetheless, a systematic strategy for choos-ing optimal ?
will be explored in future work.
Inpractise, a range from 0.3 ?
0.5 is recommended.5 ConclusionsWe propose a novel approach ?-NCE for learningundirected topic models such as RSM efficiently,allowing large vocabulary sizes.
It is new a es-timator based on NCE, and adapted to documentswith variant lengths and weighted inputs.
We learnRSMs with ?-NCE on two classic benchmarks,where it achieves both efficiency in learning andaccuracy in retrieval and classification tasks.166ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference(SciPy), June.
Oral Presentation.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.George E Dahl, Ryan P Adams, and Hugo Larochelle.2012.
Training restricted boltzmann machines onword observations.
arXiv preprint arXiv:1202.5695.Yann Dauphin and Yoshua Bengio.
2013.
Stochasticratio matching of rbms for sparse high-dimensionalinputs.
In Advances in Neural Information Process-ing Systems, pages 1340?1348.Michael Gutmann and Aapo Hyv?arinen.
2010.
Noise-contrastive estimation: A new estimation princi-ple for unnormalized statistical models.
In Inter-national Conference on Artificial Intelligence andStatistics, pages 297?304.Geoffrey E Hinton and Ruslan R Salakhutdinov.
2009.Replicated softmax: an undirected topic model.
InAdvances in neural information processing systems,pages 1607?1614.Geoffrey Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural com-putation, 14(8):1771?1800.Geoffrey Hinton.
2010.
A practical guide to train-ing restricted boltzmann machines.
Momentum,9(1):926.Thomas Hofmann.
2000.
Learning the similarity ofdocuments: An information-geometric approach todocument retrieval and categorization.Aapo Hyv?arinen.
2007.
Some extensions of scorematching.
Computational statistics & data analysis,51(5):2499?2512.Richard A Kronmal and Arthur V Peterson Jr. 1979.On the alias method for generating random variablesfrom a discrete distribution.
The American Statisti-cian, 33(4):214?218.Andrew L Maas and Andrew Y Ng.
2010.
A prob-abilistic model for semantic word vectors.
In NIPSWorkshop on Deep Learning and Unsupervised Fea-ture Learning.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 142?150.
As-sociation for Computational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Andriy Mnih and Geoffrey E Hinton.
2009.
A scal-able hierarchical distributed language model.
InAdvances in neural information processing systems,pages 1081?1088.Andriy Mnih and Yee Whye Teh.
2012.
A fast andsimple algorithm for training neural probabilisticlanguage models.
arXiv preprint arXiv:1206.6426.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international workshop on artifi-cial intelligence and statistics, pages 246?252.
Cite-seer.Ruslan Salakhutdinov and Geoffrey Hinton.
2009.
Se-mantic hashing.
International Journal of Approxi-mate Reasoning, 50(7):969?978.Gerard Salton and Michael J McGill.
1983.
Introduc-tion to modern information retrieval.Nitish Srivastava, Ruslan R Salakhutdinov, and Ge-offrey E Hinton.
2013.
Modeling documentswith deep boltzmann machines.
arXiv preprintarXiv:1309.6865.Tijmen Tieleman.
2008.
Training restricted boltz-mann machines using approximations to the likeli-hood gradient.
In Proceedings of the 25th interna-tional conference on Machine learning, pages 1064?1071.
ACM.167
