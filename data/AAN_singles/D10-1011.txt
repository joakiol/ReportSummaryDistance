Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 108?115,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsPositional Language Models for Clinical Information RetrievalFlorian BoudinDIRO, Universite?
de Montre?alCP.
6128, succ.
Centre-villeH3C 3J7 Montre?al, Canadaboudinfl@iro.umontreal.caJian-Yun NieDIRO, Universite?
de Montre?alCP.
6128, succ.
Centre-villeH3C 3J7 Montre?al, Canadanie@iro.umontreal.caMartin DawesDepartment of Family MedicineMcGill University, 515 Pine AveH2W 1S4 Montre?al, Canadamartin.dawes@mcgill.caAbstractThe PECO framework is a knowledge repre-sentation for formulating clinical questions.Queries are decomposed into four aspects,which are Patient-Problem (P), Exposure (E),Comparison (C) and Outcome (O).
However,no test collection is available to evaluate suchframework in information retrieval.
In thiswork, we first present the construction of alarge test collection extracted from system-atic literature reviews.
We then describe ananalysis of the distribution of PECO elementsthroughout the relevant documents and pro-pose a language modeling approach that usesthese distributions as a weighting strategy.
Inour experiments carried out on a collection of1.5 million documents and 423 queries, ourmethod was found to lead to an improvementof 28% in MAP and 50% in P@5, as com-pared to the state-of-the-art method.1 IntroductionIn recent years, the volume of health and biomedi-cal literature available in electronic form has grownexponentially.
MEDLINE, the authoritative reposi-tory of citations from the medical and bio-medicaldomain, contains more than 18 million citations.Searching for clinically relevant information withinthis large amount of data is a difficult task that med-ical professionals are often unable to complete in atimely manner.
A better access to clinical evidencerepresents a high impact application for physicians.Evidence-Based Medicine (EBM) is a widely ac-cepted paradigm for medical practice (Sackett et al,1996).
EBM is defined as the conscientious, explicitand judicious use of current best evidence in makingdecisions about patient care.
Practice EBM meansintegrating individual clinical expertise with the bestavailable external clinical evidence from systematicresearch.
It involves tracking down the best evi-dence from randomized trials or meta-analyses withwhich to answer clinical questions.
Richardson etal.
(1995) identified the following four aspects as thekey elements of a well-built clinical question:?
Patient-problem: what are the patient charac-teristics (e.g.
age range, gender, etc.)?
What isthe primary condition or disease??
Exposure-intervention: what is the main in-tervention (e.g.
drug, treatment, duration, etc.)??
Comparison: what is the exposure comparedto (e.g.
placebo, another drug, etc.)??
Outcome: what are the clinical outcomes (e.g.healing, morbidity, side effects, etc.
)?These elements are known as the PECO elements.Physicians are educated to formulate their clinicalquestions in respect to this structure.
For example, inthe following question: ?In patients of all ages withParkinson?s disease, does a Treadmill training com-pared to no training allows to increase the walkingdistance??
one can identify the following elements:?
P: Patients of all ages with Parkinson?s disease?
E: Treadmill training?
C: No treadmill training?
O: Walking distanceIn spite of this well-defined question structure,physicians still use keyword-based queries whenthey search for clinical evidence.
An explanation of108that is the almost total absence of PECO search in-terfaces.
PubMed1, the most used search interface,does not allow users to formulate PECO queriesyet.
For the previously mentioned clinical question,a physician would use the query ?Treadmill ANDParkinson?s disease?.
There is intuitively much togain by using a PECO structured query in the re-trieval process.
This structure specifies the role ofeach concept in the desired documents, which isa clear advantage over a keyword-based approach.One can for example differentiate two queries inwhich a disease would be a patient condition or aclinical outcome.
This conceptual decomposition ofqueries is also particularly useful in a sense that itcan be used to balance the importance of each ele-ment in the search process.Another important factor that prevented re-searchers from testing approaches to clinical infor-mation retrieval (IR) based on PECO elements isthe lack of a test collection, which contains a set ofdocuments, a set of queries and the relevance judg-ments.
The construction of such a test collection iscostly in manpower.
In this paper, we take advan-tage of the systematic reviews about clinical ques-tions from Cochrane.
Each Cochrane review ex-amines in depth a clinical question and survey allthe available relevant publications.
The reviews arewritten for medical professionals.
We transformedthem into a TREC-like test collection, which con-tains 423 queries and 8926 relevant documents ex-tracted from MEDLINE.
In a second part of this pa-per, we present a model integrating the PECO frame-work in a language modeling approach to IR.
An in-tuitive method would try to annotate the conceptsin documents into PECO categories.
One can thenmatch the PECO elements in the query to the ele-ments detected in documents.
However, as previousstudies have shown, it is very difficult to automat-ically annotate accurately PECO elements in docu-ments.
To by-pass this issue, we propose an alter-native that relies on the observed positional distri-bution of these elements in documents.
We will seethat different types of element have different distri-butions.
By weighting words according to their posi-tions, we can indirectly weigh the importance of dif-ferent types of element in search.
As we will show1www.pubmed.govin this paper, this approach turns out to be highlyeffective.This paper is organized as follows.
We first brieflyreview the previous work, followed by a descriptionof the test collection we have constructed.
Next,we give the details of the method we propose andpresent our experiments and results.
Lastly, we con-clude with a discussion and directions for furtherwork.2 Related workThe need to answer clinical questions related to apatient care using IR systems has been well stud-ied and documented (Hersh et al, 2000; Niu et al,2003; Pluye et al, 2005).
There are a limited butgrowing number of studies trying to use the PECOelements in the retrieval process.
(Demner-Fushmanand Lin, 2007) is one of the few such studies, inwhich a series of knowledge extractors is used todetect PECO elements in documents.
These ele-ments are later used to re-rank a list of retrieved ci-tations from PubMed.
Results reported indicate thattheir method can bring relevant citations into higher-ranking positions, and from these abstracts gener-ate responses that answer clinicians?
questions.
Thisstudy demonstrates the value of the PECO frame-work as a method for structuring clinical questions.However, as the focus has been put on the post-retrieval step (for question-answering), it is not clearwhether PECO elements are useful at the retrievalstep.
Intuitively, the integration of PECO elementsin the retrieval process can also lead to higher re-trieval effectiveness.The most obvious scenario for testing this wouldbe to recognize PECO elements in documents priorto indexing.
When a PECO-structured query is for-mulated, it is matched against the PECO elementsin the documents (Dawes et al, 2007).
Neverthe-less, the task of automatically identifying PECO el-ements is a very difficult one.
There are two majorreasons for that.
First, previous studies have indi-cated that there is a low to moderate agreement rateamong humans for annotating PECO elements.
Thisis due to the lack of standard definition for the el-ement?
boundaries (e.g.
can be words, phrases orsentences) but also to the existence of several lev-els of annotation.
Indeed, there are a high number109of possible candidates for each element and one hasto choose if it is a main element (i.e.
playing a ma-jor role in the clinical study) or secondary elements.Second is the lack of sufficient annotated data thatcan be used to train automatic tagging tools.Despite all these difficulties, several efficientdetection methods have been proposed (Demner-Fushman and Lin, 2007; Chung, 2009).
Nearly allof them are however restricted to a coarse-grain an-notation level (i.e.
tagging entire sentences as de-scribing one element).
This kind of coarser-grainidentification is more robust and more feasible thanthe one at concept level, and it could be sufficient inthe context of IR.
In fact, for IR purposes, what isthe most important is to correctly weight the wordsin documents and queries.
From this perspective,an annotation at the sentence level may be suffi-cient.
Notwithstanding, experiments conducted us-ing a collection of documents that were annotated ata sentence-level only showed a small increase in re-trieval accuracy (Boudin et al, 2010b) compared toa traditional bag-of-words approach.More recently, Boudin et al (2010a) proposed analternative to the PECO detection issue that relieson assigning different weights to words according totheir positions in the document.
A location-basedweighting strategy is used to emphasize the mostinformative parts of documents.
They show thata large improvement in retrieval effectiveness canbe obtained this way and indicate that the weightslearned automatically are correlated to the observeddistribution of PECO elements in documents.
In thiswork, we propose to go one step further in this direc-tion by analyzing the distribution of PECO elementsin a large number of documents and define the posi-tional probabilities of PECO elements accordingly.These probabilities will be integrated in the docu-ment language model.3 Construction of the test collectionDespite the increasing use of search engines by med-ical professionals, there is no standard test collectionfor evaluating clinical IR.
Constructing such a re-source from scratch would require considerable timeand money.
One way to overcome this obstacle isto use already available systematic reviews.
Sys-tematic reviews try to identify, appraise, select andsynthesize all high quality research evidence rele-vant to a clinical question.
The best-known sourceof systematic reviews in the healthcare domain is theCochrane collaboration2.
It consists of a group ofover 15,000 specialists who systematically identifyand review randomized trials of the effects of treat-ments.
In particular, a review contains a referencesection, listing all the relevant studies to the clinicalquestion.
These references can be considered as rel-evant documents.
In our work, we propose to usethese reviews as a way to semi-automatically build atest collection.
As the reviews are made by special-ists in the area independently from our study, we canavoid bias in our test collection.We gathered a subset of Cochrane systematic re-views and asked a group of annotators, one professorand four Master students in family medicine, to cre-ate PECO-structured queries corresponding to theclinical questions.
As clinical questions answeredin these reviews cover various aspects of one topic,multiple variants of precise PECO queries were gen-erated for each review.
Moreover, in order to be ableto compare a PECO-based search strategy to a realworld scenario, this group have also provided thekeyword-based queries that they would have usedto search with PubMed.
Below is an example ofqueries generated from the systematic review about?Aspirin with or without an antiemetic for acute mi-graine headaches in adults?
:Keyword-based query[aspirin and migraine]PECO-structured queries1.
[adults 18 years or more with migraine]P[aspirin alone]E[placebo]C[pain free]O2.
[adults 18 years or more with migraine]P[aspirin plus an antiemetic]E[placebo]C[pain free]O3.
[adults 18 years or more with migraine]P[aspirin plus metoclopramide]E[active comparator]C[use of rescue medication]O2www.cochrane.org110All the citations included in the ?References?
sec-tion of the systematic review were extracted andselected as relevant documents.
These citationswere manually mapped to PubMed unique identi-fiers (PMID).
This is a long process that was under-taken by two different workers to minimize the num-ber of errors.
At this step, only articles published injournals referenced in PubMed are considered (e.g.conference proceedings are not included).0 20 40 60 80 100 120Number of references in each review0510152025NumberofsystematicreviewsFigure 1: Histogram of the number of queries versus thenumber of relevant documents.We selected in sequential order from the setof new systematic reviews3 and processed 156Cochrane reviews.
There was no restriction aboutthe topics covered or the number of included refer-ences.
The resulting test collection is composed of423 queries and 8926 relevant citations (2596 differ-ent citations).
This number reduces to 8138 citationsonce we remove the citations without any text in theabstract (i.e.
certain citations, especially old ones,only contain a title).
Figure 1 shows the statisticsderived from the number of relevant documents byquery.
In this test collection, the average number ofdocuments per query is approximately 19 while theaverage length of a document is 246 words.4 Distribution of PECO elementsThe observation that PECO elements are not evenlydistributed throughout the documents is not new.
Infact, most existing tagging methods used location-based features.
This information turns out to be veryuseful because of the standard structure of medicalcitations.
Actually, many scientific journals explic-itly recommend authors to write their abstracts in3http://mrw.interscience.wiley.com/cochrane/cochrane clsysrev new fs.htmlcompliance to the ordered rhetorical structure: In-troduction, Methods, Results and Discussion.
Theserhetorical categories are highly correlated to the dis-tributions of PECO elements, as some elements aremore likely to occur in certain categories (e.g.
clin-ical outcomes are more likely to appear in the con-clusion).
The position is thus a strong indicator ofwhether a text segment contains a PECO element ornot.To the best of our knowledge, the first analysisof the distribution of PECO elements in documentswas described in(Boudin et al, 2010a).
A small col-lection of manually annotated abstracts was used tocompute the probability that a PECO element oc-curs in a specific part of the documents.
This studyis however limited by the small number of anno-tated documents (approximately 50 citations) andthe moderate agreement rate among human annota-tors.
Here we propose to use our test collection tocompute more reliable statistics.The idea is to use the pairs of PECO-structuredquery and relevant document, assuming that if a doc-ument is relevant then it should contain the sameelements as the query.
Of course, this is obvi-ously not always the case.
Errors can be introducedby synonyms or homonyms and relevant documentsmay not contain all of the elements described in thequery.
But, with more than 8100 documents, it isquite safe to say that this method produce fairly reli-able results.
Moreover, a filtering process is appliedto queries removing all non-informative words (e.g.stopwords, numbers, etc.)
from being counted.There are several ways to look at the distributionof PECO elements in documents.
One can use therhetorical structure of abstracts to do that.
However,the high granularity level of such analysis wouldmake it less precise for IR purposes.
Furthermore,most of the citations available in PubMed are de-void of explicitly marked sections.
It is possible toautomatically detect these sections but only with anon-negligible error rate (McKnight and Srinivasan,2003).
In our study, we chose to use a fixed num-ber of partitions by dividing documents into parts ofequal length.
This choice is motivated by its repeata-bility and ease to implement, but also for compari-son with previous studies.We divided each relevant document into 10 partsof equal length on a word level (from P1 to P10).
We111computed statistics on the number of query wordsthat occur in each of these parts.
For each PECO el-ement, the distribution of query words among theparts of the documents is not uniform (Figure 2).We observe distinctive distributions, especially forPatient-Problem and Exposure elements, indicatingthat first and last parts of the documents have higherchance to contain these elements.
This gives us aclear and robust indication on which specific partsshould be enhanced when searching for a given el-ement.
Our proposed model will exploit the typicaldistributions of PECO elements in documents.P1 P2 P3 P4 P5 P6 P7 P8 P9 P100.000.050.100.150.200.25 P elementsP1 P2 P3 P4 P5 P6 P7 P8 P9 P10E elementsP1 P2 P3 P4 P5 P6 P7 P8 P9 P10                                           Parts of the documents0.000.050.100.150.200.25Proportionof PECOelements inpartC elementsP1 P2 P3 P4 P5 P6 P7 P8 P9 P10O elementsFigure 2: Distribution of each PECO element throughoutthe different parts of the documents.5 Retrieval MethodIn this work, we use the language modeling ap-proach to information retrieval.
This approach as-sumes that queries and documents are generatedfrom some probability distribution of text (Ponte andCroft, 1998).
Under this assumption, ranking a doc-ument D as relevant to a query Q is seen as estimat-ing P(Q|D), the probability thatQwas generated bythe same distribution as D. A typical way to scorea document D as relevant to a query Q is to com-pute the Kullback-Leibler divergence between theirrespective language models:score(Q,D) =?w?QP(w|Q) ?
logP(w|D) (1)Under the traditional bag-of-words assumption,i.e.
assuming that there is no need to model term de-pendence, a simple estimate for P(w|Q) can be ob-tained by computing Maximum Likelihood Estima-tion (MLE).
It is calculated as the number of timesthe word w appears in the query Q, divided by itslength:P(w|Q) =count(w,Q)|Q|A similar method is employed for estimatingP(w|D).
Bayesian smoothing using Dirichlet pri-ors is however applied to the maximum likelihoodestimator to compensate for data sparseness (i.e.smoothing probabilities to remove zero estimates).Given ?
the prior parameter and C the collection ofdocuments, P(w|D) is computed as:P(w|D) =count(w,D) + ?
?
P(w|C)|D| + ?5.1 Model definitionIn our model, we propose to use the distribution ofPECO elements observed in documents to empha-size the most informative parts of the documents.The idea is to get rid of the problem of preciselydetecting PECO elements by using a positional lan-guage model.
To integrate position, we estimatea series of probabilities that constraints the wordcounts to a specific part of the documents instead ofthe entire document.
Each document D is ranked bya weighted linear interpolation.
Given a documentD divided in 10 parts p ?
[P1, P2 ?
?
?P10], P(w|D)in equation 1 is redefined as:P ?
(w|D) = ?
?
P(w|D) + ?
?
Ptitle(w|D)+?
?
?pi?D?e ?
Ppi(w|D) (2)where the ?e weights for each type of element eare empirically fixed to the values of the distributionof PECO elements observed in documents.
We thenredefine the scoring function to integrate the PECOquery formulation.
The idea is to use the PECOstructure as a way to balance the importance of eachelement in the retrieval step.
The final scoring func-tion is defined as:scorefinal(Q,D) =?e?PECO?e ?
score(Qe, D)112In our model, there are a total of 7 weighting pa-rameters, 4 corresponding to the PECO elements inqueries (?P, ?E, ?C and ?O) and 3 for the documentlanguage models (?, ?
and ?).
These parameterswill be determined by cross-validation.6 ResultsIn this section, we first describe the details of ourexperimental protocol.
Then, we present the resultsobtained by our model on the constructed test col-lection.6.1 Experimental settingsAs a collection of documents, we gathered 1.5 mil-lions of citations from PubMed.
We used the fol-lowing constraints: citations with an abstract, hu-man subjects, and belonging to one of the follow-ing publication types: randomized control trials, re-views, clinical trials, letters, editorials and meta-analyses.
The set of queries and relevance judg-ments described in Section 3 is used to evaluateour model.
Relevant documents were, if not al-ready included, added to the collection.
Becauseeach query is generated from a systematic literaturereview completed at a time t, we placed an addi-tional restriction on the publication date of the re-trieved documents: only documents published be-fore time t are considered.
Before indexing, eachcitation is pre-processed to extract its title and ab-stract text and then converted into a TREC-like doc-ument format.
Abstracts are divided into 10 parts ofequal length (the ones containing less than 10 wordsare discarded).
The following fields are marked ineach document: title, P1, P2 ?
?
?
P10.
The followingevaluation measures are used:?
Precision at rank n (P@n): precision computedon the n topmost retrieved documents.?
Mean Average Precision (MAP): average ofprecision measures computed at the point ofeach relevant document in the ranked list.?
Number of relevant documents retrievedAll retrieval tasks are performed using an ?out-of-the-shelf?
version of the Lemur toolkit4.
We usethe embedded tokenization algorithm along with the4www.lemurproject.orgstandard Porter stemmer.
The number of retrieveddocuments is set to 1000 and the Dirichlet priorsmoothing parameter to ?
= 2000.
In all our exper-iments, we use the KL divergence scoring function(equation 1) as baseline.
Statistical significance iscomputed using the well-known Student?s t-test.
Todetermine reasonable weights and avoid overtuningthe parameters, we use a 10-fold cross-validation op-timizing the MAP values.6.2 ExperimentsWe first investigated the impact of using PECO-structured queries on the retrieval performance.
Asfar as we know, no quantitative evaluation of theincrease or decrease of performance in comparisonwith a keyword-based search strategy has been re-ported.
Schardt et al (2007) presented a compari-son between PubMed and a PECO search interfacebut failed to demonstrate any significant differencebetween the two search protocols.
The larger num-ber of words in PECO-structured queries, on aver-age 18.8 words per query compared to 4.3 words forkeyword queries, should capture more aspects of theinformation need.
But, it may also be a disadvan-tage due to the fact that more noise can be broughtin, causing query-drift issues.We propose two baselines using the keyword-based queries.
The first baseline (named Baseline-1) uses keyword queries with the traditional lan-guage modeling approach.
This is one of the state-of-the-art approaches in current IR research.
Thisretrieval model considers each word in a query asan equal, independent source of information.
In thesecond baseline (named Baseline-2), we considermultiword phrases.
In our test collection, queriesare often composed of multiword phrases such as?low back pain?
or ?early pregnancy?.
It is clearthat finding the exact phrase ?heart failure?
is amuch stronger indicator of relevance than just find-ing ?heart?
and ?failure?
scattered within a docu-ment.
The Indri operator #1 is used to performphrase-based retrieval.
Phrases are already indicatedin queries by the conjunction and (e.g.
vaccine andhepatitis B).
A simple regular expression is used torecognize the phrases.Results are presented in Table 1.
As expected,phrase-based retrieval leads to some increase in re-trieval precision (P@5).
However, the number of113relevant documents retrieved is decreased.
This isdue to the fact that we use exact phrase matchingthat can reduce query coverage.
One solution wouldbe to use unordered window features (Indri operator#uwn) that would require words to be close togetherbut not necessarily in an exact sequence order (Met-zler and Croft, 2005).The PECO queries use PECO-structured queriesas a bag of words.
We observe that PECO queriesdo not enhance the average precision but increasethe P@5 significantly.
The number of relevant doc-uments retrieved is also larger.
These results indi-cate that formulating clinical queries according tothe PECO framework enhance the retrieval effec-tiveness.Model MAP P@5 #rel.
ret.Baseline-1 0.129 0.151 5369Baseline-2 0.128 0.161?
4645PECO-queries 0.126 0.172?
5433Table 1: Comparing the performance measures ofkeyword-based and PECO-structured queries in terms ofMAP, precision at 5 and number of relevant documentsretrieved (#rel.
ret.).
(?
: t.test < 0.05)In a second series of experiments, we evaluatedthe model we proposed in Section 5 .
We comparedtwo variants of our model.
The first variant (namedModel-1) uses a global ?e distribution fixed accord-ing to the average distribution of all PECO elements(i.e.
the observed probability that a PECO elementoccurs in a document?
part, no matter which elementit is).
The second variant (named Model-2) uses adifferentiated ?e distribution for each type of PECOelement.
The idea is to see if, given the fact thatPECO elements have different distributions in docu-ments, using an adapted weight distribution for eachelement can improve the retrieval effectiveness.Previous studies have shown that assigning a dif-ferent weight to each PECO element in the queryleads to better results (Demner-Fushman and Lin,2007; Boudin et al, 2010a).
In order to compareour model with a similar method, we defined anotherbaseline (named Baseline-3) by fixing the parame-ters ?
= 0 and ?
= 0 in equation 2.
We performeda grid search (from 0 to 1 by step of 0.1) to findthe optimal ?
weights.
Regarding the last three pa-rameters in our full models, namely ?, ?
and ?, weconducted a second grid search to find their optimalvalues.
Performance measures obtained in 10-foldcross-validation (optimizing the MAP measure) bythese models are presented in Table 2.A significant improvement is obtained bythe Baseline-3 over the keyword-based approach(Baseline-2).
The PECO decomposition of queriesis particularly useful to balance the importance ofeach element in the scoring function.
We observe alarge improvement in retrieval effectiveness for bothmodels over the two baselines.
This strongly indi-cates that a weighting scheme based on the word po-sition in documents is effective.
These results sup-port our assumption that the distribution of PECOelements in documents can be used to weight wordsin the document language model.However, we do not observe meaningful differ-ences between Model-1 and Model-2.
This tend tosuggest that a global distribution is likely more ro-bust for IR purposes than separate distributions foreach type of element.
Another possible reason is thatour direct mapping from positional distribution toprobabilities may not be the most appropriate.
Onemay think about using a different transformation, orperforming some smoothing.
We will leave this forour future work.7 ConclusionThis paper first presented the construction of a testcollection for evaluating clinical information re-trieval.
From a set of systematic reviews, a groupof annotators were asked to generate structured clin-ical queries and collect relevance judgments.
Theresulting test collection is composed of 423 queriesand 8926 relevant documents.
This test collectionprovides a basis for researchers to experiment withPECO-structured queries in clinical IR.
The test col-lection introduced in this paper, along with the man-ual given to the group of annotators, will be availablefor download5.In a second step, this paper addressed the prob-lem of using the PECO framework in clinical IR.
Astraightforward idea is to identify PECO elements indocuments and use the elements in the retrieval pro-cess.
However, this approach does not work well be-5http://www-etud.iro.umontreal.ca/?boudinfl/pecodr/114Model MAP % rel.
P@5 % rel.
#rel.
ret.Baseline-2 0.128 - 0.161 - 4645Baseline-3 0.144 +12.5%?
0.196 +21.7%?
5780Model-1 0.164 +28.1%?
0.241 +49.7%?
5768Model-2 0.163 +27.3%?
0.240 +49.1%?
5770Table 2: 10-fold cross validation scores for the Baseline-2, Baseline-3 and the two variants of our proposed model(Model-1 and Model-2).
Relative increase over the Baseline-2 is given, #rel.
ret.
is the number of relevant documentsretrieved.
(?
: t.test < 0.01, ?
: t.test < 0.05)cause of the difficulty to automatically detect theseelements.
Instead, we proposed a less demandingapproach that uses the distribution of PECO ele-ments in documents to re-weight terms in the doc-ument model.
The observation of variable distribu-tions in our test collection led us to believe that theposition information can be used as a robust indica-tor of the presence of a PECO element.
This strategyturns out to be promising.
On a data set composedof 1.5 million citations extracted with PubMed, ourbest model obtains an increase of 28% for MAPand nearly 50% for P@5 over the classical languagemodeling approach.In future work, we intend to expand our analy-sis of the distribution of PECO elements to a largernumber of citations.
One way to do that wouldbe to automatically extract PubMed citations thatcontain structural markers associated to PECO cate-gories (Chung, 2009).ReferencesFlorian Boudin, Jian-Yun Nie, and Martin Dawes.
2010a.Clinical Information Retrieval using Document andPICO Structure.
In Proceedings of the HLT-NAACL2010 conference, pages 822?830.Florian Boudin, Lixin Shi, and Jian-Yun Nie.
2010b.
Im-proving Medical Information Retrieval with PICO El-ement Detection.
In Proceedings of the ECIR 2010conference, pages 50?61.Grace Y. Chung.
2009.
Sentence retrieval for abstractsof randomized controlled trials.
BMC Medical Infor-matics and Decision Making, 9(1).Thomas Owens Sheri Keitz Connie Schardt, MarthaB Adams and Paul Fontelo.
2007.
Utilization of thePICO framework to improve searching PubMed forclinical questions.
BMC Medical Informatics and De-cision Making, 7(1).Martin Dawes, Pierre Pluye, Laura Shea, Roland Grad,Arlene Greenberg, and Jian-Yun Nie.
2007.
The iden-tification of clinically important elements within med-ical journal abstracts: PatientPopulationProblem, Ex-posureIntervention, Comparison, Outcome, Durationand Results (PECODR).
Informatics in Primary care,15(1):9?16.D.
Demner-Fushman and J. Lin.
2007.
Answeringclinical questions with knowledge-based and statisticaltechniques.
Computational Linguistics, 33(1):63?103.William R. Hersh, Katherine Crabtree, David H. Hickam,Lynetta Sacherek, Linda Rose, and Charles P. Fried-man.
2000.
Factors associated with successful an-swering of clinical questions using an information re-trieval system.
Bulletin of the Medical Library Asso-ciation, 88(4):323?331.Larry McKnight and Padmini Srinivasan.
2003.
Catego-rization of sentence types in medical abstracts.
Pro-ceedings of the AMIA annual symposium.Donald Metzler and W. Bruce Croft.
2005.
A Markovrandom field model for term dependencies.
In Pro-ceedings of the SIGIR conference, pages 472?479.Yun Niu, Graeme Hirst, Gregory McArthur, and PatriciaRodriguez-Gianolli.
2003.
Answering clinical ques-tions with role identification.
In Proceedings of theACL 2003 Workshop on Natural Language Processingin Biomedicine, pages 73?80.Pierre Pluye, Roland M. Grad, Lynn G. Dunikowski,and Randolph Stephenson.
2005.
Impact of clinicalinformation-retrieval technology on physicians: a lit-erature review of quantitative, qualitative and mixedmethods studies.
International Journal of Medical In-formatics, 74(9):745?768.Jay M. Ponte and W. Bruce Croft.
1998.
A languagemodeling approach to information retrieval.
In Pro-ceedings of the SIGIR conference, pages 275?281.Scott W. Richardson, Mark C. Wilson, Jim Nishikawa,and Robert S. Hayward.
1995.
The well-built clini-cal question: a key to evidence-based decisions.
ACPJournal Club, 123(3):A12?13.David L. Sackett, William Rosenberg, J.
A. Muir Gray,Brian Haynes, and W. Scott Richardson.
1996.
Ev-idence based medicine: what it is and what it isn?t.British medical journal, 312:71?72.115
