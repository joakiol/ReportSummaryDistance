Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 952?961, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExploring Topic Coherence over many models and many topicsKeith Stevens1,2 Philip Kegelmeyer3 David Andrzejewski2 David Buttler21University of California Los Angeles; Los Angeles , California, USA2Lawrence Livermore National Lab; Livermore, California, USA3Sandia National Lab; Livermore, California, USA{stevens35,andrzejewski1,buttler1}@llnl.govwpk@sandia.govAbstractWe apply two new automated semantic eval-uations to three distinct latent topic models.Both metrics have been shown to align withhuman evaluations and provide a balance be-tween internal measures of information gainand comparisons to human ratings of coher-ent topics.
We improve upon the measuresby introducing new aggregate measures thatallows for comparing complete topic models.We further compare the automated measuresto other metrics for topic models, compar-ison to manually crafted semantic tests anddocument classification.
Our experiments re-veal that LDA and LSA each have differentstrengths; LDA best learns descriptive topicswhile LSA is best at creating a compact se-mantic representation of documents and wordsin a corpus.1 IntroductionTopic models learn bags of related words from largecorpora without any supervision.
Based on thewords used within a document, they mine topic levelrelations by assuming that a single document cov-ers a small set of concise topics.
Once learned,these topics often correlate well with human con-cepts.
For example, one model might produce topicsthat cover ideas such as government affairs, sports,and movies.
With these unsupervised methods, wecan extract useful semantic information in a varietyof tasks that depend on identifying unique topics orconcepts, such as distributional semantics (Jurgensand Stevens, 2010), word sense induction (Van deCruys and Apidianaki, 2011; Brody and Lapata,2009), and information retrieval (Andrzejewski andButtler, 2011).When using a topic model, we are primarily con-cerned with the degree to which the learned top-ics match human judgments and help us differen-tiate between ideas.
But until recently, the evalua-tion of these models has been ad hoc and applica-tion specific.
Evaluations have ranged from fullyautomated intrinsic evaluations to manually craftedextrinsic evaluations.
Previous extrinsic evaluationshave used the learned topics to compactly representa small fixed vocabulary and compared this distribu-tional space to human judgments of similarity (Jur-gens and Stevens, 2010).
But these evaluations arehand constructed and often costly to perform fordomain-specific topics.
Conversely, intrinsic mea-sures have evaluated the amount of information en-coded by the topics, where perplexity is one com-mon example(Wallach et al 2009), however, Changet al(2009) found that these intrinsic measures donot always correlate with semantically interpretabletopics.
Furthermore, few evaluations have used thesame metrics to compare distinct approaches suchas Latent Dirichlet Allocation (LDA) (Blei et al2003), Latent Semantic Analysis (LSA) (Landauerand Dutnais, 1997), and Non-negative Matrix Fac-torization (NMF) (Lee and Seung, 2000).
This hasmade it difficult to know which method is most use-ful for a given application, or in terms of extractinguseful topics.We now provide a comprehensive and automatedevaluation of these three distinct models (LDA,LSA, NMF), for automatically learning semantictopics.
While these models have seen significant im-provements, they still represent the core differencesbetween each approach to modeling topics.
For ourevaluation, we use two recent automated coherencemeasures (Mimno et al 2011; Newman et al 2010)952originally designed for LDA that bridge the gap be-tween comparisons to human judgments and intrin-sic measures such as perplexity.
We consider severalkey questions:1.
How many topics should be learned?2.
How many learned topics are useful?3.
How do these topics relate to often used semantic tests?4.
How well do these topics identify similar documents?We begin by summarizing the three topic mod-els and highlighting their key differences.
We thendescribe the two metrics.
Afterwards, we focus ona series of experiments that address our four keyquestions and finally conclude with some overall re-marks.2 Topic ModelsWe evaluate three latent factor models that have seenwidespread usage:1.
Latent Dirichlet Allocation2.
Latent Semantic Analysis with Singular Value De-composition3.
Latent Semantic Analysis with Non-negative Ma-trix FactorizationEach of these models were designed with differ-ent goals and are supported by different statisticaltheories.
We consider both LSA models as topicmodels as they have been used in a variety of sim-ilar contexts such as distributional similarity (Jur-gens and Stevens, 2010) and word sense induction(Van de Cruys and Apidianaki, 2011; Brody andLapata, 2009).
We evaluate these distinct modelson two shared tasks (1) grouping together similarwords while separating unrelated words and (2) dis-tinguishing between documents focusing on differ-ent concepts.We distill the different models into a shared repre-sentation consisting of two sets of learned relations:how words interact with topics and how topics inter-act with documents.
For a corpus withD documentsand V words, we denote these relations in terms ofT topics as(1) a V ?
T matrix, W , that indicates the strengtheach word has in each topic, and(2) a T ?
D matrix, H , that indicates the strengtheach topic has in each document.T serves as a common parameter to each model.2.1 Latent Dirichlet AllocationLatent Dirichlet Allocation (Blei et al 2003) learnsthe relationships between words, topics, and docu-ments by assuming documents are generated by aparticular probabilistic model.
It first assumes thatthere are a fixed set of topics, T used throughout thecorpus, and each topic z is associated with a multi-nomial distribution over the vocabulary?z , which isdrawn from a Dirichlet prior Dir(?).
A given docu-ment Di is then generated by the following process1.
Choose ?i ?
Dir(?
), a topic distribution for Di2.
For each word wj ?
Di:(a) Select a topic zj ?
?i(b) Select the word wj ?
?zjIn this model, the ?
distributions represent theprobability of each topic appearing in each docu-ment and the ?
distributions represent the proba-bility of words being used for each topic.
Thesetwo sets of distributions correspond to our H and Wmatrices, respectively.
The process above defines agenerative model; given the observed corpus, we usecollapsed Gibbs sampling implementation found inMallet1 to infer the values of the latent variables ?and ?
(Griffiths and Steyvers, 2004).
The model re-lies only on two additional hyper parameters, ?
and?, that guide the distributions.2.2 Latent Semantic AnalysisLatent Semantic Analysis (Landauer and Dutnais,1997; Landauer et al 1998) learns topics by firstforming a traditional term by document matrix usedin information retrieval and then smoothing thecounts to enhance the weight of informative words.Based on the original LSA model, we use the Log-Entropy transform.
LSA then decomposes thissmoothed, term by document matrix in order to gen-eralize observed relations between words and docu-ments.
For both LSA models, we used implementa-tions found in the S-Space package.2Traditionally, LSA has used the Singular ValueDecomposition, but we also consider Non-negativeMatrix Factorization as we?ve seen NMF appliedin similar situations (Pauca et al 2004) and others1http://mallet.cs.umass.edu/2https://github.com/fozziethebeat/S-Space953Model Label Top Words UMass UCIHigh Quality TopicsLDA interview told asked wanted interview people made thought time called knew -2.52 1.29wine wine wines bottle grapes made winery cabernet grape pinot red -1.97 1.30NMF grilling grilled sweet spicy fried pork dish shrimp menu dishes sauce -1.01 1.98cloning embryonic cloned embryo human research stem embryos cell cloning cells -1.84 1.46SVD cooking sauce food restaurant water oil salt chicken pepper wine cup -1.87 -1.21stocks fund funds investors weapons stocks mutual stock movie film show -2.30 -1.88Low Quality TopicsLDA rates 10-yr rate 3-month percent 6-month bds bd 30-yr funds robot -1.94 -12.32charity fund contributions .com family apartment charities rent 22d children assistance -2.43 -8.88NMF plants stem fruitful stems trunk fruiting currants branches fence currant espalier -3.12 -12.59farming buzzards groundhog prune hoof pruned pruning vines wheelbarrow tree clematis -1.90 -12.56SVD city building city area buildings p.m. floors house listed eat-in a.m. -2.70 -8.03time p.m. system study a.m. office political found school night yesterday -1.67 -7.02Table 1: Top 10 words from several high and low quality topics when ordered by the UCI CoherenceMeasure.
Topic labels were chosen in an ad hoc manner only to briefly summarize the topic?s focus.have found a connection between NMF and Proba-bilistic Latent Semantic Analysis (Ding et al 2008),an extension to LSA.We later refer to these two LSAmodels simply as SVD and NMF to emphasize thedifference in factorization method.Singular Value Decomposition decomposes Minto three smaller matricesM = U?V Tand minimizes Frobenius norm of M ?s reconstruc-tion error with the constraint that the rows of U andV are orthonormal eigenvectors.
Interestingly, thedecomposition is agnostic to the number of desireddimensions.
Instead, the rows and columns in U andV T are ordered based on their descriptive power, i.e.how well they remove noise, which is encoded bythe diagonal singular value matrix ?.
As such, re-duction is done by retaining the first T rows andcolumns from U and V T .
For our generalization,we use W = U?
and H = ?V T .
We note thatvalues in U and V T can be both negative and pos-itive, preventing a straightforward interpretation asunnormalized probabilitiesNon-negative Matrix Factorization also factor-izes M by minimizing the reconstruction error, butwith only one constraint: the decomposed matricesconsist of only non-negative values.
In this respect,we can consider it to be learning an unnormalizedprobability distributions over topics.
We use theoriginal Euclidean least squares definition of NMF3.Formally, NMF is defined asM = WHwhere H and W map directly onto our generaliza-tion.
As in the original NMF work, we learn theseunnormalized probabilities by initializing each set ofprobabilities at random and update them accordingto the following iterative update rulesW = W MHTWHHT H = HWTMWTWH3 Coherence MeasuresTopic Coherence measures score a single topic bymeasuring the degree of semantic similarity betweenhigh scoring words in the topic.
These measure-ments help distinguish between topics that are se-mantically interpretable topics and topics that are ar-tifacts of statistical inference, see Table 1 for exam-ples ordered by the UCI measure.
For our evalua-tions, we consider two new coherence measures de-signed for LDA, both of which have been shown tomatch well with human judgements of topic quality:(1) The UCI measure (Newman et al 2010) and (2)The UMass measure (Mimno et al 2011).Both measures compute the coherence of a topicas the sum of pairwise distributional similarity3We note that the alternative KL-Divergence form of NMFhas been directly linked to PLSA (Ding et al 2008)954scores over the set of topic words, V .
We generalizethis ascoherence(V ) =?
(vi,vj)?Vscore(vi, vj , )where V is a set of word describing the topic and indicates a smoothing factor which guarantees thatscore returns real numbers.
(We will be exploringthe effect of the choice of ; the original authors used = 1.
)The UCI metric defines a word pair?s score tobe the pointwise mutual information (PMI) betweentwo words, i.e.score(vi, vj , ) = logp(vi, vj) + p(vi)p(vj)The word probabilities are computed by countingword co-occurrence frequencies in a sliding windowover an external corpus, such as Wikipedia.
To somedegree, this metric can be thought of as an externalcomparison to known semantic evaluations.The UMass metric defines the score to be basedon document co-occurrence:score(vi, vj , ) = logD(vi, vj) + D(vj)whereD(x, y) counts the number of documents con-taining words x and y and D(x) counts the num-ber of documents containing x.
Significantly, theUMass metric computes these counts over the orig-inal corpus used to train the topic models, ratherthan an external corpus.
This metric is more intrin-sic in nature.
It attempts to confirm that the modelslearned data known to be in the corpus.4 EvaluationWe evaluate the quality of our three topic models(LDA, SVD, and NMF) with three experiments.
Wefocus first on evaluating aggregate coherence meth-ods for a complete topic model and consider thedifferences between each model as we learn an in-creasing number of topics.
Secondly, we comparecoherence scores to previous semantic evaluations.Lastly, we use the learned topics in a classifica-tion task and evaluate whether or not coherent top-ics are equally informative when discriminating be-tween documents.For all our experiments, we trained our models on92,600 New York Times articles from 2003 (Sand-haus, 2008).
For all articles, we removed stop wordsand any words occurring less than 200 times in thecorpus, which left 35,836 unique tokens.
All doc-uments were tokenized with OpenNLP?s MaxEnt4tokenizer.
For the UCI measure, we compute thePMI between words using a 20 word sliding win-dow passed over the WaCkypedia corpus (Baroni etal., 2009).
In all experiments, we compute the co-herence with the top 10 words from each topic thathad the highest weight, in terms of LDA and NMFthis corresponds with a high probability of the termdescribing the topic but for SVD there is no clearsemantic interpretation.4.1 Aggregate methods for topic coherenceBefore we can compare topic models, we require anaggregate measure that represents the quality of acomplete model, rather than individual topics.
Weconsider two aggregates methods: (1) the averagecoherence of all topics and (2) the entropy of the co-herence for all topics.
The average coherence pro-vides a quick summarization of a model?s qualitywhereas the entropy provides an alternate summa-rization that differentiates between two interestingsituations.
Since entropy measures the complexityof a probability distribution, it can easily differenti-ate between uniform distributions and multimodal,distributions.
This distinction is relevant when usersprefer to have roughly uniform topic quality insteadof a wide gap between high- and low-quality topics,or vice versa.
We compute the entropy by droppingthe log and  factor from each scoring function.Figure 1 shows the average coherence scores foreach model as we vary the number of topics.
Theseaverage scores indicate some simple relationshipsbetween the models: LDA and NMF have approx-imately the same performance and both models areconsistently better than SVD.
All of the modelsquickly reach a stable average score at around 100topics.
This initially suggests that learning more4http://incubator.apache.org/opennlp/955Number of topicsAverage Topic Coherence?5?4?3?2?10100 200 300 400 500(a) UMassNumber of topicsAverage Topic Coherence?10?8?6?4?202100 200 300 400 500MethodLDANMFSVD(b) UCIFigure 1: Average Topic Coherence for each modelNumber of topicsCoherenceEntropy01234567100 200 300 400 500(a) UMassNumber of topicsCoherenceEntropy01234567100 200 300 400 500MethodLDANMFSVD(b) UCIFigure 2: Entropy of the Topic Coherence for each modeltopics neither increases or decreases the quality ofthe model, but Figure 2 indicates otherwise.
Whilethe entropy for the UMass score stays stable for allmodels, NMF produces erratic entropy results underthe UCI score as we learn more topics.
As entropy ishigher for even distributions and lower for all otherdistributions, these results suggest that the NMF islearning topics with drastically different levels ofquality, i.e.
some with high quality and some withvery low quality, but the average coherence over alltopics do not account for this.Low quality topics may be composed of highlyunrelated words that can?t be fit into another topic,and in this case, our smoothing factor, , may be ar-tificially increasing the score for unrelated words.Following the practice of the original use of thesemetrics, in Figures 1 and 2 we set  = 1.
In Fig-ure 3, we consider  = 10?12, which should sig-nificantly reduce the score for completely unrelatedwords.
Here, we see a significant change in the per-formance of NMF, the average coherence decreasesdramatically as we learn more topics.
Similarly, per-formance of SVD drops dramatically and well belowthe other models.
In figure 4 we lastly compute theaverage coherence using only the top 10% most co-herence topics with  = 10?12.
Here, NMF againperforms on par with LDA.
With the top 10% topicsstill having a high average coherence but the full set956Number of topicsAverage Topic Coherence?5?4?3?2?10100 200 300 400 500(a) UMassNumber of topicsAverage Topic Coherence?10?8?6?4?202100 200 300 400 500MethodLDANMFSVD(b) UCIFigure 3: Average Topic Coherence with  = 10?12Number of topicsAverage Coherence oftop 10%?5?4?3?2?10100 200 300 400 500(a) UMassNumber of topicsAverage Coherence oftop 10%?10?8?6?4?202100 200 300 400 500MethodLDANMFSVD(b) UCIFigure 4: Average Topic Coherence of the top 10% topics with  = 10?12of topics having a low coherence, NMF appears tobe learning more low quality topics once it?s learnedthe first 100 topics, whereas LDA learns fewer lowquality topics in general.4.2 Word Similarity TasksThe initial evaluations for each coherence mea-sure asked human judges to directly evaluate top-ics (Newman et al 2010; Mimno et al 2011).
Weexpand upon this comparison to human judgmentsby considering word similarity tasks that have of-ten been used to evaluate distributional semanticspaces (Jurgens and Stevens, 2010).
Here, we usethe learned topics as generalized semantics describ-ing our knowledge about words.
If a model?s topicsgeneralize the knowledge accurately, we would ex-pect similar words, such as ?cat?
and ?dog?, to berepresented with a similar set of topics.
Rather thanevaluating individual topics, this similarity task con-siders the knowledge within the entire set of topics,the topics act as more compact representation for theknown words in a corpus.We use the Rubenstein and Goodenough (1965)and Finkelstein et al(2002) word similarity tasks.In each task, human judges were asked to evaluatethe similarity or relatedness between different sets ofword pairs.
Fifty-One Evaluators for the Rubensteinand Goodenough (1965) dataset were given 65 pairs957Tscore0.00.10.20.30.40.50.6100 200 300 400 500modelLDANMFSVD(a) Rubenstein & GoodenoughTscore0.00.10.20.30.40.5100 200 300 400 500modelLDANMFSVD(b) Wordsim 353/Finklestein et.
al.Figure 5: Word Similarity Evaluations for each modelTopicsCorrelation0.00.20.40.6100 200 300 400 500(a) UMassTopicsCorrelation0.00.20.40.6100 200 300 400 500modelLDANMFSVD(b) UCIFigure 7: Correlation between topic coherence and topic ranking in classificationof words and asked to rate their similarity on a scalefrom 0 to 4, where a higher score indicates a moresimilar word pair.
Finkelstein et al(2002) broadensthe word similarity evaluation and asked 13 to 16different subjects to rate 353 word pairs on a scalefrom 0 to 10 based on their relatedness, where relat-edness includes similarity and other semantic rela-tions.
We can evaluate each topic model by comput-ing the cosine similarity between each pair of wordsin the evaluate set and then compare the model?sratings to the human ratings by ranked correlation.A high correlation signifies that the topics closelymodel human judgments.Figure 5 displays the results.
SVD and LDAboth surpass NMF on the Rubenstein & Goode-nough test while SVD is clearly the best model onthe Finklestein et.
al test.
While our first experi-ment showed that SVDwas the worst model in termsof topic coherence scores, this experiment indicatesthat SVD provides an accurate, stable, and reliableapproximation to human judgements of similarityand relatedness between word pairs in comparisonto other topic models.4.3 Coherence versus ClassificationFor our final experiment, we examine the relation-ship between topic coherence and classification ac-curacy for each topic model.
We suspect that highly958scoreCorrelation0.010.020.030.040.050.060.07?25 ?20 ?15 ?10 ?5(a) UMassscoreCorrelation0.010.020.030.040.050.060.07?30 ?20 ?10 0modelLDANMFSVD(b) UCIFigure 8: Comparison between topic coherence and topic rank with 500 topicsTopicsAccuracy20304050607080100 200 300 400 500ModelLDANMFSVDFigure 6: Classification accuracy for each modelcoherent topics, and coherent topic models, will per-form better for classification.
We address this ques-tion by performing a document classification taskusing the topic representations of documents as in-put features and examine the relationship betweentopic coherence and the usefulness of the corre-sponding feature for classification.We trained each topic model with all 92,600 NewYork Times articles as before.
We use the sec-tion labels provided for each article as class labels,where each label indicates the on-line section(s) un-der which the article was published and should thusbe related to the topics contained in each article.
Toreduce the noise in our data set we narrow down thearticles to those that have only one label and whoselabel is applied to at least 2000 documents.
This re-sults in 57,696 articles with label distributions listedin Table 2.
We then represent each document usingcolumns in the topic by document matrix H learnedfor each topic model.Label Count Label CountNew York and Region 11219 U.S. 3675Paid Death Notices 11152 Arts 3437Opinion 8038 World 3330Business 7494 Style 2137Sports 7214Table 2: Section label counts for New York Timesarticles used for classificationFor each topic model trained on N topics, weperformed stratified 10-fold cross-validation on the57,696 labeled articles.
In each fold, we build anautomatically-sized bagged ensemble of unprunedCART-style decision trees(Banfield et al 2007) on90% of the dataset5, use that ensemble to assign la-bels to the other 10%, and measure the accuracy ofthat assignment.
Figure 6 shows the average classifi-cation accuracy over all ten folds for each model.
In-terestingly, SVD has slightly, but statistically signif-icantly, higher accuracy results than both NMF andLDA.
Furthermore, performance quickly increases5The precise choice of the classifier scheme matters little, aslong as it is accurate, speedy, and robust to label noise; all ofwhich is true of the choice here.959and plateaus with well under 50 topics.Our bagged decision trees can also determine theimportance of each feature during classification.
Weevaluate the strength of each topic during classifi-cation by tracking the number of times each nodein our decision trees observe each topic, please see(Caruana et al 2006) for more details.
Figure 8 plotthe relationship between this feature ranking and thetopic coherence for each topic when training LDA,SVD, and NMF on 500 topics.
Most topics for eachmodel provide little classification information, butSVD shows a much higher rank for several topicswith a relatively higher coherence score.
Interest-ingly, for all models, the most coherent topics are notthe most informative.
Figure 7 plots a more compactview of this same relationship: the Spearman rankcorrelation between classification feature rank andtopic coherence.
NMF shows the highest correlationbetween rank and coherence, but none of the mod-els show a high correlation when using more than100 topics.
SVD has the lowest correlation, whichis probably due to the model?s overall low coherenceyet high classification accuracy.5 Discussion and ConclusionThrough our experiments, we made several excit-ing and interesting discoveries.
First, we discov-ered that the coherence metrics depend heavily onthe smoothing factor .
The original value, 1.0 cre-ated a positive bias towards NMF from both met-rics even when NMF generated incoherent topics.The high smoothing factor also gave a significant in-crease to SVD scores.
We suspect that this was notan issue in previous studies with the coherence mea-sures as LDA prefers to form topics from words thatco-occur frequently, whereas NMF and SVD haveno such preferences and often create low quality top-ics from completely unrelated words.
Therefore, wesuggest a smaller  value in general.We also found that the UCI measure often agreedwith the UMass measure, but the UCI-entropy ag-gregate method induced more separation betweenLSA, SVD, and NMF in terms of topic coherence.This measure also revealed the importance of thesmoothing factor for topic coherence measures.With respects to human judgements, we foundthat coherence scores do not always indicate a bet-ter representation of distributional information.
TheSVD model consistently out performed both LDAand NMF models, which each had higher coherencescores, when attempting to predict human judge-ments of similarity.Lastly, we found all models capable of producingtopics that improved document classification.
At thesame time, SVD provided the most information dur-ing classification and outperformed the other mod-els, which again had more coherent topics.
Our com-parison between topic coherence scores and featureimportance in classification revealed that relativelyhigh quality topics, but not the most coherent topics,drive most of the classification decisions, and mosttopics do not affect the accuracy.Overall, we see that each topic model paradigmhas it?s own strengths and weaknesses.
Latent Se-mantic Analysis with Singular Value Decompositionfails to form individual topics that aggregate similarwords, but it does remarkably well when consider-ing all the learned topics as similar words developa similar topic representation.
These topics simi-larly perform well during classification.
Conversely,both Non Negative Matrix factorization and LatentDirichlet Allocation learn concise and coherent top-ics and achieved similar performance on our evalua-tions.
However, NMF learns more incoherent topicsthan LDA and SVD.
For applications in which a hu-man end-user will interact with learned topics, theflexibility of LDA and the coherence advantages ofLDA warrant strong consideration.
All of code forthis work will be made available through an opensource project.66 AcknowledgmentsThis work was performed under the auspices ofthe U.S. Department of Energy by Lawrence Liv-ermore National Laboratory under Contract DE-AC52-07NA27344 (LLNL-CONF-522871) and bySandia National Laboratory under Contract DE-AC04-94AL85000.ReferencesDavid Andrzejewski and David Buttler.
2011.
Latenttopic feedback for information retrieval.
In Proceed-6https://github.com/fozziethebeat/TopicModelComparison960ings of the 17th ACM SIGKDD international confer-ence on Knowledge discovery and data mining, KDD?11, pages 600?608, New York, NY, USA.
ACM.Robert E. Banfield, Lawrence O.
Hall, Kevin W. Bowyer,andW.
Philip Kegelmeyer.
2007.
A comparison of de-cision tree ensemble creation techniques.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,29(1):173?180, January.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022, March.Samuel Brody and Mirella Lapata.
2009.
Bayesian wordsense induction.
In Proceedings of the 12th Con-ference of the European Chapter of the Associationfor Computational Linguistics, EACL ?09, pages 103?111, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Rich Caruana, Mohamed Elhawary, Art Munson, MirekRiedewald, Daria Sorokina, Daniel Fink, Wesley M.Hochachka, and Steve Kelling.
2006.
Mining cit-izen science data to predict orevalence of wild birdspecies.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discovery anddata mining, KDD ?06, pages 909?915, New York,NY, USA.
ACM.Jonathan Chang, Sean Gerrish, Chong Wang, andDavid M Blei.
2009.
Reading tea leaves : How hu-mans interpret topic models.
New York, 31:1?9.Chris Ding, Tao Li, and Wei Peng.
2008.
On the equiv-alence between non-negative matrix factorization andprobabilistic latent semantic indexing.
Comput.
Stat.Data Anal., 52:3913?3927, April.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: the conceptrevisited.
ACM Trans.
Inf.
Syst., 20:116?131, January.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101(Suppl.
1):5228?5235, April.David Jurgens and Keith Stevens.
2010.
The s-spacepackage: an open source package for word space mod-els.
In Proceedings of the ACL 2010 System Demon-strations, ACLDemos ?10, pages 30?35, Stroudsburg,PA, USA.
Association for Computational Linguistics.Thomas K Landauer and Susan T. Dutnais.
1997.
A so-lution to platos problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological review, pages 211?240.Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.1998.
An Introduction to Latent Semantic Analysis.Discourse Processes, (25):259?284.Daniel D. Lee and H. Sebastian Seung.
2000.
Algo-rithms for non-negative matrix factorization.
In InNIPS, pages 556?562.
MIT Press.David Mimno, Hanna Wallach, Edmund Talley, MiriamLeenders, and Andrew McCallum.
2011.
Optimizingsemantic coherence in topic models.
In Proceedings ofthe 2011 Conference on Emperical Methods in Natu-ral Language Processing, pages 262?272, Edinburgh,Scotland, UK.
Association of Computational Linguis-tics.David Newman, Youn Noh, Edmund Talley, SarvnazKarimi, and Timothy Baldwin.
2010.
Evaluating topicmodels for digital libraries.
In Proceedings of the 10thannual joint conference on Digital libraries, JCDL?10, pages 215?224, New York, NY, USA.
ACM.V Paul Pauca, Farial Shahnaz, Michael W Berry, andRobert J Plemmons, 2004.
Text mining using nonnega-tive matrix factorizations, volume 54, pages 452?456.SIAM.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8:627?633, October.Evan Sandhaus.
2008.
The New York Times AnnotatedCorpus.Tim Van de Cruys and Marianna Apidianaki.
2011.
La-tent semantic word sense induction and disambigua-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, HLT ?11,pages 1476?1485, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, andDavid Mimno.
2009.
Evaluation methods for topicmodels.
In Proceedings of the 26th International Con-ference on Machine Learning (ICML).
Omnipress.961
