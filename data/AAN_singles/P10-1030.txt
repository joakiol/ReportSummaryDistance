Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 286?295,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning 5000 Relational ExtractorsRaphael Hoffmann, Congle Zhang, Daniel S. WeldComputer Science & EngineeringUniversity of WashingtonSeattle, WA-98195, USA{raphaelh,clzhang,weld}@cs.washington.eduAbstractMany researchers are trying to use informationextraction (IE) to create large-scale knowl-edge bases from natural language text on theWeb.
However, the primary approach (su-pervised learning of relation-specific extrac-tors) requires manually-labeled training datafor each relation and doesn?t scale to the thou-sands of relations encoded in Web text.This paper presents LUCHS, a self-supervised,relation-specific IE system which learns 5025relations ?
more than an order of magnitudegreater than any previous approach ?
with anaverage F1 score of 61%.
Crucial to LUCHS?sperformance is an automated system for dy-namic lexicon learning, which allows it tolearn accurately from heuristically-generatedtraining data, which is often noisy and sparse.1 IntroductionInformation extraction (IE), the process of gen-erating relational data from natural-language text,has gained popularity for its potential applicationsin Web search, question answering and other tasks.Two main approaches have been attempted:?
Supervised learning of relation-specific ex-tractors (e.g., (Freitag, 1998)), and?
?Open?
IE ?
self-supervised learning ofunlexicalized, relation-independent extractors(e.g., Textrunner (Banko et al, 2007)).Unfortunately, both methods have problems.Supervised approaches require manually-labeledtraining data for each relation and hence can?tscale to handle the thousands of relations encodedin Web text.
Open extraction is more scalable,but has lower precision and recall.
Furthermore,open extraction doesn?t canonicalize relations, soany application using the output must deal withhomonymy and synonymy.A third approach, sometimes refered to as weaksupervision, is to heuristically match values froma database to text, thus generating a set of train-ing data for self-supervised learning of relation-specific extractors (Craven and Kumlien, 1999).With the Kylin system (Wu and Weld, 2007) ap-plied this idea to Wikipedia by matching valuesof an article?s infobox1 attributes to correspondingsentences in the article, and suggested that theirapproach could extract thousands of relations (Wuet al, 2008).
Unfortunately, however, they nevertested the idea on more than a dozen relations.
In-deed, no one has demonstrated a practical way toextract more than about one hundred relations.We note that Wikipedia?s infobox ?ontology?
isa particularly interesting target for extraction.
As aby-product of thousands of contributors, it is broadin coverage and growing quickly.
Unfortunately,the schemata are surprisingly noisy and most aresparsely populated; challenging conditions for ex-traction.This paper presents LUCHS, an autonomous,self-supervised system, which learns 5025 rela-tional extractors ?
an order of magnitude greaterthan any previous effort.
Like Kylin, LUCHS cre-ates training data by matching Wikipedia attributevalues with corresponding sentences, but by itself,this method was insufficient for accurate extrac-tion of most relations.
Thus, LUCHS introducesa new technique, dynamic lexicon features, whichdramatically improves performance when learningfrom sparse data and that way enables scalability.1.1 Dynamic Lexicon FeaturesFigure 1 summarizes the architecture of LUCHS.At the highest level, LUCHS?s offline training pro-cess resembles that of Kylin.
Wikipedia pages1A sizable fraction of Wikipedia articles have associatedinfoboxes ?
relational summaries of the key aspects of thesubject of the article.
For example, the infobox for Alan Tur-ing?s Wikipedia page lists the values of 10 attributes, includ-ing his birthdate, nationality and doctoral advisor.286Matcher HarvesterCRFLearnerFiltered ListsWWWLexiconLearnerClassifierLearnerTraining DataExtractorTraining DataLexiconsTuplesPagesArticleClassifier ExtractorExtractorClassified PagesExtractionLearningFigure 1: Architecture of LUCHS.
In order tohandle sparsity in its heuristically-generated train-ing data, LUCHS generates custom lexicon featureswhen learning each relational extractor.containing infoboxes are used to train a classi-fier that can predict the appropriate schema forpages missing infoboxes.
Additionally, the val-ues of infobox attributes are compared with articlesentences to heuristically generate training data.LUCHS?s major innovation is a feature-generationprocess, which starts by harvesting HTML listsfrom a 5B document Web crawl, discarding 98%to create a set of 49M semantically-relevant lists.When learning an extractor for relation R, LUCHSextracts seed phrases from R?s training data anduses a semi-supervised learning algorithm to cre-ate several relation-specific lexicons at differentpoints on a precision-recall spectrum.
These lex-icons form Boolean features which, along withlexical and dependency parser-based features, areused to produce a CRF extractor for each relation?
one which performs much better than lexicon-free extraction on sparse training data.At runtime, LUCHS feeds pages to the articleclassfier, which predicts which infobox schemais most appropriate for extraction.
Then a smallset of relation-specific extractors are applied toeach sentence, outputting tuples.
Our experimentsdemonstrate a high F1 score, 61%, across the 5025relational extractors learned.1.2 SummaryThis paper makes several contributions:?
We present LUCHS, a self-supervised IE sys-tem capable of learning more than an orderof magnitude more relation-specific extractorsthan previous systems.?
We describe the construction and use of dy-namic lexicon features, a novel technique, thatenables hyper-lexicalized extractors whichcope effectively with sparse training data.?
We evaluate the overall end-to-end perfor-mance of LUCHS, showing an F1 score of 61%when extracting relations from randomly se-lected Wikipedia pages.?
We present a comprehensive set of additionalexperiments, evaluating LUCHS?s individualcomponents, measuring the effect of dynamiclexicon features, testing sensitivity to varyingamounts of training data, and categorizing thetypes of relations LUCHS can extract.2 Heuristic Generation of Training DataWikipedia is an ideal starting point for our long-term goal of creating a massive knowledge base ofextracted facts for two reasons.
First, it is com-prehensive, containing a diverse body of contentwith significant depth.
Perhaps more importantly,Wikipedia?s structure facilitates self-supervisedextraction.
Infoboxes are short, manually-createdtabular summaries of many articles?
key facts ?effectively defining a relational schema for thatclass of entity.
Since the same facts are often ex-pressed in both article and ontology, matching val-ues of the ontology to the article can deliver valu-able, though noisy, training data.For example, the Wikipedia article on ?Jerry Se-infeld?
contains the sentence ?Seinfeld was bornin Brooklyn, New York.?
and the article?s infoboxcontains the attribute ?birth place = Brooklyn?.By matching the attribute?s value ?Brooklyn?
tothe sentence, we can heuristically generate train-ing data for a birth place extractor.
This data isnoisy; some attributes will not find matches, whileothers will find many co-incidental matches.3 Learning ExtractorsWe first assume that each Wikipedia infobox at-tribute corresponds to a unique relation (but seeSection 5.6) for which we would like to learn aspecific extractor.
A major challenge with suchan approach is scalability.
Running a relation-specific extractor for each of Wikipedia?s 34,000unique infobox attributes on each of Wikipedia?s50 million sentences would require 1.7 trillion ex-tractor executions.We therefore choose a hierarchical approachthat combines both article classifiers and rela-tion extractors.
For each infobox schema, LUCHStrains a classifier that predicts if an article is likelyto contain that schema.
Only when an article287is likely to contain a schema, does LUCHS runthat schema?s relation extractors.
To extract in-fobox attributes from all of Wikipedia, LUCHSnow needs orders of magnitude fewer executions.While this approach does not propagate infor-mation from extractors back to article classifiers,experiments confirm that our article classifiersnonetheless deliver accurate results (Section 5.2),reducing the potential benefit of joint inference.
Inaddition, our approach reduces the need for extrac-tors to keep track of the larger context, thus sim-plifying the extraction problem.We briefly summarize article classification: Weuse a linear, multi-class classifier with six kinds offeatures: words in the article title, words in thefirst sentence, words in the first sentence whichare direct objects to the verb ?to be?, article sec-tion headers, Wikipedia categories, and their an-cestor categories.
We use the voted perceptron al-gorithm (Freund and Schapire, 1999) for training.More challenging are the attribute extractors,which we wish to be simple, fast, and able to wellcapture local dependencies.
We use a linear-chainconditional random field (CRF) ?
an undirectedgraphical model connecting a sequence of inputand output random variables, x = (x0, .
.
.
, xT )and y = (y0, .
.
.
, yT ) (Lafferty et al, 2001).
In-put variables are assigned words w. The statesof output variables represent discrete labels l, e.g.Argi-of-Relj and Other.
In our case, variablesare connected in a chain, following the first-orderMarkov assumption.
We train to maximize condi-tional likelihood of output variables given an inputprobability distribution.
The CRF models p(y|x)are represented with a log-linear distributionp(y|x) =1Z(x)expT?t=1K?k=1?kfk(yt?1, yt, x, t)where feature functions, f , encode sufficientstatistics of (x, y), T is the length of the sequence,K is the number of feature functions, and ?k areparameters representing feature weights, whichwe learn during training.
Z(x) is a partition func-tion used to normalize the probabilities to 1.
Fea-ture functions allow complex, overlapping globalfeatures with lookahead.Common techniques for learning the weights ?kinclude numeric optimization algorithms such asstochastic gradient descent or L-BFGS.
In our ex-periments, we again use the simpler and more effi-cient voted-perceptron algorithm (Collins, 2002).The linear-chain layout enables efficient interenceusing the dynamic programming-based Viterbi al-gorithm (Lafferty et al, 2001).We evaluate nine kinds of Boolean features:Words For each input word w we introduce fea-ture fww (yt?1, yt, x, t) := 1[xt=w].State Transitions For each transition be-tween output labels li, lj we add featuref tranli,lj (yt?1, yt, x, t) := 1[yt?1=li?yt=lj ].Word Contextualization For parameters p ands we add features fprevw (yt?1, yt, x, t) :=1[w?
{xt?p,...,xt?1}] and fsubw (yt?1, yt, x, t) :=1[w?
{xt+1,...,xt+s}] which capture a window ofwords appearing before and after each position t.Capitalization We add featurefcap(yt?1, yt, x, t) := 1[xtis capitalized].Digits We add feature fdig(yt?1, yt, x, t) :=1[xtis digits].Dependencies We set fdep(yt?1, yt, x, t) to thelemmatized sequence of words from xt to the rootof the dependency tree, computed using the Stan-ford parser (Marneffe et al, 2006).First Sentence We set f fs(yt?1, yt, x, t) :=1[xtin first sentence of article].Gaussians For numeric attributes, we fit a Gaus-sian (?, ?)
and add feature fgaui (yt?1, yt, x, t) :=1[|xt??|<i?]
for parameters i.Lexicons For non-numeric attributes, and for alexicon l, i.e.
a set of related words, we add fea-ture f lexl (yt?1, yt, x, t) := 1[xt?l].
Lexicons areexplained in the following section.4 Extraction with LexiconsIt is often possible to group words that are likelyto be assigned similar labels, even if many of thesewords do not appear in our training set.
The ob-tained lexicons then provide an elegant way to im-prove the generalization ability of an extractor, es-pecially when only little training data is available.However, there is a danger of overfitting, whichwe discuss in Section 4.2.4.The next section explains how we mine the Webto obtain a large corpus of quality lists.
Then Sec-tion 4.2 presents our semi-supervised algorithmfor learning semantic lexicons from these lists.2884.1 Harvesting Lists from the WebDomain-independence requires access to an ex-tremely large number of lists, but our tight in-tegration of lexicon acquisition and CRF learn-ing requires that relevant lists be accessed instan-taneously.
Approaches using search engines orwrappers at query time (Etzioni et al, 2004; Wangand Cohen, 2008) are too slow; we must extractand index lists prior to learning.We begin with a 5 billion page Web crawl.LUCHS can be combined with any list harvestingtechnique, but we choose a simple approach, ex-tracting lists defined by HTML <ul> or <ol>tags.
The set of lists obtained in this way is ex-tremely noisy ?
many lists comprise navigationbars, tag sets, spam links, or a series of long textparagraphs.
This is consistent with the observationthat less than 2% of Web tables are relational (Ca-farella et al, 2008).We therefore apply a series of filtering steps.We remove lists of only one or two items, listscontaining long phrases, and duplicate lists fromthe same host.
After filtering we obtain 49 millionlists, containing 56 million unique phrases.4.2 Semi-Supervised Learning of LexiconsWhile training a CRF extractor for a given rela-tion, LUCHS uses its corpus of lists to automati-cally generate a set of semantic lexicons ?
spe-cific to that relation.
The technique proceeds inthree steps, which have been engineered to run ex-tremely quickly:1.
Seed phrases are extracted from the labeledtraining set.2.
A learning algorithm expands the seedphrases into a set of lexicons.3.
The semantic lexicons are added as featuresto the CRF learning algorithm.4.2.1 Extracting Seed PhrasesFor each training sentence LUCHS first identifiessubsequences of labeled words, and for each suchlabeled subsequence, LUCHS creates one or moreseed phrases p. Typically, a set of seeds con-sists precisely of the labeled subsequences.
How-ever, if the labeled subsequences are long and havesubstructure, e.g., ?San Remo, Italy?, our systemsplits at the separator token, and creates additionalseed sets from prefixes and postfixes.4.2.2 From Seeds to LexiconsTo expand a set of seeds into a lexicon, LUCHSmust identify relevant lists in the corpus.
Rele-vancy can be computed by defining a similarity be-tween lists using the vector-space model.
Specifi-cally, let L denote the corpus of lists, and P be theset of unique phrases from L. Each list l0 ?
L canbe represented as a vector of weighted phrases p ?P appearing on the list, l0 = (l0p1 l0p2 .
.
.
l0p|P|).
Fol-lowing the notion of inverse document frequency,a phrase?s weight is inversely proportional to thenumber of lists containing the phrase.
Popularphrases which appear on many lists thus receivea small weight, whereas rare phrases are weightedhigher:l0pi =1|{l ?
L|p ?
l}|Unlike the vector space model for documents, weignore term frequency, since the vast majority oflists in our corpus don?t contain duplicates.
Thisvector representation supports the simple cosinedefinition of list similarity, which for lists l0, l1 ?L is defined assimcos :=l0 ?
l1?l0?
?l1?.Intuitively, two lists are similar if they have manyoverlapping phrases, the phrases are not too com-mon, and the lists don?t contain many otherphrases.
By representing the seed set as anothervector, we can find similar lists, hopefully contain-ing related phrases.
We then create a semantic lex-icon by collecting phrases from a range of relatedlists.For example, one lexicon may be created as theunion of all phrases on lists that have non-zerosimilarity to the seed list.
Unfortunately, due tothe noisy nature of the Web lists such a lexiconmay be very large and may contain many irrele-vant phrases.
We expect that lists with higher sim-ilarity are more likely to contain phrases which arerelated to our seeds; hence, by varying the sim-ilarity threshold one may produce lexicons rep-resenting different compromises between lexiconprecision and recall.
Not knowing which lexiconwill be most useful to the extractors, LUCHS gen-erates several and lets the extractors learn appro-priate weights.However, since list similarities vary dependingon the seeds, fixed thresholds are not an option.
If#similarlists denotes the number of lists that havenon-zero similarity to the seed list and #lexicons289the total number of lexicons we want to generate,LUCHS sets lexicon i ?
{0, .
.
.
,#lexicons ?
1}to be the union of prases on the#similarlistsi/#lexiconsmost similar lists.24.2.3 Efficiently Creating LexiconsWe create lexicons from lists that are similar toour seed vector, so we only consider lists that haveat least one phrase in common.
Importantly, ourindex structures allow LUCHS to select the rele-vant lists efficiently.
For each seed, LUCHS re-trieves the set of containing lists as a sorted se-quence of list identifiers.
These sequences arethen merged yielding a sequence of list identifierswith associated seed-hit counts.
Precomputed listlengths and inverse document frequencies are alsoretrieved from indices, allowing efficient compu-tation of similarity.
The worst case complexity isO(log(S)SK) where S is the number of seeds andK the maximum number of lists to consider perseed.4.2.4 Preventing Lexicon OverfittingFinally, we integrate the acquired semantic lexi-cons as features into the CRF.
Although Section 3discussed how to use lexicons as CRF features,there are some subtleties.
Recall that the lexi-cons were created from seeds extracted from thetraining set.
If we now train the CRF on the sameexamples that generated the lexicon features, thenthe CRF will likely overfit, and weight the lexiconfeatures too highly!Before training, we therefore split the trainingset into k partitions.
For each example in a par-tition we assign features based on lexicons gener-ated from only the k?1 remaining partitions.
Thisavoids overfitting and ensures that we will not per-form much worse than without lexicon features.When we apply the CRF to our test set, we use thelexicons based on all k partitions.
We refer to thistechnique as cross-training.5 ExperimentsWe start by evaluating end-to-end performance ofLUCHS when applied to Wikipedia text, then an-alyze the characteristics of its components.
Ourexperiments use the 10/2008 English Wikipediadump.2For practical reasons, we exclude the case i = #lexiconsin our experiments.0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0recallprecisionFigure 2: Precision / recall curve for end-to-endsystem performance on 100 random articles.5.1 Overall Extraction PerformanceTo evaluate the end-to-end performance ofLUCHS, we test the pipeline which first classifiesincoming pages, activating a small set of extrac-tors on the text.
To ensure adequate training andtest data, we limit ourselves to infobox classeswith at least ten instances; there exist 1,583 suchclasses, together comprising 981,387 articles.
Weonly consider the first ten sentences for each ar-ticle, and we only consider 5025 attributes.3 Wecreate a test set by sampling 100 articles ran-domly; these articles are not used to train articleclassifiers or extractors.
Each test article is thenautomatically classified, and a random attributeof the predicted schema is selected for extraction.Gold labels for the selected attribute and article arecreated manually by a human judge and comparedto the token-level predictions from the extractorswhich are trainined on the remaining articles withheuristic matches.Overall, LUCHS reaches a precision of .55 at arecall of .68, giving an F1-score of .61 (Figure 2).Analyzing the errors in more detail, we find that in11 of 100 cases an article was incorrectly classi-fied.
We note that in at least two of these cases thepredicted class could also be considered correct.For example, instead of Infobox Minor Planet theextractor predicted Infobox Planet.On five of the selected attributes the extrac-tor failed because the attributes could be consid-ered unlearnable: The flexibility of Wikipedia?sinfobox system allows contributors to introduceattributes for formatting, for example defining el-3Attributes were selected to have at least 10 heuristicmatches, to have 10% of values covered by matches, and 10%of articles with attribute in infobox covered by matches.290ement order.
In the future we wish to train LUCHSto ignore this type of attribute.We also compared the heuristic matches con-tained in the selected 100 articles to the gold stan-dard: The matches reach a precision of .90 at arecall of .33, giving an F1-score of .48.
So whilemost heuristic matches hit mentions of attributevalues, many other mentions go unmatched.
Man-ual analysis shows that these values are often miss-ing from an infobox, are formatted differently, orare inconsistent to what is stated in the article.So why did the low recall of the heuristicmatches not adversely affect recall of our extrac-tors?
For most articles, an attribute can be as-signed a single unique value.
When training anattribute extractor, only articles that contained aheuristic match for that attribute were considered,thus avoiding many cases of unmatched mentions.Subsequent experiments evaluate the perfor-mance of LUCHS components in more detail.5.2 Article ClassificationThe first step in LUCHS?s run-time pipeline is de-termining which infobox schemata are most likelyto be found in a given article.
To test this, we ran-domly split our 981,387 articles into 4/5 for train-ing and 1/5 for testing, and train a single multi-class classifier.
For this experiment, we use theoriginal infobox class of an article as its gold la-bel.
We compute the accuracy of the prediction at.92.
Since some classes can be considered inter-changeable, this number represents a lower boundon performance.5.3 Factors Affecting Extraction AccuracyWe now evaluate attribute extraction assumingperfect article classification.
To keep training timemanageable, we sample 100 articles for trainingand 100 articles for testing4 for each of 100 ran-dom attributes.
We again only consider the firstten sentences of each article, and we only con-sider articles that have heuristic matches with theattribute.
We measure F1-score at a token-level,taking the heuristic matches as ground-truth.We first test the performance of extractorstrained using our basic features (Section 3)5, notincluding lexicons and Gaussians.
We begin us-ing word features and obtain a token-level F1-score of .311 for text and .311 for numeric at-tributes.
Adding any of our additional features4These numbers are smaller for attributes with less train-ing data available, but the same split is maintained.5For contextualization features we choose p, s = 5.Features F1-ScoreText attributesBaseline .491Baseline + Lexicons w/o CT .367Baseline + Lexicons .545Numeric attributesBaseline .586Baseline + Gaussians w/o CT .623Baseline + Gaussians .627Table 1: Impact of Lexicon and Gaussian features.Cross-Training (CT) is essential to improve per-formance.improves these scores, but the relative improve-ments vary: For both text and numeric attributes,contextualization and dependency features deliverthe largest improvement.
We then iteratively addthe feature with largest improvement until no fur-ther improvement is observed.
We finally obtainan F1-score of .491 for text and .586 for numericattributes.
For text attributes the extractor usesword, contextualization, first sentence, capitaliza-tion, and digit features; for numeric attributes theextractor uses word, contextualization, digit, firstsentence, and dependency features.
We use theseextractors as a baseline to evaluate our lexicon andGaussian features.Varying the size of the training sets affects re-sults: Taking more articles raises the F1-score, buttaking more sentences per article reduces it.
Thisis because Wikipedia articles often summarize atopic in the first few paragraphs and later discussrelated topics, necessitating reference resolutionwhich we plan to add in future work.5.4 Lexicon and Gaussian FeaturesWe next study how our distribution features6 im-pact the quality of the baseline extractors (Table1).
Without cross-training we observe a reductionin performance, due to overfitting.
Cross-trainingavoids this, and substantially improves results overthe baseline.
While cross-training is particularlycritical for lexicon features, it is less needed forGaussians where only two parameters, mean anddeviation, are fitted to the training set.The relative improvements depend on the num-ber of available training examples (Table 2).
Lex-icon and Gaussian features especially benefit ex-tractors for sparse attributes.
Here we can also seethat the improvements are mainly due to increasesin recall.6We set the number of lexicon and Gaussian features to 4.291# Train F1-B F1-LUCHS ?F1 ?Pr ?ReText attributes10 .379 .439 +16% +10% +20%25 .447 .504 +13% +7% +20%100 .491 .545 +11% +5% +17%Numeric attributes10 .484 .531 +10% +4% +13%25 .552 .596 +8% +4% +10%100 .586 .627 +7% +5% +8%Table 2: Lexicon and Gaussian features greatly ex-pand F1 score (F1-LUCHS) over the baseline (F1-B), in particular for attributes with few training ex-amples.
Gains are mainly due to increased recall.5.5 Scaling to All of WikipediaFinally, we take our best extractors and run themon all 5025 attributes, again assuming perfect ar-ticle classification and using heuristic matches asgold-standard.
Figure 3 shows the distribution ofobtained F1 scores.
810 text attributes and 328 nu-meric attributes reach a score of 0.80 or higher.The performance depends on the number ofavailable training examples, and that number isgoverned by a long-tailed distribution.
For ex-ample, 61% of the attributes in our set have 50or fewer examples, 36% have 20 or fewer.
Inter-estingly, the number of training examples had asmaller effect on performance than expected.
Fig-ure 4 shows the correlation between these vari-ables.
Lexicon and Gaussian features enables ac-ceptable performance even for sparse attributes.Averaging across all attributes we obtain F1scores of 0.56 and 0.60 for textual and numericvalues respectively.
We note that these scoresassume that all attributes are equally important,weighting rare attributes just like common ones.If we weight scores by the number of attribute in-stances, we obtain F1 scores of 0.64 (textual) and0.78 (numeric).
In each case, precision is slightlyhigher than recall.5.6 Towards an Attribute OntologyThe true promise of relation-specific extractorscomes when an ontology ties the system together.By learning a probabilistic model of selectionalpreferences, one can use joint inference to improveextraction accuracy.
One can also answer scien-tific questions, such as ?How many of the learnedWikipedia attributes are distinct??
It is clear thatmany duplicates exist due to collaborative sloppi-ness, but semantic similarity is a matter of opinionand an exact answer is impossible.0% 20% 40% 60% 80% 100%0.00.20.40.60.81.0Text attr.
(3962)Numeric attr.
(1063)# AttributesF1 ScoreFigure 3: F1 scores among attributes, ranked byscore.
810 text attributes (20%) and 328 numericattributes (31%) had an F1-score of .80 or higher.0 20 40 60 80 1000.00.20.40.60.8Text attr.Numeric attr.# Training ExamplesAverage F1 ScoreFigure 4: Average F1 score by number of trainingexamples.
While more training data helps, evensparse attributes reach acceptable performance.Nevertheless, we clustered the textual attributesin several ways.
First, we cleaned the attributenames heuristically and performed spell check.The ?distance?
between two attributes was calcu-lated with a combination of edit distance and IRmetrics with Wordnet synonyms; then hierarchicalagglomerative clustering was performed.
We man-ually assigned names to the clusters and cleanedthem, splitting and joining as needed.
The result istoo crude to be called an ontology, but we continueits elaboration.
There are a total of 3962 attributesgrouped in about 1282 clusters (not yet countingattributes with numerical values); the largest clus-ter, location, has 115 similar attributes.
Figure 5shows the confusion matrix between attributes inthe biggest clusters; the shade of the i, jth pixelindicates the F1 score achieved by training on in-stances of attribute i and testing on attribute j.292locationbirthplaceptitle countryfullnamecity nationalitynationalitybirth namedateofbirthdateofdeathdate statesFigure 5: Confusion matrix for extractor accuracytraining on one attribute then testing on another.Note the extraction similarity between title andfull-name, as well as between dates of birth anddeath.
Space constraints allow us to show only1000 of LUCHS?s 5025 extracted attributes, thosein the largest clusters.6 Related WorkLarge-scale extraction A popular approach to IEis supervised learning of relation-specific extrac-tors (Freitag, 1998).
Open IE, self-supervisedlearning of unlexicalized, relation-independent ex-tractors (Banko et al, 2007), is a more scalableapproach, but suffers from lower precision andrecall, and doesn?t canonicalize the relations.
Athird approach, weak supervision, performs self-supervised learning of relation-specific extractorsfrom noisy training data, heuristically generatedby matching database values to text.
(Craven andKumlien, 1999; Hirschman et al, 2002) apply thistechnique to the biological domain, and (Mintzet al, 2009) apply it to 102 relations from Free-base.
LUCHS differs from these approaches in thatits ?database?
?
the set of infobox values ?
itselfis noisy, contains many more relations, and hasfew instances per relation.
Whereas the existingapproaches focus on syntactic extraction patterns,LUCHS focuses on lexical information enhancedby dynamic lexicon learning.Extraction from Wikipedia Wikipedia hasbecome an interesting target for extraction.
(Suchanek et al, 2008) build a knowledgebasefrom Wikipedia?s semi-structured data.
(Wang etal., 2007) propose a semisupervised positive-onlylearning technique.
Although that extracts fromtext, its reliance on hyperlinks and other semi-structured data limits extraction.
(Wu and Weld,2007; Wu et al, 2008)?s systems generate train-ing data similar to LUCHS, but were only on a fewinfobox classes.
In contrast, LUCHS shows thatthe idea scales to more than 5000 relations, butthat additional techniques, such as dynamic lexi-con learning, are necessary to deal with sparsity.Extraction with lexicons While lexicons havebeen commonly used for IE (Cohen and Sarawagi,2004; Agichtein and Ganti, 2004; Bellare and Mc-Callum, 2007), many approaches assume that lex-icons are clean and are supplied by a user beforetraining.
Other approaches (Talukdar et al, 2006;Miller et al, 2004; Riloff, 1993) learn lexiconsautomatically from distributional patterns in text.
(Wang et al, 2009) learns lexicons from Web listsfor query tagging.
LUCHS differs from these ap-proaches in that it is not limited to a small set ofwell-defined relations.
Rather than creating largelexicons of common entities, LUCHS attempts toefficiently instantiate a series of lexicons from asmall set of seeds to bias extractors of sparse at-tributes.
Crucual to LUCHS?s different setting isalso the need to avoid overfitting.Set expansion A large amount of work haslooked at automatically generating sets of relateditems.
Starting with a set of seed terms, (Etzioniet al, 2004) extract lists by learning wrappers forWeb pages containing those terms.
(Wang and Co-hen, 2007; Wang and Cohen, 2008) extend theidea, computing term relatedness through a ran-dom walk algorithm that takes into account seeds,documents, wrappers and mentions.
Other ap-proaches include Bayesian methods (Ghahramaniand Heller, 2005) and graph label propagation al-gorithms (Talukdar et al, 2008; Bengio et al,2006).
The goal of set expansion techniques isto generate high precision sets of related items;hence, these techniques are evaluated based onlexicon precision and recall.
For LUCHS, which isevaluated based on the quality of an extractor us-ing the lexicons, lexicon precision is not important?
as long as it does not confuse the extractor.7 Future WorkWe envision a Web-scale machine reading systemwhich simultaneously learns ontologies and ex-tractors, and we believe that LUCHS?s approachof leveraging noisy semi-structured information(such as lists or formatting templates) is a key to-wards this goal.
For future work, we plan to en-hance LUCHS in two major ways.First, we note that a big weakness is that thesystem currently only works for Wikipedia pages.293For example, LUCHS assumes that each page cor-responds to exactly one schema and that the sub-ject of relations on a page are the same.
Also,LUCHS makes predictions on a token basis, thussometimes failing to recognize larger segments.To remove these limitations we plan to add adeeper linguistic analysis, making better use ofparse and dependency information and includingcoreference resolution.
We also plan to employrelation-independent Open extraction techniques,e.g.
as suggested in (Wu and Weld, 2008) (retrain-ing).Second, we note that LUCHS?s performancemay benefit substantially from an attribute ontol-ogy.
As we showed in Section 5.6, LUCHS?s cur-rent extractors can also greatly facilitate learninga full attribute ontology.
We therefore plan to in-terleave extractor learning and ontology inference,hence jointly learning ontology and extractors.8 ConclusionMany researchers are trying to use IE to cre-ate large-scale knowledge bases from natural lan-guage text on the Web, but existing relation-specific techniques do not scale to the thousandsof relations encoded in Web text ?
while relation-independent techniques suffer from lower preci-sion and recall, and do not canonicalize the rela-tions.
This paper shows that ?
with new techniques?
self-supervised learning of relation-specific ex-tractors from Wikipedia infoboxes does scale.In particular, we present LUCHS, a self-supervised IE system capable of learning morethan an order of magnitude more relation-specificextractors than previous systems.
LUCHS usesdynamic lexicon features that enable hyper-lexicalized extractors which cope effectively withsparse training data.
We show an overall perfor-mance of 61% F1 score, and present experimentsevaluating LUCHS?s individual components.Datasets generated in this work are available tothe community7.AcknowledgmentsWe thank Jesse Davis, Oren Etzioni, Andrey Kolobov,Mausam, Fei Wu, and the anonymous reviewers for helpfulcomments and suggestions.This material is based upon work supported by a WRF /TJ Cable Professorship, a gift from Google and by the AirForce Research Laboratory (AFRL) under prime contract no.FA8750-09-C-0181.
Any opinions, findings, and conclusionor recommendations expressed in this material are those of7http://www.cs.washington.edu/ai/iwpthe author(s) and do not necessarily reflect the view of theAir Force Research Laboratory (AFRL).ReferencesEugene Agichtein and Venkatesh Ganti.
2004.
Mining refer-ence tables for automatic text segmentation.
In Proceed-ings of the Tenth ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining (KDD-2004),pages 20?29.So?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary G. Ives.
2007.Dbpedia: A nucleus for a web of open data.
In Proceed-ings of the 6th International Semantic Web Conferenceand 2nd Asian Semantic Web Conference (ISWC/ASWC-2007), pages 722?735.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proceedings of the20th International Joint Conference on Artificial Intelli-gence (IJCAI-2007), pages 2670?2676.Kedar Bellare and Andrew McCallum.
2007.
Learning ex-tractors from unlabeled text using relevant databases.
InSixth International Workshop on Information Integrationon the Web.Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.2006.
Label propagation and quadratic criterion.
InOlivier Chapelle, Bernhard Scho?lkopf, and AlexanderZien, editors, Semi-Supervised Learning, pages 193?216.MIT Press.Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eu-gene Wu, and Yang Zhang.
2008.
Webtables: exploringthe power of tables on the web.
Proceedings of the In-ternational Conference on Very Large Databases (VLDB-2008), 1(1):538?549.Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr.,and Tom M. Mitchell.
2009a.
Coupling semi-supervisedlearning of categories and relations.
In NAACL HLT 2009Workskop on Semi-supervised Learning for Natural Lan-guage Processing.Andrew Carlson, Scott Gaffney, and Flavian Vasile.
2009b.Learning a named entity tagger from gazetteers with thepartial perceptron.
In AAAI Spring Symposium on Learn-ing by Reading and Learning to Read.William W. Cohen and Sunita Sarawagi.
2004.
Exploitingdictionaries in named entity extraction: combining semi-markov extraction processes and data integration methods.In Proceedings of the Tenth ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining(KDD-2004), pages 89?98.Michael Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments with per-ceptron algorithms.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural Language Process-ing (EMNLP-2002).Mark Craven and Johan Kumlien.
1999.
Constructing bi-ological knowledge bases by extracting information fromtext sources.
In Proceedings of the Seventh InternationalConference on Intelligent Systems for Molecular Biology(ISMB-1999), pages 77?86.294Benjamin Van Durme and Marius Pasca.
2008.
Finding cars,goddesses and enzymes: Parametrizable acquisition of la-beled instances for open-domain information extraction.In Proceedings of the Twenty-Third AAAI Conference onArtificial Intelligence (AAAI-2008), pages 1243?1248.Oren Etzioni, Michael J. Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S.Weld, and Alexander Yates.
2004.
Methods for domain-independent information extraction from the web: An ex-perimental comparison.
In Proceedings of the NineteenthNational Conference on Artificial Intelligence (AAAI-2004), pages 391?398.Dayne Freitag.
1998.
Toward general-purpose learning forinformation extraction.
In Proceedings of the 17th inter-national conference on Computational linguistics, pages404?408.
Association for Computational Linguistics.Yoav Freund and Robert E. Schapire.
1999.
Large marginclassification using the perceptron algorithm.
MachineLearning, 37(3):277?296.Zoubin Ghahramani and Katherine A. Heller.
2005.Bayesian sets.
In Neural Information Processing Systems(NIPS-2005).Lynette Hirschman, Alexander A. Morgan, and Alexander S.Yeh.
2002.
Rutabaga by any other name: extractingbiological names.
Journal of Biomedical Informatics,35(4):247?259.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
InProceedings of the Eighteenth International Conferenceon Machine Learning (ICML-2001), pages 282?289.Marie-Catherine De Marneffe, Bill Maccartney, and Christo-pher D. Manning.
2006.
Generating typed dependencyparses from phrase structure parses.
In Proceedings of thefifth international conference on Language Resources andEvaluation (LREC-2006).Scott Miller, Jethran Guinness, and Alex Zamanian.
2004.Name tagging with word clusters and discriminative train-ing.
In Proceedings of the Human Language TechnologyConference of the North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL-2004).Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky.2009.
Distant supervision for relation extraction withoutlabeled data.
In The Annual Meeting of the Associationfor Computational Linguistics (ACL-2009).Marius Pasca.
2009.
Outclassing wikipedia in open-domaininformation extraction: Weakly-supervised acquisition ofattributes over conceptual hierarchies.
In Proceedingsof the 12th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL-2009),pages 639?647.Ellen Riloff.
1993.
Automatically constructing a dictionaryfor information extraction tasks.
In Proceedings of the11th National Conference on Artificial Intelligence (AAAI-1993), pages 811?816.Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.2008.
Yago: A large ontology from wikipedia and word-net.
Elsevier Journal of Web Semantics, 6(3):203?217.Fabian M. Suchanek, Mauro Sozio, and Gerhard Weikum.2009.
Sofie: A self-organizing framework for informa-tion extraction.
In Proceedings of the 18th InternationalConference on World Wide Web (WWW-2009).Partha Pratim Talukdar, Thorsten Brants, Mark Liberman,and Fernando Pereira.
2006.
A context pattern inductionmethod for named entity extraction.
In The Tenth Confer-ence on Natural Language Learning (CoNLL-X-2006).Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,Deepak Ravichandran, Rahul Bhagat, and FernandoPereira.
2008.
Weakly-supervised acquisition of labeledclass instances using graph random walks.
In EMNLP,pages 582?590.Richard C. Wang and William W. Cohen.
2007.
Language-independent set expansion of named entities using theweb.
In Proceedings of the 7th IEEE International Con-ference on Data Mining (ICDM-2007), pages 342?350.Richard C. Wang and William W. Cohen.
2008.
Iterative setexpansion of named entities using the web.
In Proceed-ings of the 8th IEEE International Conference on DataMining (ICDM-2008).Gang Wang, Yong Yu, and Haiping Zhu.
2007.
Pore:Positive-only relation extraction from wikipedia text.In Proceedings of the 6th International Semantic WebConference and 2nd Asian Semantic Web Conference(ISWC/ASWC-2007), pages 580?594.Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Alex Acero.2009.
Semi-supervised acquisition of semantic classes ?from the web and for the web.
In International Confer-ence on Information and Knowledge Management (CIKM-2009), pages 37?46.Fei Wu and Daniel S. Weld.
2007.
Autonomously seman-tifying wikipedia.
In Proceedings of the InternationalConference on Information and Knowledge Management(CIKM-2007), pages 41?50.Fei Wu and Daniel S. Weld.
2008.
Automatically refin-ing the wikipedia infobox ontology.
In Proceedings ofthe 17th International Conference on World Wide Web(WWW-2008), pages 635?644.Fei Wu and Daniel S. Weld.
2010.
Open information ex-traction using wikipedia.
In The Annual Meeting of theAssociation for Computational Linguistics (ACL-2010).Fei Wu, Raphael Hoffmann, and Daniel S. Weld.
2008.
In-formation extraction from wikipedia: moving down thelong tail.
In Proceedings of the 14th ACM SIGKDD Inter-national Conference on Knowledge Discovery and DataMining (KDD-2008), pages 731?739.295
