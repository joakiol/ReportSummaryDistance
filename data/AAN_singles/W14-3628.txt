Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 207?216,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsUnsupervised Word Segmentation ImprovesDialectal Arabic to English Machine TranslationKamla Al-Mannai1, Hassan Sajjad1, Alaa Khader2, Fahad Al Obaidli1,Preslav Nakov1, Stephan Vogel1Qatar Computing Research Institute1, Carnegie Mellon University in Qatar2{kamlmannai,hsajjad,faalobaidli,pnakov,svogel}@qf.org.qa1, akhader@cmu.edu2AbstractWe demonstrate the feasibility of usingunsupervised morphological segmentationfor dialects of Arabic, which are poor inlinguistics resources.
Our experiments us-ing a Qatari Arabic to English machinetranslation system show that unsupervisedsegmentation helps to improve the transla-tion quality as compared to using no seg-mentation or to using ATB segmentation,which was especially designed for Mod-ern Standard Arabic (MSA).
We use MSAand other dialects to improve Qatari Ara-bic to English machine translation, and weshow that a uniform segmentation schemeacross them yields an improvement of 1.5BLEU points over using no segmentation.1 IntroductionThe Arabic language has many varieties, wherethe Modern Standard Arabic (MSA) coexists withvarious dialects.
Dialects differ from MSA andfrom each other lexically, phonologically, mor-phologically and syntactically.
MSA has stan-dard orthography and is used in formal contexts(e.g., publications, newspaper articles, etc.
), whilethe dialects are usually limited to daily verbal in-teractions.
However, with the recent rise of socialmedia, it has become increasingly common to usedialects in written communication as well, whichhas constituted the research in dialectal Arabic(DA) as a separate field within the broader fieldof natural language processing (NLP).As DA NLP is still in its infancy, there is lackof basic computational resources and tools, whichare needed in order to apply standard NLP ap-proaches to the dialects of Arabic.
For instance,statistical approaches need a lot of training data,which makes it very hard, if not impossible, toapply them to resource-poor languages; this isespecially true for statistical machine translation(SMT) of Arabic dialects.The Arabic language and its dialects are highlyinflectional, and a word can appear in many moreinflected forms compared to English.
Consider theArabic wordsIJ.??
,I.??K,I.?
?K, and??J.?
?K: theyall belong to one root word I.??
?playing?
/lEb/.Each morphological variation is derived from aroot word with different affixes addressing differ-ent functions.
This causes data sparseness, andcovering all possible word forms of a root wordmay not be always possible.
Considering the dif-ferent variants of Arabic, the problem is exacer-abated as dialects could use different choices of af-fixes for the same function.
For example, the MSAword??J.?
?K/yalEabuwn/, meaning ?they are play-ing?, could be found as??J.?
?K/ylEbuwn/ in Gulf,as @?J.??K??
/Eam yilEabuA/ in Levantine, and as@?J.?
?JK./biylEabwA/ in Egyptian Arabic.One possible solution is to use a morphologicalsegmenter that segments words into simpler unitssuch as stems and affixes, which might be coveredin the training set (Zollmann et al., 2006; Tsai etal., 2010).
When applied to dialects, this may re-duce the lexical gap between dialects and MSA bymatching the common stems.
Unfortunately, thereare no standard morphological segmentation toolsfor dialects.
Due to the difference in morphology,tools designed for MSA do not work well for di-alects.
Developing rule-based segmenters for eachdialect might appear to be the ideal solution, but,as the orthography of dialects is not standardized,crafting linguistic rules for them is very hard.In this paper, we focus on training an unsuper-vised model for word segmentation, which we ap-ply to SMT for a given Arabic dialect.
We train apre-existing unsupervised segmentation model onthe Arabic side of the training bi-text (and on someother monolingual data), and then we optimize itsparameters based on the resulting SMT quality.Similarly, a multi-dialectal word segmenter couldbe developed by training on multi-dialectal data.207In particular, we develop a Qatari Arabic to En-glish (QA-EN) SMT system, which we train on asmall pre-existing bi-text.
As part of the devel-opment of the unsupervised segmentation model,we also collected some additional monolingualdata for Qatari Arabic.
Qatari Arabic is a subdi-alect of the more general Gulf dialect, among withSaudi, Kuwaiti, Emirati, Bahraini, and Omani; wecollected additional monologual data for each ofthese subdialects, and we release this data to theresearch community.We train an unsupervised segmentation tool,Morphessor, and its MAP model (Creutz and La-gus, 2007), using different variations of the col-lected Qatari data.
We optimize the single hy-perparameter of the MAP model by maximizingthe translation quality of the QA-EN SMT sys-tem in terms of BLEU.
Our experimental resultsdemonstrate that the resulting unsupervised seg-menter yields improvements in translation qualitywhen compared to (i) using no segmentation and(ii) using an MSA-based ATB segmenter.We further develop a multi-dialectal word seg-mentation model, which we train on the Arabicside of the multi-dialectal training data, whichconsists of Qatari Arabic, Egyptian Arabic (EGY),Levantine Arabic (LEV) and MSA to English,i.e., a scaled combination of all the available par-allel data.
We train a QA-EN SMT system usingthe segmented multi-dialectal data, and we showan absolute gain of 1.5 BLEU points compared toa baseline that uses no segmentation.The rest of the paper is organized as follows:First, we provide an overview of related work onDialectal Arabic NLP (Section 2).
Next, we dis-cuss and we illustrate the linguistic differences be-tween different Arabic dialects in comparison withand with a focus on Qatari Arabic (Section 3).Then, we provide statistics about the corpora wecollected and used in our experiments, followed byan illustration of the orthographic normalizationschemes we applied (Section 4).
We next providea high-level description of our approach, whichuses morphological segmentation to combine re-sources for other Arabic dialects in a QA-EN SMTsystem effectively (Section 4.3).
We also explainour experimental setup and we present the results(Section 5).
We then discuss translating in thereverse direction, i.e., into Qatari Arabic (Section6).
Finally, we point to possible directions for fu-ture work and we conclude the paper (Section 7).2 Related WorkNLP for DA is still in its early stages of develop-ment and many challenges need to be overcomedsuch as the lack of suitable tools and resources.Collecting resources for dialectal Arabic:Several researchers have directed efforts to de-velop DA computational resources (Maamouri etal., 2006; Al-Sabbagh and Girju, 2010; Zaidan andCallison-Burch, 2011; Salama et al., 2014).
Zbibet al.
(2012) built two dialectal Arabic-Englishparallel corpora for Egyptian and Levantine Ara-bic using crowdsourcing.
Bouamor et al.
(2014)presented a multi-dialectal Arabic parallel corpus,which covers five Arabic dialects besides MSAand English.
Mubarak and Darwish (2014) col-lected a multi-dialectal corpus using Twitter.
Un-like previous work, we focus on Gulf subdialects,particularly Qatari Arabic.
The monolingual datathat we collected is a high-quality dialectal re-source and originates from dialect-specific sourcessuch as novels and forums.Adapting SMT resources for other Arabic di-alects: Many researchers have explored the po-tential of using MSA as a pivot language for im-proving SMT of Arabic dialects (Bakr et al., 2008;Sawaf, 2010; Salloum and Habash, 2011; Sajjad etal., 2013a; Jeblee et al., 2014).
This often involvesDA-MSA conversion schemes as an alternative inthe absence of DA-MSA parallel resources.
Incontrast, limited work has been done on lever-aging available resources for other dialects.
Re-cently, Zbib et al.
(2012) have shown that usinga small amount of dialectal data could yield greatimprovements for SMT.
Here, we investigate thepotential of improving the resource adaptability ofArabic dialects.
Our work is different as we usean unsupervised segmenter that helps in improv-ing the lexical overlap between dialects and MSA.Building morphological segmenters for theArabic dialects: Researchers have already fo-cused efforts on crafting and extending existingMSA tools to DA by mainly using a set of rules(Habash et al., 2012).
Habash and Rambow(2006) presented MAGEAD, a knowledge-basedmorphological analyzer and generator for Egyp-tian and Levantine Arabic.
Chiang et al.
(2006)developed a Levantine morphological analyzer ontop of an existing MSA analyzer using an explicitknowledge base.208Riesa and Yarowsky (2006) trained a supervisedtrie-based model using a small lexicon of dialec-tal affixes.
In our work, we eliminate the needfor linguistic knowledge by training an unsuper-vised model using available resources.
The unsu-pervised mode of learning allowed us to develop amulti-dialectal morphological segmenter.3 Arabic DialectsIn this section, we highlight some of the linguis-tic differences between Arabic dialects and MSA,with a focus on the Qatari dialect.3.1 Phonological VariationsThe Gulf dialect often preserves the phonologicalrepresentation of MSA, which is not the case withmany other Arabic dialects.
For example, in Egyp-tian (EGY) and in some Levantine (LEV) dialects,the MSA consonantsH /v/,?
/q/, andX /*/ arerealized asH /t/, glottal stop /?/, and?
/Z/, re-spectively.
While, their MSA pronunciations arepreserved in Gulf Arabic.In Gulf Arabic, there are some phonological dif-ferences between countries such as Kuwait (KW),Saudi Arabia (SA), Bahrain (BH), Qatar (QA),United Arab Emirates (AE), and Oman (OM).Here, we focus our discussion on Qatari Arabic,and we compare it to MSA and other dialects.The QA dialect borrows two Persian charactersnamely h/J/ and?
/V/.
For instance, the MSAletter h./j/ is converted to /J/ in QA, e.g., ?A?Jk.@?meeting?
is pronounced as /<jtimAE/ in MSAand /<JtimAE/ in QA.
The Persian character h/J/is also used in place of ?
/k/ in some MSA wordswhen they are used in QA.
For example, ????
?fish?/samak/ is pronounced i??
?/smaJ/ in QA, whilethe EGY and the LEV dialects maintain the MSApronunciation.
The Persian?
/V/ is used to mapthe sound of the English letter ?v?
in borrowed for-eign words, e.g., ?KYJ?
?video?
is pronounced as?KYJ?
/Viydyw/ as opposed to /fiydywu/; the formin which it is written in MSA.The MSA consonant?
/D/ is not used in theQA dialect.
It is substituted by?
/Z/ in Qatari.
Forexample, the MSA pronunciation /HaD/ of?k?to encourage?
is transformed to?k /HaZ/ in QA,but it is maintained in EGY.Meanwhile, the MSA consonant?
/Z/ is re-alized as /D/ in EGY.
For example, the MSApronunciation /HaZ/ of?k ?luck?
is maintainedin QA and transformed to /HaD/ in EGY.
Thischange is consistent in all words within each di-alect.
However, such phonological variations be-tween dialects have the potential to add ambiguityto dialectal Arabic.The MSA consonant h./j/ can be used to distin-guish between different dialects, particularly Gulfsubdialects.
h./j/ is pronounced as ?/y/ in KW,BH, QA, AE,?
/q/ in OM, much like in EGY,and h./j/ in SA, much like in LEV.
For exam-ple, the MSA word Yj.??
?mosque?
/masjid/ ispronounced as /masjid/ in MSA, SA, LEV, Y??
?/masqid/ in OM, EGY, YJ??
/masyid/ in KW,BH, QA, AE, while the MSA pronunciation ispreserved in SA.
This change does not apply tonames.
However, we should note that it is not con-sistent in QA, e.g., the MSA pronunciation of h./j/in ?J.k.?mountain?
/jabal/ and h.QK.?tower?
/burj/ ispreserved in QA.3.2 Morphological VariationsIn Arabic, a root can produce surface wordformsby means of inflectional and derivational morpho-logical processes (Habash, 2010).An inflectional word form is a variant of a rootword with the same meaning but expressing a dif-ferent function, e.g., gender, number, case.
It isusually formed by adding a prefix, a suffix, or acircumfix to a stem word.
Note that Arabic di-alects can make different lexical choices for affix-ations compared to MSA.
For example, the MSAfuture prefix ?
/s/ is replaced by H./b/ in QAand by ?
/h/ in EGY and LEV.
Thus, the MSAword ??AJ?
?he will eat?
/say>kul/ becomes ??
AJK./biyAkil/ in QA and ??
AJ?
/hayAkul/ in EGY andLEV.A derivational word form is formed by applyinga pattern to a root word, e.g., ?player?
is derivedfrom ?play?
using the pattern noun + ?er?.
Anexample of an Arabic derivational form is ???K?do?
/tafaE?al/.
The root is ???
/faEal/ and it usesthe imperative patternH+???.
In EGY, @ /A/ isadded as a prefix; so, it becomes ??
?K@ /AitfaE?il/.209Meanwhile, the original form is preserved in QA.Changing the structure of a pattern in a dialectwill result in producing a new dialect-specific or-thography for every word that is represented by thestructure.
For example, the MSA word ??
?K?learn?/taEal?am/ becomes ??
?K@ /AitEalim/ in EGY, whilethe MSA form is preserved in QA.3.3 Lexical VariationsLexical variations are among the most obviousdifferences between Arabic dialects.
For exam-ple, the MSA word @XA?
?what?
/mA*A/ would befound as ??
/$uw/ in LEV, ?K@/<yh/ in EGY, and?J?
/$nuw/ in GLF.
We can find lexical variationsin subdialects as well.
For example, the MSAnegation word??
/lan/, ?not?, is expressed as I.?/mab/ in QA, as ??
/muw/ in KW, and as I.??
/ma-hab/ in SA.3.4 Orthographic VariationsDue to the lack of orthographic standardization ofdialectal Arabic, some MSA words can be foundin dialectal text with both MSA and phonologi-cal spellings.
For example, the MSA word???g.?gathering?
/jamEap/ can be also spelled as ???
?/yamEah/, which is a phonetic variation in QA.Some dialectal words also vary in spelling dueto variation in their pronunciation, e.g.,???
@/A$uwf/, a QA word meaning ?I see?, can be alsospelled as??k.
@ /Ajuwf/.In dialectal Arabic, different orthographicforms are also possible for entire phrases.
Forinstance, words followed or preceded by pro-nouns are commonly reduced to a single word,e.g., A??I??
/glt lahA/ ?I told her?
is written asA??J??.
Also, commonly used religious phrases canbe found written as a single unit, e.g., ?<?
@ Z A?
A?/mA $A?
A?lah/ ?God has willed it?
as ?<?A?
?.4 MethodologyIn the section, we present some statistics about theArabic dialectal data that we have collected.
Weprocessed it to remove orthographic inconsisten-cies.
Then, we used a pre-existing unsupervisedmorphological segmenter, Morfessor, in order tosegment the text.Corpus QCA AVIAQAAVIAOSents 14.7 0.9 2Tokens 115 6.7 15Table 1: Statistics about the collected parallel cor-pora (in thousands).
AVIAOshows the statisticsabout the AVIA corpus excluding Qatari data.4.1 Data CollectionWe did an extensive search for available monolin-gual and bilingual resources for the Gulf dialect,with a focus on Qatari Arabic.
Tables 1 and 2present some statistics about the corpora we col-lected.
More detailed description follows below.Bilingual corpora:?
The QCA speech corpus, comprises 14.7ksentences that are phonetically transcribed fromTV broadcasts in Qatari Arabic and translated toEnglish; see (Elmahdy et al., 2014) for more de-tail.
The corpus was designed for speech recog-nition and we faced several normalization-relatedissues that we had to resolve before it could beused for machine translation and language mod-eling.
One example is the usage of five Per-sian characters to represent some sounds in Ara-bic words.
Moreover, the English side had somegrammatical and spelling errors.
We normalizedthe Arabic side and corrected the English side ofthe corpus as described in Section 4.2.
The cor-pus can be found at http://sprosig.isle.illinois.edu/corpora/1.?
The AVIA corpus1is designed as a refer-ence source of dialectal Arabic.
It consists of 3ksentences in four Gulf subdialects: Emirati (AE),Kuwaiti (KW), Qatari (QA), and Hejazi (SA).2The data consists of dialectal sentences that con-tain words commonly used in daily conversation.Monolingual corpora: We further collectedmonolingual corpora consisting of a total of 2.7Mtokens for various Gulf subdialects.
The Qataripart of the data consists of 470K tokens.
Most ofthe corpus is a collection of novels, belonging tothe romance genre.3For the Qatari dialect, we alsocollected Qatari forum data.41http://terpconnect.umd.edu/?nlynn/AVIA/Level3/2The website also contains small parallel corpora forMSA, EGY and LEV to English, but here we focus on Gulfsubdialects only.3http://forum.te3p.com/264311-52.html4www.qatarshares.com/vb/index.php210Corpus Novel ForumAE BH KW OM QA SA QATokens 573 244 178 372 412 614 69Types 43 22 27 27 43 71 15Table 2: Statistics about the collected monolingualcorpora (in thousands of words).To the best of our knowledge, this is the firstcollection of monolingual corpora for Gulf Ara-bic subdialects.
It can be helpful for, e.g., lan-guage modeling when translating into Arabic, forlearning the similarities and differences betweenGulf subdialects, etc.
Table 2 shows some statis-tics about the data after punctuation tokenization.4.2 Orthographic NormalizationThe inconsistency in the orthographic spelling ofthe same word can increase data sparseness.
Thus,we normalize the Arabic text in the collected re-sources by applying the reduced orthographic nor-malization scheme, e.g., Tah Marbota is reduced toHah.
We also normalize extended lines betweenletters, e.g., Q??
?sugar?
/sukar/ is changedto Q?
?, and we reduce character elongations tobe just two characters long.
In order to main-tain consistency among different resources, we re-move supplementary diacritics, e.g.,Y??
?knots?/Euqad/ is normalized to Y?
?, and we map Per-sian letters to their phonological correspondencesin Qatari Arabic5, i.e., ?
/G/ to?
/g/,?
/V/ to?/f/, H/P/ to H./b/, andP and h/J/ to h./j/.For the English texts, the orthographic varia-tions were already normalized.
However, the En-glish side of the QCA corpus had some spellingand grammatical errors, which we corrected man-ually.
On the grammatical side, we only correcteda subset of the data, which we used for tuning andtesting our SMT system (see Section 5).4.3 Morphological DecompositionThere is no general Arabic morphological seg-menter that works for all variations of Arabic.
Themost commonly used segmenters for Arabic weredesigned for MSA (Habash et al., 2009; Green andDeNero, 2012).
Due to the lexical and morpholog-ical differences between dialects and MSA, theseMSA-based morphological tools do not work wellfor dialects.5This issue relates to the QCA corpus.In this work, we used an unsupervised morpho-logical segmenter, Morfessor-categories MAP6,an unsupervised model with a single hyper-parameter (Creutz and Lagus, 2007).
We choseMorfessor because of its superior performance onArabic compared to other unsupervised models(Siivola et al., 2007; Poon et al., 2009).The model has a single hyperparameter, the per-plexity threshold parameter B, which controls thegranularity of segmentation.
The recommendedvalue ranges from 1 to 400 where 1 means max-imum fine-grained segmentation, and 400 restrictsit to the least segmented output.
We set the thresh-old empirically to 70, as shown in Section 5.1.5 Experimental SetupWe performed an extrinsic evaluation of the varia-tions in segmentation by building a Qatari Arabicto English machine translation system on each ofthem.
We also tested Morfessor on other availabledialects and on MSA, and we will show below howa uniform segmentation can help to better adapt re-sources for dialects and MSA for SMT.
This sec-tion describes our experimental setup.Datasets: We divided the QCA corpus into 1ksentences each for development and testing, andwe used the remaining 12k for training.We adapted parallel corpora for Egyptian, Lev-antine and MSA to English to be used for QatariArabic to English SMT.
For MSA, we used par-allel corpora of TED talks (Cettolo et al., 2012)and the AMARA corpus (Abdelali et al., 2014),which consists of educational videos.
Since theQCA corpus is in the speech domain, we believethat an MSA corpus of spoken domain would bemore helpful than a text domain such as News.
ForEgyptian and Levantine, we used the parallel cor-pus provided by Zbib et al.
(2012).
There is noGulf?English parallel data available in the litera-ture.
The data that we found was a very small col-lection of subdialects of Gulf Arabic; we did notuse it for MT experiments.
However, we used theQatari part of the AVIA corpus to train Morfessor.Machine translation system settings: We useda phrase-based statistical machine translationmodel as implemented in the Moses toolkit(Koehn et al., 2007) for machine translation.6This is an extension of the basic Morfessor method andis based on a Maximum a Posteriori model.211We built separate directed word alignmentsfor source-to-target and target-to-source usingIBM model 4 (Brown et al., 1993), and wesymmetrized them using the grow-diag-final-andheuristics (Koehn et al., 2003).
We then extractedphrase pairs with a maximum length of seven, andwe scored them using maximum likelihood esti-mation with Kneser-Ney smoothing (Kneser andNey, 1995).
We also built a lexicalized reorderingmodel, msd-bidirectional-fe.
We built a 5-gramlanguage model on the English side of QCA-trainusing KenLM (Heafield, 2011).
Finally, we built alog-linear model using the above features.We tuned the model weights by optimizingBLEU (Papineni et al., 2002) on the tuning set, us-ing PRO (Hopkins and May, 2011) with sentence-level BLEU+1 optimization (Nakov et al., 2012).In testing, we used minimum Bayes risk decoding(Kumar and Byrne, 2004), cube pruning, and theoperation sequence model (Durrani et al., 2011).Baseline: Our baseline Qatari Arabic to EnglishMT system is trained on the QCA bitext withoutany segmentation of Qatari Arabic.
For the exper-iments described in this paper, we used the Englishside of the QCA corpus for language modeling.5.1 Experimental ResultsIn this section, we first present our work on usingMorfessor for segmenting Qatari Arabic.
We trieddifferent values of its parameter, and we trained itusing corpora of different sizes to find balancedsettings that improve SMT quality as comparedwith no segmentation and with segmentation us-ing the Stanford ATB segmenter.
We further ap-plied our selected settings to segment MSA, EGYand LEV and used them for Qatari Arabic to En-glish machine translation.
Our results show that auniform segmentation scheme across different di-alects improves machine translation.Morfessor training variations: We trainedMorfessor using three corpora: (i) QCA,(ii) AVIAQAplus Qatari Novels, and (iii) a com-bination thereof.
Table 3 shows the results forour SMT system when trained on the QCA par-allel corpus, which was segmented using differenttraining models of Morfessor with B = 40.
Theresult for segmented Qatari Arabic is always bet-ter than the baseline, irrespective of the trainingmodel used for segmentation.
We can see that theMorfessor model trained on a large monolingualcorpus, i.e., on (ii) or (iii), yields better results.Morfessor BLEU OOV%Baseline 12.2 16.6QCA 12.5 0.6AVIAQA, Novels 13.5 0.8QCA, AVIAQA, Novels 13.4 0.7Table 3: Study of the effect of varying the train-ing datasets for Morfessor on the Qatari to EnglishSMT.
?Baseline?
shows the output of the MT sys-tem with no segmentation.B 10 40 70 100 130BLEU 13.3 13.5 13.8 12.9 12.6OOV 0.3 0.8 1.4 2.8 2.8After mergingBLEU 12.5 13.4 13.7 12.8 12.3OOV 1.5 1.9 3.9 6.5 9.8Table 4: The effect of varying the perplexitythreshold parameter B of Morfessor on SMT qual-ity.
?After merging?
are the results using the post-processed Qatari segmented data.The high reduction in OOV in Table 3 is be-cause of the fine-grained segmentation.
We trieddifferent values for the perplexity parameter Bin order to find a good balance between betterBLEU scores and linguistically correct segmen-tations.
The first part of Table 4 shows the ef-fect of different values of B on the quality of themachine translation system trained on AVIAQA,Qatari Novels.
We achieved the best SMT score atB = 70.We further analyzed the output of Morfessorat B = 70 and we noticed that it tends to gener-ate very small segments of length two and threecharacters long.
The segmentation produces morethan one stem in a word and does not generate le-gal word units.
For example, the word??AJ??@?
?and the industry?
/wAlSinAEp/ is segmented asPRE/?
+ PRE/?
@ + STM/?
+ PRE/?
+ PRE/ @+ STM/?
+ SUF/?.
We apply a post-processingstep that merges all stems in a word and affixesbetween them to one stem.
So, a word can haveonly one stem.
For example, the word??AJ??
@?would be segmented as PRE/?@?
+ STM/?AJ?
+SUF/?.
This yielded linguistically correct segmen-tations in many cases.
The second part of Table4 shows the effect of the post-processing on theBLEU score.
We can see that it remains almostthe same with an increase in OOV rate.212For rest of the experiments in this paper, weused a value of 70 for the perplexity thresholdparameter plus the post-processing on segmenta-tion.
We trained Morfessor on the concatenationof QCA, AVIAQA and Novels.7Using other Arabic variations: In this section,we present experiments using MSA, EGY andLEV to English bitexts combined with the QCAbitext for Qatari Arabic to English machine trans-lation.
We explored three segmentation options forthe Arabic side of the data: (i) no segmentation,(ii) ATB segmentation, and (iii) unsupervised seg-mentation using Morfessor.The QCA corpus is of much smaller size com-pared to other Arabic variants, say MSA.
It is pos-sible that in the training of the machine transla-tion models, the large corpus dominates the QCAcorpus.
In order to avoid that, we balanced thetwo corpora by replicating the smaller corpus Xnumber of times in order to make it approximatelyequal to the large corpus (Nakov and Ng, 2009).8The complete procedure is described below.In a nutshell, for building a machine transla-tion system using the MSA plus Qatari corpus, wefirst balanced the Qatari corpus to make it approx-imately equal to MSA and concatenated them.
Fortraining Morfessor, the Qatari Arabic data con-sisted of QCA, Novels and AVIAQA, while forSMT, it consisted of QCA only.
In both cases,we balanced it to be approximately equal to MSA.We then trained Morfessor on the balanced (QCA,Novels, AVIAQA) plus MSA data and we seg-mented the Arabic side of the balanced QCA plusMSA training data for machine translation.
Webuilt a machine translation system on the seg-mented data.
We segmented the testing and tuningdata sets similarly.
We used the same balancingwhen we combined EGY-EN and LEV-EN withthe Qatari Arabic ?
English data.We also tried training multiple unsupervisedmodels, but this yielded lower SMT quality com-pared to using a single model trained on multi-dialects.
Using different models could resultin having different segmentation schemes, whichmight not help in reducing the vocabulary mis-match between different variants of Arabic.7We did not see a big difference in training Morfessorwith and without the QCA corpus, and we decided to usethe complete data for training.8Due to the spoken nature of the QCA corpus, it containsshorter sentences.
Thus, we balanced the corpora based onthe number of tokens rather than on the number of sentences.Train NONE ATB MorfessorQCA 12.2 12.9 13.7?QCA,MSA 12.7 13.3 14.6?QCA,EGY 13.0 13.5 14.5?QCA,LEV 13.8 13.7 15.2Table 5: BLEU scores for Qatari Arabic to EnglishSMT using three different segmentation settings.
?QCA means the modified QCA corpus with num-ber of tokens approximately equal to MSA, EGYand LEV in the respective experiments.Table 5 shows the results.
There are two thingsto point here.
First, the SMT systems that usedthe unsupervised morphological segmenter, Mor-fessor, outperformed the systems that used no seg-mentation and those using the ATB segmentation.The Morfessor-based systems showed consistentimprovements compared to the ATB-based sys-tems over the no-segmentation systems.
This val-idates our point that unsupervised morphologicalsegmentation generalizes well for a variety of di-alects and these SMT results complement that.The second observation is that adding a bitext forother dialects and MSA improves machine trans-lation quality for Qatari?English SMT.6 Translation into Qatari ArabicOur monolingual corpora of Gulf subdialectscould be also helpful when translating English intoQatari Arabic.
We conducted a few basic experi-ments in this direction but without segmentation.We trained an English to Qatari Arabic SMTsystem on the QCA bitext, using the same settingsas described in Section 5.
We then normalized theoutput of the translation system using the QCRI-Normalizer (Sajjad et al., 2013b).9As a languagemodel, we used the Arabic side of the QCA cor-pus, novels and forum data, standalone and to-gether.
Table 6 presents the results of the effect ofvarying the language model on the quality of theSMT system.
The best system shows an improve-ment of 0.22 BLEU points absolute compared tothe baseline system that only uses the Arabic sideof the QCA corpus for LM training.The SMT system achieved the largest gain whenadding QA forum data to the QCA data.
SA andAE monolingual data also showed good improve-ments.
This might be due to their relatively largesizes; we need further investigation.9http://alt.qcri.org/tools/213LM BLEUQCA 2.78QCA+QA-Novels 2.64QCA+QA-Novels+BH-Novels 2.86QCA+QA-Novels+KW-Novels 2.78QCA+QA-Novels+AE-Novels 2.92QCA+QA-Novels+SA-Novels 2.96QCA+ALL-Novels 2.80QCA+QA-Novels+QForum 3.00Table 6: Results for English to Qatari SMT forvarying language models.
In all cases, the transla-tion model is trained on the QCA bitext only.Note the quite low BLEU scores, especiallycompared to the reverse translation direction.
Onereason is the morphologically rich nature of QatariArabic, which makes translating into it a hardproblem.
The small amount of training data fur-ther adds to it.
We expect to see larger gains com-pared to Qatari Arabic to English machine transla-tion when segmentation is used.7 Conclusion and Future WorkWe have demonstrated the feasibility of usingan unsupervised morphological segmenter to in-crease the resource adaptability of Arabic variants.We evaluated the segmentation on a Qatari dialectby building a Qatari Arabic to English machinetranslation system.
We further adapted MSA,EGY and LEV in the simplest machine translationsettings and we showed a consistent improvementof 1.5 BLEU points when compared to the respec-tive baseline system that uses no segmentation.In the future, we would like to explore theimpact of segmentation on both the translationmodel and the language model when translatinginto Qatari Arabic.
This involves greater chal-lenges, as a desegmenter is required for the trans-lation output with every segmentation scheme.ReferencesAhmed Abdelali, Francisco Guzman, Hassan Sajjad,and Stephan Vogel.
2014.
The AMARA corpus:Building parallel language resources for the educa-tional domain.
In Proceedings of the 9th Interna-tional Conference on Language Resources and Eval-uation, Reykjavik, Iceland, May.Rania Al-Sabbagh and Roxana Girju.
2010.
Miningthe web for the induction of a dialectical Arabic lexi-con.
In Proceedings of the 7th International Confer-ence on Language Resources and Evaluation, Val-letta, Malta, May.Hitham Abo Bakr, Khaled Shaalan, and IbrahimZiedan.
2008.
A hybrid approach for convert-ing written Egyptian colloquial dialect into dia-critized Arabic.
In Proceedings of the 6th Inter-national Conference on Informatics and Systems,Cairo, Egypt, March.Houda Bouamor, Nizar Habash, and Kemal Oflazer.2014.
A multidialectal parallel corpus of Arabic.
InProceedings of the 9th edition of the Language Re-sources and Evaluation Conference, Reykjavik, Ice-land, May.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and R. L. Mercer.
1993.
The mathe-matics of statistical machine translation: parameterestimation.
Computational Linguistics, 19(2), June.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
Wit3: Web inventory of transcribedand translated talks.
In Proceedings of the 16thCon-ference of the European Association for MachineTranslation, Trento, Italy, May.David Chiang, Mona Diab, Nizar Habash, Owen Ram-bow, and Safiullah Shareef.
2006.
Parsing Arabicdialects.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Com-putational Linguistics, Trento, Italy, April.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4(1), January.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, Port-land, OR, June.Mohamed Elmahdy, Mark Hasegawa-Johnson, andEiman Mustafawi.
2014.
Development of a TVbroadcasts speech recognition system for QatariArabic.
In Proceedings of the 9th edition of the Lan-guage Resources and Evaluation Conference, Reyk-javik, Iceland, May.Spence Green and John DeNero.
2012.
A class-basedagreement model for generating accurately inflectedtranslations.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics, Jeju Island, Korea, July.Nizar Habash and Owen Rambow.
2006.
MAGEAD:a morphological analyzer and generator for the Ara-bic dialects.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics, Sydney, Australia, July.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.Mada+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, pos214tagging, stemming and lemmatization.
In Proceed-ings of the 2nd International Conference on Ara-bic Language Resources and Tools (MEDAR), Cairo,Egypt, April.Nizar Habash, Ramy Eskander, and Abdelati Hawwari.2012.
A morphological analyzer for Egyptian Ara-bic.
In Proceedings of the 12th Meeting of the Spe-cial Interest Group on Computational Morphologyand Phonology, Montreal, Canada, June.Nizar Y Habash.
2010.
Introduction to Arabic naturallanguage processing.
Synthesis Lectures on HumanLanguage Technologies, 3(1), August.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the 6thWorkshop on Statistical Machine Translation, Edin-burgh, UK, July.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,Scotland, UK, July.Serena Jeblee, Weston Feely, Houda Bouamor, AlonLavie, Nizar Habash, and Kemal Oflazer.
2014.Domain and Dialect Adaptation for Machine Trans-lation into Egyptian Arabic.
In Proceedings ofthe Arabic Natural Language Processing Workshop,Doha, Qatar, October.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for ngram langauge modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, Detroit,Michigan, May.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology and NorthAmerican Association for Computational Linguis-tics Conference, Edmonton, Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics, Demonstra-tion Program, Prague, Czech Republic, June.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapterof the Association for Computational Linguistics,Boston, MA, May.Mohamed Maamouri, Ann Bies, Tim Buckwalter,Mona Diab, Nizar Habash, Owen Rambow, andDalila Tabessi.
2006.
Developing and using a pi-lot dialectal Arabic treebank.
In Proceedings ofthe 5th International Conference on Language Re-sources and Evaluation, Genova, Italy, May.Hamdy Mubarak and Kareem Darwish.
2014.
UsingTwitter to collect a multi-dialectal corpus of Arabic.In Proceedings of the Arabic Natural Language Pro-cessing Workshop, Doha, Qatar, October.Preslav Nakov and Hwee Tou Ng.
2009.
Improvedstatistical machine translation for resource-poor lan-guages using related resource-rich languages.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, Suntec,Singapore, August.Preslav Nakov, Francisco Guzman, and Stephan Vo-gel.
2012.
Optimizing for sentence-level BLEU+1yields short translations.
In Proceedings of the24th International Conference on ComputationalLinguistics, Mumbai, India, December.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th annual meeting on association for com-putational linguistics, Philadelphia, PA, July.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentationwith log-linear models.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Asso-ciation for Computational Linguistics, Denver, CO,June.Jason Riesa and David Yarowsky.
2006.
Minimallysupervised morphological segmentation with appli-cations to machine translation.
In Proceedings ofthe 7th Conference of the Association for MachineTranslation in the Americas, MA, USA, August.Hassan Sajjad, Kareem Darwish, and Yonatan Be-linkov.
2013a.
Translating dialectal Arabic to En-glish.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics,Sofia, Bulgaria, August.Hassan Sajjad, Francisco Guzman, Preslav Nakov,Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli,and Stephan Vogel.
2013b.
QCRI at IWSLT 2013:Experiments in Arabic-English and English-ArabicSpoken Language Translation.
In Proceedings of the10th International Workshop on Spoken LanguageTranslation, Hiedelberg, Germany, December.Ahmed Salama, Houda Bouamor, Behrang Mohit, andKemal Oflazer.
2014.
YouDACC: the youtube di-alectal Arabic commentary corpus.
In Proceedingsof the 9th edition of the Language Resources andEvaluation Conference, Reykjavik, Iceland, May.Wael Salloum and Nizar Habash.
2011.
Dialectalto standard Arabic paraphrasing to improve Arabic-English statistical machine translation.
In Proceed-ings of the First Workshop on Algorithms and Re-sources for Modelling of Dialects and Language Va-rieties, Edinburgh, Scotland, July.215Hassan Sawaf.
2010.
Arabic dialect handling in hybridmachine translation.
In Proceedings of the 9th Con-ference of the Association for Machine Translationin the Americas, Denver, CO, October.Vesa Siivola, Mathias Creutz, and Mikko Kurimo.2007.
Morfessor and VariKN machine learningtools for speech and language technology.
InProceedings of the 8th International Conferenceon Speech Communication and Technology (Inter-speech), Antwerpen, Belgium, August.Ming-Feng Tsai, Preslav Nakov, and Hwee Tou Ng.2010.
Morphological analysis for resource-poormachine translation.
Technical report, Kent Ridge,Singapore, December.Omar F Zaidan and Chris Callison-Burch.
2011.
TheArabic online commentary dataset: an annotateddataset of informal Arabic with high dialectal con-tent.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: short papers-Volume2, Portland, OR, June.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012.
Machine translation of Arabic di-alects.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Montreal, Canada, June.Andreas Zollmann, Ashish Venugopal, and StephanVogel.
2006.
Bridging the inflection morphol-ogy gap for Arabic statistical machine translation.In Proceedings of the Human Language Technol-ogy Conference of the NAACL, Companion Volume:Short Papers, New York, NY, June.216
