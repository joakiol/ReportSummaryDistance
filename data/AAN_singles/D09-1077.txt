Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 737?745,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPWord Buffering Models for Improved Speech Repair Parsing?Tim MillerUniversity of Minnesota ?
Twin Citiestmill@cs.umn.eduAbstractThis paper describes a time-series modelfor parsing transcribed speech containingdisfluencies.
This model differs from pre-vious parsers in its explicit modeling of abuffer of recent words, which allows it torecognize repairs more easily due to thefrequent overlap in words between errorsand their repairs.
The parser implement-ing this model is evaluated on the stan-dard Switchboard transcribed speech pars-ing task for overall parsing accuracy andedited word detection.1 IntroductionSpeech repair is a phenomenon in spontaneousspeech where a speaker interrupts the flow ofspeech (at what?s called the interruption point),backtracks some number of words (the reparan-dum), and continues the utterance with materialmeant to replace the reparandum (the alteration).1The utterance can be rendered syntactically cor-rect by excising all the words that the speakerskipped over when backtracking.
Speech with re-pair is difficult for machines to process because inaddition to detecting repair, a system must knowwhat words are meant to be excised, and parsingsystems must determine how to form a grammat-ical structure out of the set of words comprisingboth the error speech and the correct speech.Recent approaches to syntactic modeling ofspeech with repairs have shown that significantgains in parsing accuracy can be achieved by mod-eling the syntax of repairs (Hale et al, 2006;Core and Schubert, 1999).
In addition, othershave shown that a parser based on a time-seriesmodel that explicitly represents the incomplete?This research was supported by NSF CAREER award0447685.
The views expressed are not necessarily endorsedby the sponsors .1This terminology follows Shriberg (1994).constituents in fluent and disfluent speech can alsoimprove parsing accuracy (Miller and Schuler,2008).
However, these parsing approaches are stillnot as accurate at detecting reparanda as classifica-tion systems which use a variety of features to de-tect repairs (Charniak and Johnson, 2001; Johnsonand Charniak, 2004; Heeman and Allen, 1999).One highly salient feature which classificationsystems use to detect repair is the repetition ofwords between the error and the repair.
Johnsonand Charniak report that 60% of words in the al-terations are copies of words in reparanda in theSwitchboard corpus.
Typically, this informationis not available to a parser trained on context-freegrammars.Meanwhile, psycholinguistic models suggestthat the human language system makes use ofbuffers both to keep track of recent input (Bad-deley et al, 1998) and to smooth out generation(Levelt, 1989).
These buffers are hypothesizedto contain representations of recent phonologicalevents, suggesting that there is a short windowwhere new input might be compared to recent in-put.
This could be represented as a buffer whichpredicts or detects repeated input in certain con-strained circumstances.This paper describes a hybrid parsing sys-tem operating on transcribed speech which com-bines an incremental parser implemented as aprobabilistic time-series model, as in Miller andSchuler, with a buffer of recent words meant toloosely model something like a phonological loop,which should better account for word repetition ef-fects in speech repair.2 BackgroundThis work uses the Switchboard corpus (Godfreyet al, 1992) for both training and testing.
Thiscorpus contains transcribed and syntactically an-notated conversations between human interlocu-tors.
The reparanda in speech repairs are ulti-737mately dominated by the EDITED label, and incases where the reparandum ends with an unfin-ished constituent, the lowest constituent label isaugmented with the -UNF tag.
These annotationsprovide necessary but not sufficient informationfor parsing speech with repairs, and thus many im-provements in performing this task come as the re-sult of modifying these annotations in the trainingdata.As mentioned above, both Hale and colleagues(2006) and Miller and Schuler (2008) showedthat speech repairs contain syntactic regularities,which can improve the parsing of transcribedspeech with repairs when modeled properly.
Haleet al used ?daughter annotation?, which adds thelabel of an EDITED node?s child to the EDITEDlabel itself, and ?-UNF propagation?, which la-bels every node between an original -UNF nodeand the EDITED with an -UNF tag.
Miller andSchuler used a ?right-corner transform?
to convertstandard phrase structure trees of the Penn Tree-bank into ?right-corner trees?, which have highlyleft-branching structure and non-standard tree cat-egories representing incomplete constituents be-ing recognized.
These trees can be mapped into afixed-depth Hierarchical Hidden Markov Model toachieve improved parsing and reparandum-findingresults over standard CYK parsers.Work by Johnson and Charniak (2004; 2001)uses much of the same structure, but is not a pars-ing approach per se.
In earlier work, they used aboosting algorithm using word identity and cate-gory features to classify individual words as partof a reparandum or not, and achieved very im-pressive accuracy.
More recent work uses a tree-adjoining grammar (TAG) to model the overlap inwords and part-of-speech tags between reparan-dum and alteration as context sensitive syntaxtrees.
A parser is then used to rank the multipleoutputs of the TAG model with reparandum wordsremoved.Another approach that makes use of the corre-spondence between words in the reparandum andalteration is Heeman and Allen (1999).
This ap-proach uses several sources of evidence, includingword and POS correspondence, to predict repairbeginnings and correct them (by predicting howfar back they are intended to retrace).
This modelincludes random variables between words that cor-respond to repair state, and in a repair state, allowswords in the reparandum to ?license?
words in the.
.
.. .
.. .
.. .
.f3t?1f2t?1f1t?1q1t?1q2t?1q3t?1ot?1f3tf2tf1tq1tq2tq3totFigure 1: Graphical representation of the depen-dency structure in a standard Hierarchic HiddenMarkov Model with D = 3 hidden levels thatcan be used to parse syntax.
Circles denote ran-dom variables, and edges denote conditional de-pendencies.
Shaded circles denote variables withobserved values.alteration with high probability, accounting for thehigh percentage of copied words and POS tags be-tween reparandum and alteration.3 Model DescriptionThis work is based on a standard Hierarchical Hid-den Markov Model parser (Schuler, 2009), withthe addition of two new random variables fortracking the state of speech repair.
The HHMMframework is a desirable starting point for thiswork for two reasons: First, its definition in termsof a graphical model makes it easy to think aboutand to add new random variables.
Second, theHHMM parser operates incrementally in a left-to-right fashion on word input, which allows this sys-tem to run in a single pass, conditioning currentwords on a hypothesized buffer and interruptionpoint variable.
The incremental nature of this sys-tem is a constraint that other systems are not boundby, but makes this model more psycholinguisti-cally plausible.
In comparison, a CYK parsingframework attempting to use the same probabilis-tic model of word dependency between reparandaand alterations would need to do a second pass af-ter obtaining the most likely parses, in order to tellif a particular word?s generation probability in aspecific parse is influenced by a recent repair.The graphical model representation of thisframework is illustrated in Figures 1 and 4.
Theoriginal model, shown in Figure 1, has complexvariables Q and F broken down into several qdtand fdtfor time step t and depth d. These ran-738dom variables will be explained shortly, but fornow suffice it to say that in this work they are un-altered from the original HHMM parsing frame-work, while those labeled I and B (Figure 4) areadditions specific to the system described in thispaper.
This section will next describe the stan-dard HHMM parsing framework, before describ-ing how this work augments it.3.1 Right-corner TransformThe HHMM parser consists of stacks of a fixeddepth, which contain hypotheses of constituentsthat are being processed.
In order to minimizethe number of stack levels needed in processing,the phrase structure trees in the training set aremodified using a ?right-corner transform?, whichconverts right expansion in trees to left expansion,leaving heavily left-branching structure requiringlittle depth.
The right-corner transform used inthis paper is simply the left-right dual of a left-corner transform (Johnson, 1998a).The right-corner transform can be defined asa recursive algorithm on phrase-structure trees inChomsky Normal Form (CNF).
Trees are con-verted to CNF first by binarizing using stan-dard linguistically-motivated techniques (Kleinand Manning, 2003; Johnson, 1998b).
Remainingunbinarized structure is binarized in a brute forcefashion, creating right-branching structure by cre-ating a single node which dominates the two right-most children of a ?super-binary?
tree, with the la-bel being the concatenation of its children?s labels(see Figure 2).Taking this CNF phrase structure tree as input,the right-corner transform algorithm keeps trackof two separate trees, the original and the newright-corner tree it is building.
This process be-gins at the right-most preterminal of the originaltree, and works its way up along the right ?spine?,while building its way down a corresponding leftspine of the new right-corner tree.
The trees be-low shows the first step of the algorithm, with thetree on the left being disassembled, the tree on theright being built from its parts, and the workingpositions in the trees shown in bold.ABbXY:?
ZzAA/Z?ZzThe bottom right corner of the original tree ismade the top right corner of the new tree, and theleft corner of the new tree is made the newworkingposition and given a ?slash?
category A/Z.
The?slash?
category label A/Z represents a tree thatis the start of a constituent of type A that needsa right-child of type Z in order to complete.
Thenew right-corner of the original tree is the parent(X) of the previous right corner, and its subtree isnow added to the right-corner derivation:ABbXY:?AA/ZA/X?Y:?ZzAfter the first step, the subtrees moved over tothe right-corner tree may have more complex sub-structure than a single word (in this case, ?
rep-resents that possibly complex structure).
After be-ing attached to the right-corner tree in the correctplace, the algorithm is recursively applied to thatnow right-branching substructure.Again, the left child is given a new slash cat-egory: The ?active constituent?
(the left side of aslash category) is inherited from the root, and the?awaited constituent?
(the right side of a slash cat-egory) is taken from the constituent label of theright-corner it came from.This algorithm proceeds iteratively up the rightspine of the original tree, moving structure to theright-corner tree and recursively transforming it asit is added.
The final step occurs when the originalroot (A in this case) is reduced to having a singlechild, in which case its child is added as a childof the leftmost current branch of the right-cornertree, and it is transformed recursively.Figures 2 and 3 show an example tree fromthe Switchboard corpus before and after the right-corner transform is applied.739SINTJsoINTJ SINTJuhSNPyouVPVBPlivePPINinNPdallasFigure 2: Input to the right-corner transform.
Thistree also shows an example of the ?brute-force?
bi-narization done on super-binary branches that can-not be otherwise be binarized with linguistically-motivated rules.SS/NPS/PPS/VPS/SS/INTJ SINTJsoINTJuhNPyouVBPliveINinNPdallasFigure 3: Right-corner transformed version of thetree in Figure 2.3.2 Hierarchical Hidden Markov ModelA Hierarchical Hidden Markov Model is essen-tially an HMM with a specific factorization thatis useful in many domains ?
the hidden state ateach time step is factored into d random variableswhich function as a stack, and d additional ran-dom variables which regulate the operations of thestack through time.
For the model of speech repairpresented here, an interruption point is identifiedby one of these regulator variables firing earlierthan it would in fluent speech.
This concept willbe formalized below.
The stack regulating randomvariables are typically marginalized out when per-forming inference on a sequence.While the vertical direction of the hidden sub-states (at a fixed t) represents a stack at a sin-gle point in time, the horizontal direction of thehidden sub-states (at a fixed d) can be viewed asa simple HMM at depth d, expanding the statefrom the HMM above it across multiple time stepsand causing the HMM below it to expand its ownstates.
This interpretation will be useful when for-mally defining the transitions between the stack el-ements at different time steps below.Formally, HMMs characterize speech or text asa sequence of hidden states qt(which may con-sist of speech sounds, words, and/or other hypoth-esized syntactic or semantic information), and ob-served states otat corresponding time steps t (typ-ically short, overlapping frames of an audio sig-nal, or words or characters in a text processingapplication).
A most likely sequence of hiddenstates q?1..Tcan then be hypothesized given any se-quence of observed states o1..T, using Bayes?
Law(Equation 2) and Markov independence assump-tions (Equation 3) to define a full P(q1..T| o1..T)probability as the product of a Language Model(?L) prior probability and an Observation Model(?O) likelihood probability:q?1..T= argmaxq1..TP(q1..T| o1..T) (1)= argmaxq1..TP(q1..T) ?
P(o1..T| q1..T) (2)def= argmaxq1..TT?t=1P?L(qt| qt?1)?P?O(ot| qt)(3)Language model transitions P?L(qt| qt?1) overcomplex hidden states qtcan be modeled us-ing synchronized levels of stacked-up compo-nent HMMs in a Hierarchic Hidden MarkovModel (HHMM) (Murphy and Paskin, 2001).HHMM transition probabilities are calculated intwo phases: a ?reduce?
phase (resulting in an in-termediate, marginalized state ft), in which com-ponent HMMs may terminate; and a ?shift?
phase(resulting in a modeled state qt), in which unter-minated HMMs transition, and terminated HMMsare re-initialized from their parent HMMs.
Vari-ables over intermediate ftand modeled qtstatesare factored into sequences of depth-specific vari-ables ?
one for each of D levels in the HMM hi-erarchy:ft= ?f1t.
.
.
fDt?
(4)qt= ?q1t.
.
.
qDt?
(5)Transition probabilities are then calculated as aproduct of transition probabilities at each level, us-ing level-specific ?reduce?
?F and ?shift?
?Q mod-740els:P?L(qt|qt?1) =?ftP(ft|qt?1)?P(qt|ftqt?1) (6)def=?f1..DtD?d=1P?F(fdt| fd+1tqdt?1qd?1t?1)?P?Q(qdt|fd+1tfdtqdt?1qd?1t)(7)with fD+1tand q0tdefined as constants.Shift and reduce probabilities are now definedin terms of finitely recursive FSAs with probabil-ity distributions over transition, recursive expan-sion, and final-state status of states at each hierar-chy level.
In the HHMM used in this paper, eachintermediate state variable is a reduction state vari-able fdt?
G ?
{0,1} (where G is the set of allnonterminal symbols from the original grammar),representing a reduction to the final syntactic statein G, a horizontal transition to a new awaited cate-gory, or a top-down transition to a new active cat-egory.
Each modeled state variable is a syntacticelement (qdt?
G ?
G) with an active and awaitedcategory represented with the slash notation.The intermediate variable fdtis probabilisticallydetermined given a reduction at the stack level be-low, but is deterministically 0 in the case of a non-reduction at the stack level below.
2P?F(fdt| fd+1tqdt?1qd?1t?1)def={if fd+1t/?
G : [fdt=0]if fd+1t?
G : P?F-Reduce(fdt| qdt?1, qd?1t?1)(8)where fD+1 ?
G and q0t= ROOT.Shift probabilities at each level are definedusing level-specific transition ?Q-T and expan-sion ?Q-E models:P?Q(qdt| fd+1tfdtqdt?1qd?1t)def=??
?if fd+1t/?G, fdt/?G : [qdt= qdt?1]if fd+1t?G, fdt/?G : P?Q-T(qdt| fd+1tfdtqdt?1qd?1t)if fd+1t?G, fdt?G : P?Q-E(qdt| qd?1t)(9)where fD+1 ?
G and q0t= ROOT.
This modelis conditioned on final-state switching variables atand immediately below the current HHMM level.If there is no final state immediately below the cur-rent level (the first case above), it deterministically2Here [?]
is an indicator function: [?]
= 1 if ?
is true, 0otherwise.copies the current HHMM state forward to thenext time step.
If there is a final state immediatelybelow the current level (the second case above),it transitions the HHMM state at the current level,according to the distribution ?Q-T. And if the stateat the current level is final (the third case above), itre-initializes this state given the state at the levelabove, according to the distribution ?Q-E. Theoverall effect is that higher-level HMMs are al-lowed to transition only when lower-level HMMsterminate.
An HHMM therefore behaves like aprobabilistic implementation of a pushdown au-tomaton (or ?shift-reduce?
parser) with a finitestack, where the maximum stack depth is equal tothe number of levels in the HHMM hierarchy.All of the probability distributions definedabove can be estimated by training on a corpus ofright-corner transformed trees, by mapping tree el-ements onto the random variables in the HHMMand computing conditional probability tables ateach random variable.
This process is described inmore detail in other work (Schuler et al, in press).3.3 Interruption Point and Word BufferThis paper expands upon this standard HHMMparsing model by adding two new sub-models tothe hidden variables described above, an interrup-tion point (I) variable, and a word buffer (B) .This model is illustrated in Figure 4, which takesFigure 1 as a starting point and adds random vari-ables just mentioned.Buffers are hypothesized to be used in the hu-man language system to smooth out delivery ofspeech (Levelt, 1989).
In this work, a buffer ofthat sort is placed between the syntax generatingelements and the observed evidence (words).
Itsrole in this model is not to smooth the flow ofspeech, but to keep a short memory that enablesthe speaker to conveniently and helpfully restartwhen a repair is produced.
This in turn gives as-sistance to a listener trying to understand what thespeaker is saying, since the listener also has thelast few words in memory.The I variable implements a state machine thatkeeps track of the repair status at each time point.The domain of this variable is {0,1,ET}, where1 indicates the first word of an alteration, ET in-dicates editing terms in between reparandum andalteration, and 0 indicating no repair.33Actually, 0 can occur during an alteration, but in thosecases that fact is indicated by the state of the buffer.741f3t?2f2t?2f1t?2q1t?2q2t?2q3t?2ot?2Qt?2Ft?2it?2bt?2f3t?1f2t?1f1t?1q1t?1q2t?1q3t?1ot?1Qt?1Ft?1it?1bt?1f3tf2tf1tq1tq2tq3totQtFtitbtFigure 4: Extended HHMM parsing model with variables for interruption points (I) and a modeled wordbuffer (B).
Arrows within and between complex hidden variables F andQ have been removed for clarity.The value of I is deterministically constrainedin this work by its inputs, but it can be conceivedas a conditional probability P(it| it?1, qt, qt?1, rt)to allow footholds for future research.4 Whiledepending formally on many values, in practiceits dependencies are highly context-dependent andconstrained:P(it| it?1, qt, qt?1, qt)def=??????????????????
?if it?1=1 : [it=0]if it?1=ET ?
(INTJ ?
PRN) ?
qt: [it=ET]if it?1=ET : [it=1]if it?1=0 ?
EDITED ?
(qt?1?
ft)?
(INTJ ?
PRN) ?
qt: [it=ET]if it?1=0 ?
EDITED ?
(qt?1?
ft) : [it=1]if it?1=0 : [it=0]These conditions are meant to be evaluated ina short-circuiting fashion, i.e., the first conditionwhich is true starting from the top is applied.
Thedefault (last) case is most common, going fromnon-repair to non-repair state.
When the syntaxgenerated something with the category EDITEDat the last time step (as evidenced by either themodeled state variable qt?1or the reduction statevariable ftdepending on the length of the reparan-dum), the interruption point variable is triggered tochange, either to ET if an interjection (INTJ) or4Most obviously, this variable could be made prior to itsconditions to be their cause, if a suitable model for the causa-tion of interruption points was designed using prosodic cues.For this work, it is simply an intermediary that is not strictlynecessary but makes the model design more intuitive.parenthetical (PRN) followed, otherwise to 1 forthe first word of an alteration.
The ET state con-tinues as long as the syntax at the current level isgenerating something containing INTJ or PRN.The random variable for the word buffer is morecomplex, containing at each time step t an integerindex for keeping track of a current position in thebuffer (ct?
?0, 1, .
.
.
, n?
1?
for buffer size n),and an array of several recently generated words(~wt).
This can be represented as the following con-ditional probability:P(bt| bt?1, it, qt) = P(ct| ct?1, it)?P(~wt| ~wt?1, ct) (10)The operation of the buffer is governed by fourcases:Case 1: During normal operation (i.e.
for fluentspeech), the interruption point variable is 0 andat the previous time step the buffer index pointsat the end of the buffer (it=0 ?
ct?1=n?1).
Inthis simple case, the buffer pointer remains point-ing at the end position in the buffer (ct=n?
1),and the last n?
1 items in the buffer are determin-istically copied backwards one position.
A newword is generated probabilistically to occupy thelast position in the buffer (where ctis pointing).This probability is estimated empirically using thesame model used in a standard HHMM to gener-ate words, by conditioning the word on the deepestnon-empty qtvalue in the stack.Case 2: When an editing term is being gener-ated, (it=ET), the buffer is not in use.
Practi-742cally, this means that the value of the index c andall wj are just copied over from time t?1 to timet.
This makes sense psycholinguistically, becausea buffer used to smooth speech rates would by def-inition not be used when speech is interrupted bya repair.
It also makes sense from a purely engi-neering point of view, since words used as editingterms are usually stock phrases and filled pausesthat are not likely to have much predictive valuefor the alteration, and are thus not worth keeping inthe buffer.
The probability of the actual observedword is modeled the same way word probabilitiesare modeled in a standard HHMM, conditioned onthe deepest non-empty qtvalue, and ignoring thebuffer.Case 3: The alteration case applies to the firstword after the reparandum and optional editingterms (it=1).
In this case, the index ctfor the cur-rent position of the buffer is obtained by subtract-ing a number of words to replace, with that num-ber drawn from a prior distribution.
This distribu-tion is based on the function f(k) = 1.22 ?
0.45k.This function was taken from Shriberg (1996),where it was estimated based on several differ-ent training corpora, and provided a remarkablefit to all of them.
Since this model uses a fixedsize buffer, the values are precomputed and renor-malized to form a probability distribution.
Witha buffer size of only n = 4, approximately 96%of the probability mass of the original function isaccounted for.After the indices are computed, the buffer at po-sition ctis given a word value.
The model firstdecides whether to substitute or copy the previousword over.
The probability governing this decisionis also determined empirically, by computing howoften the first word in a alteration in the Switch-board training set is a copy of the first word it ismeant to replace.
If the copy operation is selected,the word is added to the buffer without further di-luting its probability.
If, however, the substitutionoperation was selected, the word is added to thebuffer with probability distributed across all pos-sible words.Case 4: The final case to account foris alterations of length greater than one(it=0 ?
ct?16= n?1).
This occurs when thecurrent index was moved back more than oneposition, and so even though i is set to 0, thecurrent index into the buffer is not pointing at theend.
In this case, again the index ctis selectedaccording to a prior probability distribution.
Thevalue selected from the distribution correspondsto different actions that may be selected whenretracing the words in the reparandum to generatethe alteration.The first option is that the current index remainsin place, which corresponds to an insertion oper-ation, where the alteration is given an extra wordrelative to the reparandum at its current position.Following an insertion, a new word is generatedand placed in the buffer at the current index, withprobability conditioned on the syntax at the mostrecent time step.
The second option is to continuethe alignment, moving the current index forwardone position in the buffer, and then either perform-ing a substitution or copy operation in alignmentwith a word from the alteration.
Word probabil-ities for the copy and substitution operations aregenerated in the same way as for the first word ofan alteration.
Finally, the current index may skipforward more than one value, performing a dele-tion operation.
Deletion skips over words in thereparandum that do not correspond to words in thealteration.
After the deletion moves the current in-dex pointer forward, a word is again either copiedor substituted against the newly aligned word.The prior probability distributions over align-ment operations is estimated from data in theSwitchboard in a similar manner to Johnson andCharniak (2004).
Briefly, using the disfluency-annotated section of the Switchboard corpus (.dpsfiles), a list of reparanda and alterations corre-sponding to one another are compiled.
For eachpair, the minimal cost alignment is computed,where a copy operation has cost 0, substitutionhas cost 4, and deletion and insertion each havecost 7.
Using these alignments, probabilities arecomputed using relative frequency counts for boththe first word of an alteration, and for subsequentoperations.
Copy and substitution are the most fre-quent operations (copying gives information aboutthe repair itself, while substitution can correct thereason for the error), insertion is somewhat lessfrequent (presumably for specifying further infor-mation), and deletion is relatively rare (usually arepair is not made to remove information).4 EvaluationThis model was evaluated on the Switchboardcorpus (Godfrey et al, 1992) of conversationaltelephone speech between two human interlocu-743System Precision Recall F-ScorePlain CYK 18.01 17.73 17.87Hale et al CYK 40.90 35.41 37.96Hale et al Lex.
n/a n/a 70.0TAG 82.0 77.8 79.7Plain HHMM 43.90 47.36 45.57HHMM-Back 44.12 57.49 49.93HHMM-Retrace 48.82 59.41 53.59Table 1: Table of results of edit-finding accuracy.Italics indicate reported, rather than reproduced,results.System Configuration Parseval-F Edited-FPlain CYK 71.03 17.9Hale et al CYK 68.47 37.96Hale et al Lex.
80.16 70.0Plain HHMM 74.23 45.57HHMM-Back 74.58 49.93HHMM-Retrace 74.23 53.59Table 2: Table of parsing results.tors.
The input to this system is the gold standardword transcriptions, segmented into individual ut-terances.
The standard train/test breakdown wasused, with sections 2 and 3 used for training, andsubsections 0 and 1 of section 4 used for testing.Several held-out sentences from the end of section4 were used during development.For training, the data set was first standardizedby removing punctuation, empty categories, ty-pos, all categories representing repair structure,and partial words ?
anything that would be diffi-cult or impossible to obtain reliably with a speechrecognizer.The two metrics used here are the standard Par-seval F-measure, and Edit-finding F. The first takesthe F-score of labeled precision and recall of thenon-terminals in a hypothesized tree relative to thegold standard tree.
The second measure markswords in the gold standard as edited if they aredominated by a node labeled EDITED, and mea-sures the F-score of the hypothesized edited wordsrelative to the gold standard.Results are shown in Tables 1 and 2.
Table 1shows detailed results on edited word finding, withtwo test systems and several related approaches.The first two lines show results from a re-implementation of Hale et al parsers.
In boththose cases, gold standard part-of-speech (POS)tags were supplied to the parser.
The follow-ing two lines are reported results of a lexicalizedparser from Hale et al and the TAG system ofJohnson and Charniak.
The final three lines areevaluations of HHMM systems.
The first is animplementation of Miller and Schuler, run with-out gold standard POS tags as input.
The secondHHMM result is a systemmuch like that describedin this paper, but designed to approximate the bestresult that can come from simply trying to matchthe first word of an alteration with a recent word.Levelt (1989) notes that in over 90% of repairs, thefirst word of the alteration is either identical or amember of the same category as the first word ofthe reparandum, and this clue is enough for listen-ers to understand what the alteration is meant toreplace.
This implementation keeps the I variableto model repair state, but rather than a modeledbuffer being part of the hidden state, it keeps anobserved buffer that simply tracks the last n wordsseen (n = 4 in this experiment).
This buffer isused only to generate the first word of a repair, andonly when the syntactic state allows the word.
Fi-nally, the system described in Section 3 is shownon the final line.Table 2 shows overall parsing accuracy results,with the same set of systems, with the exceptionof the TAG system which did not report parsingresults.5 Discussion and ConclusionThese results first show that the main contributionof this paper, a model for a buffer of recent wordswhich influences speech repairs, results in drasticimprovements in the ability of an HHMM systemto discover edited words.
This model does this ina single pass through the observed words, incre-mentally forming hypotheses about the state of thesyntactic process as well as the state of repair, justas humans must recognize spontaneous speech.Another interesting result is the relative effec-tiveness of a buffer that is not modeled, but ratherjust a collection of words used to condition the firstwords of repair (?HHMM-Back?).
While this re-sult is superior to the plain HHMM system, it stillfalls well short of the retracing model using a mod-eled buffer.
This suggests that, though one wordis sufficient to align a reparandum and alterationwhen the existence of a repair is given, more in-formation is often necessary when the task is notjust alignment of repair but also detection of re-744pair.
A model that takes into account informationsources that identify the existence of repair, suchas prosodic cues (Hale et al, 2006; Lickley, 1996),may thus result in improved performance for thesimpler unmodeled buffer.These results also confirm that parsing sponta-neous speech with an HHMM can be far superiorto a CKY parser, even when the CKY parser isgiven the advantage of correct POS tags as input.Second, even the baseline HHMM system alsoimproves over the CYK parser in finding editedwords, again without the advantage of correct POStags as input.In conclusion, the model described here uses abuffer inspired by the phonological loop used inthe human auditory system to keep a short mem-ory of recent input.
This model, when used to as-sist in the detection and correction of repair, re-sults in a large increase in accuracy in detectionof repair over other most basic parsing systems.This system does not reach the performance lev-els of lexicalized parsers, nor multi-pass classifi-cation systems.
Future work will explore ways toapply additional features of these systems or othersources of information to account for the remain-der of the performance gap.ReferencesAlan Baddeley, Susan Gathercole, and Costanza Pa-pagno.
1998.
The phonological loop as a languagelearning device.
Psychological Review, 105(1):158?173, January.Eugene Charniak and Mark Johnson.
2001.
Edit de-tection and parsing for transcribed speech.
In 2ndMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 118?126.Mark G. Core and Lenhart K. Schubert.
1999.
A syn-tactic framework for speech repairs and other disrup-tions.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics (ACL99).John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speech cor-pus for research and development.
In Proc.
ICASSP,pages 517?520.John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,Mary Harper, Anna Krasnyanskaya, Matthew Lease,Yang Liu, Brian Roark, Matthew Snover, and RobinStewart.
2006.
PCFGs with syntactic and prosodicindicators of speech repairs.
In Proceedings of the45th Annual Conference of the Association for Com-putational Linguistics (COLING-ACL).Peter A. Heeman and James F. Allen.
1999.
Speechrepairs, intonational phrases, and discourse markers:Modeling speakers?
utterances in spoken dialogue.Computational Linguistics, 25:527?571.Mark Johnson and Eugene Charniak.
2004.
A tag-based noisy channel model of speech repairs.
InProceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics (ACL ?04),pages 33?39, Barcelona, Spain.Mark Johnson.
1998a.
Finite state approximation ofconstraint-based grammars using left-corner gram-mar transforms.
In Proceedings of COLING/ACL,pages 619?623.Mark Johnson.
1998b.
PCFG models of linguistic treerepresentation.
Computational Linguistics, 24:613?632.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 423?430.Willem J.M.
Levelt.
1989.
Speaking: From Intentionto Articulation.
MIT Press.R.
J. Lickley.
1996.
Juncture cues to disfluency.
InProceedings of The Fourth International Conferenceon Spoken Language Processing (ICSLP ?96), pages2478?2481.Tim Miller and William Schuler.
2008.
A syntac-tic time-series model for parsing fluent and dis-fluent speech.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(COLING?08).Kevin P. Murphy and Mark A. Paskin.
2001.
Lin-ear time inference in hierarchical HMMs.
In Proc.NIPS, pages 833?840.William Schuler, Samir AbdelRahman, TimMiller, andLane Schwartz.
in press.
Broad-coverage incremen-tal parsing using human-like memory constraints.Computational Linguistics.William Schuler.
2009.
Parsing with a boundedstack using a model-based right-corner transform.In Proceedings of the North American Associationfor Computational Linguistics (NAACL ?09), Boul-der, Colorado.Elizabeth Shriberg.
1994.
Preliminaries to a Theoryof Speech Disfluencies.
Ph.D. thesis, University ofCalifornia at Berkeley.Elizabeth Shriberg.
1996.
Disfluencies in Switch-board.
In Proceedings of International Conferenceon Spoken Language Processing.745
