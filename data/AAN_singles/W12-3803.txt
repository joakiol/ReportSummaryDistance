Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),pages 19?27, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational LinguisticsLinking Uncertainty in Physicians?
Narratives to Diagnostic CorrectnessWilson McCoyDepartment of InteractiveGames and Mediawgm4143@rit.eduCecilia Ovesdotter AlmDepartment of Englishcoagla@rit.eduCara CalvelliCollege of HealthSciences and Technologycfcscl@rit.eduJeff B. PelzCenter forImaging Sciencepelz@cis.rit.eduPengcheng ShiComputing andInformation Sciencespengcheng.shi@rit.eduRochester Institute of TechnologyAnne HaakeComputing andInformation Sciencesanne.haake@rit.eduAbstractIn the medical domain, misdiagnoses and di-agnostic uncertainty put lives at risk and in-cur substantial financial costs.
Clearly, medi-cal reasoning and decision-making need to bebetter understood.
We explore a possible linkbetween linguistic expression and diagnosticcorrectness.
We report on an unusual data setof spoken diagnostic narratives used to com-putationally model and predict diagnostic cor-rectness based on automatically extracted andlinguistically motivated features that capturephysicians?
uncertainty.
A multimodal dataset was collected as dermatologists viewed im-ages of skin conditions and explained their di-agnostic process and observations aloud.
Wediscuss experimentation and analysis in initialand secondary pilot studies.
In both cases,we experimented with computational model-ing using features from the acoustic-prosodicand lexical-structural linguistic modalities.1 IntroductionUp to 20% of post-mortem diagnoses in the UnitedStates are inconsistent with the diagnosis beforedeath (Graber, 2005).
These misdiagnoses cost bothhuman lives and estimated millions of dollars everyyear.
To find where and why misdiagnoses occur, itis necessary to improve our understanding of doc-tors?
diagnostic reasoning and how it is linked to di-agnostic uncertainty and correctness.
Our contribu-tion begins to explore the computational modelingof this phenomenon in diagnostic narratives.
From acognitive science perspective, we are contributing tothe research on medical reasoning and how it is lin-guistically expressed.
In the long term, this area ofwork could be a useful decision-making componentfor flagging diagnoses that need further review.The study used an unusual multimodal data setcollected in a modified Master-Apprentice interac-tion scenario.
It comprises both gaze and linguisticdata.
The present study focuses on the linguistic datawhich in turn can be conceptualized as consisting ofboth acoustic-prosodic and lexical-structural modal-ities.
This data set can further be used to link visionand language research to understand human cogni-tion in expert decision-making scenarios.We report on a study conducted in two phases.First, an initial pilot study involved a preliminary an-notation of a small subset of the collected diagnos-tic narratives and also investigated the prediction ofdiagnostic correctness using a set of linguistic fea-tures from speech recordings and their verbal tran-scriptions.
This provided initial features relevant toclassification, helped us identify annotation issues,and gave us insight on how to improve the annota-tion scheme used for annotating ground truth data.Next, a second pilot study was performed, build-ing on what was learned in the initial pilot study.The second pilot study involved a larger data setwith a revised and improved annotation scheme thatconsidered gradient correctness at different steps ofthe diagnostic reasoning process: (1) medical lesionmorphology (e.g.
recognizing the lesion type as ascaly erythematous plaque), (2) differential diagno-sis (i.e.
providing a set of possible final diagnoses),and (3) final diagnosis (e.g.
identifying the diseasecondition as psoriasis).
We also experiment with19classification using an expanded feature set moti-vated by the initial pilot study and by previouslypublished research.
We report on results that con-sider different algorithms, feature set modalities, di-agnostic reasoning steps, and coarse vs. fine grainedclasses as explained below in Section 4.3.2 Previous WorkMuch work has been done in the area of medi-cal decision-making.
Pelaccia et al (2011) haveviewed clinical reasoning through the lens of dual-process theory.
They posit that two systems are atwork in the mind of a clinician: the intuitive systemwhich quickly produces a response based on expe-rience and a holistic view of the situation, versusthe analytic system which slowly and logically stepsthrough the problem with conscious use of knowl-edge.
Croskerry (2009) stated that ?
[i]f the presen-tation is not recognized, or if it is unduly ambiguousor there is uncertainty, [analytic] processes engageinstead?
(p. 1022); for instance, if a clinician is un-familiar with a disease or unsure of their intuitiveanswer.
We assume that different reasoning systemsmay cause changes in linguistic behaviors.
For ex-ample, when engaging the slower analytic system, itseems reasonable that frequent pausing could appearas an indication of, e.g., uncertainty or thoughtful-ness.Several studies have explored the task of detect-ing uncertainty through language.
Uncertainty de-tection necessitates inference of extra-propositionalmeaning and is arguably a subjective natural lan-guage problem, i.e.
part of a family of problemsthat are increasingly receiving attention in compu-tational linguistics.
These problems involve moredynamic classification targets and different perfor-mance expectations (Alm, 2011).
Pon-Barry andShieber (2009) have shown encouraging results infinding uncertainty using acoustic-prosodic featuresat the word, word?s local context, and whole utter-ance levels.
Henriksson and Velupillai (2010) used?speculative words?
(e.g., could, generally, should,may, sort of, etc.)
as well as ?certainty ampli-fiers?
(e.g., definitely, positively, must, etc.)
to deter-mine uncertainty in text.
Velupillai (2010) also ap-plied the same approach to medical texts and notedthat acoustic-prosodic features should be consideredalongside salient lexical-structural features as indi-cators of uncertainty.
In this work, we draw on theinsight of such previous work, but we also extendthe types of linguistic evidence considered for iden-tifying possible links to diagnostic correctness.As another type of linguistic evidence, disfluen-cies make up potentially important linguistic evi-dence.
Zwarts and Johnson (2011) found that theoccurrence of disfluencies that had been removedcould be predicted to a satisfactory degree.
Pakho-mov (1999) observed that such disfluencies are justas common in monologues as in dialogues eventhough there is no need for the speakers to indicatethat they wish to continue speaking.
This finding isimportant for the work presented here because ourmodified use of the Master-Apprentice scenario re-sults in a particular dialogic interaction with the lis-tener remaining silent.
Perhaps most importantly,Clark and Fox Tree (2002) postulated that filledpauses (e.g., um, uh, er, etc.)
play a meaningfulrole in speech.
For example, they may signal thatthe speaker is yet to finish speaking or searching fora word.
There is some controversy about this claim,however, as explained by Corley and Stewart (2008).The scholarly controversy about the role of disfluen-cies indicates that more research is needed to under-stand the disfluency phenomenon, including how itrelates to extra-propositional meaning.3 Data SetThe original elicitation experiment included 16physicians with dermatological expertise.
Of these,12 were attending physicians and 4 were residents(i.e.
dermatologists in training).
The observers wereshown a series of 50 images of dermatological con-ditions.
The summary of this collected data is shownin Table 1, with reference to the pilot studies.The physicians were instructed to narrate, in En-glish, their thoughts and observations about each im-age to a student, who remained silent, as they arrivedat a differential diagnosis or a possible final diagno-sis.
This data elicitation approach is a modified ver-sion of the Master-Apprentice interaction scenario(Beyer and Holtzblatt, 1997).
This elicitation setupis shown in Figure 1.
It allows us to extract in-formation about the Master?s (i.e.
in this case, thephysician?s) cognitive process by coaxing them to20Data parameters Quantity# of participating doctors 16# of images for whichnarratives were collected 50# of time-aligned narrativesin the initial pilot study 160# of time-aligned narrativesin the second pilot study 707Table 1: This table summarizes the data.
Of the collectednarratives, 707 are included in this work; audio is unavail-able for some narratives.vocalize their thoughts in rich detail.
This teaching-oriented scenario really is a monologue, yet inducesa feeling of dialogic interaction in the Master.Figure 1: The Master-Apprentice interaction scenario al-lows us to extract information about the Master?s (here:doctor?s) cognitive processes.The form of narratives collected can be analyzedin many ways.
Figure 2 shows two narratives, re-cently elicited and similar to the ones in the study?sdata set, that are used here with permission as ex-amples.
In terms of diagnostic reasoning styles, re-ferring to Pelaccia et al (2011), we can propose thatobserver A may be using the intuitive system andthat observer B may be using the analytical system.Observer A does not provide a differential diagnosisand jumps straight to his/her final diagnosis, whichin this case is correct.
We can postulate that observerA looks at the general area of the lesion and usesprevious experience or heuristic knowledge to cometo the correct diagnosis.
This presumed use of theintuitive system could potentially relate to the depthof previous experience with a disease, for example.Observer B, on the other hand, might be using theA.
This patient has a pinkish papule withsurrounding hypopigmentation in a field ofother cherry hemagiomas and nevoid typelesions.
The only diagnosis that comes tomind to me is Sutton?s nevus.B.
I think I?m looking at an abdomen, possibly.I see a hypopigmented oval-shaped patch inthe center of the image.
I see that thereare two brown macules as well.
In the centerof the hypopigmented oval patch thereappears to be an area that may be a pinkmacule.
Differential diagnosis includeshalo nevus, melanoma, post-inflammatoryhypopigmentation.
I favor a diagnosis ofmaybe post-inflammatory hypopigmentation.Figure 2: Two narratives collected in a recent elicitationsetup and used here with permission.
Narratives A and Bare not part of the studied data set, but exemplify data setnarratives which could not be distributed.
Observers Aand B are both looking at an image of a halo or Sutton?snevus as seen in Figure 3.
Disfluencies are considered inthe experimental work but have been removed for read-ability in these examples.Figure 3: The image of a halo or Sutton?s nevus viewedby the observers and the subject of example narratives.analytical system.
Observer B steps through the di-agnosis in a methodical process and uses evidencepresented to rationalize the choice of final diagno-sis.
Observer B also provides a differential diagno-sis unlike observer A.
This suggests that observerB is taking advantage of a process of elimination todecide on a final diagnosis.Another way to evaluate these narratives is interms of correctness and the related concept of diag-21nostic completeness.
Whereas these newly elicitednarrative examples have not been annotated by doc-tors, some observations can still be made.
From thepoint of view of final diagnosis, observer A is cor-rect, unlike observer B.
Assessment of diagnosticcorrectness and completeness can also be made onintermediate steps in the diagnostic process (e.g.
dif-ferential diagnoses or medical lesion morphologicaldescription).
Including such steps in the diagnos-tic process is considered good practice.
Observer Adoes not supply a differential diagnosis and insteadskips to the final diagnosis.
Observer B providesthe correct answer in the differential diagnosis butgives the incorrect final diagnosis.
Observer B fullydescribes the medical lesion morphology presented.Observer A, however, only describes the pink lesionand does not discuss the other two brown lesions.The speech of the diagnostic narratives wasrecorded.
At the same time, the observers?
eye-movements were tracked; the eye-tracking dataare considered in another report (Li et al, 2010).We leave the integration of the linguistic and eye-tracking data for future work.After the collection of the raw audio data, theutterances were manually transcribed and time-aligned at the word level with the speech anal-ysis tool Praat (Boersma, 2001).1 A sample ofthe transcription process output is shown in Fig-ure 4.
Given our experimental context, off-the-shelfautomatic speech recognizers could not transcribethe narratives to the desired quality and resourceswere not available to create our own automatic tran-1See http://www.fon.hum.uva.nl/praat/.Figure 4: Transcripts were time-aligned in Praat whichwas also used to extract acoustic-prosodic features.scriber.
Manual transcription also preserved disflu-encies, which we believe convey meaningful infor-mation.
Disfluencies were transcribed to includefilled pauses (e.g.
uh, um), false starts (e.g.
pur-reddish purple), repetitions, and click sounds.This study is strengthened by its involvement ofmedical experts.
Trained dermatologists were re-cruited in the original elicitation experiment as wellas the creation and application of both annotationschemes.
This is crucial in a knowledge-rich domainsuch as medicine because the annotation schememust reflect the domain knowledge.
Another studyreports on annotation details (McCoy et al, Forth-coming 2012).4 Classification StudyThis section discusses the classification work, firstexplaining the methodology for the initial pilot studyfollowed by interpretation of results.
Next, themethodology of the second pilot study is described.4.1 Generic Model OverviewThis work applies computational modeling de-signed to predict diagnostic correctness in physi-cians?
narratives based on linguistic features fromthe acoustic-prosodic and lexical-structural modali-ties of language, shown in Table 2.
Some tests dis-cussed in 4.2 and 4.3 were performed with thesemodalities separated.
These features are inspiredby previous work conducted by Szarvas (2008),Szarvas et al (2008), Litman et al (2009), Liscombeet al (2005), and Su et al (2010).We can formally express the created model in thefollowing way: Let ni be an instance in a set of nar-ratives N , let j be a classification method, and letli be a label in a set of class labels L. We want toestablish a function f(ni, j) : li where li is the labelassigned to the narrative based on linguistic featuresfrom a set F , where F = f1, f2, ...fk, as describedin Table 2.
The baseline for each classifier is de-fined as the majority class ratio.
Using scripts inPraat (Boersma, 2001), Python, and NLTK (Bird etal., 2009), we automatically extracted features foreach narrative.
Each narrative was annotated withmultiple labels relating to its diagnostic correctness.The labeling schemes used in the initial and secondpilot studies, respectively, are described in subsec-22tions 4.2 and 4.3.4.2 Initial Pilot StudyThe initial pilot classification study allowed the op-portunity to refine the prediction target annotationscheme, as well as to explore a preliminary set of lin-guistic features.
160 narratives were assigned labelsLinguistic Feature at the narrative levelModalityAcoustic- Total durationprosodic Percent silenceTime silent# of silences *Time speaking# of utterances *Initial silence lengthF0 mean (avg.
pitch) ?F0 min (min.
pitch) ?F0 max (max.
pitch) ?dB mean (avg.
intensity) ?dB max (max.
intensity) ?Lexical- # of wordsstructural words per minute# of disfluencies ?# of certainty amplifiers * ?# of speculative words * ?# of stop words * ?# of content words * ?# of negations * ?# of nouns ?# of verbs ?# of adjectives ?# of adverbs ?Unigram of tokensBigram of tokensTrigram of tokensTable 2: Features used by their respective modalities.Features marked with a * were only included in the sec-ond pilot study.
Features marked with ?
were includedtwice; once as their raw value and again as a z-score nor-malized to its speaker?s data in the training set.
Featuresmarked with ?were also included twice; once as their rawcount and again as their value divided by the total numberof words in that narrative.
Disfluencies were consideredas words towards the total word count, silences were not.No feature selection was applied.of correct or incorrect for two steps of the diagnos-tic process: diagnostic category and final diagno-sis.
These annotations were done by a dermatologistwho did not participate in the elicitation study (co-author Cara Calvelli).
For final diagnosis, 70% weremarked as correct, and for diagnostic category, 80%were marked as correct.
An outcome of the anno-tation study was learning that the initial annotationscheme needed to be refined.
For example, diagnos-tic category had a fuzzy interpretation, and correct-ness and completeness of diagnoses are found alonga gradient in medicine.
This led us to pursue an im-proved annotation scheme with new class labels inthe second pilot study, as well as the adoption of agradient scale of correctness.For the initial pilot study, basic features were ex-tracted from the diagnostic narratives in two modal-ities: acoustic-prosodic and lexical-structural (seeTable 2).
To understand the fundamental aspectsof the problem, the initial pilot study experimentedwith the linguistic modalities separately and to-gether, using three foundational algorithms, as im-plemented in NLTK (Naive Bayes, Maximum En-tropy, Decision Tree), and a maximum vote classi-fier based on majority consensus of the three basicclassifiers.
The majority class baselines were 70%for diagnosis and 80% for diagnostic category.
Thesmall pilot data set was split into an 80% training setand a 20% testing set.
The following results wereobtained with the maximum vote classifier.Utilizing only acoustic-prosodic features, themaximum vote classifier performed 5% above thebaseline when testing final diagnosis and 6% belowit for diagnostic category.
F0 min and initial silencelength appeared as important features.
This initial si-lence length could signal that the observers are ableto glean more information from the image, and us-ing this information, they can make a more accuratediagnosis.Utilizing only lexical-structural features, themodel performed near the baseline (+1%) for finaldiagnosis and 9% better than the baseline for diag-nostic category.
When combining acoustic-prosodicand lexical-structural modalities, the majority voteclassifier performed above the baseline by 5% for fi-nal diagnosis and 9% for diagnostic category.
Weare cautious in our interpretation of these findings.For example, the small size of the data set and the23particulars of the data split may have guided the re-sults, and the concept of diagnostic category turnedout to be fuzzy and problematic.
Nevertheless, thestudy helped us refine our approach for the secondpilot study and redefine the annotation scheme.4.3 Second Pilot StudyFor the second pilot study, we hoped to gain furtherinsight into primarily two questions: (1) How accu-rately do the tested models perform on three steps ofthe diagnostic process, and what might influence theperformance?
(2) In our study scenario, is a certainlinguistic modality more important for the classifi-cation problem?The annotation scheme was revised according tofindings from the initial pilot study.
These revisionswere guided by dermatologist and co-author CaraCalvelli.
The initial pilot study scheme only anno-tated for diagnostic category and final diagnosis.
Werealized that diagnostic category was too slippery ofa concept, prone to misunderstanding, to be useful.Instead, we replaced it with two new and more ex-plicit parts of the diagnostic process: medical lesionmorphology and differential diagnosis.For final diagnosis, the class label options of cor-rect and incorrect could not characterize narrativesin which observers had not provided a final diag-nosis.
Therefore, a third class label of none wasadded.
New class labels were also created that cor-responded to the diagnostic steps of medical lesionmorphology and differential diagnosis.
Medical le-sion morphology, which is often descriptively com-plex, allowed the label options correct, incorrect,and none, as well as correct but incomplete to dealwith correct but under-described medical morpholo-gies.
Differential diagnosis considered whether ornot the final diagnosis appeared in the differentialand thus involved the labels yes, no, and no differ-ential given.
Table 3 summarizes the refined anno-tation scheme.The examples in Figure 2 above can now be ana-lyzed according to the new annotation scheme.
Ob-server A has a final diagnosis which should be la-beled as correct but does not give a differential diag-nosis, so the differential diagnosis label should be nodifferential given.
Observer A also misses parts ofthe morphological description so the assigned med-ical lesion morphology would likely be correct butincomplete.
Observer B provides what seems to bea full morphological description as well as lists thecorrect final diagnosis in the differential diagnosis,yet is incorrect regarding final diagnosis.
This narra-tive?s labels for medical lesion morphology and dif-ferential diagnosis would most likely be correct andyes respectively.
Further refinements may turn outuseful as the data set expands.Diagnostic step Possible labels Count RatioMedical Correct 537 .83Lesion Incorrect 36 .06Morphology None Given 40 .06Incomplete 32 .05Differential Yes 167 .24Diagnosis No 101 .14No Differential 434 .62Final Correct 428 .62Diagnosis Incorrect 229 .33None Given 35 .05Table 3: Labels for various steps of the diagnostic processas well as their count and ratios of the total narratives, af-ter eliminating those with no annotator agreement.
Theselabels are explained in section 4.3.Three dermatologists annotated the narratives, as-signing a label of correctness for each step in thediagnostic process for a given narrative.
Table 3shows the ratios of labels in the collected annota-tions.
Medical lesion morphology is largely correctwith only smaller ratios being assigned to other cat-egories.
Secondly, a large ratio of narratives wereassigned no differential given but of those that didprovide a differential diagnosis, the correct final di-agnosis was more likely to be included than not.
Re-garding final diagnosis, a label of correct was mostoften assigned and few narratives did not provideany final diagnosis.
These class imbalances, exist-ing at each level, indicated that the smaller classeswith fewer instances would be quite challenging fora computational classifier to learn.Any narrative for which there was not agreementfor at least 2 of the 3 dermatologists in a diagnosticstep was discarded from the set of narratives consid-ered in that diagnostic step.22Because narratives with disagreement were removed, thetotal numbers of narratives in the experiment sets differ slightlyon the various step of the diagnostic process.24Comparing classification in terms of algorithms,diagnostic steps, and individual classesWeka (Witten and Frank, 2005)3 was used withfour classification algorithms, which have a widelyaccepted use in computational linguistics.4Standard performance measures were used toevaluate the classifiers.
Both acoustic-prosodic andlexical-structural features were used in a leave-one-out cross-validation scenario, given the small size ofthe data set.
The results are shown in Table 4.
Ac-curacy is considered in relation to the majority classbaseline in each case.
With this in mind, the highaccuracies found when testing medical lesion mor-phology are caused by a large class imbalance.
Dif-ferential diagnosis?
best result is 5% more accuratethan its baseline while final diagnosis and medicallesion morphology are closer to their baselines.Final Dx Diff.
Dx M. L. M.Baseline .62 .62 .83C4.5 .57 .62 .77SVM .63 .67 .83Naive Bayes .55 .61 .51Log Regression .53 .64 .66Table 4: Accuracy ratios of four algorithms (implementedin Weka) as well as diagnostic steps?
majority class base-lines.
Experiments used algorithms?
default parametersfor final diagnosis (3 labels), differential diagnosis (3 la-bels), and medical lesion morphology (4 labels) usingleave-one-out cross-validation.In all scenarios, the SVM algorithm reached orexceeded the majority class baseline.
For this rea-son, other experiments used SVM.
The results forthe SVM algorithm when considering precision andrecall for each class label, at each diagnostic step,are shown in Table 5.
Precision is calculated as thenumber of true positives for a given class divided bythe number of narratives classified as the given class.Recall is calculated as the number of true positivesfor a given class divided by the number of narra-tives belonging to the given class.
As Table 5 shows,and as expected, labels representing large propor-tions were better identified than labels representing3See http://www.cs.waikato.ac.nz/ml/weka/.4In this initial experimentation, not all features used wereindependent, although this is not ideal for some algorithms.Dx step Labels Precision RecallMedical Correct .83 .99Lesion Incorrect 0 0Morphology None Given 0 0Incomplete 0 0Differential Yes .49 .44Diagnosis No .26 .10No Diff.
.76 .89Final Correct .67 .84Diagnosis Incorrect .32 .47None Given 0 0Table 5: Precision and recall of class labels.
These wereobtained using the Weka SVM algorithm with default pa-rameters using leave-one-out cross-validation.
These cor-respond to the experiment for SVM in Table 4.Final Diagnosis Diff.
DiagnosisBaseline .62 .62Lex.-struct.
.62 .67Acous.-pros.
.65 .62All .63 .67Table 6: Accuracy ratios for various modalities.
Testswere performed for final diagnosis and differential diag-nosis tags with Weka?s SVM algorithm using a leave-out-out cross-validation method.
Lexical-structural andacoustic-prosodic cases used only features in their respec-tive set.intermediate proportions, and classes with few in-stances did poorly.Experimentation with types of featureTo test if one linguistic modality was more impor-tant for classification, experiments were run in eachof three different ways: with only lexical-structuralfeatures, with only acoustic-prosodic features, andwith all features.
We considered the final diagnosisand differential diagnosis scenarios.
It was decidednot to run this experiment in terms of medical lesionmorphology because of its extreme class imbalancewith a high baseline of 83%.
Medical lesion mor-phology also differs in being a descriptive step un-like the other two which are more like conclusions.Again, a leave-one-out cross-validation method wasused.
The results are shown in Table 6.These results show that, regarding final diagnosis,considering only acoustic-prosodic features seemed25to yield somewhat higher accuracy than when fea-tures were combined.
This might reflect that, con-ceptually, final diagnosis captures a global end stepin the decision-making process, and we extractedvoice features at a global level (across the narrative).In the case of differential diagnosis, the lexical-structural features performed best, matching the ac-curacy of the combined feature set (5% over the ma-jority class baseline).
Future study could determinewhich individual features in these sets were most im-portant.Experiments with alternative label groupings forsome diagnostic stepsAnother set of experiments examined perfor-mance for adjusted label combinations.
To learnmore about the model, experiments were run inwhich selected classes were combined or only cer-tain classes were considered.
The class proportionsthus changed due to the combinations and/or re-moval of classes.
This was done utilizing all fea-tures, the Weka SVM algorithm, and a leave-one-out methodology.
Only logically relevant tests thatincreased class balance are reported here.5An experiment was run on the differential diagno-sis step.
The no differential given label was ignoredto allow the binary classification of narratives thatincluded differential diagnoses.
The new majorityclass baseline for this test was 62% and this classi-fication performed 1% over its baseline.
A similarexperiment was run on the final diagnosis diagnos-tic step.
Class labels of incorrect and none givenwere combined to form binary set of class labelswith a 62% baseline.
This classification performed6% over the baseline, i.e., slightly improved perfor-mance compared to the scenario with three class la-bels.5 ConclusionIn these pilot studies, initial insight has been gainedregarding the computational linguistic modeling ofextra-propositional meaning but we acknowledgethat these results need to be confirmed with newdata.This paper extracted features, which could pos-sibly relate to uncertainty, at the global level of a5Other experiments were run but are not reported becausethey have no use in future implementations.narrative to classify correctness of three diagnosticreasoning steps.
These steps are in essence localphenomena and a better understanding of how un-certainty is locally expressed in the diagnostic pro-cess is needed.
Also, this work does not considerparametrization of algorithms or the role of featureselection.
In future work, by considering only thefeatures that are most important, a better understand-ing of linguistic expression in relation to diagnosticcorrectness could be achieved, and likely result inbetter performing models.
One possible future adap-tation would be the utilization of the Unified Medi-cal Language System to improve the lexical featuresused Woods et al (2006).Other future work includes integrating eye move-ment data into prediction models.
The gaze modal-ity informs us as to where the observers were look-ing when they were verbalizing their diagnostic pro-cess.
We can thus map the narratives to how gazewas positioned on an image.
Behavioral indicatorsof doctors?
diagnostic reasoning likely extend be-yond language.
By integrating gaze and linguisticinformation, much could be learned regarding per-ceptual and conceptual knowledge.Through this study, we have moved towards un-derstanding reasoning in medical narratives, and wehave come one step closer to linking the spokenwords of doctors to their cognitive processes.
In amuch more refined, future form, certainty or cor-rectness detection could become useful to help un-derstanding medical reasoning or help guide medi-cal reasoning or detect misdiagnosis.AcknowledgementsThis research supported by NIH 1 R21 LM010039-01A1, NSF IIS-0941452, RIT GCCIS Seed Fund-ing, and RIT Research Computing (http://rc.rit.edu).We would like to thank Lowell A. Goldsmith, M.D.and the anonymous reviewers for their comments.ReferencesCecilia Ovesdotter Alm.
2011.
Subjective Natural Lan-guage Problems: Motivations, Applications, Charac-terizations, and Implications.
Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics, pages 107?112.26Hugh Beyer and Karen Holtzblatt.
1997.
Contextual De-sign: Defining Customer-Centered Systems.
MorganKaufmann.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia.Paul Boersma.
2001.
Praat, a system for doing phoneticsby computer.
Glot International, pages 341?345.Herbert Clark and Jean Fox Tree.
2002.
Using uh and umin spontaneous speaking.
Cognition, pages 73?111.Martin Corley and Oliver Stewart.
2008.
Hesitation dis-fluencies in spontaneous speech: The meaning of um.Language and Linguistics Compass, 5(2):589?602.Pat Croskerry.
2009.
A universal model of diagnosticreasoning.
Academic Medicine, pages 1022?1028.Mark Graber.
2005.
Diagnostic errors in medicine: Acase of neglect.
The Joint Commission Journal onQuality and Patient Safety, pages 106?113.Aron Henriksson and Sumithra Velupillai.
2010.
Levelsof certainty in knowledge-intensive corpora: An ini-tial annotation study.
Proceedings of the Workshop onNegation and Speculation in Natural Language Pro-cessing, pages 41?45.Rui Li, Preethi Vaidyanathan, Sai Mulpuru, Jeff Pelz,Pengcheng Shi, Cara Calvelli, and Anne Haake.
2010.Human-centric approaches to image understandingand retrieval.
Image Processing Workshop, WesternNew York, pages 62?65.Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-ditti.
2005.
Detecting certainness in spoken tutorialdialogues.
Proceedings of Interspeech, pages 1837?1840.Diane Litman, Mihail Rotaru, and Greg Nicholas.
2009.Classifying turn-level uncertainty using word-levelprosody.
Proceedings of Interspeech, pages 2003?2006.Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli,Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake.Forthcoming-2012.
Annotation schemes to encodedomain knowledge in medical narratives.
Proceedingsof the Sixth Linguistic Annotation Workshop.Sergey Pakhomov.
1999.
Modeling filled pauses in med-ical dictations.
Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguisticson Computational Linguistics, pages 619?624.Thierry Pelaccia, Jacques Tardif, Emmanuel Triby, andBernard Charlin.
2011.
An analysis of clinical rea-soning through a recent and comprehensive approach:the dual-process theory.
Medical Education Online,16:5890.Heather Pon-Barry and Stuart Shieber.
2009.
The im-portance of sub-utterance prosody in predicting levelof certainty.
Proceedings of NAACL HLT, pages 105?108.Qi Su, Chu-Ren Huang, and Helen Kai-yun Chen.
2010.Evidentiality for text trustworthiness detection.
Pro-ceedings of the 2010 Workshop on NLP and Linguis-tics: Finding the Common Ground ACL 2010, pages10?17.Gyorgy Szarvas, Veronika Vincze, Richard Farkas, andJanos Csirik.
2008.
The bioscope corpus: annotationfor negation, uncertainty and their scope in biomedicaltexts.
BioNLP 2008: Current Trends in BiomedicalNatural Language Processing, pages 38?45.Gyorgy Szarvas.
2008.
Hedge classification in biomed-ical texts with a weakly supervised selection of key-words.
Proceedings of 46th Annual Meeting of theAssociation of Computational Linguistics, pages 281?289.Sumithra Velupillai.
2010.
Towards a better understand-ing of uncertainties and speculations in Swedish clin-ical text - analysis of an initial annotation trial.
Pro-ceedings of the Workshop on Negation and Speculationin Natural Language Processing, pages 14?22.Ian H.Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann.James Woods, Charles Sneiderman, Karam Hameed,Michael Ackerman, and Charlie Hatton.
2006.
Usingumls metathesaurus concepts to describe medical im-ages: dermatology vocabulary.
Computers in Biologyand Medicine 36, pages 89?100.Simon Zwarts and Mark Johnson.
2011.
The impact oflanguage models and loss functions on repair disflu-ency detection.
Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics,pages 703?711.27
