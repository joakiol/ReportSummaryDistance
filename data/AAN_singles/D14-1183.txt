Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1758?1763,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLarge-scale Reordering Model for Statistical Machine Translationusing Dual Multinomial Logistic RegressionAbdullah Alrajehaband Mahesan NiranjanbaComputer Research Institute, King Abdulaziz City for Science and Technology (KACST)Riyadh, Saudi Arabia, asrajeh@kacst.edu.sabSchool of Electronics and Computer Science, University of SouthamptonSouthampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.ukAbstractPhrase reordering is a challenge for statis-tical machine translation systems.
Posingphrase movements as a prediction prob-lem using contextual features modeled bymaximum entropy-based classifier is su-perior to the commonly used lexicalizedreordering model.
However, Trainingthis discriminative model using large-scaleparallel corpus might be computationallyexpensive.
In this paper, we explore recentadvancements in solving large-scale clas-sification problems.
Using the dual prob-lem to multinomial logistic regression, wemanaged to shrink the training data whileiterating and produce significant saving incomputation and memory while preserv-ing the accuracy.1 IntroductionPhrase reordering is a common problem whentranslating between two grammatically differentlanguages.
Analogous to speech recognition sys-tems, statistical machine translation (SMT) sys-tems relied on language models to produce morefluent output.
While early work penalized phrasemovements without considering reorderings aris-ing from vastly differing grammatical structuresacross language pairs like Arabic-English (Koehn,2004a), many researchers considered lexicalizedreordering models that attempted to learn orienta-tion based on the training corpus (Tillmann, 2004;Kumar and Byrne, 2005; Koehn et al., 2005).Building on this, some researchers have bor-rowed powerful ideas from the machine learningliterature, to pose the phrase movement problemas a prediction problem using contextual input fea-tures whose importance is modeled as weights ofa linear classifier trained by entropic criteria.
Theapproach (so called maximum entropy classifieror simply MaxEnt) is a popular choice (Zens andNey, 2006; Xiong et al., 2006; Nguyen et al.,2009; Xiang et al., 2011).
Max-margin structureclassifiers were also proposed (Ni et al., 2011).Alternatively, Cherry (2013) proposed recently us-ing sparse features optimize the translation qualitywith the decoder instead of training a classifier in-dependently.While large-scale parallel corpus is advanta-geous for improving such reordering model, thisimprovement comes at a price of computationalcomplexity.
This issue is particularly pronouncedwhen discriminative models are considered suchas maximum entropy-based model due to the re-quired iterative learning.Advancements in solving large-scale classifica-tion problems have been shown to be effectivesuch as dual coordinate descent method for linearsupport vector machines (Hsieh et al., 2008).
Sim-ilarly, Yu et al.
(2011) proposed a two-level dualcoordinate descent method for maximum entropyclassifier.In this work we explore the dual problem tomultinomial logistic regression for building large-scale reordering model (section 3).
One of themain advantages of solving the dual problem isproviding a mechanism to shrink the training datawhich is a serious issue in building such large-scale system.
We present empirical results com-paring between the primal and the dual problems(section 4).
Our approach is shown to be fast andmemory-efficient.2 Baseline SystemIn statistical machine translation, the most likelytranslation ebestof an input sentence f can befound by maximizing the probability p(e|f), asfollows:ebest= arg maxep(e|f).
(1)1758A log-linear combination of different models(features) is used for direct modeling of the poste-rior probability p(e|f) (Papineni et al., 1998; Ochand Ney, 2002):ebest= arg maxen?i=1?ihi(f , e) (2)where the feature hi(f , e) is a score functionover sentence pairs.
The translation model and thelanguage model are the main features in any sys-tem although additional features h(.)
can be inte-grated easily (such as word penalty).
State-of-the-art systems usually have around ten features.The language model, which ensures fluenttranslation, plays an important role in reordering;however, it has a bias towards short translations(Koehn, 2010).
Therefore, a need for developing aspecific model for the reordering problem.2.1 Lexicalized Reordering ModelAdding a lexicalized reordering model consis-tently improved the translation quality for sev-eral language pairs (Koehn et al., 2005).
Re-ordering modeling involves formulating phrasemovements as a classification problem where eachphrase position considered as a class (Tillmann,2004).
Some researchers classified phrase move-ments into three categories (monotone, swap, anddiscontinuous) but the classes can be extended toany arbitrary number (Koehn and Monz, 2005).
Ingeneral, the distribution of phrase orientation is:p(ok|?fi, e?i) =1Zh(?fi, e?i, ok) .
(3)This lexicalized reordering model is estimatedby relative frequency where each phrase pair(?fi, e?i) with such an orientation (ok) is countedand then normalized to yield the probability as fol-lows:p(ok|?fi, e?i) =count(?fi, e?i, ok)?ocount(?fi, e?i, o).
(4)The orientation of a current phrase pair is de-fined with respect to the previous target phrase.Galley and Manning (2008) extended the model totackle long-distance reorderings.
Their hierarchi-cal model enables phrase movements that are morecomplex than swaps between adjacent phrases.3 Multinomial Logistic RegressionMultinomial logistic regression (MLR), alsoknown as maximum entropy classifier (Zens andNey, 2006), is a probabilistic model for the multi-class problem.
The class probability is given by:p(ok|?fi, e?i) =exp(w>k?
(?fi, e?i))?k?exp(w>k??
(?fi, e?i)), (5)where ?
(?fi, e?i) is the feature vector of the i-thphrase pair.
An equivalent notation to w>k?
(?fi, e?i)is w>f(?
(?fi, e?i), ok) where w is a long vectorcomposed of all classes parameters (i.e.
w>=[w>1.
.
.w>K] ) and f(., .)
is a joint feature vec-tor decomposed via the orthogonal feature rep-resentation (Rousu et al., 2006).
This repre-sentation simply means there is no crosstalk be-tween two different feature vectors.
For example,f(?
(?fi, e?i), o1)>= [?
(?fi, e?i)>0 .
.
.
0].The model?s parameters can be estimated byminimizing the following regularized negativelog-likelihood P(w) as follows (Bishop, 2006):minw12?2K?k=1?wk?2?N?i=1K?k=1p?iklog p(ok|?fi, e?i)(6)Here ?
is a penalty parameter and p?
is the em-pirical distribution where p?ikequals zero for allok6= oi.Solving the primal optimization problem (6) us-ing the gradient:?P(w)?wk=wk?2?N?i=1(p?ik?
p(ok|?fi, e?i))?
(?fi, e?i),(7)do not constitute a closed-form solution.
In ourexperiments, we used stochastic gradient decentmethod (i.e.
online learning) to estimate w whichis shown to be fast and effictive for large-scaleproblems (Bottou, 2010).
The method approxi-mates (7) by a gradient at a single randomly pickedphrase pair.
The update rule is:w?k= wk?
?i?kPi(w), (8)where ?iis a positive learning rate.17593.1 The Dual ProblemLebanon and Lafferty (2002) derived an equiva-lent dual problem to (6).
Introducing Lagrangemultipliers ?, the dual becomesminw12?2K?k=1?wk(?
)?2+N?i=1K?k=1?iklog?ik,s.t.K?k=1?ik= 1 and ?ik?
0 ,?i, k, (9)wherewk(?)
= ?2N?i=1(p?ik?
?ik)?
(?fi, e?i) (10)As mentioned in the introduction, Yu et al.
(2011) proposed a two-level dual coordinate de-scent method to minimize D(?)
in (9) but it hassome numerical difficulties.
Collins et al.
(2008)proposed simple exponentiated gradient (EG) al-gorithm for Conditional Random Feild (CRF).
Thealgorithm is applicable to our problem, a specialcase of CRF.
The rule update is:??ik=?ikexp(??i?ikD(?))?k??ik?exp(??i?ik?D(?))(11)where?ikD(?)
??D(?)?
?ik= 1 + log?ik+(wy(?)>?
(?fi, e?i)?wk(?)>?
(?fi, e?i)).
(12)Here y represents the true class (i.e.
oy= oi).To improve the convergence, ?iis adaptively ad-justed for each example.
If the objective function(9) did not decrease, ?iis halved for number of tri-als (Collins et al., 2008).
Calculating the functiondifference below is the main cost in EG algorithm,D(??)?D(?)
=K?k=1(??iklog??ik?
?iklog?ik)?K?k=1(??ik?
?ik)wk(?)>?
(?fi, e?i)+?22??
(?fi, e?i)?2K?k=1(??ik?
?ik)2.
(13)Clearly, the cost is affordable because wk(?)
ismaintained throughout the algorithm as follows:wk(??)
= wk(?)??2(??ik??ik)?
(?fi, e?i) (14)Following Yu et al.
(2011), we initialize ?ikasfollows:?ik={(1?
) if ok= oi;K?1else.
(15)where  is a small positive value.
This is becausethe objective function (9) is not well defined at?ik= 0 due to the logarithm appearance.Finally, the optimal dual variables are achievedwhen the following condition is satisfied for all ex-amples (Yu et al., 2011):maxk?ikD(?)
= mink?ikD(?)
(16)This condition is the key to accelerate EG al-gorithm.
Unlike the primal problem (6), the dualvariables ?ikare associated with each example(i.e.
phrase pair) therefore a training example canbe disregarded once its optimal dual variables ob-tained.
More data shrinking can be achieved bytolerating a small difference between the two val-ues in (16).
Algorithm 1 presents the overall pro-cedure (shrinking step is from line 6 to 9).Algorithm 1 Shrinking stochastic exponentiatedgradient method for training the dual problemRequire: training set S = {?
(?fi, e?i), oi}Ni=11: Given ?
and the corresponding w(?
)2: repeat3: Randomly pick i from S4: Claculate?ikD(?)
?k by (12)5: vi= maxk?ikD(?)?mink?ikD(?
)6: if vi?
 then7: Remove i from S8: Continue from line 39: end if10: ?
= 0.511: for t = 1 to maxTrial do12: Calculate ?
?ik?k by (11)13: if D(??)?D(?)
?
0 then14: Update ?
and w(?)
by (14)15: Break16: end if17: ?
= 0.5 ?18: end for19: until vi?
 ?i17604 ExperimentsWe used MultiUN which is a large-scale parallelcorpus extracted from the United Nations website(Eisele and Chen, 2010).
We have used Arabicand English portion of MultiUN where the Englishside is about 300 million words.We simplify the problem by classifying phrasemovements into three categories (monotone,swap, discontinuous).
To train the reorderingmodels, we used GIZA++ to produce word align-ments (Och and Ney, 2000).
Then, we used theextract tool that comes with the Moses toolkit(Koehn et al., 2007) in order to extract phrase pairsalong with their orientation classes.As shown in Table 1, each extracted phrase pairis represented by linguistic features as follows:?
Aligned source and target words in a phrasepair.
Each word alignment is a feature.?
Words within a window around the sourcephrase to capture the context.
We choose ad-jacent words of the phrase boundary.The extracted phrase pairs after filtering are47,227,789.
The features that occur more than 10times are 670,154.Sentence pair:f : f1f21f3f4f52f63.e : e11e2e33e4e52.Extracted phrase pairs (?f , e?)
:?fi||| e?i||| oi||| alignment ||| contextf1f2||| e1||| mono ||| 0-0 1-0 ||| f3f3f4f5||| e4e5||| swap ||| 0-1 2-0 ||| f2f6f6||| e2e3||| other ||| 0-0 0-1 ||| f5All linguistic features:1. f1&e12.
f2&e13.
f34.
f3&e55.
f5&e46.
f27.
f68.
f6&e29.
f6&e310.
f5Bag-of-words representation:a phrase pair is represented as a vector where each featureis a discrete number (0=not exist).?
(?fi, e?i) 1 2 3 4 5 6 7 8 9 10?
(?f1, e?1) = 1 1 1 0 0 0 0 0 0 0?
(?f2, e?2) = 0 0 0 1 1 1 1 0 0 0?
(?f3, e?3) = 0 0 0 0 0 0 1 1 1 1Table 1: A generic example of the process ofphrase pair extraction and representation.4.1 ClassificationWe trained our reordering models by both primaland dual classifiers for 100 iterations.
For the dualMLR, different shrinking levels have been tried byvarying the parameter () in Algorithm 1.
Table 2reports the training time and classification errorrate of these models.Training the dual MLR with moderate shrinkinglevel (i.e.
 = 0.1) is almost four times faster thantraining the primal one.
Choosing larger value for() leads to faster training but might harm the per-formance as shown below.Classifier Training Time Error RatePrimal MLR 1 hour 9 mins 17.81%Dual MLR :0.1 18 minutes 17.95%Dual MLR :1.0 13 minutes 21.13%Dual MLR :0.01 22 minutes 17.89%Table 2: Performance of the primal and dual MLRbased on held-out data.Figure 1 shows the percentage of active set dur-ing training dual MLR with various shrinking lev-els.
Interestingly, the dual MLR could disregardmore than 99% of the data after a couple of iter-ations.
For very large corpus, the data might notfit in memory and training primal MLR will takelong time due to severe disk-swapping.
In this sit-uation, using dual MLR is very beneficial.2 4 6 8 10 12 14 16 18 200102030405060708090100Training iterationPercentage of activephrase pairs?
= 0.1?
= 1.0?
= 0.01Figure 1: Percentage of active set in dual MLR.As the data size decreases, each iteration takes farless computation time (see Table 2 for total time).17614.2 TranslationWe used the Moses toolkit (Koehn et al., 2007)with its default settings to build three phrase-basedtranslation systems.
They differ in how their re-ordering models were estimated.
The languagemodel is a 5-gram with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995).
We tunedthe system by using MERT technique (Och, 2003).As commonly used in statistical machine trans-lation, we evaluated the translation performanceby BLEU score (Papineni et al., 2002).
The testsets are NIST MT06 and MT08 where the En-glish sides are 35,481 words (1056 sentences) and116,840 words (3252 sentences), respectively.
Ta-ble 3 shows the BLEU scores for the translationsystems.
We also computed statistical significancefor the models using the paired bootstrap resam-pling method (Koehn, 2004b).Translation System MT06 MT08Baseline + Lexical.
model 30.86 34.22Baseline + Primal MLR 31.37* 34.85*Baseline + Dual MLR :0.1 31.36* 34.87*Table 3: BLEU scores for Arabic-English transla-tion systems with different reordering models (*:better than the lexicalized model with at least 95%statistical significance).5 ConclusionIn training such system with large data sizes andbig dimensionality, computational complexity be-come a serious issue.
In SMT, maximum entropy-based reordering model is often introduced as abetter alternative to the commonly used lexical-ized one.
However, training this discriminativemodel using large-scale corpus might be compu-tationally expensive due to the iterative learning.In this paper, we propose training the modelusing the dual MLR with shrinking method.
Itis almost four times faster than the primal MLR(also know as MaxEnt) and much more memory-efficient.
For very large corpus, the data might notfit in memory and training primal MLR will takelong time due to severe disk-swapping.
In this sit-uation, using dual MLR is very beneficial.
Theproposed method is also useful for many classi-fication problems in natural language processingthat require large-scale data.ReferencesChristopher M. Bishop.
2006.
Pattern Recognitionand Machine Learning (Information Science andStatistics).
Springer-Verlag New York, Inc., Secau-cus, NJ, USA.L?eon Bottou.
2010.
Large-scale machine learningwith stochastic gradient descent.
In Yves Lecheval-lier and Gilbert Saporta, editors, Proceedings ofthe 19th International Conference on Computa-tional Statistics (COMPSTAT?2010), pages 177?187, Paris, France, August.
Springer.Colin Cherry.
2013.
Improved reordering for phrase-based translation using sparse features.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 22?31, Atlanta, Georgia, June.
Association for Compu-tational Linguistics.Michael Collins, Amir Globerson, Terry Koo, XavierCarreras, and Peter L. Bartlett.
2008.
Exponen-tiated gradient algorithms for conditional randomfields and max-margin markov networks.
Journalof Machine Learning Research, 9:1775?1822, June.Andreas Eisele and Yu Chen.
2010.
Multiun:A multilingual corpus from united nation docu-ments.
In Daniel Tapias, Mike Rosner, Ste-lios Piperidis, Jan Odjik, Joseph Mariani, BenteMaegaard, Khalid Choukri, and Nicoletta Calzo-lari (Conference Chair), editors, Proceedings of theSeventh conference on International Language Re-sources and Evaluation, pages 2868?2872.
Euro-pean Language Resources Association (ELRA), 5.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 848?856, Hawaii, October.
Associationfor Computational Linguistics.Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,S.
Sathiya Keerthi, and S. Sundararajan.
2008.
Adual coordinate descent method for large-scale lin-ear svm.
In Proceedings of the 25th InternationalConference on Machine Learning, ICML ?08, pages408?415.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
IEEEInternational Conference on Acoustics, Speech andSignal Processing, pages 181?184.Philipp Koehn and Christof Monz.
2005.
Sharedtask: Statistical machine translation between euro-pean languages.
In Proceedings of ACL Workshopon Building and Using Parallel Texts, pages 119?124.
Association for Computational Linguistics.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system description1762for the 2005 IWSLT speech translation evaluation.In Proceedings of International Workshop on Spo-ken Language Translation, Pittsburgh, PA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Christopher J. Dyer, Ond?rej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of the ACL 2007 Demoand Poster Sessions, pages 177?180.Philipp Koehn.
2004a.
Pharaoh: a beam search de-coder for phrase-based statistical machine transla-tion models.
In Proceedings of 6th Conference of theAssociation for Machine Translation in the Ameri-cas (AMTA), pages 115?124, Washington DC.Philipp Koehn.
2004b.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Shankar Kumar and William Byrne.
2005.
Lo-cal phrase reordering models for statistical machinetranslation.
In Proceedings of Human LanguageTechnology Conference and Conference on Empiri-cal Methods in Natural Language Processing, pages161?168, Vancouver, British Columbia, Canada,October.
Association for Computational Linguistics.Guy Lebanon and John D. Lafferty.
2002.
Boostingand maximum likelihood for exponential models.
InT.G.
Dietterich, S. Becker, and Z. Ghahramani, ed-itors, Advances in Neural Information ProcessingSystems 14, pages 447?454.
MIT Press.Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,and Thai Phuong Nguyen.
2009.
Improving a lex-icalized hierarchical reordering model using maxi-mum entropy.
In Proceedings of the Twelfth Ma-chine Translation Summit (MT Summit XII).
Inter-national Association for Machine Translation.Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-hesan Niranjan.
2011.
Exploitation of machinelearning techniques in modelling phrase movementsfor machine translation.
Journal of Machine Learn-ing Research, 12:1?30, February.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting of the Association of Compu-tational Linguistics (ACL).Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL).Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kishore Papineni, Salim Roukos, and Todd Ward.1998.
Maximum likelihood and discriminativetraining of direct translation models.
In Proceedingsof ICASSP, pages 189?192.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318, Stroudsburg, PA,USA.
Association for Computational Linguistics.Juho Rousu, Craig Saunders, Sandor Szedmak, andJohn Shawe-Taylor.
2006.
Kernel-based learning ofhierarchical multilabel classification models.
Jour-nal of Machine Learning Research, pages 1601?1626.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of HLT-NAACL: Short Papers, pages 101?104.Bing Xiang, Niyu Ge, and Abraham Ittycheriah.
2011.Improving reordering for statistical machine transla-tion with smoothed priors and syntactic features.
InProceedings of SSST-5, Fifth Workshop on Syntax,Semantics and Structure in Statistical Translation,pages 61?69, Portland, Oregon, USA.
Associationfor Computational Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model forstatistical machine translation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the ACL,pages 521?528, Sydney, July.
Association for Com-putational Linguistics.Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.2011.
Dual coordinate descent methods for logisticregression and maximum entropy models.
MachineLearning, 85(1-2):41?75, October.Richard Zens and Hermann Ney.
2006.
Discrimina-tive reordering models for statistical machine trans-lation.
In Proceedings on the Workshop on Statis-tical Machine Translation, pages 55?63, New YorkCity, June.
Association for Computational Linguis-tics.1763
