Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,pages 25?28, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Towards Intelligent Search Assistance for Inquiry-Based LearningWeijian XuanMHRIUniversity of MichiganAnn Arbor, MI 48109wxuan@umich.eduMeilan ZhangSchool of EducationUniversity of MichiganAnn Arbor, MI 48109meilanz@umich.eduAbstractIn Online Inquiry-Based Learning (OIBL)learners search for information to answerdriving questions.
While learners conductsequential related searches, the search en-gines interpret each query in isolation,and thus are unable to utilize task context.Consequently, learners usually get lessrelevant search results.
We are developinga NLP-based search agent to bridge thegap between learners and search engines.Our algorithms utilize contextual featuresto provide user with search term sugges-tions and results re-ranking.
Our pilotstudy indicates that our method can effec-tively enhance the quality of OIBL.1 IntroductionMajor science education standards call on studentsto engage in Online Inquiry-Based Learning wherethey pose scientific Driving Questions (DQ), plantheir search, collect and analyze online informa-tion, and synthesize their findings into an argu-ment.
In collaboration with National ScienceDigital Library (NSDL), we are developing an in-tegrated Online Inquiry-Based Learning Environ-ment (OIBLE), called IdeaKeeper (Quintana andZhang, 2004), to help learners fulfill the promise ofOIBL.
IdeaKeeper is among the first reportedOIBLE that integrates various online search en-gines with support for inquiry planning, informa-tion search, analysis and synthesis.Our observation reveals that searching is one ofthe bottlenecks impeding students?
learning ex-perience.
Students demonstrate various problemsin search.
First, they repeatedly search for verysimilar keywords on search engines.
Second, theyare usually unable to develop effective searchterms.
Many search keywords students generateare either too broad or too narrow.
Although learn-ers have specific search purposes, many times theyare unable to express the purposes in keyword-based queries.
In fact, by analyzing the search logs,we found that the average query length is onlyabout 2 words.
In such typical cases in OIBL, in-formative contexts are not presented in queries,and thus the requests become ambiguous.
As a re-sult, the search engines may not interpret the queryas the learners intended to.
Therefore, the resultsare usually not satisfactory.
Given the self-regulated nature of OIBL and limited self-controlskills of K-12 students, the problem is even moreserious, as students may shift their focus off thetask if they constantly fail to find relevant informa-tion for their DQ.2 Related WorkIn Information Retrieval field, many algorithmsbased on relevance feedback are proposed(Buckley, et al, 1994; Salton and Buckley, 1990).However, current general web search engines arestill unable to interactively improve research re-sults.
In NLP domain, there are considerable ef-forts on Question Answering systems that attemptto answer a question by returning concise facts.While some QA systems are promising(Harabagiu, et al, 2000; Ravichandran and Hovy,2002), they can only handle factual questions as inTREC (Voorhees, 2001), and the context for thewhole task is largely not considered.
There areproposals on using context in search.
Huang et al(2001) proposed a term suggestion method for in-teractive web search.
More existing systems thatutilize contextual information in search are re-viewed by Lawrence (2000).
However, one prob-lem is that ?context?
is defined differently in each25study.
Few attempts target at inquiry-based learn-ing, which has some unique features, e.g., DQ/SQ.We are developing an OnLine Inquiry SearchAssistance (OLISA).
OLISA applies Natural Lan-guage Processing (NLP) and Information Retrieval(IR) techniques to provide students query termsuggestions and re-rank results returned fromsearch engines by the relevance to the currentquery as well as to the DQ.
OLISA is not a built-incomponent of IdeaKeeper, but can be very easilyplugged into IdeaKeeper or other OIBL systems asa value-added search agent.
The main advantage ofOLISA is that it utilizes the context of the wholelearning task.
Our pilot study demonstrated that itis a simple and effective initiative toward auto-matically improving the quality of web search inOIBLE.3 Method3.1 Utilizing Learning ContextOLISA acquires search context by parsing OIBLlogs and by monitoring search history.
For exam-ple, in the planning phase of a learning task, Ide-aKeeper asks students to input DQ, Sub-Questions(SQs), potential keywords, and to answer somequestions such as ?what do I know?, ?what do Iwant to know?, etc.The context information is represented as bag-of-words feature vectors.
To calculate the vectors,we first remove common terms.
We compiled acorpus of 30 million words from 6700 full-lengthdocuments collected from diverse resources.
Wordfrequencies are calculated for 168K unique wordsin the corpus.
A word is considered common if it isin the 1000 most frequent word list.
Remainingwords are stemmed using Porter?s algorithm(Porter, 1980).All contextual information are combined toform a main feature vector ( ),where  is the weight of the ith term in com-bined context.
It?s defined by product of term fre-quency (tf) and inverse document frequency (idf).
)()(2)(1 ,,,cncc WWW L)(ciWComparing with traditional tf  measure, we donot assign a uniform weight to all words in context.Rather, we consider DQ/SQ and the current querymore important than the rest of context.
We definetheir differently from other context.
tf)()( *))/#ln(#1( dqidqi tfwordInDQextwordInConttf +=  (1)The  is calculated similarly.
For the termfrequency of current query , we assign it a lar-ger weight as it represents the current informationneeds:)(sqitf)(qitf)()( *)/#(# qiqi tfywordInQuerextwordInConttf =  (2)Therefore,)()()()()( otherisqidqiqici tftftftftf +++=            (3)The inverse document frequency is defined by:)/ln()( ici nNidf =                        (4)where N is total number of documents in the cor-pus, and ni is the number of documents containingith term.
The term weight is defined by:??
?+?+=2)()(2)()()()1(ln)1ln(ciciciciciidftfidftfW         (5)These context feature vectors are calculated forlater use in re-ranking search results.Meanwhile, we use Brill's tagger (Brill, 1995)to determine parts of speech (POS) of words inDQ/SQ.
Heuristic rules (Zhang and Xuan, 2005)based on POS are used to extract noun phrases.Noun phrases containing words with high termweight are considered as keyphrases.
The key-phrase weight is defined by:)( )()()( icjjcjcP PPhraseWwhereWW i ?=?
(6)3.2 Term SuggestionWhen a user commits a query, OLISA will firstsearch it on selected search engines (Google asdefault).
If the total hit exceeds certain threshold (2million as default), we consider the query poten-tially too general.
In addition to the original query,we will call term suggestion component to narrowdown the search concept by expanding the query.WordNet (Fellbaum, 1998) is used during the ex-pansion.
Below is the outline of our heuristic algo-rithm in generating term suggestion.for each keyword in original query doif the keyword is part of a keyphrase thenform queries by merging each phrase with the original queryif multiple keyphrases are involved thenselect up to #maxPhrase keyphrases with highest weightsif #queries>0 then return queriesfor each keyword that has hyponyms in WordNet doif some hyponym occur at least once in learning context thenform queries by merging the hyponym with the original queryelse form suggestions by merging the hyponym with the original queryif #queries>0 or #suggestions> 0 then return queries and suggestionsfor each keyword in original query that has synonyms in WordNet doif some synonym is part of a keyphrase thenform suggestions by merging keywords in phrase with original queryif multiple keyphrases are involved thenselect up to #maxPhrase keyphrases with highest weightsreturn suggestions26On the other hand, if the total hit is below cer-tain threshold, the query is potentially too specific.Thus term suggestion component is called to gen-eralize the query.
The procedure is similar to thealgorithm above, but will be done in the reversedirection.
For example, keywords will replacephrases and hypernyms will replace hyponyms.Since there are cases where learners desire specificsearch terms, both original and expanded querieswill be submitted, and results for the former will bepresented at the top of the returned list.If no new queries are constructed, OLISA willreturn the results from original query along withsuggestions.
Otherwise, OLISA will send requestsfor each expanded query to selected search en-gines.
Since by default we return up to RT=100search engine results to user, we will extract thetop RQ=RT/(#newQuery+1) entries from results ofeach new query and original query.
These resultswill be re-ranked by an algorithm that we will de-scribe later.
Then the combined results will be pre-sented to the user in IdeaKeeper along with a list ofexpanded queries and suggestions.3.3 Query ReformulationFrom our observation, in OIBLE students oftensubmit questions in natural language.
However,most of the time, such type of queries does not re-turn desirable results.
Therefore, we loosely followKwok (2001) to reformulate queries.
We applyLink Grammar Parser (Sleator and Temperley,1993) to parse sentence structure.
For example,one student asked ?What is fat good for?.
Theparser generates the following linkage:+----------------Xp----------------+|       +----------Bsw---------+   ||       |    +----Paf----+     |   |+---Wq--+    +-SIs+      +-MVp-+   ||       |    |    |      |     |   |LEFT-WALL what is.v fat.n good.a for.p ?where ?SI?
is used in subject-verb inversion.
Bygetting this linkage, we are able to reformulate thequery as ?fat is good for?.
Meanwhile, regular ex-pressions are developed to eliminate interrogativewords, e.g.
?what?
and ?where?.Search engines may return very different resultsfor the original query and the reformulated queries.For example, for the example above, Google re-turned 620 hits for the reformulated query, butonly 2 hits for the quoted original question.By sending request in both original and reformu-lated forms, we can significantly improve recallratio without losing much precision.3.4 Integrating Multiple Search EnginesWe enhanced the searching component of Ide-aKeeper by integrating multiple search engines(e.g.
Google, AskJeeves, NSDL, etc.).
IdeaKeeperwill parse and transform search results and presentusers with a uniform format of results from differ-ent search engines.
A spelling check function forsearch keywords is built in OLISA, which com-bined spelling check results from Google as well assuggestions from our own program based on a lo-cal frequency-based dictionary.3.5 Search Results Re-RankingAfter query reformulation OLISA will send re-quests to selected search engines.
For performanceissue, we only retrieve a total of 100 snippets (RQsnippets from each query) from web search en-gines.
Feature vector is calculated for each snippetin the measure similar to (5), except that tf is ac-tual frequency without assigning additional weight.The similarity between learning context C andeach document D (i.e.
snippet) is calculated as:??
?=nidinicinidiciWWWWDCSimilarity2)(2)()()(),(           (7)The higher the similarity score, the more rele-vant it will be to user?s query as well as to theoverall learning context.OLISA re-ranks snippets by similarity scores.To avoid confusion to learners, the snippets fromthe original query and the expanded queries are re-ranked independently.
RQ re-ranked results fromoriginal query appear at the top as default, fol-lowed by other re-ranked results with signs indicat-ing corresponding queries.
The expanded queriesand further search term suggestions are shown in adropdown list in IdeaKeeper.4 Preliminary Results and DiscussionOLISA is under development.
While thoroughevaluation is needed, our preliminary results dem-onstrate its effectiveness.
We conducted field stud-ies with middle school students for OIBL projectsusing IdeaKeeper.
Fig.1 shows a case of usingOLISA search function in IdeaKeeper.
By video27taping some students?
search session, we foundthat enhanced search functions of OLISA signifi-cantly saved students?
effort and improve their ex-perience on search.
The term suggestions werefrequently used in these sessions.Fig.
1 Using OLISA function in IdeaKeeperOur initials results also demonstrate that calcu-lation on the snippets returned by search engines issimple and efficient.
Therefore, we don?t need toretrieve each full document behind.
We want topoint out that in our feature vector calculation eachpast query is combined into previous context.
Sothe learning context is interactively changing.Previous research has found that in OIBL pro-jects, students often spend considerable timesearching for sites due to their limited search skills.Consequently, students have little time on higher-order cognitive and metacognitive activities, suchas evaluation, sense making, synthesis, and reflec-tion.
By supporting students' search, OLISA helpsstudent focus more on higher-order activities,which provide rich opportunities for deep learningto occur.Our future work includes fine-tuning the pa-rameters in our algorithms and conducting moreevaluation of each component of OLISA.
We arealso considering taking into account the snippets ordocuments users selected, because they also repre-sent user feedback.
How to determine the relativeweight of words in selected documents, and how todisambiguate polysemies using WordNet or otherresources are topics of future research.ReferencesBrill, E. (1995).
Transformation-Based Error-DrivenLearning and Natural Language Processing: A CaseStudy in Part of Speech Tagging, ComputationalLinguistics, 21, 543-565.Buckley, C., Salton, G. and Allan, J.
(1994).
The Effectof Adding Relevance Information in a RelevanceFeedback Environment.
Proceedings of the 17th an-nual international ACM SIGIR conference on Re-search and development in information retrieval.Dublin, Ireland, 292-300.Fellbaum, C., Ed.
(1998) WordNet: An Electronic Lexi-cal Database.
MIT Press, Cambridge, MA.Harabagiu, S.M., Pasca, M.A.
and Maiorano, S.J.
(2000) Experiments with Open-Domain TextualQuestion Answering.
Proceedings of the 17th confer-ence on Computational linguistics.
Saarbrucken,Germany, 292-298.Huang, C.-K., Oyang, Y.-J.
and Chien, L.-F. (2001) AContextual Term Suggestion Mechanism for Interac-tive Web Search.
Web Intelligence, 272-281Kwok, C., Etzioni, O. and Weld, D. (2001) ScalingQuestion answering to the Web.
ACM Transactionson Information Systems, 19, 242-262.Lawrence, S. (2000) Context in Web Search, IEEE DataEngineering Bulletin, 23, 25?32.Porter, M.F.
(1980) An algorithm for suffix stripping.Program, 14, 130-137.Quintana, C. and Zhang, M. (2004) The Digital Ide-aKeeper: Integrating Digital Libraries with a Scaf-folded Environment for Online Inquiry.
JCDL'04.Tuscon, AZ, 388-388.Ravichandran, D. and Hovy, E. (2002) Learning Sur-face Text Patterns for a Question Answering System.Proceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics, Philadelphia,PA, 41-47.Salton, G. and Buckley, C. (1990) Improving retrievalperformance by relevance feedback.
Journal of theAmerican Society for Information Science, 41, 288-297.Sleator, D. and Temperley, D. (1993) Parsing Englishwith a Link Grammar.
Proceedings of the Third In-ternational Workshop on Parsing Technologies.Voorhees, E. (2001) Overview of the TREC 2001 Ques-tion Answering Track.
Proceedings of the 10th TextRetrieval Conference (TREC10).
Gaithersburg, MD,157-165.Zhang, M. and Xuan, W. (2005) Towards DiscoveringLinguistic Features from Scientific Abstracts.
Pro-ceedings of the 26th ICAME and the 6th AAACLconference.
Ann Arbor, MI.28
