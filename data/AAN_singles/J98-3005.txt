Generating Natural Language Summariesfrom Multiple On-Line SourcesDragomir R. Radev*Columbia UniversityKathleen R. McKeown*Columbia UniversityWe present a methodology for summarization of news about current events in the form of brief-ings that include appropriate background (historical) information.
The system that we developed,SUMMONS, uses the output of systems developed for the DARPA Message Understanding Con-ferences to generate summaries of multiple documents on the same or related events, presentingsimilarities and differences, contradictions, and generalizations among sources of information.We describe the various components ofthe system, showing how information from multiple arti-cles is combined, organized into a paragraph, and finally, realized as English sentences.
A featureof our work is the extraction of descriptions of entities such as people and places for reuse toenhance abriefing.1.
IntroductionOne of the major problems with the Internet is the abundance of information and theresulting difficulty for a typical computer user to read all existing documents on aspecific topic.
Even within the domain of current news, the user's task is infeasible.There exist now more than 100 sources of live newswire on the Internet, mostly ac-cessible through the World-Wide Web (Berners-Lee 1992).
Some of the most popularsites include news agencies and television stations like Reuters News (Reuters 1996),CNN's Web (CNN 1996), and ClariNet's e.News on-line newspaper (ClariNet 1996),as well as on-line versions of print media such as the New York Times on the Webedition (NYT 1996).For the typical user, it is nearly impossible to go through megabytes ofnews everyday to select articles he wishes to read.
Even when the user can actually select all newsrelevant to the topic of interest, he will still be faced with the problem of selecting asmall subset hat he can actually read in a limited time from the immense corpus ofnews available.
Hence, there is a need for search and selection services, as well as forsummarization facilities.There currently exist more than 40 search and selection services on the World-Wide Web, such as DEC's Altavista (Altavista 1996), Lycos (Lycos 1996), and DejaNews(DejaNews 1997), all of which allow keyword searches for recent news.
However, onlyrecently have there been practical results in the area of summarization.Summaries can be used to determine if any of the retrieved articles are relevant(thereby allowing the user to avoid reading those that are not) or can be read in placeof the articles to learn about information of interest to the user.
Existing summarizationsystems (e.g., Preston and Williams 1994; Cuts 1994; NetSumm 1996; Kupiec, Pedersen,and Chen 1995; Rau, Brandow, and Mitze, 1994) typically use statistical techniques to* Department of Computer Science, 450 Computer Science Building, Columbia University, New York, NY10027.
E-maih {radev, kathy}@cs.columbia.edu(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 3extract relevant sentences from a news article.
This domain-independent approachproduces a summary of a single article at a time, which can indicate to the userwhat the article is about.
In contrast, our work focuses on generation of a summarythat briefs the user on information in which he has indicated interest.
Such briefingspull together information of interest from multiple sources, aggregating informationto provide generalizations, similarities, and differences across articles, and changesin perspective across time.
Briefings do not necessarily fully summarize the articlesretrieved, but they update the user on information he has specified is of interest.Some characteristics that distinguish a briefing from the general concept of a sum-mary are:Briefings are used to keep a person up to date on a certain event.
Thus,they need to convey information about he event using appropriatehistorical references and the context of prior news.Briefings focus on certain types of information that are present in thesource text in which the reader has expressed interest.
They deliberatelyignore facts that are tangential to the user's interests, whether or notthese facts are the focus of the article.
In other words, briefings are moreuser-centered than general summaries; the latter convey information thatthe writer has considered important, whereas briefings are based oninformation that the user is looking for.We present a system, called SUMMONS 1 (McKeown and Radev 1995; Radev 1996;Radev and McKeown 1997), shown in Figure 1, which introduces novel techniques inthe following areas:?
It briefs the user on information of interest using tools related toinformation extraction, conceptual combination, and text generation.?
It combines information from multiple news articles into a coherentsummary using symbolic techniques.?
It augments he resulting summaries using descriptions of entitiesobtained from on-line sources.As can be expected from a knowledge-based summarization system, SUMMONSworks in a restricted omain.
We have chosen the domain of news on terrorism forseveral reasons.
First, there is already a large body of related research projects in in-formation extraction, knowledge representation, and text planning in the domain ofterrorism.
For example, earlier systems developed under the DARPA Message Under-standing Conference (MUC) were in the terrorist domain, and thus, we can build onthese systems without having to start from scratch.
The domain is important to a vari-ety of users, including casual news readers, journalists, and security analysts.
Finally,SUMMONS is being developed as part of a general environment for illustrated brief-ing over live multimedia information (Aho et al 1997).
Of all MUC system domains,terrorism is more likely to have a variety of related images than other domains thatwere explored, such as mergers and acquisitions or management succession.In order to extract information of interest o the user, SUMMONS makes use ofcomponents from several MUC systems.
The output of such modules is in the form of1 SUMMarizing Online NewS articles470Radev and McKeown Generating Natural Language Summaries'- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.  "
.
.
.
.
.
.
.
.
.
.
.
.
.
d~4/~'&~ ; &~-s~ c-~  i6 ,~ J .
- /~ ' ,$ .
*Y  G~.~ .
.
.
.
i :~ 1 DESCRIPTIO~ CATEGORIZER \[COMBINER I .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
'EXTENDEDSUMMARYGENERATORSENTENCE PLANNER II ........... ~ - ?
?
~  .... L\] SENTENCE GENERATOR.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,~, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,Figure 1SUMMONS architecture.templates that represent certain pieces of information found in the source news articles,such as victims, perpetrators, or type of event.
By relying on these systems, the taskwe have addressed to date is happily more restricted than direct summarization f fulltext.
This has allowed us to focus on issues related to the combination of informationin the templates and the generation of text to express them.In order to port our system to other domains, we would need to develop newtemplates and the information extraction rules required for them.
While this is a taskwe leave to those working in the information extraction field, we note that there do ex-ist tools for semi-automatically acquiring such rules (Lehnert et al 1993; Fisher et al1995).
This helps to alleviate the otherwise knowledge-intensive nature of the task.We are working on the development of tools for domain-independent types of infor-mation extraction.
For example, our work on extracting descriptions of individualsand organizations and representing them in a formalism that facilitates reuse of thedescriptions in summaries can be used in any domain.In the remainder of this section, we highlight he novel techniques of SUMMONSand explain why they are important for our work.1.1 Summarization of Multiple ArticlesWith a few exceptions (cf.
Section 2), all existing summarizers provide summariesof single articles by extracting sentences from them.
If such systems were appliedto a series of articles, they might be able to extract sentences that have words incommon with the other articles, but they would be unable to indicate how sentencesthat were extracted from different articles were similar.
Moreover, they would certainlynot be able to indicate significant differences between articles.
In contrast, our work471Computational Linguistics Volume 24, Number 3focuses on processing of information from multiple sources to highlight agreementsand contradictions a part of the summary.1.2 Summarization from Multiple SourcesGiven the omnipresence of on-line news services, one can expect hat any interestingnews event will be covered by several, if not most of them.
If different sources presentthe same information, the user clearly only needs to have access to one of them.Practicall~ this assumption doesn't hold, as different sources provide updates from adifferent perspective and at different times.
An intelligent summarizer's task, therefore,is to attain as much information from the multiple sources as possible, combine it, andpresent it in a concise form to the user.
For example, if two sources of informationreport a different number of casualties in a particular incident, SUMMONS will reportthe contradiction and attribute the contradictory information to its sources, rather thanselect one of the contradictory pieces without he other.1.3 Symbolic Summarization through Text Understanding and GenerationAn inherent problem to summarizers based on sentence xtraction is the lack of dis-course-level f uency in the output.
The extracted sentences fit together only in thecase they are adjacent in the source document.
Because SUMMONS uses languagegeneration techniques to determine the content and wording of the summary basedon information extracted from input articles, it has all necessary information to producea fluent surface summary.1.4 Automatic Acquisition of Lexical Resources for GenerationWe show how the summary generated using symbolic techniques can be enhancedso that it includes descriptions of entities (such as people, places, or organizations)it contains.
If a user tunes in to news on a given event several days after the firstreport, references to and descriptions of the event, people, and organizations involvedmay not be adequate.
We collect such descriptions from on-line sources of past newsand represent them using our generation formalism for reuse in later generation ofsummaries.1.5 Structure of the paperThe following section positions our research in the context of prior work in the area.Section 3 describes the system architecture that we have developed for the summa-rization task.
The next two sections describe in more detail how a base summary isgenerated from multiple source articles and how the base summary is extended usingdescriptions extracted from on-line sources.
Section 6 describes the current status ofour system.
We conclude this article in Sections 7 and 8 by describing some directionsfor future work in symbolic summarization f heterogeneous sources.2.
Related WorkPrevious work related to summarization falls into three main categories.
In the first,full text is accepted as input and some percentage of the text is produced as output.Typically, statistical approaches, augmented with keyword or phrase matching, areused to lift from the article full sentences that can serve as a summary.
Most of thework in this category produces a summary for a single article, although there are afew exceptions.
The other two categories correspond to the two stages of processingthat would have to be carried out if sentence xtraction were not used: analysis ofthe input document to identify information that should appear in a summary and472Radev and McKeown Generating Natural Language Summariesgeneration of a textual summary from a set of facts that are to be included.
In thissection, we first present work on sentence xtraction, next turn to work on identifyinginformation in an article that should appear in a summary, and conclude with workon generation of summaries from data, showing how this task differs from the moregeneral anguage generation task.This is a systems-oriented perspective on summarization-related work focusingon techniques that have been implemented for the task.
There is also a large body ofwork on the nature of abstracting from a library science point of view (Borko 1975).This work distinguishes between different ypes of abstracts, most notably, indicativeabstracts that tell what an article is about, and informative abstracts, that includemajor results from the article and can be read in place of it.
SUMMONS generatessummaries that are informative in nature.
Research in psychology and education alsofocuses on how to teach people to write summaries (e.g., Endres-Niggemeyer 1993;Rothkegel 1993).
This type of work can aid the development of summarization sys-tems by providing insights into the human process of summarization that could besimulated in systems.2.1 Summarization through Sentence ExtractionTo allow summarization i arbitrary domains, researchers have traditionally appliedstatistical techniques (Luhn 1958; Paice 1990; Preston and Williams 1994; Rau, Brandow,and Mitze 1994).
This approach can be better termed extraction rather than summa-rization, since it attempts to identify and extract key sentences from an article usingstatistical techniques that locate important phrases using various statistical measures.This has been successful in different domains (Preston and Williams 1994) and is,in fact, the approach used in recent commercial summarizers (Apple \[Boguraev andKennedy 1997\], Microsoft, and inXight).
Rau, Brandow, and Mitze (1994) report thatstatistical summaries of individual news articles were rated lower by evaluators thansummaries formed by simply using the lead sentence or two from the article.
Thisfollows the principle of the "inverted pyramid" in news writing, which puts the mostsalient information in the beginning of the article and leaves elaborations for laterparagraphs, allowing editors to cut from the end of the text without compromisingthe readability of the remaining text.Paice (1990) also notes that problems for this approach center around the fluencyof the resulting summary.
For example, extracted sentences may accidentally includepronouns that have no previous reference in the extracted text or, in the case of ex-tracting several sentences, may result in incoherent text when the extracted sentencesare not consecutive in the original text and do not naturally follow one another.
Paicedescribes techniques for modifying the extracted text to replace unresolved references.Summaries that consist of sentences plucked from texts have been shown to be usefulindicators of content, but they are often judged to be highly unreadable (Brandow,Mitze, and Rau 1990).A more recent approach (Kupiec, Pedersen, and Chen 1995) uses a corpus ofarticles with summaries to train a statistical summarization system.
During training,the system uses abstracts of existing articles to identify the features of sentences thatare typically included in abstracts.
In order to avoid problems noted by Paice, thesystem produces an itemized list of sentences from the article thus eliminating theimplication that these sentences function together coherently as a full paragraph.
Aswith the other statistical approaches, this work is aimed at summarization of singlearticles.Work presented at the 1997 ACL Workshop on Intelligent Scalable Text Summa-rization primarily focused on the use of sentence xtraction.
Alternatives to the use473Computational Linguistics Volume 24, Number 3of frequency of key phrases included the identification and representation f lexicalchains (Halliday and Hasan 1976) to find the major themes of an article followed bythe extraction of one or two sentences per chain (Barzilay and Elhadad 1997), trainingover the position of summary sentences in the full article (Hovy and Lin 1997), andthe construction of a graph of important opics to identify paragraphs that should beextracted (Mitra, Singhal, and Buckley 1997).While most of the work in this category focuses on summarization of single arti-cles, early work is beginning to emerge on summarization across multiple documents.In ongoing work at Carnegie Mellon, Carbonell (personal communication) is develop-ing statistical techniques to identify similar sentences and phrases across articles.
Theaim is to identify sentences that are representative of more than one article.Mani and Bloedorn (1997) link similar words and phrases from a pair of articlesusing WordNet (Miller et al 1990) semantic relations.
They show extracted sentencesfrom the two articles side by side in the output.While useful in general sentence xtraction approaches cannot handle the task thatwe address, aggregate summarization across multiple documents, since this requires rea-soning about similarities and differences across documents to produce generalizationsor contradictions at a conceptual level.2.2 Identifying Information in Input ArticlesWork in summarization using symbolic techniques has tended to focus more on iden-tifying information in text that can serve as a summary (Young and Hayes 1985;Rau 1988; Hahn 1990) than on generating the summar~ and often relies heavily ondomain-dependent scripts (DeJong 1979; Tait 1983).
The DARPA message understand-ing systems (MUC 1992), which process news articles in specific domains to extractspecified types of information, also fall within this category.
As output, work of thistype produces templates that identify important pieces of information in the text, rep-:resenting them as attribute-value pairs that could be part of a database ntry.
The:message understanding systems, in particular, have been developed over a long pe-riod, have undergone repeated evaluation and development, including moves to newdomains, and as a result, are quite robust.
They are impressive in their ability to han-dle large quantities of free-form text as input.
As stand-alone systems, however, theydo not address the task of summarization since they do not combine and rephraseextracted information as part of a textual summary.A recent approach to symbolic summarization is being carried out at CambridgeUniversity on identifying strategies for summarization (Sparck Jones 1993).
This workstudies how various discourse processing techniques (e.g., rhetorical structure rela-tions) can be used to both identify important information and form the actual sum-mary.
While promising, this work does not involve an implementation as of yet, butprovides a framework and strategies for future work.
Marcu (1997) uses a rhetoricalparser to build rhetorical structure trees for arbitrary texts and produces a summaryby extracting sentences that span the major rhetorical nodes of the tree.In addition to domain-specific information extraction systems, there has also beena large body of work on identifying people and organizations in text through propernoun extraction.
These are domain-independent techniques that can also be used toextract information for a summary.
Techniques for proper noun extraction include theuse of regular grammars to delimit and identify proper nouns (Mani et al 1993; Paiket al 1994), the use of extensive name lists, place names, titles and "gazetteers" inconjunction with partial grammars in order to recognize proper nouns as unknownwords in close proximity to known words (Cowie et al 1992; Aberdeen et al 1992),statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso474Radev and McKeown Generating Natural Language Summarieset al 1992), and the use of concept-based pattern matchers that use semantic onceptsas pattern categories as well as part-of-speech information (Weischedel et al 1993;Lehnert et al 1993).
In addition, some researchers have explored the use of both localcontext surrounding the hypothesized proper nouns (McDonald 1993; Coates-Stephens1991) and the larger discourse context (Mani et al 1993) to improve the accuracy ofproper noun extraction when large known-word lists are not available.
In a way similarto this research, our work also aims at extracting proper nouns without he aid of largeword lists.
We use a regular grammar encoding part-of-speech ategories to extractcertain text patterns (descriptions) and we use WordNet (Miller et al 1990) to providesemantic filtering.Another system, called MURAX (Kupiec 1993), is similar to ours from a differentperspective.
MURAX also extracts information from a text to serve directly in responseto a user question.
MURAX uses lexicosyntactic patterns, collocational nalysis, alongwith information retrieval statistics, to find the string of words in a text that is mostlikely to serve as ,an answer to a user's wh-query.
Ultimately, this approach could beused to extract information on items of interest in a user profile, where each questionmay represent a different point of interest.
In our work, we also reuse strings (i.e.,descriptions) as part of the summar34 but the string that is extracted may be merged,or regenerated, aspart of a larger textual summary.2.3 Summary GenerationSummarization of data using symbolic techniques has met with more success thansummarization f text.
Summary generation is distinguished from the more traditionallanguage generation problem by the fact that summarization is concerned with con-veying the maximal amount of information within minimal space.
This goal is achievedthrough two distinct subprocesses, conceptual and linguistic summarization.
Concep-tual summarization is a form of content selection.
It must determine which conceptsout of a large number of concepts in the input should be included in the summary.Linguistic summarization is concerned with expressing that information in the mostconcise way possible.We have worked on the problem of summarization of data within the contextof three separate systems.
STREAK (Robin and McKeown 1993; Robin 1994; Robinand McKeown 1995) generates summaries of basketball games, using a revision-basedapproach to summarization.
It builds a first draft using fixed information that mustappear in the summary (e.g., in basketball summaries, the score and who won andlost is always present).
In a second pass, it uses revision rules to opportunisticallyadd in information, as allowed by the form of the existing text.
Using this approach,information that might otherwise appear as separate sentences gets added in as mod-ifiers of the existing sentences, or new words that can simultaneously convey bothpieces of information are selected.
PLANDoc (McKeown, Kukich, and Shaw 1994a;McKeown, Robin, and Kukich 1995; Shaw 1995) generates summaries of the activi-ties of telephone planning engineers, using linguistic summarization both to order itsinput messages and to combine them into single sentences.
Focus has been on thecombined use of conjunction, ellipsis, and paraphrase to result in concise, yet fluentreports (Shaw 1995).
ZEDDoc (Passonneau et al 1997; Kukich et al 1997) generatesWeb traffic summaries for advertisement management software.
It makes use of anontology over the domain to combine information at the conceptual level.All of these systems take tabular data as input.
The research focus has been onlinguistic summarization.
SUMMONS, on the other hand, focuses on conceptual sum-marization of both structured and full-text data.At least four previous ystems developed elsewhere use natural language to sum-475Computational Linguistics Volume 24, Number 3marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG(Bourbeau et al 1990), and LFS (Iordanskaja et al 1994).
All of these use some formsof conceptual and linguistic summarization and the techniques can be adapted forour current work on summarization of multiple articles.
In related work, Dalianisand Hovy (1993) have also looked at the problem of summarization, identifying eightaggregation operators (e.g., conjunction around noun phrases) that apply during gen-eration to create more concise text.3.
System OverviewThe overall architecture of our summarization system given earlier in Figure 1 drawson research in software agents (Genesereth and Ketchpel 1994) to allow connections toa variety of different ypes of data sources.
Facilities are used to provide a transparentinterface to heterogeneous data sources that run on several machines and may bewritten in different programming languages.
Currently, we have incorporated facilitiesto various live news streams, the CIA World Factbook, and past newspaper archives.The architecture allows for the incorporation of additional facilitators and data sourcesas our work progresses.The system extracts data from the different sources and then combines it into aconceptual representation f the summary.
The summarization component, shown onthe left side of the figure, consists of a base summary generator, which combines infor-mation from multiple input articles and organizes that information using a paragraphplanner.
The structured conceptual representation of the summary is passed to thelexical chooser, shown at the bottom of the diagram.
The lexical chooser also receivesinput from the World Factbook and possible descriptions of people or organizationsto augment he base summary.
The full content is then passed through a sentencegenerator, implemented using the FUF/SURGE language generation system (Elhadad1993; Robin 1994).
FUF is a functional unification formalism that uses a large systemicgrammar of English, called SURGE, to fill in syntactic onstraints, build a syntactictree, choose closed class words, and eventually linearize the tree as a sentence.The right side of the figure shows how proper nouns and their descriptions areextracted from past news.
An entity extractor identifies proper nouns in the pastnewswire archives, along with descriptions.
Descriptions are then categorized usingthe WordNet hierarchy.
Finally, an FD or functional description (Elhadad 1993) forthe description is generated so that it can be reused in fluent ways in the final sum-mary.
FDs mix functional, semantic, syntactic, and lexical information in a recursiveattribute-value format that serves as the basic data structure for all information withinFUF / SURGE.4.
Generating the SummarySUMMONS produces a summary from sets of templates that contain the salientfacts reported in the input articles and that are produced by the message understand-ing systems.
These systems extract specific pieces of information from a given newsarticle.
An example of a template produced by MUC systems and used in our systemis shown in Figures 2 and 3.
To test our system, we used the templates produced bysystems participating in MUC-4 (MUC 1992) as input.
MUC-4 systems operate on theterrorist domain and extract information by filling fields such as perpetrator, victim,~md type of event, for a total number of 25 fields per template.
In addition, we filledthe same template forms by hand from current news articles for further testing.
Cur-rently, work is under way in our group on the building of an information extraction476Radev and McKeown Generating Natural Language SummariesMESSAGE: IDMESSAGE: TEMPLATEINCIDENT: DATEINCIDENT: LOCATIONINCIDENT: TYPEINCIDENT: STAGE OF EXECUTIONINCIDENT: INSTRUMENT IDINCIDENT: INSTRUMENT TYPEPERP: INCIDENT CATEGORYPERP: INDIVIDUAL IDPERP: ORGANIZATION IDPERP: ORG.
CONFIDENCEPHYS TGT: IDPHYS TGT: TYPEPHYS TGT: NUMBERPHYS TGT: FOREIGN NATION?
PHYS TGT: EFFECT OF INCIDENTPHYS TGT: TOTAL NUMBERHUM TGT: NAMEHUM TGT: DESCRIPTIONHUM TGT: TYPEHUM TGT: NUMBERHUM TGT: FOREIGN NATIONHUM TGT: EFFECT OF INCIDENTHUM TGT: TOTAL NUMBERTST3-MUC4-0010201 NOV 89EL SALVADORATTACKACCOMPLISHEDTERRORIST ACT"TERRORIST""THE FMLN"REPORTED: "THE FMLN""1 CIVILIAN"CIVILIAN: "1 CIVILIAN"1:"1 CWILIAN"DEATH: "1 CIVILIAN"Figure 2Sample MUC-4 template.
(message(system (id "TST3-MUC4-O010")(template-no 2))(source (secondary "NCCOSC"))(incident (date "01 NOV 89")(location "El Salvador")(type attack)(stage accomplished))(perpetrator (category terr-act)(org-id "THE FMLN")(org-conf rep-fact))(victim (description civilian)(number 1)Figure 3Parsed MUC-4 template.modu le  s imi lar  to the ones used  in the MUC conferences, wh ich  we wi l l  later use asan input  to SUMMONS.
We are bas ing  our  imp lementat ion  on the tools deve lopedat the Un ivers i ty  of Massachuset ts  (Fisher et al 1995).
The resul t ing sys tem wi l l  noton ly  be able to generate  summar ies  f rom preparsed  templates  but  wi l l  also producesummar ies  d i rect ly  f rom raw text by  merg ing  the message  unders tand ing  componentw i th  the current  vers ion of SUMMONS.Our  work  prov ides  a methodo logy  for deve lop ing  summar izat ion  systems,  iden-tifies p lann ing  operators  for combin ing  in format ion  in a concise summary ,  and  usesempi r i ca l ly  col lected phrases  to mark  summar ized  mater ia l .
We have  col lected a cor-pus  of newswi re  summar ies  that we  used  as data  for deve lop ing  the p lann ing  opera-tors and  for gather ing  a large set of lexical construct ions used  in summar izat ion .
This477Computational Linguistics Volume 24, Number 3Reuters reported that 18 people were killed in a Jerusalem bombing Sunday.
Thenext day, a bomb in Tel Aviv killed at least 10 people and wounded 30 accordingto Israel radio.
Reuters reported that at least 12 people were killed and 105wounded.
Later the same day, Reuters reported that the radical Muslim group Hamashad claimed responsibility for the act.Figure 4Sample output from SUMMONS.corpus will eventually aid in a full system evaluation.
Since news articles often sum-marize previous reports of the same event, our corpus also includes short summariesof previous articles.We used this corpus to develop both the content planner (i.e., the module that de-termines what information to include in the summary) and the linguistic component(i.e., the module that determines the words and surface syntactic form of the sum-mary) of our system.
We used the corpus to identify planning operators that are usedto combine information; this includes techniques for linking information together in arelated way (e.g., identifying changes, similarities, trends) as well as making general-izations.
We also identified phrases that are used to mark summaries and used theseto build the system lexicon.
An example summary produced by the system is shownin Figure 4.
This paragraph sun~arizes four articles about two separate terrorist actsthat took place in Israel in March of 1996 using two different planning operators.While the system we report on is fully implemented, our work is undergoingcontinuous development.
Currently, the system includes eight different planning op-erators, a testbed of 200 input templates grouped into sets on the same event, andcan produce fully lexicalized summaries for approximately half of the cases (the restof the templates were either not complete or the information extracted in them wasirrelevant o the task).
We haven't performed an evaluation beyond the testbed.Our work provides a methodology for increasing the vocabulary size and therobustness of the system using a collected corpus, and moreover, it shows how sum-marization can be used to evaluate the message understanding systems, identifyingfuture research directions that would not be pursued under the current MUC eval-uation cycle.
2 Due to inherent difficulties in the summarization task, our work is asubstantial first step and provides the framework for a number of different researchdirections.The rest of this section describes the summarizer, specifying the planning operatorsused for summarization as well as a detailed discussion of the summarization algo-rithm showing how summaries of different length are generated.
We provide examplesof the summarization markers we collected for the lexicon and show the demands that:summarization creates for interpretation.4.1 Overview of the Summarization ComponentThe summarization component of SUMMONS is based on the traditional anguagegeneration system architecture (McKeown 1985; McDonald and Pustejovsky 1986;Hovy 1988).
A typical language generator is divided into two main components, a2 Participating systems in the DARPA message understanding program are evaluated on a regular basis.Participants are given a set of training text to tune their systems over a period of t ime and theirsystems are tested on unseen text at fol low-up conferences.478Radev and McKeown Generating Natural Language Summariescontent planner, which selects information from an underlying knowledge base toinclude in a text, and a linguistic component, which selects words to refer to con-cepts contained in the selected information and arranges those words, appropriatelyinflecting them, to form an English sentence.
The content planner produces a con-ceptual representation f text meaning (e.g., a frame, a logical form, or an internalrepresentation f text) and typically does not include any linguistic information.
Thelinguistic omponent uses a lexicon and a grammar of English to realize the concep-tual representation into a sentence.
The lexicon contains the vocabulary for the systemand encodes constraints about when each word can be used.
As shown in Figure 1,the content planner used by SUMMONS determines what information from the inputMUC templates should be included in the summary using a set of planning operatorsthat are specific to summarization a d, to some extent, to the terrorist domain.
Its lin-guistic component determines the phrases and surface syntactic form of the summary.The linguistic omponent consists of a lexical chooser, which determines the high-levelsentence structure of each sentence and the words that realize each semantic role, andthe FUF/SURGE (Elhadad 1991; Elhadad 1993) sentence generator.Input to SUMMONS is a set of templates, where each template represents theinformation extracted from one or more articles by a message understanding system.However, we constructed by hand an additional set of templates that include alsoterrorist events that have taken place after the period of time covered in MUC-4, suchas the World Trade Center bombing, the Hebron Mosque massacre and more recentincidents in Israel, as well as the disaster in Oklahoma City.
These incidents were nothandled by the original message understanding systems.
We also created by hand aset of templates unrelated to real newswire articles, which we used for testing sometechniques of our system.
We enriched the templates for all these cases by adding fourslots: the primary source, the secondary source, and the times at which both sourcesmade their reports.
3 We found having the source of the report immensely useful fordiscovering and reporting contradictions and generalizations, because often differentreports of an event are in conflict.
Also, source information can indicate the level ofconfidence of the report, particularly when reported information changes over time.For example, if several secondary sources all report he same facts for a single event,citing multiple primary sources, it is more likely that this is the way the event reallyhappened, while if there are many contradictions between reports, it is likely that thefacts are not yet fully known.Members of our research group are currently working on event racking (Aho etal.
1997).
Their prototype uses pattern-matching techniques to track changes to on-linenews sources and provide a live feed of articles that relate to a changing event.SUMMONS's summarization component generates a base summary, which con-tains facts extracted from the input set of articles.
The base summary is later enhancedwith additional facts from on-line structured databases with descriptions ofindividualsextracted from previous news to produce the extended summary.
The base summaryis a paragraph consisting of one or more sentences, where the length of the summaryis controlled by a variable input parameter.
In the absence of a specific user model, thebase summary is produced.
Otherwise, the extended summary (base summary withadded descriptions of entities) is generated instead.
Similarly, the default is that thesummary contains references to contradictory and updated information.
However, if3 A primary source isusually adirect witness of the event, and a secondary source ismost often apressagency or journalist, reporting the event.479Computational Linguistics Volume 24, Number 3the user profile makes it explicit, only the latest and the most trusted (as per the user'spreference of sources) facts are included.SUMMONS rates information in terms of importance, where information that ap-pears in only one article is given a lower rating and information that is synthesizedfrom multiple articles is rated more highly.Development of the text generation component of SUMMONS was made eas-ier because of the language generation tools and framework available at ColumbiaUniversity.
No changes in the FUF sentence generator were needed.
In addition, thelexical chooser and content planner were based on the design used in the PLANDocautomated ocumentation system described in Section 2.3.In particular, we used FUF to implement he lexical chooser, representing thelexicon as a grammar as we have done in many previous ystems (Elhadad 1993; Robin1994; McKeown, Robin, and Tanenblatt 1993; Feiner and McKeown 1991).
The maineffort in porting the approach to SUMMONS was in identifying the words and phrasesneeded for the domain.
The content planner features everal stages.
It first groups newsarticles together, identifies commonalities between them, and notes how the discourseinfluences wording by setting realization flags, which denote such discourse featuresas "similarity" and "contradiction."
Realization flags (McKeown, Kukich, and Shaw1994b) guide the choice of connectives in the generation stage.Before lexical choice, SUMMONS maps the templates into FDs that are expectedas input to FUF and uses a domain ontology (derived from the ontologies representedin the message understanding systems) to enrich the input.
For example, grenadesand bombs are both explosives, while diplomats and civilians are both considered tobe human targets.4.2 Methodology: Collecting and Using a Summary CorpusIn order to produce plausible and understandable summaries, we used available on-line corpora as models, including the Wall Street Journal and current newswire fromReuters and the Associated Press.
The corpus of summaries i 2.5 MB in size.
We havemanually grouped 300 articles in threads related to single events or series of similarevents.From the corpora collected in this way, we extracted manually, and after carefulinvestigation, several hundred language constructions that we found relevant o thetypes of summaries we want to produce.
In addition to the summary cue phrasescollected from the corpus, we also tried to incorporate as many phrases as possiblethat have relevance to the message understanding conference domain.
Due to domainvariety, such phrases were essentially scarce in the newswire corpora and we neededto collect them from other sources (e.g., modifying templates that we acquired fromthe summary corpora to provide a wider coverage).Since one of the features of a briefing is conciseness, we have tried to assemblesmall paragraph summaries that, in essence, describe a single event and the changeof perception of the event over time, or a series of related events with no more thana few sentences.4.3 Summary Operators for Content PlanningThe main point of departure for SUMMONS from previous work is in the stage ofidentifying what information to include and how to group it together, as well as theuse of a corpus to guide this and later processes.
In PLANDoc, successive items tosummarize are very similar and the problem is to form a grouping that puts the mostsimilar items together, allowing the use of conjunction and ellipsis to delete repetitivematerial.
For summarizing multiple news articles, the task is almost the opposite; we480Radev and McKeown Generating Natural Language Summaries((#TEMPLATES == 2) &&(T \[i\].
INCIDENT.
LOCATION == T \[2\].
INCIDENT.
LOCATION) a&(T \[i\].
INCIDENT.TIME < T \[2\].
INCIDENT.TIME) St& ...(T \[i\].
SECSOURCE.
SOURCE !
= T \[2\].
SECSOURCE.
SOURCE) ) ==>(apply (' ' contradict ion' ', ' 'with-new-account'  ', T \[i\], T \[2\] ) )Figure 5Rules for the contradiction operator.need to find the differences from one article to the next, identifying how the reportedfacts have changed.
Thus, the main problem was the identification of summarizationstrategies, which indicate how information is linked together to form a concise andcohesive summary.
As we have found in other work (Robin 1994), what informationis included is often dependent on the language available to make concise additions.Thus, using a corpus summary was critical to identifying the different summariespossible.We have developed a set of heuristics derived from the corpora that decide whattypes of simple sentences constitute a summary, in what order they need to be listed, aswell as the ways in which simple sentences are combined into more complex ones.
Inaddition, we have specified which summarization-specific phrases are to be includedin different types of summaries.The system identifies a preeminent set of templates from the input to the MUCsystem.
This set needs to contain a large number of similar fields.
If this holds, we canmerge the set into a simpler structure, keeping the common features and marking thedistinct features as Elhadad (1993) and McKeown, Kukich, and Shaw (1994b) suggest.At each step, a summary operator is selected based on existing similarities be-tween articles in the database.
This operator is then applied to the input templates,resulting in a new template that combines, or synthesizes, information from the old.Each operator is independent of the others and several can be applied in succession tothe input templates.
Each of the seven major operators i further subdivided to covervarious modifications to its input.
Figure 5 shows part of the rules for the Contradic-tion operator.
Given two templates, if INCIDENT.LOCATION is the same, the timeof first report is before time of second report, the report sources are different, and atleast one other slot differs in value, apply the contradiction operator to combine thetemplates.A summary operator encodes a means for linking information in two differenttemplates.
Often it results in the synthesis of new information.
For example, a gen-eralization may be formed from two independent facts.
Alternatively, since we aresummarizing reports written over time, highlighting how knowledge of the eventchanged is important and, therefore, summaries sometimes must identify differencesbetween reports.
A description of the operators we identified in our corpus follows,accompanied by an example of system output for each operator.
Each example pri-marily summarizes two or three input templates, as this is the result of applying asingle operator once.
More complex summaries can be produced by applying multipleoperators on the same input, as shown in the examples; ee Figures 6 to 11 in Section4.5.4.3.1 Change of Perspective.
When an initial report gets a fact wrong or has incom-plete information, the change is usually included in the summary.
In order for the"change of perspective" operator to apply, the SOURCE field must be the same, while481Computational Linguistics Volume 24, Number 3the value of another field changes o that it is not compatible with the original value.For example, if the number of victims changes, we know that the first report waswrong if the number goes down, while the source had incomplete information (or addi-tional people died) if the number goes up.
The first two sentences from the followingexample were generated using the change of perspective operator.
The initial estimateof "at least 10 people" killed in the incident becomes "at least 12 people."
Similarly,the change in the number of wounded people is also reported.Example 1March 4th, Reuters reported that a bomb in Tel Aviv killed at least 10 people andwounded 30.
Later the same day, Reuters reported that at least 12 people were killed and105 wounded.4.3.2 Contradiction.
When two sources report conflicting information about he sameevent, a contradiction arises.
In the absence of values indicating the reliability of thesources, a summary cannot report either of them as true, but can indicate that the factsare not clear.
The number of sources that contradict each other can indicate the level ofconfusion about he event.
Note that the current output of the message understandingsystems does not include sources.
However, SUMMONS uses this feature to reportdisagreement between output by different systems.
A summary might indicate thatone of the sources determined that 20 people were killed, while the other source de-termined that only 5 were indeed killed.
The difference between this example and theprevious one on change of perspective is the source of the update.
If the same sourceannounces a change, then we know that it is reporting a change in the facts.
Other-wise, an additional source presents information that is not necessarily more correctthan the information presented by the earlier source and we can therefore concludethat we have a contradiction.Example 2The afternoon of February 26, 1993, Reuters reported that a suspected bomb killed atleast six people in the World Trade Center.
However, Associated Press announced thatexactly five people were killed in the blast.4.3.3 Addition.
When a subsequent report indicates that additional facts are known,this is reported in a summary.
Additional results of the event may occur after the initialreport or additional information may become known.
The operator determines this bythe way the value of a template slot changes.
Since the former template doesn't containa value for the perpetrator slot and the latter contains information about claimedresponsibility, we can apply the addition operator.Example 3On Monday, a bomb in Tel Aviv killed at least 10 people and wounded 30 accordingto Israel radio.
Later the same day, Reuters reported that the radical Muslim group Hamashad claimed responsibility for the act.4.3.4 Refinement.
In subsequent reports a more general piece of information may berefined.
Thus, if an event is originally reported to have occurred in New York City,the location might later be specified as a particular borough of the city.
Similarly, if aterrorist group is identified as Palesfinian, later the exact name of the terrorist groupmay be determined.
Since the update is assigned a higher value of "importance," itwill be favored over the original article in a shorter summary.
Unlike the previous482Radev and McKeown Generating Natural Language Summariesexample, there was a value for the perpetrator slot in the first template, while thesecond one further elaborates on it, identifying the perpetrator more specifically.Example 4On Monday, Reuters announced that a suicide bomber killed at least 10 people in TelAviv.
Later the same day, Reuters reported that the Islamic fundamentalist group Hamasclaimed responsibility.4.3.5 Agreement.
If two sources have the same values for a specific slot, this willheighten the reader's confidence in their veracity and thus, agreement between sourcesis usually reported.Example 5The morning of March 1st 1994, UPI reported that a man was kidnapped in the Bronx.Later, this was confirmed by Reuters.4.3.6 Superset/Generalization.
If the same event is reported from different sources andall of them have incomplete information, it is possible to combine information fromthem to produce a more complete summary.
This operator is also used to aggregatemultiple events as shown in the example.Example 6Reuters reported that 18 people were killed in a Jerusalem bombing Sunday.
The nextday, a bomb in Tel Aviv killed at least 10 people and wounded 30 according to Israelradio.
A total off at least 28 people were killed in the two terrorist acts in Israel over the lasttwo days.It should be noted that in this example, the third sentence will not be generatedif there is a restriction on the length of the summary.4.3.7 Trend.
There is a trend if two or more articles reflect similar patterns over time.Thus, we might notice that three consecutive bombings occurred at the same locationand summarize them into a single sentence.Example 7This is the third terrorist act committed by Hamas in four weeks.4.3.8 No Information.
Since we are interested in conveying information about he pri-mary and secondary sources of a certain piece of news, and since these are generallytrusted sources of information, we ought also to pay attention to the lack of infor-mation from a certain source when such is expected to be present.
For example, itmight be the case that a certain ews agency reports a terrorist act in a given countr3~but the authorities of that country don't give out any information.
Since there is aninfinite number of sources that might not confirm a given fact (or the system will nothave access to the appropriate templates), we have included this operator only as anillustration of a concept hat further highlights the domain-specificity of the system.Example 8Two bombs exploded in Baghdad, Iraqi dissidents reported Friday.
There was no con-firmation of the incidents by the Iraqi National Congress.483Computational Linguistics Volume 24, Number 34.4 AlgorithmThe algorithm used in the system to sort, combine, and generalize the input templatesis described in the following subsections.4.4.1 Input.
At this stage, the system receives a set of templates from the messageunderstanding conferences or a similar set of templates from a related domain.
Alltemplates are described as lists of attribute/value pairs (as shown later in Figure 7).These pairs (with the exception of the source information) are defined in the MUC-4guidelines.4.4.2 Preprocessing.
This stage includes the following substages:?
The templates are sorted in chronological order.?
Templates that have obviously been incorrectly generated by a MUCsystem are identified and filtered manually.
This includes templates leftblank or mostly unfilled by the MUC system.?
A database of all fields and templates i created.
This database is usedlater as a basis for grouping and collapsing templates.?
All irrelevant fields or fields containing bad values are manually markedas such and don't participate in further analyses.?
Templates related to the same event are manually grouped into sets forcombination using SUMMONS.?
Knowledge of the source of the information is marked as the specificmessage understanding system for the site submitting the template if itis not present in the input template.
Note that since the current messageunderstanding systems do not extract he source, this is the most specificwe can be for such cases.We are experimenting with some techniques to automate the preprocessing stage.Our preliminary impressions how that by restricting SUMMONS to templates inwhich at least five or six slots are filled, we can eliminate most of the irrelevanttemplates.4.4.3 Heuristic Combination.
The template database is scanned for relationships be-tween templates, which will trigger certain operators.
Since slots are matched amongtemplates in chronological order, there is only one sequence in which they can beapplied.Such patterns trigger reordering of the templates and modification of their indi-vidual importance values.
As an example, if two templates are combined with therefinement operator, the importance value of the combined template will be greaterthan the sum of the individual importance of the constituent templates.
At the sametime, the values of these two templates are lowered (still keeping a higher value onthe later one, which is assumed to be the more correct of the two).All templates directly extracted from the MUC output are assigned an initial im-portance value of 100.
Currently, with each application of an operator, we lower thevalue of a contributing individual template by 20 points and give any newly producedtemplate that combines information from already existing contributing templates avalue greater than the sum of the values of the contributing templates after those val-ues have been updated.
Furthermore, some operators reduce the importance values of484Radev and McKeown Generating Natural Language Summariesexisting templates even further (e.g., the refinement operator reduces the importance ofchronologically earlier templates by additional increments of 20 points because theycontain outdated information).
Thus, the final summary will contain only the com-bined template if there are restrictions on length.
Otherwise, text corresponding to theconstituent templates will also be generated.The value of the importance of the template corresponds also to the position inthe summary paragraph, as more important templates will be generated first.Each new template contains information indicating whether its constituent tem-plates are obsolete and thus no longer needed.
Also, at this stage the coverage vector(a data structure that keeps track of which templates have already been combined andwhich ones are still to be considered in applying operators) is updated to point to thetemplates that are still active and can be further combined.
This way we make surethat all templates still have a chance of participating in the actual summary.The resulting templates are combined into small paragraphs according to the eventor series of events that they describe.
Each paragraph is then realized by the linguisticcomponent.
Each set of templates produces a single paragraph.4.4.4 Discourse Planning.
Given the relative importance of the templates included inthe database after the heuristic ombination stage, the content planner organizes thepresentation of information within a paragraph.It looks at consecutive t mplates in the database, marked as separate paragraphsfrom the previous tage, and assigns values to "realization switches" that control ocalchoices uch as tense and voice.
They also govern the presence or absence of certainconstituents o avoid repetition of constituents and to satisfy anaphora constraints.4.4.5 Ordering of TempIates and Linguistic Generation.
After all templates have beenconverted into FDs, SUMMONS carries out the following steps to produce the basesummary:?
Templates are sorted according to the order of the value of theimportance slot.
Only the top templates are realized.
Templates withhigher importance values appear with priority in the summary if arestriction on length is specified.?
An intermediate module, the ontologizer (part of the Base SummaryGenerator shown in Figure 1), converts factual information from thetemplate database into data structures compatible with the ontology ofthe MUC domain.
This is used, for example, to make generalizations(e.g., that Medellin and Bogot~i are in Colombia).?
The lexical chooser component of SUMMONS is a functional (systemic)grammar that emphasizes the use of summarization phrases originatingfrom the summary corpora.
For example, it can generate verbs ornominal constructs for nodes in the MUC hierarchy (e.g., "kidnapping"vs. "X kidnapped Y").?
Surface generation from the augmented template FDs is performed usingFUF and SURGE.
We have written additional generation code to handleparagraph-level constructions related to the summarization perators.4.5 An Example of System OperationThis subsection describes how the algorithm is applied to a set of four templates bytracing the computational process that transforms the raw source into a final natural485Computational Linguistics Volume 24, Number 3Article 1: JERUSALEM - A Muslim suicide bomber blew apart 18 people on aJerusalem bus and wounded 10 in a mirror-image of an attack one week ago.
Thecarnage by Hamas could rob Israel's Prime Minister Shimon Peres of the May 29election victory he needs to pursue Middle East peacemaking.
Peres declaredall-out war on Hamas but his tough talk did little to impress tunned residentsof Jerusalem who said the election would turn on the issue of personal security.Article 2: JERUSALEM - A bomb at a busy Tel Aviv shopping mall killed at least10 people and wounded 30, Israel radio said quoting police.
Army radio said theblast was apparently caused by a suicide bomber.
Police said there were manywounded.Article 3: A bomb blast ripped through the commercial heart of Tel AvivMonday, killing at least 13 people and wounding more than 100.
Israeli police sayan Islamic suicide bomber blew himself up outside a crowded shopping mall.
Itwas the fourth deadly bombing in Israel in nine days.
The Islamic fundamentalistgroup Hamas claimed responsibility for the attacks, which have killed at least 54people.
Hamas is intent on stopping the Middle East peace process.
PresidentClinton joined the voices of international condemnation after the latest attack.
Hesaid the "forces of terror shall not triumph" over peacemaking efforts.Article 4: TEL AVIV (Reuters) - A Muslim suicide bomber killed at least 12people and wounded 105, including children, outside a crowded Tel Avivshopping mall Monday, police said.
Sunday, a Hamas suicide bomber killed 18people on a Jerusalem bus.
Hamas has now killed at least 56 people in fourattacks in nine days.
The windows of stores lining both sides of Dizengoff Streetwere shattered, the charred skeletons of cars lay in the street, the sidewalks werestrewn with blood.
The last attack on Dizengoff was in October 1994 when aHamas suicide bomber killed 22 people on a bus.Figure 6Fragments of input articles 1--4.language summary.
Excerpts from the four input news articles are shown in Figure 6.The four news articles are transformed into four templates that correspond to fourseparate accounts of two related events and will be included in the set of templatesfrom which the template combiner will work.
Only the relevant fields are shown.Let's now consider the four templates in the order that they appear  in the list oftemplates.
These templates are shown in Figures 7 to 10.
They are generated man-ually from the input newswire texts.
Information about the pr imary and secondarysources of information is added (PRIMSOURCE and SECSOURCE) .
The differencesin the templates (which will trigger certain operators) are shown in bold face.
Thesummary  generated by the system was shown earlier in Figure 4 and is repeated herein Figure 11.The first two sentences are generated from template one.
The subsequent sentencesare generated using different operators that are triggered according to changing valuesfor certain attributes in the three remaining templates.As previous templates didn't  contain information about the perpetrator, SUM-MONS applies the refinement operator to generate the fourth sentence.
Sentence threeis generated using the change of perspective operator, as the number  of victims re-ported in articles two and three is different.The description for Hamas  ("radical Muslim group") was added by the extractiongenerator (see Section 5).
Typically, a description is included in the source text andshould be extracted by the message understanding system.
In cases in which a de-scription doesn't  appear or is not extracted, SUMMONS generates a description fromthe database of extracted escriptions.
We are currently working on an algorithm that486Radev and McKeown Generating Natural Language SummariesMESSAGE: IDSECSOURCE: SOURCESECSOURCE: DATEPRIMSOURCE: SOURCEINCIDENT: DATEINCIDENT: LOCATIONINCIDENT: TYPEHUM TGT: NUMBERPERP: ORGANIZATION IDTST-REU-0001ReutersMarch 3, 1996 11:30March 3, 1996JerusalemBombing"killed: 18""wounded: 10"Figure 7Template for article one.MESSAGE: IDSECSOURCE: SOURCESECSOURCE: DATEPRIMSOURCE: SOURCEINCIDENT: DATEINCIDENT: LOCATIONINCIDENT: TYPEHUM TGT: NUMBERPERP: ORGANIZATION IDTST-REU-0002ReutersMarch 4, 1996 07:20Israel RadioMarch 4, 1996Tel AvivBombing"killed: at least 10""wounded: 30"Figure 8Template for article two.MESSAGE: IDSECSOURCE: SOURCESECSOURCE: DATEPRIMSOURCE: SOURCEINCIDENT: DATEINCIDENT: LOCATIONINCIDENT: TYPEHUM TGT: NUMBERPERP: ORGANIZATION IDFigure 9Template for article three.TST-REU-0003ReutersMarch 4, 1996 14:20March 4, 1996Tel AvivBombing"killed: at least 13""wounded: more than 100""gamas"wil l  select the best  descr ip t ion  based  on such parameters  as the user  mode l  (whatin format ion  has a l ready  been presented  to the user?
), the at t i tude towards  the ent i ty(is it favorable?
),  or  a h istor ical  mode l  that  descr ibes the changes in the prof i le of aperson  over  the per iod  of t ime (what  was  the prev ious  occupat ion  of the person  whois be ing  descr ibed?)
.487Computational Linguistics Volume 24, Number 3MESSAGE: IDSECSOURCE: SOURCESECSOURCE: DATEPRIMSOURCE: SOURCEINCIDENT: DATEINCIDENT: LOCATIONINCIDENT: TYPEHUM TGT: NUMBERPERP: ORGANIZATION IDTST-REU-0004ReutersMarch 4, 1996 14:30March 4, 1996Tel AvivBombing"killed: at least 12""wounded: 105""Hamas"Figure 10Template for article four.Reuters reported that 18 people were killed in a Jerusalem bombing Sunday.
Thenext day, a bomb in Tel Aviv killed at least 10 people and wounded 30 accordingto Israel radio.
Reuters reported that at least 12 people were killed and 105wounded.
Later the same day, Reuters reported that the radical Muslim group Hamashad claimed responsibility for the act.Figure 11SUMMONS output based on the four articles.5.
Generating DescriptionsWhen a summary refers to an entity (person, place, or organization), it can makeuse of descriptions extracted by the MUC systems.
Problems arise when informationneeded for the summary is either missing from the input article(s) or not extractedby the information extraction system.
In such cases, the information may be readilyavailable in other current news stories, in past news, or in on-line databases.
If thesummarization system can find the needed information i other on-line sources, then itcan produce an improved summary by merging information extracted from the inputarticles with information from the other sources (Radev and McKeown 1997).In the news domain, asummary needs to refer to people, places, and organizationsand provide descriptions that clearly identify the entity for the reader.
Such descrip-tions may not be present in the original text that is being summarized.
For example,the American pilot Scott O'Grady, downed in Bosnia in June of 1995, was unknownto the American public prior to the incident.
To a reader who tuned into news on thisevent days later, descriptions from the initial articles might be more useful.
A sum-marizer that has access to different descriptions will be able to select he descriptionthat best suits both the reader and the series of articles being summarized.
Similarly,in the example in Section 4, if the user hasn't been informed about what Hamas is andno description is available in the source template, older descriptions in the FD formatcan be retrieved and used.In this section, we describe an enhancement to the base summarization system,called the profile manager, which tracks prior references to a given entity by extract-ing descriptions for later use in summarization.
The component includes the entity488Radev and McKeown Generating Natural Language Summariesextractor and description extractor modules hown in Figure 1 and has the followingfeatures:?
It builds a database of profiles for entities by storing descriptions from acollected corpus of past news.?
It operates in real time, allowing for connections with the latest breaking,on-line news to extract information about the most recently mentionedindividuals and organizations.?
It collects and merges information from sources, thus allowing for amore complete record and reuse of information.?
As it parses and identifies descriptions, it builds a lexicalized, syntacticrepresentation f the description i  a form suitable for input to theFUF/SURGE language generation system.As a result, SUMMONS will be able to combine descriptions from articles appear-ing only a few minutes before the ones being summarized with descriptions from pastnews in a permanent s orage for future use.Since the profile manager constructs a lexicalized, syntactic FD from the extracteddescription, the generator can reuse the description in new contexts, merging it withother descriptions, into a new grammatical sentence.
This would not be possible ifonly canned strings were used, with no information about their internal structure.Thus, in addition to collecting a knowledge source that provides identifying featuresof individuals, the profile manager also provides a lexicon of domain-appropriatephrases that can be integrated with individual words from a generator's lexicon toproduce summary wording in a flexible fashion.We have extended the profile manager by semantically categorizing descriptionsusing WordNet, so that a generator can more easily determine which description isrelevant in different contexts.The profile manager can also be used in a real-time fashion to monitor entitiesand the changes of descriptions associated with them over the course of time.The rest of this section discusses the stages involved in the collection and reuseof descriptions.5.1 Creation of a Database of ProfilesIn this subsection, we describe the description management module of SUMMONSshown in Figure 1.
We explain how entity names and descriptions for them are ex-tracted from old newswire and how these descriptions are converted to FDs for surfacegeneration.5.1.1 Extraction of Entity Names  from Old Newswi re .
To seed the database withan initial set of descriptions, we used a 1.7 MB corpus containing Reuters newswirefrom February to June of 1995.
Later, we used a Web-based interface that allowedanyone on the Internet o type in an entity name and force a robot to search fordocuments containing mentions of the entity and extract he relevant descriptions.These descriptions are then also added to the database.At this stage, search is limited to the database of retrieved escriptions only, thusreducing search time, as no connections will be made to external news sources at thetime of the query.
Only when a suitable stored description cannot be found will thesystem initiate search of additional text.489Computational Linguistics Volume 24, Number 3Table 1Two-word and three-word escriptions retrieved by the system.Two-Word Descriptions Three-Word DescriptionsStage Entities Unique Entities Entities Unique EntitiesPOS tagging only 9,079 1,546 2,617 604After WordNet checkup 1,509 395 81 26?
Extraction of candidates for proper nouns.
After tagging the corpususing the POS part-of-speech tagger (Church 1988), we used a CREP(Duford 1993) regular grammar to first extract all possible candidates forentities.
These consist of all sequences of words that were tagged asproper nouns (NP) by POS.
Our manual analysis howed that out of atotal of 2150 entities recovered in this way, 1139 (52.9%) are not names ofentities.
Among these are bigrams uch as Prime Minister or EgyptianPresident hat were tagged as NP by POS.
Table 1 shows how manyentities we retrieve at this stage, and of them, how many pass thesemantic filtering test.?
Weeding out of false candidates.
Our system analyzed all candidates forentity names using WordNet (Miller et al 1990) and removed fromconsideration those that contain words appearing in WordNet'sdictionary.
This resulted in a list of 421 unique entity names that weused for the automatic description extraction stage.
All 421 entity namesretrieved by the system are indeed proper nouns.5.1.2 Extraction of Descriptions.
There are two occasions on which we extract de-scriptions using finite-state t chniques.
The first case is when the entity that we wantto describe was already extracted automatically (see Section 5.1.1) and exists in thedatabase of descriptions.
The second case is when we want a description to be retrievedin real time based on a request from the generation component.In the first stage, the profile manager generates finite-state representations of theentities that need to be described.
These full expressions are used as input to thedescription extraction module, which uses them to find candidate sentences in thecorpus for extracting descriptions.
Since the need for a description may arise at alater time than when the entity was found and may require searching new text, thedescription finder must first locate these expressions in the text.These representations are fed to CREP, which extracts noun phrases on eitherside of the entity (either pre-modifiers or appositions) from the news corpus.
Thefinite-state grammar for noun phrases that we use represents a variety of differentsyntactic structures for both pre-modifiers and appositions.
Thus, they may range froma simple noun (e.g., "president Bill Clinton") to a much longer expression (e.g., "GilbertoRodriguez Orejuela, the head of the Cali cocaine cartel").
Other forms of descriptions, uchas relative clauses, are the focus of ongoing implementation.Table 2 shows some of the different patterns retrieved.
For example, when theprofile manager has retrieved the description the political arm of the Irish RepublicanArmy for Sinn Fein, it looks at the head noun in the description NP (arm), which wemanually added to the list of trigger words to be categorized as an organization (see490Radev and McKeown Generating Natural Language SummariesTable 2Examples of retrieved escriptions.Example Trigger Term Semantic CategoryIslamic Resistance Movement Hamas movementradical Muslim group Hamas groupAddis Ababa, the Ethiopian capital capitalSouth Africa's main black opposition leader,Mangosuthu Buthelezi leaderBoerge Ousland, 33 33maverick French ex-soccer boss Bernard Tapie bossItaly's former prime minister, Silvio Berlusconi ministerSinn Fein, the political arm of the Irish Republican Army armorganizationorganizationlocationoccupationageoccupationoccupationorganizationnext subsection).
It is important to notice that even though WordNet ypically presentsproblems with disambiguation f words retrieved from arbitrary text, we don't haveany trouble disambiguating arm in this case due to the constraints on the context inwhich it appears (as an apposition describing an entity).5.1.3 Categorization of Descriptions.
We use WordNet o group extracted descriptionsinto categories.
For the head noun of the description NP, we try to find a WordNethypemym that can restrict he semantics of the description.
Currently, we identify con-cepts such as "profession, .... nationality," and "organization."
Each of these conceptsis triggered by one or more words (which we call trigger terms) in the description.Table 2 shows some examples of descriptions and the concepts under which they areclassified based on the WordNet hypernyms for some trigger words.
For example, allof the following triggers in the list (minister, head, administrator, and commissioner) canbe traced up to leader in the WordNet hierarchy.
We have currently a list of 75 suchtrigger words that we have compiled manually.5.1.4 Organization of Descriptions in a Database of Profiles.
For each retrieved entitywe create a new profile in a database of profiles.
We keep information about thesurface string that is used to describe the entity in newswire (e.g., Addis Ababa), thesource of the description and the date that the entry has been made in the database(e.g., "reuters95_06_25").
In addition to these pieces of metainformation, all retrieveddescriptions and their frequencies are also stored.Currently, our system doesn't have the capability of matching references to thesame entity that use different wordings.
As a result, we keep separate profiles foreach of the following: Robert Dole, Dole, and Bob Dole.
We use each of these strings asthe key in the database of descriptions.Figure 12 shows the profile associated with the key John Major.
It can be seenthat four different descriptions have been used in the parsed corpus to describe JohnMajor.
Two of the four are common and are used in SUMMONS, whereas the othertwo result from incorrect processing by POS and/or CREEThe database of profiles is updated every time a query retrieves new descriptionsmatching a certain key.5.2 Generation of DescriptionsWhen presenting an entity to the user, the content planner of a language generationsystem may decide to include some background information about it if the user has491Computational Linguistics Volume 24, Number 3KEY: john majorSOURCE: reuters95_03-06_.nwsDESCRIPTION: british prime ministerFREQUENCY: 75DESCRIPTION: prime ministerFREQUENCY: 58DESCRIPTION: a defiant british prime ministerFREQUENCY: 2DESCRIPTION: his british counterpartFREQUENCY: IFigure 12Profile forJohnM~or.Italy~NPNP ~s@$ former~JJ prime@JJminister@NN Silvio@NPNP Berlusconi@NPNPFigure 13Retrieved escription for Silvio Berlusconi.
"cat npcomplex appositionrestrictive nodistinctcarcdrcatpossessorclassifierheadcar \[ catcommoncat common \]lex "Italy"cat noun-compound \]classifier flex "former"\]\]head lex "prime"lex "minister" \]person-name J \]first-name flex "Silvio" \]last-name lex "Berlusconi" \]Figure 14Generated FD for Silvio Berlusconi.not previously seen the entity.
When the extracted information doesn't contain anappropriate description, the system can use some descriptions retrieved by the profilemanager.5.2.1 Transformation f Descriptions into Functional Descriptions.
In order to reusethe extracted escriptions in the generation of summaries, we have developed a mod-ule that converts finite-state descriptions retrieved by the description extractor intofunctional descriptions that we can use directly in generation.
A description retrievedby the system is shown in Figure 13.
The corresponding FD is shown in Figure 14.5.2.2 Regenerating Descriptions.
We have completed tools to extract he descriptionsand to represent them into FDs but we haven't yet implemented the module forincluding them in the output summary.
We have focused so far on identifying whenthis kind of generation will be needed:492Radev and McKeown Generating Natural Language SummariesGrammaticality.
The deeper epresentation allows for grammaticaltransformations, such as aggregation: e.g., president Yeltsin + presidentClinton can be generated as presidents Yeltsin and Clinton.Unification with existing ontologies.
For example, if an ontologycontains information about he word president as being a realization ofthe concept "head of state," then under certain conditions, thedescription can be replaced by a different one that realizes the concept of"head of state.
"Generation of referring expressions.
In the previous example, ifpresident Bill Clinton is used in a sentence, then head of state can be used asa referring expression i  a subsequent sentence.Modification/Update of descriptions.
If we have retrieved prime ministeras a description for Silvio Berlusconi, and later we obtain knowledgethat someone lse has become Italy's prime minister, then we cangenerate former prime minister using a transformation f the old FD.Lexical choice.
When different descriptions are automatically marked forsemantics, the profile manager can prefer to generate one over anotherbased on semantic features.
This is useful if a summary discusses eventsrelated to one description associated with the entity more than the others.For example, when an article concerns Bill Clinton on the campaign trail,then the description democratic presidential candidate is more appropriate.On the other hand, when an article concerns an international summit ofworld leaders, then the description U.S. President is more appropriate.Merging lexicons.
The lexicon generated automatically b  the systemcan be merged with a manually compiled omain lexicon.6.
System Status6.1 Summary GenerationCurrently, our system can produce simple summaries consisting of one- to three- sen-tence paragraphs, which are limited to the MUC domain and to a few additional eventsfor which we have manually created MUC-like templates.
We have also implementedthe modules to connect o the World Factbook.
We have converted all ontologies re-lated to the MUC and the Factbook into FDs.
The user model which would allowusers to specify preferred sources of information, frequency of briefings, etc., hasn'tbeen fully implemented yet.A problem that we haven't addressed is related to the clustering of articles accord-ing to their relevance to a specific event.
This is an area that requires further esearch.Another such area is the development of algorithms for grouping together articles thatbelong to the same topic.Finally, one of our main topics for future work is the development of techniquesthat can generate summary updates.
To do this, we must make use of a discoursemodel that represents he content and wording of summaries that have already beenpresented to the user.
When generating an update, the summarizer must avoid repeat-ing content and, at the same time, must be able to generate references to entities andevents that were previously described.493Computational Linguistics Volume 24, Number 36.2 The Description GeneratorAt the current stage, the description generator has the following coverage:?
Syntactic coverage.
Currently, the system includes an extensivefinite-state gran~nar that can handle various premodifiers andappositions.
The grammar matches arbitrary noun phrases in each ofthese two cases to the extent hat the POS part-of-speech tagger providesa correct agging.?
Precision.
In Section 5.1.1 we showed the precision of the extraction ofentity names.
Similarly, we have computed the precision of retrieved 611descriptions using randomly selected entities from the list retrieved inSection 5.1.1.
Of the 611 descriptions, 551 (90.2%) were correct.
Theothers included a roughly equal number of cases of incorrect NPattachment and incorrect part-of-speech assignment.?
Length of descriptions.
The longest description retrieved by the systemwas nine lexical items long: Maurizio Gucci, the former head of Italy'sGucci fashion dynasty.
The shortest descriptions are one lexical item inlength---e.g.
President Bill Clinton.?
Protocol coverage.
We have implemented retrieval facilities to extractdescriptions using the NNTP (Usenet News) and HTTP (World-WideWeb) protocols.
These modules can be easily reused in other systemswith similar architecture to ours.6.2.1 Limitations.
Our system currently doesn't handle entity cross-referencing.
It willnot realize that Clinton and Bill Clinton refer to the same person.
Nor will it link aperson's profile with the profile of the organization of which he is a member.
Weshould note that extensive research in this field exists and we plan to make use of oneof the proposed methods (Wacholder, Ravin, and Choi 1997) to solve this problem.6.3 PortabilityAn important issue is portability of SUMMONS to other domains.
There are no a priorirestrictions in our approach that would limit SUMMONS to template-based inputs(and hence, shallow knowledge representation schemes without recursion).
It wouldbe interesting to determine the actual number of different representation schemes fornews in general.Since there exist systems that can learn extraction rules for unrestricted domains(Lehnert et al 1993), the information extraction doesn't seem to present any funda-mental bottleneck either.
Rather the questions are: how many man-hours are requiredto convert o each new domain?
and how many of the rules from one domain areapplicable to each new domain?
There are no clear answers to these questions.
Thelibrary of planning operators used in SUMMONS is extensible and can be ported toother domains, although it is likely that new operators will be needed.
In addition,new vocabulary will also be needed.
The authors plan to perform aportability analysisand report on it in the future.6.4 Suggested EvaluationGiven that no alternative approaches toconceptual summarization f multiple articlesexist, we have found it very hard to perform an adequate evaluation of the summariesgenerated by SUMMONS.
We consider several potential evaluations: qualitative (usersatisfaction and readability) and task-based.
In a task-based evaluation, one set of494Radev and McKeown Generating Natural Language Surrunariesjudges would have access to the full set of articles, while another set of evaluatorswould have the summaries generated by SUMMONS.
The task would involve decisionmaking (e.g., deciding whether the same organization has been involved in multipleincidents).
The time for decision making will be plotted against the accuracy of theanswers provided by the judges from the two sets.
A third set of judges might haveaccess to summaries generated by sununarizers based on sentence xtraction frommultiple documents.
Similar evaluation techniques have been proposed for single-document summarizers (Jing et al 1998).7.
Future WorkThe prototype system that we have developed serves as the springboard for researchin a variety of directions.
First and foremost is the need to use statistical techniquesto increase the robustness and vocabulary of the system.
Since we were looking forphrasings that mark summarization i a full article that includes other material as well,for a first pass we found it necessary to do a manual analysis in order to determinewhich phrases were used for summarization.
In other words, we knew of no automaticway of identifying summary phrases.
However, having an initial seed set of summaryphrases might allow us to automate a second pass analysis of the corpus by lookingfor variant patterns of the ones we have found.By using automated, statistical techniques to find additional phrases, we couldincrease the size of the lexicon and use the additional phrases to identify new sum-marization strategies to add to our stock of operators.Our summary generator could be used both for evaluating message understand-ing systems by using the summaries to highlight differences between systems and foridentifying weaknesses in the current systems.
We have already noted a number ofdrawbacks with the current output, which makes summarization more difficult, giv-ing the generator less information to work with.
For example, it is only sometimesindicated in the output that a reference to a person, place, or event is identical to anearlier reference; there is no connection across articles; the source of the report is notincluded.
Finally, the structure of the template representation is somewhat shallow,being closer to a database record than a knowledge representation.
This means thatthe generator's knowledge of different features of the event and relations betweenthem is somewhat shallow.7.1 Generation of DescriptionsOne of the more important current goals is to increase coverage of the system byproviding interfaces to a large number of on-line sources of news.
We would ideallywant to build a comprehensive and shareable database of profiles that can be queriedover the World-Wide Web.
The database will have a defined interface that will allowfor systems uch as SUMMONS to connect o it.Another goal of our research is the generation of evolving summaries that con-tinuously update the user on a given topic of interest.
In that case, the system willhave a model containing all prior interaction with the user.
To avoid repetitiveness,such a system will have to resort to using different descriptions (as well as referringexpressions) to address a specific entity.
4 We will be investigating an algorithm thatwill select a proper ordering of multiple descriptions referring to the same personwithin the same discourse.4 Our corpus analysis upports this proposition--a l rge number of threads of summaries on the sametopic from the Reuters and UPI newswire used up to 10 different referring expressions (mostly of thetype of descriptions discussed in this paper, but also anaphoric references) to refer to the same ntity.495Computational Linguistics Volume 24, Number 3After we collect a series of descriptions for each possible ntity, we need to decidehow to select among them.
There are two scenarios.
In the first one, we have to pickone single description from the database that best fits the summary we are generat-ing.
In the second scenario, the evolving summary, we have to generate a sequenceof descriptions, which might possibly view the entity from different perspectives.
Weare investigating algorithms that will decide the order of generation of the differ-ent descriptions.
Among the factors that will influence the selection and ordering ofdescriptions, we can note the user's interests, his knowledge of the entity, and thefocus of the summary (e.g., democratic presidential candidate for Bill Clinton, versus U.S.president).We can also select one description over another based on how recently they havebeen included in the database, whether or not one of them has been used in a sum-mary already, whether the summary is an update to an earlier summary, and whetheranother description from the same category has been used already.
We have yet todecide under what circumstances a description eeds to be generated at all.We are interested in implementing existing algorithms or designing our own thatwill match different instances of the same entity appearing in different syntactic forms,e.g., to establish that PLO is an alias for the Palestine Liberation Organization.
We willinvestigate using co-occurrence information to match acronyms to full organizationnames as well as alternative spellings of the same name.We will also look into connecting the current interface with news available on theInternet and with an existing search engine such as Lycos, AltaVista, or Yahoo.
Wecan then use the existing indices of all Web documents mentioning a given entity asa news corpus on which to perform the extraction of descriptions.8.
ConclusionOur prototype system demonstrates the feasibility of generating briefings of a seriesof domain-specific news articles on the same event, highlighting changes over timeas well as similarities and differences among sources and including some historicalinformation about he participants.
The ability to automatically provide summaries ofheterogeneous material will critically help in the effective use of the Internet in orderto avoid overload with information.
We show how planning operators can be used tosynthesize summary content from a set of templates, each representing a single arti-cle.
These planning operators are empirically based, coming from analysis of existingsummaries, and allow for the generation of concise briefings.
Our framework allowsfor experimentation with summaries of different lengths and for the combination ofmultiple, independent summary operators to produce more complex summaries withadded descriptions.AcknowledgmentsThis work was partially supported by NSFgrants GER-90-24069, IRI-96-19124,IRI-96-18797, and CDA-96-25374, as well asa grant from Columbia University'sStrategic Initiative Fund sponsored by theProvost's Office.The authors are grateful to the followingpeople for their invaluable commentsduring the writing of the paper and atpresentations of work related to the contentof the paper: Alfred Aho, Shih-Fu Chang,Eleazar Eskin, Vasileios Hatzivassiloglou,Alejandro Jaimes, Hongyan Jing, JudithKlavans, Min-Yen Kan, Carl Sable, EricSiegel, John Smith, Nina Wacholder, KaziZaman as well as the anonymous reviewersand the editors of the special issue onnatural language generation.ReferencesAberdeen, John, John Burger, DennisConnolly, Susan Roberts, and Marc Vilain.1992.
MITRE-Bedford: Description of the496Radev and McKeown Generating Natural Language SummariesALEMBIC system as used for MUC-4.
InProceedings ofthe Fourth MessageUnderstanding Conference (MUC-4), pages215-222, McLean, VA, June.Aho, Alfred, Shih-Fu Chang, KathleenMcKeown, Dragomir Radev, John Smith,and Kazi Zaman.
1997.
Columbia digitalnews system: An environment forbriefing and search over multimediainformation.
In Proceedings ofADL,Washington, DC, April.Altavista.
1996.
WWW site, URL:http://altavista.digital.com.Aysuo, Damaris, Sean Boisen, Heidi Fox,Herb Gish, Robert Ingria, and RalphWeischedel.
1992.
BBN: Description of thePLUM system as used for MUC-4.
InProceedings ofthe Fourth MessageUnderstanding Conference (MUC-4), pages169-176, McLean, VA, June.Barzilay, Regina and Michael Elhadad.
1997.Using lexical chains for textsummarization.
I  Proceedings oftheWorkshop on Intelligent Scalable TextSummarization, pages 10-17, Madrid,Spain, August.
Association forComputational Linguistics.Berners-Lee, Tim.
1992.
World-Wide Web:The information universe.
ElectronicNetworking, 2(1):52-58.Boguraev, Branimir and ChristopherKennedy.
1997.
Salience-based contentcharacterization f text documents.
InProceedings ofthe Workshop on IntelligentScalable Text Summarization, pages 2-9,Madrid, Spain, August.
Association forComputational Linguistics.Borko, H. 1975.
Abstracting Concepts andMethods.
Academic Press, New York.Bourbeau, Laurent, Denis Carcagno, E.Goldberg, Richard Kittredge, and AlainPolgu~re.
1990.
Bilingual generation ofweather forecasts in an operationsenvironment.
In Hans Karlgren, editor,Proceedings ofthe 13th InternationalConference on Computational Linguistics(COLING-90), volume 3, pages 318-320,Helsinki, Finland.Brandow, Ronald, Karl Mitze, and Lisa F.Rau.
1990.
Automatic ondensation felectronic publications by sentenceselection.
Information Processing andManagement, 26:135--170.Church, Kenneth W. 1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Proceedings oftheSecond Conference on Applied NaturalLanguage Processing (ANLP-88), pages136-143, Austin, TX, February.Association for ComputationalLinguistics.ClariNet.
1996.
WWW site, URL:http: / / www.clari.net.CNN Interactive.
1996.
WWW site, URL:http: / / www.crm.com.Coates-Stephens, Sam.
1991.
Automaticlexical acquisition using within-textdescriptions of proper nouns.
InProceedings ofthe Seventh Annual Conferenceof the UW Centre for the New OED and TextResearch, pages 154-169.Cowie, Jim, Louise Guthrie, Yorick Wilks,James Pustejovsk~ and Scott Waterman.1992.
CRL/NMSU and Brandeis:Description of the MucBruce system asused for MUC-4.
In Proceedings oftheFourth Message Understanding Conference(MUC-4), pages 223-232, McLean, VI,June.Cuts, Short.
1994.
Science and TechnologySection.
Economist, 17:85-86, December.Dalianis, Hercules and Edward Hovy.
1993.Aggregation i  natural anguagegeneration.
Proceedings ofthe 4th EuropeanWorkshop on Natural Language Generation.DejaNews.
1997.
WWW site, URL:http: / / www.dejanews.com.DeJong, G. F. 1979.
Skimming Stories in RealTime: An Experiment in IntegratedUnderstanding.
Ph.D. thesis, ComputerScience Department, Yale University.Duford, Darrin.
1993.
CREP: A regularexpression-matching textual corpus tool.Technical Report CUCS-005-93, ColumbiaUniversity.Elhadad, Michael.
1991.
FUF: The universalunifier--user manual, version 5.0.Technical Report CUCS-038-91, ColumbiaUniversity.Elhadad, Michael.
1993.
Using Argumentationto Control Lexical Choice: A Unij~'cation-basedImplementation.
Ph.D. thesis, ComputerScience Department, Columbia University.Endres-Niggemeyer, Brigitte.
1993.
Anempirical process model of abstracting.
InWorkshop on Summarizing Text for IntelligentCommunication, Dagstuhl, Germany,December.Feiner, Steven and Kathleen McKeown.1991.
Automating the generation ofcoordinated multimedia explanations.IEEE Computer, 24(10):33-41, October.Fisher, David, Stephen Soderland, JosephMcCarthy Fangfang Feng, and WendyLehnert.
1995.
Description of the UMasssystem as used for MUC-6.
In Proceedingsof the Sixth Message UnderstandingConference (MUC-6), pages 221-236.Genesereth, Michael and Steven Ketchpel.1994.
Software agents.
Communications ofthe ACM, 37(7):48-53, July.497Computational Linguistics Volume 24, Number 3Hahn, Udo.
1990.
Topic parsing: accountingfor text macro structures in full-textanalysis.
Information Processing andManagement, 26:135--170.Halliday, Michael and Ruqaiya Hasan.
1976.Cohesion in English.
English LanguageSeries.
Longman, London.Hov~ Eduard H. 1988.
Planning coherentmultisentential text.
In Proceedings ofthe26th Annual Meeting of the Association forComputational Linguistics, Buffalo, NY,June.
Association for ComputationalLinguistics.Hovy, Eduard and Chin Yew Lin.
1997.Automated text summarization iSUMMARIST.
In Proceedings oftheWorkshop on Intelligent Scalable TextSummarization, pages 18-24, Madrid,Spain, August.
Association forComputational Linguistics.Iordanskaja, Lidija, M. Kim, RichardKittredge, Benoit Lavoie, and AlainPolgu~re.
1994.
Generation of extendedbilingual statistical reports.
In Proceedingsof the 15th International Conference onComputational Linguistics (COLING-94),Kyoto, Japan.Jing, Hongyan, Regina Barzilay, andKathleen McKeown.
1998.
Summarizationevaluation methods: Experiments andanalysis.
In Symposium on Intelligent TextSummarization, Stanford, CA, March.Kukich, Karen K. 1983.
Design of aknowledge-based report generator.
InProceedings ofthe 21st Annual Meeting,pages 145-150, Cambridge, MA, June.Association for ComputationalLinguistics.Kukich, Karen, Rebecca Passonneau,Kathleen McKeown, Dragomir Radev,Vasileios Hatzivassiloglou, and HongyanJing.
1997.
Software re-use and evolutionin text generation applications.
InACL/EACL Workshop - From Research toCommercial Applications: Making NLPTechnology Work in Practice, Madrid, Spain.Kupiec, Julian M. 1993.
MURAX: A robustlinguistic approach for questionanswering using an on-line encyclopedia.In Proceedings, 16th Annual InternationalACM SIGIR Conference on Research andDevelopment i  Information Retrieval.Kupiec, Julian M., Jan Pedersen, andFrancine Chen.
1995.
A trainabledocument summarizer.
In Proceedings, 18thAnnual International ACM SIGIR Conferenceon Research and Development i  InformationRetrieval, pages 68-73, Seattle, WA, July.Lehnert, Wendy, Joe McCarthy, StephenSoderland, Ellen Riloff, Claire Cardie,Jonathan Peterson, and Fangfang Feng.1993.
UMass/Hughes: Description of theCIRCUS system used for MUC-5.
InProceedings ofthe Fifth MessageUnderstanding Conference (MUC-5), pages277-291, Baltimore, MD, August.Luhn, Hans P. 1958.
The automatic reationof literature abstracts.
IBM Journal, pages159-165.Lycos, Inc. 1996.
Home Page.
WWW site,URL: http: / / www.lycos.com.Mani, Inderjeet and Eric Bloedorn.
1997.Multi-document summarization by graphsearch and matching.
In Proceedings oftheFourteenth National Conference on ArtificialIntelligence (AAAI-97), pages 622-628,Providence, RI.
American Association forArtificial Intelligence.Mani, Inderjeet, Richard T. Macmillan,Susann Luperfoy, Elaine Lusher, andSharon Laskowski.
1993.
Indentifyingunknown proper names in newswire text.In Proceedings ofthe Workshop on Acquisitionof Lexical Knowledge from Text, pages 44--54,Columbus, OH, June.
Special InterestGroup on the Lexicon of the Associationfor Computational Linguistics.Marcu, Daniel.
1997.
From discoursestructures to text summaries.
InProceedings ofthe Workshop on IntelligentScalable Text Summarization, pages 82-88,Madrid, Spain, August.
Association forComputational Linguistics.McDonald, David D. 1993.
Internal andexternal evidence in the identification andsemantic ategorization f proper names.In Proceedings ofthe Workshop on Acquisitionof Lexical Knowledge from Text, pages 32-43,Columbus, OH, June.
Special InterestGroup on the Lexicon of the Associationfor Computational Linguistics.McDonald, David D. and James D.Pustejovsky.
1986.
Description-directednatural anguage generation.
InProceedings ofthe 9th IJCAL pages 799-805.IJCAI.McKeown, Kathleen R. 1985.
Text Generation:Using Discourse Strategies and FocusConstraints o Generate Natural LanguageTexts.
Cambridge University Press,Cambridge, England.McKeown, Kathleen R., Karen Kukich, andJames Shaw.
1994a.
Practical issues inautomatic documentation generation.
InProceedings ofthe 4th Conference on AppliedNatural Language Processing, Stuttgart,Germany, October.
Association forComputational Linguistics.McKeown, Kathleen R., Karen K. Kukich,and James Shaw.
1994b.
Practical issues inautomatic documentation generation.
InProceedings ofthe ACL Applied Natural498Radev and McKeown Generating Natural Language SummariesLanguage Conference, Stuttgart, Germany,October.McKeown, Kathleen R. and DragomirRadev.
1995.
Generating summaries ofmultiple news articles.
In Proceedings ofthe18th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 74-82, Seattle,WA, July.McKeown, Kathleen R., Jacques Robin, andKaren Kukich.
1995.
Generating concisenatural language summaries.
Journal ofInformation Processing and Management,31(5):703-733.McKeown, Kathleen R., Jacques Robin, andMichael Tanenblatt.
1993.
Tailoring lexicalchoice to the user's vocabulary inmultimedia explanation generation.
InProceedings ofthe 31st Annual Meeting,Columbus, OH., June.
Association forComputational Linguistics.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine J. Miller.
1990.
Introduction toWordNet: An on-line lexical database.International Journal of Lexicography (SpecialIssue), 3(4):235-312.Mitra, Mandar, Amit Singhal, and ChrisBuckley.
1997.
Automatic textsummarization by paragraph extraction.In Proceedings ofthe Workshop on IntelligentScalable Text Summarization, pages 39-46,Madrid, Spain, August.
Association forComputational Linguistics.MUC, Message Understanding Conference.1992.
Proceedings ofthe Fourth MessageUnderstanding Conference (MUC-4).DARPA Software and Intelligent SystemsTechnology Office.NetSumrn.
1996.
Home Page.
WWW site,URL: http: / / www.labs.bt.com /innovate / informat / netsumm /index.htm.New York Times.
1996.
WWW site, URL:http: / / www.nytimes.com.Paice, Chris.
1990.
Constructing literatureabstracts by computer: Techniques andprospects.
Information Processing andManagement, 26:171-186.Paik, Woojin, Elizabeth D. Lidd~ EdmundYu, and Mary McKenna.
1994.Interpretation f proper nouns forinformation retrieval.
In Proceedings oftheHuman Language Technology Workshop,pages 309-313, Plainsboro, NJ, March.ARPA Software and Intelligent SystemsTechnology Office, Morgan Kaufmann,San Francisco, CA.Passormeau, Rebecca, Karen Kukich,Kathleen McKeown, Dragomir Radev,and Hongyan Jing.
1997.
SummarizingWeb traffic: A portability exercise.Technical Report CUCS-009-97, ColumbiaUniversity, Department of ComputerScience, New York.Preston, Keith and Sandra Williams.
1994.Managing the information overload.Physics in Business, June.Radev, Dragornir R. 1996.
An architecturefor distributed natural anguagesummarization.
I  Proceedings ofthe 8thInternational Workshop on Natural LanguageGeneration: Demonstrations and Posters,pages 45-48, Herstmonceux, England,June.Radev, Dragomir R. and Kathleen R.McKeown.
1997.
Building a generationknowledge source usinginternet-accessible newswire.
InProceedings ofthe 5th Conference on AppliedNatural Language Processing, Washington,DC, April.Rau, Lisa F. 1988.
Conceptual informationextraction and information retrieval fromnatural language input.
In Proceedings,RAIO-88: Conference on User-Oriented,Content-Based, Text and Image Handling,pages 424-437, Cambridge, MA.Rau, Lisa F., Ron Brandow, and Karl Mitze.1994.
Domain-independentsummarization f news.
In SummarizingText for Intelligent Communication, pages71-75, Dagstuhl, Germany.Reuters News.
1996.
WWW site, URL:http: / / www.yahoo.com / headlines/.Robin, Jacques.
1994.
Revision-BasedGeneration of Natural Language SummariesProviding Historical Background.
Ph.D.thesis, Computer Science Department,Columbia University.Robin, Jacques and Kathleen R. McKeown,1993.
Corpus analysis for revision-basedgeneration of complex sentences.
InProceedings ofthe 11 th National Conference onArtificial Intelligence, Washington, DC, July.Robin, Jacques and Kathleen R. McKeown.1995.
Empirically designing andevaluating a new revisior~-based modelfor summary generation.
ArtificialIntelligence Journal.
In press.R6sner, Michael.
1987.
SEMTEX: A textgenerator for German.
In Gerard Kempen,editor, Natural Language Generation: NewResults in Artificial Intelligence, Psychology,and Linguistics.
Martinus NinjhoffPublishers.Rothkegel, Annely.
1993.
Abstracting fromthe perspective of text production.
InWorkshop on Summarizing Text for IntelligentCommunication, Dagstuhl, Germany,December.499Computational Linguistics Volume 24, Number 3Shaw, James.
1995.
Conciseness throughaggregation i text generation.
InProceedings of the 33rd Association forComputational Linguistics Annual Meeting(Student Session), pages 329-331.Spar& Jones, K. 1993.
What might be in asummary?
In Proceedings of InformationRetrieval 93: Von der Modellierung zurAnwendung, pages 9-26,Universitatsverlag Knstanz.Tait, John I.
1983.
Automatic Summarsing ofEnglish Texts.
Ph.D. thesis, University ofCambridge, Cambridge, England.Wacholder, Nina, Yael Ravin, and MisookChoi.
Disambiguation of proper names intext.
In Proceedings of the Fifth AppliedNatural Language Processing Conference,Washington DC.
Association forComputational Linguistics.Weischedel, Ralph, Damaris Ayuso, SeanBoisen, Heidi Fox, Robert Ingria,Tomoyoshi Matsukawa, ConstantinePapageorgiou, Dawn McLaughlin,Masaichiro Kitagawa, Tsutomu Sakai,June Abe, Hiroto Hosihi, YoichiMiyamoto, and Scott Miller.
1993.
BBN:Description of the PLUM system as usedfor MUC-5.
In Proceedings of the FifthMessage Understanding Conference (MUC-5),pages 93-108, Baltimore, MD, August.Young, S. R. and P. J. Hayes.
1985.Automatic lassification andsummarization f banking telexes.
InProceedings of the Second Conference onArtificial Intelligence Applications, pages402-408.500
