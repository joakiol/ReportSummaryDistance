Word Sense Disambiguation with Very Large Neural NetworksExtracted from Machine Readable DictionariesJean VERONIS (* and **) and Nancy M. IDE (**)*Groupe Repr6sentation et Traitement des ConnaissancesCENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE31, Ch.
Joseph Aiguiert3402 Marseille Cedex 09 (France)** Department of Computer ScienceVASSAR COLLEGEPoughkeepsie, New York 12601 (U.S.A.)AbstractIn this paper, we describe a means for automatically building very large neural networks(VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use ofthese networks for word sense disambiguation.
Our method brings together two earlier,independent approaches to word sense disambiguation: the use of machine-readabledictionaries and spreading and activation models.
The automatic construction of VLNNsenables real-size xperiments with neural networks for natural language processing, which inturn provides insight into their behavior and design and can lead to possible improvements.1.
\]IntroductionAutomated language understanding requires thedetermination f the concept which a given use of aword represents, a process referred to as word sensedisambiguation (WSD).
WSD is typically effected innatural llanguage processing systems by utilizingsemantic teature lists for each word in the system'slexicon, together with restriction mechanisms such ascase role selection.
However, it is often impractical tomanually encode such information, especially forgeneralized text where the variety and meaning ofwords is potentially unrestricted.
Furthermore,restriction mechanisms usually operate within a singlesentence~ and thus the broader context cannot assist inthe disambiguation process.in this paper, we describe a means tor automaticallybuilding Very Large Neural Networks (VLNNs) fromdefinition texts in machine-readable dictionaries, anddenmnstrate he use of these networks for WSD.
Ourmethod brings together two earlier, independentapproaches to WSD: the use of machine-readabledictionaries and spreading and activation models.
Theautomatic onstruction of VLNNs enables real-sizeexperiments with neural networks, which in turnThe authors would like to acknowledge the contributions ofSt~phanc tlari6 and Gavin Huntlcy to the work presented in thispaper.provides insight into their behavior and design and canlead to possible improvements.2.
Previous work2.1.
Machine-readable dictionaries Jbr WSDThere have been several attempts to exploit theinformation in maclfine-readable versions of everydaydictionaries ( ee, tor instance, Amsler, 1980; Calzolari,1984; Chodorow, Byrd and Heidorn, 1985;Markowitz, Ahlswede and Evens, 1986; Byrd et al,1987; V&onis, Ide and Wurbel, 1989), in which anenormous amount of lexical and semantic knowledge isalready "encoded".
Such information is not systematicor even complete, and its extraction from machine-readable dictionaries is not always straightforward.However, it has been shown that even in its base form,information from machine-readable dictionaries can beused, for example, to assist in the disambiguation fprepositional phrase attachment (Jensen and Bluet,1987), or to find subject domains in texts (Walker andAmsler, 1986).The most general and well-known attempt to utilizeinformation i  machine-readable dictionaries for WSDis that of Lesk (1986), which computes the degree ofoverlap--that is, number of shared words--in definitiontexts of words that appear in a ten-word window of1 389context.
The sense of a word with the greatest numberof overlaps with senses of other words in the windowis chosen as the correct one.
For example, consider thedefinitions of pen and sheep from the Collins EnglishDictionary, the dictionary used in our experiments, infigure 1.Figure 1: Definitions of PEN, SHEEP, GOATand PAGE in the Collins English Dictionarypen 1 1. an implement for writing or drawing using ink, formerlyconsisting of a sharpened and split quill, and now of a metal nibattached to a holder.
2. the writing end of such an implement; nib.
3.style of writing.
4. the pen.
a. writing as an occupation, b. thewritten word.
5, the long horny internal shell of a squid.
6. to writeor compose.pen  2 1. an enclosure in which domestic animals are kept.
2.anyplace of confinement.
3. a dock for servicing submarines.
4. toenclose or keep in a pen.pen 3 short for penitentiary.pen 4 a female swan.sheep L any of various bovid mammals of the genus O~is andrelated genera having transversely ribbed horns and a narrow face,There are many breeds of domestic sheep, raised for their wool and formeat.
2. :Barbary sheep.
3. a meek or timid person.
4. separatethe sheep from the goats, to pick out the members of any groupwho are superior in some respects.goat 1. any sure-footed agile bovid mammal of the genus Capra,naturally inhabiting rough stony ground in Europe, Asia, and NAfrica, typically having a brown-grey colouring and a beard.Domesticated varieties (C. hircus) are reared for milk, meat, and wool.3.
a lecherous man.
4. a bad or inferior member of any group 6. act(or play) the (giddy) goat.
to fool around.
7. get (someone's)goat.
to cause annoyance to (someone)page I 1. one side of one of the leaves of a book, newspaper, letter,etc.
or the written or printed matter it bears.
2. such a leaf consideredas a unit 3. an episode, phase, or period 4.
Printing.
the type as setup for printing a page.
6. to look through (a book, report, etc.
); leafthrough.page 2 1. a boy employed to run errands, carry messages, etc., forthe guests in a hotel, club, etc.
2. a youth in attendance at officialfunctions or ceremonies.
3. a. a boy in training for knighthood inpersonal attendance on a knight, b. a youth in the personal service ofa person of rank.
4. an attendant at Congress or other legislativebody.
5. a boy or girl employed in the debating chamber of the houseof Commons, the Senate, or a legislative assembly to carry messagesfor members.
6. to call out the name of (a person).
7. to call (aperson) by an electronic device, such as bleep, g. to act as a page toor attend as a page.If these two words appear together in context, theappropriate senses of pen (2.1: "enclosure") and sheep(1: "mammal") will be chosen because the definitions ofthese two senses have the word domestic in common.However, with one word as a basis, the relation istenuous and wholly dependent upon a particulardictionary's wording.
The method also fails to take intoaccount less immediate r lationships between words.As a result, it will not determine the correct sense of penin the context of goat.
The correct sense of pen (2.1:enclosure ) and the correct sense of goat (1: mammal )do not share any words in common in their definitionsin the Collins English Dictionary; however, a strategy390which takes into account a longer path throughdefinitions will find that animal is in the definition ofpen 2.1, each of mammal and animal appear in thedefinition of the other, and mammal is in the definitionof goat 1.Similarly, Lesk's method would also be unable todetermine the correct sense of pen (1.1: writingutensil ) in the context of page, because seven of thethirteen senses of pen have the same number ofoverlaps with senses of page.
Six of the senses of penshare only the word write with the correct sense of page(1.1: "leaf of a book").
However, pen 1.1 also containswords such as draw and ink, and page 1.1 containsbook, newspaper, letter, and print.
These other wordsare heavily interconnected in a complex network whichcannot be discovered by simply counting overlaps.Wilks et al (forthcoming) build on Lesk's method bycomputing the degree of overlap for related word-setsconstructed using co-occurrence data from definitiontexts, but their method suffers from the same problems,in addition to combinatorial problems thai preventdisambiguating more than one word at a time.2.2.
Neural networks for WSDNeural network approaches to WSD have beensuggested (Cottrell and Small, 1983; Waltz and Pollack,1985).
These models consist of networks in which thenodes ("neurons") represent words or concepts,connected by "activatory" links: the words activate theconcepts to which they are semantically related, andvice versa.
In addition, "lateral" inhibitory links usuallyinterconnect competing senses of a given word.Initially, the nodes corresponding tothe words in thesentence to be analyzed are activated.
These wordsactivate their neighbors in the next cycle in turn, theseneighbors activate their immediate neighbors, and soon.
After a number of cycles, the network stabilizes in astate in which one sense for each input word is moreactivated than the others, using a parallel, analog,relaxation process.Neural network approaches to WSD seem able tocapture most of what cannot be handled by overlapstrategies such as Lesk's.
However, the networks usedin experiments o far are hand-coded and thusnecessarily very small (at most, a few dozen words andconcepts).
Due to a lack of real-size data, it is not clearthat he same neural net models will scale up for realisticapplication.
Further, some approaches rely on "context-setting" nodes to prime particular word senses in orderto force 1the correct interpretation?
But as Waltz andPollack point out, it is possible that such words (e.g.,writing in the context of pen ) are not explicitly presentin the text under analysis, but may be inferred by thereader from the presence of other, related words (e.g.,page, book, inkwell, etc.).
To solve this problem,words in such networks have been represented by setsof semantic "microfeatures" (Waltz and Pollack, 1985;Bookman, 1987) which correspond to fundamentalsemantic distinctions (animate/inanimate, edible/inedible, threatening/safe, etc.
), characteristic durationof events (second, minute, hour, day, etc.
), locations(city, country, continent, etc.
), and other similardistinctions that humans typically make about situationsin the world.
To be comprehensive, the authors uggestthat these features must number in the thousands.
Eachconcept iin the network is linked, via bidirectionalactivatory or inhibitory links, to only a subset of thecomplete microfeature s t. A given concept theoreticallyshares everal microfeatures with concepts to which it isclosely related, and will therefore activate the nodescorresponding to closely related concepts when it isactivated :itself.ttowever, such schemes are problematic due to thedifficulties of designing an appropriate set ofmicrofeatures, which in essence consists of designingsemantic primitives.
This becomes clear when oneexmnines the sample microfeatures given by Waltz ~mdPollack: they specify micro.f carfares uch as CASINO andCANYON, but it is obviously questionable whether suchconcepts constitute fundamental semantic distinctions.More practically, it is simply difficult to imagine howvectors of several thousands of microfeamrcs for eachone of the lens of thousands of words and hundreds ofthousands of senses can be realistically encoded byhand.3.
Word sense disambiguation with VLNNsOur approach to WSD takes advantage of bothstrategies outlined above, but enables us to addresssolutions to their shortcomings.
This work has beencarried out in tile context of a joint project of VassarCollege and the Groupe Reprdsentation et Traitementdes Connaissances of the Centre National de laRecherche Scientifique (CNRS), which is concernedwith the construction and exploitation of a large lexicaldata base of English and French.
At present, theVassar/CNRS data base includes, through the courtesyof several editors and research institutions, severalEnglish and French dictionaries (the Collins EnglishDictionary, the Oxford Advanced Learner's Dictionary,the COBUILD Dictionary, the Longman) Dictionary ofContemporary English, theWebster's 9th Dictionary,and the ZYZOMYS CD-ROM dictionary from HachettePublishers) as well as several other lexical and textualmaterials (the Brown Corpus of American English, theCNRS BDLex data base, the MRC PsycholinguisticData Base, etc.
).We build VLNNs utilizing definitions in the CollinsEnglish Dictionary.
Like Lesk and Wilks, we assumethat there are significant semantic relations between aword and the words used to define it.
The connectionsin the network reflect these relations.
All of theknowledge represented in the network is automaticallygenerated from a machine-readable dictionary, andtherefore no hand coding is required.
Further, thelexicon m~d the knowledge it contains potentially coverall of English (90,000 words), and as a result thisinformation cml potentially be used to help dismnbiguateunrestricted text.3.1.
Topology of the networkIn our model, words are complex units.
Each word inthe input is represented by a word node connected byexcitatory links to sense nodes (figure 2) representingthe different possible senses tbr that word in the CollinsEnglish Dictionary.
Each sense node is in turnconnected by excitatory links to word nodesrcpreseming the words in tile definition of that sense.This process is repeated a number of times, creating anincreasingly complex and interconnected network.Ideally, the network would include the entire dictionary,but for practical reasons we limit the number ofrepetitions and thus restrict tile size of the network to afew thousand nodes and 10 to 20 thousand transitions.All words in the network are reduced to their lemmas,and grammatical words are excluded.
The differentsense nodes tor a given word are interconnected bylateral inhibitory links.3 391Figure 2.
Topology of the network~.
, : '  .i\ [ ~  Word NodeSense Node~ .
Excitatory Link.......................... Inhibitory LinkWhen the network is run, the input word nodes areactivated first.
Then each input word node sendsactivation to its sense nodes, which in turn sendactivation to the word nodes to which they areconnected, and so on throughout he network for anumber of cycles.
At each cycle, word and sense nodesreceive feedback from connected nodes.
Competingsense nodes send inhibition to one another.
Feedbackand inhibition cooperate in a "winner-take-all" strategyto activate increasingly related word and sense nodesand deactivate the unrelated or weakly related nodes.Eventually, after a few dozen cycles, the networkstabilizes in a configuration where only the sense nodeswith the strongest relations to other nodes in thenetwork are activated.
Because of the "winner-take-all"strategy, at most one sense node per word willultimately be activated.Our model does not use microfeatures, because, as wewill show below, the context is taken into account bythe number of nodes in the network and the extent towhich they are heavily interconnected.
So far, we donot consider the syntax of the input sentence, in order tolocus on the semantic properties ofthe model.
However, it is clear thatsyntactic information can assist inthe disambiguation process incertain cases, and a networkincluding a syntactic layer, such asthat proposed by Waltz andPol lack, would undoubtedlyenhance the model's behavior.3.2.
ResultsThe network finds the correctsense in cases where Lesk'sstrategy succeeds.
For example, ifthe input consists of pen andsheep, pen 2.1 and sheep 1 arecorrect ly  act ivated.
Moreinterestingly, the network selects" the appropriate senses in caseswhere Lesk's strategy fails.Figures 3 and 4 show the state ofthe network after being run withpen and goat, and pen and page, respectively.
Thefigures represent only the most activated part of eachnetwork after 100 cycles.
Over the course of the run,the network reinforces only a small cluster of the mostsemantically relevant words and senses, and filters outtile rest of the thousands of nodes.
The correct sense foreach word in each context (pen 2.1 with goat 1, and pen1.1 withpage 1.1) is the only one activated at the end ofthe run.This model solves the context-setting problemmentioned above without any use of microfeatures.Sense 1.1 of pen would also be activated if it appearedin the context of a large number of other words--e.g.,book, ink, inkwell, pencil, paper, write, draw, sketch,etc.--which ave a similar semantic relationship to pen.For example, figure 5 shows the state of the networkafter being run with pen and book.
It is apparent that thesubset of nodes activated is similar to those which wereactivated by page.392 4Figure 3.
State of the network after being run with "pen" and "goat"\[ are the most activated }Figure 4.
State of the network after being run with "pen" and "page"~ \[ The darker nodes \]Figure 5.
State of the network after being run with "pen" and "book"r The darker nodes \] ~~ , ook393The examples given here utilize only two words asinput, in order to show clearly the behavior of thenetwork.
In fact, the performance of the networkimproves with additional input, since additional contextcan only contribute more to the disambiguation process.For example, given the sentence The young page putthe sheep in the pen, the network correctly chooses thecorrect senses of page (2.3: "a youth in personalservice"), sheep (1), and pen (2.1).
This example isparticularly difficult, because page and sheep competeagainst each other to activate different senses of pen, asdemonstrated in the examples above.
However, theword young reinforces sense 2.3 of page, whichenables sheep to win the struggle.
Inter-sententialcontext could be used as well, by retaining the mostactivated nodes within the network during subsequentruns.By running various experiments on VLNNs, we havediscovered that when the simple models proposed so farare scaled up, several improvements are necessary.
Wehave, for instance, discovered that "gang effects"appear due to extreme imbalance among words havingfew senses and hence few connections, and wordscontaining up to 80 senses and several hundredconnections, and that therefore dampening is required.tn addition, we have found that is is necessary to treat aword node and its sense nodes as a complex, ecologicalunit rather than as separate ntities.
In our model, wordnodes corttrol the behavior of sense nodes by means ofa differential neuron that prevents, for example, a sensenode from becoming more activated than its masterword node.
Our experimentation with VLNNs has alsoshed light on the role of and need for various otherparameters, uch as thresholds, decay, etc.4.
Conclus ionThe use of word relations implicitly encoded inmachine-readable dictionaries, coupled with the neuralnetwork strategy, seems to offer a promising approachto WSD.
This approach succeeds where the Leskstrategy fails, and it does not require determining andencoding microfeatures or other semantic information.The model is also more robust than the Lesk strategy,since it does not rely on the presence or absence of aparticular word or words and can filter out some degreeof "noise" (such as inclusion of some wrong lemmasdue to lack of information about part-of-speech oroccasional activation of misleading homographs).
How-ever, there are clearly several improvements which canbe made: for instance, the part-of-speech for inputwords and words in definitions can be used to extractonly the correct lemmas from the dictionary, thefrequency of use for particular senses of each word canbe used to help choose among competing senses, andadditional knowledge can be extracted from otherdictionaries and thesauri.
It is also conceivable that thenetwork could "learn" by giving more weight to linkswhich have been heavily activated over numerous runson large samples of text.
The model we describe here isonly a first step toward a fuller understanding andrefinement of the use of VLNNs for languageprocessing, and it opens several interesting avenues forfurther application and research.ReferencesAMSLER, R. A.
(1980).
The structure of the Merriam-WebsterPocket Dictionary.
Ph.D. Dissertation, University ofTexas at Austin.BOOKMAN, L.A. (1987).
A Microfeature Based Scheme forModelling Semantics.
Proc.
IJCAI'87, Milan, ltMy, 611-14.BYRD, R. J., CALZOLARI, N., CHODOROV, M. S., KLAVANS,J.
L., NEFF, M. S., RIZK, O.
(1987) Tools and methodsfor computation',fl linguistics.
Computational Linguistics,13, 3/4, 219-240.CALZOLARI, N.(1984).
Detecting patterns ina lexical data base.COLING'84, 170-173.CItODOROW, M. S., BYRD.
R. J., HEIDORN, G. E. (1985).Extracting semantic hierarchies from a large on-linedictionary.
ACL Conf., 299-304.COTTRELL, G. W., SMALL, S. L. (1983).
A connectionistscheme for modelling word sense disambiguafion.Cognition and Brain Theory, 6, 89-120.JENSEN, K., BINOT, J.-L. (1987).
Disambiguating prepositionalphrases by using on-line dictionary definitions.Computational Linguistics, 13, 3/4, 251-260.LESK, M. (1986).
Automated Sense Disambiguafion UsingMachine-readable Dictionaries: ttow to Tell a Pine Conefrom an Ice Cream Cone.
Proc.
1986 SIGDOCConference.MARKOWITZ, J., AIILSWEDE, T., EVENS, M. (1986).Semantically significant patterns in dictionary definitions.ACL Conf., 112-119.VI'~.RONIS, J., IDE, N.M., WURBEL, N. (1989).
Extractiond'informations s6mantiques dans les dictionnaires courants,7 ~me Congr~s Reconnaissance d s Formes et lnteUigenceArtificielle, AFCET, Paris, 1381-1395.WALKER, D.E., AMSLER, R.A. (1986).
The use of machine-readable dictionaries in sublanguage analysis.
In R.GRISHMAN and R. K1TTEDGE (Eds.).
Analysing Languagein restricted omaim', Lawrence Erlbaum: Itillsdale, NJ.WALTZ, D. L., POLLACK, J.
B.
(1985).
Massively ParallelParsing: A Strongly Interactive Model of Natural LanguageInterpretation.
Cognitive Science, 9, 51-74.WILKS, Y., D. FASS, C. GUO, J. MACDONALD, T. PLATE, B.SLATOR (forthcoming).
Providing Machine TractableDictionary Tools.
In J. PUSTEOVSKY (Ed.
), Theoreticaland Computational Issues in Lexical Semantics.6394
