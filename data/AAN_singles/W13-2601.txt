Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 1?10,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsWhyisenglishsoeasytosegment?Abdellah Fourtassi1, Benjamin Bo?rschinger2,3Mark Johnson3 and Emmanuel Dupoux1(1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris(2) Department of Computing, Macquarie University(3) Department of Computational Linguistics, Heidelberg University{abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au?AbstractCross-linguistic studies on unsupervisedword segmentation have consistentlyshown that English is easier to segmentthan other languages.
In this paper, wepropose an explanation of this findingbased on the notion of segmentationambiguity.
We show that English has avery low segmentation ambiguity com-pared to Japanese and that this differencecorrelates with the segmentation perfor-mance in a unigram model.
We suggestthat segmentation ambiguity is linkedto a trade-off between syllable structurecomplexity and word length distribution.1 IntroductionDuring the course of language acquisition, in-fants must learn to segment words from continu-ous speech.
Experimental studies show that theystart doing so from around 7.5 months of age(Jusczyk and Aslin, 1995).
Further studies indi-cate that infants are sensitive to a number of wordboundary cues, like prosody (Jusczyk et al 1999;Mattys et al 1999), transition probabilities (Saf-fran et al 1996; Pelucchi et al 2009), phonotac-tics (Mattys et al 2001), coarticulation (Johnsonand Jusczyk, 2001) and combine these cues withdifferent weights (Weiss et al 2010).Computational models of word segmentationhave played a major role in assessing the relevanceand reliability of different statistical cues presentin the speech input.
Some of these models focusmainly on boundary detection, and assess differ-ent strategies to identify them (Christiansen et al1998; Xanthos, 2004; Swingley, 2005; Daland andPierrehumbert, 2011).
Other models, sometimescalled lexicon-building algorithms, learn the lexi-con and the segmentation at the same time and useknowledge about the extracted lexicon to segmentnovel utterances.
State-of-the-art lexicon-buildingsegmentation algorithms are typically reported toyield better performance than word boundary de-tection algorithms (Brent, 1999; Venkataraman,2001; Batchelder, 2002; Goldwater, 2007; John-son, 2008b; Fleck, 2008; Blanchard et al 2010).As seen in Table 1, however, the performancevaries considerably across languages with Englishwinning by a high margin.
This raises a general-izability issue for NLP applications, but also forthe modeling of language acquisition since, obvi-ously, it is not the case that in some languages,infants fail to acquire an adult lexicon.
Are theseperformance differences only due to the fact thatthe algorithms might be optimized for English?
Ordo they also reflect some intrinsic linguistic differ-ences between languages?Lang.
F-score Model ReferenceEnglish 0.89 AG Johnson (2009)Chinese 0.77 AG Johnson (2010)Spanish 0.58 DP Bigram Fleck (2008)Arabic 0.56 WordEnds Fleck (2008)Sesotho 0.55 AG Johnson (2008)Japanese 0.55 BootLex Batchelder (2002)French 0.54 NGS-u Boruta (2011)Table 1: State-of-the-art unsupervised segmentation scoresfor eight languages.The aim of the present work is to understandwhy English usually scores better than other lan-guages, as far as unsupervised segmentation isconcerned.
As a comparison point, we choseJapanese because it is among the languages thathave given the poorest word segmentation scores.In fact, Boruta et al(2011) found an F-scorearound 0.41 using both Brent (1999)?s MBDP-1and Venkataraman (2001)?s NGS-u models, andBatchelder (2002) found an F-score that goesfrom 0.40 to 0.55 depending on the corpus used.Japanese also differs typologically from Englishalong several phonological dimensions such as1number of syllabic types, phonotactic constraintsand rhythmic structure.
Although most lexicon-building segmentation algorithms do not attemptto model these dimensions, they still might be rel-evant to speech segmentation and help explain theperformance difference.The structure of the paper is as follows.
First,we present the class of lexical-building segmen-tation algorithm that we use in this paper (Adap-tor Grammar), and our English and Japanese cor-pora.
We then present data replicating the basicfinding that segmentation performance is better forEnglish than for Japanese.
We then explore the hy-pothesis that this finding is due to an intrinsic dif-ference in segmentation ambiguity in the two lan-guages, and suggest that the source of this differ-ence rests in the structure of the phonological lexi-con in the two languages.
Finally, we use these in-sights to try and reduce the gap between Japaneseand English segmentation through a modificationof the Unigram model where multiple linguisticlevels are learned jointly.2 Computational Framework andCorpora2.1 Adaptor GrammarIn this study, we use the Adaptor Grammar frame-work (Johnson et al 2007) to test different mod-els of word segmentation on English and JapaneseCorpora.
This framework makes it possible toexpress a class of hierarchical non-parametricBayesian models using an extension of probabilis-tic context-free grammars called Adaptor Gram-mar (AG).
It allows one to easily define modelsthat incorporate different assumptions about lin-guistic structure and is therefore a useful practicaltool for exploring different hypotheses about wordsegmentation (Johnson, 2008b; Johnson, 2008a;Johnson et al 2010; Bo?rschinger et al 2012).For mathematical details and a description ofthe inference procedure for AGs, we refer thereader to Johnson et al(2007).
Briefly, AG usesthe non-parametric Pitman-Yor-Process (Pitmanand Yor, 1997) which, as in Minimum Descrip-tion lengths models, finds a compact representa-tion of the input by re-using frequent structures(here, words).2.2 CorporaIn the present study, we used both Child Di-rected Speech (CDS) and Adult Directed Speech(ADS) corpora.
English CDS was derived fromthe Bernstein-Ratner corpus (Bernstein-Ratner,1987), which consists in transcribed verbal inter-action of parents with nine children between 1and 2 years of age.
We used the 9,790 utter-ances that were phonemically transcribed by Brentand Cartwright (1996).
Japanese CDS consists inthe first 10, 000 utterances of the Hamasaki cor-pus (Hamasaki, 2002).
It provides a phonemictranscript of spontaneous speech to a single childcollected from when the child was 2 up to whenit was 3.5 years old.
Both CDS corpora are avail-able from the CHILDES database (MacWhinney,2000).As for English ADS, we used the first 10,000utterances of the Buckeye Speech Corpus (Pitt etal., 2007) which consists in spontaneous conver-sations with 40 speakers in American English.
Tomake it comparable to the other corpora in thispaper, we only used the idealized phonemic tran-scription.
Finally, for Japanese ADS, we usedthe first 10,000 utterances of a phonemic tran-scription of the Corpus of Spontaneous Japanese(Maekawa et al 2000).
It consists of recordedspontaneous conversations, or public speeches indifferent fields ranging from engineering to hu-manities.
For each corpus, we present elementarystatistics in Table 2.3 Unsupervised segmentation with theUnigram Model3.1 SetupIn this experiment we used the Adaptor Gram-mar framework to implement a Unigram model ofword segmentation (Johnson et al 2007).
Thismodel has been shown to be equivalent to the orig-inal MBDP-1 segmentation model (see Goldwater(2007)).
The model is defined as:?Utterance?Word+Word?
Phoneme+?In the AG framework, an underlined non-terminal indicates that this non-terminal isadapted, i.e.
that the AG will cache (and learnprobabilities for) entire sub-trees rooted in thisnon-terminal.
Here, Word is the only unit that themodel effectively learns, and there are no depen-dencies between the words to be learned.
Thisgrammar states that an utterance must be analyzedin terms of one or more Words, where a Word is a2Corpus Child Directed Speech Adult Directed Speech?
English Japanese English JapaneseTokens?
Utterances 9, 790 10, 000 10, 000 10, 000?
Words 33, 399 27, 362 57, 185 87, 156?
Phonemes 95, 809 108, 427 183, 196 289, 264Types?
Words 1, 321 2, 389 3, 708 4, 206?
Phonemes 50 30 44 25Average Lengths?
Words per utterance 3.41 2.74 5.72 8.72?
Phonemes per utterance 9.79 10.84 18.32 28.93?
Phonemes per word 2.87 3.96 3.20 3.32Table 2 : Characteristics of phonemically transcribed corporasequence of Phonemes.We ran the model twice on each corpus for2,000 iterations with hyper-parameter samplingand we collected samples throughout the process,following the methodology of Johnson and Gold-water (2009)1.
For evaluation, we performed theirMinimum Bayes Risk decoding using the col-lected samples to get a single score.3.2 EvaluationFor the evaluation, we used the same measures asBrent (1999), Venkataraman (2001) and Goldwa-ter (2007), namely token Precision (P), Recall (R)and F-score (F).
Precision is defined as the num-ber of correct word tokens found out of all tokensposited.
Recall is the number of correct word to-kens found out of all tokens in the gold standard.The F-score is defined as the harmonic mean ofPrecision and Recall , F = 2?P?RP+R .We will refer to these scores as the segmentationscores.
In addition, we define similar measures forword boundaries and word types in the lexicon.3.3 Results and discussionThe results are shown in Table 3.
As expected,the model yields substantially better scores in En-glish than Japanese, for both CDS and ADS.
Inaddition, we found that in both languages, ADSyields slightly worse results than CDS.
This is tobe expected because ADS uses between 60% and300% longer utterances than CDS, and as a resultpresents the learner with a more difficult segmen-tation problem.
Moreover, ADS includes between1We used incremental initialization70% and 280% more word types than CDS, mak-ing it a more difficult lexical learning problem.Note, however, that despite these large differencesin corpus statistics, the difference in segmentationperformance between ADS and CDS are smallcompared to the differences between Japanese andEnglish.An error analysis on English data shows thatmost errors come from the Unigrammodel mistak-ing high frequency collocations for single words(see also Goldwater (2007)).
This leads to anunder-segmentation of chunks like ?a boy?
or ?isit?
2.
Yet, the model also tends to break off fre-quent morphological affixes, especially ?-ing?
and?-s?
, leading to an over-segmentation of wordslike ?talk ing?
or ?black s?.Similarly, Japanese data shows both over-and under-segmentation errors.
However, over-segmentation is more severe than for English, asit does not only affect affixes, but surfaces asbreaking apart multi-syllabic words.
In addition,Japanese segmentation faces another kind of er-ror which acts across word boundaries.
For exam-ple, ?ni kashite?
is segmented as ?nika shite?
and?nurete inakatta?
as ?nure tei na katta?.
This leadsto an output lexicon that, on the one hand, allowsfor a more compact analysis of the corpus thanthe true lexicon: the number of word types dropsfrom 2,389 to 1,463 in CDS and from 4,206 to2,372 in ADS although the average token length ?and consequently, overall number of tokens ?
doesnot change as dramatically, dropping from 3.96 to2For ease of presentation, we use orthography to presentexamples although all experiments are run on phonemic tran-scripts.3?
Child Directed Speech Adult Directed Speech?
English Japanese English Japanese?
F P R F P R F P R F P RSegmentation 0.77 0.76 0.77 0.55 0.51 0.61 0.69 0.66 0.73 0.50 0.48 0.52Boundaries 0.87 0.87 0.88 0.72 0.63 0.83 0.86 0.81 0.91 0.76 0.74 0.79Lexicon 0.62 0.65 0.59 0.33 0.43 0.26 0.41 0.48 0.36 0.30 0.42 0.23Table 3 : Word segmentation scores of the Unigram model3.31 for CDS and from 3.32 to 3.12 in ADS.
Onthe other hand, however, most of the output lex-icon items are not valid Japanese words and thisleads to the bad lexicon F-scores.
This, in turn,leads to the bad overall segmentation performance.In brief, we have shown that, across two dif-ferent corpora, English yields consistently bettersegmentation results than Japanese for the Uni-gram model.
This confirms and extends the resultsof Boruta et al(2011) and Batchelder (2002).
Itstrongly suggests that the difference is neither dueto a specific choice of model nor to particularitiesof the corpora, but reflects a fundamental propertyof these two languages.In the following section, we introduce the no-tion of segmentation ambiguity, it to English andJapanese data, and show that it correlates with seg-mentation performance.4 Intrinsic Segmentation AmbiguityLexicon-based segmentation algorithms likeMBDP-1, NGS-u and the AG Unigram modellearn the lexicon and the segmentation at thesame time.
This makes it difficult, in case ofpoor performance, to see whether the problemcomes from the intrinsic segmentability of thelanguage or from the quality of the extractedlexicon.
Our claim is that Japanese is intrinsicallymore difficult to segment than English, even whena good lexicon is already assumed.
We explorethis hypothesis by studying segmentation alone,assuming a perfect (Gold) lexicon.4.1 Segmentation ambiguityWithout any information, a string of N phonemescould be segmented in 2N?1 ways.
When a lexi-con is provided, the set of possible segmentationsis reduced to a smaller number.
To illustrate this,suppose we have to segment the input utterance:/ay s k r iy m/ 3, and that the lexicon contains thefollowing words : /ay/ (I), /s k r iy m/ (scream),/ay s/ (ice), /k r iy m/ (cream).
Only two segmen-tations are possible : /ay skriym/ (I scream) and/ays kriym/ (ice cream).We are interested in the ambiguity generated bythe different possible parses that result from such asupervised segmentation.
In order to quantify thisidea in general, we define a Normalized Segmenta-tion Entropy.
To do this, we need to assign a prob-ability to every possible segmentation.
To this end,we use a unigram model where the probability of alexical item is its normalized frequency in the cor-pus and the probability of a parse is the productof the probabilities of its terms.
In order to obtaina measure that does not depend on the utterancelength, we normalize by the number of possibleboundaries in the utterance.
So for an utterance oflength N , the Normalized Segmentation Entropy(NSE) is computed using Shannon formula (Shan-non, 1948) as follows:?NSE = ?
?i Pilog2(Pi)/(N ?
1)?where Pi is the probability of the parse i .For CDS data we found Normalized Segmen-tation Entropies of 0.0021 bits for English and0.0156 bits for Japanese.
In ADS data wefound similar results with 0.0032 bits for Englishand 0.0275 bits for Japanese.
This means thatJapanese needs between 7 and 8 times more bitsthan English to encode segmentation information.This is a very large difference, which is of thesame magnitude in CDS and ADS.
These differ-ences clearly show that intrinsically, Japanese ismore ambiguous than English with regards to seg-mentation.One can refine this analysis by distinguishingtwo sources of ambiguity: ambiguity across wordboundaries, as in ?ice cream / [ay s] [k r iy m]?3We use ARPABET notation to represent phonemic input.4Figure 1 : Correlation between Normalized Segmentation Entropy (in bits) and the segmentation F-score for CDS (left) andADS (Right)vs ?I scream / [ay] [s k r iy m]?.
And ambigu-ity within the lexicon, that occurs when a lexicalitem is composed of two or more sub-words (likein ?Butterfly?
).Since we are mainly investigating lexicon-building models, it is important to measure the am-biguity within the lexicon itself, in the ideal casewhere this lexicon is perfect.
To this end, we com-puted the average number of segmentations for alexicon item.
For example, the word ?butterfly?has two possible segmentations : the original word?butterfly?
and a segmentation comprising the twosub-words : ?butter?
and ?fly?.
For English to-kens, we found an average of 1.039 in CDS and1.057 in ADS.
For Japanese tokens, we found anaverage of 1.811 in CDS and 1.978 in ADS.
En-glish?s averages are close to 1, indicating that itdoesn?t exhibit lexicon ambiguity.
Japanese, how-ever, has averages close to 2 which means that lex-ical ambiguity is quite systematic in both CDS andADS.4.2 Segmentation ambiguity and supervisedsegmentationThe intrinsic ambiguity in Japanese only showsthat a given sentence has multiple possible seg-mentations.
What remains to be demonstrated isthat these multiple segmentations result in system-atic segmentation errors.
To do this we proposea supervised segmentation algorithm that enumer-ates all possible segmentations of an utterancebased on the gold lexicon, and selects the segmen-tation with the highest probability.
In CDS data,this algorithm yields a segmentation F-score equalto 0.99 for English and 0.95 for Japanese.
In ADSwe find an F-score of 0.96 for English and 0.93 forJapanese.
These results show that lexical informa-tion alone plus word frequency eliminates almostall segmentation errors in English, especially forCDS.
As for Japanese, even if the scores remainimpressively high, the lexicon alone is not suffi-cient to eliminate all the errors.
In other words,even with a gold lexicon, English remains easierto segment than Japanese.To quantify the link between segmentation en-tropy and segmentation errors, we binned the sen-tences of our corpus in 10 bins according to theNormalized Segmentation Entropy, and correlatethis with the average segmentation F-score foreach bin.
As shown Figure 1, we found significantcorrelations: (R = ?0.86, p < 0.001) for CDSand (R = ?0.93, p < 0.001) for ADS, showingthat segmentation ambiguity has a strong effecteven on supervised segmentation scores.
The cor-relation within language was also significant butonly in the Japanese data : R = ?0.70 for CDSand R = ?0.62 for ADS.
?Next, we explore one possible reason for thisstructural difference between Japanese and En-glish, especially at the level of the lexicon.4.3 Syllable structure and lexicalcomposition of Japanese and EnglishOne of the most salient differences between En-glish and Japanese phonology concerns their syl-lable structure.
This is illustrated in Figure 2(above), where we plotted the frequency of the dif-ferent syllabic structures of monosyllabic tokensin English and Japanese CDS.
The statistics showthat English has a very rich syllabic compositionwhere a diversity of consonant clusters is allowed,whereas Japanese syllable structure is quite simpleand mostly composed of the default CV type.
Thisdifference is bound to have an effect on the struc-ture of the lexicon.
Indeed, Japanese has to use5Figure 2 : Trade-off between the complexity of syllable structure (above) and the word token length in terms of syllables(below) for English and Japanese CDS.multisyllabic words in order to achieve a large sizelexicon, whereas, in principle, English could usemostly monosyllables.
In Figure 2 (below) we dis-play the distribution of word length as measuredin syllables in the two languages for the CDS cor-pora.
The English data is indeed mostly composedof mono-syllabic words whereas the Japanese oneis made of words of more varied lengths.
Overall,we have documented a trade-off between the di-versity of syllable structure on the one hand, andthe diversity of word lengths on the other (see Ta-ble 4 for a summary of this tradeoff expressed interms of entropy).?
CDS ADS?
Eng.
Jap.
Eng.
Jap.Syllable types 2.40 1.38 2.58 1.03Token lengths 0.62 2.04 0.99 1.69Table 4 : Entropies of syllable types and token lengths interms of syllables (in bits)We suggest that this trade-off is responsible forthe difference in the lexicon ambiguity across thetwo languages.
Specifically, the combination ofa small number of syllable types and, as a conse-quence, the tendency for multi-syllabic word typesin Japanese makes it likely that a long word willbe composed of smaller ones.
This cannot happenvery often in English, since most words are mono-syllabic, and words smaller than a syllable are notallowed.5 Improving Japanese unsupervisedsegmentationWe showed in the previous section that ambigu-ity impacts segmentation even with a gold lexicon,mainly because the lexicon itself could be ambigu-ous.
In an unsupervised segmentation setting, theproblem is worse because ambiguity within andacross word boundaries leads to a bad lexicon,which in turn results in more segmentation errors.In this section, we explore the possibility of miti-gating some of these negative consequences.In section 3, we saw that when the Unigrammodel tries to learn Japanese words, it produces anoutput lexicon composed of both over- and under-segmented words in addition to words that re-sult from a segmentation across word boundaries.One way to address this is by learning multiplekinds of units jointly, rather than just words; in-deed, previous work has shown that richer mod-els with multiple levels improve segmentation forEnglish (Johnson, 2008a; Johnson and Goldwater,2009).5.1 Two dependency levelsAs a first step, we will allow the model to notjust learn words but to also memorize sequences ofwords.
Johnson (2008a) introduced these units as?collocations?
but we choose to use the more neu-tral notion of level for reasons that become clearshortly.
Concretely, the grammar is:6?
CDS ADS?
English Japanese English Japanese?
F P R F P R F P R F P RLevel 1?
Segmentation 0.81 0.77 0.86 0.42 0.33 0.55 0.70 0.63 0.78 0.42 0.35 0.50?
Boundaries 0.91 0.84 0.98 0.63 0.47 0.96 0.86 0.76 0.98 0.73 0.61 0.90?
Lexicon 0.64 0.79 0.54 0.18 0.55 0.10 0.36 0.56 0.26 0.15 0.68 0.08Level 2?
Segmentation 0.33 0.45 0.26 0.59 0.65 0.53 0.50 0.60 0.43 0.45 0.54 0.38?
Boundaries 0.56 0.98 0.40 0.71 0.87 0.60 0.76 0.95 0.64 0.73 0.92 0.60?
Lexicon 0.36 0.25 0.59 0.47 0.44 0.49 0.46 0.38 0.56 0.43 0.37 0.50Table 5 : Word segmentation scores of the two levels model?Utterance?
level2+level2?
level1+level1?
Phoneme+?We run this model under the same conditionsas the Unigram model but evaluate two differentsituations.
The model has no inductive bias thatwould force it to equate level1 with words, ratherthan level2.
Consequently, we evaluate the seg-mentation that is the result of taking there to be aboundary between every level1 constituent (Level1 in Table 5) and between every level2 constituent(Level 2 in Table 5 ).
From these results , we seethat English data has better scores when the lowerlevel represents the Word unit and when the higherlevel captures regularities above the word.
How-ever, Japanese data is best segmented when thehigher level is the Word unit and the lower levelcaptures sub-word regularities.Level 1 generally tends to over-segment utter-ances as can be seen by comparing the BoundaryRecall and Precision scores (Goldwater, 2007).
Infact when the Recall is much higher than the Pre-cision, we can say that the model has a tendencyto over-segment.
Conversely, we see that Level 2tends to under-segment utterances as the Bound-ary Precision is higher than the Recall.Over-segmentation at Level 1 seems to benefitEnglish since it counteracts the tendency of theUnigram model to cluster high frequency colloca-tions.
As far as segmentation is concerned, thiseffect seems to outweigh the negative effect ofbreaking words apart (especially in CDS), as En-glish words are mostly monosyllabic.For Japanese, under-segmentation at Level 2seems to be slightly less harmful than over-segmentation at Level 1, as it prevents, to someextent, multi-syllabic words to be split.
However,the scores are not very different from the ones wehad with the Unigram model and slightly worsefor the ADS.
What seems to be missing is an inter-mediate level where over- and under-segmentationwould counteract one another.5.2 Three dependency levelsWe add a third dependency level to our model asfollows :?Utterance?
level3+level3?
level2+level2?
level1+level1?
Phoneme+?As with the previous model, we test each of thethree levels as the word unit, the results are shownin Table 6.Except for English CDS, all the corporahave their best scores with this intermediatelevel.
Level 1 tends to over-segment Japaneseutterances into syllables and English utterancesinto morphemes.
Level 3, however, tends tohighly under-segment both languages.
EnglishCDS seems to be already under-segmented atLevel 2, very likely caused by the large numberof word collocations like ?is-it?
and ?what-is?,an observation also made by Bo?rschinger et al(2012) using different English CDS corpora.English ADS is quantitatively more sensitive toover-segmentation than CDS mainly because ithas a richer morphological structure and relativelylonger words in terms of syllables (Table 4).7?
CDS ADS?
English Japanese English Japanese?
F P R F P R F P R F P RLevel 1?
Segmentation 0.79 0.74 0.85 0.27 0.20 0.41 0.35 0.28 0.48 0.37 0.30 0.47?
Boundaries 0.89 0.81 0.99 0.56 0.39 0.99 0.68 0.52 0.99 0.70 0.57 0.93?
Lexicon 0.58 0.76 0.46 0.10 0.47 0.05 0.13 0.39 0.07 0.10 0.70 0.05Level 2?
Segmentation 0.49 0.60 0.42 0.70 0.70 0.70 0.77 0.76 0.79 0.60 0.65 0.55?
Boundaries 0.71 0.97 0.56 0.81 0.82 0.81 0.90 0.88 0.92 0.81 0.90 0.74?
Lexicon 0.51 0.41 0.64 0.53 0.59 0.47 0.58 0.69 0.50 0.51 0.57 0.46Level 3?
Segmentation 0.18 0.31 0.12 0.39 0.53 0.30 0.43 0.55 0.36 0.28 0.42 0.21?
Boundaries 0.26 0.99 0.15 0.46 0.93 0.31 0.71 0.98 0.55 0.59 0.96 0.43?
Lexicon 0.17 0.10 0.38 0.32 0.25 0.41 0.37 0.28 0.51 0.27 0.20 0.42Table 6 : Word segmentation scores of the three levels model6 ConclusionIn this paper we identified a property of lan-guage, segmentation ambiguity, which we quan-tified through Normalized Segmentation Entropy.We showed that this quantity predicts performancein a supervised segmentation task.With this tool we found that English was in-trinsically less ambiguous than Japanese, account-ing for the systematic difference found in this pa-per.
More generally, we suspect that SegmentationAmbiguity would, to some extent, explain muchof the difference observed across languages (Ta-ble 1).
Further work needs to be carried out to testthe robustness of this hypothesis on a larger scale.We showed that allowing the system to learnat multiple levels of structure generally improvesperformance, and compensates partially for thenegative effect of segmentation ambiguity on un-supervised segmentation (where a bad lexicon am-plifies the effect of segmentation ambiguity).
Yet,we end up with a situation where the best level ofstructure may not be the same across corpora orlanguages, which raises the question as to how todetermine which level is the correct lexical level,i.e., the level that can sustain successful grammat-ical and semantic learning.
Further research isneeded to answer this question.Generally speaking, ambiguity is a challenge inmany speech and language processing tasks: forexample part-of-speech tagging and word sensedisambiguation tackle lexical ambiguity, proba-bilistic parsing deals with syntactic ambiguity andspeech act interpretation deals with pragmatic am-biguities.
However, to our knowledge, ambiguityhas rarely been considered as a serious problem inword segmentation tasks.As we have shown, the lexicon-based approachdoes not completely solve the segmentation am-biguity problem since the lexicon itself could bemore or less ambiguous depending on the lan-guage.
Evidently, however, infants in all lan-guages manage to overcome this ambiguity.
It hasto be the case, therefore, that they solve this prob-lem through the use of alternative strategies, forinstance by relying on sub-lexical cues (see Jaroszand Johnson (2013)) or by incorporating semanticor syntactic constraints (Johnson et al 2010).
Itremains a major challenge to integrate these strate-gies within a common model that can learn withcomparable performance across typologically dis-tinct languages.AcknowledgementsThe research leading to these results has received fundingfrom the European Research Council (FP/2007-2013) / ERCGrant Agreement n. ERC-2011-AdG-295810 BOOTPHON,from the Agence Nationale pour la Recherche (ANR-2010-BLAN-1901-1 BOOTLANG, ANR-11-0001-02 PSL* andANR-10-LABX-0087) and the Fondation de France.
Thisresearch was also supported under the Australian ResearchCouncil?s Discovery Projects funding scheme (project num-bers DP110102506 and DP110102593).8ReferencesEleanor Olds Batchelder.
2002.
Bootstrapping the lex-icon: A computational model of infant speech seg-mentation.
Cognition, 83(2):167?206.N.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck,editors, Children?s Language, volume 6.
Erlbaum,Hillsdale, NJ.Daniel Blanchard, Jeffrey Heinz, and RobertaGolinkoff.
2010.
Modeling the contribution ofphonotactic cues to the problem of word segmenta-tion.
Journal of Child Language, 37(3):487?511.Benjamin Bo?rschinger, Katherine Demuth, and MarkJohnson.
2012.
Studying the effect of input sizefor Bayesian word segmentation on the Providencecorpus.
In Proceedings of the 24th InternationalConference on Computational Linguistics (Coling2012), pages 325?340, Mumbai, India.
Coling 2012Organizing Committee.Luc Boruta, Sharon Peperkamp, Beno?
?t Crabbe?, andEmmanuel Dupoux.
2011.
Testing the robustnessof online word segmentation: Effects of linguisticdiversity and phonetic variation.
In Proceedings ofthe 2nd Workshop on Cognitive Modeling and Com-putational Linguistics, pages 1?9, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.M.
Brent and T. Cartwright.
1996.
Distributional regu-larity and phonotactic constraints are useful for seg-mentation.
Cognition, 61:93?125.M.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.Machine Learning, 34:71?105.Morten H Christiansen, Joseph Allen, and Mark S Sei-denberg.
1998.
Learning to segment speech usingmultiple cues: A connectionist model.
Languageand cognitive processes, 13(2-3):221?268.Robert Daland and Janet B Pierrehumbert.
2011.Learning diphone-based segmentation.
CognitiveScience, 35(1):119?155.Margaret M. Fleck.
2008.
Lexicalized phonotac-tic word segmentation.
In Proceedings of ACL-08:HLT, pages 130?138, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.Sharon Goldwater.
2007.
Nonparametric BayesianModels of Lexical Acquisition.
Ph.D. thesis, BrownUniversity.Naomi Hamasaki.
2002.
The timing shift of two-year-olds responses to caretakers yes/no questions.
InStudies in language sciences (2)Papers from the 2ndAnnual Conference of the Japanese Society for Lan-guage Sciences, pages 193?206.Gaja Jarosz and J Alex Johnson.
2013.
The richnessof distributional cues to word boundaries in speechto young children.
Language Learning and Devel-opment, (ahead-of-print):1?36.Mark Johnson and Katherine Demuth.
2010.
Unsuper-vised phonemic Chinese word segmentation usingAdaptor Grammars.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics (Coling 2010), pages 528?536, Beijing, China,August.
Coling 2010 Organizing Committee.Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparameteric Bayesian inference: exper-iments on unsupervised word segmentation withadaptor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, pages 317?325,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Elizabeth K. Johnson and Peter W. Jusczyk.
2001.Word segmentation by 8-month-olds: When speechcues count more than statistics.
Journal of Memoryand Language, 44:1?20.Mark Johnson, Thomas Griffiths, and Sharon Gold-water.
2007.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 139?146, Rochester, New York.
Associ-ation for Computational Linguistics.Mark Johnson, Katherine Demuth, Michael Frank, andBevan Jones.
2010.
Synergies in learning wordsand their referents.
In J. Lafferty, C. K. I. Williams,J.
Shawe-Taylor, R.S.
Zemel, and A. Culotta, ed-itors, Advances in Neural Information ProcessingSystems 23, pages 1018?1026.Mark Johnson.
2008a.
Unsupervised word segmen-tation for Sesotho using Adaptor Grammars.
InProceedings of the Tenth Meeting of ACL SpecialInterest Group on Computational Morphology andPhonology, pages 20?27, Columbus, Ohio, June.Association for Computational Linguistics.Mark Johnson.
2008b.
Using Adaptor Grammars toidentify synergies in the unsupervised acquisition oflinguistic structure.
In Proceedings of the 46th An-nual Meeting of the Association of ComputationalLinguistics, pages 398?406, Columbus, Ohio.
Asso-ciation for Computational Linguistics.Peter W Jusczyk and Richard N Aslin.
1995.
Infantsdetection of the sound patterns of words in fluentspeech.
Cognitive psychology, 29(1):1?23.Peter W. Jusczyk, E. A. Hohne, and A. Bauman.1999.
Infants?
sensitivity to allophonic cues forword segmentation.
Perception and Psychophysics,61:1465?1476.9Brian MacWhinney.
2000.
The CHILDES Project:Tools for Analyzing Talk.
Transcription, format andprograms, volume 1.
Lawrence Erlbaum.Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-toshi Isahara.
2000.
Spontaneous speech corpus ofjapanese.
In proc.
LREC, volume 2, pages 947?952.Sven L Mattys, Peter W Jusczyk, Paul A Luce, James LMorgan, et al1999.
Phonotactic and prosodic ef-fects on word segmentation in infants.
Cognitivepsychology, 38(4):465?494.Sven L Mattys, Peter W Jusczyk, et al2001.
Doinfants segment words or recurring contiguous pat-terns?
Journal of experimental psychology, humanperception and performance, 27(3):644?655.Bruna Pelucchi, Jessica F Hay, and Jenny R Saffran.2009.
Learning in reverse: Eight-month-old infantstrack backward transitional probabilities.
Cognition,113(2):244?247.J.
Pitman and M. Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
Annals of Probability, 25:855?900.M.
A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-mond, E. Hume, and Fosler-Lussier.
2007.
Buckeyecorpus of conversational speech.J.
Saffran, R. Aslin, and E. Newport.
1996.
Sta-tistical learning by 8-month-old infants.
Science,274:1926?1928.Claude Shannon.
1948.
A mathematical theory ofcommunication.
Bell System Technical Journal,27(3):379?423.Daniel Swingley.
2005.
Statistical clustering and thecontents of the infant vocabulary.
Cognitive Psy-chology, 50:86?132.A.
Venkataraman.
2001.
A statistical model for worddiscovery in transcribed speech.
ComputationalLinguistics, 27(3):351?372.Daniel J Weiss, Chip Gerfen, and Aaron D Mitchel.2010.
Colliding cues in word segmentation: therole of cue strength and general cognitive processes.Language and Cognitive Processes, 25(3):402?422.Aris Xanthos.
2004.
Combining utterance-boundaryand predictability approaches to speech segmenta-tion.
In First Workshop on Psycho-computationalModels of Human Language Acquisition, page 93.10
