Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 212?217,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsResolving Lexical Ambiguity inTensor Regression Models of MeaningDimitri KartsaklisUniversity of OxfordDepartment ofComputer ScienceWolfson Bldg, Parks RoadOxford, OX1 3QD, UKdimitri.kartsaklis@cs.ox.ac.ukNal KalchbrennerUniversity of OxfordDepartment ofComputer ScienceWolfson Bldg, Parks RoadOxford, OX1 3QD, UKnkalch@cs.ox.ac.ukMehrnoosh SadrzadehQueen Mary Univ.
of LondonSchool of Electronic Engineeringand Computer ScienceMile End RoadLondon, E1 4NS, UKmehrnoosh.sadrzadeh@qmul.ac.ukAbstractThis paper provides a method for improv-ing tensor-based compositional distribu-tional models of meaning by the additionof an explicit disambiguation step prior tocomposition.
In contrast with previous re-search where this hypothesis has been suc-cessfully tested against relatively simplecompositional models, in our work we usea robust model trained with linear regres-sion.
The results we get in two experi-ments show the superiority of the prior dis-ambiguation method and suggest that theeffectiveness of this approach is model-independent.1 IntroductionThe provision of compositionality in distributionalmodels of meaning, where a word is represented asa vector of co-occurrence counts with every otherword in the vocabulary, offers a solution to thefact that no text corpus, regardless of its size, iscapable of providing reliable co-occurrence statis-tics for anything but very short text constituents.By composing the vectors for the words withina sentence, we are still able to create a vectorialrepresentation for that sentence that is very usefulin a variety of natural language processing tasks,such as paraphrase detection, sentiment analysisor machine translation.
Hence, given a sentencew1w2.
.
.
wn, a compositional distributional modelprovides a function f such that:?
?s = f(??w1,?
?w2, .
.
.
,?
?wn) (1)where?
?wiis the distributional vector of the ithword in the sentence and?
?s the resulting compos-ite sentential vector.An interesting question that has attracted the at-tention of researchers lately refers to the way inwhich these models affect ambiguous words; inother words, given a sentence such as ?a man waswaiting by the bank?, we are interested to know towhat extent a composite vector can appropriatelyreflect the intended use of word ?bank?
in that con-text, and how such a vector would differ, for exam-ple, from the vector of the sentence ?a fishermanwas waiting by the bank?.Recent experimental evidence (Reddy et al,2011; Kartsaklis et al, 2013; Kartsaklis andSadrzadeh, 2013) suggests that for a number ofcompositional models the introduction of a dis-ambiguation step prior to the actual composi-tional process results in better composite represen-tations.
In other words, the suggestion is that Eq.1 should be replaced by:?
?s = f(?(?
?w1), ?(?
?w2), .
.
.
, ?(?
?wn)) (2)where the purpose of function ?
is to return a dis-ambiguated version of each word vector given therest of the context (e.g.
all the other words in thesentence).
The composition operation, whateverthat could be, is then applied on these unambigu-ous representations of the words, instead of theoriginal distributional vectors.Until now this idea has been verified on rela-tively simple compositional functions, usually in-volving some form of element-wise operation be-tween the word vectors, such as addition or mul-tiplication.
An exception to this is the work ofKartsaklis and Sadrzadeh (2013), who apply Eq.2 on partial tensor-based compositional models.In a tensor-based model, relational words suchas verbs and adjectives are represented by multi-linear maps; composition takes place as the ap-plication of those maps on vectors representingthe arguments (usually nouns).
What makes themodels of the above work ?partial?
is that the au-thors used simplified versions of the linear maps,projected onto spaces of order lower than that re-quired by the theoretical framework.
As a result,a certain amount of transformational power wastraded off for efficiency.A potential explanation then for the effective-ness of the proposed prior disambiguation methodcan be sought on the limitations imposed by thecompositional models under test.
After all, theidea of having disambiguation emerge as a direct212consequence of the compositional process, with-out the introduction of any explicit step, seemsmore natural and closer to the way the humanmind resolves lexical ambiguities.The purpose of this paper is to investigatethe hypothesis whether prior disambiguation isimportant in a pure tensor-based compositionalmodel, where no simplifying assumptions havebeen made.
We create such a model by using lin-ear regression, and we explain how an explicit dis-ambiguation step can be introduced to this modelprior to composition.
We then proceed by com-paring the composite vectors produced by this ap-proach with those produced by the model alone ina number of experiments.
The results show a clearsuperiority of the priorly disambiguated modelsfollowing Eq.
2, confirming previous research andsuggesting that the reasons behind the success ofthis approach are more fundamental than the formof the compositional function.2 Composition in distributional modelsCompositional distributional models of meaningvary in sophistication, from simple element-wiseoperations between vectors such as addition andmultiplication (Mitchell and Lapata, 2008) to deeplearning techniques based on neural networks(Socher et al, 2011; Socher et al, 2012; Kalch-brenner and Blunsom, 2013a).
Tensor-based mod-els, formalized by Coecke et al (2010), comprisea third class of models lying somewhere in be-tween these two extremes.
Under this setting rela-tional words such as verbs and adjectives are rep-resented by multi-linear maps (tensors of variousorders) acting on a number of arguments.
An ad-jective for example is a linear map f : N ?
N(where N is our basic vector space for nouns),which takes as input a noun and returns a mod-ified version of it.
Since every map of this sortcan be represented by a matrix living in the ten-sor product space N ?
N , we now see that themeaning of a phrase such as ?red car?
is given byred ??
?car, where red is an adjective matrix and?
indicates matrix multiplication.
The same con-cept applies for functions of higher order, such asa transitive verb (a function of two arguments, soa tensor of order 3).
For these cases, matrix mul-tiplication generalizes to the more generic notionof tensor contraction.
The meaning of a sentencesuch as ?kids play games?
is computed as:???kidsT?
play ?????
?games (3)where play here is an order-3 tensor (a ?cube?
)and ?
now represents tensor contraction.
A con-cise introduction to compositional distributionalmodels can be found in (Kartsaklis, 2014).3 Disambiguation and compositionThe idea of separating disambiguation from com-position first appears in a work of Reddy et al(2011), where the authors show that the intro-duction of an explicit disambiguation step priorto simple element-wise composition is beneficialfor noun-noun compounds.
Subsequent work byKartsaklis et al (2013) reports very similar find-ings for verb-object structures, again on additiveand multiplicative models.
Finally, in (Kartsaklisand Sadrzadeh, 2013) these experiments were ex-tended to include tensor-based models followingthe categorical framework of Coecke et al (2010),where again all ?unambiguous?
models presentsuperior performance compared to their ?ambigu-ous?
versions.However, in this last work one of the dimen-sions of the tensors was kept empty (filled inwith zeros).
This simplified the calculations butalso weakened the effectiveness of the multi-linearmaps.
If, for example, instead of using an order-3tensor for a transitive verb, one uses some of thematrix instantiations of Kartsaklis and Sadrzadeh,Eq.
3 is reduced to one of the following forms:play  (???kids?????
?games) ,??
?kids (play ??????games)(???kidsT?
play)????
?games(4)where symbol  denotes element-wise multipli-cation and play is a matrix.
Here, the model doesnot fully exploit the space provided by the theo-retical framework (i.e.
an order-3 tensor), whichhas two disadvantages: firstly, we lose space thatcould hold valuable information about the verb inthis case and relational words in general; secondly,the generally non-commutative tensor contractionoperation is now partly relying on element-wisemultiplication, which is commutative, thus forgets(part of the) order of composition.In the next section we will see how to apply lin-ear regression in order to create full tensors forverbs and use them for a compositional model thatavoids these pitfalls.4 Creating tensors for verbsThe essence of any tensor-based compositionalmodel is the way we choose to create our sentence-producing maps, i.e.
the verbs.
In this paper weadopt a method proposed by Baroni and Zampar-elli (2010) for building adjective matrices, whichcan be generally applied to any relational word.213In order to create a matrix for, say, the intransi-tive verb ?play?, we first collect all instances ofthe verb occurring with some subject in the train-ing corpus, and then we create non-compositionalholistic vectors for these elementary sentences fol-lowing exactly the same methodology as if theywere words.
We now have a dataset with instancesof the form ?????subji,???????subjiplay?
(e.g.
the vector of?kids?
paired with the holistic vector of ?kids play?,and so on), that can be used to train a linear regres-sion model in order to produce an appropriate ma-trix for verb ?play?.
The premise of a model likethis is that the multiplication of the verb matrixwith the vector of a new subject will produce a re-sult that approximates the distributional behaviourof all these elementary two-word exemplars usedin training.We present examples and experiments basedon this method, constructing ambiguous and dis-ambiguated tensors of order 2 (that is, matrices)for verbs taking one argument.
In principle, ourmethod is directly applicable to tensors of higherorder, following a multi-step process similar tothat of Grefenstette et al (2013) who create order-3 tensors for transitive verbs using similar means.Instead of using subject-verb constructs as abovewe concentrate on elementary verb phrases of theform verb-object (e.g.
?play football?, ?admit stu-dent?
), since in general objects comprise strongercontexts for disambiguating the usage of a verb.5 Experimental settingOur basic vector space is trained from the ukWaCcorpus (Ferraresi et al, 2008), originally using asa basis the 2,000 content words with the highestfrequency (but excluding a list of stop words aswell as the 50 most frequent content words sincethey exhibit low information content).
We cre-ated vectors for all content words with at least100 occurrences in the corpus.
As context weconsidered a 5-word window from either side ofthe target word, while as our weighting schemewe used local mutual information (i.e.
point-wisemutual information multiplied by raw counts).This initial semantic space achieved a score of0.77 Spearman?s ?
(and 0.71 Pearson?s r) on thewell-known benchmark dataset of Rubenstein andGoodenough (1965).
In order to reduce the time ofregression training, our vector space was normal-ized and projected onto a 300-dimensional spaceusing singular value decomposition (SVD).
Theperformance of the reduced space on the R&Gdataset was again very satisfying, specifically 0.73Spearman?s ?
and 0.72 Pearson?s r.In order to create the vector space of the holisticverb phrase vectors, we first collected all instanceswhere a verb participating in the experiments ap-peared at least 100 times in a verb-object relation-ship with some noun in the corpus.
As context ofa verb phrase we considered any content word thatfalls into a 5-word window from either side of theverb or the object.
For the 68 verbs participatingin our experiments, this procedure resulted in 22kverb phrases, a vector space that again was pro-jected into 300 dimensions using SVD.Linear regression For each verb we use simplelinear regression with gradient descent directly ap-plied on matrices X and Y, where the rows of Xcorrespond to vectors of the nouns that appear asobjects for the given verb and the rows ofY to theholistic vectors of the corresponding verb phrases.Our objective function then becomes:?W = argminW12m(?WXT?YT?2+ ?
?W?2)(5)wherem is the number of training examples and ?a regularization parameter.
The matrix W is usedas the tensor for the specific verb.6 Supervised disambiguationIn our first experiment we test the effectivenessof a prior disambiguation step for a tensor-basedmodel in a ?sandbox?
using supervised learning.The goal is to create composite vectors for a num-ber of elementary verb phrases of the form verb-object with and without an explicit disambiguationstep, and evaluate which model approximates bet-ter the holistic vectors of these verb phrases.The verb phrases of our dataset are based on the5 ambiguous verbs of Table 1.
Each verb has beencombined with two different sets of nouns that ap-pear in a verb-object relationship with that verbin the corpus (a total of 343 verb phrases).
Thenouns of each set have been manually selected inorder to explicitly represent a different meaning ofthe verb.
As an example, in the verb ?play?
we im-pose the two distinct meanings of using a musicalinstrument and participating in a sport; so the firstVerb Meaning 1 Meaning 2break violate (56) break (22)catch capture (28) be on time (21)play musical instrument (47) sports (29)admit permit to enter (12) acknowledge (25)draw attract (64) sketch (39)Table 1: Ambiguous verbs for the supervised task.The numbers in parentheses refer to the collectedtraining examples for each case.214set of objects contains nouns such as ?oboe?, ?pi-ano?, ?guitar?, and so on, while in the second setwe see nouns such as ?football?, ?baseball?
etc.In more detail, the creation of the dataset wasdone in the following way: First, all verb entrieswith more than one definition in the Oxford JuniorDictionary (Sansome et al, 2000) were collectedinto a list.
Next, a linguist (native speaker of En-glish) annotated the semantic difference betweenthe definitions of each verb in a scale from 1 (sim-ilar) to 5 (distinct).
Only verbs with definitionsexhibiting completely distinct meanings (markedwith 5) were kept for the next step.
For each oneof these verbs, a list was constructed with all thenouns that appear at least 50 times under a verb-object relationship in the corpus with the specificverb.
Then, each object in the list was manuallyannotated as exclusively belonging to one of thetwo senses; so, an object could be selected only ifit was related to a single sense, but not both.
Forexample, ?attention?
was a valid object for the at-tract sense of verb ?draw?, since it is unrelated tothe sketch sense of that verb.
On the other hand,?car?
is not an appropriate object for either senseof ?draw?, since it could actually appear under bothof them in different contexts.
The verbs of Table1 were the ones with the highest numbers of ex-emplars per sense, creating a dataset of significantsize for the intended task (each holistic vector iscompared with 343 composite vectors).We proceed as follows: We apply linear regres-sion in order to train verb matrices using jointlythe object sets for both meanings of each verb, aswell as separately?so in this latter case we gettwo matrices for each verb, one for each sense.
Foreach verb phrase, we create a composite vector bymatrix-multiplying the verb matrix with the vectorof the specific object.
Then we use 4-fold crossvalidation to evaluate which version of compositevectors (the one created by the ambiguous tensorsor the one created by the unambiguous ones) ap-proximates better the holistic vectors of the verbphrases in our test set.
This is done by comparingeach holistic vector with all the composite ones,and then evaluating the rank of the correct com-posite vector within the list of results.In order to get a proper mixing of objects fromboth senses of a verb in training and testing sets,we set the cross-validation process as follows: Wefirst split both sets of objects in 4 parts.
For eachfold then, our training set is comprised by34of set#1 plus34of set #2, while the test set consists ofthe remaining14of set #1 plus14of set #2.
Thedata points of the training set are presented in theAccuracy MRR Avg SimAmb.
Dis.
Amb.
Dis.
Amb.
Dis.break 0.19 0.28 0.41 0.50 0.41 0.43catch 0.35 0.37 0.58 0.61 0.51 0.57play 0.20 0.28 0.41 0.49 0.60 0.68admit 0.33 0.43 0.57 0.64 0.41 0.46draw 0.24 0.29 0.45 0.51 0.40 0.44Table 2: Results for the supervised task.
?Amb.
?refers to models without the explicit disambigua-tion step, and ?Dis.?
to models with that step.learning algorithm in random order.We measure approximation in three differentmetrics.
The first one, accuracy, is the strictest,and evaluates in how many cases the compositevector of a verb phrase is the closest one (the firstone in the result list) to the corresponding holisticvector.
A more relaxed and perhaps more repre-sentative method is to calculate the mean recipro-cal rank (MRR), which is given by:MRR =1mm?i=11ranki(6)where m is the number of objects and rankirefersto the rank of the correct composite vector for theith object.Finally, a third way to evaluate the efficiency ofeach model is to simply calculate the average co-sine similarity between every holistic vector andits corresponding composite vector.
The resultsare presented in Table 2, reflecting a clear supe-riority (p < 0.001 for average cosine similarity)of the prior disambiguation method for every verband every metric.7 Unsupervised disambiguationIn Section 6 we used a controlled procedure to col-lect genuinely ambiguous verbs and we trained ourmodels from manually annotated data.
In this sec-tion we briefly outline how the process of creat-ing tensors for distinct senses of a verb can be au-tomated, and we test this idea on a generic verbphrase similarity task.First, we use unsupervised learning in order todetect the latent senses of each verb in the corpus,following a procedure first described by Sch?utze(1998).
For every occurrence of the verb, we cre-ate a vector representing the surrounding contextby averaging the vectors of every other word inthe same sentence.
Then, we apply hierarchicalagglomerative clustering (HAC) in order to clusterthese context vectors, hoping that different groupsof contexts will correspond to the different sensesunder which the word has been used in the corpus.The clustering algorithm uses Ward?s method as215inter-cluster measure, and Pearson correlation formeasuring the distance of vectors within a clus-ter.
Since HAC returns a dendrogram embeddingall possible groupings, we measure the quality ofeach partitioning by using the variance ratio crite-rion (Cali?nski and Harabasz, 1974) and we selectthe partitioning that achieves the best score (so thenumber of senses varies from verb to verb).The next step is to classify every noun that hasbeen used as an object with that verb to the mostprobable verb sense, and then use these sets ofnouns as before for training tensors for the vari-ous verb senses.
Being equipped with a number ofsense clusters created as above for every verb, theclassification of each object to a relevant sense isbased on the cosine distance of the object vectorfrom the centroids of the clusters.1Every sensewith less than 3 training exemplars is merged tothe dominant sense of the verb.
The union of allobject sets is used for training a single unambigu-ous tensor for the verb.
As usual, data points arepresented to learning algorithm in random order.No objects in our test set are used for training.We test this system on a verb phase similaritytask introduced in (Mitchell and Lapata, 2010).The goal is to assess the similarity between pairsof short verb phrases (verb-object constructs) andevaluate the results against human annotations.The dataset consists of 72 verb phrases, pairedin three different ways to form groups of variousdegrees of phrase similarity?a total of 108 verbphrase pairs.The experiment has the following form: For ev-ery pair of verb phrases, we construct compositevectors and then we evaluate their cosine similar-ity.
For the ambiguous regression model, the com-position is done by matrix-multiplying the am-biguous verb matrix (learned by the union of allobject sets) with the vector of the noun.
For thedisambiguated version, we first detect the mostprobable sense of the verb given the noun, againby comparing the vector of the noun with thecentroids of the verb clusters; then, we matrix-multiply the corresponding unambiguous tensorcreated exclusively from objects that have beenclassified as closer to this specific sense of theverb with the noun.
We also test a numberof baselines: the ?verbs-only?
model is a non-compositional baseline where only the two verbsare compared; ?additive?
and ?multiplicative?
com-pose the word vectors of each phrase by applyingsimple element-wise operations.1In general, our approach is quite close to the multi-prototype models of Reisinger and Mooney (2010).Model Spearman?s ?Verbs-only 0.331Additive 0.379Multiplicative 0.301Linear regression (ambiguous) 0.349Linear regression (disamb.)
0.399Holistic verb phrase vectors 0.403Human agreement 0.550Table 3: Results for the phrase similarity task.
Thedifference between the ambiguous and the disam-biguated version is s.s. with p < 0.001.The results are presented in Table 3, whereagain the version with the prior disambiguationstep shows performance superior to that of the am-biguous version.
There are two interesting obser-vations that can be made on the basis of Table3.
First of all, the regression model is based onthe assumption that the holistic vectors of the ex-emplar verb phrases follow an ideal distributionalbehaviour that the model aims to approximate asclose as possible.
The results of Table 3 confirmthis: using just the holistic vectors of the corre-sponding verb phrases (no composition is involvedhere) returns the best correlation with human an-notations (0.403), providing a proof that the holis-tic vectors of the verb phrases are indeed reli-able representations of each verb phrase?s mean-ing.
Next, observe that the prior disambiguationmodel approximates this behaviour very closely(0.399) on unseen data, with a difference not sta-tistically significant.
This is very important, sincea regression model can only perform as well as itstraining dataset alows it; and in our case this isachieved to a very satisfactory level.8 Conclusion and future workThis paper adds to existing evidence from previ-ous research that the introduction of an explicitdisambiguation step before the composition im-proves the quality of the produced composed rep-resentations.
The use of a robust regression modelrejects the hypothesis that the proposed methodol-ogy is helpful only for relatively ?weak?
composi-tional approaches.
As for future work, an interest-ing direction would be to see how a prior disam-biguation step can affect deep learning composi-tional settings similar to (Socher et al, 2012) and(Kalchbrenner and Blunsom, 2013b).AcknowledgementsWe would like to thank the three anonymousreviewers for their fruitful comments.
Supportby EPSRC grant EP/F042728/1 is gratefully ac-knowledged by D. Kartsaklis and M. Sadrzadeh.216ReferencesM.
Baroni and R. Zamparelli.
2010.
Nouns are Vec-tors, Adjectives are Matrices.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP).T.
Cali?nski and J. Harabasz.
1974.
A Dendrite Methodfor Cluster Analysis.
Communications in Statistics-Theory and Methods, 3(1):1?27.B.
Coecke, M. Sadrzadeh, and S. Clark.
2010.
Math-ematical Foundations for Distributed CompositionalModel of Meaning.
Lambek Festschrift.
LinguisticAnalysis, 36:345?384.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukWaC, a very large web-derived corpus of English.In Proceedings of the 4th Web as Corpus Workshop(WAC-4) Can we beat Google, pages 47?54.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for composi-tional distributional semantics.
In Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS 2013).N.
Kalchbrenner and P. Blunsom.
2013a.
Recurrentconvolutional neural networks for discourse compo-sitionality.
In Proceedings of the 2013 Workshop onContinuous Vector Space Models and their Compo-sitionality, Sofia, Bulgaria, August.Nal Kalchbrenner and Phil Blunsom.
2013b.
Re-current continuous translation models.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing (EMNLP), Seattle,USA, October.
Association for Computational Lin-guistics.Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2013.Prior disambiguation of word tensors for construct-ing sentence vectors.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), Seattle, USA, October.D.
Kartsaklis, M. Sadrzadeh, and S. Pulman.
2013.Separating Disambiguation from Composition inDistributional Semantics.
In Proceedings of 17thConference on Computational Natural LanguageLearning (CoNLL-2013), Sofia, Bulgaria, August.Dimitri Kartsaklis.
2014.
Compositional operators indistributional semantics.
Springer Science Reviews,April.
DOI: 10.1007/s40362-014-0017-z.J.
Mitchell and M. Lapata.
2008.
Vector-based Mod-els of Semantic Composition.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics, pages 236?244.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1439.Siva Reddy, Ioannis Klapaftis, Diana McCarthy, andSuresh Manandhar.
2011.
Dynamic and static pro-totype vectors for semantic composition.
In Pro-ceedings of 5th International Joint Conference onNatural Language Processing, pages 705?713.Joseph Reisinger and Raymond J Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 109?117.
Association for Computational Lin-guistics.H.
Rubenstein and J.B. Goodenough.
1965.
Contex-tual Correlates of Synonymy.
Communications ofthe ACM, 8(10):627?633.R.
Sansome, D. Reid, and A. Spooner.
2000.
The Ox-ford Junior Dictionary.
Oxford University Press.H.
Sch?utze.
1998.
Automatic Word Sense Discrimina-tion.
Computational Linguistics, 24:97?123.R.
Socher, E.H. Huang, J. Pennington, A.Y.
Ng, andC.D.
Manning.
2011.
Dynamic Pooling and Un-folding Recursive Autoencoders for Paraphrase De-tection.
Advances in Neural Information ProcessingSystems, 24.R.
Socher, B. Huval, C. Manning, and Ng.
A.2012.
Semantic compositionality through recursivematrix-vector spaces.
In Conference on EmpiricalMethods in Natural Language Processing 2012.217
