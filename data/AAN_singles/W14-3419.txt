Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128?133,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsFFTM: A Fuzzy Feature Transformation Method for Medical DocumentsAmir Karami, Aryya GangopadhyayInformation Systems DepartmentUniversity of Maryland Baltimore CountyBaltimore, MD, 21250amir3@umbc.edu,gangopad@umbc.eduAbstractThe vast array of medical text data repre-sents a valuable resource that can be an-alyzed to advance the state of the art inmedicine.
Currently, text mining meth-ods are being used to analyze medical re-search and clinical text data.
Some of themain challenges in text analysis are highdimensionality and noisy data.
There is aneed to develop novel feature transforma-tion methods that help reduce the dimen-sionality of data and improve the perfor-mance of machine learning algorithms.
Inthis paper we present a feature transfor-mation method named FFTM.
We illus-trate the efficacy of our method using lo-cal term weighting, global term weighting,and Fuzzy clustering methods and showthat the quality of text analysis in medicaltext documents can be improved.
We com-pare FFTM with Latent Dirichlet Alloca-tion (LDA) by using two different datasetsand statistical tests show that FFTM out-performs LDA.1 IntroductionThe exponential growth of medical text datamakes it difficult to extract useful information in astructured format.
Some important features of textdata are sparsity and high dimensionality.
Thismeans that while there may be a large numberof terms in most of the documents in a corpus,any one document may contain a small percentageof those terms (Aggarwal and Zhai, 2012).
Thischaracteristic of medical text data makes featuretransformation an important step in text analysis.Feature transformation is a pre-processing step inmany machine-learning methods that is used tocharacterize text data in terms of a different num-ber of attributes in lower dimensions.
This tech-nique has a direct impact on the quality of textmining methods.
Topic models such as LDA hasbeen used as one of popular feature transforma-tion techniques (Ramage et al., 2010).
However,fuzzy clustering methods, particularly in combina-tion with term weighting methods, have not beenexplored much in medical text mining.In this research, we propose a new methodcalled FFTM to extract features from free-textdata.
The rest of the paper is organized in the fol-lowing sections.
In the section 2, we review re-lated work.
Section 3 contains details about ourmethod.
Section 4 describes our experiments, per-formance evaluation, and discussions of our re-sults.
Finally we present a summary, limitations,and future work in the last section.2 Related WorkText analysis is an important topic in medical in-formatics that is challenging due to high sparsedimensionality data.
Big dimension and diver-sity of text datasets have been motivated medi-cal researchers to use more feature transforma-tion methods.
Feature transformation methods en-capsulate a text corpus in smaller dimensions bymerging the initial features.
Topic model is one ofpopular feature transformation methods.
Amongtopic models, LDA (Blei et al., 2003) has beenconsidered more due to its better performance(Ghassemi et al., 2012; Lee et al., 2010).One of methods that has not been fully con-sidered in medical text mining is Fuzzy cluster-ing.
Although most of Fuzzy Clusterings workin medical literature is based on image analysis(Saha and Maulik, 2014; Cui et al., 2013; Beeviand Sathik, 2012), a few work have been donein medical text mining (Ben-Arieh and Gullipalli,2012; Fenza et al., 2012) by using fuzzy cluster-ing.
The main difference between our method andother document fuzzy clustering such as (Singh etal., 2011) is that our method use fuzzy clusteringand word weighting as a pre-processing step for128feature transformation before implementing anyclassification and clustering algorithms; however,other methods use fuzzy clustering as a final stepto cluster the documents.
Our main contributionis to improve the quality of input data to improvethe output of fuzzy clustering.
Among fuzzy clus-tering methods, Fuzzy C-means (Bezdek, 1981)is the most popular one (Bataineh et al., 2011).In this research, we propose a novel method thatcombines local term weighting and global termweighting with fuzzy clustering.3 MethodIn this section, we detail our Fuzzy Feature Trans-formation Method (FFTM) and describe the steps.We begin with a brief review of LDA.LDA is a topic model that can extract hiddentopics from a collection of documents.
It assumesthat each document is a mixture of topics.
The out-put of LDA are the topic distributions over docu-ments and the word distributions over topics.
Inthis research, we use the topics distributions overdocuments.
LDA uses term frequency for localterm weighting.Now we introduce FFTM concepts and nota-tions.
This model has three main steps includ-ing Local Term Weighting (LTW), Global TermWeighting (GTM), and Fuzzy Clustering (Algo-rithm 1).
In this algorithm, each step is the output of each step will be the input of the next step.Step 1: The first step is to calculate LTW.Among different LTW methods we use term fre-quency as a popular method.
Symbol fijdefinesthe number of times term i happens in documentj.We have n documents and m words.Letb(fij) ={1 fij> 00 fij= 0(1)pij=fij?jfij(2)The outputs of this step are b(fij), fij, and pij.We use them as inputs for the second step.Step 2: The next step is to calculate GTW.
Weexplore four GTW methods in this paper includ-ing Entropy, Inverse Document Frequency (IDF),Probabilistic Inverse Document Frequency (Pro-bIDF), and Normal(Table 1).IDF assigns higher weights to rare terms andlower weights to common terms (Papineni, 2001).ProbIDF is similar to IDF and assigns very lowAlgorithm 1 FFTM algorithmFunctions:E():Entropy;I():IDF;PI():ProbIDF;NO():Normal; FC():Fuzzy Clustering.Input: Document Term MatrixOutput: Clustering membership value (?ij)for all documents and clusters.1: Remove stop wordsStep 1: Calculate LTW2: fori = 1 to ndo3: forj = 1 to mdo4: Calculate fij, b(fij), pij5: endfor6: endforStep 2: Calculate GTW7: fori = 1 to ndo8: forj = 1 to mdo9: Execute E(pij,n),I(fij,n),PI(b(fij),n),NO(fij,n)10: endfor11: endforStep 3: Perform Fuzzy Clustering12: Execute FC(E),FC(I),FC(PI),FC(NO)Table1: GTW MethodsName FormulaEntropy 1 +?jpijlog2(pij)log2nIDF log2n?jfijProbIDF log2n??jb(fij)?jb(fij)Normal1?
?jf2ijnegative weight for the terms happen in every doc-ument (Kolda, 1998).
In Entropy, it gives higherweight for the terms happen less in few documents(Dumais, 1992).
Finally, Normal is used to correctdiscrepancies in document lengths and also nor-malize the document vectors.
The outputs of thisstep are the inputs of the last step.Step 3: Fuzzy clustering is a soft clusteringtechnique that finds the degree of membership foreach data point in each cluster, as opposed toassigning a data point only one cluster.
Fuzzyclustering is a synthesis between clustering andfuzzy set theory.
Among fuzzy clustering meth-ods, Fuzzy C-means (FCM) is the most popularone and its goal is to minimize an objective func-129tion by considering constraints:Min Jq(?, V,X) =c?i=1n?j=1(?ij)qD2ij(3)subject to:0 ?
?ij?
1; (4)i ?
{1, .., c} and j ?
{1, ..., n} (5)c?i=1?ij= 1 (6)0 <n?j=1?ij< n; (7)Where:n= number of datac= number of clusters?ij= membership valueq= fuzzifier, 1 < q ?
?V = cluster center vectorDij= d(xj, vi)= distance between xjand viBy optimizing eq.3:?ij=1?ck=1(DijDkj)2q?1(8)vi=?nj=1(?ij)qxj?nj=1(?ij)q(9)The iterations in the clustering algorithms con-tinue till the the maximum changes in ?ijbecomesless than or equal to a pre-specified threshold.
Thecomputational time complexity is O(n).
We use?ijas the degree of clusters?
membership for eachdocument.4 Experimental ResultsIn this section, we evaluate FFTM against LDAusing two measures: document clustering inter-nal metrics and document classification evalua-tion metrics by using one available text datasets.We use Weka1for classification evaluation, MAL-LET2package with its default setting for imple-menting LDA, Matlab fcm package3for imple-menting FCM clustering, and CVAP Matlab pack-age4for clustering validation.1http://www.cs.waikato.ac.nz/ml/weka/2http://mallet.cs.umass.edu/3http://tinyurl.com/kl33w674http://tinyurl.com/kb5bwnm4.1 DatasetsWe leverage two available datasets in this re-search.
Our first test dataset called DeidentifiedMedical Text5is an unlabeled corpus of 2434nursing notes with 12,877 terms after removingstop words.
The second dataset6is a labeled cor-pus of English scientific medical abstracts fromSpringer website.
It is included 41 medical jour-nals ranging from Neurology to Radiology.
In thisresearch, we use the first 10 journals including:Arthroscopy, Federal health standard sheet, Theanesthetist, The surgeon, The gynecologist, Thedermatologist, The internist, The neurologist, TheOphthalmology, The orthopedist, and The pathol-ogist.
In our experiments we select three subsetsfrom the above journals, the first two with 4012terms and 171 documents, first five with 14189terms and 1527 documents, and then all ten re-spectively with 23870 terms and 3764 documentsto track the performance of FFTM and LDA byincreasing the number of documents and labels.4.2 Document ClusteringThe first evaluation comparing FFTM with LDA isdocument clustering by using the first dataset.
In-ternal and external validation are two major meth-ods for clustering validation; however, compari-son between these two major methods shows thatinternal validation is more more precise (Rend?onet al., 2011).
We evaluate different number of fea-tures (topics) and clusters by using two internalclustering validation methods including Silhouetteindex and Calinski-Harabasz index using K-meanswith 500 iterations.
Silhouette index shows thathow closely related are objects in a cluster andhow distinct a cluster from other other clusters.The higher value means the better result.The Sil-houette index (S) is defined as:S(i) =(b(i)?
a(i))Max{a(i), b(i)}(10)Where a(i) is the average dissimilarity of sam-ple i with the same data in a cluster and b(i) is theminimum average dissimilarity of sample i withother data that are not in the same cluster.Calinski-Harabasz index (CH) valuates thecluster validity based on the average between- andwithin-cluster sum of squares.It is defined as:5http://tinyurl.com/kfz2hm46http://tinyurl.com/m2c8se61302 3 4 5 6 7 800.20.40.60.8# ClustersSilhouetteIndex(a) 50 Features2 3 4 5 6 7 800.20.40.60.8# ClustersSilhouetteIndex(b) 100 Features2 3 4 5 6 7 800.20.40.60.8# ClustersSilhouetteIndex(c) 150 FeaturesFigure1: Clustering Validation with Silhouette Index2 3 4 5 6 7 800.511.5?104# ClustersCalinski-HarabaszIndex(a) 50 Features2 3 4 5 6 7 800.511.5?104# ClustersCalinski-HarabaszIndex(b) 100 Features2 3 4 5 6 7 800.511.5?104# ClustersCalinski-HarabaszIndex(c) 150 FeaturesFFTM(Entropy) FFTM(IDF ) FFTM(ProbIDF ) FFTM(Normal)LDAFigure2: Clustering Validation with Calinski-Harabasz IndexCH =trace(SB)trace(SW).np?
1np?
k(11)Where (SB) is the between-cluster scatter ma-trix, (SW) the internal scatter matrix, npis thenumber of clustered samples, and k is the numberof clusters.
Higher value indicates a better clus-tering.
We track the performance of both FFTMand LDA using different number of clusters rang-ing from 2 to 8 with different number of featuresincluding 50, 100, and 150.
Both Silhouette in-dex and Calinski-Harabasz index show that FFTMis the best method with all ranges of features andclusters (Figures 1 and 2).
The gap between FFTMand LDA does not change a lot by using differentnumber of features and clusters.
LDA has the low-est performance and Normal has the best perfor-mance among GTW methods in different ranges offeatures and clusters.
According to the paired dif-ference test, the improvement of FFTM over LDAis statistically significant with a p?
value < 0.05using the two internal clustering validation meth-ods.4.3 Document ClassificationThe second evaluation measure is document clas-sification by using the second datasest.
We evalu-ate different number of classes and features (top-ics) with accuracy, F-measure, and ROC usingRandom Forest.
Accuracy is the portion of true re-sults in a dataset.
F-measure is another measure ofclassification evaluation that considers both preci-sion and recall.
ROC curves plot False Positive onthe X axis vs.
True Positive on the Y axis to findthe trade off between them; therefore, the closer tothe upper left indicates better performance.
Weassume more documents and classes have moretopics;therefore, we choose 100 features for twoclasses, 150 features for five classes, and 200 fea-tures for ten classes.
In addition, we use 10 crossvalidation as test option.This experiment shows that FFTM has the bestperformance in different number of features andlabels (Table 2).
LDA has the lowest performanceand the average performance of ProbIDF has thebest among GTW methods in all ranges of featuresand clusters.
According to the paired differencetest, the improvement of FFTM over LDA is sta-tistically significant with a p?
value < 0.05.131Table2: The Second Dataset Classification PerformanceMethod #Features # Labels Acc % F-Measure ROCFFTM(Entropy) 100 2 96.49 0.959 0.982FFTM(IDF) 100 2 98.24 0.982 0.996FFTM(ProIDF) 100 2 97.66 0.977 0.987FFTM(Normal) 100 2 92.39 0.912 0.971LDA 100 2 90.06 0.9 0.969FFTM(Entropy) 150 5 71.84 0.694 0.874FFTM(IDF) 150 5 70.79 0.686 0.859FFTM(ProIDF) 150 5 70.39 0.674 0.859FFTM(Normal) 150 5 68.11 0.649 0.851LDA 150 5 66.27 0.637 0.815FFTM(Entropy) 200 10 51.06 0.501 0.828FFTM(IDF) 200 10 51.73 0.506 0.826FFTM(ProIDF) 200 10 53.72 0.525 0.836FFTM(Normal) 200 10 50.05 0.485 0.815LDA 200 10 47.68 0.459 0.7925 ConclusionThe explosive growth of medical text data makestext analysis as a key requirement to find patternsin datasets;however, the typical high dimensional-ity of such features motivates researchers to utilizedimension reduction techniques such as LDA.
Al-though LDA has been considered more recently inmedical text analysis (Jimeno-Yepes et al., 2011),fuzzy clustering methods such as FCM has notbeen used in medical text clustering, but rather inimage processing.
In the current study, we pro-pose a method called FFTM to combine LTW andGTM with Fuzzy clustering, and compare its per-formance with that of LDA.
We use different setsof data including different number of features, dif-ferent number of clusters, and different number ofclasses.The findings of this study show that com-bining FCM with LTW and GTW methods cansignificantly improve medical documents analysis.We conclude that different factors including num-ber of features, number of clusters, and classescan affect the outputs of machine learning algo-rithms.
In addition, the performance of FFTM isimproved by using GTW methods.
This methodproposed in this paper may be applied to othermedical documents to improve text analysis out-puts.
One limitation of this paper is that we useone clustering method, one classification method,and two internal clustering validation methods forevaluation.
Our future direction is to explore moremachine learning algorithms and clustering vali-dation methods for evaluation and also other fuzzyclustering algorithms for feature transformation.The main goal of future research is to present anefficient and effective medical topic model usingfuzzy set theory.ReferencesCharuC Aggarwal and ChengXiang Zhai.
2012.
Anintroduction to text mining.
In Mining Text Data,pages 1?10.
Springer.KMBataineh, MNaji, and MSaqer.
2011.
A compar-ison study between various fuzzy clustering algo-rithms.
Jordan Journal of Mechanical & IndustrialEngineering, 5(4).Zulaikha Beevi and Mohamed Sathik.
2012.
A ro-bust segmentation approach for noisy medical im-ages using fuzzy clustering with spatial probability.International Arab Journal of Information Technol-ogy (IAJIT), 9(1).David Ben-Arieh and DeepKumar Gullipalli.
2012.Data envelopment analysis of clinics with sparsedata: Fuzzy clustering approach.
Computers & In-dustrial Engineering, 63(1):13?21.JamesC Bezdek.
1981.
Pattern recognition with fuzzyobjective function algorithms.
Kluwer AcademicPublishers.DavidM Blei, AndrewY Ng, and MichaelI Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Wenchao Cui, YiWang, Yangyu Fan, Yan Feng, andTao Lei.
2013.
Global and local fuzzy clusteringwith spatial information for medical image segmen-tation.
In Signal and Information Processing (Chi-naSIP), 2013 IEEE China Summit & InternationalConference on, pages 533?537.
IEEE.Susan Dumais.
1992.
Enhancing performance in latentsemantic indexing (lsi) retrieval.Giuseppe Fenza, Domenico Furno, and Vincenzo Loia.2012.
Hybrid approach for context-aware servicediscovery in healthcare domain.
Journal of Com-puter and System Sciences, 78(4):1232?1247.132Marzyeh Ghassemi, Tristan Naumann, Rohit Joshi, andAnna Rumshisky.
2012.
Topic models for mortalitymodeling in intensive care units.
In ICML MachineLearning for Clinical Data Analysis Workshop.Antonio Jimeno-Yepes, Bart?omiej Wilkowski,JamesG Mork, Elizabeth VanLenten, DinaDemnerFushman, and AlanR Aronson.
2011.
A bottom-upapproach to medline indexing recommendations.In AMIA Annual Symposium Proceedings, volume2011, page 1583.
American Medical InformaticsAssociation.TamaraG Kolda.
1998.
Limited-memory matrix meth-ods with applications.Sangno Lee, Jeff Baker, Jaeki Song, and JamesCWetherbe.
2010.
An empirical comparison offour text mining methods.
In System Sciences(HICSS), 2010 43rd Hawaii International Confer-ence on, pages 1?10.
IEEE.Kishore Papineni.
2001.
Why inverse document fre-quency?
In Proceedings of the second meeting ofthe North American Chapter of the Association forComputational Linguistics on Language technolo-gies, pages 1?8.
Association for Computational Lin-guistics.Daniel Ramage, SusanT Dumais, and DanielJ Liebling.2010.
Characterizing microblogs with topic models.In ICWSM.Er?endira Rend?on, Itzel Abundez, Alejandra Arizmendi,and ElviaM Quiroz.
2011.
Internal versus externalcluster validation indexes.
International Journal ofcomputers and communications, 5(1):27?34.Indrajit Saha and Ujjwal Maulik.
2014.
Multiobjectivedifferential evolution-based fuzzy clustering for mrbrain image segmentation image segmentation.
InAdvanced Computational Approaches to BiomedicalEngineering, pages 71?86.
Springer.VivekKumar Singh, Nisha Tiwari, and Shekhar Garg.2011.
Document clustering using k-means, heuris-tic k-means and fuzzy c-means.
In ComputationalIntelligence and Communication Networks (CICN),2011 International Conference on, pages 297?301.IEEE.133
