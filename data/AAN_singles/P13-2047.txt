Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 262?267,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsEnglish?Russian MT evaluation campaignPavel BraslavskiKontur Labs /Ural FederalUniversity,Russiapbras@yandex.ruAlexander BeloborodovUral Federal UniversityRussiaxander-beloborodov@yandex.ruMaxim KhalilovTAUS LabsThe Netherlandsmaxim@tauslabs.comSerge SharoffUniversity of LeedsUKs.sharoff@leeds.ac.ukAbstractThis paper presents the settings and the re-sults of the ROMIP 2013 MT shared taskfor the English?Russian language direc-tion.
The quality of generated translationswas assessed using automatic metrics andhuman evaluation.
We also discuss waysto reduce human evaluation efforts usingpairwise sentence comparisons by humanjudges to simulate sort operations.1 IntroductionMachine Translation (MT) between English andRussian was one of the first translation directionstested at the dawn of MT research in the 1950s(Hutchins, 2000).
Since then the MT paradigmschanged many times, many systems for this lan-guage pair appeared (and disappeared), but as faras we know there was no systematic quantitativeevaluation of a range of systems, analogous toDARPA?94 (White et al, 1994) and later evalua-tion campaigns.
The Workshop on Statistical MT(WMT) in 2013 has announced a Russian evalua-tion track for the first time.1 However, this evalu-ation is currently ongoing, it should include newmethods for building statistical MT (SMT) sys-tems for Russian from the data provided in thistrack, but it will not cover the performance of ex-isting systems, especially rule-based (RBMT) orhybrid ones.Evaluation campaigns play an important rolein promotion of the progress for MT technolo-gies.
Recently, there have been a number ofMT shared tasks for combinations of several Eu-ropean, Asian and Semitic languages (Callison-Burch et al, 2011; Callison-Burch et al, 2012;Federico et al, 2012), which we took into accountin designing the campaign for the English-Russiandirection.
The evaluation has been held in the1http://www.statmt.org/wmt13/context of ROMIP,2 which stands for Russian In-formation Retrieval Evaluation Seminar and is aTREC-like3 Russian initiative started in 2002.One of the main challenges in developing MTsystems for Russian and for evaluating them is theneed to deal with its free word order and com-plex morphology.
Long-distance dependenciesare common, and this creates problems for bothRBMT and SMT systems (especially for phrase-based ones).
Complex morphology also leadsto considerable sparseness for word alignment inSMT.The language direction was chosen to beEnglish?Russian, first because of the availabil-ity of native speakers for evaluation, second be-cause the systems taking part in this evaluation aremostly used in translation of English texts for theRussian readers.2 Corpus preparationIn designing the set of texts for evaluation, wehad two issues in mind.
First, it is known thatthe domain and genre can influence MT perfor-mance (Langlais, 2002; Babych et al, 2007), sowe wanted to control the set of genres.
Second,we were aiming at using sources allowing distri-bution of texts under a Creative Commons licence.In the end two genres were used coming from twosources.
The newswire texts were collected fromthe English Wikinews website.4 The second genrewas represented by ?regulations?
(laws, contracts,rules, etc), which were collected from the Webusing a genre classification method described in(Sharoff, 2010).
The method provided a sufficientaccuracy (74%) for the initial selection of texts un-der the category of ?regulations,?
which was fol-lowed by a manual check to reject texts clearlyoutside of this genre category.2http://romip.ru/en/3http://trec.nist.gov/4http://en.wikinews.org/262The initial corpus consists of 8,356 originalEnglish texts that make up 148,864 sentences.We chose to retain the entire texts in the cor-pus rather than individual sentences, since someMT systems may use information beyond isolatedsentences.
100,889 sentences originated fromWikinews; 47,975 sentences came from the ?reg-ulations?
corpus.
The first 1,002 sentences werepublished in advance to allow potential partici-pants time to adjust their systems to the corpus for-mat.
The remaining 147,862 sentences were thecorpus for testing translation into Russian.
Twoexamples of texts in the corpus:90237 Ambassadors from the United States ofAmerica, Australia and Britain have all metwith Fijian military officers to seek insur-ances that there wasn?t going to be a coup.102835 If you are given a discount for bookingmore than one person onto the same date andyou later wish to transfer some of the dele-gates to another event, the fees will be recal-culated and you will be asked to pay addi-tional fees due as well as any administrativecharge.For automatic evaluation we randomly selected947 ?clean?
sentences, i.e.
those with clear sen-tence boundaries, no HTML markup remains, etc.
(such flaws sometimes occur in corpora collectedfrom the Web).
759 sentences originated fromthe ?news?
part of the corpus, the remaining 188came from the ?regulations?
part.
The sentencescame from sources without published translationsinto Russian, so that some of the participating sys-tems do not get unfair advantage by using them fortraining.
These sentences were translated by pro-fessional translators.
For manual evaluation, werandomly selected 330 sentences out of 947 usedfor automatic evaluation, specifically, 190 fromthe ?news?
part and 140 from the ?regulations?
part.The organisers also provided participants withaccess to the following additional resources:?
1 million sentences from the English-Russianparallel corpus released by Yandex (the sameas used in WMT13)5;?
119 thousand sentences from the English-Russian parallel corpus from the TAUS DataRepository.6These resources are not related to the test corpusof the evaluation campaign.
Their purpose was5https://translate.yandex.ru/corpus?lang=en6https://www.tausdata.orgto make it easier to participate in the shared taskfor teams without sufficient data for this languagepair.3 Evaluation methodologyThe main idea of manual evaluation was (1) tomake the assessment as simple as possible for ahuman judge and (2) to make the results of evalu-ation unambiguous.
We opted for pairwise com-parison of MT outputs.
This is different fromsimultaneous ranking of several MT outputs, ascommonly used in WMT evaluation campaigns.In case of a large number of participating sys-tems each assessor ranks only a subset of MT out-puts.
However, a fair overall ranking cannot be al-ways derived from such partial rankings (Callison-Burch et al, 2012).
The pairwise comparisonswe used can be directly converted into unambigu-ous overall rankings.
This task is also much sim-pler for human judges to complete.
On the otherhand, pairwise comparisons require a larger num-ber of evaluation decisions, which is feasible onlyfor few participants (and we indeed had relativelyfew submissions in this campaign).
Below we alsodiscuss how to reduce the amount of human effortsfor evaluation.In our case the assessors were asked to make apairwise comparison of two sentences translatedby two different MT systems against a gold stan-dard translation.
The question for them was tojudge translation adequacy, i.e., which MT outputconveys information from the reference translationbetter.
The source English sentence was not pre-sented to the assessors, because we think that wecan have more trust in understanding of the sourcetext by a professional translator.
The translatoralso had access to the entire text, while the asses-sors could only see a single sentence.For human evaluation we employed the multi-functional TAUS DQF tool7 in the ?Quick Com-parison?
mode.Assessors?
judgements resulted in rankings foreach sentence in the test set.
In case of ties theranks were averaged, e.g.
when the ranks of thesystems in positions 2-4 and 7-8 were tied, theirranks became: 1 3 3 3 5 6 7.5 7.5.
Toproduce the final ranking, the sentence-level rankswere averaged over all sentences.Pairwise comparisons are time-consuming: n7https://tauslabs.com/dynamic-quality/dqf-tools-mt263Metric OS1 OS2 OS3 OS4 P1 P2 P3 P4 P5 P6 P7Automatic metrics ALL (947 sentences)BLEU 0.150 0.141 0.133 0.124 0.157 0.112 0.105 0.073 0.094 0.071 0.073NIST 5.12 4.94 4.80 4.67 5.00 4.46 4.11 2.38 4.16 3.362 3.38Meteor 0.258 0.240 0.231 0.240 0.251 0.207 0.169 0.133 0.178 0.136 0.149TER 0.755 0.766 0.764 0.758 0.758 0.796 0.901 0.931 0.826 0.934 0.830GTM 0.351 0.338 0.332 0.336 0.349 0.303 0.246 0.207 0.275 0.208 0.230Automatic metrics NEWS (759 sentences)BLEU 0.137 0.131 0.123 0.114 0.153 0.103 0.096 0.070 0.083 0.066 0.067NIST 4.86 4.72 4.55 4.35 4.79 4.26 3.83 2.47 3.90 3.20 3.19Meteor 0.241 0.224 0.214 0.222 0.242 0.192 0.156 0.127 0.161 0.126 0.136TER 0.772 0.776 0.784 0.777 0.768 0.809 0.908 0.936 0.844 0.938 0.839GTM 0.335 0.324 0.317 0.320 0.339 0.290 0.233 0.201 0.257 0.199 0.217Table 1: Automatic evaluation resultscases require n(n?1)2 pairwise decisions.
In thisstudy we also simulated a ?human-assisted?
in-sertion sort algorithm and its variant with binarysearch.
The idea is to run a standard sort algo-rithm and ask a human judge each time a compar-ison operation is required.
This assumes that hu-man perception of quality is transitive: if we knowthat A < B and B < C, we can spare evaluationof A and C. This approach also implies that sen-tence pairs to judge are generated and presented toassessors on the fly; each decision contributes toselection of the pairs to be judged in the next step.If the systems are pre-sorted in a reasonable way(e.g.
by an MT metric, under assumption that au-tomatic pre-ranking is closer to the ?ideal?
rankingthan a random one), then we can potentially saveeven more pairwise comparison operations.
Pre-sorting makes ranking somewhat biased in favourof the order established by an MT metric.
For ex-ample, if it favours one system against another,while in human judgement they are equal, the finalranking will preserve the initial order.
Insertionsort of n sentences requires n?
1 comparisons inthe best case of already sorted data and n(n?1)2 inthe worst case (reversely ordered data).
Insertionsort with binary search requires?
n log n compar-isons regardless of the initial order.
For this studywe ran exhaustive pairwise evaluation and used itsresults to simulate human-assisted sorting.In addition to human evaluation, we also ransystem-level automatic evaluations using BLEU(Papineni et al, 2001), NIST (Doddington,2002), METEOR (Banerjee and Lavie, 2005),TER (Snover et al, 2009), and GTM (Turian etal., 2003).
We also wanted to estimate the correla-tions of these metrics with human judgements forthe English?Russian pair on the corpus level andon the level of individual sentences.4 ResultsWe received results from five teams, two teamssubmitted two runs each, which totals seven par-ticipants?
runs (referred to as P1..P7 in the pa-per).
The participants represent SMT, RBMT,and hybrid approaches.
They included establishedgroups from academia and industry, as well as newresearch teams.
The evaluation runs also includedthe translations of the 947 test sentences producedby four free online systems in their default modes(referred to as OS1..OS4).
For 11 runs automaticevaluation measures were calculated; eight runsunderwent manual evaluation (four online systemsplus four participants?
runs; no manual evaluationwas done by agreement with the participants forthe runs P3, P6, and P7 to reduce the workload).ID Name and informationOS1 Phrase-based SMTOS2 Phrase-based SMTOS3 Hybrid (RBMT+statistical PE)OS4 Dependency-based SMTP1 Compreno, Hybrid, ABBYY CorpP2 Pharaon, Moses, Yandex&TAUS dataP3,4 Balagur, Moses, Yandex&news dataP5 ETAP-3, RBMT, (Boguslavsky, 1995)P6,7 Pereved, Moses, Internet dataOS3 is a hybrid system based on RBMT withSMT post-editing (PE).
P1 is a hybrid system withanalysis and generation driven by statistical evalu-ation of hypotheses.264All (330 sentences)OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest)3.159 3.350 3.530 3.961 4.082 5.447 5.998 6.473News (190 sentences)OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest)2.947 3.450 3.482 4.084 4.242 5.474 5,968 6,353Regulations (140 sentences)P1 (highest) OS3 OS1 OS2 OS4 P5 P2 P4 (lowest)3.214 3.446 3.596 3.793 3.864 5.411 6.039 6.636Simulated dynamic ranking (insertion sort)P1 (highest) OS1 OS3 OS2 OS4 P5 P4 P2 (lowest)3.318 3.327 3.588 4.221 4.300 5.227 5.900 6.118Simulated dynamic ranking (binary insertion sort)OS1 (highest) P1 OS3 OS2 OS4 P5 P2 P4 (lowest)2.924 3.045 3.303 3.812 4.267 5.833 5.903 6.882Table 2: Human evaluation resultsTable 1 gives the automatic scores for eachof participating runs and four online systems.OS1 usually has the highest overall score (exceptBLEU), it also has the highest scores for ?regula-tions?
(more formal texts), P1 scores are better forthe news documents.14 assessors were recruited for evaluation (par-ticipating team members and volunteers); the to-tal volume of evaluation is 10,920 pairwise sen-tence comparisons.
Table 2 presents the rankingsof the participating systems using averaged ranksfrom the human evaluation.
There is no statisti-cally significant difference (using Welch?s t-test atp ?
0.05) in the overall ranks within the follow-ing groups: (OS1, OS3, P1) < (OS2, OS4) < P5< (P2, P4).
OS3 (mostly RBMT) belongs to thetroika of leaders in human evaluation contrary tothe results of its automatic scores (Table 1).
Sim-ilarly, P5 is consistently ranked higher than P2 bythe assessors, while the automatic scores suggestthe opposite.
This observation confirms the well-known fact that the automatic scores underesti-mate RBMT systems, e.g., (B?char et al, 2012).To investigate applicability of the automaticmeasures to the English-Russian language direc-tion, we computed Spearman?s ?
correlation be-tween the ranks given by the evaluators and bythe respective measures.
Because of the amountof variation for each measure on the sentencelevel, robust estimates, such as the median andthe trimmed mean, are more informative thanthe mean, since they discard the outliers (Huber,1996).
The results are listed in Table 3.
All mea-sures exhibit reasonable correlation on the corpuslevel (330 sentences), but the sentence-level re-sults are less impressive.
While TER and GTMare known to provide better correlation with post-editing efforts for English (O?Brien, 2011), freeword order and greater data sparseness on the sen-tence level makes TER much less reliable for Rus-sian.
METEOR (with its built-in Russian lemma-tisation) and GTM offer the best correlation withhuman judgements.The lower part of Table 2 also reports the resultsof simulated dynamic ranking (using the NISTrankings as the initial order for the sort operation).It resulted in a slightly different final ranking ofthe systems since we did not account for ties and?averaged ranks?.
However, the ranking is prac-tically the same up to the statistically significantrank differences in reference ranking (see above).The advantage is that it requires a significantlylower number of pairwise comparisons.
Insertionsort yielded 5,131 comparisons (15.5 per sentence;56% of exhaustive comparisons for 330 sentencesand 8 systems); binary insertion sort yielded 4,327comparisons (13.1 per sentence; 47% of exhaus-tive comparisons).Out of the original set of 330 sentences forhuman evaluation, 60 sentences were evaluatedby two annotators (which resulted in 60*28=1680pairwise comparisons), so we were able to calcu-late the standard Kohen?s ?
and Krippendorff?s ?scores (Artstein and Poesio, 2008).
The results ofinter-annotator agreement are: percentage agree-ment 0.56, ?
= 0.34, ?
= 0.48, which is simi-265Sentence level CorpusMetric Median Mean Trimmed levelBLEU 0.357 0.298 0.348 0.833NIST 0.357 0.291 0.347 0.810Meteor 0.429 0.348 0.393 0.714TER 0.214 0.186 0.204 0.619GTM 0.429 0.340 0.392 0.714Table 3: Correlation to human judgementslar to sentence ranking reported in other evaluationcampaigns (Callison-Burch et al, 2012; Callison-Burch et al, 2011).
It was interesting to see theagreement results distinguishing the top three sys-tems against the rest, i.e.
by ignoring the assess-ments for the pairs within each group, ?
= 0.53,which indicates that the judges agree on the dif-ference in quality between the top three systemsand the rest.
On the other hand, the agreement re-sults within the top three systems are low: ?
=0.23, ?
= 0.33, which is again in line with the re-sults for similar evaluations between closely per-forming systems (Callison-Burch et al, 2011).5 Conclusions and future plansThis was the first attempt at making properquantitative and qualitative evaluation of theEnglish?Russian MT systems.
In the future edi-tions, we will be aiming at developing a newtest corpus with a wider genre palette.
Wewill probably complement the campaign withRussian?English translation direction.
We hopeto attract more participants, including interna-tional ones and plan to prepare a ?light version?for students and young researchers.
We will alsoaddress the problem of tailoring automatic evalu-ation measures to Russian ?
accounting for com-plex morphology and free word order.
To thisend we will re-use human evaluation data gath-ered within the 2013 campaign.
While the cam-paign was based exclusively on data in one lan-guage direction, the correlation results for auto-matic MT quality measures should be applicableto other languages with free word order and com-plex morphology.We have made the corpus comprising the sourcesentences, their human translations, translationsby participating MT systems and the human eval-uation data publicly available.88http://romip.ru/mteval/AcknowledgementsWe would like to thank the translators, asses-sors, as well as Anna Tsygankova, Maxim Gubin,and Marina Nekrestyanova for project coordina-tion and organisational help.
Research on corpuspreparation methods was supported by EU FP7funding, contract No 251534 (HyghTra).
Our spe-cial gratitude goes to Yandex and ABBYY whopartially covered the expenses incurred on corpustranslation.
We?re also grateful to the anonymousreviewers for their useful comments.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Bogdan Babych, Anthony Hartley, Serge Sharoff, andOlga Mudraya.
2007.
Assisting translators in in-direct lexical transfer.
In Proc.
of 45th ACL, pages739?746, Prague.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, Michigan, June.Hanna B?char, Rapha?l Rubino, Yifan He, Yanjun Ma,and Josef van Genabith.
2012.
An evaluationof statistical post-editing systems applied to RBMTand SMT systems.
In Proceedings of COLING?12,Mumbai.Igor Boguslavsky.
1995.
A bi-directional Russian-to-English machine translation system (ETAP-3).
InProceedings of the Machine Translation Summit V,Luxembourg.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar F Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
InProceedings of the Sixth Workshop on StatisticalMachine Translation, pages 22?64.
Association forComputational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?al, Canada, June.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology, pages 138?145, San Diego, CA.266Marcelo Federico, Mauro Cettolo, Luisa Bentivogli,Michael Paul, and Sebastian Stuker.
2012.Overview of the IWSLT 2012 evaluation campaign.In Proceedings of the International Workshop onSpoken Language Translation (IWSLT), pages 12?34, Hong Kong, December.Peter J. Huber.
1996.
Robust Statistical Procedures.Society for Industrial and Applied Mathematics.John Hutchins, editor.
2000.
Early years in ma-chine translation: Memoirs and biographies of pi-oneers.
John Benjamins, Amsterdam, Philadel-phia.
http://www.hutchinsweb.me.uk/EarlyYears-2000-TOC.htm.Philippe Langlais.
2002.
Improving a general-purposestatistical translation engine by terminological lexi-cons.
In Proceedings of Second international work-shop on computational terminology (COMPUTERM2002), pages 1?7, Taipei, Taiwan.
http://acl.ldc.upenn.edu/W/W02/W02-1405.pdf.Sharon O?Brien.
2011.
Towards predictingpost-editing productivity.
Machine translation,25(3):197?215.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
Technical Re-port RC22176 (W0109-022), IBM Thomas J. Wat-son Research Center.Serge Sharoff.
2010.
In the garden and in the jun-gle: Comparing genres in the BNC and Internet.In Alexander Mehler, Serge Sharoff, and MarinaSantini, editors, Genres on the Web: Computa-tional Models and Empirical Studies, pages 149?166.
Springer, Berlin/New York.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments witha tunable MT metric.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages259?268, Athens, Greece, March.Joseph Turian, Luke Shen, and I. Dan Melamed.
2003.Evaluation of machine translation and its evaluation.In Proceedings of Machine Translation Summit IX,New Orleans, LA, USA, September.John S. White, Theresa O?Connell, and FrancisO?Mara.
1994.
The ARPA MT evaluation method-ologies: Evolution, lessons, and further approaches.In Proceedings of AMTA?94, pages 193?205.267
