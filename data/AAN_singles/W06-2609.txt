Learning to Identify Definitions using Syntactic FeaturesIsmail Fahmi and Gosse BoumaInformation ScienceGroningen University{i.fahmi,g.bouma}@rug.nlAbstractThis paper describes an approach to learn-ing concept definitions which operates onfully parsed text.
A subcorpus of theDutch version of Wikipedia was searchedfor sentences which have the syntacticproperties of definitions.
Next, we ex-perimented with various text classifica-tion techniques to distinguish actual defi-nitions from other sentences.
A maximumentropy classifier which incorporates fea-tures referring to the position of the sen-tence in the document as well as varioussyntactic features, gives the best results.1 IntroductionAnswering definition questions is a challenge forquestion answering systems.
Much work in QAhas focused on answering factoid questions, whichare characterized by the fact that given the ques-tion, one can typically make strong predictionsabout the type of expected answer (i.e.
a date,name of a person, amount, etc.).
Definition ques-tions require a different approach, as a definitioncan be a phrase or sentence for which only veryglobal characteristics hold.In the CLEF 2005 QA task, 60 out of 200 ques-tions were asking for the definition of a namedentity (a person or organization) such as Who isGoodwill Zwelithini?
or What is IKEA?
Answersare phrases such as current king of the Zulu nation,or Swedish home furnishings retailer.
For answer-ing definition questions restricted to named enti-ties, it generally suffices to search for noun phrasesconsisting of the named entity and a preceding orfollowing nominal phrase.
Bouma et al (2005) ex-tract all such noun phrases from the Dutch CLEFcorpus off-line, and return the most frequent headsof co-occurring nominal phrases expanded withadjectival or prepositional modifiers as answer tonamed entity definition questions.
The resultingsystem answers 50% of the CLEF 2005 definitionquestions correctly.For a Dutch medical QA system, which is beingdeveloped as part of the IMIX project1, several setsof test questions were collected.
Approximately15% of the questions are definition questions, suchas What is a runner?s knee?
and What is cere-brovascular accident?.
Answers to such questions(asking for the definition of a concept) are typi-cally found in sentences such as A runner?s kneeis a degenerative condition of the cartilage sur-face of the back of the knee cap, or patella or Acerebrovascular accident is a decrease in the num-ber of circulating white blood cells (leukocytes)in the blood.
One approach to finding answers toconcept definitions simply searches the corpus forsentences consisting of a subject, a copular verb,and a predicative phrase.
If the concept matchesthe subject, the predicative phrase can be returnedas answer.
A preliminary evaluation of this tech-nique in Tjong Kim Sang et al (2005) revealedthat only 18% of the extracted sentences (froma corpus consisting of a mixture of encyclopedictexts and web documents) is actually a definition.For instance, sentences such as RSI is a majorproblem in the Netherlands, every suicide attemptis an emergency or an infection of the lungs is themost serious complication are of the relevant syn-tactic form, but do not constitute definitions.In this paper, we concentrate on a method forimproving the precision of recognizing definitionsentences.
In particular, we investigate to what1www.let.rug.nl/?gosse/Imix64extent machine learning techniques can be usedto distinguish definitions from non-definitions ina corpus of sentences containing a subject, copu-lar verb, and predicative phrase.
A manually an-notated subsection of the corpus was divided intodefinition and non-definition sentences.
Next, wetrained various classifiers using unigram and bi-gram features, and various syntactic features.
Thebest classifier achieves a 60% error reduction com-pared to our baseline system.2 Previous workWork on identifying definitions from free text ini-tially relied on manually crafted patterns withoutapplying any machine learning technique.
Kla-vans and Muresan (2000) set up a pattern extractorfor their Definder system using a tagger and a fi-nite state grammar.
Joho and Sanderson (2000) re-trieve descriptive phrases (dp) of query nouns (qn)from text to answer definition questions like Whois qn?
Patterns such as ?dp especially qn?, as uti-lized by Hearst (1992), are used to extract namesand their descriptions.Similar patterns are also applied by Liu et al(2003) to mine definitions of topic-specific con-cepts on the Web.
As an additional assumption,specific documents dedicated to the concepts canbe identified if they have particular HTML and hy-perlink structures.Hildebrandt et al (2004) exploit surface pat-terns to extract as many relevant ?nuggets?
of in-formation of a concept as possible.
Similar to ourwork, a copular pattern NP1 be NP2 is used asone of the extraction patterns.
Nuggets which donot begin with a determiner are discarded to fil-ter out spurious nuggets (e.g., progressive tense).Nuggets extracted from every article in a corpusare then stored in a relational database.
In the end,answering definition questions becomes as simpleas looking up relevant terms from the database.This strategy is similar to our approach for answer-ing definition questions.The use of machine learning techniques can befound in Miliaraki and Androutsopoulos (2004)and Androutsopoulos and Galanis (2005) Theyuse similar patterns as (Joho and Sanderson,2000) to construct training attributes.
Sager andL?Homme (1994) note that the definition of aterm should at least always contain genus (term?scategory) and species (term?s properties).
Blair-Goldensohn et al (2004) uses machine learn-ing and manually crafted lexico-syntactic patternsto match sentences containing both a genus andspecies phrase for a given term.There is an intuition that most of definitionsentences are located at the beginning of docu-ments.
This lead to the use of sentence num-ber as a good indicator of potential definition sen-tences.
Joho and Sanderson (2000) use the posi-tion of the sentences as one of their ranking crite-ria, while Miliaraki and Androutsopoulos (2004),Androutsopoulos and Galanis (2005) and Blair-Goldensohn et al (2004) apply it as one of theirlearning attributes.3 Syntactic properties of potentialdefinition sentencesTo answer medical definition sentences, we usedthe medical pages of Dutch Wikipedia2 as source.Medical pages were selected by selecting all pagesmentioned on the Healthcare index page, and re-cursively including pages mentioned on retrievedpages as well.The corpus was parsed syntactically by Alpino,a robust wide-coverage parser for Dutch (Maloufand van Noord, 2004).
The result of parsing (il-lustrated in Figure 1) is a dependency graph.
TheAlpino-parser comes with an integrated named en-tity classifier which assigns distinct part-of-speechtags to person, organization, and geographicalnamed entities.Potential definition sentences are sentences con-taining a form of the verb zijn3 (to be) with asubject and nominal predicative phrase as sisters.The syntactic pattern does not match sentences inwhich zijn is used as a possessive pronoun (his)and sentences where a form of zijn is used as anauxiliary.
In the latter case, no predicative phrasecomplement will be found.
On the other hand,we do include sentences in which the predicativephrase precedes the subject, as in Onderdeel vande testis is de Leydig-cel (the Leydig cel is part ofthe testis).
As word order in Dutch is less strictthan in English, it becomes relevant to includesuch non-canonical word orders as well.A number of non-definition sentences that willbe extracted using this method can be filtered bysimple lexical methods.
For instance, if the subjectis headed by (the Dutch equivalents of) cause, con-2nl.wikipedia.org3Note that the example uses ben (the first person singularform of the verb) as root for zijn.65?smainsunounstikstof0hdverbben1predcnpdetdeteen2modadjscheikundig3hdnounelement4modpphdprepmet5obj1conjcnjnphdnounsymbool6appnameN7crdvgen8cnjnphdnounatoom nummer9appnum710Figure 1: Parse of (the Dutch equivalent of) Nitrogen is a chemical element with symbol N and atomicnumber 7.
Nodes are labelled with depedency relations and categories or part-of-speech tags, root forms,and string positions.sequence, example, problem, result, feature, pos-sibility, symptom, sign, etc., or contains the deter-miner geen (no), the sentence will not be includedin the list of potential definitions.However, even after applying the lexical filter,not all extracted sentences are definitions.
In thenext sections, we describe experiments aimed atincreasing the accuracy of the extraction method.4 Annotating training examplesTo create evaluation and training data, 2500 ex-tracted sentences were manually annotated as def-inition, non-definition, or undecided.
One of thecriteria for undecided sentences is that it mentionsa characteristic of a definition but is not reallya (complete) definition, for example, Benzeen iscarcinogeen (Benzene is a carcinogen).
The resultof this annotation is given in Table 1.
The anno-tated data was used both to evaluate the accuracyof the syntactic extraction method, and to trainingand evaluate material for the machine learning ex-periments as discussed in the next sections.After discarding the undecided sentences, weare left with 2299 sentences, 1366 of which aredefinitions.
This means that the accuracy of theextraction method using only syntax was 59%.44This is considerably higher than the estimated accuracyof 18% reported in Tjong Kim Sang et al (2005).
This isprobably partly due to the fact that the current corpus con-sists of encyclopedic material only, whereas the corpus usedIf we take sentence postion into account as well,and classify all first sentences as definitions andall other sentences as non-definitions, a baselineaccuracy of 75,9% is obtained.It is obvious from Table 1 that the first sen-tences of Wikipedia lemmas that match the syn-tactic pattern are almost always definitions.
Itseems that e.g.
Google?s5 define query feature,when restricted to Dutch at least, relies heavilyon this fact to answer definition queries.
How-ever it is also obvious that definition sentences canalso be found in other positions.
For documentsfrom other sources, which are not as structured asWikipedia, the first position sentence is likely tobe an even weaker predictor of definition vs. non-definition sentences.5 Attributes of definition sentencesWe aim at finding the best attributes for classifyingdefinition sentences.
We experimented with com-binations of the following attributes:Text properties: bag-of-words, bigrams, androot forms.
Punctuation is included as Klavansand Muresan (2000) observe that it can be used torecognize definitions (i.e.
definitions tend to con-in Tjong Kim Sang et al (2005) contained web materialfrom various sources, such as patient discussion groups, aswell.
The latter tends to contain more subjective and context-dependent material.5google.com66Sentence Def Non-def Undecidedpositionfirst 831 18 31other 535 915 170Total 1366 933 201Table 1: Number of sentences in the first andother position of documents annotated as defini-tion, non-definition, and undecided.tain parentheses more often than non-definitions).No stopword filtering is applied as in our exper-iments it consistently decreased accuracy.
Notethat we include all bigrams in a sentence as fea-ture.
A different use of n-grams has been exploredby Androutsopoulos and Galanis (2005) who addonly n-grams (n ?
{1,2,3}) occurring frequentlyeither directly before or after a target term.Document property: the position of each sen-tence in the document.
This attribute has been fre-quently used in previous work and is motivated bythe observation that definitions are likely to be lo-cated in the beginning of a document.Syntactic properties: position of each sub-ject in the sentence (initial, e.g.
X is Y; or non-initial, e.g.
Y is X), and of each subject andpredicative complement: type of determiner (def-inite, indefinite, other).
These attributes have notbeen investigated in previous work.
In our exper-iments, sentence-initial subjects appear in 92% ofthe definition sentences and and 76% of the non-definition sentences.
These values show that adefinition sentence with a copular pattern tendsto put its subject in the beginning.
Two otherattributes are used to encode the type of deter-miner of the subject and predicative compelement.As shown in Table 2, the majority of subjects indefinition sentences have no determiner (62%),e.g.
Paracetamol is een pijnstillend en koortsver-lagend middel (Paracetamol is an pain alleviat-ing and a fever reducing medicine), while in non-definition sentences subject determiners tend to bedefinite (50%), e.g.
De werkzame stof is acetyl-salicylzuur (The operative substance is acetylsal-icylacid).
Predicative complements, as shown inTable 3, tend to contain indefinite determiners indefinition sentences (64%), e.g.
een pijnstillend.
.
.
medicijn (a pain alleviating.
.
.
medicine), whilein non-definition the determiner tends to be def-inite (33%), e.g.
Een fenomeen is de Landsge-meinde (A phenomenon is the Landsgemeinde).Type Definition Non-defdefinite 23 50indefinite 13 12nodeterminer 62 29other 2 9Table 2: Percentage of determiner types of sub-jects in definition and non-definition sentences.Type Definition Non-defdefinite 23 33indefinite 64 29nodeterminer 8 1other 4 28Table 3: Percentage of determiner types ofpredicative complements in definition and non-definition sentences.Named entity tags: named entity class (NEC)of subjects, e.g.
location, person, organization,or no-class.
A significant difference in the dis-tribution of this feature between definition andnon-definition sentences can be observed in Table4.
More definition sentences have named entityclasses contained in their subjects (40.63%) com-pared to non-definition sentences (11.58%).
Wealso experimented with named entity classes con-tained in predicative complements but it turnedout that very few predicates contained named en-tities, and thus no significant differences in distri-bution between definition and non-definition sen-tences could be observed.Features for lexical patterns, as used in (An-droutsopoulos and Galanis, 2005), e.g.
qn which(is|was|are|were) dp, are not added because in thisexperiment we investigate only a copular pattern.WordNet-based attributes are also excluded, giventhat coverage for Dutch (using EuroWordNet)tends to be less good than for English, and even forEnglish their contribution is sometimes insignifi-cant (Miliaraki and Androutsopoulos, 2004).Type Definition Non-defno-nec 59 88location 10 4organization 8 3person 22 4Table 4: Percentage of named-entity classes ofsubjects in definition and non-definition sentences.67word bigrams only bigram + synt + posis a first senta other sentare is ais indef pred) is no det subjthe init subjis DIGITS aare the arethis isor other det predis of ) isthis/these noninit subjatomic number def subjatomic number DIGITS thewith symbol is DIGITSand atomic number are thethat thischemical ora chemical other det subjchemical element is ofTable 5: 20 most informative features for the sys-tems using word bigrams only and word bigramsin combination with syntactic and sentence posi-tion features (word features have been translatedinto English).We use the text classification tool Rainbow6(McCallum, 2000) to perform most of our experi-ments.
Each sentence is represented as a string ofwords, possibly followed by bigrams, root forms,(combinations of) syntactic features, etc.All experiments were performed by selectingonly the 2000 highest ranked features accordingto information gain.
In the experiments which in-clude syntactic features, the most informative fea-tures tend to contain a fair number of syntactic fea-tures.
This is illustrated for the configuration usingbigrams, sentence position, and syntax in table 5.It supports our intuition that the position of sub-jects and the type of determiner of subjects andpredicative complements are clues to recognizingdefinition sentences.To investigate the effect of each attribute, weset up several configurations of training examplesas described in Table 6.
We start with using onlybag-of-words or bigrams, and then combine themwith other attribute sets.6www.cs.cmu.edu/?mccallum/bow/rainbow/Cfg Description1 using only bag-of-words2 using only bigrams3 combining bigrams & bag-of-words4 adding syntactic properties toconfig.
35 adding syntactic properties& NEC to config.
36 adding sentence position toconfig.
37 adding root forms toconfig.
38 adding syntactic properties &sentence position to config.
39 adding syntactic properties, sentenceposition & NEC to config.
310 adding syntactic properties, sentenceposition & root forms to config.
3)11 using all attributes (adding NECto configuration 10)Table 6: The description of the attribute configu-rations.6 Learning-based methodsWe apply three supervised learning methods toeach of the attribute configurations in Table 6,namely naive Bayes, maximum entropy, and sup-port vector machines (SVMs).
Naive Bayes is afast and easy to use classifier based on the prob-abilistic model of text and has often been used intext classification tasks as a baseline.
Maximumentropy is a general estimation technique that hasbeen used in many fields such as information re-trieval and machine learning.
Some experimentsin text classification show that maximum entropyoften outperforms naive Bayes, e.g.
on two ofthree data sets in Nigam et al (1999).
SVMs area new learning method but have been reported byJoachims (1998) to be well suited for learning intext classification.We experiment with three kernel types ofSVMs: linear, polynomial, and radial base func-tion (RBF).
Rainbow (McCallum, 2000) is used toexamine these learning methods, except the RBFkernel for which libsvm (Chang and Lin, 2001)is used.
Miliaraki and Androutsopoulos (2004)use a SVM with simple inner product (polyno-mial of first degree) kernel because higher degreepolynomial kernels were reported as giving no im-provement.
However we want to experiment with68Cfg NB ME svm1a svm2b svm3c1 85.75 ?
0.57 85.35 ?
0.77 77.65 ?
0.87 78.39 ?
0.67 81.95 ?
0.822 87.77 ?
0.51 88.65 ?
0.54 84.02 ?
0.47 84.26 ?
0.52 85.38 ?
0.773 89.82 ?
0.53 88.82 ?
0.66 83.93 ?
0.57 84.24 ?
0.54 87.04 ?
0.954 85.22 ?
0.35 89.08 ?
0.50 84.93 ?
0.57 85.57 ?
0.53 87.77 ?
0.895 85.44 ?
0.45 91.38 ?
0.42 86.90 ?
0.48 86.90 ?
0.53 87.60 ?
0.876 90.26 ?
0.71 90.70 ?
0.48 85.26 ?
0.56 86.05 ?
0.64 88.52 ?
0.927 88.60 ?
0.81 88.99 ?
0.51 83.38 ?
0.38 84.69 ?
0.43 87.08 ?
0.878 86.40 ?
0.51 92.21 ?
0.27 86.57 ?
0.42 87.29 ?
0.47 88.77 ?
0.779 87.12 ?
0.52 90.83 ?
0.43 87.21 ?
0.42 87.99 ?
0.53 89.04 ?
0.6710 87.60 ?
0.38 91.16 ?
0.43 86.68 ?
0.40 86.97 ?
0.41 88.91 ?
0.6811 86.72 ?
0.46 91.16 ?
0.35 87.47 ?
0.40 87.05 ?
0.63 89.47 ?
0.67aSVM with linear kernel (Rainbow)bSVM with polynomial kernel (Rainbow)cSVM with RBF kernel (libsvm)Table 7: Accuracy and standard error (%) estimates for the dataset using naive Bayes (NB), maximumentropy (ME), and three SVM settings at the different attribute configurations.the RBF (gaussian) kernel by selecting model pa-rameters C (penalty for misclassification) and ?
(function of the deviation of the Gaussian Kernel)so that the classifier can accurately predict testingdata.
This experiment is based on the argumentthat if a complete model selection using the gaus-sian kernel has been conducted, there is no needto consider linear SVM, because the RBF kernelwith certain parameters (C , ?)
has the same per-formance as the linear kernel with a penalty pa-rameter C?
(Keerthi and Lin, 2003).Given the finite dataset, we use k-fold cross-validation (k = 20) to estimate the future perfor-mance of each classifier induced by its learningmethod and dataset.
This estimation method intro-duces lower bias compared to a bootstrap methodwhich has extremely large bias on some problems(Kohavi, 1995).7 EvaluationWe evaluated each configuration of Section 5 andeach learning method of Section 6 on the datasetwhich consists of 1336 definitions and 963 non-definitions sentences.
Table 7 reports the accuracyand standard error estimated from this experiment.In all experiment runs, all of the classifiers in allconfigurations outperform our baseline (75.9%).The best accuracy of each classifier (bold) is be-tween 11.57% to 16.31% above the baseline.The bigram only attributes (config.
2) clearlyoutperform the simplest setting (bag-of-word onlyattributes) for all classifiers.
The combination ofboth attributes (config.
3) achieves some improve-ment between 0.17% to 4.41% from configuration2.
It is surprising that naive Bayes shows the bestand relatively high accuracy in this base config-uration (89.82%) and even outperforms all othersettings.Adding syntactic properties (config.
4) or posi-tion of sentences in documents (config.
6) to thebase configuration clearly gives some improve-ment (in 4 and 5 classifiers respectively for eachconfiguration).
But, adding root forms (config.7) does not significantly contribute to an improve-ment.
These results show that in general, syntacticproperties can improve the performance of mostclassifiers.
The results also support the intuitionthat the position of sentences in documents playsimportant role in identifying definition sentences.Moreover, this intuition is also supported by theresult that the best performance of naive Bayes isachieved at configuration 6 (90.26%).
Comparedto the syntactic features, sentence positions givebetter accuracy in all classifiers.The above results demonstrate an interestingfinding that a simple attribute set which consists ofbag-of-words, bigrams, and sentence position un-der a fast and simple classifier (e.g.
naive Bayes)could give a relatively high accuracy.
One expla-nation that we can think of is that candidate sen-tences have been syntactically very well extractedwith our filter.
Thus, the sentences are biased bythe filter from which important words and bigramsof definitions can be found in most of the sen-69tences.
For example, the word and bigrams is een(is a), een (a), zijn (are), is (is), zijn de (are the),and is van (is of) are good clues to definitions andconsequently have high information gain.
We haveto test this result in a future work on candidate def-inition sentences which are extracted by filters us-ing various other syntactic patterns.More improvement is shown when both syntac-tic properties and sentence position are added to-gether (config.
8).
All of the classifiers in this con-figuration obtain more error reduction comparedto the base configuration.
Moreover, the best ac-curacy of this experiment is shown by maximumentropy at this configuration (92.21%).
This maybe a sign that our proposed syntactic properties aregood indicators to identify definition sentences.Other interesting findings can be found in theaddition of named entity classes to configuration3 (config.
5), to configuration 8 (config.
9) andto configuration 10 (config.
11).
In these con-figurations, adding NEC increases accuracies ofalmost all classifiers.
On the other hand, addingroot forms to configuration 3 (config.
7) and toconfiguration 8 (config.
10) does not improve ac-curacies.
However, the best accuracies of naiveBayes (90.26%) and maximum entropy (92.21%)are achieved when named entity and root forms arenot included as attributes.We now evaluate the classifiers.
It is clearfrom the table that SVM1 and SVM2 settings cannot achieve better accuracy compared to the naiveBayes setting, while SVM3 setting marginally out-performs naive Bayes (on 6 out of 11 configura-tions).
This result is contrary to the superiority ofSVMs in many text classification tasks.
Huang etal.
(2003) reported that both classifiers show sim-ilar predictive accuracy and AUC (area under theROC (Receiver Operating Characteristics) curve)scores.
This performance of naive Bayes supportsthe motivation behind its renaisance in machinelearning (Lewis, 1998).From the three SVM settings, SVM with RBFkernel appears as the best classifier for our taskin which it outperforms other SVMs settings inall configurations.
This result supports the abovementioned argument that if the bestC and ?
can beselected, we do not need to consider linear SVM(e.g.
the svm1 setting).Among all of the classifiers, maximum entropyshows the best accuracy.
It wins at 9 out of 11configurations in all experiments.
This result con-firms previous reports e.g.
in Nigam et al (1999)that maximum entropy performs better than naiveBayes in some text classification tasks.8 Conclusions and future workWe have presented an experiment in identifyingdefinition sentences using syntactic properties andlearning-based methods.
Our method is concen-trated on improving the precision of recognizingdefinition sentences.
The first step is extractingcandidate definition sentences from a fully parsedtext using syntactic properties of definitions.
Todistinguish definition from non-definition sen-tences, we investigated several machine learningmethods, namely naive Bayes, maximum entropy,and SVMs.
We also experimented with several at-tribute configurations.
In this selection, we com-bine text properties, document properties, and syn-tactic properties of the sentences.
We have shownthat adding syntactic properties, in particular theposition of subjects in the sentence, type of de-terminer of each subject and predicative comple-ment, improves the accuracy of most machinelearning techniques, and leads to the most accu-rate result overall.Our method has been evaluated on a subset ofmanually annotated data from Wikipedia.
Thecombination of highly structured text material anda syntactic filter leads to a relatively high initialbaseline.Our results on the performance of SVMs do notconfirm the superiority of this learning method for(text) classification tasks.
Naive Bayes, which iswell known from its simplicity, appears to givereasonably high accuracy.
Moreover, it achievesa high accuracy on simple attribute configurationsets (containing no syntactic properties).
In gen-eral, our method will give the best result if allproperties except named entity classes and rootforms are used as attributes and maximum entropyis applied as a classifier.We are currently working on using more syn-tactic patterns to extract candidate definition sen-tences.
This will increase the number of definitionsentences that we can identify from text.ReferencesI.
Androutsopoulos and D. Galanis.
2005.
A prac-tically unsupervised learning method to identifysingle-snippet answers to definition questions on theweb.
In Human Language Technology Conference70and Conference on Empirical Methods in NaturalLanguage Processing (HLT-EMNLP 2005), Vancou-ver, Canada.S.
Blair-Goldensohn, K. McKeown, and A.H. Schlaik-jer.
2004.
Answering definitional questions: A hy-brid approach.
In New Directions in Question An-swering, pages 47?58.Gosse Bouma, Jori Mur, Gertjan van Noord, Lonnekevan der Plas, and Jo?rg Tiedemann.
2005.
Questionanswering for Dutch using dependency relations.
InWorking Notes for the CLEF 2005 Workshop, Vi-enna.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
of the 14thCOLING, pages 539?545, Nantes, France.W.
Hildebrandt, B. Katz, and J.J. Lin.
2004.
An-swering definition questions with multiple knowl-edge sources.
In HLT-NAACL, pages 49?56.Jin Huang, Jingjing Lu, and Charles X. Ling.
2003.Comparing naive bayes, decision trees, and svmwith auc and accuracy.
In ICDM ?03: Proceedingsof the Third IEEE International Conference on DataMining, Washington, DC, USA.
IEEE Computer So-ciety.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: learning with many rele-vant features.
In Claire N?edellec and C?eline Rou-veirol, editors, Proceedings of ECML-98, 10th Euro-pean Conference on Machine Learning, pages 137?142, Chemnitz, DE.
Springer Verlag, Heidelberg,DE.H.
Joho and M. Sanderson.
2000.
Retrieving descrip-tive phrases from large amounts of free text.
InCIKM, pages 180?186.S.
Sathiya Keerthi and Chih-Jen Lin.
2003.
Asymp-totic behaviors of support vector machines withgaussian kernel.
Neural Comput., 15(7):1667?1689.J.L.
Klavans and S. Muresan.
2000.
Definder: Rule-based methods for the extraction of medical termi-nology and their associated definitions from on-linetext.
In American Medical Informatics Assoc 2000.Ron Kohavi.
1995.
A study of cross-validation andbootstrap for accuracy estimation and model selec-tion.
In IJCAI, pages 1137?1145.David D. Lewis.
1998.
Naive (Bayes) at forty: The in-dependence assumption in information retrieval.
InClaire Ne?dellec and Ce?line Rouveirol, editors, Pro-ceedings of ECML-98, 10th European Conferenceon Machine Learning, pages 4?15, Chemnitz, DE.Springer Verlag, Heidelberg, DE.B.
Liu, C.W.
Chin, and H.T.
Ng.
2003.
Mining topic-specific concepts and definitions on the web.
InWWW ?03: Proceedings of the 12th internationalconference on World Wide Web, pages 251?260,New York, NY, USA.
ACM Press.Robert Malouf and Gertjan van Noord.
2004.
Widecoverage parsing with stochastic attribute valuegrammars.
In IJCNLP-04 Workshop Beyond Shal-low Analyses - Formalisms and statistical modelingfor deep analyses, Hainan.A McCallum.
2000.
Bow: A toolkit for statisti-cal language modeling, text retrieval, classificationand clustering.
http://www.cs.cmu.edu/?mccallum/bow.S.
Miliaraki and I. Androutsopoulos.
2004.
Learningto identify single-snippet answers to definition ques-tions.
In 20th International Conference on Compu-tational Linguistics (COLING 2004), pages 1360?1366, Geneva, Switzerland.
COLING 2004.K.
Nigam, J. Lafferty, and A. McCallum.
1999.
Usingmaximum entropy for text classification.
In IJCAI-99 Workshop on Machine Learning for InformationFiltering, pages 61?67.Juan C. Sager and M.C.
L?Homme.
1994.
A modelfor definition of concepts.
Terminology, pages 351?374.Erik Tjong Kim Sang, Gosse Bouma, and Maartende Rijke.
2005.
Developing offline strategies foranswering medical questions.
In Diego Molla?
andJose?
Luis Vicedo, editors, AAAI 2005 workshop onQuestion Answering in Restricted Domains.71
