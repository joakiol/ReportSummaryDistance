Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173?2182,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSimVerb-3500: A Large-Scale Evaluation Set of Verb SimilarityDaniela Gerz1, Ivan Vulic?1, Felix Hill1, Roi Reichart2, and Anna Korhonen11Language Technology Lab, DTAL, University of Cambridge2Faculty of Industrial Engineering and Management, Technion, IIT1{dsg40,iv250,fh295,alk23}@cam.ac.uk2roiri@ie.technion.ac.ilAbstractVerbs play a critical role in the meaning ofsentences, but these ubiquitous words have re-ceived little attention in recent distributional se-mantics research.
We introduce SimVerb-3500,an evaluation resource that provides humanratings for the similarity of 3,500 verb pairs.SimVerb-3500 covers all normed verb typesfrom the USF free-association database, pro-viding at least three examples for every Verb-Net class.
This broad coverage facilitates de-tailed analyses of how syntactic and seman-tic phenomena together influence human un-derstanding of verb meaning.
Further, withsignificantly larger development and test setsthan existing benchmarks, SimVerb-3500 en-ables more robust evaluation of representationlearning architectures and promotes the devel-opment of methods tailored to verbs.
We hopethat SimVerb-3500 will enable a richer under-standing of the diversity and complexity ofverb semantics and guide the development ofsystems that can effectively represent and in-terpret this meaning.1 IntroductionVerbs are famously both complex and variable.
Theyexpress the semantics of an event as well the rela-tional information among participants in that event,and they display a rich range of syntactic and seman-tic behaviour (Jackendoff, 1972; Gruber, 1976; Levin,1993).
Verbs play a key role at almost every level oflinguistic analysis.
Information related to their predi-cate argument structure can benefit many NLP tasks(e.g.
parsing, semantic role labelling, information ex-traction) and applications (e.g.
machine translation,text mining) as well as research on human languageacquisition and processing (Korhonen, 2010).
Precisemethods for representing and understanding verb se-mantics will undoubtedly be necessary for machinesto interpret the meaning of sentences with similaraccuracy to humans.Numerous algorithms for acquiring word represen-tations from text and/or more structured knowledgebases have been developed in recent years (Mikolovet al, 2013; Pennington et al, 2014; Faruqui et al,2015).
These representations (or embeddings) typ-ically contain powerful features that are applicableto many language applications (Collobert and We-ston, 2008; Turian et al, 2010).
Nevertheless, thepredominant approaches to distributed representationlearning apply a single learning algorithm and repre-sentational form for all words in a vocabulary.
Thisis despite evidence that applying different learningalgorithms to word types such as nouns, adjectivesand verbs can significantly increase the ultimate use-fulness of representations (Schwartz et al, 2015).One factor behind the lack of more nuanced wordrepresentation learning methods is the scarcity of sat-isfactory ways to evaluate or analyse representationsof particular word types.
Resources such as MEN(Bruni et al, 2014), Rare Words (Luong et al, 2013)and SimLex-999 (Hill et al, 2015) focus either onwords from a single class or small samples of differ-ent word types, with automatic approaches alreadyreaching or surpassing the inter-annotator agreementceiling.
Consequently, for word classes such as verbs,whose semantics is critical for language understand-ing, it is practically impossible to achieve statisticallyrobust analyses and comparisons between different2173representation learning architectures.To overcome this barrier to verb semantics re-search, we introduce SimVerb-3500 ?
an extensiveintrinsic evaluation resource that is unprecedentedin both size and coverage.
SimVerb-3500 includes827 verb types from the University of South FloridaFree Association Norms (USF) (Nelson et al, 2004),and at least 3 member verbs from each of the 101top-level VerbNet classes (Kipper et al, 2008).
Thiscoverage enables researchers to better understandthe complex diversity of syntactic-semantic verb be-haviours, and provides direct links to other estab-lished semantic resources such as WordNet (Miller,1995) and PropBank (Palmer et al, 2005).
More-over, the large standardised development and test setsin SimVerb-3500 allow for principled tuning of hy-perparameters, a critical aspect of achieving strongperformance with the latest representation learningarchitectures.In ?
2, we discuss previous evaluation resourcestargeting verb similarity.
We present the newSimVerb-3500 data set alng with our design choicesand the pair selection process in ?
3, while the anno-tation process is detailed in ?
4.
In ?
5 we report theperformance of a diverse range of popular representa-tion learning architectures, together with benchmarkperformance on existing evaluation sets.
In ?
6, weshow how SimVerb-3500 enables a variety of newlinguistic analyses, which were previously impossi-ble due to the lack of coverage and scale in existingresources.2 Related WorkA natural way to evaluate representation quality is byjudging the similarity of representations assigned tosimilar words.
The most popular evaluation sets atpresent consist of word pairs with similarity ratingsproduced by human annotators.1 Nevertheless, wefind that all available datasets of this kind are insuf-ficient for judging verb similarity due to their smallsize or narrow coverage of verbs.In particular, a number of word pair evaluationsets are prominent in the distributional semantics1In some existing evaluation sets pairs are scored for relat-edness which has some overlap with similarity.
SimVerb-3500focuses on similarity as this is a more focused semantic rela-tion that seems to yield a higher agreement between humanannotators.
For a broader discussion see (Hill et al, 2015).literature.Representative examples include RG-65 (Ruben-stein and Goodenough, 1965) and WordSim-353(Finkelstein et al, 2002; Agirre et al, 2009) whichare small (65 and 353 word pairs, respectively).Larger evaluation sets such as the Rare Words evalu-ation set (Luong et al, 2013) (2034 word pairs) andthe evaluations sets from Silberer and Lapata (2014)are dominated by noun pairs and the former also fo-cuses on low-frequency phenomena.
Therefore, thesedatasets do not provide a representative sample ofverbs (Hill et al, 2015).Two datasets that do focus on verb pairs to someextent are the data set of Baker et al (2014) andSimlex-999 (Hill et al, 2015).
These datasets, how-ever, still contain a limited number of verb pairs (134and 222, respectively), making them unrepresentativeof the rich variety of verb semantic phenomena.In this paper we provide a remedy for this problemby presenting a more comprehensive and representa-tive verb pair evaluation resource.3 The SimVerb-3500 Data SetIn this section, we discuss the design principles be-hind SimVerb-3500.
We first demonstrate that a newevaluation resource for verb similarity is a necessity.We then describe how the final verb pairs were se-lected with the goal to be representative, that is, toguarantee a wide coverage of two standard semanticresources: USF and VerbNet.3.1 Design MotivationHill et al (2015) argue that comprehensive high-quality evaluation resources have to satisfy the fol-lowing three criteria: (C1) Representative (the re-source covers the full range of concepts occurringin natural language); (C2) Clearly defined (it clearlydefines the annotated relation, e.g., similarity); (C3)Consistent and reliable (untrained native speakersmust be able to quantify the target relation consis-tently relying on simple instructions).Building on the same annotation guidelines asSimlex-999 that explicitly targets similarity, we en-sure that criteria C2 and C3 are satisfied.
However,even SimLex, as the most extensive evaluation re-source for verb similarity available at present, is stillof limited size, spanning only 222 verb pairs and 1702174distinct verb lemmas in total.
Given that 39 out of the101 top-level VerbNet classes are not represented atall in SimLex, while 20 classes have only one mem-ber verb,2 one may conclude that the criterion C1 isnot at all satisfied with current resources.There is another fundamental limitation of allcurrent verb similarity evaluation resources: auto-matic approaches have reached or surpassed the inter-annotator agreement ceiling.
For instance, while theaverage pairwise correlation between annotators onSL-222 is Spearman?s ?
correlation of 0.717, thebest performing automatic system reaches ?
= 0.727(Mrk?ic?
et al, 2016).
SimVerb-3500 does not inheritthis anomaly (see Tab.
2) and demonstrates that therestill exists an evident gap between the human andsystem performance.In order to satisfy C1-C3, the new SimVerb-3500evaluation set contains similarity ratings for 3,500verb pairs, containing 827 verb types in total and3 member verbs for each top-level VerbNet class.The rating scale goes from 0 (not similar at all) to10 (synonymous).
We employed the SimLex-999annotation guidelines.
In particular, we instructedannotators to give low ratings to antonyms, and todistinguish between similarity and relatedness.
Pairsthat are related but not similar (e.g., to snore / tosnooze, to walk / to crawl) thus have a fairly lowrating.
Several example pairs are provided in Tab.
1.3.2 Choice of Verb Pairs and CoverageTo ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs issteered by two standard semantic resources availableonline: (1) the USF norms data set3 (Nelson et al,2004), and (2) the VerbNet verb lexicon4 (Kipper etal., 2004; Kipper et al, 2008).The USF norms data set (further USF) is thelargest database of free association collected for En-glish.
It was generated by presenting human subjectswith one of 5, 000 cue concepts and asking them towrite the first word coming to mind that is associatedwith that concept.
Each cue concept c was normed in2Note that verbs in VerbNet are soft clustered, and one verbtype may be associated with more than one class.
When comput-ing coverage, we assume that such verbs attribute to counts ofall their associated classes.3http://w3.usf.edu/FreeAssociation/4http://verbs.colorado.edu/verb-index/Pair Ratingto reply / to respond 9.79to snooze / to nap 8.80to cook / to bake 7.80to participate / to join 5.64to snore / to snooze 4.15to walk / to crawl 2.32to stay / to leave 0.17to snooze / to happen 0.00Table 1: Example verb pairs from SimVerb-3500.this way by over 10 participants, resulting in a set ofassociates a for each cue, for a total of over 72, 000(c, a) pairs.
For each such pair, the proportion of par-ticipants who produced associate a when presentedwith cue c can be used as a proxy for the strength ofassociation between the two words.The norming process guarantees that two words ina pair have a degree of semantic association whichcorrelates well with semantic relatedness and simi-larity.
Sampling from the USF set ensures that bothrelated but non-similar pairs (e.g., to run / to sweat)as well as similar pairs (e.g., to reply / to respond)are represented in the final list of pairs.
Further, therich annotations of the output USF data (e.g., con-creteness scores, association strength) can be directlycombined with the SimVerb-3500 similarity scoresto yield additional analyses and insight.VerbNet (VN) is the largest online verb lexiconcurrently available for English.
It is hierarchical,domain-independent, and broad-coverage.
VN is or-ganised into verb classes extending the classes fromLevin (1993) through further refinement to achievesyntactic and semantic coherence among class mem-bers.
According to the official VerbNet guidelines,5?Verb Classes are numbered according to shared se-mantics and syntax, and classes which share a top-level number (9-109) have corresponding semanticrelationships.?
For instance, all verbs from the top-level Class 9 are labelled ?Verbs of Putting?, all verbsfrom Class 30 are labelled ?Verbs of Perception?,while Class 39 contains ?Verbs of Ingesting?.Among others, three basic types of informationare covered in VN: (1) verb subcategorization frames(SCFs), which describe the syntactic realization ofthe predicate-argument structure (e.g.
The windowbroke), (2) selectional preferences (SPs), which cap-ture the semantic preferences verbs have for their5http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf2175arguments (e.g.
a breakable physical object broke)and (3) lexical-semantic verb classes (VCs) whichprovide a shared level of abstraction for verbs similarin their (morpho-)syntactic and semantic properties(e.g.
BREAK verbs, sharing the VN class 45.1, andthe top-level VN class 45).6 The basic overview ofthe VerbNet structure already suggests that measur-ing verb similarity is far from trivial as it revolvesaround a complex interplay between various semanticand syntactic properties.The wide coverage of VN in SimVerb-3500assures the wide coverage of distinct verbgroups/classes and their related linguistic phenom-ena.
Finally, VerbNet enables further connections ofSimVerb-3500 to other important lexical resourcessuch as FrameNet (Baker et al, 1998), WordNet(Miller, 1995), and PropBank (Palmer et al, 2005)through the sets of mappings created by the SemLinkproject initiative (Loper et al, 2007).7Sampling Procedure We next sketch the completesampling procedure which resulted in the final set of3500 distinct verb pairs finally annotated in a crowd-sourcing study (?
4).
(Step 1) We extracted all possible verb pairs fromUSF based on the associated POS tags available aspart of USF annotations.
To ensure that semanticassociation between verbs in a pair is not accidental,we then discarded all such USF pairs that had beenassociated by 2 or less participants in USF.
(Step 2) We then manually cleaned and simplifiedthe list of pairs by removing all pairs with multi-wordverbs (e.g., quit / give up), all pairs that containedthe non-infinitive form of a verb (e.g., accomplished /finished, hidden / find), removing all pairs containingat least one auxiliary verb (e.g., must / to see, must / tobe).
The first two steps resulted in 3,072 USF-basedverb pairs.
(Step 3) After this stage, we noticed that several top-level VN classes are not part of the extracted set.For instance, 5 VN classes did not have any memberverbs included, 22 VN classes had only 1 verb, and 6VN classes had 2 verbs included in the current set.We resolved the VerbNet coverage issue by sam-pling from such ?under-represented?
VN classes di-rectly.
Note that this step is not related to USF at6https://verbs.colorado.edu/verb-index/vn/break-45.1.php7https://verbs.colorado.edu/semlink/all.
For each such class we sampled additional verbtypes until the class was represented by 3 or 4 mem-ber verbs (chosen randomly).8 Following that, wesampled at least 2 verb pairs for each previously?under-represented?
VN class by pairing 2 memberverbs from each such class.
This procedure resultedin 81 additional pairs, now 3,153 in total.
(Step 4) Finally, to complement this set with a sam-ple of entirely unassociated pairs, we followed theSimLex-999 setup.
We paired up the verbs from the3,153 associated pairs at random.
From these ran-dom parings, we excluded those that coincidentallyoccurred elsewhere in USF (and therefore had a de-gree of association).
We sampled the remaining 347pairs from this resulting set of unassociated pairs.
(Output) The final SimVerb-3500 data set contains3,500 verb pairs in total, covering all associated verbpairs from USF, and (almost) all top-level VerbNetclasses.
All pairs were manually checked post-hocby the authors plus 2 additional native English speak-ers to verify that the final data set does not containunknown or invalid verb types.Frequency Statistics The 3,500 pairs consist of827 distinct verbs.
29 top-level VN classes are rep-resented by 3 member verbs, while the three mostrepresented classes cover 79, 85, and 93 memberverbs.
40 verbs are not members of any VN class.We performed an initial frequency analysis ofSimVerb-3500 relying on the BNC counts availableonline (Kilgarriff, 1997).9 After ranking all BNCverbs according to their frequency, we divided thelist into quartiles: Q1 (most frequent verbs in BNC)- Q4 (least frequent verbs in BNC).
Out of the 827SimVerb-3500 verb types, 677 are contained in Q1,122 in Q2, 18 in Q3, 4 in Q4 (to enroll, to hitchhike,to implode, to whelp), while 6 verbs are not coveredin the BNC list.
2,818 verb pairs contain Q1 verbs,while there are 43 verb pairs with both verbs not inQ1.
Further empirical analyses are provided in ?
6.108The following three VN classes are exceptions: (1) Class56, consisting of words that are dominantly tagged as nouns,but can be used as verbs exceptionally (e.g., holiday, summer,honeymoon); (2) Class 91, consisting of 2 verbs (count, matter);(3) Class 93, consisting of 2 single word verbs (adopt, assume).9https://www.kilgarriff.co.uk/bnc-readme.html10Annotations such as VerbNet class membership, relationsbetween WordNet synsets of each verb, and frequency statisticsare available as supplementary material.21764 Word Pair ScoringWe employ the Prolific Academic (PA) crowdsourc-ing platform,11 an online marketplace very similar toAmazon Mechanical Turk and to CrowdFlower.4.1 Survey StructureFollowing the SimLex-999 annotation guidelines, wehad each of the 3500 verb pairs rated by at least 10annotators.
To distribute the workload, we dividedthe 3500 pairs into 70 tranches, with 79 pairs each.Out of the 79 pairs, 50 are unique to one tranche,while 20 manually chosen pairs are in all tranches toensure consistency.
The remaining 9 are duplicatepairs displayed to the same participant multiple timesto detect inconsistent annotations.Participants see 7-8 pairs per page.
Pairs are ratedon a scale of 0-6 by moving a slider.
The first pageshows 7 pairs, 5 unique ones and 2 from the con-sistency set.
The following pages are structured thesame but display one extra pair from the previouspage.
Participants are explicitly asked to give theseduplicate pairs the same rating.
We use them asquality control so that we can identify and excludeparticipants giving several inconsistent answers.Checkpoint Questions The survey contains threecontrol questions in which participants are asked toselect the most similar pair out of three choices.
Forinstance, the first checkpoint is: Which of these pairsof words is the *most* similar?
1. to run / to jog 2. torun / to walk 3. to jog / to sweat.
One checkpoint oc-curs right after the instructions and the other two laterin the survey.
The purpose is to check that annotatorshave understood the guidelines and to have anotherquality control measure for ensuring that they arepaying attention throughout the survey.
If just oneof the checkpoint questions is answered incorrectly,the survey ends immediately and all scores from theannotator in question are discarded.Participants 843 raters participated in the study,producing over 65,000 ratings.
Unlike other crowd-sourcing platforms, PA collects and stores detaileddemographic information from the participants up-front.
This information was used to carefully selectthe pool of eligible participants.
We restricted thepool to native English speakers with a 90% approval11https://prolific.ac/ (We chose PA for logistic reasons.
)rate (maximum rate on PA), of age 18-50, born andcurrently residing in the US (45% out of 843 raters),UK (53%), or Ireland (2%).
54% of the raters werefemale and 46% male, with the average age of 30.Participants took 8 minutes on average to completethe survey containing 79 questions.4.2 Post-ProcessingWe excluded ratings of annotators who (a) answeredone of the checkpoint questions incorrectly (75% ofexclusions); (b) did not give equal ratings to dupli-cate pairs; (c) showed suspicious rating patterns (e.g.,randomly alternating between two ratings or usingone single rating throughout).
The final acceptancerate was 84%.
We then calculated the average of allratings from the accepted raters ( ?
10 ) for each pair.The score was finally scaled linearly from the 0-6 tothe 0-10 interval as in (Hill et al, 2015).5 AnalysisInter-Annotator Agreement We employ twomeasures.
IAA-1 (pairwise) computes the averagepairwise Spearman?s ?
correlation between any tworaters ?
a common choice in previous data collec-tion in distributional semantics (Pad?
et al, 2007;Reisinger and Mooney, 2010a; Silberer and Lapata,2014; Hill et al, 2015).A complementary measure would smooth individ-ual annotator effects.
For this aim, our IAA-2 (mean)measure compares the average correlation of a hu-man rater with the average of all the other raters.SimVerb-3500 obtains ?
= 0.84 (IAA-1) and ?
= 0.86(IAA-2), a very good agreement compared to otherbenchmarks (see Tab.
2).Vector Space Models We compare the perfor-mance of prominent representation models onSimVerb-3500.
We include: (1) unsupervised mod-els that learn from distributional information in text,including the skip-gram negative-sampling model(SGNS) with various contexts (BOW = bag of words;DEPS = dependency contexts) as in Levy and Gold-berg (2014), the symmetric-pattern based vectorsby Schwartz et al (2015), and count-based PMI-weighted vectors (Baroni et al, 2014); (2) Mod-els that rely on linguistic hand-crafted resources orcurated knowledge bases.
Here, we use sparse bi-nary vectors built from linguistic resources (Non-2177Eval set IAA-1 IAA-2 ALL TEXTWSIM 0.67 0.65 0.79 0.79(203) SGNS-BOW SGNS-BOWSIMLEX 0.67 0.78 0.74 0.56(999) Paragram+CF SymPat+SGNSSL-222 0.72 - 0.73 0.58(222) Paragram+CF SymPatSIMVERB 0.84 0.86 0.63 0.36(3500) Paragram+CF SGNS-DEPSTable 2: An overview of word similarity evaluation benchmarks.ALL is the current best reported score on each data set acrossall models (including the models that exploit curated knowledgebases and hand-crafted lexical resources, see supplementarymaterial).
TEXT denotes the best reported score for a modelthat learns solely on the basis of distributional information.
Allscores are Spearman?s ?
correlations.Distributional, (Faruqui and Dyer, 2015)), and vec-tors fine-tuned to a paraphrase database (Paragram,(Wieting et al, 2015)) further refined using linguisticconstraints (Paragram+CF, (Mrk?ic?
et al, 2016)).Descriptions of these models are in the supplemen-tary material.Comparison to SimLex-999 (SL-222) 170 pairsfrom SL-222 also appear in SimVerb-3500.
The cor-relation between the two data sets calculated on theshared pairs is ?
= 0.91.
This proves, as expected,that the ratings are consistent across the two data sets.Tab.
3 shows a comparison of models?
perfor-mance on SimVerb-3500 against SL-222.
Since thenumber of evaluation pairs may influence the results,we ideally want to compare sets of equal size for a faircomparison.
Picking one random subset of 222 pairswould bias the results towards the selected pairs, andeven using 10-fold cross-validation we found varia-tions up to 0.05 depending on which subsets wereused.
Therefore, we employ a 2-level 10-fold cross-validation where new random subsets are picked ineach iteration of each model.
The numbers reportedas CV-222 are averages of these ten 10-fold cross-validation runs.
The reported results come very closeto the correlation on the full data set for all models.Most models perform much better on SL-222, es-pecially those employing additional databases or lin-guistic resources.
The performance of the best scor-ing Paragram+CF model is even on par with theIAA-1 of 0.72.
The same model obtains the high-est score on SV-3500 (?
= 0.628), with a clear gapto IAA-1 of 0.84.
We attribute these differences inperformance largely to SimVerb-3500 being a moreextensive and diverse resource in terms of verb pairs.Development Set A common problem in scoredword pair datasets is the lack of a standard split todevelopment and test sets.
Previous works oftenoptimise models on the entire dataset, which leads tooverfitting (Faruqui et al, 2016) or use custom splits,e.g., 10-fold cross-validation (Schwartz et al, 2015),which make results incomparable with others.
Thelack of standard splits stems mostly from small sizeand poor coverage ?
issues which we have solvedwith SimVerb-3500.Our development set contains 500 pairs, selectedto ensure a broad coverage in terms of similarityranges (i.e., non-similar and highly similar pairs, aswell as pairs of medium similarity are represented)and top-level VN classes (each class is representedby at least 1 member verb).
The test set includesthe remaining 3,000 verb pairs.
The performances ofrepresentation learning architectures on the dev andtest sets are reported in Tab.
3.
The ranking of modelsis identical on the test and the full SV-3500 set, withslight differences in ranking on the development set.6 Evaluating SubsetsThe large coverage and scale of SimVerb-3500 en-ables model evaluation based on selected criteria.
Inthis section, we showcase a few example analyses.Frequency In the first analysis, we select pairsbased on their lemma frequency in the BNC corpusand form three groups, with 390-490 pairs in eachgroup (Fig.
1).
The results from Fig.
1 suggest thatthe performance of all models improves as the fre-quency of the verbs in the pair increases, with muchsteeper curves for the purely distributional models(e.g., SGNS and SymPat).
The non-distributionalnon data-driven model of Faruqui and Dyer (2015) isonly slightly affected by frequency.WordNet Synsets Intuitively, representations forverbs with more diverse usage patterns are more dif-ficult to learn with statistical models.
To examinethis hypothesis, we resort to WordNet (Miller, 1995),where different semantic usages of words are listedas so-called synsets.
Fig.
2 shows a clear downwardtrend for all models, confirming that polysemous2178Model SV-3500 CV-222 SL-222 DEV-500 TEST-3000SGNS-BOW-PW (d=300) 0.274 0.279 0.328 0.333 0.265SGNS-DEPS-PW (d=300) 0.313 0.314 0.390 0.401 0.304SGNS-UDEPS-PW (d=300) 0.259 0.262 0.347 0.313 0.250SGNS-BOW-8B (d=500) 0.348 0.343 0.307 0.378 0.350SGNS-DEPS-8B (d=500) 0.356 0.347 0.385 0.389 0.351SYMPAT-8B (d=500) 0.328 0.336 0.544 0.276 0.347COUNT-SVD (d=500) 0.196 0.200 0.059 0.259 0.186NON-DISTRIBUTIONAL 0.596 0.596 0.689 0.632 0.600PARAGRAM (d=25) 0.418 0.432 0.531 0.443 0.433PARAGRAM (d=300) 0.540 0.528 0.590 0.525 0.537PARAGRAM+CF (d=300) 0.628 0.625 0.727 0.611 0.624Table 3: Evaluation of state-of-the representation learning models on the full SimVerb-3500 set (SV-3500), the Simlex-999verb subset containing 222 pairs (SL-222), cross-validated subsets of 222 pairs from SV-3500 (CV-222), and the SimVerb-3500development (DEV-500) and test set (TEST-3000).0.10.20.30.40.50.60.7[5000,+?
> [1000, 5000 > [0, 1000 >Spearman?s?Lemma occurrences in BNCSGNS-BOW-8BSGNS-DEPS-8BSymPat-500-8BNon-DistributionalPARAGRAM-300PARAGRAM-300Figure 1: Subset-based evaluation, where subsets are createdbased on the frequency of verb lemmas in the BNC corpus.
Eachof the three frequency groups contains 390-490 verb pairs.
Tobe included in each group it is required that both verbs in a pairare contained in the same frequency interval (x axis).verbs are more difficult for current verb representa-tion models.
Nevertheless, approaches which useadditional information beyond corpus co-occurrenceare again more robust.
Their performance only dropssubstantially for verbs with more than 10 synsets,while the performance of other models deteriorates al-ready when tackling verbs with more than 5 synsets.VerbNet Classes Another analysis enabled bySimVerb-3500 is investigating the connection be-tween VerbNet classes and human similarity judg-ments.
We find that verbs in the same top-level Verb-Net class are often not assigned high similarity score.Out of 1378 pairs where verbs share the top-levelVerbNet class, 603 have a score lower than 5.
Tab.
4reports scores per VerbNet class.
When a verb be-0.10.20.30.40.50.60.7[0, 5 > [5, 10 > [10,+?
>Spearman?s?Number of WN synsetsSGNS-BOW-8BSGNS-DEPS-8BSymPat-500-8BNon-DistributionalPARAGRAM-300PARAGRAM-300Figure 2: Subset-based evaluation, where subsets are createdbased on the number of synsets in WordNet (x axis).
To beincluded in each subset it is required that both verbs in a pairhave the number of synsets in the same interval.longs to multiple classes, we count it for each class(see Footnote 2).
We run the analysis on the fivelargest VN classes, each with more than 100 pairswith paired verbs belonging to the same class.The results indicate clear differences betweenclasses (e.g., Class 31 vs Class 51), and suggest thatfurther developments in verb representation learningshould also focus on constructing specialised repre-sentations at the finer-grained level of VN classes.Lexical Relations SimVerb-3500 contains rela-tion annotations (e.g., antonyms, synonyms, hyper-/hyponyms, no relation) for all pairs extracted au-tomatically from WordNet.
Evaluating per-relationsubsets, we observe that some models draw theirstrength from good performance across different re-2179Model #13 #31 #37 #45 #51SGNS-BOW-8B 0.210 0.308 0.352 0.270 0.170SGNS-DEPS-8B 0.289 0.270 0.306 0.238 0.225SYMPAT-8B (d=500) 0.171 0.320 0.143 0.195 0.113NON-DISTR 0.571 0.483 0.372 0.501 0.499PARAGRAM (d=300) 0.571 0.504 0.567 0.531 0.387PARAGRAM+CF 0.735 0.575 0.666 0.622 0.614Table 4: Spearman?s ?
correlation between human judgmentsand model?s cosine similarity by VerbNet Class.
We choseclasses #13 Verbs of Change of Possession, #31 Verbs of Psycho-logical State, #37 Verbs of Communication, #45 Verbs of Changeof State, and #51 Verbs of Motion as examples.
All are largeclasses with more than 100 pairs each, and the frequencies ofmember verbs are distributed in a similar way.Model NR SYN HYPSGNS-BOW-PW (d=300) 0.096 0.288 0.292SGNS-DEPS-PW (d=300) 0.132 0.290 0.336SGNS-BOW-8B (d=500) 0.292 0.273 0.338SGNS-DEPS-8B (d=500) 0.157 0.323 0.378SYMPAT-8B-DENSE (d=300) 0.225 0.182 0.265SYMPAT-8B-DENSE (d=500) 0.248 0.260 0.251NON-DISTRIBUTIONAL 0.126 0.379 0.488PARAGRAM (d=300) 0.254 0.356 0.439PARAGRAM+CF (d=300) 0.250 0.417 0.475Table 5: Spearman?s ?
correlation between human judgmentsand model?s cosine similarity based on pair relation type.
Re-lations are based on WordNet, and included in the dataset.
Theclasses are of different size, 373 pairs with no relation (NR),306 synonym (SYN) pairs, and 800 hyper/hyponym (HYP) pairs.Frequencies of member verbs are distributed in a similar way.lations.
Others have low performance on these pairs,but do very well on synonyms and hyper-/hyponyms.Selected results of this analysis are in Tab.
5.12Human Agreement Motivated by the varying per-formance of computational models regarding fre-quency and ambiguous words with many synsets,we analyse what disagreement effects may be cap-tured in human ratings.
We therefore compute theaverage standard deviation of ratings per subset:avgstdd(S) = 1n?p?S ?
(rp), where S is one subsetof pairs, n is the number of pairs in this subset, p isone pair, and rp are all human ratings for this pair.12 Evaluation based on Spearman?s ?
may be problematicwith certain categories, e.g., with antonyms.
It evaluates pairsaccording to their ranking; for antonyms the ranking is arbitrary -every antonym pair should have a very low rating, hence they arenot included in Tab.
5.
A similar effect occurs with highly rankedsynonyms, but to a much lesser degree than with antonyms.While the standard deviation of ratings is diversefor individual pairs, overall the average standard de-viations per subset are almost identical.
For boththe frequency and the WordNet synset analyses it isaround ?1.3 across all subsets, and with only littledifference for the subsets based on VerbNet.
The onlysubsets where we found significant variations is thegrouping by relations, where ratings tend to be moresimilar especially on antonyms (0.86) and pairs withno relation (0.92), much less similar on synonyms(1.34) and all other relations (?1.4).
These findingssuggest that humans are much less influenced by fre-quency or polysemy in their understanding of verbsemantics compared to computational models.7 ConclusionsSimVerb-3500 is a verb similarity resource for analy-sis and evaluation that will be of use to researchersinvolved in understanding how humans or machinesrepresent the meaning of verbs, and, by extension,scenes, events and full sentences.
The size and cover-age of syntactico-semantic phenomena in SimVerb-3500 makes it possible to compare the strengths andweaknesses of various representation models via sta-tistically robust analyses on specific word classes.To demonstrate the utility of SimVerb-3500, weconducted a selection of analyses with existingrepresentation-learning models.
One clear conclu-sion is that distributional models trained on raw text(e.g.
SGNS) perform very poorly on low frequencyand highly polysemous verbs.
This degradation inperformance can be partially mitigated by focusingmodels on more principled distributional contexts,such as those defined by symmetric patterns.
Moregenerally, the finding suggests that, in order to modelthe diverse spectrum of verb semantics, we may re-quire algorithms that are better suited to fast learningfrom few examples (Lake et al, 2011), and havesome flexibility with respect to sense-level distinc-tions (Reisinger and Mooney, 2010b; Vilnis and Mc-Callum, 2015).
In future work we aim to apply suchmethods to the task of verb acquisition.Beyond the preliminary conclusions from these ini-tial analyses, the benefit of SimLex-3500 will becomeclear as researchers use it to probe the relationshipbetween architectures, algorithms and representationquality for a wide range of verb classes.
Better under-2180standing of how to represent the full diversity of verbsshould in turn yield improved methods for encodingand interpreting the facts, propositions, relations andevents that constitute much of the important informa-tion in language.AcknowledgmentsThis work is supported by the ERC ConsolidatorGrant LEXICAL (648909).ReferencesEneko Agirre, Enrique Alfonseca, Keith B.
Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In NAACL-HLT,pages 19?27.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In ACL-COLING, pages 86?90.Simon Baker, Roi Reichart, and Anna Korhonen.
2014.An unsupervised model for instance level subcatego-rization acquisition.
In EMNLP, pages 278?289.Marco Baroni, Georgiana Dinu, and Germ?n Kruszewski.2014.
Don?t count, predict!
a systematic comparison ofcontext-counting vs. context-predicting semantic vec-tors.
In ACL, pages 238?247.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Artifi-cial Intelligence Research, 49:1?47.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deep neu-ral networks with multitask learning.
In ICML, pages160?167.Manaal Faruqui and Chris Dyer.
2015.
Non-distributionalword vector representations.
In ACL, pages 464?469.Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, ChrisDyer, Eduard H. Hovy, and Noah A. Smith.
2015.Retrofitting word vectors to semantic lexicons.
InNAACL-HLT, pages 1606?1615.Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,and Chris Dyer.
2016.
Problems with evaluation ofword embeddings using word similarity tasks.
CoRR,abs/1605.02276.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, EhudRivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.2002.
Placing search in context: The concept revisited.ACM Transactions on Information Systems, 20(1):116?131.Jeffrey Gruber.
1976.
Lexical structure in syntax andsemantics.
North-Holland Pub.
Co.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.SimLex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguistics,41(4):665?695.Ray S. Jackendoff.
1972.
Semantic interpretation ingenerative grammar.
MIT Press.Adam Kilgarriff.
1997.
Putting frequencies in the dictio-nary.
International Journal of Lexicography, 10(2):135?155.Karin Kipper, Benjamin Snyder, and Martha Palmer.
2004.Extending a verb-lexicon using a semantically anno-tated corpus.
In LREC, pages 1557?1560.Karin Kipper, Anna Korhonen, Neville Ryant, and MarthaPalmer.
2008.
A large-scale classification of Englishverbs.
Language Resource and Evaluation, 42(1):21?40.Anna Korhonen.
2010.
Automatic lexical classification:bridging research and practice.
Philosophical Transac-tions of the Royal Society of London A: Mathematical,Physical and Engineering Sciences, 368(1924):3621?3632.Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross,and Joshua B. Tenenbaum.
2011.
One shot learning ofsimple visual concepts.
In CogSci.Beth Levin.
1993.
English verb classes and alternation,A preliminary investigation.
The University of ChicagoPress.Omer Levy and Yoav Goldberg.
2014.
Dependency-basedword embeddings.
In ACL, pages 302?308.Edward Loper, Szu-Ting Yi, and Martha Palmer.
2007.Combining lexical resources: Mapping between Prop-Bank and VerbNet.
In IWCS.Thang Luong, Richard Socher, and Christopher Manning.2013.
Better word representations with recursive neuralnetworks for morphology.
In CoNLL, pages 104?113.Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jef-frey Dean.
2013.
Efficient estimation of word repre-sentations in vector space.
In ICLR: Workshop Papers.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38(11):39?41.Nikola Mrk?ic?, Diarmuid ?
S?aghdha, Blaise Thomson,Milica Ga?ic?, Lina Maria Rojas-Barahona, Pei-HaoSu, David Vandyke, Tsung-Hsien Wen, and Steve J.Young.
2016.
Counter-fitting word vectors to linguisticconstraints.
In NAACL-HLT, pages 142?148.Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.Schreiber.
2004.
The University of South Florida freeassociation, rhyme, and word fragment norms.
Be-havior Research Methods, Instruments, & Computers,36(3):402?407.Sebastian Pad?, Ulrike Pad?, and Katrin Erk.
2007.
Flex-ible, corpus-based modelling of human plausibilityjudgements.
In EMNLP-CoNLL, pages 400?409.2181Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005.The Proposition Bank: An annotated corpus of seman-tic roles.
Computational Linguistics, 31(1):71?106.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
GloVe: Global vectors for word repre-sentation.
In EMNLP, pages 1532?1543.Joseph Reisinger and Raymond J. Mooney.
2010a.
Amixture model with sharing for lexical semantics.
InEMNLP, pages 1173?1182.Joseph Reisinger and Raymond J Mooney.
2010b.
Multi-prototype vector-space models of word meaning.
InNAACL-HTL, pages 109?117.Herbert Rubenstein and John B Goodenough.
1965.
Con-textual correlates of synonymy.
Communications of theACM, 8(10):627?633.Roy Schwartz, Roi Reichart, and Ari Rappoport.
2015.Symmetric pattern based word embeddings for im-proved word similarity prediction.
In CoNLL, pages258?267.Carina Silberer and Mirella Lapata.
2014.
Learninggrounded meaning representations with autoencoders.In ACL, pages 721?732.Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In ACL, pages384?394.Luke Vilnis and Andrew McCallum.
2015.
Word repre-sentations via Gaussian embedding.
ICLR.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2015.
From paraphrase database to composi-tional paraphrase model and back.
Transactions of theACL, 3:345?358.2182
