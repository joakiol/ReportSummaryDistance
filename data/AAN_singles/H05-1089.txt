Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 708?715, Vancouver, October 2005. c?2005 Association for Computational LinguisticsUsing Sketches to Estimate AssociationsPing LiDepartment of StatisticsStanford UniversityStanford, California 94305pingli@stat.stanford.eduKenneth W. ChurchMicrosoft ResearchOne Microsoft WayRedmond, Washington 98052church@microsoft.comAbstractWe should not have to look at the en-tire corpus (e.g., the Web) to know if twowords are associated or not.1 A powerfulsampling technique called Sketches wasoriginally introduced to remove duplicateWeb pages.
We generalize sketches toestimate contingency tables and associa-tions, using a maximum likelihood esti-mator to find the most likely contingencytable given the sample, the margins (doc-ument frequencies) and the size of thecollection.
Not unsurprisingly, computa-tional work and statistical accuracy (vari-ance or errors) depend on sampling rate,as will be shown both theoretically andempirically.
Sampling methods becomemore and more important with larger andlarger collections.
At Web scale, samplingrates as low as 10?4 may suffice.1 IntroductionWord associations (co-occurrences) have a widerange of applications including: Speech Recogni-tion, Optical Character Recognition and InformationRetrieval (IR) (Church and Hanks, 1991; Dunning,1993; Manning and Schutze, 1999).
It is easy tocompute association scores for a small corpus, butmore challenging to compute lots of scores for lotsof data (e.g.
the Web), with billions of web pages(D) and millions of word types (V ).
For a smallcorpus, one could compute pair-wise associations bymultiplying the (0/1) term-by-document matrix withits transpose (Deerwester et al, 1999).
But this isprobably infeasible at Web scale.1This work was conducted at Microsoft while the first authorwas an intern.
The authors thank Chris Meek, David Hecker-man, Robert Moore, Jonathan Goldstein, Trevor Hastie, DavidSiegmund, Art Own, Robert Tibshirani and Andrew Ng.Approximations are often good enough.
Weshould not have to look at every document to de-termine that two words are strongly associated.
Anumber of sampling-based randomized algorithmshave been implemented at Web scale (Broder, 1997;Charikar, 2002; Ravichandran et al, 2005).2A conventional random sample is constructed byselecting Ds documents from a corpus of D doc-uments.
The (corpus) sampling rate is DsD .
Ofcourse, word distributions have long tails.
Thereare a few high frequency words and many low fre-quency words.
It would be convenient if the sam-pling rate could vary from word to word, unlike con-ventional sampling where the sampling rate is fixedacross the vocabulary.
In particular, in our experi-ments, we will impose a floor to make sure that thesample contains at least 20 documents for each term.
(When working at Web scale, one might raise thefloor somewhat to perhaps 104.
)Sampling is obviously helpful at the top of thefrequency range, but not necessarily at the bottom(especially if frequencies fall below the floor).
Thequestion is: how about ?ordinary?
words?
To answerthis question, we randomly picked 15 pages froma Learners?
dictionary (Hornby, 1989), and selectedthe first entry on each page.
According to Google,there are 10 million pages/word (median value, ag-gregated over the 15 words), no where near the floor.Sampling can make it possible to work in mem-ory, avoiding disk.
At Web scale (D ?
10 billionpages), inverted indexes are large (1500 GBs/billionpages)3, probably too large for memory.
But a sam-ple is more manageable; the inverted index for a10?4 sample of the entire web could fit in memoryon a single PC (1.5 GB).2http://labs.google.com/sets produces fascinating sets, al-though we don?t know how it works.
Given the seeds, ?Amer-ica?
and ?China,?
http://labs.google.com/sets returns: ?Amer-ica, China, Japan, India, Italy, Spain, Brazil, Persia, Europe,Australia, France, Asia, Canada.
?3This estimate is extrapolated from Brin and Page (1998),who report an inverted index of 37.2 GBs for 24 million pages.708Table 1: The number of intermediate results after thefirst join can be reduced from 504,000 to 120,000,by starting with ?Schwarzenegger & Austria?
ratherthan the baseline (?Schwarzenegger & Terminator?
).The standard practice of starting with the two leastfrequent terms is a good rule of thumb, but one cando better, given (estimates of) joint frequencies.Query Hits (Google)Austria 88,200,000Governor 37,300,000Schwarzenegger 4,030,000Terminator 3,480,000Governor & Schwarzenegger 1,220,000Governor & Austria 708,000Schwarzenegger & Terminator 504,000Terminator & Austria 171,000Governor & Terminator 132,000Schwarzenegger & Austria 120,0001.1 An Application: The GovernatorGoogle returns the top k hits, plus an estimate ofhow many hits there are.
Table 1 shows the numberof hits for four words and their pair-wise combina-tions.
Accurate estimates of associations would haveapplications in Database query planning (Garcia-Molina et al, 2002).
Query optimizers construct aplan to minimize a cost function (e.g., intermediatewrites).
The optimizer could do better if it couldestimate a table like Table 1.
But efficiency is im-portant.
We certainly don?t want to spend more timeoptimizing the plan than executing it.Suppose the optimizer wanted to construct a planfor the query: ?Governor Schwarzenegger Termi-nator Austria.?
The standard solution starts withthe two least frequent terms: ?Schwarzenegger?
and?Terminator.?
That plan generates 504,000 interme-diate writes after the first join.
An improvementstarts with ?Schwarzenegger?
with ?Austria,?
reduc-ing the 504,000 down to 120,000.In addition to counting hits, Table 1 could alsohelp find the top k pages.
When joining the first pairof terms, we?d like to know how far down the rank-ing we should go.
Accurate estimates of associationswould help the optimizer make such decisions.It is desirable that estimates be consistent, as wellas accurate.
Google, for example, reports 6 millionhits for ?America, China, Britain,?
and 23 million for?America, China, Britain, Japan.?
Joint frequenciesdecrease monotonically: s ?
S =?
hits(s) ?
hits(S).f = a + cf = a + bD = a+b+c+dacy ~y~xxd yxb(a)x~xy ~ya bc d yxs s s ss sssn  = a + bsD = a +b + c +dn  = a + cs sss(b)Figure 1: (a): A contingency table for word x andword y.
Cell a is the number of documents that con-tain both x and y, b is the number that contain x butnot y, c is the number that contain y but not x, andd is the number that contain neither x nor y. Themargins, fx = a + b and fy = a + c are known asdocument frequencies in IR.
D is the total numberof documents in the collection.
(b): A sample con-tingency table, with ?s?
indicating the sample space.1.2 Sampling and EstimationTwo-way associations are often represented as two-way contingency tables (Figure 1(a)).
Our task is toconstruct a sample contingency table (Figure 1(b)),and estimate 1(a) from 1(b).
We will use a max-imum likelihood estimator (MLE) to find the mostlikely contingency table, given the sample and vari-ous other constraints.
We will propose a samplingprocedure that bridges two popular choices: (A)sampling over documents and (B) sampling overpostings.
The estimation task is straightforward andwell-understood for (A).
As we consider more flexi-ble sampling procedures such as (B), the estimationtask becomes more challenging.Flexible sampling procedures are desirable.
Manystudies focus on rare words (Dunning, 1993; Moore,2004); butterflies are more interesting than moths.The sampling rate can be adjusted on a word-by-word basis with (B), but not with (A).
The samplingrate determines the trade-off between computationalwork and statistical accuracy.We assume a standard inverted index.
For eachword x, there are a set of postings, X. X contains aset of document IDs, one for each document contain-ing x.
The size of postings, fx = |X|, correspondsto the margins of the contingency tables in Figure1(a), also known as document frequencies in IR.The postings lists are approximated by sketches,skX, first introduced by Broder (1997) for remov-ing duplicate web pages.
Assuming that documentIDs are random (e.g., achieved by a random permu-tation), we can compute skX, a random sample of709X, by simply selecting the first few elements of X.In Section 3, we will propose using sketchesto construct sample contingency tables.
With thisnovel construction, the contingency table (and sum-mary statistics based on the table) can be estimatedusing conventional statistical methods such as MLE.2 Broder?s Sketch AlgorithmOne could randomly sample two postings and inter-sect the samples to estimate associations.
The sketchtechnique introduced by Broder (1997) is a signifi-cant improvement, as demonstrated in Figure 2.Assume that each document in the corpus of sizeD is assigned a unique random ID between 1 and D.The postings for word x is a sorted list of fx doc IDs.The sketch, skX, is the first (smallest) sx doc IDs inX.
Broder used MINs(Z) to denote the s smallestelements in the set, Z .
Thus, skX = MINsx(X).Similarly, Y denotes the postings for word y, andskY denotes its sketch, MINsy(Y ).
Broder assumedsx = sy = s.Broder defined resemblance (R) and sample re-semblance (Rs) to be:R = aa + b + c , Rs =|MINs(skX ?
skY ) ?
skX ?
skY ||MINs(skX ?
skY )|.Broder (1997) proved that Rs is an unbiased esti-mator of R. One could use Rs to estimate a but hedidn?t do that, and it is not recommended.4Sketches were designed to improve the coverageof a, as illustrated by Monte Carlo simulation in Fig-ure 2.
The figure plots, E(asa), percentage of inter-sections, as a function of (postings) sampling rate,sf , where fx = fy = f , sx = sy = s. The solid lines(sketches), E (asa)?
sf , are above the dashed curve(random sampling), E (asa)= s2f2 .
The difference isparticularly important at low sampling rates.3 Generalizing Sketches: R?
TablesSketches were first proposed for estimating resem-blance (R).
This section generalizes the method toconstruct sample contingency tables, from which wecan estimate associations: R, LLR, cosine, etc.4There are at least three problems with estimating a fromRs.
First, the estimate is biased.
Secondly, this estimate usesjust s of the 2 ?
s samples; larger samples ?
smaller errors.Thirdly, we would rather not impose the restriction: sx = sy.0  0.2 0.4 0.6 0.8 100.51Sampling ratesPercentageof inersectionsRandom samplingSketchFigure 2: Sketches (solid curves) dominate randomsampling (dashed curve).
a=0.22, 0.38, 0.65, 0.80,0.85f , f=0.2D, D=105.
There is only one dashedcurve across all values of a.
There are different butindistinguishable solid curves depending on a.Recall that the doc IDs span the integers from 1to D with no gaps.
When we compare two sketches,skX and skY , we have effectively looked at Ds =min{skX(sx), skY(sy)} documents, where skX(j) isthe jth smallest element in skX.
The followingconstruction generates the sample contingency ta-ble, as, bs, cs, ds (as in Figure 1(b)).
The exampleshown in Figure 3 may help explain the procedure.Ds = min{skX(sx), skY(sy)}, as = |skX ?
skY |,nx = sx ?
|{j : skX(j) > Ds}|,ny = sy ?
|{j : skY(j) > Ds}|,bs = nx ?
as, cs = ny ?
as, ds = Ds ?
as ?
bs ?
cs.Given the sample contingency table, we are nowready to estimate the contingency table.
It is suffi-cient to estimate a, since the rest of the table can bedetermined from fx, fy and D. For practical appli-cations, we recommend the convenient closed-formapproximation (8) in Section 5.1.4 Margin-Free (MF) BaselineBefore considering the proposed MLE method, weintroduce a baseline estimator that will not work aswell because it does not take advantage of the mar-gins.
The baseline is the multivariate hypergeomet-ric model, usually simplified as a multinomial by as-suming ?sample-with-replacement.
?The sample expectations are (Siegrist, 1997),E(as) = DsD a, E(bs) =DsD b,E(cs) = DsD c, E(ds) =DsD d. (1)710Y:  2   4   5   8   15    19   21     24   27   28   31fX:  3   4   7   9   10   15   18      19   24   25   28= 11 = 5 = 18f a Dx y = 11 s= 5= 7= 7sy= 7sxb c= 5= 2as s s= 3n nx yds = 8(a)9     10     11    12    13    14    15   161      2      3      4      5      6      7      817   18     19    20    .
.
.
.
.
.
D = 36(b)Figure 3: (a): The two sketches, skX and skY(larger shaded box), are used to construct a sam-ple contingency table: as, bs, cs, ds.
skX consistsof the first sx = 7 doc IDs in X, the postings forword x.
Similarly, skY consists of the first sy = 7doc IDs in Y , the postings for word y.
There are 11doc IDs in both X and Y , and a = 5 doc IDs inthe intersection: {4, 15, 19, 24, 28}.
(a) shows thatDs = min(18, 21) = 18.
Doc IDs 19 and 21 areexcluded because we cannot determine if they are inthe intersection or not, without looking outside thebox.
As it turns out, 19 is in the intersection and21 is not.
(b) enumerates the Ds = 18 documents,showing which documents contain x (small circles)and which contain y (small squares).
Both proce-dures, (a) and (b), produce the same sample contin-gency table: as = 2, bs = 5, cs = 3 and ds = 8.The margin-free estimator and its variance area?MF =DDsas, Var(a?MF ) =DDs11a + 1D?aD ?DsD ?
1 .
(2)For the multinomial simplification, we havea?MF,r = DDsas, Var(a?MF,r) = DDs11a + 1D?a.
(3)where ?r?
indicates ?sample-with-replacement.
?The term D?DsD?1 ?
D?DsD is often called the?finite-sample correction factor?
(Siegrist, 1997).5 The Proposed MLE MethodThe task is to estimate the contingency table fromthe samples, the margins and D. We would like touse a maximum likelihood estimator for the mostprobable a, which maximizes the (full) likelihood(probability mass function, PMF) P (as, bs, cs, ds; a).Unfortunately, we do not know the exact expres-sion for P (as, bs, cs, ds; a), but we do know the con-ditional probability P (as, bs, cs, ds|Ds; a).
Since thedoc IDs are uniformly random, sampling the firstDs contiguous documents is statistically equivalentto randomly sampling Ds documents from the cor-pus.
Based on this key observation and Figure 3,conditional on Ds, P (as, bs, cs, ds|Ds; a) is the PMFof a two-way sample contingency table.We factor the full likelihood into:P (as, bs, cs, ds; a) = P (as, bs, cs, ds|Ds; a)?
P (Ds; a).P (Ds; a) is difficult.
However, since we do not ex-pect a strong dependency of Ds on a, we maxi-mize the partial likelihood instead, and assume thatis good enough.
An example of partial likelihood isthe Cox proportional hazards model in survival anal-ysis (Venables and Ripley, 2002, Section 13.3) .Our partial likelihood isP (as, bs, cs, ds|Ds; a) =` aas?`fx?abs?`fy?acs?`D?fx?fy+ads?`DDs??as?1Yi=0(a?
i) ?bs?1Yi=0(fx ?
a?
i) ?cs?1Yi=0(fy ?
a?
i)?ds?1Yi=0(D ?
fx ?
fy + a?
i), (4)where(nm)= n!m!(n?m)!
.
???
is ?proportional to.
?We now derive an MLE for (4), a result that wasnot previously known, to the best of our knowledge.Let a?MLE maximizes logP (as, bs, cs, ds|Ds; a):as?1Xi=0log(a?
i) +bs?1Xi=0log (fx ?
a?
i)+cs?1Xi=0log (fy ?
a?
i) +ds?1Xi=0log (D ?
fx ?
fy + a?
i) ,whose first derivative, ?
logP (as,bs,cs,ds|Ds;a)?a , isas?1Xi=01a?
i ?bs?1Xi=01fx ?
a?
i?cs?1Xi=01fy ?
a?
i+ds?1Xi=01D ?
fx ?
fy + a?
i.
(5)Since the second derivative, ?2 logP (as,bs,cs,ds|Ds;a)?a2 ,is negative, the log likelihood function is concave,hence has a unique maximum.
One could numeri-cally solve (5) for ?
logP (as,bs,cs,ds|Ds;a)?a = 0.
How-ever, we derive the exact solution using the follow-ing updating formula from (4):711P (as, bs, cs, ds|Ds; a) = P (as, bs, cs, ds|Ds; a?
1)?fx ?
a + 1?
bsfx ?
a + 1fy ?
a + 1?
csfy ?
a + 1D ?
fx ?
fy + aD ?
fx ?
fy + a?
dsaa?
as= P (as, bs, cs, ds|Ds; a?
1)?
g(a).
(6)Since our MLE is unique, it suffices to find a fromg(a) = 1, which is a cubic function in a.5.1 A Convenient Practical ApproximationRather than solving the cubic equation for the ex-act MLE, the following approximation may be moreconvenient.
Assume we sample nx = as + bs fromX and obtain as co-occurrences without knowledgeof the samples from Y .
Further assuming ?sample-with-replacement,?
as is then binomially distributed,as ?
Binom(nx, afx ).
Similarly, assume as ?Binom(ny, afy ).
Under these assumptions, the PMFof as is a product of two binomial PMFs:fxnx!?
afx?as ?fx ?
afx?bsfyny!?
afy?as ?fy ?
afy?cs?
a2as (fx ?
a)bs (fy ?
a)cs .
(7)Setting the first derivative of the logarithm of (7) tobe zero, we obtain 2asa ?
bsfx?a ?csfy?a = 0, which isquadratic in a and has a solution:a?MLE,a = fx (2as + cs) + fy (2as + bs)2 (2as + bs + cs)?q(fx (2as + cs)?
fy (2as + bs))2 + 4fxfybscs2 (2as + bs + cs).
(8)Section 6 shows that a?MLE,a is very close to a?MLE .5.2 Theoretical Evaluation: Bias and VarianceHow good are the estimates?
A popular metricis mean square error (MSE): MSE(a?)
= E (a??
a)2 =Var (a?)
+Bias2 (a?).
If a?
is unbiased, MSE(a?)
=Var (a?)
=SE2 (a?
), where SE is the standard error.
Here all ex-pectations are conditional on Ds.Large sample theory (Lehmann and Casella,1998, Chapter 6) says that, under ?sample-with-replacement,?
a?MLE is asymptotically unbiased andconverges to Normal with mean a and variance 1I(a) ,where I(a), the Fisher Information, isI(a) = ?E?
?2?a2 logP (as, bs, cs, ds|Ds; a, r)?.
(9)Under ?sample-with-replacement,?
we haveP (as, bs, cs, ds|Ds; a, r) ??
aD?as?
?fx ?
aD?bs?
?fy ?
aD?cs?
?D ?
fx ?
fy + aD?ds, (10)Therefore, the Fisher Information, I(a), isE(as)a2 +E(bs)(fx ?
a)2+ E(cs)(fy ?
a)2+ E(ds)(D ?
fx ?
fy + a)2.
(11)We plug (1) from the margin-free model into (11)as an approximation, to obtainVar (a?MLE) ?DDs ?
11a + 1fx?a +1fy?a +1D?fx?fy+a, (12)which is 1I(a) multiplied byD?DsD , the ?finite-sample correction factor,?
to consider ?sample-without-replacement.
?We can see that Var (a?MLE) is less thanVar (a?MF ) in (2).
In addition, a?MLE is asymptoti-cally unbiased while a?MF is no longer unbiased un-der margin constraints.
Therefore, we expect a?MLEhas smaller MSE than a?MF .
In other words, the pro-posed MLE method is more accurate than the MFbaseline, in terms of variance, bias and mean squareerror.
If we know the margins, we ought to use them.5.3 Unconditional Bias and Variancea?MLE is also unconditionally unbiased:E (a?MLE ?
a) = E (E (a?MLE ?
a|Ds)) ?
E(0) = 0.
(13)The unconditional variance is useful because oftenwe would like to estimate the errors before knowingDs (e.g., for choosing sample sizes).To compute the unconditional variance of a?MLE ,we should replace DDs with E(DDs)in (12).
Weresort to an approximation for E?DDs?.
Note thatskX(sx) is the order statistics of a discrete randomvariable (Siegrist, 1997) with expectationE`skX(sx)?= sx(D + 1)fx + 1?
sxfxD.
(14)By Jensen?s inequality, we know thatE?DsD??
minE`skX(sx)?D ,E`skY(sy)?D!= min?sxfx, syfy?(15)E?
DDs??
1E`DsD?
?
max?fxsx, fysy?.
(16)712Table 2: Gold standard joint frequencies, a. Docu-ment frequencies are shown in parentheses.
Thesewords are frequent, suitable for evaluating our algo-rithms at very low sampling rates.THIS HAVE HELP PROGRAMTHIS (27633) ?
13517 7221 3682HAVE (17396) 13517 ?
5781 3029HELP (10791) 7221 5781 ?
1949PROGRAM (5327) 3682 3029 1949 ?Replacing the inequalities with equalities underes-timates the variance, but only slightly.5.4 SmoothingAlthough not a major emphasis here, our evalua-tions will show that a?MLE+S , a smoothed versionof the proposed MLE method, is effective, espe-cially at low sampling rates.
a?MLE+S uses ?add-one?
smoothing.
Given that such a simple methodis as effective as it is, it would be worth consideringmore sophisticated methods such as Good-Turing.5.5 How Many Samples Are Sufficient?The answer depends on the trade-off between com-putation and estimation errors.
One simple rule isto sample ?2%.?
(12) implies that the standard er-ror is proportional topD/Ds ?
1.
Figure 4(a) plotspD/Ds ?
1 as a function of sampling rate, Ds/D, in-dicating a ?elbow?
about 2%.
However, 2% is toolarge for high frequency words.A more reasonable metric is the ?coefficient ofvariation,?
cv = SE(a?
)a .
At Web scale (10 billionpages), we expect that a very small sampling ratesuch as 10?4 or 10?5 will suffice to achieve a rea-sonable cv (e.g., 0.5).
See Figure 4(b).6 EvaluationTwo sets of experiments were run on a collection ofD = 216 web pages, provided by MSN.
The first ex-periment considered 4 English words shown in Ta-ble 2, and the second experiment considers 968 En-glish words with mean df = 2135 and median df =1135.
They form 468,028 word pairs, with mean co-occurrences = 188 and median = 74.6.1 Small Dataset Monte Carlo ExperimentFigure 5 evaluates the various estimate methods byMSE over a wide range of sampling rates.
Doc IDs0   0.02 0.05 0.1 0.150102030Samplig ratesRelativeSE(a)105 106 107 108 109 101010?510?310?1100DSamplingratesfx= 0.0001?Dfx=0.01?D0.001fy = 0.1?
fxa = 0.05 ?
fy(b)Figure 4: How large should the sampling rate be?
(a): We can sample up to the ?elbow point?
(2%),but after that there are diminishing returns.
(b): Ananalysis based on cv = SEa = 0.5 suggests that we canget away with much lower sampling rates.
The threecurves plot the critical value for the sampling rate,DsD , as a function of corpus size, D. At Web scale,D ?
1010, sampling rates above 10?3 to 10?5 sat-isfy cv < 0.5, at least for these settings of fx, fyand a.
The settings were chosen to simulate ?ordi-nary?
words.
The three curves correspond to threechoices of fx: D/100, D/1000, and D/10, 000.fy = fx/10, a = fy/20.
SE is based on (12).were randomly permuted 105 times.
For each per-mutation we constructed sketches from the invertedindex at a series of sampling rates.
The figure showsthat the proposed method, a?MLE , is considerablybetter (by 20% ?
40%) than the margin-free base-line, a?MF .
Smoothing is effective at low samplingrates.
The recommended approximation, a?MLE,a, isremarkably close to the exact solution.Figure 6 shows agreement between the theoreti-cal and empirical unconditional variances.
Smooth-ing reduces variances, at low sampling rates.
Weused the empirical E?DDS?to compute the theoreti-cal variances.
The approximation, max(fxsx ,fysy), is> 0.95E?DDS?at sampling rates > 0.01.Figure 7 verifies that the proposed MLE is unbi-ased, unlike the margin-free baselines.6.2 Large Dataset ExperimentThe large experiment considers 968 English words(468,028 pairs) over a range of sampling rates.
Afloor of 20 was imposed on sample sizes.As reported in Figure 8, the large experiment con-firms once again that proposed method, a?MLE , isconsiderably better than the margin-free baseline (by7130.001 0.01 0.1 100.20.4NormalizedMSE0.5MFMLE,aMLEMLE+SINDTHIS ?
HAVE0.001 0.01 0.1 100.20.4THIS ?
HELP0.001 0.01 0.1 100.20.4NormalizedMSE0.5THIS ?
PROGRAM0.001 0.01 0.1 100.20.4 HAVE ?
HELP0.001 0.01 0.1 100.20.40.5NormalizedMSE0.5Sampling ratesHAVE ?
PROGRAMINDMFMLE+SMLE,aMLE0.001 0.01 0.1 100.20.40.6Sampling ratesHELP ?
PROGRAMFigure 5: The proposed method, a?MLE outperformsthe margin-free baseline, a?MF , in terms of MSE0.5a .The recommended approximation, a?MLE,a, is closeto a?MLE .
Smoothing, a?MLE+S , is effective at lowsampling rates.
All methods are better than assum-ing independence (IND).15% ?
30%).
The recommended approximation,a?MLE,a, is close to a?MLE .
Smoothing, a?MLE+Shelps at low sampling rates.6.3 Rank Retrieval: Top k Associated PairsWe computed a gold standard similarity cosine rank-ing of the 468,028 pairs using a 100% sample: cos =a?fxfy.
We then compared the gold standard to rank-ings based on smaller samples.
Figure 9(a) com-pares the two lists in terms of agreement in the top k.For 3 ?
k ?
200, with a sampling rate of 0.005, theagreement is consistently 70% or higher.
Increasingsampling rate, increases agreement.The same comparisons are evaluated in terms ofprecision and recall in Figure 9(b), by fixing the top1% of the gold standard list but varying the top per-centages of the sample list.
Again, increasing sam-pling rate, increases agreement.0.001 0.01 0.1 100.10.2Sampling ratesNormalizedstandarderrorMLEMLE+STheore.HAVE ?
PROGRAM0.001 0.01 0.1 100.10.20.30.4Sampling ratesMLEMLE+STheore.HELP ?
PROGRAMFigure 6: The theoretical and empirical variancesshow remarkable agreement, in terms of SE(a?
)a .Smoothing reduces variances at low sampling rates.0.001 0.01 0.1 100.020.05Sampling ratesNormalizedabsolutebiasHAVE ?
PROGRAMMFMLEMLE+S0.001 0.01 0.1 100.20.040.06Sampling ratesHELP ?
PROGRAMMLE+SMFMLEFigure 7: Biases in terms of |E(a?
)?a|a .
a?MLE is prac-tically unbiased, unlike a?MF .
Smoothing increasesbias slightly.7 ConclusionWe proposed a novel sketch-based procedure forconstructing sample contingency tables.
Themethod bridges two popular choices: (A) sam-pling over documents and (B) sampling over post-ings.
Well-understood maximum likelihood estima-tion (MLE) techniques can be applied to sketches(or to traditional samples) to estimate word associa-tions.
We derived an exact cubic solution, a?MLE , aswell as a quadratic approximation, a?MLE,a.
The ap-proximation is recommended because it is close tothe exact solution, and easy to compute.The proposed MLE methods were compared em-pirically and theoretically to a margin-free (MF)baseline, finding large improvements.
When weknow the margins, we ought to use them.Sample-based methods (MLE & MF) are oftenbetter than sample-free methods.
Associations areoften estimated without samples.
It is popular toassume independence: (Garcia-Molina et al, 2002,Chapter 16.4), i.e., a?
?
fxfyD .
Independence led tolarge errors in our experiments.Not unsurprisingly, there is a trade-off betweencomputational work (space and time) and statistical7140.001 0.01 0.1 100.20.40.6Sampling ratesRelativeavg.abs.
error INDMLE+SMLEMFMLE,aFigure 8: We report the (normalized) mean absoluteerrors (divided by the mean co-occurrences, 188).All curves are averaged over three permutations.The proposed MLE and the recommended approxi-mation are very close and both are significantly bet-ter than the margin-free (MF) baseline.
Smoothing,a?MLE+S , helps at low sampling rates.
All estima-tors do better than assuming independence.accuracy (variance or errors); reducing the samplingrate saves work, but costs accuracy.
We derivedformulas for variance, showing precisely how accu-racy depends on sampling rate.
Sampling methodsbecome more and more important with larger andlarger collections.
At Web scale, sampling rates aslow as 10?4 may suffice for ?ordinary?
words.We have recently generalized the sampling algo-rithm and estimation method to multi-way associa-tions; see (Li and Church, 2005).ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scale hypertextual web search engine.
In Proceedingsof the Seventh International World Wide Web Confer-ence, pages 107?117, Brisbane, Australia.A.
Broder.
1997.
On the resemblance and containmentof documents.
In Proceedings of the Compression andComplexity of Sequences, pages 21?29, Positano, Italy.M.
S. Charikar.
2002.
Similarity estimation techniquesfrom rounding algorithms.
In Proceedings of the thiry-fourth annual ACM symposium on Theory of comput-ing, pages 380?388, Montreal, Quebec, Canada.K.
Church and P. Hanks.
1991.
Word association norms,mutual information and lexicography.
ComputationalLinguistics, 16(1):22?29.S.
Deerwester, S. T. Dumais, G. W. Furnas, and T. K.Landauer.
1999.
Indexing by latent semantic analy-3 10 100 200020406080100TopPercentageof agreement (%)0.50.005(a)0 0.2 0.4 0.6 0.8 100.20.40.60.81RecallPrecisionTop 1 %0.005 0.010.030.020.5(b)Figure 9: (a): Percentage of agreements in the goldstandard and reconstructed (from samples) top 3 to200 list.
(b):Precision-recall curves in retrieving thetop 1% gold standard pairs, at different samplingrates.
For example, 60% recall and 70% precisionis achieved at sampling rate = 0.02.sis.
Journal of the American Society for InformationScience, 41(6):391?407.T.
Dunning.
1993.
Accurate methods for the statistics ofsurprise and coincidence.
Computational Linguistics,19(1):61?74.H.
Garcia-Molina, J. D. Ullman, and J. D. Widom.
2002.Database Systems: the Complete Book.
Prentice Hall,New York, NY.A.
S. Hornby, editor.
1989.
Oxford Advanced Learner?sDictionary.
Oxford University Press, Oxford, UK.E.
L. Lehmann and G. Casella.
1998.
Theory of PointEstimation.
Springer, New York, NY, second edition.P.
Li and K. W. Church.
2005.
Using sketches to esti-mate two-way and multi-way associations.
Technicalreport, Microsoft Research, Redmond, WA.C.
D. Manning and H. Schutze.
1999.
Foundations ofStatistical Natural Language Processing.
The MITPress, Cambridge, MA.R.
C. Moore.
2004.
On log-likelihood-ratios and thesignificance of rare events.
In Proceedings of EMNLP2004, pages 333?340, Barcelona, Spain.D.
Ravichandran, P. Pantel, and E. Hovy.
2005.
Ran-domized algorithms and NLP: Using locality sensitivehash function for high speed noun clustering.
In Pro-ceedings of ACL, pages 622?629, Ann Arbor.K.
Siegrist.
1997.
Finite Sampling Models,http://www.ds.unifi.it/VL/VL EN/urn/index.html.
Vir-tual Laboratories in Probability and Statistics.W.
N. Venables and B. D. Ripley.
2002.
Modern Ap-plied Statistics with S. Springer-Verlag, New York,NY, fourth edition.715
