Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 125?133,Beijing, August 2010A Utility-Driven Approach to Question Ranking in Social QARazvan BunescuSchool of EECSOhio Universitybunescu@ohio.eduYunfeng HuangSchool of EECSOhio Universityyh324906@ohio.eduAbstractWe generalize the task of finding questionparaphrases in a question repository to anovel formulation in which known ques-tions are ranked based on their utility toa new, reference question.
We manuallyannotate a dataset of 60 groups of ques-tions with a partial order relation reflect-ing the relative utility of questions insideeach group, and use it to evaluate mean-ing and structure aware utility functions.Experimental evaluation demonstrates theimportance of using structural informa-tion in estimating the relative usefulnessof questions, holding the promise of in-creased usability for social QA sites.1 IntroductionOpen domain Question Answering (QA) is oneof the most complex and challenging tasks innatural language processing.
While building onideas from Information Retrieval (IR), questionanswering is generally seen as a more difficulttask due to constraints on both the input represen-tation (natural language questions vs. keyword-based queries) and the form of the output (fo-cused answers vs. entire documents).
Recently,community-driven QA sites such as Yahoo!
An-swers and WikiAnswers have established a newapproach to question answering in which the bur-den of dealing with the inherent complexity ofopen domain QA is shifted from the computersystem to volunteer contributors.
The computeris no longer required to perform a deep linguis-tic analysis of questions and generate correspond-ing answers, and instead acts as a mediator be-tween users submitting questions and volunteersproviding the answers.
In most implementationsof community-driven QA, the mediator systemhas a well defined strategy for enticing volun-teers to post high quality answers on the website.In general, the overall objective is to minimizethe response time and maximize the accuracy ofthe answers, measures that are highly correlatedwith user satisfaction.
For any submitted ques-tion, one useful strategy is to search the QA repos-itory for similar questions that have already beenanswered, and provide the corresponding rankedlist of answers, if such a question is found.
Thesuccess of this approach depends on the definitionand implementation of the question-to-questionsimilarity function.
In the simplest solution, thesystem searches for previously answered ques-tions based on exact string matching with thereference question.
Alternatively, sites such asWikiAnswers allow the users to mark questionsthey think are rephrasings (?alternate wordings?,or paraphrases) of existing questions.
These ques-tion clusters are then taken into account when per-forming exact string matching, therefore increas-ing the likelihood of finding previously answeredquestions that are semantically equivalent to thereference question.
Like the original question an-swering task, the solution to question rephrasing isalso based on volunteer contributions.
In order tolessen the amount of work required from the con-tributors, an alternative solution is to build a sys-tem that automatically finds rephrasings of ques-tions, especially since question rephrasing seemsto be computationally less demanding than ques-tion answering.
The question rephrasing subtaskhas spawned a diverse set of approaches.
(Herm-125jakob et al, 2002) derive a set of phrasal patternsfor question reformulation by generalizing surfacepatterns acquired automatically from a large cor-pus of web documents.
The focus of the work in(Tomuro, 2003) is on deriving reformulation pat-terns for the interrogative part of a question.
In(Jeon et al, 2005), word translation probabilitiesare trained on pairs of semantically similar ques-tions that are automatically extracted from an FAQarchive, and then used in a language model thatretrieves question reformulations.
(Jijkoun and deRijke, 2005) describe an FAQ question retrievalsystem in which weighted combinations of simi-larity functions corresponding to questions, exist-ing answers, FAQ titles and pages are computedusing a vector space model.
(Zhao et al, 2007)exploit the Encarta logs to automatically extractclusters containing question paraphrases and fur-ther train a perceptron to recognize question para-phrases inside each cluster based on a combina-tion of lexical, syntactic and semantic similarityfeatures.
More recently, (Bernhard and Gurevych,2008) evaluated various string similarity measuresand vector space based similarity measures on thetask of retrieving question paraphrases from theWikiAnswers repository.According to previous work in this domain, aquestion is considered a rephrasing of a referencequestion Q0 if it uses an alternate wording to ex-press an identical information need.
For example,Q0 and Q1 below may be considered rephrasingsof each other, and consequently they are expectedto have the same answer.Q0 What should I feed my turtle?Q1 What do I feed my pet turtle?Community-driven QA sites are bound to face sit-uations in which paraphrasings of a new ques-tion cannot be found in the QA repository.
Webelieve that computing a ranked list of existingquestions that partially address the original infor-mation need could be useful to the user, at leastuntil other users volunteer to give an exact an-swer to the original, unanswered reference ques-tion.
For example, in the absence of any additionalinformation about the reference question Q0, theexpected answers to questions Q2 and Q3 abovemay be seen as partially overlapping in informa-tion content with the expected answer for the ref-erence question.
An answer to questionQ4, on theother hand, is less likely to benefit the user, eventhough it has a significant lexical overlap with thereference question.Q2 What kind of fish should I feed my turtle?Q3 What do you feed a turtle that is the size of aquarter?Q4 What kind of food should I feed a turtle dove?In this paper, we propose a generalization ofthe question paraphrasing problem to a questionranking problem, in which questions are rankedin a partial order based on the relative informationoverlap between their expected answers and theexpected answer of the reference question.
Theexpectation in this approach is that the user whosubmits a reference question will find the answersof the highly ranked question to be more usefulthan the answers associated with the lower rankedquestions.
For the reference question Q0 above,the system is expected to produce a partial orderin whichQ1 is ranked higher thanQ2, Q3 andQ4,whereas Q2 and Q3 are ranked higher than Q4.
InSection 2 we give further details on the questionranking task and describe a dataset of questionsthat have been manually annotated with partial or-der information.
Section 3 presents a set of initialapproaches to question ranking, followed by theirexperimental evaluation in Section 4.
The paperends with a discussion of future work, and con-clusion.2 A Partially Ordered Dataset forQuestion RankingIn order to enable the evaluation of question rank-ing approaches, we created a dataset of 60 groupsof questions.
Each group consists of a referencequestion (e.g.
Q0 above) that is associated witha partially ordered set of questions (e.g.
Q1 toQ4 above).
The 60 reference questions have beenselected to represent a diverse set of question cat-egories from Yahoo!
Answers.
For each refer-ence questions, its corresponding partially orderedset is created from questions in Yahoo!
Answers126REFERENCE QUESTION (Qr)Q5 What?s a good summer camp to go to in FL?PARAPHRASING QUESTIONS (P )Q6 What camps are good for a vacation during the summer in FL?Q7 What summer camps in FL do you recommend?USEFUL QUESTIONS (U )Q8 Does anyone know a good art summer camp to go to in FL?Q9 Are there any good artsy camps for girls in FL?Q10 What are some summer camps for like singing in Florida?Q11 What is a good cooking summer camp in FL?Q12 Do you know of any summer camps in Tampa, FL?Q13 What is a good summer camp in Sarasota FL for a 12 year old?Q14 Can you please help me find a surfing summer camp for beginners in Treasure Coast, FL?Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?Q16 Does anyone know any volleyball camps in Miramar, FL?Q17 Does anyone know about any cool science camps in Miami?Q18 What?s a good summer camp you?ve ever been to?NEUTRAL QUESTIONS (N )Q19 What?s a good summer camp in Canada?Q20 What?s the summer like in Florida?Table 1: A question group.and other online repositories that have a high co-sine similarity with the reference question.
Due tothe significant lexical overlap between the ques-tions, this is a rather difficult dataset, especiallyfor ranking methods that rely exclusively on bag-of-words measures.
Inside each group, the ques-tions are manually annotated with a partial orderrelation, according to their utility with respect tothe reference question.
We shall use the notation?Qi ?
Qj |Qr?
to encode the fact that question Qiis more useful than question Qj with respect tothe reference question Qr.
Similarly, ?Qi = Qj?will be used to express the fact that questions QiandQj are reformulations of each other (the refor-mulation relation is independent of the referencequestion).
The partial ordering among the ques-tions Q0 to Q4 above can therefore be expressedconcisely as follows: ?Q0 = Q1?, ?Q1 ?
Q2|Q0?,?Q1 ?
Q3|Q0?, ?Q2 ?
Q4|Q0?, ?Q3 ?
Q4|Q0?.Note that we do not explicitly annotate the rela-tion ?Q1 ?
Q4|Q0?, since it can be inferred basedon the transitivity of the more useful than relation:?Q1 ?
Q2|Q0?
?
?Q2 ?
Q4|Q0?
?
?Q1 ?Q4|Q0?.
Also note that no relation is specifiedbetween Q2 and Q3, and similarly no relation canbe inferred between these two questions.
This re-flects our belief that, in the absence of any addi-tional information regarding the user or the ?tur-tle?
referenced in Q0, we cannot compare ques-tions Q2 and Q3 in terms of their usefulness withrespect to Q0.Table 1 shows another reference question Q5from our dataset, together with its annotated groupof questionsQ6 toQ20.
In order to make the anno-tation process easier and reproducible, we divideit into two levels of annotation.
During the firstannotation stage (L1), each question group is par-titioned manually into 3 subgroups of questions:?
P is the set of paraphrasing questions.?
U is the set of useful questions.?
N is the set of neutral questions.A question is deemed useful if its expected answermay overlap in information content with the ex-pected answer of the reference question.
The ex-pected answer of a neutral question, on the other127hand, should be irrelevant with respect to the ref-erence question.
LetQr be the reference question,Qp ?
P a paraphrasing question, Qu ?
U a usefulquestion, and Qn ?
N a neutral question.
Thenthe following relations are assumed to hold amongthese questions:1.
?Qp ?
Qu|Qr?
: a paraphrasing question ismore useful than a useful question.2.
?Qu ?
Qn|Qr?
: a useful question is moreuseful than a neutral question.We also assume that, by transitivity, the followingternary relations also hold: ?Qp ?
Qn|Qr?, i.e.
aparaphrasing question is more useful than a neu-tral question.
Furthermore, if Qp1 , Qp2 ?
P aretwo paraphrasing questions, this implies ?Qp1 =Qp2 |Qr?.For the vast majority of questions, the firstannotation stage is straightforward and non-controversial.
In the second annotation stage (L2),we perform a finer annotation of relations betweenquestions in the middle group U .
Table 1 showstwo such relations (using indentation): ?Q8 ?Q9|Q5?
and ?Q8 ?
Q10|Q5?.
Question Q8 wouldhave been a rephrasing of the reference question,were it not for the noun ?art?
modifying the focusnoun phrase ?summer camp?.
Therefore, the in-formation content of the answer to Q8 is strictlysubsumed in the information content associatedwith the answer to Q5.
Similarly, in Q9 the fo-cus noun phrase is further specialized through theprepositional phrase ?for girls?.
Therefore, (ananswer to) Q9 is less useful to Q5 than (an an-swer to) Q8, i.e.
?Q8 ?
Q9|Q5?.
Furthermore,the focus ?art summer camp?
in Q8 conceptuallysubsumes the focus ?summer camps for singing?in Q10, therefore ?Q8 ?
Q10|Q5?.Table 2 below presents the following statisticson the annotated dataset: the number of referencequestions (Qr), the total number of paraphrasings(P), the total number of useful questions (U), thetotal number of neutral questions (N ), and the to-tal number of more useful than ordered pairs en-coded in the dataset, either explicitly or throughtransitivity, in the two annotation levels L1 andL2.Qr P U N L1 L260 177 847 427 7,378 7,639Table 2: Dataset statistics.3 Question Ranking MethodsAn ideal question ranking method would take anarbitrary triplet of questions Qr, Qi and Qj asinput, and output an ordering between Qi andQj with respect to the reference question Qr,i.e.
one of ?Qi ?
Qj |Qr?, ?Qi = Qj |Qr?, or?Qj ?
Qi|Qr?.
One approach is to design ausefulness function u(Qi, Qr) that measures howuseful question Qi is for the reference questionQr, and define the more useful than (?)
relationas follows:?Qi ?
Qj |Qr?
?
u(Qi, Qr) > u(Qj , Qr)If we define I(Q) to be the information need as-sociated with question Q, then u(Qi, Qr) couldbe defined as a measure of the relative overlap be-tween I(Qi) and I(Qr).
Unfortunately, the infor-mation need is a concept that, in general, is de-fined only intensionally and therefore it is diffi-cult to measure.
For lack of an operational def-inition of the information need, we will approxi-mate u(Qi, Qr) directly as a measure of the simi-larity between Qi and Qr.
The similarity betweentwo questions can be seen as a special case oftext-to-text similarity, consequently one possibil-ity is to use a general text-to-text similarity func-tion such as cosine similarity in the vector spacemodel (Baeza-Yates and Ribeiro-Neto, 1999):cos(Qi, Qr) =QTi Qr?Qi?
?Qr?Here, Qi and Qr denote the corresponding tf?idfvectors.
As a measure of question-to-questionsimilarity, cosine has two major drawbacks:1.
As an exclusively lexical measure, it is obliv-ious to the meanings of words in each ques-tion.2.
Questions are treated as bags-of-words,and thus important structural information ismissed.1283.1 Meaning Aware MeasuresThe three questions below illustrate the first prob-lem associated with cosine similarity.
Q22 andQ23 have the same cosine similarity with Q21,they are therefore indistinguishable in terms oftheir usefulness to the reference question Q21,even though we expectQ22 to be more useful thanQ23 (a place that sells hydrangea often sells othertypes of plants too, possibly including cacti).Q21 Where can I buy a hydrangea?Q22 Where can I buy a cactus?Q23 Where can I buy an iPad?To alleviate the lexical chasm, we can redefineu(Qi, Qr) to be the similarity measure proposedby (Mihalcea et al, 2006) as follows:mcs(Qi, Qr) =Xw?
{Qi}(maxSim(w,Qr) ?
idf(w))Xw?{Qi}idf(w)+Xw?
{Qr}(maxSim(w,Qi) ?
idf(w))Xw?
{Qr}idf(w)Since scaling factors are immaterial for ranking,we have ignored the normalization constant con-tained in the original measure.
For each wordw ?
Qi, maxSim(w,Qr) computes the maxi-mum semantic similarity betweenw and any wordwr ?
Qr.
The similarity scores are then weightedby the corresponding idf?s, and normalized.
Asimilar score is computed for each word w ?
Qr.The score computed by maxSim depends on theactual function used to compute the word-to-wordsemantic similarity.
In this paper, we evaluatedfour of the knowledge-based measures exploredin (Mihalcea et al, 2006): wup (Wu and Palmer,1994), res (Resnik, 1995), lin (Lin, 1998), andjcn (Jiang and Conrath, 1997).
Since all thesemeasures are defined on pairs of WordNet con-cepts, their analogues on word pairs (wi, wr) arecomputed by selecting pairs of WordNet synsets(ci, cr) such that wi belongs to concept ci, wr be-longs to concept cr, and (ci, cr) maximizes thesimilarity function.
The measure introduced in(Wu and Palmer, 1994) finds the least commonsubsumer (LCS) of the two input concepts in theWordNet hierarchy, and computes the ratio be-tween its depth and the sum of the depths of thetwo concepts:wup(ci, cr) =2 ?
depth(lcs(ci, cr))depth(ci) + depth(cr)Resnik?s measure is based on the InformationContent (IC) of a concept c defined as the negativelog probability ?
logP (c) of finding that conceptin a large corpus:res(ci, cr) = IC(lcs(ci, cr))Lin?s similarity measure can be seen as a normal-ized version of Resnik?s information content:lin(ci, cr) =2 ?
IC(lcs(ci, cr))IC(ci) + IC(cr)Jiang & Conrath?s measure is closely related tolin and is computed as follows:jcn(ci, cr) = [IC(ci) + IC(cr) ?
2 ?
IC(lcs(ci, cr))]?13.2 Structure Aware MeasuresCosine similarity, henceforth referred as cos,treats questions as bags-of-words.
The meta-measure proposed in (Mihalcea et al, 2006),henceforth called mcs, treats questions as bags-of-concepts.
Consequently, both cos and mcs maymiss important structural information.
If we con-sider the question Q24 below as reference, ques-tion Q26 will be deemed more useful than Q25when using cos or mcs because of the higher rel-ative lexical and conceptual overlap with Q24.However, this is contrary to the actual ordering?Q25 ?
Q26|Q24?, which reflects that fact thatQ25, which expects the same answer type as Q24,should be deemed more useful than Q26, whichhas a different answer type.Q24 What are some good thriller movies?Q25 What are some thriller movies with happyending?Q26 What are some good songs from a thrillermovie?129The analysis above shows the importance of us-ing the answer type when computing the simi-larity between two questions.
However, insteadof relying exclusively on a predefined hierarchyof answer types, we have decided to identify thequestion focus of a question, defined as the set ofmaximal noun phrases in the question that coreferwith the expected answer.
Focus nouns such asmovies and songs provide more discriminative in-formation than general answer types such as prod-ucts.
We use answer types only for questions suchas Q27 or Q28 below that lack an explicit questionfocus.
In such cases, an artificial question focusis created from the answer type (e.g.
location forQ27, or method for Q28) and added to the set ofquestion words.Q27 Where can I buy a good coffee maker?Q28 How do I make a pizza?Let qsim be a general bag-of-words question sim-ilarity measure (e.g.
cos or mcs).
Furthermore, letwsim by a generic word meaning similarity mea-sure (e.g.
wup, res, lin or jcn).
The equation be-low describes a modification of qsim that makes itaware of the questions focus:qsimf (Qi, Qr) = wsim(fi, fr) ?qsim(Qi?
{fi}, Qr?
{fr})Here, Qi and Qr refer both to the questions andtheir sets of words, while fi and fr stand for thecorresponding focus words.
We define qsim toreturn 1 if one of its arguments is an empty set,i.e.
qsim(?, ) = qsim( , ?)
= 1.
The newsimilarity measure qsimf multiplies the seman-tic similarity between the two focus words withthe bag-of-words similarity between the remain-ing words in the two questions.
Consequently, theword ?movie?
in Q26 will not be compared withthe word ?movies?
in Q24, and therefore Q26 willreceive a lower utility score than Q25.In addition to the question focus, the main verbof a question can also provide key informationin estimating question-to-question similarity.
Wedefine the main verb to be the content verb thatis highest in the dependency tree of the question,e.g.
buy for Q27, or make for Q28.
If the questiondoes not contain a content verb, the main verb isdefined to be the highest verb in the dependencytree, as for example are in Q24 to Q26.
The utilityof a question?s main verb in judging its similarityto other questions can be seen more clearly in thequestions below, where Q29 is the reference:Q29 How can I transfer music from iTunes to myiPod?Q30 How can I upload music to my iPod?Q31 How can I play music in iTunes?The fact that upload, as the main verb of Q30, ismore semantically related to transfer (upload is ahyponym of transfer in WordNet) is essential indeciding that ?Q30 ?
Q31|Q29?, i.e.
Q30 is moreuseful than Q31 to Q29.Like the focus word, the main verb can be in-corporated in the question similarity function asfollows:qsimfv(Qi, Qr) = wsim(fi, fr) ?
wsim(vi, vr) ?qsim(Qi?
{fi, vi}, Qr?
{fr, vr})The new measure qsimfv takes into accountboth the focus words and the main verbs whenestimating the semantic similarity between ques-tions.
When decomposing the questions into focuswords, main verbs and the remaining words, wehave chosen to multiply the corresponding sim-ilarities instead of, for example, summing them.Consequently, a close to zero score in each ofthem would drive the entire similarity to zero.This reflects the belief that question similarity issensitive to each component of a question.4 Experimental EvaluationWe use the question ranking dataset described inSection 2 to evaluate the two similarity measurescos and mcs, as well as their structured versionscosf , cosfv, mcsf , and mcsfv.
We report oneset of results for each of the four word similaritymeasures wup, res, lin or jcn.
Each question simi-larity measure is evaluated in terms of its accuracyon the set of ordered pairs for each of the two an-notation levels described in Section 2.
Thus, forthe first annotation level (L1) , we evaluate onlyover the set of relations defined across the three130Question Word similarity (wsim)similarity wup res lin jcn(qsim) L1 L2 L1 L2 L1 L2 L1 L2cos 69.1 69.3 69.1 69.3 69.1 69.3 69.1 69.3cosf 69.9 70.1 72.5 72.7 71.0 71.2 69.6 69.8cosfv 69.9 70.1 72.5 72.6 71.0 71.2 69.6 69.8mcs 62.6 62.5 65.0 65.0 65.6 65.7 66.8 66.9mcsf 64.2 64.4 68.5 68.5 68.8 68.9 67.2 67.4mcsfv 65.8 66.0 68.8 68.8 69.7 69.8 67.7 67.8Table 3: Accuracy results, with and without meaning and structure information.sets R, U , and N .
If ?Qi ?
Qj |Qr?
is a rela-tion specified in the annotation, we consider thetuple ?Qi, Qj , Qr?
correctly classified if and onlyif u(Qi, Qr) > u(Qj , Qr), where u is the ques-tion similarity measure (Section 3).
For the sec-ond annotation level (L2), we also consider the re-lations annotated between useful questions insidethe group U .We used the NLTK 1 implementation of the foursimilarity measures wup, res, lin or jcn.
The idfvalues for each word were computed from fre-quency counts over the entire Wikipedia.
For eachquestion, the focus is identified automatically byan SVM tagger trained on a separate corpus of2,000 questions manually annotated with focus in-formation.
The SVM tagger uses a combinationof lexico-syntactic features and a quadratic ker-nel to achieve a 93.5% accuracy in a 10-fold crossvalidation evaluation on the 2,000 questions.
Themain verb of a question is identified deterministi-cally using a breadth first traversal of the depen-dency tree.The overall accuracy results presented in Ta-ble 3 show that using the focus word improves theperformance across all 8 combinations of questionand word similarity measures.
For cosine simi-larity, the best performing system uses the focuswords and Resnik?s similarity function to obtain a3.4% increase in accuracy.
For the meaning awaresimilarity mcs, the best performing system usesthe focus words, the main verb and Lin?s wordsimilarity to achieve a 4.1% increase in accu-racy.
The improvement due to accounting for fo-cus words is consistent, whereas adding the main1http://www.nltk.orgverb seems to improve the performance only formcs, although not by a large margin.
The secondlevel of annotation brings 261 more relations inthe dataset, some of them more difficult to anno-tate when compared with the three groups in thefirst level.
Nevertheless, the performance eitherremains the same (somewhat expected due to therelatively small number of additional relations), oris marginally better.
The random baseline ?
as-signing a random similarity value to each pair ofquestions ?
results in 50% accuracy.
A somewhatunexpected result is that mcs does not performbetter than cos on this dataset.
After analysingthe result in more detail, we have noticed that mcsseems to be less resilient than cos to variations inthe length of the questions.
The Microsoft para-phrase corpus was specifically designed such that?the length of the shorter of the two sentences, inwords, is at least 66% that of the longer?
(Dolanand Brockett, 2005), whereas in our dataset thetwo questions in a pair can have significantly dif-ferent lengths 2.The questions in each of the 60 groups have ahigh degree of lexical overlap, making the datasetespecially difficult.
In this context, we believe theresults are encouraging.
We expect to obtain fur-ther improvements in accuracy by allowing rela-tions between all the words in a question to in-fluence the overall similarity measure.
For exam-ple, question Q19 has the same focus word as thereference question Q5 (repeated below), yet thedifference between the focus word prepositionalmodifiers makes it a neutral question.2Our implementation of mcs did performed better thancos on the Microsoft dataset.131Q5 What?s a good summer camp to go to in FL?Q19 What?s a good summer camp in Canada?Some of the questions in our dataset illustrate theneed to design a word similarity function specif-ically tailored to reflect how words change therelative usefulness of a question.
In the set ofquestions below, in deciding that Q33 and Q34are more useful than Q36 for the reference ques-tion Q32, an ideal question ranker needs to knowthat the ?Mayflower Hotel?
and the ?QueensboroBridge?
are in the proximity of ?Midtown Man-hattan?, and that proximity relations are relevantwhen asking for directions.
A coarse measureof proximity can be obtained for the pair (?Man-hattan?, ?Queensboro Bridge?)
by following themeronymy links connecting the two entities inWordNet.
However, a different strategy needs tobe devised for entities such as ?Mayflower Hotel?,?JFK?, or ?La Guardia?
which are not covered inWordNet.Q32 What is the best way to get to MidtownMan-hattan from JFK?Q33 What?s the best way from JFK to MayflowerHotel?Q34 What?s the best way from JFK to Queens-boro Bridge?Q35 How do I get from Manhattan to JFK airportby train?Q36 What is the best way to get to LaGuardiafrom JFK?Finally, to realize why question Q35 is useful oneneeds to know that, once directions on how to getby train from location X to location Y are known,then normally it suffices to reverse the list of stopsin order to obtain directions on how to get from Yback to X.5 Future WorkWe plan to integrate the entire dependency struc-ture of the question in the overall similarity mea-sure, possibly by defining kernels between ques-tions in a maximum margin model for ranking.We also plan to extend the word similarity func-tions to better reflect the types of relations thatare relevant when measuring question utility, suchas proximity relations between locations.
Further-more, we intend to take advantage of databases ofinterrogative paraphrases and paraphrase patternsthat were created in previous research on questionreformulation.6 ConclusionWe presented a novel question ranking task inwhich previously known questions are orderedbased on their relative utility with respect to a new,reference question.
We created a dataset of 60groups of questions 3 annotated with a partial or-der relation reflecting the relative utility of ques-tions inside each group, and used it to evaluatethe ranking performance of several meaning andstructure aware utility functions.
Experimental re-sults demonstrate the importance of using struc-tural information in judging the relative usefulnessof questions.
We believe that the new perspectiveon ranking questions has the potential to signifi-cantly improve the usability of social QA sites.AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful suggestions.ReferencesBaeza-Yates, Ricardo and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.
ACM Press,New York.Bernhard, Delphine and Iryna Gurevych.
2008.
An-swering learners?
questions by retrieving questionparaphrases from social Q&A sites.
In EANL ?08:Proceedings of the Third Workshop on InnovativeUse of NLP for Building Educational Applications,pages 44?52, Morristown, NJ, USA.
Association forComputational Linguistics.Dolan, William B. and Chris Brockett.
2005.
Auto-matically constructing a corpus of sentential para-phrases.
In Proceedings of the Third InternationalWorkshop on Paraphrasing (IWP2005), pages 9?16.3The dataset will be made publicly available.132Hermjakob, Ulf, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural language based reformula-tion resource and web exploitation for question an-swering.
In Proceedings of TREC-2002.Jeon, Jiwoon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM in-ternational conference on Information and knowl-edge management (CIKM?05), pages 84?90, NewYork, NY, USA.
ACM.Jiang, J.J. and D.W. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proceedings of the International Conference onResearch in Computational Linguistics, pages 19?33.Jijkoun, Valentin and Maarten de Rijke.
2005.
Re-trieving answers from frequently asked questionspages on the Web.
In Proceedings of the 14th ACMinternational conference on Information and knowl-edge management (CIKM?05), pages 76?83, NewYork, NY, USA.
ACM.Lin, Dekang.
1998.
An information-theoretic def-inition of similarity.
In Proceedings of the Fif-teenth International Conference on Machine Learn-ing (ICML ?98), pages 296?304, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Mihalcea, Rada, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Proceed-ings of the 21st national conference on Artificial in-telligence (AAAI?06), pages 775?780.
AAAI Press.Resnik, Philip.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In IJ-CAI?95: Proceedings of the 14th international jointconference on Artificial intelligence, pages 448?453, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.Tomuro, Noriko.
2003.
Interrogative reformulationpatterns and acquisition of question paraphrases.
InProceedings of the Second International Workshopon Paraphrasing, pages 33?40, Morristown, NJ,USA.
Association for Computational Linguistics.Wu, Zhibiao and Martha Palmer.
1994.
Verbs se-mantics and lexical selection.
In Proceedings of the32nd annual meeting on Association for Computa-tional Linguistics, pages 133?138, Morristown, NJ,USA.
Association for Computational Linguistics.Zhao, Shiqi, Ming Zhou, and Ting Liu.
2007.
Learn-ing question paraphrases for QA from Encarta logs.In Proceedings of the 20th international joint con-ference on Artifical intelligence (IJCAI?07), pages1795?1800, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.133
