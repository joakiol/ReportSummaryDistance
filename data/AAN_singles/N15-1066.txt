Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 641?650,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsDiamonds in the Rough: Event Extraction from Imperfect Microblog DataAnder Intxaurrondo?, Eneko Agirre?, Oier Lopez de Lacalle?, Mihai Surdeanu?
?IXA NLP Group, University of the Basque Country?University of Arizona{ander.intxaurrondo, e.agirre, oier.lopezdelacalle}@ehu.eusmsurdeanu@email.arizona.eduAbstractWe introduce a distantly supervised event ex-traction approach that extracts complex eventtemplates from microblogs.
We show that thisnear real-time data source is more challeng-ing than news because it contains informationthat is both approximate (e.g., with values thatare close but different from the gold truth) andambiguous (due to the brevity of the texts),impacting both the evaluation and extractionmethods.
For the former, we propose a novel,?soft?, F1 metric that incorporates similaritybetween extracted fillers and the gold truth,giving partial credit to different but similarvalues.
With respect to extraction method-ology, we propose two extensions to the dis-tant supervision paradigm: to address approx-imate information, we allow positive trainingexamples to be generated from informationthat is similar but not identical to gold values;to address ambiguity, we aggregate contextsacross tweets discussing the same event.
Weevaluate our contributions on the complex do-main of earthquakes, with events with up to20 arguments.
Our results indicate that, de-spite their simplicity, our contributions yielda statistically-significant improvement of 33%(relative) over a strong distantly-supervisedsystem.
The dataset containing the knowledgebase, relevant tweets and manual annotationsis publicly available.1 IntroductionTwitter is an excellent source of near real-time dataon recent events, motivating the need for informa-tion extraction (IE) systems that operate on tweetsrather than traditional news articles.
However, us-ing this data comes with its own challenges: tweetstend to use colloquial speech, noisy syntax and dis-course, and, more importantly, the information re-ported is often inaccurate (e.g., reporting a differ-ent but similar magnitude for an earthquake) andambiguous (e.g., reporting multiple potential earth-quake locations, with insufficient context to guesswhich is the correct one).1The top rows in Ta-ble 1 show examples of these problems for an ac-tual event in our dataset on earthquakes.
This comesin contrast with ?traditional?
IE work on newswiredocuments, where information is considerably moreaccurate than microblog material, and none of theabove observations hold (Grishman and Sundheim,1996; Doddington et al, 2004).As an example of the benefits of event extractionfrom a near real-time social-media resource, the lastrow in Table 1 lists a motivating example, where oursystem extracts the correct depth of an earthquakefrom the text tweeted by the U.S. Geological Sur-vey, which is novel information that is missing inour manually-curated knowledge base.In this work we take a classic event extraction(EE) task, where events are defined by templatescontaining a predefined set of arguments, and imple-ment it using data from Twitter.
We avoid the pro-hibitive cost of manual annotation through distantsupervision (DS): we automatically generate train-1We focus on microblogs here because they commonly con-tain inaccurate and/or ambiguous information.
However, we be-lieve that our contributions extend beyond microblogs becausethese innacuracies, especially inaccurate information, may ap-pear in news article as well.641Earthquake in Honduras.
So strong itApproximate strong it was felt in Guatemalainformation as well.
7.1 offshore atlantic.DTN Indonesia: Peru EarthquakeAmbiguous Destroys Homes, Injures 100...information 6.9 magnitude earthquake rocks Peru.U.S.G.S.
reports 6.9 Earthquake inPeru.
NO TSUNAMI threat to Hawaii.Information #Earthquake M 7.0 ?
Ryukyu Islands,not in the Japan T20:31:27 UTC , 25.95 128.40knowledge depth: 22 km <USGS URL>base Local tsunami alert issuedTable 1: Challenges and opportunities for event extrac-tion from Twitter.
The first row shows a tweet with ap-proximate information (in bold); the correct magnitude is7.3 (cf.
Table 2).
The second row shows a first tweet withambiguous information, which leads our baseline modelto extract the incorrect country (in bold; correct countryis Peru).
The following two tweets help disambiguate thecontext.
The last row shows a tweet containing informa-tion (in bold) that is missing in the knowledge base.ing data by aligning a knowledge base of knownevent instances with tweets (Mintz et al, 2009;Hoffmann et al, 2011), which is then used to train asupervised extraction model (sequence tagger in ourcase).
In seminal work on event extraction, (Bensonet al, 2011) applied DS to both detect tweets aboutlocal events and then extracted values about two ar-guments (artist and venue).
In our work, we work onautomatically selected tweets, and scale the task tocomplex events with a large number of arguments.We focus on the domain of earthquakes, where eachevent has up to 20 arguments.
Table 2 summarizesthis task.The contributions of this work are the following:1.
To our knowledge, this is one of the first worksthat analyzes the problem of distantly supervisedextraction of complex events with many argumentsfrom microblogs.2.
Our analysis shows (Section 3) that the biggestbarrier is that information on Twitter can be inaccu-rate (containing approximately correct event argu-ment values) and ambiguous (with insufficient con-text for accurate extraction).
The top two blocks inTable 1 show an example of each.
These challengesimpact both evaluation and system development.3.
The analysis also highlights the need to adaptevaluation metrics to approximately correct infor-mation, which may appear both in text and in theknowledge base itself.
For example, for a partic-ular earthquake, the USGS reports a depth of 22km., while NOAA reports 25 km2.
We propose anew evaluation metric that gives partial credit to ex-tracted argument values based on their similarity toexisting values in the knowledge base.4.
We introduce two simple strategies that addressthe above barriers for system development: approx-imate matching, which addresses inaccurate valuesby allowing the distant supervision process to mapvalues from the knowledge base to text even whenthey do not match exactly; and feature aggrega-tion, which responds to small, ambiguous contextsby aggregating information across multiple tweetsfor the same event.
For example, the first strategyconsiders the 7.1 magnitude in the first tweet in Ta-ble 1 as a training example because it is close to thevalue in the knowledge base (7.3).
The second strat-egy classifies all instances of Peru jointly using asingle set of features, extracted from all availabletweets for the corresponding earthquake.
For ex-ample, this feature set contains three values for thefeature previous-word (:, rocks, and in).
Eachapproach yields 19% relative improvement, 33% incombination.5.
We release a public dataset containing a knowl-edge base of earthquake instances and correspond-ing tweets for each earthquake3.2 Experimental frameworkIn this section we detail the creation of the knowl-edge base of earthquake events, the collection pro-cess for potentially-relevant tweets, and, lastly, ourdistant supervision framework, which serves as aplatform for our contributions (Sections 5 and 6).2.1 Knowledge base and tweet dataset creationThe knowledge base (KB) was created from thelist of globally significant earthquakes during the21st century, as reported by Wikipedia.4We se-2http://bit.ly/aq9Vxa and http://1.usa.gov/1p1gELB3http://ixa.eus/Ixa/Argitalpenak/Artikuluak/1425465524/publikoak/earthquake-kb-dataset.zip4https://en.wikipedia.org/wiki/List_of_21st-century_earthquakes.
Accessed on July 9th,642Argument Arg.
# KB Example # DS # MAName Type Values Values Values ValuesDate D 108 2009-5-28 291 706Time T 108 T08:24:00 378 589Country L 108 Honduras 6294 6327Region L 77 2598 2663City L 77 1426 1723Latitude N 108 16.733 2 28Longitude N 108 -86.22 4 28Dead N 71 7 143 984Injured N 39 22 192Missing N 8 - 18Magnitude N 108 7.3 933 3403Depth (km) N 99 10 27 313Countries Guatemala,affected(*) L 37 Belize 436 357Regionsaffected(*) L 4 - 36Landslides B 8 7 9Tsunami B 10 408 273Aftershocks N 20 5 22Foreshocks N 3 6 -Duration T 7 - 1Peakaccel.
N 8 - -TOTAL 1,116 13,562 17,672Table 2: Event arguments and types in the earthquake do-main (first and second column), summary statistics forthe knowledge base, i.e., the gold truth (third column),and values for one example earthquake (4th column).
(*)indicates multi-valued arguments (all other are single-valued).
The two rightmost columns give statistics forthe number of mentions in the tweets per argument, asobtained through manual annotation (MA) or distant su-pervision (DS) (cf.
Section 2.4).
The argument types arethe following: D date, T time, L location, N numeric, andB boolean.lected earthquakes from the beginning of 2009,with the last reported earthquake happening on July7th, 2013, and constructed the KB from the aboveWikipedia list page and the individual infoboxes.Where necessary, argument values were normal-ized.5See Table 2 for a summary and an example.We used the Topsy API6to search for tweets thatare potentially relevant for each earthquake.
Weformed a query using the word ?earthquake?
plusthe location, encoded as a disjunction of city, region,and country arguments.
We retrieved tweets fromthe day before the date and time of the earthquake,up to seven days after.
This procedure might also re-trieve tweets about aftershocks, which we considerto be different events.
We applied an aggressivemethod to discard aftershock tweets: we only kept2013, at 2PM CET.5Time and date expressions were converted to TimeML.
Nu-merical values in English were converted to numbers, latitudeand longitudes were converted to decimal format.6http://api.topsy.com/doc/tweets up to the first tweet that mentions a time ex-pression more than a minute different from that ofthe main earthquake (after adjusting for time zone).For example, this heuristic removes all tweets start-ing with ?A 4.9 earthquake occurred in Ryukyu Is-lands, Japan on 2010-2-27 T10:33:21 at epicenter.
?because the main earthquake occurred on February26th at 8:31PM UTC.
It is important to note thatidentifying event-relevant tweets is not the focus ofthis work (hence the simple heuristics used for tweetextraction).
We focus instead on the extraction ofinformation from such tweets.
In a complete sys-tem, our approach would follow a component thatdetects event tweets automatically (Benson et al,2011).
The final dataset contains 108 earthquakesand 7,841 tweets, 72 tweets per earthquake on av-erage, a maximum of 654 and a minimum of 2.
19earthquakes had less than 10 tweets.2.2 Manual annotation of tweetsIn order to analyze the challenges faced by our EEsystem based on distant supervision, we also man-ually annotated all tweets.7The manual annotationincluded any mention of an event argument in thetweets.
This included information already in the KB,but also information that is missing, caused by: vari-ations of dates and times, similar but not identicallatitude/longitude values, different reported num-bers for dead/injured/missing etc.
The first tweet inTable 1 is an example of this situation: even thoughthe reported magnitude is different from the valuein the KB (cf.
example in Table 2), it was anno-tated during this process.
In total, we annotated17,672 mentions (at an average of two event argu-ments per tweet).
Table 2 shows the breakdown perargument (the MA column), compared to the auto-matic annotations generated through distant super-vision (the DS column).
Note that some of the ar-guments have a very different coverage in the tweetscompared with the KB.
For example, latitude andlongitude are rarely present in tweets, but affectedcountries are commonly mentioned.
The quality ofthe manual annotation was assessed on a 5% sampleof the dataset, which was annotated by an additionalexpert.
The agreement was very high: 90% ITA and85% Fleiss Kappa.
Disagreements were generally7These manual annotations are used solely for post-hocanalysis, not to train our system.643due to missed argument mentions.
Note that the costof annotation was around 75 hours, confirming thecost-saving properties of distant supervision.2.3 Dataset and experiment organizationWe sorted the list of earthquakes in the KB chrono-logically, and chose the earliest 75% of the earth-quakes as the training dataset, and the most recent(25%) for testing.
The training set contained 81earthquakes and their corresponding 6078 tweets,while the testing set contained 27 earthquakes and1763 tweets.
All development experiments wereperformed using 5-fold cross-validation over thetraining partition, where the folds were organizedrandomly by earthquake.
Each fold contained tweetsfor around 15 earthquakes, but the number of tweetsvaried widely, with one fold having 585 tweets andanother 2229.The evaluation compares the argument values in-duced by our system with those in the gold KB,and computes precision, recall and F1 using theofficial scorer from the Knowledge Base Popula-tion (KBP) Slot Filling (SF) shared task (Surdeanu,2013).
We also incorporated the notion of equiva-lence classes proposed in the SF task.
For instance,if the system predicted Guerrero State for the ar-gument region, when the KB contains just Guer-rero, we consider this result correct because the twostrings are equivalent in this context.
Our equiv-alence classes also include countries, regions, andcities with hashtags, unnormalized temporal expres-sions, etc.
Where applicable, we checked statisti-cal significance of performance differences using thebootstrap resampling technique proposed in (Berg-Kirkpatrick et al, 2012), in which we draw manysimulated test sets by sampling with replacementfrom the set of earthquakes in the test partition.2.4 Distant supervision for event extractionFor the initial extraction experiment, we followeda traditional distant supervision approach (Mintz etal., 2009), which has four steps: the KB of pastevents is aligned to the text; a supervised systemis trained on the resulting annotated text; the sys-tem is run on test data; and the output slot valuesare inferred from the annotations produced by thesystem.
We thus started by aligning the informationin the KB to the training tweets using strict match-ing8.
Table 2 compares the number of mentions au-tomatically generated through DS against the num-ber of manually annotated mentions.
As expected,the strict matching criterion yields fewer mentionsthan the manual annotation.As an example of this process, given the Hondurasearthquake in Table 2, this procedure will annotatetwo argument mentions in the first tweet from Ta-ble 1, country and affected-country, as fol-lows:Earthquake in <country>Honduras</country>.So strong it was felt in <affected-country>Guatemala</affected-country> aswell.
7.1 offshore atlantic.Note that the magnitude in the tweet is differentfrom the one reported in the KB and it will thus beleft unmarked (we revisit this issue in Section 5).Using this automatically-generated data, wetrained a sequential tagger based on ConditionalRandom Fields (CRF)9.
Based on the output of theCRF, we inferred the arguments values using noisy-or (Surdeanu et al, 2012), which selects the valuewith the largest probability for each single-valuedargument by aggregating the individual mentionprobabilities produced by the CRF.10In the case ofmulti-valued arguments (affected-country andaffected-region) we choose all values that hadbeen annotated by the sequential tagger.3 Initial results and analysisThe left block in Table 3 reports the results on devel-opment (5-fold cross-validation) of the initial event8We identified two types of arguments: those that have bi-nary (yes/no) values (tsunami and landslides) andthose having other values.
For the first type, we search thetweets corresponding to the target earthquake for a small num-ber of strings (e.g., tsunami and tsunamis), and annotate allmatches (e.g., <tsunami> tsunami </tsunami>).
For non-binary valued arguments, we searched the tweets for exact oc-currences of the corresponding values, and annotated all match-ing strings.
When the same value appears in more than oneargument for the same earthquake (e.g., 7 as both magnitudeand number of dead people), we choose the most common label(e.g., magnitude cf.
Table 2).9We used the linear CRF in Stanford?s CoreNLP package,with the default features (word form, PoS, lemma, NERC) forthe macro configuration: http://nlp.stanford.edu/software/corenlp.shtml.10For multi-token mentions (e.g.
New Zealand) we use theaverage of the token probabilities.644Strict EvaluationSystem Prec.
Rec.
F1DS-CRF 53.1 22.0 31.1MA-CRF 44.1 26.1 32.8Lenient EvaluationDS-CRF 67.4 27.9 39.4MA-CRF 62.1 36.8 46.2Table 3: Development: Results for the distant supervisionsystem (DS-CRF).
We also include results for the sameCRF trained on manual annotations (MA-CRF).
The reg-ular evaluation is shown in the left columns and lenientevaluation (cf.
Section 4) in the right.extraction system based on a distantly-supervisedCRF (DS-CRF), which notably attains higher pre-cision than recall.
These results are fair, e.g., theyare comparable to those of (Benson et al, 2011),even though their events had much fewer argumenttypes than ours (two vs. twenty).
More importantly,we use this system?s output to analyze where the ap-proach could be improved.
For the sake of compari-son, we trained the same CRF with the manually an-notated tweets, cf.
Section 2 (MA-CRF).
The MA-CRF results in Table 3 indicate that the main losswhen doing distant supervision is in recall, but theoverall F1 is close.
This is remarkable, as the muchmore expensive MA-CRF (75 hours of human anno-tation) is taken to be an upperbound for DS-CRF.Manual inspection showed that that DS-CRF re-turns fewer argument values than MA-CRF (328vs.
469), from ?easier?
(more common) argumentswhich have a higher chance of appearing both in thetext and the KB.
Importantly, MA-CRF has lowerprecision than its distant supervision counterpart be-cause it is trained on manual annotations, which in-cluded many mentions not in the KB.
The conse-quence of this strategy is that MA-CRF tends toproduce spurious mentions (i.e., mentions not in theKB) at evaluation time, which lowers precision.In addition, we analyzed the annotations cre-ated through distant supervision11, which produced13,562 argument mentions in the training tweets (cf.Table 2, which also includes a breakdown by ar-11Note that these are the argument mention annotations usedto train DS-CRF, not the arguments inferred by the DS-CRFsystem.gument).
This data contains incorrectly annotatedstrings (false positives) and also misses relevant ar-gument values (false negatives).
A comparison ofthese DS annotations against the manual annotationson all training tweets (17,672 mentions) yielded that97.4% were correct, but that 27.4% of the gold man-ual annotations were missed.
This is an importantresult: it demonstrates that, unlike in the problemof relation extraction (RE) where the major issue isthe large percentage (higher than 30%) of false pos-itives in automatically-created annotations (Riedelet al, 2010), here the fundamental roadblock ismissing annotations (i.e., false negatives).
We ex-plain this difference by the fact that for this eventextraction domain, it is trivial to identify domain-relevant tweets, which reduces the number of falsepositives for event arguments.
We believe this gen-eralizes to many other EE domains, e.g., airplanecrashes (Reschke et al, 2014) or terrorist attacks,where the event context can be summarized accu-rately with a small number of keywords (e.g., flightnumber and date for the airplane crashes domain).We also did a post-hoc analysis of the quality ofthe arguments induced by DS-CRF.
One of the mostsignificant outcomes of the analysis is that a largeportion of numeric values (31.3%) were partiallycorrect, in that the returned values were very simi-lar to those in the KB (see for instance the 7.1 vs.7.3 example in Section 1).
This strongly suggeststhat the evaluation metric should be more lenient,and give credit to argument values that are similar tothe gold ones.4 Lenient evaluationThe previous analysis suggests that traditional eval-uation measures unnecessarily penalize argumentscontaining values that do not match the gold truthexactly.
Rather than giving no credit when predictedvalues are different from gold ones, we devised asimple extension to the KBP evaluation measuresthat take into account the similarity between the val-ues of system and gold arguments, where the simi-larity depends on the type of each slot (cf.
Table 2).For numeric values, we use the following formula,where x is the predicted value, and g the gold value:sim(x, g) = max(1?|x?
g|g, 0)(1)645For example, given a gold value of 7.3, a systemvalue of 7.2 would have a similarity of 0.98, and asystem value of 14.6 or larger would have a similar-ity 0.
If both values are equal, similarity is 1.For the other slot types, the similarity function isdiscrete, with values set to 1 (proposed slot is cor-rect) or 0 (incorrect) as follows.
We consider a pro-posed temporal argument as correct if it is within aspan of 5 minutes of the corresponding gold tem-poral value.
Durations are judged as correct if theyare within 10 seconds of the gold values.
We con-sidered proposed dates as correct if they differ by atmost one day from the gold date.12For location arguments, we use GeoNames13toobtain the coordinates of the locations produced bythe system that do not match the information in theKB.
Based on the average size of countries, regions,and cities, we consider these additional locationsas correct if they are at the following distance (orcloser) from the gold locations: 500 kms for coun-tries, 50 kms for regions, and 10 kms for cities.The original KBP scorer increases the value ofTrue Positives (TP) by 1 every time a predicted argu-ment matches its gold value.
In the proposed lenientscorer, TP is increased by the similarity between thepredicted and gold values.
The precision and recallwill be thus calculated as follows (SYS for numberof predicted argument values, GOLD for number ofgold argument values):prec =?sim(x, g)SYS(2)rec =?sim(x, g)GOLD(3)The right block in Table 3 lists the results underthis lenient evaluation for the experiment initiallyreported in the left block in the same table.
As ex-pected, these results are higher than the ones usingthe strict measure, but maintain the relative order ofthe systems in each of the evaluation measures.
Thedifference in precision between DS-CRF and MA-CRF decreases, indicating that the new measure as-signs partial credit to the larger amount of argumentvalues extracted by MA-CRF.
The difference in re-12These thresholds might change in other domains, but ad-justing these values is trivial.13http://www.geonames.org/0.450.50.550.60.650.70.750.80.850.90.9510  0.05  0.1  0.15  0.2  0.25  0.3PrecisionRecallDS-CRFDSappr-CRFFigure 1: Test: Precision/Recall curves for regular DSand approximate DS on test (lenient evaluation).System Prec.
Rec.
F1DS-CRF 68.4 21.3 32.5DSappr-CRF 70.6 27.8 39.9 ?Table 4: Test: Regular (DS-CRF) and approximate DS(DSappr-CRF) results, with lenient evaluation.
?
indicatesstatistically significant improvement over DS-CRF (p <0.05).call values remains large.
We address this in the nextsection.5 Approximate distant supervisionThe previous section demonstrated that many tweetscontain argument values which are similar but notidentical to the data in the knowledge base.
Thesevalues would not be annotated during alignment bytraditional distant supervision, which expects an ex-act match between knowledge base values and tweettexts.
This means that DS-CRF will be trained withless data than what is available (e.g., without the7.1 magnitude example in the tweet in Section 2.4).Here we demonstrate that a simple extension to dis-tant supervision that annotates values close to thevalues in the knowledge base, results in improvedperformance.The proposed alignment algorithm scans thetraining tweets, and labels named and numeric en-tities as positive argument examples (with the cor-responding label from the KB), if they are deemedsimilar to the gold values according to the similar-ity formulas introduced in the previous section.
This6460.450.50.550.60.650.70.750.80.850.90.9510  0.05  0.1  0.15  0.2  0.25  0.3  0.35PrecisionRecallDS-CRFDSaggr-CRFDScomb-CRFFigure 2: Test: P/R curves for DS-CRF, feature aggrega-tion and combination with approximate DS (lenient eval-uation).is a trivial process for discrete similarities, but re-quires some care for continuous similarity functions,which are triggered for numeric arguments.
In thissituation, numeric entities are considered as positiveexamples only if their similarity function returns avalue over a certain threshold with a known argu-ment in the KB.
If a numeric mention has more thanone matching argument in the KB, the algorithmchooses the argument label with the highest simi-larity value; if all have the same similarity, the algo-rithm chooses the most frequent label in training.We tuned the threshold hyper parameter for nu-meric values over the training dataset using 5-foldcross validation, which yielded 0.95 as the optimalvalue.
Table 4 shows the results for the test parti-tion using this threshold, and Figure 1 shows thecorresponding P/R curves.
Both results are gen-erated using the proposed lenient evaluation.
Theresults in the table show that, despite its simplic-ity, the proposed alignment algorithm yields consid-erable, statistically-significant improvements.
TheP/R curves show that the improvement holds for allrecall points14.6 Feature aggregationThe second block in Table 1 illustrates a commonscenario on Twitter, where a short, ambiguous tweetderails the extraction.
We address this problem of14The curves for the strict evaluation are similar, and wereomitted for brevity.System Prec.
Rec.
F1DS-CRF 68.4 21.3 32.5DSaggr-CRF 70.1 26.6 38.6 ?DScomb-CRF 69.2 31.2 43.1 ?MA-CRF 69.1 37.9 48.9Table 5: Test: Results for regular DS (DS-CRF), DS withfeature aggregation (DSaggr-CRF), and the DS model thatcombines feature aggregation and approximate matching(DScomb-CRF), with lenient evaluation.
?
indicates statis-tically significant improvement over DS-CRF (p < 0.05).We include the results of the CRF trained on manual an-notations (MA-CRF) as a performance ceiling for thistask.insufficient local context with a method inspired bywork in relation extraction, where relation instancesbetween identical entities are classified jointly usingthe conjunction of features from all instances (Mintzet al, 2009).
We adapt this idea to our sequencetagging EE model as follows:1: We focus on location, date and temporal enti-ties (both earthquake time and duration) which areargument candidates that are often ambiguous, i.e.,they may be classified as more than one argumenttype.
For example, a location entity may be labeledas country, region, country-affected, etc.We exclude numeric entities due to potential featurecollisions between different argument types: we ob-served that, in training, several earthquakes had dif-ferent numeric arguments with the same value.
Forexample, the magnitude and depth of the 2012 Zo-han earthquake were 5.6.
Applying feature aggrega-tion to examples of these arguments would lead tocollisions between features from different classes.152: For each token that appears in one of these namedentities, we identify all its instances across the rele-vant tweets, and share features across all these tokeninstances.
For example, for the tweets in the sec-ond block in Table 1, our approach identifies Peruas an argument mention candidate.
All three in-stances of Peru are then classified using the sameshared features, e.g., using three values for the fea-15Initial experiments confirmed this hypothesis: feature ag-gregation did not improve results for numeric arguments indevelopment.
In future work, we will explore multi-instancemulti-label algorithms to handle this situation (Surdeanu et al,2012).647ture previous-word (:, rocks, and in).
This pro-cess is repeated for each earthquake individually,because tokens may be labeled differently in differ-ent earthquakes.
This approach produced 37% morefeatures than the DS-CRF baseline.16The positive effect of feature aggregation is con-firmed by the formal evaluation on the test dataset.Table 5 shows a statistically significant improvementin overall F1, for the lenient evaluation.
The P/Rcurves (Fig.
2) indicate that DSaggr-CRF?s improve-ment comes from both better recall and better preci-sion that the DS-CRF baseline.Table 5 and Fig.
2 also show that the combina-tion of approximate matching and aggregation out-performs the individual models, demonstrating thatfeature aggregation is complementary to approxi-mate matching.
The combined model attains a rela-tive improvement of 33% over the DS-CRF baseline,reaching approximately 88% of the ceiling perfor-mance for this task (MA-CRF row, the CRF trainedon manual annotations).7 Related workThere has been considerable recent interest in IEfrom Twitter.
However, in general, these worksuse supervised learning frameworks (Popescu etal., 2011; Ritter et al, 2012), and/or they use ei-ther a coarse representation of events, which re-duces to topic modeling or classification of entiretweets (Popescu et al, 2011; Becker et al, 2011;Ritter et al, 2012), or a simplified representationof events with few arguments (Sakaki et al, 2010;Popescu et al, 2011; Benson et al, 2011; Ritteret al, 2012).
In contrast, our work uses a com-plex event representation with 20 arguments, anddoes not require any manual annotation of tweets.Our work is closest, but complementary to the workof (Benson et al, 2011), which also uses distant su-pervision for event extraction: We provide solutionsfor two problems they do not address (inaccurate andambiguous information) and we focus on more com-plex events (20 arguments vs. two).This paper is also complementary to systemswhich detect event-relevant tweets (Sakaki et al,16We also tried skip-chain CRFs (Getoor and Taskar, 2007),but found that our simpler approach converges considerablyfaster and produces slightly better results.
We do not show thoseresults for brevity.System Prec.
Rec.
F1DS-CRF 66.21 20.66 31.49DSaggr-CRF 68.27 25.92 37.58 ?DScomb-CRF 61.53 27.61 38.25 ?MA-CRF 68.76 27.61 39.40Table 6: Test: Replica of the experiments in Table 5 usinga threshold of 0.95 for the lenient evaluation measure.
Allother settings are identical to the experiments in Table 5.?
indicates statistically significant improvement over DS-CRF (p < 0.05).2010; Petrovi?c et al, 2010).
In future work, we planto replace our simple method of extracting relevanttweets by one of these approaches, producing a sys-tem that monitors microblogs in realtime to automat-ically construct event-specific knowledge bases.Our work uses the framework of distant supervi-sion, which has also received considerable attentionrecently.
Nevertheless, most of these works focus onthe extraction of binary relations from well-formeddocuments (Mintz et al, 2009; Riedel et al, 2010;Hoffmann et al, 2011; Surdeanu et al, 2012).
Weuse the much noisier Twitter as the underlying text,and extract complex events instead of binary rela-tions.
We note, however, that the idea of feature ag-gregation is inspired by these works (Mintz et al,2009; Riedel et al, 2010), but, to our knowledge,we are the first to apply it to event extraction andsequence tagging.
In the DS space, our work is clos-est to (Reschke et al, 2014), which use it to extractcomplex events (airplane crashes) from newswiretext.
Because they focus on newswire, they do notneed to address the potential for inaccurate or am-biguous information, which is the main focus of ourwork.8 Discussion: An alternate evaluationmeasureDesigning relevant measures for lenient evaluations,such as the one discussed here, is an open researchissue.
For example, the method proposed in Sec-tion 4 gives partial credit to all reported (positive)numeric values in the interval [0, 2g], where g isthe correct value for the corresponding slot (see theequation in Section 4).
But other, stricter, measures648are certainly possible.17For example, one strictervariant of our proposed measure would assign par-tial credit only for predicted values that have a sim-ilarity of 0.95 or higher with the gold truth (inlinewith our approximate DS training process).
For ex-ample, for the same gold numeric value g, the mea-sure assigns partial credit only for predicted valuesin the interval [0.95g, 1.05g].We repeated the experiments in Table 5 using thisalternate evaluation measure.
The result are summa-rized in Table 6.
The results reported in Table 5 donot alter the findings of the paper.
In fact, under thisstricter evaluation measure, our results are stronger:DScomb-CRF, which combines both our ideas, ap-proaches with nearly 1 F1 point MA-CRF, whichtrains on manually annotated data.9 ConclusionsTo our knowledge, this is one of the first works thatanalyzes the problem of distantly supervised com-plex event extraction on microblogs.
This near real-time data source is challenging, with inaccurate in-formation and short, ambiguous texts, as shown byour empirical analysis of the dataset.
We proposedtwo simple techniques to address these problems:(a) a novel distant supervision paradigm, which im-plements an alignment algorithm that allows textsnippets that are similar but not identical to argu-ment values in the knowledge base to be annotated(thus producing better training data); and (b) a fea-ture aggregation strategy that provides richer infor-mation across tweets to cope with ambiguity.
Ourresults on earthquake-related tweets show that eachimprovement yields 19% significant improvementwhen applied on top of a strong system based on se-quence tagging (CRFs).
We show that these contri-butions are complementary: a model that combinesboth performs better than each of the above individ-ual models, with an improvement of 33% over thebaseline.
All in all, our approach attains approxi-mately 88% of the ceiling performance for this task,which is obtained by a system trained on manually-annotated tweets, validating the hypothesis that dis-tant supervision is useful for a complex event extrac-tion task.17We thank the anonymous reviewer for the suggestion.In addition, we devised a lenient evaluation mea-sure which incorporates the similarity between theextracted argument values and the gold truth, ratherthan considering as correct only the extractions thatexactly match the gold values.
We show that thisevaluation models the event extraction task better,and, furthermore, is more realistic, especially inview of imperfect knowledge bases.Lastly, we release a dataset containing an eventknowledge base constructed from Wikipedia infor-mation on earthquakes, which contains 108 earth-quakes, 20 different argument types, and 1,116 argu-ment values.
The dataset alo includes a collectionof relevant tweets about these earthquakes, totaling7,841 tweets.
The dataset is publicly available.18AcknowledgementsThis work was partially funded by MINECO(CHIST-ERA READERS project ?
PCIN-2013-002-C02-01, EXTRECM project ?
TIN2013-46616-C2-1-R, SKaTeR project ?
TIN2012-38584-C06-02), and the European Commission (QTLEAP ?FP7-ICT-2013.4.1-610516).
Ander Intxaurrondo issupported by a PhD grant from the Basque Coun-try Government.
The IXA group is funded by theBasque Government (A type Research Group).18http://ixa.eus/Ixa/Argitalpenak/Artikuluak/1425465524/publikoak/earthquake-kb-dataset.zip649ReferencesHila Becker, Mor Naaman, and Luis Gravano.
2011.
Be-yond trending topics: Real-world event identication ontwitter.
In Proceedings of the Conference of the Asso-ciation for the Advancement of Articial Intelligence.Edward Benson, Aria Haghighi, and Regina Barzilay.2011.
Event discovery in social media feeds.
In Pro-ceedings of the Conference of the Association for Com-putational Linguistics (ACL).Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.2012.
An empirical investigation of statistical signifi-cance in nlp.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, EMNLP-CoNLL ?12, pages 995?1005, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, and R. Weischedel.
2004.
The automaticcontent extraction (ace) program ?
tasks, data, andevaluation.
In Proceedings of LREC.L.
Getoor and B. Taskar, 2007.
Introduction to statisticalrelational learning.
MIT Press.R.
Grishman and B. Sundheim.
1996.
Message under-standing conference - 6: A brief history.
In Proceed-ings of the International Conference on ComputationalLinguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL).Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics.Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applica-tion to twitter.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 181?189, Los Angeles, California, June.
Asso-ciation for Computational Linguistics.Ana-Maria Popescu, Marco Pennacchiotti, and DeepaParanjpe.
2011.
Extracting events and event descrip-tions from twitter.
In Proceedings of the 20th Interna-tional Conference on World Wide Web.Kevin Reschke, Martin Jankowiak, Mihai Surdeanu,Christopher D. Manning, and Daniel Jurafsky.
2014.Event extraction using distant supervision.
In Pro-ceedings of LREC.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the European Confer-ence on Machine Learning and Knowledge Discoveryin Databases (ECML PKDD ?10).Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.2012.
Open domain event extraction from twitter.
InProceedings of KDD.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes twitter users: Real-timeevent detection by social sensors.
In Proceedings ofthe 19th International Conference on World Wide Web,WWW ?10, pages 851?860, New York, NY, USA.ACM.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, andChristopher D. Manning.
2012.
Multi-instance multi-label learning for relation extraction.
In Proceed-ings of the 2012 Conference on Empirical Methods inNatural Language Processing and Natural LanguageLearning (EMNLP-CoNLL).Mihai Surdeanu.
2013.
Overview of the tac2013 knowl-edge base population evaluation: English slot fillingand temporal slot filling.
In Proceedings of the TAC-KBP 2013 Workshop.650
