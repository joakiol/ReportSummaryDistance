Semantic Role Labeling Using Dependency TreesKadri HaciogluCenter for Spoken Language ResearchUniversity of Colorado at Boulderhacioglu@cslr.colorado.eduAbstractIn this paper, a novel semantic role labeler basedon dependency trees is developed.
This is ac-complished by formulating the semantic role la-beling as a classification problem of dependencyrelations into one of several semantic roles.
Adependency tree is created from a constituencyparse of an input sentence.
The dependency treeis then linearized into a sequence of dependencyrelations.
A number of features are extracted foreach dependency relation using a predefined lin-guistic context.
Finally, the features are input to aset of one-versus-all support vector machine(SVM) classifiers to determine the correspondingsemantic role label.
We report results onCoNLL2004 shared task data using the represen-tation and scoring scheme adopted for that task.1   IntroductionIn semantic role labeling (SRL) the goal is to groupsequences of words together and classify them byusing semantic labels.
For semantic representationwe select the predicate-argument structure that ex-ists in most languages.
In this structure a word isspecified as a predicate and a number of wordgroups are considered as arguments accompanyingthe predicate.
Those arguments are assigned differ-ent semantic categories depending on the roles thatthey play with respect to the predicate.We illustrate the predicate-argument structure inFigure 1 for the sentence ?We are prepared to pur-sue aggressively completion of this transaction hesays?
taken from the PropBank corpus.
The chosenpredicate is the word pursue, and its arguments withtheir associated word groups are illustrated.
Notethat the word prepared is another predicate of thesentence possibly with different argument labelsattached to the same or different word groups.
Forexample, the word we is A1 of prepared.
This proc-ess of selecting a predicate in a sentence, groupingsequences of words and assigning the semantic rolesthey play with respect to the chosen predicate is of-ten referred to as semantic role labeling.
We believethat a highly accurate extraction of this structure isvital for high performance in many NLP tasks suchas information extraction, question answering,summarization and machine translation.Figure 1: Predicate-argument structure of samplesentence.
Argument labels are in PropBank-style.Semantic role labeling based on predicate-argument structure was first explored in detail by(Gildea and Jurafsky, 2002).
Since then severalvariants of the basic approach have been introducedusing different features and different classifiersbased on various machine learning techniques(Gildea and Palmer, 2002; Gildea and Hockenmaier,2003; Surdeanu et.
al., 2003;  Chen and Rambow,2003; Fleischman and Hovy, 2003; Hacioglu andWard, 2003; Thompson et.
al., 2003; Pradhan et.
al.,2003b; Hacioglu, 2004).
Large semantically anno-tated databases, like FrameNet (Baker et.al, 1998)and PropBank (Kingsbury and Palmer, 2002) havebeen used to train and test the classifiers.
Most ofthose approaches can be divided into one of the fol-lowing three broad classes with respect to the typeof tokens classified; namely, constituent-by-constituent (C-by-C), phrase-by-phrase (P-by-P) andword-by-word (W-by-W) semantic role labelers.In C-by-C semantic role labeling, the syntactictree representation of a sentence is linearized into asequence of its syntactic constituents (non-terminals).
Then each constituent is classified intoone of several semantic roles using a number of fea-tures derived from the sentence structure or a lin-guistic context defined for the constituent token.
Inthe P-by-P and W-by-W methods (Hacioglu, 2004;Hacioglu and Ward, 2003) the problem is formu-lated as a chunking task and the features are derivedfor each base phrase and word, respectively.
Thetokens were classified into one of the semantic la-bels using an IOB (inside-outside-begin) representa-tion and a bank of SVM classifiers; a one-versus-allclassifier has been used for each class.Predicate:pursueA0  A1 AM-MNRwe  completion of this transaction  aggressivelyFigure 2.
Example of a dependency tree augmented with semantic roles.
Semantic labels correspond to thepredicate posted.
The same tree with different semantic labels also exists in the corpus for predicate abated.In this paper, we introduce another approach thatwe refer to as the relation-by-relation (R-by-R) se-mantic role labeling.
The method is based on de-pendency trees generated from constituency trees.Although the system currently does not use moreinformation than C-by-C systems, the information isstructured in a different manner and, consequently,the nature of some linguistic features is quite differ-ent.
We point out that this information restructuringis very useful in localizing the semantic roles asso-ciated with the selected predicate, since the depend-ency trees directly encode the argument structure oflexical units populated at their nodes through de-pendency relations.A related work is reported in (Gildea and Hock-enmaier, 2003).
However, they use CombinatoryCategorical Grammar (CCG) to derive the depend-ency relations.
In addition, our method differs in theselection of dependency relations for labeling, in thecreation of features and in the implementation of theclassifier.Recently, there has been some interest in develop-ing a deterministic machine-learning based approachfor dependency parsing (Yamada and Matsumato,2003).
In addition to relatively easier portability toother domains and languages the deterministic de-pendency parsing promises algorithms that are ro-bust and efficient.
Therefore, an SRL algorithmbased on dependency structures is expected to bene-fit from those properties.2   Dependency Bank (DepBank)In this section, we describe the corpus that we auto-matically created using the syntactic annotations ofthe Penn TreeBank with the semantic annotations ofthe PropBank.
Hereafter, we refer to this new corpusas DepBank.Firstly, we convert constituency trees into de-pendency trees1.
The functional tags are removedfrom constituency trees before the conversion, sincethe current state-of-the-art syntactic parsers do notexploit those tags.
Secondly, we trace the depend-ency trees to determine the word sequences coveredby the dependency relation nodes.
Finally, we aug-ment those nodes with their semantic role labels thatcover the same sequence of words.
The relationsthat do not align with any semantic role are taggedusing the label  ?O?.
In Figure 2, we illustrate asample dependency tree from the DepBank.
It corre-sponds to the predicate posted of the following sen-tence (semantic roles are also indicated):[A0 The dollar] [V posted]  [A1 gains] [AM-LOCin quiet training] [AM-ADV as concerns about equi-ties abated]We note that the other predicate in the sentence isabated and the same tree with different semanticlabels is also instantiated in the DepBank for it.
Thedependency relation nodes are indicated by ?R:?
inFigure 2.
The lexical nodes are indicated by ?W:?.The dependency relation types are paired with thecorresponding semantic role labels.
The only excep-tion is the node that belongs to the predicate; thesemantic label V is used with the lemma of thepredicate.
The lexical nodes include the word itselfand its part-of-speech (POS) tag.3   Semantic Role Labeling of RelationsIn the proposed approach, we first linearize the de-pendency tree in a bottom-up left-to-right mannerinto a sequence of dependency relations.
During this1 engconst2dep, from the University of Maryland, is used.
Spe-cial thanks to R. Hwa, A. Lopez and M. Diab.process we filter out the dependency relations thatare less likely to be an argument.
The selectionmechanism is based on simple heuristics derivedfrom dependency trees.
Then we extract a set of fea-tures for each dependency relation.
Finally, we inputthe features to a bank of SVM classifiers.
A one-versus-all SVM classifier is used for each semanticrole.3.1 Dependency Relation SelectionIn dependency tree representations, we observe thatthe semantic roles are highly localized with respectto the chosen predicate.
We exploit this observationto devise a method for deciding whether a depend-ency relation is likely to be a semantic role or not.We define a tree-structured family of a predicate as ameasure of locality.
It is a set of dependency relationnodes that consists of the predicate?s parent, chil-dren, grandchildren, siblings, siblings?
children andsiblings?
grandchildren with respect to its depend-ency tree.
Any relation that does not belong to thisset is skipped while we linearize the dependencytree in a bottom-up left-to-right manner.
Further se-lection is performed on the family members that arelocated at the leaves of the tree.
For example, a leafmember with det dependency relation is not consid-ered for semantic labeling.
Our selection mechanismreduces the data for semantic role labeling by ap-proximately 3-4 fold with nearly 1% miss of seman-tic labels, since a quite large number of nodes in thedependency trees are not associated with any seman-tic role.3.2  FeaturesFor each candidate dependency relation we extract aset of features.
In the following, we explain thesefeatures and give examples for their values referringto the dependency tree shown in Figure 1 (featurevalues for the relation node R:mod with the seman-tic label [A0] is given in parentheses).
The featuresthat are specific to the dependency relation (i.e.
to-ken-level features) areType: This feature indicates the type of the de-pendency relation  (mod)Family membership:  This feature indicates howthe dependency relation is related to the predicate inthe family (child)Position:  This feature indicates the position ofthe headword of the dependency relation with re-spect to the predicate position in the sentence (be-fore)Headword: the modified (head) word in the rela-tion (posted).Dependent word: the modifying word in the re-lation (dollar)POS tag of  headword: (VBD)POS tag of dependent word: (NN)Path: the chain of relations from   relation nodeto predicate.
(mod?
*)and the features that are specific to the  predicate(i.e.
sentence-level features):POS pattern of predicate?s children: This fea-ture indicates the left-to-right chain of the POS tagsof the immediate words that depend on the predi-cate.
(NN-NNS-IN-IN)Relation pattern of predicate?s children: Thisfeature indicates the left-to-right chain of the rela-tion labels of the predicate?s children (mod-obj-p-obj)POS pattern of predicate?s siblings: This fea-ture indicates the left-to-right chain of the POS tagsof the headwords of the siblings of predicate.
(-)Relation pattern of predicate?s siblings: Thisfeature indicates the left-to-right chain of the rela-tion labels of the predicate?s siblings.
(-).3.3  ClassifierWe selected support vector machines (Vapnik,1995) to implement the semantic role classifiers.The motivation for this selection was the ability ofSVMs to handle an extremely large number of inter-acting or overlapping features with quite strong gen-eralization properties.
Support vector machines forSRL were first used in (Hacioglu and Ward, 2003)as word-by-word (W-by-W) classifiers.
The systemwas then applied to the constituent-by-constituent(C-by-C) classification in (Hacioglu et.
al., 2003)and phrase-by-phrase (P-by-P) classification in (Ha-cioglu, 2004).
Several extensions of the basic sys-tem with state-of-the-art performance were reportedin (Pradhan et.al, 2003; Pradhan et.
al.
2004; Ha-cioglu et.
al.
2004).
All  SVM classifiers for seman-tic argument labeling were  realized using theTinySVM with a polynomial kernel of degree 2 andthe general purpose SVM based chunker YamCha2.4   ExperimentsExperiments were carried out using a part of theFebruary 2004 release of the PropBank.
Sections 15through 18 were used for training, Section 20 wasused for developing and Section 21 was used fortesting.
This is exactly the same data used forCoNLL2004 shared task on SRL.
Therefore, theresults can be directly compared to the performanceof the systems that used or that will use the samedata.
The system performance is evaluated by usingprecision, recall and F metrics.
In the experiments,2 http://cl.aist-nara.ac.jp/~taku-ku/software/the gold standard constituency parses were used.Therefore, the results provide an upper bound on theperformance with automatic parses.
Table 1 presentsthe results on the DepBank development set.
Theresults on the CoNLL2004 development set are alsoillustrated.
After we project the predicted semanticrole labels in the DepBank dev set onto theCoNLL2004 dev set (directly created from thePropBank) we observe a sharp drop in the recall per-formance.
The drop is due to the loss of approxi-mately 8% of semantic roles in the DepBank dev setduring the conversion process; not all phrase nodesin constituency trees find an equivalent relationnode in dependency trees.
However, this mismatchis significantly less than the 23% mismatch reportedin (Gildea and Hockenmaier, 2003) between theCCGBank and an earlier version of the PropBank.Dev  Set Precision Recall F1DepBank 85.6% 83.6%  84.6CoNLL 84.9% 75.2% 79.8Table 1: Results on DepBank and CoNLL04 sets.5   ConclusionsWe have automatically created a new corpus of de-pendency trees augmented with semantic role labels.Using this corpus, we have developed and experi-mented with a novel SRL system that classifies de-pendency relations.
This is quite different fromprevious research on semantic role labeling.
Wehave presented encouraging intermediate results.Currently, we are investigating the reasons of mis-match between PropBank and DepBank semanticannotations.
We also plan to add new features, ex-periment with automatic parses, and compare andcombine the system with our state-of-the-art C-by-Csystem.AcknowledgementsThis research was supported in part by the ARDAAQUAINT Program via contract OCG4423B and bythe NSF grant ISS-9978025.ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe 1998.
The Berkley FrameNet Project.
InProc.
of  CoLING-ACL?98.John Chen and Owen Rambow.
2003.
Use of DeepLinguistic Features for the Recognition and La-beling of Semantic Arguments.
In Proc.
ofEMNLP-2003Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Lin-guistics, 28:3, pages 245-288.Daniel Gildea  and Martha Palmer.
2002.
The Ne-cessity of Syntactic Parsing for Predicate Argu-ment Recognition.
In Proc.
of ACL?02.Daniel Gildea and Julia Hockenmaier.
2003.
Identi-fying Semantic Roles Using Combinatory Cate-gorical Grammar.
In Proc.
of EMNL?03, Japan.Micheal Fleischman and Eduard Hovy.
2003.
AMaximum Entropy Approach to FrameNet Tag-ging.
In Proc.
of  HLT/NAACL-03.Kadri Hacioglu and Wayne Ward.
2003.
Targetword detection and semantic role chunking usingsupport vector machines.
In  Proc.
ofHLT/NAACL-03.Kadri Hacioglu, Sameer Pradhan, Wayne Ward,James H. Martin and Daniel Jurafsky.
2003.
Shal-low Semantic Parsing using Support Vector Ma-chines.
CSLR Technical Report, CSLR-TR-2003-1.Kadri Hacioglu.
2004.
A Semantic Chunking ModelBased on Tagging.
In Proc.
of  HLT/NAACL-04.Kadri Hacioglu, Sameer Pradhan, Wayne Ward,James H. Martin and Daniel Jurafsky.
2004.
Se-mantic Role Labeling by Tagging SyntacticChunks.
CONLL-2004 Shared Task.Paul Kingsbury, Martha Palmer, 2002.
From Tree-Bank to PropBank.
In Proc.
of  LREC-2002.Sameer Pradhan, Kadri Hacioglu, Wayne Ward,James H. Martin, Dan Jurafsky.
2003.
SemanticRole Parsing: Adding Semantic Structure to Un-structured Text.
In Proc.
of ICDM 2003.Sameer Pradhan, Kadri Hacioglu, Wayne Ward,James H. Martin, Dan Jurafsky.
2004.
SupportVector Learning for Semantic Argument Classifi-cation.
To appear in Journal of Machine Learn-ing.Mihai Surdeanu, Sanda Harabagiu, John Williams,and Paul Aarseth.
2003.
Using Predicate-Argument Structure for  Information Extraction.In Proc.
of ACL03.Cynthia A. Thompson, Roger Levy, and ChristopherD.
Manning.
2003.
A Generative Model for Se-mantic Role Labeling.
In Proc.of  ECML-03.Vladamir Vapnik 1995.
The Nature of StatisticalLearning Theory.
Springer Verlag, New York,USA.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Sta-tistical Dependency Analysis with Support VectorMachines.
In Proc.
of  IWPT?03.
