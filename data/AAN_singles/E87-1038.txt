COPING WITH DYNAMIC SYNTACTIC STRATEGIES: AN EXPERIMENTAL ENVIRONMENT FOR ANEXPERIMENTAL PARSEROliviero StockI.P.
- Consiglio Nazionale delle RicercheVia dei Monti Tiburtini 50900157 Roma, ItalyABSTRACTAn environment built around WEDNESDAY 2, a chartbased parser is introduced.
The env i ronment  is inparticular oriented towards exploring dynamic aspectsof parsing.
It includses a number of specialized toolsthat  consent an easy, graphics-based interaction withthe parser.
It is shown in particular how a combinationof the characteristics of the parser (based on the lexiconand on dynamic unification) and of the env i ronmentallow a nonspecialized user to explore heuristics thatmay alter the basica control of the system.
In this way,for instance, a psychol inguist  may explore ideas onhuman parsing strategies, or a "language engineer" mayfind useful heuristics for parsing within a particularapplication.1.
Int roduct ionComputer-based env i ronments  for the l inguist  areconceived as sophisticated workbenches, bu i l t  on AIworkstat ions around a specif ic parser ,  where  thel inguist can try out his/her ideas about a grammar for acertain natural  anguage.
In doing so, he/she can takeadvantage of rich and easy-to-use graphic interfacesthat  "know" about linguistics.
Of course behind all thislies the idea that cooperation with linguists will providebetter esults in NLP.
To substantiate his assumption itmay be recalled that  some of the most interesting recentideas on syntax have been developed by means of jointcont r ibut ions  from l ingu is ts  and  computat iona ll inguists.
Lexical -Funct ional  Grammar  \ [Bresnan &Kap lan  1982\], GPSG \ [Gazdar  1981\], Funct iona lGrammar \[Kay 1979\], DCG \[Pereira & Warren 1980\],TAG \[Joshi & Levy 1982\] are some of these ideas.Instances of the tools introduced above are the LFGenvironment, which was probably the first of its kind, anenv i ronment  bu i l t  by Ron Kap lan  for Lex ica l -Funct iona l  Grammars ,  DPATR,  bu i l t  by Laur iKarttunen and conceived as an environment that wouldsuit  l inguists of a number  of di f ferent schools al lcommitted to a view of parsing as a" process that  makesuse of an unification algorithm.We have developed an environment with a somewhatdifferent purpose.
Besides a number of tools for enteringdata  in graph ic  mode and inspect ing  resu l t ingstructures, it provides a means for experimenting withstrategies in the course of the parsing process.
We thinkthat  this can be a valuable tool for gaining insight in thecognitive aspects of language processing as well as fortailoring the behaviour of the processor when used witha particular (sub)language.In this way an attempt can be made to answer basicquestions when following a nondeterministic approach:what heuristics to apply when facing a certain choicepoint, what to do when facing a failure point, i.e.
whichof the pending processes to activate, taking account ofinformation resulting from the failure?Of course this kind of environment makes sense onlybecause the parser it works on has some characteristicsthat  make it a psychologically interesting realization.2.
Mot ivat ion of  the parserWe shall classify psychologically motivated parsers inthree main categories.
First: those that  embody a strongclaim on the specification of the general control structureof the human parsing mechanism.
The authors usuallyconsider the level of basic control of the system as thelevel they are simulating and are not concerned withmore particular heuristics.
An instance of this class ofparsers is Marcus's parser \[Marcus 1979\], based on theclaim that,  basically, parsing is a deterministic process:only sentences that  we perceive as "surprising" (the socalled garden paths) actual ly imply backt rack ing .234Connectionist parsers are also instances of this category.The second category  refers to genera l  l inguist icperformance notions such as the "Lexical PreferencePrinciple" and the "Final Argument Principle" \[Fodor,13resnan and Kaplan 19821.
It includes theories ofprocessing like the one expressed by Wanner  andMaratsos for ATNs in the mid Sevent ies .
In th iscategory the arguments  are at  the level of generalstructural preference analysis.
A third category tendsto consider at every moment of the parsing process, thefull complexity of the data and the hypothesized partialinternal representation of the sentence, including, atleast in principle, interaction with knowledge of theworld, aspects of memory, and particular task-orientedbehaviour.Worth mentioning here is Church and Pati l 's \[1982\]work which at tempts  to put order in the chaos ofcomplexity and "computational load".Our parser lies between the second and the third of theabove categor ies .
The parser  is seen  as anondeterministic apparatus  that  d isambiguates andgives a "shallow" interpretation and an incrementalfunctional representation f each processed fragment ofthe sentence.
The state of the parser is supposed to becognitively meaningful at every moment ofthe process.Furthermore, in part icular ,  we are concerned withaspects of flexible word ordering.
This phenomenon isspecially relevant in Italian, where, for declarat ivesentences, Subject-Verb-Object is only the most likelyorder - the other five permutations of Subject Verb andObject may occur as well.
We shall briefly describe theparser and its environment and, by way of example,i l lustrate its behaviour  in analyz ing "osci l lat ing"sentences, i.e.
sentences in which one first perceives afragment in one way, then changes one's mind and takesit in a different way, then, as further input comes in,going back to the previous pat tern  (and posssib lycontinuing like this till the end of the sentence).3.
The  parserWEDNESDAY 2 \[Stock 1986\] is a parser based onl inguist ic  knowledge d is t r ibuted  fundamenta l lythrough the lexicon.
A word reading includes:- a semantic representation f the word, in the form of asemantic net shred;- static syntactic information, including the category,features, indication of l inguist ic functions that  arebound to part icular nodes in the net.
One particularspecification is the Main node, head of the syntact icconstituent the word occurs in;- dynamic syntactic information, including impulses toconnect pieces of semant ic  information, guided bysyntactic onstraints.
Impulses look for "f i l lers" on agiven search space (usually a substr ing).
They havea l ternat ives,  (for instance the word TELL has animpulse to merge its object node with the "main"  ofeither an NP or a subordinate clause).
An alternativeincludes: a contextual  condition of appl icabi l i ty ,  acategory, features, marking, side effects (through which,for example ,  coreference between subject  of asubordinate clause and a function of the main clause canbe indicated).
Impulses may also be directed to adifferent search space than the normal one (see below);- measures of likelihood.
These are measures that  areused for deriving an overall measure of likelihood of apar t ia l  ana lys i s .
Measures  are inc luded for thelikelihood of that  particular eading of the word and foraspects attached to an impulse: a) for one part icu laralternative b) for the relative position the filler c) for theoverall necessity of finding a filler.- a characterization f idioms involving that  word.
(For adescription of the part of the parser that  deals with theinterpretation f flexible idioms see \[Stock 1987\]).The only other data are in the form of s imple (nonaugmented)  t rans i t ion networks that  only providerestrictions on search spaces where impulses can lookfor fillers.
In more traditional words it deals with thedistribution of constituents.
A dist inguishing symbol,$EXP, indicates that  only the occurrence of somethingexpected by preceding words (i.e.
for which an impulsewas set up) will allow the transition.The parser is based on of the idea of chart  parsing \[Kay1980, Kaplan 1973\] \[see Stock 1986\].
What is relevanthere is the fact that  "edges" correspond to search spaces.Edges are complex data structures provided with a richamount  of in fo rmat ion  inc lud ing  a semant icinterpretation of the fragment, syntactic data, pendingimpulses, an overall measure of likelihood, etc.
Data onan edge are  "unified"dynamieally as indicated belowAn agenda is provided which includes four kinds oftasks: lexical tasks, traoersal tasks, insertion tasks,virtual tasks.
A lexieal task specifies a possible readingof a word to be inserted in the chart.
A traversal  taskspecifies an active edge and an inactive edge that  canextend it.
An insertion task specifies a nondeterministieunification act, and a virtual task involves extension ofan edge to include an inactive edge far away in thestr ing (used for long distance dependencies).235LA.4~,~ ~ot 2 vv ,~ 2YtN I t  g; z n,l ~L p,kp  , , ,  =I ~ ~tPaaR~ to:I d v im+ l~+ 4?
.+t l l l \ [  x :  4 ?I'~ ++ .
.
.
.
.l l ?
i$ , i .
kK  tO 6l I v  PI ' (PI /~RK t ,~Yl fM leg :  & m~oit 14 ~,bUH\[I~II i~  5vt~l tX :  ?
u ,I I~  P~(P  vo ?\[ I b  P I (P I~R~ fG  7Vth l t  X: ; I ,+  i meI :4  +IlIF rooY ( I I LK :  8$1,?~ J ~E|47ve  Im~ I l L~i .
, .
ml t .- .
.
'=m.~'"mI I?
+ P-PU I  c , .
Pe, P -&| -T~ C'O;~?IThe parser works asymmetrically with respect to the"arrival*' of the Main node: before the Main nodearrives, an extension of an edge has almost no effect.
Onthe arrival of the Main, all the present impulses are"unleashed" and must find satisfaction.
If all this doesnot happen then the new edge supposedly to be added tothe chart is not added: the situation is recognized as afailure.
After the arrival of the Main, each new headmust find an impulse to merge with, and each incomingimpulse must find satisfaction.
Again, if all this does nothappen, the new edge will not be added to the chart..4.
Overv iew of the env i ronmentWEDNESDAY 2 and its env i ronment  areimp lemented  on a Xerox  L isp Mach ine .
Theenvironment is composed of a series of specialized tools,each one based on one or more windows (fig 1).Using a mouse the user selects a desired behaviour frommenus attached to the windows.
We have the followingwindows:Fig.
I- the main WEDNESDAY 2 window, in which thesentence is entered.
Menus attached to this windowspecify different modalities (including "through" and"stepping", "all parsings" or "one parsing") and anumber of facilities;- a window where one can view, enter and modifytransition networks graphically (fig.
2).- a window where one can view, enter and modify thelexicon.
As a word entry is a complex object forWEDNESDAY 2, entering a new word can be greatlyfacilitated by a set of subwindows, each specialized inone aspect of the word, "knowing" how it may be andfacilitating editing.
The lexicon is a lexicon of roots: amorphological analyzer and a lexicon manager  areintegrated in the system.
Let us briefly describe thispoint.
A lexicalist theory such as ours requires that alarge quantity of information be included in the lexicon.This information has different origins: some comes fromthe root and some from the affixes.
All the informationmust be put into a coherent data structure, through a aparticularly constrained unification based process.236, .
.
.
~, II I II I\ '  xI m,Z,?,,TFig.
2?
?~1 ~aveVlE~3-PP-O i - -0~'YERI~-O!
l~-{l~l/ / \1-01 - IIIF-OI~I V1E\]~3- I IO -AOC~1 PJ~O~ZXS ~miii!ii~iCt  NIL NILll-OOd ,'(3 NIL\[31BJ X2 NILTest ~llP Beferel lke/~o?i lFe4t l lcel  M4rk  ~det fec taNI~ER(A-ObJ)( (T  PP/mARK I ~JL =(oea)( (T  NP .1 NIL NIL N\[(SUBJ)(NUST .8)( (T  NP .~ N|L NIL NIFurthermore we must emphasize the fact that, just as inLFG, phenomena such as passivization are treated inFig.3the lexicon (the Subject and Object functions and therelated impulses attached to the active form are237rearranged).
This is something that the morphologicalanalyzer must deal with.
The internal behaviour of themorphological analyzer  is beyond the scope of thepresent paper.
We shal l  instead briefly discuss thelexicon manager, the role of which will be emphasizedhere.The lexicon manager deals with the complex process ofenter ing data,  mainta in ing,  and .preprocessing thelexicon.
One notable aspect is that  we have arranged thelexicon on a hierachical baseis according to inheritance,so that  properties of a particular word can be inheritedfrom a word class and a word class can inherit aspectsfrom another class.
One consequence of this is that wecan introduce a graphic aspect (fig 3) and the user canbrowse through the lattice (the lexicon appears as a treeof classes where one has special ized editors at  eachlevel).
What is even more relevant is the fact that  onecan factorize knowledge that  is in the lexicon, so that  ffone par t i cu la r  phenomenon needs to be t reateddifferently, the change of information is immediate forthe words concerned.
Of course this means also thatthere is a space gain: the same information eeds not tobe duplicated: complete word data are reconstructedwhen required.There is also a modality by which one can enter thesyntactic aspects of a word through examples, a laTEAM \[Grosz 19841.
The results are less precise, butmay be useful in a more application-oriented use of theenvironment.- a window showing the present configuration of thechart;- a window that  permits zooming into one edge, showingseveral aspects of the edge, including: its s t ructura laspect, its l ikel ihood, the  funct iona l  aspect ,  thespecification of unrealized impulses etc.- a window displaying in graphic form the semant icinterpretation of an edge as a semantic net, o r ,  if oneprefers so (this is usually the case when the net is toocomplex) in logic format;- a window where one can manipulate the agenda (fig 4).Attached to this window we have a menu including a setof functionalities that  the tasks included in the agendato be manipulated: MOVE BEFORE, MOVE AFTER,DELETE, SWITCH,UNDO etc.
One just  points to thetwo particular tasks one wishes to operate on with themouse and then to the menu entry.
In this way thedesired effect is obtained.
The effect corresponds toapplying a different scheduling function: the tasks willbe picked up in the order here prescribed by hand.
Thistool, when the parser is in the "stepping" modality,LT vertex: 8 ?~Lt: PREPMARK - .
:  1LT veMex: 6 ~ PREP LH: 1I"T A :9  a:15 t~WL.N: .56 NEWTr,LT  vertex: 5 ,;al; N LIt: .
2LI" vertex: 6 ?a~ A0 J  Lq: .
6LI" vertex: 5 cJut: V LH: .
6L I  vertex: 4 caC PREPART eel: 1Llr vertex: 2 ~:ax: PREP LM: 1G4\ ]~mmK-4rl~sk~mmRFig.
4provides a very easy way of a l ter ing the de fau l tbehav iour  of the sys tem and of t ry ing  out  newstrategies.
This modality of schedul ing by hand iscomplemented by a series of counters that  providecontrol over the penetrance of these strategies.
(Thepenetrance of a nondeterministic algorithm is the ratiobetween the steps that  lead to the solution and the stepsthat  are carried out as a whole in trying to obtain thesolution.
Of course this measure is included between 0and 1.
)Dynamically, one tries to find sensible strategies,  byin teract ing  w i th  the  agenda.
When,  a f te rexperimenting formalizable heuristics have been triedout, they can be introduced permanently into the systemthrough a given specialized function.
This is the onlyplace where some knowledge of LISP and of the internalstructure ofWEDNESAY 2 is required.5.
An  example  o f  exp lo ra t ion :osc i l l a t ing  sentencesWe shall  now briefly discuss a processing example thatwe have been able to understand using the environmentment ioned above.
The following example is a goodinstance of flexibility and parsing problems present inItalian:a Napoli preferisco Romaa Milano.The complete sentence reads "while in Naples I preferRome to Milan".
The problem arises during the parsingprocess with the fact that  the "to" argument of "prefer "in Ital ian may occur before the verb, and the locativepreposition "in" is a, the same word as the mark ingpreposition corresponding to "to".238The reader/hearer fi st takes a Napoli as an adverbiallocation , then, as the verb preferisc9 is perceived, aNapoli is clearly reinterpreted as an argument of theverb, {with a sense of surprise).
As the sentence proceedsafter the object Rorna, the new word a_ causes things tochange again and we go back with a sense of surprise tothe first hypothesis.The following things should be noted: - when thissecond reconsideration takes place, we feel the surprise,but this does not cause us to reconsider the sentence, weonly go back adding more to an hypothesis that we werealready working at; -the surprise seems to be caused notby a heavy computat ional  load, but by a suddenreadjustment of the weights of the hypotheses.
In a senseit is a matter of memory, rather than computation.We have been in a position to get WEDNESDAY 2 toperform natural ly in such situations, taking advantageof the environment.
The following simple heurist icswere found: a) try solutions that  satisfy the impulses (ifthere are alternatives consider likelihoods); b) maintainviscosity (prefer the path you are already following); andc) follow the alternative that  yields the edge with thegreatest likelihood, among edges of comparable l ngths.The likelihood of an edge depends on: 1) the likelihood ofthe "included" edges; 2) the level ofobligatoriness of thefilled impulses; 3) the likelihood of a particular elativeposition of an argument in the string; 4) the likelihood ofthat  t rans i t ion in the network, given the previoustransition.The critical points in the sentence are the following(note that  we distinguish between a PP and a "markedNP" possible argument of a verb, where the prepositionhas no semantics asociated:i) At the beginning: only the PP edge is expanded, (notthe one including a ~marked NP ' ,  because of stat icpreference for the former expressed in the lexicon and inthe transition etwork.ii) After the verb is detected: on the one hand there is anedge that, if extended, would not satisfy an obligatoryimpulse, on the other hand, one that  would possiblysatisfy one .
The ~marked NP" alternative is chosenbecause of a) of the above heuristics.iii) After the object Roma: when the preposition a_ comesin, the edge that  may extend the sentence with a PP onthe one hand, and on the other hand a cycling activeedge that is a promising satisfaction for an impulse arecompared.
Since this relative position of the argument isso favourable for the particular verb preferisco (.9 to .1for this position compared to the antecedent one), theparser proceeds with the a l ternat ive view, tak ing aNap.o!i.
as a modh'\]er So it goes on, after reentering thatworking hypothesis.
The object is a l ready  there ,analyzed for the other reading and does not need to bereanalyzed.
So a Milano is taken as the filler for theimpulse and the analysis is concluded properly.It should be noted that the Final Argument Principle\[Fodor, Kaplan and Bresnan 1982\] does not work withthe flexibility characteristic of Italian.
(The principlewould cause the reading "I prefer \[Rome \[ in Milan\]\] toNaples" to be chosen at point iii) above).Conc lus ionsWe have introduced an env i ronment  bu i l t  a roundWEDNESDAY 2, a nondeterministic parser, or ientedtowards experimenting with dynamic strategies.
Thecombination of interest ing theories and such toolsrealizes both meanings of the word "experimental": 1)something that  implements new ideas in a prototype; 2)something built for the sake of making experiments.
Weth ink  that  this approach, possibly in tegrated wi thexperiments in psycholinguistics, can help increase ourunderstanding of parsing.AcknowledgementsFederico Cecconi's help in the graphic aspects andlexicon management has been precious.ReferencesChurch, K. & Patil, R. Coping with syntactic ambiguityor how to put the block in the box on the table.
AmericanJournal of Computational Linguistics, 8; 139o149 (1982)Ferrari,G.
& Stock,O.
Strategy selection for an ATNsyntactic parser.
Proceedings of the 18th Meeting of theAssociation for Computational Linguistics, Philadelphia(1980)Ford, M., Bresnan, J.
& Kaplan, R. A competence basedtheory of syntact ic closure.
In Bresnan, J .
,  Ed.
TheMental Representation f Grammatical Relations.
TheMIT Press, Cambridge, (1982)Gazdar, G. Phrase structure grammar.
In Jacobson andPul lman (Eds.
), The Nature of Syntactic Representation.Dordrecht: Reidel ( 1981 )239Grosz, B.
TEAM, a transportable natural anguageinterface system.
In Proceedings of the Conference onApplied Natural Language Processing, Santa Monica~1983~Joshi, A., & Levy, L. Phrase structure trees bear morefruits then you would have thought.
American Journalof Computational Linguistics,8; 1-ll (1982)Kaplan, R. A general syntactic processor.
In Rustin, R.{Ed.
), Natural Language Processing.
Englewood Cliffs,N.J.
: Prentice-Hall (1973)Kaplan,R.
& Bresnan,J.
Lexical-Functional Grammar: aformal system for grammatical representation.
InBresnan,J .
,  Ed.
The Mental Representation ofGrammatical Relations.
The MIT Press, Cambridge,173-281 (1982)Kay, M. Algorithm Schemata nd Data Structures inSyntactic Processing.
Xerox, Palo Alto Research Center(October 1980)Kay, M. Functional Grammar.
In Proceedings ofthe 5thMeeting of the Berkeley Linguistic Society, Berkeley,142-158(1979}Marcus, M. An overview of a theory of syntacticrecognition for natural anguage.
(AI memo 531).Cambridge, Mass: Artificial Intelligence Laboratory,(1979)Pereira, F. & Warren, D., Definite Clause Grammars forlanguage analysis.
A survey of the formalism and acomparison with Augmented Transition Networks.Artificial Intelligence, 13; 231-278 (1980)Small, S. Word expert parsing: a theory of distributedword-based natural language understanding.
(TechnicalReport TR-954 NSG-7253).
Maryland: University ofMaryland (1980)Stock, O.
Dynamic Unification in Lexieally BasedParsing.
In Proceedings of the Seuenth EuropeanConference on Artificial Intelligence.
Brighton, 212-221(1986)Stock, O.
Putting Idioms into a Lexicon Based Parser'sHead.
To appear in Proceedings of the 25th Meeting ofthe Association for Computational Linguistics.
Stanford,Ca.
\[1987\]Thompson, H.S.
Chart parsing and rule schemata inGPSG.
In Proceedings ofthe 19th Annual Meeting of theAssociation for Computational Linguistics.
Alexandria,Va.
(1981)240
