Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 250?258,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsOn the Robustness of Syntactic and SemanticFeatures for Automatic MT EvaluationJesu?s Gime?nez and Llu?
?s Ma`rquezTALP Research Center, LSI DepartmentUniversitat Polite`cnica de CatalunyaJordi Girona Salgado 1?3, E-08034, Barcelona{jgimenez,lluism}@lsi.upc.eduAbstractLinguistic metrics based on syntactic andsemantic information have proven veryeffective for Automatic MT Evaluation.However, no results have been presentedso far on their performance when appliedto heavily ill-formed low quality transla-tions.
In order to glean some light into thisissue, in this work we present an empiricalstudy on the behavior of a heterogeneousset of metrics based on linguistic analysisin the paradigmatic case of speech transla-tion between non-related languages.
Cor-roborating previous findings, we have ver-ified that metrics based on deep linguis-tic analysis exhibit a very robust and sta-ble behavior at the system level.
How-ever, these metrics suffer a significant de-crease at the sentence level.
This is inmany cases attributable to a loss of recall,due to parsing errors or to a lack of parsingat all, which may be partially amelioratedby backing off to lexical similarity.1 IntroductionRecently, there is a growing interest in the devel-opment of automatic evaluation metrics which ex-ploit linguistic knowledge at the syntactic and se-mantic levels.
For instance, we may find metricswhich compute similarities over shallow syntac-tic structures/sequences (Gime?nez and Ma`rquez,2007; Popovic and Ney, 2007), constituencytrees (Liu and Gildea, 2005) and dependencytrees (Liu and Gildea, 2005; Amigo?
et al, 2006;Mehay and Brew, 2007; Owczarzak et al, 2007).We may also find metrics operating over shallowsemantic structures, such as named entities and se-mantic roles (Gime?nez and Ma`rquez, 2007).Linguistic metrics have been proven to producemore reliable system rankings than metrics limit-ing their scope to the lexical dimension, in partic-ular when applied to test beds with a rich systemtypology, i.e., test beds in which there are auto-matic outputs produced by systems based on dif-ferent paradigms, e.g., statistical, rule-based andhuman-aided (Gime?nez and Ma`rquez, 2007).
Thereason is that they are able to capture deep MTquality distinctions which occur beyond the shal-low level of lexical similarities.However, these metrics have the limitation ofrelying on automatic linguistic processors, toolswhich are not equally available for all languagesand whose performance may vary depending onthe type of analysis conducted and the applica-tion domain.
Thus, it could be argued that lin-guistic metrics should suffer a significant qualitydrop when applied to a different translation do-main, or to ill-formed sentences.
Clearly, met-ric scores computed on partial or wrong syntac-tic/semantic structures will be less informed.
But,should this necessarily lead to less reliable eval-uations?
In this work, we have analyzed this is-sue by conducting a contrastive empirical study onthe behavior of a heterogeneous set of metrics overseveral evaluation scenarios of decreasing transla-tion quality.
In particular, we have studied the caseof Chinese-to-English speech translation, which isa paradigmatic example of low quality and heavilyill-formed output.The rest of the paper is organized as follows.
InSection 2, prior to presenting experimental work,we describe the set of metrics employed in ourexperiments.
We also introduce a novel familyof metrics which operate at the properly semanticlevel by analyzing similarities over discourse rep-resentations.
Experimental work is then presentedin Section 3.
Metrics are evaluated both in terms ofhuman likeness and human acceptability (Amigo?et al, 2006).
Finally, in Section 4, main conclu-sions are summarized and future work is outlined.2502 A Heterogeneous Metric SetWe have used a heterogeneous set of metrics se-lected out from the metric repository providedwith the IQMT evaluation package (Gime?nez andMa`rquez, 2007)1.
We have considered severalmetric representatives from different linguisticlevels (lexical, syntactic and semantic).
A briefdescription of the metric set is available in Ap-pendix A.In addition, taking advantage of newly availablesemantic processors, we have designed a novelfamily of metrics based on the Discourse Repre-sentation Theory, a theoretical framework offer-ing a representation language for the examinationof contextually dependent meaning in discourse(Kamp, 1981).
A discourse is represented in adiscourse representation structure (DRS), which isessentially a variation of first-order predicate cal-culus ?its forms are pairs of first-order formulaeand the free variables that occur in them.2.1 Exploiting Semantic Similarity forAutomatic MT Evaluation?DR?
metrics analyze similarities between auto-matic and reference translations by comparingtheir respective DRSs.
These are automaticallyobtained using the C&C Tools (Clark and Cur-ran, 2004)2.
Sentences are first parsed on the basisof a combinatory categorial grammar (Bos et al,2004).
Then, the BOXER component (Bos, 2005)extracts DRSs.
As an illustration, Figure 1 showsthe DRS representation for the sentence ?Everyman loves Mary.?.
The reader may find the out-put of the BOXER component (top) together withthe equivalent first-order formula (bottom).DRS may be viewed as semantic trees, whichare built through the application of two types ofDRS conditions:basic conditions: one-place properties (pred-icates), two-place properties (relations),named entities, time-expressions, cardinalexpressions and equalities.complex conditions: disjunction, implication,negation, question, and propositional attitudeoperations.Three kinds of metrics have been defined:1http://www.lsi.upc.edu/?nlp/IQMT2http://svn.ask.it.usyd.edu.au/trac/candcDR-STM-l (Semantic Tree Matching) Thesemetrics are similar to the Syntactic TreeMatching metric defined by Liu and Gildea(2005), in this case applied to DRSs insteadof constituency trees.
All semantic subpathsin the candidate and the reference trees areretrieved.
The fraction of matching subpathsof a given length, l ?
[1..9], is computed.Then, average accumulated scores up to agiven length are retrieved.
For instance, ?DR-STM-4?
corresponds to the average accumu-lated proportion of matching subpaths up tolength-4.DR-Or-t These metrics compute lexical overlap-ping3 between discourse representation struc-tures (i.e., discourse referents and discourseconditions) according to their type ?t?.
Forinstance, ?DR-Or -pred?
roughly reflects lexi-cal overlapping between the referents associ-ated to predicates (i.e., one-place properties),whereas ?DR-Or -imp?
reflects lexical overlap-ping between referents associated to implica-tion conditions.
We also introduce the ?DR-Or -??
metric, which computes average lexicaloverlapping over all DRS types.DR-Orp-t These metrics compute morphosyn-tactic overlapping (i.e., between parts ofspeech associated to lexical items) betweendiscourse representation structures of thesame type t. We also define the ?DR-Orp-?
?metric, which computes average morphosyn-tactic overlapping over all DRS types.Note that in the case of some complex condi-tions, such as implication or question, the respec-tive order of the associated referents in the treeis important.
We take this aspect into accountby making order information explicit in the con-struction of the semantic tree.
We also make ex-plicit the type, symbol, value and date of condi-tions when these are applicable (e.g., predicates,relations, named entities, time expressions, cardi-nal expressions, or anaphoric conditions).Finally, the extension to the evaluation settingbased on multiple references is computed by as-signing the maximum score attained against eachindividual reference.3Overlapping is measured following the formulae anddefinitions by Gime?nez and Ma`rquez (2007).
A short defi-nition may be found in Appendix A.251Formally:?y named(y,mary, per) ?
(?x man(x) ?
?z love(z) ?
event(z) ?
agent(z, x) ?
patient(z, y))Figure 1: DRS representation for ?Every man loves Mary.
?3 Experimental WorkIn this section, we present an empirical study onthe behavior of a heterogeneous set of metricsbased on linguistic analysis in the case of speechtranslation between non-related languages.3.1 Evaluation ScenariosWe have used the test bed from the Chinese-to-English translation task at the ?2006 Evalua-tion Campaign on Spoken Language Translation?
(Paul, 2006)4.
The test set comprises 500 transla-tion test cases corresponding to simple conversa-tions (question/answer scenario) in the travel do-main.
In addition, there are 3 different evalua-tion subscenarios of increasing translation diffi-culty, according to the translation source:CRR: Translation of correct recognition results(as produced by human transcribers).ASR read: Translation of automatic read speechrecognition results.ASR spont: Translation of automatic sponta-neous speech recognition results.For the purpose of automatic evaluation, 7 hu-man reference translations and automatic outputsby 14 different MT systems for each evaluationsubscenario are available.
In addition, we counton the results of a process of manual evaluation.4http://www.slc.atr.jp/IWSLT2006/For each subscenario, 400 test cases from 6 differ-ent system outputs were evaluated, by three humanassessors each, in terms of adequacy and fluencyon a 1-5 scale (LDC, 2005).
A brief numerical de-scription of these test beds is available in Table 1.It includes the number of human references andsystem outputs available, as well as the numberof sentences per output, and the number of systemoutputs and sentences per system assessed.
For thesake of completeness, we report the performanceof the Automatic Speech Recognition (ASR) sys-tem, in terms of accuracy, over the source Chineseutterances, both at the word and sentence levels.Also, in order to give an idea of the translationquality exhibited by automatic systems, averageadequacy and fluency scores are also provided.3.2 Meta-EvaluationOur experiment requires a mechanism for evaluat-ing the quality of evaluation metrics, i.e., a meta-evaluation criterion.
The two most prominent are:?
Human Acceptability: Metrics are evaluatedin terms of their ability to capture the de-gree of acceptability to humans of automatictranslations, i.e., their ability to emulate hu-man assessors.
The underlying assumptionis that good translations should be acceptableto human evaluators.
Human acceptability isusually measured on the basis of correlationbetween automatic metric scores and humanassessments of translation quality.252CRR ASR read ASR spont#human-references 7 7 7#system-outputs 14 14 13#sentences 500 500 500#outputsassessed 6 6 6#sentencesassessed 400 400 400Word Recognition Accuracy ?
0.74 0.68Sentence Recognition Accuracy ?
0.23 0.17Average Adequacy 1.40 1.02 0.93Average Fluency 1.16 0.98 0.98Table 1: IWSLT 2006 MT Evaluation Campaign.
Chinese-to-English test bed description?
Human Likeness: Metrics are evaluated interms of their ability to capture the fea-tures which distinguish human from auto-matic translations.
The underlying assump-tion is that good translations should resemblehuman translations.
Human likeness is usu-ally measured on the basis of discriminativepower (Lin and Och, 2004b; Amigo?
et al,2005).In this work, metrics are evaluated both in termsof human acceptability and human likeness.
In thecase of human acceptability, metric quality is mea-sured on the basis of correlation with human as-sessments both at the sentence and document (i.e.,system) levels.
We compute Pearson correlationcoefficients.
The sum of adequacy and fluency isused as a global measure of quality.
Assessmentsfrom different judges have been averaged.In the case of human likeness, we use the proba-bilistic KING measure defined inside the QARLAFramework (Amigo?
et al, 2005).
KING repre-sents the probability, estimated over the set of testcases, that the score attained by a human referenceis equal or greater than the score attained by anyautomatic translation.
Although KING computa-tions do not require human assessments, for thesake of comparison, we have limited to the set oftest cases counting on human assessments.3.3 ResultsTable 2 presents meta-evaluation results for a setof metric representatives from different linguisticlevels over the three subscenarios defined (?CRR?,?ASR read?
and ?ASR spont?).
Highest scoresin each column have been highlighted.
Lowestscores appear in italics.System-level BehaviorAt the system level (Rsys, columns 7-9), the high-est quality is in general attained by metrics basedon deep linguistic analysis, either syntactic or se-mantic.
Among lexical metrics, the highest cor-relation is attained by BLEU and the variant ofGTM rewarding longer matchings (e = 2).As to the impact of sentence ill-formedness,while most metrics at the lexical level suffer a sig-nificant variation across the three subscenarios, theperformance of metrics at deeper linguistic levelsis in general quite stable.
However, in the case ofthe translation of automatically recognized spon-taneous speech (ASR spont) we have found thatthe ?SR-Or-??
and ?SR-Mr-??
metrics, respectivelybased on lexical overlapping and matching oversemantic roles, suffer a very significant decreasefar below the performance of most lexical metrics.Although ?SR-Or-??
has performed well on othertest beds (Gime?nez and Ma`rquez, 2007), its lowperformance over the BTEC data suggests that itis not fully portable across all kind of evaluationscenarios.Finally, it is highly remarkable the degree of ro-bustness exhibited by semantic metrics introducedin Section 2.1.
In particular, the metric variantsbased on lexical and morphosyntactic overlappingover discourse representations (?DR-Or-??
and ?DR-Orp-?
?, respectively), obtain a high system-levelcorrelation with human assessments across thethree subscenarios.Sentence-level BehaviorAt the sentence level (KING and Rsnt, columns1-6), highest quality is attained in most cases bymetrics based on lexical matching.
This result wasexpected since all MT systems are statistical andthe test set is in-domain, that is it belongs to the253Human Likeness Human AcceptabilityKING Rsnt RsysASR ASR ASR ASR ASR ASRLevel Metric CRR read spont CRR read spont CRR read spont1-WER 0.63 0.69 0.71 0.47 0.50 0.48 0.50 0.32 0.521-PER 0.71 0.79 0.79 0.44 0.48 0.45 0.67 0.39 0.601-TER 0.69 0.75 0.77 0.49 0.52 0.50 0.66 0.36 0.62BLEU 0.69 0.72 0.73 0.54 0.53 0.52 0.79 0.74 0.62Lexical NIST 0.79 0.84 0.85 0.53 0.54 0.53 0.12 0.26 -0.02GTM (e = 1) 0.75 0.81 0.83 0.50 0.52 0.52 0.35 0.10 -0.09GTM (e = 2) 0.72 0.78 0.79 0.62 0.64 0.61 0.78 0.65 0.62METEORwnsyn 0.81 0.86 0.86 0.44 0.50 0.48 0.55 0.39 0.08ROUGEW 1.2 0.74 0.79 0.81 0.58 0.60 0.58 0.53 0.69 0.43Ol 0.74 0.81 0.82 0.57 0.62 0.58 0.77 0.51 0.34SP-Op-?
0.75 0.80 0.82 0.54 0.59 0.56 0.77 0.54 0.48SP-Oc-?
0.74 0.81 0.82 0.54 0.59 0.55 0.82 0.52 0.49Shallow SP-NISTl 0.79 0.84 0.85 0.52 0.53 0.52 0.10 0.25 -0.03Syntactic SP-NISTp 0.74 0.78 0.80 0.44 0.42 0.43 -0.02 0.24 0.04SP-NISTiob 0.65 0.69 0.70 0.33 0.32 0.35 -0.09 0.17 -0.09SP-NISTc 0.55 0.59 0.59 0.24 0.22 0.25 -0.07 0.19 0.08CP-Op-?
0.75 0.81 0.82 0.57 0.63 0.59 0.84 0.67 0.52CP-Oc-?
0.74 0.80 0.82 0.60 0.64 0.61 0.71 0.53 0.43DP-Ol-?
0.68 0.75 0.76 0.48 0.50 0.50 0.84 0.77 0.67DP-Oc-?
0.71 0.76 0.77 0.41 0.46 0.43 0.76 0.65 0.71Syntactic DP-Or -?
0.75 0.80 0.81 0.51 0.53 0.51 0.81 0.75 0.62DP-HWCw 0.54 0.57 0.57 0.29 0.32 0.28 0.73 0.74 0.37DP-HWCc 0.48 0.51 0.52 0.17 0.18 0.22 0.73 0.64 0.67DP-HWCr 0.44 0.49 0.48 0.20 0.21 0.25 0.71 0.58 0.56CP-STM 0.71 0.77 0.80 0.53 0.56 0.54 0.65 0.58 0.47SR-Mr -?
0.40 0.43 0.45 0.29 0.28 0.29 0.52 0.60 0.20SR-Or -?
0.45 0.49 0.51 0.35 0.35 0.36 0.56 0.58 0.14Shallow SR-Or 0.31 0.33 0.35 0.16 0.15 0.18 0.68 0.73 0.53Semantic SR-Mrv -?
0.38 0.41 0.42 0.33 0.34 0.34 0.79 0.81 0.42SR-Orv -?
0.40 0.44 0.45 0.36 0.38 0.38 0.64 0.72 0.72SR-Orv 0.36 0.40 0.40 0.27 0.31 0.29 0.34 0.78 0.38DR-Or -?
0.67 0.73 0.75 0.48 0.53 0.50 0.86 0.74 0.77Semantic DR-Orp-?
0.59 0.64 0.65 0.34 0.35 0.33 0.84 0.78 0.95DR-STM 0.58 0.63 0.65 0.23 0.26 0.26 0.75 0.62 0.67Table 2: Meta-evaluation results for a set of metric representatives from different linguistic levelssame domain in which systems have been trained.Therefore, translation outputs have a strong ten-dency to share the sublanguage (i.e., word selec-tion and word ordering) represented by the prede-fined set of human reference translations.Metrics based on lexical overlapping andmatching over shallow syntactic categories andsyntactic structures (?SP-Op-?
?, ?SP-Oc-?
?, ?CP-Op-??,?CP-Oc-?
?, ?DP-Ol-?
?, ?DP-Oc-?
?, and ?DP-Or-??)
per-form similarly to lexical metrics.
However, com-puting NIST scores over base phrase chunk se-quences (?SP-NISTiob?, ?SP-NISTc?)
is not as effec-tive.
Metrics based on head-word chain match-ing (?DP-HWCw?, ?DP-HWCc?, ?DP-HWCr?)
suffer alsoa significant decrease.
Interestingly, the metricbased on syntactic tree matching (?CP-STM?)
per-formed well in all scenarios.Metrics at the shallow semantic level suffer alsoa severe drop in performance.
Particularly signif-icant is the case case of the ?SR-Or?
metric, whichdoes not consider any lexical information.
Inter-estingly, the ?SR-Orv?
variant, which only differsin that it distinguishes between SRs associated todifferent verbs, performs slightly better.At the semantic level, metrics based on lex-ical and morphosyntactic overlapping over dis-course representations (?DR-Or-??
and ?DR-Orp-??
)suffer only a minor decrease, whereas semantictree matching (?DR-STM?)
reports as a specially badpredictor of human acceptability (Rsnt).However, the most remarkable result, in rela-tion to the goal of this work, is that the behaviorof syntactic and semantic metrics across the threeevaluation subscenarios is, in general, quite stable?the three values in each subrow are in a verysimilar range.
Therefore, answering the questionposed in the introduction, sentence ill-formednessis not a limiting factor in the performance of lin-guistic metrics.254Human Likeness Human AcceptabilityKING Rsnt RsysASR ASR ASR ASR ASR ASRLevel Metric CRR read spont CRR read spont CRR read spontLexical NIST 0.79 0.84 0.85 0.53 0.54 0.53 0.12 0.26 -0.02GTM (e = 2) 0.72 0.78 0.79 0.62 0.64 0.61 0.78 0.65 0.62METEORwnsyn 0.81 0.86 0.86 0.44 0.50 0.48 0.55 0.39 0.08Ol 0.74 0.81 0.82 0.57 0.62 0.58 0.77 0.51 0.34CP-Op-?
0.75 0.81 0.82 0.57 0.63 0.59 0.84 0.67 0.52Syntactic CP-Oc-?
0.74 0.80 0.82 0.60 0.64 0.61 0.71 0.53 0.43DP-Ol-?
0.68 0.75 0.76 0.48 0.50 0.50 0.84 0.77 0.67SR-Mr -?
0.40 0.43 0.45 0.29 0.28 0.29 0.52 0.60 0.20SR-Mr -?b 0.68 0.72 0.73 0.31 0.30 0.31 0.52 0.60 0.20SR-Mr -?i 0.84 0.86 0.88 0.34 0.34 0.34 0.56 0.63 0.25SR-Or -?
0.45 0.49 0.51 0.35 0.35 0.36 0.56 0.58 0.14SR-Or -?b 0.71 0.75 0.78 0.38 0.38 0.38 0.56 0.58 0.14SR-Or -?i 0.84 0.88 0.89 0.41 0.41 0.41 0.62 0.60 0.22SR-Or 0.31 0.33 0.35 0.16 0.15 0.18 0.68 0.73 0.53SR-Or b 0.54 0.58 0.60 0.19 0.18 0.20 0.68 0.73 0.53Shallow SR-Or i 0.72 0.77 0.79 0.26 0.26 0.27 0.80 0.73 0.67Semantic SR-Mrv -?
0.38 0.41 0.42 0.33 0.34 0.34 0.79 0.81 0.42SR-Mrv -?b 0.70 0.73 0.74 0.34 0.35 0.34 0.79 0.81 0.42SR-Mrv -?i 0.88 0.90 0.92 0.36 0.38 0.37 0.81 0.82 0.45SR-Orv -?
0.40 0.44 0.45 0.36 0.38 0.38 0.64 0.72 0.72SR-Orv -?b 0.72 0.76 0.77 0.38 0.40 0.39 0.64 0.72 0.72SR-Orv -?i 0.88 0.90 0.91 0.40 0.42 0.41 0.69 0.74 0.74SR-Orv 0.36 0.40 0.40 0.27 0.31 0.29 0.34 0.78 0.38SR-Orv b 0.66 0.70 0.71 0.29 0.32 0.30 0.34 0.78 0.38SR-Orv i 0.83 0.86 0.88 0.33 0.36 0.33 0.49 0.82 0.56DR-Or -?
0.67 0.73 0.75 0.48 0.53 0.50 0.86 0.74 0.77DR-Or -?b 0.69 0.75 0.77 0.50 0.53 0.50 0.90 0.69 0.56Semantic DR-Or -?i 0.83 0.87 0.89 0.53 0.57 0.53 0.88 0.70 0.61DR-Orp-?
0.59 0.64 0.65 0.34 0.35 0.33 0.84 0.78 0.95DR-Orp-?b 0.61 0.65 0.67 0.35 0.36 0.34 0.86 0.71 0.57DR-Orp-?i 0.80 0.84 0.85 0.43 0.46 0.43 0.90 0.75 0.70DR-STM 0.58 0.63 0.65 0.23 0.26 0.26 0.75 0.62 0.67DR-STM-b 0.64 0.68 0.71 0.23 0.26 0.27 0.75 0.62 0.67DR-STM-i 0.83 0.87 0.87 0.33 0.36 0.36 0.84 0.63 0.66Table 3: Meta-evaluation results.
Improved sentence-level evaluation of SR and DR metricsImproved Sentence-level BehaviorBy inspecting particular instances, we have foundthat linguistic metrics are, in many cases, unable toproduce any evaluation result.
The number of un-scored sentences is particularly significant in thecase of SR metrics.
For instance, the ?SR-Or-?
?metric is unable to confer an evaluation score in57% of the cases.
Several reasons explain this fact.The first and most important is that linguistic met-rics rely on automatic processors trained on out-of-domain data, which are, thus, prone to error.Second, we argue that the test bed itself does notallow for fully exploiting the capabilities of thesemetrics.
Apart from being based on a reduced vo-cabulary (2,346 distinct words), test cases consistmostly of very short segments (14.64 words on av-erage), which in their turn consist of even shortersentences (8.55 words on average)5.5Vocabulary size and segment/sentence average lengthshave been computed over the set of reference translations.A possible solution could be to back off toa measure of lexical similarity in those cases inwhich linguistic processors are unable to produceany linguistic analysis.
This should significantlyincrease their recall.
With that purpose, we havedesigned two new variants for each of these met-rics.
Given a linguistic metric x, we define:?
xb ?
by backing off to lexical overlapping,Ol, only when the linguistic processor wasnot able to produce a parsing.
Lexical scoresare conveniently scaled so that they are in asimilar range to x scores.
Specifically, wemultiply them by the average x score attainedover all other test cases for which the parsersucceeded.
Formally, given a test case t be-longing to a set of test cases T :xb(t) ={x(t) if t ?
ok(T )Ol(t)Pj?ok(T ) x(j)|ok(T )| otherwise255where ok(T ) is the subset of test cases in Twhich were successfully parsed.?
xi ?
by linearly interpolating x and Olscores for all test cases, via arithmetic mean:xi(t) =x(t) + Ol(t)2In both cases, system-level scores are calculatedby averaging over all sentence-level scores.Table 3 shows meta-evaluation results on theperformance of these variants for several repre-sentatives from the SR and DR families.
For thesake of comparison, we also show the scores at-tained by the base versions, and by some of thetop-scoring metrics from other linguistic levels.The first observation is that in all cases the newvariants outperform their respective base metric,being linear interpolation the best alternative.
Theincrease is particularly significant in terms of hu-man likeness.
New variants even outperform lex-ical metrics, including the Ol metric, which sug-gests that, in spite of its simplicity, this is a validcombination scheme.
However, in terms of humanacceptability, the gain is only moderate, and stilltheir performance is far from top-scoring metrics.Sentence-level improvements are also reflectedat the system level, although to a lesser extent.Interestingly, in the case of the translation of au-tomatically recognized spontaneous speech (ASRspont, column 9), mixing with lexical overlap-ping improves the low-performance ?SR-Or?
and?SR-Orv?
metrics, at the same time that it causesa significant drop in the high-performance ?DR-Or?and ?DR-Orp?
metrics.
Still, the performance of lin-guistic metrics at the sentence level is under theperformance of lexical metrics.
This is not sur-prising.
After all, apart from relying on automaticprocessors, linguistic metrics focus on very par-tial aspects of quality.
However, since they operateat complementary quality dimensions, their scoresare suitable for being combined.4 Conclusions and Future WorkWe have presented an empirical study on the ro-bustness of a heterogeneous set of metrics operat-ing at different linguistic levels for the particularcase of Chinese-to-English speech translation ofbasic travel expressions.
As an additional contri-bution, we have presented a novel family of met-rics which operate at the semantic level by analyz-ing discourse representations.Corroborating previous findings by Gime?nezand Ma`rquez (2007), results at the system level,show that metrics guided by deeper linguisticknowledge, either syntactic or semantic, are, ingeneral, more effective and stable than metricswhich limit their scope to the lexical dimension.However, at the sentence level, results indicatethat metrics based on deep linguistic analysis arenot as reliable overall quality estimators as lexicalmetrics, at least when applied to low quality trans-lations, as it is the case.
This behavior is mainly at-tributable a drop in recall due to parsing errors.
Byinspecting particular sentences we have observedthat in many cases these metrics are unable to pro-duce any result.
In that respect, we have showedthat backing off to lexical similarity is a valid andeffective strategy so as to improve the performanceof these metrics.But the most remarkable result, in relation to thegoal of this work, is that syntactic and semanticmetrics exhibit a very robust behavior across thethree evaluation subscenarios of decreasing trans-lation quality analyzed.
Therefore, sentence ill-formedness is not a limiting factor in the perfor-mance of linguistic metrics.
The quality drop,when moving from the system to the sentencelevel, seems, thus, more related to a shift in theapplication domain.For future work, we are currently studying thepossibility of further improving the sentence-levelbehavior of present evaluation methods by com-bining the outcomes of metrics at different linguis-tic levels into a single measure of quality (citationomitted for the sake of anonymity).AcknowledgementsThis research has been funded by the Span-ish Ministry of Education and Science, projectOpenMT (TIN2006-15307-C03-02).
Our NLPgroup has been recognized as a Quality ResearchGroup (2005 SGR-00130) by DURSI, the Re-search Department of the Catalan Government.We are grateful to the SLT Evaluation Campaignorganizers and participants for providing suchvaluable test beds.ReferencesEnrique Amigo?, Julio Gonzalo, Anselmo Pe nas, andFelisa Verdejo.
2005.
QARLA: a Framework forthe Evaluation of Automatic Sumarization.
In Pro-256ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, andLlu?
?s Ma`rquez.
2006.
MT Evaluation: Human-Like vs. Human Acceptable.
In Proceedings ofthe Joint 21st International Conference on Com-putational Linguistics and the 44th Annual Meet-ing of the Association for Computational Linguistics(COLING-ACL).Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for MT and/or Summariza-tion.Johan Bos, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-Coverage Semantic Representations from a CCGParser.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COL-ING), pages 1240?1246.Johan Bos.
2005.
Towards Wide-Coverage Seman-tic Interpretation.
In Proceedings of the Sixth In-ternational Workshop on Computational Semantics,pages 42?53.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and Log-Linear Models.
In Pro-ceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages104?111.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2nd In-ternation Conference on Human Language Technol-ogy, pages 138?145.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2007.
Linguis-tic Features for Automatic Evaluation of Heteroge-neous MT Systems.
In Proceedings of the ACLWorkshop on Statistical Machine Translation, pages256?264.Hans Kamp.
1981.
A Theory of Truth and Seman-tic Representation.
In J.A.G.
Groenendijk, T.M.V.Janssen, , and M.B.J.
Stokhof, editors, Formal Meth-ods in the Study of Language, pages 277?322, Ams-terdam.
Mathematisch Centrum.LDC.
2005.
Linguistic Data Annotation Specification:Assessment of Adequacy and Fluency in Trans-lations.
Revision 1.5.
Technical report, Linguis-tic Data Consortium.
http://www.ldc.upenn.edu/-Projects/TIDES/Translation/TransAssess04.pdf.Chin-Yew Lin and Franz Josef Och.
2004a.
Au-tomatic Evaluation of Machine Translation Qual-ity Using Longest Common Subsequence and Skip-Bigram Statics.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics (ACL).Chin-Yew Lin and Franz Josef Och.
2004b.
OR-ANGE: a Method for Evaluating Automatic Evalu-ation Metrics for Machine Translation.
In Proceed-ings of the 20th International Conference on Com-putational Linguistics (COLING).Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization.Dennis Mehay and Chris Brew.
2007.
BLEUATRE:Flattening Syntactic Dependencies for MT Evalu-ation.
In Proceedings of the 11th Conference onTheoretical and Methodological Issues in MachineTranslation (TMI).I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Transla-tion.
In Proceedings of the Joint Conference on Hu-man Language Technology and the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL).Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
An Evaluation Tool for Ma-chine Translation: Fast Evaluation for MT Research.In Proceedings of the 2nd International Conferenceon Language Resources and Evaluation.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007.
Dependency-Based Automatic Eval-uation for Machine Translation.
In Proceedings ofSSST, NAACL-HLT/AMTA Workshop on Syntax andStructure in Statistical Translation, pages 80?87.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation, RC22176.
Technicalreport, IBM T.J. Watson Research Center.Michael Paul.
2006.
Overview of the IWSLT 2006Evaluation Campaign.
In Proceedings of the In-ternational Workshop on Spoken Language Trans-lation, pages 1?15.Maja Popovic and Hermann Ney.
2007.
Word ErrorRates: Decomposition over POS classes and Appli-cations for Error Analysis.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 48?55, Prague, Czech Republic, June.Association for Computational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of AMTA, pages 223?231.C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, andH.
Sawaf.
1997.
Accelerated DP based Search forStatistical Translation.
In Proceedings of EuropeanConference on Speech Communication and Technol-ogy.257A Metric SetMetrics are grouped according to the linguistic di-mension at which they operate:?
Lexical SimilarityWER (Nie?en et al, 2000).PER (Tillmann et al, 1997).BLEU (Papineni et al, 2001).NIST (Doddington, 2002).GTM (Melamed et al, 2003).ROUGE (Lin and Och, 2004a).METEOR.
(Banerjee and Lavie, 2005).TER (Snover et al, 2006).Ol (Gime?nez and Ma`rquez, 2007).
Ol is ashort name for lexical overlapping.
Au-tomatic and reference translations areconsidered as unordered sets of lexicalitems.
Ol is computed as the cardinal-ity of the intersection of the two sets di-vided into the cardinality of their union.?
Shallow Syntactic Similarity (SP)SP-Op-?.
Average lexical overlapping overparts-of-speech.SP-Oc-?.
Average lexical overlapping overbase phrase chunk types.SP-NIST.
NIST score over sequences of:SP-NISTl Lemmas.SP-NISTp Parts-of-speech.SP-NISTc Base phrase chunks.SP-NISTiob Chunk IOB labels.?
Syntactic SimilarityOn Dependency Parsing (DP)DP-HWC Head-word chain matching(HWCM), as presented by Liu andGildea (2005), but slightly modi-fied so as to consider different head-word chain types:DP-HWCw words.DP-HWCc categories.DP-HWCr relations.In all cases only chains up to length4 are considered.DP-Ol|Oc|Or These metrics cor-respond exactly to the LEVEL,GRAM and TREE metrics intro-duced by Amigo?
et al (2006):DP-Ol-?
Average overlapping be-tween words hanging at the samelevel of the tree.DP-Oc-?
Average overlapping be-tween words assigned the samegrammatical category.DP-Or-?
Average overlapping be-tween words ruled by the sametype of grammatical relations.On Constituency Parsing (CP)CP-STM Syntactic tree matching(STM), as presented by Liu andGildea (2005), i.e., limited up tolength-4 subtrees.CP-Op-?
Average lexical overlap-ping over parts-of-speech, similarlyto ?SP-Op-?
?, except that parts-of-speech are now consistent with thefull parsing.CP-Oc-?
Average lexical overlappingover phrase constituents.
The differ-ence between this metric and ?SP-Oc-??
is in the phrase scope.
In con-trast to base phrase chunks, con-stituents allow for phrase embed-ding and overlapping.?
Shallow-Semantic SimilarityOn Semantic Roles (SR)SR-Or-?
Average lexical overlappingbetween SRs of the same type.SR-Mr-?
Average lexical matchingbetween SRs of the same type.SR-Or Overlapping between semanticroles independently from their lexi-cal realization.We also consider a more restrictive ver-sion of these metrics (?SR-Mrv -?
?, ?SR-Orv -?
?, and ?SR-Orv ?
), which requireSRs to be associated to the same verb.?
Semantic SimilarityOn Discourse Representations (DR)DR-STM Average semantic treematching considering semanticsubtrees up to length 4.DR-Or-?
Average lexical overlappingbetween DRSs of the same type.DR-Orp-?
Average morphosyntacticoverlapping between DRSs of thesame type.258
