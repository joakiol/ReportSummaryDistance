CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 233?237Manchester, August 2008Semantic Dependency Parsing using N-best Semantic Role Sequences andRoleset InformationJoo-Young Lee, Han-Cheol Cho, and Hae-Chang RimNatural Language Processing Lab.Korea UniversitySeoul, South Korea{jylee,hccho,rim}@nlp.korea.ac.krAbstractIn this paper, we describe a syntactic andsemantic dependency parsing system sub-mitted to the shared task of CoNLL 2008.The proposed system consists of five mod-ules: syntactic dependency parser, predi-cate identifier, local semantic role labeler,global role sequence candidate generator,and role sequence selector.
The syntac-tic dependency parser is based on MaltParser and the sequence candidate gen-erator is based on CKY style algorithm.The remaining three modules are imple-mented by using maximum entropy classi-fiers.
The proposed system achieves 76.90of labeled F1 for the overall task, 84.82 oflabeled attachment, and 68.71 of labeledF1 on the WSJ+Brown test set.1 IntroductionIn the framework of the CoNLL08 shared task(Surdeanu et al, 2008), a system takes POS taggedsentences as input and produces sentences parsedfor syntactic and semantic dependencies as output.A syntactic dependency is represented by an IDof head word and a dependency relation betweenthe head word and its modifier in a sentence.
ASemantic dependency is represented by predicaterolesets and semantic arguments for each predi-cate.The task combines two sub-tasks: syntacticdependency parsing and semantic role labeling.Among the sub-tasks, we mainly focus on the se-mantic role labeling task.
Compared to previousc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.CoNLL 2004 and 2005 shared tasks (Carreras andMa`rquez, 2004; Carreras and Ma`rquez, 2005) andother semantic role labeling research, major dif-ferences of our semantic role labeling task are 1)considering nominal predicates and 2) identify-ing roleset of predicates.
Based on our observa-tion that verbal predicate and nominal predicatehave have different characteristics, we decide tobuild diffent classification modeles for each pred-icate types.
The modeles use same features but,their statistical parameters are different.
In thispaper, maximum entropy1 is used as the classifi-cation model, but any other classification modelssuch as Naive Bayse, SVM, etc.
also can be used.To identify roleset, we investigate a roleset matchscoring method which evaluate how likely a rolesetis matched with the given predicate.2 System DescriptionThe proposed system sequentially performs syn-tactic dependency parsing, predicate identification,local semantic role classification, global sequencegeneration, and roleset information based selec-tion.2.1 Syntactic Dependency ParsingIn the proposed system, Malt Parser (Nivre etal., 2007) is adopted as the syntactic dependencyparser.
Although the training and test set ofCoNLL08 use non-projective dependency gram-mar, we decide to use projective parsing algorithm,Nivre arc-standard, and projective/non-projective conversion functions that Malt Parserprovides.
The reason is that non-projective parsingshows worse performance than projective parsingwith conversion in our preliminary experiment.1We use Zhang Le?s MaxEnt toolkit, http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html233We projectize the non-projective training sen-tences in the training set to generate projective sen-tences.
And then, the parser is trained with thetransformed sentences.
Finally, the parsing resultis converted into non-projective structure by usinga function of Malt Parser.2.2 Predicate IdentificationUnlike previous semantic role labeling task (Car-reras and Ma`rquez, 2004; Carreras and Ma`rquez,2005), predicates of sentences are not providedwith input in the CoNLL08.
It means that a sys-tem needs to identify which words in a sentenceare predicates.We limit predicate candidates to the words thatexist in the frameset list of Propbank and Nom-bank.
Propbank and Nombank provide lists ofabout 3,100 verbal predicates and about 4,400nominal predicates.
After dependency parsing,words which are located in the frameset list are se-lected as predicate candidates.
The predicate iden-tifier determines if a candidate is a predicate or not.The identifier is implemented by using two maxi-mum entropy models, the one is for verbal predi-cates and the other is for nominal predicates.
Thefollowing features are used for predicate identifi-cation:Common FeaturesFor Predicate Identification- Lemma of Previous Word- Lemma of Current Word- Lemma of Next Word- POS of Previous Word- POS of Current Word- POS of Next Word- Dependency Label of Previous Word- Dependency Label of Current Word- Dependency Label of Next WordAdditional Features for Verbal Predicate- Lemma + POS of Current Word- Trigram Lemma of Previous, Current,and Next WordAdditional Features for Nominal Predicate- Lemma of Head of Current Word- POS of Head of Current Word- Dependency Label of Head of Current WordVerbal predicate identifier shows 87.91 of F1 andnominal predicate identifier shows 81.58 of F1.Through a brief error analysis, we found that mainbottle neck for verbal predicate is auxiliary verbbe and have.2.3 Local Semantic Role LabelingPrediate identification is followed by argument la-beling.
For the given predicate, the system firsteliminates inappropriate argument candidates.
Theargument identification uses different strategies forverbs, nouns, and other predicates.The argument classifier extracts features and la-bels semantic roles.
None is used to indicate thata word is not a semantic argument.
The classifieralso uses different maximum entropy models forverbs, nouns, and other predicates2.3.1 Argument Candidate IdentificationAs mentioned by Pradhan et al (2004), ar-gument identification poses a significant bottle-neck to improving performance of Semantic RoleLabeling system.
We tried an algorithm moti-vated from Hacioglu (2004) which defined a tree-structured family membership of a predicate toidentify more probable argument candidates andprune the others.
However, we find that it worksfor verb and other predicate type, but does notwork properly for noun predicate type.
The mainreason is due to the characteristics of argumentsof noun predicates.
First of all, a noun predicatecan be an argument for itself, whereas a verb pred-icate cannot be.
Secondly, dependency relationpaths from a noun predicate to its arguments areusually shorter than a verb predicate.
Althoughsome dependency relation paths are long, they ac-tually involve non-informative relations like IN,MD, or TO.
Finally, major long distance relationpaths could be identified by several path patternsacquired from the corpus.Based on the above analysis, we specify a newargument identification strategy for nominal pred-icate type.
The argument identifier regards a pred-icate and its nearest neighbors - its parent and chil-dren - as argument candidates.
However, if thePOS tag of a nearest neighbor is IN, MD, or TO, itwill be ignored and the next nearest candidates willbe used.
Moreover, several patterns (three consec-utive nouns, adjective and two consecutive nouns,two nouns combined with conjunction, and etc.
)are applied to find long distance argument candi-dates.2342.3.2 Argument ClassificationFor argument classification, various featureshave been used.
Primarily, we tested a set of fea-tures suggested by Hacioglu (2004).
The voice ofthe predicate, left and right words, its POS tag fora predicate, and lexical clues for adjunctive argu-ments also have been tested.
Based on the typeof predicate (i.e.
verb predicate, noun predicate,and other predicate) three classification models aretrained by using maximum entropy with the fol-lowing same features:Features for Argument Classification- Dependen Relation Type- Family Membership- Position- Lemma of Head Word- POS of Head Word- Path- POS Pattern of Predicate?s Children- Relation Pattern of Predicate?s Children- POS Pattern of Predicate?s Siblings- Relation Pattern of Predicate?s Siblings- POS of candidate- Lemma of Left Word of Candidate- POS of Left Word of Candidate- Lemma of Right Word of Candidate- POS of Right Word of CandidateThe classifier produces a list of possible seman-tic roles and its probabilities for each word in thegiven sentence.2.4 Global Semantic Role SequenceGenerationFor local semantic role labeling, we assume thatsemantic roles of words are independent of eachother.
Toutanova et al (2005) and Surdeanu etal.
(2007) show that global constraint and opti-mization are important in semantic role labeling.We use CKY-based dynamic programming strat-egy, similar to Surdeanu et al (2007), to verifywhether role sequences satisfy global constraintand generate candidates of global semantic role se-quences.In this paper, we just use one constraint: noduplicate arguments are allowed for verbal pred-icates.
For verbal predicates, CKY module buildsa list of all kinds of combinations of semantic rolesaugmented with their probabilities.
While buildingthe list of semantic role sequences, it removes thesequences that violate the global constraint.
Theoutput of CKY module is the list of semantic rolesequences satisfying the global constraint.2.5 Global Sequence Selection using RolesetInformationFinally, we need to select the most likely semanticrole sequence.
In addition, we need to identify aroleset for a predicate.
We perform these tasks byfinding a role sequence and roleset maximizing ascore on the following formula:?
?
c+ ?
?
rf + ?
?
mc (1)where, c, rf , mc are role sequence score, relativefrequence of roleset, and matching score with role-set respectively.
?, ?, ?
are tuning parameters ofeach factor and decided empirically by using de-velopment set.
In this paper, we set ?, ?, ?
to 0.5,0.3, 0.2, respectively.The role sequence score is calculated in theglobal semantic role sequence generation ex-plained in Section 2.4.
The relative frequency of aroleset means how many times the roleset occurredin the training set compared to the total occurrenceof the predicate.
It can be easily estimated byMLE.The remaining problem is how to calculate thematching score.
We use maximum entropy modelsas binary classifiers which output match and not-match and use probability of match as matchingscore.
The features used for the roleset matchingclassifiers are based on following intuitions:?
If core roles (e.g., A0, A2, etc) defined ina roleset occur in a given role sequence, itseems to be the right roleset for the role se-quence.?
If matched core roles are close to or have de-pendency relations with a predicate, it seemsto be the right roleset.?
If a roleset has a particle and the predicate ofa sentence also has that particle, it seems tobe the right roleset.
For example, the lemmaof predicate node for the roleset cut.05in frameset file ?cut.xml.gz?
is cut back, sothe particle of cut.05 is back.
If the predicateof a sentence also has particle ?back?, it seemsto be the right roleset.?
If example node of a roleset in frameset filehas a functional word for certain core role that235also exists in a given sentence, it seems to bethe right roleset.
For example, example nodeis defined as follows2:<roleset id="cut.09" ...><example><text>As the building?s new owner,Chase will have its work cutout for it.</text><arg n="1">its work</arg><rel>cut out</rel><arg n="2" f="for">it</arg></example></roleset>Here, semantic role A2 has functional wordfor.
If a given role sequence has A2 and itsword is ?for?, than this role sequence probablymatches that roleset.Based on these intuitions, we use following fea-tures for roleset matching:?
Core Role Matching Count The number ofcore roles exist in both roleset definition andgiven role sequence?
Distance of Matched Core Role Distancebetween predicate and core role which ex-ists in both roleset and given role sequence.We use number of word and dependency pathlength as a distance?
Indication for Same Particle It becomesyes if given predicate and roleset have sameparticle.
(otherwise no)?
Indication for Same Functional Word It be-comes yes if one of core argument is same tothe functional word of roleset.
(otherwise no)To train the roleset match classifiers, we extractsemantic role sequence and its roleset from train-ing data as a positive example.
And then, we gen-erate negative examples by changing its roleset toother roleset of that predicate.
For example, theabove sentence in <text> node3 becomes a pos-itive example for cut.09 and negative examplesfor other roleset such as cut.01, cut.02, etc.2Some nodes are omitted to simplify the definition of ex-ample.3Of cause, we assume that this sentence exist in trainingcorpus.
So, we will extract it from corpus, not from framesetfile.WSJ+Brown WSJ BrownLM 76.90 77.96 68.34LA 84.82 85.69 77.83LF 68.71 69.95 58.63Table 1: System performance.
LM, LA, LF meansmacro labeled F1 for the overall task, labeled at-tachment for syntactic dependencies, and labeledF1 for semantic dependencies, respectivelyLabeled Prec.
Labeled Rec.
Labeled F188.68 73.89 80.28Table 2: Performance of Local Semantic Role La-beler n WSJ test set.
Gold parsing result, correctpredicates, and correct rolesets are used.3 Experimental ResultWe have tested our system with the test set andobtained official results as shown in Table 1.
Wehave also experimented on each module and ob-tained promising results.We have tried to find the upper bound of thelocal semantic role labeling module.
Table 2shows the performance when gold syntactic pars-ing result, correct predicates, and correct rolesetsare given.
Comparing to phrase structure parserbased semantic role labelings such as Pradhan etal.
(2005) and Toutanova et al (2005), our localsemantic role labeler needs to enhance the perfor-mance.
We will try to add some lexical features orchunk features in future works.Next, we have analyzed the effect of rolesetbased selector.
Table 3 shows the effect of match-ing score and relative frequency which are theweighted factor of selection described in section2.5.
Here, baseline means that it selects a role se-quence which has the highest score in CKY mod-ule and roleset is chosen randomly.
The resultsshow that roleset matching score and relative fre-quency of roleset are effective to choose the correctrole sequence and identify roleset.4 ConclusionIn this paper, we have described a syntactic andsemantic dependency parsing system with five dif-ferent modules.
Each module is developed withmaximum entropy classifiers based on differentpredicate types.
In particular, dependency relationcompression method and extracted path patternsare used to improve the performance in the argu-236Prec.
Rec.
F1Baseline (c) 69.34 58.42 63.41+ mc 71.40 60.20 65.32+ rf 75.94 63.98 69.45+ mc, rf 76.46 64.45 69.95Table 3: Semantic scores of global sequence selec-tion in WSJ test set.
mc, rf means matching scoreand relative frequency, respectivelyment candidate identification.
The roleset match-ing method is devised to select the most appropri-ate role sequence and to identify the correct role-set.However, the current features for roleset match-ing seem to be not enough and other useful featuresare expected to be found in the future work.
Thereis also a room for improving the method to inte-grate the role sequence score, matching score, andthe relative frequency.ReferencesJoakim Nivre, Jens Nilsson, Johan Hall, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, Erwin Marsi.
2007.
MaltParser: ALanguage-Independent System for Data-Driven De-pendency Parsing.
Natural Language Engineering,13(2):95?135.Kadri Hacioglu.
2008.
Semantic role labeling usingdependency trees.
In COLING ?04: Proceedings ofProceedings of the 20th international conference onComputational Linguistics.
Morristown, NJ, USA.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semanticrole labeling.
In ACL ?05: Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics.
Morristown, NJ, USA.Mihai Surdeanu and Richard Johansson and AdamMeyers and Llu?
?s Ma`rquez and Joakim Nivre.
2008.The CoNLL-2008 Shared Task on Joint Parsing ofSyntactic and Semantic Dependencies.
In Proceed-ings of the 12th Conference on Computational Natu-ral Language Learning (CoNLL-2008).Mihai Surdeanu, Llu?
?s Ma`rquez, Xavier Carreras, andPere Comas.
2007.
Combination Strategies for Se-mantic Role Labeling.
The Journal of Artificial In-telligence Research, 29:105?151.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Jurafsky.2005.
Support Vector Learning for Semantic Argu-ment Classification.
Machine Learning.
60:11?39.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow Se-mantic Parsing Using Support Vector Machines.
InProceedings of the Human Language TechnologyConference/North American chapter of the Associ-ation of Computational Linguistics (HLT/NAACL).Boston, MA, USA.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 Shared Task: Semantic RoleLabeling.
In Proceedings of CoNLL-2005.Xavier Carreras and Llu?
?s Ma`rquez.
2004.
Introduc-tion to the CoNLL-2004 Shared Task: Semantic RoleLabeling.
In Proceedings of CoNLL-2004.237
