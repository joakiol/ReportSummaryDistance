SESSION 2: DARPA RESOURCE MANAGEMENT ANDATIS BENCHMARK TEST POSTER SESSIONDavid S. PaIlettNational  Institute of Standards and TechnologyBui ld ing 225, Room A216Gaithersburg, MD 20837I .
INTRODUCTION human peripheralFollowing precedents established as early as the March1987 DARPA Speech Recognition Workshop, previously-unreleased Benchmark Test Material was selected and released toDARPA contractors and others prior to the February 1991meeting.
Results were reported to NIST and scored using"official" scoring software and reference answers and the resultswere reported to the participants.All papers in this poster session at the DARPA speechworkshop reported results obtained using the Benchmark TestMaterial.
The Workshop Planning Committee suggested athree-part session format consisting of: (1) Introductoryremarks, (2) one hour to review and discuss posters, and (3)open discussion.Section H of this paper presents an overview describing theapproaches used by the participants in this session, whileSection III summarizes the open discussion.
Section IVdescribes the benchmark test material selection process andbenchmark test protocols.
Section V presents tabulations ofthese results, and discussion of these results is included inSection VI.II.
SESS ION OVERVIEWA total of fourteen papers were presented in poster form.Eleven of the papers were presented by DARPA SLScontractors, and three were from non-DARPA sites.Five papers dealt with speech recognition systems:(1) The group at Dragon Systems reported speaker-dependentsystem results for the Resource Management (RM) test set,using the word-pair grammar \[1\].
Dragon's results wereobtained on a 25 Mhz 80486-based PC, with an RMvocabulary modelled using "roughly 30,000 phonemes incontext or PICs", and making use of the Dragon rapidmatch module.
(2) Doug Paul of MIT Lincoln Laboratory reported speechrecognition results for both the RM and ATIS SPREC testmaterial \[2\].
Recent work includes: variations insemiphone modelling, a "very simple improved urationmodel" responsible for reducing the error rate by about10%, a new training strategy, and modifications to therecognizer to use back-off bigrarn language models.
(3) The Spoken Language Group at MIT's Laboratory forComputer Science also reported results for both the RMand ATIS SPREC test material \[3\].
The MIT SUMMITsystem is a "segment-based" speech recognition system,including a front end that incorporates a model of theauditory system, a hierarchicalsegmentation algorithm to identify a network of possibleacoustical segments, segmental measurements, and astatistical classifier to produce a phonetic network.
Thebest-scoring word sequence is derived by matching thephonetic network against a pronunciation etwork.Recent developments have incorporated more complexcontext-dependency modelling as well as an improvedcorrective training procedure.
(4) Francis Kubala et al reported on BBN's BYBLOS resultsfor both the RM and ATIS SPREC test material \[4\].
Thereported RM speaker-independent r sults include results fora SI model built using only 12 training speakers.
BBN'sATIS results include speaker-independent r sults for twoconditions.
"The first is a controlled condition using aspecific training set and bigram grammar" \[similar to thatused by Paul \[2\]\].
The second condition makes use ofaugmented training data (collected at BBN) and a 4-grarnclass grammar.
(5) A collaborative effort involving Marl Ostendorf and hercolleagues at Boston University and others at BBN makesuse of a general formalism for integrating two or morespeech recognition technologies \[5\].
"In this formalism,one system uses the N-best search strategy to generate alist of candidate sentences; the list is restored by othersystems; and the different scores are combined to optimizeperforrnanee."
Ostendorf et al "report on combining theBU system based on stochastic segment models and theBBN system based on hidden Markov models.
"Six papers were presented by DARPA contractors describingintegrations of speech and natural anguage processing intoATIS systems.
(1) The Spoken Language Group at MIT's Laboratory forComputer Science presented a status report on the MITATIS system \[6\].
A context-independent version of theSUMMIT system (described in \[3\]) including a word-pairgrammar with perplexity 92 has been incorporated.
Theback-end has been redesigned, and the parser now producesan intermediate semantic-frame r presentation "whichserves as the focal-point for all back- end operations.
"Results are reported for both the February '91 ATISbenchmark test set and for a test set collected at MIT.
(2) The Speech and Natural Language Groups at SRI reportedresults for both the RM and ATIS SPREC speechrecognition test sets and for the ATIS NL and SLS tests \[7\].The primary emphasis of the SRI presentation was todescribe improvements o the SRI DECIPHER speechrecognition system, a component in SRI's ATIS system.49Recent "significant" performance improvements areattributed to the addition of tied-mixture HMM modelling.Other approaches discussed include experiments with male-female separation, speaker adaptation, rejection of out-of-vocabulary input, and language modelling (including theuse of multi-word lexical units).
SRI's "simple serialintegration of speech and natural anguage processing" issaid to work well "because the speech recognition systemuses a statistical language model to improve recognitionperformance, and because the natural anguage processinguses a template matching approach (described elsewhere inthis proceedings) that makes it somewhat insensitive torecognition errors".
(3) Wayne Ward presented one of two papers from CMUdescribing the CMU ATIS System, "PHOENIX" \[8\].
Thespeech recognition component consists of a recentvocabulary-independent version of SPHINX, presentlywithout incorporation of out-of- vocabulary models.PHOENIX's "concept of flexible parsing combines frame-based semantics with a semantic phrase grammar," so thatthe "operation of the parser can be viewed as 'phrasespotting.'"
Language modelling included a bigram modelfor the recognizer and a grammar for the parser.
(4) The second paper from CMU, by Sheryl Young, describedthe "structure and operation of SOUL (for Semantically-Oriented Understanding of Language)" \[9\].
SOUL can usesemantic and pragmatic knowledge to correct, reject and/orclarify the outputs of the PHOENIX case frame parser in theATIS domain.
(5) BBN's NL group reported on the BBN DELPHI naturallanguage system and the integration of this system withthe BBN BYBLOS system (described in \[4\]), using an N-best architecture \[10\].
The BBN authors cite a number ofimprovements o the DELPHI system that are described inother papers in this Proceedings.
(6) Recent work on the Unisys ATIS Spoken Language Systemwas described by Norton et al \[11\].
"Enhancements to thesystem's semantic processing for handling non-transparent argument structure and enhancements o thesystem's pragmatic processing of material in answersdisplayed to the user" are described.
In addition to theUnisys system's NL results, results were reported for thecase of SLS systems consisting of the Unisys naturallanguage system coupled with two ATIS speechrecognition systems: (1) the MIT SUMMIT system(described in \[3\]) and (2) the MIT Lincoln Labs system(described in \[2\]).
The Unisys system's natural anguageconstraints were also used to select he first-best of N-bestspeech recognition results (for the SPREC tests) based onsyntactic, semantic and pragmatic knowledge.Three papers were presented by non-DARPA sites.
(1) Douglas O'Shaughnessy described "the initial developmentof a natural anguage text processor, as the first step in anINRS \[INRS-Telecommunications, University of Quebec\]dialogue- by-voice system \[12\].
A keyword slot-fillingapproach is used, rather than a "standard parser forEnglish.
"(2) In one of two papers from AT&T Bell Laboratories includedin this session, Evelyne Tzoukermarm described "The Useof a Commercial Natural Language Interface in the ATISTask" \[13\].
Tzoukermarm relates their "experience inadapting \[a commercial natural language interface\] tohandle domain dependent ATIS queries."
The discussion oferror analysis notes that, in contrast o the "well-formed"written English for which the commercial product wasdesigned, spontaneous peech contains repetitions,restarts, deletions, interjections and ellipsis, as well as theomission of punctuation marks that "might give thesystem information".
(3) The second AT&T Bell Laboratories paper, by Pieraccini,Levin and Lee, proposes "a model for a statisticalrepresentation f the conceptual structure in a restrictedsubset of spoken natural language" \[14\].
The "technique ofease decoding" is applied to the Class A sentences in theATIS domain, with sentences analyzed in terms of 7general cases: QUERY, OBJECT, ATTRIBUTE,RESTRICTION, Q\[UERY\] ATTRIBUTE, AND, andDUMMY.
Unlike other papers in this session, this paperimplements a non-standard test paradigm that preventsexplicit comparisons with the results cited for othersystems.
To address this shortcoming, the authors indicatethat they "are developing a module that translates theconceptual representation into an SQL query".
Presumablythe SQL queries, in conjunction with the ATIS relationaldatabase, will permit use of existing DARPA ATIS query-answer performance evaluation procedures.H I .
D ISCUSSIONFollowing review of the posters, a number of issues werediscussed.
(1) Differences between ATIS Test Sets:It was noted that there were a number of differences between theJune 1990 and February 1991 ATIS test sets, including evidenceof greater-than-expected incidence of dysfluencies in the speechand "skewed" or disproportionate r presentation of somesyntactic/semantic phenomena.
Doug Paul noted that the testset perplexity for the June 1990 "Class A" test set was 18, incontrast with 22 for the present "Class A" test set, and aperplexity of 45 for the "non-Class A" test material (i.e., allother utterances).
Inferences about "progress" or "trends" maythus be complicated by these differences between test sets.
(2) Limited training material:Also noted was the fact that only a limited amount of fully"canonized" training material--for training acoustic modelsand for studying such phenomena s dialogue modelling--wasavailable prior to this meeting, in some cases limiting systemdevelopment.
This factor was cited in a number of papers e.g.,\[2, 4, 8, 10\]).
(3) Limitations on the future value of the ResourceManagement Corpora:Hy Murveit noted his belief that demonstrable progress inrecognizing speaker independent RM1 speech was limited by"how much information we can tease out of \[3990\] training50utterances".
Richard Schwartz took exception to this, citing IV.
BENCHMARK TEST MATERIAL  ANDsteady progress in recognizing RM speech.
PROTOCOLS(4) Properties of ATIS-domain speech:Richard Schwartz shared some analysis of the ATIS-domain testset speakers.
He noted that there was one speaker in the test setwith "24 instances of 'uh' in 12 sentences", \[which leads to\] "a50% word error rate" for that speaker.
On the basis of hisanalysis, he noted that "people don't know how to talk to asystem", and suggested that there ought o be more user/speakerfeedback during the data collection process so that theincidence of dysfluencies would be reduced.
In response, HyMurveit noted that if we regard the two worst speakers in thetest material as atypical, then "the current word error rate isclose to 15%, and with some success in modelling the 'urns' and'ers', the error rate may be only 10%, or abeut wice as bad asfor RM".
Correlation was noted between difficulty inrecognizing both the speech and \[in understanding\] thenaturallanguage for the "bad" speakers, so that the suggestion thatthese speakers may be atypical may be warranted.Patti Price noted that the \[speech recognition\] error ratessuggest that "ATIS is more difficult, but we don't know why".
Itmay be that ATIS speech is "more casual", but we need to studythese issues in more detail, especially as they affect datacollection.
(5) Selection of the February ATIS test material:Victor Zue and Rich Schwartz asked about selection of theFebruary 1991 ATIS test set, asking if there had been screeningto select or reject potential test material on the basis of theincidence of dysfluencies noted in the transcriptions.
NISTnoted that the only such screening was to partition some of theutterances into the "Optional" categories on the basis ofevidence of verbal deletions in the "lexical SNOR"transcriptions, ince this evidence does not appear in theconventional SNOR transcriptions.
For the June test set, therewas no such screening, since attention had not been directed tothe subset containing verbal deletions.
(6) Use of "Baseline" or "Reference" Conditions:John Makhoul noted that there "too many uncontrolledvariables" (e.g., algorithms, training materials, grammar) tomake comparisons of the ATIS speech recognition systemsbeneficial using the SPREC results.
BBN had advocated use of a"baseline" condition and provided SPREC data for both a"baseline" and an "augmented" training condition to permitsuch comparisons \[4\].
MIT/LL also made use of this "baseline"condition \[2\].
Makhoul noted that a similar situation (i.e.,"too many uncontrolled variables") applies for the case of theNL results.
Hy Murveit noted that SRI's reluctance to"lock intoa baseline condition" was based on a reluctance to choose onewith the 'wrong operating point'", based on inadequatetraining.
Francis Kubala noted, however, that choosing a"baseline that undershoots" \[performance\] ought not to be aproblem if one wished to "demonstrate clear wins", and thatsuch a baseline could be changed over time.
John Makhoul alsonoted that reporting error rates is in general preferable toreporting "scores".Benchmark Test MaterialOne portion of the test material consisted of beth "Speaker-Dependent" and "Speaker-Independent" test sets from theResource Management (RM1) Corpus, for use in tests of speechrecognition technology.
Each of these test sets consisted of300 sentence utterances.
The most recent ests using the RM1corpus were conducted prior to the October 1989 Meeting,sixteen months ago.
A second portion of the test materialconsisted of Air Travel Information System (ATIS) domainspeech material and related transcriptions.
This material wascollected at TI in recent months, using the "Wizard" protocoldescribed by Hemphill at the June Meeting \[15\].
There were atotal of 9 speakers in the ATIS test set.This material was partitioned into four subsets: one subsetconsisting of an extension of the "Class A" category used at theJune meeting (expanded to include "testably ambiguous"queries) and containing 145 queries, a second subset consistingof 38 Class D1 query pairs, and two additional smaller"optional" subsets that included examples of "verbal deletion"and/or "verbal correction" (i.e., Optional Class A and OptionalClass D1).
The transcriptions u ed as input to the NL systemsand for scoring the ATIS SPREC tests were provided using arecently developed "lexical SNOR" format.CMU reported benchmark Resource Management results thatwere not represented in the poster session.
These data fromCMU are included in the tables of reported results.
A paperdescribing how these results were achieved appears in \[17\].Benchmark  Test  P ro toco lsIn addition to the Resource Management speech recognitiontests, for which there is considerable precedent, the ATISmaterial could be used for three tests: (1) spontaneous ATIS-domain SPeech RECognition component tests (designated asSPREC in this paper), (2) ATIS-domain Natural Languagesystem component tests (designated as NL), and (3) completeATIS-domain Spoken Language System tests (designated asSLS).
During the June meeting, several sites reported resultsfor NL tests, with CMU being the sole site to report completeSLS test results at that meeting \[16\].The SPREC test design was outlined by an ad hoc WorkingGroup chaired by Victor Zue, with scoring software adapted forthis purpose by NIST.
This is the first time that the SPREC testhas been implemented.In computing results tabulated for the NL and SLS tests, themost reeeent version of the NIST "comparator" was used tocompare the hypothesized CAS-format answers against NIST's"canonical" reference answers, as described in a previous paper\[16\].
Answers are scored as either "True", "False", or (if theNo_Answer option has been exercised) "No Answer".A DARPA SLS Coordinating Committee decision inNovember, 1990 suggested computation of a "weighted errorpercentage" on the basis that (on "intuitive grounds") "a falseanswer is twice as bad as no answer".
The weighted error sodefmed consists of two times the percentage of total queries inthe subset hat are scored "False" plus the percentage scored53.
"No Answer".
A single-number "score" may be derived bysubtracting the weighted error from 100%, providing a single-number score that may range from -100%, for the case of allfalse answers, to +100% for all true answers.A "Class D1 test protocol" was developed and used on a trialbasis for these tests.
Class D1 consists of query pairs for whichthe second query ("Q2") has been classified as "contextdependent", and for which an answerable prior query ("QI") hasbeen identified as defining the context for Q2.
Scoring of ClassD1 query pairs was for the answers provided only for Q2,regardless of the answers provided for the context-settingquery, Q1.The Class D1 test protocol had never previously beenimplemented, and its usage was regarded by many participantsmore as a "debugging of a test protocol" than as a validindicator of systems' abilities to handle context-dependentqueries.
It is also the case that the amount of labelled "ClassDi" training material was extremely small and that it was notwidely available until shortly before the test --thus limitingsystem developers' abilities to make adequate use of thetraining material.
Future implementations of the Class D1 testprotocol will undoubtedly yield more significant results.The "optional" test subsets are not discussed extensively inSection VI since these subsets axe too small, and their usage toolimited, to have significance.V.
BENCHMARK TEST  RESULTSTables 1 - 4 (included at the end of the text of this paper)present abulations of results reported to NIST for uniformscoring against the final "official" sets of referencetranscriptions and reference answers.Some of these numbers may differ slightly from thosereported at the meeting or in some of the papers in thisproceedings, since earlier results reported at the Asilomarmeeting were derived with: (1) a slightly larger Class A test set(148 vs. 145 queries), since the classification of 3 utterances,originally included in the Class A subset, was reconsidered,after the meeting, and determined tobe "unanswerable" and thusnot Class A, and (2) the reference answers for several utteranceswere corrected andlor modified in response to comments fromthe participants in these tests.
However, these differences arenot likely to be statistically significant.Designation of a set of results as "LATE" signifies that theresults were received at NIST some time after midnight onFebruary 6th, 1991.
"COB" on that date had been designated asthe due date for submission of results.
In some cases priornotice had been given to NIST that results would arrive "late",and in a few cases, late results were invited for the sake ofcompleteness and to permit informative comparisons withearlier esults.Resource  Management  (RM1)  SpeechRecogn i t ion  TestsTable 1 presents a tabulation of speech recognition systemresults for the (read speech) RM1 test material.AT IS  Spontaneous  Speech  Recogn i t ionComponent  Tests  (SPREC)Table 2 presents a tabulation of SPREC results for speechrecognition systems (or SLS speech recognition components)results for the spontaneous speech in the ATIS domain.AT IS  Natura l  Language Component  Tests(NL)Table 3 presents a tabulation of natural anguage systemresults for the ATIS NL system components (or systems).In Tables 3 and 4, both the number of queries (and thecorresponding percentage of the total number of queries in agiven category) are shown for the categories "True" (or correct),"False" (incorrect) and "No Answer".
The "Weighted Error"percentage was computed by multiplying the percentage ofFalse answers by 2 and adding the percentage of "No_Answer"responses.
The column labelled "Score" was computed bysubtracting the Weighted Error (%) from 100%.AT IS  Spoken Language Systems Tests  (SLS)Table 4 presents a tabulation of spoken language systemresults for complete ATIS systems.V I .
D ISCUSSION OF  BENCHMARK TESTRESULTSRM1 Speech  Recogn i t ion  Resu l ts  (Table 1)Focusing on the Speaker Independent test set results, withuse of the Word Pair grammar, the word error ranges from 9.7%to 3.6%, while the sentence error ranges from 47.3% to 19.3%.Using the NIST implementation f the McNemar test used inprevious tests \[16\], the differences between the sentence rror-level results for the system with the lowest reported word andsentence rror rates (sys4, the CMU system described inreference \[18\]) and all other systems in this category aresignificant for all but sysl0 and sysl l  (two BBN systemsdescribed in reference \[4\]).
The sentence- error-level-performance differences between the CMU system and the twoBBN systems are not significant.There are three sets of results for the BU-BBN collaborativeeffort described in \[5\].
The first of these (designated sys7),with a santenee error rate of 27.0%, results from the hybrid BU-BBN system.
The second of these (sysl2), with a sentenceerror rate of 27.7%, results from the top answer from the BBNN-best system used for this study.
The third (sysl3), with asentence error rate of 47.3%, results from the top answer of theBU context-independent, gender-dependent segment modelsystem.Lowest overall word and sentence rror rates (1.8% and12.0%, respectively) are reported for the case of the speaker-dependent Word-Pair grammar system results (sys5) reported byPaul, at MIT/LL, described in \[2\].In addition to results reported in this session, note thatresults were reported to NIST for two systems not described inthis session.
Huang et al at CMU reported results for an HMMsystem incorporating a "shared semi-continuons model".
That52system is described in a paper to be presented at ICASSP-91\[17\].
Gauvain and Lee at AT&T Bell Laboratories reportedresults for an investigation "into the use of Bayesian learningof the parameters of a Gaussian mixture density", and this studyis reported in another paper in this proceedings \[18\].AT IS  SPREC Resu l ts  (Tab le  2)Focusing on the word error for the 145 utterances in theClass A test set, the range is from 46.1% to 15.7%, while thesentence error ranges from 91.0% to 52.4%.The McNemar sentence-error-level significance test (notshown) indicates that the system with the lowest reported wordand sentence rror rate for the Class A utterances ( ys24-a, theUnisys implementation f syntactic, semantic and pragmaticconstraints in selecting the first candidate from an N-bestlisting provided by BBN, described in \[11\]) has an error ratethat is significantly less than all but two other systems,(sysl8-a, the BBN "augmented training" system, and sys06-a,the SRI system).
Performance differences (at the sentence errorlevel) between these three systems, however, are notsignificant.Comparison of the results for the BBN "baseline" and"augmented" training condition (sysl8 and sysl9) gives someindication of the benefits of additional (in this case, domain-specific) training and a more powerful 4-gram statistical classgrammar.
The McNemar test indicates that the difference inperformance between sysl8-a and 19-a is significant.Comparisons of results for similar systems for the twolarger test subsets (i.e., Class A results vs. Class D1 results)suggest that the Class D1 material is somewhat more difficult orecognize (i.e., the error rates are higher).
An interestinghypothesis that may account, in part, for this phenomenon isoffered by Norton et al: "...this higher error rate in contextdependent spontaneous tterances may be due in part to thepresence of prosodic phenomena common in dialogue such asdestressing 'old' information" \[11\].Typical SPREC error rates are higher still for the two"optional" test subsets.
This ought not to be surprising in viewof the fact that these utterance subsets are, by definition andselection, more dysfluent (i.e., contain verbal deletions).Not shown in Table 2, but indicated by other analyses, ishigh inter-subject variability for the SPREC tests as well as forthe NL and SiS tests.AT IS  NL  Resu l ts  (Tab le  3)For the Class A subset, results are tabulated for eight NLsystems at 5 DARPA contractors' ites, and at AT&T BellLaboratories and at INRS-Telecom.
For the DARPA contractor'ssystems, the weighted error ranges from 51.7% to 31.0%.The two sets of CMU results include data for the PHOENIXsystem described in \[8\] (sysOl), and for the PHOENIX systemintegrated with the SOUL module described by Young in \[9\](sys02).For the Class A test material, the lowest weighted errorfigures (31.0%) are found for both the SRI system described byMurveit et al in \[7\] (sys13-a), and for the CMU PHOENIX +SOUL system of \[9\] (sys02-a).For the Class D1 and Optional Class D1 subsets, theweighted error percentages are substantially higher than for theClass A results.
For the Class D1 test material, the lowestweighted error figures (36.8%) are found for the Unisys systemdescribed by Norton et al in \[11\] (sys09-d).Note that two sets of results are reported for the Class D1material for BBN (denoted syslS-d and sys23-d).
Subsequent tosubmission of the initial results for sysl5-d, BBN'srepresentatives notified NIST that "...there was a small bug inthe component that translates the result of the understanding(i.e., the output of the discourse component) into SQL... \[andthat since\] the bug in our system.., was NOT in theUNDERSTANDING or the DISCOURSE component but betweenthe output of those components and the SQL backend and ...\[since\] one small quick fix in the backend corrected theproblem, we concluded that it is reasonable to send you newanswers for our Class D test" \[19\].
The data designated assys23-d is derived from these "new answers".AT IS  SLS  Resu l ts  (Tab le  4)For the Class A subset, results are tabulated for 7 SLSsystems at 5 DARPA contractors' sites.
Non-DARPAcontractors declined to participate in the SLS tests.
Theweighted error ranges considerably, from 90.3% to 41.4%, withthe best (lowest weighted error) results for the SRI systemdescribed in \[7\] and in other SRI papers in this proceedings.The low SRI SLS weighted error rate (41.4%) appears to be aconsequence of both a well-performing ATIS speechrecognition component and a well-performing natural languagecomponent (i.e., a SPREC test word error rate of 18.0% and anNL weighted error rate of 31.0%).Not surprisingly, weighted error figures for complete SLSsystems are higher than for corresponding NL components(processing the lexical SNOR formatted versions of the sameutterances).
The relative increase in weighted error rate appearsto correspond to the relative performance of the speechrecognition component.By comparing comparable data from Tables 3 and 4, notethat for the SRI system the weighted error rate for the Class Asubset increases from 31.0% (for the NL component) o41.4%(for the complete SLS system.Two SLS systems made use of BBN's BYBLOS ATIS SPRECdata: the BBN HARC system (sys16-a) and the Unisys-BBNSPREC system (sys22-a).
Comparing the increases of weightederror rates for NL vs. SLS systems, one can note anapproximate increase in weighted error rate of only 8 or 9percentage points for these systems (i.e., from 49.0% for theBBN DELPHI NL system to 57.2% for the BBN HARC SISsystem, and from 51.7% for the Unisys NL system to 60.7% forthe Unisys-((BBN SPREC)) SLS system).
This relatively smallincrease in error rate is probably attributable to the BBN"augmented training" (sysl8-a) SPREC test word error rate of(only) 16.1%, which is not significantly different from SRI'sSPREC test results of 18.0%.53In contrast, a substantially larger increase in error rate canbe noted for the CMU systems (i.e., 35.9% and 31.0% for thetwo CMU NL systems vs. 65.5% for the SLS system), probablydue to performance of the CMU SPREC system with error ratesthat are significantly higher than for the SRI SPREC system.Unisys reported results for three system configurations:using speech recognition results provided by the MIT/LCSATIS SPREC system (designated sysl0-a), by the MIT/LL ATISSPREC system (sys ll-a), and by the BBN BYBLOS/ATISsystem (sys22-a).
In this case, better performance on the SLStest (i.e., lower weighted error) correlates with betterperformance on the SPREC results, as would be expected.As was also the case for the NL results, the weighted errorresults for the Class D1 test subset are substantially higher thanfor the Class A results.V I I .
ACKNOWLEDGEMENTToo many individuals have served as points-of-contact athe research sites involved in these benchmark tests to beindividually thanked, but their efforts and patience in seeingthat information and data are made available are greatlyappreciated.
My colleagues at NIST deserve special thanks fortheir efforts and effieiancy in making these tests possible andin tabulation of the results.
In particular, Bill Fisher has had akey role, both as Chairman of the DARPA SLS PerformanceEvaluation Working Group and as the individual responsible forATIS test material selection and in reviewing the "canonical"auxiliary files.
Jon Fiscus and John Garofolo, also at NIST,have been responsible for implementation f scoring softwareand for preparation of corpora on CD-ROM.VIII.
REFERENCES1.
Baker, J., et al, "Dragon Systems Resource ManagementBenchmark Test Results--February 1991" (in thisProceedings).2.
Paul, D. B.
"New Results with the Lincoln Tied-MixtureHMM CSR System" (in this Proceedings).3.
Phillips, M., Glass, J. and Zue, V., "Modelling ContextDependency in Acoustic-Phonetic and Lexical Representations"(in this Proceedings).4.
Kubala, F. et al, "BYBLOS Speech Recognition BenchmarkResults" (in this Proceedings).5.
Ostendorf, M. et al, "Integration of Diverse RecognitionMethodologies Through Reevaluation of N-Best SentenceHypotheses" (in this Proceedings).6.
Seneff, S. et al, "Development and Preliminary Evaluationof the M1T ATIS System" (in this Proceedings).7.
Murveit, H. et al, "SRI's Speech and Natural LanguageEvaluation" (in this Proceedings).8.
Ward, W., "Current Status of the CMU ATIS System" (in thisProceedings).9.
Young, S., "Using Semantics to Correct Parser Output forATIS Utterances" (in this Proceedings).10.
Austin, S. et al, "BBN HARC and Delphi Results on theATIS Benchmarks--February 1991" (in this Proceedings).11.
Norton, L. et al, "Augmented Role filling Capabilities forSemantic Interpretation of Spoken Language" (in thisProceedings).12.
O'Shaughnessy, D., "A Textual Processor to Handle ATISQueries" (in this Proceedings).13.
Tzoukermann, E., "The Use of a Commercial NaturalLanguage Interface in the ATIS Task" (in this Proceedings).14.
Pieraccini, R., Levin, E. and Lee, C.H., "StochasticRepresentation f Conceptual Structure in the ATIS Task" (inthis Proceedings).15.
Hemphill, C.T., Godfrey, J.J., and Doddington, G.R., "TheATIS Spoken Language System Pilot Corpus" in Proceedings ofthe DARPA\] Speech and Natural Language Workshop" June1990, pp.
96 -101.16.
Pallett, D.S., et al "DARPA ATIS Test Results: June 1990"in Proceedings of the DARPA\] Speech and Natural LanguageWorkshop" June 1990, pp.
114 - 121.17.
Huang, X. et al, "Improved Acoustic Modelling for theSPHINX Speech Recognition System", (to be presented atICASSP-91).18.
Gauvaln, J. and Lee, C.H., "Bayesian Learning of GaussianMixture Densities for Hidden Marker Models" (in thisProceedings).19.
ARPANET communication from M. Bates and R. Ingria(BBN) to Dave Pallett (NIST), February 13, 1991.54NIST- ID  Corr  Subsysl 84.3 12.5sys4 86.2 11.5sys6 83.2 14.2sys8 81.9 14.8sys9 82.2 14.4sysl0 83.3 13.6FEB91 RM1 SPEECH RECOGNIT ION TESTSPEAKER- INDEPENDENT WITHOUTArr.Dei Ins Err  S .Er r  Date3.2 1.8 17.6 66.0 Jan-312.3 3.2 17.0 66.7 Feb-52.7 2.9 19.7 71.7 Feb-ll3.3 2.5 20.7 74.7 Feb-63.4 2.0 19.8 70.7 Feb-73.1 2.1 18.8 69.3 Feb-7GRAMMARDescriptionSRI Spkr-Indep no grammarCMU Spkr-Indep no grammarM1T-LL Spkr-Indep no grammar-LATEAT&T Spkr-Indep no grammarAT&T-R Spkr-Indep no grammar-LATEBBN Spkr-Indep (109) no grammar-LATEN IST- ID  Corr  Subsysl 95.9 3.0sys2 93.3 6.0sys4 96.8 2.5sys6 96.2 2.8sys7 95.7 3.3sys8 95.5 3.5sysl0 96.7 2.3sysl l  96.7 2.8sysl2 95.7 3.3sysl3 93.0 5.3sysl4 96.1 3.0SPEAKER-INDEPENDENT WORD-PAIRArr.GRAMMARDel Ins Err S.Err Date Description1.0 0.8 4.8 26.0 Jan-310.7 1.2 8.0 33.7 Feb-40.8 0.4 3.6 19.3 Feb-51.0 0.6 4.4 23.3 Feb-61.0 1.2 5.6 27.0 Feb-61.0 0.7 5.2 28.0 Feb-60.9 0.5 3.8 21.0 Feb-70.6 0.5 3.8 23.0 Feb-71.0 1.1 5.4 27.7 Feb-81.8 2.6 9.7 47.3 Feb-120.8 0.7 4.5 25.7 Feb-28SRI Spkr-Indep Word-PairM1T Spkr-Indep Word-PairCMU Spkr-Indep Word-PairMIT-LL Spkr-Indep Word-PairBU-BBN Spkr-Indep Word-PairAT&T Spkr-Indep Word-PairBBN Spkr-Indep (109) Word-Pair-LATEBBN Spkr-Indep (12) Word-Pair-LATEBU-BBN (W/O BU SSM) Spkr-Indep Word-Pair-LATEBU Segment Model Spkr-Indep Word-Pair-LATEAT&T Sex-Modelled Spkr-Indep Word-Pair-LATEN IST- ID  Corr  Subsys5 92.5 5.8SPEAKER.DEPENDENT WITHOUT GRAMMARArr.Del Ins Err  S .Er r  Date Description1.7 1.3 8.7 44.0 Feb-6 Mrr-LL Spkr-Dep no grammarN IST- ID  Corr  Subsys3 94.1 4.5sys5 98.3 1.0SPEAKER-DEPENDENTDel  Ins Err  S .Err1.4 1.5 7.5 34.30.7 0.1 1.8 12.0WORD-PAIR  GRAMMARArr.Date DescriptionFeb-5 Dragon Spkr-Dep Word-PairFeb-6 MIT-LL Spkr-Dep Word-PairTABLE 1.Key to Table 1: The following key is provided as an aid in cross-referencing the NIST-ID numbers to the sites submittingresults and to descriptions of the systems in the references cited in this paper.KEY: RM1 SPEECH RECOGNIT ION TEST REFERENCESN IST- ID  Site Reference N IST- ID  Site Referencesysl SRI \[7\] sys8 AT&T \[18\]sys2 MIT-LCS \[3\] sys9 AT&T \[18\]sys3 Dragon \[1\] sysl0 BBN \[4\]sys4 CMU \[17\] sysl 1 BBN \[4\]sys5 MIT-LL \[2\] sys 12 BU-BBN \[5\]sys6 AT&T \[2\] sys13 BU-BBN \[5\]sys7 BU-BBN \[5\] sys14 AT&T \[18\]55Feb91 ATIS SPREC TestClass AArr .N IST - ID  Corr  Sub Del Ins  Err  S .Err  Datesys04-a 73.6 19.5 7.0 2.6 29.0 79.3 Feb-6sys05-a 79.8 16.2 4.0 5.9 26.1 88.3 Feb-6sys06-a 86.4 10.5 3.1 4.4 18.0 60.0 Feb-6sys08-a 65.2 28.1 6.8 8.8 43.6 86.2 Feb-6sysl4-a 63.3 30.4 6.3 9.4 46.1 91.0 Feb-6sysl8-a 87.6 9.9 2.6 3.7 16.1 54.5 Feb-llsysl9-a 82.5 14.5 3.0 5.3 22.8 67.6 Feb-llsys24-a 88.4 9.4 2.2 4.1 15.7 52.4 Feb-13DescriptionCMU Class-A SPRECMIT-LL Class-A SPRECSRI Class-A SPRECMIT-LCS Class-A SPRECUnisys/MIT-LCS Class-A SPRECBBN Class-A SPREC Caugmented")-LATEBBN Class-A SPREC ("baseline")-LATEUnisys/BBN Class-A Spree-LATEN IST- IDsysO4-dsys05-dsys06-dsysl4-dsysl8-dsysl9-dsys24-dCorr73.770.778.652.081.577.182.4Sub17.622.216.434.113.417.313.2Class D1Arr .Del Ins  Err  S .Err  Date8.6 0.7 26.9 77.6 Feb-67.1 3.9 33.2 91.4 Feb-ll4.9 1.2 22.5 67.2 Feb-613.9 6.6 54.6 91.4 Feb-65.1 2.0 20.5 58.6 Feb-ll5.6 3.4 26.3 74.1 Feb-ll4.4 2.4 20.0 58.6 Feb-13DescriptionCMU Class-D1 SPRECM1T-LL Class-D1 SPREC-LATESRI Class-D1 SPRECUnisys/MIT-LCS Class-D1 SPRECBBN Class-D1 SPREC ("augmented")-LATEBBN Class-D1 SPREC Cbaseline")-LATEUnisys/BBN Class-D1 Spree-LATEOptional ClassArr .N IST - ID  Corr  Sub Del  Ins  Err S .Err  Datesys04-ao 61.7 29.6 8.7 5.3 43.7 81.8 Feb-6sys05-ao 74.3 22.8 2.9 13.1 38.8 100.0 Feb-11sys06-ao 76.7 18.9 4.4 6.8 30.1 90.9 Feb-6sysl4-ao 51.5 42.2 6.3 14.6 63.1 100.0 Feb-6sysl8-ao 74.8 23.8 1.5 11.7 36.9 90.9 Feb-11sysl9-ao 74.8 23.3 1.9 16.0 41.3 100.0 Feb-11sys24-ao 75.2 23.3 1.5 12.6 37.4 90.9 Feb-13ADescriptionCMU Optional Class-A SPRECMIT-LL Optional Class-A SPREC-LATESRI Optional Class-A SPRECUnisys/MIT-I.,CS Optional Class-A SPRECBBN Optional Class-A SPREC ("augmented")-LATEBBN Optional Class-A SPREC Cbaseline")-LATEUnisys/BBN Optional Class-A Spree-LATEOptionalArr.N IST - ID  Corr  Sub Del  Ins  Err S .Err  Datesys04-do 73.7 22.8 3.5 7.0 33.3 75.0 Feb-6sysO5-do 80.7 15.8 3.5 21.1 40.4 100.0 Feb-11sysO6-do 87.7 10.5 1.8 22.8 35.1 100.0 Feb-6sys14-do 59.6 40.4 0.0 15.8 56.1 100.0 Feb-6sys18-do 82.5 14.0 3.5 21.1 38.6 75.0 Feb-11sys19-do 82.5 15.8 1.8 14.0 31.6 100.0 Feb-llsys24-do 84.2 12.3 3.5 24.6 40.4 75.0 Feb-13Class D1DescriptionCMU Optional Class-D1 SPRECM1T-LL Optional Class-D1 SPREC-LATESRI Optional Class-D SPRECUnisys/MIT-LCS Optional Class-D1 SPRECBBN Optional Class-D1 SPREC ("augmented")-LATEBBN Optional Class-D1 SPREC ("baseline")-LATEUnisys/BBN Optional Class-D1 Spree-LATETABLE 2.56NIST-ID TruesysOl-a 117 (80.6%)sysO2-a 117 (80.6%)sysO7-a 82 (56.5%)sysO9-a 84 (57.9%)sysl2-a 69 (47.5%)sysl3-a 109 (75.1%)sysl5-a 85 (58.6%)sysl7-a 56 (38.6%)FEB91 ATIS NL TESTFa lseClass AArr.No Ans, W. Err Score Date Descr ip t ion24 (16.5%) 4 (2.7%) 35.917 (11.7%) 11 (7.5%) 31.02 (1.3%) 61 (42.0%) 44.814 (9.6%) 47 (32.4%) 51.760 (41.3%) 16 (11.0%) *9 (6.2%) 27 (18.6%) 31.011 (7.5%) 49 (33.7%) 49.089 (61.3%) 0 (0%) *64.169.055.248.3*69.051.0Feb-6 CMU Class-A NLFeb-6 CMU Class-A NL withknowledge-based moduleFeb-6 MIT-LCS Class-A NLFeb-6 Unisys Class-A NLFeb-7 AT&T Class-A NL-LATEFeb-6 SRI Class-A NLFeb-7 BBN Class-A NL (DELPHI)-LATEFeb-7 INRS Class-A NL-LATENIST-ID True Fa lsesys01-d 25 (65.7%) 11 (28.9%)sys02-d 25 (65.7%) 6 (15.7%)sysO7-d 18 (47.3%) 2 (5.2%)sysO9-d 24 (63.1%) 0 (0%)sysl2-d 17 (44.7%) 18 (47.3%)sysl3-d 22 (57.8%) 3 (7.8%)sysl5-d 10 (26.3%) 3 (7.8%)sys23-d 26 (68.4%) 3 (7.8%)Class D1Arr.No Ans.
W. Err Score Date Descr ip t ion2 (5.2%) 63.27 (18.4%) 50.018 (47.3%) 57.914 (36.8%) 36.83 (7.8%) *13 (34.2%) 50.025 (65.7%) 81.69 (23.6%) 39.536.850.042.163.2*50.018.460.5Feb-6 CMU Class-D1 NLFeb-6 CMU Class-D1 NL withknowledge-based moduleFeb-6 MIT-LCS Class-D1 NLFeb-6 Unisys Class-D1 NLFeb-7 AT&T Class-D1 NI_,-LATEFeb-6 SRI Class-D1 NLFeb-7 BBN Class-D1 NL (DELPHIyBUG-LATEFeb-13 BBNClass-D1 NL(DELPHI)/DEBUGGED-LATENIST-ID True Fa lsesys09-ao 1 (9.0%) 0 (0%)sys12-ao 2 (18.1%) 8 (72.7%)sys13-ao 3 (27.2%) 1 (9.0%)sys17-ao 3 (27.2%) 8 (72.7%)Optional  Class AArr.No Ans.
W. Err Score  Date Descr ip t ion10 (90.9%) 90.9 9.1 Feb-6 Unisys Optional Class-A NL1 (9.0%) * * Feb-7 AT&T Optional Class-A NL-LATE7 (63.6%) 81.8 18.2 Feb-6 SRI Optional Class-A NL0 (0%) * * Feb-7 INRS OptionalClass-A NL-LATENIST-ID Truesys09-do 0 (0%)sys12-do 0 (0%)sys13-do 0 (0%)Optional  Class D1Arr.Fa lse  No Ans.
W. Err Score Date Descr ip t ion1 (50.0%) 1 (50.0%) 150.0 -50.0 Feb-6 Unisys Optional Class-D1 NL2 (100.0%) 0 (0%) * * Feb-7 AT&T Optional Class-D1 NL-LATE2 (100.0%) 0 (0%) 200.0 -100.0 Feb-6 SRI Optional Class-D1 NL* Non-DARPA contractors, unpublished by mutual agreementTABLE 3.57FEB91 ATIS SLS TESTNIST-ID Truesys03-a 89 (61.3%)sysl0-a 29 (20.0%)sysll-a 32 (22.0%)sys16-a 84 (57.9%)sys20-a 46 (31.7%)sys21-a 96 (66.2%)sys22-a 77 (53.1%)FalseClass AArr.No Ans.
W. Err Score Date Descr ipt ion39 (26.8%) 17 (11.7%) 65.5 34.515 (10.3%) 101 (69.6%) 90.3 9.75 (3.4%) 108 (74.4%) 81.4 18.622 (15.1%) 39 (26.8%) 57.2 42.819 (13.1%) 80 (55.1%) 81.4 18.611 (7.5%) 38 (26.2%) 41.4 58.620 (13.7%) 48 (33.1%) 60.7 39.3Feb-6 CMU Class-A SLSFeb-6 Unisys Class-A SLS(MIT-LCS SPREC)Feb-6 Unisys Class-A SLS (MIT-LL SPREC)Feb-11 BBN Class-A SLS (HARC)-LATEFeb-6 M1T-LCS Class-A SISFeb-6 SRI Class-A SLSFeb-13 Unisys Class-A SLS (BBN SPREC)-LATENIST-ID Truesys03-d 16 (42.1%)sys21-d 15 (39.4%)sys26-d 7 (18.4%)FalseClass D1Arr.No Ans.
W. Err Score Date Descr ip t ion20 (52.6%) 2 (5.2%) 110.5 -10.511 (28.9%) 12 (31.5%) 89.5 10.53 (7.8%) 28 (73.6%) 89.5 10.5Feb-6 CMU Class-D1 SLSFeb-6 SRI Class-D1 SLSFeb-16 BBN Class-D1 SLS (HARC)-LATENIST-ID Truesysl0-ao 2 (18.1%)sys21-ao 3 (27.2%)sys22-ao 2 (18.1%)SPREC)-LATEFalseo (o%)o (o~)o (o~)Optional Class AArr.No Ans.
W. Err Score Date Descr ip t ion9 (81.8%) 81.8 18.2 Feb-6 Unisys Optional Class-A SLS(MIT-LCS SPREC)8 (72.7%) 72.7 27.3 Feb-6 SRI Optional Class-A SLS9 (81.8%) 81.8 18.2 Feb-13 Unisys optional Class-A SLS (BBNNIST-ID True Falsesys21-do 0 (0%) 1 (50.0%)Optional Class D1Arr.No Ans.
W. Err Score Date Descr ip t ion1 (50.0%) 150.0 -50.0 Feb-6 SRI Optional Class-D1 SLSTABLE 4.Key to Tables 2, 3 and 4: The following key is provided as an aid in cross-referencing the NIST-ID numbersto the sites submitting ATIS results and to descriptions of the systems in the references cited in this paper.Note: key for these tables differs from that for the RM1 results of Table 1.KEY: ATIS SPREC, NL, AND SLS TEST REFERENCESNIST- ID  Site Reference NIST-ID Site ReferencesysOl CMU \[8\] sysl3 SRI \[7\]sys02 CMU \[9\] sys14 Unisys \[11\]sys03 CMU \[8\] sysl5 BBN \[10\]sys04 CMU \[8\] sys16 BBN \[10\]sys05 MIT-LL \[2\] sys 17 INRS \[12\]sys06 SRI \[7\] sys18 BBN \[4\]sys07 MIT-LCS \[6\] sys19 BBN \[4\]sys08 MIT-LCS \[3\] sys20 MIT-LCS \[6\]sys09 Unisys \[11\] sys21 SRI \[7\]sys 10 Unisys \[11\] sys22 Unisys \[11\]sys 11 Unisys \[11\] sys23 BBN 10\]sys12 AT&T \[13\] sys24 Unisys \[11\]58
