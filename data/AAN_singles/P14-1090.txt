Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956?966,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsInformation Extraction over Structured Data:Question Answering with FreebaseXuchen Yao1and Benjamin Van Durme1,21Center for Language and Speech Processing2Human Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD, USAAbstractAnswering natural language questions us-ing the Freebase knowledge base has re-cently been explored as a platform for ad-vancing the state of the art in open do-main semantic parsing.
Those efforts mapquestions to sophisticated meaning repre-sentations that are then attempted to bematched against viable answer candidatesin the knowledge base.
Here we showthat relatively modest information extrac-tion techniques, when paired with a web-scale corpus, can outperform these sophis-ticated approaches by roughly 34% rela-tive gain.1 IntroductionQuestion answering (QA) from a knowledge base(KB) has a long history within natural languageprocessing, going back to the 1960s and 1970s,with systems such as Baseball (Green Jr et al,1961) and Lunar (Woods, 1977).
These systemswere limited to closed-domains due to a lack ofknowledge resources, computing power, and abil-ity to robustly understand natural language.
Withthe recent growth in KBs such as DBPedia (Aueret al, 2007), Freebase (Bollacker et al, 2008)and Yago2 (Hoffart et al, 2011), it has be-come more practical to consider answering ques-tions across wider domains, with commercial sys-tems including Google Now, based on Google?sKnowledge Graph, and Facebook GraphSearch, based on social network connections.The AI community has tended to approach thisproblem with a focus on first understanding the in-tent of the question, via shallow or deep forms ofsemantic parsing (c.f.
?3 for a discussion).
Typ-ically questions are converted into some mean-ing representation (e.g., the lambda calculus), thenmapped to database queries.
Performance is thusbounded by the accuracy of the original seman-tic parsing, and the well-formedness of resultantdatabase queries.1The Information Extraction (IE) community ap-proaches QA differently: first performing rela-tively coarse information retrieval as a way totriage the set of possible answer candidates, andonly then attempting to perform deeper analysis.Researchers in semantic parsing have recentlyexplored QA over Freebase as a way of movingbeyond closed domains such as GeoQuery (Tangand Mooney, 2001).
While making semantic pars-ing more robust is a laudable goal, here we providea more rigorous IE baseline against which thoseefforts should be compared: we show that ?tradi-tional?
IE methodology can significantly outper-form prior state-of-the-art as reported in the se-mantic parsing literature, with a relative gain of34% F1as compared to Berant et al (2013).2 ApproachWe will view a KB as an interlinked collection of?topics?.
When given a question about one or sev-eral topics, we can select a ?view?
of the KB con-cerning only involved topics, then inspect everyrelated node within a few hops of relations to thetopic node in order to extract the answer.
We callsuch a view a topic graph and assume answers canbe found within the graph.
We aim to maximallyautomate the answer extraction process, by mas-sively combining discriminative features for boththe question and the topic graph.
With a high per-formance learner we have found that a system withmillions of features can be trained within hours,leading to intuitive, human interpretable features.For example, we learn that given a question con-cerning money, such as: what money is used in1As an example, 50% of errors of the CCG-backed(Kwiatkowski et al, 2013) system were contributed by pars-ing or structural matching failure.956ukraine, the expected answer type is likely cur-rency.
We formalize this approach in ?4.One challenge for natural language queryingagainst a KB is the relative informality of queriesas compared to the grammar of a KB.
For exam-ple, for the question: who cheated on celebrityA, answers can be retrieved via the Freebase rela-tion celebrity.infidelity.participant, but the con-nection between the phrase cheated on and theformal KB relation is not explicit.
To allevi-ate this problem, the best attempt so far is tomap from ReVerb (Fader et al, 2011) predicate-argument triples to Freebase relation triples (Caiand Yates, 2013; Berant et al, 2013).
Note thatto boost precision, ReVerb has already pruneddown less frequent or credible triples, yielding notas much coverage as its text source, ClueWeb.Here we instead directly mine relation mappingsfrom ClueWeb and show that both direct relationmapping precision and indirect QA F1improve bya large margin.
Details in ?5.Finally, we tested our system, jacana-freebase,2on a realistic dataset generouslycontributed by Berant et al (2013), who collectedthousands of commonly asked questions bycrawling the Google Suggest service.
Ourmethod achieves state-of-the-art performancewith F1at 42.0%, a 34% relative increase fromthe previous F1of 31.4%.3 BackgroundQA from a KB faces two prominent challenges:model and data.
The model challenge involvesfinding the best meaning representation for thequestion, converting it into a query and exe-cuting the query on the KB.
Most work ap-proaches this via the bridge of various interme-diate representations, including combinatory cat-egorial grammar (Zettlemoyer and Collins, 2005,2007, 2009; Kwiatkowski et al, 2010, 2011,2013), synchronous context-free grammars (Wongand Mooney, 2007), dependency trees (Liang etal., 2011; Berant et al, 2013), string kernels (Kateand Mooney, 2006; Chen and Mooney, 2011),and tree transducers (Jones et al, 2012).
Theseworks successfully showed their effectiveness inQA, despite the fact that most of them requirehand-labeled logic annotations.
More recent re-search started to minimize this direct supervisionby using latent meaning representations (Berant et2https://code.google.com/p/jacanaal., 2013; Kwiatkowski et al, 2013) or distant su-pervision (Krishnamurthy and Mitchell, 2012).We instead attack the problem of QA from a KBfrom an IE perspective: we learn directly the pat-tern of QA pairs, represented by the dependencyparse of questions and the Freebase structure ofanswer candidates, without the use of intermedi-ate, general purpose meaning representations.The data challenge is more formally framed asontology or (textual) schema matching (Hobbs,1985; Rahm and Bernstein, 2001; Euzenat andShvaiko, 2007): matching structure of two on-tologies/databases or (in extension) mapping be-tween KB relations and NL text.
In terms ofthe latter, Cai and Yates (2013) and Berant et al(2013) applied pattern matching and relation inter-section between Freebase relations and predicate-argument triples from the ReVerb OpenIE sys-tem (Fader et al, 2011).
Kwiatkowski et al(2013) expanded their CCG lexicon with Wik-tionary word tags towards more domain indepen-dence.
Fader et al (2013) learned question para-phrases from aligning multiple questions with thesame answers generated by WikiAnswers.
Thekey factor to their success is to have a huge textsource.
Our work pushes the data challenge to thelimit by mining directly from ClueWeb, a 5TBcollection of web data.Finally, the KB community has developed othermeans for QA without semantic parsing (Lopez etal., 2005; Frank et al, 2007; Unger et al, 2012;Yahya et al, 2012; Shekarpour et al, 2013).
Mostof these work executed SPARQL queries on in-terlinked data represented by RDF (Resource De-scription Framework) triples, or simply performedtriple matching.
Heuristics and manual templateswere also commonly used (Chu-Carroll et al,2012).
We propose instead to learn discriminativefeatures from the data with shallow question anal-ysis.
The final system captures intuitive patternsof QA pairs automatically.4 Graph FeaturesOur model is inspired by an intuition on how ev-eryday people search for answers.
If you askedsomeone: what is the name of justin bieberbrother,3and gave them access to Freebase, thatperson might first determine that the question3All examples used in this paper come from the train-ing data crawled from Google Suggest.
They are low-ercased and some contain typos.957is about Justin Bieber (or his brother), go toJustin Bieber?s Freebase page, and search for hisbrother?s name.
Unfortunately Freebase does notcontain an exact relation called brother, but in-stead sibling.
Thus further inference (i.e., brother?
male sibling) has to be made.
In the followingwe describe how we represent this process.4.1 Question GraphIn answering our example query a person mighttake into consideration multiple constraints.
Withregards to the question, we know we are lookingfor the name of a person based on the following:?
the dependency relation nsubj(what, name)and prep of(name, brother) indicates that thequestion seeks the information of a name;4?
the dependency relation prep of(name,brother) indicates that the name is about abrother (but we do not know whether it is aperson name yet);?
the dependency relation nn(brother, bieber)and the facts that, (i) Bieber is a person and (ii)a person?s brother should also be a person, indi-cate that the name is about a person.This motivates the design of dependency-basedfeatures.
We show one example in Figure 1(a),left side.
The following linguistic information isof interest:?
question word (qword), such as what/who/howmany.
We use a list of 9 common qwords.5?
question focus (qfocus), a cue of expected an-swer types, such as name/money/time.
Wekeep our analysis simple and do not use a ques-tion classifier, but simply extract the noun de-pendent of qword as qfocus.?
question verb (qverb), such as is/play/take, ex-tracted from the main verb of the question.Question verbs are also good hints of answertypes.
For instance, play is likely to be followedby an instrument, a movie or a sports team.?
question topic (qtopic).
The topic of the ques-tion helps us find relevant Freebase pages.
Wesimply apply a named entity recognizer to findthe question topic.
Note that there can be morethan one topic in the question.Then we convert the dependency parse into a moregeneric question graph, in the following steps:4We use the Stanford collapsed dependency form.5who, when, what, where, how, which, why, whom,whose.1.
if a node was tagged with a question feature,then replace this node with its question feature,e.g., what?
qword=what;2.
(special case) if a qtopic node was tagged asa named entity, then replace this node withits its named entity form, e.g., bieber ?qtopic=person;3. drop any leaf node that is a determiner, prepo-sition or punctuation.The converted graph is shown in Figure 1(a),right side.
We call this a question feature graph,with every node and relation a potential featurefor this question.
Then features are extractedin the following form: with s the source andt the target node, for every edge e(s, t) in thegraph, extract s, t, s | t and s | e | t asfeatures.
For the edge, prep of(qfocus=name,brother), this would mean the following features:qfocus=name, brother, qfocus=name|brother,and qfocus=name|prep of|brother.We show with examples why these featuresmake sense later in ?6 Table 6.
Furthermore, thereason that we have kept some lexical features,such as brother, is that we hope to learn fromtraining a high correlation between brother andsome Freebase relations and properties (such assibling and male) if we do not possess an exter-nal resource to help us identify such a correlation.4.2 Freebase Topic GraphGiven a topic, we selectively roll out the Free-base graph by choosing those nodes within a fewhops of relationship to the topic node, and forma topic graph.
Besides incoming and/or outgo-ing relationships, nodes also have properties: astring that describes the attribute of a node, forinstance, node type, gender or height (for a per-son).
One major difference between relations andproperties is that both arguments of a relation arenodes, while only one argument of a property is anode, the other a string.
Arguments of relations areusually interconnected, e.g., London can be theplace of birth for Justin Bieber, or capital ofthe UK.
Arguments of properties are attributes thatare only ?attached?
to certain nodes and have nooutgoing edges.
Figure 1(b) shows an example.Both relationship and property of a node areimportant to identifying the answer.
They con-nect the nodes with the question and describesome unique characteristics.
For instance, with-out the properties type:person and gender:male,958whatis namethe brotherjustin biebernsubj     copnnprep_of    detnnqwordqtopicqfocusqtopicqword=whatqfocus=namebrotherqtopic=person qtopic=personnsubjnnprep_ofnnqverb=becopqverb(a) Dependence parse with annotated question features in dashed boxes (left) and converted feature graph (right) withonly relevant and general information about the original question kept.
Note that the left is a real but incorrect parse.Justin Bieberdummy nodeJazmyn Bieberperson.sibling_sJaxon Biebersibling siblingpersontypepersontypefemalegendermalegenderLondonawards_wonplace_of_birth?...
type personmalegender(b) A view of Freebase graph on the Justin Bieber topic with nodes in solid boxes and properties indashed boxes.
The hatching node, Jaxon Bieber, is the answer.
Freebase uses a dummy parent nodefor a list of nodes with the same relation.Figure 1: Dependency parse and excerpted Freebase topic graph on the question what is the name ofjustin bieber brother.959we would not have known the node Jaxon Bieberrepresents a male person.
These properties, alongwith the sibling relationship to the topic node, areimportant cues for answering the question.
Thusfor the Freebase graph, we use relations (with di-rections) and properties as features for each node.Additionally, we have analyzed how Freebaserelations map back to the question.
Some of themapping can be simply detected as paraphras-ing or lexical overlap.
For example, the per-son.parents relationship helps answering ques-tions about parenthood.
However, most Freebaserelations are framed in a way that is not com-monly addressed in natural language questions.For instance, for common celebrity gossip ques-tions like who cheated on celebrity A, it ishard for a system to find the Freebase relationcelebrity.infidelity.participant as the target rela-tion if it had not observed this pattern in training.Thus assuming there is an alignment model thatis able to tell how likely one relation maps to theoriginal question, we add extra alignment-basedfeatures for the incoming and outgoing relation ofeach node.
Specifically, for each relation rel ina topic graph, we compute P (rel | question) torank the relations.
Finally the ranking (e.g., top1/2/5/10/100 and beyond) of each relation is usedas features instead of a pure probability.
We de-scribe such an alignment model in ?
5.4.3 Feature ProductionWe combine question features and Freebase fea-tures (per node) by doing a pairwise concatena-tion.
In this way we hope to capture the associa-tion between question patterns and answer nodes.For instance, in a loglinear model setting, we ex-pect to learn a high feature weight for features like:qfocus=money|node type=currencyand a very low weight for:qfocus=money|node type=person.This combination greatly enlarges the totalnumber of features, but owing to progress in large-scale machine learning such feature spaces are lessof a concern than they once were (concrete num-bers in ?
6 Model Tuning).5 Relation MappingIn this section we describe a ?translation?
table be-tween Freebase relations and NL words was built.5.1 FormulaThe objective is to find the most likely rela-tion a question prompts.
For instance, for thequestion who is the father of King GeorgeVI, the most likely relation we look for is peo-ple.person.parents.
To put it more formally,given a question Q of a word vector w, we wantto find out the relation R that maximizes the prob-ability P (R | Q).More interestingly, for the question who isthe father of the Periodic Table, the ac-tual relation that encodes its original mean-ing is law.invention.inventor, rather than peo-ple.person.parents.
This simple example pointsout that every part of the question could changewhat the question inquires eventually.
Thus weneed to count for each word w in Q.
Due to thebias and incompleteness of any data source, weapproximate the true probability of P with?P un-der our specific model.
For the simplicity of com-putation, we assume conditional independence be-tween words and apply Naive Bayes:?P (R | Q) ?
?P (Q | R)?P (R)?
?P (w | R)?P (R)?
?w?P (w | R)?P (R)where?P (R) is the prior probability of a relationR and?P (w | R) is the conditional probability ofword w given R.It is possible that we do not observe a certainrelation R when computing the above equation.In this case we back off to the ?sub-relations?
: arelation R is a concatenation of a series of sub-relations R = r = r1.r2.r3.
.
.
.. For instance, thesub-relations of people.person.parents are peo-ple, person, and parents.
Again, we assume con-ditional independence between sub-relations andapply Naive Bayes:?Pbackoff(R | Q) ?
?P (r | Q)?
?r?P (r | Q)?
?r?P (Q | r)?P (r)?
?r?w?P (w | r)?P (r)One other reason that we estimated?P (w | r) and?P (r) for sub-relations isthat Freebase relations share some com-mon structures in between them.
For in-stance, both people.person.parents andfictional universe.fictional character.parents960indicate the parent relationship but the latter ismuch less commonly annotated.
We hope that theshared sub-relation, parents, can help better esti-mate for the less annotated.
Note that the backoffmodel would have a much smaller value than theoriginal, due to double multiplication?r?w.
Inpractice we normalize it by the sub-relations sizeto keep it at the same scale with?P (R | Q).Finally, to estimate the prior and conditionalprobability, we need a massive data collection.5.2 StepsThe ClueWeb096dataset is a collection of 1 billionwebpages (5TB compressed in raw HTML) in 10languages by Carnegie Mellon University in 2009.FACC1, the Freebase Annotation of the ClueWebCorpus version 1 (Gabrilovich et al, 2013), con-tains index and offset of Freebase entities withinthe English portion of ClueWeb.
Out of all 500million English documents, 340 million were au-tomatically annotated with at least one entity, withan average of 15 entity mentions per document.The precision and recall of annotation were esti-mated at 80?85% and 70?85% (Orr et al, 2013).Given these two resources, for each binary Free-base relation, we can find a collection of sentenceseach of which contains both of its arguments, thensimply learn how words in these sentences are as-sociated with this relation, i.e.,?P (w | R) and?P (w | r).
By counting how many times each rela-tion R was annotated, we can estimate?P (R) and?P (r).
The learning task can be framed in the fol-lowing short steps:1.
We split each HTML document by sentences(Kiss and Strunk, 2006) using NLTK (Bird andLoper, 2004) and extracted those with at leasttwo Freebase entities which has at least one di-rect established relation according to Freebase.2.
The extraction formed two parallel corpora,one with ?relation - sentence?
pairs (for esti-mating?P (w | R) and?P (R)) and the other with?subrelations - sentence?
pairs (for?P (w | r)and?P (r)).
Each corpus has 1.2 billion pairs.3.
The tricky part was to align these 1.2 billionpairs.
Since the relations on one side of thesepairs are not natural sentences, we ran themost simple IBM alignment Model 1 (Brownet al, 1993) to estimate the translation proba-bility with GIZA++ (Och and Ney, 2003).
Tospeed up, the 1.2 billion pairs were split into6http://lemurproject.org/clueweb09/0 ?
10 ?
102?
103?
104> 1047.0% 0.7% 1.2% 0.4% 1.3% 89.5%Table 1: Percentage of answer relations (the in-coming relation connected to the answer node)with respect to how many sentences we learnedthis relation from in CluewebMapping.
For in-stance, the first column says there are 7% of an-swer relations for which we cannot find a mapping(so we had to use the backoff probability estima-tion); the last column says there are 89.5% of an-swer relations that we were able to learn the map-ping between this relation and text based on morethan 10 thousand relation-sentence pairs.
The totalnumber of answer relations is 7886.100 even chunks.
We ran 5 iterations of EM oneach one and finally aligned the 1.2 billion pairsfrom both directions.
To symmetrize the align-ment, common MT heuristics INTERSECTION,UNION, GROW-DIAG-FINAL, and GROW-DIAG-FINAL-AND (Koehn, 2010) were separately ap-plied and evaluated later.4.
Treating the aligned pairs as observation, theco-occurrence matrix between aligning rela-tions and words was computed.
There were10,484 relations and sub-relations in all, and wekept the top 20,000 words.5.
From the co-occurrence matrix we computed?P (w | R),?P (R),?P (w | r) and?P (r).Hand-checking the learned probabilities showsboth success, failure and some bias.
For in-stance, for the film.actor.film relation (mappingfrom film names to actor names), the top wordsgiven by?P (w | R) are won, star, among, show.For the film.film.directed by relation, some im-portant stop words that could indicate this re-lation, such as by and with, rank directly afterdirector and direct.
However, due to signifi-cant popular interest in certain news categories,and the resultant catering by websites to thoseinformation desires, then for example we alsolearned a heavily correlated connection betweenJennifer Aniston and celebrity.infidelity.victim,and between some other you-know-who namesand celebrity.infidelity.participant.We next formally evaluate how the learned map-ping help predict relations from words.9615.3 EvaluationBoth ClueWeb and its Freebase annotation has abias.
Thus we were firstly interested in the cov-erage of mined relation mappings.
As a com-parison, we used a dataset of relation mappingcontributed by Berant et al (2013) and Lin et al(2012).
The idea is very similar: they intersectedFreebase relations with predicates in (arg1, predi-cate, arg2) triples extracted from ReVerb to learnthe mapping between Freebase relations and triplepredicates.
Note the scale difference: althoughReVerb was also extracted from ClueWeb09,there were only 15 million triples to intersect withthe relations, while we had 1.2 billion alignmentpairs.
We call this dataset ReverbMapping andours CluewebMapping.The evaluation dataset, WEBQUESTIONS, wasalso contributed by Berant et al (2013).
It con-tains 3778 training and 2032 test questions col-lected from the Google Suggest service.
All ques-tions were annotated with answers from Freebase.Some questions have more than one answer, suchas what to see near sedona arizona?.We evaluated on the training set in two aspects:coverage and prediction performance.
We defineanswer node as the node that is the answer andanswer relation as the relation from the answernode to its direct parent.
Then we computed howmuch and how well the answer relation was trig-gered by ReverbMapping and CluewebMapping.Thus for the question, who is the father of KingGeorge VI, we ask two questions: does the map-ping, 1.
(coverage) contain the answer relationpeople.person.parents?
2.
(precision) predictthe answer relation from the question?Table 1 shows the coverage of CluewebMap-ping, which covers 93.0% of all answer rela-tions.
Among them, we were able to learn the rulemapping using more than 10 thousand relation-sentence pairs for each of the 89.5% of all an-swer relations.
In contrast, ReverbMapping covers89.7% of the answer relations.Next we evaluated the prediction performance,using the evaluation metrics of information re-trieval.
For each question, we extracted all rela-tions in its corresponding topic graph, and rankedeach relation with whether it is the answer re-lation.
For instance, for the previous exam-ple question, we want to rank the relation peo-ple.person.parents as number 1.
We com-puted standard MAP (Mean Average Precision)and MRR (Mean Reciprocal Rank), shown in Ta-ble 2(a).
As a simple baseline, ?word overlap?counts the overlap between relations and the ques-tion.
CluewebMapping ranks each relation by?P (R | Q).
ReverbMapping does the same, ex-cept that we took a uniform distribution on?P (w |R) and?P (R) since the contributed dataset didnot include co-occurrence counts to estimate theseprobabilities.7Note that the median rank fromCluewebMapping is only 12, indicating that halfof all answer relations are ranked in the top 12.Table 2(b) further shows the percentage ofanswer relations with respect to their rank-ing.
CluewebMapping successfully ranked 19%of answer relations as top 1.
A sampleof these includes person.place of birth, loca-tion.containedby, country.currency used, reg-ular tv appearance.actor, etc.
These percentagenumbers are good clue for feature design: for in-stance, we may be confident in a relation if it isranked top 5 or 10 by CluewebMapping.To conclude, we found that CluewebMappingprovides satisfying coverage on the 3778 trainingquestions: only 7% were missing, despite the bi-ased nature of web data.
Also, CluewebMappinggives reasonably good precision on its prediction,despite the noisy nature of web data.
We move onto fully evaluate the final QA F1.6 ExperimentsWe evaluate the final F1in this section.
The sys-tem of comparison is that of Berant et al (2013).Data We re-used WEBQUESTIONS, a datasetcollected by Berant et al (2013).
It contains 5810questions crawled from the Google Suggest ser-vice, with answers annotated on Amazon Mechan-ical Turk.
All questions contain at least one an-swer from Freebase.
This dataset has been split by65%/35% into TRAIN-ALL and TEST.
We furtherrandomly divided TRAIN-ALL by 80%/20% to asmaller TRAIN and development set DEV.
Notethat our DEV set is different from that of Berantet al (2013), but the final result on TEST is di-rectly comparable.
Results are reported in termsof macro F1with partial credit (following Berantet al (2013)) if a predicted answer list does nothave a perfect match with all gold answers, as a7The way we used ReverbMapping was not how Berant etal.
(2013) originally used it: they employed a discriminativelog-linear model to judge relations and that might yield betterperformance.
As a fair comparison, ranking of CluewebMap-ping under uniform distribution is also included in Table 2(a).962Median Rank MAP MRRword overlap 471 0.0380 0.0590ReverbMapping 60 0.0691 0.0829CluewebMapping 12 0.2074 0.2900with uniform dist.
61 0.0544 0.0561(a) Ranking on answer relations.
Best result onCluewebMapping was under the GROW-DIAG-FINAL-ANDheuristics (row 3) when symmetrizing alignment from bothdirections.
The last row shows ranking of CluewebMappingunder uniform distribution (assuming counting on words andrelations is not known).1 ?
5 ?
10 ?
50 ?
100 > 100w.
o.
3.5 4.7 2.5 3.9 4.1 81.3R.M.
2.6 9.1 8.6 26.0 13.0 40.7C.M.
19.0 19.9 8.9 22.3 7.5 22.4(b) Percentage of answer relations w.r.t.
ranking number(header).
w.o.
: word overlap; R.M.
: ReverbMapping; C.M.
:CluewebMapping.Table 2: Evaluation on answer relation rankingprediction on 3778 training questions.lot of questions in WEBQUESTIONS contain morethan one answer.Search With an Information Retrieval (IR)front-end, we need to locate the exact Freebasetopic node a question is about.
For this pur-pose we used the Freebase Search API (Freebase,2013a).All named entities8in a question were sentto this API, which returned a ranked list of rele-vant topics.
We also evaluated how well the searchAPI served the IR purpose.
WEBQUESTIONS notonly has answers annotated, but also which Free-base topic nodes the answers come from.
Thuswe evaluated the ranking of retrieval with the goldstandard annotation on TRAIN-ALL, shown in Ta-ble 3.
The top 2 results of the Search API con-tain gold standard topics for more than 90% of thequestions and the top 10 results contain more than95%.
We took this as a ?good enough?
IR front-end and used it on TEST.Once a topic is obtained we query the FreebaseTopic API (Freebase, 2013b) to retrieve all rele-vant information, resulting in a topic graph.
TheAPI returns almost identical information as dis-played via a web browser to a user viewing thistopic.
Given that turkers annotated answers basedon the topic page via a browser, this supports theassumption that the same answer would be locatedin the topic graph, which is then passed to the QAengine for feature extraction and classification.8When no named entities are detected, we fall back tonoun phrases.top 1 2 3 5 10# 3263 3456 3532 3574 3604% 86.4 91.5 93.5 94.6 95.4Table 3: Evaluation on the Freebase Search API:how many questions?
top n retrieved results con-tain the gold standard topic.
Total number of ques-tions is 3778 (size of TRAIN-ALL).
There wereonly 5 questions with no retrieved results.P R F1basic 57.3 30.1 39.5+ word overlap 56.0 31.4 40.2+ CluewebMapping 59.9 35.4 44.5+both 59.0 35.4 44.3Table 4: F1on DEV with different feature settings.Model Tuning We treat QA on Freebase as abinary classification task: for each node in thetopic graph, we extract features and judge whetherit is the answer node.
Every question was pro-cessed by the Stanford CoreNLP suite with thecaseless model.
Then the question features (?4.1)and node features (?4.2) were combined (?4.3)for each node.
The learning problem is chal-lenging: for about 3000 questions in TRAIN,there are 3 million nodes (1000 nodes per topicgraph), and 7 million feature types.
We em-ployed a high-performance machine learning tool,Classias (Okazaki, 2009).
Training usuallytook around 4 hours.
We experimented with vari-ous discriminative learners on DEV, including lo-gistic regression, perceptron and SVM, and foundL1 regularized logistic regression to give the bestresult.
The L1 regularization encourages sparsefeatures by driving feature weights towards zero,which was ideal for the over-generated featurespace.
After training, we had around 30 thousandfeatures with non-zero weights, a 200 fold reduc-tion from the original features.Also, we did an ablation test on DEV abouthow additional features on the mapping betweenFreebase relations and the original questions help,with three feature settings: 1) ?basic?
features in-clude feature productions read off from the fea-ture graph (Figure 1); 2) ?+ word overlap?
addsadditional features on whether sub-relations haveoverlap with the question; and 3) ?+ CluewebMap-ping?
adds the ranking of relation prediction giventhe question according to CluewebMapping.
Ta-ble 4 shows that the additional CluewebMapping963P R F1Gold Retrieval 45.4 52.2 48.6Freebase Search API 38.8 45.8 42.0Berant et al (2013) - - 31.4Table 5: F1on TEST with Gold Retrieval andFreebase Search API as the IR front end.
Berantet al (2013) actually reported accuracy on thisdataset.
However, since their system predicted an-swers for almost every question (p.c.
), it is roughlythat precision=recall=F1=accuracy for them.features improved overall F1by 5%, a 13% rel-ative improvement: a remarkable gain given thatthe model already learned a strong correlation be-tween question types and answer types (explainedmore in discussion and Table 6 later).Finally, the ratio of positive vs. negative exam-ples affect final F1: the more positive examples,the lower the precision and the higher the recall.Under the original setting, this ratio was about1 : 275.
This produced precision around 60%and recall around 35% (c.f.
Table 4).
To optimizefor F1, we down-sampled the negative examples to20%, i.e., a new ratio of 1 : 55.
This boosted thefinal F1on DEV to 48%.
We report the final TESTresult under this down-sampled training.
In prac-tice the precision/recall balance can be adjusted bythe positive/negative ratio.Test Results Table 5 gives the final F1on TEST.
?Gold Retrieval?
always ranked the correct topicnode top 1, a perfect IR front-end assumption.
Ina more realistic scenario, we had already evaluatedthat the Freebase Search API returned the correcttopic node 95% of the time in its top 10 results (c.f.Table 3), thus we also tested on the top 10 resultsreturned by the Search API.
To keep things sim-ple, we did not perform answer voting, but sim-ply extracted answers from the first (ranked by theSearch API) topic node with predicted answer(s)found.
The final F1of 42.0% gives a relative im-provement over previous best result (Berant et al,2013) of 31.4% by one third.One question of interest is whether our system,aided by the massive web data, can be fairly com-pared to the semantic parsing approaches (notethat Berant et al (2013) also used ClueWeb in-directly through ReVerb).
Thus we took outthe word overlapping and CluewebMapping basedfeatures, and the new F1on TEST was 36.9%.The other question of interest is that whetherour system has acquired some level of ?machinewgt.
feature5.56 qfocus=money|type=Currency5.35 qverb=die|type=Cause Of Death5.11 qword=when|type=datetime4.56 qverb=border|rel=location.adjoins3.90 qword=why|incoming relation rank=top 32.94 qverb=go|qtopic=location|type=Tourist attraction-3.94 qtopic=location|rel=location.imports exports.date-2.93 qtopic=person|rel=education.end dateTable 6: A sample of the top 50 most positive/neg-ative features.
Features are production betweenquestion and node features (c.f.
Figure 1).intelligence?
: how much does it know what thequestion inquires?
We discuss it below throughfeature and error analysis.Discussion The combination between questionsand Freebase nodes captures some real gist of QApattern typing, shown in Table 6 with sampled fea-tures and weights.
Our system learned, for in-stance, when the question asks for geographic ad-jacency information (qverb=border), the correctanswer relation to look for is location.adjoins.Detailed comparison with the output from Berantet al (2013) is a work in progress and will be pre-sented in a follow-up report.7 ConclusionWe proposed an automatic method for QuestionAnswering from structured data source (Free-base).
Our approach associates question featureswith answer patterns described by Freebase andhas achieved state-of-the-art results on a balancedand realistic QA corpus.
To compensate for theproblem of domain mismatch or overfitting, weexploited ClueWeb, mined mappings between KBrelations and natural language text, and showedthat it helped both relation prediction and an-swer extraction.
Our method employs relativelylightweight machinery but has good performance.We hope that this result establishes a new baselineagainst which semantic parsing researchers canmeasure their progress towards deeper languageunderstanding and answering of human questions.Acknowledgments We thank the Allen Institutefor Artificial Intelligence for funding this work.We are also grateful to Jonathan Berant, TomKwiatkowski, Qingqing Cai, Adam Lopez, ChrisCallison-Burch and Peter Clark for helpful discus-sion and to the reviewers for insightful comments.964ReferencesS?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.2007.
DBPedia: A nucleus for a web of open data.In The semantic web, pages 722?735.
Springer.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic Parsing on Freebase fromQuestion-Answer Pairs.
In Proceedings of EMNLP.Steven Bird and Edward Loper.
2004.
NLTK: The Nat-ural Language Toolkit.
In Proceedings of the ACLWorkshop on Effective Tools and Methodologies forTeaching Natural Language Processing and Compu-tational Linguistics.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Managementof data, pages 1247?1250.
ACM.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational linguistics, 19(2):263?311.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In Proceedings of ACL.David L Chen and Raymond J Mooney.
2011.
Learn-ing to Interpret Natural Language Navigation In-structions from Observations.
In AAAI, volume 2,pages 1?2.J.
Chu-Carroll, J.
Fan, B. K. Boguraev, D. Carmel,D.
Sheinwald, and C. Welty.
2012.
Finding needlesin the haystack: Search and candidate generation.IBM Journal of Research and Development.J?er?ome Euzenat and Pavel Shvaiko.
2007.
Ontologymatching.
Springer.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of EMNLP.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-Driven Learning for Open Ques-tion Answering.
In Proceedings of ACL.Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, HansUszkoreit, Berthold Crysmann, Brigitte J?org, andUlrich Sch?afer.
2007.
Question answering fromstructured knowledge sources.
Journal of AppliedLogic, 5(1):20?48.Freebase.
2013a.
Freebase Search API.https://developers.google.com/freebase/v1/search-overview.Freebase.
2013b.
Freebase Topic API.https://developers.google.com/freebase/v1/topic-overview.Evgeniy Gabrilovich, Michael Ringgaard, , and Amar-nag Subramanya.
2013.
FACC1: Freebase anno-tation of ClueWeb corpora, Version 1 (Release date2013-06-26, Format version 1, Correction level 0).http://lemurproject.org/clueweb09/FACC1/, June.Bert F Green Jr, Alice K Wolf, Carol Chomsky, andKenneth Laughery.
1961.
Baseball: an automaticquestion-answerer.
In Papers presented at the May9-11, 1961, western joint IRE-AIEE-ACM computerconference, pages 219?224.
ACM.Jerry R Hobbs.
1985.
Ontological promiscuity.
InProceedings of ACL.Johannes Hoffart, Fabian M Suchanek, KlausBerberich, Edwin Lewis-Kelham, Gerard De Melo,and Gerhard Weikum.
2011.
Yago2: exploring andquerying world knowledge in time, space, context,and many languages.
In Proceedings of the 20thinternational conference companion on World WideWeb, pages 229?232.
ACM.Bevan Keeley Jones, Mark Johnson, and Sharon Gold-water.
2012.
Semantic parsing with bayesian treetransducers.
In Proceedings of ACL.Rohit J Kate and Raymond J Mooney.
2006.
Usingstring-kernels for learning semantic parsers.
In Pro-ceedings of ACL.Tibor Kiss and Jan Strunk.
2006.
Unsupervised mul-tilingual sentence boundary detection.
Computa-tional Linguistics, 32(4):485?525.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA.Jayant Krishnamurthy and Tom M Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of EMNLP-CoNLL.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proceedings of EMNLP, pages1223?1233.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical generaliza-tion in CCG grammar induction for semantic pars-ing.
In Proceedings of EMNLP.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling Semantic Parsers withOn-the-fly Ontology Matching.
In Proceedings ofEMNLP.Percy Liang, Michael I. Jordan, and Dan Klein.2011.
Learning Dependency-Based CompositionalSemantics.
In Proceedings of ACL.Thomas Lin, Oren Etzioni, et al 2012.
Entity Linkingat Web Scale.
In Proceedings of Knowledge Extrac-tion Workshop (AKBC-WEKEX), pages 84?88.965Vanessa Lopez, Michele Pasin, and Enrico Motta.2005.
Aqualog: An ontology-portable question an-swering system for the semantic web.
In The Seman-tic Web: Research and Applications, pages 546?562.Springer.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Naoaki Okazaki.
2009.
Classias: a collection ofmachine-learning algorithms for classification.Dave Orr, Amar Subramanya, Evgeniy Gabrilovich,and Michael Ringgaard.
2013.
11 billionclues in 800 million documents: A web re-search corpus annotated with freebase concepts.http://googleresearch.blogspot.com/2013/07/11-billion-clues-in-800-million.html, July.Erhard Rahm and Philip A Bernstein.
2001.
A surveyof approaches to automatic schema matching.
theVLDB Journal, 10(4):334?350.Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,and S?oren Auer.
2013.
Question answering on in-terlinked data.
In Proceedings of WWW.Lappoon R Tang and Raymond J Mooney.
2001.
Us-ing multiple clause constructors in inductive logicprogramming for semantic parsing.
In MachineLearning: ECML 2001, pages 466?477.
Springer.Christina Unger, Lorenz B?uhmann, Jens Lehmann,Axel-Cyrille Ngonga Ngomo, Daniel Gerber, andPhilipp Cimiano.
2012.
Template-based questionanswering over RDF data.
In Proceedings of the21st international conference on World Wide Web.Yuk Wah Wong and Raymond J Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Proceedings of ACL.William A Woods.
1977.
Lunar rocks in natural en-glish: Explorations in natural language question an-swering.
Linguistic structures processing, 5:521?569.Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural language questions for theweb of data.
In Proceedings of EMNLP.Luke S Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
Uncertainty in Artificial Intelligence(UAI).Luke S Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proceedings of EMNLP-CoNLL.Luke S Zettlemoyer and Michael Collins.
2009.Learning context-dependent mappings from sen-tences to logical form.
In Proceedings of ACL-CoNLL.966
