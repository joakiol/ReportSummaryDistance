Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1442?1451,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCross-Sentence Inference for Process KnowledgeSamuel Louvan+, Chetan Naik+, Sadhana Kumaravel+, Heeyoung Kwon+,Niranjan Balasubramanian+, Peter Clark?+Stony Brook University, ?Allen Institute for AI,{slouvan, cnaik, skumaravel, heekwon,niranjan}@cs.stonybrook.edu,peterc@allenai.orgAbstractFor AI systems to reason about real world situ-ations, they need to recognize which processesare at play and which entities play key roles inthem.
Our goal is to extract this kind of role-based knowledge about processes, from mul-tiple sentence-level descriptions.
This knowl-edge is hard to acquire; while semantic rolelabeling (SRL) systems can extract sentencelevel role information about individual men-tions of a process, their results are often noisyand they do not attempt create a globally con-sistent characterization of a process.To overcome this, we extend standard withinsentence joint inference to inference acrossmultiple sentences.
This cross sentence in-ference promotes role assignments that arecompatible across different descriptions of thesame process.
When formulated as an IntegerLinear Program, this leads to improvementsover within-sentence inference by nearly 3%in F1.
The resulting role-based knowledge isof high quality (with a F1 of nearly 82).1 IntroductionKnowledge about processes is essential for AI sys-tems in order to understand and reason about theworld.
At the simplest level, even knowing whichclass of entities play key roles can be useful fortasks involving recognition and reasoning about pro-cesses.
For instance, given a description ?a pud-dle drying in the sun?, one can recognize this as aninstance of the process evaporation using a macro-level role knowledge: Among others, the typical un-dergoer of evaporation is a kind of liquid (the pud-1) Evaporation is the process by which liquids areconverted to their gaseous forms.2) Evaporation is the process by which water isconverted into water vapor.3) Water vapor rises from water due to evaporation.4) Clouds arise as water evaporates in the sun.Table 1: Example sentences for the process evaporation.
Un-derlined spans correspond to fillers for the undergoer role.dle), and the enabler is usually a heat source (thesun).Our goal is to acquire this kind of role-basedknowledge about processes from sentence-level de-scriptions in grade level texts.
Semantic role label-ing (SRL) systems can be trained to identify theseprocess specific roles.
However, these were de-veloped for sentence-level interpretation and onlyensure within sentence consistency of labels (Pun-yakanok et al, 2004; Toutanova et al, 2005; Lewiset al, 2015), limiting their ability to generate co-herent characterizations of the process overall.
Inparticular, the same process participant may appearin text at different syntactic positions, with differentwording, and with different verbs, which makes ithard to extract globally consistent descriptions.
Inthis work, we propose a cross sentence inferencemethod to address this problem.To illustrate the challenge consider some exam-ple sentences on evaporation shown in Table 1.Theunderlined spans correspond to fillers for an un-dergoer role i.e., the main entity that is undergo-ing evaporation.
However, the filler water occursas different syntactic arguments with different mainactions.
Without large amounts of process-specifictraining data, a supervised classifier will not able to1442learn these variations reliably.
Nevertheless, sinceall these sentences are describing evaporation, it ishighly likely that water plays a single role.
This ex-pectation can be encoded as a factor during inferenceto promote consistency and improve accuracy, and isthe basis of our approach.We formalize this cross sentence joint inferenceidea as an Integer Linear Program (ILP).
Our cen-tral idea is to collect all sentences for a single pro-cess, generate candidate arguments, and assign rolesthat are globally consistent for all arguments withinthe process.
This requires a notion of consistency,which we model as pairwise alignment of argumentsthat should receive the same label.
Argument-levelentailment alone turns out to be ineffective for thispurpose.Therefore, we develop an alignment classifier thatuses the compatibility of contexts in which the can-didate arguments are embedded.
We transform theoriginal role-label training data to create alignmentpairs from arguments that get assigned the same la-bel, thus avoiding the need for additional labeling.Finally, the ILP combines the output of the SRLclassifier and the alignment classifier in an objectivefunction in order to find globally consistent assign-ments.An empirical evaluation on a process datasetshows that proposed cross sentence formulation out-performs a strong within sentence joint inferencebaseline, which uses scores from a custom built roleclassifier that is better suited for the target domain.In summary, this work makes the following con-tributions:1.
A cross-sentence, collective role-labeling andalignment method for harvesting processknowledge.2.
A high quality semantic resource that providesknowledge about scientific processes discussedin grade-level texts including physical, biolog-ical, and natural processes.3.
An evaluation which shows that the proposedcross sentence inference yields high qualityprocess knowledge.2 Related WorkRole-based representations have been shown to beuseful for Open-domain factoid question answer-ing (Shen and Lapata, 2007; Pizzato and Molla?,2008), grade-level science exams (Jauhar et al,2016) , and comprehension questions on processdescriptions (Berant et al, 2014).
Similar to pro-cess comprehension work, we target semantic rep-resentations about processes but we focus only ona high-level summary of the process, rather thandetailed sequential representation of sub-events in-volved.
Moreover, we seek to aggregate knowledgefrom multiple descriptions rather than understand asingle discourse about each process.There has been substantial prior work on se-mantic role labeling itself, that we leverage in thiswork.
First, there are several systems trained onthe PropBank dataset, e.g., EasySRL (Lewis et al,2015), Mate (Bjo?rkelund et al, 2009), Generalized-Inference (Punyakanok et al, 2004).
Although use-ful, the PropBank roles are verb (predicate) specific,and thus do not produce consistent labels for a pro-cess (that may be expressed using several differentverbs).
In constrast, frame-semantic parsers, e.g.,SEMAFOR (Das et al, 2010), trained on FrameNet-annotated data (Baker et al, 1998) do produce con-cept (frame)-specific labels, but the FrameNet train-ing data has poor (< 50%) coverage of the gradescience process terms.
Building a resource likeFrameNet for a list of scientific processes is expen-sive.Several unsupervised, and semi-supervised ap-proaches have been proposed to address these issuesfor PropBank style predicate-specific roles (Swierand Stevenson, 2004; Lang and Lapata, 2011;Fu?rstenau and Lapata, 2009; Fu?rstenau and Lapata,2012; Lang and Lapata, 2010; Klementiev, 2012).
Akey idea here is to cluster syntactic signatures of thearguments and use the discovered clusters as roles.Another line of research has sought to perform jointtraining for syntactic parsing and semantic role la-beling (Lewis et al, 2015), and in using PropBankrole labels to improve FrameNet processing usingpivot features (Kshirsagar et al, 2015).Some SRL methods account for context informa-tion from multiple sentences (Ruppenhofer et al,2010; Roth and Lapata, 2015).
They focus on an-1443Process Undergoer Enabler Action Resultevaporation liquid heat changes gaswater heat energy convert water vaporweathering rock weather disintegration smaller rockssolid material heating breaking down smaller particlesphotosynthesis carbon dioxide solar energy convert energyCO2 light energy transforms foodTable 2: Examples of Target Knowledge Rolesnotating individual event mentions in a documentusing discourse-level evidence such as co-referencechains.
Our task is to aggregate knowledge aboutprocesses from multiple sentences in different doc-uments.
Although both tasks require raw SRL-styleinput, the different nature of the process task meansthat a different solution framework is needed.Our goal is to acquire high quality semanticrole based knowledge about processes.
This al-lows us an unique opportunity to jointly inter-pret sentences that are discussing the same pro-cess.
We build on ideas from previous within sen-tence joint inference (Punyakanok et al, 2004), ar-gument similarity notions in semi and unsupervisedapproaches (Fu?rstenau and Lapata, 2012), and com-bining PropBank roles to propose a cross-sentenceinference technique (Kshirsagar et al, 2015).
Theinference can be integrated with existing trained su-pervised learning pipelines, which can provide ascore for role assignments for a given span.3 ApproachProcesses are complex events with many partici-pating entities and inter-related sub-events.
In thiswork, we aim for a relatively simple macro-levelrole-based knowledge about processes.
Our task isto find classes of entities that are likely to fill keyroles within a process namely, the undergoer, en-abler, result, and action1 (different verbs denotingthe main action when the process is occurring, e.g.,?dry?).
We select these roles based on an initialanalysis of grade science questions that involve rec-ognizing instances of processes from their descrip-tions.
Table 2 shows some examples of the targetknowledge roles.1For simplicity, we abuse the notion of a role to also include themain action as a role.We develop a scalable pipeline for gathering suchrole-based process knowledge.
The input to our sys-tem is the name of a process, e.g., ?evaporate?.
Thenwe use a set of query patterns to find sentences thatdescribe the process.
A semantic role classifier thenidentifies the target roles in these sentences.
Theoutput is a list of typical fillers for the four processroles.This setting presents a unique opportunity, wherethe goal is to perform semantic role labeling on a setof closely related sentences, sentences that describethe same process.
This allows us to design a jointinference method that can promote expectations ofconsistency amongst the extracted role fillers.There is no large scale training data that canbe readily used for this task.
Because we tar-get process-specific and not verb-specific semanticroles, existing ProbBank (Kingsbury and Palmer,2003) trained SRL systems cannot be used di-rectly.
Frame-semantic parsers trained on FrameNetdata (Baker et al, 1998) are also not directly usablebecause FrameNet lacks coverage for many of theprocesses discussed in the science domain.
There-fore, we create a process dataset that covers a rel-atively small number of processes, but demonstratethat the role extraction generalizes to previously un-seen processes as well.3.1 Cross-Sentence InferenceGiven a set of sentences about a process, we wantto extract role fillers that are globally consistent i.e.,we want role assignments that are compatible.
Ourapproach is based on two observations: First, anygiven role is likely to have similar fillers for a par-ticular process.
For instance, the undergoers of theevaporation process are likely to be similar ?
theyare usually liquids.
Second, similar arguments are1444?role ?sentS11 S12S21 S22?alignFigure 1: A factor graph representation of cross sentenceinference.
S11 and S12 denote role assignments for argu-ments a11 and a12 in one sentence, and S21 and S22 de-note for arguments a21 and a22 in another.
The ?role fac-tors score each role assignment to the arguments, and the?align factors score the compatibility of the connected ar-guments.
?sent factors encode sentence level constraints.unlikely to fill different roles for the same process.In evaporation, for example, it is highly unlikely thatwater is an undergoer in one sentence but is a re-sult in another.
These role-specific selectional pref-erences vary for each process and can be learned ifthere are enough example role fillers for each pro-cess during training (Zapirain et al, 2009; Zapirainet al, 2013).
Since, we wish to handle processes forwhich we have no training data, we approximate thisby modeling whether two arguments should receivethe same role given their similarity and their contextsimilarity.Figure 1 illustrates the cross sentence inferenceproblem using a factor graph.
The Sij random vari-ables denote the role label assignment for the jthargument in sentence i.
Each assignment to an ar-gument Sij is scored by a combination of the roleclassifier?s score (factor ?role), and its pairwise com-patibility with the assignments to other arguments(factor ?align).
The factors ?sent capture two basicwithin sentence constraints.3.2 Inference using ILPWe formulate the cross sentence inference task usingan Integer Linear Program shown in Figure 2.
TheILP seeks an assignment that maximizes a combina-tion of individual role assignment scores and theirglobal compatibility, which is measured as the simi-larity of fillers for the same role minus similarity ofarg maxz?k?i,jzijk(?
?role(aij , k)?
??
?Role classifier score+(1?
?)[?
(aij , k)??
(aij , k)]?
??
?Global compatibility)where compatibility with same roles is:?
(aij , k) =1N?k?l,mzlmk?align(aij , alm)and compatibility with other roles is:?
(aij) =2N?k?
?l,m?n6=kzlmn ?align(aij , alm)?
??
?Penalty when role n 6= ksubject to:?kzijk ?
1 ?
aij ?
sentencei?jzijk ?
1 ?
aij ?
sentencei, k ?
RN?k : Approximate number of arguments with role kN?k?
: Approximate number of arguments with role n 6= kFigure 2: An Integer Linear Program formulation of the Cross-sentence Inference.fillers of different roles.The decision variables zijk denote role assign-ments to arguments.
When zijk is set it denotes thatargument j in sentence i (aij) has been assigned rolek.
The objective function uses three components toassign scores to an assignment.1.
Classifier Score ?role(aij , k) ?
This is the scoreof a sentence-level role classifier for assigningrole k to argument aij .2.
Within Role Compatibility ?
(aij , k) ?
This isa measure of argument aij?s compatibility withother arguments which have also been assignedthe same role k. We measure compatibility us-ing a notion of alignment.
An argument is saidto align with another if they are similar to eachother in some respect (either lexically or se-mantically).
As we show later, we develop analignment cclassifier which predicts an alignmentscore ?align for each pair of arguments.
The com-patibility is defined as a normalized sum of thealignment scores for argument aij paired with1445other arguments that have also been assigned therole k. Without some normalization roles withmany arguments will receive higher compatibil-ity scores.To avoid this, we normalize by (1/N?k),where N?k refers to the number of arguments thatthe role classifier originally labeled with role k,an approximation to the number of argumentsthat are currently assigned role k by the ILP.3.
Across Role Incompatibility ?
(aij , k) ?
This isa measure of how well aij aligns with the otherarguments that are assigned a different role (n 6=k).
For good assignments this quantity should below.
Therefore we add this as a penalty to the ob-jective.
As with ?, we use an approximation fornormalization (1/N?k?
), which is the product ofother roles and the number of arguments in othersentences that can receive these roles.
BecauseN?k?
is typically higher, we boost this score by 2to balance against ?.Last, we use two sets of hard constraints to en-force the standard within-sentence expectations forroles: 1) A single argument can receive only one rolelabel, and 2) A sentence cannot have more than oneargument with the same label, except for the NONErole.We use an off-the-shelf solver in Gurobi(www.gurobi.com) to find an approximate solutionto the resulting optimization problem.3.3 Role Classifier (?role)The role classifier provides a score for each role la-bel for a given argument.
Although existing SRLand frame semantic parsers do not directly producethe role information we need (Section 2), we buildon them by using their outputs for building a processrole classifier.Before we can assign role labels, we firstneed to generate candidate arguments.
UsingEasySRL (Lewis et al, 2015), a state-of-the-art SRLsystem, we generate the candidate argument spansfor each predicate (verbs) in the sentence.
Then, us-ing a linear SVM classifier (Fan et al, 2008), wescore the candidate arguments and the predicates forour four roles and a special NONE role to indicatethe argument is not one of the four.
The classifieris trained with a set of annotated examples (see Sec-tion 4) with the following sets of features.i) Lexical and Syntactic ?
We use a small set ofstandard SRL features such as lexical and syntacticcontexts of arguments (e.g., head word, its POS tag)and predicate-argument path features (e.g, depen-dency paths).
We also add features that are specificto the nature of the process sentences.
In particular,we encode syntactic relationships of arguments withrespect to the process name mention in the sentence.We use Stanford CoreNLP toolkit (Manning et al,2014) to obtain POS tags, and dependency parses tobuild these features.ii) PropBank roles ?
While they do not have a 1-to-1 correspondence with process roles, we use theEasySRL roles coupled with the specific predicateas a feature to provide useful evidence towards theprocess role.iii) Framenet Frames ?
We use the frames evokedby the words in the sentence to allow better featuresharing among related processes.
For instance, thecontexts of undergoers in evaporation and conden-sation are likely to be similar as they are both statechanges which evoke the same Undergo Changeframe in FrameNet.iv) Query patterns ?
We use query patterns to findsentences that are likely to contain the target rolesof interest.
The query pattern that retrieved a sen-tence can help bias the classifier towards roles thatare likely to be expressed in it.3.4 Alignment Classifier (?align)Our goal with alignment is to identify arguments thatshould receive similar role labels.
One way to do thisargument alignment is to use techniques developedfor phrase level entailment or similarity which of-ten use resources such as WordNet and distributionalembeddings such as word2vec (Mikolov et al, 2013)vectors.
It turns out that this simple entailment orargument similarity, by itself, is not enough in manycases for our task2.
Moreover, the enabler, and theresult roles are often long phrases whose text-basedsimilarity is not reliable.
A more robust approach isnecessary.
Lexical and syntactic similarity of argu-ments and the context in which they are embeddedcan provide valuable additional information.
Table 3shows a complete list of features we use to train thealignment classifier.2We used an approach that combined WordNet-based phrasesimilarity method, and Word2Vec vector similarity, where thevectors were learned from a general news domain.1446LexicalEntailment score of arguments.Word2vec similarity of argument vectors.Word2Vec similarity of head nodes of arguments.Word2Vec similarity of parent of the head nodes.Word2Vec similarity of verbs of argument sentences.Jaccard similarity of children of the head node.SyntacticSimilarities of frames to right and left of arguments.Jaccard similarity of POS tags of argument.POS tag of head nodes match (boolean).POS tag of head node parents match (boolean).Similarity of dep.
path from arg to process name.Similarity of POS tags on arg to process name path.Similarity of POS tags of arg?s children.Similarity of the dependencies of the arg?s head.SentenceQuery patterns match argument sentences (boolean).Table 3: Alignment Classifier Features.
Similarities of setswere calculated using Jaccard co-efficient.Fortunately, learning this classifier does not re-quire any additional training data.
The original datawith annotated semantic role labels can be easilytransformed to generate supervision for this classi-fier.
For any given process, we consider all pairsof arguments in different sentences (i.e., (aij , alm) :i 6= l) and label them as aligned if they are labeledwith the same role, or unaligned otherwise.4 EvaluationOur goal is to generate knowledge about processesdiscussed in grade-level science exams.
Since ex-isting semantic resources such as FrameNet do notprovide adequate coverage for these, we created adataset of process sentences annotated with the fourprocess roles: undergoer, enabler, action, and result.4.1 DatasetThis dataset consists of 1205 role fillers extractedfrom 537 sentences retrieved from the web.
Wefirst compiled the target processes from a list ofprocess-oriented questions found in two collections:(i) New York Regents science exams (Clark, 2015),and (ii) helpteaching.com, a Web-based collectionQuery Patterns?name?
is the process of ?x??name?
is the process by which ?x??name?
{occurs when} ?x??name?
{ helps to | causes } ?x?Table 4: Example query patterns used to find process descrip-tion sentences.of practice questions.
Then, we identified 127 pro-cess questions from which we obtained a set of 180unique target processes.
For each target process,we queried the web using Google to find definition-style sentences, which describe the target process.For each process we discarded some noisy sentencesthrough a combination of automatic and manual fil-tering.Table 4 shows some examples of the 14 querypatterns that we used to find process descriptions.Because these patterns are not process-specific, theywork for unseen processes as well.To find role fillers from these sentences, we firstprocessed each sentence using EasySRL (Lewis etal., 2015) to generate candidate arguments.
Some ofthe query patterns can be used to generate additionalarguments.
For example, in the pattern ??name?
isthe process of ?x??
if ?x?
is a noun then it is likely tobe an undergoer, and thus can be a good candidate.3.
Then two annotators annotated the candidate ar-guments with the target roles if one were applicableand marked them as NONE otherwise.
Disagree-ments were resolved by a third annotator.
The an-notations spanned a random subset of 54 target pro-cesses.
The role label distribution is shown below:Role No.
of instanceUndergoer 77Enabler 154Action 315Result 194NONE 465Table 5: Role distributionWe conducted five fold cross validation experi-ments to test role extraction.
To ensure that we aretesting the generalization of the approach to unseen3These patterns are ambiguous and are not adequate by them-selves for accurately extracting the roles.
We use them as fea-tures.1447processes, we generated the folds such that the pro-cesses in the test fold were unseen during training.We compared the basic role classifier described inSection 3.3, the within sentence and the cross sen-tence inference models.
We tune the ILP parame-ter ?
for cross sentence inference based on a coarse-grained sweep on the training folds.We also compared with a simple baseline thatlearned a mapping from PropBank roles producedby EasySRL system to the process roles by usingthe roles and the verb as features.
We also add theFrameNet frames invoked by the lexical unit in thesentence.
Note this is essentially a subset of thefeatures we use in our role classifier.
As a sec-ond baseline, we compare with a (nearly) out-of-the-box application of SEMAFOR (Das et al, 2010), aFrameNet based frame-semantic parser.
We modi-fied SEMAFOR to override the frame identificationstep since the process frame information is alreadyassociated with the test sentences.4.2 Cross-Sentence Inference ResultsTable 6 compares performance of the different meth-ods.
The learned role mapping of shallow seman-tic roles performs better than SEMAFOR but worsethan the simple role classifier.
SEMAFOR uses alarge set of features which help it scale for a di-verse set of frames in FrameNet.
However, manyof these many not be well suited for the process sen-tences in our relatively smaller dataset.
Therefore,we use our custom role classifier as a strong base-line to demonstrate within and cross sentence gains.Enforcing sentence-level consistency through jointMethod Prec.
Rec.
F1Role mapping 56.62 59.60 58.07SEMAFOR 40.72 50.54 45.10Role class.
(?role) 78.48 78.62 78.55+ within sent.
86.25 73.91 79.60+ cross sent.
89.84 75.36 81.97?
?Table 6: Process role inference performance.
??
indicatessignificant improvement over Role + within sentence system.inference, shown as (+within sent.
), improves overthe baseline which does not use any consistency.
Itgains precision (by nearly 8 points), while loosingrecall in the trade-off (by about 4.7 points) to yieldan overall gain in F1 by 1.05 points.
Enforcing crosssentence consistency, shown as (+cross sent.)
pro-vides additional gains beyond within sentence infer-ence by another 2.38 points in F1 4.
Table 7 showshow the gains are distributed across different roles.Cross sentence inference provides improvements forall roles, with the biggest for undergoers (nearly 4points).Method Und.
Ena.
Act.
Res.Role Class.
65.38 73.84 83.58 77.30+ within 66.01 73.11 86.70 76.11+ cross 70.00 74.31 89.30 78.00Table 7: Performance (F1) across all roles.Figure 3 shows the precision/recall plots for thebasic role classifier and within and cross sentence in-ference.
Both inference models trade recall for gainsin precision.
Cross sentence yields higher precisionat most recall levels, for a smaller overall loss in re-call compared to within sentence (1.6 versus 4.9).0.0 0.2 0.4 0.6 0.8 1.0Recall0.70.80.91.0Precison Role Classifier+ within sent.+ cross sent.Figure 3: Precision/Recall trade-offs for process role inference.y-axis is truncated at 0.7 to better visualize the differences.4.3 AblationsTable 8 shows the utility of various components ofcross sentence inference.
Using argument entail-ment alone turns out to be ineffective and only pro-duces a minor improvement (0.16 in F1).
How-ever, the alignment classifier scores are much moreeffective and yield about 2.37 points gain in F1.Both within and across role compatibilities, ?
and?, yield statistically significant improvements5 over4The single parameter in ILP turned out to be stable across thefolds and obtained this best value at ?
= 0.8.5Significance measured using approximate randomization test14480 10 20 30 40 50# Arguments?10123F1GainsFigure 4: Cross sentence gains in F1 when varying the numberof most similar arguments used to assess compatibilities.within sentence inference.
Combining these com-plementary compatibilities provides the best gains.Method Prec.
Rec.
F1within sent.
86.25 73.91 79.60+ Entailmentcross sent.
w/ ?
85.13 72.64 78.39cross sent.
w/?
85.98 73.36 79.17cross sent.
w/ ?
+?
86.62 73.91 79.76+ Alignment Classifiercross sent.
w/ ?
89.07 75.36 81.64?
?cross sent.
w/?
88.72 75.54 81.60?
?cross sent.
w/ ?
+?
89.84 75.36 81.97?
?Table 8: Performance impact of inference components.
?
?indicates significant improvement over within sentence.We also studied the effect of varying the numberof arguments that ILP uses to measure the compat-ibility of role assignments.
Specifically, we allowinference to use just the top k alignments from thealignment classifier.
Figure 4 shows the main trend.Using just the top similar argument already yields a1 point gain in F1.
Using more arguments tends toincrease gains in general but with some fluctuations.Finding an assignment that respects all compatibili-ties across many argument pairs can be difficult.
Asseen in the figure, at some of the shorter span lengthswe see a slightly larger gain (+0.3) compared to us-ing all spans.
This hints at benefits of a more flexibleformulation that makes joint decisions on alignmentand role label assignments.Table 9 shows an ablation of the alignment clas-sifier features.
Entailment of arguments is the mostinformative feature for argument alignment.
Addinglexical and syntactic context compatibilities addssignificant boosts in precision and recall.
Know-ing that the arguments are retrieved by the samequery pattern (sentence feature) only provides mi-nor improvements.
Even though the overall classi-fication performance is far from perfect, cross sen-tence can benefit from alignment as long as it pro-vides a higher score for argument pairs that shouldalign compared to those that should not.Feature P R F1Entailment score only 39.55 14.59 21.32+Lexical 50.75 26.02 34.40+Syntactic 62.31 31.47 41.82+Sentence 62.33 31.41 41.53Table 9: Performance of different feature groups for alignment.4.3.1 Error AnalysisWe conduct an error analysis over a random setof 50 errors observed for cross sentence inference.In addition to issues from noisy web sentences andnested arguments from bad candidate extraction, wefind the following main types of errors:?
Dissimilar role fillers (27.5 %) ?
In some pro-cesses, the fillers for the result role have highlevels of variability that makes alignment errorprone.
For the process camouflage, for instance,the result roles include ?disorientation?, ?protectfrom predator?, ?remain undetected?
etc.?
Bad role classifier scores (37.5%) ?
For some in-stances the role classifier assign high scores to in-correct labels, effectively preventing the ILP fromflipping to the correct role.
For example, the ar-gument that follows ?causes?
tends to be a re-sult in many cases but not always, leading to highscoring errors.
For example, in the sentence with?...when heat from the sun causes water on earth?s...?, the role classifier incorrectly assigns ?water?to a result role with high confidence.?
Improper Weighting (7.5%)?
Sometimes the ILPdoes not improve upon a bad top choice fromthe role classifier.
In some of these cases, ratherthan the fixed lambda, a different weighted com-bination of role and alignment classifier scores1449would have helped the ILP to flip.
For example,the argument ?under autumn conditions?
from thesentence ?hibernation occurs when the insects aremaintained under autumn conditions.?
has a goodrole score and is similar to other correctly labeledenablers such as ?cold , winter conditions?
but yetis unable to improve.The rest (27.5 %) are due to noisy web sentences,incorrect argument extraction and errors outside thescope of cross sentence inference.5 ConclusionsSimple role-based knowledge is essential for rec-ognizing and reasoning about situations involvingprocesses.
In this work we developed a cross sen-tence inference method for automatically acquiringsuch role-based knowledge for new processes.
Themain idea is to enforce compatibility among rolesextracted from sentences belonging to a single pro-cess.
We find that the compatibility can be effec-tively assessed using an alignment classifier builtwithout any additional supervision.
Empirical eval-uation on a process dataset shows that cross sentenceinference using an Integer Linear Program helps im-prove the accuracy of process knowledge extraction.6 AcknowledgementThe authors would like to thank the anonymous re-viewers for helpful comments, Meghana Kshirsagar,Sam Thomson, Mike Lewis for answering imple-mentation details of their systems, and the StonyBrook NLP Lab members for their valuable feed-back and suggestions.
This work is supported inpart by Foreign Fulbright PhD Fellowship and bythe grant from Allen Institute for Artificial Intelli-gence.ReferencesCollin F Baker, Charles J Fillmore, and John B Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 17th international conference on Computa-tional linguistics-Volume 1, pages 86?90.
Associationfor Computational Linguistics.Jonathan Berant, Vivek Srikumar, Pei-Chun Chen,Abby Vander Linden, Brittany Harding, Brad Huang,Peter Clark, and Christopher D. Manning.
2014.Modeling biological processes for reading comprehen-sion.
In Proceedings of EMNLP.Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 43?48.
Association for Computational Linguis-tics.Peter Clark.
2015.
Elementary school science and mathtests as a driver for ai: Take the aristo challenge.
toappear.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Proc.
of NAACL-HLT.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
The Journal of MachineLearning Research, 9:1871?1874.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graphalignment for semi-supervised semantic role labeling.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 11?20, Singapore.Hagen Fu?rstenau and Mirella Lapata.
2012.
Semi-supervised semantic role labeling via structural align-ment.
Computational Linguistics, 38(1):135?171.Sujay Kumar Jauhar, Peter D. Turney, and Eduard H.Hovy.
2016.
Tables as semi-structured knowledge forquestion answering.
In ACL.Paul Kingsbury and Martha Palmer.
2003.
Propbank: thenext level of treebank.
In Proceedings of Treebanksand lexical Theories, volume 3.
Citeseer.Ivan Titov Alexandre Klementiev.
2012.
Semi-supervised semantic role labeling: Approaching froman unsupervised perspective.
In Proceedings of theCOLING Conference.Meghana Kshirsagar, Sam Thomson, Nathan Schneider,Jaime G. Carbonell, Noah A. Smith, and Chris Dyer.2015.
Frame-semantic role labeling with heteroge-neous annotations.
In ACL.Joel Lang and Mirella Lapata.
2010.
Unsupervised in-duction of semantic roles.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 939?947, Los Angeles, Cal-ifornia, June.
Association for Computational Linguis-tics.Joel Lang and Mirella Lapata.
2011.
Unsupervised se-mantic role induction with graph partitioning.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 1320?1331, Ed-inburgh, Scotland, UK., July.
Association for Compu-tational Linguistics.Mike Lewis, Luheng He, and Luke Zettlemoyer.
2015.Joint a* ccg parsing and semantic role labelling.
InEmpirical Methods in Natural Language Processing.1450Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Rose Finkel, Steven Bethard, and David Mc-Closky.
2014.
The stanford corenlp natural languageprocessing toolkit.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Luiz Augusto Pizzato and Diego Molla?.
2008.
Indexingon semantic roles for question answering.
In Coling2008: Proceedings of the 2nd workshop on Informa-tion Retrieval for Question Answering, pages 74?81.Association for Computational Linguistics.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-mak.
2004.
Semantic role labeling via integer linearprogramming inference.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics,COLING ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.Michael Roth and Mirella Lapata.
2015.
Context-aware frame-semantic role labeling.
Transactions ofthe Association for Computational Linguistics (TACL),3:449?460.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2010.
Semeval-2010 task 10: Linking events and their participantsin discourse.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, SemEval?10, pages 45?50, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Dan Shen and Mirella Lapata.
2007.
Using seman-tic roles to improve question answering.
In EMNLP-CoNLL, pages 12?21.Robert S Swier and Suzanne Stevenson.
2004.
Unsuper-vised semantic role labelling.
In EMNLP.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semantic rolelabeling.
In ACL.Ben?at Zapirain, Eneko Agirre, and Llu?
?s Ma`rquez i Villo-dre.
2009.
Generalizing over lexical features: Selec-tional preferences for semantic role classification.
InACL.Ben?at Zapirain, Eneko Agirre, Llu?
?s Ma`rquez i Villodre,and Mihai Surdeanu.
2013.
Selectional preferencesfor semantic role classification.
Computational Lin-guistics, 39:631?663.1451
