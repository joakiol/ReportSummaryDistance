DLUT: Chinese Personal Name Disambiguation with RichFeaturesDongliang WangDepartment of Computer Scienceand Engineering, Dalian Universityof Technologywdl129@163.comDegen HuangDepartment of Computer Scienceand Engineering, Dalian Universityof Technologyhuangdg@dlut.edu.cnAbstractIn this paper we describe a person clus-tering system for a given document setand report the results we have obtainedon the test set of Chinese personal name(CPN) disambiguation task of CIPS-SIGHAN 2010.
This task consists ofclustering a set of Xinhua news docu-ments that mention an ambiguous CPNaccording to named entity in reality.Several features including named entities(NE) and common nouns generated fromthe documents and a variety of rules areemployed in our system.
This systemachieves F = 86.36% with B_Cubedscoring metrics and F = 90.78% with pu-rity_based metrics.1 IntroductionAs the amount of web information expands at anever more rapid pace, extraction of informationfor specific named entity is more and more im-portant.
Usually there are named-entity ambigu-ity in web data, for example more than one per-son use a same name, therefore it is difficult todecide which document refers to a specificnamed entity.The goal of CPN disambiguation is to cluster-ing input Xinhua news corpus by the entity eachdocument refers to.
The new documents whichspan a time of fourteen years are extracted onweb.As description of CPN disambiguation task ofCIPS-SIGHAN 2010, Chinese personal namedisambiguation is potentially more challengingdue to the need for word segmentation, whichcould introduce errors that can in large part beavoided in the English task.In this paper we employ a CPN disambigua-tion system that extracts NE and common nounsfrom the input corpus as features, and then com-putes the similarity of each two documents in thecorpus based on feature vector.
Hierarchical Ag-glomerative Clustering (HAC) algorithm (AKJain et al, 1999) is used to implement clustering.After a great deal of analysis of news corpus,we constitute several rules, the experimentsshow that these rules can improve the result ofthis task.The remainder of this paper is organized asfollows.
Section 2 introduces the preprocessingof test corpus, and in section 3 we present themethodology of our system.
In section 4 we pre-sent the experimental results and give a conclu-sion in section 5.2 PreprocessingIn this step, we mainly complete the works asfollows.Firstly, corpuses including a given namestring are in different files, one document onefile.
In order to convenient for processing, wecombine these documents into one file, distin-guish them with document id.Secondly, some news corpuses have severalsubtitles but usually only part of them includingfocused name string, the others are noise of dis-ambiguate of focused named entity, for examplea news about sports may contain several subti-tles about basketball, swimming, race and so on.These noises are removed from the corpus by us.Lastly, there is a lack of date-line in a fewdocuments; in general, these data-lines are rec-ognized as part of text, they can be recognizedthrough simple matching method.
Because data-lines have consistent format as ????**?*??
?.3 MethodologyThe system follows a procedure include: wordsegmentation, the detection of ambiguous ob-jects, feature extractions, computation of docu-ment similarity and clustering.First, the text is segmented by a word segmen-tation system explored by Luo and Huang(2009).
The second step is extract all featuresfrom segmented text, all features are put intotwo feature vectors: NE vector and commonnoun vector.
Then we will compute the distancebetween corresponding vectors of each twodocuments, the standard SoftTFIDF (Chen andMartin, 2007) are employed to compute the dis-tance between two feature vectors.
Lastly, weuse the HAC algorithm for clustering of docu-ments.3.1 Word SegmentationWord segmentation is a base and difficult workof natural language processing (NLP) and aprecondition of feature extraction.
In this paper,the word segmentation system explored by Luoand Huang (2009) are employed to do this work.This system training on the corpus of 2000?s?People?s Daily?.
In addition, this system canrecognize named entities including personalname, location name and organization name.
Wecan extract these NEs by part-of-speech (POS)directly.3.2 The Detection of Ambiguous EntitiesGiven a name string, the documents can be di-vided into three groups:(1) Documents which contain names that areexactly match the query name string.
(2) Documents which contain names that havea substring exactly match the query name string.
(3) Documents which contain the query namestring that is not personal name.After word segmentation, all personal namesare labeled by system, when we find one per-sonal name or its substring match the queryname string; we will cluster this document ac-cording to the name.
If we failure all over thedocument, it?s considered that this documentbelong to category (3), it will be discarded.The ambiguous personal name in a documentmay refer to multiple entities, for example anews about party of namesakes, but this is a verysmall probability event, so we assume that allmentions in one document refers to the sameentity, viz.
?one person one document?.Although we assume that ?one person onedocument?, the same personal name may occurmore than once.
Some times the word segmenta-tion system will give the same personal namedifferent labels in one document, for example apersonal name ?????
may be recognized as????
and ?????
in different sentence inone document.
Suppose that P1, P2, ?
, Pn arerecognized names that match the query namestring, T1, T2, ?
, Tn are the corresponding oc-cur times.
We use the following method to en-sure the final needed personal name:(1) If Ti > Tj for j = 1, 2, ?, i-1, i+1, ?
, n,Pi is selected as the final needed personal name,else go to step (2).
(2) Define S = { T1, T2, ?
, Tn }, E1 = {T11,T12, ?, T1m}, E2 = S ?
E1 satisfying T11 = T12= ...= T1m, E1 ?
S and Ti > Tj (Ti ?
E1, Tj ?
E2).Fi shows the word before Pi and Bi after Pi.
Foreach Ti ?
E1, connect Fi, Ti and Bi into a newstring named Ri, we can get R = {R11, R12, ?,R1m} corresponding to E1, the longest commonsubstring of R are considered the final neededpersonal name.3.3 FeaturesWe define local sentence as sentences whichcontain the query name string, the features ex-tracted from local sentences named local fea-tures.
Otherwise, all sentences except local sen-tences in a document are named global sentences;the features extracted from global sentences areglobal features.
The reason to distinguish them isbecause they have different contribution to simi-larity computation.
Local features are generallyconsidered more important than global features,therefore a high weight should be given to localfeatures.Named entities are important informationabout focused name.
In this paper, NEs includepersonal names, location names and organizationnames.
Location name and organization nameusually indicate the region and department offocused name, and personal names usually havehigh co-occurrence rate, for example ????
?and ????
are two names of table tennis players,so they always appear in a same news documentabout table tennis.
The NE features which havebeen tagged by segmentation system can be ex-tracted from the document directly.We also consider the features of commonnouns.
Semantically independent common nounssuch as person?s job and person?s hobby etc usu-ally include some useful information about theambiguous object.
We attempt to capture thesenoun features and use them as elements in fea-ture vector.Location names in data-line.
The locationname in the data-line indicates the place thenews had occurred, if two documents have thesame date-line location name, and then there is agood chance that these two documents refer thesame person.Appellation of query name.
Appellation usu-ally demonstrate a person?s identity, for example,if the appellation of the query name is ???
?, itshows that he or she is a journalist.
As locationnames in data-line, if two query names have thesame appellation, the possibility of them refer tothe same person increased.
The word segmenta-tion system doesn?t clearly marked out appella-tion but marked as common noun.
In generally,appellations appear neighbor in front of name, sowe collect the common nouns neighbor front ofquery names as their appellations.So far, we have developed four feature vec-tors: local NE vector, local common noun vector,global NE vector and global common noun vec-tor.
Given feature vectors, we need to find a wayto learn the similarity matrix.
In this paper, wechoose the standard TF-IDF method to calculatethe similarity matrix.
Location name in date-lineand appellation of query name will be used inrule method without similarity calculation.3.4 Similarity MatrixGiven a pair of feature vectors consisting of NEsor common nouns, we need to choose a similar-ity scheme to calculate the similarity matrix.
Thestandard TF-IDF method is introduced here, thena little change for Chinese string.Standard TF-IDF: Given a pair of vector Sand T, S = (s1, s2, ?, sn), T = (t1, t2, ?, tm).Here, si (i = 1, ?, n) and tj (j = 1, ?, m) are NEor common noun.
We define:}),(,,;{);;(??>??
?=vwdistTvSwwTSCLOSE(1)Where dist(w;v) is the Jaro-Winkler dis-tance function (Winkler, 1999), which willbe introduced later.
);(max);( vwdistTwD Tv?=              (2)Then the standard TF-IDF SoftTFIDF is com-puted as:=),( TSSoftTFIDF),(*),(*),();;( TwDTwVSwVTSCLOSEw?
?
?
(3)?
?=SwSwVSwVSwV2''),(),(),(            (4))log(*)1log(),(,'wSw IDFTFSwV +=    (5)Where SwTF ,  is the frequency of substringw in S, and wIDF is the inverse of the fractionof documents in the corpus that contain w .
Sup-pose Nt is total number of documents, Nw is totalnumber of documents which contain w .
ThenwIDF  computed as:wtNNIDF =?
(6)The Jaro-Winkler distance Jw of two givenstrings s1 and s2 as shown in formula (7), l isthe length of common prefix at the start of thestring up to a maximum of 4 characters, p is aconstant scaling factor for how much the score isadjusted upwards for having common prefixes,the value for p is 0.1.
)1( jjw dlpdd ?+=                    (7))|2||1|( mtmsmsmd j?++=              (8)In formula (8) m is the number of matchingcharacters, t is the number of transpositions.
Inorder to be consistent with the English strings, aChinese character is seen as two English charac-ters.Corresponding to four feature vectors, we cancalculate the four similarities: S(gNE), S(gCN),S(lNE), S(lCN).
The similarity between twodocuments (DS) is computed as:2)()(*)1(2)()(*gCNSlCNSgNESlNESDS+?++=??
(9)As time is tight, we just give ?
a value of 0.8with out experiment because we consider NEshave stronger instructions.3.5 ClusteringClustering is a key work of this task, it is veryimportant to choose a clustering algorithm.
Herewe use HAC algorithm to do clustering.
HACalgorithm is an unsupervised clustering algo-rithm, which can be described as follows:(1) Initialization.
Every document is re-garded as a separate class.
(2) Repetition.
Computing the similarity ofeach of the two classes, merge the two classeswhose similarity are the highest and higher thanthe threshold value of ?
into a new class.
(3) Termination.
Repeat step (2) until allclasses don?t satisfy the clustering condition.Suppose document class F = {f1, f2, ?, fn}and K = {k1, k2, ?, km}, fi and kj are documentsin class F and class K, then the similarity be-tween F and K is:nmkfSKJS ji ji*),(),( ,?=                  (9)If two documents have different query name,obviously they refer to different person, onlydocuments which have same query name will beclustered.
Before clustering, several rules areafforded to improve the clustering condition.These rules are generally applicable to newscorpus.
(1) If two documents have the same queryname and both of them are reporter, and bothdate-lines have the same location name, thencombine the two documents into one class.
(2) If two documents have the same queryname and another same personal name, thencombine the two documents into one class.
(3) If two documents have the same queryname and both date-lines have the same locationname, then double the similarity, else halve thesimilarity.
(4) If two documents have the same queryname and both personal names have the sameappellation, then double the similarity, else halvethe similarity.4 EvaluationIn order to prove the validity of the rule ap-proach, a group of experiments are performed onthe train set of Chinese personal name disam-biguation task of CIPS-SIGHAN 2010.
The re-sult is shown in Table 1.
R1 is the result withoutrules, and R2 shows the accuracy after addingthe rules.The system performance on the test set ofCPN disambiguation task of CIPS-SIGHAN2010 is F = 90.78% evaluated with P_IP evalua-tion, and F = 86.36% with B_Cubed evaluation.The accuracy is shown in Table 2.B_Cubed Precision Recall FR1 70.56 86.77 74.74R2 78.05 84.99 79.60P_IP Purity InversePurityFR1 77.22 90.48 81.20R2 82.92 88.30 84.29Table 1.
Experimental results for system withrules and without rules on training setB_Cubed Precision Recall F82.96 91.33 86.36P_IP Purity InversePurityF87.94 94.21 90.78Table 2.
The results on test set5  ConclusionWe described our system that disambiguatesChinese personal names in Xinhua corpus.
Wemainly focus on extracting rich features fromdocuments and computing the similarity of eachtwo documents.
Several rules are introduced toimprove the accuracy and have proved effective.ReferencesAnil K. Jain, M. Narasimha Murty, and Patrick J.Flynn.
1999.
Data clustering: A review.
ACMComputing Surveys, 31(3): 264-323.Bradley Malin.
2005.
Unsupervised Name Disam-biguation via Network Similarity.
In proceedingsSIAM Conference on Data Mining, 2005.Chen Ying, James Martin.
2007.
CU-COMSEM: Ex-ploring Rich Features for Unsupervised Web Per-sonal Name Disambiguation.
In proceedings ofSemeval 2007, Association for ComputationalLinguistics, 2007.Chen Ying, Sophia Y. M. Lee and Churen Huang.2009.
PolyUHK:A Robust Information ExtractionSystem for Web Personal Names.
In proceedingsof Semeval 2009, Association for ComputationalLinguistics, 2009.Gusfield, Dan.
1997.
Algorithms on Strings, Treesand Sequences.
Cambridge University Press,Cambridge, UKJavier Artiles, J. Gonzalo and S. Sekine.
WePS2Evaluation Campaign: Overview of the Web Peo-ple Search Clustering Task.
In proceedings of Se-meval 2009, Association for Computational Lin-guistics, 2009.Luo Yanyan, Degen Huang.
2009.
Chinese word seg-mentation based on the marginal probabilitiesGenerated by CRFs.
Journal of Chinese Informa-tion Processing, 23(5): 3-8.Octavian Popescu, B. Magnini.
2007.
IRST-BP: WebPeople Search Using Name Entities.
In proceed-ings of Semeval 2007, Association for Computa-tional Linguistics, 2007.William E. Winkler.
1999.
The state of record linkageand current research problems.
Statistics of In-come Division, Internal Revenue Service Publica-tion R99/04.
