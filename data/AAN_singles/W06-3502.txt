Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 9?16,New York City, June 2006. c?2006 Association for Computational LinguisticsBackbone Extraction and Pruning for Speeding Up a Deep Parser forDialogue SystemsMyroslava O. DzikovskaHuman Communication Research CentreUniversity of EdinburghEdinburgh, EH8 9LW, United Kingdom,mdzikovs@inf.ed.ac.ukCarolyn P. Rose?Carnegie Mellon UniversityLanguage Technologies Institute,Pittsburgh, PA 15213,cprose@cs.cmu.eduAbstractIn this paper we discuss issues related tospeeding up parsing with wide-coverageunification grammars.
We demonstratethat state-of-the-art optimisation tech-niques based on backbone parsing beforeunification do not provide a general so-lution, because they depend on specificproperties of the grammar formalism thatdo not hold for all unification based gram-mars.
As an alternative, we describe anoptimisation technique that combines am-biguity packing at the constituent structurelevel with pruning based on local features.1 IntroductionIn this paper we investigate the problem of scalingup a deep unification-based parser developed specif-ically for the purpose of robust interpretation in dia-logue systems by improving its speed and coveragefor longer utterances.
While typical sentences in di-alogue contexts are shorter than in expository textdomains, longer utterances are important in discus-sion oriented domains.
For example, in educationalapplications of dialogue it is important to elicit deepexplanation from students and then offer focusedfeedback based on the details of what students say.The choice of instructional dialogue as a target ap-plication influenced the choice of parser we neededto use for interpretation in a dialogue system.
Sev-eral deep, wide-coverage parsers are currently avail-able (Copestake and Flickinger, 2000; Rose?, 2000;Baldridge, 2002; Maxwell and Kaplan, 1994), butmany of these have not been designed with issues re-lated to interpretation in a dialogue context in mind.The TRIPS grammar (Dzikovska et al, 2005) is awide-coverage unification grammar that has beenused very successfully in several task-oriented di-alogue systems.
It supports interpretation of frag-ments and lexical semantic features (see Section 2for a more detailed discussion), and provides addi-tional robustness through ?robust?
rules that covercommon grammar mistakes found in dialogue suchas missing articles or incorrect agreement.
Theseenhancements help parsing dialogue (both spokenand typed), but they significantly increase grammarambiguity, a common concern in building grammarsfor robust parsing (Schneider and McCoy, 1998).
Itis specifically these robustness-efficiency trade-offsthat we address in this paper.Much work has been done related to enhanc-ing the efficiency of deep interpretation systems(Copestake and Flickinger, 2000; Swift et al, 2004;Maxwell and Kaplan, 1994), which forms the foun-dation that we build on in this work.
For example,techniques for speeding up unification in HPSG leadto dramatic improvements in efficiency (Kiefer etal., 1999).
Likewise ambiguity packing and CFGbackbone parsing (Maxwell and Kaplan, 1994; vanNoord, 1997) are known to increase parsing effi-ciency.
However, as we show in this paper, thesetechniques depend on specific grammar propertiesthat do not hold for all grammars.
This claim is con-sistent with observations of Carroll (1994) that pars-ing software optimisation techniques tend to be lim-ited in their applicability to the individual grammarsthey were developed for.
While we used TRIPS asour example unification-based grammar, this inves-tigation is important not only for this project, but inthe general context of speeding up a wide-coverageunification grammar which incorporates fragment9rules and lexical semantics, which may not be im-mediately provided by other available systems.In the remainder of the paper, we begin with abrief description of the TRIPS parser and grammar,and motivate the choice of LCFLEX parsing algo-rithm to provide a fast parsing foundation.
We thendiscuss the backbone extraction and pruning tech-niques that we used, and evaluate them in compar-ison with the original parsing algorithm.
We con-clude with discussion of some implications for im-plementing grammars that build deep syntactic andsemantic representations.2 MotivationThe work reported in this paper was done as partof the process of developing a dialogue system thatincorporates deep natural language understanding.We needed a grammar that provides lexical seman-tic interpretation, supports parsing fragmentary ut-terance in dialogue, and could be used to start de-velopment without large quantities of corpus data.TRIPS fulfilled our requirements better than sim-ilar alternatives, such as LINGO ERG (Copestakeand Flickinger, 2000) or XLE (Maxwell and Kaplan,1994).TRIPS produces logical forms which include se-mantic classes and roles in a domain-independentframe-based formalism derived from FrameNet andVerbNet (Dzikovska et al, 2004; Kipper et al,2000).
Lexical semantic features are known to behelpful in both deep (Tetreault, 2005) and shal-low interpretation tasks (Narayanan and Harabagiu,2004).
Apart from TRIPS, these have not been in-tegrated into existing deep grammars.
While bothLINGO-ERG and XLE include semantic featuresrelated to scoping, in our applications the avail-ability of semantic classes and semantic role as-signments was more important to interpretation,and these features are not currently available fromthose parsers.
Finally, TRIPS provides a domain-independent parse selection model, as well as rulesfor interpreting discourse fragments (as was alsodone in HPSG (Schlangen and Lascarides, 2003), afeature actively used in interpretation.While TRIPS provides the capabilities we need,its parse times for long sentences (above 15 wordslong) are intolerably long.
We considered two pos-sible techniques for speeding up parsing: speedingup unification using the techniques similar to theLINGO system (Copestake and Flickinger, 2000),or using backbone extraction (Maxwell and Ka-plan, 1994; Rose?
and Lavie, 2001; Briscoe and Car-roll, 1994).
TRIPS already uses a fast unificationalgorithm similar to quasi-destructive unification,avoiding copying during unification.1 However,the TRIPS grammar retains the notion of phrasestructure, and thus it was more natural to chose touse backbone extraction with ambiguity packing tospeed up the parsing.As a foundation for our optimisation work, westarted with the freely available LCFLEX parser(Rose?
and Lavie, 2001).
LCFLEX is an all-pathsparser that uses left-corner prediction and ambigu-ity packing to make all-paths parsing tractable, andwhich was shown to be efficient for long sentenceswith somewhat less complex unification augmentedcontext-free grammars.
We show that all-paths pars-ing with LCFLEX is not tractable for the ambiguitylevel in the TRIPS grammar, but that by introduc-ing a pruning method that uses ambiguity packing toguide pruning decisions, we can achieve significantimprovements in both speed and coverage comparedto the original TRIPS parser.3 The TRIPS and LCFLEX algorithms3.1 The TRIPS parserThe TRIPS parser we use as a baseline is a bottom-up chart parser with lexical entries and rules repre-sented as attribute-value structures.
To achieve pars-ing efficiency, TRIPS uses a best-first beam searchalgorithm based on the scores from a parse selectionmodel (Dzikovska et al, 2005; Elsner et al, 2005).The constituents on the parser?s agenda are groupedinto buckets based on their scores.
At each step, thebucket with the highest scoring constituents is se-lected to build/extend chart edges.
The parsing stopsonce N requested analyses are found.
This guaran-tees that the parser returns the N -best list of analysesaccording to the parse selection model used, unlessthe parser reaches the chart size limit.1Other enhancements used by LINGO depend on disallow-ing disjunctive features, and relying instead on the type system.The TRIPS grammar is untyped and uses disjunctive features,and converting it to a typed system would require as yet unde-termined amount of additional work.10In addition to best-first parsing, the TRIPS parseruses a chart size limit, to prevent the parser fromrunning too long on unparseable utterances, similarto (Frank et al, 2003).
TRIPS is much slower pro-cessing utterances not covered in the grammar, be-cause it continues its search until it reaches the chartlimit.
Thus, a lower chart limit improves parsingefficiency.
However, we show in our evaluation thatthe chart limit necessary to obtain good performancein most cases is too low to find parses for utteranceswith 15 or more words, even if they are covered bythe grammar.The integration of lexical semantics in the TRIPSlexicon has a major impact on parsing in TRIPS.Each word in the TRIPS lexicon is associated with asemantic type from a domain-independent ontology.This enables word sense disambiguation and seman-tic role labelling for the logical form produced bythe grammar.
Multiple word senses result in addi-tional ambiguity on top of syntactic ambiguity, but itis controlled in part with the use of weak selectionalrestrictions, similar to the restrictions employed bythe VerbNet lexicon (Kipper et al, 2000).
Check-ing semantic restrictions is an integral part of TRIPSparsing, and removing them significantly decreasesspeed and increases ambiguity of the TRIPS parser(Dzikovska, 2004).
We show that it also has an im-pact on parsing with a CFG backbone in Section 4.1.3.2 LCFLEXThe LCFLEX parser (Rose?
and Lavie, 2001) is anall-paths robust left corner chart parser designed toincorporate various robustness techniques such asword skipping, flexible unification, and constituentinsertion.
Its left corner chart parsing algorithmis similar to that described by Briscoe and Carroll(1994).
The system supports grammatical specifi-cation in a unification framework that consists ofcontext-free grammar rules augmented with featurebundles associated with the non-terminals of therules.
LCFLEX can be used in two parsing modes:either context-free parsing can be done first, fol-lowed by applying the unification rules, or unifica-tion can be done interleaved with context-free pars-ing.
The context free backbone allows for efficientleft corner predictions using a pre-compiled left cor-ner prediction table, such as that described in (vanNoord, 1997).
To enhance its efficiency, it incor-porates a provably optimal ambiguity packing algo-rithm (Lavie and Rose?, 2004).These efficiency techniques make feasible all-path parsing with the LCFLEX CARMEL grammar(Rose?, 2000).
However, CARMEL was engineeredwith fast all-paths parsing in mind, resulting in cer-tain compromises in terms of coverage.
For exam-ple, it has only very limited coverage for noun-nouncompounding, or headless noun phrases, which are amajor source of ambiguity with the TRIPS grammar.4 Combining LCFLEX and TRIPS4.1 Adding CFG BackboneA simplified TRIPS grammar rule for verb phrasesand a sample verb entry are shown in Figure 1.
Thefeatures for building semantic representations areomitted for brevity.
Each constituent has an assignedcategory that corresponds to its phrasal type, and aset of (complex-valued) features.The backbone extraction algorithm is reason-ably straightforward, with CFG non-terminals cor-responding directly to TRIPS constituent categories.To each CFG rule we attach a corresponding TRIPSunification rule.
After parsing is complete, theparses found are scored and ordered with the parseselection model, and therefore parsing accuracy inall-paths mode is the same or better than TRIPS ac-curacy for the same model.For constituents with subcategorized arguments(verbs, nouns, adverbial prepositions), our back-bone generation algorithm takes the subcategoriza-tion frame into account.
For example, the TRIPSVP rule will split into 27 CFG rules correspondingto different subcategorization frames: VP?
V intr,VP?
V NP NP, VP?
V NP CP NP CP, etc.
Foreach lexical entry, its appropriate CFG category isdetermined based on the subcategorization framefrom TRIPS lexical representation.
This improvesparsing efficiency using the prediction algorithms inTFLEX operating on the CFG backbone.
The ver-sion of the TRIPS grammar used in testing con-tained 379 grammar rules with 21 parts of speech(terminal symbols) and 31 constituent types (non-terminal symbols), which were expanded into 1121CFG rules with 85 terminals and 36 non-terminalsduring backbone extraction.We found, however, that the previously used tech-11(a) ((VP (SUBJ ?
!subj) (CLASS ?lf))-vp1-role .99(V (LF ?lf) (SUBJ ?
!subj) (DOBJ ?dobj)(IOBJ ?iobj) (COMP3 ?comp3))?iobj ?dobj ?comp3)(b) ((V (agr 3s) (LF LF::Filling)(SUBJ (NP (agr 3s)))(DOBJ (NP (case obj))) (IOBJ -) (COMP3 -)))Figure 1: (a) A simplified VP rule from the TRIPSgrammar; (b) a simplified verb entry for a transitiveverb.
Question marks denote variables.nique of context-free parsing first followed by fullre-unification was not suitable for parsing with theTRIPS grammar.
The CFG structure extracted fromthe TRIPS grammar contains 43 loops resultingfrom lexical coercion rules or elliptical construc-tions.
A small number of loops from lexical coer-cion were both obvious and easy to avoid, becausethey are in the form N?
N. However, there werelonger loops, for example, NP ?
SPEC for sen-tences like ?John?s car?
and SPEC ?
NP for head-less noun phrases in sentences like ?I want three?.LCFLEX uses a re-unification algorithm that asso-ciates a set of unification rules with each CFG pro-duction, which are reapplied at a later stage.
Tobe able to apply a unification rule corresponding toN?
N production, it has to be explicitly present inthe chart, leading to an infinite number of N con-stituents produced.
Applying the extra CFG rulesexpanding the loops during re-unification wouldcomplicate the algorithm significantly.
Instead, weimplemented loop detection during CFG parsing.The feature structures prevent loops in unifica-tion, and we considered including certain grammat-ical features into backbone extraction as done in(Briscoe and Carroll, 1994).
However, in the TRIPSgrammar the feature values responsible for break-ing loops belonged to multi-valued features (6 val-ued in the worst case), with values which may de-pend on other multiple-valued features in daughterconstituents.
Thus adding the extra features resultedin major backbone size increases because of cate-gory splitting.
This can be remedied with additionalpre-compilation (Kiefer and Krieger, 2004), how-ever, this requires that all lexical entries be knownin advance.
One nice feature of the TRIPS lex-icon is that it includes a mechanism for dynami-cally adding lexical entries for unknown words fromwide-coverage lexicons such as VerbNet (Kipper etal., 2000), which would be impractical to use in pre-compilation.Therefore, to use CFG parsing before unificationin our system, we implemented a loop detector thatchecked the CFG structure to disallow loops.
How-ever, the next problem that we encountered is mas-sive ambiguity in the CFG structure.
Even a veryshort phrase such as ?a train?
had over 700 possi-ble CFG analyses, and took 910 msec to parse com-pared to 10 msec with interleaved unification.
CFGambiguity is so high because noun phrase fragmentsare allowed as top-level categories, and lexical am-biguity is compounded with semantic ambiguity androbust rules normally disallowed by features duringunification.
Thus, in our combined algorithm we hadto use unification interleaved with parsing to filterout the CFG constituents.4.2 Ambiguity PackingFor building semantic representations in parallelwith parsing, ambiguity packing presents a set ofknown problems (Oepen and Carroll, 2000).
Onepossible solution is to exclude semantic features dur-ing an initial unification stage, use ambiguity pack-ing, and re-unify with semantic features in a post-processing stage.
In our case, we found this strategydifficult to implement, since selectional restrictionsare used to limit the ambiguity created by multipleword senses during syntactic parsing.
Therefore, wechose to do ambiguity packing on the CFG structureonly, keeping the multiple feature structures associ-ated with each packed CFG constituent.To begin to evaluate the contribution of ambiguitypacking on efficiency, we ran a test on the first 39 ut-terances in a hold out set not used in the formal eval-uation below.
Sentences ranged from 1 to 17 wordsin length, 16 of which had 6 or more words.
On thisset, the average parse time without ambiguity pack-ing was 10 seconds per utterance, and 30 seconds perutterance on utterances with 6 or more words.
Withambiguity packing turned on, the average parse timedecreased to 5 seconds per utterance, and 13.5 sec-onds per utterance on the utterances with more than6 words.
While this evaluation showed that ambi-12guity packing improves parsing efficiency, we deter-mined that further enhancements were necessary.4.3 PruningWe added a pruning technique based on the scor-ing model discussed above and ambiguity packingto enhance system performance.
As an illustration,consider an example from a corpus used in our eval-uation where the TRIPS grammar generates a largenumber of analyses, ?we have a heart attack vic-tim at marketplace mall?.
The phrase ?a heart at-tack victim?
has at least two interpretations,?a [N1heart [N1 attack [N1 victim]]]?
and ?a [N1 [N1 heart[N1 attack]] [N1 victim]]?.
The prepositional phrase?at marketplace mall?
can attach either to the nounphrase or to the verb.
Overall, this results in 4 basicinterpretations, with additional ambiguity resultingfrom different possible senses of ?have?.The best-first parsing algorithm in TRIPS usesparse selection scores to suppress less likely inter-pretations.
In our example, the TRIPS parser willchose the higher-scoring one of the two interpreta-tions for ?a heart attack victim?, and use it first.
Forthis NP the features associated with both interpreta-tions are identical with respect to further processing,thus TRIPS will never come back to the other in-terpretation, effectively pruning it.
?At?
also has 2possible interpretations due to word sense ambigu-ity: LF::TIME-LOC and LF::SPATIAL-LOC.
Theformer has a slightly higher preference, and TRIPSwill try it first.
But then it will be unable to find aninterpretation for ?at Marketplace Mall?, and back-track to LF::SPATIAL-LOC to find a correct parse.Without chart size limits the parser is guaran-teed to find a parse eventually through backtracking.However, this algorithm does not work quite as wellwith chart size limits.
If there are many similarly-scored constituents in the chart for different parts ofthe utterance, the best-first algorithm expands themfirst, and the the chart size limit tends to interferebefore TRIPS can backtrack to an appropriate lower-scoring analysis.Ambiguity packing offers an opportunity to makepruning more strategic by focusing specifically oncompeting interpretations for the same utterancespan.
The simplest pruning idea would be for ev-ery ambiguity packed constituent to eliminate the in-terpretations with low TRIPS scores.
However, weneed to make sure that we don?t prune constituentsthat are required higher up in the tree to make aparse.
Consider our example again.The constituent for ?at?
will be ambiguitypacked with its two meanings.
But if we pruneLF::SPATIAL-LOC at that point, the parse for ?atMarketplace Mall?
will fail later.
Formally, the com-peting interpretations for ?at?
have non-local fea-tures, namely, the subcategorized complement (timeversus location) is different for those interpretations,and is checked higher up in the parse.
But for ?aheart attack victim?
the ambiguity-packed interpre-tations differ only in local features.
All features as-sociated with this NP checked higher up come fromthe head noun ?victim?
and are identical in all inter-pretations.
Therefore we can eliminate the low scor-ing interpretations with little risk of discarding thoseessential for finding a complete parse.
Thus, forany constituent where ambiguity-packed non-headdaughters differ only in local features, we prunethe interpretations coming from them to a specifiedprune beam width based on their TRIPS scores.This pruning heuristic based on local featurescan be generalised to different unification grammars.For example, in HPSG pruning would be safe at allpoints where a head is combined with ambiguity-packed non-head constituents, due to the localityprinciple.
In the TRIPS grammar, if a trips ruleuses subcategorization features, the same localityprinciple holds.
This heuristic has perfect precisionthough not complete recall, but, as our evaluationshows, it is sufficient to significantly improve per-formance in comparison with the TRIPS parser.5 EvaluationThe purpose of our evaluation is to explore the ex-tent to which we can achieve a better balance be-tween parse time and coverage using backbone pars-ing with pruning compared to the original best-firstalgorithm.
For our comparison we used an excerptfrom the Monroe corpus that has been used in previ-ous TRIPS research on parsing speed and accuracy(Swift et al, 2004) consisting of dialogues s2, s4,s16 and s17.
Dialogue s2 was a hold out set used forpilot testing and setting parameters.
The other threedialogues were set aside for testing.
Altogether, thetest set contained 1042 utterances, ranging from 1 to1345 words in length (mean 5.38 words/utt, st. dev.
5.7words/utt).
Using our hold-out set, we determinedthat a beam width of three was an optimal setting.Thus, we compared TFLEX using a beam width of 3to three different versions of TRIPS that varied onlyin terms of the maximum chart size, giving us a ver-sion that is significantly faster than TFLEX overall,one that has parse times that are statistically indis-tinguishable from TFLEX, and one that is signifi-cantly slower.
We show that while lower chart sizesin TRIPS yield speed ups in parse time, they comewith a cost in terms of coverage.5.1 Evaluation MethodologyBecause our goal is to explore the parse time versuscoverage trade-offs of two different parsing architec-tures, the two evaluation measures that we report areaverage parse time per sentence and probability offinding at least one parse, the latter being a measureestimating the effect of parse algorithm on parsingcoverage.Since the scoring model is the same in TRIPS andTFLEX, then as long as TFLEX can find at least oneparse (which happened in all but 1 instances on ourheld-out set), the set returned will include the oneproduced by TRIPS.
We spot-checked the TFLEXutterances in the test set for which TRIPS couldnot find a parse to verify that the parses producedwere reasonable.
The parses produced by TFLEX onthese sentences were typically acceptable, with er-rors mainly stemming from attachment disambigua-tion problems.5.2 ResultsWe first compared parsers in terms of probability ofproducing at least one parse (see Figure 2).
Sincethe distribution of sentence lengths in the test corpuswas heavily skewed toward shorter sentences, wegrouped sentences into equivalence classes based ona range of sentence lengths with a 5-word increment,with all of the sentences over 20 words aggregatedin the same class.
Given a large number of short sen-tences, there was no significant difference overall inlikelihood to find a parse.
However, on sentencesgreater than 10 words long, TFLEX is significantlymore likely to produce a parse than any of the TRIPSparsers (evaluated using a binary logistic regression,N = 334, G = 16.8, DF = 1, p < .001).
Fur-Parser <= 20 words >= 6 wordsTFLEX 6.2 (20.2) 29.1 (96.3)TRIPS-1500 2.3 (5.4) 6.9 (8.2)TRIPS-5000 7.7 (30.2) 28.1 (56.4)TRIPS-10000 22.7 (134.4) 107.6 (407.4)Table 1: The average parse times for TRIPS andTFLEX on utterances 6 words or more.thermore, for sentences greater than 20 words long,no form of TRIPS parser ever returned a completeparse.Next we compared the parsers in terms of aver-age parse time on the whole data set across equiva-lence classes of sentences, assigned based on Aggre-gated Sentence Length (see Figure 2 and Table 1).An ANOVA with Parser and Aggregated SentenceLength as independent variables and Parse Time asthe dependent variable showed a significant effectof Parser on Parse Time (F (3, 4164) = 270.03,p < .001).
Using a Bonferroni post-hoc analysis, wedetermined that TFLEX is significantly faster thanTRIPS-10000 (p < .001), statistically indistinguish-able in terms of parse time from TRIPS-5000, andsignificantly slower than TRIPS-1500 (p < .001).Since none of the TRIPS parsers ever returned aparse for sentences greater than 20 words long, werecomputed this analysis excluding the latter.
Westill find a significant effect of Parser on Parse Time(F (3, 4068) = 18.6, p < .001).
However, a post-hoc analysis reveals that parse times for TFLEX,TRIPS-1500, and TRIPS-5000 are statistically in-distinguishable for this subset, whereas TFLEX issignificantly faster than TRIPS-10000 (p < .001).See Table 1 for for parse times of all four parsers.Since TFLEX and TRIPS both spent 95% of theircomputational effort on sentences with 6 or morewords, we also include results for this subset of thecorpus.Thus, TFLEX presents a superior balance of cov-erage and efficiency especially for long sentences(10 words or more) since for these sentences it issignificantly more likely to find a parse than any ver-sion of TRIPS, even a version where the chart size isexpanded to an extent that it becomes significantlyslower (i.e., TRIPS-10000).
And while TRIPS-1500is consistently faster than the other parsers, it isnot significantly faster than TFLEX on sentences 2014Figure 2: Parse times and probability of getting a parse depending on (aggregated) sentence lengths.
5denotes sentences with 5 or fewer words, 25 sentences with more than 20 words.words long or less, which is the subset of sentencesfor which it is able to find a parse.5.3 Discussion and Future WorkThe most obvious lesson learned in this experienceis that the speed up techniques developed for specificgrammars and unification formalisms do not transfereasily to other unification grammars.
The featuresthat make TRIPS interesting ?
the inclusion of lex-ical semantics, and the rules for parsing fragments?
also make it less amenable to using existing effi-ciency techniques.Grammars with an explicit CFG backbone nor-mally restrict the grammar writer from writinggrammar loops, a restriction not imposed by gen-eral unification grammars.
As we showed, therecan be a substantial number of loops in a CFG dueto the need to cover various elliptical constructions,which makes CFG parsing not interleaved with uni-fication less attractive in cases where we want toavoid expensive CFG precompilation.
Moreover, aswe found with the TRIPS grammar, in the contextof robust parsing with lexical semantics the ambigu-ity in a CFG backbone grows large enough to makeCFG parsing followed by unification inefficient.
Wedescribed an alternative technique that uses pruningbased on a parse selection model.Another option for speeding up parsing that wehave not discussed in detail is using a typed gram-mar without disjunction and speeding up unificationas done in HPSG grammars (Kiefer et al, 1999).
Inorder to do this, we must first address the issue ofintegrating the type of lexical semantics that we re-quire with HPSG?s type system.
Adding lexical se-mantics while retaining the speed benefits obtainedthrough this type system would require that the se-mantic type ontology be expressed in the same for-malism as the syntactic types.
We plan to furtherexplore this option in our future work.Though longer sentences were relatively rarein our test set, using the system in an educa-tional domain (our ultimate goal) means that thelonger sentences are particularly important, becausethey often correspond to significant instructionalevents, specifically answers to deep questions suchas ?why?
and ?how?
questions.
Our evaluation hasbeen designed to show system performance with ut-terances of different length, which would roughlycorrespond to the performance in interpreting shortand long student answers.
Since delays in respond-ing can de-motivate the student and decrease thequality of the dialogue, improving handling of longutterances can have an important effect on the sys-tem performance.
Evaluating this in practice is apossible direction for future work.6 ConclusionsWe described a combination of efficient parsingtechniques to improve parsing speed and coveragewith the TRIPS deep parsing grammar.
We showedthat context-free parsing was inefficient on a back-bone extracted from an existing unification gram-mar, and demonstrated how to make all-path pars-ing more tractable by a new pruning algorithm based15on ambiguity packing and local features, general-isable to other unification grammars.
We demon-strated that our pruning algorithm provides betterefficiency-coverage balance than the best-first pars-ing with chart limits utilised by the TRIPS parser,and discussed the design implications for other ro-bust parsing grammars.AcknowledgementsWe thank Mary Swift and James Allen for theirhelp with the TRIPS code and useful comments.This material is based on work supported by grantsfrom the Office of Naval Research under numbersN000140510048 and N000140510043.ReferencesJ.
Baldridge.
2002.
Lexically Specified DerivationalControl in Combinatory Categorial Grammar.
Ph.D.thesis, University of Edinburgh.T.
Briscoe and J. Carroll.
1994.
Generalized proba-bilistic LR parsing of natural language (corpora) withunification-based grammars.
Computational Linguis-tics, 19(1):25?59.J.
Carroll.
1994.
Relating complexity to practical per-formance in parsing with wide-coverage unificationgrammars.
In Proceedings of ACL-2004.A.
Copestake and D. Flickinger.
2000.
An opensource grammar development environment and broad-coverage English grammar using HPSG.
In Proceed-ings of LREC-2000, Athens, Greece.M.
O. Dzikovska, M. D. Swift, and J. F. Allen.
2004.Building a computational lexicon and ontology withframenet.
In LREC workshop on Building Lexical Re-sources from Semantically Annotated Corpora.M.
Dzikovska, M. Swift, J. Allen, and W. de Beaumont.2005.
Generic parsing for multi-domain semantic in-terpretation.
In Proceedings of the 9th InternationalWorkshop on Parsing Technologies (IWPT-05).M.
O. Dzikovska.
2004.
A Practical Semantic Represen-tation For Natural Language Parsing.
Ph.D. thesis,University of Rochester.M.
Elsner, M. Swift, J. Allen, and D. Gildea.
2005.
On-line statistics for a unification-based dialogue parser.In Proceedings of the 9th International Workshop onParsing Technologies (IWPT-05).A.
Frank, B. Kiefer, B. Crysmann, M. Becker, andU.
Schafer.
2003.
Integrated shallow and deep pars-ing: TopP meets HPSG.
In Proceedings of ACL 2003.B.
Kiefer and H.-U.
Krieger.
2004.
A context-free ap-proximation of head-driven phrase structure grammar.In H. Bunt, J. Carroll, and G. Satta, editors, New De-velopments in Parsing Technology.
Kluwer.B.
Kiefer, H. Krieger, J. Carroll, and R. Malouf.
1999.Bag of useful techniques for efficient and robust pars-ing.
In Proceedings of ACL 1999.K.
Kipper, H. T. Dang, and M. Palmer.
2000.
Class-based construction of a verb lexicon.
In Proceedingsof the 7th Conference on Artificial Intelligence and ofthe 12th Conference on Innovative Applications of Ar-tificial Intelligence.A.
Lavie and C. P. Rose?.
2004.
Optimal ambiguity pack-ing in context-free parsers with interleaved unification.In H. Bunt, J. Carroll, and G. Satta, editors, Current Is-sues in Parsing Technologies.
Kluwer Academic Press.J.
T. Maxwell and R. M. Kaplan.
1994.
The interfacebetween phrasal and functional constraints.
Computa-tional Linguistics, 19(4):571?590.S.
Narayanan and S. Harabagiu.
2004.
Question answer-ing based on semantic structures.
In Proceedings ofInternational Conference on Computational Linguis-tics (COLING 2004), Geneva, Switzerland.S.
Oepen and J. Carroll.
2000.
Ambiguity packing inconstraint-based parsing - practical results.
In Pro-ceedings of NAACL?00.C.
P. Rose?
and A. Lavie.
2001.
Balancing robustnessand efficiency in unification-augmented context-freeparsers for large practical applications.
In J. Junquaand G. Van Noord, editors, Robustness in Languageand Speech Technology.
Kluwer Academic Press.C.
Rose?.
2000.
A framework for robust semantic in-terpretation.
In Proceedings 1st Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.D.
Schlangen and A. Lascarides.
2003.
The interpreta-tion of non-sentential utterances in dialogue.
In Pro-ceedings of the 4th SIGdial Workshop on Discourseand Dialogue, Japan, May.D.
Schneider and K. F. McCoy.
1998.
Recognizing syn-tactic errors in the writing of second language learners.In Proceedings of COLING-ACL?98.M.
Swift, J. Allen, and D. Gildea.
2004.
Skeletons inthe parser: Using a shallow parser to improve deepparsing.
In Proceedings of COLING-04.J.
Tetreault.
2005.
Empirical Evaluations of PronounResolution.
Ph.D. thesis, University of Rochester.G.
van Noord.
1997.
An efficient implementation ofthe head-corner parser.
Computational Linguistics,23(3):425?456.16
