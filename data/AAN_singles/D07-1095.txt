Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
906?914, Prague, June 2007. c?2007 Association for Computational LinguisticsInducing Search Keys for Name FilteringL.
Karl BrantingThe MITRE Corporation7467 Ridge Road, Suite 140Hanover, MD 21076, U.S.A.lbranting@mitre.orgAbstractThis paper describes ETK (Ensemble ofTransformation-based Keys) a new algo-rithm for inducing search keys for namefiltering.
ETK has the low computationalcost and ability to filter by phonetic sim-ilarity characteristic of phonetic keys suchas Soundex, but is adaptable to alternativesimilarity models.
The accuracy of ETK ina preliminary empirical evaluation suggeststhat it is well-suited for phonetic filteringapplications such as recognizing alternativecross-lingual transliterations.1 IntroductionThe task of name matching?recognizing when twoorthographically distinct strings are likely to denotethe same individual?occurs in a wide variety of im-portant applications, including law enforcement, na-tional security, and maintenance of government andcommercial records.
Coreference resolution, speechunderstanding, and detection of aliases and duplicatenames all require name matching.The orthographic variations that give rise to thename-matching task can result from a variety of fac-tors, including transcription and OCR errors, andspelling variations.
In many applications, cross-lingual transliterations are a particularly importantsource of variation.
For example, romanized Ara-bic names are phonetic transcriptions of sounds thathave no direct equivalent in English, e.g., ?Mo-hamed?
or ?Muhammet?
are two of many possibletransliterations for the same Arabic name.Name matching can be viewed as a type of rangequery in which the input is a set of patterns (suchas names on an immigration-control watch list), acollection of text strings (such as a passenger list), adistance metric for calculating the degree of relevantdissimilarity between pairs of strings, and a matchthreshold expressing the maximum allowable dis-tance between matching names.
The goal is to findall text/pattern pairs whose distance under the metricis less than or equal to the threshold.
In the simplestcase, patterns and the text strings with which theyare matched are both individual words.
In the gen-eral case, the text may not be segmented into stringscorresponding to possible names.Distance metrics for name matching are typicallycomputationally expensive.
For example, determin-ing the edit distance between two strings of length nand m requires, in the general case, nm steps.
Met-rics based on algorithms that learn from examplesof strings that should match (Bilenko et al, 2003;Ristad and Yianilos, 1998) and metrics that use pho-netic similarity criterion, e.g., (Kondrak, 2000) areno less expensive than edit distance.The computational expense of distance metricsmeans that tractable name matching on large textstypically requires an inexpensive, high-recall filter-ing step to find a subset of the original text towhich the expensive similarity metric will be ap-plied.
Desiderata for filtering include the following:1.
High recall.
The recall of the entire name-matching process is bounded by the recall ofthe filtering step, so high filtering recall is es-sential.2.
Efficiency.
Filtering is useful only to the ex-tend that it requires less computational expensethan applying the similarity metric to each pat-tern/text pair.
The computational expense of906a filtering algorithm itself must therefore beless than the cost of the metric calls eliminatedthrough the filtering process.
Typically, the costof the metric is so much higher than the filter-ing cost that the latter can be neglected.
Underthese circumstances, precision is a satisfactoryproxy for efficiency.3.
Adaptability to specific distance metrics.High precision and recall are achievable in fil-tering only if the filtering criterion correspondsto the distance metric.
For example, if a dis-tance metric is based on phonetic differencesbetween strings, a filtering algorithm that se-lects candidate text strings based on ortho-graphic differences may perform poorly.
Sim-ilarly, poor performance may result from useof a filtering algorithm based on phonetic dif-ferences if the distance metric is based on or-thographic differences.
For example, ?LAYTON?and ?LEIGHTON?
differ by a large edit distancebut are phonetically identical (in most dialects),whereas ?BOUGH?
and ?ROUGH?
are orthograph-ically similar but phonetically dissimilar.
Anideal filtering algorithm should be adaptable toany particular distance metric.This paper describes ETK (Ensemble ofTransformation-based Keys) a new algorithmfor inducing filters that satisfy the three criteriaabove.
ETK is similar to phonetic search keyalgorithms such as Soundex and shares phoneticsearch key algorithms?
low computational expenseand ability to filter by phonetic similarity.
However,ETK has the advantage that it is adaptable to alter-native distance metrics and is therefore applicableto a wider range of circumstances than static keyalgorithms.The next section describes previous work in namefiltering.
Section 3 describes the ETK algorithm indetail, and a preliminary evaluation on English andGerman surnames is set forth in Section 4.2 Previous WorkThe division of the retrieval task into an inexpensive,high-recall filtering stage followed by a more expen-sive high-precision stage emerged independently ina variety of different areas of computer science.
Thisapproach is termed two-stage retrieval in the Infor-mation Retrieval literature (Shin and Zhang, 1998),MAC/FAC by some researchers in analogy (Gen-tner and Forbus, 1991), blocking in the statisticalrecord linkage literature (Cohen et al, 2003), and fil-tering in the approximate string matching literature(Navarro, 2001).The two most common approaches to filteringthat have been applied to name matching are in-dexing by phonetic keys and indexing by ngrams.Two less well known filtering algorithms that oftenhave higher recall than filtering by phonetic keys orngrams are pivot-based retrieval and partition filter-ing.Phonetic Key Indexing.
In phonetic key index-ing, names are indexed by a phonetic representa-tion created by a key function that maps sequencesof characters to phonetic categories.
Such keyfunctions partition the name space into equivalenceclasses of names having identical phonetic represen-tations.
Each member of a partition is indexed by theshared phonetic representation.The oldest phonetic key function is apparentlySoundex, which was patented in 1918 and 1922by Russell and Odell (U.S.
Patents 1,261,167 and1,435,663) and described in (Knuth, 1975).
DespiteSoundex?s has many well-known limitations, includ-ing inability to handle different first letters withidentical pronunciations (e.g., Soundex of ?Kris?is K620, but Soundex of ?Chris?
is C620), trun-cation of long names, and bias towards Englishpronunciations, Soundex is still in use in manylaw enforcement and national security applications(Dizard, 2004).
A number of alternative phoneticencodings have been developed in response to thelimitations of Soundex, e.g., (Taft, 1970; Gadd,1990; Zobel and Dart, 1996; Philips, 1990; Philips,2000; Hodge and Austin, 2001; Christen, 2006).While each of these alternatives has some advan-tages over Soundex, none is adaptable to alterna-tive distance metrics.
For purposes of comparison,Phonex (Gadd, 1990) was included in the evalua-tion below because it was found to be the most ac-curate phonetic key for last names in an evaluationby (Christen, 2006).Ngram Filtering.
The second common filteringalgorithm for names is ngram indexing, under which907each pattern string is indexed by every n-elementsubstring, i.e., every sequence of n contiguous char-acters occurring in the pattern string (typically, theoriginal string is padded with special leading andtrailing characters to distinguish the start and end ofthe name).
The candidates for each target string areretrieved using the ngrams in the target as indices(Cohen et al, 2003).
Typical values for n are 3 or 4.Pivot-Based Retrieval.
Pivot-based retrievaltechniques are applicable to domains, such as namematching, in which entities are not amenable tovector representation but for which the distancemetric satisfies the triangle inequality (Chavez etal., 2001).1The key idea is to organize the index around asmall group of elements, called pivots.
In retrieval,the distance between the query probe q and any ele-ment e can be estimated based on the distances ofeach to one or more pivots.
There are numerouspivot-based metric space indexing algorithms.
Aninstructive survey of these algorithms is set forth in(Chavez et al, 2001).One of the oldest, and often best-performing,pivot-based indices is Burkhart-Keller Trees (BKT)(Burkhard and Keller, 1973; Baeza-Yates andNavarro, 1998).
BKT is suitable for discrete-valueddistance metrics.
Construction of a BKT starts withselection of an arbitrary element as the root of thetree.
The ith child of the root consists of all ele-ments of distance i from the root.
A new BKT isrecursively constructed for each child until the num-ber of elements in a child falls below a predefinedbucket size.A range query on a BKT with distance metricd, probe q, range k, and pivot p is performed asfollows.
If the BKT is a leaf node, then the dis-tance metric d is applied between q and each elementof the leaf node, and those elements e for whichd(q, e) < k are returned.
Otherwise, all subtreeswith index i for which |d(q, e) ?
i| ?
k are recur-sively searched.While all names within k of a query are guaran-1Edit distance satisfies the triangle inequality because anystring A can be transformed into another string C by firsttransforming A to any other string B, then transforming Binto C. Thus, edit-distance(A,C) cannot be greater than edit-distance(A,B) + edit-distance(B,C) for any strings A, B, andC.teed to be retrieved by a BKT (i.e., recall is 100%),there are no guarantees on precision.
During search,one application of the distance metric is required ateach internal node traversed, and a distance-metricapplication is required for each candidate element inleaf nodes reached during the traversal.
The numberof nodes searched is exponential in k (Chavez et al,2001).Partition Filtering.
Partition filtering (Wu andManber, 1991; Navarro and Baeza-Yates, 1999), isan improvement over ngram filtering that relies onthe observation that if a pattern string P of lengthm is divided into segments of length ?
m(k+1)?, thenany string that matches P with at most k errors mustcontain an exact match for at least one of the seg-ments (intuitively, it would take at least k + 1 er-rors, e.g., edit operations, to alter all of these seg-ments).
Strings indexed by ?
m(k+1)?-length segmentscan be retrieved by an efficient exact string match-ing algorithm, such as suffix trees or Aho-Corasicktrees.
This is necessary because partitions, unlikengrams, vary in length.Partition filtering differs from ngram filtering intwo respects.
First, ngrams overlap, whereas par-tition filtering involves partitioning each string intonon-overlapping segments.
Second, the choice ofn in ngram filtering is typically independent of k,whereas the size of the segments in filtering is cho-sen based on k. Since in most applications n is in-dependent of k, ngram retrieval, like phonetic keyindexing, lacks any guaranteed lower bound on re-call, whereas partition filtering guarantees 100% re-call when the distance metric is edit distance.3 The ETK algorithm3.1 MotivationAny key function partitions the universe of stringsinto equivalence classes of strings that share a com-mon key.
If a key function is to serve as a fil-ter, matching names must be members of the sameequivalence class.
However, no single partition canproduce equivalence classes that both include allmatching pairs and exclude all non-matching pairs.22For example, suppose that for strings A, B, and C and dis-tance metric d, d(A,B) = .9, d(B,C) = .9, d(A,C) = 1.7, andsuppose that 1.0 is the match threshold.
A query on A would re-quire a partition that puts A and B in the same equivalence class908A search key that creates partitions in which thereis a low probability that non-matching pairs share acommon equivalence class will have high precision,although possibly low recall.
However, the recall ofan ensemble of search keys, each having non-zerorecall and each being independent of the others, canbe expected to be greater than the recall of any in-dividual key.
A high-precision and high-recall indexcan therefore be constructed if one can find, for agiven similarity metric and match threshold, a suf-ficiently large set of key functions that (1) are in-dependent, (2) each have high-precision under themetric and threshold, and (3) have non-zero recall.The objective of ETK is to learn a set of inde-pendent, high-precision key functions from trainingdata consisting of equivalence classes of names thatsatisfy the matching criteria.
The similarity metricand threshold are implicit in the training data.
Thus,under this approach a key function can be learnedeven if the similarity model is unknown, providedthat sufficient equivalence classes are available.For each equivalence class, ETK attempts to findthe shortest transformation rules capable of con-verting all members of the equivalence class intoan identical orthographic representation.
The entirecollection of transformation rules for all equivalenceclasses, which in general has many inconsistencies,is then partitioned into separate consistent subsets.Each subset of transformation rules constitutes anindependent key function.
Each pattern name is in-dexed by each key produced by applying a key func-tion to it, and the candidate matches for a new nameconsist of all pattern names that share at least onekey.The equivalence classes of matching names canbe obtained either through some a priori source(such as alias lists or manual construction) or by ap-plying the similarity metric to pairs in a training set,e.g., repeated leave-one-out retrievals with a knowndistance metric.
In the former case, the keys arepurely empirical; in the later the key functions arein effect a way of compiling the distance metric tospeed retrieval.and C into a different equivalence class, a query on C would re-quire a partition that puts B and C in the same equivalence classand A in a different equivalence class, and a query on B wouldrequire a partition in which all three were in the same equiva-lence class.
Thus, three independent keys would be needed tosatisfy all three queries while excluding non-matching names.3.2 ProcedureInducing Transformation Rules.
The inductiveprocess starts with a collection of equivalenceclasses under a given distance metric and matchthreshold k. A collection of transformation rules arederived from these equivalence classes as follows.For each equivalence class EC:?
The element of EC with the least mean pair-wise edit distance to the other class members(breaking ties by preferring shorter elements)is selected as the centroid.
For example, ifEC is {LEIGHTON LAYTON SLEIGHTON}, thenLEIGHTON would be the centroid because it hasa smaller edit distance to the other elementsthan they do to each other.?
For each element E other than the centroid, dy-namic programming is used to find an align-ment of E with the centroid that maximizes thenumber of corresponding identical characters.3For example, the alignment of LEIGHTON andLAYTON would be:LAY--TONLEIGHTON?
For each character c of the centroid, all win-dows of characters in E of length from 1 tosome constant maxWindow centered on thecharacter in the source corresponding to c arefound, skipping blank characters.
Each map-ping from a window to c constitutes a rule.For example, for maxWindow 7 and the align-ment above, the transformation rules for the Ein LEIGHTON would be:$$LAYTO ?
E$LAYT ?
ELAY ?
EA ?
E3See (Damper et al, 2004) for details on alignment by dy-namic programming.
The approach taken here assigns a slightlyhigher association weight for aligned identical consonants thanfor aligned identical vowels so that, ceteris paribus, consonantalignment is preferred to vowel alignment and assigns a slightlyhigher association weight to non-identical letters that are bothvowels or both consonants than to vowel/consonant alignments.909Transformation rules derived from multipleequivalence classes typically have many incon-sistencies, i.e., rules with identical left-handsides (LHSs) but different right hand sides(RHSs).
All RHSs for a given LHS are groupedtogether and ranked by frequency of occurrencein the training data.
For example, the frequencyof alternative rules for the middle charactersLAN and LEI for the U.S. name pronunciationset with k = 1 discussed below is:LAY ?
E 5 LEI ?
A 2LAY ?
A 4 LEI ?
E 1LAY ?
AN 3 LEI ?
- 1Key Formation.
The transformation rules are sub-divided two different ways: by LHS, e.g., separatingrules for LAY from those for LEI, and by RHS fre-quency, e.g., separating LAY ?
E (the most frequentrule for LAY) from LAY?
A (the next most frequent).The highest frequency RHS rules from the exampleabove are:LAY ?
ELEI ?
Aand the next most frequent are:LAY ?
ALEI ?
EIf rules are divided into l LHS subsets, and eachsubset is further subdivided by taking the r highestranked RHSs (with RHSs ranked lower than r ig-nored), the result is a total of lr subsets.
Each ofthese lr subsets defines a key function.
For each po-sition in a word to which the key function is to beapplied (padded with leading and training markers),the rule with the longest (i.e., most specific) LHSthat matches the window centered at that position isused to determine the corresponding character in thekey.
If no rules apply, the character in the key is thesame as that in the original word.For example, suppose that the word to which thekey is to be applied is CREIGHTON and transforma-tions include LEIGHTO ?
-, EIGHT ?
- and IGH ?
G.The character in the key corresponding to the G inCREIGHTON would be - (i.e., a deletion) because theEIGHT is the longest LHS matching at that position.The key consists of the concatenation of the RHSsproduced by successively applying the key functionto each position in the orginal word.This procedure is similar to window-basedpronunciation-learning algorithms, e.g., (Sejnowskiand Rosenberg, 1987; Bakiri and Dietterich, 1999),but differs in that the objective is not determininga correct pronunciation, but is instead transformingwords that are similar under a given metric into asingle, consistent orthographic representation.3.3 Filtering with ETKThe lr subsets of transformation rules induced froma given set of equivalence classes define an ensem-ble of key functions.
To filter potential matches withthis ensemble, each pattern is added to a hash tableindexed by each key generated by a key function.Candidate matches to a text string consist of all pat-terns indexed by the keys generated from the text bythe ensemble of key functions.
For example, sup-pose that (as is the case with the rule sets for Amer-ican names, pronunciation distance, and k = 0)patterns ROLLINS and ROWLAND have keys that in-clude {ROWLINS ROLINS} and {RONLLAND ROLAN},respectively, and that text RAWLINS has keys that in-clude {ROWLINS RALINS}.
Then ROLLINS but notROWLAND would be retrieved because it is indexedby a key shared with ROWLINS.44 EvaluationThe retrieval accuracy of ETK was compared tothat of BKT, filtering by partition, ngram filtering,Phonex, and Soundex on sets of U.S. and Germannames.
The U.S. name set consisted of the 5,000most common last names identified during the mostrecent U.S. Census5 which have pronunciations incmudict, the CMU pronouncing dictionary.6 TheGerman name set consisted of the first 5963 en-tries in the HADI-BOMP collection7 whose part ofspeech is NAM.The filtering algorithms were compared with re-spect to two alternative distance metrics.
The firstwas pronunciation distance, which consists of edit4In the evaluation below, the original string itself is addedas an additional index key.
This addition slightly increases bothrecall and precision.5The names were taken from the 1990U.S.
Census collection of 88,799 last names athttp://www.census.gov/genealogy/names/names files.html.6http://www.speech.cs.cmu.edu/cgi-bin/cmudict.7http://www.ikp.uni-bonn.de/dt/forsch/phonetik/bomp.910distance between pronunciations represented us-ing the cmudict phoneme set for U.S. names andthe HADI-BOMP phoneme set for German names.Stress values were removed from cmudict pronun-ciations, and syllable divisions were removed fromHADI-BOMP pronunciations.
When there weremultiple pronunciations for a name in cmudict, thefirst was used.
In cmudict, for example, MEUSEand MEWES have pronunciation distance of 0 be-cause both have pronunciation M Y UW Z.
In HADI-BOMP, HELGARD and HERBART have pronunciationdistance 2 because their pronunciations are h E l g a rt and h E r b a r t. The second distance metric was editdistance with unit weights for insertions, deletions,and substitutions.
In practice, appropriate distancemetrics might be Jaro (Jaro, 1995), Winkler (Win-kler, 1999), or some metric specialized to a particu-lar phonetic or error model.
Pronunciation and editdistance were chosen as representative of phoneticand non-phonetic metrics.Training data for ETK for a given language,match threshold k, and similarity metric consisted ofall sets of at least 2 names containing only elementswere within k of some element of the set under themetric.
These training sets were created by perform-ing a retrieval on every name in each collection us-ing BKT, which has 100% recall.
For each retrieval,the true positives from BKT?s return set were de-termined by applying the similarity metric betweeneach return set element and the query.
If there wereat least 2 true positives (including the query itself),the set of true positives was included in the trainingset.8ETK was tested using cross validation, so thatnames in the training set and those in the testing setwere disjoint.
Specifically, all names in the testingset were removed from each collection in the train-ing set.
If at least 2 names remained, the collectionwas retained.
ETK?s maxWindow size was 7, as inthe examples above.In BKT, the bucket size (maximum number of el-ements in any leaf node) was 2, and the longest el-ement (rather than a random element) was selected8Note that each set of true positives is a cluster having thequery as its centroid and radius k under the distance metric.The triangle inequality guarantees that the maximum distancebetween any pair of names in the collection is no greater than2k.as the root of each subtree.
The rationale for thischoice is that there is typically more variance in dis-tance from a longer word than from a shorter word,and greater variance increases the branching factorin BKT, reducing tree depth and therefore the num-ber of nodes visited during search.Since the importance of precision in filtering isthat it determines the number of calls to the sim-ilarity metric required for a given level of recall,precision figures for BKT include internal calls tothe similarity metric, that is, calls during indexing.Thus, precision of BKT is the number of true posi-tives divided by the number of all positives plus thenumber of internal metric calls.In Soundex and Phonex indexing, each name wasindexed by its Soundex (Phonex) key.
Similarly,in ngram filtering each name was indexed by allits ngrams, with special leading and trailing char-acters added.
Retrieval was performed by findingthe Soundex or Phonex encoding or the ngrams ofeach query and retrieving every name indexed by theSoundex or Phonex encoding or any ngram.
Preci-sion was measured with duplicates removed.In partition filtering, each name was indexed byeach of its k + 1 partitions, and the partitions them-selves were organized in an Aho-Curasick tree (Gus-field, 1999).
Retrieval was performed by apply-ing the Aho-Curasick tree to the query to determineall partitions occurring in the query and retrievingthe names corresponding to each partition, remov-ing duplicates.4.1 Optimizing LHS and RHS SubdivisionsThe first experiment was performed to clarify the op-timal sizes of l, the number of LHS subdivisions,and r, the number of RHS ranks.
ETK was testedon the U.S. name set with k = 1, pronunciation dis-tance as similarity metric, and 10-fold cross valida-tion for l ?
{1, 2, 4, 8, 16, 32} and r ?
{1, 2}.As shown in Table 1, when l = 1, r = 2 hashigher f-measure than r = 1, but when l is 2 orgreater, the best value for r is 1.
Overall, the highestf-measure is obtained with l = 8 and r = 1.
In theexperiments below, the value of 16 was used for lbecause this leads to slightly higher recall at a smallcost in decreased f-measure.911Table 1: F-measure for l ?
{1, 2, 4, 8, 16, 32} andr ?
{1, 2, } on U.S. names with pronunciation dis-tance and k = 1 in 10-fold cross validation.1 2 4 8 16 321 0.1431 0.2112 0.3039 0.3550 0.3428 0.29282 0.1469 0.1858 0.1520 0.0729 0.0264 0.01084.2 Comparison of ETK to Other FilterAlgorithmsThe retrieval accuracy of ETK was compared to thatof BKT, partition, ngram, Phonex, and Soundex onthe U.S. and German name sets for pronunciationdistance with k ?
{0, 1, 2} and for edit distancewith k ?
{1, 2}.
In tests involving pronuncia-tion distance BKT was tested under two conditions:with the pronunciation distance function availableto BKT during indexing and retrieval; and the dis-tance function unavailable, so that BKT indexingand retrieval was performed on the surface formeven though the actual similarity metric was pro-nunciation distance.
This is intended to simulatethe situation in which examples of matching namesare available but the underlying similarity metric isunknown.
Ngram and partition filtering were per-formed on letters only.Tables 2 and 3 show recall, precision, and f-measure for pronunciation distance on U.S. and Ger-man names, respectively, with k ?
{0, 1, 2}, l =16, and r = 1.
ETK has the highest f-measureunder all conditions because its precision is con-sistently higher than that of the other algorithms.This is because each key function in ETK appliesonly transformations representing orthographic dif-ferences between names in the same equivalenceclass.
Thus, the transformations are very conserva-tive.
BKT always has recall of 1.0 when the pronun-ciation model is available, but in many cases a modelmay be unavailable.
When no model is available, nosingle algorithm consistently has the highest recall.Ngrams, partition, Phonex, and BKT each had thehighest recall in at least one language/error thresh-old combination.Tables 4 and 5 show recall, precision, and f-measure for edit distance on U.S. and Germannames, respectively, with k ?
{1, 2}, l = 16, andr = 1 (k = 0 would be an exact match on thesurface form, for which all algorithms would haveTable 2: Recall, precision, and f-measure for pro-nunciation distance on U.S. surnames.
K is maxi-mum permitted error.
BKT-NM is BKT without thepronunciation model.
Best results are shown in bold,including highest recall in addition to BKT.recall precision f-measureBKT 1.0000 0.0152 0.0299BKT-NM 0.0510 0.0003 0.0006partition 0.1298 0.0168 0.0298k=0 soundex 0.8350 0.0331 0.0637phonex 0.8811 0.0173 0.0339ngrams 0.7457 0.0034 0.0068ETK 0.5642 0.3314 0.4175BKT 1.0000 0.0039 0.0078BKT-NM 0.5704 0.0019 0.0038partition 0.6157 0.0092 0.0181k=1 soundex 0.4422 0.1803 0.2562phonex 0.4969 0.1008 0.1676ngrams 0.4453 0.0213 0.0406ETK 0.4862 0.2647 0.3428BKT 1.0000 0.0088 0.0174BKT-NM 0.7588 0.0050 0.0099partition 0.6948 0.0122 0.0240k=2 soundex 0.1298 0.4350 0.2000phonex 0.1708 0.2860 0.2139ngrams 0.2063 0.0825 0.1178ETK 0.4502 0.1953 0.2724recall 1.0).
Again, ETK has the highest f-measurebecause of its consistently high precision.4.3 Training Set SizeThe sensitivity of ETK to training set size was testedby performing 50-fold cross-validation with train-ing sets for pronunciation distance on U.S. names ofsizes in {48, 96, 191, 381, 762, 1524, 3047} drawnfrom the 3047 equivalence classes in the 5000 U.S.names with pronunciation distance and k = 1.
Asshown in Figure 1, the learning curve rises steeplyfor the entire range of training set sizes consideredin this experiment.5 ConclusionThe experimental results demonstrate the feasibil-ity of basing search keys on transformation rulesacquired from examples.
If sufficient examples ofnames that match under a given distance metric anderror threshold are available, keys can be inducedthat lead to good performance in comparison to al-ternative filtering algorithms.
Moreover, the resultsinvolving pronunciation distance illustrate how pho-netic keys can be learned that are specific to indi-912Table 3: Recall, precision, and f-measure for pro-nunciation distance on German names.
K is maxi-mum permitted error.
Best results are shown in bold.recall precision f-measureBKT 1.0000 0.0056 0.0110BKT-NM 0.1600 0.0003 0.0007partition 0.1223 0.0059 0.0112k=0 soundex 0.7059 0.0125 0.0235phonex 0.8997 0.0061 0.0122ngrams 0.9348 0.0016 0.0031ETK 0.7715 0.3606 0.4915BKT 1.0000 0.0013 0.0026BKT-NM 0.7923 0.0006 0.0013partition 0.7865 0.0031 0.0062k=1 soundex 0.3969 0.0533 0.0940phonex 0.5048 0.0270 0.0512ngrams 0.6866 0.0090 0.0178ETK 0.5503 0.3820 0.4510BKT 1.0000 0.0018 0.0037BKT-NM 0.8533 0.0010 0.0021partition 0.8384 0.0029 0.0058k=2 soundex 0.1311 0.1209 0.1258phonex 0.1693 0.0640 0.0929ngrams 0.2801 0.0255 0.0468ETK 0.3496 0.1687 0.2276Table 4: Recall, precision, and f-measure for editdistance on U.S. surnames.recall precision f-measureBKT 1.0000 0.0024 0.0048partition 1.0000 0.0106 0.0210k=0 soundex 0.3537 0.1010 0.1572phonex 0.3937 0.0564 0.0986ngrams 0.8408 0.0288 0.0557ETK 0.6768 0.3244 0.4386BKT 1.0000 0.0052 0.0103partition 1.0000 0.0139 0.0275k=1 soundex 0.1038 0.2692 0.1498phonex 0.1288 0.1696 0.1464ngrams 0.4112 0.1300 0.1976ETK 0.4001 0.3565 0.3770Table 5: Recall, precision, and f-measure for editdistance on German names.recall precision f-measureBKT 1.0000 0.0009 0.0018partition 1.0000 0.0045 0.0091k=0 soundex 0.5266 0.0826 0.1429phonex 0.6101 0.0374 0.0704ngrams 0.8880 0.0134 0.0264ETK 0.6647 0.4957 0.5679BKT 1.0000 0.0017 0.0034partition 1.0000 0.0048 0.0096k=1 soundex 0.1592 0.2052 0.1793phonex 0.2019 0.1063 0.1392ngrams 0.4036 0.0516 0.0915ETK 0.3986 0.3466 0.3708Figure 1: F-measure for U.S. names for training setscontaining varying numbers of collections, with k =1, l = 16, and r = 1.
Each training instance consistsof all names within k of some centriod under themetric.vidual match criteria.
In filtering under pronunci-ation distance, ETK?s f-measure for German nameswas similar to its f-measure for U.S. names (actuallyhigher for k ?
{0, 1}) whereas Soundex and Phonexwere approximately an order of magnitude lower.Although ETK consistently had the highest f-measure in this experiment, it does not follow thatETK is necessarily the most desirable name filter forany particular application.
In many applications re-call may be much more important than precision.
Insuch cases, it may be essential to choose the highestrecall algorithm notwithstanding a lower f-measure.However, the highest recall algorthms can lead to avery large number of distance-metric applications.For example, in some data sets the number of nodesexamined by BKT during retrieval is a significantproportion of the entire pattern set.ETK has the disadvantage of requiring a large setof training examples consisting of equivalence setsof strings that match under the metric and maximumallowable error.
Where such large numbers of equiv-alence sets are unavailable, it may be better to usesimpler and less-informed filters.A number of variations of ETK are possible.
Forexample, keys could consist of finite-state trans-ducers trained from consistent subsets of mappingsrather than transformation rules.
There are alsomany possible alternatives to ETK?s window-basedapproach to deriving mappings from examples.913In summary, this work has demonstrated that en-sembles of keys induced from equivalence classesof names under a specific distance metric and max-imum allowable error can filter names with high f-measure.
The experimental results illustrate the ben-efits both of acquiring keys that are adapted to spe-cific similarity criteria and of indexing with multipleindependent keys.ReferencesR.
A. Baeza-Yates and G. Navarro.
1998.
Fast approx-imate string matching in a dictionary.
In String Pro-cessing and Information Retrieval, pages 14?22.G.
Bakiri and T. Dietterich.
1999.
Achieving high-accuracy text-to-speech with machine learning.
Datamining in speech synthesis.M.
Bilenko, W. W. Cohen, S. Fienberg, R. J. Mooney,and P. Ravikumar.
2003.
Adaptive name-matchingin information integration.
IEEE Intelligent Systems,18(5):16?23.W.
A. Burkhard and R. M. Keller.
1973.
Some ap-proaches to best-match file searching.
Commun.
ACM,16(4):230?236.E.
Chavez, G. Navarro, R. A. Baeza-Yates, and J. L. Mar-roquin.
2001.
Searching in metric spaces.
ACM Com-puting Surveys, 33(3):273?321.P.
Christen.
2006.
A comparison of personal namematching: Techniques and practical issues.
In Pro-ceedings of the ICDM 2006 Workshop on Mining Com-plex Data (MCD), December.W.
W. Cohen, P. Ravikumar, and S. E. Fienberg.
2003.A comparison of string distance metrics for name-matching tasks.
In Proceedings of the IJCAI-2003Workshop on Information Integration on the Web,pages 73?78, Acapulco, Mexico, August.R.
I. Damper, Y. Marchand, J. D. S. Marsters, and A. I.Bazin.
2004.
Aligning letters and phonemes forspeech synthesis.
In Proceedings of 5th InternationalSpeech Communication Association (ISCA) Workshopon Speech Synthesis, pages 209?214, Pittsburgh, PA.W.
Dizard.
2004.
Obsolete algorithm tangles ter-rorst/criminal watch lists.
Government ComputerNews, 23(12), August 17.T.
Gadd.
1990.
Phonix: The algorithm.
Program: auto-mated library and information systems, 24(4).D.
Gentner and K. Forbus.
1991.
MAC/FAC: A model ofsimilarity-based retrieval.
In Thirteenth Annual Con-ference of the Cognitive Science Society, pages 504?509.D.
Gusfield.
1999.
Algorithms on Strings, Trees, andSequences.
Cambridge University Press.V.
J. Hodge and J. Austin.
2001.
An evaluation of pho-netic spell checkers.
Technical report, Department ofComputer Science, University of York.M.
A. Jaro.
1995.
Probabilistic linkage of large publichealth data files.
Statistics in Medicine, 14(5?7):491?498.D.
E. Knuth.
1975.
Fundamental Algorithms, volumeIII of The Art of Computer Programming.
Addison-Wesley, Reading, Massachusetts.G.
Kondrak.
2000.
A new algorithm for the alignment ofphonetic sequences.
In Proceedings of the first confer-ence on North American chapter of the Association forComputational Linguistics, pages 288?295, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.G.
Navarro and R. Baeza-Yates.
1999.
Very fast andsimple approximate string matching.
Information Pro-cessing Letters, 72:65?70.G.
Navarro.
2001.
A guided tour to approximate stringmatching.
ACM Computing Surveys, 33(1):31?88,March.L.
Philips.
1990.
Hanging on the metaphone.
ComputerLanguage Magazine, 7(12), December.L.
Philips.
2000.
The double metaphone search algo-rithm.
C/C++ Users Journal, 18(1), June 1.E.
S. Ristad and P. N. Yianilos.
1998.
Learning string-edit distance.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 20(5):522?532.T.J Sejnowski and C.R.
Rosenberg.
1987.
Parallel net-works that learn to pronounce english text.
ComplexSystems, 1:145?168.D.
Shin and B. Zhang.
1998.
A two-stage retrieval modelfor the TREC-7 ad hoc task.
In Text REtrieval Confer-ence, pages 439?445.R.
Taft.
1970.
Name search techniques: New york stateidentification and intelligence system.
Technical Re-port 1, State of New York.W.
E. Winkler.
1999.
The state of record linkage and cur-rent research problems.
Technical report, StatisticalResearch Division, U.S. Census Bureau, Washington,DC.S.
Wu and U. Manber.
1991.
Fast text searching witherrors.
Technical Report TR-91-11, University of Ari-zona.J.
Zobel and P. Dart.
1996.
Phonetic string match-ing: lessons from information retrieval.
SIGIR Forum,166?172.914
