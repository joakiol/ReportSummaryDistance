Creating a Finite-State Parser with Application SemanticsOwen RambowUniversity of PennsylvaniaPhiladelphia, PA 19104USASrinivas BangaloreAT&T Labs ?
ResearchFlorham Park, NJ 07932USATahir ButtJohns Hopkins UniversityBaltimore, MD 21218USAAlexis NasrUniversite?
Paris 775005 ParisFranceRichard SproatAT&T Labs ?
ResearchFlorham Park, NJ 07932USArambow@unagi.cis.upenn.eduAbstractParsli is a finite-state (FS) parser which can betailored to the lexicon, syntax, and semanticsof a particular application using a hand-editabledeclarative lexicon.
The lexicon is defined interms of a lexicalized Tree Adjoining Grammar,which is subsequently mapped to a FS represen-tation.
This approach gives the application de-signer better and easier control over the naturallanguage understanding component than usingan off-the-shelf parser.
We present results usingParsli on an application that creates 3D-imagesfrom typed input.1 Parsing and Application-SpecificSemanticsOne type of Natural Language Understanding(NLU) application is exemplified by the databaseaccess problem: the user may type in free sourcelanguage text, but the NLU component mustmap this text to a fixed set of actions dictatedby the underlying application program.
Wewill call such NLU applications ?application-semantic NLU?.
Other examples of application-semantic NLU include interfaces to command-based applications (such as airline reservationsystems), often in the guise of dialog systems.Several general-purpose off-the-shelf (OTS)parsers have become widely available (Lin,1994; Collins, 1997).
For application-semanticNLU, it is possible to use such an OTS parser inconjunction with a post-processor which trans-fers the output of the parser (be it phrase struc-ture or dependency) to the domain semantics.
Inaddition to mapping the parser output to appli-cation semantics, the post-processor often mustalso ?correct?
the output of the parser: the parsermay be tailored for a particular domain (such asWall Street Journal (WSJ) text), but the new do-main presents linguistic constructions not foundin the original domain (such as questions).
Itmay also be the case that the OTS parser consis-tently misanalyzes certain lexemes because theydo not occur in the OTS corpus, or occur therewith different syntactic properties.
While manyof the parsers can be retrained, often an anno-tated corpus is not available in the applicationdomain (since, for example, the application it-self is still under development and there is notyet a user community).
The process of retrainingmay also be quite complex in practice.
A furtherdisadvantage of this approach is that the post-processor must typically be written by hand, asprocedural code.
In addition, the application-semantic NLU may not even exploit the strengthsof the OTS parser, because the NLU requiredfor the application is not only different (ques-tions), but generally simpler (the WSJ containsvery long and syntactically complex sentenceswhich are not likely to be found as input in in-teractive systems, including dialog systems).This discussion suggests that we (i) need aneasy way to specify application semantics for aparser and (ii) that we do not usually need the fullpower of a full recursive parser.
In this paper, wesuggest that application-semantic NLP may bebetter served by a lexicalized finite-state (FS)parser.
We present PARSLI, a FS parser whichcan be tailored to the application semantics us-ing a hand-editable declarative lexicon.
This ap-proach gives the application designer better andeasier control over the NLU component.
Further-more, while the finite-state approach may not besufficient for WSJ text (given its syntactic com-plexity), it is sufficient for most interactive sys-tems, and the advantage in speed offered by FSapproaches in more crucial in interactive appli-cations.
Finally, in speech-based systems, thelattice that is output from the speech recognitioncomponent can easily used as input to a FS-basedparser.2 Sample Application: WORDSEYEWORDSEYE (Coyne and Sproat, 2001) is asystem for converting English text into three-dimensional graphical scenes that represent thattext.
WORDSEYE performs syntactic and se-mantic analysis on the input text, producing adescription of the arrangement of objects in ascene.
An image is then generated from thisscene description.
At the core of WORDSEYEis the notion of a ?pose?, which can be looselydefined as a figure (e.g.
a human figure) in a con-figuration suggestive of a particular action.For WORDSEYE, the NLP task is thus tomap from an input sentence to a representationthat the graphics engine can directly interpret interms of poses.
The graphical component canrender a fixed set of situations (as determined byits designer); each situation has several actors insituation-specific poses, and each situation canbe described linguistically using a given set ofverbs.
For example, the graphical componentmay have a way of depicting a commercial trans-action, with two humans in particular poses (thebuyer and the seller), the goods being purchased,and the payment amount.
In English, we havedifferent verbs that can be used to describe thissituation (buy, sell, cost, and so on).
These verbshave different mappings of their syntactic argu-ments to the components in the graphical repre-sentation.
We assume a mapping from syntax todomain semantics, leaving to lexical semanticsthe question of how such a mapping is devisedand derived.
(For many applications, such map-pings can be derived by hand, with the seman-tic representation an ad-hoc notation.)
We showa sample of such mapping in Figure 1.
Here,we assume that the graphics engine of WORD-SEYE knows how to depict a TRANSACTIONwhen some of the semantic arguments of a trans-action (such as CUSTOMER, ITEM, AMOUNT)are specified.We show some sample transductions in Fig-ure 2.
In the output, syntactic constituents arebracketed.
Following each argument is informa-tion about its grammatical function (?GF=0?
forexample) and about its semantic role (ITEM forexample).
If a lexical item has a semantics ofits own, the semantics replaces the lexical item(this is the case for verbs), otherwise the lexicalitem remains in place.
In the case of the transi-tive cost, the verbal semantics in Figure 1 spec-ifies an implicit CUSTOMER argument.
This isgenerated when cost is used transitively, as canbe seen in Figure 2.3 Mapping Tree Adjoining Grammarto Finite State MachinesWhat is crucial for being able to define a map-ping from words to application semantics is avery abstract notion of grammatical function: indevising such a mapping, we are not interestedin how English realizes certain syntactic argu-ments, i.e., in the phrase structure of the verbalprojection.
Instead, we just want to be able to re-fer to syntactic functions, such as subject or indi-rect object.
Tree Adjoining Grammar (TAG) rep-resents the entire syntactic projection from a lex-eme in its elementary structures in an elementarytree; because of this, each elementary tree canbe associated with a lexical item (lexicalization,(Joshi and Schabes, 1991)).
Each lexical itemcan be associated with one or more trees whichrepresent the lexeme?s valency; these trees arereferred to as its supertags.
In a derivation, sub-stituting or adjoining the tree of one lexeme intothat of another creates a direct dependency be-tween them.
The syntactic functions are labeledwith integers starting with zero (to avoid discus-sions about names), and are retained across op-erations such as topicalization, dative shift andpassivization.A TAG consists of a set of elementary trees oftwo types, initial trees and auxiliary trees.
Thesetrees are then combined using two operations,substitution and adjunction.
In substitution, aninitial tree is appended to a specially markednode with the same label as the initial tree?s rootnode.
In adjunction, a non-substitution node isrewritten by an auxiliary tree, which has a spe-cially marked frontier node called the footnode.The effect is to insert the auxiliary tree into themiddle of the other tree.We distinguish two types of auxiliary trees.Adjunct auxiliary trees are used for adjuncts;they have the property that the footnode is al-Verb Supertag Verb semantics Argument semanticspaid A nx0Vnx1 transaction 0=Customer 1=Amountcost A nx0Vnx1 transaction 0=Item 1=Amount Implicit=Customercost A nx0Vnx2nx1 transaction 0=Item 1=Amount 2=Customerbought, purchased A nx0Vnx1 transaction 0=Customer 1=Itemsocks A NXN none noneFigure 1: Sample entries for a commercial transaction situationIn: I bought socksOut: ( ( I ) GF=0 AS=CUSTOMER TRANSACTION ( socks ) GF=1 AS=ITEM )In:the pajamas cost my mother-in-law 12 dollarsOut: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION ( ( my ) mother-in-law ) GF=2 AS=CUSTOMER ( (12 ) dollars ) GF=1 AS=AMOUNT )In: the pajamas cost 12 dollarsOut: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION IMP:CUSTOMER ( ( 12 ) dollars ) GF=1AS=AMOUNT )Figure 2: Sample transductions generated by Parsli (?GF?
for grammatical function, ?AS?
for argu-ment semantics, ?Imp?
for implicit argument)ways a daughter node of the root node, and thelabel on these nodes is not, linguistically speak-ing, part of the projection of the lexical item ofthat tree.
For example, an adjective will projectto AdjP, but the root- and footnode of its tree willbe labeled NP, since an adjective adjoins to NP.We will refer to the root- and footnode of an ad-junct auxiliary tree as its passive valency struc-ture.
Note that the tree for an adjective also spec-ifies whether it adjoins from the left (footnodeon right) or right (footnode on left).
Predicativeauxiliary trees are projected from verbs whichsubcategorize for clauses.
Since a verb projectsto a clausal category, and has a node labeled witha clausal category on its frontier (for the argu-ment), the resulting tree can be interpreted as anauxiliary tree, which is useful in analyzing long-distance wh-movement (Frank, 2001).To derive a finite-state transducer (FST) froma TAG, we do a depth-first traversal of each ele-mentary tree (but excluding the passive valencystructure, if present) to obtain a sequence of non-terminal nodes.
For predicative auxiliary trees,we stop at the footnode.
Each node becomes twostates of the FST, one state representing the nodeon the downward traversal on the left side, theother representing the state on the upward traver-sal, on the right side.
For leaf nodes, the twostates are juxtaposed.
The states are linearly con-nected with   -transitions, with the left node stateof the root node the start state, and its right nodestate the final state (except for predicative auxil-iary trees ?
see above).
To each non-leaf state,we add one self loop transition for each tree inthe grammar that can adjoin at that state fromthe specified direction (i.e., for a state represent-ing a node on the downward traversal, the auxil-iary tree must adjoin from the left), labeled withthe tree name.
For each pair of adjacent statesrepresenting a substitution node, we add transi-tions between them labeled with the names ofthe trees that can substitute there.
We output thenumber of the grammatical function, and the ar-gument semantics, if any is specified.
For thelexical head, we transition on the head, and out-put the semantics if defined, or simply the lex-eme otherwise.
There are no other types of leafnodes since we do not traverse the passive va-lency structure of adjunct auxiliary tees.
At thebeginning of each FST, an   -transition outputs anopen-bracket, and at the end, an   -transition out-puts a close-bracket.
The result of this phase ofthe conversion is a set of FSTs, one per elemen-tary tree of the grammar.
We will refer to themas ?elementary FSTs?.0 1<epsilon>:( 2A_NXG:GF=0A_NXN:GF=03<epsilon>:FE=Customer 4ordered:transaction 5A_NXG:GF=1A_NXN:GF=16<epsilon>:FE=Item 7<epsilon>:)Figure 4: FST corresponding to TAG tree in Figure 3SNPArg0VPVorderedNPArg1Figure 3: TAG tree for word ordered; the dow-narrow indicates a substitution node for the nom-inal argument4 Constructing the ParserIn our approach, each elementary FST describesthe syntactic potential of a set of (syntacticallysimilar) words (as explained in Section 3).
Thereare several ways of associating words with FSTs.Since FSTs correspond directly to supertags (i.e.,trees in a TAG grammar), the basic way toachieve such a mapping is to list words pairedwith supertags, along with the desired seman-tic associated with each argument position (seeFigure 1).
The parser can also be divided intoa lexical machine which transduces words toclasses, and a syntactic machine, which trans-duces classes to semantics.
This approach hasthe advantage of reducing the size of the over-all machine since the syntax is factored from thelexicon.The lexical machine transduces input words toclasses.
To determine the mapping from word tosupertag, we use the lexical probability where 	 is the word and  the class.
Theseare derived by maximum likelihood estimationfrom a corpus.
Once we have determined for allwords which classes we want to pair them with,we create a disjunctive FST for all words associ-ated with a given supertag machine, which trans-duces the words to the class name.
We replacesthe class?s FST (as determined by its associatedsupertag(s)) with the disjunctive head FST.
Theweights on the lexical transitions are the nega-tive logarithm of the emit probability 	 (ob-tained in the same manner as are the lexical prob-abilities).For the syntactic machine, we take each ele-mentary tree machine which corresponds to aninitial tree (i.e., a tree which need not be ad-joined) and form their union.
We then performa series of iterative replacements; in each iter-ation, we replace each arc labeled by the nameof an elementary tree machine by the lexicalizedversion of that tree machine.
Of course, in eachiteration, there are many more replacements thanin the previous iteration.
We use 5 rounds of iter-ation; obviously, the number of iterations restrictthe syntactic complexity (but not the length) ofrecognized input.
However, because we outputbrackets in the FSTs, we obtain a parse withfull syntactic/lexical semantic (i.e., dependency)structure, not a ?shallow parse?.This construction is in many ways similar tosimilar constructions proposed for CFGs, in par-ticular that of (Nederhof, 2000).
One differenceis that, since we start from TAG, recursion is al-ready factored, and we need not find cycles in therules of the grammar.5 Experimental ResultsWe present results in which our classes are de-fined entirely with respect to syntactic behav-ior.
This is because we do not have availablean important corpus annotated with semantics.We train on the Wall Street Journal (WSJ) cor-pus.
We evaluate by taking a list of 205 sen-tences which are chosen at random from entriesto WORDSEYE made by the developers (whowere testing the graphical component using a dif-ferent parser).
Their average length is 6.3 words.We annotated the sentences by hand for the de-sired dependency structure, and then comparedthe structural output of PARSLI to the gold stan-dard (we disregarded the functional and seman-tic annotations produced by PARSLI).
We eval-uate performance using accuracy, the ration ofn Correctness Accuracy Nb2 1.00 1.00 124 0.83 0.84 306 0.70 0.82 1218 0.62 0.80 17812 0.59 0.79 20216 0.58 0.79 20420 0.58 0.78 205Figure 5: Results for sentences with  or fewerwords; Nb refers to the number of sentences inthis categoryn Correctness Accuracy1 0.58 0.782 0.60 0.794 0.62 0.818 0.69 0.8512 0.68 0.8620 0.70 0.8730 0.73 0.89Figure 6: Results for  -best analysesthe number of dependency arcs which are cor-rectly found (same head and daughter nodes) inthe best parse for each sentence to the numberof arcs in the entire test corpus.
We also reportthe percentage of sentences for which we find thecorrect dependency tree (correctness).
For ourtest corpus, we obtain an accuracy of 0.78 anda correctness of 0.58.
The average transductiontime per sentence (including initialization of theparser) is 0.29 s. Figure 5 shows the dependenceof the scores on sentence length.
As expected,the longer the sentence, the worse the score.We can obtain the n-best paths through theFST; the scores for n-best paths are summarizedin Figure 6.
Since the scores keep increasing, webelieve that we can further improve our 1-bestresults by better choosing the correct path.
Weintend to adapt the FSTs to use probabilities ofattaching particular supertags to other supertags(rather than uniform weights for all attachments)in order to better model the probability of differ-ent analyses.
Another option, of course, is bilex-ical probabilities.6 Discussion and OutlookWe have presented PARSLI, a system that takesa high-level specification of domain lexical se-mantics and generates a finite-state parser thattransduces input to the specified semantics.PARSLI uses Tree Adjoining Grammar as an in-terface between syntax and lexical semantics.Initial evaluation results are encouraging, and weexpect to greatly improve on current 1-best re-sults by using probabilities of syntactic combi-nation.
While we have argued that many appli-cations do not need a fully recursive parser, thesame approach to using TAG as an intermediatebetween application semantics and syntax can beused in a chart parser; for a chart parser using theFS machines discussed in this paper, see (Nasr etal., 2002).ReferencesMichael Collins.
1997.
Three generative, lex-icalised models for statistical parsing.
InProceedings of the 35th Annual Meeting ofthe Association for Computational Linguis-tics, Madrid, Spain, July.Bob Coyne and Richard Sproat.
2001.
Word-sEye: An automatic text-to-scene conversionsystem.
In SIGGRAPH 2001, Los Angeles,CA.Robert Frank.
2001.
Phrase Structure Composi-tion and Syntactic Dependencies.
MIT Press,Cambridge, Mass.Aravind K. Joshi and Yves Schabes.
1991.
Tree-adjoining grammars and lexicalized gram-mars.
In Maurice Nivat and Andreas Podel-ski, editors, Definability and Recognizabilityof Sets of Trees.
Elsevier.Dekang Lin.
1994.
PRINCIPAR?an efficient,broad-coverage, principle-based parser.
Incoling94, pages 482?488, Kyoto, Japan.Alexis Nasr, Owen Rambow, John Chen, andSrinivas Bangalore.
2002.
Context-free pars-ing of a tree adjoining grammar using finite-state machines.
In Proceedings of the SixthInternational Workshop on tree AdjoiningGrammar and related Formalisms (TAG+6),Venice, Italy.Mark-Jan Nederhof.
2000.
Practical experi-ments with regular approximation of context-free languages.
Computational Linguistics,26(1):17?44.
