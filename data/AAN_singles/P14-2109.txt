Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668?673,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsParser Evaluation Using Derivation Trees:A Complement to evalbSeth Kulick and Ann Bies and Justin MottLinguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104{skulick,bies,jmott}@ldc.upenn.eduAnthony Kroch and Mark Liberman and Beatrice SantoriniDepartment of Linguistics, University of Pennsylvania, Philadelphia, PA 19104{kroch,myl,beatrice}@ling.upenn.eduAbstractThis paper introduces a new technique forphrase-structure parser analysis, catego-rizing possible treebank structures by inte-grating regular expressions into derivationtrees.
We analyze the performance of theBerkeley parser on OntoNotes WSJ andthe English Web Treebank.
This providessome insight into the evalb scores, andthe problem of domain adaptation with theweb data.
We also analyze a ?test-on-train?
dataset, showing a wide variance inhow the parser is generalizing from differ-ent structures in the training material.1 IntroductionPhrase-structure parsing is usually evaluated usingevalb (Sekine and Collins, 2008), which providesa score based on matching brackets.
While thismetric serves a valuable purpose in pushing parserresearch forward, it has limited utility for under-standing what sorts of errors a parser is making.This is the case even if the score is broken downby brackets (NP, VP, etc.
), because the bracketscan represent different types of structures.
Wewould also like to have answers to such questionsas ?How does the parser do on non-recursive NPs,separate from NPs resulting from modification?On PP attachment??
etc.Answering such questions is the goal of thiswork, which combines two strands of research.First, inspired by the tradition of Tree Adjoin-ing Grammar-based research (Joshi and Schabes,1997; Bangalore and Joshi, 2010), we use a de-composition of the full trees into ?elementarytrees?
(henceforth ?etrees?
), with a derivation treethat records how the etrees relate to each other,as in Kulick et al (2011).
In particular, we usethe ?spinal?
structure approach of (Shen et al,2008; Shen and Joshi, 2008), where etrees are con-strained to be unary-branching.Second, we use a set of regular expressions(henceforth ?regexes?)
that categorize the possiblestructures in the treebank.
These are best thoughtof as an extension of head-finding rules, which notonly find a head but simultaneously identify eachparent/children relation as one of a limited numberof types of structures (right-modification, etc.
).The crucial step is that we integrate theseregexes into the spinal etrees.
The derivation treesprovide elements of a dependency analysis, whichallow us to calculate scores for head identificationand attachment for different projections (e.g., PP).The regexes allow us to also provide scores basedon spans of different construction types.
Togetherthese two aspects break down the evalb bracketsinto more meaningful categories, and the simulta-neous head and span scoring allows us to separatethese aspects in the analysis.After describing in more detail the basic frame-work, we show some aspects of the resulting anal-ysis of the performance of the Berkeley parser(Petrov et al, 2008) on three datasets: (a)OntoNotes WSJ sections 2-21 (Weischedel et al,2011)1, (b) OntoNotes WSJ section 22, and (c)the ?Answers?
section of the English Web Tree-bank (Bies et al, 2012).
We trained the parser onsections 2-21, and so (a) is ?test-on-train?.
Thesethree results together show how the parser is gen-eralizing from the training data, and what aspectsof the ?domain adaptation?
problem to the webmaterial are particularly important.22 Framework for analyzing parsingperformanceWe first describe the use of the regexes in tree de-composition, and then give some examples of in-1We refer only to the WSJ treebank portion of OntoNotes,which is roughly a subset of the Penn Treebank (Marcus etal., 1999) with annotation revisions including the addition ofNML nodes.2We parse (c) while training on (a) to follow the procedurein Petrov and McDonald (2012)668corporating these regexes into the derivation trees.2.1 Use of regular expressionsDecomposing the original phrase-structure treeinto the smaller components requires somemethod of determining the ?head?
of a nonter-minal, from among its children nodes, similar toparsing work such as Collins (1999).
As describedabove, we are also interested in the type of lin-guistic construction represented by that one-levelstructure, each of which instantiates one of a fewtypes - recursive coordination, simple head-and-sister, etc.
We address both tasks together with theregexes.
In contrast to the sort of head rules in(Collins, 1999), these refer as little as possible tospecific POS tags.
Instead of explicitly listing thePOS tags of possible heads, the heads are in mostcases determined by their location in the structure.Sample regexes are shown in Figure 1.
Thereare 49 regexes used.3Regexes ADJP-t andADVP-t in (a) identify their terminal head tobe the rightmost terminal, possibly preceded bysome number of terminals or nonterminals, rely-ing on a mapping that maps all terminals (exceptCC, which is mapped to CONJ) to TAG and allnonterminals (except CONJP and NML) to NT.Structures with a CONJ/CONJP/NML child donot match this rule and are handled by differentregexes, which are all mutually exclusive.
In somecases, we need to search for particular nonterminalheads, such as with the (b) regexes S-vp and SQ-vp, which identify the rightmost VP among thechildren of a S or SQ as the head.
(c) NP-modris a regex for a recursive NP with a right modifier.In this case, the NP on the left is identified as thehead.
(d) VP-crd is also a regex for a recursivestructure, in this case for VP coordination, pick-ing out the leftmost conjunct as the head of thestructure.
The regex names roughly describe theirpurpose - ?mod?
for right-modification, ?crd?
forcoordination, etc.
The suffix ?-t?
is for the simplenon-recursive case in which the head is a terminal.2.2 Regexes in the derivation treesThe names of these regexes are incorporated intothe etrees themselves, as labels of the nontermi-nals.
This allows an etree to contain information3Some among the 49 are duplicates, used for differentnonterminals, as with (a) and (b) in Figure 1.
We derivedthe regexes via an iterative process of inspection of tree de-composition on dataset (a), together with taking advantage ofthe treebanking experience from some of the co-authors.(a)ADJP-t,ADVP-t:?
(TAG|NT|NML)*(head:TAG) (NT)*$(b)S-vp, SQ-vp: ?([?
]+)*(head:VP)$(c)NP-modr:?
(head:NP)(SBAR|S|VP|ADJP|PP|ADVP|NP)+$(d)VP-crd: ?
(head:VP) (VP)*CONJ VP$Figure 1: Some sample regexessuch as ?this node represents right modification?.For example, Figure 2 shows the derivation treeresulting from the decomposition of the tree inFigure 4.
Each structure within a circle is oneetree, and the derivation as a whole indicates howthese etrees are combined.
Here we indicate witharrows that point to the relevant regex.
For ex-ample, the PP-t etree #a6 points to the NP-modrregex, which consists of the NP-t together withthe PP-t.
The nonterminals of the spinal etrees arethe names of the regexes, with the simpler non-terminal labels trivially derivable from the regexnames.4The tree in Figure 5 is the parser output corre-sponding to the gold tree in Figure 4, and in thiscase gets the PP-t attachment wrong, while every-thing else is the same as the gold.5This is reflectedin the derivation tree in Figure 3, in which the NP-modr regex is absent, with the NP-t and PP-t etrees#b5 and #b6 both pointing to the VP-t regex in#b3.
We show in Section 2.3 how this derivationtree representation is used to score this attachmenterror directly, rather than obscuring it as an NPbracket error as evalb would do.2.3 ScoringWe decompose both the gold and parser outputtrees into derivation trees with spinal etrees, andscore based on the regexes projected by each word.There is a match for a regex if the correspondingwords in gold/parser files project to that regex, aprecision error if the parser file does but the golddoes not, and a recall error if the gold does but theparser file does not.For example, comparing the trees in Figures 4and 5 via their derivation trees in Figures 2 andFigures 3, the word ?trip?
has a match for the regexNP-t, but a recall error for NP-modr.
The word4We do not have space here to discuss the data structurein complete detail, but multiple regex names at a node, such aVP-aux and VP-t at tree a3 in Figure 2, indicate multiple VPnonterminals.5We leave function tags aside for future work.
The goldtree is shown without the SBJ function tag.669#a1 TheyNP-t tripNP-modr  NP-t toPP-t#a6#a3#a4themakeVP-auxVP-tS-vp#a2will #a7FloridaNP-t#a5Figure 2: Derivation Tree for Figure 4#b1TheyNP-t toPP-t#b6tripNP-t#b4themakeVP-auxVP-t#b3 S-vp#b2will #b7FloridaNP-t#b5Figure 3: Derivation Tree for Figure 5)SNPTheyVPwill VPmake NPNPthe tripPPto NPFloridaSNPTheyVPwill VPmake NPthe tripPPto NPFlorida1Figure 4: Gold treeSNPTheyVPwill VPmake NPNPthe tripPPto NPFloridaSNPTheyVPwill VPmake NPthe tripPPto NPFlorida1Figure 5: Parser output treeCorpus tokens brackets coverage % evalb2-21 g 650877 578597 571243 98.7p 575744 569480 98.9 93.822 g 32092 24819 24532 98.8p 24801 24528 98.9 90.1Ans g 53960 48492 47348 97.6p 48750 47423 97.3 80.8Table 1: Corpus information for gold(g) andparsed(p) sections of each corpus?make?
has a match for the regexes VP-t, VP-aux, and S-vp, and so on.
Summing such scoresover the corresponding gold/parser trees gives usF-scores for each regex.There are two modifications/extensions to theseF-scores that we also use:(1) For each regex match, we score whether itmatches based on the span as well.
For exam-ple, ?make?
is a match for VP-t in Figures 2and 3, and is also a match for the span as well,since in both derivation trees it includes the words?make.
.
.Florida?.
It is this matching for span aswell as head that allows us to compare our resultsto evalb.
We call the match just for the head the ?F-h?
score and the match that also includes the spaninformation the ?F-s?
score.
The F-s score roughlycorresponds to the evalb score.
However, the F-s score is for separate syntactic constructions (in-cluding also head identification), although we canalso sum it over all the structures, as done later inFigure 6.
The simultaneous F-h and F-s scores letus identify constructions where the parser has thehead projection correct, but gets the span wrong.
(2) Since the derivation tree is really a depen-dency tree with more complex nodes (Rambowand Joshi, 1997; Kulick et al, 2012), we can alsoscore each regex for attachment.6For example,while ?to?
is a match for PP-t, its attachment isnot, since in Figure 2 it is a child of the ?trip?
etree(#a5) and in Figure 3 it is a child of the ?make?etree (#b3).
Therefore our analysis results in anattachment score for every regex.2.4 Comparison with previous workThis work is in the same basic line of researchas the inter-annotator agreement analysis work inKulick et al (2013).
However, that work didnot utilize regexes, and focused on comparing se-quences of identical strings.
The current workscores on general categories of structures, without6A regex intermediate in a etree, such as VP-t above, isconsidered to have a default null attachment.
Also, the at-tachment score is not relevant for regexes that already expressa recursive structure, such as NP-modr.
In Figure 2, NP-t inetree #a5 is considered as having the attachment to #a3.670Sections 2-21 (Ontonotes) Section 22 (Ontonotes) Answers (English Web Treebank)regex %gold F-h F-s att spanR %gold F-h F-s att spanR %gold F-h F-s att spanRNP-t 30.7 98.9 97.6 96.5 99.6 31.1 98.0 95.8 94.4 99.6 28.5 95.4 91.5 90.9 99.3VP-t 13.5 98.8 94.5 98.4 95.8 13.4 98.1 91.7 97.3 93.7 16.0 96.7 81.7 96.1 85.4PP-t 12.2 99.2 91.0 90.5 92.0 12.1 98.7 86.4 86.1 88.2 8.4 96.4 80.5 80.7 84.7S-vp 12.2 97.9 92.8 96.8 96.3 11.9 96.5 89.1 95.4 95.0 14.2 94.1 72.9 88.0 84.1NP-modr 8.6 88.4 80.3 - 91.5 8.5 82.9 71.8 - 87.9 4.4 69.0 54.2 - 80.5VP-aux 5.5 97.9 94.0 - 96.1 5.0 96.5 91.0 - 94.6 6.2 94.4 81.7 - 86.7SBAR-s 3.7 96.1 91.1 91.8 95.3 3.5 94.3 87.2 86.4 93.5 4.0 84.8 68.2 81.9 81.9ADVP-t 2.7 95.2 93.3 93.9 98.6 3.0 89.6 84.5 88.0 95.9 4.5 84.0 78.2 80.3 96.8NML-t 2.3 91.6 90.3 97.6 99.8 2.6 85.6 82.2 93.5 99.8 0.7 42.1 37.7 88.8 100.0ADJP-t 1.9 94.6 88.4 95.5 94.6 1.8 86.8 77.0 93.6 90.7 2.5 84.7 67.0 88.1 84.2QP-t 1.0 95.3 93.8 98.3 99.6 1.2 91.0 89.0 97.1 100.0 0.2 57.7 57.7 94.4 100.0NP-crd 0.8 80.3 73.7 - 92.4 0.6 68.6 58.4 - 86.1 0.5 55.3 47.8 - 88.1VP-crd 0.4 84.3 82.8 - 98.2 0.4 75.3 73.5 - 97.6 0.8 65.5 58.3 - 89.8S-crd 0.3 83.7 83.2 - 99.6 0.4 70.9 68.6 - 96.7 0.8 68.5 63.0 - 93.4SQ-v 0.1 88.3 82.0 93.3 97.8 0.1 66.7 66.7 88.9 100.0 0.9 81.9 72.4 93.4 95.8FRAG-nt 0.1 49.9 48.6 95.4 97.9 0.1 28.6 28.6 100.0 100.0 0.8 22.7 21.3 96.3 96.3Table 2: Scores for the most frequent categories of brackets in the three datasets of corpora, as determinedby the regexes.
% gold is the frequency of this regex type compared to all the brackets in the gold.
F-his the score based on matching heads, F-s also incorporates the span information, att is the attachmentaccuracy for words that match in F-h, and spanR is the span-right accuracy for words that match in F-h.the reliance on sequences of individual strings.73 Analysis of parsing resultsWe worked with the three datasets as describedin the introduction.
We trained the parser on sec-tions 2-21 of OntoNotes WSJ, and parsed the threedatasets with the gold tags, since at present wewish to analyze the parser performance in isola-tion from Part-of-Speech tagging errors.
Table 1shows the sizes of the three corpora in terms oftokens and brackets, for both the gold and parsedversions, with the evalb scores for the parsed ver-sions.
The score is lower for Answers, as alsofound by Petrov and McDonald (2012).To facilitate comparison of our analysis withevalb, we used corpora versions with the samebracket deletion (empty yields and most punctua-tion) as evalb.
We ran the gold and parsed versionsthrough our regex decomposition and derivationtree creation.
Table 1 shows the number and per-centage of brackets handled by our regexes.
Thehigh coverage (%) reinforces the point that there isa limited number of core structures in the treebank.In the results below in Table 2 and Figure 6 wecombine the nonterminals that are not covered byone of the regexes with the simple non-recursiveregex case for that nonterminal.87In future work we will compare our approach to thatof Kummerfeld et al (2012), who also move beyond evalbscores in an effort to provide more meaningful error analysis.8We also combine a few other non-recursive regexes to-gether with NP-t, such as the special one for possessives.We present the results in two ways.
Table 2 liststhe most frequent categories in the three datasets,with their percentage of the overall number ofbrackets (%gold), their score based just on thehead identification (F-h), their score based on headidentification and (left and right) span (F-s), andthe attachment (att) and span-right (spanR) scoresfor those that match based on the head.9The two graphs in Figure 6 show the cumu-lative results based on F-h and F-s, respectively.These show the cumulative score in order of thefrequency of categories.
For example, for sections2-21, the score for NP-t is shown first, with 30.7%of the brackets, and then together with the VP-tcategory, they cover 45.2% of the brackets, etc.10The benefit of the approach described here is thatnow we can see the contribution to the evalb scoreof the particular types of constructions, and withinthose constructions, how well the parser is doingat getting the same head projection, but failing or9The score for the left edge is almost always very high forevery category, and we just list here the right edge score.
Theattachment score does not apply to the recursive categories,as mentioned above.10The final F-s value is lower than the evalb score - e.g.92.5 for sections 2-21 (the rightmost point in the graph forsections 2-21 in the F-s graph in Figure 6) compared to the93.8 evalb score.
Space prevents full explanation, but thereare two reasons for this.
One is that there are cases in whichbracket spans match, but the head, as found by our regexes, isdifferent in the gold and parser trees.
The other cases is whenbrackets match, and may even have the same head, but theirregex is different.
In future work we will provide a full ac-counting of such cases, but they do not affect the main aspectsof the analysis.671F-scores by head identificationcumulative % of all brackets0 5 10 20 30 40 50 60 70 80 90 10089.291.293.295.297.299.02-211 23456789101114131512221 23456789101114131512answers1243685710121314159111:NP-t     2:VP-t3:PP-t     4:S-vp5:NP-modr  6:VP-aux7:SBAR-s   8:ADVP-t9:NML-t   10:ADJP-t11:QP-t    12:SQ-vp13:S-crd   14:VP-crd15:FRAG-ntF-scores by head identification and spancumulative % of all backets0 5 10 20 30 40 50 60 70 80 90 10078.082.086.090.094.097.62-2112345678910111413151222123456789101114131512answers1243685710 121314159111:NP-t     2:VP-t3:PP-t     4:S-vp5:NP-modr  6:VP-aux7:SBAR-s   8:ADVP-t9:NML-t   10:ADJP-t11:QP-t    12:SQ-vp13:S-crd   14:VP-crd15:FRAG-ntFigure 6: Cumulative scores based on F-h (left) and F-s (right).
These graphs are both cumulative inexactly the same way, in that each point represents the total percentage of brackets accounted for so far.So for the 2-21 line, point 1, meaning the NP non-recursive regex, accounts for 30.7% of the brackets,point 2, meaning the VP non-recursive regex, accounts for another 13.5%, so 44.2% cumulatively, etc.not on the spans.3.1 Analysis and future workAs this is work-in-progress, the analysis is not yetcomplete.
We highlight a few points here.
(1) The high performance on the OntoNotes WSJmaterial is in large part due to the score on thenon-recursive regexes of NP-t, VP-t, S-vp, and theauxiliaries (points 1, 2, 4, 6 in the graphs).
Criticalto this is the fact that the parser does well on deter-mining the right edge of verbal structures, whichaffects the F-s score for VP-t (non-recursive), VP-aux, and S-vp.
The spanR score for VP-t is 95.8for Sections 2-21 and 93.7 for Section 22.
(2) We wouldn?t expect the test-on-training evalbscore to be 100%, since it has to back off fromthe training data, but the results for the differentcategories vary widely, with e.g., the NP-modr F-h score much lower than other frequent regexes.This variance from the test-on-training dataset car-ries over almost exactly to Section 22.
(3) The different distribution of structures inAnswers hurts performance.
For example, themediocre performance of the parser on SQ-vpbarely affects the score with OntoNotes, but hasa larger negative effect with Answers, due to itsincreased frequency in the latter.
(4) While the different distribution of construc-tions is a problem for Answers, more critical isthe poor performance of the parser on determin-ing the right edge of verbal constructions.
This isonly 85.4 for VP-t in Answers, compared to theOntoNotes results mentioned in (1).
Since this af-fects the F-s scores for VP-t, VP-aux, and S-vp,the negative effect is large.
Preliminary investi-gation shows that this is due in part to incorrectPP and SBAR placement (the PP-t and SBAR-sattachment scores (80.7 and 81.9) are worse forAnswers compared to Section 22 (86.1 and 86.4)),and coordinated S-clauses with no conjunction.In sum, there is a wealth of information fromthis new type of analysis that we will use in our on-going work to better understand what the parser islearning and how it works on different genres.AcknowledgmentsThis material is based upon work supported by Na-tional Science Foundation Grant # BCS-114749(first, fourth, and sixth authors) and by the DefenseAdvanced Research Projects Agency (DARPA)under Contract No.
HR0011-11-C-0145 (first,second, and third authors).
The content does notnecessarily reflect the position or the policy of theGovernment, and no official endorsement shouldbe inferred.672ReferencesSrinivas Bangalore and Aravind K. Joshi, editors.2010.
Supertagging: Using Complex Lexical De-scriptions in Natural Language Processing.
MITPress.Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.2012.
English Web Treebank.
LDC2012T13.
Lin-guistic Data Consortium.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,Department of Computer and Information Sciences,University of Pennsylvania.A.K.
Joshi and Y. Schabes.
1997.
Tree-adjoininggrammars.
In G. Rozenberg and A. Salomaa, ed-itors, Handbook of Formal Languages, Volume 3:Beyond Words, pages 69?124.
Springer, New York.Seth Kulick, Ann Bies, and Justin Mott.
2011.
Usingderivation trees for treebank error detection.
Asso-ciation for Computational Linguistics.Seth Kulick, Ann Bies, and Justin Mott.
2012.
Usingsupertags and encoded annotation principles for im-proved dependency to phrase structure conversion.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 305?314, Montr?eal, Canada, June.
Associa-tion for Computational Linguistics.Seth Kulick, Ann Bies, Justin Mott, MohamedMaamouri, Beatrice Santorini, and Anthony Kroch.2013.
Using derivation trees for informative tree-bank inter-annotator agreement evaluation.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages550?555, Atlanta, Georgia, June.
Association forComputational Linguistics.Jonathan K. Kummerfeld, David Hall, James R. Cur-ran, and Dan Klein.
2012.
Parser showdown at thewall street corral: An empirical investigation of errortypes in parser output.
In Proceedings of the 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 1048?1059, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.LDC99T42, Linguistic Data Consortium, Philadel-phia.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Pro-ceedings of the First Workshop on Syntactic Analysisof Non-Canonical Language.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2008.
The Berkeley Parser.https://code.google.com/p/berkeleyparser/.Owen Rambow and Aravind Joshi.
1997.
A formallook at dependency grammars and phrase-structuregrammars, with special consideration of word-orderphenomena.
In L. Wanner, editor, Recent Trendsin Meaning-Text Theory, pages 167?190.
John Ben-jamins, Amsterdam and Philadelphia.Satoshi Sekine and Michael Collins.
2008.
Evalb.http://nlp.cs.nyu.edu/evalb/.Libin Shen and Aravind Joshi.
2008.
LTAG depen-dency parsing with bidirectional incremental con-struction.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 495?504, Honolulu, Hawaii, October.
As-sociation for Computational Linguistics.Libin Shen, Lucas Champollion, and Aravind Joshi.2008.
LTAG-spinal and the Treebank: A newresource for incremental, dependency and seman-tic parsing.
Language Resources and Evaluation,42(1):1?19.Ralph Weischedel, Martha Palmer, Mitchell Marcus,Eduard Hovy, Sameer Pradhan, Lance Ramshaw,Nianwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2011.
OntoNotes 4.0.
LinguisticData Consortium LDC2011T03.673
