Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 55?62,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPBasic Language Resources for Diverse Asian Languages:A Streamlined Approach for Resource CreationHeather Simpson, Kazuaki Maeda, Christopher CieriLinguistic Data ConsortiumUniversity of Pennsylvania3600 Market St., Suite 810Philadelphia, PA 19104, USA{hsimpson, maeda, ccieri}@ldc.upenn.eduAbstractThe REFLEX-LCTL (Research on En-glish and Foreign Language Exploitation-Less Commonly Taught Languages) pro-gram, sponsored by the United Statesgovernment, was an effort in simultane-ous creation of basic language resourcesand technologies for under-resourced lan-guages, with the aim to enrich sparse ar-eas in language technology resources andencourage new research.
We were taskedto produce basic language resources for 8Asian languages: Bengali, Pashto, Pun-jabi, Tamil, Tagalog, Thai, Urdu andUzbek, and 5 languages from Europe andAfrica, and distribute them to research anddevelopment also funded by the program.This paper will discuss the streamlined ap-proach to language resource developmentwe designed to support simultaneous cre-ation of multiple resources for multiple lan-guages.1 IntroductionOver the past decade, the scope of interest in lan-guage resource creation has increased across mul-tiple disciplines.
The differing approaches of thesedisciplines are reflected in the terms used for newlytargeted groups of languages.
Less commonlytargeted languages (LCTLs) research may focuson development of basic linguistic technologiesor language-aware applications.
The REFLEX-LCTL (Research on English and Foreign LanguageExploitation-Less Commonly Taught Languages)program, sponsored by the United States govern-ment, was an effort in simultaneous creation of ba-sic language resources and technologies for LCTLs,namely languages which have large numbers ofspeakers, but are nonetheless infrequently studiedby language learners or researchers in the U.S.Under the REFLEX-LCTL program, we pro-duced ?language packs?
of basic language resourcesfor 13 such languages.
This paper focuses on theresources created for the 8 Asian languages: Ben-gali, Pashto, Punjabi, Tamil, Tagalog, Thai, Urduand Uzbek.1Our approach to language pack creation was tomaximize our resources.
We accomplished this in anumber of ways.
We engaged in thorough planningto identify required tasks and their interdependen-cies, and the human and technical resources neededto accomplish them.
We focused intensely on iden-tifying available digital resources at the start oflanguage pack creation, so we could immediatelybegin assessment of their usability, and work onfilling the resource gaps identified.
We developedannotation tools usable for multiple tasks and alllanguages and designed to make manual annota-tion more efficient.
We developed standards fordata representations, to support efficient creation,processing, and end use.2 Planning For Basic LanguageResources Creation2.1 Language SelectionThe selection of REFLEX-LCTL target languageswas based on a number of criteria, while operatingwithin a fixed budget.
The most basic criterionwas that the language be spoken by large num-ber of native speakers, but poorly represented interms of available language resources.
The Indiclanguages (Bengali, Punjabi, Urdu) were chosento give researchers the opportunity to experimentwith bootstrapping techniques with resources in re-lated languages.
In order to maximize the useful-ness and generality of our methods, the projectadopted the additional goals of variation in the ex-pected availability of raw resources and also varia-tion in linguistic characteristics both within the setof selected languages and in comparison to morewell-resourced languages.
Though we are focus-ing, in this paper, on the Asian languages, LCTLlanguages are linguistically and geographically di-verse, representing major language families andmajor geographical regions.The following short descriptions of the AsianLCTL languages are intended to provide some con-text for the language pack discussions.
The lan-1The other languages were: Amazigh (Berber),Hungarian, Kurdish, Tigrinya and Yoruba.55guage demographic information is taken from Eth-nologue (Gordon, 2005).2.2 BengaliBengali is spoken mostly in Bangladesh and India.The language pack for Bengali was the first com-plete language pack we created, and it served asa pilot language pack for the rest of the project.There were a relatively large number of raw mate-rials and existing lexicons to support our lexicondevelopment, and our language pack included thelargest lexicon among the Asian language packs.2.3 UrduUrdu, spoken primarily in Pakistan and part ofIndia, is closely related to Hindi.
Urdu is tra-ditionally written using Perso-Arabic script, andhas vocabulary borrowed from Arabic and Persian.This was a language which had a large amount ofavailable digital resources in comparison with otherLCTLs, but did not meet our original expectationsfor raw digital text.2.4 ThaiThai is a Tai-Kadai language spoken by more than20 million people, mainly in Thailand.
Thai wasanother language which was relatively rich in avail-able digital language resources.
The Thai languagepack includes the largest amount of monolingualtext and found parallel text among the languagepacks.
For Thai, tokenization, or word segmenta-tion, was probably the most challenging aspect ofthe resource creation effort.
For the initial versionof the language pack, we used a tokenization tooladopted from an opensource software package.2.5 TamilTamil is a Dravidian language with more than 60million speakers in India, Sri Lanka, Singapore andother countries.
We benefited from having locallanguage experts available for consultation on thislanguage pack2.6 PunjabiPunjabi, also written as Panjabi, is an Indo-European language spoken in both India and Pak-istan.
Ethnologue and ISO 639-3 distinguish threevariations of Punjabi: Eastern, Mirpur and West-ern, and the Eastern variation has the largest pop-ulation of speakers.
We were able to obtain rela-tively large amounts of monolingual text and ex-isting parallel text.2.7 TagalogTagalog is an Austronesian language spoken by 15million people, primarily in the Philippines.
Themonolingual text we produced is the smallest (774K words) among the eight Asian language packs,due in part to the prevalence of English in for-mal communication mediums such as news publi-cations.2.8 PashtoPashto an Indo-European language spoken primar-ily in Afghanistan and parts of Pakistan.
It is oneof the two official languages in Afghanistan.
Eth-nologue and ISO 639-3 distinguish three varietiesof Pashto: Northern, Central and Southern.
Ma-jor sources of data for this language pack includedBBC, Radio Free America and Voice of America.2.9 UzbekUzbek is primarily spoken in Uzbekistan and inother Asian republics of the former Soviet Union.The creation of the language pack for Uzbek, whichis a Turkic language, and the official language ofUzbekistan, was outsourced to BUTE (BudapestUniversity of Technology and Economics) in Hun-gary.
Even though the Uzbek government officiallydecided to use a Latin script in 1992, the Cyrillicalphabet used between the 1940?s and 1990?s arestill commonly found.
Our language pack containsall resources in Latin script and includes an en-coding converter for converting between the Latinscript and the Cyrillic script.2.10 Designing Language PacksWithin the REFLEX-LCTL program, a languagepack is a standardized package containing the fol-lowing language resources:?
Documentation?
Grammatical Sketch?
Data?
Monolingual Text?
Parallel Text?
Bilingual Lexicon?
Named Entity Annotated Text?
POS Tagged Text?
Tools?
Tokenizer?
Sentence Segmenter?
Character Encoding Conversion Tool?
Name Transliterators?
Named Entity Tagger?
POS Tagger?
Morphological AnalyzerGrammatical sketches are summaries (approxi-mately 50 pages) of the features of the written lan-guage.
The primary target audience are languageengineers with a basic grounding in linguistic con-cepts.56Monolingual text is the foundation for all otherlanguage pack resources.
We provided monolingualtext in both tokenized and non-tokenized format.Parallel text is an important resource for devel-opment of machine translation technologies, andallows inductive lexicon creation.
The bilinguallexicons also support a variety of language tech-nologies.
The named entity annotations and partof speech tagged text can be used to create auto-matic taggers.The language packs also include basic data pro-cessing and conversion tools, such as tokenizers,sentence segmenters, character encoding convert-ers and name transliterators, as well as more ad-vanced tools, such as POS taggers, named entitytaggers, and morphological analyzers.These language packs include 6 of the 9 text re-sources and tools in 4 of the 15 text-based moduleslisted in the current BLARK matrix (ELDA, 2008).When we had a relatively stable definition of thedeliverables for language pack, we were able to be-gin planning for the downstream processes3 Standards for DataRepresentationAn important step in planning was to define stan-dards for language pack data representation, toallow all downstream processes to run more effi-ciently.3.1 Language CodesWe decided to use the ISO 639-32 (also Ethnologue,15th edition (Gordon, 2005)) three-letter languagecodes throughout the language packs.
For exam-ple, the language code is stored in every text datafile in the language packs.
The ISO 639-3 lan-guage codes for our eight languages are as follows:Urdu (URD), Thai (THA), Bengali (BEN), Tamil(TAM), Punjabi (PAN), Tagalog (TGL), Pashto(PBU) and Uzbek (UZN).
When there were multi-ple ISO 639-3 codes for a target language, the codefor the sublanguage for which the majority of thewritten text can be obtained was used.3.2 File FormatsOne of the first tasks in planning for this datacreation effort was to define file formats for themonolingual text, parallel text, lexicons and an-notation files.
This designing process was led byus and a group of experts selected from the re-search sites participating in the REFLEX-LCTLprogram.
The requirements included the follow-ing:?
Monolingual and parallel text files should beable to represent sentence segmentation, and2See http://www.sil.org/iso639-3/ for more de-tailsboth tokenized and non-tokenized text.?
For parallel text, the text and the target lan-guage and the translations in English shouldbe stored in separate aligned files.?
Unique IDs should be assigned to sen-tences/segments, so that the segment-levelmapping in parallel text is clear.?
Annotation files should be in a stand-off for-mat: i.e., annotations should be separate fromthe source text files.?
Lexicon files should be able to represent at theminimum of word, stem, part-of-speech, glossand morphological analysis.?
File formats should be XML-based.?
Files should be encoded in UTF-8 (UNI-CODE).After several cycles of prototyping and exchang-ing feedback, we settled on the following originalfile formats named ?LCTL text format?
(LTF - filename extension: .ltf.xml), ?LCTL annotation for-mat?
(LAF - file name extension: .laf.xml), and?LCTL lexicon format?
(LLF - file name exten-sion .llf.xml.
Appendix A shows the DTD for LTFformat.3.3 Evaluation and Training DataPartitionTo support evaluation of language technologiesbased on the data included in the language packs,we designated approximately 10% of our primarydata as the evaluation data and the rest as thetraining data.
Any data that was included as ?as-is?, (e.g.
found parallel text), was included in thetraining partition.3.4 Directory Structure and File NamingConventionsGiving all language packs a consistent design andstructure allows users to navigate the contents withease.
As such, we defined the directory structurewithin each language pack to be the following.The top directory was named as follows.LCTL_{Language}_{Version}/For example, the version 1.0 of the Urdu lan-guage pack would have the top directory namedLCTL Urdu v1.0.The top directory name is also used as the officialtitle for the package, so the full name rather thanthe language code was used for maximum clarityfor users not familiar with the ISO coding.Under the main directory, the following subdi-rectories are defined:57Documentation/Grammatical_Sketch/Tools/Lexicon/Monolingual_Text/Parallel_Text/Named_Entity_Annotations/POS_Tagged_Text/The Parallel Text directory was divided into?Found?
and ?Translation?
directories.
The Founddirectory contains parallel text that was availableas raw digital text, which we processed into ourstandardized formats.
The Translation directorycontains manually translated text, created by ourtranslators or subcontractors as well as part offound parallel text which we were able to alignat the sentence-level.
The data directories (e.g.,Monolingual Text, Parallel Text, Named EntityAnnotation, POS Tagged Text) were further di-vided into evaluation data (?Eval?)
and trainingdata (?Train?)
directories as requested by the pro-gram.We used the following format for text corporafile names wherever possible:{SRC}_{LNG}_{YYYYMMDD}.
{SeqID}{SRC} is a code assigned for the source; {LNG}is the ISO 639-3 language code; {YYYYMMDD}is the publication/posting date of the document;and {SeqID} is a unique ID within the documentsfrom the same publication/posting date.4 Building Technical InfrastructureIn creating the language resources included in theLCTL language packs, we developed a variety ofsoftware tools designed for humans, including na-tive speaker informants without technical exper-tise, to provide data needed for the resource cre-ation efforts as efficiently as possible.
In particular,the following tools played crucial roles in the cre-ation of language packs.4.1 Annotation Collection Kit Interface(ACK)In order to facilitate efficient annotation of a vari-ety of tasks and materials, we created a web-basedjudgment/annotation tool, named the AnnotationCollection Kit interface (ACK).
ACK allows a taskmanager to create annotation ?kits,?
which consistof question text and predefined list and/or free-form answer categories.
Any UTF-8 text may bespecified for questions or answers.
ACK is idealfor remote creation of multiple types of text-basedannotation, by allowing individual ?kits?
to be up-loaded onto a specific server URL which any re-mote user can access.
In fact, using this toolwe were able to support native speaker annota-tors working on part-of-speech (POS) annotationin Thailand.When annotators make judgments in ACK, theyare stored in a relational database.
The results canbe downloaded in CSV (comma-separated value) orXML format, so anyone with secure access to theserver can easily access the results.ACK was designed so that anyone with even abasic knowledge of a scripting language such asPerl or Python would be able to create the ACKannotation kits, which are essentially sets of datacorresponding to a sets of annotation decisions inthe form of radio buttons, check boxes, pull-downmenus, or comment fields.
Indeed some of the lin-guists on the LCTL project created their own ACKkits when needed.
Although they are limited inscope, creative use of ACK kits can yield a greatdeal of helpful types of annotation.For example, for POS annotation, the annota-tors were given monolingual text from our corpus,word by word, in order, and asked to select thecorrect part of speech for that word in context.We also used ACK to add/edit glosses and partof speech tags for lexicon entries, to perform mor-phological tagging, and various other tasks thatrequired judgment from native speakers.Figure 1: ACK - Annotation Collection KitFigure 1 shows a screen shot of ACK.4.2 Named Entity Annotation ToolFor named entity annotation task, we chose to em-ploy very simple annotation guidelines, to facili-tate maximum efficiency and accuracy from native-speaker annotators regardless of linguistic training.We used an named entity (NE) annotation toolcalled SimpleNET, which we previously developedfor the named entity annotation task for anotherproject.
SimpleNET requires almost no trainingin tool usage, and annotations can be made eitherwith the keyboard or the mouse.
The NE anno-tated text in the LCTL language packs was createdwith this tool.
This tool is written in Python us-ing the QT GUI toolkit, which allows the display58of bidirectional text.Figure 2: SimpleNET Annotation ToolFigure 2 shows a screen shot of SimpleNET.4.3 Named Entity Taggers and POSTaggersWe created common development frameworks forcreating named entity taggers and part-of-speechtaggers for the LCTL languages.
These frame-works allowed us to create taggers for any newlanguage given enough properly-formatted trainingdata and test data.
Included are core code writtenin Java as well as data processing utilities writtenin Perl and Python.
The framework for creatingPOS taggers was centered around the MALLETtoolkit (McCallum, 2002).34.4 Data Package Creation andDistributionAs per LDC?s usual mechanisms for small corpora,language packs were to be packaged as a tar gzip(.tgz) file, and distributed to the REFLEX-LCTLparticipating research sites.
The distribution of thecompleted languages packs were handled by our se-cure web downloading system.
Access instructionswere sent to the participating research sites, andall downloads were logged for future reference.5 Steps for Creating EachLanguage Pack5.1 Identifying Local Language Expertsand Native SpeakersAn intermediate step between planning and cre-ation was to identify and contact any available lo-cal experts in the targeted languages, and recruitadditional native speakers to serve as annotatorsand language informants.
Annotators were notnecessarily linguists or other language experts, but3We thank Partha Pratim Talukdar for providingframeworks for creating taggers.they were native speakers with reading and writingproficiencies who received training as needed fromin-house language experts for creating our anno-tated corpora, and helped to identify and evaluateharvestable online resources.Intensive recruiting efforts were conducted fornative speakers of each language.
Our recruitingstrategy utilized such resources as online discussionboards and student associations for those languagecommunities, and we were also able to capitalizeon the diversity of the student/staff body at theUniversity of Pennsylvania by recruiting throughposted signs on campus.5.2 Identifying Available LanguageResourcesThe first step in creating each language pack wasto identify resources that are already available.
Tothis end we implemented a series of ?Harvest Fes-tivals?
; intensive sessions where our entire team,along with native speaker informants, convenedto search the web for available resources.
Avail-able resources were immediately documented ona shared and group editable internal wiki page.By bringing together native speakers, linguists,programmers, information managers and projectsmanagers in the same room, we were able to min-imize communications latency, brainstorm as agroup, and quickly build upon each other?s efforts.This approach was generally quite successful, es-pecially for the text corpora and lexicons, andbrought us some of our most useful data.5.3 Investigating Intellectual PropertyRightsAs soon as Raw digital resources were identi-fied, our local intellectual property rights special-ist began investigation into their usability for theREFLEX-LCTL language packs.
It was necessaryto contact many individual data providers to ob-tain an agreement, so the process was quite lengthyand in some cases permission was not granted un-til shortly before the package was scheduled forrelease to the REFLEX community.
Our generalpractice was to process all likely candidate datapools and remove data as necessary in later stages,thus ensuring that IPR was not a bottleneck inlanguage pack creation.
The exception to this wasfor large data sources, where removal would havesignificantly affected the quantity of data in thedeliverable.5.4 Creating Basic Text Processing ToolsThe next step was to create the language-specificbasic data processing tools, such as encoding con-verter, sentence segmenter and tokenizer.The goal for this project was to include whateverencoding converters were needed to convert all of59the raw text and lexical resources collected or cre-ated into the standard encoding selected for thattarget language.Dividing text into individual sentences is a nec-essary first step for many processes including thehuman translation that dominated much of our ef-fort.
Simple in principle, LCTL sentence segmen-tation can prove tantalizingly complex.
Our goalwas to produce a sentence segmenter that acceptstext in our standard encoding as input and outputssegmented sentences in the same encoding.Word segmentation, or tokenization, is also chal-lenging for languages such as Thai.
For Thai,our approach was to utilize an existing opensourceword segmentation tool, and enhancing it by usinga larger word list than the provided one.We designed the basic format conversion tools,such as the text-to-LTF converter, to be able tojust plug in language-specific tokenizers and seg-menters.5.5 Creating Monolingual TextThe monolingual text corpora in the languagespacks were primarily created by identifying andharvesting available resources from the Internet,such as news, newsgroups and weblogs in the tar-get language.
Once the IPR expert determinedthat we can use the resources for the REFLEX-LCTL program, we harvested the document files ?recent documents as archived documents.
The har-vested files were then analyzed and the files thatactually have usable contents, such as news arti-cles and weblog postings were kept and convertedinto the LCTL Text format.
The formatting pro-cess was typically done in the following steps: 1)convert the harvested document or article in htmlor other web format to a plain text format, strip-ping html tags, advertisements, links and othernon-contents; 2) convert the plain text files intoUTF-8, 2) verify the contents with native speak-ers, and if necessary, further remove non-contents,or divide a file into multiple files; 3) convert theplain text files into the LCTL Text format, apply-ing sentence segmentation and tokenization.
Eachdocument file is assigned a unique document ID.Essential information about the document such asthe publication date was kept in the final files.5.6 Creating Parallel TextEach language pack contains at least 250,000 wordsof parallel text.
Part of this data was found re-sources harvested from online resources, such asbilingual news web site.
The found parallel doc-uments were converted into the LTF format, andaligned at the sentence level, producing segment-mapping tables between the LTF files in the LCTLlanguage and the LTF files in English.The rest of this data was created by manuallytranslating documents in the LCTL language intoEnglish, or documents in English into the LCTLlanguage.
A subset of the monolingual text corpuswas selected for translation into English.In addition, about 65,000 words of Englishsource text were selected as the ?Common EnglishSubset?
for translation into each LCTL language.Having the same set of parallel documents for alllanguages will facilitate comparison between anyor all of the diverse LCTL languages.
The Com-mon Subset included : newswire text, weblogs, aphrasebook and an elicitation corpus.
The phrase-book contained common phrases used in daily life,such as ?I?m here?, and ?I have to go?.
Theelicitation corpus, provided by Carnegie MellonUniversity (Alvarez et al, 2006), included expres-sions, such as ?male name 1 will write a bookfor female name 1, where male name 1 and fe-male name 1 are common names in the LCTL lan-guage.
The set of elicitation expressions is designedto elicit lexical distinctions which differ across lan-guages.The manual translation tasks were outsourcedto translation agencies or independent translators.Since the translators were instructed to translatetext which had already been sentence-segmented,the creation of sentence-level mappings was trivial.However, we found that it was important to createa sentence-numbered template for the translatorsto use, otherwise we were likely to receive trans-lations where the source text sentence boundarieswere not respected.5.7 Creating LexiconsBilingual lexicons are also an important resourcethat can support a variety of human language tech-nologies, such as machine translation and informa-tion extraction.
The goal for this resource was alexicon of at least 10,000 lemmas with glosses andparts of speech for each language.
For most of thelanguages, we were able to identify existing lexi-cons, either digital or printed, to consult with andextract information for a subset of the lexical en-tries; however, in all cases we needed to processthem substantially before they could be used effi-ciently.
We performed quality checking, normaliz-ing, added parts of speech and glosses, added en-tries and removed irrelevant entries.5.8 Creating Annotated CorporaA subset of the target language text in each lan-guage pack received up to three types of annota-tions: part-of-speech tags, morphological analysis,and named entity tags.
Named entity annotationswere created for all language packs.Annotations were created by native speakers us-ing the annotation tools discussed in section 4.605.9 Creating Morphological AnalyzersTo address the requirement to include some kindof tool for morphological analysis in each languagepack, we created either a morphological analyzerimplementing hand-written rules or a stemmer us-ing an unsupervised statistical approach, such asthe approach described in (Hammarstrom, 2006).5.10 Creating Named Entity TaggersWe created a named entity tagger for each lan-guage pack using our common development frame-work for named entity taggers4.3.
The tagger wascreated using the named entity annotated text wecreated for the language packs.5.11 Creating Part-of-Speech TaggersSimilarly, we created a POS tagger for each lan-guage pack using our common development frame-work for POS taggers (See Section 4.3).
We coor-dinated the POS tag sets for the taggers and lexi-cons.5.12 Creating Name TransliteratorsA transliterator that converts the language?s na-tive script into Latin script is a desired resource.For some languages, this is not a straightforwardtask.
For example, not all vowels are explicitly rep-resented in Bengali script, and there can be mul-tiple pronunciations possible for a given Bengalicharacter.
Names, especially names foreign to thetarget language exhibit a wide variety of spelling,and in HLTs, make up a large percentage of theout-of-vocabulary terms.
We focused on creatinga transliterator to for romanization of names in theLCTL languages.
This resource was generally cre-ated by the programming team with consultationfrom native speakers.5.13 Writing Grammatical SketchesThe grammatical sketches provide an outline of thefeatures of the written language, to provide thelanguage engineers with description of challengesspecific to the languages in creating language tech-nologies.
These sketches were written mainly bysenior linguists in our group, for readers who donot necessarily have training in linguistics.
Theformat of these documents was either html or pdf.6 Summary of CompletedLanguage PacksTable 1 summarizes the contents of the 8 Asianlanguage packs .4 All of the language packs havealready been distributed to REFLEX-LCTL par-ticipating research sites.
The packs continue tobe used to develop and test language technologies.For example, the Urdu pack was used to support a4The numbers represent the number of tokens.task in the 2006 NIST Open MT Evaluation cam-paign (of Standards and Technology, 2009).
Oncea language pack has been used for evaluation it willbe placed into queue for general release.7 ConclusionWe have developed an efficient approach for creat-ing basic text language resources for diverse lan-guages.
Our process integrated the efforts of soft-ware programmers, native speakers, language spe-cialists, and translation agencies to identify andbuilt on already available resources, and create newresources as efficiently as possible.Using our streamlined processes, we were ableto complete language packs for eight diverse Asianlanguages.
We hope that the completed resourceswill provide valuable support for research and tech-nology development for these languages.We faced various challenges at the beginning ofthe project which led us to revisions of our meth-ods, and some of these challenges would surely beencountered during a similar effort.
We hope thatour approach as described here will be of service tofuture endeavors in HLT development for under-resourced languages.ReferencesAlison Alvarez, Lori S. Levin, Robert E. Fred-erking, Simon Fung, and Donna Gates.
2006.The MILE corpus for less commonly taught lan-guages.
In Proceedings of HLT-NAACL 2006.ELDA.
2008.
BLARK Resource/ModulesMatrix.
From Evaluations and Language Re-sources Distribution Agency (ELDA) web sitehttp://www.elda.org/blark/matrice res mod.php, accessed on 2/23/2008.Raymond G. Gordon, Jr., editor.
2005.
Eth-nologue: Languages of the World, Fifteenthedition, Online version.
SIL International.http://www.ethnologue.com/.Harald Hammarstrom, 2006.
Poor Man?s Stem-ming: Unsupervised Recognition of Same-StemWords.
Springer Berlin / Heidelberg.Andrew Kachites McCallum.
2002.
MAL-LET: A machine learning for language toolkit.http://mallet.cs.umass.edu.National Institute of Standards andTechnology.
2009.
NIST OpenMachine Translation Evaluation.http://www.itl.nist.gov/iad/mig/tests/mt/,accessed on June 7, 2009.61Urdu Thai Bengali Tamil Punjabi Tagalog Pashto UzbekMono Text 14,804 39,700 2,640 1,112 13,739 774 5,958 790Parallel Text (L ?
E) 1,300 694 237 308 203 180 206Parallel Text (Found) 947 1,496 243 230Parallel Text (E ?
L) 65 65 65 65 65 65 65 65Lexicon 26 232 482 10 108 18 10 25Encoding Converter Yes Yes Yes Yes Yes Yes Yes YesSentence Segmenter Yes Yes Yes Yes Yes Yes Yes YesWord Segmenter Yes Yes Yes Yes Yes Yes Yes YesPOS Tagger Yes Yes Yes Yes Yes Yes Yes YesPOS Tagged Text 5 5 59Morphological Analyzer Yes Yes Yes Yes Yes Yes Yes YesMorph-Tagged Text 11 144NE Annotated Text 233 218 138 132 157 136 165 93Named Entity Tagger Yes Yes Yes Yes Yes Yes Yes YesName Transliterator Yes Yes Yes Yes Yes Yes Yes YesDescriptive Grammar Yes Yes Yes Yes Yes Yes Yes YesTable 1: Language Packs for Asian Languages (Data Volume in 1000 Words)A DTD for LTF Files<!ELEMENT LCTL_TEXT (DOC+) ><!ATTLIST LCTL_TEXT lang CDATA #IMPLIEDsource_file CDATA #IMPLIEDsource_type CDATA #IMPLIEDauthor CDATA #IMPLIEDencoding CDATA #IMPLIED ><!ELEMENT DOC (HEADLINE|DATELINE|AUTHORLINE|TEXT)+ ><!ATTLIST DOC id ID #REQUIREDlang CDATA #IMPLIED><!ELEMENT HEADLINE (SEG+) ><!ELEMENT DATELINE (#PCDATA) ><!ELEMENT AUTHORLINE (#PCDATA) ><!ELEMENT TEXT (P|SEG)+ ><!ELEMENT P (SEG+) ><!ELEMENT SEG (ORIGINAL_TEXT?, TOKEN*) ><!ATTLIST SEG id ID #REQUIREDstart_token IDREF #IMPLIEDend_token IDREF #IMPLIEDstart_char CDATA #IMPLIEDend_char CDATA #IMPLIED><!ELEMENT ORIGINAL_TEXT (#PCDATA) ><!ELEMENT TOKEN (#PCDATA) ><!ATTLIST TOKEN id ID #REQUIREDattach (LEFT|RIGHT|BOTH)#IMPLIEDpos CDATA #IMPLIEDmorph CDATA #IMPLIEDgloss CDATA #IMPLIEDstart_char CDATA #IMPLIEDend_char CDATA #IMPLIED>62
