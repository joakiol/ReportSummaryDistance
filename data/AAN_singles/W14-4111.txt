Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 60?65,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPredicting MOOC Dropout over Weeks Using Machine Learning MethodsMarius Kloft, Felix Stiehler, Zhilin Zheng, Niels PinkwartDepartment of Computer ScienceHumboldt University of BerlinBerlin, Germany{kloft, felix.stiehler, zhilin.zheng, pinkwart}@hu-berlin.deAbstractWith high dropout rates as observed inmany current larger-scale online courses,mechanisms that are able to predict stu-dent dropout become increasingly impor-tant.
While this problem is partially solvedfor students that are active in online fo-rums, this is not yet the case for the moregeneral student population.
In this pa-per, we present an approach that works onclick-stream data.
Among other features,the machine learning algorithm takes theweekly history of student data into ac-count and thus is able to notice changesin student behavior over time.
In the laterphases of a course (i.e., once such his-tory data is available), this approach is ableto predict dropout significantly better thanbaseline methods.1 IntroductionIn the past few years, with their dramatically in-creasing popularity, Massive Open Online Courses(MOOCs) have become a way of online learningused across the world by millions of people.
Asa result of efforts conducted (sometimes jointly)by academia and industry, many MOOC providers(such as Coursera, Udacity, Edx, or iversity) haveemerged, which are able to deliver well-designedonline courses to learners.
In typical MOOC plat-forms, learners can not only access lecture videos,assignments and examinations, but can also usecollaborative learning features such as online dis-cussion forums.
Despite all the MOOC featuresand benefits, however, one of the critical issues re-lated to MOOCs is their high dropout rate, whichputs the efficacy of the learning technology intoquestion.
According to the online data providedby Jordan (2014), most MOOCs have comple-tion rates of less than 13%.
While discussionsare still ongoing as to whether these numbers areactually a problem indicating partial MOOC fail-ures or whether they merely indicate that the com-munity of MOOC learners is diverse and by farnot every participant intends to complete a course,researchers and MOOC providers are certainlyinterested in methods for increasing completionrates.
The analysis of MOOC data can be of helphere.
For instance, a linguistic analysis of theMOOC forum data can discover valuable indica-tors for predicting dropout of students (Wen etal., 2014).
However, only few MOOC students(roughly 5-10%) use the discussion forums (Roseand Siemens, 2014), so that dropout predictors forthe remaining 90% would be desirable.
In orderto get insights into the learning behaviors of thismajority of participants, the clickstream data ofthe MOOC platform usage is the primary sourcefor analysis in addition to the forum data.
That isalso the motivation of the shared task proposed bythe MOOC workshop at the Conference on Em-pirical Methods in Natural Language Processing(EMNLP 2014) (Rose and Siemens, 2014).
Ad-dressing this task, we propose a machine learningmethod based on support vector machines for pre-dicting dropout between MOOC course weeks inthis paper.The rest of this paper is organized as follows.We begin with the description of the data set andfeatures extracted from the data set.
We then de-scribe our prediction model.
Next, the predictionresults and some experimental findings are pre-sented.
Finally, we conclude our work in this pa-per.2 DatasetThe dataset we used in this paper was prepared forthe shared task launched by the Modeling LargeScale Social Interaction in Massively Open On-line Courses Workshop at the Conference on Em-pirical Methods in Natural Language Processing60(EMNLP 2014) (Rose and Siemens, 2014).
Thedata was collected from a psychology MOOCcourse which was launched in March 2013.
Thewhole course lasted for 12 weeks with 11,607 par-ticipants in the beginning week and 3,861 partici-pants staying until the last course week.
Overall,20,828 students participated, with approximately81.4% lost at last.
Note that the data cover thewhole life cycle of this online course up to 19weeks.
The original dataset for this task had twotypes of data: clickstream data and forum data.
Inthis paper, we only make use of clickstream datato train our prediction model and we do not furtherconsider forum data.
Obviously, this will lowerthe prediction quality for the 5% of students thatuse the forum, but it will hopefully shed light onthe utility of the clickstream data for the larger setof all participants.
The clickstream data includes3,475,485 web log records which can be gener-ally classified into two types: the page view logand the lecture video log.
In the following sec-tion, we will describe attributes extracted from theraw clickstream data which (we believed) could becorrelated to drop-out over the 12 course weeks.2.1 Attributes descriptionOur model is an attempt to predict the participants?drop-out during the next week (defined as no ac-tivity in that week and in any future week) usingthe data of the current and past weeks.
Conse-quently, all attributes are computed for each par-ticipant and for each week.
Note that this resultsin having more data for later course weeks, sincethe approach allows for comparing a student?s cur-rent activity with the activity of that student in thepast weeks.
The complete attributes list is shownin Table 1.2.2 Attribute GenerationThe attributes required for the predictions are ex-tracted by parsing the clickstream file where eachline represents a web request.
For each linethe corresponding Coursera ID is taken from thedatabase containing the forum data and the courseweek is calculated from the timestamp relative tothe start date of the course.
Then the request isanalysed regarding its type and every present at-tribute is saved.After collecting the raw attributes, the dataneeds to be post-processed.
There are 3 kinds ofattributes: attributes that need to be summed up,attributes that need to be averaged and attributesFigure 1: Several basic properties of the analyzeddata set.that need to be decided by majority vote.
Afterthe post-processing the data consists of lists of at-tributes each correlated to a unique tuple consist-ing of the Coursera ID and the course week num-ber.
Invalid attributes are getting replaced withthe median of that week.
Note that every missingweek is getting replaced by the median of the at-tributes of active users in that week that were alsoactive in the original week.2.3 A First Glance on the Data SetWe have visualized several basic properties of thedata in Figure 1.
We observe that the number ofactive user quickly decreases over time.
Further-more the dropout probability is especially high inthe first two weeks, and then of course at the endof the course starting around week 11 and 12.3 Methodology & ResultsIn this section we concisely describe the employedfeature extraction and selection pipeline, as wellas the employed machine learning algorithms.
Foreach week of the course (i = 1, .
.
.
, 19) we com-puted the dropout label of each of the nipartici-pants (user ids) being active in that week, basedon checking whether there is any activity associ-ated to the same user id in proceeding next week.61ID Attributes1 Number of requests: total number of requests including page views and video click actions2 Number of sessions: number of sessions is supposed to be a reflection of high engagement,because more sessions indicate more often logging into the learning platform3 Number of active days: we define a day as an active day if the student had at least onesession on that day4 Number of page views: the page views include lecture pages, wiki pages, homework pagesand forum pages5 Number of page views per session: the average number of pages viewed by each partici-pant per session6 Number of video views: total number of video click actions7 Number of video views per session: average number of video click actions per session8 Number of forum views: number of course discussion forum views9 Number of wiki views: number of course wiki page views10 Number of homework page views11 Number of straight-through video plays: this is a video action attribute.
Straight-troughplaying video means that the participates played video without any jump (e.g.
pause, re-sume, jump backward and jump forward).
Since the lecture videos are the most importantlearning resource for the learning participants, the video playing should be investigated asother researchers did (Brotherton and Abowd, 2004).
In this paper, five video behaviorsare taken into account including the number of full plays as well as four others: start-stopduring video plays, skip-ahead during video plays, relisten during video plays and the useof low play rate12 Number of start-stop during video plays: start-stop during video plays stands for a lecturevideo being paused and resumed13 Number of skip-ahead during video plays: skip-ahead means that the participant playeda video with a forward jump14 Number of relisten during video plays: relisten means that a backward jump was madeas the participant was playing a video15 Number of slow play rate use: this attribute is considered as an indicator of weak under-standing of the lecturer?s lecture presentation, possibly because of language difficulties or alack of relevant background knowledge16 Most common request time: our attempt with this attribute is to separate day time learningfrom night time learning.
We define night time from 19:00 to 6:59 in the morning and theother half day as day time17 Number of requests from outside of Coursera: this is to discover how many requestsfrom third-party tools (such as e-mail clients and social networks) to the course were made,which could be an indicator of the participant?s social behavior18 Number of screen pixels: the screen pixels is an indicator of the device that the studentused.
Typically, mobile devices come with fewer pixels19 Most active day: through this attribute, we can investigate if starting late or early couldhave an impact on dropout20 Country: this information could reflect geographical differences in learning across theworld21 Operating System22 BrowserTable 1: Attributes list.This resulted in label vectors yi?
{?1, 1}nifori = 1, .
.
.
, 19, where +1 indicates dropout (andthus ?1 indicates no dropout).
We experimentedon the 22 numerical features described in the pre-62vious section.
The features with ids 1?19 could berepresented a single real number, while all otherfeatures had to be embedded into a multidimen-sional space.
For simplicity we thus first focusedon features 1?19.
For each week i of the course,this results in a matrix Xpreliminaryi?
R19?ni,the rows and columns of which correspond to thefeatures and user ids, respectively.
We then en-riched the matrices by considering also the ?his-tory?
of the features, that is, for the data of weeki, all the features of the previous weeks were ap-pended (as additional rows) to the actual data ma-trix, resulting in Xi?
R19i?ni.
We can writethis as Xi= (x1, .
.
.
, xni), where xjis the fea-ture vector of the jth user.
Box plots of these fea-tures showed that the distribution is highly skewedand non-normal, and furthermore all features arenon-negative.
We thus tried two standard featurestransformations: 1. logarithmic transformation 2.box-cox transformation.
Subsequent box plots in-dicated that both lead to fairly non-skewed distri-butions.
The logarithmic transformation is how-ever much faster and lead to better results in laterpipeline steps, which is why it was taken for theremaining experiments.Subsequently, all features were centered andnormalized to unit standard deviation.
We thenperformed simple t-tests for each feature and com-puted also the Fisher score fj=??+???
?2++?2?, where?
?and ?2?are the mean and variance of the pos-itive (dropout) and negative class, respectively.Both t-tests and Fisher scores lead to comparableresults; however, we have made superior experi-ences with the Fisher score, which is why we focuson this approach in the following methodology.We found that the video features (id 11?15), themost common request time (id 17), and the mostactive day feature (id 19) consistently achievedscores very close to zero, which is why they werediscarded.
The remaining features are shown inFigure 2 (a similar plot was generated using t-testsand found to be consistent with the Fisher scores,but is omitted due to space constraints).
The re-sults indicate that features related to a more bal-anced behaviour pattern over the course of a week(especially the number of sessions and number ofactive days) were (weakly) predictive of dropoutin the beginning of the course.
From week 6 to12 we could also measure a rising importance ofthe number of wiki page views (id 9) and home-work submission page views (id 10).
Past week 12features related to activity in a more general waylike the number of requests (id 1) or the number ofpage views (id 4) became the most predicative.We proceeded with an exploratory analysis,where we performed a principal component anal-ysis (PCA) for each week, the result is shown inFigure 3.
The plot indicates that the users that havedropped out can be better separated from the usersthat did not drop out when the week id increases.To follow up on this we trained, for each week,a linear support vector machine (SVM) (Cortesand Vapnik, 1995) using the -s 2 option in LI-BLINEAR (Fan et al., 2008), which is one of thefastest solvers to train linear SVMs (Fan et al.,2008).
The SVM computes an affine-linear pre-diction function f(x) := ?w, x?+b, based on max-imizing the (soft) margin between positive andnegative examples: (w, b) := argminw,b12||w||2+C?ni=1max(0, 1?yi(?w, xi?+ b).
Note that thisis very similar to regularized logistic regression,which uses the term 1/(1+exp(?yi(?w, xi?+b)))instead of max(0, 1?yi(?w, xi?+ b), but with ad-ditional sparsity properties (only a subset of datapoints are active in the final solution) that makeit more robust to outliers.
The prediction accuracywas estimated via 5-fold cross validation.
The reg-ularization parameter was found to have little in-fluence on the prediction accuracy, which is why itwas set to the default value C = 1.
We comparedour SVM to the trivial baseline of a classifier thatconstantly predicts either -1 or 1; if the dropoutprobability in week i is denoted by pi, then theclassification accuracy of such a classifier is givenby acctrivial:= max(pi, 1?pi).
The result of thisexperiment is shown in Figure 4.
Note that wefound it beneficial to use the ?history?
features,that is the information about the previous weeksonly within the weeks 1?12.
For the weeks 13?19we switched the history features off (also the PCAabove is computed without the history features).We observe from the figure that for weeks 1?8 wecan not predict the dropout well, while then theprediction accuracy steadily increases.
Our hy-pothesis here is that this could result from the moreand more history features being available for thelater weeks.4 ConclusionWe proposed a machine learning framework forthe prediction of dropout in Massive Open On-line Courses solely from clickstream data.
At the63Figure 4: SVM classification accuracies per week.The baseline accuracy is computed as max(pi, 1?pi), where pidenotes the weekwise dropout prob-ability.heart of our approach lies the extraction of numer-ical features capturing the activity level of users(e.g., number of requests) as well technical fea-tures (e.g., number of screen pixels in the em-ployed device/computer).
We detected significantsignals in the data and achieved an increase in pre-diction accuracy up to 15% for some weeks of thecourse.
We found the prediction is better at the endof the course, while at the beginning we still detectrather weak signals.
While this paper focuses onclickstream data, the approach could in principlealso combined with forum data (e.g., using mul-tiple kernel learning (Kloft et al., 2011)), whichwe would like to tackle in future work.
Further-more, another interesting direction is to explorenon-scalar features (e.g., country, OS, browser,etc.)
and non-linear support vector machines.ReferencesKaty Jordan.
MOOC Completion Rates: The Data.Availabe at:http://www.katyjordan.com/MOOCproject.html.
[Accessed: 27/08/2014].Miaomiao Wen, Diyi Yang and Carolyn P. Rose.
Lin-guistic Reflections of Student Engagement in Mas-sive Open Online Courses.
ICWSM?14, 2014.Carolyn Rose and George Siemens.
Shared Task onPrediction of Dropout Over Time in Massively OpenOnline Courses.
Proceedings of the 2014 EmpiricalMethods in Natural Language Processing Workshopon Modeling Large Scale Social Interaction in Mas-sively Open Online Courses, Qatar, October 2014.Jason A. Brotherton and Gregory D. Abowd.
Lessonslearned from eClass: Assessing automated captureand access in the classroom.
ACM Transactionson Computer-Human Interaction, Vol.
11, No.
2, pp.121?155, June 2004.C.
Cortes and V. Vapnik.
Support-vector networks.Machine learning, 20(3):273?297, 1995.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
LIBLINEAR: A library for large linearclassification.
Journal of Machine Learning Re-search (JMLR), 9:1871?1874, 2008.M.
Kloft, U. Brefeld, S. Sonnenburg, and A. Zien.
`p-norm multiple kernel learning.
Journal of MachineLearning Research, 12:953?997, Mar 2011.64Figure 2: Fisher scores indicate which features are predictive of the dropout.
Features are ordered fromleft to right with increasing ids; i.e., pink indicates the number of requests (feature id 1), cyan the numberof sessions (feature id 2), etc.
In particular, we observe that features related to a more balanced behaviourpattern such as the number of active days (feature id 3) are the most important ones in the first couple ofweeks while more general features like the number of requests rise in importance past week 12.Figure 3: Result of principal component analysis.
The data becomes more non-isotropic within the laterweeks (from week 13), and can also be separated better.65
