Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 54?61,Sydney, July 2006. c?2006 Association for Computational LinguisticsManual Annotation of Opinion Categories in MeetingsSwapna Somasundaran1,    Janyce Wiebe1,    Paul Hoffmann2,    Diane Litman11Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 152602Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260{swapna,wiebe,hoffmanp,litman}@cs.pitt.eduAbstractThis paper applies the categories from anopinion annotation scheme developed formonologue text to the genre of multipartymeetings.
We describe modifications tothe coding guidelines that were requiredto extend the categories to the new typeof data, and present the results of an in-ter-annotator agreement study.
As re-searchers have found with other types ofannotations in speech data, inter-annotator agreement is higher when theannotators both read and listen to the datathan when they only read the transcripts.Previous work exploited prosodic cluesto perform automatic detection of speakeremotion (Liscombe et al 2003).
Ourfindings suggest that doing so to recog-nize opinion categories would be a prom-ising line of work.1 IntroductionSubjectivity refers to aspects of language thatexpress opinions, beliefs, evaluations and specu-lations (Wiebe et al 2005).
Many natural lan-guage processing applications could benefit frombeing able to distinguish between facts and opin-ions of various types, including speech-orientedapplications such as meeting browsers, meetingsummarizers, and speech-oriented question an-swering (QA) systems.
Meeting browsers couldfind instances in meetings where opinions aboutkey topics are expressed.
Summarizers could in-clude strong arguments for and against issues, tomake the final outcome of the meeting more un-derstandable.
A preliminary user survey(Lisowska 2003) showed that users would like tobe able to query meeting records with subjectivequestions like ?Show me the conflicts of opin-ions between X and Y?
, ?Who made the highestnumber of positive/negative comments?
and?Give me all the contributions of participant X infavor of alternative A regarding the issue I.?
AQA system with a component to recognize opin-ions would be able to help find answers to suchquestions.Consider the following example from a meet-ing about an investment firm choosing which carto buy1.
(In the examples, the words and phrasesdescribing or expressing the opinion are under-lined):(1)2 OCK: Revenues of lessthan a million and losses oflike five million you knowthat's patheticHere, the speaker, OCK, shows his strong nega-tive evaluation by using the expression ?That?spathetic.?
(2) OCK: No it might just bea piece of junk cheap pieceof junk that's not a goodinvestmentIn (2), the speaker uses the term ?just a piece ofjunk?
to express his negative evaluation and usesthis to argue for his belief that it is ?not a goodinvestment.?
(3) OCK: Yeah I think that'sthe wrong image for an in-vestment bank he wants sta-bility and s safety and youdon't want flashy like zip-1 Throughout this paper we take examples from a meetingwhere a group of people are deciding on a new car for aninvestment bank.
The management wants to attract youngerinvestors with a sporty car.2 We have presented the examples the way they were ut-tered by the speaker.
Hence they may show many falsestarts and repetitions.
Capitalization was added to improvereadability.54ping around the corner kindof thing you knowThe example above shows that the speaker has anegative judgment towards the suggestion of asports car (that was made in the previous turn)which is indicated by the words ?wrong image.
?The speaker then goes on to positively argue forwhat he wants.
He further argues against the cur-rent suggestion by using more negative termslike ?flashy?
and ?zipping around the corner.
?The speaker believes that ?zipping around thecorner?
is bad as it would give a wrong impres-sion of the bank to the customers.
In the absenceof such analyses, the decision making processand rationale behind the outcomes of meetings,which form an important part of the organiza-tion?s memory, might remain unavailable.In this paper, we perform annotation of ameeting corpus to lay the foundation for researchon opinion detection in speech.
We show howcategories from an opinion (subjectivity) annota-tion scheme, which was developed for news arti-cles, can be applied to the genre of multi-partymeetings.
The new genre poses challenges as it issignificantly different from the text domain,where opinion analysis has traditionally beenapplied.
Specifically, differences arise because:1) There are many participants interacting withone another, each expressing his or her ownopinion, and eliciting reactions in the process.2) Social interactions may constrain how openlypeople express their opinions; i.e., they are oftenindirect in their negative evaluations.We also  explore the influence of speech on hu-man perception of opinions.Specifically, we annotated some meeting datawith the opinion categories Sentiment and Argu-ing as defined in Wilson and Wiebe (2005).
Inour annotation we first distinguish whether aSentiment or Arguing is being expressed.
If oneis, we then mark the polarity (i.e., positive ornegative) and the intensity (i.e., how strong theopinion is).
Annotating the individual opinionexpressions is useful in this genre, because wesee many utterances that have more than onetype of opinion (e.g.
(3) above).
To investigatehow opinions are expressed in speech, we divideour annotation into two tasks, one in which theannotator only reads the raw text, and the otherin which the annotator reads the raw text and alsolistens to the speech.
We measure inter-annotatoragreement for both tasks.We found that the opinion categories applywell to the multi-party meeting data, althoughthere is some room for improvement: the Kappavalues range from 0.32 to 0.69.
As has beenfound for other types of annotations in speech,agreement is higher when the annotators bothread and listen to the data than when they onlyread the transcripts.
Interestingly, the advantagesare more dramatic for some categories than oth-ers.
And, in both conditions, agreement is higherfor the positive than for the negative categories.We discuss possible reasons for these disparities.Prosodic clues have been exploited to performautomatic detection of speaker emotion (Lis-combe et al 2003).
Our findings suggest thatdoing so to recognize opinion categories is apromising line of work.The rest of the paper is organized as follows:In Section 2 we discuss the data and the annota-tion scheme and present examples.
We then pre-sent our inter-annotator agreement results in Sec-tion 3, and in Section 4 we discuss issues andobservations.
Related work is described in Sec-tion 5.
Conclusions and Future Work are pre-sented in Section 6.2 Annotation2.1 DataThe data is from the ISL meeting corpus (Bur-ger et al 2002).
We chose task oriented meet-ings from the games/scenario and discussiongenres, as we felt they would be closest to theapplications for which the opinion analysis willbe useful.
The ISL speech is accompanied byrich transcriptions, which are tagged according toVERBMOBIL conventions.
However, since real-time applications only have access to ASR out-put, we gave the annotators raw text, from whichall VERBMOBIL tags, punctuation, and capitali-zations were removed.In order to see how annotations would be af-fected by the presence or absence of speech, wedivided each raw text document into 2 segments.One part was annotated while reading the rawtext only.
For the annotation of the other part,speech as well as the raw text was provided.2.2 Opinion Category DefinitionsWe base our annotation definitions on thescheme developed by Wiebe et al (2005) fornews articles.
That scheme centers on the notionof subjectivity, the linguistic expression of pri-vate states.
Private states are internal mentalstates that cannot be objectively observed or veri-fied (Quirk et al 1985) and include opinions,beliefs, judgments, evaluations, thoughts, andfeelings.
Amongst these many forms of subjec-55tivity, we focus on the Sentiment and Arguingcategories proposed by Wilson and Wiebe(2005).
The categories are broken down by po-larity and defined as follows:Positive Sentiments: positive emotions,evaluations, judgments and stances.
(4) TBC: Well ca How aboutone of the the newer Cadil-lac the Lexus is goodIn (4), taken from the discussion of which car tobuy, the speaker uses the term ?good?
to expresshis positive evaluation of the Lexus .Negative Sentiments: negative emotions,evaluations, judgments and stances.
(5) OCK: I think these areall really bad choicesIn (5), the speaker expresses his negative evalua-tion of the choices for the company car.
Note that?really?
makes the evaluation more intense.Positive Arguing:  arguing for something, ar-guing that something is true or is so, arguing thatsomething did happen or will happen, etc.
(6) ZDN: Yeah definitelymoon roofIn (6), the speaker is arguing that whatever carthey get should have a moon roof.Negative Arguing: arguing against some-thing, arguing that something is not true or is notso, arguing that something did not happen or willnot happen, etc.
(7) OCK: Like a Lexus orperhaps a Stretch Lexussomething like that but thatmight be too a little tooluxuriousIn the above example, the speaker is using theterm ?a little too luxurious?
to argue against aLexus for the car choice.In an initial tagging experiment, we appliedthe above definitions, without modification, tosome sample meeting data.
The definitions cov-ered much of the arguing and sentiment we ob-served.
However, we felt that some cases of Ar-guing that are more prevalent in meeting than innews data needed to be highlighted more, namelyArguing opinions that are implicit or that under-lie what is explicitly said.
Thus we add the fol-lowing to the arguing definitions.Positive Arguing: expressing support for orbacking the acceptance of an object, viewpoint,idea or stance by providing reasoning, justifica-tions, judgment, evaluations or beliefs.
This sup-port or backing may be explicit or implicit.
(8) MHJ: That's That's why Iwanna What about the thechild safety locks I think Ithink that would be a goodthing because if our custom-ers happen to have childrenExample (8) is marked as both Positive Arguingand Positive Sentiment.
The more explicit one isthe Positive Sentiment that the locks are good.The underlying Argument is that the companycar they choose should have child safety locks.Negative Arguing: expressing lack of supportfor or attacking the acceptance of an object,viewpoint, idea or stance by providing reasoning,justifications, judgment, evaluations or beliefs.This may be explicit or implicit.
(9) OCK: Town Car But it's alittle a It's a little likeyour grandf Yeah your grand-father would drive thatExample (9) is explicitly stating who would drivea Town Car, while implicitly arguing againstchoosing the Town Car (as they want youngerinvestors).2.3 Annotation GuidelinesDue to genre differences, we also needed tomodify the annotation guidelines.
For each Argu-ing or Sentiment the annotator perceives, he orshe identifies the words or phrases used to ex-press it (the text span), and then creates an anno-tation consisting of the following.?
Opinion Category and Polarity?
Opinion Intensity?
Annotator CertaintyOpinion Category and Polarity: These aredefined in the previous sub-section.
Note that thetarget of an opinion is what the opinion is about.For example, the target of ?John loves baseball?is baseball.
An opinion may or may not have aseparate target.
For example, ?want stability?
in?We want stability?
denotes a Positive Senti-ment, and there is no separate target.
In contrast,?good?
in ?The Lexus is good?
expresses a Posi-tive Sentiment and there is a separate target,namely the Lexus.In addition to Sentiments toward a topic ofdiscussion, we also mark Sentiments towardother team members (e.g.
?Man you guysare so limited?).
We do not markagreements or disagreements as Sentiments, asthese are different dialog acts (though they some-times co-occur with Sentiments and Arguing).Intensity: We use a slightly modified versionof Craggs and Wood's (2004) emotion intensity56annotation scheme.
According to that scheme,there are 5 levels of intensity.
Level ?0?
denotesa lack of the emotion (Sentiment or Arguing inour case), ?1?
denotes traces of emotion, ?2?
de-notes a low level of emotion, ?3?
denotes a clearexpression while ?4?
denotes a strong expres-sion.
Our intensity levels mean the same, but wedo not mark intensity level 0 as this level impliesthe absence of opinion.If a turn has multiple, separate expressionsmarked with the same opinion tag (category andpolarity), and all expressions refer to the sametarget, then the annotators merge all the expres-sions into a larger text span, including the sepa-rating text in between the  expressions.
This re-sulting text span has the same opinion tag as itsconstituents, and it has an intensity that is greaterthan or equal to the highest intensity of the con-stituent expressions that were merged.Annotator Certainty: The annotators use thistag if they are not sure that a given opinion ispresent, or if, given the context, there are multi-ple possible interpretations of the utterance andthe annotator is not sure which interpretation iscorrect.
This attribute is distinct from the Inten-sity attribute, because the Intensity attribute indi-cates the strength of the opinion, while the Anno-tator Certainty attribute indicates whether theannotator is sure about a given tag (whatever theintensity is).2.4 ExamplesWe conclude this section with some examplesof annotations from our corpus.
(10) OCK: So Lexun had reve-nues of a hundred and fiftymillion last year and prof-its of like six million.That's pretty goodAnnotation: Text span=That'spretty good Cate-gory=Positive Sentiment In-tensity=3 Annotator Cer-tainty=CertainThe annotator marked the text span ?That?spretty good?
as Positive Sentiment because thisthis expression is used by OCK to show his fa-vorable judgment towards the company reve-nues.
The intensity is 3, as it is a clear expressionof Sentiment.
(11) OCK: No it might justbe a piece of junk Cheappiece of junk that?s not agood investmentAnnotation1: Text span=itmight just be a piece ofjunk Cheap piece of junkthat?s not a good investmentCategory=Negative SentimentIntensity=4 Annotator Cer-tainty=CertainAnnotation2: Text span=Cheappiece of junk that?s not agood investment Category=Negative Arguing Inten-sity=3 Annotator Certainty=CertainIn the above example, there are multiple expres-sions of opinions.
In Annotation1, the expres-sions ?it might just be a piece of junk?, ?cheappiece of junk?
and ?not a good investment?
ex-press negative evaluations towards the car choice(suggested by another participant in a previousturn).
Each of these expressions is a clear case ofNegative Sentiment (Intensity=3).
As they are allof the same category and polarity and towardsthe same target, they have been merged by theannotator into one long expression of Inten-sity=4.
In Annotation2, the sub-expression?cheap piece of junk that is not a good invest-ment?
is also used by the speaker OCK to argueagainst the car choice.
Hence the annotator hasmarked this as Negative Arguing.3 Guideline Development and Inter-Annotator Agreement3.1 Annotator TrainingTwo annotators (both co-authors) underwentthree rounds of tagging.
After each round, dis-crepancies were discussed, and the guidelineswere modified to reflect the resolved ambiguities.A total of 1266 utterances belonging to sectionsof four meetings (two of the discussion genre andtwo of the game genre) were used in this phase.3.2 AgreementThe unit for which agreement was calculatedwas the turn.
The ISL transcript provides demar-cation of speaker turns along with the speaker ID.If an expression is marked in a turn, the turn isassigned the label of that expression.
If there aremultiple expressions marked within a turn withdifferent category tags, the turn is assigned allthose categories.
This does not pose a problemfor our evaluation, as we evaluate each categoryseparately.A previously unseen section of a meeting con-taining 639 utterances was selected and divided57into 2 segments.
One part of 319 utterances wasannotated using raw text as the only signal, andthe remaining 320 utterances were annotated us-ing text and speech.
Cohen?s Kappa (1960) wasused to calculate inter-annotator agreement.
Wecalculated inter-annotator agreement for bothconditions: raw-text-only and raw-text+speech.This was done for each of the categories: Posi-tive Sentiment, Positive Arguing, Negative Sen-timent, and Negative Arguing.
To evaluate acategory, we did the following:?
For each turn, if both annotators taggedthe turn with the given category, or bothdid not tag the turn with the category, thenit is a match.?
Otherwise it is a mismatchTable 1 shows the inter-annotator Kappa val-ues on the test set.Agreement (Kappa) Raw Text onlyRaw Text+ SpeechPositive Arguing 0.54 0.60Negative Arguing 0.32 0.65Positive Sentiment 0.57 0.69Negative Sentiment 0.41 0.61Table 1 Inter-annotator agreement on differentcategories.With raw-text-only annotation, the Kappavalue is in the moderate range according toLandis and Koch (1977), except for NegativeArguing for which it is 0.32.
Positive Arguingand Positive Sentiment were more reliably de-tected than Negative Arguing and Negative Sen-timent.
We believe this is because participantswere more comfortable with directly expressingtheir positive sentiments in front of other partici-pants.
Given only the raw text data, inter-annotator reliability measures for Negative Argu-ing and Negative Sentiment are the lowest.
Webelieve this might be due to the fact that partici-pants in social interactions are not very forthrightwith their Negative Sentiments and Arguing.Negative Sentiments and Arguing towards some-thing may be expressed by saying that somethingelse is better.
For example, consider the follow-ing response of one participant to another par-ticipant?s suggestion of aluminum wheels for thecompany car(12) ZDN: Yeah see what kindof wheels you know they haveto look dignified to go withthe carThe above example was marked as Negative Ar-guing by one annotator (i.e., they should not getaluminum wheels) while the other annotator didnot mark it at all.
The implied Negative Arguingtoward getting aluminum wheels can be inferredfrom the statement that the wheels should lookdignified.
However the annotators were not sure,as the participant chose to focus on what is desir-able (i.e., dignified wheels).
This utterance isactually both a general statement of what is de-sirable, and an implication that aluminum wheelsare not dignified.
But this may be difficult to as-certain with the raw text signal only.When the annotators had speech to guide theirjudgments, the Kappa values go up significantlyfor each category.
All the agreement numbers forraw text+speech are in the substantial range ac-cording to Landis and Koch (1977).
We observethat with speech, Kappa for Negative Arguinghas doubled over the Kappa obtained withoutspeech.
The Kappa for Negative Sentiment(text+speech) shows a 1.5 times improvementover the one with only raw text.
Both these ob-servations indicate that speech is able to help theannotators tag negativity more reliably.
It is quitelikely that a seemingly neutral sentence couldsound negative, depending on the way words arestressed or pauses are inserted.
Comparing theagreement on Positive Sentiment, we get a 1.2times improvement by using speech.
Similarly,agreement improves by 1.1 times for PositiveArguing when speech is used.
The improvementwith speech for the Positive categories is not ashigh as compared to negative categories, whichconforms to our belief that people are moreforthcoming about their positive judgments,evaluations, and beliefs.In order to test if the turns where annotatorswere uncertain were the places that caused mis-match, we calculated the Kappa with the annota-tor-uncertain cases removed.
The correspondingKappa values are shown in Table 2Agreement ( Kappa) Raw Text onlyRaw Text+ SpeechPositive Arguing 0.52 0.63Negative Arguing 0.36 0.63Positive Sentiment 0.60 0.73Negative Sentiment 0.50 0.61Table-2 Inter-annotator agreement on differentcategories, Annotator Uncertain cases removed.The trends observed in Table 1 are seen in Ta-ble 2 as well, namely annotation reliability im-proving with speech.
Comparing Tables 1 and 2,58we see that for the raw text, the inter-annotatoragreement goes up by 0.04 points for NegativeArguing and goes up by 0.09 points for NegativeSentiment.
However, the agreement for NegativeArguing and Negative Sentiment on raw-text+speech between Tables 1 and 2 remains almostthe same.
We believe this is  because we had20% fewer Annotator Uncertainty tags in theraw-text+speech annotation as compared to raw-text-only, thus indicating that some types of un-certainties seen in raw-text-only were resolved inthe raw-text+speech due to the speech input.
Theremaining cases of Annotator Uncertainty couldhave been due to other factors, as discussed inthe next sectionTable 3 shows Kappa with the low intensitytags removed.
The hypothesis was that low in-tensity might be borderline cases, and that re-moving these might increase inter-annotator reli-ability.Agreement ( Kappa) Raw Text onlyRaw Text+ SpeechPositive Arguing 0.53 0.66Negative Arguing 0.26 0.65Positive Sentiment 0.65 0.74Negative Sentiment 0.45 0.59Table-3 Inter-annotator agreement on differentcategories, Intensity 1, 2 removed.Comparing Tables 1 and 3 (the raw-text col-umns), we see that there is an improvement inthe agreement on sentiment (both positive andnegative) if the low intensity cases are removed.The agreement for Negative Sentiment (raw-text)goes up marginally by 0.04 points.
Surprisingly,the agreement for Negative Arguing (raw-text)goes down by 0.06 points.
Similarly in raw-text+speech results, removal of low intensitycases does not improve the agreement for Nega-tive Arguing while hurting Negative Sentimentcategory (by 0.02 points).
One possible explana-tion is that it may be equally difficult to detectNegative categories at both low and high intensi-ties.
Recall that in (12) it was difficult to detect ifthere is  Negative Arguing at all.
If the annotatordecided that it is indeed a Negative Arguing, it isput at intensity level=3 (i.e., a clear case).4 DiscussionThere were a number of interesting subjectiv-ity related phenomena in meetings that we ob-served during our annotation.
These are issuesthat will need to be addressed for improving in-ter-annotator reliability.Global and local context for arguing: In thecontext of a meeting, participants argue for (posi-tively) or against (negatively) a topic.
This maybecome ambiguous when the participant uses anexplicit local Positive Arguing and an implicitglobal Negative Arguing.
Consider the followingspeaker turn, at a point in the meeting when oneparticipant has suggested that the company carshould have a moon roof and another participanthas opposed it, by saying that a moon roof wouldcompromise the headroom.
(13) OCK: We wanna make surethere's adequate headroomfor all those six foot sixinvestorsIn the above example, the speaker OCK, in thelocal context of the turn, is arguing positivelythat headroom is important.
However, in theglobal context of the meeting, he is arguingagainst the idea of a moon roof that was sug-gested by a participant.
Such cases occur whenone object (or opinion) is endorsed which auto-matically precludes another, mutually exclusiveobject (or opinion).Sarcasm/Humor: The meetings we analyzedhad a large amount of sarcasm and humor.
Issuesarose with sarcasm due to our approach of mark-ing opinions towards the content of the meeting(which forms the target of the opinion).
Sarcasmis difficult to annotate because sarcasm can be1) On topic: Here the target is the topic of dis-cussion and hence sarcasm is used as a NegativeSentiment.2) Off topic: Here the target is not a topic un-der discussion, and the aim is to purely elicitlaughter.3) Allied topic: In this case, the target is re-lated to the topic in some way, and it?s difficultto determine if the aim of the sarcasm/humor wasto elicit laughter or to imply something negativetowards the topic.Multiple modalities: In addition to text andspeech, gestures and visual diagrams play an im-portant role in some types of meetings.
In onemeeting that we analyzed, participants wereworking together to figure out how to protect anegg when it is dropped from a long distance,given the materials they have.
It was evident theywere using some gestures to describe their ideas(?we can put tape like this?)
and that they drewdiagrams to get points across.
In the absence ofvisual input, annotators would need to guess59what was happening.
This might further hurt theinter-annotator reliability.5 Related WorkOur opinion categories are from the subjectiv-ity schemes described in Wiebe et al (2005) andWilson and Wiebe (2005).
Wiebe et al (2005)perform expression level annotation of opinionsand subjectivity in text.
They define their annota-tions as an experiencer having some type of atti-tude (such as Sentiment or Arguing), of a certainintensity, towards a target.
Wilson and Wiebe(2005) extend this basic annotation scheme toinclude different types of subjectivity, includingPositive Sentiment, Negative Sentiment, PositiveArguing, and Negative Arguing.Speech was found to improve inter-annotatoragreement in discourse segmentation of mono-logs (Hirschberg and Nakatani 1996).
Acousticclues have been successfully employed for thereliable detection of the speaker?s emotions, in-cluding frustration, annoyance, anger, happiness,sadness, and boredom (Liscombe et al 2003).Devillers et al (2003) performed perceptual testswith and without speech in detecting thespeaker?s fear, anger, satisfaction and embar-rassment.
Though related, our work is not con-cerned with the speaker?s emotions, but ratheropinions toward the issues and topics addressedin the meeting.Most annotation work in multiparty conversa-tion has focused on exchange structures and dis-course functional units like common grounding(Nakatani and Traum, 1998).
In common ground-ing research, the focus is on whether the partici-pants of the discourse are able to understand eachother, and not their opinions towards the contentof the discourse.
Other tagging schemes like theone proposed by Flammia and Zue (1997) focuson information seeking and question answeringexchanges where one participant is purely seek-ing information, while the other is providing it.The SWBD DAMSL (Jurafsky et al, 1997) an-notation scheme over the Switchboard telephonicconversation corpus labels shallow discoursestructures.
The SWBD-DAMSL had a label ?sv?for opinions.
However, due to poor inter-annotator agreement, the authors discarded theseannotations.
The ICSI MRDA annotation scheme(Rajdip et al, 2003) adopts the SWBD DAMSLscheme, but does not distinguish between theopinionated and objective statements.
The ISLmeeting corpus (Burger and Sloane, 2004) is an-notated with dialog acts and discourse moves likeinitiation and response, which in turn consist ofdialog tags such as query, align, and statement.Their statement dialog category would not onlyinclude Sentiment and Arguing tags discussed inthis paper, but it would also include objectivestatements and other types of subjectivity.
?Hot spots?
in meetings closely relate to ourwork because they find sections in the meetingwhere participants are involved in debates orhigh arousal activity (Wrede and Shriberg 2003).While that work distinguishes between higharousal and low arousal, it does not distinguishbetween  opinion or non-opinion or the differenttypes of opinion.
However, Janin et al (2004)suggest that there is a relationship between dia-log acts and involvement, and that involved ut-terances contain significantly more evaluativeand subjective statements as well as extremelypositive or negative answers.
Thus we believe itmay be beneficial for such works to make thesedistinctions.Another closely related work that finds par-ticipants?
positions regarding issues is argumentdiagramming (Rienks et al 2005).
This ap-proach, based on the IBIS system (Kunz and Rit-tel 1970), divides a discourse into issues, andfinds lines of deliberated arguments.
Howeverthey do not distinguish between subjective andobjective contributions towards the meeting.6 Conclusions and Future WorkIn this paper we performed an annotationstudy of opinions in meetings, and investigatedthe effects of speech.
We have shown that it ispossible to reliably detect opinions within multi-party conversations.
Our consistently betteragreement results with text+speech input overtext-only input suggest that speech is a reliableindicator of opinions.
We have also found thatAnnotator Uncertainty decreased with speechinput.
Our results also show that speech is a moreinformative indicator for negative versus positivecategories.
We hypothesize that this is due to thefact the people express their positive attitudesmore explicitly.
The speech signal is thus evenmore important for discerning negative opinions.This experience has also helped us gain insightsto the ambiguities that arise due to sarcasm andhumor.Our promising results open many new avenuesfor research.
It will be interesting to see how ourcategories relate to other discourse structures,both at the shallow level (agree-ment/disagreement) as well as at the deeper level60(intentions/goals).
It will also be interesting toinvestigate how other forms of subjectivity likespeculation and intention are expressed in multi-party discourse.
Finding prosodic correlates ofspeech as well as lexical clues that help in opin-ion detection would be useful in building subjec-tivity detection applications for multiparty meet-ings.ReferencesSusanne Burger and Zachary A Sloane.
2004.
The ISLMeeting Corpus: Categorical Features of Commu-nicative Group Interactions.
NIST Meeting Recog-nition Workshop 2004, NIST 2004, Montreal, Can-ada, 2004-05-17Susanne Burger, Victoria MacLaren and Hua Yu.2002.
The ISL Meeting Corpus: The Impact ofMeeting Type on Speech Style.
ICSLP-2002.
Den-ver, CO: ISCA, 9 2002.Jacob Cohen.
1960.
A coefficient of agreement fornominal scales.
Educational and PsychologicalMeas., 20:37?46.Richard Craggs and Mary McGee Wood.
2004.
Acategorical annotation scheme for emotion in thelinguistic content of dialogue.
Affective DialogueSystems.
2004.Laurence Devillers, Lori Lamel and Ioana Vasilescu.2003.
Emotion detection in task-oriented spokendialogs.
IEEE International Conference on Multi-media and Expo (ICME).Rajdip Dhillon, Sonali Bhagat, Hannah Carvey andElizabeth Shriberg.
2003.
?Meeting Recorder Pro-ject: Dialog Act Labeling Guide,?
ICSI TechnicalReport TR-04-002, Version 3, October 2003Giovanni Flammia and Victor Zue.
1997.
LearningThe Structure of Mixed Initiative Dialogues UsingA Corpus of Annotated Conversations.
Eurospeech1997, Rhodes, Greece 1997, p1871?1874Julia Hirschberg and Christine Nakatani.
1996.
A Pro-sodic Analysis of Discourse Segments in Direction-Giving Monologues Annual Meeting- AssociationFor Computational Linguistics 1996, VOL 34,pages 286-293Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhil-lon, Jane Edwards, Javier Mac?
?as-Guarasa, NelsonMorgan, Barbara Peskin, Elizabeth Shriberg, An-dreas Stolcke, Chuck Wooters and Britta Wrede.2004.
?The ICSI Meeting Project: Resources andResearch,?
ICASSP-2004 Meeting RecognitionWorkshop.
Montreal; Canada: NIST, 5 2004Daniel Jurafsky, Elizabeth Shriberg and Debra Biasca,1997.
Switchboard-DAMSL Labeling ProjectCoder?s Manual.http://stripe.colorado.edu/?jurafsky/manual.august1Werner Kunz and Horst W. J. Rittel.
1970.
Issues aselements of information systems.
Working PaperWP-131, Univ.
Stuttgart, Inst.
Fuer Grundlagen derPlanung, 1970Richard Landis and Gary Koch.
1977.
The Measure-ment of Observer Agreement for Categorical DataBiometrics, Vol.
33, No.
1 (Mar., 1977) , pp.
159-174Agnes Lisowska.
2003.
Multimodal interface designfor the multimodal meeting domain: Preliminaryindications from a query analysis study.
TechnicalReport IM2.
Technical report, ISSCO/TIM/ETI.Universit de Genve, Switserland, November 2003.Jackson Liscombe, Jennifer Venditti and JuliaHirschberg.
2003.
Classifying Subject Ratings ofEmotional Speech Using Acoustic Features.
Eu-rospeech 2003.Christine Nakatani and David Traum.
1998.
Draft:Discourse Structure Coding Manual version2/27/98Randolph Quirk, Sidney Greenbaum, Geoffry Leechand Jan Svartvik.
1985.
A Comprehensive Gram-mar of the English Language.
Longman, NewYork.sRutger Rienks, Dirk Heylen and Erik van der Wei-jden.
2005.
Argument diagramming of meetingconversations.
In Vinciarelli, A. and Odobez, J.,editors, Multimodal Multiparty Meeting Process-ing, Workshop at the 7th International Conferenceon Multimodal Interfaces, pages 85?92, Trento, It-alyJanyce Wiebe, Theresa Wilson and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources andEvaluation (formerly Computers and the Humani-ties), volume 39, issue 2-3, pp.
165-210.Theresa Wilson and Janyce Wiebe.
2005.
Annotatingattributions and private states.
ACL Workshop onFrontiers in Corpus Annotation II: Pie in the Sky.Britta Wrede and Elizabeth Shriberg.
2003.
Spotting"Hotspots" in Meetings: Human Judgments andProsodic Cues.
Eurospeech 2003, Geneva61
