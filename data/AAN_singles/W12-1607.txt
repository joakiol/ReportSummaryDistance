Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 60?69,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsHierarchical Conversation Structure Prediction in Multi-Party ChatElijah Mayfield, David Adamson, and Carolyn Penstein Rose?Language Technologies InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213{emayfiel, dadamson, cprose}@cs.cmu.eduAbstractConversational practices do not occur at a sin-gle unit of analysis.
To understand the inter-play between social positioning, informationsharing, and rhetorical strategy in language,various granularities are necessary.
In thiswork we present a machine learning modelfor multi-party chat which predicts conversa-tion structure across differing units of analy-sis.
First, we mark sentence-level behavior us-ing an information sharing annotation scheme.By taking advantage of Integer Linear Pro-gramming and a sociolinguistic framework,we enforce structural relationships betweensentence-level annotations and sequences ofinteraction.
Then, we show that clusteringthese sequences can effectively disentanglethe threads of conversation.
This model ishighly accurate, performing near human accu-racy, and performs analysis on-line, openingthe door to real-time analysis of the discourseof conversation.1 IntroductionWhen defining a unit of analysis for studying lan-guage, one size does not fit all.
Part-of-speech tag-ging is performed on individual words in sequences,while parse trees represent language at the sentencelevel.
Individual tasks can be performed at the lex-ical, sentence, or document level, or even to arbi-trary length spans of text (Wiebe et al, 2005), whilerhetorical patterns are annotated in a tree-like struc-ture across sentences or paragraphs.In dialogue, the most common unit of analysis isthe utterance, usually through dialogue acts.
Here,too, the issue of granularity and specificity of tagshas been a persistent issue, along with the inte-gration of larger discourse structure.
Both theory-driven and empirical work has argued for a col-lapsing of annotations into fewer categories, basedon either marking the dominant function of a giventurn (Popescu-Belis, 2008) or identifying a singleconstruct of interest and annotating only as nec-essary to distinguish that construct.
We take thelatter approach in this work, predicting conversa-tion structure particularly as it relates to informa-tion sharing and authority in dialogue.
We use sys-temic functional linguistics?
Negotiation annotationscheme (Mayfield and Rose?, 2011) to identify utter-ances as either giving or receiving information.
Thisannotation scheme is of particular interest because inaddition to sentence-level annotation, well-definedsequences of interaction are incorporated into theannotation process.
This sequential structure hasbeen shown to be useful in secondary analysis ofannotated data (Mayfield et al, 2012a), as well asproviding structure which improves the accuracy ofautomated annotations.This research introduces a model to predict infor-mation sharing tags and Negotiation sequence struc-ture jointly with thread disentanglement.
We showthat performance can be improved using integer lin-ear programming to enforce constraints on sequencestructure.
Structuring and annotation of conversa-tion is available quickly and with comparatively lit-tle effort compared to manual annotation.
More-over, all of our results in this paper were obtainedusing data a real-world, chat-based internet commu-nity, with a mix of long-time expert and first-time60novice users, showing that the model is robust to thechallenges of messy data in natural environments.The remainder of this paper is structured as fol-lows.
First, we review relevant work in annota-tion at the levels of utterance, sequence, and thread,and applications of each.
We then introduce thedomain of our data and the framework we use forannotation of conversation structure.
In Section 4we define a supervised, on-line machine learningmodel which performs this annotation and structur-ing across granularities.
In Section 5, we evaluatethis model and show that it approaches or matcheshuman reliability on all tasks.
We conclude with dis-cussion of the utility of this conversation structuringalgorithm for new analyses of conversation.2 Related WorkResearch on multi-party conversation structure iswidely varied, due to the multifunctional nature oflanguage.
These structures have been used in di-verse fields such as computer-supported collabora-tive work (O?Neill and Martin, 2003), dialogue sys-tems (Bohus and Horvitz, 2011), and research onmeetings (Renals et al, 2012).
Much work in an-notation has been inspired by speech act theory anddialogue acts (Traum, 1994; Shriberg et al, 2004),which operate primarily on the granularity of indi-vidual utterances.
A challenge of tagging is the issueof specificity of tags, as previous work has shownthat most utterances have multiple functions (Bunt,2011).
General tagsets have attempted to capturemulti-functionality through independent dimensionswhich produce potentially millions of possible an-notations, though in practice the number of varia-tions remains in the hundreds (Jurafsky et al, 1998).Situated work has jointly modelled speech act anddomain-specific topics (Laws et al, 2012).Additional structure inspired by linguistics, suchas adjacency pairs (Schegloff, 2007) or dialoguegames (Carlson, 1983), has been used to build dis-course relations between turns.
This additionalstructure has been shown to improve performanceof automated analysis (Poesio and Mikheev, 1998).Identification of this fine-grained structure of an in-teraction has been studied in prior work, with appli-cations in agreement detection (Galley et al, 2004),addressee detection (op den Akker and Traum,2009), and real-world applications, such as cus-tomer service conversations (Kim et al, 2010).Higher-order structure has also been explored in dia-logue, from complex graph-like relations (Wolf andGibson, 2005) to simpler segmentation-based ap-proaches (Malioutov and Barzilay, 2006).
Utterancelevel-tagging can take into account nearby structure,e.g.
forward-looking and backward-looking func-tions in DAMSL (Core and Allen, 1997), while dia-logue management systems in intelligent agents of-ten have a plan unfolding over a whole dialogue(Ferguson and Allen, 1998).In recent years, threading and maintaining of mul-tiple ?floors?
has grown in popularity (Elsner andCharniak, 2010), especially in text-based media.This level of analysis is designed with the goal ofseparating out sub-conversations which are indepen-dently coherent.
There is a common ground emerg-ing in the thread detection literature on best prac-tices for automated prediction.
Early work viewedthe problem as a time series analysis task (Binghamet al, 2003).
Treating thread detection as a cluster-ing problem, with lines representing instances, wasgiven great attention in Shen et al (2006).
Subse-quent researchers have treated the thread detectiontask as based in discourse coherence, and have pur-sued topic modelling (Adams, 2008) or entity refer-ence grids (Elsner and Charniak, 2011) to define thatconcept of coherence.Other work integrates local discourse structurewith the topic-based threads of discourse.
Ai et al(2007) utilizes information state, a dialogue man-agement component which loosely parallels threadstructure, to improve dialogue act tagging.
In thecontext of Twitter conversations, Ritter et al (2010)suggests using dialogue act tags as a middle layer to-wards conversation reconstruction.
Low-level struc-ture between utterances has also been used as afoundation for modelling larger-level sociologicalphenomena between speakers in a dialogue, for in-stance, identifying leadership (Strzalkowski et al,2011) and rapport between providers and patientsin support groups (Ogura et al, 2008).
Theseworks have all pointed to the utility of incorporat-ing sentence-level annotations, low-level interactionstructure, and overarching themes into a unified sys-tem.
To our knowledge, however, this work is thefirst to present a single system for simultaneous an-61Negotiation/Threads Seq User TextK2 1 C [M], fast question, did your son have a biopsy?K2 1 C or does that happen when he comes homeK1 2 V i have 3 dogs.K1 2 V man?s best friendf 2 S :-Do 2 C and womenK2 3 J what kind of dogs???
?K1 4 C [D], I keep seeing that you are typing and then it stopsK2 5 C how are you doing this weekK1 3 V the puppies are a maltese/yorkie mix and the full grown is a pomara-nian/yorkie.K1 1 M No, he did not have a biopsy.K1 1 M The surgeon examined him and said that by feel, he did not think thelump was cancerous, and he should just wait until he got home.f 1 C that has to be very hardo 7 M A question, however?
[J], you would probably know.K2 7 M He was told that they could not just do a needle biopsy, that he wouldhave to remove the whole lump in order to tell if it was malignant.o 8 D Yes.K1 8 D I was waiting for [M] to answer.K1 7 J That sounds odd to meTable 1: An example excerpt with Negotiation labels, sequences, and threads structure (columns) annotated.notation and structuring at all three levels.3 Data and AnnotationOur data comes from the Cancer Support Commu-nity, which provides chatrooms, forums, and otherresources for support groups for cancer patients.Each conversation took place in the context of aweekly meeting, with several patient participants aswell as a professional therapist facilitating the dis-cussion.
In total, our annotated corpus consists of45 conversations.
This data was sampled from threegroup sizes - 15 conversations from small groups (2patients, in addition to the trained facilitator), 15from medium-sized groups (3-4 patients), and 15from large groups (5 or more patients).3.1 AnnotationOur data is annotated at the three levels of granu-larity described previously in this paper: sentences,sequences, and threads.
In this section we definethose annotations in greater detail.
Sentence-leveland sequence-level annotations were performed us-ing the Negotiation framework from systemic func-tional linguistics (Martin and Rose, 2003).
Oncesequences were identified, those sequences weregrouped together into threads based on shared topic.We annotate our data using an adaptation of theNegotiation framework.
This framework has beenproven reliable and reproducible in previous work(Mayfield and Rose?, 2011).
By assigning aggregatescores over a conversation, the framework also givesus a notion of Authoritativeness.
This metric, de-fined later in Section 5, allows us to test whetherautomated codes faithfully reproduce human judg-ments of information sharing behavior at a per-userlevel.
This metric has proven to be a statisticallysignificant indicator of outcome variables in direc-tion giving (Mayfield et al, 2011) and collaborativelearning domains (Howley et al, 2011).In particular, Negotiation labels define whethereach speaker is a source or recipient of information.Our annotation scheme has four turn-level codesand a rigidly defined information sharing structure,rooted in sociolinguistic observation.
We describe62each in detail below.Sentences containing new information are markedas K1, as the speaker is the ?primary knower,?
thesource of information.
These sentences can be gen-eral facts and world knowledge, but can also con-tain opinions, retelling of narrative, or other contex-tualized information, so long as the writer acts asthe source of that information.
Sentences requestinginformation, on the other hand, are marked K2, or?secondary knower,?
when the writer is signallingthat they want information from other participantsin the chat.
This can be direct question asking, butcan also include requests for elaboration or indirectillocutionary acts (e.g.
?I?d like to hear more.?
).In addition to these primary moves, we also use asocial feedback code, f, for sentences consisting ofaffective feedback or sentiment, but which do notcontain new information.
These moves can includeemoticons, fixed expressions such as ?good luck,?
orpurely social banter.
All other moves, such as typocorrection or floor grabbing, are labelled o.This annotation scheme is highly flexible andadaptive to new domains, and is not specific to med-ical topics or chatroom-based media.
It also gives usa well-defined structure of an interaction: each se-quence consists of exactly one primary knower (K1)move, which can consist of any number of primaryknower sentences from a single speaker.
If a K2move occurs in the sequence, it occurs before anyK1 moves.
Feedback moves (f) may come at anytime so long as the speaker is responding to anotherspeaker in the same sequence.
Sentences labeledo are idiosyncratic and may appear anywhere in asequence.
In section 4.3, we represent these con-straints formally.In addition to grouping sentences together into se-quences structurally, we also group those sequencesinto threads.
These threads are based on annotatorjudgement, but generally map to the idea that a sin-gle thread should be on a single theme, e.g.
?han-dling visiting relatives at holidays.?
These threadsare both intrinsically interesting for identifying thetopics of a conversation, as well as being a usefulpreprocessing step for any additional, topic-basedannotation that may be desired for later analysis.We iteratively developed a coding manual forthese layers of annotation; to test reliability at eachiteration of instructions, two annotators each inde-Figure 1: Structured output at each phase of the two-pass machine learning model.
In pass one, utterances aregrouped into sequences with organizational structure; thesecond pass groups sequences based on shared themes.pendently annotated one full conversation.
Inter-annotator reliability is high for sentence-level an-notation (?
= 0.75).
Following Elsner and Char-niak (2010), we use micro-averaged f-score to eval-uate inter-rater agreement on higher-level structure.We find that inter-annotator agreement is high forboth sequence-level structure (f = 0.82) and thread-level structure (f = 0.80).
A detailed descriptionof the annotation process is available in Mayfield etal.
(2012b).
After establishing reliability, our entirecorpus was annotated by one human coder.4 Conversation Structure PredictionIn previous work, the Negotiation framework hasbeen automatically coded with high accuracy (May-field and Rose?, 2011).
However, that work restrictedthe domain to a task-based, two-person dialogue,and structure was viewed as a segmentation, ratherthan threading, formulation.
At each turn, a se-quence could continue or a new sequence could be-gin.Here, we extend this automated coding to largergroups speaking in unstructured, social chat, and weextend the structured element of this coding schemeto structure by sequence and thread.
To our knowl-edge, this is also the first attempt to utilize functionalsequences of interaction as a preprocessing step forthread disentanglement in chat.
We now present acomprehensive machine learning model which an-notates a conversation by utterance, groups utter-ances topics by local structure into sequences, andassigns sequences to threads.634.1 On-Line Instance CreationThis is a two-pass algorithm.
The first pass la-bels sentences and detects sequences, and the secondpass groups these sequences into threads.
We followShen et al (2006) in treating the sequence detectionproblem as a single-pass clustering algorithm.
Theirmodel is equivalent to the Previous Cluster modeldescribed below, albeit with more complex features.In that work a threshold was defined in order for anew message to be added to an existing cluster.
Ifthat threshold is not passed, a new cluster is formed.Modelling the probability that a new cluster shouldbe formed is similar to a context-sensitive threshold,and because we do not impose a hard threshold, wecan pass the set of probabilities for cluster assign-ments to a structured prediction system.4.2 Model DefinitionsAt its core, our model relies on three probabilisticclassifiers.
One of these models is a classificationmodel, and the other two treat sequence and threadstructure as clusters.
All models use the LightSIDE(Mayfield and Rose?, 2010) with the LibLinear algo-rithm (Fan et al, 2008) for machine learning..Negotiation Classifier (Neg)The Negotiation model takes a single sentence asinput.
The output of this model is a distribution overthe four possible sentence-level labels described insection 3.1.
The set of features for this model con-sists of unigrams, bigrams, and part-of-speech bi-grams.
Part-of-speech tagging was performed usingthe Stanford tagger (Toutanova et al, 2003) withinLightSIDE.Cluster Classifiers (PC, NC)We use two models of cluster assignment prob-ability.
The Previous Cluster (PC) classifiertakes as input a previous set of sentences C ={c1, c2, .
.
.
, cn} and set of new sentences N ={N1, N2, .
.
.
, Nm}.
To evaluate whether c?
shouldbe added to this cluster, we train a binary proba-bilistic classifier that predicts the probability that thesentences inN belong to the same cluster as the sen-tences already inC.
In the first pass, each inputN tothe PC classifier is a set containing a single sentence,and each C is the set of sentences in a previously-identified sequence.
In the second pass, each N is asequence as predicted by the first pass.The PC model uses two features.
The first is atime-based feature, measuring the amount of timethat has elapsed between the last sentence in C andthe first sentence in N .
The time feature is repre-sented differently between sequence prediction andthread prediction.
Elsner and Charniak (2010) rec-ommends using bucketed nominal values based onthe log time, to group together very recent and verydistant posts.
We follow this for sequence predic-tion.
Due to the more complex structure of the se-quence grouping task in the second pass, we use araw numeric time feature.
The second feature is acoherence metric, the cosine similarity between thecentroid of C and the centroid of N .
We define thecentroid based on TF-IDF weighted unigram vec-tors.We impose a threshold after which previous clus-ters are no longer considered as options for thePC classifier.
Because sequences are shorter thanthreads, we set these thresholds separately, at 90 sec-onds for sequences and 120 seconds for threads.
Ap-proximately 1% of correct assignments are impossi-ble due to these thresholds.The New Cluster (NC) classifier takes as inputa set of sentences n = {n1, n2, .
.
.
, nm}, and pre-dicts the probability that a given sentence is initiat-ing a new sequence (or, in the second pass, whethera given sequence is initiating a new thread).
Thismodel contains only unigram features.At each sentence s we consider the set of possibleprevious cluster assignments C = {c1, c2, .
.
.
, cn},and define psc(s, c) to be the probability that swill be assigned to cluster c. We define pnc(s) =?sNC(s).
The addition of a weight parameter tothe output of the NC classifier allows us to tune thelikelihood of transitioning to a new cluster.
This pre-diction structure is illustrated in Figure 2.
In thefirst pass, these cluster probabilities are used in con-junction with the output of the Negotiation classifierto form a structured output; in the second pass, themaximum cluster probability is chosen.4.3 Constraining Sequence Structure with ILPIn past work the Negotiation framework has bene-fited from enforced constraints of linguistically sup-ported rules on sequence structure (Mayfield and64Figure 2: The output of the cluster classifier in either passis a set of probabilities corresponding to possible clus-ter assignments, including that of creating a new cluster.In the second pass, the input is a set of sentences (a se-quence) rather than a single sentence, and output assign-ments are to threads rather than sequences.Rose?, 2011).
Constraints on the structure of anno-tations are easily defined using Integer Linear Pro-gramming.
Recent work has used boolean logic(Chang et al, 2008) to allow intuitive rules abouta domain to be enforced at classification time.
ILPinference was performed using Learning-Based Java(Rizzolo and Roth, 2010).First, we define the classification task.
Opti-mization is performed given the set of probabilitiesN (s) as the distribution output of the Neg classifiergiven sentence s as input, and the set of probabilitiesC(s) = pnc(s) ?
psc(s, c), ?c ?
C. Instance classi-fication requires maximizing the objective function:arg maxn?N (s),c?C(s)n+ cWe impose constraints on sequence prediction.
Ifthe most likely output from this function assignsa label that is incompatible with the assigned se-quence, either the label is changed or a new se-quence is assigned so that constraints are met.
Foreach constraint, we give the intuition from sec-tion 3.1, followed by our formulation of that con-straint.
us is shorthand for the user who wrotesentence s; ns is shorthand for a proposed Ne-gotiation label of sentence s; while cs is a pro-posed sequence assignment for s, c?
is shorthandfor assignment to a new sequence, and Sc ={(nc,1, uc,1), (nc,2, uc,2), .
.
.
, (nc,k, uc,k)} is the setof Negotiation labels n and users u associated withsentences (sc,1 .
.
.
sc,k) already in sequence c.1.
K2 moves, if any, occur before K1 moves.
((cs = c) ?
(ns = K2))?
(@i ?
Sc s.t.
nc,i = K1)2. f moves may occur at any time but must be re-sponding to a different speaker in the same se-quence.
((cs = c) ?
(ns = f))?
(?i ?
Sc s.t.
uc,i 6= us)3.
Functionally, therefore, f moves may not initi-ate a sequence).
(cs = c?)
?
(ns 6= f)4.
Speakers do not respond to their own requestsfor information (the speakers of K2 and K1moves in the same sequence must be different).
((cs = c) ?
(ns = K1))?
(?i ?
Sc, ((nc,i = K2) ?
(uc,i 6= us)))5.
Each sequence consists of at most one continu-ous series of K1 moves from the same speaker.
(cs = c) ?
((?i ?
Sc s.t.
(nc,i = K1))?
( (uc,i = us) ?
(?j > i,(uc,j = us) ?
(nc,i = K1)) )Human annotators treated these rules as hard con-straints, as the classifier does.
In circumstanceswhere these rules would be broken (for instance, dueto barge-in or trailing off), a new sequence begins.5 Evaluation5.1 MethodsTo evaluate the performance of this model, we wishto know how it replicates human annotation at eachgranularity.
For Negotiation labels, agreement ismeasured by terms of absolute accuracy and kappaagreement above chance.
We also include a measureof aggregate information sharing behavior per user.This score, which we term Information Authorita-tiveness (Auth), is defined per user as the percentage65of their contentful sentences (K1 or K2) which weregiving information (K1).
To measure performanceon this measure, we measure the r2 coefficient be-tween user authoritativeness scores calculated fromthe predicted labels compared to actual labels.
Thisis equivalent to measuring the variance explained byour model, where each data point represents a singleuser?s predicted and actual authoritativeness scoresover the course of a whole conversation (n = 215).Sequence and thread agreement is evaluated bymicro-averaged f-score (MAF), defined in priorwork for a gold sequence i with size ni, and a pro-posed sequence j with size nj , based on precisionand recall metrics:P = nijnj R =nijni F (i, j) =2?P?RP+RMAF across an entire conversation is then aweighted sum of f-scores across all sequences1:MAF =?inin maxj F (i, j)We implemented multiple baselines to testwhether our methods improve upon simpler ap-proaches.
For sequence and thread prediction, weimplement the following baselines.
Speaker Shiftpredicts a new thread every time a new writer adds aline to the chat.
Turn Windows predicts a new se-quence or thread after every n turns.
Pause Lengthpredicts a new sequence or thread every time that agap of n seconds has occurred between lines of chat.For both of the previous two baselines, we vary theparameter n to optimize performance and providea challenging baseline.
None of these models useany features or constraints, and are based on heuris-tics.
To compare to our model, we present both anUnconstrained model, which uses machine learn-ing and does not impose sequence constraints fromSection 4.3, as well as our full Constrained model.Evaluation is performed using 15-fold cross-validation.
In each fold, one small, one medium,and one large conversation are held out as a test set,and classifiers are trained on the remaining 42 con-versations.
Significance is evaluated using a pairedstudent?s t-test per conversation (n = 45).Sentence-Level (Human ?
= 0.75)Model Accuracy ?
Auth r2Unconstrained .7736 .5870 .7498Constrained .7777 .5961 .7355Sequence-Level (Human MAF = 0.82)Model Precision Recall MAFSpeaker Shift .7178 .5140 .5991Turn Windows .7207 .6233 .6685Pause Length .8479 .6582 .7411Unconstrained .7909 .7068 .7465Constrained .8557 .7116 .7770Thread-Level (Human MAF = 0.80)Model Precision Recall MAFTurn Windows .5994 .7173 .6531Pause Length .6145 .6316 .6229Unconstrained .7132 .5781 .6386Constrained .6805 .6024 .6391Table 2: Tuned optimal annotation performances of base-line heuristics compared to our machine learning model.5.2 ResultsResults of experimentation show that all modelsare highly accurate in their respective tasks.
Withsentence-level annotation approaching 0.6 ?, theoutput of the model is reliable enough to allowautomatically annotated data to be included reli-ably alongside human annotations.
Performance forsequence-based modelling is even stronger, with nostatistically significant difference in f-score betweenthe machine learning model and human agreement.Table 2 reports our best results after tuning tomaximize performance of baseline models, our orig-inal machine learning model, and the model withILP constraints enforced between Negotiation labelsand sequence.
In all three cases, we see machineperformance approaching, but not matching, humanagreement.
Incorporating ILP constraints improvesper-sentence Negotiation label classification by asmall but significant amount (p < .001).Clustering performance is highly robust, asdemonstrated in Figure 3, which shows the effect ofchanging window sizes and pause lengths and valuesof ?s for machine learned models.
Our thread disen-tanglement performance matches our baselines, and1This metric extends identically to a gold thread i and pro-posed thread j.66Figure 3: Parameter sensitivity on sequence-level (top)and thread-level (bottom) annotation models.is in line with heuristic-based assignments from El-sner and Charniak (2010).
In sequence clustering,we observe improvement across all metrics.
TheConstrained model achieves a higher f-score than allother models (p < 0.0001).
We determine througha two-tailed confidence interval that sequence clus-tering performance is statistically indistinguishablefrom human annotation (p < 0.05).Error analysis suggests that the constraints are toopunishing on the most constrained labels, K2 and f.The differences in performance between constrainedand unconstrained models is largely due to higherrecall for both K1 and o move prediction, whilerecall for K2 and f moves lowered slightly.
Onepossibility for future work may include compensat-ing for this by artificially inflating the likelihood ofhighly-constrained Negotiation labels.
Additionally,we see that the most common mistakes involve dis-tinguishing between K1 and f moves.
While manyf moves are obviously non-content-bearing (?Wow,what fun!?
), others, especially those based in humor,may look grammatical and contentful (?We?ve got tostop meeting this way.?).
Better detection of humorand a more well-defined definition of what informa-tion is being shared will improve this aspect of themodel.
Overall, these errors do not limit the efficacyof the model for enabling future analysis.6 Conclusion and Future WorkThis work has presented a unified machine learn-ing model for annotating information sharing actson a sentence-by-sentence granularity; grouping se-quences of sentences based on functional structure;and then grouping those sequences into topic-basedthreads.
The model performs at a high accuracy,approaching human agreement at the sentence andthread level.
Thread-level accuracy matched but didnot exceed simpler baselines, suggesting that thismodel could benefit from a more elaborate repre-sentation of coherence and topic.
At the level of se-quences, the model performs statistically the sameas human annotation.The automatic annotation and structuring of di-alogue that this model performs is a vital prepro-cessing task to organize and structure conversationaldata in numerous domains.
Our model allows re-searchers to abstract away from vocabulary-basedapproaches, instead working with interaction-levelunits of analysis.
This is especially important inthe context of interdisciplinary research, where otherrepresentations may be overly specialized towardsone task, and vocabulary may differ for spurious rea-sons across populations and cultures.Our evaluation was performed on a noisy, real-world chatroom corpus, and still performed very ac-curately.
Coherent interfacing between granularitiesof analysis is always a challenge.
Segmentation,tokenization, and overlapping or inconsistent struc-tured output are nontrivial problems.
By incorpo-rating sentence-level annotation, discourse-level se-quence structure, and topical thread disentanglementinto a single model, we have shown one way to re-duce or eliminate this interfacing burden and allowgreater structural awareness in real-world systems.Future work will improve this model?s accuracy fur-ther, test its generality in new domains such as spo-ken multi-party interactions, and evaluate its useful-ness in imposing structure for secondary analysis.67AcknowledgmentsThe research reported here was supported by Na-tional Science Foundation grant IIS-0968485, Of-fice of Naval Research grant N000141110221, andin part by the Pittsburgh Science of Learning Center,which is funded by the National Science Foundationgrant SBE-0836012.ReferencesPaige H. Adams.
2008.
Conversation Thread Extractionand Topic Detection in Text-based Chat.
Ph.D. thesis.Hua Ai, Antonio Roque, Anton Leuski, and DavidTraum.
2007.
Using information state to improve dia-logue move identification in a spoken dialogue system.In Proceedings of Interspeech.Ella Bingham, Ata Kaban, and Mark Girolami.
2003.Topic identification in dynamical text by complexitypursuit.
In Neural Processing Letters.Dan Bohus and Eric Horvitz.
2011.
Multiparty turn tak-ing in situated dialog.
In Procedings of SIGDIAL.Harry Bunt.
2011.
Multifunctionality in dialogue.
InComputer Speech and Language.Lauri Carlson.
1983.
Dialogue Games: An Approach toDiscourse Analysis.
Massachussetts Institute of Tech-nology.Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, andDan Roth.
2008.
Learning and inference with con-straints.
In Proceedings of the Association for the Ad-vancement of Artificial Intelligence.Mark G Core and James F Allen.
1997.
Coding dialogswith the damsl annotation scheme.
In AAAI Fall Sym-posium on Communicative Action in Humans and Ma-chines.Micha Elsner and Eugene Charniak.
2010.
Disentan-gling chat.
Computational Linguistics.Micha Elsner and Eugene Charniak.
2011.
Disentan-gling chat with local coherence models.
In Proceed-ings of the Association for Computational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.George Ferguson and James Allen.
1998.
Trips: An in-tegrated intelligent problem-solving assistant.
In Pro-ceedings of AAAI.Michael Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agreementand disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependencies.In Proceedings of ACL.Iris Howley, Elijah Mayfield, and Carolyn Penstein Rose?.2011.
Missing something?
authority in collaborativelearning.
In Proceedings of Computer Supported Col-laborative Learning.Daniel Jurafsky, Rebecca Bates, Noah Coccaro, RachelMartin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,Andreas Stolcke, Paul Taylor, and Carol Van Ess-Dykema.
1998.
Switchboard discourse languagemodelling final report.
Technical report.Su Nam Kim, Lawrence Cavedon, and Timothy Bald-win.
2010.
Classifying dialogue acts in one-on-onelive chats.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.M Barton Laws, Mary Catherine Beach, Yoojin Lee,William H. Rogers, Somnath Saha, P Todd Korthuis,Victoria Sharp, and Ira B Wilson.
2012.
Provider-patient adherence dialogue in hiv care: Results of amultisite study.
AIDS Behavior.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of ACL/COLING.J.R.
Martin and David Rose.
2003.
Working with Dis-course: Meaning Beyond the Clause.
Continuum.Elijah Mayfield and Carolyn Penstein Rose?.
2010.
Aninteractive tool for supporting error analysis for textmining.
In NAACL Demonstration Session.Elijah Mayfield and Carolyn Penstein Rose?.
2011.
Rec-ognizing authority in dialogue with an integer linearprogramming constrained model.
In Proceedings ofAssociation for Computational Linguistics.Elijah Mayfield, Michael Garbus, David Adamson, andCarolyn Penstein Rose?.
2011.
Data-driven interac-tion patterns: Authority and information sharing in di-alogue.
In Proceedings of AAAI Fall Symposium onBuilding Common Ground with Intelligent Agents.Elijah Mayfield, David Adamson, Alexander I Rudnicky,and Carolyn Penstein Rose?.
2012a.
Computationalrepresentations of discourse practices across popula-tions in task-based dialogue.
In Proceedings of theInternational Conference on Intercultural Collabora-tion.Elijah Mayfield, Miaomiao Wen, Mitch Golant, and Car-olyn Penstein Rose?.
2012b.
Discovering habits of ef-fective online support group chatrooms.
In ACM Con-ference on Supporting Group Work.Kanayo Ogura, Takashi Kusumi, and Asako Miura.2008.
Analysis of community development usingchat logs: A virtual support group of cancer patients.In Proceedings of the IEEE Symposium on UniversalCommunication.Jacki O?Neill and David Martin.
2003.
Text chat in ac-tion.
In Proceedings of the International Conferenceon Supporting Group Work.68Rieks op den Akker and David Traum.
2009.
A compari-son of addressee detection methods for multiparty con-versations.
In Workshop on the Semantics and Prag-matics of Dialogue.Massimo Poesio and Andrei Mikheev.
1998.
The pre-dictive power of game structure in dialogue act recog-nition: Experimental results using maximum entropyestimation.
In Proceedings of the International Con-ference on Spoken Language Processing.Andrei Popescu-Belis.
2008.
Dimensionality of dialogueact tagsets: An empirical analysis of large corpora.
InLanguage Resources and Evaluation.Steve Renals, Herve?
Bourlard, Jean Carletta, and AndreiPopescu-Belis.
2012.
Multimodal Signal Processing:Human Interactions in Meetings.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.
In Pro-ceedings of NAACL.Nicholas Rizzolo and Dan Roth.
2010.
Learning basedjava for rapid development of nlp systems.
In Pro-ceedings of the International Conference on LanguageResources and Evaluation.E.
Schegloff.
2007.
Sequence organization in interac-tion: A primer in conversation analysis.
CambridgeUniversity Press.Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.2006.
Thread detection in dynamic text messagestreams.
In Proceedings of SIGIR.Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, JeremyAng, and Hannah Carvey.
2004.
The icsi meetingrecorder dialog act (mrda) corpus.
In Proceedings ofSIGDIAL.Tomek Strzalkowski, George Aaron Broadwell, JenniferStromer-Galley, Samira Shaikh, Ting Liu, and SarahTaylor.
2011.
Modeling socio-cultural phenomena inonline multi-party discourse.
In AAAI Workshop onAnalyzing Microtext.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of NAACL.David Traum.
1994.
A computational theory of ground-ing in natural language conversation.
Ph.D. thesis.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation.Florian Wolf and Edward Gibson.
2005.
Representingdiscourse coherence: A corpus-based study.
Compu-tational Linguistics.69
