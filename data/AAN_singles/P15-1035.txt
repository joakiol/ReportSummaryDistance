Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 355?364,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsJoint Information Extraction and Reasoning:A Scalable Statistical Relational Learning ApproachWilliam Yang WangLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAyww@cs.cmu.eduWilliam W. CohenMachine Learning DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213, USAwcohen@cs.cmu.eduAbstractA standard pipeline for statistical rela-tional learning involves two steps: onefirst constructs the knowledge base (KB)from text, and then performs the learn-ing and reasoning tasks using probabilis-tic first-order logics.
However, a key is-sue is that information extraction (IE) er-rors from text affect the quality of the KB,and propagate to the reasoning task.
Inthis paper, we propose a statistical rela-tional learning model for joint informationextraction and reasoning.
More specifi-cally, we incorporate context-based entityextraction with structure learning (SL) ina scalable probabilistic logic framework.We then propose a latent context inven-tion (LCI) approach to improve the per-formance.
In experiments, we show thatour approach outperforms state-of-the-artbaselines over three real-world Wikipediadatasets from multiple domains; that jointlearning and inference for IE and SL sig-nificantly improve both tasks; that latentcontext invention further improves the re-sults.1 IntroductionInformation extraction (IE) is often an early stagein a pipeline that contains non-trivial downstreamtasks, such as question answering (Moll?a et al,2006), machine translation (Babych and Hartley,2003), or other applications (Wang and Hua, 2014;Li et al, 2014).
Knowledge bases (KBs) populatedby IE techniques have also been used as an inputto systems that learn rules allowing further infer-ences to be drawn from the KB (Lao et al, 2011),a task sometimes called KB completion (Socher etal., 2013; Wang et al, 2014; West et al, 2014).Pipelines of this sort frequently suffer from errorcascades, which reduces performance of the fullsystem1.In this paper, we address this issue, and pro-pose a joint model system for IE and KB com-pletion in a statistical relational learning (SRL)setting (Sutton and McCallum, 2006; Getoor andTaskar, 2007).
In particular, we outline a systemwhich takes as input a partially-populated KB anda set of relation mentions in context, and jointlylearns: 1) how to extract new KB facts from therelation mentions, and; 2) a set of logical rules thatallow one to infer new KB facts.
Evaluation of theKB facts inferred by the joint system shows thatthe joint model outperforms its individual com-ponents.
We also introduce a novel extension ofthis model called Latent Context Invention (LCI),which associates latent states with context featuresfor the IE component of the model.
We show thatLCI further improves performance, leading to asubstantial improvement over prior state-of-the-artmethods for joint relation-learning and IE.To summarize our contributions:?
We present a joint model for IE and re-lational learning in a statistical relationallearning setting which outperforms universalschemas (Riedel et al, 2013), a state-of-the-art joint method;?
We incorporate latent context into the jointSRL model, bringing additional improve-ments.In next section, we discuss related work.
Wedescribe our approach in Section 3.
The detailsof the datasets are introduced in Section 4.
Weshow experimental results in Section 5, discuss inSection 6, and conclude in Section 7.1For example, KBP slot filling is known for its com-plex pipeline, and the best overall F1 scores (Wiegand andKlakow, 2013; Angeli et al, 2014) for recent competitionsare within the range of 30-40.3552 Related WorkIn NLP, our work clearly aligns with recent workon joint models of individual text processing tasks.For example, Finkel and Manning (2009) work onthe problem of joint IE and parsing, where theyuse tree representations to combine named entitiesand syntactic chunks.
Recently, Devlin et al (De-vlin et al, 2014) use a joint neural network modelfor machine translation, and obtain an impressive6.3 BLEU point improvement over a hierarchicalphrase-based system.In information extraction, weak supervi-sion (Craven et al, 1999; Mintz et al, 2009) is acommon technique for extracting knowledge fromtext, without large-scale annotations.
In extractingInfobox information from Wikipedia text, Wu andWeld (2007; 2010) also use a similar idea.
In anopen IE project, Banko et al (2007) use a seedKB, and utilize weak supervision techniques toextend it.
Note that weakly supervised extractionapproaches can be noisy, as a pair of entities incontext may be associated with one, none, orseveral of the possible relation labels, a propertywhich complicates the application of distantsupervision methods (Mintz et al, 2009; Riedel etal., 2010; Hoffmann et al, 2011; Surdeanu et al,2012).Lao et al (2012) learned syntactic rules for find-ing relations defined by ?lexico-semantic?
pathsspanning KB relations and text data.
Wang etal.
(2015) extends the methods used by Lao etal.
to learn mutually recursive relations.
Recently,Riedel et al (2013) propose a matrix factoriza-tion technique for relation embedding, but theirmethod requires a large amount of negative andunlabeled examples.
Weston et al (2013) con-nect text with KB embedding by adding a scoringterm, though no shared parameters/embeddingsare used.
All these prior works make use of textand KBs.
Unlike these prior works, our method isposed in an SRL setting, using a scalable proba-bilistic first-order logic, and allows learning of re-lational rules that are mutually recursive, thus al-lowing learning of multi-step inferences.
Unlikesome prior methods, our method also does not re-quire negative examples, or large numbers of un-labeled examples.3 Our ApproachIn this section, we first briefly review the se-mantics, inference, and learning procedures of aabout(X,Z) :- handLabeled(X,Z) # base.about(X,Z) :- sim(X,Y),about(Y,Z) # prop.sim(X,Y) :- links(X,Y) # sim,link.sim(X,Y) :-hasWord(X,W),hasWord(Y,W),linkedBy(X,Y,W) # sim,word.linkedBy(X,Y,W) :- true # by(W).Table 1: A simple program in ProPPR.
See text forexplanation.newly proposed scalable probabilistic logic calledProPPR (Wang et al, 2013; Wang et al, 2014).Then, we describe the joint model for informationextraction and relational learning.
Finally, a latentcontext invention theory is proposed for enhancingthe performance of the joint model.3.1 ProPPR: BackgroundBelow we will give an informal description ofProPPR, based on a small example.
More formaldescriptions can be found elsewhere (Wang et al,2013).ProPPR (for Programming with PersonalizedPageRank) is a stochastic extension of the logicprogramming language Prolog.
A simple programin ProPPR is shown in Table 1.
Roughly speak-ing, the upper-case tokens are variables, and the?:-?
symbol means that the left-hand side (the headof a rule) is implied by the conjunction of condi-tions on the right-hand size (the body).
In additionto the rules shown, a ProPPR program would in-clude a database of facts: in this example, factswould take the form handLabeled(page,label),hasWord(page,word), or linkedBy(page1,page2),representing labeled training data, a document-term matrix, and hyperlinks, respectively.
Thecondition ?true?
in the last rule is ?syntactic sugar?for an empty body.In ProPPR, a user issues a query, such as?about(a,X)?
?, and the answer is a set of possiblebindings for the free variables in the query (herethere is just one such varable, ?X?).
To answer thequery, ProPPR builds a proof graph.
Each nodein the graph is a list of conditions R1, .
.
.
, Rkthatremain to prove, interpreted as a conjunction.
Tofind the children of a node R1, .
.
.
, Rk, you lookfor either1.
database facts that match R1, in which casethe appropriate variables are bound, and R1is removed from the list, or;356Figure 1: A partial proof graph for the query about(a,Z).
The upper right shows the link structure betweendocuments a, b, c, and d, and some of the words in the documents.
Restart links are not shown.2.
a rule A ?
B1, .
.
.
, Bmwith a head A thatmatches R1, in which case again the appro-priate variables are bound, andR1is replacedwith the body of the rule, resulting in the newlist B1, .
.
.
, Bm, R2, .
.
.
, Rk.The procedures for ?matching?
and ?appropriatelybinding variables?
are illustrated in Figure 1.2Anempty list of conditions (written 2 in the fig-ure) corresponds to a complete proof of the ini-tial query, and by collecting the required variablebindings, this proof can be used to determine ananswer to the initial query.In Prolog, this proof graph is constructed on-the-fly in a depth-first, left-to-right way, returningthe first solution found, and backtracking, if re-quested, to find additional solutions.
In ProPPR,however, we will define a stochastic process onthe graph, which will generate a score for eachnode, and hence a score for each answer to thequery.
The stochastic process used in ProPPR ispersonalized PageRank (Page et al, 1998; Csa-logny et al, 2005), also known as random-walk-with-restart.
Intuitively, this process upweightssolution nodes that are reachable by many shortproofs (i.e., short paths from the query node.)
For-mally, personalized PageRank is the fixed point ofthe iterationpt+1= ?
?v0+ (1?
?
)Wpt(1)2The edge annotations will be discussed later.where p[u] is the weight assigned to u, v0isthe seed (i.e., query) node, ?v0is a vector with?v0[v0] = 1 and ?v0[u] = 0 for u 6= v, and Wis a matrix of transition probabilities, i.e., W [v, u]is the probability of transitioning from node u to achild node v. The parameter ?
is the reset proba-bility, and the transition probabilities we use willbe discussed below.Like Prolog, ProPPR?s proof graph is also con-structed on-the-fly, but rather than using depth-first search, we use PageRank-Nibble, a fast ap-proximate technique for incrementally exploring alarge graph from a an initial ?seed?
node (Ander-sen et al, 2008).
PageRank-Nibble takes a param-eter  and will return an approximation?p to thepersonalized PageRank vector p, such that eachnode?s estimated probability is within  of correct.We close this background section with some fi-nal brief comments about ProPPR.Scalability.
ProPPR is currently limited in thatit uses memory to store the fact databases, and theproof graphs constructed from them.
ProPPR usesa special-purpose scheme based on sparse matrixrepresentations to store facts which are triples,which allows it to accomodate databases with hun-dreds of millions of facts in tens of gigabytes.With respect to run-time, ProPPR?s scalabil-ity is improved by the fast approximate inferencescheme used, which is often an order of mag-nitude faster than power iteration for moderate-sized problems (Wang et al, 2013).
Experimen-357Figure 2: The data generation example as described in subsection 3.2.tation and learning are also sped up because withPageRank-Nibble, each query is answered using a?small?
?size O(1?)?proof graph.
Many opera-tions required in learning and experimentation canthus be easily parallized on a multi-core machine,by simply distributing different proof graphs todifferent threads.Parameter learning.
Personalized PageRankscores are defined by a transition probabilitymatrix W , which is parameterized as follows.ProPPR allows ?feature generators?
to be attachedto its rules, as indicated by the code after the hash-tags in the example program.3Since edges in theproof graph correspond to rule matches, the edgescan also be labeled by features, and a weightedcombination of these features can be used to de-fine a total weight for each edge, which finally canbe normalized used to define the transition matrixW .
Learning can be used to tune these weights todata; ProPPR?s learning uses a parallelized SGDmethod, in which inference on different examplesis performed in different threads, and weight up-3For instance, when matching the rule ?sim(X,Y) :-links(X,Y)?
to a condition such as ?sim(a,X)?
the two fea-tures ?sim?
and ?link?
are generated; likewise when match-ing the rule ?linkedBy(X,Y,W) :- true?
to the condition?linkedBy(a,c,sprinter)?
the feature ?by(sprinter)?
is gener-ated.dates are synchronized.Structure learning.
Prior work (Wang et al,2014) has studied the problem of learning aProPPR theory, rather than simply tuning parame-ters in an existing theory, a process called structurelearning (SL).
In particular, Wang et al (2014)propose a scheme called the structural gradientwhich scores rules in some (possibly large) user-defined space R of potential rules, which can beviewed as instantiations of rule templates, such asthe ones shown in the left-hand side of Table 2.For completeness, we will summarize brieflythe approach used in (Wang et al, 2014).
Thespace of potential rulesR is defined by a ?second-order abductive theory?, which conceptually is aninterpreter that constructs proofs using all rules inR.
Each rule template is mapped to two clausesin the interpreter: one simulates the template (forany binding), and one ?abduces?
the specific bind-ing (facts) from the KB.
Associated with the useof the abductive rule is a feature corresponding toa particular binding for the template.
The gradientof these features indicates which instantiated rulescan be usefully added to the theory.
More detailscan be found in (Wang et al, 2014).358Rule template ProPPR clauseStructure learning(a) P(X,Y) :- R(X,Y) interp(P,X,Y) :- interp0(R,X,Y),abduce if(P,R).abduce if(P,R) :- true # f if(P,R).
(b) P(X,Y) :- R(Y,X) interp(P,X,Y) :- interp0(R,Y,X),abduce ifInv(P,R).abduce ifInv(P,R) :- true # f ifInv(P,R).
(c) P(X,Y) :- R1(X,Z),R2(Z,Y) interp(P,X,Y) :- interp0(R1,X,Z),interp0(R2,Z,Y),abduce chain(P,R1,R2).abduce chain(P,R1,R2) :- true # f chain(P,R1,R2).base case for SL interpreter interp0(P,X,Y) :- rel(R,X,Y).insertion point for learned rules interp0(P,X,Y) :- any rules learned by SL.Information extraction(d) R(X,Y) :- link(X,Y,W), interp(R,X,Y) :- link(X,Y,W),abduce indicates(W,R).indicates(W,R).
abduce indicates(W,R) :- true #f ind1(W,R).
(e) R(X,Y) :- link(X,Y,W1), interp(R,X,Y) :- link(X,Y,W1),link(X,Y,W2),link(X,Y,W2), abduce indicates(W1,W2,R).indicates(W1,W2,R).
abduce indicates(W1,W2,R) :- true #f ind2(W1,W2,R).Latent context invention(f) R(X,Y) :- latent(L), interp(R,X,Y) :- latent(L),link(X,Y,W),abduce latent(W,L,R).link(X,Y,W), abduce latent(W,L,R) :- true #f latent1(W,L,R).indicates(W,L,R)(g) R(X,Y) :- latent(L1),latent(L2) interp(R,X,Y) :- latent(L1),latent(L2),link(X,Y,W),link(X,Y,W), abduce latent(W,L1,L2,R).indicates(W,L1,L2,R) abduce latent(W,L1,L2,R) :- true #f latent2(W,L1,L2,R).Table 2: The ProPPR template and clauses for joint structure learning and information extraction.3.2 Joint Model for IE and SRLDataset Generation The KBs and text used inour experiments were derived from Wikipedia.Briefly, we choose a set of closely-related pagesfrom a hand-selected Wikipedia list.
These pagesdefine a set of entities E , and a set of commonly-used Infobox relations R between these entitiesdefine a KB.
The relation mentions are hyperlinksbetween the pages, and the features of these rela-tion mentions are words that appear nearby theselinks.
This information is encoded in a single rela-tion link(X,Y,W), which indicates that there is hy-perlink between Wikipedia pages X to Y whichis near the context word W .
The Infobox relationtriples are stored in another relation, rel(R,X,Y).4Figure 2 shows an example.
We first find the?European royal families?
to find a list of enti-4In more detail, the extraction process was as follows.
(1)We used a DBpedia dump of categories and hyperlink struc-ture to find pages in a category; sometimes, this includedcrawling a supercategory page to find categories and then en-tities.
(2) We used the DBpedia hyperlink graph to find thetarget entity pages, downloaded the most recent (2014) ver-sion of each of these pages, and collected relevant hyperlinksand anchor text, together with 80 characters of context to ei-ther side.ties E .
This list contains the page ?Louis VI ofFrance?, the source entity, which contains an out-link to the target entity page ?Philip I of France?.On the source page, we can find the following text:?Louis was born in Paris, the son of Philip I andhis first wife, Bertha of Holland.?
From Infoboxdata, we also may know of a relationship betweenthe source and target entities: in this case, the tar-get entity is the parent of the source entity.Theory for Joint IE and SL The structure learn-ing templates we used are identical to those usedin prior work (Wang et al, 2014), and are summa-rized by the clauses (a-c) in Table 2.
In the tem-plates in the left-hand side of the table, P , R, R1and R2 are variables in the template, which willbe bound to specific relations found to be usefulin prediction.
(The interpreter rules on the right-hand side are provided for completeness, and canbe ignored by readers not deeply familiar with thework of (Wang et al, 2014).
)The second block of the table contains the tem-plates used for IE.
For example, to understandtemplate (d), recall that the predicate link in-dicates a hyperlink from Wikipedia page X to359Y , which includes the context word W betweentwo entities X and Y .
The abductive predicateabduce indicates activates a feature template, inwhich we learn the degree of association of a con-text word and a relation from the training data.These rules essentially act as a trainable classi-fier which classifies entity pairs based on the hy-perlinks they that contain them, and classifies thehyperlinks according to the relation they reflect,based on context-word features.Notice that the learner will attempt to tune wordassociations to match the gold rel facts used astraining data, and that doing this does not requireassigning labels to individual links, as would bedone in a traditional distant supervision setting:instead these labels are essentially left latent in thismodel.
Similar to ?deep learning?
approaches, thelatent assignments are provided not by EM, but byhill-climbing search in parameter space.A natural extension to this model is to add abilexical version of this classifier in clause (e),where we learn a feature which conjoins wordW1, word W2, and relation R.Combining the clauses from (a) to (e), we de-rive a hybrid theory for joint SL and IE: the struc-ture learning section involves a second-order prob-abilistic logic theory, where it searches the rela-tional KB to form plausible first-order relationalinference clauses.
The information extraction sec-tion from (d) to (e) exploits the distributional sim-ilarity of contextual words for each relation, andextracts relation triples from the text, using distantsupervision and latent labels for relation mentions(which in our case are hyperlinks).
Training thistheory as a whole trains it to perform joint reason-ing to facts for multiple relations, based on rela-tions that are known (from the partial KB) or in-ferred from the IE part of the theory.
Both param-eters for the IE portion of the theory and inferencerules between KB relations are learned.5Latent Context Invention Note that so far boththe IE clauses (d-e) are fully observable: thereare no latent predicates or variables.
Recentwork (Riedel et al, 2013) suggests that learninglatent representations for words improves perfor-mance in predicting relations.
Perhaps this is be-cause such latent representations can better modelthe semantic information in surface forms, whichare often ambiguous.5In in addition to finding rules which instantiate the tem-plates, weights on these rules are also learned.We call our method latent context invention(LCI), and it is inspired from literature in predi-cate invention (Kok and Domingos, 2007).6LCIapplies the idea of predicate invention to the con-text space: instead of inventing new predicates, wenow invent a latent context property that capturesthe regularities among the similar relational lex-ical items.
To do this, we introduce some addi-tional rules of the form latent(1) :- true, latent(2):- true, etc, and allow the learner to find appro-priate weights for pairing these arbitrarily-chosenvalues with specific words.
This is implementedby template (f) in Table 2.
Adding this to the jointtheory means that we will learn to map surface-level lexical items (words) to the ?invented?
latentcontext values and also to relation.Another view of LCI is that we are learning a la-tent embedding of words jointly with relations.
Intemplate (f) we model a single latent dimension,but to model higher-dimensional latent variables,we can add the clauses such as (g), which con-structs a two-dimensional latent space.
Below wewill call this variant method hLCI.4 DatasetsUsing the data generation process that we de-scribed in subsection 3.2, we extract two datasetsfrom the supercategories of ?European royal fam-ilies?
and ?American people of English descent,and third geographic dataset using three lists: ?Listof countries by population?, ?List of largest citiesand second largest cities by country?
and ?List ofnational capitals by population?.For the royal dataset, we have 2,258 pageswith 67,483 source-context-target mentions, andwe use 40,000 for training, and 27,483 for test-ing.
There are 15 relations7.
In the Amer-ican dataset, we have 679 pages with 11,726mentions, and we use 7,000 for training, and4,726 for testing.
This dataset includes 30 re-lations8.
As for the Geo dataset, there are 4976To give some background on this nomenclature, we notethat the SL method is inspired by Cropper and Muggleton?sMetagol system (Cropper and Muggleton, 2014), which in-cludes predicate invention.
In principle predicates could beinvented by SL, by extending the interpreter to consider ?in-vented?
predicate symbols as binding to its template vari-ables (e.g., P and R); however, in practice invented predi-cates leads to close dependencies between learned rules, andare highly sensitive to the level of noise in the data.7birthPlace, child, commander, deathPlace, keyPerson,knownFor, monarch, parent, partner, predecessor, relation,restingPlace, spouse, successor, territory8architect, associatedBand, associatedMusicalArtist, au-360pages with 43,475 mentions, and we use 30,000for training, and 13,375 for testing.
There are10 relations9.
The datasets are freely availablefor download at http://www.cs.cmu.edu/?yww/data/jointIE+Reason.zip.5 ExperimentsTo evaluate these methods, we use the setting ofKnowledge Base completion (Socher et al, 2013;Wang et al, 2014; West et al, 2014).
We ran-domly remove a fixed percentage of facts in atraining knowledge base, train the learner fromthe partial KB, and use the learned model to pre-dict facts in the test KB.
KB completion is a well-studied task in SRL, where multiple relations areoften needed to fill in missing facts, and thusreconstruct the incomplete KB.
Following priorwork (Riedel et al, 2013; Wang et al, 2013), weuse mean average precision (MAP) as the evalua-tion metric.5.1 BaselinesTo understand the performance of our joint model,we compare with three prior methods.
Struc-ture Learning (SL) includes the second-order re-lation learning templates (a-c) from Table 2.
In-formation Extraction (IE) includes only tem-plates (d) and (e).
Markov Logic Networks(MLN) is the Alchemy?s implementation10ofMarkov Logic Networks (Richardson and Domin-gos, 2006), using the first-order clauses learnedfrom SL method11.
We used conjugate gradientweight learning (Lowd and Domingos, 2007) with10 iterations.
Finally, Universal Schema is astate-of-the-art matrix factorization based univer-sal method for jointly learning surface patterns andrelations.
We used the code and parameter settingsfor the best-performing model (NFE) from (Riedelet al, 2013).As a final baseline method, we considereda simpler approach to clustering context words,thor, birthPlace, child, cinematography, deathPlace, direc-tor, format, foundationOrganisation, foundationPerson, in-fluenced, instrument, keyPerson, knownFor, location, mus-icComposer, narrator, parent, president, producer, relation,relative, religion, restingPlace, spouse, starring, successor,writer9archipelago, capital, country, daylightSavingTimeZone,largestSettlement, leaderTitle, mottoFor, timeZone, twinCity,twinCountry10http://alchemy.cs.washington.edu/11We also experimented with Alchemy?s structure learn-ing, but it was not able to generate results in 24 hours.which we called Text Clustering, which used thefollowing template:R(X,Y) :-clusterID(C),link(X,Y,W),cluster(C,W),related(R,W).Here surface patterns are grouped to form latentclusters in a relation-independent fashion.5.2 The Effectiveness of the Joint ModelOur experimental results are shown in 3.
The left-most part of the table concerns the Royal dataset.We see that the universal schema approach out-performs the MLN baseline in most cases, butProPPR?s SL method substantially improves overMLN?s conjugated gradient learning method, andthe universal schema approach.
This is perhapssurprising, as the universal schema approach isalso a joint method: we note that in our datasets,unlike the New York Times corpus used in (Riedelet al, 2013), large numbers of unlabeled examplesare not available.
The unigram and bilexical IEmodels in ProPPR also perform well?better thanSL on this data.
The joint model outperforms thebaselines, as well as the separate models.
The dif-ference is most pronounced when the backgroundKB gets noisier: the improvement with 10% miss-ing setting is about 1.5 to 2.3% MAP, while with50% missing data, the absolute MAP improve-ment is from 8% to 10%.In the next few columns of Table 3, we show theKB completion results for the Geo dataset.
Thisdataset has fewer relations, and the most com-mon one is country.
The overall MAP scores aremuch higher than the previous dataset.
MLN?s re-sults are good, but still generally below the uni-versal schema method.
On this dataset, the uni-versal schema method performs better than the IEonly model for ProPPR in most settings.
However,the ProPPRjoint model still shows large improve-ments over individual models and the baselines:the absolute MAP improvement is 22.4%.Finally, in the rightmost columns of Table 3,we see that the overall MAP scores for the Ameri-can dataset are relatively lower than other datasets,perhaps because it is the smallest of the three.The universal schema approach consistently out-performs the MLN model, but not ProPPR.
On thisdataset the SL-only model in ProPPR outperformsthe IE-only models; however, the joint models stilloutperform individual ProPPR models from 1.5%to 6.4% in MAP.361Royal Geo American% missing 10% 20% 30% 40% 50% 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%BaselinesMLN 60.8 43.7 44.9 38.8 38.8 80.4 79.2 68.1 66.0 68.0 54.0 56.0 51.2 41.0 13.8Universal Schema 48.2 53.0 52.9 47.3 41.2 82.0 84.0 75.7 77.0 65.2 56.7 51.4 55.9 54.7 51.3SL 79.5 77.2 74.8 65.5 61.9 83.8 80.4 77.1 72.8 67.2 73.1 70.0 71.3 67.1 61.7IE onlyIE (U) 81.3 78.5 76.4 75.7 70.6 83.9 79.4 73.1 71.6 65.2 63.4 61.0 60.2 61.4 54.4IE (U+B) 81.1 78.1 76.2 75.5 70.3 84.0 79.5 73.3 71.6 65.3 64.3 61.2 61.1 62.1 55.7JointSL+IE (U) 82.8 80.9 79.1 77.9 78.6 89.5 89.4 89.3 88.1 87.6 74.0 73.3 73.7 70.5 68.0SL+IE (U+B) 83.4 82.0 80.7 79.7 80.3 89.6 89.6 89.5 88.4 87.7 74.6 73.5 74.2 70.9 68.4Joint + LatentJoint + Clustering 83.5 82.3 81.2 80.2 80.7 89.8 89.6 89.5 88.8 88.4 74.6 73.9 74.4 71.5 69.7Joint + LCI 83.5 82.5 81.5 80.6 81.1 89.9 89.8 89.7 89.1 89.0 74.6 74.1 74.5 72.3 70.3Joint + LCI + hLCI 83.5 82.5 81.7 81.0 81.3 89.9 89.7 89.7 89.6 89.5 74.6 74.4 74.6 73.6 72.1Table 3: The MAP results for KB completion on three datasets.
U: unigram.
B: bigram.
Best result ineach column is highlighted in bold.The averaged training runtimes on an ordinaryPC for unigram joint model on the above Royal,Geo, American datasets are 38, 36, and 29 sec-onds respectively, while the average testing timesare 11, 10, and 9 seconds.
For bilexical joint mod-els, the averaged training times are 25, 10, and 10minutes respectively, whereas the testing times are111, 28, and 26 seconds respectively.5.3 The Effectiveness of LCIFinally we consider the latent context invention(LCI) approach.
The last three rows of Table 3show the performances of LCI and hHCI.
We com-pare it here with the best previous approach, thejoint IE + SL model, and text clustering approach.For the Royal dataset, first, the LCI and hLCImodels clearly improve over joint IE and SL.
Innoisy conditions of missing 50% facts, the biggestimprovement of LCI/hLCI is 2.4% absolute MAP.From the Geo dataset, we see that the joint mod-els and joint+latent models have similar perfor-mances in relatively clean conditions (10%-30%)facts missing.
However, in noisy conditions, wethe LCI and hLCI model has an advantage of be-tween 1.5% to 1.8% in absolute MAP.Finally, the results for the American datasetshow a consistent trend: again, in noisy condi-tions (missing 40% to 50% facts), the latent con-text models outperform the joint IE + SL modelsby 2.9% and 3.7% absolute MAP scores.Although the LCI approach is inspired by pred-icate invention in inductive logic programming,our result is also consistent with theories of gen-eralized latent variable modeling in probabilis-tic graphical models and statistics (Skrondal andRabe-Hesketh, 2004): modeling hidden variableshelps take into account the measurement (observa-tion) errors (Fornell and Larcker, 1981) and resultsin a more robust model.6 DiscussionsCompared to state-of-the-art joint models (Riedelet al, 2013) that learn the latent factor represen-tations, our method gives strong improvements inperformance on three datasets with various set-tings.
Our model is also trained to retrieve a targetentity from a relation name plus a source entity,and does not require large samples of unlabeled ornegative examples in training.Another advantage of the ProPPR model is thatthey are explainable.
For example, below are thefeatures with the highest weights after joint learn-ing from the Royal dataset, written as predicatesor rules:indicates(?mother?,parent)indicates(?king?,parent)indicates(?spouse?,spouse)indicates(?married?,spouse)indicates(?succeeded?,successor)indicates(?son?,successor)parent(X,Y) :- successor(Y,X)successor(X,Y) :- parent(Y,X)spouse(X,Y) :- spouse(Y,X)parent(X,Y) :- predecessor(X,Y)successor(Y,X) :- spouse(X,Y)predecessor(X,Y) :- parent(X,Y)Here we see that our model is able to learn that thekeywords ?mother?
and ?king?
that are indicators362of the relation parent, that the keywords ?spouse?and ?married?
indicate the relation spouse, and thekeywords ?succeeded?
and ?son?
indicate the re-lation successor.
Interestingly, our joint model isalso able to learn the inverse relation successor forthe relation parent, as well as the similar relationalpredicate predecessor for parent.7 ConclusionsIn this paper, we address the issue of joint infor-mation extraction and relational inference.
To bemore specific, we introduce a holistic probabilis-tic logic programming approach for fusing IE con-texts with relational KBs, using locally groundableinference on a joint proof graph.
We then proposea latent context invention technique that learnsrelation-specific latent clusterings for words.
Inexperiments, we show that joint modeling for IEand SRL improves over prior state-of-the-art base-lines by large margins, and that the LCI modeloutperforms various fully baselines on three real-world Wikipedia dataset from different domains.In the future, we are interested in extending thesetechniques to also exploit unlabeled data.AcknowledgmentThis work was sponsored in part by DARPA grantFA87501220342 to CMU and a Google ResearchAward.ReferencesReid Andersen, Fan R. K. Chung, and Kevin J. Lang.2008.
Local partitioning for directed graphs usingpagerank.
Internet Mathematics, 5(1):3?22.Gabor Angeli, Julie Tibshirani, Jean Y Wu, andChristopher D Manning.
2014.
Combining distantand partial supervision for relation extraction.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Bogdan Babych and Anthony Hartley.
2003.
Im-proving machine translation quality with automaticnamed entity recognition.
In Proceedings of the7th International EAMT workshop on MT and otherLanguage Technology Tools, Improving MT throughother Language Technology Tools: Resources andTools for Building MT, pages 1?8.
Association forComputational Linguistics.Michele Banko, Michael J Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction for the web.
In IJCAI,volume 7, pages 2670?2676.Mark Craven, Johan Kumlien, et al 1999.
Construct-ing biological knowledge bases by extracting infor-mation from text sources.
In ISMB, volume 1999,pages 77?86.Andrew Cropper and Stephen H Muggleton.
2014.Can predicate invention in meta-interpretive learn-ing compensate for incomplete background knowl-edge?
Proceedings of the 24th International Con-ference on Inductive Logic Programming.Kroly Csalogny, Dniel Fogaras, Balzs Rcz, and TamsSarls.
2005.
Towards scaling fully personalizedPageRank: Algorithms, lower bounds, and experi-ments.
Internet Mathematics, 2(3):333?358.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In 52nd Annual Meet-ing of the Association for Computational Linguis-tics, Baltimore, MD, USA, June.Jenny Rose Finkel and Christopher D Manning.
2009.Joint parsing and named entity recognition.
In Pro-ceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 326?334.
Association for Computa-tional Linguistics.Claes Fornell and David F Larcker.
1981.
Evaluatingstructural equation models with unobservable vari-ables and measurement error.
Journal of marketingresearch, pages 39?50.Lise Getoor and Ben Taskar.
2007.
Introduction tostatistical relational learning.
MIT press.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies-Volume 1, pages 541?550.
Association for Compu-tational Linguistics.Stanley Kok and Pedro Domingos.
2007.
Statisticalpredicate invention.
In Proceedings of the 24th in-ternational conference on Machine learning, pages433?440.
ACM.Ni Lao, Tom M. Mitchell, and William W. Cohen.2011.
Random walk inference and learning in alarge scale knowledge base.
In EMNLP, pages 529?539.
ACL.Ni Lao, Amarnag Subramanya, Fernando C. N. Pereira,and William W. Cohen.
2012.
Reading the webwith learned syntactic-semantic inference rules.
InEMNLP-CoNLL, pages 1017?1026.
ACL.Jiwei Li, Alan Ritter, and Eduard Hovy.
2014.Weakly supervised user profile extraction from twit-ter.
ACL.363Daniel Lowd and Pedro Domingos.
2007.
Efficientweight learning for markov logic networks.
InKnowledge Discovery in Databases: PKDD 2007,pages 200?211.
Springer.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Diego Moll?a, Menno Van Zaanen, and Daniel Smith.2006.
Named entity recognition for question an-swering.
Proceedings of ALTW, pages 51?58.Larry Page, Sergey Brin, R. Motwani, and T. Wino-grad.
1998.
The PageRank citation ranking: Bring-ing order to the web.
In Technical Report, ComputerScience department, Stanford University.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Mach.
Learn., 62(1-2):107?136.Sebastian Riedel, Limin Yao, and Andrew McCal-lum.
2010.
Modeling relations and their men-tions without labeled text.
In Machine Learning andKnowledge Discovery in Databases, pages 148?163.Springer.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M Marlin.
2013.
Relation extraction withmatrix factorization and universal schemas.
In Pro-ceedings of NAACL-HLT, pages 74?84.Anders Skrondal and Sophia Rabe-Hesketh.
2004.Generalized latent variable modeling: Multilevel,longitudinal, and structural equation models.
CRCPress.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in Neural Information Processing Systems,pages 926?934.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 455?465.
Association for Computational Linguistics.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
Introduction to statistical relational learn-ing, pages 93?128.William Yang Wang and Zhenhao Hua.
2014.
Asemiparametric gaussian copula regression modelfor predicting financial risks from earnings calls.
InProceedings of the 52th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2014),Baltimore, MD, USA, June.
ACL.William Yang Wang, Kathryn Mazaitis, and William WCohen.
2013.
Programming with personalizedpagerank: a locally groundable first-order proba-bilistic logic.
In Proceedings of the 22nd ACM in-ternational conference on Conference on informa-tion & knowledge management, pages 2129?2138.ACM.William Yang Wang, Kathryn Mazaitis, and William WCohen.
2014.
Structure learning via parameterlearning.
Proceedings of the 23rd ACM Interna-tional Conference on Information and KnowledgeManagement (CIKM 2014).William Yang Wang, Kathryn Mazaitis, Ni Lao, TomMitchell, and William W Cohen.
2015.
Efficient in-ference and learning in a large knowledge base: Rea-soning with extracted information using a locallygroundable first-order probabilistic logic.
MachineLearning Journal.Robert West, Evgeniy Gabrilovich, Kevin Murphy,Shaohua Sun, Rahul Gupta, and Dekang Lin.
2014.Knowledge base completion via search-based ques-tion answering.
In Proceedings of the 23rd interna-tional conference on World wide web, pages 515?526.
International World Wide Web ConferencesSteering Committee.Jason Weston, Antoine Bordes, Oksana Yakhnenko,Nicolas Usunier, et al 2013.
Connecting languageand knowledge bases with embedding models for re-lation extraction.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1366?1371.Benjamin Roth Tassilo Barth Michael Wiegand andMittul Singh Dietrich Klakow.
2013.
Effective slotfilling based on shallow distant supervision methods.Proceedings of NIST KBP workshop.Fei Wu and Daniel S Weld.
2007.
Autonomously se-mantifying wikipedia.
In Proceedings of the six-teenth ACM conference on Conference on infor-mation and knowledge management, pages 41?50.ACM.Fei Wu and Daniel S Weld.
2010.
Open informationextraction using wikipedia.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 118?127.
Association forComputational Linguistics.364
