Constructing Text Sense RepresentationsRonald Winnem?llerRegional Computer CentreUniversity of HamburgHamburg, Germanyronald.winnemoeller@rrz.uni-hamburg.deAbstractIn this paper we present a novel approach to maptextual entities such as words, phrases, sentences,paragraphs or arbitrary text fragments onto artificialstructures which we call ?Text Sense RepresentationTrees?
(TSR trees).
These TSR trees represent anabstract notion of the meaning of the respective text,subjective to an abstract ?common?
understandingwithin the World Wide Web.
TSR Trees can beused to support text and language processing sys-tems such as text categorizers, classifiers, automaticsummarizers and applications of the Semantic Web.We will explain how to construct the TSR tree struc-tures and how to use them properly; furthermore wedescribe some preliminary evaluation results.1 IntroductionMany important tasks in the field of Natural Lan-guage Processing (NLP) such as text categorization,text summarization, (semi-) automatic translationand such require a certain amount of world knowl-edge and knowledge about text meaning and sense(Allen, 1995; R. Cole et al, 1995).Handling the amount of textual data in the WorldWideWeb also increasingly requires advanced auto-matic text and language processing techniques: suc-cessful search engines like Google (Google, Inc.,2004) already employ text retrieval and informationextraction methods based on shallow semantic in-formation.There are many methodologies to generate wordsense representations, but efficiency and effectiv-ity of fully automated techniques tends to be low(Diana Zaiu Inkpen and Graeme Hirst, 2003).
Fur-thermore, formalisation and quantification of eval-uation methods is difficult because in general wordsense related techniques are only verifyable throughtheoretical examination, application on language orhuman judges (Alexander Budanitsky and GraemeHirst, 2001), i.e.
there is no inherent validationbecause there is no direct connection to the worldas perceived by humans.
In the case of frequencybased word sense representations corpus related dif-ficulties arise (number of tagged entities, corpusquality, etc.).
In order to overcome these limita-tions, we developed a methodology to generate anduse explicit computer-usable representations of textsenses.A common understanding of the ?sense?
of wordsis defined by the ways the word is used in context,i.e.
the interpretation of the word that is consistentwith the text meaning1 - as summarized by S. G.Pulman in (R. Cole et al, 1995, Section 3.5).
Ex-tending this definition onto full texts, we introduceour notion of ?Text Sense Representation?
(TSR) as?the set of possible computer usable interpretationsof a text without respect to a particular linguisticcontext?2.TSR Trees provide detailed answers to questionslike ?how close are these n words topically relatedto each other?
?, ?are these m sentences really aboutthe same topic??
or ?how much does paragraph xcontribute to topic y??.
They cannot tell e.g.
a tele-phone is a physical artifact, it?s purpose is to enabledistant communication, etc.TSR Trees are not meant to substitute meaningacquired through conceptual or linguistic analysisbut are rather aimed at:?
augmenting deeper (linguistic or conceptual)methodologies by providing additional analy-sis clues?
standalone usage in generic shallow methods(e.g.
in shallow text categorization) and spe-cific applications (e.g.
anti-spam functionality)2 Related WorkOur notion of semantics is closely related to the no-tion of ?naive semantics?
discussed in (K. Dahlgren1In this paper, we would like to extend the notion of ?WordSense?
onto ?Text Sense?, i.e.
texts of arbitrary length2Allen instead uses the term ?logical form?
for this kindof context-independent meaning representation, c.f.
(Allen,1995), p.14et al, 1989).
This article describes ?naive se-mantics?
as ?a level of world knowledge that isgeneral and common to many speakers of a lan-guage?, i.e.
commonsense knowledge associatedwith words.
Naive semantics identifies words withconcepts which vary in type.A discussion of fundamental corpus-related as-pects of word senses is provided by Kilgariff (AdamKilgariff, 1997).
Kilgariff herein questions the useof word sense disambiguation and concludes thatword senses can only be defined ?relative to a setof interests?
and the ?basic units of word meanings?are occurrences of words in contexts.
Our notion ofTSR trees aims at aggregating text meaning in it?stopical context in order to construct a context inde-pendent representation.In Literature, there are several strong directionsof representing text meaning or text sense: oneprominent approach uses frame-based representa-tion languages in combination with first order logicsemantics.
The analyzed text is matched againstthe frame database in order to construct text mean-ing representations.
An example of this approach ispresented by Clark et.
al.
(P. Clark et al, 2003).Dahlgren et.
al.
present ?KT?, a complex text un-derstanding system based on naive semantics.
KTalso uses frames to represent semantic content.A project that is based on a roughly similar no-tion of text meaning representation (TMR) conceptsis the ?kosmos project (Mahesh, 1996; Kavi Ma-hesh and Sergei Nirenburg, 1996).
It is aimed at thecreation of a machine translation system that uses abroad-coverage ontology and various input sourcesin order to translate english to spanish texts and viceversa.
TMR concepts within ?kosmos are hand-written frame-based data structures.
Text meaningis represented by instances thereof that are derivedby semantic rules from a linguistic rule database.Frame-based meaning representations are alsothe basis of AutoSlog-TS, an information extrac-tion system that automatically acquires conceptualpatterns from untagged texts, using only a prepro-cessed training corpus (Ellen Riloff and Jay Shoen,1995).
The thusly constructed concepts can be seenas text meaning representations.Approaches of computing text meaning similar-ities include using web directories for generatingpath-shaped data structures for text categorization(Fabrizio Sebastiani, 2003; Giuseppe Attardi et al,1998).
Sebastiani herein purports his efforts in bymining the structure of both web ?catalogues?
(webdirectories) for extracting category labels and min-ing web page structure for the actual classificationtask.
This is an example for using path- and graphbased methods rather than frame based structures.Another example would be the methodology de-scribed in this article.3 TSR TreesIn this Section we will informally describe our twoalgorithms for constructing Text Sense Representa-tion Trees.
The first algorithm builds ?initial?
TSRtrees of single input words or very short phrases(Section 3.1), the second generates ?derived?
TSRtrees for arbitrary texts from pre-computed TSRtrees.3.1 Building Initial TSR TreesThe algorithm for building initial TSR trees is basedon the retrieval of pages from a ?web directory?
A?web directory?
(other sources use the term ?webcatalogue?
(Fabrizio Sebastiani, 2003)) is a brows-able taxonomy of web pages.
These web pages areparsed and category descriptions and weight valuesare extracted from them.
The extracted informationis then merged into term-specific TSR trees, option-ally normalized and pruned.In the following explanations we will use the no-tions ?input term?, ?input word?
and ?input phrase?as follows: An input term is any text that is usedas input to an algorithm or program.
An inputword is any singular word that is an input term.
Aword is defined as sequence of alphanumeric sym-bols not interrupted by whitespace.
An input phraseis any sequence of input words that are separated bywhitespace or other non-alphanumeric characters.Our algorithm takes single words or very shortphrases as input terms and assumes that every partof the input phrase has pragmatic and semantic rel-evance.
Input term selection is therefore a funda-mental prerequisite in this context.The output of our algorithm consists of a treestructure of labeled and weighted nodes.
The la-bels are short phrases that provide some meaningfulcontext while the weights are simple integer num-bers.
Each tree node has exactly one label and oneweight attached to it.The following five steps will explain how to gen-erate initial TSR Trees:a. Retrieval The input term is redirected as inputto a web directory.
33Since our prototype was based on the ?Open DirectoryProject?
(ODP), consisting of a web directory and a search en-gine (Netscape Inc., 2004), we will refer to this particular ser-vice throughout this article and use it as implementation datasource.
Nonetheless, our algorithm is not restricted to the ODPbut can use other web directories like Yahoo Inc. (Yahoo Inc.,2004) or even internet newsgroups.Figure 1: Category Path Listing for ?account?
ex-ample (excerpt)Figure 2: TSR tree for ?account?
example (excerpt)The web directory to use is not assumed to meetstrict requirements in terms of sensible category la-bels, balanced branches, etc.
but can be any taxo-nomic structure provided it can be transformed intoweighted paths and is large enough to cover a sub-stantial subset of the target language.Outcome of this redirection is a HTML-formattedlist of categories including the number of hits foreach category.b.
Tree Construction The lines of the outputlist returned by the web directory are then parsedand converted into a sequence of weighted categoryterms.
Because each sequence represents a differentcontextual use of the word (in the symbolic sense),each sequence also represents a different sense ofthat word in that topical context.Each term contains a singular category path labeland the number of query hits within that category.An excerpt of the account example terms is exem-plified in the Figure 1:After that, all terms are merged into a single hier-archical tree with weighted and labeled nodes.
Fig-ure 2 provides an example hereof.The resulting tree then reprenrepresentsts the in-put text phrase.
Even though the uniqueness ofthis representation cannot be guaranteed in theory, aclash of two different terms representation is highlyunlikely.The tree generation process obviously fails if theinput term cannot be found within the web directoryand hence no categorical context is available for thatterm.Figure 3: The complete (unpruned) ?account?
TSRtreeFigure 4: Pruning the ?account?
TSR tree by thresh-old 5%c.
Normalization In order to enable uniform pro-cessing of arbitrary trees, each tree has to be ?nor-malized?
by weight adjustment: The sum of all nodeweights is computed as 100 percent.
All weights arethen recalculated as percentage of this overall treeweight sum.
The sum weight is attached to the TSRtree root as ?tree weight?.d.
Node Pruning (optional) Due to the nature ofthe underlying web directory, there are sometimesfalse positives in wrong categories, i.e.
when a termis used in a rather esoteric way (e.g.
as part of afigure of speech, etc.
).In order to sort out such ?semantical noise?,?insignificant?
nodes can be deleted using a com-mon heuristic.
Some preliminary experiments haveshown that using a certain threshold on the nodeweight percentage is a good heuristic.
An exampleof a processed TSR tree is shown in Figure 4. whilethe corresponding unprocessed TSR tree is depictedin Figure 34e.
List Transformation (optional) It is possibleto transform a TSR tree into a list: by iterating theTSR tree and selecting only the nodes with the high-4The labels within this figure might be printed too small toread but it is the shape of the structure that is important ratherthan individual node labels.Figure 5: List representation of ?account?
example(from excerpt)est weight at each respective depth, a TSR list iseasily created.
This list represents the most com-mon meaning of the input term.
Since this mean-ing is applicable in most cases, sufficiently robustalgorithms may use these lists successfully, e.g.
forsimple text classification purposes.
An example list,derived from the ?account?
example (Figure 2), isdepicted in Figure 5.f.
External Representation (optional) Lastly,the tree is converted into an external representationin the RDF (O. Lassila and R. Swick, 1999) lan-guage.
We chose this particular paradigm because itis an open standard and well suited for representinggraph and tree based data.
Furthermore, a numberof tools for dealing with RDF already exist ?
RDFis one of the basic building blocks of the SemanticWeb (O. Lassila and R. Swick, 1999) and we ex-pect RDF based TSR trees to be of great use in thatdomain (e.g.
for classification and information ex-traction).Summary In this section we presented the con-struction of computer usable complex TSR trees byutilizing an underlying web directory containing ex-plicit world knowledge.
The generated trees are ourbasic building blocks to represent the ?sense?
of theinput term in a programmatically usable way.The construction of a TSR tree can therefore beseen as the result of a (shallow) text ?understand-ing?
process as defined in (Allen, 1995).3.2 Constructing Derived TSR TreesTSR trees can also be constructed by merging ex-istent TSR trees.
This process provides means ofdealing with complex phrases: through adding TSRtrees (applying the set union operation) it is possi-bly to acquire TSR trees of arbitrary text fragments,i.e.
to build TSR trees by merging the TSR trees ofits constituents.By using the derivation process, TSR trees canbe built for arbitrary input texts while maintainingcomparability through the respective tree features(see 4.1).Since TSR trees consist of weighted paths, out-of-context senses of single terms will be eliminatedin the merging process.
This makes using TSR treesin large texts a very robust algorithm (preliminaryexperiments have shown that virtually all errors oc-cur in preprocessing steps such as language identi-fication, etc.).
Superficial investigation showed thatTSR trees generated from complex descriptions areof higher quality than TSR trees from single terms(less ?semantic noise?, features are more expres-sive).On the other hand, the derivation process (in con-junction with a dictionary) can also be used to buildTSR trees of descriptions of words that cannot befound in the web directory as a substitute for theword itself.It is a matter of current research whether TSRtrees derived from dictionary-like descriptions ofterms are in general preferrable to the use of ini-tial TSR trees (see the discussion of the ?distance?feature in 4.2).4 Using Text Sense Representation TreesIn this Section, the term ?feature?
will be used quitesynonymous to the term ?operation?
: a feature is abit of information that is retrieved by application ofthe corresponding operation on a TSR tree.It is important to note that even though the TSRtrees themselves are very subjective to the underly-ing web directory, the resulting features do not showthis weakness.
Any NLP application implementingalgorithms that are based on TSR trees should notrely on the tree representations themselves (in termsof tree structure or node labels), but rather on the op-erational TSR tree features discussed in this section.4.1 Simple TSR Tree FeaturesAt first, we define a set of four features that can becomputed for single TSR trees:1.
Tree Weight.
The individual tree?s weight canbe interpreted as quantitative measure of the in-put term within the web directory.
By compar-ing the weight of individual trees, it is possibleto determine which term occurs more often inwritten language (as defined by the web direc-tory itself).2.
Generality.
The tree?s breadth factor is an in-dicator for the ?generality?
of the input term,i.e.
the broader the tree, the more general theuse of the word in written language and themore textual contexts the word can be used in.General terms tend to be not specific to partic-ular web pages, hence will show up in a num-ber of pages throughout the web taxonomy.
Incontrast, less general terms tend to occur onlyon pages specific to a particular category in theweb taxonomy.3.
Domain Dependency.
The tree?s depth factorcan be interpreted as ?domain dependency in-dicator?
of the input term.
Deep structures onlyoccur when the input term is most often locatedat deep subcategory levels in the web directorywhich is an indicator for restricted use of thatterm within a particular domain of interest.4.
Category Label.
Usually the node labelsthemselves provide clues to its respective termsmeaning.
Even though these clues may bequite subjective and in some cases misleadingor incomplete, in most cases they can serve ashints for human skimming of the categories in-volved.
Since these labels are provided as en-glish words or short phrases, they might them-selves be subject to initial TSR tree building(see Section 4.3).4.2 Advanced TSR Tree Features andOperationsWhile operations on single TSR trees provide sim-ple text processing clues, operations on sets of treesare much more informative:1.
Difference.
A number of ?difference?
featuresare available that can be used to compare indi-vidual features of a number of trees:?
Tree weight difference?
Breadth difference?
Depth differenceThese difference features arise from compar-isons of the simple TSR tree features, hencethey describe numerical differences betweensuch values.
For example, a high weight dif-ference shows a high difference between therespective terms?
general use in language.It is important to note that the difference fea-tures are not only usable in respect to com-plete trees but can be applied to tree branchesas well, e.g.
in order to analyze tree behaviourin certain contexts.2.
Distance.
The ?distance?
feature is com-puted by counting the number of ?edit?
op-erations (add node, delete node, transformnode) it takes to transform one tree into an-other.
This feature is designed much after the?Levenshtein-distance feature?
in the field oftext-processing (D. S. Hirschberg, 1997).In general, this feature describes a notionof ?semantic relatedness?
between two inputterms, i.e.
a high distance value is expectedbetween largely unrelated terms such as ?air?and ?bezier curve?
while a low value is ex-pected between closely related terms such as?account?
and ?cash?.The distance feature can be implemented byapplying the set difference operation: the sub-tracting of TSR tree from one another resultsin a number of remaining nodes, i.e.
the actualdistance value.Recent findings have shown though that thissimple procedure is only applicable on treesof roughly the same number of nodes: obvi-ously, the computed distance of two words canachieve a high value when one word is muchmore common in language than the other (andis thusly represented by a much larger tree).This is true even when these two words areactually synonyms of each other, or just twodifferent lexical representations of the sameword, like ?foto?
and ?photo?.
In fact, becausethe co-occurence of different lexical represen-tations of the same word in the same text isquite seldom, is is very likely that in these sit-uations a high distance will show.It can be reasoned that these difficulties willprominently lead to the use of TSR trees de-rived from term descriptions rather than initialtrees (see 3.2).4.3 TSR Tree Translation and Corpus RelatedFeaturesIn some cases, a need for ?higher-level?
operationswill occur, e.g.
when two agents cooperate, whouse different web taxonomies.
Our approach is ableto deal with these situations through translation ofTSR trees of category labels (this can be interpretedas a simple classification task).Sometimes, information about TSR tree featuresof a corpus as a whole is important.
In these cases,the individual TSR trees of all items that constitutethe respective corpus are merged into one ?corpus?TSR tree.
Afterwards, the corpus tree can be an-alyzed using the features described in Section 4.1and Section 4.2.5 Preliminary Evaluation ResultsFor testing, we set up some preliminary experi-ments5:We built a prototype system based on aTomcat application server6 that was able to gener-ate TSR trees for lists of input terms and store these5Exhaustive evaluation is the goal of our current researchefforts6c.f.
http://jakarta.apache.orgtrees along with their width, weight and depth fea-tures in an SQL database.
From this database weextracted the data used in the evaluation process.We applied each feature explained in Section 4 ona set of words taken from 4 corpora.
These corporawere constructed as follows:The Basic corpus: The 100 first terms from adictionary of ?basic english words?
like account,brother, crush, building, cry, etc.
The Med cor-pus: The 100 first terms from a specialized med-ical dictionary.
The Comp corpus: The 100 firstterms from a specialized dictionary of computer sci-ence.
The Top?
corpus: The 100 terms that wereranked as ?top 100?
by the Wortschatz engine (UweQuasthoff, 2000).We expected terms of the Basic and Top corporato show high weight and breadth and low depth val-ues.
We also expected terms from the Med andComp corpora to be of high depth but differing inweight and breadth.These Expectations were supported by our resultsfrom generating and comparing the respective cor-pus TSR trees (see below).For brevity, we will only present a summary ofour findings here.Single Tree Features Comparing the outcome ofapplying single TSR tree features onto the four cor-pora showed some interesting results:1.
Tree Weight.
Terms from the Med corpus areoften not represented within the web directorywhich means that a TSR tree cannot be builtfor these terms.
In general, terms from theMed corpus have a very low tree weight value(in most cases < 10).
Strangely, some wordssuch as ?by?, ?and?, ?because?
etc.
from theTop corpus also have low ratings.
Examiningthe actual web directory pages exhibits thatthese terms seldom contributed to a web pagessemantic context and thusly were seldomrepresented in the web directory.
It appearsthat all input terms were interpreted by theODP search engine as being semanticallyrelevant, e.g.
the word ?about?
only generatedhits in categories about movie titles, e.g.Arts:Movies:Titles:A:About a Boy,Arts:Movies:Titles:A:About Adam,etc.This strongly indicates that the input to the al-gorithm should be a noun or a common nounphrase.Terms from the Basic corpus and the Compcorpus are rated comparably high, e.g.
somecommon words from the Basic corpus such as?air?, ?animal?, etc.
were assigned very highweight values (weight > 100).2.
Generality.
The generality values listing ex-hibits that indeed mostly general terms areidentified by this feature.
Surprisingly, someterms such as ?software?
and ?design?
werealso attributed high generality.
Further investi-gation shows that ?generality?
is a context de-pendent feature, e.g.
the term ?software?
isvery general for the computer domain.
Only atthe first tree level, a domain independent gen-erality factor can be attributed to this feature.We also found that pruning has its greatest ef-fect on this feature; this leads to the conclu-sion that the generality feature should be ap-plied on TSR trees that are not pruned accord-ing to some threshold.3.
Domain Dependency.
Except a very fewcases, all top rated terms are in the Comp orin the Med corpus i.e.
the two specialized cor-pora.
These terms are apparently more specificin context than the lower rating terms.Advanced Tree Features Even though we testedthe Multi Tree features on only a few test cases(about 30), we are confident that future evaluationwill confirm our preliminary results.1.
Difference.
Computing the difference of twoor three single TSR trees turned out to be lessinformative than the distance value betweenthese trees but a small number of experimentslead us to the conclusion that TSR trees of largetext fragments can be compared by differencefeatures with a conclusive outcome.2.
Distance.
Using node labels and weights forcomparison in any case resulted in a 100% dis-tance.
This effect derived from the fact thateven though some trees were similar in struc-ture, their respective weights differed in everycase.
The distance feature therefore is appli-cable to node labels only or has to introducearithmetical means for adjusting weights.
Af-ter correcting the distance algorithm, it workedas expected on trees with about the same nodenumber (High distance between e.g.
?blood?and ?air?, low distance between ?account?
and?credit?).
We also achieved reasonable resultson trees differing in node number when apply-ing a methodology of filtering homonymousaspects of the respective larger TSR tree (i.e.by using the node number of the smaller tree asupper bound and filtering first level tree nodes).Nonetheless we did not yet manage to find anabsolute numerical expression that describesthe distance feature appropriately.TSR Tree Translation and Corpus Features1.
Corpus Tree Features.We have merged all of the terms of each re-spective corpus in order to generate a ?corpusrepresentation tree?.
These corpus representa-tions can be used to demonstrate certain prop-erties of the chosen corpora.
Our experimentsexhibit that terms from the ?general?
corpora(Basic and Top) had a higher generality valuethan terms from the more specialized corpora(Comp and Med).
The same results also con-firm our hypothesis of the WWW occurrenceproperty of the computer corpus, since it is alsowell represented in the web dictionary.6 ConclusionsIn this paper, we have introduced a novel concept ofrepresenting text senses as defined in Section 1.According to the results of our preliminary eval-uation, our approach shows the following advan-tages:TSR trees can be used to unambiguously repre-sent text senses.
There is a fundamental semanticrelationship between TSR trees and their respectiveinput terms.
Their use is efficient: TSR trees can?
once retrieved and computed ?
be re-used with-out necessary modifications.
In that sense they canbe used ?stand-alone?.
Application of TSR treefeatures is very fast (one SELECT SQL statementwithin our prototype system).
Meaning represen-tation within TSR trees is robust: generating treesof text fragments7 by merging the TSR trees of itsconstituents reduces potential errors.TSR trees are in close interaction with the seman-tic context of the input terms, it is therefore possibleto determine topical relationships between textualfragments.Nonetheless, our findings also exhibit someweaknesses and dependencies:If an input term cannot be found within the webdirectory in use, a corresponding initial TSR treecannot be built.
This is a big problem for lan-guages that are not well represented in the web di-rectory (there is a strong bias towards the englishlanguage).
Very specialized domains (e.g.
med-ical topics) are also underrepresented in the webdirectory and hence problematic for the same rea-son.
Observations show that there is a strong bias7e.g.
sentences, paragraphs or static size text windowstowards computer and business related topics.
Oneapproach to solving these problems would be to usederived TSR trees in place of directly acquired TSRtrees.
It is yet a matter of current research to whichdegree intial TSR trees should be substituted by de-rived TSR trees.TSR tree usage usually depends on the outputquality of a number of preprocessing steps, e.g.language identification, noun phrase identification,morphological analysis, etc.7 Future WorkWe will continue research on the TSR tree topic.
Inparticular, we will investigate the relationship be-tween derived and initial TSR trees and in turn wewill find a more appropriate ?distance?
feature.
Weare also evaluating a new feature based on compar-ing tree labels.We will also thoroughly evaluate our approachagainst application based testing methodologies,e.g.
on text classification.
We will also implement anumber of example applications in the fields of textclassification and text summarization.ReferencesAdam Kilgariff.
1997.
I don?t believe in wordsenses.
Computers and the Humanities, 31(2):91?113.Alexander Budanitsky and Graeme Hirst.
2001.Semantic Distance in WordNet: An experimen-tal, application-oriented evaluation of five mea-sures.
In Workshop on WordNet and Other Lex-ical Resources, second meeting of the NorthAmerican Chapter of the Association for Compu-tational Linguists, Pittsburgh.James Allen.
1995.
Natural Language Understand-ing.
Benjaming/Cummings Publish.
Corp, CA, 2edition.D.
S. Hirschberg.
1997.
Serial computations ofLevenshtein distances.
In A. Apostolico andZ.
Galil, editors, Pattern matching algorithms,pages 123?141.
Oxford University Press.Diana Zaiu Inkpen and Graeme Hirst.
2003.Automatic sense disambiguation of the near-synonyms in a dictionary entry.
In Proceedingsof the 4th Conference on Intelligent Text Process-ing and Computational Linguistics, pages 258?267, Mexico City.Ellen Riloff and Jay Shoen.
1995.
AutomaticallyAcquiring Conceptual Patterns Without an Anno-tated Corpus.
In Proceedings of the Third Work-shop on Very Large Corpora, pages 148?161.Fabrizio Sebastiani.
2003.
Text categorization.
InAlessandro Zanasi, editor, Text Mining and itsApplications.
WIT Press, Southampton, UK.
In-vited chapter.
Forthcoming.Giuseppe Attardi, Sergio Di Marco, David F. Salvi,and Fabrizio Sebastiani.
1998.
Categorization bycontext.
In David Schwartz, Monica Divitini, andTerje Brasethvik, editors, Proceedings of IIIS-98,First International Workshop on Innovative Inter-net Information Systems, pages 1?13, Pisa, IT.Google, Inc. 2004.
Google.http://www.google.com.K.
Dahlgren, J. McDowell, and E. Stabler.
1989.Knoweldge representation for commonsense rea-soning with text.
Computational Linguistics,15(3).Kavi Mahesh and Sergei Nirenburg.
1996.
Mean-ing representation for knowpedge sharing in prac-tical machine translation.
In Florida ArtificialIntelligence Research Symposium, FLAIRS?96,Special Track on Information Interchange, pages19?22, Key West, FL.Kavi Mahesh.
1996.
Ontology development formachine translation: Ideology and methodology.Technical Report MCCS?96?292, ComputingResearch Laboratory New Mexico State Univer-sity.Netscape Inc. 2004.
Open directory project.http://dmoz.org.O.
Lassila and R. Swick.
1999.
Resource Descrip-tion Framework (RDF) Model and Syntax Speci-fication.
World Wide Web Consortium.P.
Clark, P. Harrison, and J. Thompson.
2003.
AKnowledge-Driven Approach to Text MeaningProcessing.
In Proceedings of the HLT Workshopon Text Meaning Processing, pages 1?6.
ACLPress.R.
Cole, J. Mariani, H. Uszkoreit, A. Zaenen, andV.
Zue.
1995.
Survey of the state of the art in hu-man language technology.
http://www.coli.uni-sb.de/ hansu/publ.html.Uwe Quasthoff.
2000.
Deutscher wortschatz on-line.
http://wortschatz.uni-leipzig.de.Yahoo Inc. 2004.
Yahoo.
http://www.yahoo.com.
