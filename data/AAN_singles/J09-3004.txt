Bootstrapping Distributional FeatureVector QualityMaayan Zhitomirsky-Geffet?Bar-Ilan UniversityIdo Dagan?
?Bar-Ilan UniversityThis article presents a novel bootstrapping approach for improving the quality of feature vectorweighting in distributional word similarity.
The method was motivated by attempts to utilizedistributional similarity for identifying the concrete semantic relationship of lexical entailment.Our analysis revealed that a major reason for the rather loose semantic similarity obtained bydistributional similarity methods is insufficient quality of the word feature vectors, caused bydeficient feature weighting.
This observation led to the definition of a bootstrapping schemewhich yields improved feature weights, and hence higher quality feature vectors.
The under-lying idea of our approach is that features which are common to similar words are also mostcharacteristic for their meanings, and thus should be promoted.
This idea is realized via abootstrapping step applied to an initial standard approximation of the similarity space.
Thesuperior performance of the bootstrapping method was assessed in two different experiments,one based on direct human gold-standard annotation and the other based on an automaticallycreated disambiguation dataset.
These results are further supported by applying a novel quanti-tative measurement of the quality of feature weighting functions.
Improved feature weightingalso allows massive feature reduction, which indicates that the most characteristic featuresfor a word are indeed concentrated at the top ranks of its vector.
Finally, experiments withthree prominent similarity measures and two feature weighting functions showed that thebootstrapping scheme is robust and is independent of the original functions over which it isapplied.1.
Introduction1.1 MotivationDistributional word similarity has long been an active research area (Hindle 1990; Ruge1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and?
Department of Information Science, Bar-Ilan University, Ramat-Gan, Israel.E-mail: zhitomim@mail.biu.ac.il.??
Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel.
E-mail: dagan@cs.biu.ac.il.Submission received: 6 December 2006; revised submission received: 9 July 2008; accepted for publication:21 November 2008.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 3Weir 2005).
This paradigm is inspired by Harris?s distributional hypothesis (Harris1968), which states that semantically similar words tend to appear in similar contexts.In a computational realization, each word is characterized by a weighted feature vector,where features typically correspond to other words that co-occur with the characterizedword in the context.
Distributional similarity measures quantify the degree of similaritybetween a pair of such feature vectors.
It is then assumed that two words that occurwithin similar contexts, as measured by similarity of their context vectors, are indeedsemantically similar.The distributional word similarity measures were often applied for two types ofinferences.
The first type is making similarity-based generalizations for smoothingword co-occurrence probabilities, in applications such as language modeling and dis-ambiguation.
For example, assume that we need to estimate the likelihood of the verb?object co-occurrence pair visit?country, although it did not appear in our sample corpus.Co-occurrences of the verb visit with words that are distributionally similar to country,such as state, city, and region, however, do appear in the corpus.
Consequently, wemay infer that visit?country is also a plausible expression, using some mathematicalscheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus,and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan,Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005).
The rationale behind thisinference is that if two words are distributionally similar then the occurrence of oneword in some contexts indicates that the other word is also likely to occur in suchcontexts.A second type of semantic inference, which primarily motivated our own research,is meaning-preserving lexical substitution.
Many NLP applications, such as questionanswering, information retrieval, information extraction, and (multi-document) sum-marization, need to recognize that one word can be substituted by another one in agiven context while preserving, or entailing the original meaning.
Naturally, recogniz-ing such substitutable lexical entailments is a prominent component within the textualentailment recognition paradigm, which models semantic inference as an application-independent task (Dagan, Glickman, and Magnini 2006).
Accordingly, several textualentailment systems did utilize the output of distributional similarity measures to modelentailing lexical substitutions (Jijkoun and de Rijke 2005; Adams 2006; Ferrandez et al2006; Nicholson, Stokes, and Baldwin 2006; Vanderwende, Menezes, and Snow 2006).In some of these papers the distributional information typically complements man-ual lexical resources in textual entailment systems, most notably WordNet (Fellbaum1998).Lexical substitution typically requires that the meaning of one word entailsthe meaning of the other.
For instance, in question answering, the word companyin a question can be substituted in an answer text by firm, automaker, or subsidiary,whose meanings entail the meaning of company.
However, as it turns out, traditionaldistributional similarity measures do not capture well such lexical substitutionrelationships, but rather capture a somewhat broader (and looser) notion of semanticsimilarity.
For example, quite distant co-hyponyms such as party and companyalso come out as distributionally similar to country, due to a partial overlap oftheir semantic properties.
Clearly, the meanings of these words do not entail eachother.Motivated by these observations, our long-term goal is to investigate whether thedistributional similarity scheme may be improved to yield tighter semantic similarities,and eventually better approximation of lexical entailments.
This article presents onecomponent of this research plan, which focuses on improving the underlying semantic436Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Qualityquality of distributional word feature vectors.
The article describes the methodology,definitions, and analysis of our investigation and the resulting bootstrapping schemefor feature weighting which yielded improved empirical performance.1.2 Main Contributions and OutlineAs a starting point for our investigation, an operational definition was needed forevaluating the correctness of candidate pairs of similar words.
Following the lexicalsubstitution motivation, in Section 3 we formulate the substitutable lexical entailmentrelation (or lexical entailment, for brevity), refining earlier definitions in Geffet andDagan (2004, 2005).
Generally speaking, this relation holds for a pair of words if apossible meaning of one word entails a meaning of the other, and the entailing word cansubstitute the entailed one in some typical contexts.
Lexical entailment overlaps partlywith traditional lexical semantic relationships, while capturing more generally thelexical substitution needs of applications.
Empirically, high inter-annotator agreementwas obtained when judging the output of distributional similarity measures for lexicalentailment.Next, we analyzed the typical behavior of existingword similaritymeasures relativeto the lexical entailment criterion.
Choosing the commonly used measure of Lin (1998)as a representative case, the analysis shows that quite noisy feature vectors are a majorcause for generating rather ?loose?
semantic similarities.
On the other hand, one mayexpect that features which seem to be most characteristic for a word?s meaning shouldreceive the highest feature weights.
This does not seem to be the case, however, forcommon feature weighting functions, such as Point-wise Mutual Information (Churchand Patrick 1990; Hindle 1990).Following these observations, we developed a bootstrapping formula that improvesthe original feature weights (Section 4), leading to better feature vectors and bettersimilarity predictions.
The general idea is to promote the weights of features that arecommon for semantically similar words, since these features are likely to be most char-acteristic for the word?s meaning.
This idea is implemented by a bootstrapping scheme,where the initial (and cruder) similarity measure provides an initial approximation forsemantic word similarity.
The bootstrapping method yields a high concentration ofsemantically characteristic features among the top-ranked features of the vector, whichalso allows aggressive feature reduction.The bootstrapping scheme was evaluated in two experimental settings, which cor-respond to the two types of applications for distributional similarity.
First, it achievedsignificant improvements in predicting lexical entailment as assessed by human judg-ments, when applied over several base similarity measures (Section 5).
Additionalanalysis relative to the lexical entailment dataset revealed cleaner and more charac-teristic feature vectors for the bootstrapping method.
To obtain a quantitative analysisof this behavior, we defined a measure called average common-feature rank ratio.This measure captures the idea that a prominent feature for a word is expected to beprominent also for semantically similar words, while being less prominent for unrelatedwords.
To the best of our knowledge this is the first proposedmeasure for direct analysisof the quality of feature weighting functions, without the need to employ them withinsome vector similarity measure.As a second evaluation, we applied the bootstrapping scheme for similarity-basedprediction of co-occurrence likelihood within a typical pseudo-word sense disambigua-tion experiment, obtaining substantial error reductions (Section 7).
Section 8 concludes437Computational Linguistics Volume 35, Number 3this article, suggesting the relevance of our analysis and bootstrapping scheme for thegeneral use of distributional feature vectors.12.
Background: Distributional Similarity ModelsThis section reviews the components of the distributional similarity approach andspecifies the measures and functions that were utilized by our work.The Distributional Hypothesis assumes that semantically similar words appear insimilar contexts, suggesting that semantic similarity can be detected by comparingcontexts of words.
This is the underlying principle of the vector-based distributionalsimilarity model, which comprises two phases.
First, context features for each word areconstructed and assigned weights; then, the weighted feature vectors of pairs of wordsare compared by a vector similarity measure.
The following two subsections reviewtypical methods for each phase.2.1 Features and Weighting FunctionsIn the typical computational setting, word contexts are represented by feature vectors.A feature represents another word (or term) w?
with which w co-occurs, and possiblyspecifies also the syntactic relationship between the twowords, as in Grefenstette (1994),Lin (1998), and Weeds and Weir (2005).
Thus, a word (or term) w is represented bya feature vector, where each entry in the vector corresponds to a feature f .
Pado andLapata (2007) demonstrate that using syntactic dependency-based features helps todistinguish among classes of lexical relations, which seems to be more difficult whenusing ?bag of words?
features that are based on co-occurrence in a text window.A syntactic-based feature f for a word w is defined as a triple:?
fw, syn rel, f role?where fw is a context word (or term) that co-occurs with w under the syntactic depen-dency relation syn rel.
The feature role ( f role) corresponds to the role of the feature wordfw in the syntactic dependency, being either the head (denoted h) or the modifier (de-noted m) of the relation.
For example, given the word company, the feature ?earnings, gen,h?
corresponds to the genitive relationship company?s earnings, and ?investor, pcomp of, m?corresponds to the prepositional complement relationship the company of the investor.2Throughout this article we use syntactic dependency relationships generated by theMinipar dependency parser (Lin 1993).
Table 1 lists common Minipar dependencyrelations involving nouns.
Minipar also identifies multi-word expressions, which is1 A preliminary version of the bootstrapping method was presented in Geffet and Dagan (2004).
Thatpaper presented initial results for the bootstrapping scheme, when applied only over Lin?s measure andtested by the manually judged dataset of lexical entailment.
The current research extends our initialresults in many respects.
It refines the definition of lexical entailment; utilizes a revised test set of largerscope and higher quality, annotated by three assessors; extends the experiments to two additionalsimilarity measures; provides comparative qualitative and quantitative analysis of the bootstrappedvectors, while employing our proposed average common-feature rank ratio; and presents an additionalevaluation based on a pseudo-WSD task.2 Following a common practice, we consider the relationship between a head noun (company in theexample) and the nominal complement of a modifying prepositional phrase (investor) as a single directdependency relationship.
The preposition itself is encoded in the dependency relation name, with adistinct relation for each preposition.438Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector QualityTable 1Common grammatical relations of Minipar involving nouns.Relation Descriptionappo appositioncomp1 first complementdet determinergen genitive markermod the relationship between a word and its adjunct modifierpnmod post nominal modifierpcomp nominal complement of prepositionspost post determinervrel passive verb modifier of nounsobj object of verbsobj2 second object of ditransitive verbssubj subject of verbss surface subjectadvantageous for detecting distributional similarity for such terms.
For example,Curran (2004) reports that multi-word expressions make up between 14?25% of thesynonyms in a gold-standard thesaurus.Thus, in our representation the corpus is first transformed to a set S of dependencyrelationship instances of the form ?w,f ?, where each pair corresponds to a single co-occurrence of w and f in the corpus.
f is termed as a feature of w. Then, a wordw is represented by a feature vector, where each entry in the vector corresponds toone feature f .
The value of the entry is determined by a feature weighting functionweight(w, f ), which quantifies the degree of statistical association between w and f in theset S. For example, some feature weighting functions are based on the logarithm of theword?feature co-occurrence frequency (Ruge 1992), or on the conditional probability ofthe feature given the word (Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999;Lee 1999).Probably the most widely used feature weighting function is (point-wise) MutualInformation (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch,Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovskiand Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004;Weeds, Weir, and McCarthy 2004), defined by:weightMI(w, f ) = log2P(w,f )P(w)P( f ) (1)We calculate the MI weights by the following statistics in the space of co-occurrenceinstances S:weightMI(w, f ) = log2count(w, f ) ?
nrelscount(w) ?
count( f )(2)where count(w, f ) is the frequency of the co-occurrence pair ?w,f ?
in S, count(w) andcount( f ) are the independent frequencies of w and f in S, and nrels is the size of S. HighMI weights are assumed to correspond to strong word?feature associations.439Computational Linguistics Volume 35, Number 3Curran and Moens (2002) argue that, generally, informative features are statis-tically correlated with their corresponding headword.
Thus, they suggest that anystatistical test used for collocations is a good starting point for improving feature-weight functions.
In their experiments the t-test-based metric yielded the best empiricalperformance.However, a known weakness of MI and most of the other statistical weightingfunctions used for collocation extraction, including t-test and ?2, is their tendency toinflate the weights for rare features (Dunning 1993).
In addition, a major property oflexical collocations is their ?non-substitutability?, as termed in Manning and Schutze(1999).
That is, typically neither a headword nor a modifier in the collocation can besubstituted by their synonyms or other related terms.
This implies that using modifierswithin strong collocations as features for a head word would provide a rather smallamount of common features for semantically similar words.
Hence, these functionsseem less suitable for learning broader substitutability relationships, such as lexicalentailment.Similarity measures that utilize MI weights showed good performance, however.In particular, a common practice is to filter out features by minimal frequency andweight thresholds.
Then, a word?s vector is constructed from the remaining (not filtered)features that are strongly associated with the word.
These features are denoted here asactive features.In the current work we use MI for data analysis, and for the evaluations of vectorquality and word similarity performance.2.2 Vector Similarity MeasuresOnce feature vectors have been constructed the similarity between two words is de-fined by some vector similarity measure.
Similarity measures which have been usedin the cited papers include weighted Jaccard (Grefenstette 1994), Cosine (Ruge 1992),and various information theoretic measures, as introduced and reviewed in Lee (1997,1999).
In the current work we experiment with the following three popular similaritymeasures.1.
The basic Jaccard measure compares the number of common features withthe overall number of features for a pair of words.
One of the weightedgeneralizations of this scheme to non-binary values replaces intersectionwith minimum weight, union with maximum weight, and set cardinalitywith summation.
This measure is commonly referred to as weighted Jaccard(WJ) (Grefenstette 1994; Dagan, Marcus, and Markovitch 1995; Dagan2000; Gasperin and Vieira 2004), defined as follows:simWJ(w, v) =?f?F(w)?F(v) min(weight(w,f ),weight(v,f ))?f?F(w)?F(v) max(weight(w,f ),weight(v,f ))(3)where F(w) and F(v) are the sets of active features of the two words wand v. The appealing property of this measure is that it considers theassociation weights rather than just the number of common features.2.
The standard Cosine measure (COS), which is popularly employed forinformation retrieval (Salton and McGill 1983) and also utilized for440Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Qualitylearning distributionally similar words (Ruge 1992; Caraballo 1999; Gauch,Wang, and Rachakonda 1999; Pantel and Ravichandran 2004), is defined asfollows:simCOS(w, v) =?f (weight(w,f )?weight(v,f ))?
?f (weight(w,f ))2??
?f (weight(v,f ))2 (4)This measure computes the cosine of the angle between the two featurevectors, which normalizes the vector lengths and thus avoids inflateddiscrimination between vectors of significantly different lengths.3.
A popular state of the art measure has been developed by Lin (1998),motivated by Information Theory principles.
This measure behaves quitesimilarly to the weighted Jaccard measure (Weeds, Weir, and McCarthy2004), and is defined as follows:simLIN(w, v) =?f?F(w)?F(v) (weightMI (w,f )+weightMI (v,f ))?f?F(w) weightMI (w,f )+?f?F(v) weightMI (v,f )(5)where F(w) and F(v) are the active features of the two words.
The weightfunction used originally by Lin is MI (Equation 1).It is interesting to note that a relatively recent work by Weeds and Weir (2005) inves-tigates a more generic similarity framework.
Within their framework, the similarity oftwo nouns is viewed as the ability to predict the distribution of one of them based onthat of the other.
Their proposed formula combines the precision and recall of a potential?retrieval?
of similar words based on the features of the target word.
The precision ofw?s prediction of v?s feature distribution indicates howmany of the features of the wordw co-occurred with the word v. The recall of w?s prediction of v?s features indicateshow many of the features of v co-occurred with w. Words with both high precisionand high recall can be obtained by computing their harmonic mean, mh (or F-score),and a weighted arithmetic mean.
However, after empirical tuning of weights for thearithmetic mean, Weeds and Weir?s formula practically reduces to Lin?s measure, aswas anticipated by their own analysis (in Section 4 of their paper).Consequently, we choose the Lin measure (Equation 5) (henceforth denoted as LIN)as representative for the state of the art and utilize it for data analysis and as a startingpoint for improvement.
To further explore and evaluate our new weighting scheme,independently of a single similarity measure, we conduct evaluations also with theother two similarity measures of weighted Jaccard and Cosine.3.
Substitutable Lexical EntailmentAs mentioned in the Introduction, the long term research goal which inspired our workis modeling meaning?entailing lexical substitution.
Motivated by this goal, weproposed in earlier work (Geffet and Dagan 2004, 2005) a new type of lexicalrelationship which aims to capture such lexical substitution needs.
Here we adopt thatapproach and formulate a refined definition for this relationship, termed substitutablelexical entailment.
In the context of the current article, utilizing a concrete target notionof word similarity enabled us to apply direct human judgment for the ?correctness?
(relative to the defined notion) of candidate word pairs suggested by distributionalsimilarity.
Utilizing these judgments we could analyze the behavior of alternative441Computational Linguistics Volume 35, Number 3distributional vector representations and, in particular, conduct error analysis for wordpair candidates that were judged negatively.The discussion in the Introduction suggested that multiple text understandingapplications need to identify term pairs whose meanings are both entailing and sub-stitutable.
Such pairs seem to be most appropriate for lexical substitution in a meaningpreserving scenario.
Tomodel this goal we present an operational definition for a lexicalsemantic relationship that integrates the two aspects of entailment and substitutability,3which is termed substitutable lexical entailment (or lexical entailment, for brevity).This relationship holds for a given directional pair of terms (w, v), saying that w entailsv, if the following two conditions are fulfilled:1.
Word meaning entailment: the meaning of a possible sense of w implies apossible sense of v;2.
Substitutability: w can substitute for v in some naturally occurring sentence,such that the meaning of the modified sentence would entail the meaningof the original one.To operationally assess the first condition (by annotators) we propose consideringthe meaning of terms by existential statements of the form ?there exists an instance ofthe meaning of the term w in some context?
(notice that, unlike propositions, it is notintuitive for annotators to assign truth values to terms).
For example, the word companywould correspond to the existential statement ?there exists an instance of the conceptcompany in some context.?
Thus, if in some context ?there is a company?
(in the senseof ?commercial organization?)
then necessarily ?there is a firm?
in that context (in thecorresponding sense).
Therefore, we conclude that the meaning of company implies themeaning of firm.
On the other hand, ?there is an organization?
does not necessarily implythe existence of company, since organization might stand for some non-profit association,as well.
Therefore, we conclude that organization does not entail company.To assess the second condition, the annotators need to identify some natural con-text in which the lexical substitution would satisfy entailment between the modifiedsentence and the original one.
Practically, in our experiments presented in Section 5 thehuman assessors could consult external lexical resources and the entireWeb to obtain allthe senses of the words and possible sentences for substitution.
We note that the task ofidentifying the common sense of two given words is quite easy since they mutually dis-ambiguate each other, and once the common sense is known it naturally helps finding acorresponding common context.
We note that this condition is important, in particular,in order to eliminate cases of anaphora and co-reference in contexts, where two wordsquite different in their meaning can sometimes appear in the same contexts only dueto the text pragmatics in a particular situation.
For example, in some situations workerand demonstrator could be used interchangeably in text, but clearly it is a discourse co-reference rather than common meaning that makes the substitution possible.
Instead,we are interested in identifying word pairs in which one word?s meaning providesa reference to the entailed word?s meaning.
This purpose is exactly captured by theexistential propositions of the first criterion above.3 The WordNet definition of the lexical entailment relation is specified only for verbs and, therefore, is notfelicitous for general purposes: A verb X entails Y if X cannot be done unless Y is, or has been, done (e.g.,snore and sleep).442Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector QualityAs reported further in Section 5.1, we observed that assessing these two conditionsfor candidate word similarity pairs was quite intuitive for annotators, and yielded goodcross-annotator agreement.
Overall, substitutable lexical entailment captures directlythe typical lexical substitution scenario in text understanding applications, as well as ingeneric textual entailment modeling.
In fact, this relation partially overlaps with severaltraditional lexical semantic relations that are known as relevant for lexical substitution,such as synonymy, hyponymy, and some cases of meronymy.
For example, we saythat the meaning of company is lexically entailed by the meaning of firm (synonym)or automaker (hyponym), while the word government entails minister (meronym) as Thegovernment voted for the new law entails A minister in the government voted for the new law.On the other hand, lexical entailment is not just a superset of other known relations,but it is rather designed to select those sub-cases of other lexical relations that are neededfor applied entailment inference.
For example, lexical entailment does not cover all casesof meronyms (e.g., division does not entail company), but only some sub-cases of part-whole relationship mentioned herein.
In addition, some other relations are also coveredby lexical entailment, like ocean and water and murder and death, which do not seem todirectly correspond to meronymy or hyponymy relations.Notice also that whereas lexical entailment is a directional relation that specifieswhich word of the pair entails the other, the relation may hold in both directionsfor a pair of words, as is the case for synonyms.
More detailed motivations for thesubstitutable lexical entailment relation and analysis of its relationship to traditionallexical semantic relations appear in Geffet (2006) and Geffet and Dagan (2004, 2005).4.
Bootstrapping Feature WeightsTo gain a better understanding of distributional similarity behavior we first analyzedthe output of the LIN measure, as a representative case for the state of the art, andregarding lexical entailment as a reference evaluation criterion.
We judge as correct,with respect to lexical entailment, those candidate pairs of the distributional similaritymethod for which entailment holds at least in one direction.For example, the word area is entailed by country, since the existence of countryentails the existence of area, and the sentence There is no rain in subtropical countries duringthe summer period entails the sentence There is no rain in subtropical areas during the summerperiod.
As another example, democracy is a type of country in the political sense, thus theexistence entailment holds and also the sentence Israel is a democracy in the Middle Eastentails Israel is a country in the Middle East.On the other hand, our analysis revealed that many candidate word similarity pairssuggested by distributional similarity measures do not correspond to ?tight?
semanticrelationships.
In particular, many word pairs suggested by the LIN measure do notsatisfy the lexical entailment relation, as demonstrated in Table 2.A deeper look at the corresponding word feature vectors reveals typical reasonsfor these lexical entailment prediction errors.
Most relevant for the scope of the cur-rent article, in many cases highly ranked features in a word vector (when sorting thefeatures by their weight) do not seem very characteristic for the word meaning.
Thisis demonstrated in Table 3, which shows the top 10 features in the vector for country.As can be seen, some of the top features are either too specific (landlocked, airspace),and are thus less reliable, or too general (destination, ambition), thus not indicative andmay co-occur with many different types of words.
On the other hand, intuitively morecharacteristic features of country, like population and governor, occur further down the443Computational Linguistics Volume 35, Number 3Table 2The top 20 most similar words for country (and their ranks) in the similarity list of LIN, followedby the next four words in the similarity list that were judged as entailing at least in one direction.nation 1 *city 7 economy 13 *company 19region 2 territory 8 *neighbor 14 *industry 20state 3 area 9 *sector 15 kingdom 30*world 4 *town 10 *member 16 place 35*island 5 republic 11 *party 17 colony 41*province 6 *north 12 government 18 democracy 82Twelve out of 20 top similarities (60%) were judged as mutually non-entailing and are markedwith an asterisk.
The similarity data was produced as described in Section 5.Table 3The top 10 ranked features for country produced by MI, the weighting function employed in theLIN method.Feature weightMICommercial bank, gen, h 8.08Destination, pcomp of, m 7.97Airspace, pcomp of, h 7.83Landlocked, mod, m 7.79Trade balance, gen, h 7.78Sovereignty, pcomp of, h 7.78Ambition, nn, h 7.77Bourse, gen, h 7.72Politician, gen, h 7.54Border, pcomp of, h 7.53sorted feature list, at positions 461 and 832.
Overall, features that seem to characterizethe word meaning well are scattered across the ranked feature list, while many non-indicative features receive high weights.
This behavior often yields high similarityscores for word pairs whose semantic similarity is rather loose while missing somemuch tighter similarities.Furthermore, we observed that characteristic features for a word w, which shouldreceive higher weights, are expected to be common for w and other words that aresemantically similar to it.
This observation suggests a computational scheme whichwould promote the weights of features that are common for semantically similar words.Of course, there is an inherent circularity in such a scheme: to determine which featuresshould receive high weights we need to know which words are semantically similar,while computing distributional semantic similarity already requires pre-determinedfeature weights.This kind of circularity can be approached by a bootstrapping scheme.
We firstcompute initial distributional similarity values, based on an initial feature weightingfunction.
Then, to learn more accurate feature weights for a word w, we promotefeatures that characterize other words that are initially known to be similar to w. Bythe same rationale, features that do not characterize many words that are sufficientlysimilar to w are demoted.
Even if such features happen to have a strong direct statis-tical association with w they would not be considered reliable, because they are notsupported by additional words that have a similar meaning to that of w.444Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality4.1 Bootstrapped Feature Weight DefinitionThe bootstrapped feature weight is defined as follows.
First, some standard wordsimilarity measure sim is computed to obtain an initial approximation of the similarityspace.
Then, we define the word set of a feature f , denoted by WS( f ), as the set of wordsfor which f is an active feature.
Recall from Section 2.2 that an active feature is a featurethat is strongly associated with the word, that is, its (initial) weight is higher than anempirically predefined threshold, ?weight.
The semantic neighborhood of w, denoted byN(w), is defined as the set of all words v which are considered sufficiently similar tow, satisfying sim(w, v) > ?sim, where ?sim is a second empirically determined threshold.The bootstrapped feature weight, denoted weightB, is then defined by:weightB(w, f ) =?v?WS( f )?N(w) sim(w, v) (6)That is, we identify all words v that are in the semantic neighborhood of w and are alsocharacterized by f , and then sum the values of their similarities to w.Intuitively, summing these similarity values captures simultaneously a desiredbalance between feature specificity and generality, addressing the observations in thebeginning of this section.
Some features might characterize just a single word that isvery similar to w, but then the sum of similarities will include a single element, yieldinga relatively low weight.
This is why the sum of similarities is used rather than anaverage value, which might become too high by chance when computed over just asingle element (or very few elements).
Relatively generic features, which occur withmany words and are thus less indicative, may characterize more words within N(w)but then on average the similarity values of these words with w is likely to be lower,contributing smaller values to the sum.
To receive a high overall weight a reliable featurehas to characterize multiple words that are highly similar to w.We note that the bootstrapped weight is a sum of word similarity values ratherthan a direct function of word?feature association values, which is the more commonapproach.
It thus does not depend on the exact statistical co-occurrence level betweenw and f .
Instead, it depends on a more global assessment of the association betweenf and the semantic vicinity of w. We notice that the bootstrapped weight is deter-mined separately relative to each individual word.
This differs from measures that areglobal word-independent functions of the feature, such as the feature entropy used inGrefenstette (1994) and the feature term strength relative to a predefined class as em-ployed in Pekar, Krkoska, and Staab (2004) for supervised word classification.4.2 Feature Reduction and Similarity Re-ComputationOnce the bootstrappedweights have been computed, their accuracy is sufficient to allowfor aggressive feature reduction.
As shown in the following section, in our experimentsit sufficed to use only the top 100 features for each word in order to obtain optimalword similarity results, because the most informative features now receive the highestweights.Finally, similarity betweenwords is re-computed over the reduced vectors using thesim function with weightB replacing the original feature weights.
The resulting similaritymeasure is further referred to as simB.445Computational Linguistics Volume 35, Number 35.
Evaluation by Lexical EntailmentTo test the effectiveness of the bootstrapped weighting scheme, we first evaluatedwhether it contributes to better prediction of lexical entailment.
This evaluation wasbased on gold-standard annotations determined by human judgments of the substi-tutable lexical entailment relation, as defined in Section 3.
The new similarity scheme,simB, based on the bootstrapped weights, was first computed using the standard LINmethod as the initial similarity measure.
The resulting similarity lists of simLIN (theoriginal LIN method) and simBLIN (Bootstrapped LIN) schemes were evaluated for a sam-ple of nouns (Section 5.2).
Then, the evaluation was extended (Section 5.3) to apply thebootstrapping scheme over the two additional similarity measures that were presentedin Section 2.2, simWJ (weighted Jaccard) and simCOS (Cosine).
Along with these lexicalentailment evaluations we also analyzed directly the quality of the bootstrapped fea-ture vectors, according to the average common-feature rank ratio measure, which wasdefined in Section 6.5.1 Experimental SettingOur experiments were conducted using statistics from an 18 million token subset of theReuters RCV1 corpus (known as Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19), parsed by Lin?s Minipar dependency parser (Lin 1993).The test set of candidate word similarity pairs was constructed for a sample of30 randomly selected nouns whose corpus frequency exceeds 500.
In our primary exper-iment we computed the top 40 most similar words for each noun by the simLIN and bysimBLIN measures, yielding 1,200 pairs for each method, and 2,400 pairs altogether.
About800 of these pairs were common for the two methods, therefore leaving approximately1,600 distinct candidate word similarity pairs.
Because the lexical entailment relation isdirectional, each candidate pair was duplicated to create two directional pairs, yieldinga test set of 3,200 pairs.
Thus, for each pair of words,w and v, the two ordered pairs (w, v)and (v,w) were created to be judged separately for entailment in the specified direction(whether the first word entails the other).
Consequently, a non-directional candidatesimilarity pair w, v is considered as a correct entailment if it was assessed as an entailingpair at least in one direction.The assessors were only provided with a list of word pairs without any contextualinformation and could consult any available dictionary, WordNet, and the Web.The judgment criterion follows the criterion presented in Section 3.
In particular,the judges were asked to apply the two operational conditions, existence and sub-stitutability in context, to each given pair.
Prior to performing the final test of theannotation experiment, the judges were presented with an annotated set of entailingand non-entailing pairs along with the existential statements and sample sentences forsubstitution, demonstrating how the two conditions could be applied in different casesof entailment.
In addition, they had to judge a training set of several dozen pairs andthen discuss their judgment decisions with each other to gain a better understanding ofthe two criteria.The following example illustrates this process.
Given a non-directional pair{company, organization} two directional pairs are created: (company, organization) and(organization, company).
The former pair is judged as a correct entailment: the existenceof a company entails the existence of an organization, and the meaning of the sentence:John works for a large company entails the meaning of the sentence with substitution:John works for a large organization.
Hence, company lexically entails organization, but not446Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Qualityvice versa (as shown in Section 3.3), therefore the second pair is judged as not entailing.Eventually, the non-directional pair {company, organization} is considered as a correctentailment.Finally, the test set of 3,200 pairs was split into three disjoint subsets that werejudged by three native English speaking assessors, each of whom possessed a Bach-elors degree in English Linguistics.
For each subset a different pair of assessors wasassigned, each person judging the entire subset.
The judges were grouped into threedifferent pairs (i.e., JudgeI+JudgeII, JudgeII+JudgeIII, and JudgeI+JudgeIII).
Each pairwas assigned initially to judge all the word similarities in each subset, and the thirdassessor was employed in cases of disagreement between the first two.
The majorityvote was taken as the final decision.
Hence, each assessor had to fully annotate twothirds of the data and for a third subset she only had to judge the pairs for which therewas disagreement between the other two judges.
This was done in order to measure theagreement achieved for different pairs of annotators.The output pairs from bothmethodsweremixed so the assessors could not associatea pair with the method that proposed it.
We note that this evaluation methodology,in which human assessors judge the correctness of candidate pairs by some semanticsubstitutability criterion, is similar to common evaluation methodologies used for para-phrase acquisition (Barzilay and McKeown 2001; Lin and Pantel 2001; Szpektor et al2004).Measuring human agreement level for this task, the proportions of matching de-cisions were 93.5% between Judge I and Judge II, 90% for Judge I and Judge III, and91.2% for Judge II and Judge III.
The corresponding kappa values are 0.83, 0.80, and 0.80,which is regarded as ?very good agreement?
(Landis and Koch 1997).
It is interestingto note that after some discussion most of the disagreements were settled, and the fewremaining mismatches were due to different understandings of word meanings.
Thesefindings seem to have a similar flavor to the human agreement findings reported for theRecognizing Textual Entailment challenges (Bar-Haim et al 2006; Dagan, Glickman, andMagnini 2006), in which entailment was judged for pairs of sentences.
In fact, the kappavalues obtained in our evaluation are substantially higher than reported for sentence-level textual entailment, which suggests that it is easier to make entailment judgmentsat the lexical level than at the full sentence level.The parameter values of the algorithms were tuned using a development set ofsimilarity pairs generated for 10 additional nouns, distinct from the 30 nouns used forthe test set.
The parameters were optimized by running the algorithm systematicallywith various values across the parameter scales and judging a sample subset of theresults.
weightMI = 4 was found as the optimal MI threshold for active feature weights(features included in the feature vectors), yielding a 10% precision increase of simLIN andremoving over 50% of the data relative to no feature filtering.
Accordingly, this valuealso serves as the ?weight threshold in the bootstrapping scheme (Section 4).
As for the?sim parameter, the best results on the development set were obtained for ?sim = 0.04,?sim = 0.02, and ?sim = 0.01 when bootstrapping over the initial similarity measuresLIN, WJ, and COS, respectively.5.2 Evaluation Results for simBLINWe measured the contribution of the improved feature vectors to the resulting preci-sion of simLIN and simBLIN in predicting lexical entailment.
The results are presented inTable 4, where precision and error reduction values were computed for the top 20,30, and 40 word similarity pairs produced by each method.
It can be seen that the447Computational Linguistics Volume 35, Number 3Table 4Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and theoriginal LIN method.Top-n Correct Error RateWords Entailments (%) Reduction (%)simLIN simBLINTop 20 52.0 57.9 12.3Top 30 48.2 56.2 15.4Top 40 41.0 49.7 14.7Bootstrapped LIN method outperformed the original LIN approach by 6?9 precisionpoints at all top-n levels.
As expected, the precision for the shorter top 20 list is higherfor both methods, thus leaving a bit less room for improvement.Overall, the Bootstrapped LIN method extracted 104 (21%) more correct similaritypairs than the other measure and reduced the number of errors by almost 15%.
We alsocomputed the relative recall, which shows the percentage of correct word similaritiesfound by each method relative to the joint set of similarities that were extracted byboth methods.
The overall relative recall of the Bootstrapped LIN was quite high (94%),exceeding LIN?s relative recall (of 78%) by 16 percentage points.
We found that thebootstrapped method covers over 90% of the correct similarities learned by the originalmethod, while also identifying many additional correct pairs.It should be noted at this point that the current limited precision levels are deter-mined not just by the quality of the feature vectors but significantly by the nature of thevector comparison measure itself (i.e., the LIN formula, as well weighted Jaccard andCosine as reported in Section 5.3).
It was observed in other work (Geffet and Dagan2005) that these common types of vector comparison schemes exhibit certain flawsin predicting lexical entailment.
Our present work thus shows that the bootstrappingmethod yields a significant improvement in feature vector quality, but future researchis needed to investigate improved vector comparison schemes.An additional indication of the improved vector quality is the massive featurereduction allowed by having the most characteristic features concentrated at the topranks of the vectors.
The vectors of active features of LIN, as constructed after standardfeature filtering (Section 5.1), could be further reduced by the bootstrapped weightingto about one third of their size.
As illustrated in Figure 1, changing the vector sizesignificantly affects the similarity results.
In simBLIN the best result was obtained withthe top 100 features per word, while using less than 100 or more than 150 featurescaused a 5?10% decrease in performance.
On the other hand, an attempt to cut off thelower ranked features of the MI weighting always resulted in a noticeable decrease inprecision.
These results show that for MI weighting many important features appearfurther down in the ranked vectors, while for the bootstrapped weighting adding toomany features adds mostly noise, since most characteristic features are concentratedat the top ranks.
Thus, in addition to better feature weighting, the bootstrapping stepprovides effective feature reduction, which improves vector quality and consequentlythe similarity results.We note that the optimal vector size we obtained conforms to previous results?for example, by Widdows (2003), Curran (2004), and Curran and Moens (2002)?whoalso used reduced vectors of up to 100 features as optimal for learning hyponymy andsynonymy, respectively.
In Widdows the known SVD method for dimension reduction448Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector QualityFigure 1Percentage of correct entailments within the top 40 candidate pairs of each of the methods,LIN and Bootstrapped LIN (denoted as LINB in the figure), when using varying numbers oftop-ranked features in the feature vector.
The value of ?All?
corresponds to the full size ofvectors and is typically in the range of 300?400 features.of LSA-based vectors is applied, whereas in Curran, and Curran and Moens, onlythe strongly associated verbs (direct and indirect objects of the noun) are selected as?canonical features?
that are expected to be shared by true synonyms.Finally, we tried executing an additional bootstrapping iteration of weightB calcula-tion over the similarity results of simBLIN.
The resulting increase in precision was muchsmaller, of about 2%, showing that most of the potential benefit is exploited in thefirst bootstrapping iteration (which is not uncommon for natural language data).
Onthe other hand, computing the bootstrapping weight twice increases computation timesignificantly, which led us to suggest a single bootstrapping iteration as a reasonablecost-effectiveness tradeoff for our data.5.3 Evaluation for simBWJ and simBCOSTo further validate the behavior of the bootstrapping schemewe experimentedwith twoadditional similaritymeasures, weighted Jaccard (simWJ) and Cosine (simCOS) (describedin Section 2.2).
For each of the additional measures the experiment repeats the mainthree steps described in Section 4: Initially, the basic similarity lists are calculated foreach of the measures using MI weighting; then, the bootstrapped weighting, weightB, iscomputed based on the initial similarities, yielding new word feature vectors; finally,the similarity values are recomputed by the same vector similarity measure using thenew feature vectors.To assess the effectiveness of weightB we computed the four alternative outputsimilarity lists, using the simWJ and simCOS similarity measures, each with the weightMI449Computational Linguistics Volume 35, Number 3Table 5Comparative precision values for the top 20 similarity lists of the three selected similaritymeasures, with MI and Bootstrapped feature weighting for each.Measure LIN?LINB WJ?WJB COS?COSBCorrect Similarities (%) 52.0?57.9 51.0?54.8 46.1?50.9and weightB weighting functions.
The four lists were judged for lexical entailment bythree assessors, according to the same procedure described in Section 5.1.
To make theadditional manual evaluation affordable we judged the top 20 similar words in each listfor each of the 30 target nouns of Section 5.1.Table 5 summarizes the precision values achieved by LIN, WJ, and COS withboth weightMI and weightB.
As shown in the table, bootstrapped weighting consistentlycontributed between 4?6 points to the accuracy of each method in the top 20 similaritylist.
We view the results as quite positive, considering that improving over top 20similarities is a much more challenging task than improving over longer similarity lists,while the improvement was achieved only by modifying the feature vectors withoutchanging the similarity measure itself (as hinted in Section 5.2).
Our results are alsocompatible with previous findings in the literature (Dagan, Lee, and Pereira 1999;Weeds, Weir, and McCarthy 2004) that found LIN and WJ to be more accurate forsimilarity acquisition than COS.
Overall, the results demonstrate that the bootstrappedweighting scheme consistently produces improved results.An interesting behavior of the bootstrapping process is that the most prominentfeatures for a given target word converge across the different initial similarity measures,as exemplified in Table 6.
In particular, although the initial similarity lists overlap onlypartly,4 the overlap of the top 30 features for our 30-word sample was ranging between88% and 100%.
This provides additional evidence that the quality of the bootstrappedweighting is quite similar for various initial similarity measures.6.
Analyzing the Bootstrapped Feature Vector QualityIn this section we provide an in-depth analysis of the bootstrapping feature weightingquality compared to the state-of-the-art MI weighting function.6.1 Qualitative ObservationsThe problematic feature ranking noticed at the beginning of Section 4 can be revealedmore objectively by examining the common features which contribute most to the wordsimilarity scores.
To that end, we examine the common features of the two given wordsand sort them by the sum of their weights in both word vectors.
Table 7 shows the top10 common features by this sorting for a pair of truly similar (lexically entailing) words(country?state), and for a pair of non-entailing words (country?party).
For each commonfeature the table shows its two corresponding ranks in the feature vectors of the twowords.4 Overlap rate was about 40% between COS and WJ or LIN, and 70% between WJ and LIN.
The overlapwas computed following the procedure of Weeds, Weir, and McCarthy (2004), disregarding the order ofthe similar words in the lists.
Interestingly, they obtained roughly similar figures, of 28% overlap for COSand WJ, 32% overlap for COS and LIN, and 81% overlap between LIN and WJ.450Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector QualityTable 6Top 30 features of town by bootstrapped weighting based on LIN, WJ, and COS as initialsimilarities.
The three sets of words are almost identical, with relatively minor rankingdifferences.LINB WJB COSBsouthern southern northernnorthern northern southernoffice office remoteeastern official easternremote coastal officialofficial eastern basedtroop northeastern northeasternnortheastern remote officepeople troop coastalcoastal people northwesternattack based peoplebased populated attackpopulated attack troopnorthwestern home homebase northwestern southhome south westernsouth western citywest west populatedwestern resident baseneighboring neighboring residentresident house northplant city westpolice base neighboringheld trip triplocate camp surroundingtrip held policecity north heldsite locate locatecamp surrounding housesurrounding police campIt can be observed in Table 7 that for both word pairs the common features arescattered across the pair of feature vectors, making it difficult to distinguish betweenthe truly similar and the non-similar pairs.
We suggest, on the other hand, that thedesired behavior of effective feature weighting is that the common features of trulysimilar words would be concentrated at the top ranks of both word vectors.
In otherwords, if the two words are semantically similar then we expect them to share theirmost characteristic features, which are in turn expected to appear at the higher ranksof each feature vector.
The common features for non-similar words are expected to bescattered all across each of the vectors.
In fact, these expectations correspond exactly tothe rationale behind distributional similarity measures: Such measures are designed toassign higher similarity scores for vector pairs that share highly weighted features.Comparatively, we illustrate the behavior of the Bootstrapped LIN method relative tothe observations regarding the original LIN method, using the same running example.Table 8 shows the top 10 features of country.
We observe that the list now containsfeatures that are intuitively quite indicative and reliable, while many too specific oridiomatic features, and too general ones, were demoted (compare with Table 3).
Table 9shows that most of the top 10 common features for country?state are now ranked highly451Computational Linguistics Volume 35, Number 3for both words.
On the other hand, there are only two common features (among thetop 100 features) for the incorrect pair country?party, bothwith quite low ranks (comparewith Table 7), while the rest of the common features for this pair did not pass the top 100cutoff.Consequently, Table 10 demonstrates a much more accurate similarity list for coun-try, wheremany incorrect (non-entailing) word similarities, like party and company, weredemoted.
Instead, additional correct similarities, like kingdom and land, were promoted(compare with Table 2).
In this particular case all the remaining errors correspond towords that are related quite closely to country, denoting geographic concepts.
Many ofthese errors are context dependent entailments which might be substitutable in somecases, but they violate the word meaning entailment condition (e.g., country?neighbor,country?port).
Apparently, these words tend to occur in contexts that are typical forcountry in the Reuters corpus.
Some errors violating the substitutability condition oflexical entailment were identified as well, such as industry?product.
These cases arequite hard to differentiate from correct entailments, since the two words are usuallyclosely related to each other and also share highly ranked features, because they oftenappear in similar characteristic contexts.
It may therefore be difficult to filter out suchTable 7LIN (MI) weighting: The top 10 common features for country?state and country?party, along withtheir corresponding ranks in each of the two feature vectors.
The features are sorted by the sumof their feature weights with both words.Country?State Ranks Country?Party RanksBroadcast, pcomp in, h 24 50 Brass, nn, h 64 22Goods, mod, h 140 16 Concluding, pcomp of, h 73 20Civil servant, gen, h 64 54 Representation, pcomp of, h 82 27Bloc, gen, h 30 77 Patriarch, pcomp of, h 128 28Nonaligned, mod, m 55 60 Friendly, mod, m 58 83Neighboring, mod, m 15 165 Expel, pcomp from, h 59 30Statistic, pcomp on, h 165 43 Heartland, pcomp of, h 102 23Border, pcomp of, h 10 247 Surprising, pcomp of, h 114 38Northwest, mod, h 41 174 Issue, pcomp between, h 103 51Trip, pcomp to, h 105 34 Contravention, pcomp in, m 129 43Table 8Top 10 features of country by the Bootstrapped feature weighting.Feature WeightBIndustry, gen, h 1.21Airport, gen, h 1.16Visit, pcomp to, h 1.06Neighboring, mod, m 1.04Law, gen, h 1.02Economy, gen, h 1.02Population, gen, h 0.93Stock market, gen, h 0.92Governor, pcomp of, h 0.92Parliament, gen, h 0.91452Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector QualityTable 9Bootstrapped weighting: top 10 common features for country?state and country?party along withtheir corresponding ranks in the two (sorted) feature vectors.Country?State Ranks Country?Party RanksNeighboring, mod, m 3 1 Relation, pcomp with, h 12 26Industry, gen, h 1 11 Minister, pcomp from, h 77 49Impoverished, mod, m 8 8Governor, pcomp of, h 10 9Population, gen, h 6 16City, gen, h 17 18Economy, gen, h 5 15Parliament, gen, h 10 22Citizen, pcomp of, h 14 25Law, gen, h 4 33Table 10Top 20 most similar words for country and their ranks in the similarity list by the BootstrappedLIN measure.nation 1 territory 6 *province 11 zone 16state 2 *neighbor 7 *city 12 land 17*island 3 colony 8 *town 13 place 18region 4 *port 9 kingdom 14 economy 19area 5 republic 10 *district 15 *world 20Note that four of the incorrect similarities from Table 2 were replaced with correct entailmentsresulting in a 20% increase of precision (reaching 60%).non-substitutable similarities merely by the standard distributional similarity scheme,suggesting that additional mechanisms and data types would be required.6.2 The Average Common-Feature Rank RatioIt should be noted at this point that these observations regarding feature weight be-havior are based on subjective intuition of how characteristic features are for a wordmeaning, which is quite difficult to assess systematically.
Therefore, we next propose aquantitative measure for analyzing the quality of feature vector weights.More formally, given a pair of feature vectors for words w and v we first definetheir average common-feature rankwith respect to the top-n common features, denotedacfrn, as follows:acfrn(w, v) =1n?f?top?n(F(w)?F(v))12 [rank(w, f )+ rank(v, f )](7)where rank(w, f ) is the rank of feature f in the vector of the word w when features aresorted by their weight, and F(w) is the set of features in w?s vector.
top-n is the set oftop n common features to consider, where common features are sorted by the sum oftheir weights in the two word vectors (the same sorting as in Table 7).
In other words,acfrn(w, v) is the average rank in the two feature vectors of their top n common features.453Computational Linguistics Volume 35, Number 3Using this measure, we expect that a good feature weighting function wouldtypically yield lower values of acfrn for truly similar words (as low ranking valuescorrespond to higher positions in the vectors) than for non-similar words.
Hence, givena pre-judged test set of pairs of similar and non-similar words, we define the ratio,acfr-ratio, between the average acfrn of the set of all the non-similar words, denoted asNon-Sim, and the average acfrn of the set of all the known pairs of similar words, Sim, tobe an objective measure for feature weighting quality, as follows:acfrn ?
ratio =1|Non?Sim|?w,v?Non?Sim acfrn(w,v)1|Sim|?w,v?Sim acfrn(w,v)(8)As an illustration, the two word pairs in Table 7 yielded acfr10(country, state) = 78and acfr10(country, party) = 64.
Both values are quite high, showing no principal differ-ence between the tighter lexically entailing similarity versus a pair of non-similar (orrather loosely related) words.
This behavior indicates the deficiency of the MI featureweighting function in this case.
On the other hand, the corresponding values for thetwo pairs produced by the Bootstrapped LIN method (for the features in Table 9) areacfr10(country, state) = 12 and acfr10(country, party) = 41.
These figures clearly reflect thedesired distinction between similar and non-similar words, showing that the commonfeatures of the similar words are indeed concentrated at much higher ranks in thevectors than the common features of the non-similar words.In recent work on distributional similarity (Curran 2004; Weeds and Weir 2005) avariety of alternative weighting functions were compared.
However, the quality of theseweighting functions was evaluated only through their impact on the performance ofa particular word similarity measure, as we did in Section 5.
Our acfr-ratio measureprovides the first attempt to analyze the quality of weighting functions directly, relativeto a pre-judged word similarity set, without reference to a concrete similarity measure.6.3 An Empirical Assessment of the acfr-ratioIn this subsection we report an empirical comparison of the acfr-ratio obtained for theMIand BootstrappedLIN weighting functions.
To that end, we have run the Minipar systemon the full Reuters RCV1 corpus, which contains 2.5 GB of English news stories, andthen calculated theMI-weighted feature vectors.
The optimized threshold on the featureweights, ?weight, was set to 0.2.
Further, to compute the Bootstrapped LIN feature weightsa ?sim of 0.02 was applied to the LIN similarity values.
In this experiment we employedthe full bootstrapped vectors (i.e., without applying feature reduction by the top 100cutoff).
This was done to avoid the effect of the feature vector size on the acfrn metric,which tends to naturally assign higher scores to shorter vectors.As computing the acfr-ratio requires a pre-judged sample of candidate word simi-larity pairs, we utilized the annotated test sample of candidate pairs of word similaritiesdescribed in Section 5, which contains both entailing and non-entailing pairs.First, we computed the average common-feature rank scores (acfrn) (with varyingvalues of n) forweightMI and forweightB over all the pairs in the test sample.
Interestingly,the mean acfrn scores for weightB range within 110?264 for n = 10. .
.
100, while thecorresponding range for weightMI is by an order of magnitude higher: 780?1,254, despitethe insignificant differences in vector sizes.
Therefore, we conclude that the commonfeatures that are relevant to establishing distributional similarity in general (regardlessof entailment) are much more scattered across the vectors by MI weighting, while withbootstrapping they tend to appear at higher positions in the vectors.
These figures454Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Qualityreflect a desired behavior of the bootstrapping function which concentrates most of theprominent common features for all the distributionally similar words (whether entailingor not) at the lower ranks of their vectors.
In particular, this explains the ability of ourmethod to perform a massive feature reduction as demonstrated in Section 5, and toproduce more informative vectors, while demoting and eliminating much of the noisein the original vectors.Next, we aim to measure the discriminative power of the compared methods todistinguish between entailing and non-entailing pairs.
To this end we calculated theacfr-ratio, which captures the difference in the average common feature ranks betweenentailing vs. non-entailing pairs, for both the MI-based and bootstrapped vectors.The obtained results are presented in Figure 2.
As can be seen the acfr-ratio valuesare consistently higher for Bootstrapped LIN than for MI.
That is, the bootstrappingmethod assigns much higher acfrn scores to entailing words than to non-entailing ones,whereas for MI the corresponding acfrn scores for entailing and non-entailing pairs areroughly equal.
In particular, we notice that the largest gaps in acfr-ratio occur for lowernumbers of top common features, whose weights are indeed the most important andinfluential in distributional similarity measures.
Thus, these findings suggest a directindication of an improved quality of the bootstrapped feature vectors.7.
A Pseudo-Word Sense Disambiguation EvaluationThe lexical entailment evaluation reported herein corresponds to the lexical substitutionapplication of distributional similarity.
The other type of application, as reviewed in theIntroduction, is similarity-based prediction of word co-occurrence likelihood, neededfor disambiguation applications.
Comparative evaluations of distributional similaritymethods for this type of application were commonly conducted using a pseudo-wordsense disambiguation scheme, which is replicated here.
In the next subsections we firstdescribe how distributional similarity can help improve word sense disambiguation(WSD).
Then we describe how the pseudo-word sense disambiguation task, whichFigure 2Comparison between the acfr-ratio for MI and Bootstrapped LIN methods, when using varyingnumbers of common top-ranked features in the words?
feature vectors.455Computational Linguistics Volume 35, Number 3corresponds to the general WSD setting, was used to evaluate the co-occurrence like-lihood predictions obtained by alternative similarity methods.7.1 Similarity Modeling for Word Sense DisambiguationWSD methods need to identify the correct sense of an ambiguous word in a givencontext.
For example, a test instance for the verb save might be presented in the con-text saving Private Ryan.
The disambiguation method must decide whether save in thisparticular context means rescue, preserve, keep, lay aside, or some other alternative.Sense recognition is typically based on context features collected from a sense-annotated training corpus.
For example, the system might learn from the annotatedtraining data that the word soldier is a typical object for the rescuing sense of save, as in:They saved the soldier.
In this setting, distributional similarity is used to reduce the datasparseness problem via similarity-based generalization.
The general idea is to predictthe likelihood of unobserved word co-occurrences based on observed co-occurrencesof distributionally similar words.
For example, assume that the noun private did notoccur as a direct object of save in the training data.
Yet, some of the words that aredistributionally similar to private, like soldier or sergeant, might have occurred with save.Thus, a WSD system may infer that the co-occurrence save private is more likely for therescuing sense of save because private is distributionally similar to soldier, which did co-occur with this sense of save in the annotated training corpus.
In general terms, theWSDmethod estimates the co-occurrence likelihood for the target sense and a given contextword based on training data for words that are distributionally similar to the contextword.This idea of similarity-based estimation of co-occurrence likelihood was appliedin Dagan, Marcus, and Markovitch (1995) to enhance WSD performance in machinetranslation and recently in Gliozzo, Giuliano, and Strapparava (2005), who employed aLatent Semantic Analysis (LSA)-based kernel function as a similarity-based represen-tation for WSD.
Other works employed the same idea for pseudo-word sense dis-ambiguation, as explained in the next subsection.7.2 The Pseudo-Word Sense Disambiguation SettingSense disambiguation typically requires annotated training data, created with consid-erable human effort.
Yarowsky (1992) suggested that when using WSD as a test bedfor comparative algorithmic evaluation it is possible to set up a pseudo-word sensedisambiguation scheme.
This scheme was later adopted in several experiments, andwas popular for comparative evaluations of similarity-based co-occurrence likelihoodestimation (Dagan, Lee, and Pereira 1999; Lee 1999; Weeds andWeir 2005).
We followedclosely the same experimental scheme, as described subsequently.First, a list of pseudo-words is constructed by ?merging?
pairs of words into a singlepseudo word.
In our experiment each pseudo-word constitutes a pair of randomlychosen verbs, (v, v?
), where each verb represents an alternative ?sense?
of the pseudo-word.
The two verbs are chosen to have almost identical probability of occurrence,which avoids a word frequency bias on the co-occurrence likelihood predictions.Next, we consider occurrences of pairs of the form ?n, (v, v?)?
, where (v, v?)
is apseudo-word and n is a noun representing the object of the pseudo-word.
Such pairsare constructed from all co-occurrences of either v or v?
with the object n in the corpus.For example, given the pseudo-word (rescue, keep) and the verb?object co-occurrence inthe corpus rescue?private we construct the pair ?private, (rescue, keep)?.
Given such a test456Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Qualitypair, the disambiguation task is to decide which of the two verbs is more likely to co-occur with the given object noun, aiming to recover the original verb from which thispair was constructed.
In this example we would like to predict that rescue is more likelyto co-occur with private as an object than keep.In our experiment 80% of the constructed pairs were used for training, providingthe co-occurrence statistics for the original known verb in each pair (i.e., either ?n, v?or ?n, v??).
From the remaining 20% of the pairs those occurring in the training corpuswere discarded, leaving as a test set only pairs which do not appear in the training part.Thus, predicting the co-occurrence likelihood of the noun with each of the two verbscannot rely on direct frequency estimation for the co-occurrences, but rather only onsimilarity-based information.To make the similarity-based predictions we first compute the distributional sim-ilarity scores for all pairs of nouns based on the training set statistics, where the co-occurring verbs serve as the features in the distributional vectors of the nouns.
Then,given a test pair ?(v,v?
), n?
our task is to predict which of the two verbs is more likelyto co-occur with n. This verb is thus predicted as being the original verb from whichthe pair was constructed.
To this end, the noun n is substituted in turn with each of itsk distributionally most similar nouns, ni, and then both of the obtained ?similar?
pairs?ni, v?
and ?ni, v??
are sought in the training set.Next, wewould like to predict that the more likely co-occurrence between ?n, v?
and?n, v??
is the one for which more pairs of similar words were found in the training set.Several approaches were used in the literature to quantify this decision procedure andwe have followed the most recent one from Weeds and Weir (2005).
Each similar nounni is given a vote, which is equal to the difference between the frequencies of the twoco-occurrences (ni, v) and (ni, v?
), and which it casts to the verb with which it co-occursmore frequently.
The votes for each of the two verbs are summed over all k similarnouns ni and the one with most votes wins.
The winning verb is considered correct if itis indeed the original verb from which the pair was constructed, and a tie is recordedif the votes for both verbs are equal.
Finally, the overall performance of the predictionmethod is calculated by its error rate:error = 1T(#of incorrect choices+#of ties2) (9)where T is the number of test instances.In the experiment, we used the 1,000 most frequent nouns in our subset of theReuters corpus (of Section 5.1).
The training and test data were created as describedherein, using the Minipar parser (Lin 1993) to produce verb?object co-occurrence pairs.The k = 40 most similar nouns for each test noun were computed by each of the threeexamined similarity measures LIN, WJ, and COS (as in Section 5), with and withoutbootstrapping.
The six similarity lists were utilized in turn for the pseudo-word sensedisambiguation task, calculating the corresponding error rate.7.3 ResultsTable 11 shows the error rate improvements after applying the bootstrapped weightingfor each of the three similarity measures.
The largest error reduction, by over 15%, wasobtained for the LIN method, with quite similar results for WJ.
This result is better thanthe one reported by Weeds and Weir (2005), who achieved about 6% error reductioncompared to LIN.457Computational Linguistics Volume 35, Number 3Table 11The comparative error rates of the pseudo-disambiguation task for the three examined similaritymeasures, with and without applying the bootstrapped weighting for each of them.Measure LIN?LINB WJ?WJB COS?COSBError rate 0.157?0.133 0.150?0.132 0.155?0.145This experiment shows that learning tighter semantic similarities, based on the im-proved bootstrapped feature vectors, correlates also with better similarity-based infer-ence for co-occurrence likelihood prediction.
Furthermore, we have seen once again thatthe bootstrapping scheme does not depend on a specific similarity measure, reducingthe error rates for all three measures.8.
ConclusionsThe primary contribution of this article is the proposal of a bootstrapping methodthat substantially improves the quality of distributional feature vectors, as needed forstatistical word similarity.
The main idea is that features which are common for similarwords are also most characteristic for their meanings and thus should be promoted.
Infact, beyond its intuitive appeal, this idea corresponds to the underlying rationale ofthe distributional similarity scheme: Semantically similar words are expected to shareexactly those context features which are most characteristic for their meaning.The superior empirical performance of the resulting vectors was assessed in thecontext of the two primary applications of distributional word similarity.
The first islexical substitution, which was represented in our work by a human gold standardfor the substitutable lexical entailment relation.
The second is co-occurrence likelihoodprediction, which was assessed by the automatically computed scores of the commonpseudo-word sense disambiguation evaluation.
An additional outcome of the improvedfeature weighting is massive feature reduction.Experimenting with three prominent similarity measures showed that the boot-strapping scheme is robust and performs well when applied over different measures.Notably, our experiments show that the underlying assumption behind the boot-strapping scheme is valid, that is, available similarity metrics do provide a reason-able approximation of the semantic similarity space which can be then exploited viabootstrapping.The methodology of our investigation has yielded several additional contributions:1.
Utilizing a refined definition of substitutable lexical entailment both as anend goal and as an analysis vehicle for distributional similarity.
It wasshown that the refined definition can be judged directly by human subjectswith very good agreement.
Overall, lexical entailment is suggested as auseful model for lexical substitution needs in semantic-orientedapplications.2.
A thorough error analysis of state of the art distributional similarityperformance was conducted.
The main observation was deficient qualityof the feature vectors, which reduces the eventual quality of similaritymeasures.458Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality3.
Inspired by the qualitative analysis, we proposed a new analytic measurefor feature vector quality, namely average common-feature rank ratio(acfr-ratio), which is based on the common ranks of the features for pairs ofwords.
This measure estimates the ability of a feature weighting method todistinguish between pairs of similar vs. non-similar words.
To the best ofour knowledge this is the first proposed measure for direct analysis of thequality of feature weighting functions, without the need to employ themwithin some vector similarity measure.The ability to identify the most characteristic features of words can have additional ben-efits, beyond their impact on traditional word similarity measures (as evaluated in thisarticle).
A demonstration of such potential appears in Geffet and Dagan (2005), whichpresents a novel feature inclusion scheme for vector comparison.
That scheme utilizesour bootstrapping method to identify the most characteristic features of a word andthen tests whether these particular features co-occur also with a hypothesized entailedword.
The empirical success reported in that paper provides additional evidence for theutility of the bootstrapping method.More generally, our motivation and methodology can be extended in several di-rections by future work on acquiring lexical entailment or other lexical-semantic rela-tions.
One direction is to explore better vector comparison methods that will utilizethe improved feature weighting, as shown in Geffet and Dagan (2005).
Another direc-tion is to integrate distributional similarity and pattern-based acquisition approaches,which were shown to provide largely complementary information (Mirkin, Dagan, andGeffet 2006).
An additional potential is to integrate automatically acquired relationshipswith the information found in WordNet, which seems to suffer from several seriouslimitations (Curran 2005), and typically overlaps to a rather limited extent with theoutput of automatic acquisition methods.
As a parallel direction, future research shouldexplore in detail the impact of different lexical-semantic acquisition methods on textunderstanding applications.Finally, our proposed bootstrapping scheme seems to have a general appeal forimproving feature vector quality in additional unsupervised settings.
We thus hope thatthis idea will be explored further in other NLP and machine learning contexts.ReferencesAdams, Rod.
2006.
Textual entailmentthrough extended lexical overlap.
InProceedings of the Second PASCAL ChallengesWorkshop on Recognising Textual Entailment,pages 68?73, Venice.Bar-Haim, Roy, Ido Dagan, Bill Dolan,Lisa Ferro, Danilo Giampiccolo, BernardoMagnini, and Idan Szpektor.
2006.
Thesecond PASCAL recognising textualentailment challenge.
In Proceedings of theSecond PASCAL Challenges Workshop onRecognising Textual Entailment, pages 1?9,Venice.Baroni, Marco and S. Vegnaduzzo.
2004.Identifying subjective adjectives throughweb-based mutual information.
InProceedings of KONVENS?04, pages 17?24,Vienna.Barzilay, Regina and Kathleen McKeown.2001.
Extracting paraphrases from aparallel corpus.
In Proceedings of ACL /EACL?01, pages 50?57, Toulouse.Caraballo, Sharon A.
1999.
Automaticconstruction of a hypernym-labeled nounhierarchy from text.
In Proceedings ofACL?99, pages 120?126, College Park, MD.Chklovski, Timothy and Patrick Pantel.2004.
VerbOcean: Mining the web forfine-grained Semantic Verb Relations.
InProceedings of EMNLP?04, pages 33?40,Barcelona.Church, Kenneth W. and Hanks Patrick.1990.
Word association norms, mutualinformation, and lexicography.Computational Linguistics, 16(1):22?29.Curran, James R. 2004.
From Distributionalto Semantic Similarity.
Ph.D. Thesis,459Computational Linguistics Volume 35, Number 3School of Informatics of the Universityof Edinburgh, Scotland.Curran, James R. 2005.
Supersense tagging ofunknown nouns using semantic similarity.In Proceedings of ACL?2005, pages 26?33,Ann Arbor, MI.Curran, James R. and Marc Moens.
2002.Improvements in automatic thesaurusextraction.
In Proceedings of the Workshopon Unsupervised Lexical Acquisition,pages 59?67, Philadelphia, PA.Dagan, Ido.
2000.
Contextual WordSimilarity.
In Rob Dale, Hermann Moisl,and Harold Somers, editors, Handbookof Natural Language Processing.
MarcelDekker Inc, Chapter 19, pages 459?476,New York, NY.Dagan, Ido, Oren Glickman, and BernardoMagnini.
2006.
The PASCAL recognisingtextual entailment challenge.
Lecture Notesin Computer Science, 3944:177?190.Dagan, Ido, Lillian Lee, and FernandoPereira.
1999.
Similarity-based models ofco-occurrence probabilities.
MachineLearning, 34(1-3):43?69.Dagan, Ido, Shaul Marcus, and ShaulMarkovitch.
1995.
Contextual wordsimilarity and estimation from sparsedata.
Computer, Speech and Language,9:123?152.Dunning, Ted E. 1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):61?74.Essen, U., and V. Steinbiss.
1992.Co-occurrence smoothing for stochasticlanguage modeling.
In ICASSP?92,1:161?164, Piscataway, NJ.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Ferrandez, O., R. M. Terol, R. Munoz, P.Martinez-Barco, and M. Palomar.
2006.An approach based on logic forms andWordNet relationships to textualentailment performance.
In Proceedingsof the Second PASCAL ChallengesWorkshop on Recognising Textual Entailment,pages 22?26, Venice.Gasperin, Caroline and Renata Vieira.
2004.Using word similarity lists for resolvingindirect anaphora.
In Proceedings ofACL?04 Workshop on Reference Resolution,pages 40?46, Barcelona.Gauch, Susan, J. Wang, and S. MaheshRachakonda.
1999.
A corpus analysisapproach for automatic query expansionand its extension to multiple databases.ACM Transactions on Information Systems(TOIS), 17(3):250?269.Geffet, Maayan.
2006.
Refining theDistributional Similarity Scheme for LexicalEntailment.
Ph.D. Thesis.
School ofComputer Science and Engineering,Hebrew University, Jerusalem, Israel.Geffet, Maayan and Ido Dagan.
2004.
Featurevector quality and distributional similarity.In Proceedings of COLING?04, Articlenumber: 247, Geneva.Geffet, Maayan and Ido Dagan.
2005.
Thedistributional inclusion hypotheses andlexical entailment.
In Proceedings ofACL?05, pages 107?114, Ann Arbor, MI.Gliozzo, Alfio, Claudio Giuliano, and CarloStrapparava.
2005.
Domain kernels forword sense disambiguation.
In Proceedingsof ACL?05, pages 403?410, Ann Arbor, MI.Grefenstette, Gregory.
1994.
Exploration inAutomatic Thesaurus Discovery.
KluwerAcademic Publishers, Norwell, MA.Harris, Zelig S. 1968.
Mathematical structuresof language.
Wiley, New Jersey.Hindle, D. 1990.
Noun classification frompredicate-argument structures.
InProceedings of ACL?90, pages 268?275,Pittsburgh, PA.Jijkoun, Valentin and Maarten de Rijke.
2005.Recognizing textual entailment: Is wordsimilarity enough?
In Joaquin QuinoneroCandela, Ido Dagan, Bernardo Magnini,and Florence d?Alche-Buc, editors, MachineLearning Challenges, Evaluating PredictiveUncertainty, Visual Object Classificationand Recognizing Textual Entailment, FirstPASCAL Machine Learning ChallengesWorkshop, MLCW 2005, Southampton, UK,Lecture Notes in Computer Science 3944,pages 449?460, Springer, New York, NY.Karov, Y. and S. Edelman.
1996.Learning similarity-based word sensedisambiguation from sparse data.In E. Ejerhed and I. Dagan, editors,Fourth Workshop on Very Large Corpora.Association for Computational Linguistics,Somerset, NJ, pages 42?55.Landis, J. R. and G. G. Koch.
1997.
Themeasurements of observer agreement forcategorical data.
Biometrics, 33:159?174.Lee, Lillian.
1997.
Similarity-Based Approachesto Natural Language Processing.
Ph.D. thesis,Harvard University, Cambridge, MA.Lee, Lillian.1999.
Measures of distributionalsimilarity.
In Proceedings of ACL?99,pages 25?32, College Park, MD.Lin, Dekang.
1993.
Principle-based parsingwithout overgeneration.
In Proceedings ofACL?93, pages 112?120, Columbus, OH.Lin, Dekang.
1998.
Automatic retrieval andclustering of similar words.
In Proceedings460Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Qualityof COLING/ACL?98, pages 768?774,Montreal.Lin, Dekang and Patrick Pantel.
2001.Discovery of inference rules for questionanswering.
Natural Language Engineering,7(4):343?360.Luk, Alpha K. 1995.
Statistical sensedisambiguation with relatively smallcorpora using dictionary definitions.In Proceedings of ACL?95, pages 181?188,Cambridge, MA.Manning, Christopher D. and HinrichSchutze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press,Cambridge, MA.Mirkin, Shachar, Ido Dagan, and MaayanGeffet.
2006.
Integrating pattern-basedand distributional similarity methodsfor lexical entailment acquisition.
InProceedings of the COLING/ACL?06 MainConference Poster Sessions, pages 579?586,Sydney.Ng, H. T. 1997.
Exemplar-based wordsense disambiguation: Some recentimprovements.
In Proceedings ofEMNLP?
97, pages 208?213,Providence, RI.Ng, H. T. and H. B. Lee.
1996.
Integratingmultiple knowledge sources todisambiguate word sense: Anexemplar-based approach.
InProceedings of ACL?1996, pages 40?47,Santa Cruz, CA.Nicholson, Jeremy, Nicola Stokes, andTimothy Baldwin.
2006.
Detectingentailment using an extendedimplementation of the basic elementsoverlap metric.
In Proceedings of theSecond PASCAL Challenges Workshopon Recognising Textual Entailment,pages 122?127, Venice.Pado, Sebastian and Mirella Lapata.
2007.Dependency-based construction ofsemantic space models.
ComputationalLinguistics, 33(2):161?199.Pantel, Patrick and Deepak Ravichandran.2004.
Automatically labeling semanticclasses.
In Proceedings of HLT/NAACL?04,pages 321?328, Boston, MA.Pantel, Patrick, D. Ravichandran, andE.
Hovy.
2004.
Towards terascaleknowledge acquisition.
In Proceedings ofCOLING?04, Article number: 771, Geneva.Pekar, Viktor, M. Krkoska, and S. Staab.2004.
Feature weighting forco-occurrence-based classificationof Words.
In Proceedings of COLING?04,Article number: 799, Geneva.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributionalclustering of English words.
InProceedings of ACL?93, pages 183?190,Colombus, OH.Ruge, Gerda.
1992.
Experiments onlinguistically-based term associations.Information Processing & Management,28(3):317?332.Salton, G. and M. J. McGill.
1983.Introduction to Modern InformationRetrieval.
McGraw-Hill, New York, NY.Szpektor, Idan, H. Tanev, Ido Dagan, andB.
Coppola.
2004.
Scaling web-basedacquisition of entailment relations.
InProceedings of EMNLP?04, pages 41?48,Barcelona.Vanderwende, Lucy, Arul Menezes, and RionSnow.
2006.
Microsoft research at RTE-2:Syntactic contributions in the entailmenttask: An implementation.
In Proceedingsof the Second PASCAL Challenges Workshopon Recognising Textual Entailment,pages 27?32, Venice.Weeds, Julie and David Weir.
2005.Co-occurrence retrieval: A flexibleframework for lexical distributionalsimilarity.
Computational Linguistics,31(4):439?476.Weeds, Julie, D. Weir, and D. McCarthy.2004.
Characterizing measures of lexicaldistributional similarity.
In Proceedingsof COLING?04, pages 1015?1021,Switzerland.Widdows, D. 2003.
Unsupervised methodsfor developing taxonomies by combiningsyntactic and statistical information.In Proceedings of HLT/NAACL 2003,pages 197?204, Edmonton.Yarowsky, D. 1992.
Word-sensedisambiguation using statistical modelsof Roget?s categories trained on largecorpora.
In Proceedings of COLING?92,pages 454?460, Nantes.461
