c?
2003 Association for Computational LinguisticsUsing the Web to Obtain Frequencies forUnseen BigramsFrank Keller?
Mirella Lapata?University of Edinburgh University of SheffieldThis article shows that the Web can be employed to obtain frequencies for bigrams that are unseenin a given corpus.
We describe a method for retrieving counts for adjective-noun, noun-noun,and verb-object bigrams from the Web by querying a search engine.
We evaluate this methodby demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) areliable correlation between Web frequencies and plausibility judgments; (c) a reliable correla-tion between Web frequencies and frequencies recreated using class-based smoothing; (d) a goodperformance of Web frequencies in a pseudodisambiguation task.1.
IntroductionIn two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLPalgorithms are typically optimized, tested, and compared on fairly small data sets (cor-pora with millions of words), even though data sets several orders of magnitude largerare available, at least for some NLP tasks.
Banko and Brill (2001a, 2001b) experimentwith context-sensitive spelling correction, a task for which large amounts of data canbe obtained straightforwardly, as no manual annotation is required.
They demonstratethat the learning algorithms typically used for spelling correction benefit significantlyfrom larger training sets, and that their performance shows no sign of reaching anasymptote as the size of the training set increases.Arguably, the largest data set that is available for NLP is the Web,1 which currentlyconsists of at least 3,033 million pages.2 Data retrieved from the Web therefore provideenormous potential for training NLP algorithms, if Banko and Brill?s (2001a, 2001b)findings for spelling corrections generalize; potential applications include tasks thatinvolve word n-grams and simple surface syntax.
There is a small body of existing re-search that tries to harness the potential of the Web for NLP.
Grefenstette and Nioche(2000) and Jones and Ghani (2000) use the Web to generate corpora for languagesfor which electronic resources are scarce, and Resnik (1999) describes a method formining the Web in order to obtain bilingual texts.
Mihalcea and Moldovan (1999) andAgirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001)proposes a method for resolving PP attachment ambiguities based on Web data, Mark-ert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora,?
School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
E-mail: keller@inf.ed.ac.uk?
Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK.E-mail: mlap@dcs.shef.ac.uk1 A reviewer points out that information providers such as Lexis Nexis ?http://www.lexisnexis.com/?might have databases that are even larger than the Web.
Lexis Nexis provides full-text access to newssources (including newspapers, wire services, and broadcast transcripts) and legal data (including caselaw, codes, regulations, legal news, and law reviews).2 This is the number of pages indexed by Google in December 2002, as estimated by Search EngineShowdown ?http://www.searchengineshowdown.com/?.460Computational Linguistics Volume 29, Number 3and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve languagemodeling.A particularly interesting application is proposed by Grefenstette (1998), who usesthe Web for example-based machine translation.
His task is to translate compoundsfrom French into English, with corpus evidence serving as a filter for candidate transla-tions.
An example is the French compound groupe de travail.
There are five translationsof groupe and three translations for travail (in the dictionary that Grefenstette [1998]is using), resulting in 15 possible candidate translations.
Only one of them, namely,work group, has a high corpus frequency, which makes it likely that this is the correcttranslation into English.
Grefenstette (1998) observes that this approach suffers froman acute data sparseness problem if the counts are obtained from a conventional cor-pus.
However, as Grefenstette (1998) demonstrates, this problem can be overcome byobtaining counts through Web searches, instead of relying on a corpus.
Grefenstette(1998) therefore effectively uses the Web as a way of obtaining counts for compoundsthat are sparse in a given corpus.Although this is an important initial result, it raises the question of the generalityof the proposed approach to overcoming data sparseness.
It remains to be shown thatWeb counts are generally useful for approximating data that are sparse or unseenin a given corpus.
It seems possible, for instance, that Grefenstette?s (1998) resultsare limited to his particular task (filtering potential translations) or to his particularlinguistic phenomenon (noun-noun compounds).
Another potential problem is the factthat Web counts are far more noisy than counts obtained from a well-edited, carefullybalanced corpus.
The effect of this noise on the usefulness of the Web counts is largelyunexplored.Zhu and Rosenfeld (2001) use Web-based n-gram counts for language modeling.They obtain a standard language model from a 103-million-word corpus and employWeb-based counts to interpolate unreliable trigram estimates.
They compare their in-terpolated model against a baseline trigram language model (without interpolation)and show that the interpolated model yields an absolute reduction in word error rateof .93% over the baseline.
Zhu and Rosenfeld?s (2001) results demonstrate that theWeb can be a source of data for language modeling.
It is not clear, however, whethertheir result carries over to tasks that employ linguistically meaningful word sequences(e.g., head-modifier pairs or predicate-argument tuples) rather than simply adjacentwords.
Furthermore, Zhu and Rosenfeld (2001) do not undertake any studies that eval-uate Web frequencies directly (i.e., without a task such as language modeling).
Thiscould be done, for instance, by comparing Web frequencies to corpus frequencies, orto frequencies re-created by smoothing techniques.The aim of the present article is to generalize Grefenstette?s (1998) and Zhu andRosenfeld?s (2001) findings by testing the hypothesis that the Web can be employedto obtain frequencies for bigrams that are unseen in a given corpus.
Instead of hav-ing a particular task in mind (which would introduce a sampling bias), we rely onsets of bigrams that are randomly selected from a corpus.
We use a Web-based ap-proach for bigrams that encode meaningful syntactic relations and obtain Web fre-quencies not only for noun-noun bigrams, but also for adjective-noun and verb-objectbigrams.
We thus explore whether this approach generalizes to different predicate-argument combinations.
We evaluate our Web counts in four ways: (a) comparisonwith actual corpus frequencies from two different corpora, (b) comparison with humanplausibility judgments, (c) comparison with frequencies re-created using class-basedsmoothing, and (d) performance in a pseudodisambiguation task on data sets from theliterature.461Keller and Lapata Web Frequencies for Unseen Bigrams2.
Obtaining Frequencies from the Web2.1 Sampling Bigrams from the BNCThe data sets used in the present experiment were obtained from the British NationalCorpus (BNC) (see Burnard [1995]).
The BNC is a large, synchronic corpus, consistingof 90 million words of text and 10 million words of speech.
The BNC is a balancedcorpus (i.e., it was compiled so as to represent a wide range of present day BritishEnglish).
The written part includes samples from newspapers, magazines, books (bothacademic and fiction), letters, and school and university essays, among other kinds oftext.
The spoken part consists of spontaneous conversations, recorded from volunteersbalanced by age, region, and social class.
Other samples of spoken language are alsoincluded, ranging from business or government meetings to radio shows and phone-ins.
The corpus represents many different styles and varieties and is not limited toany particular subject field, genre, or register.For the present study, the BNC was used to extract data for three types of predicate-argument relations.
The first type is adjective-noun bigrams, in which we assumethat the noun is the predicate that takes the adjective as its argument.3 The secondpredicate-argument type we investigated is noun-noun compounds.
For these, weassume that the rightmost noun is the predicate that selects the leftmost noun asits argument (as compound nouns are generally right-headed in English).
Third, weincluded verb-object bigrams, in which the verb is the predicate that selects the objectas its argument.
We considered only direct NP objects; the bigram consists of the verband the head noun of the object.
For each of the three predicate-argument relations,we gathered two data sets, one containing seen bigrams (i.e., bigrams that occur inthe BNC) and one with unseen bigrams (i.e., bigrams that do not occur in the BNC).For the seen adjective-noun bigrams, we used the data of Lapata, McDonald, andKeller (1999), who compiled a set of 90 bigrams as follows.
First, 30 adjectives wererandomly chosen from a part-of-speech-tagged and lemmatized version of the BNCso that each adjective had exactly two senses according to WordNet (Miller et al 1990)and was unambiguously tagged as ?adjective?
98.6% of the time.
Lapata, McDonald,and Keller used the part-of-speech-tagged version that is made available with the BNCand was tagged using CLAWS4 (Leech, Garside, and Bryant 1994), a probabilistic part-of-speech tagger, with error rate ranging from 3% to 4%.
The lemmatized version ofthe corpus was obtained using Karp et al?s (1992) morphological analyzer.The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million words;that is, they covered the whole range from fairly infrequent to highly frequent items.Gsearch (Corley et al 2001), a chart parser that detects syntactic patterns in a taggedcorpus by exploiting a user-specified context-free grammar and a syntactic query, wasused to extract all nouns occurring in a head-modifier relationship with one of the30 adjectives.
Examples of the syntactic patterns the parser identified are given in Ta-ble 1.
In the case of adjectives modifying compound nouns, only sequences of twonouns were included, and the rightmost-occurring noun was considered the head.Bigrams involving proper nouns or low-frequency nouns (less than 10 per millionwords) were discarded.
This was necessary because the bigrams were used in exper-iments involving native speakers (see Section 3.2), and we wanted to reduce the riskof including words unfamiliar to the experimental subjects.
For each adjective, the setof bigrams was divided into three frequency bands based on an equal division of the3 This assumption is disputed in the theoretical linguistics literature.
For instance, Pollard and Sag (1994)present an analysis in which there is mutual selection between the noun and the adjective.462Computational Linguistics Volume 29, Number 3Table 1Example of patterns used for the extraction of adjective-noun bigrams.Pattern ExampleA N educational materialA Adv N usual weekly classesA N N environmental health officersrange of log-transformed co-occurrence frequencies.
Then one bigram was chosen atrandom from each band.
This procedure ensures that the whole range of frequenciesis represented in our sample.Lapata, Keller, and McDonald (2001) compiled a set of 90 unseen adjective-nounbigrams using the same 30 adjectives.
For each adjective, Gsearch was used to com-pile a list of all nouns that did not co-occur in a head-modifier relationship with theadjective.
Again, proper nouns and low-frequency nouns were discarded from thislist.
Then each adjective was paired with three randomly chosen nouns from its listof non-co-occurring nouns.
Examples of seen and unseen adjective-noun bigrams areshown in Table 2.For the present study, we applied the procedure used by Lapata, McDonald, andKeller (1999) and Lapata, Keller, and McDonald (2001) to noun-noun bigrams and toverb-object bigrams, creating a set of 90 seen and 90 unseen bigrams for each typeof predicate-argument relationship.
More specifically, 30 nouns and 30 verbs werechosen according to the same criteria proposed for the adjective study (i.e., minimalsense ambiguity and unambiguous part of speech).
All nouns modifying one of the30 nouns were extracted from the BNC using a heuristic from Lauer (1995) that looksfor consecutive pairs of nouns that are neither preceded nor succeeded by anotherTable 2Example stimuli for seen and unseen adjective-noun, noun-noun, and verb-object bigrams(with log-transformed BNC counts).Adjective-Noun BigramsAdjective High Medium Low Unseenhungry animal 1.79 pleasure 1.38 application 0 tradition, innovation, preyguilty verdict 3.91 secret 2.56 cat 0 system, wisdom, wartimenaughty girl 2.94 dog 1.6 lunch .69 regime, rival, protocolNoun-Noun BigramsHigh Medium Low Unseen Head Nounprocess 1.14 user .95 gala 0 collection, clause, coat directorytelevision 1.53 satellite .95 edition 0 chain, care, vote broadcastplasma 1.78 nylon 1.20 unit .60 fund, theology, minute membraneVerb-Object BigramsVerb High Medium Low Unseenfulfill obligation 3.87 goal 2.20 scripture .69 participant, muscle, gradeintensify problem 1.79 effect 1.10 alarm 0 score, quota, chestchoose name 3.74 law 1.61 series 1.10 lift, bride, listener463Keller and Lapata Web Frequencies for Unseen Bigramsnoun.
Lauer?s heuristic (see (1)) effectively avoids identifying as two-word compoundsnoun sequences that are part of a larger compound.
(1) C = {(w2, w3) | w1w2w3w4; w1, w4 ?
N; w2, w3 ?
N}Here, w1 w2 w3 w4 denotes the occurrence of a sequence of four words and N is theset of words tagged as nouns in the corpus.
C is the set of compounds identified byLauer?s (1995) heuristic.Verb-object bigrams for the 30 preselected verbs were obtained from the BNCusing Cass (Abney 1996), a robust chunk parser designed for the shallow analysisof noisy text.
The parser recognizes chunks and simplex clauses (i.e., sequences ofnonrecursive clauses) using a regular expression grammar and a part-of-speech-taggedcorpus, without attempting to resolve attachment ambiguities.
It comes with a large-scale grammar for English and a built-in tool that extracts predicate-argument tuplesout of the parse trees that Cass produces.The parser?s output was postprocessed to remove bracketing errors and errors inidentifying chunk categories that could potentially result in bigrams whose membersdo not stand in a verb-argument relationship.
Tuples containing verbs or nouns at-tested in a verb-argument relationship only once were eliminated.
Particle verbs wereretained only if the particle was adjacent to the verb (e.g., come off heroin).
Verbs fol-lowed by the preposition by and a head noun were considered instances of verb-subjectrelations.
It was assumed that PPs adjacent to the verb headed by any of the preposi-tions in, to, for, with, on, at, from, of, into, through, and upon were prepositional objects (seeLapata [2001] for details on the filtering process).
Only nominal heads were retainedfrom the objects returned by the parser.
As in the adjective study, noun-noun bigramsand verb-object bigrams with proper nouns or low-frequency nouns (less than 10 permillion words) were discarded.
The sets of noun-noun and verb-object bigrams weredivided into three frequency bands, and one bigram was chosen at random from eachband.The procedure described by Lapata, Keller, and McDonald (2001) was followed forcreating sets of unseen noun-noun and verb-object bigrams: for each noun or verb, wecompiled a list of all nouns with which it did not co-occur within a noun-noun or verb-object bigram in the BNC.
Again, Lauer?s (1995) heuristic and Abney?s (1996) partialparser were used to identify bigrams, and proper nouns and low-frequency nounswere excluded.
For each noun and verb, three bigrams were formed by pairing it witha noun randomly selected from the set of the non-co-occurring nouns for that nounor verb.
Table 2 lists examples for the seen and unseen noun-noun and verb-objectbigrams generated by this procedure.The extracted bigrams are in several respects an imperfect source of informationabout adjective-noun or noun-noun modification and verb-object relations.
First noticethat both Gsearch and Cass detect syntactic patterns on part-of-speech-tagged corpora.This means that parsing errors are likely to result because of tagging mistakes.
Second,even if one assumes perfect tagging, the heuristic nature of our extraction proceduresmay introduce additional noise or miss bigrams for which detailed structural infor-mation would be needed.For instance, our method for extracting adjective-noun pairs ignores cases in whichthe adjective modifies noun sequences of length greater than two.
The heuristic in (1)considers only two-word noun sequences.
Abney?s (1996) chunker recognizes basicsyntactic units without resolving attachment ambiguities or recovering missing infor-mation (such as traces resulting from the movement of constituents).
Although pars-ing is robust and fast (since unlike in traditional parsers, no global optimization takes464Computational Linguistics Volume 29, Number 3place), the identified verb-argument relations are undoubtedly somewhat noisy, giventhe errors inherent in the part-of-speech tagging and chunk recognition procedure.When evaluated against manually annotated data, Abney?s (1996) parser identifiedchunks with 87.9% precision and 87.1% recall.
The parser further achieved a per-wordaccuracy of 92.1% (where per-word accuracy includes the chunk category and chunklength identified correctly).Despite their imperfect output, heuristic methods for the extraction of syntacticrelations are relatively common in statistical NLP.
Several statistical models employfrequencies obtained from the output of partial parsers and other heuristic methods;these include models for disambiguating the attachment site of prepositional phrases(Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns(Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for theinduction of selectional preferences (Abney and Light 1999), methods for automati-cally clustering words according to their distribution in particular syntactic contexts(Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994;Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee1999; Dagan, Lee, and Pereira 1999).
In this article we investigate alternative waysfor obtaining bigram frequencies that are potentially useful for such models despitethe fact that some of these bigrams are identified in a heuristic manner and may benoisy.2.2 Sampling Bigrams from the NANTCWe also obtained corpus counts from a second corpus, the North American NewsText Corpus (NANTC).
This corpus differs in several important respects from theBNC.
It is substantially larger, as it contains 350 million words of text.
Also, it is nota balanced corpus, as it contains material from only one genre, namely, news text.However, the text originates from a variety of sources (Los Angeles Times, WashingtonPost, New York Times News Syndicate, Reuters News Service, and Wall Street Journal).Whereas the BNC covers British English, the NANTC covers American English.
Allthese differences mean that the NANTC provides a second, independent standardagainst which to compare Web counts.
At the same time the correlation found betweenthe counts obtained from the two corpora can serve as an upper limit for the correlationthat we can expect between corpus counts and Web counts.The NANTC corpus was parsed using MINIPAR (Lin 1994, 2001), a broad-coverageparser for English.
MINIPAR employs a manually constructed grammar and a lexiconderived from WordNet with the addition of proper names (130,000 entries in total).Lexicon entries contain part-of-speech and subcategorization information.
The gram-mar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges(i.e., types of syntactic [dependency] relationships).
MINIPAR employs a distributed-chart parsing algorithm.
Instead of a single chart, each node in the grammar networkmaintains a chart containing partially built structures belonging to the grammaticalcategory represented by the node.
Grammar rules are implemented as constraints as-sociated with the nodes and edges.The output of MINIPAR is a dependency tree that represents the dependencyrelations between words in a sentence.
Table 3 shows a subset of the dependenciesMINIPAR outputs for the sentence The fat cat ate the door mat.
In contrast to Gsearchand Cass, MINIPAR produces all possible parses for a given sentence.
The parses areranked according to the product of the probabilities of their edges, and the most likelyparse is returned.
Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson1995), a domain-independent corpus of British English, and achieved a recall of 79%and precision of 89% on the dependency relations.465Keller and Lapata Web Frequencies for Unseen BigramsTable 3Examples of dependencies generated by MINIPAR for The fat cat ate the door mat.Head Relation Modifier Descriptioncat N:det:Det the determiner of nouncat N:mod:A fat adjective modifier of nouneat V:subj:N cat subject of verbeat V:obj:N mat object of verbmat N:det:Det the determiner of nounmat N:nn:N door prenominal modifier of nounFor our experiments, we concentrated solely on adjective-noun, noun-noun, andverb object relations (denoted as N:mod:A, N:nn:N, and V:obj:N in Table 3).
From thesyntactic analysis provided by the parser, we extracted all occurrences of bigrams thatwere attested both in the BNC and the NANTC corpus.
In this way, we obtainedNANTC frequency counts for the bigrams that we had randomly selected from theBNC.
Table 4 shows the NANTC counts for the set of seen bigrams from Table 2.Because of the differences in the extraction methodology (chunking versus fullparsing) and the text genre (balanced corpus versus news text), we expected thatsome BNC bigrams would not be attested in the NANTC corpus.
More precisely, zerofrequencies were returned for 23 adjective-noun, 16 verb-noun, and 37 noun-nounbigrams.
The fact that more zero frequencies were observed for noun-noun bigramsthan for the other two types is perhaps not surprising considering the ease with whichnovel compounds are created (Levi 1978).
We adjusted the zero counts by settingthem to .5.
This was necessary because all further analyses were carried out on log-transformed frequencies (see Table 4).Table 4Log-transformed NANTC counts for seen adjective-noun, noun-noun, and verb-object bigrams.Adjective-Noun BigramsAdjective High Medium Lowhungry animal .90 pleasure -.30 application .60guilty verdict 2.82 secret .95 cat -.30naughty girl .69 dog -.30 lunch -.30Noun-Noun BigramsHigh Medium Low Head Nounprocess - .30 user -.30 gala -.30 directorytelevision 2.70 satellite -.30 edition -.30 broadcastplasma - .30 nylon 0 unit 0 membraneVerb-Object BigramsVerb High Medium Lowfulfill obligation 2.38 goal 2.04 scripture -.30intensify problem 1.20 effect .60 alarm -.30choose name 2.25 law .90 series .48466Computational Linguistics Volume 29, Number 32.3 Obtaining Web CountsWeb counts for bigrams were obtained using a simple heuristic based on queries to thesearch engines AltaVista and Google.
All search terms took into account the inflectionalmorphology of nouns and verbs.The search terms for verb-object bigrams matched not only cases in which theobject was directly adjacent to the verb (e.g., fulfill obligation), but also cases in whichthere was an intervening determiner (e.g., fulfill the/an obligation).
The following searchterms were used for adjective-noun, noun-noun, and verb-object bigrams, respectively:(2) "A N", where A is the adjective and N is the singular or plural form of thenoun.
(3) "N1 N2", where N1 is the singular form of the first noun and N2 is thesingular or plural form of the second noun.
(4) "V Det N", where V is the infinitive, singular present, plural present,past, perfect, or gerund form of the verb, Det is the determiner the, thedeterminer a, or the empty string, and N is the singular or plural form ofthe noun.Note that all searches were for exact matches, which means that the words in the searchterms had to be directly adjacent to score a match.
This is encoded by enclosing thesearch term in quotation marks.
All our search terms were in lower case.
We searchedthe whole Web (as indexed by AltaVista and Google); that is, the queries were notrestricted to pages in English.Based on the Web searches, we obtained bigram frequencies by adding up thenumber of pages that matched the morphologically expanded forms of the searchterms (see the patterns in (2)?(4)).
This process can be automated straightforwardlyusing a script that generates all the search terms for a given bigram, issues an AltaVistaor Google query for each of the search terms, and then adds up the resulting numberof matches for each bigram.
We applied this process to all the bigrams in our data set,covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams (i.e., aset of 540 bigrams in total).
The queries were carried out in January 2003 (and thusthe counts are higher than those reported in Keller, Lapata, and Ourioupina [2002],which were generated about a year earlier).For some bigrams that were unseen in the BNC, our Web-based procedure returnedzero counts; that is, there were no matches for those bigrams in the Web searches.
Itis interesting to compare the Web and NANTC with respect to zero counts: Bothdata sources are larger than the BNC and hence should be able to mitigate the datasparseness problem to a certain extent.
Table 5 provides the number of zero counts forboth Web search engines and compares them to the number of bigrams that yielded nomatches in the NANTC.
We observe that the Web counts are substantially less sparsethan the NANTC counts: In the worst case, there are nine bigrams for which our Webqueries returned no matches (10% of the data), whereas up to 82 bigrams were unseenin the NANTC (91% of the data).
Recall that the NANTC is 3.5 times larger than theBNC, which does not seem to be enough to substantially mitigate data sparseness.
Allfurther analyses were carried out on log-transformed frequencies; hence we adjustedzero counts by setting them to .5.Table 6 shows descriptive statistics for the bigram counts we obtained usingAltaVista and Google.
For comparison, this table also provides descriptive statis-tics for the BNC and NANTC counts (for seen bigrams only) and for the counts467Keller and Lapata Web Frequencies for Unseen BigramsTable 5Number of zero counts returned by queries to search engines and in the NANTC (for bigramsunseen in BNC).Adjective-Noun Noun-Noun Verb-ObjectAltaVista 2 9 1Google 2 5 0NANTC 76 82 78Table 6Descriptive statistics for Web counts, corpus counts, and counts re-created using class-basedsmoothing (log-transformed).Adjective-Noun Noun-Noun Verb-ObjectMin Max Mean SD Min Max Mean SD Min Max Mean SDSeen BigramsAltaVista 1.15 5.84 3.72 1.02 .60 6.16 3.52 1.22 .48 5.86 3.42 1.13Google 1.54 6.11 4.01 1.01 .90 6.30 3.80 1.23 .60 5.96 3.70 1.11BNC 0 2.19 .89 .69 0 2.14 .74 .64 0 2.55 .68 .58NANTC - .30 2.84 .84 .96 - .30 3.02 .56 .94 -.30 3.73 1.90 .98Smoothing - .06 2.32 1.28 .51 - .70 1.71 .30 .61 -.51 2.07 .53 .57Unseen BigramsAltaVista - .30 5.00 1.50 .99 - .30 3.97 1.20 1.14 - .30 3.88 1.55 1.06Google - .30 4.11 1.79 .95 - .30 4.15 1.60 1.12 0 4.19 1.90 1.04Smoothing - .03 2.10 1.25 .46 -1.01 1.93 .28 .66 -.70 1.95 .53 .58Table 7Average factor by which Web counts are larger than BNC counts (seen bigrams).Adjective-Noun Noun-Noun Verb-ObjectAltaVista 665 691 550Google 1,306 1,151 1,064re-created using class-based smoothing (see Section 3.3 for details on the re-createdfrequencies).From these data, we computed the average factor by which the Web counts arelarger than the BNC counts.
The results are given in Table 7 and indicate that theAltaVista counts are between 550 and 691 times larger than the BNC counts, and thatthe Google counts are between 1,064 and 1,306 times larger than the BNC counts.As we know the size of the BNC (100 million words), we can use these figures toestimate the number of words available on the Web: between 55.0 and 69.1 billionwords for AltaVista, and between 106.4 and 139.6 billion words for Google.
Theseestimates are in the same order of magnitude as Grefenstette and Nioche?s (2000)estimate that 48.1 billion words of English are available on the Web (based on AltaVistacounts compiled in February 2000).
They also agree with Zhu and Rosenfeld?s (2001)estimate that the effective size of the Web is between 79 and 108 billion words (basedon AltaVista, Lycos, and FAST counts; no date given).468Computational Linguistics Volume 29, Number 32.4 Potential Sources of Noise in Web CountsThe method we used to retrieve Web counts is based on very simple heuristics; it isthus inevitable that the counts generated will contain a certain amount of noise.
Inthis section we discuss a number of potential sources of such noise.An obvious limitation of our method is that it relies on the page counts returned bythe search engines; we do not download the pages themselves for further processing.Note that many of the bigrams in our sample are very frequent (up to 106 matches;see the ?Max?
columns in Table 6), hence the effort involved in downloading all pageswould be immense (though methods for downloading a representative sample couldprobably be devised).Our approach estimates Web frequencies based not on bigram counts directly, buton page counts.
In other words, it ignores the fact that a bigram can occur more thanonce on a given Web page.
This approximation is justified, as Zhu and Rosenfeld (2001)demonstrated for unigrams, bigrams, and trigrams: Page counts and n-gram countsare highly correlated on a log-log scale.
This result is based on Zhu and Rosenfeld?squeries to AltaVista, a search engine that at the time of their research returned boththe number of pages and the overall number of matches for a given query.4Another important limitation of our approach arises from the fact that both Googleand AltaVista disregard punctuation and capitalization, even if the search term isplaced within quotation marks.
This can lead to false positives, for instance, if thematch crosses a phrase boundary, such as in (5), which matches hungry prey.
Otherfalse positives can be generated by page titles and links, such as the examples (6)and (7) which match edition broadcast.5(5) The lion will kill only when it?s hungry.
Prey can usually sense whenlions are hunting.
(6) 10th Edition Broadcast Products Catalog (as a page title)(7) Issue/Edition/Broadcast (as a link)The fact that our method does not download Web pages means that no tagging, chunk-ing, or parsing can be carried out to ensure that the matches are correct.
Instead werely on the simple adjacency of the search terms, which is enforced by using queriesenclosed within quotation marks (see Section 2.3 for details).
This means that we missany nonadjacent matches, even though a chunker or parser (such as the one used forextracting BNC or NANTC bigrams) would find them.
An example is an adjective-noun bigram in which an adverbial intervenes between the adjective and the noun(see Table 1).Furthermore, the absence of tagging, chunking, and parsing can also generate falsepositives, in particular for queries containing words with part-of-speech ambiguity.
(Recall that our bigram selection procedure ensures that the predicate word, but notthe argument word, is unambiguous in terms of its POS tagging in the BNC.)
As anexample, consider process directory, which in our data set is a noun-noun bigram (seeTable 2).
One of the matches returned by Google is (8), in which process is a verb.Another example is fund membrane, which is a noun-noun bigram in our data set butmatches (9) in Google.4 Note that this feature of AltaVista has since been discontinued; hence in the present article we had nooption but to use page counts.
However, Keller, Lapata, and Ourioupina (2002) used AltaVista matchcounts (instead of page counts) on the same data sets; their results agree with the ones reported in thepresent article very closely.5 Some of the examples in (5)?
(9) were kindly provided by a reviewer.469Keller and Lapata Web Frequencies for Unseen Bigrams(8) The global catalog server?s function is to process directory searches forthe entire forest.
(9) Green grants fund membrane technology.Another source of noise is the fact that Google (but not AltaVista) will sometimesreturn pages that do not include the search term at all.
This can happen if the searchterm is contained in a link to the page (but not on the page itself).As we did not limit our Web searches to English (even though many search enginesnow allow the target language for a search to be set), there is also a risk that false posi-tives are generated by cross-linguistic homonyms, that is, by words of other languagesthat are spelled in the same way as the English words in our data sets.
However, thisproblem is mitigated by the fact that English is by far the most common language onthe Web, as shown by Grefenstette and Nioche (2000).
Also, the chance of two suchhomonyms forming a valid bigram in another language is probably fairly small.To summarize, Web counts are certainly less sparse than the counts in a corpus of afixed size (see Section 2.3).
However, Web counts are also likely to be significantly morenoisy than counts obtained from a carefully tagged and chunked or parsed corpus, asthe examples in this section show.
It is therefore essential to carry out a comprehensiveevaluation of the Web counts generated by our method.
This is the topic of the nextsection.3.
Evaluation3.1 Evaluation against Corpus FrequenciesSince Web counts can be relatively noisy, as discussed in the previous section, it iscrucial to determine whether there is a reliable relationship between Web counts andcorpus counts.
Once this is assured, we can explore the usefulness of Web countsfor overcoming data sparseness.
We carried out a correlation analysis to determinewhether there is a linear relationship between BNC and NANTC counts and AltaVistaand Google counts.
All correlation coefficients reported in this article refer to Pear-son?s r.6 All results were obtained on log-transformed counts.7Table 8 shows the results of correlating Web counts with corpus counts from theBNC, the corpus from which our bigrams were sampled (see Section 2.1).
A highcorrelation coefficient was obtained across the board, ranging from .720 to .847 forAltaVista counts and from .720 to .850 for Google counts.
This indicates that Webcounts approximate BNC counts for the three types of bigrams under investigation.Note that there is almost no difference between the correlations achieved using Googleand AltaVista counts.It is important to check that these results are also valid for counts obtained fromother corpora.
We therefore correlated our Web counts with the counts obtained fromNANTC, a corpus that is larger than the BNC but is drawn from a single genre,namely, news text (see Section 2.2).
The results are shown in Table 9.
We find that6 Correlation analysis is a way of measuring the degree of linear association between two variables.Effectively, we are fitting a linear equation y = ax + b to the data; this means that the two variables xand y (which in our case represent frequencies or judgments) can still differ by a multiplicativeconstant a and an additive constant b, even if they are highly correlated.7 It is well-known that corpus frequencies have a Zipfian distribution.
Log-transforming them is a wayof normalizing the counts before applying statistical tests.
We apply correlation analysis on thelog-transformed data, which is equivalent to computing a log-linear regression coefficient on theuntransformed data.470Computational Linguistics Volume 29, Number 3Table 8Correlation of BNC counts with Web counts (seen bigrams).Adjective-Noun Noun-Noun Verb-ObjectAltaVista .847** .720** .762**Google .850** .720** .766**Smoothing .248* .277** .342***p < .05 (one-tailed).
**p < .01 (one-tailed).Table 9Correlation of NANTC counts with Web counts (seen bigrams).Adjective-Noun Noun-Noun Verb-ObjectAltaVista .712** .667** .788**Google .712** .662** .787**BNC .710** .672** .814**Smoothing .338** .317** .263**p < .05 (one-tailed).
**p < .01 (one-tailed).Google and AltaVista counts also correlate significantly with NANTC counts.
Thecorrelation coefficients range from .667 to .788 for AltaVista and from .662 to .787 forGoogle.
Again, there is virtually no difference between the correlations for the twosearch engines.
We also observe that the correlation between Web counts and BNC isgenerally slightly higher than the correlation between Web counts and NANTC counts.We carried out one-tailed t-tests to determine whether the differences in the correlationcoefficients were significant.
We found that both AltaVista counts (t(87) = 3.11, p < .01)and Google counts (t(87) = 3.21, p < .01) were significantly better correlated withBNC counts than with NANTC counts for adjective-noun bigrams.
The difference incorrelation coefficients was not significant for noun-noun and verb-object bigrams, foreither search engine.Table 9 also shows the correlations between BNC counts and NANTC counts.The intercorpus correlation can be regarded as an upper limit for the correlations wecan expect between counts from two corpora that differ in size and genre and thathave been obtained using different extraction methods.
The correlation between Al-taVista and Google counts and NANTC counts reached the upper limit for all threebigram types (one-tailed t-tests found no significant differences between the correla-tion coefficients).
The correlation between BNC counts and Web counts reached theupper limit for noun-noun and verb-object bigrams (no significant differences for eithersearch engine) and significantly exceeded it for adjective-noun bigrams for AltaVista(t(87) = 3.16, p < .01) and Google (t(87) = 3.26, p < .01).We conclude that simple heuristics (see Section 2.3) are sufficient to obtain usefulfrequencies from the Web; it seems that the large amount of data available for Webcounts outweighs the associated problems (noisy, unbalanced, etc.).
We found thatWeb counts were highly correlated with frequencies from two different corpora.
Fur-thermore, Web counts and corpus counts are as highly correlated as counts from twodifferent corpora (which can be regarded as an upper bound).Note that Tables 8 and 9 also provide the correlation coefficients obtained whencorpus frequencies are compared with frequencies that were re-created through class-471Keller and Lapata Web Frequencies for Unseen Bigramsbased smoothing, using the BNC as a training corpus (after removing the seen bi-grams).
This will be discussed in more detail in Section 3.3.3.2 Evaluation against Plausibility JudgmentsPrevious work has demonstrated that corpus counts correlate with human plausibilityjudgments for adjective-noun bigrams.
This result holds both for seen bigrams (La-pata, McDonald, and Keller 1999) and for unseen bigrams whose counts have beenre-created using smoothing techniques (Lapata, Keller, and McDonald 2001).
Based onthese findings, we decided to evaluate our Web counts on the task of predicting plausi-bility ratings.
If the Web counts for bigrams correlate with plausibility judgments, thenthis indicates that the counts are valid, in the sense of being useful for predicting theintuitive plausibility of predicate-argument pairs.
The degree of correlation betweenWeb counts and plausibility judgments is an indicator of the quality of the Web counts(compared to corpus counts or counts re-created using smoothing techniques).3.2.1 Method.
For seen and unseen adjective-noun bigrams, we used the two sets ofplausibility judgments collected by Lapata, McDonald, and Keller (1999) and Lapata,Keller, and McDonald (2001), respectively.
We conducted four additional experimentsto collect judgments for noun-noun and verb-object bigrams, both seen and unseen.The experimental method was the same for all six experiments.Materials.
The experimental stimuli were based on the six sets of seen or unseenbigrams extracted from the BNC as described in Section 2.1 (adjective-noun, noun-noun, and verb-object bigrams).
In the adjective-noun and noun-noun cases, the stimuliconsisted simply of the bigrams.
In the verb-object case, the bigrams were embeddedin a short sentence to make them more natural: A proper-noun subject was added.Procedure.
The experimental paradigm was magnitude estimation (ME), a tech-nique standardly used in psychophysics to measure judgments of sensory stimuli(Stevens 1975), which Bard, Robertson, and Sorace (1996) and Cowart (1997) have ap-plied to the elicitation of linguistic judgments.
The ME procedure requires subjects toestimate the magnitude of physical stimuli by assigning numerical values proportionalto the stimulus magnitude they perceive.
In contrast to the five- or seven-point scaleconventionally used to measure human intuitions, ME employs an interval scale andtherefore produces data for which parametric inferential statistics are valid.ME requires subjects to assign numbers to a series of linguistic stimuli in a propor-tional fashion.
Subjects are first exposed to a modulus item, to which they assign anarbitrary number.
All other stimuli are rated proportional to the modulus.
In this way,each subject can establish his or her own rating scale, thus yielding maximally fine-graded data and avoiding the known problems with the conventional ordinal scalesfor linguistic data (Bard, Robertson, and Sorace 1996; Cowart 1997; Schu?tze 1996).The experiments reported in this article were carried out using the WebExp soft-ware package (Keller et al 1998).
A series of previous studies has shown that dataobtained using WebExp closely replicate results obtained in a controlled laboratory set-ting; this has been demonstrated for acceptability judgments (Keller and Alexopoulou2001), coreference judgments (Keller and Asudeh 2001), and sentence completions(Corley and Scheepers 2002).In the present experiments, subjects were presented with bigram pairs and wereasked to rate the degree of plausibility proportional to a modulus item.
They first saw aset of instructions that explained the ME technique and the judgment task.
The conceptof plausibility was not defined, but examples of plausible and implausible bigramswere given (different examples for each stimulus set).
Then subjects were asked tofill in a questionnaire with basic demographic information.
The experiment proper472Computational Linguistics Volume 29, Number 3Table 10Descriptive statistics for plausibility judgments (log-transformed).
N is the number of subjectsused in each experiment.Adjective-Noun Bigrams Noun-Noun Bigrams Verb-Object BigramsN Min Max Mean SD N Min Max Mean SD N Min Max Mean SDSeen 30 -.85 .11 -.13 .22 25 -.15 .69 .40 .21 27 -.52 .45 .12 .24Unseen 41 -.56 .37 -.07 .20 25 -.49 .52 -.01 .23 21 -.51 .28 -.16 .22consisted of three phases: (1) a calibration phase, designed to familiarize subjects withthe task, in which they had to estimate the length of five horizontal lines; (2) a practicephase, in which subjects judged the plausibility of eight bigrams (similar to the onesin the stimulus set); (3) the main experiment, in which each subject judged one of thesix stimulus sets (90 bigrams).
The stimuli were presented in random order, with anew randomization being generated for each subject.Subjects.
A separate experiment was conducted for each set of stimuli.
The numberof subjects per experiment is shown in Table 10 (in the column labeled N).
All sub-jects were self-reported native speakers of English; they were recruited by postings tonewsgroups and mailing lists.
Participation was voluntary and unpaid.WebExp collects by-item response time data; subjects whose response times werevery short or very long were excluded from the sample, as they are unlikely to havecompleted the experiment adequately.
We also excluded the data of subjects who hadparticipated more than once in the same experiment, based on their demographic dataand on their Internet connection data, which is logged by WebExp.3.2.2 Results and Discussion.
The experimental data were normalized by dividingeach numerical judgment by the modulus value that the subject had assigned to thereference sentence.
This operation creates a common scale for all subjects.
Then thedata were transformed by taking the decadic logarithm.
This transformation ensuresthat the judgments are normally distributed and is standard practice for magnitudeestimation data (Bard, Robertson, and Sorace 1996; Cowart 1997; Stevens 1975).
Allfurther analyses were conducted on the normalized, log-transformed judgments.Table 10 shows the descriptive statistics for all six judgment experiments: theoriginal experiments by Lapata, McDonald, and Keller (1999) and Lapata, Keller, andMcDonald (2001) for adjective-noun bigrams, and our new ones for noun-noun andverb-object bigrams.We used correlation analysis to compare corpus counts and Web counts with plau-sibility judgments.
Table 11 (top half) lists the correlation coefficients that were ob-tained when correlating log-transformed Web counts (AltaVista and Google) and cor-pus counts (BNC and NANTC) with mean plausibility judgments for seen adjective-noun, noun-noun, and verb-object bigrams.
The results show that both AltaVista andGoogle counts correlate well with plausibility judgments for seen bigrams.
The corre-lation coefficient for AltaVista ranges from .641 to .700; for Google, it ranges from .624to .692.
The correlations for the two search engines are very similar, which is also whatwe found in Section 3.1 for the correlations between Web counts and corpus counts.Note that the Web counts consistently achieve a higher correlation with the judg-ments than the BNC counts, which range from .488 to .569.
We carried out a seriesof one-tailed t-tests to determine whether the differences between the correlation co-efficients for the Web counts and the correlation coefficients for the BNC counts weresignificant.
For the adjective-noun bigrams, the AltaVista coefficient was significantly473Keller and Lapata Web Frequencies for Unseen Bigramshigher than the BNC coefficient (t(87) = 1.76, p < .05), whereas the difference betweenthe Google coefficient and the BNC coefficient failed to reach significance.
For thenoun-noun bigrams, both the AltaVista and the Google coefficients were significantlyhigher than the BNC coefficient (t(87) = 3.11, p < .01 and t(87) = 2.95, p < .01).Also, for the verb-object bigrams, both the AltaVista coefficient and the Google coef-ficient were significantly higher than the BNC coefficient (t(87) = 2.64, p < .01 andt(87) = 2.32, p < .05).A similar picture was observed for the NANTC counts.
Again, the Web countsoutperformed the corpus counts in predicting plausibility.
For the adjective-noun bi-grams, both the AltaVista and the Google coefficient were significantly higher than theNANTC coefficient (t(87) = 1.97, p < .05; t(87) = 1.81, p < .05).
For the noun-noun bi-grams, the AltaVista coefficient was higher than the NANTC coefficient (t(87) = 1.64,p < .05), but the Google coefficient was not significantly different from the NANTC co-efficient.
For verb-object bigrams, the difference was significant for both search engines(t(87) = 2.74, p < .01; t(87) = 2.38, p < .01).In sum, for all three types of bigrams, the correlation coefficients achieved withAltaVista were significantly higher than the ones achieved by either the BNC or theNANTC.
Google counts outperformed corpus counts for all bigrams with the exceptionof adjective-noun counts from the BNC and noun-noun counts from the NANTC.The bottom panel of Table 11 shows the correlation coefficients obtained by com-paring log-transformed judgments with log-transformed Web counts for unseen adjec-tive-noun, noun-noun, and verb-object bigrams.
We observe that the Web counts con-sistently show a significant correlation with the judgments, with the coefficient rang-ing from .480 to .578 for AltaVista counts and from .473 to .595 for the Google counts.Table 11 also provides the correlations between plausibility judgments and countsre-created using class-based smoothing, which we will discuss in Section 3.3.An important question is how well humans agree when judging the plausibility ofadjective-noun, noun-noun, and verb-noun bigrams.
Intersubject agreement gives anupper bound for the task and allows us to interpret how well our Web-based methodperforms in relation to humans.
To calculate intersubject agreement we used leave-Table 11Correlation of plausibility judgments with Web counts, corpus counts, and counts re-createdusing class-based smoothing.
?Agreement?
refers to the intersubject agreement on thejudgment task.Adjective-Noun Noun-Noun Verb-ObjectSeen BigramsAltaVista .650** .700** .641**Google .641** .692** .624**BNC .569** .517** .488**NANTC .526** .597** .491**Smoothing .329** .318** .223*Agreement .630** .641** .604**Unseen BigramsAltaVista .480** .578** .551**Google .473** .595** .520**Smoothing .342** .372** .298**Agreement .550** .570** .640***p < .05 (one-tailed).
**p < .01 (one-tailed).474Computational Linguistics Volume 29, Number 3one-out resampling.
This technique is a special case of n-fold cross-validation (Weissand Kulikowski 1991) and has been previously used for measuring how well humansagree in judging semantic similarity (Resnik 1999, 2000).For each subject group, we divided the set of the subjects?
responses with size ninto a set of size n?
1 (i.e., the response data of all but one subject) and a set of size 1(i.e., the response data of a single subject).
We then correlated the mean ratings of theformer set with the ratings of the latter.
This was repeated n times (see the numberof participants in Table 6); the mean of the correlation coefficients for the seen andunseen bigrams is shown in Table 11 in the rows labeled ?Agreement.
?For both seen and unseen bigrams, we found no significant difference between theupper bound (intersubject agreement) and the correlation coefficients obtained usingeither AltaVista or Google counts.
This finding holds for all three types of bigrams.
Thesame picture emerged for the BNC and NANTC counts: These correlation coefficientswere not significantly different from the upper limit, for all three types of bigrams,both for seen and for unseen bigrams.To conclude, our evaluation demonstrated that Web counts reliably predict humanplausibility judgments, both for seen and for unseen predicate-argument bigrams.AltaVista counts for seen bigrams are a better predictor of human judgments thanBNC and NANTC counts.
These results show that our heuristic method yields validfrequencies; the simplifications we made in obtaining the Web counts (see Section 2.3),as well as the fact that Web data are noisy (see Section 2.4), seem to be outweighedby the fact that the Web is up to a thousand times larger than the BNC.3.3 Evaluation against Class-Based SmoothingThe evaluation in the last two sections established that Web counts are useful forapproximating corpus counts and for predicting plausibility judgments.
As a furtherstep in our evaluation, we correlated Web counts with counts re-created by applyinga class-based smoothing method to the BNC.We re-created co-occurrence frequencies for predicate-argument bigrams using asimplified version of Resnik?s (1993) selectional association measure proposed by La-pata, Keller, and McDonald (2001).
In a nutshell, this measure replaces Resnik?s (1993)information-theoretic approach with a simpler measure that makes no assumptionswith respect to the contribution of a semantic class to the total quantity of informationprovided by the predicate about the semantic classes of its argument.
It simply substi-tutes the argument occurring in the predicate-argument bigram with the concept bywhich it is represented in the WordNet taxonomy.
Predicate-argument co-occurrencefrequency is estimated by counting the number of times the concept correspondingto the argument is observed to co-occur with the predicate in the corpus.
Becausea given word is not always represented by a single class in the taxonomy (i.e., theargument co-occurring with a predicate can generally be the realization of one ofseveral conceptual classes), Lapata, Keller, and McDonald (2001) constructed the fre-quency counts for a predicate-argument bigram for each conceptual class by dividingthe contribution from the argument by the number of classes to which it belongs.They demonstrate that the counts re-created using this smoothing technique correlatesignificantly with plausibility judgments for adjective-noun bigrams.
They also showthat this class-based approach outperforms distance-weighted averaging (Dagan, Lee,and Pereira 1999), a smoothing method that re-creates unseen word co-occurrences onthe basis of distributional similarity (without relying on a predefined taxonomy), inpredicting plausibility.In the current study, we used the smoothing technique of Lapata, Keller, andMcDonald (2001) to re-create not only adjective-noun bigrams, but also noun-noun475Keller and Lapata Web Frequencies for Unseen BigramsTable 12Correlation of counts re-created using class-based smoothing with Web counts.Adjective-Noun Noun-Noun Verb-ObjectSeen BigramsAltaVista .344** .362** .361**Google .330** .343** .349**Unseen BigramsAltaVista .439** .386** .412**Google .444** .421** .397***p < .05 (one-tailed).
**p < .01 (one-tailed).and verb-object bigrams.
As already mentioned in Section 2.1, it was assumed that thenoun is the predicate in adjective-noun bigrams; for noun-noun bigrams, we treatedthe right noun as the predicate, and for verb-object bigrams, we treated the verb as thepredicate.
We applied Lapata, Keller, and McDonald?s (2001) technique to the unseenbigrams for all three bigram types.
We also used it on the seen bigrams, which wewere able to treat as unseen by removing all instances of the bigrams from the trainingcorpus.To test the claim that Web frequencies can be used to overcome data sparseness,we correlated the frequencies re-created using class-based smoothing on the BNC withthe frequencies obtained from the Web.
The correlation coefficients for both seen andunseen bigrams are shown in Table 12.
In all cases, a significant correlation betweenWeb counts and re-created counts is obtained.
For seen bigrams, the correlation coef-ficient ranged from .344 to .362 for AltaVista counts and from .330 to .349 for Googlecounts.
For unseen bigrams, the correlations were somewhat higher, ranging from .386to .439 for AltaVista counts and from .397 to .444 for Google counts.
For both seenand unseen bigrams, there was only a very small difference between the correlationcoefficients obtained with the two search engines.It is also interesting to compare the performance of class-based smoothing and Webcounts on the task of predicting plausibility judgments.
The correlation coefficients arelisted in Table 11.
The re-created frequencies are correlated significantly with all threetypes of bigrams, both for seen and unseen bigrams.
For the seen bigrams, we foundthat the correlation coefficients obtained using smoothed counts were significantlylower than the upper bound for all three types of bigrams (t(87) = 3.01, p < .01;t(87) = 3.23, p < .01; t(87) = 3.43, p < .01).
This result also held for the unseenbigrams: The correlations obtained using smoothing were significantly lower than theupper bound for all three types of bigrams (t(87) = 1.86, p < .05; t(87) = 1.97, p < .05;t(87) = 3.36, p < .01).Recall that the correlation coefficients obtained using the Web counts were notfound to be significantly different from the upper bound, which indicates that Webcounts are better predictors of plausibility than smoothed counts.
This fact was con-firmed by further significance testing: For seen bigrams, we found that the AltaVistacorrelation coefficients were significantly higher than correlation coefficients obtainedusing smoothing, for all three types of bigrams (t(87) = 3.31, p < .01; t(87) = 4.11,p < .01; t(87) = 4.32, p < .01).
This also held for Google counts (t(87) = 3.16, p < .01;t(87) = 4.02, p < .01; t(87) = 4.03, p < .01).
For unseen bigrams, the AltaVista coef-ficients and the coefficients obtained using smoothing were not significantly different476Computational Linguistics Volume 29, Number 3for adjective-noun bigrams, but the difference reached significance for noun-noun andverb-object bigrams (t(87) = 2.08, p < .05; t(87) = 2.53, p < .01).
For Google counts,the difference was again not significant for adjective-noun bigrams, but it reached sig-nificance for noun-noun and verb-object bigrams (t(87) = 2.34, p < .05; t(87) = 2.15,p < .05).Finally, we conducted a small study to investigate the validity of the counts thatwere re-created using class-based smoothing.
We correlated the re-created counts forthe seen bigrams with their actual BNC and NANTC frequencies.
The correlationcoefficients are reported in Tables 8 and 9.
We found that the correlation between re-created counts and corpus counts was significant for all three types of bigrams, forboth corpora.
This demonstrates that the smoothing technique we employed generatesrealistic corpus counts, in the sense that the re-created counts are correlated withthe actual counts.
However, the correlation coefficients obtained using Web countswere always substantially higher than those obtained using smoothed counts.
Thesedifferences were significant for the BNC counts for AltaVista (t(87) = 8.38, p < .01;t(87) = 5.00, p < .01; t(87) = 5.03, p < .01) and Google (t(87) = 8.35, p < .01;t(87) = 5.00, p < .01; t(87) = 5.03, p < .01).
They were also significant for the NANTCcounts for AltaVista (t(87) = 4.12, p < .01; t(87) = 3.72, p < .01; t(87) = 6.58, p < .01)and Google (t(87) = 4.08, p < .01; t(87) = 3.06, p < .01; t(87) = 6.47, p < .01).To summarize, the results presented in this section indicate that Web counts areindeed a valid way of obtaining counts for bigrams that are unseen in a given corpus:They correlate reliably with counts re-created using class-based smoothing.
For seenbigrams, we found that Web counts correlate with counts that were re-created usingsmoothing techniques (after removing the seen bigrams from the training corpus).
Forthe task of predicting plausibility judgments, we were able to show that Web countsoutperform re-created counts, both for seen and for unseen bigrams.
Finally, we foundthat Web counts for seen bigrams correlate better than re-created counts with the realcorpus counts.It is beyond the scope of the present study to undertake a full comparison betweenWeb counts and frequencies re-created using all available smoothing techniques (andall available taxonomies that might be used for class-based smoothing).
The smooth-ing method discussed above is simply one type of class-based smoothing.
Other, moresophisticated class-based methods do away with the simplifying assumption that theargument co-occurring with a given predicate (adjective, noun, verb) is distributedevenly across its conceptual classes and attempt to find the right level of generalizationin a concept hierarchy, by discounting, for example, the contribution of very generalclasses (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998).
Other smoothing ap-proaches such as discounting (Katz 1987) and distance-weighted averaging (Grishmanand Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word com-binations by exploiting only corpus-internal evidence, without relying on taxonomicinformation.
Our goal was to demonstrate that frequencies retrieved from the Webare a viable alternative to conventional smoothing methods when data are sparse; wedo not claim that our Web-based method is necessarily superior to smoothing or thatit should be generally preferred over smoothing methods.
However, the next sectionwill present a small-scale study that compares the performance of several smoothingtechniques with the performance of Web counts on a standard task from the literature.3.4 PseudodisambiguationIn the smoothing literature, re-created frequencies are typically evaluated using pseu-dodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999;Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al 1999).477Keller and Lapata Web Frequencies for Unseen BigramsThe aim of the pseudodisambiguation task is to decide whether a given algorithmre-creates frequencies that make it possible to distinguish between seen and unseenbigrams in a given corpus.
A set of pseudobigrams is constructed according to a setof criteria (detailed below) that ensure that they are unattested in the training corpus.Then the seen bigrams are removed from the training data, and the smoothing methodis used to re-create the frequencies of both the seen bigrams and the pseudobigrams.The smoothing method is then evaluated by comparing the frequencies it re-createsfor both types of bigrams.We evaluated our Web counts by applying the pseudodisambiguation procedurethat Rooth et al (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2001)employed for evaluating re-created verb-object bigram counts.
In this procedure, thenoun n from a verb-object bigram (v, n) that is seen in a given corpus is paired with arandomly chosen verb v?
that does not take n as its object within the corpus.
This resultsin an unseen verb-object bigram (v?, n).
The seen bigram is now treated as unseen (i.e.,all of its occurrences are removed from the training corpus), and the frequencies ofboth the seen and the unseen bigram are re-created (using smoothing, or Web counts,in our case).
The task is then to decide which of the two verbs v and v?
takes thenoun n as its object.
For this, the re-created bigram frequency is used: The bigramwith the higher re-created frequency (or probability) is taken to be the seen bigram.If this bigram is really the seen one, then the disambiguation is correct.
The overallpercentage of correct disambiguations is a measure of the quality of the re-createdfrequencies (or probabilities).
In the following, we will first describe in some detailthe experiments that Rooth et al (1999) and Clark and Weir (2001) conducted.
Wewill then discuss how we replicated their experiments using the Web as an alternativesmoothing method.Rooth et al (1999) used pseudodisambiguation to evaluate a class-based modelthat is derived from unlabeled data using the expectation maximization (EM) algo-rithm.
From a data set of 1,280,712 (v, n) pairs (obtained from the BNC using Carrolland Rooth?s [1998] parser), they randomly selected 3,000 pairs, with each pair con-taining a fairly frequent verb and noun (only verbs and nouns that occurred between30 and 3,000 times in the data were considered).
For each pair (v, n) a fairly frequentverb v?
was randomly chosen such that the pair (v?, n) did not occur in the data set.Given the set of (v, n, v?)
triples (a total of 1,337), the task was to decide whether (v, n)or (v?, n) was the correct (i.e., seen) pair by comparing the probabilities P(n|v) andP(n|v?).
The probabilities were re-created using Rooth et al?s (1999) EM-based clus-tering model on a training set from which all seen pairs (v, n) had been removed.
Anaccuracy of 80% on the pseudodisambiguation task was achieved (see Table 13).Prescher, Riezler, and Rooth (2000) evaluated Rooth et al?s (1999) EM-based clus-tering model again using pseudodisambiguation, but on a separate data set using aTable 13Percentage of correct disambiguations on the pseudodisambiguation task using Web countsand counts re-created using EM-based clustering (Rooth et al 1999).Data Set N AltaVista AltaVista Rooth et alConditional Probability Joint ProbabilitySubject 717 71.2 68.5 ?Objects 620 85.2 77.5 ?Subjects and objects 1,337 77.7 72.7 80.0478Computational Linguistics Volume 29, Number 3Table 14Percentage of correct disambiguations on the pseudodisambiguation task using Web countsand counts re-created using EM-based clustering (Prescher, Riezler, and Rooth 2000).Data Set N AltaVista AltaVista VA Model VO ModelConditional Probability Joint ProbabilitySubjects 159 66.7 59.1 ?
?Objects 139 70.5 66.2 ?
?Subjects and objects 298 68.5 62.4 79.0 88.3slightly different method for constructing the pseudobigrams.
They used a set of 298(v, n, n?)
BNC triples in which (v, n) was chosen as in Rooth et al (1999) but paired witha randomly chosen noun n?.
Given the set of (v, n, n?)
triples, the task was to decidewhether (v, n) or (v, n?)
was the correct pair in each triple.
Prescher, Riezler, and Rooth(2000) reported pseudodisambiguation results with two clustering models: (1) Roothet al?s (1999) clustering approach, which models the semantic fit between a verb andits argument (VA model), and (2) a refined version of this approach that models onlythe fit between a verb and its object (VO model), disregarding other arguments of theverb.
The results of the two models on the pseudodisambiguation task are shown inTable 14.At this point, it is important to note that neither Rooth et al (1999) nor Prescher,Riezler, and Rooth (2000) used pseudodisambiguation for the final evaluation of theirmodels.
Rather, the performance on the pseudodisambiguation task was used to op-timize the model parameters.
The results in Tables 13 and 14 show the pseudodisam-biguation performance achieved for the best parameter settings.
In other words, theseresults were obtained on the development set (i.e., on the same data set that was usedto optimize the parameters), not on a completely unseen test set.
This procedure iswell-justified in the context of Rooth et al?s (1999) and Prescher, Riezler, and Rooth?s(2000) work, which aimed at building models of lexical semantics, not of pseudodis-ambiguation.
Therefore, they carried out their final evaluations on unseen test sets forthe tasks of lexicon induction (Rooth et al 1999) and target language disambiguation(Prescher, Riezler, and Rooth 2000), once the model parameters had been fixed usingthe pseudodisambiguation development set.8Clark and Weir (2002) use a setting similar to that of Rooth et al (1999) andPrescher, Riezler, and Rooth (2000); here pseudodisambiguation is employed to evalu-ate the performance of a class-based probability estimation method.
In order to addressthe problem of estimating conditional probabilities in the face of sparse data, Clarkand Weir (2002) define probabilities in terms of classes in a semantic hierarchy andpropose hypothesis testing as a means of determining a suitable level of generalizationin the hierarchy.
Clark and Weir (2002) report pseudodisambiguation results on twodata sets, with an experimental setup similar to that of Rooth et al (1999).
For the firstdata set, 3,000 pairs were randomly chosen from 1.3 million (v, n) tuples extractedfrom the BNC (using the parser of Briscoe and Carroll [1997]).
The selected pairs con-8 Stefan Riezler (personal communication, 2003) points out that the main variance in Rooth et al?s (1999)pseudodisambiguation results comes from the class cardinality parameter (start values account for only2% of the performance, and iterations do not seem to make a difference at all).
Figure 3 of Rooth et al(1999) shows that a performance of more than 75% is obtained for every reasonable choice of classes.This indicates that a ?proper?
pseudodisambiguation setting with separate development and test datawould have resulted in a similar choice of class cardinality and thus achieved the same 80%performance that is cited in Table 13.479Keller and Lapata Web Frequencies for Unseen BigramsTable 15Percentage of correct disambiguations on the pseudodisambiguation task using Web countsand counts re-created using class-based smoothing (Clark and Weir 2002).Data Set N AltaVista AltaVista Clark and Li and ResnikConditional Joint Probability Weir AbeProbabilityObjects (low frequency) 3000 83.9 81.1 72.4 62.9 62.6Objects (high frequency) 3000 87.7 85.3 73.9 68.3 63.9tained relatively frequent verbs (occurring between 500 and 5,000 times in the data).The data sets were constructed as proposed by Rooth et al (1999).
The procedure forcreating the second data set was identical, but this time only verbs that occurred be-tween 100 and 1,000 times were considered.
Clark and Weir (2002) further comparedtheir approach with Resnik?s (1993) selectional association model and Li and Abe?s(1998) tree cut model on the same data sets.
These methods are directly comparable,as they can be used for class-based probability estimation and address the questionof how to find a suitable level of generalization in a hierarchy (i.e., WordNet).
Theresults of the three methods used on the two data sets are shown in Table 15.We employed the same pseudodisambiguation method to test whether Web-basedfrequencies can be used for distinguishing between seen and artificially constructedunseen bigrams.
We obtained the data sets of Rooth et al (1999), Prescher, Riezler,and Rooth (2000), and Clark and Weir (2002) described above.
Given a set of (v, n, v?
)triples, the task was to decide whether (v, n) or (v?, n) was the correct pair.
We obtainedAltaVista counts for f (v, n), f (v?, n), f (v), and f (v?)
as described in Section 2.3.9 Thenwe used two models for pseudodisambiguation: the joint probability model comparedthe joint probability estimates f (v, n) and f (v?, n) and predicted that the bigram withthe highest estimate is the seen one.
The conditional probability model compared theconditional probability estimates f (v, n)/f (v) and f (v?, n)/f (v?)
and again selected asthe seen bigram the one with the highest estimate (in both cases, ties were resolvedby choosing at random).10 The same two models were used to perform pseudodisam-biguation for the (v, n, n?)
triples, where we have to choose between (v, n) and (v, n?
).Here, the probability estimates f (v, n) and f (v, n?)
were used for the joint probabilitymodel, and f (v, n)/f (n) and f (v, n?
)/f (n?)
for the conditional probability model.The results for Rooth et al?s (1999) data set are given in Table 13.
The conditionalprobability model achieves a performance of 71.2% correct for subjects and 85.2% cor-rect for objects.
The performance on the whole data set is 77.7%, which is below theperformance of 80.0% reported by Rooth et al (1999).
However, the difference is notfound to be significant using a chi-square test comparing the number of correct andincorrect classifications (?2(1) = 2.02, p = .16).
The joint probability model performsconsistently worse than the conditional probability model: It achieves an overall accu-racy of 72.7%, which is significantly lower than the accuracy of the Rooth et al (1999)model (?2(1) = 19.50, p < .01).9 We used only AltaVista counts, as there was virtually no difference between AltaVista and Googlecounts in our previous evaluations (see Sections 3.1?3.3).
Google allows only 1,000 queries per day (forregistered users), which makes it time-consuming to obtain large numbers of Google counts.
AltaVistahas no such restriction.10 The probability estimates are P(a, b) = f (a, b)/N and P(a|b) = f (a, b)/f (a) for the joint probability andthe conditional probability, respectively.
However, the corpus size N can be ignored, as it is constant.480Computational Linguistics Volume 29, Number 3A similar picture emerges with regard to Prescher, Riezler, and Rooth?s (2000) dataset (see Table 14).
The conditional probability model achieves an accuracy of 66.7%for subjects and 70.5% for objects.
The combined performance of 68.5% is significantlylower than the performance of both the VA model (?2(1) = 7.78, p < .01) and theVO model (?2(1) = 33.28, p < .01) reported by Prescher, Riezler, and Rooth (2000).Again, the joint probability model performs worse than the conditional probabilitymodel, achieving an overall accuracy of 62.4%.We also applied our Web-based method to the pseudodisambiguation data set ofClark and Weir (2002).
Here, the conditional probability model reached a performanceof 83.9% correct on the low-frequency data set.
This is significantly higher than thehighest performance of 72.4% reported by Clark and Weir (2002) on the same dataset (?2(1) = 115.50, p < .01).
The joint probability model performs worse than theconditional model, at 81.1%.
However, this is still significantly better than the bestresult of Clark and Weir (2002) (?2(1) = 63.14, p < .01).
The same pattern is observedfor the high-frequency data set, on which the conditional probability model achieves87.7% correct and thus significantly outperforms Clark and Weir (2002), who obtained73.9% (?2(1) = 283.73, p < .01).
The joint probability model achieved 85.3% on this dataset, also significantly outperforming Clark and Weir (2002) (?2(1) = 119.35, p < .01).To summarize, we demonstrated that the simple Web-based approach proposedin this article yields results for pseudodisambiguation that outperform class-basedsmoothing techniques, such as the ones proposed by Resnik (1993), Li and Abe (1998),and Clark and Weir (2002).
We were also able to show that a Web-based approach isable to achieve the same performance as an EM-based smoothing model proposed byRooth et al (1999).
However, the Web-based approach was not able to outperform themore sophisticated EM-based model of Prescher, Riezler, and Rooth (2000).
Anotherresult we obtained is that Web-based models that use conditional probabilities (whereunigram frequencies are used to normalize the bigram frequencies) generally outper-form a more simple-minded approach that relies directly on bigram frequencies forpseudodisambiguation.There are a number of reasons why our results regarding pseudodisambiguationhave to be treated with some caution.
First of all, the two smoothing methods (i.e.,EM-based clustering and class-based probability estimation using WordNet) were notevaluated on the same data set, and therefore the two results are not directly compa-rable.
For instance, Clark and Weir?s (2002) data set is substantially less noisy thanRooth et al?s (1999) and Prescher, Riezler, and Rooth?s (2000), as it contains only wordsand nouns that occur in WordNet.
Furthermore, Stephen Clark (personal communica-tion, 2003) points out that WordNet-based approaches are at a disadvantage when itcomes to pseudodisambiguation.
Pseudodisambiguation assumes that the correct pairis unseen in the training data; this makes the task deliberately hard, because some ofthe pairs might be frequent enough that reliable corpus counts can be obtained with-out having to use WordNet (using WordNet is likely to be more noisy than using theactual counts).
Another problem with WordNet-based approaches is that they offerno systematic treatment of word sense ambiguity, which puts them at a disadvan-tage with respect to approaches that do not rely on a predefined inventory of wordsenses.Finally, recall that the results for the EM-based approaches in Tables 13 and 14 wereobtained on the development set (as pseudodisambiguation was used as a means ofparameter tuning by Rooth et al [1999] and Prescher, Riezler, and Rooth [2000]).
It ispossible that this fact inflates the performance values for the EM-based approaches(but see note 8).481Keller and Lapata Web Frequencies for Unseen Bigrams4.
ConclusionsThis article explored a novel approach to overcoming data sparseness.
If a bigramis unseen in a given corpus, conventional approaches re-create its frequency usingtechniques such as back-off, linear interpolation, class-based smoothing or distance-weighted averaging (see Dagan, Lee, and Pereira [1999] and Lee [1999] for overviews).The approach proposed here does not re-create the missing counts but instead retrievesthem from a corpus that is much larger (but also much more noisy) than any existingcorpus: it launches queries to a search engine in order to determine how often thebigram occurs on the Web.We systematically investigated the validity of this approach by using it to obtainfrequencies for predicate-argument bigrams (adjective-noun, noun-noun, and verb-object bigrams).
We first applied the approach to seen bigrams randomly sampled fromthe BNC.
We found that the counts obtained from the Web are highly correlated withthe counts obtained from the BNC.
We then obtained bigram counts from NANTC,a corpus that is substantially larger than the BNC.
Again, we found that Web countsare highly correlated with corpus counts.
This indicates that Web queries can generatefrequencies that are comparable to the ones obtained from a balanced, carefully editedcorpus such as the BNC, but also from a large news text corpus such as NANTC.Secondly, we performed an evaluation that used the Web frequencies to predicthuman plausibility judgments for predicate-argument bigrams.
The results show thatWeb counts correlate reliably with judgments, for all three types of predicate-argumentbigrams tested, both seen and unseen.
For the seen bigrams, we showed that the Webfrequencies correlate better with judged plausibility than corpus frequencies.To substantiate the claim that the Web counts can be used to overcome data sparse-ness, we compared bigram counts obtained from the Web with bigram counts re-created using a class-based smoothing technique (a variant of the one proposed byResnik [1993]).
We found that Web frequencies and re-created frequencies are reliablycorrelated, and that Web frequencies are better at predicting plausibility judgmentsthan smoothed frequencies.
This holds both for unseen bigrams and for seen bigramsthat are treated as unseen by omitting them from the training corpus.Finally, we tested the performance of our frequencies in a standard pseudodis-ambiguation task.
We applied our method to three data sets from the literature.
Theresults show that Web counts outperform counts re-created using a number of class-based smoothing techniques.
However, counts re-created using an EM-based smooth-ing approach yielded better pseudodisambiguation performance than Web counts.To summarize, we have proposed a simple heuristic method for obtaining bigramcounts from the Web.
Using four different types of evaluation, we demonstrated thatthis simple heuristic method is sufficient to obtain valid frequency estimates.
It seemsthat the large amount of data available outweighs the problems associated with usingthe Web as a corpus (such as the fact that it is noisy and unbalanced).A number of questions arise for future research: (1) Are Web frequencies suitablefor probabilistic modeling, in particular since Web counts are not perfectly normalized,as Zhu and Rosenfeld (2001) have shown?
(2) How can existing smoothing methodsbenefit from Web counts?
(3) How do the results reported in this article carry overto languages other than English (for which a much smaller amount of Web data areavailable)?
(4) What is the effect of the noise introduced by our heuristic approach?The last question could be assessed by reproducing our results using a snapshot ofthe Web, from which argument relations can be extracted more accurately using POStagging and chunking techniques.482Computational Linguistics Volume 29, Number 3Finally, it will be crucial to test the usefulness of Web-based frequencies for realisticNLP tasks.
Preliminary results are reported by Lapata and Keller (2003), who use Webcounts successfully for a range of NLP tasks, including candidate selection for ma-chine translation, context-sensitive spelling correction, bracketing and interpretationof compounds, adjective ordering, and PP attachment.AcknowledgmentsThis work was conducted while bothauthors were at the Department ofComputational Linguistics, SaarlandUniversity, Saarbru?cken.
The work wasinspired by a talk that Gregory Grefenstettegave in Saarbru?cken in 2001 about hisresearch on using the Web as a corpus.
Thepresent article is an extended and revisedversion of Keller, Lapata, and Ourioupina(2002).
Stephen Clark and Stefan Riezlerprovided valuable comments on thisresearch.
We are also grateful to fouranonymous reviewers for ComputationalLinguistics; their feedback helped tosubstantially improve the present article.Special thanks are due to Stephen Clark andDetlef Prescher for making theirpseudodisambiguation data sets available.ReferencesAbney, Steve.
1996.
Partial parsing viafinite-state cascades.
In John Carroll,editor, Workshop on Robust Parsing EighthEuropean Summer School in Logic, Languageand Information, pages 8?15, Prague, CzechRepublic.Abney, Steve and Marc Light.
1999.
Hidinga semantic class hierarchy in a Markovmodel.
In Andrew Kehler and AndreasStolcke, editors, Proceedings of the ACLWorkshop on Unsupervised Learning inNatural Language Processing, pages 1?8,College Park, MD.Agirre, Eneko and David Martinez.
2000.Exploring automatic word sensedisambiguation with decision lists and theWeb.
In Proceedings of the 18th InternationalConference on Computational Linguistics,pages 11?19, Saarbru?cken, Germany.Banko, Michele and Eric Brill.
2001a.Mitigating the paucity-of-data problem:Exploring the effect of training corpussize on classifier performance for naturallanguage processing.
In James Allan,editor, Proceedings of the First InternationalConference on Human Language TechnologyResearch.
Morgan Kaufmann, SanFrancisco.Banko, Michele and Eric Brill.
2001b.
Scalingto very very large corpora for naturallanguage disambiguation.
In Proceedings ofthe 39th Annual Meeting of the Association forComputational Linguistics and the10th Conference of the European Chapter of theAssociation for Computational Linguistics,pages 26?33, Toulouse, France.Bard, Ellen Gurman, Dan Robertson, andAntonella Sorace.
1996.
Magnitudeestimation of linguistic acceptability.Language, 72(1):32?68.Briscoe, Ted and John Carroll.
1997.Automatic extraction of subcategorizationfrom corpora.
In Proceedings of the FifthConference on Applied Natural LanguageProcessing, pages 356?363, Washington,DC.Burnard, Lou, editor, 1995.
Users ReferenceGuide, British National Corpus.
BritishNational Corpus Consortium, OxfordUniversity Computing Services, Oxford,England.Carroll, Glenn and Mats Rooth.
1998.Valence induction with a head-lexicalizedPCFG.
In Nancy Ide and Atro Voutilainen,editors, Proceedings of the Third Conferenceon Empirical Methods in Natural LanguageProcessing, pages 36?45, Granada, Spain.Clark, Stephen and David Weir.
2001.Class-based probability estimation using asemantic hierarchy.
In Proceedings of theSecond Conference of the North AmericanChapter of the Association for ComputationalLinguistics, pages 95?102, Pittsburgh, PA.Clark, Stephen and David Weir.
2002.Class-based probability estimation using asemantic hierarchy.
ComputationalLinguistics, 28(2):187?206.Corley, Martin and Christoph Scheepers.2002.
Syntactic priming in Englishsentence production: Categorical andlatency evidence from an Internet-basedstudy.
Psychonomic Bulletin and Review,9(1):126?131.Corley, Steffan, Martin Corley, Frank Keller,Matthew W. Crocker, and Shari Trewin.2001.
Finding syntactic structure inunparsed corpora: The Gsearch corpusquery system.
Computers and theHumanities, 35(2):81?94.Cowart, Wayne.
1997.
Experimental Syntax:Applying Objective Methods to SentenceJudgments.
Sage, Thousand Oaks, CA.Curran, James.
2002.
Scaling context space.In Proceedings of the 40th Annual Meeting of483Keller and Lapata Web Frequencies for Unseen Bigramsthe Association for Computational Linguistics,pages 231?238, Philadelphia.Dagan, Ido, Lillian Lee, and FernandoPereira.
1999.
Similarity-based models ofword cooccurrence probabilities.
MachineLearning, 34(1):43?69.Grefenstette, Gregory.
1994.
Explorations inAutomatic Thesaurus Discovery.
KluwerAcademic, Boston.Grefenstette, Gregory.
1998.
The WorldWide Web as a resource forexample-based machine translation tasks.In Proceedings of the ASLIB Conference onTranslating and the Computer, London.Grefenstette, Gregory and Jean Nioche.2000.
Estimation of English andnon-English language use on the WWW.In Proceedings of the RIAO Conference onContent-Based Multimedia InformationAccess, pages 237?246, Paris.Grishman, Ralph and John Sterling.
1994.Generalizing automatically generatedselectional patterns.
In Proceedings of the15th International Conference onComputational Linguistics, pages 742?747,Kyoto, Japan.Hindle, Donald and Mats Rooth.
1993.Structural ambiguity and lexical relations.Computational Linguistics, 19(1):103?120.Jones, Rosie and Rayid Ghani.
2000.Automatically building a corpus for aminority language from the Web.
InProceedings of the Student Research Workshopat the 38th Annual Meeting of the Associationfor Computational Linguistics, pages 29?36,Hong Kong.Karp, Daniel, Yves Schabes, Martin Zaidel,and Dania Egedi.
1992.
A freely availablewide coverage morphological analyzer forEnglish.
In Proceedings of the 14thInternational Conference on ComputationalLinguistics, pages 950?954, Nantes, France.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on AcousticsSpeech and Signal Processing, 33(3):400?401.Keller, Frank and Theodora Alexopoulou.2001.
Phonology competes with syntax:Experimental evidence for the interactionof word order and accent placement inthe realization of information structure.Cognition, 79(3):301?372.Keller, Frank and Ash Asudeh.
2001.Constraints on linguistic coreference:Structural vs. pragmatic factors.
InJohanna D. Moore and Keith Stenning,editors, Proceedings of the 23rd AnnualConference of the Cognitive Science Society,pages 483?488.
Erlbaum, Mahwah, NJ.Keller, Frank, Martin Corley, Steffan Corley,Lars Konieczny, and Amalia Todirascu.1998.
WebExp: A Java toolbox forWeb-based psychological experiments.Technical Report HCRC/TR-99, HumanCommunication Research Centre,University of Edinburgh.Keller, Frank, Maria Lapata, and OlgaOurioupina.
2002.
Using the Web toovercome data sparseness.
In Jan Hajic?and Yuji Matsumoto, editors, Proceedingsof the Conference on Empirical Methods inNatural Language Processing, pages230?237, Philadelphia.Lapata, Maria.
2001.
A corpus-basedaccount of regular polysemy: The case ofcontext-sensitive adjectives.
In Proceedingsof the Second Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 63?70,Pittsburgh, PA.Lapata, Maria.
2002.
The disambiguation ofnominalizations.
Computational Linguistics,28(3):357?388.Lapata, Maria and Frank Keller.
2003.Evaluating the performance ofunsupervised Web-based models for arange of NLP tasks.
Unpublishedmanuscript, University of Sheffield,Sheffield, England, and University ofEdinburgh, Edinburgh, Scotland.Lapata, Maria, Frank Keller, and ScottMcDonald.
2001.
Evaluating smoothingalgorithms against plausibility judgments.In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguisticsand the 10th Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 346?353, Toulouse,France.Lapata, Maria, Scott McDonald, and FrankKeller.
1999.
Determinants ofadjective-noun plausibility.
In Proceedingsof the Ninth Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 30?36, Bergen, Norway.Lauer, Mark.
1995.
Designing StatisticalLanguage Learners: Experiments onCompound Nouns.
Ph.D. thesis, MacquarieUniversity, Sydney, Australia.Lee, Lilian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics, pages 25?32, College Park,MD.Leech, Geoffrey, Roger Garside, and MichaelBryant.
1994.
The tagging of the Britishnational corpus.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics, pages 622?628, Kyoto, Japan.Levi, Judith N. 1978.
The Syntax andSemantics of Complex Nominals.
Academic484Computational Linguistics Volume 29, Number 3Press, New York.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217?244.Lin, Dekang.
1994.
PRINCIPAR?Anefficient broad-coverage, principle-basedparser.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics, pages 482?488, Kyoto, Japan.Lin, Dekang.
1998.
Dependency-basedevaluation of MINIPAR.
In Proceedings ofthe LREC Workshop on the Evaluation ofParsing Systems, pages 48?56, Granada,Spain.Lin, Dekang.
2001.
LaTaT: Language andtext analysis tools.
In Proceedings of theFirst International Conference on HumanLanguage Technology Research.
MorganKaufmann, San Francisco.Markert, Katja, Malvina Nissim, andNatalia N. Modjeska.
2003.
Using the Webfor nominal anaphora resolution.
InProceedings of the EACL Workshop on theComputational Treatment of Anaphora,Budapest, pages 39?46.McCarthy, Diana.
2000.
Using semanticpreferences to identify verbalparticipation in role switchingalternations.
In Proceedings of the FirstConference of the North American Chapter ofthe Association for Computational Linguistics,pages 256?263, Seattle, WA.Mihalcea, Rada and Dan Moldovan.
1999.
Amethod for word sense disambiguation ofunrestricted text.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics, pages 152?158,College Park, MD.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine J. Miller.
1990.
Introduction toWordNet: An on-line lexical database.International Journal of Lexicography,3(4):235?244.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributional clusteringof English words.
In Proceedings of the 31stAnnual Meeting of the Association forComputational Linguistics, pages 183?190,Columbus, OH.Pollard, Carl and Ivan A.
Sag.
1994.Head-Driven Phrase Structure Grammar.University of Chicago Press, Chicago.Prescher, Detlef, Stefan Riezler, and MatsRooth.
2000.
Using a probabilisticclass-based lexicon for lexical ambiguityresolution.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics, pages 649?655, Saarbru?cken,Germany.Ratnaparkhi, Adwait.
1998.
Unsupervisedstatistical models for prepositional phraseattachment.
In Proceedings of the17th International Conference onComputational Linguistics and 36th AnnualMeeting of the Association for ComputationalLinguistics, pages 1079?1085, Montre?al.Resnik, Philip.
1993.
Selection andInformation: A Class-Based Approach toLexical Relationships.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Resnik, Philip.
1999.
Mining the Web forbilingual text.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics, pages 527?534,College Park, MD.Resnik, Philip.
2000.
Measuring verbsimilarity.
In Lila R. Gleitman andAravid K. Joshi, editors, Proceedings of the22nd Annual Conference of the CognitiveScience Society, pages 399?404.
Erlbaum,Mahwah, NJ.Rooth, Mats, Stefan Riezler, Detlef Prescher,Glenn Carroll, and Franz Beil.
1999.Inducing a semantically annotated lexiconvia EM-based clustering.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics, pages 104?111,College Park, MD.Sampson, Geoffrey.
1995.
English for theComputer: The SUSANNE Corpus andAnalytic Scheme.
Oxford University Press,Oxford.Schu?tze, Carson T. 1996.
The Empirical Base ofLinguistics: Grammaticality Judgments andLinguistic Methodology.
University ofChicago Press, Chicago.Stevens, S. S. 1975.
Psychophysics:Introduction to its Perceptual, Neural, andSocial Prospects.
John Wiley, New York.Volk, Martin.
2001.
Exploiting the WWW asa corpus to resolve PP attachmentambiguities.
In Paul Rayson, AndrewWilson, Tony McEnery, Andrew Hardie,and Shereen Khoja, editors, Proceedings ofthe Corpus Linguistics Conference, pages601?606, Lancaster, England.Weiss, Sholom M. and Casimir A.Kulikowski.
1991.
Computer Systems ThatLearn: Classification and Prediction Methodsfrom Statistics, Neural Nets, MachineLearning, and Expert Systems.
MorganKaufmann, San Mateo, CA.Zhu, Xiaojin and Ronald Rosenfeld.
2001.Improving trigram language modelingwith the World Wide Web.
In Proceedingsof the International Conference on AcousticsSpeech and Signal Processing, Salt Lake City,UT.
