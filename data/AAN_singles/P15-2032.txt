Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192?198,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsPre-training of Hidden-Unit CRFsYoung-Bum Kim?Karl Stratos?Ruhi Sarikaya?
?Microsoft Corporation, Redmond, WA?Columbia University, New York, NY{ybkim, ruhi.sarikaya}@microsoft.comstratos@cs.columbia.eduAbstractIn this paper, we apply the concept of pre-training to hidden-unit conditional ran-dom fields (HUCRFs) to enable learningon unlabeled data.
We present a simpleyet effective pre-training technique thatlearns to associate words with their clus-ters, which are obtained in an unsuper-vised manner.
The learned parameters arethen used to initialize the supervised learn-ing process.
We also propose a word clus-tering technique based on canonical corre-lation analysis (CCA) that is sensitive tomultiple word senses, to further improvethe accuracy within the proposed frame-work.
We report consistent gains overstandard conditional random fields (CRFs)and HUCRFs without pre-training in se-mantic tagging, named entity recognition(NER), and part-of-speech (POS) taggingtasks, which could indicate the task inde-pendent nature of the proposed technique.1 IntroductionDespite the recent accuracy gains of the deeplearning techniques for sequence tagging prob-lems (Collobert and Weston, 2008; Collobert etal., 2011; Mohamed et al, 2010; Deoras et al,2012; Xu and Sarikaya, 2013; Yao et al, 2013;Mesnil et al, 2013; Wang and Manning, 2013;Devlin et al, 2014), conditional random fields(CRFs) (Lafferty et al, 2001; Sutton and McCal-lum, 2006) still have been widely used in manyresearch and production systems for the problemsdue to the effectiveness and simplicity of train-ing, which does not involve task specific param-eter tuning (Collins, 2002; McCallum and Li,2003; Sha and Pereira, 2003; Turian et al, 2010;Kim and Snyder, 2012; Celikyilmaz et al, 2013;Sarikaya et al, 2014; Anastasakos et al, 2014;Kim et al, 2014; Kim et al, 2015a; Kim et al,2015c; Kim et al, 2015b).
The objective functionfor CRF training operates globally over sequencestructures and can incorporate arbitrary features.Furthermore, this objective is convex and can beoptimized relatively efficiently using dynamic pro-gramming.Pre-training has been widely used in deep learn-ing (Hinton et al, 2006) and is one of the distin-guishing advantages of deep learning models.
Thebest results obtained across a wide range of tasksinvolve unsupervised pre-training phase followedby the supervised training phase.
The empiricalresults (Erhan et al, 2010) suggest that unsuper-vised pre-training has the regularization effect onthe learning process and also results in a modelparameter configuration that places the model nearthe basins of attraction of minima that support bet-ter generalization.While pre-training became a standard steps inmany deep learning model training recipes, it hasnot been applied to the family of CRFs.
Therewere several reasons for that; (i) the shallow andlinear nature of basic CRF model topology, whichlimits their expressiveness to the inner product be-tween data and model parameters, and (ii) Lackof a training criterion and configuration to employpre-training on unlabeled data in a task indepen-dent way.Hidden-unit CRFs (HUCRFs) of Maaten et al(2011) provide a deeper model topology and im-prove the expressive power of the CRFs but itdoes not address how to train them in a task inde-pendent way using unlabeled data.
In this paper,we present an effective technique for pre-trainingof HUCRFs that can potentially lead to accuracygains over HUCRF and basic linear chain CRFmodels.
We cluster words in the text and treat clus-ters as pseudo-labels to train an HUCRF.
Then wetransfer the parameters corresponding to observa-tions to initialize the training process on labeled192Figure 1: Graphical representation of hidden unitCRFs.data.
The intuition behind this is that words thatare clustered together tend to assume the same la-bels.
Therefore, learning the model parameters toassign the correct cluster ID to each word shouldaccrue to assigning the correct task specific labelduring supervised learning.This pre-training step significantly reduces thechallenges in training a high-performance HUCRFby (i) acquiring a broad feature coverage from un-labeled data and thus improving the generalizationof the model to unseen events, (ii) finding a good ainitialization point for the model parameters, and(iii) regularizing the parameter learning by min-imizing variance and introducing a bias towardsconfigurations of the parameter space that are use-ful for unsupervised learning.We also propose a word clustering techniquebased on canonical correlation analysis (CCA)that is sensitive to multiple word senses.
For ex-ample, the resulting clusters can differentiate theinstance of ?bank?
in the sense of financial insti-tutions and the land alongside the river.
This is animportant point as different senses of a word arelikely to have a different task specific tag.
Puttingthem in different clusters would enable the HU-CRF model to learn the distinction in terms of la-bel assignment.2 Model2.1 HUCRF definitionA HUCRF incorporates a layer of binary-valuedhidden units z = z1.
.
.
zn?
{0, 1} for each pairof observation sequence x = x1.
.
.
xnand labelsequence y = y1.
.
.
yn.
It is parameterized byFigure 2: Illustration of a pre-training scheme forHUCRFs.?
?
Rdand ?
?
Rd?and defines a joint probabilityof y and z conditioned on x as follows:p?,?
(y, z|x) =exp(?>?
(x, z) + ?>?
(z, y))?z??{0,1}ny??Y(x,z?)exp(?>?
(x, z?)
+ ?>?
(z?, y?
))where Y(x, z) is the set of all possible labelsequences for x and z, and ?
(x, z) ?
Rdand ?
(z, y) ?
Rd?are global feature func-tions that decompose into local featurefunctions: ?
(x, z) =?nj=1?
(x, j, zj) and?
(z, y) =?nj=1?
(zj, yj?1, yj).HUCRF forces the interaction between the ob-servations and the labels at each position j to gothrough a latent variable zj: see Figure 1 for illus-tration.
Then the probability of labels y is givenby marginalizing over the hidden units,p?,?
(y|x) =?z?{0,1}np?,?
(y, z|x)As in restricted Boltzmann machines (Larochelleand Bengio, 2008), hidden units are conditionallyindependent given observations and labels.
Thisallows for efficient inference with HUCRFs de-spite their richness (see Maaten et al (2011) fordetails).
We use a perceptron-style algorithm ofMaaten et al (2011) for training HUCRFs.2.2 Pre-training HUCRFsHow parameters are initialized for training is im-portant for HUCRFs because the objective func-tion is non-convex.
Instead of random initializa-tion, we use a simple and effective initializationscheme (in a similar spirit to the pre-training meth-ods in neural networks) that can leverage a large193body of unlabeled data.
This scheme is a simpletwo-step approach.In the first step, we cluster observed tokens inM unlabeled sequences and treat the clusters as la-bels to train an intermediate HUCRF.
Let C(u(i))be the ?cluster sequence?
of the i-th unlabeled se-quence u(i).
We compute:(?1, ?1) ?
arg max?,?M?i=1log p?,?
(C(u(i))|u(i)))In the second step, we train a final model on thelabeled data {(x(i), y(i))}Ni=1using ?1as an ini-tialization point:(?2, ?2) ?
arg max?,?
:init(?,?1)N?i=1log p?,?
(y(i)|x(i))While we can use ?1for initialization as well, wechoose to only use ?1since the label space is task-specific.
This process is illustrated in Figure 2.In summary, the first step is used to findgeneric parameters between observations and hid-den states; the second step is used to specialize theparameters to a particular task.
Note that the firststep also generates additional feature types absentin the labeled data which can be useful at test time.3 Multi-Sense Clustering via CCAThe proposed pre-training method requires assign-ing a cluster to each word in unlabeled text.
Sinceit learns to associate the words to their clusters, thequality of clusters becomes important.
A straight-forward approach would be to perform Brownclustering (Brown et al, 1992), which has beenvery effective in a variety of NLP tasks (Miller etal., 2004; Koo et al, 2008).However, Brown clustering has some undesir-able aspects for our purpose.
First, it assigns asingle cluster to each word type.
Thus a word thatcan be used very differently depending on its con-text (e.g., ?bank?)
is treated the same across thecorpus.
Second, the Brown model uses only un-igram and bigram statistics; this can be an issueif we wish to capture semantics in larger contexts.Finally, the algorithm is rather slow in practice forlarge vocabulary size.To mitigate these limitations, we propose multi-sense clustering via canonical correlation analy-sis (CCA).
While there are previous work on in-ducing multi-sense representations (Reisinger andCCA-PROJInput: samples (x(1), y(1)) .
.
.
(x(n), y(n)) ?
{0, 1}d?
{0, 1}d?, dimension kOutput: projections A ?
Rd?kand B ?
Rd??k?
Calculate B ?
Rd?d?, u ?
Rd, and v ?
Rd?
:Bi,j=n?l=1[[x(l)i= 1]][[y(l)j= 1]]ui=n?l=1[[x(l)i= 1]] vi=n?l=1[[y(l)i= 1]]?
Define??
= diag(u)?1/2Bdiag(v)?1/2.?
Calculate rank-k SVD??.
Let U ?
Rd?k(V ?
Rd?
?k)be a matrix of the left (right) singular vector corre-sponding to the largest k singular values.?
Let A = diag(u)?1/2U and B = diag(v)?1/2V .Figure 3: Algorithm for deriving CCA projectionsfrom samples of two variables.Mooney, 2010; Huang et al, 2012; Neelakantan etal., 2014), our proposed method is simpler and isshown to perform better in experiments.3.1 Review of CCACCA is a general technique that operates on apair of multi-dimensional variables.
CCA findsk dimensions (k is a parameter to be specified)in which these variables are maximally correlated.Let x(1).
.
.
x(n)?
Rdand y(1).
.
.
y(n)?
Rd?ben samples of the two variables.
For simplicity, as-sume that these variables have zero mean.
ThenCCA computes the following for i = 1 .
.
.
k:arg maxai?Rd, bi?Rd?
:a>iai?=0 ?i?<ib>ibi?=0 ?i?<i?nl=1(a>ix(l))(b>iy(l))??nl=1(a>ix(l))2?
?nl=1(b>iy(l))2In other words, each (ai, bi) is a pair of pro-jection vectors such that the correlation betweenthe projected variables a>ix(l)and b>iy(l)(nowscalars) is maximized, under the constraint thatthis projection is uncorrelated with the previousi ?
1 projections.
A method based on singu-lar value decomposition (SVD) provides an effi-cient and exact solution to this problem (Hotelling,1936).
The resulting solution A ?
Rd?k(whosei-th column is ai) and B ?
Rd?
?k(whose i-th col-umn is bi) can be used to project the variables from194Input: word-context pairs from a corpus of length n:D = {(w(l), c(l))}nl=1, dimension kOutput: cluster C(l) ?
k for l = 1 .
.
.
n?
Use the algorithm in Figure 3 to compute projectionmatrices (?W,?C) = CCA-PROJ(D, k).?
For each word type w, perform k-means clustering onCw= {?>Cc(l)?
Rk: w(l)= w} to partition occur-rences of w in the corpus into at most k clusters.?
Label each word w(l)with the cluster obtained fromthe previous step.
Let?D = {(w?
(l), c?
(l))}nl=1denotethis new dataset.?
(??W,?
?C) = CCA-PROJ(?D, k)?
Perform k-means clustering on {?>?Ww?(l)?
Rk}.?
Let C(l) be the cluster corresponding to Pi>?Wv(l).Figure 4: Algorithm for clustering of words in acorpus sensitive to multiple word senses.the original d- and d?-dimensional spaces to a k-dimensional space:x ?
Rd??
A>x ?
Rky ?
Rd???
B>y ?
RkThe new k-dimensional representation of eachvariable now contains information about the othervariable.
The value of k is usually selected to bemuch smaller than d or d?, so the representationis typically also low-dimensional.
The CCA algo-rithm is given in Figure 3: we assume that samplesare 0-1 indicator vectors.
In practice, calculatingthe CCA projections is fast since there are manyefficient SVD implantations available.
Also, CCAcan incorporate arbitrary context definitions unlikethe Brown algorithm.3.2 Multi-sense clusteringCCA projections can be used to obtain vectorrepresentations for both words and contexts.
Ifwe wished for only single-sense clusters (akinto Brown clusters), we could simply perform k-means on word embeddings.However, we can exploit context embeddings toinfer word senses.
For each word type, we createa set of context embeddings corresponding to alloccurrences of that word type.
Then we clusterthese embeddings; we use an implementation ofk-means which automatically determines the num-ber of clusters upper bounded by k. The numberof word senses, k, is set to be the number of la-bel types occurring in labeled data (for each task-specific training set).We use the resulting context clusters to deter-mine the sense of each occurrence of that wordtype.
For instance, an occurrence of ?bank?
mightbe labeled as ?bank1?
near ?financial?
or ?Chase?and ?bank2?
near ?shore?
or ?edge?.This step is for disambiguating word senses, butwhat we need for our pre-training method is thepartition of words in the corpus.
Thus we performa second round of CCA on these disambiguatedwords to obtain corresponding word embeddings.As a final step, we perform k-means clustering onthe disambiguated word embeddings to obtain thepartition of words in the corpus.
The algorithm isshown in Table 4.4 ExperimentsTo validate the effectiveness of our pre-trainingmethod, we experiment on three sequence label-ing tasks: semantic tagging, named entity recogni-tion (NER), and part-of-speech (POS) tagging.
Weused L-BFGS for training CRFs1and the averagedperceptron for training HUCRFs.
The number ofhidden variables was set to 500.4.1 Semantic taggingThe goal of semantic tagging is to assign the cor-rect semantic tag to a words in a given utter-ance.
We use a training set of 50-100k queriesacross domains and the test set of 5-10k queries.For pre-training, we collected 100-200k unlabeledtext from search log data and performed a stan-dard preprocessing step.
We use n-gram featuresup to n = 3, regular expression features, do-main specific lexicon features and Brown clus-ters.
We present the results for various config-urations in Table 1.
HUCRF with random ini-tialization from Gaussian distribution (HUCRFG)boosts the average performance up to 90.52%(from 90.39% of CRF).
HUCRF with pre-trainingwith Brown clusters (HUCRFB) and CCA-basedclusters (HUCRFC) further improves performanceto 91.36% and 91.37%, respectively.Finally, when we use multi-sense cluster(HUCRFC+), we obtain an F1-score of 92.01%.We also compare other alternative pre-trainingmethods.
HUCRF with pre-training RBM1For CRFs, we found that L-BFGS had higher perfor-mance than SGD and the average percetpron.195alarm calendar comm.
note ondevice places reminder weather home avgCRF 92.8 89.59 92.13 88.02 88.21 89.64 87.72 96.93 88.51 90.39HUCRFG91.79 89.56 92.08 88.42 88.64 90.99 89.21 96.38 87.63 90.52HUCRFR91.64 89.6 91.77 88.64 87.43 88.54 88.83 95.88 88.17 90.06HUCRFB92.86 90.58 92.8 88.72 89.37 91.14 90.05 97.63 89.08 91.36HUCRFC92.82 90.61 92.84 88.69 88.94 91.45 90.31 97.62 89.04 91.37HUCRFS91.2 90.53 92.43 88.7 88.09 90.91 89.54 97.24 88.91 90.84HUCRFNS90.8 89.88 91.54 87.83 88.15 91.02 88.2 96.77 89.02 90.36HUCRFC+92.86 91.94 93.72 89.18 89.97 93.22 91.51 97.95 89.66 92.22Table 1: Comparison of slot F1 scores on nine personal assistant domains.
The numbers in boldfaceare the best performing method.
Subscripts mean the following: G = random initialization from aGaussian distribution with variance 10?4, R = pre-training with Restricted Boltzmann Machine (RBM)using contrastive divergence of (Hinton, 2002), C = pre-training with CCA-based clusters, B = pre-training with Brown clusters, S = pre-training with skip-ngram multi-sense clusters with fixed clustersize 5, NS = pre-training with non-parametric skip-ngram multi-sense clusters, C+ = pre-training withCCA-based multi-sense clusters.
(HUCRFR) does not perform better than withrandom initialization.
The skip-gram clusters(HUCRFS, HUCRFSN) do not perform well ei-ther.
Some examples of disambiguated word oc-currences are shown below, demonstrating that thealgorithm in Figure 3 yields intuitive clusters.NER POSTest-A Test-B Test-A Test-BCRF 90.75 86.37 95.51 94.99HUCRFG89.99 86.72 95.14 95.08HUCRFR90.12 86.43 95.42 94.14HUCRFB90.27 87.24 95.55 95.33HUCRFC90.9 86.89 95.67 95.23HUCRFS90.18 86.84 95.48 95.07HUCRFNS90.14 85.66 95.35 94.82HUCRFC+92.04 88.41 95.88 95.48Table 2: F1 Score for NER task and Accuracy forPOS task.word contextBooka book(1) store within 5 miles of my addressfind comic book(1) stores in novi michiganbook(2) restaurant for tomorrowbook(2) taxi to pizza hutlook for book(3) chang dong tofu house in poconofind book(3) bindery seattleHighrestaurant nearby with high(1) ratingsshow me high(1) credit restaurant nearbythe address for shelley high(2) schooldirections to leota junior high(2) schoolwhat?s the distance to kilburn high(3) roaddomino?s pizza in high(3) ridge missouriTable 3: Examples of disambiguated word occur-rences.4.2 NER & POS taggingWe use CoNLL 2003 dataset for NER and POSwith the standard train/dev/test split.
For pre-training, we used the Reuters-RCV1 corpus.
Itcontains 205 millions tokens with 1.6 milliontypes.
We follow same preprocessing steps as insemantic tagging.
Also, we use the NER featuresused in Turian et al (2010) and POS features usedin Maaten et al (2011).We present the results for both tasks in Table 2.In both tasks, the HUCRFC+yields the best per-formance, achieving error reduction of 20% (Test-A) and 13% (Test-B) for NER as well as 15%(Test-A) and 8% (Test-B) for POS over HUCRFR.Note that HUCRF does not always perform bet-ter than CRF when initialized randomly.
How-ever, However, HUCRF consistently outperformsCRF with the pre-training methods proposed inthis work.5 ConclusionWe presented an effective technique for pre-training HUCRFs.
Our method transfers observa-tion parameters trained on clustered text to initial-ize the training process.
We also proposed a wordclustering scheme based on CCA that is sensitiveto multiple word senses.
Using our pre-trainingmethod, we reported significant improvement overseveral baselines in three sequence labeling tasks.ReferencesTasos Anastasakos, Young-Bum Kim, and Anoop Deo-ras.
2014.
Task specific continuous word represen-tations for mono and multi-lingual spoken languageunderstanding.
In ICASSP, pages 3246?3250.
IEEE.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.196Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Asli Celikyilmaz, Dilek Z Hakkani-T?ur, G?okhan T?ur,and Ruhi Sarikaya.
2013.
Semi-supervised seman-tic tagging of conversational understanding usingmarkov topic regression.
In ACL, pages 914?923.Association for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Association for Computational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In ICML,pages 160?167.
ACM.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Anoop Deoras, Ruhi Sarikaya, G?okhan T?ur, andDilek Z Hakkani-T?ur.
2012.
Joint decoding forspeech recognition and semantic tagging.
In INTER-SPEECH.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In ACL, volume 1,pages 1370?1380.Dumitru Erhan, Yoshua Bengio, Aaron Courville,Pierre-Antoine Manzagol, Pascal Vincent, and SamyBengio.
2010.
Why does unsupervised pre-traininghelp deep learning?
The Journal of Machine Learn-ing Research, 11:625?660.Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.2006.
A fast learning algorithm for deep belief nets.Neural computation, 18(7):1527?1554.Geoffrey Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural com-putation, 14(8):1771?1800.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving WordRepresentations via Global Context and MultipleWord Prototypes.
In ACL.
Association for Compu-tational Linguistics.Young-Bum Kim and Benjamin Snyder.
2012.
Univer-sal grapheme-to-phoneme prediction over latin al-phabets.
In EMNLP, pages 332?343.
Associationfor Computational Linguistics.Young-Bum Kim, Heemoon Chae, Benjamin Snyder,and Yu-Seop Kim.
2014.
Training a korean srlsystem with rich morphological features.
In ACL,pages 637?642.
Association for Computational Lin-guistics.Young-Bum Kim, Minwoo Jeong, Karl Stratos, andRuhi Sarikaya.
2015a.
Weakly supervised slottagging with partially labeled sequences from websearch click logs.
In HLT-NAACL, pages 84?92.
As-sociation for Computational Linguistics.Young-Bum Kim, Karl Stratos, Xiaohu Liu, and RuhiSarikaya.
2015b.
Compact lexicon selection withspectral methods.
In ACL.
Association for Compu-tational Linguistics.Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, andMinwoo Jeong.
2015c.
New transfer learning tech-niques for disparate label sets.
In ACL.
Associationfor Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In ICML, pages 282?289.Hugo Larochelle and Yoshua Bengio.
2008.
Classifi-cation using discriminative restricted boltzmann ma-chines.
In ICML.Laurens van der Maaten, Max Welling, andLawrence K Saul.
2011.
Hidden-unit condi-tional random fields.
In AISTAT.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional ran-dom fields, feature induction and web-enhanced lex-icons.
In HLT-NAACL, pages 188?191.
Associationfor Computational Linguistics.Gr?egoire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of recurrent-neural-network architectures and learning methods for spo-ken language understanding.
In INTERSPEECH,pages 3771?3775.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In HLT-NAACL, volume 4, pages337?342.
Citeseer.Abdel-rahman Mohamed, Dong Yu, and Li Deng.2010.
Investigation of full-sequence training of deepbelief networks for speech recognition.
In INTER-SPEECH, pages 2846?2849.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In EMNLP.
Association forComputational Linguistics.197Joseph Reisinger and Raymond J Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 109?117.
Association for Computational Lin-guistics.Ruhi Sarikaya, Asli Celikyilmaz, Anoop Deoras, andMinwoo Jeong.
2014.
Shrinkage based features forslot tagging with conditional random fields.
In Proc.of Interspeech.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings ofthe 2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology-Volume 1, pages 134?141.
Association for Computational Linguistics.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
Introduction to statistical relational learn-ing, pages 93?128.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th annual meeting of the association for compu-tational linguistics, pages 384?394.
Association forComputational Linguistics.Mengqiu Wang and Christopher D Manning.
2013.
Ef-fect of non-linear deep architecture in sequence la-beling.
In ICML Workshop on Deep Learning forAudio, Speech and Language Processing.Puyang Xu and Ruhi Sarikaya.
2013.
Convolutionalneural network based triangular crf for joint intentdetection and slot filling.
In IEEE Workshop onAutomatic Speech Recognition and Understanding(ASRU), pages 78?83.
IEEE.Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,Yangyang Shi, and Dong Yu.
2013.
Recurrent neu-ral networks for language understanding.
In INTER-SPEECH, pages 2524?2528.198
