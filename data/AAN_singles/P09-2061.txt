Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 241?244,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPHandling phrase reorderings for machine translationYizhao Ni, Craig J.
Saunders?, Sandor Szedmak and Mahesan NiranjanISIS GroupSchool of Electronics and Computer ScienceUniversity of SouthamptonSouthampton, SO17 1BJUnited Kingdomyn05r@ecs.soton.ac.uk, craig.saunders@xrce.xerox.com,{ss03v,mn}@ecs.soton.ac.ukAbstractWe propose a distance phrase reorderingmodel (DPR) for statistical machine trans-lation (SMT), where the aim is to cap-ture phrase reorderings using a structurelearning framework.
On both the reorder-ing classification and a Chinese-to-Englishtranslation task, we show improved perfor-mance over a baseline SMT system.1 IntroductionWord or phrase reordering is a common prob-lem in bilingual translations arising from dif-ferent grammatical structures.
For example,in Chinese the expression of the date follows?Year/Month/Date?, while when translated intoEnglish, ?Month/Date/Year?
is often the correctgrammar.
In general, the fluency of machine trans-lations can be greatly improved by obtaining thecorrect word order in the target language.As the reordering problem is computation-ally expensive, a word distance-based reorderingmodel is commonly used among SMT decoders(Koehn, 2004), in which the costs of phrase move-ments are linearly proportional to the reorderingdistance.
Although this model is simple and effi-cient, the content independence makes it difficultto capture many distant phrase reordering causedby the grammar.
To tackle the problem, (Koehnet al, 2005) developed a lexicalized reorderingmodel that attempted to learn the phrase reorder-ing based on content.
The model learns the localorientation (e.g.
?monotone?
order or ?switching?order) probabilities for each bilingual phrase pairusing Maximum Likelihood Estimation (MLE).These orientation probabilities are then integratedinto an SMT decoder to help finding a Viterbi?bestlocal orientation sequence.
Improvements by this?the author?s new address: Xerox Research Centre Europe6, Chemin de Maupertuis, 38240 Meylan France.model have been reported in (Koehn et al, 2005).However, the amount of the training data for eachbilingual phrase is so small that the model usuallysuffers from the data sparseness problem.
Adopt-ing the idea of predicting the orientation, (Zensand Ney, 2006) started exploiting the context andgrammar which may relate to phrase reorderings.In general, a Maximum Entropy (ME) frameworkis utilized and the feature parameters are tunedby a discriminative model.
However, the trainingtimes for ME models are usually relatively high,especially when the output classes (i.e.
phrase re-ordering orientations) increase.Alternative to the ME framework, we proposeusing a classification scheme here for phrase re-orderings and employs a structure learning frame-work.
Our results confirm that this distance phrasereordering model (DPR) can lead to improved per-formance with a reasonable time efficiency.Figure 1: The phrase reordering distance d.2 Distance phrase reordering (DPR)We adopt a discriminative model to capture thefrequent distant reordering which we call distancephrase reordering.
An ideal model would considerevery position as a class and predict the position ofthe next phrase, although in practice we must con-sider a limited set of classes (denoted as ?).
Usingthe reordering distance d (see Figure 1) as definedby (Koehn et al, 2005), we extend the two classmodel in (Xiong et al, 2006) to multiple classes(e.g.
three?class setup ?
= {d < 0, d = 0, d >0}; or five?class setup ?
= {d ?
?5,?5 < d <0, d = 0, 0 < d < 5, d ?
5}).
Note that the more241classes it has, the closer it is to the ideal model, butthe smaller amount of training samples it wouldreceive for each class.2.1 Reordering Probability model andtraining algorithmGiven a (source, target) phrase pair (?fj, e?i) with?fj= [fjl, .
.
.
, fjr] and e?i= [eil, .
.
.
, eir], the dis-tance phrase reordering probability has the formp(o|?fj, e?i) :=h(wTo?
(?fj, e?i))?o???h(wTo??
(?fj, e?i))(1)where wo= [wo,0, .
.
.
, wo,dim(?
)]Tis the weightvector measuring features?
contribution to an ori-entation o ?
?, ?
is the feature vector and h is apre-defined monotonic function.
As the reorder-ing orientations tend to be interdependent, learn-ing {wo}o?
?is more than a multi?class classifi-cation problem.
Take the five?class setup for ex-ample, if an example in class d ?
?5 is classifiedin class ?5 < d < 5, intuitively the loss should besmaller than when it is classified in class d > 5.The output (orientation) domain has an inherentstructure and the model should respect it.
Hence,we utilize the structure learning framework pro-posed in (Taskar et al, 2003) which is equivalentto minimising the sum of the classification errorsminw1NN?n=1?
(o,?fnj, e?ni,w) +?2?w?2(2)where ?
?
0 is a regularisation parameter,?
(o,?fj, e?i,w) = max{0,maxo?6=o[4(o, o?)+wTo??
(?fj, e?i)] ?wTo?
(?fj, e?i)}is a structured margin loss function with4(o, o?)
=??
?0 if o = o?0.5 if o and o?are close in ?1 elsemeasuring the distance between pseudo orienta-tion o?and the true one o. Theoretically, this lossrequires that orientation o?which are ?far away?from the true one o must be classified with a largemargin while nearby candidates are allowed tobe classified with a smaller margin.
At trainingtime, we used a perceptron?based structure learn-ing (PSL) algorithm to learn {wo}o?
?which isshown in Table 1.2.1.1 Feature Extraction and ApplicationFollowing (Zens and Ney, 2006), we considerdifferent kinds of information extracted from theInput: The samples{o, ?
(?fj, e?i)}Nn=1, step size ?Initialization: k = 0; wo,k= 0 ?o ?
?
;Repeatfor n = 1, 2, .
.
.
, N dofor o?6= o getV = maxo?
{4(o, o?)
+ wTo?,k?
(?fj, e?i)}o?= argmaxo?
{4(o, o?)
+ wTo?,k?
(?fj, e?i)}if wTo,k?
(?fj, e?i) < V thenwo,k+1= wo,k+ ??
(?fj, e?i)wo?,k+1= wo?,k?
??
(?fj, e?i)k = k + 1until convergeOutput: wo,k+1?o ?
?Table 1: Perceptron-based structure learning.phrase environment (see Table 2), where given asequence s (e.g.
s = [fjl?z, .
.
.
, fjl]), the featuresselected are ?u(s|u|p) = ?
(s|u|p, u), with theindicator function ?
(?, ?
), p = {jl?
z, .
.
.
, jr+ z}and string s|u|p= [fp, .
.
.
, fp+|u|].
Hence, thephrase features are distinguished by both thecontent u and its start position p. For exam-ple, the left side context features for phrasepair (xiang gang, Hong Kong) in Figure 1 are{?
(s10, ?zhou?
), ?
(s11, ?liu?
), ?
(s20, ?zhou liu?
)}.As required by the algorithm, we then normalisethe feature vector??t=?t??
?.To train the DPR model, the training samples{(?fnj, e?ni)}Nn=1are extracted following the phrasepair extraction procedure in (Koehn et al, 2005)and form the sample pool, where the instanceshaving the same source phrase?fjare consideredto be from the same cluster.
A sub-DPR model isthen trained for each cluster using the PSL algo-rithm.
During the decoding, the DPR model findsthe corresponding sub-DPR model for a sourcephrase?fjand generates the reordering probabilityfor each orientation class using equation (1).3 ExperimentsExperiments used the Hong Kong Laws corpus1(Chinese-to-English), where sentences of lengthsbetween 1 and 100 words were extracted and theratio of source/target lengths was no more than2 : 1.
The training and test sizes are 50, 290 and1, 000 respectively.1This bilingual Chinese-English corpus consists of mainlylegal and documentary texts from Hong Kong.
The corpus isaligned at the sentence level which are collected and revisedmanually by the author.
The full corpus will be released soon.242Features for source phrase?fjFeatures for target phrase e?iContextSource word n?grams within a window(length z) around the phrase edge [jl] and [jr]Target word n?gramsof the phrase [eil, .
.
.
, eir]SyntacticSource word class tag n-grams within awindow (length z) around the phrase edge [jl] and [jr]Target word class tagn-grams of the phrase [eil, .
.
.
, eir]Table 2: The environment for the feature extraction.
The word class tags are provided by MOSES.3.1 Classification ExperimentsFigure 2: Classification results with respect to d.We used GIZA++ to produce alignments, en-abling us to compare using a DPR model againsta baseline lexicalized reordering model (Koehn etal., 2005) that uses MLE orientation predictionand a discriminative model (Zens and Ney, 2006)that utilizes an ME framework.
Two orientationclassification tasks are carried out: one with three?class setup and one with five?class setup.
Wediscarded points that had long distance reorder-ing (|d| > 15) to avoid some alignment errorscause by GIZA++ (representing less than 5% ofthe data).
This resulted in data sizes shown in Ta-ble 3.
The classification performance is measuredby an overall precision across all classes and theclass-specific F1 measures and the experimentsare are repeated three times to asses variance.Table 4 depicts the classification results ob-tained, where we observed consistent improve-ments for the DPR model over the baseline andthe ME models.
When the number of classes(orientations) increases, the average relative im-provements of DPR for the switching classes(i.e.
d 6= 0) increase from 41.6% to 83.2% overthe baseline and from 7.8% to 14.2% over the MEmodel, which implies a potential benefit of struc-ture learning.
Figure 2 further demonstrate the av-erage accuracy for each reordering distance d. Itshows that even for long distance reordering, theDPR model still performs well, while the MLEbaseline usually performs badly (more than halfexamples are classified incorrectly).
With so manyclassification errors, the effect of this baseline inan SMT system is in doubt, even with a powerfullanguage model.
At training time, training a DPRmodel is much faster than training an ME model(both algorithms are coded in Python), especiallywhen the number of classes increase.
This is be-cause the generative iterative scaling algorithm ofan ME model requires going through all examplestwice at each round: one is for updating the condi-tional distributions p(o|?fj, e?i) and the other is forupdating {wo}o??.
Alternatively, the PSL algo-rithm only goes through all examples once at eachround, making it faster and more applicable forlarger data sets.3.2 Translation experimentsWe now test the effect of the DPR model in anMT system, using MOSES (Koehn et al, 2005)as a baseline system.
To keep the comparisonfair, our MT system just replaces MOSES?s re-ordering models with DPR while sharing all othermodels (i.e.
phrase translation probability model,4-gram language model (A. Stolcke, 2002) andbeam search decoder).
As in classification exper-iments the three-class setup shows better resultsin switching classes, we use this setup in DPR.
Indetail, all consistent phrases are extracted from thetraining sentence pairs and form the sample pool.The three-class DPR model is then trained by thePSL algorithm and the function h(z) = exp(z) isapplied to equation (1) to transform the predictionscores.
Contrasting the direct use of the reorder-ing probabilities used in (Zens and Ney, 2006),we utilize the probabilities to adjust the worddistance?based reordering cost, where the reorder-ing cost of a sentence is computed as Po(f , e) =243Settings three?class setup five?class setupClasses d < 0 d = 0 d > 0 d ?
?5 ?5 < d < 0 d = 0 0 < d < 5 d ?
5Train 181, 583 755, 854 181, 279 82, 677 98, 907 755, 854 64, 881 116, 398Test 5, 025 21, 106 5, 075 2, 239 2, 786 21, 120 1, 447 3, 629Table 3: Data statistics for the classification experiments.System three?class setup taskPrecision d < 0 d = 0 d > 0 Training time (hours)Lexicalized 77.1?
0.1 55.7?
0.1 86.5?
0.1 49.2?
0.3 1.0ME 83.7?
0.3 67.9?
0.3 90.8?
0.3 69.2?
0.1 58.6DPR 86.7?
0.1 73.3?
0.1 92.5?
0.2 74.6?
0.5 27.0System five?class setup taskPrecision d ?
?5 ?5 < d < 0 d = 0 0 < d < 5 d ?
5 Training Time (hours)Lexicalized 74.3?
0.1 44.9?
0.2 32.0?
1.5 86.4?
0.1 29.2?
1.7 46.2?
0.8 1.3ME 80.0?
0.2 52.1?
0.1 54.7?
0.7 90.4?
0.2 63.9?
0.1 61.8?
0.1 83.6DPR 84.6?
0.1 60.0?
0.7 61.4?
0.1 92.6?
0.2 75.4?
0.6 68.8?
0.5 29.2Table 4: Overall precision and class-specific F1 scores [%] using different number of orientation classes.Bold numbers refer to the best results.exp{?
?mdm?p(o|?fjm,e?im)} with tuning parameter ?.This distance?sensitive expression is able to fillthe deficiency of the three?class setup of DPR andis verified to produce better results.
For parametertuning, minimum-error-rating training (F. J. Och,2003) is used in both systems.
Note that there are7 parameters needed tuning in MOSES?s reorder-ing models, while only 1 requires tuning in DPR.The translation performance is evaluated by fourMT measurements used in (Koehn et al, 2005).Table 5 shows the translation results, where weobserve consistent improvements on most evalua-tions.
Indeed both systems produced similar wordaccuracy, but our MT system does better in phrasereordering and produces more fluent translations.4 Conclusions and Future workWe have proposed a distance phrase reorderingmodel using a structure learning framework.
Theclassification tasks have shown that DPR is bet-ter in capturing the phrase reorderings over thelexicalized reordering model and the ME model.Moreover, compared with ME DPR is much fasterand more applicable to larger data sets.
Transla-tion experiments carried out on the Chinese-to-English task show that DPR gives more fluenttranslation results, which verifies its effectiveness.For future work, we aim at improving the predic-tion accuracy for the five-class setup using a richerfeature set before applying it to an MT system, asDPR can be more powerful if it is able to providemore precise phrase position for the decoder.
Wewill also apply DPR on a larger data set to test itsperformance as well as its time efficiency.Tasks Measure MOSES DPRBLEU [%] 44.7?
1.2 47.1?
1.3CH?EN word accuracy 76.5?
0.6 76.1?
1.5NIST 8.82?
0.11 9.04?
0.26METEOR [%] 66.1?
0.8 66.4?
1.1Table 5: Four evaluations for the MT experiments.Bold numbers refer to the best results.ReferencesP.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase?based statistical machine translation models.In Proc.
of AMTA 2004, Washington DC, October.P.
Koehn, A. Axelrod, A.
B. Mayne, C. Callison?Burch, M. Osborne and D. Talbot.
2005.
Ed-inburgh system description for the 2005 IWSLTspeech translation evaluation.
In Proc.
of IWSLT,Pittsburgh, PA.F.
J. Och.
2003.
SRILM - An Extensible LanguageModeling Toolkit.
In Proc.
Intl.
Conf.
Spoken Lan-guage Processing, Colorado, September.A.
Stolcke.
2002.
Minimum error rate training in sta-tistical machine translation.
In Proc.
ACL, Japan.B.
Taskar, C. Guestrin, and D.Koller.
2003.
Max?margin Markov networks.
In Proc.
NIPS, Vancou-ver, Canada, December.D.
Xiong, Q. Liu and S. Lin.
2006.
Maximum En-tropy Based Phrase Reordering Model for StatisticalMachine Translation.
In Proc.
of ACL, Sydney, July.R.
Zens and H. Ney.
2006.
Discriminative ReorderingModels for Statistical Machine Translation.
In Proc.of ACL, pages 55?63, New York City, June.244
