Learning Intonation Rules for Concept to Speech GenerationShimei  Pan  and Kath leen  McKeownDept.
of Computer ScienceColumbia UniversityNew York, NY 10027, USA{pan, kathy) @cs.columbia.eduAbst ractIn this paper, we report on an effort to pro-vide a general-purpose spoken language gener-ation tool for Concept-to-Speech (CTS) appli-cations by extending a widely used text gener-ation package, FUF/SURGE, with an intona-tion generation component.
As a first step, weapplied machine learning and statistical modelsto learn intonation rules based on the semanticand syntactic information typically representedin FUF/SURGE at the sentence l vel.
The re-sults of this study are a set of intonation ruleslearned automatically which can be directly im-plemented in our intonation generation compo-nent.
Through 5-fold cross-validation, we showthat the learned rules achieve around 90% accu-racy for break index, boundary tone and phraseaccent and 80% accuracy for pitch accent.
Ourstudy is unique in its use of features produced bylanguage generation to control intonation.
Themethodology adopted here can be employed i-rectly when more discourse/pragmatic informa-tion is to be considered in the future.1 Mot ivat ionSpeech is rapidly becoming a viable medium forinteraction with real-world applications.
Spo-ken language interfaces to on-line informa-tion, such as plane or train schedules, throughdisplay-less ystems, such as telephone inter-faces, are well under development.
Speech in-terfaces are also widely used in applicationswhere eyes-free and hands-free communicationis critical, such as car navigation.
Natural an-guage generation (NLG) can enhance the abil-ity of such systems to communicate naturallyand effectively by allowing the system to tailor,reorganize, or summarize l ngthy database re-sponses.
For example, in our work on a mul-timedia generation system where speech andgraphics generation techniques are used to au-tomaticaily summarize patient's pre-, during,and post-, operation status to different care-givers (Dalai et al, 1996), records relevant opatient status can easily number in the thou-sands.
Through content planning, sentenceplanning and lexical selection, ,the NLG com-ponent is able to provide a concise, yet infor-mative, briefing automatically through spokenand written language coordinated with graph-ics (McKeown et al, 1997) .Integrating language generation with speechsynthesis within a Concept-to-Speech (CTS)system not only brings the individual benefitsof each; as an integrated system, CTS can takeadvantage of the availability of rich structuralinformation constructed by the underlying NLGcomponent to improve the quality of synthe-sized speech.
Together, they have the potentialof generating better speech than Text-to-Speech(TTS) systems.
In this paper, we present a se-ries of experiments that use machine learning toidentify correlation between i tonation and fea-tures produced by a robust language generationtool, the FUF/SURGE system (Elhadad, 1993;Robin, 1994).
The ultimate goal of this studyis to provide a spoken language generation toolbased on FUF/SURGE, extended with an in-tonation generation component to facilitate thedevelopment of new CTS applications.2 Re la ted  Theor iesTwo elements form the theoretical back-ground of this work: the grammar used inFUF/SURGE and Pierrehumbert's intonationtheory (Pierrehumbert, 1980).
Our studyaims at identifying the relations between thesemantic/syntactic information produced byFUF/SURGE and four intonational features ofPierrehumbert: pitch accent, phrase accent,boundary tone and intermediate/intonationalphrase boundaries.1003The FUF/SURGE grammar is primarilybased on systemic grammar (Halliday, 1985).In systemic grammar, the process (ultimatelyrealized as the verb) is the core of a clause'ssemantic structure.
Obligatory semantic roles,called participants, are associated with eachprocess.
Usually, participants convey who/whatis involved in the process.
The process alsohas non-obligatory peripheral semantic rolescalled circumstances.
Circumstances answerquestions uch as when/where/how/why.
InFUF/SURGE, this semantic description is uni-fied with a syntactic grammar to generate a syn-tactic description.
All semantic, syntactic andlexical information, which are produced uringthe generation process, are kept in a final Func-tional Description (FD), before linearizing thesyntactic structure into a linear string.
The fea-tures used in our intonation model are mainlyextracted from this final FD.The intonation theory proposed in (Pierre-humbert, 1980) is used to describe the intona-tion structure.
Based on her intonation gram-mar, the F0 pitch contour is described by a setof intonational features.
The tune of a sen-tence is formed by one or more intonationalphrases.
Each intonational phrase consists ofone or more intermediate phrases followed bya boundary tone.
A well-formed intermediatephrase has one or more pitch accents followedby a phrase accent.
Based on this theory, thereare four features which are critical in decidingthe F0 contour: the placement of intonational orintermediate phrase boundaries (break index 4and 3 in ToBI annotation convention (Beckmanand Hirschberg, 1994)), the tonal type at theseboundaries (the phrase accent and the bound-ary tone), and the F0 local maximum or mini-mum (the pitch accent).3 Re la ted  WorkPrevious work on intonation modeling primar-ily focused on TTS applications.
For exam-ple, in (Bachenko and Fitzpatrick, 1990), aset of hand-crafted rules are used to determinediscourse neutral prosodic phrasing, achievingan accuracy of approximately 85%.
Recently,researchers improved on manual developmentof rules by acquiring prosodic phrasing ruleswith machine learning tools.
In (Wang andHirschberg, 1992), Classification And Regres-sion Tree (CART) (Brieman et al, 1984) wasused to produce a decision tree to predict thelocation of prosodic phrase boundaries, yieldinga high accuracy, around 90%.
Similar methodswere also employed in predicting pitch accentfor TTS in (Hirschberg, 1993).
Hirschberg ex-ploited various features derived from text analy-sis, such as part of speech tags, information sta-tus (i.g.
given/new, contrast), and cue phrases;both hand-crafted and automatically learnedrules achieved 80-98% success depending on thetype of speech corpus.
Until recently, there hasbeen only limited effort on modeling intonationfor CTS (Davis and Hirschberg, 1988; Youngand Fallside, 1979; Prevost, 1995).
Many CTSsystems were simplified as text generation fol-lowed by TTS.
Others that do integrate genera-tion make use of the structural information pro-vided by the NLG component (Prevost, 1995).However, most previous CTS systems are notbased on large scale general NLG systems.4 Mode l ing  In tonat ionWhile previous research provides ome correla-tion between linguistic features and intonation,more knowledge is needed.
The NLG compo-nent provides very rich syntactic and semanticinformation which has not been explored beforefor intonation modeling.
This includes, for ex-ample, the semantic role played by each seman-tic constituent.
In developing a CTS, it is worthtaking advantage of these features.Previous TTS research results cannot be im-plemented irectly in our intonation generationcomponent.
Many features tudied in TTS arenot provided by FUF/SURGE.
For example,the part-of-speech (POS) tags in FUF/SURGEare different from those used in TTS.
Further-more, it make little sense to apply part of speechtagging to generated text instead of using theaccurate POS provided in a NLG system.
Fi-nally, NLG provides information that is difficultto accurately obtain from full text (e.g., com-plete syntactic parses).These motivating factors led us to carry out astudy consisting of a series of three experimentsdesigned to answer the following questions:?
How do the different features producedby FUF/SURGE contribute to determin-ing intonation??
What is the minimal number of featuresneeded to achieve the best accuracy foreach of the four intonation features??
Does intra-sentential context improve ac-curacy?1004((cat clause)(process ((type ascriptive)(mode equative)))(participant((identified ((lex "John")(cat proper)))(identifier ((lex "teacher")(cat common))))))Figure 1: Semantic description4.1 Tools and DataIn order to model intonational features au-tomatically, features from FUF/SURGE anda speech corpus are provided as input to amachine learning tool called R IPPER (Co-hen, 1995), which produces a set of classifi-cation rules based on the training examples.The performance of R IPPER is comparable tobenchmark decision tree induction systems uchas CART and C4.5.
We also employ a sta-tistical method based on a generalized linearmodel (Chambers and Hastie, 1992) providedin the S package to select salient predictors forinput to RIPPER.Figure 1 shows the input Functional Descrip-tion(FD) for the sentence " John is the teacher".After this FD is unified with the syntactic gram-mar, SURGE, the resulting FD includes hun-dreds of semantic, syntactic and lexical features.We extract 13 features hown in Table 1 whichare more closely related to intonation as indi-cated by previous research.
We have chosenfeatures which are applicable to most words toavoid unspecified values in the training data.For example, "tense" is not extracted simplybecause it can be only applied to verbs.
Table 1includes descriptions for each of the featuresused.
These are divided into semantic, syntac-tic, and semi-syntactic/semantic features whichdescribe the syntactic properties of semanticconstituents.
Finally, word position (NO.)
andthe actual word (LEX) are extracted irectlyfrom the linearized string.About 400 isolated sentences with wide cov-erage of various linguistic phenomena were cre-ated as test cases for FUF/SURGE when it wasdeveloped.
We asked two male native speakersto read 258 sentences, each sentence may be re-peated several times.
The speech was recordedon a bAT in an office.
The most fluent versiono?
each sentence was kept.
The resulting speechwas transcribed by one author based on ToBIwith break index, pitch accent, phrase accentand boundary tone labeled, using the XWAVEspeech analysis tool.
The 13 features describedin Table 1 as well as one intonation feature areused as predictors for the response intonationfeature.
The final corpus contains 258 sentencesfor each speaker, including 119 noun phrases, 37of which have embeded sentences, and 139 sen-tences.
The average sentence/phrase length is5.43 words.
The baseline performance achievedby always guessing the majority class is 67.09%for break index, 54.10% for pitch accent, 66.23%for phrase accent and 79.37% for boundary tonebased on the speech corpus from one speaker.The relatively high baseline for boundary toneis because for most of the cases, there is onlyone L% boundary tone at the end of each sen-tence in our training data.
Speaker effect on in-tonation is briefly studied in experiment 2.
Allother experiments used data from one speakerwith the above baselines.4.2 Exper iments4.2.1 In teres t ing  Combinat ionsOur first set of experiments was designedas an initial test of how the features fromFUF/SURGE contribute to intonation.
We fo-cused on how the newly available semantic fea-tures affect intonation.
We were also interestedin finding out whether the 13 selected featuresare redundant in making intonation decisions.We started from a simple model which in-cludes only 3 factors, the type of semantic on-stituent boundary before (BB) and after (BA)the word, and part of speech (POS).
The seman-tic constituent boundary can take on 6 differentvalues; for example, it can be a clause boundary,a boundary associated with a primary semanticrole (e.g., a participant), with a secondary se-mantic role (e.g., a type of modifier), amongothers.
Our purpose in this experiment wasto test how well the model can do with a lim-ited number of parameters.
Applying R IPPERto the simple model yielded rules that signifi-cantly improved performance over the baselinemodels.
For example, the accuracy of the ruleslearned for break index increases to 87.37% from67.09%; the average improvement on all 4 into-national features is 19.33%.Next, we ran two additional tests, one withadditional syntactic features and another withadditional semantic features.
The results showthat the two new models behave similarly on allintonational features; they both achieve some1005CategorySemanticSyntacticSemi-semantic&syntacticMisc.LabelBBBASEMFUNSPGSPPOSGPOSSYNFUNSPPOSSPGPOSSPSYNFUNNO.LEXDescriptionThe semantic constituent boundary before theword.The semantic constituent boundary after theword.The semantic feature of the word.The semantic role played by the immediateparental semantic onstituent of the word.The generic semantic role played by the imme-diate parental semantic onstituent of the word.The part of speech of the wordThe generic part of speech of the wordThe syntactic function of the wordThe part of speech of the immediate parentalsemantic onstituent of the word.The generic part of speech of the immediateparental semantic onstituent of the word.The syntactic function of the immediate parental isemantic constituent ofthe word.The position of the word in a sentenceThe lexical form of the wordExamplesparticipant boundaries or circumstanceboundaries tc.participant boundaries or circumstanceboundaries tc.The semantic feature of "did" in "I didknow him."
is "insistence".The SP of "teacher" in "John is theteacher" is "identifier".The GSP of "teacher" in "John is theteacher" is "participant"common noun, proper noun etc.noun is the corresponding GPOS of bothcommon noun and proper noun.The SYNFUN of "teacher" in "the teacher"is "head".The SPPOS of "teacher" is "commonnoun".
IThe SPGPOS of "teacher" in "the teacher"is "noun phrase".
\]The SPSYNFUN of "teacher" in "John is I the teacher" is "subject complement.
1, 2, 3, 4 etc.
"John", "is", "the", '%eacher"etc.Table 1: Features extractedimprovements over the simple model, and thenew semantic model (containing the featuresSEMFUN, SP and GSP in addition to BB, BAand POS) also achieves some improvements overthe syntactic model (containing GPOS, SYN-FUN, SPPOS, SPGPOS and SPSYNFUN in ad-dition to BB, BA and POS), but none of theseimprovements are statistically significant usingbinomial test.Finally, we ran an experiment using all 13features, plus one intonational feature.
The per-formance achieved by using all predictors was alittle worse than the semantic model but a littlebetter than the simple model.
Again none ofthese changes are statistically significant.This experiment suggests that there is someredundancy among features.
All the more com-plicated models failed to achieve significant im-provements over the simple model which onlyhas three features.
Thus, overall, we can con-clude from this first set of experiments thatFUF/SURGE features do improve performanceover the baseline, but they do not indicate con-clusively which features are best for each of the4 intonation models.4.2.2 Sal ient P red ic torsAlthough RIPPER has the ability to select pre-dictors for its rules which increase accuracy, it'snot clear whether all the features in the RIP-PER rules are necessary.
Our first experimentfrom FUF and SURGEseems to suggest hat irrelevant features coulddamage the performance of RIPPER becausethe model with all features generally performsworse than the semantic model.
Therefore, thepurpose of the second experiment is to find thesalient predictors and eliminate redundant andirrelevant ones.
The result of this study alsohelps us gain a better understanding of the re-lations between FUF/SURGE features and in-tonation.Since the response variables, such as breakindex and pitch accent, are categorical values,a generalized linear model is appropriate.
Wemapped all intonation features into binary val-ues as required in this framework (e.g., pitchaccent is mapped to either "accent" or "de-accent").
The resulting data are analyzed bythe generalized linear model in a step-wise fash-ion.
At each step, a predictor is selected anddropped based on how well the new model canfit the data.
For example, in the break indexmodel, after GSP is dropped, the new modelachieves the same performance as the initialmodel.
This suggests that  GSP is redundantfor break index.Since the mapping process removes distinc-tions within the original categories, it is possi-ble that the simplified model will not performas well as the original model.
To confirm thatthe simplified model still performs reasonablywell, the new simplified models are tested by1006ModelBreakIndex"PitchAccent" hrBoundaryToneSelected Features Dropped featuresBB BA GPOS SPGPOS SP-  NO LEX POS SPPOS SPSYNFUN GSP SEMFUN SYNFUNACCENTNO BB BA POS GPOS LEX SPSYNFUN SEMFUN GSPSPPOS SPGPOS SPSYN-FUN INDEXNO BB BA POS GPOS LEX SP  GSP  SEMFUNSYNFUN SPPOS SPGPOSSPSYNFUN ACCENTNO BB BA GSP  LEX POS GPOS SYN-FUN SEMFUN SP SPPOSSPGPOS SPSYNFUN AC-CENTTable 2: The New modelletting RIPPER learn new rules based only onthe selected predictors.Table 2 shows the performance of the newmodels versus the original models.
As shownin the "selected features" and "dropped fea-tures" column, almost half of the predictors aredropped (average number of factors dropped is44.64%), and the new model achieves imilarperformance.For boundary tone, the accuracy of the ruleslearned from the new model is higher than theoriginal model.
For all other three models, theaccuracy is slightly less but very close to the oldmodels.
Another interesting observation is thatthe pitch accent model appears to be more com-plicated than the other models.
Twelve featuresare kept in this model, which include syntactic,semantic and intonational features.
The otherthree models are associated with fewer features.The boundary tone model appears to be thesimplest with only 4 features elected.A similar experiment was done for data com-bined from the two speakers.
An additionalvariable called "speaker" is added into themodel.
Again, the data is analyzed by the gen-eralized linear model.
The results show that"speaker" is consistently selected by the sys-tem as an important factor in all 4 models.This means that different speakers will resultin different intonational models.
As a result, webased our experiments on a single speaker in-stead of combining the data from both speakersinto a single model.
At this point, we carriedout no other experiments o study speaker dif-ference.4.2.3 Sequent ia l  RulesThe simplified model acquired from Experiment2 was quite helpful in reducing the complexityof the remaining experiments which were de-signed to take the intra-sentential context intoconsideration.
Much of intonation is not onlyModel AccuracyNew Init ial87.94% 88.29%73.87% 73.95%86.72% 88.08%97.36% 96.79%l~ule No.
(ZonditionsNew lnit ia New Initial7 9 18 165 9 15 252 5 4 8v.s.
the original modelaffected by features from isolated words, butalso by words in context.
For example, usuallythere are no adjacent intonational or intermedi-ate phrase boundaries.
Therefore, assigning oneboundary affects when the next boundary canbe assigned.
In order to account for this type ofinteraction, we extract features of words withina window of size 2i+1 for i=0,1,2,3; thus, foreach experiment, he features of the i previousadjacent words, the i following adjacent wordsand the current word are extracted.
Only thesalient predictors elected by experiment 2 areexplored here.The results in Table 3 show that intra-sentential context appears to be important inimproving the performance of the intonationmodels.
The accuracies of break index, phraseaccent and boundary tone model, shown in the"Accuracy" columns, are around 90% after thewindow size is increased from 1 to 7.
The accu-racy of pitch accent model is around 80%.
Ex-cept the boundary tone model, the best perfor-mance for all other three models improve sig-nificantly over the simple model with p=0.0017for break index model, p=0 for both pitch ac-cent and phrase accent model.
Similarly, theyare also significantly improved over the modelwithout context information with p=0.0135 forbreak index, p=0 for both phrase accent andpitch accent.4.3 The  Ru les  LearnedIn this section we describe some typical ruleslearned with relatively high accuracy.
The fol-lowing is a 5-word window pitch accent rule.IF ACCENTI=NA and POS=advTHEN ACCENT=H* (12/0)This states that if the following word is de-accented and the current word's part of speechis "adv", then the current word should be ac-cented.
It covers 12 positive examples and no1007Size Break Index Pitch Accent Phrase Accent Boundary toneAccuracy rule condl- Accuracy rule condi- Accuracy rule condi- Accuracy rule condl-t lon~ ~ t lon~ ~ t ion~ # t ion~1 87.94% 7 18 73.87% 11 20 86.72% 5 15 97.36% 2 43 89.87% 5 11 78.87% 11 25 88.22% 7 15 97.36% 2 45 89.86% 8 26 80.30% 12 29 90.29% 8 23 97.15% 2 47 88.44% 8 20 77.73% 11 20 89.58% 9 26 97.07% 3 5Tablenegative xamples in the training data.A break index rule with a 5-word window is:IF BBI=CB and SPPOSl=relativ~pronounTHEN INDEX=3 (23/0)This rule tells us if the boundary before thenext word is a clause boundary and the nextword's semantic parent's part of speech is rel-ative pronoun, then there is an intermediatephrase boundary after the current word.
Thisrule is supported by 23 examples in the trainingdata and contradicted by none.Although the above 5-word window rules onlyinvolve words within a 3-word window, noneof these rules reappears in the 3-word windowrules.
They are partially covered by other rules.For example, there is a similar pitch accent rulein the 3-word window model:IF POS=adv THEN ACCENT=H* (22/5)This indicates a strong interaction betweenrules learned before and after.
Since R IPPERuses a local optimization strategy, the final re-sults depend on the order of selecting classifiers.If the data set is large enough, this problem canbe alleviated.5 Generat ion  Arch i tec tureThe final rules learned in Experiment 3 includeintonation features as predictors.
In order tomake use of these rules, the following procedureis applied twice in our generation component.First, intonation is modeled with FUF/SURGEfeatures only.
Although this model is not asgood as the final model, it still accounts forthe majority of the success with more than 73%accuracy for all 4 intonation features.
Then,after all words have been assigned an initialvalue, the final rules learned in Experiment 3are applied and the refined results are usedto generate an abstract intonation descriptionrepresented in the Speech Integrating MarkupLanguage(SIML) format (Pan and McKeown,1997).
This abstract description is then trans-formed into specific TTS control parameters.Our current corpus is very small.
Expand-ing the corpus with new sentences i necessary.3: System performance with different window size\[- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i, Generation\[ FeatumExtractor~ NLGSystem~-'-'---~ ComponentMachine Learning '4' I L ~ L .
1' .Figure 2: Generation System ArchitectureDiscourse, pragmatic and other semantic fea-tures will be added into our future intonationmodel.
Therefore, the rules implemented in thegeneration component must be continuously up-graded.
Implementing a fixed set of rules is un-desirable.
As a result, our current generationcomponent shown in Figure 2 focuses on facil-itating the updating of the intonation model.Two separate rule sets (with or without intona-tion features as predictors) are learned as beforeand stored in rulebasel and rulebase2 respec-tively.
A rule interpreter is designed to parsethe rules in the rule bases.
The interpreter ex-tracts features and values encoded in the rulesand passes them to the intonation generator.The features extracted from the FUF/SURGEare compared with the features from the rules.If all conditions of a rule match the featuresfrom FUF/SURGE, a word is assigned the clas-sifted value (the RHS of the rule).
Otherwise,other rules are tried until it is assigned a value.The rules are tried one by one based on the or-der in which they are learned.
After every wordis tagged with all 4 intonation features, a con-verter transforms the abstract description intospecific TTS control parameters.6 Conc lus ion  and  Future  WorkIn this paper, we describe an effective way toautomatically earn intonation rules.
This workis unique and original in its use of linguistic fea-tures provided in a general purpose NLG tool tobuild intonation models.
The machine-learnedrules consistently performed well over all into-nation features with accuracies around 90% forbreak index, phrase accent and boundary tone.1008For pitch accent, the model accuracy is around80%.
This yields a significant improvement overthe baseline models and compares well withother TTS evaluations.
Since we used differ-ent data set than those used in previous TTSexperiments, we cannot accurately quantify thedifference inresults, we plan to carry out experi-ments to evaluate CTS versus TTS performanceusing the same data set in the future.
We alsodesigned an intonation generation architecturefor our spoken language generation componentwhere the intonation generation module dynam-ically applies newly learned rules to facilitatethe updating of the intonation model.In the future, discourse and pragmatic infor-mation will be investigated based on the samemethodology.
We will collect a larger speechcorpus to improve accuracy of the rules.
Fi-nally, an integrated spoken language generationsystem based on FUF/SURGE will be devel-oped based on the results of this research.7 AcknowledgementThanks to J. Hirschberg, D. Litman, J. Klavans,V.
Hatzivassiloglou and J. Shaw for comments.This material is based upon work supported bythe National Science Foundation under GrantNo.
IRI 9528998 and the Columbia UniversityCenter for Advanced Technology in High Per-formance Computing and Communications inHealthcare (funded by the New York state Sci-ence and Technology Foundation under GrantNo.
NYSSTF CAT 97013 SC1).ReferencesJ.
Bachenko and E. Fitzpatrick.
1990.
Acomputational grammar of discourse-neutralprosodic phrasing in English.
ComputationalLinguistics, 16(3):155-170.Mary Beckman and Julia Hirschberg.
1994.The ToBI annotation conventions.
Technicalreport, Ohio State University, Columbus.L.
Brieman, J.H.
Friedman, R.A. Olshen, andC.J.
Stone.
1984.
Classification and Regres-sion Trees.
Wadsworth and Brooks, Monter-rey, CA.John Chambers and Trevor Hastie.
1992.Statistical Models In S. Wadsworth &Brooks/Cole Advanced Book & Software, Pa-cific Grove, California.William Cohen.
1995.
Fast effective rule induc-tion.
In Proceedings of the 12th InternationalConference on Machine Learning.Mukesh Dalal, Steve Feiner, Kathy McKeown,Shimei Pan, Michelle Zhou, Tobias Hoellerer,James Shaw, Yong Feng, and Jeanne Fromer.1996.
Negotiation for automated generationof temporal multimedia presentations.
InProceedings of A CM Multimedia 1996, pages55-64.J.
Davis and J. Hirschberg.
1988.
Assigningintonational features in synthesized spokendiscourse.
In Proceedings of the 26th An-nual Meeting of the Association for Compu-tational Linguistics, pages 187-193, Buffalo,New York.M.
Elhadad.
1993.
Using Argumentationto Control Lexical Choice: A FunctionalUnification Implementation.
Ph.D. thesis,Columbia University.Michael A. K. Halliday.
1985.
An Introductionto Functional Grammar.
Edward Arnold,London.Julia Hirschberg.
1993.
Pitch accent in con-text:predicting intonational prominence fromtext.
Artificial Intelligence, 63:305-340.Kathleen McKeown, Shimei Pan, James Shaw,Desmond Jordan, and Barry Allen.
1997.Language generation for multimedia health-care briefings.
In Proc.
of the Fifth A CLConf.
on ANLP, pages 277-282.Shimei Pan and Kathleen McKeown.
1997.
In-tegrating language generation with speechsynthesis in a concept to speech system.In Proceedings of A CL//EA CL '97 Concept oSpeech Workshop, Madrid, Spain.Janet Pierrehumbert.
1980.
The Phonology andPhonetics of English Intonation.
Ph.D. the-sis, Massachusetts Institute of Technology.S.
Prevost.
1995.
A Semantics of Contrast andInformation Structure for Specifying Intona-tion in Spoken Language Generation.
Ph.D.thesis, University of Pennsylvania.Jacques Robin.
1994.
Revision-Based Gener-ation of Natural Language Summaries Pro-viding Historical Background.
Ph.D. thesis,Columbia University.Michelle Wang and Julia Hirschberg.
1992.
Au-tomatic classification of intonational phraseboundaries.
Computer Speech and Language,6:175-196.S.
Young and F. Fallside.
1979.
Speech synthe-sis from concept: a method for speech out-put from information systems.
Journal of theAcoustical Society of America, 66:685-695.1009
