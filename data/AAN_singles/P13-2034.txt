Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190?195,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBroadcast News Story Segmentation Using Manifold Learning on LatentTopic DistributionsXiaoming Lu1,2, Lei Xie1?, Cheung-Chi Leung2, Bin Ma2, Haizhou Li21School of Computer Science, Northwestern Polytechnical University, China2Institute for Infocomm Research, A?STAR, Singaporeluxiaomingnpu@gmail.com, lxie@nwpu.edu.cn, {ccleung,mabin,hli}@i2r.a-star.edu.sgAbstractWe present an efficient approach forbroadcast news story segmentation using amanifold learning algorithm on latent top-ic distributions.
The latent topic distribu-tion estimated by Latent Dirichlet Alloca-tion (LDA) is used to represent each textblock.
We employ Laplacian Eigenmap-s (LE) to project the latent topic distribu-tions into low-dimensional semantic rep-resentations while preserving the intrinsiclocal geometric structure.
We evaluate t-wo approaches employing LDA and prob-abilistic latent semantic analysis (PLSA)distributions respectively.
The effects ofdifferent amounts of training data and dif-ferent numbers of latent topics on the twoapproaches are studied.
Experimental re-sults show that our proposed LDA-basedapproach can outperform the correspond-ing PLSA-based approach.
The proposedapproach provides the best performancewith the highest F1-measure of 0.7860.1 IntroductionStory segmentation refers to partitioning a mul-timedia stream into homogenous segments eachembodying a main topic or coherent story (Allan,2002).
With the explosive growth of multimediadata, it becomes difficult to retrieve the most rel-evant components.
For indexing broadcast newsprograms, it is desirable to divide each of theminto a number of independent stories.
Manual seg-mentation is accurate but labor-intensive and cost-ly.
Therefore, automatic story segmentation ap-proaches are highly demanded.Lexical-cohesion based approaches have beenwidely studied for automatic broadcast news storysegmentation (Beeferman et al, 1997; Choi, 1999;Hearst, 1997; Rosenberg and Hirschberg, 2006;?corresponding authorLo et al, 2009; Malioutov and Barzilay, 2006;Yamron et al, 1999; Tur et al, 2001).
In thiskind of approaches, the audio portion of the da-ta stream is passed to an automatic speech recog-nition (ASR) system.
Lexical cues are extractedfrom the ASR transcripts.
Lexical cohesion is thephenomenon that different stories tend to employdifferent sets of terms.
Term repetition is one ofthe most common appearances.These rigid lexical-cohesion based approach-es simply take term repetition into consideration,while term association in lexical cohesion is ig-nored.
Moreover, polysemy and synonymy are notconsidered.
To deal with these problems, sometopic model techniques which provide conceptu-al level matching have been introduced to text andstory segmentation task (Hearst, 1997).
Proba-bilistic latent semantic analysis (PLSA) (Hofman-n, 1999) is a typical instance and used widely.PLSA is the probabilistic variant of latent seman-tic analysis (LSA) (Choi et al, 2001), and offers amore solid statistical foundation.
PLSA providesmore significant improvement than LSA for storysegmentation (Lu et al, 2011; Blei and Moreno,2001).Despite the success of PLSA, there are con-cerns that the number of parameters in PLSAgrows linearly with the size of the corpus.
Thismakes PLSA not desirable if there is a consid-erable amount of data available, and causes seri-ous over-fitting problems (Blei, 2012).
To dealwith this issue, Latent Dirichlet Allocation (L-DA) (Blei et al, 2003) has been proposed.
LDAhas been proved to be effective in many segmenta-tion tasks (Arora and Ravindran, 2008; Hall et al,2008; Sun et al, 2008; Riedl and Biemann, 2012;Chien and Chueh, 2012).Recent studies have shown that intrinsic di-mensionality of natural text corpus is significant-ly lower than its ambient Euclidean space (Belkinand Niyogi, 2002; Xie et al, 2012).
Therefore,190Laplacian Eigenmaps (LE) was proposed to com-pute corresponding natural low-dimensional struc-ture.
LE is a geometrically motivated dimen-sionality reduction method.
It projects data intoa low-dimensional representation while preserv-ing the intrinsic local geometric structure infor-mation (Belkin and Niyogi, 2002).
The locali-ty preserving property attempts to make the low-dimensional data representation more robust to thenoise from ASR errors (Xie et al, 2012).To further improve the segmentation perfor-mance, using latent topic distributions and LE in-stead of term frequencies to represent text blocksis studied in this paper.
We study the effects ofthe size of training data and the number of latenttopics on the LDA-based and the PLSA-based ap-proaches.
Another related work (Lu et al, 2013)is to use local geometric information to regularizethe log-likelihood computation in PLSA.2 Our Proposed ApproachIn this paper, we propose to apply LE on the L-DA topic distributions, each of which is estimat-ed from a text block.
The low-dimensional vec-tors obtained by LE projection are used to detectstory boundaries through dynamic programming.Moreover, as in (Xie et al, 2012), we incorporatethe temporal distances between block pairs as apenalty factor in the weight matrix.2.1 Latent Dirichlet AllocationLatent Dirichlet alocation (LDA) (Blei et al,2003) is a generative probabilistic model of a cor-pus.
It considers that documents are representedas random mixtures over latent topics, where eachtopic is characterized by a distribution over terms.In LDA, given a corpus D = {d1, d2, .
.
.
, dM}and a set of terms W = (w1, w2, .
.
.
, wV ), thegenerative process can be summarized as follows:1) For each document d, pick a multinomial dis-tribution ?
from a Dirichlet distribution parameter?, denoted as ?
?
Dir(?
).2) For each term w in document d, select a topicz from the multinomial distribution ?, denoted asz ?
Multinomial(?
).3) Select a term w from P (w|z, ?
), which is amultinomial probability conditioned on the topic.An LDA model is characterized by two sets ofprior parameters ?
and ?.
?
= (?1, ?2, .
.
.
, ?K)represents the Dirichlet prior distributions for eachK latent topics.
?
is aK?V matrix, which definesthe latent topic distributions over terms.2.2 Construction of weight matrix inLaplacian EigenmapsLaplacian Eigenmaps (LE) is introduced to projecthigh-dimensional data into a low-dimensional rep-resentation while preserving its locality property.Given the ASR transcripts of N text blocks, we ap-ply LDA algorithm to compute the correspondinglatent topic distributions X = [x1, x2, .
.
.
, xN ] inRK , where K is the number of latent topics, name-ly the dimensionality of LDA distributions.We use G to denote an N-node (N is number ofLDA distributions) graph which represents the re-lationship between all the text block pairs.
If dis-tribution vectors xi and xj come from the samestory, we put an edge between nodes i and j. Wedefine a weight matrix S of the graph G to denotethe cohesive strength between the text block pairs.Each element of this weight matrix is defined as:sij = cos(xi, xj)?|i?j|, (1)where ?|i?j| serves the penalty factor for the dis-tance between i and j. ?
is a constant lower than1.0 that we tune from a set of development data.It makes the cohesive strength of two text blocksdramatically decrease when their distance is muchlarger than the normal length of a story.2.3 Data projection in Laplacian EigenmapsGiven the weight matrix S, we define C as the di-agonal matrix with its element:cij =?Ki=1sij .
(2)Finally, we obtain the Laplacian matrix L, whichis defined as:L = C?
S. (3)We use Y = [y1, y2, .
.
.
, yN ] (yi is a columnvector) to indicate the low-dimensional represen-tation of the latent topic distributions X.
The pro-jection from the latent topic distribution space tothe target space can be defined as:f : xi ?
yi.
(4)A reasonable criterion for computing an optimalmapping is to minimize the objective as follows:K?i=1K?j=1?
yi ?
yj ?2 sij .
(5)Under this constraint condition, we can preservethe local geometrical property in LDA distribu-tions.
The objective function can be transformed191as:K?i=1K?j=1(yi ?
yj)sij = tr(YTLY).
(6)Meanwhile, zero matrix and matrices with it-s rank less than K are meaningless solutions forour task.
We impose YTLY = I to prevent thissituation, where I is an identity matrix.
By theReyleigh-Ritz theorem (Lutkepohl, 1997), the so-lution can obtained by the Q smallest eigenvaluesof the generalized eigenmaps problem:XLXT y = ?XCXT y.
(7)With this formula, we calculate the mapping ma-trix Y, and its row vectors y?1, y?2, .
.
.
, y?Q are in theorder of their eigenvalues ?1 ?
?2 ?
.
.
.
?
?Q.y?i is a Q-dimensional (Q<K) eigenvectors.2.4 Story boundary detectionIn story boundary detection, dynamic program-ming (DP) approach is adopted to obtain the glob-al optimal solution.
Given the low-dimensional se-mantic representation of the test data, an objectivefunction can be defined as follows:?
=Ns?t=1(?i,j?Segt?
yi ?
yj ?2), (8)where yi and yj are the latent topic distributions oftext blocks i and j respectively, and ?
yi ?
yj ?2is the Euclidean distance between them.
Segt in-dicates these text blocks assigned to a certain hy-pothesized story.
Ns is the number of hypothe-sized stories.The story boundaries which minimize the ob-jective function ?
in Eq.
(8) form the optimal re-sult.
Compared with classical local optimal ap-proach, DP can more effectively capture the s-mooth story shifts, and achieve better segmenta-tion performance.3 Experimental setupOur experiments were evaluated on the ASR tran-scripts provided in TDT2 English Broadcast newscorpus1, which involved 1033 news programs.
Weseparated this corpus into three non-overlappingsets: a training set of 500 programs for parameterestimation in topic modeling and LE, a develop-ment set of 133 programs for empirical tuning anda test set of 400 programs for performance evalu-ation.In the training stage, ASR transcripts with man-ually labeled boundary tags were provided.
Text1http://projects.ldc.upenn.edu/TDT2/streams were broken into block units according tothe given boundary tags, with each text block be-ing a complete story.
In the segmentation stage,we divided test data into text blocks using the timelabels of pauses in the transcripts.
If the pause du-ration between two blocks last for more than 1.0sec, it was considered as a boundary candidate.
Toavoid the segmentation being suffered from ASRerrors and the out-of-vocabulary issue, phonemebigram was used as the basic term unit (Xie et al,2012).
Since the ASR transcripts were at word lev-el, we performed word-to-phoneme conversion toobtain the phoneme bigram basic units.
The fol-lowing approaches, in which DP was used in storyboundary detection, were evaluated in the experi-ments:?
PLSA-DP: PLSA topic distributions wereused to compute sentence cohesive strength.?
LDA-DP: LDA topic distributions were usedto compute sentence cohesive strength.?
PLSA-LE-DP: PLSA topic distributions fol-lowed by LE projection were used to com-pute sentence cohesive strength.?
LDA-LE-DP: LDA topic distributions fol-lowed by LE projection were used to com-pute sentence cohesion strength.For LDA, we used the implementation fromDavid M. Blei?s webpage2.
For PLSA, we usedthe Lemur Toolkit3.F1-measure was used as the evaluation crite-rion.We followed the evaluation rule: a detectedboundary candidate is considered correct if it lieswithin a 15 sec tolerant window on each side of areference boundary.
A number of parameters wereset through empirical tuning on the developent set.The penalty factor was set to 0.8.
When evaluatingthe effects of different size of the training set, thenumber of latent topics in topic modeling processwas set to 64.
After the number of latent topicswas fixed, the dimensionality after LE projectionwas set to 32.
When evaluating the effects of d-ifferent number of latent topics in topic modelingcomputation, we fixed the size of the training setto 500 news programs and changed the number oflatent topics from 16 to 256.4 Experimental results and analysis4.1 Effect of the size of training datasetWe used the training set from 100 programs to 500programs (adding 100 programs in each step) to e-2http://www.cs.princeton.edu/ blei/lda-c/3http://www.lemurproject.org/192valuate the effects of different size of training datain both PLSA-based and LDA-based approaches.Figure 1 shows the results on the development setand the test set.0.550.60.650.70.750.8100 200 300 400 500F1-measurePLSA-LE-DP LDA-LE-DPLDA-DPPLSA-DPDevelopment Set0.550.60.650.70.750.8100 200 300 400 500F1-measureNumber of programs in training dataPLSA-LE-DPLDA-LE-DPPLSA-DPLDA-DPTest SetFigure 1: Segmentation performance with differ-ent amounts of training dataLDA-LE-DP approach achieved the best result(0.7927 and 0.7860) on both the development andthe test sets, when there were 500 programs in thetraining set.
This demonstrates that LDA modeland LE projection used in combination is excellentfor the story segmentation task.
The LE projectionapplied on the latent topic representations maderelatively 9.88% and 10.93% improvement overthe LDA-based approach and the PLSA-based ap-proach, respectively on the test set.
We can revealthat employing LE on PLSA and LDA topic dis-tributions achieves much better performance thanthe corresponding approaches without using LE.We have compared the performances betweenPLSA and LDA.
We found that when the train-ing data size was small, PLSA performed betterthan LDA.
Both PLSA-based and LDA-based ap-proaches got better with the increase in the size ofthe training data set.
All the four approaches hadsimilar performances on the development set andthe test set.With the increase in the size of the training da-ta, the LDA-based approaches were improved dra-matically.
They even outperformed the PLSA-based approaches when the training data containedmore than 300 programs.
This may be attributedto the fact that LDA needs more training data toestimate the parameters.
When the training data isnot enough, its parameters estimated in the train-ing stage is not stable for the development and thetest data.
Moreover, compared with PLSA, the pa-rameters in LDA do not grow linearly with the sizeof the corpus.4.2 Effect of the number of latent topicsWe evaluated the F1-measure of the four ap-proaches with different number of latent topicsprior to LE projection.
Figure 2 shows the cor-responding results.0.60.620.640.660.680.70.720.740.760.780.816 32 48 64 80 96 128 256F1-measureNumber of latent topicsPLSA-DPLDA-DPPLSA-LE-DPLDA-LE-DPFigure 2: Segmentation performance with differ-ent numbers of latent topicsThe best performances (0.7816-0.7847) wereachieved at the number of latent topics between64 and 96.
When the number of latent topics wasincreased from 16 to 64, F1-measure increased.When the number of latent topics was larger than96, F1-measure decreased gradually.
We foundthat the best results were achieved when the num-ber of topics was close to the real number of top-ics.
There are 80 manually labeled main topics inthe test set.We observe that LE projection makes the topicmodel more stable with different numbers of latenttopics.
The best and the worst performances dif-fered by relatively 9.12% in LDA-DP and 7.97%in PLSA-DP.
However, the relative difference of2.79% and 2.46% were observed in LDA-LE-DPand PLSA-LE-DP respectively.5 ConclusionsOur proposed approach achieves the best F1-measure of 0.7860.
In the task of story segmen-tation, we believe that LDA can avoid data overfit-ting problem when there is a sufficient amount oftraining data.
This is also applicable to LDA-LE-LP.
Moreover, we find that when we apply LE pro-jection to latent topic distributions, the segmen-tation performances become less sensitive to thepredefined number of latent topics.193AcknowledgmentsThis work is supported by the National Natu-ral Science Foundation of China (61175018), theNatural Science Basic Research Plan of Shaanx-i Province (2011JM8009) and the Fok Ying TungEducation Foundation (131059).ReferencesJ.
Allan.
2002.
Topic Detection and Tracking: Event-Based Information Organization.
Kluwer AcademicPublisher, Norwell, MA.Doug Beeferman, Adam Berger, and John Lafferty.1997.
A Model of Lexical Attraction and repulsion.In Proceedings of the 8th Conference on EuropeanChapter of the Association for Computational Lin-guistics (EACL), pp.373-380.Freddy Y. Y. Choi.
2000.
Advances in Domain In-dependent Linear Text Segmentation.
In Proceed-ings of the 1st North American Chapter of the As-sociation for Computational Linguistics Conference(NAACL), pp.26-33.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Indexing.
In Proceedings of the 21st AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR),pp.20-57.Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,Haizhou Li.
2011.
Probabilistic Latent Seman-tic Analysis for Broadcast New Story Segmentation.In Proceedings of the 11th Annual Conference ofthe International Speech Communication Associa-tion (INTERSPEECH), pp.1109-1112.David M. Blei.
2012.
Probabilistic topic models.Communication of the ACM, vol.
55, pp.77-84.David M. Blei, Andrew Y. Ng, Michael I. Jordan.2003.
Latent Dirichlet Allocation.
the Journal ofMachine Learning Research, vol.
3, pp.993-1022.Marti A. Hearst.
1997.
TextTiling: Segmenting Textinto Multiparagraph subtopic passages.
Computa-tional Liguistic, vol.
23, pp.33-64.Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke,Elizabeth Shriberg.
2001.
Integrating Prosodicand Lexicial Cues for Automatic Topic Segmenta-tion.
Computational Liguistic, vol.
27, pp.31-57.Andrew Rosenberg and Julia Hirschberg.
2006.
StorySegmentation of Broadcast News in English, Man-darin and Aribic.
In Proceedings of the 7th NorthAmerican Chapter of the Association for Compu-tational Linguistics Conference (NAACL), pp.125-128.David M. Blei and Pedro J. Moreno.
2001.
Topic Seg-mentation with An Aspect Hidden Markov Model.
InProceedings of the 24th Annual International ACMSIGIR Conference on Research and Development inInformation Retrival (SIGIR), pp.343-348.Wai-Kit Lo, Wenying Xiong, Helen Meng.
2009.
Au-tomatic Story Segmentation Using a Bayesian De-cision Framwork for Statistical Models of LexicalChain Feature.
In Proceedings of the 47th AnnualMeeting of the Association for Computational Lin-guistics (ACL), pp.357-364.Igor Malioutov and Regina Barzilay.
2006.
MinimumCut Model for Spoken Lecture Segmenation.
In Pro-ceedings of the 44th Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pp.25-32.Freddy Y. Y. Choi, Peter Wiemer-Hastings, JuhannaMoore.
2001.
Latent Semantic Analysis for Tex-t Segmentation.
In Proceedings of the 2001 Con-ference on Empirical Methods on Natural LanguageProcessing (EMNLP), pp.109-117.Rachit Arora and Balaraman Ravindran.
2008.
LatentDirichlet Allocation Based Multi-document Summa-rization.
In Proceedings of the 2nd Workshop onAnalytics for Noisy Unstructured Text Data (AND),pp.91-97.David Hall, Daniel Jurafsky, Christopher D. Manning.2008.
Latent Studying the History Ideas Using TopicModels.
In Proceedings of the 2008 Conference onEmpirical Methods on Natural Language Process-ing (EMNLP), pp.363-371.Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu.
2008.Text Segmentation with LDA-based Fisher Kernel.In Proceedings of the 46th Annual Meeting of the As-socation for Computational Linguistics on HumanLanguage Technologies (HLT-ACL), pp.269-272.Mikhail Belkin and Partha Niyogi.
2002.
LaplacianEigenmaps for Dimensionality Reduction and Da-ta Representation.
Neural Computation, vol.
15,pp.1383-1396.Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang.2012.
Laplacian Eigenmaps for Automatic StorySegmentation of Broadcast News.
IEEE Transactionon Audio, Speech and Language Processing, vol.
20,pp.264-277.Deng Cai, Qiaozhu Mei, Jiawei Han, and ChengxiangZhai.
2008.
Modeling Hidden Topics on DocumentManifold.
In Proceedings of the 17th ACM Confer-ence on Information and Knowledge Managemen-t (CIKM), pp.911-120.Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,and Haizhou Li.
2013.
Broadcast News Story Seg-mentation Using Latent Topics on Data Manifold.
InProceedings of the 38th International Conference onAcoustics, Speech, and Signal Processing (ICASSP).194J.
P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. vanMulbregt.
1999.
AHiddenMarkov Model Approachto Text Segmenation and Event Tracking.
In Pro-ceedings of the 1999 International Conference onAcoustics, Speech, and Signal Processing (ICASSP),pp.333-336.Martin Riedl and Chris Biemann.
2012.
Text Segmen-tation with Topic Models.
the Journal for LanguageTechnology and Computational Linguistics, pp.47-69.P.
Fragkou , V. Petridis , Ath.
Kehagias.
2002.
A Dy-namic Programming algorithm for Linear Text StorySegmentation.
the Joural of Intelligent InformationSystems, vol.
23, pp.179-197.H.
Lutkepohl.
1997.
Handbook of Matrices.
Wiley,Chichester, UK.Jen-Tzung Chien and Chuang-Hua Chueh.
2012.Topic-Based Hieraachical Segmentation.
IEEETransaction on Audio, Speech and Language Pro-cessing, vol.
20, pp.55-66.195
