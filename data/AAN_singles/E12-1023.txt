Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224?233,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsLarge-Margin Learning of Submodular Summarization ModelsRuben SiposDept.
of Computer ScienceCornell UniversityIthaca, NY 14853 USArs@cs.cornell.eduPannaga ShivaswamyDept.
of Computer ScienceCornell UniversityIthaca, NY 14853 USApannaga@cs.cornell.eduThorsten JoachimsDept.
of Computer ScienceCornell UniversityIthaca, NY 14853 USAtj@cs.cornell.eduAbstractIn this paper, we present a supervisedlearning approach to training submodu-lar scoring functions for extractive multi-document summarization.
By taking astructured prediction approach, we pro-vide a large-margin method that directlyoptimizes a convex relaxation of the de-sired performance measure.
The learningmethod applies to all submodular summa-rization methods, and we demonstrate itseffectiveness for both pairwise as well ascoverage-based scoring functions on mul-tiple datasets.
Compared to state-of-the-art functions that were tuned manually, ourmethod significantly improves performanceand enables high-fidelity models with num-ber of parameters well beyond what couldreasonably be tuned by hand.1 IntroductionAutomatic document summarization is the prob-lem of constructing a short text describing themain points in a (set of) document(s).
Exam-ple applications range from generating short sum-maries of news articles, to presenting snippets forURLs in web-search.
In this paper we focus onextractive multi-document summarization, wherethe final summary is a subset of the sentencesfrom multiple input documents.
In this way, ex-tractive summarization avoids the hard problemof generating well-formed natural-language sen-tences, since only existing sentences from the in-put documents are presented as part of the sum-mary.A current state-of-the-art method for documentsummarization was recently proposed by Lin andBilmes (2010), using a submodular scoring func-tion based on inter-sentence similarity.
On the onehand, this scoring function rewards summariesthat are similar to many sentences in the origi-nal documents (i.e.
promotes coverage).
On theother hand, it penalizes summaries that containsentences that are similar to each other (i.e.
dis-courages redundancy).
While obtaining the exactsummary that optimizes the objective is computa-tionally hard, they show that a greedy algorithmis guaranteed to compute a good approximation.However, their work does not address how toselect a good inter-sentence similarity measure,leaving this problem as well as selecting an appro-priate trade-off between coverage and redundancyto manual tuning.To overcome this problem, we propose a su-pervised learning method that can learn boththe similarity measure as well as the cover-age/reduncancy trade-off from training data.
Fur-thermore, our learning algorithm is not limited tothe model of Lin and Bilmes (2010), but applies toall monotone submodular summarization models.Due to the diminishing-returns property of mono-tone submodular set functions and their computa-tional tractability, this class of functions providesa rich space for designing summarization meth-ods.
To illustrate the generality of our approach,we also provide experiments for a coverage-basedmodel originally developed for diversified infor-mation retrieval (Swaminathan et al 2009).In general, our method learns a parameterizedmonotone submodular scoring function from su-pervised training data, and its implementation isavailable for download.1 Given a set of docu-ments and their summaries as training examples,1http://www.cs.cornell.edu/?rs/sfour/224we formulate the learning problem as a struc-tured prediction problem and derive a maximum-margin algorithm in the structural support vec-tor machine (SVM) framework.
Note that, un-like other learning approaches, our method doesnot require a heuristic decomposition of the learn-ing task into binary classification problems (Ku-piec et al 1995), but directly optimizes a struc-tured prediction.
This enables our algorithm to di-rectly optimize the desired performance measure(e.g.
ROUGE) during training.
Furthermore, ourmethod is not limited to linear-chain dependen-cies like (Conroy and O?leary, 2001; Shen et al2007), but can learn any monotone submodularscoring function.This ability to easily train summarization mod-els makes it possible to efficiently tune modelsto various types of document collections.
In par-ticular, we find that our learning method can re-liably tune models with hundreds of parametersbased on a training set of about 30 examples.This increases the fidelity of models comparedto their hand-tuned counterparts, showing sig-nificantly improved empirical performance.
Weprovide a detailed investigation into the sourcesof these improvements, identifying further direc-tions for research.2 Related workWork on extractive summarization spans a largerange of approaches.
Starting with unsupervisedmethods, one of the widely known approachesis Maximal Marginal Relevance (MMR) (Car-bonell and Goldstein, 1998).
It uses a greedy ap-proach for selection and considers the trade-offbetween relevance and redundancy.
Later it wasextended (Goldstein et al 2000) to support multi-document settings by incorporating additional in-formation available in this case.
Good results canbe achieved by reformulating this as a knapsackpacking problem and solving it using dynamicprograming (McDonald, 2007).
Alternatively, wecan use annotated phrases as textual units and se-lect a subset that covers most concepts presentin the input (Filatova and Hatzivassiloglou, 2004)(which can also be achieved by our coverage scor-ing function if it is extended with appropriate fea-tures).A popular stochastic graph-based summariza-tion method is LexRank (Erkan and Radev, 2004).It computes sentence importance based on theconcept of eigenvector centrality in a graph ofsentence similarities.
Similarly, TextRank (Mi-halcea and Tarau, 2004) is also graph based rank-ing system for identification of important sen-tences in a document by using sentence similar-ity and PageRank (Brin and Page, 1998).
Sen-tence extraction can also be implemented usingother graph based scoring approaches (Mihalcea,2004) such as HITS (Kleinberg, 1999) and po-sitional power functions.
Graph based methodscan also be paired with clustering such as in Col-labSum (Wan et al 2007).
This approach firstuses clustering to obtain document clusters andthen uses graph based algorithm for sentence se-lection which includes inter and intra-documentsentence similarities.
Another clustering-basedalgorithm (Nomoto and Matsumoto, 2001) is adiversity-based extension of MMR that finds di-versity by clustering and then proceeds to reduceredundancy by selecting a representative for eachcluster.The manually tuned sentence pairwise model(Lin and Bilmes, 2010; Lin and Bilmes, 2011) wetook inspiration from is based on budgeted sub-modular optimization.
A summary is producedby maximizing an objective function that includescoverage and redundancy terms.
Coverage is de-fined as the sum of sentence similarities betweenthe selected summary and the rest of the sen-tences, while redundancy is the sum of pairwiseintra-summary sentence similarities.
Another ap-proach based on submodularity (Qazvinian et al2010) relies on extracting important keyphrasesfrom citation sentences for a given paper and us-ing them to build the summary.In the supervised setting, several early methods(Kupiec et al 1995) made independent binary de-cisions whether to include a particular sentencein the summary or not.
This ignores dependen-cies between sentences and can result in high re-dundancy.
The same problem arises when usinglearning-to-rank approaches such as ranking sup-port vector machines, support vector regressionand gradient boosted decision trees to select themost relevant sentences for the summary (Metzlerand Kanungo, 2008).Introducing some dependencies can improvethe performance.
One limited way of introduc-ing dependencies between sentences is by using alinear-chain HMM.
The HMM is assumed to pro-duce the summary by having a chain transitioning225between summarization and non-summarizationstates (Conroy and O?leary, 2001) while travers-ing the sentences in a document.
A more expres-sive approach is using a CRF for sequence label-ing (Shen et al 2007) which can utilize larger andnot necessarily independent feature spaces.
Thedisadvantage of using linear chain models, how-ever, is that they represent the summary as a se-quence of sentences.
Dependencies between sen-tences that are far away from each other cannotbe modeled efficiently.
In contrast to such lin-ear chain models, our approach on submodularscoring functions can model long-range depen-dencies.
In this way our method can use proper-ties of the whole summary when deciding whichsentences to include in it.More closely related to our work is that of Liet al(2009).
They use the diversified retrievalmethod proposed in Yue and Joachims (2008) fordocument summarization.
Moreover, they assumethat subtopic labels are available so that additionalconstraints for diversity, coverage and balance canbe added to the structural SVM learning prob-lem.
In contrast, our approach does not require theknowledge of subtopics (thus allowing us to ap-ply it to a wider range of tasks) and avoids addingadditional constraints (simplifying the algorithm).Furthermore, it can use different submodular ob-jective functions, for example word coverage andsentence pairwise models described later in thispaper.Another closely related work also takes a max-margin discriminative learning approach in thestructural SVM framework (Berg-Kirkpatrick etal., 2011) or by using MIRA (Martins and Smith,2009) to learn the parameters for summarizinga set of documents.
However, they do not con-sider submodular functions, but instead solve anInteger Linear Program (ILP) or an approxima-tion thereof.
The ILP encodes a compressionmodel where arbitrary parts of the parse treesof sentences in the summary can be cut and re-moved.
This allows them to select parts of sen-tences and yet preserve some gramatical struc-ture.
Their work focuses on learning a particularcompression model based on ILP inference, whileour work explores learning a general and largeclass of sentence selection models using submod-ular optimization.
The third notable approachuses SEARN (Daume?, 2006) to learn parametersfor joint summarization and compression model,however it uses vine-growth model and employssearch to to find the best policy which is then usedto generate a summary.A specific subclass of submodular (but notmonotone) functions are defined by Determinan-tal Point Processes (DPPs) (Kulesza and Taskar,2011).
While they provide an elegant probabilis-tic interpretation of the resulting summarizationmodels, the lack of monotonicity means that noefficient approximation algorithms are known forcomputing the highest-scoring summary.3 Submodular document summarizationIn this section, we illustrate how document sum-marization can be addressed using submodular setfunctions.
The set of documents to be summa-rized is split into a set of individual sentencesx = {s1, ..., sn}.
The summarization methodthen selects a subset y?
?
x of sentences that max-imizes a given scoring function Fx : 2x ?
Rsubject to a budget constraint (e.g.
less than Bcharacters).y?
= arg maxy?xFx(y) s.t.
|y| ?
B (1)In the following we restrict the admissible scoringfunctions F to be submodular.Definition 1.
Given a set x, a function F : 2x ?R is submodular iff for all u ?
U and all sets sand t such that s ?
t ?
x, we have,F (s ?
{u})?
F (s) ?
F (t ?
{u})?
F (t).Intuitively, this definition says that adding u toa subset s of t increases f at least as much asadding it to t. Using two specific submodularfunctions as examples, the following sections il-lustrate how this diminishing returns property nat-urally reflects the trade-off between maximizingcoverage while minimizing redundancy.3.1 Pairwise scoring functionThe first submodular scoring function we con-sider was proposed by Lin and Bilmes (2010) andis based on a model of pairwise sentence similar-ities.
It scores a summary y using the followingfunction, which Lin and Bilmes (2010) show issubmodular:Fx(y) =?i?x\y,j?y?
(i, j)?
?
?i,j?y:i 6=j?
(i, j).
(2)226Figure 1: Illustration of the pairwise model.
Not alledges are shown for clarity purposes.
Edge thicknessdenotes the similarity score.In the above equation, ?
(i, j) ?
0 denotes a mea-sure of similarity between pairs of sentences i andj.
The first term in Eq.
2 is a measure of how simi-lar the sentences included in summary y are to theother sentences in x.
The second term penalizesy by how similar its sentences are to each other.?
> 0 is a scalar parameter that trades off be-tween the two terms.
Maximizing Fx(y) amountsto increasing the similarity of the summary to ex-cluded sentences while minimizing repetitions inthe summary.
An example is illustrated in Figure1.
In the simplest case, ?
(i, j) may be the TFIDF(Salton and Buckley, 1988) cosine similarity, butwe will show later how to learn sophisticated sim-ilarity functions.3.2 Coverage scoring functionA second scoring function we consider wasfirst proposed for diversified document retrieval(Swaminathan et al 2009; Yue and Joachims,2008), but it naturally applies to document sum-marization as well (Li et al 2009).
It is based ona notion of word coverage, where each word v hassome importance weight ?
(v) ?
0.
A summaryy covers a word if at least one of its sentencescontains the word.
The score of a summary isthen simply the sum of the word weights its cov-ers (though we could also include a concave dis-count function that rewards covering a word mul-tiple times (Raman et al 2011)):Fx(y) =?v?V (y)?(v).
(3)In the above equation, V (y) denotes the union ofall words in y.
This function is analogous to amaximum coverage problem, which is known tobe submodular (Khuller et al 1999).Figure 2: Illustration of the coverage model.
Wordborder thickness represents importance.An example of how a summary is scored is il-lustrated in the Figure 2.
Analogous to the defini-tion of similarity ?
(i, j) in the pairwise model, thechoice of the word importance function ?
(v) iscrucial in the coverage model.
A simple heuristicis to weigh words highly that occur in many sen-tences of x, but in few other documents (Swami-nathan et al 2009).
However, we will show in thefollowing how to learn ?
(v) from training data.Algorithm 1 Greedy algorithm for finding thebest summary y?
given a scoring function Fx(y).Parameter: r > 0.y?
?
?A?
xwhile A 6= ?
dok ?
arg maxl?AFx(y?
?
{l})?
Fx(y?
)(cl)rif ck+?i?y?
ci?B and Fx(y??{k})?Fx(y?
)?0 theny?
?
y?
?
{k}end ifA?
A\{k}end while3.3 Computing a SummaryComputing the summary that maximizes either ofthe two scoring functions from above (i.e.
Eqns.
(2) and (3)) is NP-hard (McDonald, 2007).
How-ever, it is known that the greedy algorithm 1 canachieve a 1 ?
1/e approximation to the optimumsolution for any linear budget constraint (Lin andBilmes, 2010; Khuller et al 1999).
Even further,this algorithm provides a 1 ?
1/e approximationfor any monotone submodular scoring function.The algorithm starts with an empty summariza-tion.
In each step, a sentence is added to the sum-mary that results in the maximum relative increase227of the objective.
The increase is relative to theamount of budget that is used by the added sen-tence.
The algorithm terminates when the budgetB is reached.Note that the algorithm has a parameter r inthe denominator of the selection rule, which Linand Bilmes (2010) report to have some impacton performance.
In the algorithm, ci representsthe cost of the sentence (i.e., length).
Thus, thealgorithm actually selects sentences with largemarginal unity relative to their length (trade-offcontrolled by the parameter r).
Selecting r to beless than 1 gives more importance to ?informationdensity?
(i.e.
sentences that have a higher ratioof score increase per length).
The 1 ?
1e greedyapproximation guarantee holds despite this addi-tional parameter (Lin and Bilmes, 2010).
Moredetails on our choice of r and its effects are pro-vided in the experiments section.4 Learning algorithmIn this section, we propose a supervised learningmethod for training a submodular scoring func-tion to produce desirable summaries.
In particu-lar, for the pairwise and the coverage model, weshow how to learn the similarity function ?
(i, j)and the word importance weights ?
(v) respec-tively.
In particular, we parameterize ?
(i, j) and?
(v) using a linear model, allowing that each de-pends on the full set of input sentences x:?x(i, j) = wT?px(i, j) ?x(v) = wT?cx(v).
(4)In the above equations, w is a weight vector thatis learned, and ?px(i, j) and ?cx(v) are feature vec-tors.
In the pairwise model, ?px(i, j) may includefeature like the TFIDF cosine between i and j orthe number of words from the document titles thati and j share.
In the coverage model, ?cx(v) mayinclude features like a binary indicator of whetherv occurs in more than 10% of the sentences in xor whether v occurs in the document title.We propose to learn the weights following alarge-margin framework using structural SVMs(Tsochantaridis et al 2005).
Structural SVMslearn a discriminant functionh(x) = arg maxy?Yw>?
(x, y) (5)that predicts a structured output y given a (pos-sibly also structured) input x.
?
(x, y) ?
RN iscalled the joint feature-map between input x andoutput y.
Note that both submodular scoring func-tion in Eqns.
(2) and (3) can be brought into theform wT?
(x, y) for the linear parametrization inEq.
(6) and (7):?p(x, y)=?i?x\y,j?y?px(i, j)?
?
?i,j?y:i 6=j?px(i, j), (6)?c(x, y)=?v?V (y)?cx(v).
(7)After this transformation, it is easy to see thatcomputing the maximizing summary in Eq.
(1)and the structural SVM prediction rule in Eq.
(5)are equivalent.To learn the weight vector w, structural SVMsrequire training examples (x1, y1), ..., (xn, yn) ofinput/output pairs.
In document summarization,however, the ?correct?
extractive summary is typ-ically not known.
Instead, training documentsxi are typically annotated with multiple manual(non-extractive) summaries (denoted by Y i).
Todetermine a single extractive target summary yifor training, we find the extractive summary that(approximately) optimizes ROUGE score ?
orsome other loss function ?
(Y i, y) ?
with respectto Y i.yi = argminy?Y?
(Y i, y) (8)We call the yi determined in this way the ?target?summary for xi.
Note that yi is a greedily con-structed approximate target summary based on itsproximity to Y i via ?.
Because of this, we willlearn a model that can predict approximately goodsummaries yi from xi.
However, we believe thatmost of the score difference between manual sum-maries and yi (as explored in the experiments sec-tion) is due to it being an extractive summary andnot due to greedy construction.Following the structural SVM approach, wecan now formulate the problem of learning w asthe following quadratic program (QP):minw,?
?012?w?2 +Cnn?i=1?i (9)s.t.
w>?
(xi, yi)?w>?
(xi, y?i) ??
(y?i, Y i)?
?i, ?y?i 6= yi, ?1 ?
i ?
n.The above formulation ensures that the scor-ing function with the target summary (i.e.w>?
(xi, yi)) is larger than the scoring function228Algorithm 2 Cutting-plane algorithm for solvingthe learning optimization problem.Parameter: desired tolerance  > 0.?i :Wi ?
?repeatfor ?i doy?
?
arg maxywT?
(xi, y) + ?
(Y i, y)if wT?
(xi, yi) +  ?
wT?
(xi, y?)
+?
(Y i, y?)?
?i thenWi ?Wi ?
{y?
}w ?
solve QP (9) using constraintsWiend ifend foruntil noWi has changed during iterationfor any other summary y?i (i.e., w>?
(xi, y?i)).The objective function learns a large-marginweight vectorw while trading it off with an upperbound on the empirical loss.
The two quantitiesare traded off with a parameter C > 0.Even though the QP has exponentially manyconstraints in the number of sentences in the in-put documents, it can be solved approximatelyin polynomial time via a cutting plane algorithm(Tsochantaridis et al 2005).
The steps of thecutting-plane algorithm are shown in Algorithm2.
In each iteration of the algorithm, for eachtraining document xi, a summary y?i which mostviolates the constraint in (9) is found.
This is doneby findingy?
?
arg maxy?YwT?
(xi, y) + ?
(Y i, y),for which we use a variant of the greedy algorithmin Figure 1.
After a violating constraint for eachtraining example is added, the resulting quadraticprogram is solved.
These steps are repeated untilall the constraints are satisfied to a required preci-sion .Finally, special care has to be taken to appro-priately define the loss function ?
given the dis-parity of Y i and yi.
Therefore, we first define anintermediate loss function?R(Y, y?)
= max(0, 1?ROUGE1F (Y, y?
)),based on the ROUGE-1 F score.
To ensure thatthe loss function is zero for the target label as de-fined in (8), we normalized the above loss as be-low:?
(Y i, y?)
= max(0,?R(Yi, y?)?
?R(Yi, yi)),The loss ?
was used in our experiments.
Thustraining a structural SVM with this loss aims tomaximize the ROUGE-1 F score with the man-ual summaries provided in the training examples,while trading it off with margin.
Note that wecould also use a different loss function (as themethod is not tied to this particular choice), if wehad a different target evaluation metric.
Finally,once a w is obtained from structural SVM train-ing, a predicted summary for a test document xcan be obtained from (5).5 ExperimentsIn this section, we empirically evaluate the ap-proach proposed in this paper.
Following Lin andBilmes (2010), experiments were conducted ontwo different datasets (DUC ?03 and ?04).
Thesedatasets contain document sets with four manualsummaries for each set.
For each document set,we concatenated all the articles and split theminto sentences using the tool provided with the?03 dataset.
For the supervised setting we used10 resamplings with a random 20/5/5 (?03) and40/5/5 (?04) train/test/validation split.
We deter-mined the best C value in (9) using the perfor-mance on each validation set and then report aver-age performence over the corresponding test sets.Baseline performance (the approach of Lin andBilmes (2010)) was computed using all 10 testsets as a single test set.
For all experiments anddatasets, we used r = 0.3 in the greedy algorithmas recommended in Lin and Bilmes (2010) for the?03 dataset.
We find that changing r has only asmall influence on performance.2The construction of features for learning is or-ganized by word groups.
The most trivial groupis simply all words (basic).
Considering the prop-erties of the words themselves, we constructedseveral features from properties such as capital-ized words, non-stop words and words of cer-tain length (cap+stop+len).
We obtained anotherset of features from the most frequently occur-ing words in all the articles (minmax).
We alsoconsidered the position of a sentence (containing2Setting r to 1 and thus eliminating the non-linearity doeslower the score (e.g.
to 0.38466 for the pairwise model onDUC ?03 compared with the results on Figure 3).229the word) in the article as another feature (loca-tion).
All those word groups can then be furtherrefined by selecting different thresholds, weight-ing schemes (e.g.
TFIDF) and forming binnedvariants of these features.For the pairwise model we use cosine similar-ity between sentences using only words in a givenword group during computation.
For the wordcoverage model we create separate features forcovering words in different groups.
This gives usfairly comparable feature strength in both mod-els.
The only further addition is the use of differ-ent word coverage levels in the coverage model.First we consider how well does a sentence covera word (e.g.
a sentence with five instances of thesame word might cover it better than another withonly a single instance).
And secondly we look athow important it is to cover a word (e.g.
if a wordappears in a large fraction of sentences we mightwant to be sure to cover it).
Combining those twocriteria using different thresholds we get a set offeatures for each word.
Our coverage features aremotivated from the approach of Yue and Joachims(2008).
In contrast, the hand-tuned pairwise base-line uses only TFIDF weighted cosine similaritybetween sentences using all words, following theapproach in Lin and Bilmes (2010).The resulting summaries are evaluated usingROUGE version 1.5.5 (Lin and Hovy, 2003).
Weselected the ROUGE-1 F measure because it wasused by Lin and Bilmes (2010) and because it isone of the commonly used performance scores inrecent work.
However, our learning method ap-plies to other performance measures as well.
Notethat we use the ROUGE-1 F measure both for theloss function during learning, as well as for theevaluation of the predicted summaries.5.1 How does learning compare to manualtuning?In our first experiment, we compare our super-vised learning approach to the hand-tuned ap-proach.
The results from this experiment are sum-marized in Figure 3.
First, supervised trainingof the pairwise model (Lin and Bilmes, 2010)resulted in a statistically significant (p ?
0.05using paired t-test) increase in performance onboth datasets compared to our reimplementationof the manually tuned pairwise model.
Note thatour reimplementation of the approach of Lin andBilmes (2010) resulted in slightly different per-formance numbers than those reported in Lin andBilmes (2010) ?
better on DUC ?03 and somewhatlower on DUC ?04, if evaluated on the same selec-tion of test examples as theirs.
We conjecture thatthis is due to small differences in implementationand/or preprocessing of the dataset.
Furthermore,as authors of Lin and Bilmes (2010) note in theirpaper, the ?03 and ?04 datasets behave quite dif-ferently.model dataset ROUGE-1 F (stderr)pairwise DUC ?03 0.3929 (0.0074)coverage 0.3784 (0.0059)hand-tuned 0.3571 (0.0063)pairwise DUC ?04 0.4066 (0.0061)coverage 0.3992 (0.0054)hand-tuned 0.3935 (0.0052)Figure 3: Results obtained on DUC ?03 and ?04datasets using the supervised models.
Increase in per-formance over the hand-tuned is statistically signifi-cant (p ?
0.05) for the pairwise model on the bothdatasets, but only on DUC ?03 for the coverage model.Figure 3 also reports the performance forthe coverage model as trained by our algorithm.These results can be compared against those forthe pairwise model.
Since we are using featuresof comparable strength in both approaches, aswell as the same greedy algorithm and structuralSVM learning method, this comparison largelyreflects the quality of models themselves.
On the?04 dataset both models achieve the same perfor-mance while on ?03 the pairwise model performssignificantly (p ?
0.05) better than the coveragemodel.Overall, the pairwise model appears to performslightly better than the coverage model with thedatasets and features we used.
Therefore, we fo-cus on the pairwise model in the following.5.2 How fast does the algorithm learn?Hand-tuned approaches have limited flexibility.Whenever we move to a significantly differentcollection of documents we have to reinvest timeto retune it.
Learning can make this adaptationto a new collection more automatic and faster ?especially since training data has to be collectedeven for manual tuning.Figure 4 evaluates how effectively the learn-ing algorithm can make use of a given amount oftraining data.
In particular, the figure shows the230Figure 4: Learning curve for the pairwise model onDUC ?04 dataset showing ROUGE-1 F scores fordifferent numbers of learning examples (logarithmicscale).
The dashed line represents the preformance ofthe hand-tuned model.learning curve for our approach.
Even with veryfew training examples, the learning approach al-ready outperforms the baseline.
Furthermore, atthe maximum number of training examples avail-able to us the curve still increases.
We thereforeconjecture that more data would further improveperformance.5.3 Where is room for improvement?To get a rough estimate of what is actually achiev-able in terms of the final ROUGE-1 F score, welooked at different ?upper bounds?
under vari-ous scenarios (Figure 5).
First, ROUGE scoreis computed by using four manual summariesfrom different assessors, so that we can estimateinter-subject disagreement.
If one computes theROUGE score of a held-out summary against theremaining three summaries, the resulting perfor-mance is given in the row labeled human of Fig-ure 5.
It provides a reasonable estimate of humanperformance.Second, in extractive summarization we re-strict summaries to sentences from the documentsthemselves, which is likely to lead to a reduc-tion in ROUGE.
To estimate this drop, we use thegreedy algorithm to select the extractive summarythat maximizes ROUGE on the test documents.The resulting performance is given in the row ex-tractive of Figure 5.
On both dataset, the dropin performance for this (approximately3) optimal3We compared the greedy algorithm with exhaustivesearch for up to three selected sentences (more than thatwould take too long).
In about half the cases we got the samesolution, in other cases the soultion was on average about 1%extractive summary is about 10 points of ROUGE.Third, we expect some drop in performance,since our model may not be able to fit the optimalextractive summaries due to a lack of expressive-ness.
This can be estimated by looking at train-ing set performance, as reported in row model fitof Figure 5.
On both datasets, we see a drop ofabout 5 points of ROUGE performance.
Addingmore and better features might help the model fitthe data better.Finally, a last drop in performance may comefrom overfitting.
The test set ROUGE scores aregiven in the row prediction of Figure 5.
Note thatthe drop between training and test performanceis rather small, so overfitting is not an issue andis well controlled in our algorithm.
We thereforeconclude that increasing model fidelity seems likea promising direction for further improvements.bound dataset ROUGE-1 Fhuman DUC ?03 0.56235extractive 0.45497model fit 0.40873prediction 0.39294human DUC ?04 0.55221extractive 0.45199model fit 0.40963prediction 0.40662Figure 5: Upper bounds on ROUGE-1 F scores: agree-ment between manual summaries, greedily computedbest extractive summaries, best model fit on the trainset (using the best C value) and the test scores of thepairwise model.5.4 Which features are most useful?To understand which features affected the finalperformance of our approach, we assessed thestrength of each set of our features.
In particu-lar, we looked at how the final test score changeswhen we removed certain features groups (de-scribed in the beginning of Section 5) as shownin Figure 6.The most important group of features are thebasic features (pure cosine similarity betweensentences) since removing them results in thelargest drop in performance.
However, other fea-tures play a significant role too (i.e.
only the ba-sic ones are not enough to achieve good perfor-below optimal confirming that greedy selection works quitewell.231mance).
This confirms that performance can beimproved by adding richer fatures instead of us-ing only a single similarity score as in Lin andBilmes (2010).
Using learning for these complexmodel is essential, since hand-tuning is likely tobe intractable.The second most important group of featuresconsidering the drop in performance (i.e.
loca-tion) looks at positions of sentences in the arti-cles.
This makes intuitive sense because the firstsentences in news articles are usually packed withinformation.
The other three groups do not have asignificant impact on their own.removed ROUGE-1 Fgroupnone 0.40662basic 0.38681all except basic 0.39723location 0.39782sent+doc 0.39901cap+stop+len 0.40273minmax 0.40721Figure 6: Effects of removing different feature groupson the DUC ?04 dataset.
Bold font marks significantdifference (p ?
0.05) when compared to the full pari-wise model.
The most important are basic similar-ity features including all words (similar to (Lin andBilmes, 2010)).
The last feature group actually low-ered the score but is included in the model because weonly found this out later on DUC ?04 dataset.5.5 How important is it to train withmultiple summaries?While having four manual summaries may be im-portant for computing a reliable ROUGE scorefor evaluation, it is not clear whether such an ap-proach is the most efficient use of annotator re-sources for training.
In our final experiment, wetrained our method using only a single manualsummary for each set of documents.
When us-ing only a single manual summary, we arbitrarilytook the first one out of the provided four refer-ence summaries and used only it to compute thetarget label for training (instead of using averageloss towards all four of them).
Otherwise, the ex-perimental setup was the same as in the previoussubsections, using the pairwise model.For DUC ?04, the ROUGE-1 F score obtainedusing only a single summary per document setwas 0.4010, which is slightly but not significantlylower than the 0.4066 obtained with four sum-maries (as shown on Figure 3).
Similarly, on DUC?03 the performance drop from 0.3929 to 0.3838was not significant as well.Based on those results, we conjecture that hav-ing more documents sets with only a single man-ual summary is more useful for training thanfewer training examples with better labels (i.e.multiple summaries).
In both cases, we spendapproximately the same amount of effort (as thesummaries are the most expensive component ofthe training data), however having more trainingexamples helps (according to the learning curvepresented before) while spending effort on multi-ple summaries appears to have only minor benefitfor training.6 ConclusionsThis paper presented a supervised learning ap-proach to extractive document summarizationbased on structual SVMs.
The learning methodapplies to all submodular scoring functions, rang-ing from pairwise-similarity models to coverage-based approaches.
The learning problem is for-mulated into a convex quadratic program and wasthen solved approximately using a cutting-planemethod.
In an empirical evaluation, the structuralSVM approach significantly outperforms conven-tional hand-tuned models on the DUC ?03 and?04 datasets.
A key advantage of the learn-ing approach is its ability to handle large num-bers of features, providing substantial flexibilityfor building high-fidelity summarization models.Furthermore, it shows good control of overfitting,making it possible to train models even with onlya few training examples.AcknowledgmentsWe thank Claire Cardie and the members of theCornell NLP Seminar for their valuable feedback.This research was funded in part through NSFAwards IIS-0812091 and IIS-0905467.ReferencesT.
Berg-Kirkpatrick, D. Gillick and D. Klein.
JointlyLearning to Extract and Compress.
In Proceedingsof ACL, 2011.S.
Brin and L. Page.
The Anatomy of a Large-Scale232Hypertextual Web Search Engine.
In Proceedings ofWWW, 1998.J.
Carbonell and J. Goldstein.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Proceedings of SI-GIR, 1998.J.
M. Conroy and D. P. O?leary.
Text summarization viahidden markov models.
In Proceedings of SIGIR,2001.H.
Daume?
III.
Practical Structured Learning Tech-niques for Natural Language Processing.
Ph.D.Thesis, 2006.G.
Erkan and D. R. Radev.
LexRank: Graph-basedLexical Centrality as Salience in Text Summariza-tion.
In Journal of Artificial Intelligence Research,Vol.
22, 2004, pp.
457?479.E.
Filatova and V. Hatzivassiloglou.
Event-Based Ex-tractive Summarization.
In Proceedings of ACLWorkshop on Summarization, 2004.T.
Finley and T. Joachims.
Training structural SVMswhen exact inference is intractable.
In Proceedingsof ICML, 2008.D.
Gillick and Y. Liu.
A scalable global model for sum-marization.
In Proceedings of ACL Workshop onInteger Linear Programming for Natural LanguageProcessing, 2009.J.
Goldstein, V. Mittal, J. Carbonell, and M.Kantrowitz.
Multi-document summarization by sen-tence extraction.
In Proceedings of NAACL-ANLP,2000.S.
Khuller, A. Moss and J. Naor.
The budgeted maxi-mum coverage problem.
In Information ProcessingLetters, Vol.
70, Issue 1, 1999, pp.
39?45.J.
M. Kleinberg.
Authoritative sources in a hyperlinkedenvironment.
In Journal of the ACM, Vol.
46, Issue5, 1999, pp.
604-632.A.
Kulesza and B. Taskar.
Learning DeterminantalPoint Processes.
In Proceedings of UAI, 2011.J.
Kupiec, J. Pedersen, and F. Chen.
A trainable docu-ment summarizer.
In Proceedings of SIGIR, 1995.L.
Li, Ke Zhou, G. Xue, H. Zha, and Y. Yu.
Enhanc-ing Diversity, Coverage and Balance for Summa-rization through Structure Learning.
In Proceedingsof WWW, 2009.H.
Lin and J. Bilmes.
2010.
Multi-document summa-rization via budgeted maximization of submodularfunctions.
In Proceedings of NAACL-HLT, 2010.H.
Lin and J. Bilmes.
2011.
A Class of Submodu-lar Functions for Document Summarization.
In Pro-ceedings of ACL-HLT, 2011.C.
Y. Lin and E. Hovy.
Automatic evaluation of sum-maries using N-gram co-occurrence statistics.
InProceedings of NAACL, 2003.F.
T. Martins and N. A. Smith.
Summarization witha joint model for sentence extraction and compres-sion.
In Proceedings of ACL Workshop on IntegerLinear Programming for Natural Language Process-ing, 2009.R.
McDonald.
2007.
A Study of Global Inference Al-gorithms in Multi-document Summarization.
In Ad-vances in Information Retrieval, Lecture Notes inComputer Science, 2007, pp.
557?564.D.
Metzler and T. Kanungo.
Machine learned sen-tence selection strategies for query-biased summa-rization.
In Proceedings of SIGIR, 2008.R.
Mihalcea.
2004.
Graph-based ranking algorithmsfor sentence extraction, applied to text summa-rization.
In Proceedings of the ACL on Interactiveposter and demonstration sessions, 2004.R.
Mihalcea and P. Tarau.
Textrank: Bringing orderinto texts.
In Proceedings of EMNLP, 2004.T.
Nomoto and Y. Matsumoto.
A new approach to un-supervised text summarization.
In Proceedings ofSIGIR, 2001.V.
Qazvinian, D. R. Radev, and A. O?zgu?r.
2010.
Cita-tion Summarization Through Keyphrase Extraction.In Proceedings of COLING, 2010.K.
Raman, T. Joachims and P. Shivaswamy.
StructuredLearning of Two-Level Dynamic Rankings.
In Pro-ceedings of CIKM, 2011.G.
Salton and C. Buckley.
Term-weighting approachesin automatic text retrieval.
In Information process-ing and management, 1988, pp.
513?523.D.
Shen, J. T. Sun, H. Li, Q. Yang, and Z. Chen.Document summarization using conditional ran-dom fields.
In Proceedings of IJCAI, 2007.A.
Swaminathan, C. V. Mathew and D. Kirovski.Essential Pages.
In Proceedings of WI-IAT, IEEEComputer Society, 2009.I.
Tsochantaridis, T. Hofmann, T. Joachims and Y. Al-tun.
Large margin methods for structured and inter-dependent output variables.
In Journal of MachineLearning Research, Vol.
6, 2005, pp.
1453-1484.X.
Wan, J. Yang, and J. Xiao.
Collabsum: Exploit-ing multiple document clustering for collaborativesingle document summarizations.
In Proceedings ofSIGIR, 2007.Y.
Yue and T. Joachims.
Predicting diverse subsets us-ing structural svms.
In Proceedings of ICML, 2008.233
