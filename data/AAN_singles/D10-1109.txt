Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1119?1128,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsFunction-based question classification for general QAFan Bu, Xingwei Zhu, Yu Hao and Xiaoyan ZhuState Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Sci.
and Tech., Tsinghua Universitybuf08@mails.tsinghua.edu.cnetzhu192@hotmail.comhaoyu@mail.tsinghua.edu.cnzxy-dcs@tsinghua.edu.cnAbstractIn contrast with the booming increase of inter-net data, state-of-art QA (question answering)systems, otherwise, concerned data from spe-cific domains or resources such as search en-gine snippets, online forums and Wikipedia ina somewhat isolated way.
Users may welcomea more general QA system for its capabilityto answer questions of various sources, inte-grated from existed specialized sub-QA en-gines.
In this framework, question classifica-tion is the primary task.However, the current paradigms of questionclassification were focused on some speci-fied type of questions, i.e.
factoid questions,which are inappropriate for the general QA.In this paper, we propose a new question clas-sification paradigm, which includes a ques-tion taxonomy suitable to the general QA anda question classifier based on MLN (Markovlogic network), where rule-based methods andstatistical methods are unified into a singleframework in a fuzzy discriminative learningapproach.
Experiments show that our methodoutperforms traditional question classificationapproaches.1 IntroductionDuring a long period of time, researches on questionanswering are mainly focused on finding short andconcise answers from plain text for factoid questionsdriven by annual trackes such as CLEF, TREC andNTCIR.
However, people usually ask more complexquestions in real world which cannot be handled bythese QA systems tailored to factoid questions.During recent years, social collaborative applica-tions begin to flourish, such asWikipedia, Facebook,Yahoo!
Answers and etc.
A large amount of semi-structured data, which has been accumulated fromthese services, becomes new sources for questionanswering.
Previous researches show that differentsources are suitable for answering different ques-tions.
For example, the answers for factoid questionscan be extracted from webpages with high accuracy,definition questions can be answered by correspond-ing articles in wikipedia(Ye et al, 2009) while com-munity question answering services provide com-prehensive answers for complex questions(Jeon etal., 2005).
It will greatly enhance the overall per-formance if we can classify questions into severaltypes, distribute each type of questions to suitablesources and trigger corresponding strategy to sum-marize returned answers.Question classification (QC) in factoid QA is toprovide constraints on answer types that allows fur-ther processing to pinpoint and verify the answer(Li and Roth, 2004).
Usually, questions are classi-fied into a fine grained content-based taxonomy(e.g.UIUC taxonomy (Li and Roth, 2002)).
We can-not use these taxonomies directly.
To guide ques-tion distribution and answer summarization, ques-tions are classified according to their functions in-stead of contents.Motivated by related work on user goal classi-fication(Broder, 2002; Rose and Levinson, 2004) ,we propose a function-based question classificationcategory tailored to general QA.
The category con-tain six types, namely Fact, List, Reason, Solution,Definition and Navigation.
We will introduced this1119category in detail in Section 2.To classify questions effectively, we unify rule-based methods and statistical methods into a singleframework.
Each question is splited into functionalwords and content words.
We generate strict pat-terns from functional words and soft patterns fromcontent words.
Each strict pattern is a regular ex-pression while each soft pattern is a bi-gram clus-ter.
Given a question, we will evaluate its matchingdegree to each patterns.
The matching degree is ei-ther 0 or 1 for strict pattern and between 0 and 1 forsoft pattern.
Finally, Markov logic network (MLN)(Richardson and Domingos, 2006) is used to com-bine and evaluate all the patterns.The classical MLN maximize the probability ofan assignment of truth values by evaluating theweights of each formula.
However, the real worldis full of uncertainty and is unnatural to be repre-sented by a set of boolean values.
In this paper,we propose fuzzy discriminative weight learning ofMarkov logic network.
This method takes degreesof confidence of each evidence predicates into ac-count thus can model the matching degrees betweenquestions and soft patterns.The remainder of this paper is organized as fol-lows: In the next section we review related workon question classification, query classification andMarkov logic network.
Section 2 gives a detailedintroduction to our new taxonomy for general QA.Section 4 introduces fuzzy discriminative weightlearning of MLN and our methodology to extractstrict and soft patterns.
In Section 5 we compare ourmethod with previous methods on Chinese questiondata from Baidu Zhidao and Sina iAsk.
In the lastsection we conclude this work.Although we build patterns and do experimentson Chinese questions, our method does not take ad-vantage of the particularity of Chinese language andthus can be easily implemented on other languages.2 Related WorkMany question taxonomies have been proposed inQA community.
Lehnert (1977) developed the sys-tem QUALM based on thirteen conceptual cate-gories which are based on a theory of memory repre-sentation.
On the contrary, the taxonomy proposedby Graesser et al (1992) has foundations both in the-ory and in empirical research.
Both of these tax-onomies are for open-domain question answering.With the booming of internet, researches onquestion answering are becoming more practical.Most taxonomies proposed are focused on factoidquestions, such as UIUC taxonomy (Li and Roth,2002).
UIUC taxonomy contains 6 coarse classes(Abbreviation, Entity, Description, Human, Lo-cation and Numeric Value) and 50 fine classes.All coarse classes are factoid oriented except De-scription.
To classify questions effectively, Re-searchers have proposed features of different levels,such as lexical features, syntactic features (Nguyenet al, 2007; Moschitti et al, 2007) and semantic fea-tures (Moschitti et al, 2007; Li and Roth, 2004).Zhang and Lee (2003) compared five machine learn-ing methods and found SVM outperformed the oth-ers.In information retrieval community, researchershave described frameworks for understanding goalsof user searches.
Generally, web queries are classi-fied into four types: Navigational, Informational,Transactional (Broder, 2002) and Resource (Roseand Levinson, 2004).
Lee et al (2005) automaticallyclassify Navigational and Informational queriesbased on past user-click behavior and anchor-linkdistribution.
Jansen and Booth (2010) investigatethe correspondence between three user intents andeighteen topics.
The result shows that user intentsdistributed unevenly among different topics.Inspired by Rose and Levinson (2004)?s work inuser goals classification, Liu et al (2008) describea three-layers cQA oriented question taxonomy anduse it to determine the expected best answer typesand summarize answers.
Other than Navigational,Informational and Transactional, the first layercontains a new Social category which represents thequestions that do not intend to get an answer but toelicit interaction with other people.
Informationalcontains two subcategories Constant and Dynamic.Dynamic is further divided into Opinion, Context-Dependent and Open.Markov logic network (MLN) (Richardson andDomingos, 2006) is a general model combiningfirst-order logic and probabilistic graphical modelsin a single representation.
Illustratively, MLN is afirst-order knowledge base with a weight attachedto each formula.
The weights can be learnt ei-1120TYPE DESCRIPTION EXAMPLES1.
Fact People ask these questions for general facts.The expected answer will be a short phrase.Who is the presidentof United States?2.
List People ask these questions for a list of an-swers.
Each answer will be a single phraseor a phrase with explanations or comments.List Nobel pricewinners in 1990s.Which movie star doyou like best?3.
Reason People ask these questions for opinions or ex-planations.
A good answer summary shouldcontain a variety of opinions or comprehen-sive explanations.
Sentence-level summariza-tion can be employed.Is it good to drinkmilk while fasting?What do you think ofAvatar?4.
Solution People ask these questions for problem shoot-ing.
The sentences in an answer usually havelogical order thus the summary task cannot beperformed on sentence level.What should I doduring an earthquake?How to make pizzas?5.
Definition People ask these questions for description ofconcepts.
Usually these information can befound in Wikipedia.
If the answer is a toolong, we should summarize it into a shorterone.Who is Lady Gaga?What does the Matrixtell about?6.
Navigation People ask these questions for finding web-sites or resources.
Sometimes the websites aregiven by name and the resources are given di-rectly.Where can I downloadthe beta version ofStarCraft 2?Table 1: Question Taxonomy for general QAther generatively (Richardson and Domingos, 2006)or discriminatively (Singla and Domingos, 2005).Huynh and Mooney (2008) applies ?1-norm regu-larized MLE to select candidate formulas generatedby a first-order logic induction system and preventoverfitting.
MLN has been introduced to NLP andIE tasks such as semantic parsing (Poon et al, 2009)and entity relation extraction (Zhu et al, 2009).3 A Question TaxonomyWe suggest a function-based taxonomy tailored togeneral QA systems by two principles.
First, ques-tions can be distributed into suitable QA subsys-tems according to their types.
Second, we canemploy suitable answer summarization strategy foreach question type.
The taxonomy is shown in Tab.1.At first glance, classifying questions onto this tax-onomy seems a solved problem for English ques-tions because of interrogative words.
In most cases,a question starting with ?Why?
is for reason and?How?
is for solution.
But it is not always the casefor other languages.
From table 2 we can see twoquestions in Chinese share same function word ?????
but have different types.In fact, even in English, only using interroga-tive words is not enough for function-based ques-tion classification.
Sometimes the question contentis crucial.
For example, for question ?Who is thecurrent president of U.S.
?
?, the answer is ?BarakObama?
and the type is Fact.
But for question ?Whois Barak Obama?
?, it will be better if we return thefirst paragraph from the corresponding Wiki articleinstead of a short phrase ?current president of U.S.?.Therefore the question type will be Definition.Compared to Wendy Lehnert?s or ArthurGraesser?s taxonomy, our taxonomy is more prac-tical on providing useful information for question1121Question ????????
?How to cook Kung Pao Chicken?Type SolutionQuestion ??????????
?What do you think of Avatar?Type ReasonTable 2: Two Chinese questions share same functionwords but have different typesextraction and summarization.
Compared to ours,The UIUC taxonomy is too much focused on factoidquestions.
Apart from Description, all coarse typesin UIUC can be mapped into Fact.
The cQAtaxonomy proposed in Liu et al (2008) has similargoal with ours.
But it is hard to automaticallyclassify questions into that taxonomy, especially fortypes Constant, Dynamic and Social.
Actually theauthor did not give implementation in the paper aswell.
To examine reasonableness of our taxonomy,we select and manually annotate 5800 frequentasked questions from Baidu Zhidao (see Section5.1).
The distribution of six types is shown in Fig.1.
98.5 percent of questions can be categorizedinto our taxonomy.
The proportion of each type isbetween 7.5% and 23.8%.The type Navigation was originally proposed inIR community and did not cause too much concernsin previous QA researches.
But from Fig.
1 wecan see that navigational questions take a substan-tial proportion in cQA data.Moreover, we can further develop subtypes foreach type.
For example, most categories in UIUCReason18.1%Fact14.4%Solution19.7%Navigation14.8%List23.8% Definition7.5% Other1.5%Figure 1: Distribution of six types in Baidu Zhidao datataxonomy can be regarded as refinement to Fact andNavigation can be refined into Resource and Web-site.
We will not have further discussion on this is-sue.4 MethodologyMany efforts have been made to take advantage ofgrammatical , semantic and lexical features in ques-tion classification.
Zhang and Lee (2003) proposeda SVM based system which used tree kernel to in-corporate syntactic features.In this section, we propose a new question clas-sification methodology which combines rule-basedmethods and statistical methods by Markov logicnetwork.
We do not use semantic and syntactic fea-tures for two reasons.
First, the questions posted ononline communities are casually written which can-not be accurately parsed by NLP tools, especially forChinese.
Second, the semantic and syntactic pars-ing are time consuming thus unpractical to be usedin real systems.We will briefly introduce MLN and fuzzy dis-criminative learning in section 4.1.
The constructionof strict patterns and soft patterns will be shown in4.2 and 4.3.
In section 4.4 we will give details onMLN construction, inference and learning.4.1 Markov Logic NetworkA first-order knowledge base contains a set of for-mulas constructed from logic operators and symbolsfor predicates, constants, variables and functions.An atomic formula or atom is a predicate symbol.Formulas are recursively constructed from atomicformulas using logical operators.
The groundingof a predicate (formula) is a replacement of all ofits arguments (variables) by constants.
A possibleworld is an assignment of truth values to all possiblegroundings of all predicates.In first-order KB, if a possible world violateseven one formula, it has zero probability.
Markovlogic is a probabilistic extension and softens the hardconstraints by assigning a weight to each formula.When a possible world violates one formula in theKB, it is less probable.
The higher the weight, thegreater the difference in log probability between aworld that satisfies the formula and a world doesnot.
Formally, Markov logic network is defined as1122follows:Definition 1 (Richardson & Domingos 2004) AMarkov logic network L is a set of pairs (?
?, ??
),where ?
?is a formula in first-order logic and ?
?is areal number.
Together with a finite set of constantsC = {?1, ?2, ..., ????
}, it defines a Markov network?
?,?as follows:1.
?
?,?contains one binary node for each pos-sible grounding of each predicate appearing inL.
The value of the node is 1 if the ground pred-icate is true, and 0 otherwise.2.
?
?,?contains one feature for each possiblegrounding of each formula ?
?in L. The valueof this feature is 1 if the ground formula is true,and 0 otherwise.
The weight of the feature isthe ?
?associated with ?
?in L.There is an edge between two nodes of ?
?,?iffthe corresponding grounding predicates appear to-gether in at least one grounding of one formula in?.
An MLN can be regarded as a template for con-structing Markov networks.
From Definition 1 andthe definition of Markov networks, the probabilitydistribution over possible worlds ?
specified by theground Markov network ?
?,?is given by?
(?
= ?)
=1?exp(???=1????(?
))MLN weights can be learnt genera-tively(Richardson and Domingos, 2006) ordiscriminatively(Singla and Domingos, 2005).
Indiscriminative weight learning, ground atom set ?is partitioned into a set of evidence atoms ?
anda set of query atoms ?
.
The goal is to correctlypredict the latter given the former.
In this paper, wepropose fuzzy discriminative weight learning whichcan take the prior confidence of each evidence atominto account.Formally, we denote the ground formula set by?
.
Suppose each evidence atom ?
is given with aprior confidence ???
[0, 1], we define a confidencefunction ?
: ?
?
[0, 1] as follows.
For each groundatom ?, if ?
?
?
then we have ?(?)
= ?
?, else?(?)
= 1.
For each ground non-atomic formulas, ?is defined on standard fuzzy operators, which are?(??)
= 1?
?(?)?(?1?
?2) = min(?
(?1), ?(?2))?(?1?
?2) = max(?
(?1), ?
(?2))We redefined the conditional likelihood of ?given ?
as?
(???)
=1??exp???????????
(?, ?)??=1??exp????????????
(?, ?)?
?Where ?
?is the set of ground formulas involvingquery atoms, ?
?is the set of formulas with at leastone grounding involving a query atom and ???
(?, ?
)is the sum of confidence of the groundings of the i thformula involving query atoms.
The gradient of theconditional log-likelihood (CLL) is????log??(???
)= ???
(?, ?)??????(????)???
(?, ??
)= ???
(?, ?)?
??[???
(?, ?)]
(1)By fuzzy discriminative learning we can incorpo-rate evidences of different confidence levels into onelearning framework.
Fuzzy discriminative learn-ing will reduce to traditional discriminative learningwhen all prior confidences equal to 1.4.2 Strict PatternsIn our question classification task, we find functionwords are much more discriminative and less sparsethan content words.
Therefore, we extract strict pat-terns from function words and soft patterns fromcontent words.
The definition of content and func-tion words may vary with languages.
In this paper,nouns, verbs, adjectives, adverbs, numerals and pro-nouns are regarded as content words and the rest arefunction words.The outline of strict pattern extraction is shownin Alg.
1.
In line 3, we build template ???
by re-moving punctuations and replacing each characterin each content word by a single dot.
In line 4, wegenerate patterns from the template as follows.
Firstwe generate n-grams(n is between 2 and ? )
from1123Algorithm 1: Strict Pattern ExtractionInput: Question Set ?
= {?1, ?2...??
},Parameters ?
and ?Output: Pattern Set ?Initialize Pattern Set ?
;1for each Question ?
?do2String ???=ReplaceContentWords(??,?.?
);3Pattern Set ???=GeneratePatterns(???,?
);4for each Pattern ?
in ??
?do5if ?
in ?
then6UpdateTypeFreq(?,?
);7else8Add ?
to ?
;9Merge similar patterns in ?
;10Sort ?
by Information Gain on type11frequencies;return top ?
Patterns in ?
;12???
during which each dot is treated as a characterof zero length.
For coverage concern, if a gener-ated n-gram ?
is not start(end) with dot, we buildanother n-gram ??
by adding a dot before(behind) ?and add both ?
and ??
into n-gram set.
Then for eachn-gram, we replace each consecutive dot sequenceby ?.*?
and the n-gram is transformed into a regularexpression.
A example is shown in Tab.
3.
Althoughgenerated without exhaustively enumerating all pos-sible word combinations, these regular expressionscan capture most long range dependencies betweenfunction words.Each pattern consists of a regular expression aswell as its frequency in each type of questions.
StillQuestion ??????????
?Can I launch online banking serviceson internet?Template ?..??....
?Patterns .*?.*?
.*?.*?.
*(?=4) .*??.
* .*??.*?.*??.*?.
* .*?.*??.*.*?.*??.*?.
* ?.*?.*?.*??.
* ?.*??.*?.*?.*?.
*Table 3: Strict patterns generated from a questionfrom Alg.
1, in line 5-9, if a pattern ?
in question ?
?with type ?
is found in ?
, we just update the fre-quency of ?
in ?, else ?
is added to ?
with onlyfreq.
?
equals to 1.
In line 10, we merge similarpatterns in ?
.
two patterns ?1and ?2are similar iff?q?QmatchP(q,p1) ?
matchP(q,p2), in whichmatchP is defined in Section 4.4.Since a large number of patterns are generated,it is unpractical to evaluate all of them by Markovlogic network.
We sort patterns by information gainand only choose top?
?good?
patterns in line 11-12of Alg.
1.
A ?good?
pattern should be discriminativeand of wide coverage.
The information gain IG of apattern ?
is defined asIG(?)
= ?
(?)???=1?
(????)
log?
(????)+?
(?)???=1?
(????)
log?
(????)????=1?
(??)
log?
(??
)in which ?
is the number of question types, ?
(??)
isthe probability of a question having type ?
?, ?
(?)(or?
(?))
is the probability of a question matching(ornot matching) pattern ?.
?
(????
)(or ?
(????))
isthe probability of a question having type ?
?giventhe condition that the question matches(or does notmatch) pattern ?.
These probabilities can be approx-imately calculated by type and pattern frequencieson training data.
From the definition we can seethat information gain is suitable for pattern selec-tion.
The more questions a pattern ?
matches andthe more unevenly the matched questions distributeamong questions types, the higher IG(?)
will be.4.3 Soft PatternsApart from function words, content words are alsoimportant in function-based question classification.Content words usually contain topic informationwhich can be a good complement to function words.Previous research on query classification(Jansen andBooth, 2010) shows that user intents distribute un-evenly among topics.
Moreover, questions given byusers may be incomplete and contain not functionwords.
For these questions, we can only predict thequestion types from topic information.Compared with function words, content wordsdistribute much more sparsely among questions.1124When we represent topic information by contentwords (or bi-grams), since the training set are smalland less frequent words (or bi-grams) are filteredto prevent over-fitting, those features would be toosparse to predict further unseen questions.To solve this problem, we build soft patterns onquestion set.
Each question is represented by aweighted vector of content bi-grams in which theweight is bi-gram frequency.
Cosine similarity isused to compute the similarity between vectors.Then we cluster question vectors using a simplesingle-pass clustering algorithm(Frakes and Yates,1992).
That is, for each question, we compute itssimilarity with each centroid of existing cluster.
Ifthe similarity with nearest cluster is greater thana minimum similarity threshold ?1, we assign thisquestion to that cluster, else a new cluster is createdfor this question.Each cluster is defined as a soft pattern.
Unlikestrict patterns, a question can match a soft patternto some extent.
In this paper, the degree of match-ing is defined as the cosine similarity between ques-tion and centroid of cluster.
Soft patterns are flexibleand could alleviate the sparseness of content words.Also, soft patterns can be pre-filtered by informationgain described in 4.2 if necessary.4.4 ImplementationCurrently, we model patterns into MLN as follows.The main query predicate is Type(q,t), whichis true iff question q has type t. For strict pat-terns, the evidence predicate MatchP(q,p) is trueiff question q is matched by strict pattern p. Theconfidence of MatchP(q,p) is 1 for each pair of(q,p).
For soft patterns, the evidence predicateMatchC(q,c) is true iff the similarity of questionq and the cluster c is greater than a minimum simi-larity requirement ?2.
If MatchC(q,c) is false, itsconfidence is 1, else is the similarity between q andc.We represent the relationship between patternsand types by a group of formulas below.MatchP(q,+p)?Type(q,+t)????=??Type(q,t?
)The ?+p, +t?
notation signifies that the MLN con-tains an instance of this formula for each (pattern,type) pair.
For the sake of efficacy, for each pattern-type pair (p,t), if the proportion of type t in ques-tions matching p is less than a minimum require-ment ?, we remove corresponding formula fromMLN.Similarly, we incorporate soft patterns byMatchC(q,+c)?Type(q,+t)????=??Type(q,t?
)Our weight learner use ?1-regularization (Huynhand Mooney, 2008) to select formulas and preventoverfitting.
A good property of ?1-regularization isits tendency to force parameters to exact zero bystrongly penalizing small terms (Lee et al, 2006).After training, we can simply remove the formulaswith zero weights.Formally, to learn weight for each formula, weiteratively solve ?1-norm regularized optimizationproblem:?
: ?
?= argmax?log??(???)?
???
?1where ?.
?1is ?1-norm and parameter ?
controls thepenalization of non-zero weights.
We implement theOrthant-Wise Limited-memory Quasi-Newton algo-rithm(Andrew and Gao, 2007) to solve this opti-mization.Since we do not model relations among questions,the derived markov network ?
?,?can be broken upinto separated subgraphs by questions and the gradi-ent of CLL(Eq.
1) can be computed locally on eachsubgraph as????log??(???)=??(???(?
?, ??)???[???(?
?, ??
)])(2)in which ?
?and ?
?are the evidence and query atomsinvolving question ?.
Eq.
2 can be computed fastwithout approximation.We initialize formula weights to the same posi-tive value ?.
Iteration started from uniform priorcan always converge to a better local maximum thangaussian prior in our task.5 Experiments5.1 Data PreparationTo the best of our knowledge, there is not generalQA system(the system which can potentially answer1125all kinds of questions utilizing data from heteroge-neous sources) released at present.
Alteratively, wetest our methodology on cQA data based on obser-vation that questions on cQA services are of var-ious length, domain independent and wrote infor-mally(even with grammar mistakes).
General QAsystems will meet these challenges as well.In our experiments, both training and test dataare from Chinese cQA services Baidu Zhidao andSina iAsk.
To build training set, we randomly select5800 frequent-asked questions from Baidu Zhidao.A question is frequent-asked if it is lexically simi-lar to at least five other questions.
Then we ask 10native-speakers to annotate these questions accord-ing to question title and question description.
If anannotator cannot judge type from question title, hecan view the question description.
If type can bejudged from the description, the question title willbe replaced by a sentence selected from it.
If not,this question will be labeled as Other.Each question is annotated by two people.
If aquestion is labeled different types, another annotatorwill judge it and make final decision.
If this annota-tor cannot judge the type, this question will also belabeled as Other.
As a result, disagreements showup on eighteen percents of questions.
After the thirdannotator?s judgment, the distribution of each typeis shown in Fig.
1.To examine the generalization capabilities, thetest data is composed of 700 questions randomly se-lected from Baidu Zhidao and 700 questions fromSina iAsk.
The annotation process on test data is assame as the one on training data.5.2 Methods Compared and ResultsWe compare four methods listed as follows.SVM with bi-grams.
We extract bi-grams fromquestions on training data as features.
After filteringthe ones appearing only once, we collect 5700 bi-grams.
LIBSVM(Chang and Lin, 2001)is used asthe multi-class SVM classifier.
All parameters areadjusted to maximize the accuracy on test data.
Wedenote this method as ?SB?
;MLN with bi-grams.
To compare MLN andSVM, we treat bi-grams as strict patterns.
If a ques-tion contain a bi-gram, it matches the correspondingpattern.
We set ?
= 0.01, ?
= 0.3 and ?
= 0.3.As a result, 5700 bi-grams are represented by 10485formulas.
We denote this method as ?MB?
;MLNwith strict patterns and bi-grams.
We asktwo native-speakers to write strict patterns for eachtype.
The pattern writers can view training data forreference and write any Java-style regular expres-sions.
Then we carefully choose 50 most reliablepatterns.
To overcome the low coverage, We alsouse the method described in Sec.
4.2 to automati-cally extract strict patterns from training set.
We firstselect top 3000 patterns by information gain, mergethese patterns with hand-crafted ones and combinesimilar patterns.
Then we represent these patternsby formulas and learn the weight of each formula byMLN.
After removing the formula with low weights,we finally retain 2462 patterns represented by 3879formulas.
To incorporate content information, weextract bi-grams from questions with function wordsremoved and remove the ones with frequency lowerthan two.
With bi-grams added, we get 8173 formu-las in total.
All parameters here are the same as in?MB?.
We denote this method as ?MSB?
;MLN with strict patterns and soft patterns.
Toincorporate content information, We cluster ques-tions on training data with similarity threshold ?1=0.4 and get 2588 clusters(soft patterns) which arerepresented by 3491 formulas.
We these soft pat-terns with strict patterns extracted in ?MSB?, whichadd up to 7370 formulas.
We set ?2= 0.02 and theother parameters as same as in ?MB?.
We denotethis method as ?MSS?
;We separate test set into easy set and difficult set.A question is classified into easy set iff it containsfunction-words.
As a result, the easy set contains1253 questions.
We measure the accuracy of thesefour methods on easy data and the whole test data.The results are shown in Tab 4.
From the results wecan see that all methods perform better on easy ques-tions and MLN outperforms SVM using same bi-gram features.
Although MSS is inferior to MSB onF.
num Easy data All dataSB NA 0.724 0.685MB 10485 0.722 0.692MSB 8173 0.754 0.714MSS 7370 0.752 0.717Table 4: Experimental results on Chinese cQA data1126F L S R D NPrec.
0.63 0.65 0.83 0.76 0.69 0.55Recall 0.55 0.74 0.86 0.76 0.44 0.58F10.59 0.69 0.84 0.76 0.54 0.56Table 5: Precision, recall and F-score on each typeeasy questions, it shows better overall performanceand uses less formulas.We further investigate the performance on eachtype.
The precision, recall and F1-score of each typeby method MSS are shown in Tab.
5.
From the re-sults we can see that the performance on Solutionand Reason are significantly better than the others.It is because the strict patterns for this two types aresimple and effective.
A handful of patterns couldcover a wide range of questions with high precision.It is difficult to distinguish Fact from List becausestrict patterns for these two types are partly overlapeach other.
Sometimes we need content informationto determine whether the answer is unique.
SinceList appears more frequently than Fact on trainingset, MLN tend to misclassify Fact toListwhich leadto low recall of the former and low precision of thelatter.
The recall of Definition is very low becausemany definition questions on test set are short andonly consists of content words(e.g.
a noun phrase).This shortage could be remedied by building strictpatterns on POStagging sequence.fraction lines, college entrance exam??????????????
?...Fact: 56.4% List: 33.3% Solu.
: 5.5%lose weight, summer, fast???????????????...Reas.
: 53.8% Solu.
: 42.3% List: 3.8%TV series, interesting, recent??????????????
?...List: 84.0% Fact: 8.0% Navi.
: 2.0%converter, format, 3gp??????3gp?mp4????...Navi.
: 75% List: 18.8% Solu.
: 6.2%Table 6: Selected soft patterns on training data5.3 Case Study on Soft PatternsTo give an intuitive illustration of soft patterns, weshow some of them clustered on training data in Tab.6.
For each soft pattern, we list five most frequentbi-grams and its distribution on each type(only top 3frequent types are listed).From the results we can see that soft patterns areconsistent with our ordinary intuitions.
For exam-ple, if user ask a questions about ?TV series?, he islikely to ask for recommendation of recent TV seriesand the question have a great chance to be List.
Ifuser ask questions about ?lose weight?, he probablyask something like ?How can I lose weight fast??
or?Why my diet does not work??
.
Thus the type islikely to be Solution or Reason.6 Conclusion and Future WorkWe have proposed a new question taxonomy tai-lored to general QA on heterogeneous sources.This taxonomy provide indispensable informationfor question distribution and answer summarization.We build strict patterns and soft patterns to repre-sent the information in function words and contentwords.
Also, fuzzy discriminative weight learningis proposed for unifying strict and soft patterns intoMarkov logic network.Currently, we have not done anything fancy on thestructure of MLN.
We just showed that under uni-form prior and L1 regularization, the performanceof MLN is comparable to SVM.
To give full playto the advantages of MLN, future work will focuson fast structure learning.
Also, since questions ononline communities are classified into categories bytopic, we plan to perform joint question type infer-ence on function-based taxonomy as well as topic-based taxonomy by Markov logic.
The model willnot only capture the relation between patterns andtypes but also the relation between types in differenttaxonomy.AcknowledgmentThis work was supported mainly by Canada?s IDRCResearch Chair in Information Technology program,Project Number: 104519-006.
It is also supportedby the Chinese Natural Science Foundation grantNo.
60973104.1127ReferencesG.
Andrew and J. Gao.
2008.
Scalable training of L1-regularized log-linear models.
In Proc.
of ICML 2007,pp.
33-40.A.
Broder.
2002.
A taxonomy of Web search.
SIGIRForum, 36(2), 2002.C.C.
Chang and C.J.
Lin.
2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.W.B.
Frakes and R. Baeza-Yates, editors.
1992.
In-formation Retrieval: Data Structures and Algorithms.Prentice-Hall, 1992.A.C.
Graesser, N.K.
Person and J.D.
Huber.
1992.
Mech-anisms that generate questions.
Questions and Infor-mation Systems, pp.
167-187), Hillsdale, N.J.: Erl-baum.T.N.
Huynh and R.J. Mooney.
2008.
DiscriminativeStructure and Parameter Learning for Markov LogicNetworks.
In Proc.
of ICML 2008, pp.
416-423.B.J.
Jansen and D. Booth.
2010.
Classifying web queriesby topic and user intent.
In Proc.
of the 28th interna-tional conference on human factors in computing sys-tems, pp.
4285-4290.J.
Jeon, W.B.
Croft and J.H.
Lee.
2005.
Finding similarquestions in large question and answer archives.
InProc.
of ACM CIKM 2005,pp.
76-83.S.
Lee, V. Ganapathi and D. Koller.
2005.
Effi-cient structure learning of Markov networks using ?1-regularization.. Advances in Neural Information Pro-cessing Systems 18.U.
Lee, Z. Liu and J. Cho.
2005.
Automatic identificationof user goals in Web search.
In Proc.
of WWW 2005.W.
Lehnert.
1977.
Human and computational questionanswering.
Cognitive Science, vol.
1, 1977, pp.
47-63.X.
Li and D. Roth.
2002.
Learning question classifiers.In Proc.
of COLING 2002, pp.
556-562.X.
Li and D. Roth.
2004.
Learning question classifiers:the role of semantic information.
Natural LanguageEngineering.Y.
Liu, S. Li, Y. Cao, C.Y.
Lin, D. Han and Y. Yu.2008.
Understanding and summarizing answers incommunity-based question answering services.
InProc.
of COLING 2008.A.
Moschitti, S. Quarteroni, R. Basili and S. Manand-har.
2007.
Exploiting Syntactic and Shallow SemanticKernels for Question/Answer Classification.
In Proc.of ACL 2007.M.L.
Nguyen, T.T.
Nguyen and A. Shimazu.
2007.
Sub-tree Mining for Question Classification Problem.
InProc.
of IJCAI 2007.H.
Poon and P. Domingos.
2009.
Unsupervised semanticparsing.
In Proc.
of EMNLP 2009, pp.
1-10D.E.
Rose and D. Levinson.
2004.
Understanding usergoals in web search.
In Proc.
of WWW 2004.M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine Learning 62:107-136.P.
Singla and P. Domingos.
2005.
Discriminative Train-ing of Markov Logic Networks.
In Proc.
of AAAI2005.S.
Ye, T.S.
Chua and J. Lu.
2009.
Summarizing Defini-tion from Wikipedia.
In Proc.
of ACL 2009.D.
Zhang and W.S.
Lee.
2003.
Question classificationusing support vector machines.
In Proc.
of ACM SI-GIR 2003, pp.
26-32.J.
Zhu , Z. Nie, X. Liu, B. Zhang and J.R. Wen.
2009.StatSnowball: a Statistical Approach to Extracting En-tity Relationships.
In Proc.
of WWW 2009, pp.
101-1101128
