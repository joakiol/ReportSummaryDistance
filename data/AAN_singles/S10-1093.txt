Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417?420,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsKyoto: An Integrated System for Specific Domain WSDAitor Soroa, Eneko Agirre, Oier Lopez de LacalleUniversity of the Basque Countrya.soroa@ehu.esMonica MonachiniIstituto di Linguistica Computazionalemonica.monachini@ilc.cnr.itJessie Lo, Shu-Kai HsiehNational Taiwan Normal Universityshukai@ntnu.edu.twWauter Bosma, Piek VossenVrije Universiteit{p.vossen,w.bosma}@let.vu.nlAbstractThis document describes the prelimi-nary release of the integrated Kyoto sys-tem for specific domain WSD.
The sys-tem uses concept miners (Tybots) to ex-tract domain-related terms and producesa domain-related thesaurus, followed byknowledge-based WSD based on word-net graphs (UKB).
The resulting systemcan be applied to any language with alexical knowledge base, and is based onpublicly available software and resources.Our participation in Semeval task #17 fo-cused on producing running systems forall languages in the task, and we attainedgood results in all except Chinese.
Dueto the pressure of the time-constraints inthe competition, the system is still underdevelopment, and we expect results to im-prove in the near future.1 IntroductionIn this paper we describe the participation of theintegrated Kyoto system on the ?SemEval-2010task #17: All-words Word Sense Disambigua-tion on a Specific Domain?
task (Agirre et al,2010).
The goal of our participation was to eval-uate the preliminary release of the integrated sys-tem for specific domain WSD developed for theKyoto project1.
Besides, we wanted to test theperformance of our domain specific WSD system(Agirre et al, 2009) on this test set, and to inte-grate the thesaurus construction software (Tybots)developed for the project.
The system can be runfor any language and domain if provided with alexical knowledge base and some background doc-uments on the domain.We will first present the components of our sys-tem, followed by the experimental design and the1http://www.kyoto-project.euresults.
Finally, the conclusions are presented.2 The Kyoto System for Domain SpecificWSDWe will present in turn UKB, the Tybots, and thelexical knowledge-bases used.2.1 UKBUKB is a knowledge-based unsupervised WSDsystem which exploits the structure of an under-lying Language Knowledge Base (LKB) and findsthe most relevant concepts given an input con-text (Agirre and Soroa, 2009).
UKB starts by tak-ing the LKB as a graph of concepts G = (V,E)with a set of vertices V derived from LKB con-cepts and a set of edges E representing relationsamong them.
Giving an input context, UKB ap-plies the so called Personalized PageRank (Haveli-wala, 2002) over it to obtain the most representa-tive senses for the context.PageRank (Brin and Page, 1998) is a methodfor scoring the vertices V of a graph accordingto each node?s structural importance.
The algo-rithm can be viewed as random walk process thatpostulate the existence of a particle that randomlytraverses the graph, but at any time may jump toa new vertex with a given damping factor (alsocalled teleport probability).
After PageRank cal-culation, the final weight of node i represents theproportion of time that a random particle spendsvisiting node i after a sufficiently long time.
Instandard PageRank, the teleport vector is chosenuniformly, whereas for Personalized PageRank itis chosen from a nonuniform distribution of nodes,specified by a teleport vector.UKB concentrates the initial probability massof the teleport vector in the words occurring inthe context of the target word, causing all randomjumps on the walk to return to these words andthus assigning a higher rank to the senses linked tothese words.
Moreover, the high rank of the words417spreads through the links in the graph and makeall the nodes in its vicinity also receive high ranks.Given a target word, the system checks which isthe relative ranking of its senses, and the WSDsystem would output the one ranking highest.UKB is very flexible and can be use to performWSD on different settings, depending on the con-text used for disambiguating a word instance.
Inthis paper we use it to perform general and do-main specific WSD, as shown in section 3.
PageR-ank is calculated by applying an iterative algo-rithm until convergence below a given thresholdis achieved.
Following usual practice, we used adamping value of 0.85 and set the threshold valueat 0.001.
We did not optimize these parameters.2.2 TybotsTybots (Term Yielding Robots) are text miningsoftware that mine domain terms from corpus(e.g.
web pages), organizing them in a hierar-chical structure, connecting them to wordnets andontologies to create a semantic model for the do-main (Bosma and Vossen, 2010).
The software isfreely available using Subversion2.
Tybots try toestablish a view on the terminology of the domainwhich is as complete as possible, discovering rela-tions between terms and ranking terms by domainrelevance.Preceding term extraction, we perform tok-enization, part-of-speech tagging and lemmatiza-tion, which is stored in Kyoto Annotation For-mat (KAF) (Bosma et al, 2009).
Tybots workthrough KAF documents, acquire domain relevantterms based on the syntactic features, gather co-occurrence statistics to decide which terms are sig-nificant in the domain and produce a thesauruswith sets of related words.
Section 3.3 describesthe specific settings that we used.2.3 Lexical Knowledge basesWe used the following wordnets, as suggested bythe organizers:WN30g: English WordNet 3.0 with gloss relations(Fellbaum, 1998).Dutch: The Dutch LKB is part of the Cor-netto database version 1.3 (Vossen et al, 2008).The Cornetto database can be obtained fromthe Dutch/Flanders Taalunie3.
Cornetto com-prises taxonomic relations and equivalence rela-2http://kyoto.let.vu.nl/svn/kyoto/trunk3http://www.inl.nl/nl/lexica/780#entries #synsets #rels.
#WN30gMonolingualChinese 8,186 14,243 20,433 20,584Dutch 83,812 70,024 224,493 83,669Italian 46,724 49,513 65,567 52,524WN30g 147,306 117,522 525,351 n/aBilingualChinese-eng 8,186 141,561 566,368Dutch-eng 83,812 188,511 833,513Italian-eng 46,724 167,094 643,442Table 1: Wordnets and their sizes (entries, synsets,relations and links to WN30g).tions from both WordNet 2.0 and 3.0.
Cornettoconcepts are mapped to English WordNet 3.0.Italian: Italwordnet (Roventini et al, 2003) wascreated in the framework of the EuroWordNet,employs the same set of semantic relations usedin EuroWordNet, and includes links to WordNet3.0 synsets.Chinese: The Chinese WordNet (Version 1.6) isnow partially open to the public4(Tsai et al,2001).
The Chinese WordNet is also mapped toWordNet 3.0.Table 1 shows the sizes of the graphs createdusing each LKB as a source.
The upper part showsthe number of lexical entries, synsets and relationsof each LKB.
It also depicts the number of links toEnglish WordNet 3.0 synsets.In addition, we also created bilingual graphs forDutch, Italian and Chinese, comprising the orig-inal monolingual LKB, the links to WordNet 3.0and WordNet 3.0 itself.
We expected this richergraphs to perform better performance.
The sizesof the bilingual graphs are shown in the lower sideof Table 1.3 Experimental settingAll test documents were lemmatized and PoS-tagged using the linguistic processors availablewithin the Kyoto project.
In this section we de-scribe the submitted runs.3.1 UKB parametersWe use UKB with the default parameters.
In par-ticular, we don?t use dictionary weights, which inthe case of English come from annotated corpora.This is done in order to make the system fully un-supervised.
It?s also worth mentioning that in thedefault setting parts of speech were not used.4http://cwn.ling.sinica.edu.tw418RANK RUN P R R-NOUN R-VERBChinese- 1sense 0.562 0.562 0.589 0.5181 Best 0.559 0.559 - -- Random 0.321 0.321 0.326 0.3124 kyoto-3 0.322 0.296 0.257 0.3603 kyoto-2 0.342 0.285 0.251 0.3425 kyoto-1 0.310 0.258 0.256 0.261Dutch1 kyoto-3 0.526 0.526 0.575 0.4502 kyoto-2 0.519 0.519 0.561 0.454- 1sense 0.480 0.480 0.600 0.2913 kyoto-1 0.465 0.465 0.505 0.403- Random 0.328 0.328 0.350 0.293English1 Best 0.570 0.555 - -- 1sense 0.505 0.505 0.519 0.45410 kyoto-2 0.481 0.481 0.487 0.46222 kyoto-1 0.384 0.384 0.382 0.391- Random 0.232 0.232 0.253 0.172Italian1 kyoto-3 0.529 0.529 0.530 0.5282 kyoto-2 0.521 0.521 0.522 0.5193 kyoto-1 0.496 0.496 0.507 0.468- 1sense 0.462 0.462 0.472 0.437- Random 0.294 0.294 0.308 0.257Table 2: Overall results of our runs, including pre-cision (P) and recall (R), overall and for each PoS.We include the First Sense (1sense) and randombaselines, as well as the best run, as provided bythe organizers.3.2 Run1: UKB using contextThe first run is an application of the UKB tool inthe standard setting, as described in (Agirre andSoroa, 2009).
Given the input text, we split it insentences, and we disambiguate each sentence at atime.
We extract the lemmas which have an entryin the LKB and then apply Personalized PageR-ank over all of them, obtaining a score for everyconcept of the LKB.
To disambiguate the words inthe sentence we just choose its associated concept(sense) with maximum score.In our experiments we build a context of at least20 content words for each sentence to be disam-biguated, taking the sentences immediately beforewhen necessary.
UKB allows two main methodsof disambiguation, namely ppr and ppr w2w.
Weused the latter method, as it has been shown to per-form best.In this setting we used the monolingual graphsfor each language (cf.
section 2.3).
Note thatin this run there is no domain adaptation, it thusserves us as a baseline for assessing the benefits ofapplying domain adaptation techniques.3.3 Run2: UKB using related wordsInstead of disambiguating words using their con-text of occurrence, we follow the method de-scribed in (Agirre et al, 2009).
The idea is to firstobtain a list of related words for each of the tar-get words, as collected from a domain corpus.
Ona second step each target word is disambiguatedusing the N most related words as context (seebelow).
For instance, in order to disambiguatethe word environment, we would not take intoaccount the context of occurrence (as in Section3.2), but we would use the list of most relatedwords in the thesaurus (e.g.
?biodiversity, agri-culture, ecosystem, nature, life, climate, .
.
.?).
Us-ing UKB over these contexts we obtain the mostpredominant sense for each target word in the do-main(McCarthy et al, 2007), which is used to la-bel all occurrences of the target word in the testdataset.In order to build the thesaurus with the lists ofrelated words, we used Tybots (c.f.
section 2.2),one for each corpus of the evaluation dataset, i.e.Chinese, Dutch, English, and Italian.
We used thebackground documents provided by the organiz-ers, which we processed using the linguistic pro-cessors of the project to obtain the documents inKAF.
We used the Tybots with the following set-tings.
We discarded co-occurring words with fre-quencies below 105.
Distributional similarity wascomputed using (Lin, 1998).
Finally, we used upto 50 related words for each target word.As in run1, we used the monolingual graphs forthe LKBs in each language.3.4 Run3: UKB using related words andbilingual graphsThe third run is exactly the same as run2, exceptthat we used bilingual graphs instead of monolin-gual ones for all languages other than English (cf.section 2.3).
There is no run3 for English.4 ResultsTable 2 shows the results of our system on thedifferent languages.
We will analyze different as-pects of the results in turn.Domain adaptation: Using Personalized Pager-ank over related words (run2 and run3) con-sistently outperforms the standard setting (run1)in all languages.
This result is consistent with5In the case of Dutch we did not use any threshold due tothe small size of the background corpus.419our previous work on English (Agirre et al,2009), and shows that domain adaptation worksfor knowledge-based systems.Monolingual vs. Bilingual graphs: As ex-pected, we obtained better results using the bilin-gual graphs (run3) than with monolingual graphs(run2), showing that the English WordNet has aricher set of relations, and that those relations canbe successfully ported to other languages.
Thisconfirms that aligning different wordnets at thesynset level is highly beneficial.Overall results: the results of our runs are highlysatisfactory.
In two languages (Dutch and Ital-ian) our best runs perform better than the firstsense baseline, which is typically hard to beat forknowledge-based systems.
In English, our systemperforms close but below the first sense baseline,and in Chinese our method performed below therandom baseline.The poor results obtained for Chinese can bedue the LKB topology; an analysis over the graphshows that it is formed by a large number ofsmall components, unrelated with each other.
This?flat?
structure heavily penalizes the graph basedmethod, which is many times unable to discrimi-nate among the concepts of a word.
We are cur-rently inspecting the results, and we don?t discardbugs, due to the preliminary status of our software.In particular, we need to re-examine the output ofthe Tybot for Chinese.5 ConclusionsThis paper describes the results of the prelimi-nary release of he integrated Kyoto system for do-main specific WSD.
It comprises Tybots to con-struct a domain-related thesaurus, and UKB forknowledge-based WSD based on wordnet graphs.We applied our system to all languages in thedataset, obtaining good results.
In fact, our sys-tem can be applied to any language with a lexicalknowledge base, and is based on publicly availablesoftware and resources.
We used the wordnets andbackground texts provided by the organizers of thetask.Our results show that we were succesful inadapting our system to the domain, as we man-aged to beat the first sense baseline in two lan-guages.
Our results also show that adding the En-glish WordNet to the other language wordnets viathe available links is beneficial.Our participation focused on producing runningsystems for all languages in the task, and we at-tained good results in all except Chinese.
Due tothe pressure and the time-constraints in the com-petition, the system is still under development.
Weare currently revising our system for bugs and fine-tuning it.AcknowledgmentsThis work task is partially funded by the Eu-ropean Commission (KYOTO ICT-2007-211423),the Spanish Research Department (KNOW-2TIN2009-14715-C04-01) and the Basque Govern-ment (BERBATEK IE09-262).ReferencesE.
Agirre and A. Soroa.
2009.
Personalizing pagerank forword sense disambiguation.
In Proceedings of EACL09,pages 33?41.
Association for Computational Linguistics.E.
Agirre, O. L?opez de Lacalle, and A. Soroa.
2009.Knowledge-based wsd on specific domains: Performingbetter than generic supervised wsd.
In Proceedigns of IJ-CAI.
pp.
1501-1506.?.E.
Agirre, O. L?opez de Lacalle, C. Fellbaum, S.K.
Hsieh,M.
Tesconi, M. Monachini, P. Vossen, and R. Segers.2010.
Semeval-2010 task 17: All-words word sense dis-ambiguation on a specific domain.
In Same volume.W.
E. Bosma and P. Vossen.
2010.
Bootstrapping languageneutral term extraction.
In Proceedings of LREC2010,May.W.
E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,A.
Marchetti, M. Monachini, and C. Aliprandi.
2009.KAF: a generic semantic annotation format.
In Proceed-ings of the GL2009 Workshop on Semantic Annotation.S.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual web search engine.
Computer Networks andISDN Systems, 30(1-7).C.
Fellbaum.
1998.
WordNet: An Electronical LexicalDatabase.
The MIT Press, Cambridge, MA.T.
H. Haveliwala.
2002.
Topic-sensitive pagerank.
In WWW?02: Proceedings of the 11th international conference onWWW, pages 517?526, New York, NY, USA.
ACM.D.
Lin.
1998.
Automatic retrieval and clustering of similarwords.
In Proceedings of ACL98, Montreal, Canada.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2007.Unsupervised acquisition of predominant word senses.Computational Linguistics, 33(4).A.
Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,and A. Zampolli.
2003.
Italwordnet: building a largesemantic database for the automatic treatment of Italian.Linguistica Computazionale, Special Issue (XVIII-XIX),pages 745?791.B.S.
Tsai, C.R.
Huang, S.c. Tseng, J.Y.
Lin, K.J.
Chen, andY.S.
Chuang.
2001.
Definition and tests for lexical se-mantic relations in Chinese.
In Proceedings CLSW 2001.P.
Vossen, I. Maks, R. Segers, H. van der Vliet, and H. vanZutphen.
2008.
The cornetto database: the architectureand alignment issues.
In Proceedings GWC 2008, pages485?506.420
