Proceedings of NAACL HLT 2007, pages 155?163,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsHigh-Performance, Language-Independent Morphological SegmentationSajib Dasgupta and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{sajib,vince}@hlt.utdallas.eduAbstractThis paper introduces an unsupervisedmorphological segmentation algorithmthat shows robust performance for fourlanguages with different levels of mor-phological complexity.
In particular, ouralgorithm outperforms Goldsmith?s Lin-guistica and Creutz and Lagus?s Mor-phessor for English and Bengali, andachieves performance that is comparableto the best results for all three PASCALevaluation datasets.
Improvements arisefrom (1) the use of relative corpus fre-quency and suffix level similarity for de-tecting incorrect morpheme attachmentsand (2) the induction of orthographic rulesand allomorphs for segmenting wordswhere roots exhibit spelling changes dur-ing morpheme attachments.1 IntroductionMorphological analysis is the task of segmenting aword into morphemes, the smallest meaning-bearing elements of natural languages.
Thoughvery successful, knowledge-based morphologicalanalyzers operate by relying on manually designedsegmentation heuristics (e.g.
Koskenniemi (1983)),which require a lot of linguistic expertise and aretime-consuming to construct.
As a result, researchin morphological analysis has exhibited a shift tounsupervised approaches, in which a word is typi-cally segmented based on morphemes that areautomatically induced from an unannotated corpus.Unsupervised approaches have achieved consider-able success for English and many European lan-guages (e.g.
Goldsmith (2001), Schone and Juraf-sky (2001), Freitag (2005)).
The recent PASCALChallenge on Unsupervised Segmentation ofWords into Morphemes1  has further intensifiedinterest in this problem, selecting as target lan-guages English as well as two highly agglutinativelanguages, Turkish and Finnish.
However, theevaluation of the Challenge reveals that (1) thesuccess of existing unsupervised morphologicalparsers does not carry over to the two agglutinativelanguages, and (2) no segmentation algorithmachieves good performance for all three languages.Motivated by these state-of-the-art results, ourgoal in this paper is to develop an unsupervisedmorphological segmentation algorithm that canwork well across different languages.
With thisgoal in mind, we evaluate our algorithm on fourlanguages with different levels of morphologicalcomplexity, namely English, Turkish, Finnish andBengali.
It is worth noting that Bengali is an under-investigated Indo-Aryan language that is highlyinflectional and lies between English and Turk-ish/Finnish in terms of morphological complexity.Experimental results demonstrate the robustness ofour algorithm across languages: it not only outper-forms Goldsmith?s (2001) Linguistica and Creutzand Lagus?s (2005) Morphessor for English andBengali, but also compares favorably to the best-performing PASCAL morphological parsers whenevaluated on all three datasets in the Challenge.The performance improvements of our segmen-tation algorithm over existing morphological ana-lyzers can be attributed to our extending Keshavaand Pitler?s (2006) segmentation method, the bestperformer for English in the aforementioned1http://www.cis.hut.fi/morphochallenge2005/155PASCAL Challenge, with the capability of han-dling two under-investigated problems:Detecting incorrect attachments.
Many existingmorphological parsers incorrectly segment ?candi-date?
as ?candid?+?ate?, since they fail to identifythat the morpheme ?ate?
should not attach to theword ?candid?.
Schone and Jurafsky?s (2001) workrepresents one of the few attempts to address thisinappropriate morpheme attachment problem, in-troducing a method that exploits the semantic re-latedness between word pairs.
In contrast, wepropose two arguably simpler, yet effective tech-niques that rely on relative corpus frequency andsuffix level similarity to solve the problem.Inducing orthographic rules and allomorphs.One problem with Keshava and Pitler?s algorithmis that it fails to segment words where the rootsexhibit spelling changes during attachment to mor-phemes (e.g.
?denial?
= ?deny?+?al?).
To addressthis problem, we automatically acquire allomorphsand orthographic change rules from an unannotatedcorpus.
These rules also allow us to output the ac-tual segmentation of the words that exhibit spellingchanges during morpheme attachment, thus avoid-ing the segmentation of ?denial?
as ?deni?+?al?, asis typically done in existing morphological parsers.In addition to addressing the aforementionedproblems, our segmentation algorithm has two ap-pealing features.
First, it can segment words withany number of morphemes, whereas many analyz-ers can only be applied to words with one root andone suffix (e.g.
D?Jean (1998), Snover and Brent(2001)).
Second, it exhibits robust performanceeven when inducing morphemes from a very largevocabulary, whereas Goldsmith?s (2001) andFreitag?s (2005) morphological analyzers performwell only when a small vocabulary is employed,showing deteriorating performance as the vocabu-lary size increases.The rest of this paper is organized as follows.Section 2 presents related work on unsupervisedmorphological analysis.
In Section 3, we describeour basic morpheme induction algorithm.
We thenshow how to exploit the induced morphemes to (1)detect incorrect attachments by using relative cor-pus frequency (Section 4) and suffix level similar-ity (Section 5) and (2) induce orthographic rulesand allomorphs (Section 6).
Section 7 describesour algorithm for segmenting a word using the in-duced morphemes.
We present evaluation resultsin Section 8 and conclude in Section 9.2 Related WorkAs mentioned in the introduction, the problem ofunsupervised morphological learning has been ex-tensively studied for English and many otherEuropean languages.
In this section, we will givean overview of the related work on this problem.Harris (1955) develops a strategy for identifyingmorpheme boundaries that checks whether thenumber of different letters following a sequence ofletters exceeds some given threshold.
D?Jean(1998) improves Harris?s segmentation algorithmby first inducing a list of 100 most frequent mor-phemes and then using those morphemes for wordsegmentation.
The aforementioned PASCAL Chal-lenge on Unsupervised Word Segmentation hasundoubtedly intensified interest in this problem.Among the participating groups, Keshava and Pit-ler?s (2006) segmentation algorithm combines theideas of D?Jean and Harris and achieves the bestresult for the English dataset, but it only offers me-diocre performance for Finnish and Turkish.There is another class of unsupervised morpho-logical learning algorithms whose design is drivenby the Minimum Description Length (MDL) prin-ciple.
Specifically, EM is used to iteratively seg-ment a list of words using some predefinedheuristics until the length of the morphologicalgrammar converges to a minimum.
Brent et al(1995) are the first to introduce an information-theoretic notion of compression to represent theMDL framework.
Goldsmith (2001) also adoptsthe MDL approach, providing a new compressionsystem that incorporates signatures when measur-ing the length of the morphological grammar.Creutz (2003) proposes a probabilistic maximum aposteriori formulation that uses prior distributionsof morpheme length and frequency to measure thegoodness of an induced morpheme, achieving bet-ter results for Finnish but worse results for Englishin comparison to Goldsmith?s Linguistica.3 The Basic Morpheme Induction Algo-rithmOur unsupervised segmentation algorithm is com-posed of two steps: (1) inducing prefixes, suffixesand roots from a vocabulary that consists of wordstaken from a large corpus, and (2) segmenting aword using these induced morphemes.
This sectiondescribes our basic morpheme induction method.1563.1 Extracting a List of Candidate AffixesThe first step of our morpheme induction methodinvolves extracting a list of candidate prefixes andsuffixes.
We rely on a fairly simple idea originallyproposed by Keshava and Pitler (2006) for extract-ing candidate affixes.
Assume that    andare twocharacter sequences and  is the concatenation of and.
If  and    are both found in the vocabu-lary, then we extractas a candidate suffix.
Simi-larly, if  andare both found in the vocabulary,then we extract    as a candidate prefix.The above affix induction method is arguablyoverly simplistic and therefore can generate manyspurious affixes.
To filter spurious affixes, we (1)score each affix by multiplying its frequency (i.e.the number of distinct words to which each affixattaches) and its length2, and then (2) retain onlythe K top-scoring affixes, where K is set differentlyfor prefixes and suffixes.
The value of K is some-what dependent on the vocabulary size, as the af-fixes in a larger vocabulary system are generatedfrom a larger number of words.
For example, weset the thresholds to 70 for prefixes and 50 for suf-fixes for English; on the other hand, since the Fin-nish vocabulary is almost six times larger than thatof English, we set the corresponding thresholds tobe approximately six times larger (400 and 300 forprefixes and suffixes respectively).33.2 Detecting Composite SuffixesNext, we detect and remove composite suffixes (i.e.suffixes that are formed by combining multiplesuffixes [e.g.
?ers?
= ?er?+?s?])
from our inducedsuffix list, because their presence can lead to un-der-segmentation of words (e.g.
?walkers?, whosecorrect segmentation is ?walk?+?er?+?s?, will beerroneously segmented as ?walk?+?ers?).
Compos-ite suffix detection is a particularly important prob-lem for languages like Bengali in which compositesuffixes are abundant (see Dasgupta and Ng(2007)).
Note, however, that simple concatenationof multiple suffixes does not always produce acomposite suffix.
For example, ?ent?, ?en?
and ?t?all are valid suffixes in English, but ?ent?
is not a2The dependence on frequency and length is motivated by the observation thatless-frequent and shorter affixes (especially those of length 1) are more likely tobe erroneous (see Goldsmith (2001)).3Since this method for setting our vocabulary-dependent thresholds is fairlysimple, the use of these thresholds should not be viewed as rendering our seg-mentation algorithm language-dependent.composite suffix.
Hence, we need a more sophisti-cated method for composite suffix detection.Our detection method is motivated by the fol-lowing observation: if xy is a composite suffix anda word w combines with xy, then it is highly likelythat w will also combine with its first componentsuffix x.
Note that this property does not hold fornon-composite suffixes.
For instance, words thatcombine with the non-composite suffix ?ent?
(e.g.?absorb?)
do not combine with its first componentsuffix ?en?.
Consequently, given two suffixes xand y, our method posits xy as a composite suffix ifxy and x are similar in terms of the words to whichthey attach.
Specifically, we consider xy and x tobe similar if their similarity value as computed bythe formula below is greater than 0.6:|||?|)|(),(WWxyxPxxySimilarity == ,where |W  | is the number of distinct words thatcombine with both xy and x, and |W| is the numberof  distinct words that combine with xy.3.3 Extracting a List of Candidate RootsFinally, we extract a list of candidate roots usingthe induced list of affixes as follows.
For eachword, w, in the vocabulary, we check whether w isdivisible, i.e.
whether w can be segmented as r+xor p+r, where p is an induced prefix, x is an in-duced suffix, and r is a word in the vocabulary.
Wethen add w to the root list if it is not divisible.Note, however, that the resulting root list may con-tain compound words (i.e.
words with multipleroots).
Hence, we make another pass over our rootlist to remove any word that is a concatenation ofmultiple words in the vocabulary.4 Detecting Incorrect Attachments UsingRelative FrequencyOur induced root list is not perfect: many correctroots are missing due to over-segmentation.
Forexample, since ?candidate?
and ?candid?
are in thevocabulary and ?ate?
is an induced suffix, our rootinduction method will incorrectly segment ?candi-date?
as ?candid?+?ate?
; as a result, it does notconsider ?candidate?
as a root.
So, to improve theroot induction method, we need to determine thatthe attachment of the morpheme ?ate?
to the rootword ?candid?
is incorrect.
In this section, we pro-pose a simple yet novel idea of using relative cor-157pus frequency to determine whether the attachmentof a morpheme to a root word is plausible or not.Consider again the two words ?candidate?
and?candid?.
While ?candidate?
occurs 6380 times inour corpus, ?candid?
occurs only 119 times.
Thisfrequency disparity can be an important clue todetermining that there is no morphological relationbetween ?candidate?
and ?candid?.
Similar obser-vation is also made by Yarowsky and Wicentowski(2000), who successfully employ relative fre-quency similarity or disparity to rank candidateVBD/VB pairs (e.g.
?sang?/?sing?)
that are irregu-lar in nature.
Unlike Yarowsky and Wicentowski,however, our goal is to detect incorrect affix at-tachments and improve morphological analysis.Our incorrect attachment detection algorithm,which exploits frequency disparity, is based on thefollowing hypothesis: if a word w is formed byattaching an affix m to a root word r, then the cor-pus frequency of w is likely to be less than that of r(i.e.
the frequency ratio of w to r is less than one).In other words, we hypothesize that the inflectionalor derivational forms of a root word occur less fre-quently in a corpus than the root itself.To illustrate this hypothesis, Table 1 showssome randomly chosen English words togetherwith their word-root frequency ratios (WRFRs).The <word, root> pairs in the left side of the tableare examples of correct attachments, whereas thosein the right side are not.
Note that only those wordsthat represent correct attachments have a WRFRless than 1.The question, then, is: to what extent does ourhypothesis hold?
To investigate this question, wegenerated examples of correct attachments by ran-domly selecting 400 words from our English vo-cabulary and then removing those that are rootwords, proper nouns, or compound words.
We thenmanually segmented each of the remaining 378words as Prefix+Root or Root+Suffix with the aidof the CELEX lexical database (Baayean et al,1996).
Somewhat surprisingly, we found that theWRFR is less than 1 in only 71.7% of these at-tachments.
When the same experiment was re-peated on 287 hand-segmented Bengali words, thehypothesis achieves a higher accuracy of 83.6%.To better understand why our hypothesis doesnot work well for English, we measured its accu-racy separately for the Root+Suffix words and thePrefix+Root words, and found that the hypothesisfails mostly on the suffixal attachments (see Table2).
Though surprising at first glance, the relativelypoor accuracy on suffixal attachments can be at-tributed to the fact that many words in English(e.g.
?amusement?, ?winner?)
appear more fre-quently in our corpus than their corresponding rootforms (e.g.
?amuse?, ?win?).
For Bengali, our hy-pothesis fails mainly on verbs, whose inflectedforms occur more often in our corpus than theirroots.
This violation of the hypothesis can be at-tributed to the grammatical rule that the main verbof a Bengali sentence has to be inflected accordingto the subject in order to maintain sentence order.To improve the accuracy of our hypothesis ondetecting correct attachments, we relax our initialhypothesis as follows: if an attachment is correct,then the corresponding WRFR is less than somepredefined threshold t, where t > 1.
However, wedo not want t to be too large, since our algorithmmay then determine many incorrect attachments ascorrect.
In addition, since our hypothesis has a highaccuracy for prefixal attachments than suffixal at-tachments, the threshold we employ for prefixescan be smaller than that for suffixes.
Taking intoaccount these considerations, we use a threshold of10 for suffixes and 2 for prefixes for all the lan-guages we consider in this paper.Correct Parses Incorrect ParsesWord Root WRFR Word Root WRFRbear-able bear 0.01 candid-ate candid 53.6attend-ance attend 0.24 medic-al medic 483.9arrest-ing arrest 0.06 prim-ary prim 327.4sub-group group 0.0002 ac-cord cord 24.0re-cycle cycle 0.028 ad-diction diction 52.7un-settle settle 0.018 de-crease crease 20.7Table 1: Word-root frequency ratiosRoot+Suffix Prefix+Root Overall# of words 344 34 378WRFR < 1 70.1% 88.2% 71.7%Table 2: Hypothesis validation for EnglishNow we can employ our hypothesis to detect in-correct attachments and improve root induction asfollows.
For each word, w, in the vocabulary, wecheck whether (1) w can be segmented as r+x orp+r, where p and x are valid prefixes and suffixesrespectively and r is another word in the vocabu-lary, and (2) the WRFR for w and r is less than ourpredefined thresholds (10 for suffixes and 2 forprefixes).
If both conditions are satisfied, it meansthat w is divisible.
Hence, we add w into the list ofroots if at least one of the conditions is violated.1585 Suffix Level SimilarityMany of the incorrect suffixal attachments have aWRFR between 1 and 10, but the detection algo-rithm described in the previous section will deter-mine all of them as correct attachments.
Hence, inthis section, we propose another technique, whichwe call suffix level similarity, to identify some ofthese incorrect attachments.Suffix level similarity is motivated by the fol-lowing observation: if a word w combines with asuffix x, then w should also combine with the suf-fixes that are ?morphologically similar?
to x. Toexemplify, consider the suffix ?ate?
and the rootword ?candid?.
The words that combine with thesuffix ?ate?
(e.g.
?alien?, ?fabric?, ?origin?)
alsocombine with suffixes like ?ated?, ?ation?
and ?s?.Given this observation, the question of whether?candid?
combines with the suffix ?ate?
then liesin whether or not ?candid?
combines with ?ated?,?s?
and ?ation?.
The fact that ?candid?
does notcombine with many of the above suffixes providessuggestive evidence that ?candidate?
cannot bederived from ?candid?.More specifically, to check whether a word wcombines with a suffix x using suffix level simial-rity, we (1) find the set of words Wx that can com-bine with x; (2) find the set of suffixes Sx thatattach to all of the words in Wx under the constraintthat Sx does not contain x; and (3) find the 10 suf-fixes in Sx that are most ?similar?
to x.
The ques-tion, then, is how to define similarity.
Intuitively, agood similarity metric should reflect, for instance,the fact that ?ated?
is a better suffix to consider inthe attachment decision for ?ate?
than ?s?
(i.e.?ated?
is more similar to ?ate?
than ?s?
), since ?s?attaches to most nouns and verbs in English andhence should not be a distinguishing feature forincorrect attachment detection.We employ a probabilistic measure (PM) thatcomputes the similarity between suffixes x and y asthe product of (1) the probability of a word com-bining with y given that it combines with x and (2)the probability of a word combining with x giventhat it combines with y.
More specifically,,*)|(*)|(),(21 nnnnyxPxyPyxPM ==where n1 is the number of distinct words that com-bine with x, n2 is the number of distinct words thatcombine with y, and n is the number of distinctwords that combine with both x and y.4After getting the 10 suffixes that are most simi-lar to x, we employ them as features and use theassociated similarity values (we scale them linearlybetween 1 and 10) as the weights of these 10 fea-tures.
The decision of whether a suffix x can attachto a word w depends on whether the following ine-quality is satisfied:,101twf ii >where fi is a boolean variable that has the value 1 ifw combines with xi, where xi is one of the 10 suf-fixes that are most similar to x; wi is the scaledsimilarity between x and xi; and t is a predefinedthreshold that is greater than 0.One potential problem with suffix level similar-ity is that it is an overly strict condition for thosewords that combine with only one or two suffixesin the vocabulary.
For example, if the word ?char-acter?
has just one variant in the vocabulary (e.g.?characters?
), suffix level similarity will determinethe attachment of ?s?
to ?character?
as incorrect,since the weighted sum in the above inequality willbe 0.
To address this sparseness problem, we relyon both relative corpus frequency and suffix levelsimilarity to identify incorrect attachments.
Spe-cifically, if the WRFR of a <word, root> pair isbetween 1 and 10, we determine that an attachmentto the root is incorrect if-WRFR +    * (suffix level similarity) < 0,where    is set to 0.15.Finally, since long words have a higher chanceof getting segmented, we do not apply suffix levelsimilarity to words whose length is greater than 10.6 Inducing Orthographic Rules and Al-lomorphsThe biggest drawback of the system, describedthus far, is its failure to segment words where theroots exhibit spelling changes during attachment tomorphemes (e.g.
?denial?
= ?deny?+?al?).
Thereasons are (1) the system does not have anyknowledge of language-specific orthographic rules(e.g.
in English, the character ?y?
at the morphemeboundary is changed to ?i?
when the root combines4Note that this metric has the desirable property of returning a low similarityvalue for ?s?
: while n is likely to be large, it will be offset by a large n2.159with the suffix ?al?
), and (2) the vocabulary weemploy for morpheme induction does not normallycontain the allomorphic variations of the roots(e.g.
?deni?
is allomorphic variation of ?deny?).
Tosegment these words correctly, we need to generatethe allomorphs and orthographic rules automati-cally given a set of induced roots and affixes.Before giving the details of the generationmethod, we note that the induction of orthographicrules is a challenging problem, since different lan-guages exhibit orthographic changes in differentways.
For some languages (e.g.
English) rules aremostly predictable, whereas for others (e.g.
Fin-nish) rules are highly irregular.
It is hard to obtaina generalized mapping function that aligns every<root, allomorph> pair, considering the fact thatour system is unsupervised.
An additional chal-lenge is to ensure that the incorporation of theseorthographic rules will not adversely affect systemperformance (i.e.
they will not be applied to regu-lar words and thus segment them incorrectly).Yarowsky and Wicentowski (2000) propose aninteresting algorithm that employs four similaritymeasures to successfully identify the most prob-able root of a highly irregular word.
Unlike them,however, our goal is to (1) check whether thelearned rules can actually improve an unsupervisedmorphological system, not just to align <root, al-lomorph> pair, and (2) examine whether our sys-tem is extendable to different languages.Taking into consideration the aforementionedchallenges, our induction algorithm will (1) handleorthographic character changes that occur only atmorpheme boundaries; (2) generate allomorphs forsuffixal attachments only5, assuming that roots ex-hibit the character changes during attachment, notsuffixes; and (3) learn rules that aligns <root, allo-morph> pairs of edit distance 1 (which may in-volve 1-character replacement, deletion orinsertion).
Despite these limitations, we will seethat the incorporation of the induced rules im-proves segmentation accuracy significantly.Let us first discuss how we learn a replacementrule, which identifies <allomorph, root> pairswhere the last character of the root is replaced byanother character.
The steps are as follows:(1) Inducing candidate allomorphsIf   Ais a word in the vocabulary (e.g.
?denial?,where   =?den?, A=?i?, and=?al?
),   is an in-5We only learn rules for suffixes of length greater than 1, since most suffixes oflength 1 do not participate in orthographic changes.duced suffix,   B is an induced root (e.g.
?deny?,where B=?y?
), and the attachment of   to   B iscorrect according to relative corpus frequency (seeSection 4), then we hypothesize that   A is an allo-morph of   B.
For each induced suffix, we use thishypothesis to generate the allomorphs and identifythose that are generated from at least two suffixesas candidate allomorphs.
We denote the list of<candidate allomorph, root, suffix> tuples by L.(2) Learning orthographic rulesEvery <candidate allomorph, root, suffix> tuple aslearned above is associated with an orthographicrule.
For example, from the words ?denial?, ?deny?and suffix ?al?, we learn the rule ?y:i / _ + al?6;from ?social?, ?sock?
and ?al?, we learn the rule?k:i / _ + al?, which, however, is erroneous.
So, wecheck whether each of the learned rules occurs fre-quently enough for all the <allomorph, root> pairsassociated with a suffix, with the goal of filteringthe low-frequency orthographic rules.
Specifically,for each suffix, we repeat the following steps:(a) Counting the frequency of rules.
Let L   be thelist of <candidate allomorph, root> pairs in L thatare associated with the suffix.
For each pair p inL  , we first check whether its candidate allomorphappears in any other <candidate allomorph, root>pairs in L  .
If not, we increment the frequency ofthe orthographic rule associated with p by 1.
Forexample, the pair <?deni?, ?deny?> increases thefrequency of the rule ?y:i?
by 1 on condition that?deni?
does not appear in any other pairs.
(b) Filtering the rules.
We first remove the infre-quent rules, specifically those that are induced byless than 15% of the tuples in L  .
Then we checkwhether there exists two rules of the form A:B andA:C in the induced rule list.
If so, then we have amorphologically undesirable situation where thecharacter A changes to B and C under the sameenvironment (i.e.
 ).
To address this problem, wefirst calculate the strength of a rule as follows:=@@):():():(AfrequencyBAfrequencyBAstrengthWe then retain only those rules whose fre-quency*strength is greater than some predefinedthreshold.
We denote the list of rules that satisfythe above constraints by R  .
(c) Identifying valid allomorphs.
For each rule inR  , we identify the associated <candidate allo-6This is the Chomsky and Halle notation for representing orthographic rules.
a:b/ c _ d means a changes to b when the left context is c and the right context is d.160morph, root> pairs in L  .
We refer to the candidateallomorphs in each of those pairs as valid allo-morphs and add them to the list of roots.
We alsoremove from the original root list the words thatcan be segmented by the induced allomorphs andthe associated rules (e.g.
?denial?).
(d) Identifying composite suffixes.
For each rulein R  , we also check whether it can identify com-posite suffixes where the first component suffix?slast character is replaced during attachment to thesecond component suffix (e.g.
?liness?
=?ly?+?ness?).
Specifically, if (1) A:B / _   is a rulein R  , (2)   A  (say ?liness?
),   (say ?ness?)
and   B(say ?ly?)
are induced suffixes, and (3)   A  satis-fies the requirements of a composite suffix (seeSection 3.2), then we determine that   A  is a com-posite suffix composed of   B and.We employ the same procedure for learning in-sertion and deletion rules, except that strength isalways set to 1 for these two types of rules.
Thethreshold we set at step (b) is somewhat dependenton the vocabulary size, since the frequency countof each rule will naturally be larger when a largervocabulary is used.
Following our method for set-ting vocabulary-dependent thresholds (see Section3.1), we set the threshold to 4 for English and 25for Finnish, for instance.Finally, we adapt our candidate allomorph de-tection method described above to induce allo-morphs that are generated through orthographicchanges of edit distance greater than 1.
Specifi-cally, if  is a word in the induced root list (e.g.
?stability?7, where   =?stabil?
and=?ity?
),   is aninduced suffix, and the attachment ofto    is cor-rect according to suffix level similarity, then wehypothesize that    (?stabil?)
is a candidate allo-morph.
For each induced suffix, we use this hy-pothesis to generate candidate allomorphs andconsider as valid allomorphs only those that aregenerated from at least three different suffixes.87 Word SegmentationAfter inducing the morphemes, we can use them tosegment a word w in the test set.
Specifically, we7The correct segmentation of ?stability?
is ?stable?+?ity?.
The ?stabil?-?stable?allomorph-root pair is an example of an orthographic change of edit distance 2.8This technique can also be used to induce out-of-vocabulary (OOV) roots.
Forexample, the presence of ?perplexity?, ?perplexed?
and ?perplexing?
in a vo-cabulary allows us to induce the root ?perplex?.
OOV root induction is particu-larly important for languages like Bengali, where verb roots mostly take theimperative form and hence are absent in a vocabulary created from a newspapercorpus, which normally comprises only the first and third person verb forms.
(1) generate all possible segmentations of w usingonly the induced affixes and roots, and (2) apply asequence of tests to remove candidate segmenta-tions until we are left with only one candidate,which we take to be the final segmentation of w.Our first test involves removing any candidatesegmentation m1m2 ?
mn that violates any of thelinguistic constraints below:?
At least one of m1, m2, ?, mn is a root.?
For 1 ?
i < n, if mi is a prefix, then mi+1 mustbe a root or a prefix.?
For 1 < i ?
n, if mi is a suffix, then mi-1 mustbe a root or a suffix.?
m1 can?t be a suffix and mn can?t be a prefix.Next, we apply our second test, in which we re-tain only those candidate segmentations that havethe smallest number of morphemes.
For example,if ?friendly?
has two candidate segmentations?friend?+?ly?
and ?fri?+?end?+?ly?, we will selectthe first one to be the segmentation of w.If more than one candidate segmentation still ex-ists, we score each remaining candidate using theheuristic below, selecting the highest-scoring can-didate to be the final segmentation of w.  Basically,we score each candidate segmentation by addingthe strength of each morpheme in the segmenta-tion, where (1) the strength of an affix is the num-ber of distinct words in the vocabulary to whichthe affix attaches, multiplied by the length of theaffix, and (2) the strength of a root is the number ofdistinct morphemes with which the root combines,again multiplied by the length of the root.8 EvaluationIn this section, we will first evaluate our segmenta-tion algorithm for English and Bengali, and thenexamine its performance on the PASCAL datasets.8.1 Experimental SetupVocabulary creation.
We extracted our Englishvocabulary from the Wall Street Journal corpus ofthe Penn Treebank and the BLLIP corpus, preproc-essing the documents by first tokenizing them andthen removing capitalized words, punctuations andnumbers.
In addition, we removed words of fre-quency 1 from BLLIP, because many of them areproper nouns and misspelled words.
The final Eng-lish vocabulary consists of approximately 60K dis-tinct words.
We applied the same pre-processing161steps to five years of articles taken from the Ben-gali newspaper Prothom Alo to generate our Ben-gali vocabulary, which consists of 140K words.Test set preparation.
To create our English testset, we randomly chose 000 words from our vo-cabulary that are at least 4-character long9 and alsoappear in CELEX.
Although 95% of the time weadopted the segmentation proposed by CELEX, insome cases the CELEX segmentations are errone-ous (e.g.
?rolling?
and ?lodging?
remain unseg-mented in CELEX).
As a result, we cross-checkwith the online version of Merriam-Webster tomake the necessary changes.
To create the Bengalitest set, we randomly chose 5000 words from ourvocabulary and manually removed proper nounsand misspelled words from the set before giving itto two of our linguists for hand-segmentation.
Thefinal test set contains 4191 words.Evaluation metrics.
We use two standard metrics-- exact accuracy and F-score -- to evaluate theperformance of our segmentation algorithm on thetest sets.
Exact accuracy is the percentage of thewords whose proposed segmentation is identical tothe correct segmentation.
F-score is the harmonicmean of recall and precision, which are computedbased on the placement of morpheme boundaries.108.2 Results for English and BengaliThe baseline systems.
We use two publicly avail-able and widely used unsupervised morphologicallearning systems -- Goldsmith?s (2001) Linguis-tica11 and Creutz and Lagus?s (2005) Morphessor1.012 -- as our baseline systems.
The first two rowsof Table 3 show the results of these systems for ourtest sets (with all the training parameters set totheir default values).
As we can see, Linguisticaperforms substantially better for English in termsof both exact accuracy and F-score, whereas Mor-phessor outperforms Linguistica for Bengali.Our segmentation algorithm.
Results of oursegmentation algorithm are shown in rows 3-6 ofTable 3.
Specifically, row 3 shows the results ofour basic segmentation system as described in Sec-tion 3.
Rows 4-6 show the results where our threetechniques (i.e.
relative frequency, suffix level9Words of length less than 4 do not have any morphological segmentation inEnglish.
Hence, by imposing this length restriction on the words in our test set,we effectively make the evaluation more challenging.
This is also the reason forour using words that are at least 3-character long in the Bengali test set.10See http://www.cis.hut.fi/morphochallenge2005/evaluation.shtml for details.11http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000/12http://www.cis.hut.fi/projects/morpho/similarity and allomorph detection) are incorpo-rated into the basic system one after the other.
It isworth mentioning that (1) our basic algorithm al-ready outperforms the baseline systems in terms ofboth exact accuracy and F-score; and (2) whileeach of our additions to the basic algorithm boostssystem performance, relative corpus frequency andallomorph detection contribute to performance im-provements particularly significantly.
As we cansee, the best segmentation performance is achievedwhen all of our three additions are applied to thebasic algorithm.EnglishBengaliA P R F A P R FLinguistica 68.984.8 75.7 80.0 36.3 58.2 63.3 60.6Morphessor 64.969.6 85.3 76.6 56.5 89.7 67.4 76.9Basic in-duction68.1 79.4 82.8 81.1 57.7 79.6 81.2 80.4Relativefrequency74.0 86.4 82.5 84.4 63.2 85.6 79.9 82.7Suffix levelsimilarity74.9 88.6 82.3 85.3 66.1 89.7 78.8 83.9Allomorphdetection78.3 88.3 86.4 87.4 68.3 89.3 81.3 85.1Table 3: Results (reported in terms of exact accu-racy (A), precision (P), recall (R) and F-score (F))8.3 PASCAL Challenge ResultsTo get an idea of how our algorithm performs incomparison to the PASCAL participants, we con-ducted evaluations on the PASCAL datasets forEnglish, Finnish and Turkish.
Table 4 shows the F-scores of four segmentation algorithms for thesethree datasets: the best-performing PASCAL sys-tem (Winner), Morphessor, our system that usesthe basic morpheme induction algorithm (Basic),and our system with all three extensions incorpo-rated (Complete).
Below we discuss these results.English.
There are 533 test cases in this dataset.Using the vocabulary created as described in Sec-tion 8.1, our Complete algorithm achieves an F-score of 79.4%, which outperforms the winner(Keshava and Pitler, 2006) by 2.6%.
Although ourbasic morpheme induction algorithm is similar tothat of Keshava and Pitler, a closer examination ofthe results reveals that F-score increases signifi-cantly with the incorporation of relative frequencyand allomorph detection.Finnish and Turkish.
The real challenge in thePASCAL Challenge is the evaluation on Finnish162and Turkish due to their morphological richness.We use the 400K and 300K most frequent wordsfrom the Finnish and Turkish datasets provided bythe organizers as our vocabulary.
When tested onthe gold standard of 661 Finnish and 775 Turkishwords, our Complete system achieves F-scores of65.2% and 66.2%, which are better than the win-ner?s scores (Bernhard (2006)).
In addition, Com-plete outperforms Basic by 3-6% in F-score; theseresults suggest that the new techniques proposed inthis paper (especially allomorph detection) are alsovery effective for Finnish and Turkish.English Finnish TurkishWinner 76.8 64.7 65.3Morphessor 66.2 66.4 70.1Basic 75.8 59.2 63.4Complete 79.4 65.2 66.2Table 4: F-scores for the PASCAL gold standardsAs mentioned in the introduction, none of theparticipating PASCAL systems offers robust per-formance across different languages.
For instance,Keshava and Pitler?s algorithm, the winner forEnglish, has F-scores of only 47% and 54% forFinnish and Turkish respectively, whereas Bern-hard?s algorithm, the winner for Finnish and Turk-ish, achieves an F-score of only 66% for English.On the other hand, our algorithm outperforms thewinners for all the languages in the competition,demonstrating its robustness across languages.Finally, although Morphessor achieves better re-sults for Turkish and Finnish than our Completesystem, it performs poorly for English, having anF-score of only 66.2%.
On the other hand, our re-sults for Finnish and Turkish are not significantlypoorer than those of Morphessor.9 ConclusionsWe have presented an unsupervised word segmen-tation algorithm that offers robust performanceacross languages with different levels of morpho-logical complexity.
Our algorithm not only outper-forms Linguistica and Morphessor for English andBengali, but also compares favorably to the best-performing PASCAL morphological parsers whenevaluated against all three target languages --English, Turkish, and Finnish -- in the Challenge.Experimental results indicate that the use of rela-tive corpus frequency for incorrect attachment de-tection and the induction of orthographic rules andallomorphs have contributed to the performance ofour algorithm particularly significantly.ReferencesR.
H. Baayen, R. Piepenbrock and L. Gulikers.
1996.
TheCELEX2 lexical database (CD-ROM), LDC, Univ ofPennsylvania, Philadephia, PA.D.
Bernhard.
2006.
Unsupervised morphological segmentationbased on segment predictability and word segment align-ment.
In PASCAL Challenge Workshop on UnsupervisedSegmentation of Words into Morphemes.M.
R. Brent, S. K. Murthy and A. Lundberg.
1995.
Discover-ing morphemic suffixes: A case study in minimum descrip-tion length induction.
In Proceedings of the FifthInternational Workshop on AI and Statistics.M.
Creutz.
2003.
Unsupervised segmentation of words usingprior distributions of morph length and frequency.
In Pro-ceedings of the ACL, pages 280-287.M.
Creutz and K. Lagus.
2005.
Unsupervised morpheme seg-mentation and morphology induction from text corpora us-ing Morfessor 1.0.
In Computer and Information Science,Report A81, Helsinki University of Technology.S.
Dasgupta and V. Ng.
2007.
Unsupervised word segmenta-tion for Bangla.
In Proceedings of ICON, pages 15-24.H.
D?Jean.
1998.
Morphemes as necessary concepts for struc-tures discovery from untagged corpora.
In Workshop onParadigms and Grounding in Natural Language Learning,pages 295-299.D.
Freitag.
2005.
Morphology induction from term clusters.
InProceedings of CoNLL, pages 128-135.J.
Goldsmith.
2001.
Unsupervised learning of the morphologyof a natural language.
In Computational Linguistics 27(2),pages 153-198.Z.
Harris.
1955.
From phoneme to morpheme.
In Language,31(2): 190-222.S.
Keshava and E. Pitler.
2006.
A simpler, intuitive approachto morpheme induction.
In PASCAL Challenge Workshopon Unsupervised Segmentation of Words into Morphemes.K.
Koskenniemi.
1983.
Two-level morphology: a generalcomputational model for word-form recognition and pro-duction.
Publication No.
11.
Helsinki: University of Hel-sinki Department of General Linguistics.P.
Schone and D. Jurafsky.
2001.
Knowledge-free induction ofinflectional morphologies.
In Proceedings of the NAACL,pages 183-191.M.
G. Snover and M. R. Brent.
2001.
A Bayesian model formorpheme and paradigm identification.
In Proceedings ofthe ACL, pages 482-490.D.
Yarowsky and R. Wicentowski.
2000.
Minimally super-vised morphological analysis by multimodal alignment.
InProceedings of the ACL, pages 207-216.163
