Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 334?339,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsStacking for Statistical Machine Translation?Majid Razmara and Anoop SarkarSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, Canada{razmara,anoop}@sfu.caAbstractWe propose the use of stacking, an ensem-ble learning technique, to the statistical machinetranslation (SMT) models.
A diverse ensem-ble of weak learners is created using the sameSMT engine (a hierarchical phrase-based sys-tem) by manipulating the training data and astrong model is created by combining the weakmodels on-the-fly.
Experimental results on twolanguage pairs and three different sizes of train-ing data show significant improvements of upto 4 BLEU points over a conventionally trainedSMT model.1 IntroductionEnsemble-based methods have been widely usedin machine learning with the aim of reduc-ing the instability of classifiers and regressorsand/or increase their bias.
The idea behindensemble learning is to combine multiple mod-els, weak learners, in an attempt to produce astrong model with less error.
It has also beensuccessfully applied to a wide variety of tasks inNLP (Tomeh et al, 2010; Surdeanu and Man-ning, 2010; F. T. Martins et al, 2008; Sang, 2002)and recently has attracted attention in the statis-tical machine translation community in variouswork (Xiao et al, 2013; Song et al, 2011; Xiaoet al, 2010; Lagarda and Casacuberta, 2008).In this paper, we propose a method to adoptstacking (Wolpert, 1992), an ensemble learningtechnique, to SMT.
We manipulate the full set oftraining data, creating k disjoint sets of held-outand held-in data sets as in k-fold cross-validationand build a model on each partition.
This createsa diverse ensemble of statistical machine transla-tion models where each member of the ensemblehas different feature function values for the SMTlog-linear model (Koehn, 2010).
The weights ofmodel are then tuned using minimum error ratetraining (Och, 2003) on the held-out fold to pro-vide k weak models.
We then create a strong?This research was partially supported by an NSERC,Canada (RGPIN: 264905) grant and a Google Faculty Awardto the second author.model by stacking another meta-learner on top ofweak models to combine them into a single model.The particular second-tier model we use is a modelcombination approach called ensemble decodingwhich combines hypotheses from the weak mod-els on-the-fly in the decoder.Using this approach, we take advantage of thediversity created by manipulating the training dataand obtain a significant and consistent improve-ment over a conventionally trained SMT modelwith a fixed training and tuning set.2 Ensemble Learning MethodsTwo well-known instances of general frameworkof ensemble learning are bagging and boosting.Bagging (Breiman, 1996a) (bootstrap aggregat-ing) takes a number of samples with replacementfrom a training set.
The generated sample setmay have 0, 1 or more instances of each origi-nal training instance.
This procedure is repeateda number of times and the base learner is ap-plied to each sample to produce a weak learner.These models are aggregated by doing a uniformvoting for classification or averaging the predic-tions for regression.
Bagging reduces the vari-ance of the base model while leaving the bias rela-tively unchanged and is most useful when a smallchange in the training data affects the predictionof the model (i.e.
the model is unstable) (Breiman,1996a).
Bagging has been recently applied toSMT (Xiao et al, 2013; Song et al, 2011)Boosting (Schapire, 1990) constructs a stronglearner by repeatedly choosing a weak learnerand applying it on a re-weighted training set.
Ineach iteration, a weak model is learned on thetraining data, whose instance weights are modi-fied from the previous iteration to concentrate onexamples on which the model predictions werepoor.
By putting more weight on the wronglypredicted examples, a diverse ensemble of weaklearners is created.
Boosting has also been used inSMT (Xiao et al, 2013; Xiao et al, 2010; Lagarda334Algorithm 1: Stacking for SMTInput: D = {?fj , ej?
}Nj=1 .
A parallel corpusInput: k .
# of folds (i.e.
weak learners)Output: STRONGMODEL s1: D1, .
.
.
,Dk ?
SPLIT(D, k)2: for i = 1?
k do3: T i ?
D ?Di .
Use all but current partition astraining set.4: ?i?
TRAIN(T i) .
Train feature functions.5: Mi?
TUNE(?i, Di) .
Tune the model on thecurrent partition.6: end for7: s?
COMBINEMODELS(M1 , .
.
.,Mk) .
Combine allthe base models to produce a strong stacked model.and Casacuberta, 2008).Stacking (or stacked generalization) (Wolpert,1992) is another ensemble learning algorithm thatuses a second-level learning algorithm on top ofthe base learners to reduce the bias.
The firstlevel consists of predictors g1, .
.
.
, gk where gi :Rd ?
R, receiving input x ?
Rd and produc-ing a prediction gi(x).
The next level consistsof a single function h : Rd+k ?
R that takes?x, g1(x), .
.
.
, gk(x)?
as input and produces an en-semble prediction y?
= h(x, g1(x), .
.
.
, gk(x)).Two categories of ensemble learning are ho-mogeneous learning and heterogeneous learning.In homogeneous learning, a single base learneris used, and diversity is generated by data sam-pling, feature sampling, randomization and pa-rameter settings, among other strategies.
In het-erogeneous learning different learning algorithmsare applied to the same training data to create apool of diverse models.
In this paper, we focus onhomogeneous ensemble learning by manipulatingthe training data.In the primary form of stacking (Wolpert,1992), the training data is split into multiple dis-joint sets of held-out and held-in data sets usingk-fold cross-validation and k models are trainedon the held-in partitions and run on held-out par-titions.
Then a meta-learner uses the predictionsof all models on their held-out sets and the actuallabels to learn a final model.
The details of thefirst-layer and second-layer predictors are consid-ered to be a ?black art?
(Wolpert, 1992).Breiman (1996b) linearly combines the weaklearners in the stacking framework.
The weightsof the base learners are learned using ridge regres-sion: s(x) = ?k ?kmk(x), where mk is a basemodel trained on the k-th partition of the data ands is the resulting strong model created by linearlyinterpolating the weak learners.Stacking (aka blending) has been used in thesystem that won the Netflix Prize1, which used amulti-level stacking algorithm.Stacking has been actively used in statisticalparsing: Nivre and McDonald (2008) integratedtwo models for dependency parsing by letting onemodel learn from features generated by the other;F. T. Martins et al (2008) further formalized thestacking algorithm and improved on Nivre andMcDonald (2008); Surdeanu and Manning (2010)includes a detailed analysis of ensemble modelsfor statistical parsing: i) the diversity of baseparsers is more important than the complexity ofthe models; ii) unweighted voting performs as wellas weighted voting; and iii) ensemble models thatcombine at decoding time significantly outperformmodels that combine multiple models at trainingtime.3 Our ApproachIn this paper, we propose a method to apply stack-ing to statistical machine translation (SMT) andour method is the first to successfully exploitstacking for statistical machine translation.
Weuse a standard statistical machine translation en-gine and produce multiple diverse models by par-titioning the training set using the k-fold cross-validation technique.
A diverse ensemble of weaksystems is created by learning a model on eachk?1 fold and tuning the statistical machine trans-lation log-linear weights on the remaining fold.However, instead of learning a model on the outputof base models as in (Wolpert, 1992), we combinehypotheses from the base models in the decoderwith uniform weights.
For the base learner, weuse Kriya (Sankaran et al, 2012), an in-house hier-archical phrase-based machine translation system,to produce multiple weak models.
These mod-els are combined together using Ensemble Decod-ing (Razmara et al, 2012) to produce a strongmodel in the decoder.
This method is briefly ex-plained in next section.3.1 Ensemble DecodingSMT Log-linear models (Koehn, 2010) find themost likely target language output e given thesource language input f using a vector of featurefunctions ?
:p(e|f) ?
exp(w ?
?
)1http://www.netflixprize.com/335Ensemble decoding combines several modelsdynamically at decoding time.
The scores arecombined for each partial hypothesis using auser-defined mixture operation ?
over componentmodels.p(e|f) ?
exp(w1 ?
?1 ?w2 ?
?2 ?
.
.
.
)We previously successfully applied ensembledecoding to domain adaptation in SMT andshowed that it performed better than approachesthat pre-compute linear mixtures of different mod-els (Razmara et al, 2012).
Several mixture oper-ations were proposed, allowing the user to encodebelief about the relative strengths of the compo-nent models.
These mixture operations receivetwo or more probabilities and return the mixtureprobability p(e?
| f?)
for each rule e?, f?
used in thedecoder.
Different options for these operationsare:?
Weighted Sum (wsum) is defined as:p(e?
| f?)
?M?m?m exp(wm ?
?m)where m denotes the index of componentmodels, M is the total number of them and?m is the weight for component m.?
Weighted Max (wmax) is defined as:p(e?
| f?)
?
maxm(?m exp(wm ?
?m))?
Prod or log-wsum is defined as:p(e?
| f?)
?
exp( M?m?m (wm ?
?m))?
Model Switching (Switch): Each cell in theCKY chart is populated only by rules fromone of the models and the other models?
rulesare discarded.
Each component model is con-sidered as an expert on different spans of thesource.
A binary indicator function ?(f?
,m)picks a component model for each span:?(f?
,m) =??
?1, m = argmaxn?M?(f?
, n)0, otherwiseThe criteria for choosing a model for eachcell, ?(f?
, n), could be based on maxTrain size Src tokens Tgt tokensFr - En0+dev 67K 58K10k+dev 365K 327K100k+dev 3M 2.8MEs - En0+dev 60K 58K10k+dev 341K 326K100k+dev 2.9M 2.8MTable 1: Statistics of the training set for different systems anddifferent language pairs.
(SW:MAX), i.e.
for each cell, the model thathas the highest weighted score wins:?(f?
, n) = ?n maxe (wn ?
?n(e?, f?
))Alternatively, we can pick the model withhighest weighted sum of the probabilities ofthe rules (SW:SUM).
This sum has to take intoaccount the translation table limit (ttl), on thenumber of rules suggested by each model foreach cell:?(f?
, n) = ?n?e?exp(wn ?
?n(e?, f?
))The probability of each phrase-pair (e?, f?)
isthen:p(e?
| f?)
=M?m?(f?
,m) pm(e?
| f?
)4 Experiments & ResultsWe experimented with two language pairs: Frenchto English and Spanish to English on the Europarlcorpus (v7) (Koehn, 2005) and used ACL/WMT2005 2 data for dev and test sets.For the base models, we used an in-houseimplementation of hierarchical phrase-based sys-tems, Kriya (Sankaran et al, 2012), which usesthe same features mentioned in (Chiang, 2005):forward and backward relative-frequency and lex-ical TM probabilities; LM; word, phrase and glue-rules penalty.
GIZA++ (Och and Ney, 2003) hasbeen used for word alignment with phrase lengthlimit of 10.
Feature weights were optimized usingMERT (Och, 2003).
We built a 5-gram languagemodel on the English side of Europarl and used theKneser-Ney smoothing method and SRILM (Stol-cke, 2002) as the language model toolkit.2http://www.statmt.org/wpt05/mt-shared-task/336Direction k-fold Resub Mean WSUM WMAX PROD SW:MAX SW:SUMFr - En2 18.08 19.67 22.32 22.48 22.06 21.70 21.814 18.08 21.80 23.14 23.48 23.55 22.83 22.958 18.08 22.47 23.76 23.75 23.78 23.02 23.47Es - En2 18.61 19.23 21.62 21.33 21.49 21.48 21.514 18.61 21.52 23.42 22.81 22.91 22.81 22.928 18.61 22.20 23.69 23.89 23.51 22.92 23.26Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).Direction Corpus k-fold Baseline BMA WSUM WMAX PROD SW:MAX SW:SUMFr - En 10k+dev 6 28.75 29.49 29.87 29.78 29.21 29.69 29.59100k+dev 11 / 51 29.53 29.75 34.00 34.07 33.11 34.05 33.96Es - En 10k+dev 6 28.21 28.76 29.59 29.51 29.15 29.10 29.21100k+dev 11 / 51 33.25 33.44 34.21 34.00 33.17 34.19 34.22Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.4.1 Training on devsetWe first consider the scenario in which there isno parallel data between a language pair excepta small bi-text used as a devset.
We use no spe-cific training data and construct a SMT systemcompletely on the devset by using our approachand compare to two different baselines.
A natu-ral baseline when having a limited parallel text isto do re-substitution validation where the modelis trained on the whole devset and is tuned on thesame set.
This validation process suffers seriouslyfrom over-fitting.
The second baseline is the meanof BLEU scores of all base models.Table 2 summarizes the BLEU scores on thetestset when using stacking only on the devset ontwo different language pairs.
As the table shows,increasing the number of folds results in higherBLEU scores.
However, doing such will generallylead to higher variance among base learners.Figure 1 shows the BLEU score of each of thebase models resulted from a 20-fold partitioningof the devset alng with the strong models?
BLEUscores.
As the figure shows, the strong models aregenerally superior to the base models whose meanis represented as a horizontal line.4.2 Training on train+devWhen we have some training data, we can usethe cross-validation-style partitioning to create ksplits.
We then train a system on k ?
1 folds andtune on the devset.
However, each system eventu-ally wastes a fold of the training data.
In order totake advantage of that remaining fold, we concate-nate the devset to the training set and partition thewhole union.
In this way, we use all data availableto us.
We experimented with two sizes of train-ing data: 10k sentence pairs and 100k, that withthe addition of the devset, we have 12k and 102ksentence-pair corpora.Table 1 summarizes statistics of the data setsused in this scenario.
Table 3 reports the BLEUscores when using stacking on these two corpussizes.
The baselines are the conventional systemswhich are built on the training-set only and tunedon the devset as well as Bayesian Model Averaging(BMA, see ?5).
For the 100k+dev corpus, we sam-pled 11 partitions from all 51 possible partitionsby taking every fifth partition as training data.
Theresults in Table 3 show that stacking can improveover the baseline BLEU scores by up to 4 points.Examining the performance of the differentmixture operations, we can see that WSUM andWMAX typically outperform other mixture oper-ations.
Different mixture operations can be domi-nant in different language pairs and different sizesof training sets.5 Related WorkXiao et al (2013) have applied both boostingand bagging on three different statistical machinetranslation engines: phrase-based (Koehn et al,2003), hierarchical phrase-based (Chiang, 2005)and syntax-based (Galley et al, 2006) and showedSMT can benefit from these methods as well.Duan et al (2009) creates an ensemble of mod-els by using feature subspace method in the ma-chine learning literature (Ho, 1998).
Each mem-ber of the ensemble is built by removing one non-LM feature in the log-linear framework or varyingthe order of language model.
Finally they use asentence-level system combination on the outputsof the base models to pick the best system for each337wsumwmaxsw:maxsw:sumprodtestset BLEUModelsMeanBase ModelsStrong ModelsFigure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation.
Thehorizontal line shows the mean of base models?
scores.sentence.
Though, they do not combine the hy-potheses search spaces of individual base models.Our work is most similar to that of Duan etal.
(2010) which uses Bayesian model averaging(BMA) (Hoeting et al, 1999) for SMT.
They usedsampling without replacement to create a num-ber of base models whose phrase-tables are com-bined with that of the baseline (trained on the fulltraining-set) using linear mixture models (Fosterand Kuhn, 2007).Our approach differs from this approach in anumber of ways: i) we use cross-validation-stylepartitioning for creating training subsets whilethey do sampling without replacement (80% of thetraining set); ii) in our approach a number of basemodels are trained and tuned and they are com-bined on-the-fly in the decoder using ensemble de-coding which has been shown to be more effectivethan offline combination of phrase-table-only fea-tures; iii) in Duan et al (2010)?s method, each sys-tem gives up 20% of the training data in exchangefor more diversity, but in contrast, our method notonly uses all available data for training, but pro-motes diversity through allowing each model totune on a different data set; iv) our approach takesadvantage of held out data (the tuning set) in thetraining of base models which is beneficial espe-cially when little parallel data is available or tun-ing/test sets and training sets are from different do-mains.Empirical results (Table 3) also show that ourapproach outperforms the Bayesian model averag-ing approach (BMA).6 Conclusion & Future WorkIn this paper, we proposed a novel method on ap-plying stacking to the statistical machine transla-tion task.
The results when using no, 10k and 100ksentence-pair training sets (along with a develop-ment set for tuning) show that stacking can yieldan improvement of up to 4 BLEU points over con-ventionally trained SMT models which use a fixedtraining and tuning set.Future work includes experimenting with largertraining sets to investigate how useful this ap-proach can be when having different sizes of train-ing data.ReferencesLeo Breiman.
1996a.
Bagging predictors.
MachineLearning, 24(2):123?140, August.Leo Breiman.
1996b.
Stacked regressions.
MachineLearning, 24(1):49?64, July.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In ACL?05: Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 263?270, Morristown, NJ, USA.
ACL.Nan Duan, Mu Li, Tong Xiao, and Ming Zhou.
2009.The feature subspace method for smt system combi-nation.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 3 - Volume 3, EMNLP ?09, pages 1096?1104, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Nan Duan, Hong Sun, and Ming Zhou.
2010.
Transla-tion model generalization using probability averag-ing for machine translation.
In Proceedings of the33823rd International Conference on ComputationalLinguistics, COLING ?10, pages 304?312, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages157?166, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,StatMT ?07, pages 128?135, Stroudsburg, PA, USA.ACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and trainingof context-rich syntactic translation models.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, ACL-44, pages 961?968, Stroudsburg, PA,USA.
Association for Computational Linguistics.Tin Kam Ho.
1998.
The random subspace method forconstructing decision forests.
IEEE Trans.
PatternAnal.
Mach.
Intell., 20(8):832?844, August.Jennifer A. Hoeting, David Madigan, Adrian E.Raftery, and Chris T. Volinsky.
1999.
BayesianModel Averaging: A Tutorial.
Statistical Science,14(4):382?401.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the Human Language Technology Con-ference of the NAACL, pages 127?133, Edmonton,May.
NAACL.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT summit, volume 5.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA,1st edition.Antonio Lagarda and Francisco Casacuberta.
2008.Applying boosting to statistical machine translation.In Annual Meeting of European Association for Ma-chine Translation (EAMT), pages 88?96.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL-08: HLT, pages950?958, Columbus, Ohio, June.
Association forComputational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedings ofthe 41th Annual Meeting of the ACL, Sapporo, July.ACL.Majid Razmara, George Foster, Baskaran Sankaran,and Anoop Sarkar.
2012.
Mixing multiple transla-tion models in statistical machine translation.
In The50th Annual Meeting of the Association for Compu-tational Linguistics, Proceedings of the Conference,July 8-14, 2012, Jeju Island, Korea - Volume 1: LongPapers, pages 940?949.
The Association for Com-puter Linguistics.Erik F. Tjong Kim Sang.
2002.
Memory-based shal-low parsing.
J. Mach.
Learn.
Res., 2:559?594,March.Baskaran Sankaran, Majid Razmara, and AnoopSarkar.
2012.
Kriya an end-to-end hierarchicalphrase-based mt system.
The Prague Bulletin ofMathematical Linguistics, 97(97), April.Robert E. Schapire.
1990.
The strength of weak learn-ability.
Mach.
Learn., 5(2):197?227, July.Linfeng Song, Haitao Mi, Yajuan Lu?, and Qun Liu.2011.
Bagging-based system combination for do-main adaption.
In Proceedings of the 13th MachineTranslation Summit (MT Summit XIII), pages 293?299.
International Association for Machine Transla-tion, September.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference on Spoken Language Processing,pages 257?286.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: cheapand good?
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 649?652, Stroudsburg, PA,USA.
Association for Computational Linguistics.Nadi Tomeh, Alexandre Allauzen, Guillaume Wis-niewski, and Franc?ois Yvon.
2010.
Refining wordalignment with discriminative training.
In Proceed-ings of The Ninth Conference of the Association forMachine Translation in the Americas (AMTA 2010).David H. Wolpert.
1992.
Stacked generalization.
Neu-ral Networks, 5:241?259.Tong Xiao, Jingbo Zhu, Muhua Zhu, and HuizhenWang.
2010.
Boosting-based system combina-tion for machine translation.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 739?748,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Tong Xiao, Jingbo Zhu, and Tongran Liu.
2013.
Bag-ging and boosting statistical machine translation sys-tems.
Artificial Intelligence, 195:496?527, Febru-ary.339
