Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146?1151,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDeveloping Age and Gender Predictive Lexica over Social MediaMaarten Sap1Gregory Park1Johannes C. Eichstaedt1Margaret L. Kern1David Stillwell3Michal Kosinski3Lyle H. Ungar2and H. Andrew Schwartz21Department of Psychology, University of Pennsylvania2Computer & Information Science, University of Pennsylvania3Psychometrics Centre, University of Cambridgemaarten@sas.upenn.eduAbstractDemographic lexica have potential forwidespread use in social science, economic,and business applications.
We derive predic-tive lexica (words and weights) for age andgender using regression and classificationmodels from word usage in Facebook, blog,and Twitter data with associated demographiclabels.
The lexica, made publicly available,1achieved state-of-the-art accuracy in languagebased age and gender prediction over Face-book and Twitter, and were evaluated forgeneralization across social media genres aswell as in limited message situations.1 IntroductionUse of social media has enabled the study of psycho-logical and social questions at an unprecedented scale(Lazer et al., 2009).
This allows more data-driven dis-covery alongside the typical hypothesis-testing socialscience process (Schwartz et al., 2013b).
Social me-dia may track disease rates (Paul and Dredze, 2011;Google, 2014), psychological well-being (Dodds et al.,2011; De Choudhury et al., 2013; Schwartz et al.,2013a), and a host of other behavioral, psychologicaland medical phenomena (Kosinski et al., 2013).Unlike traditional hypothesis-driven social science,such large-scale social media studies rarely take intoaccount?or have access to?age and gender informa-tion, which can have a major impact on many ques-tions.
For example, females live almost five yearslonger than males (cdc, 2014; Marengoni et al., 2011).Men and women, on average, differ markedly in theirinterests and work preferences (Su et al., 2009).
Withage, personalities gradually change, typically becom-ing less open to experiences but more agreeable andconscientious (McCrae et al., 1999).
Additionally, so-cial media language varies by age (Kern et al., 2014;Pennebaker and Stone, 2003) and gender (Huffaker andCalvert, 2005).
Twitter may have a male bias (Misloveet al., 2011), while social media in general skew to-wards being young and female (pew, 2014).Accessible tools to predict demographic variablescan substantially enhance social media?s utility for so-1download at http://www.wwbp.org/data.htmlcial science, economic, and business applications.
Forexample, one can post-stratify population-level resultsto reflect a representative sample, understand variationacross age and gender groups, or produce personalizedmarketing, services, and sentiment recommendations;a movie may be generally disliked, except by people ina certain age group, whereas a product might be usedprimarily by one gender.This paper describes the creation of age and gen-der predictive lexica from a dataset of Facebook userswho agreed to share their status updates and reportedtheir age and gender.
The lexica, in the form of wordswith associated weights, are derived from a penalizedlinear regression (for continuous valued age) and sup-port vector classification (for binary-valued gender).
Inthis modality, the lexica are simply a transparent andportable means for distributing predictive models basedon words.
We test generalization and adapt the lex-ica to blogs and Twitter, plus consider situations whenlimited messages are available.
In addition to use inthe computational linguistics community, we believethe lexicon format will make it easier for social sci-entists to leverage data-driven models where manuallycreated lexica currently dominate2(Dodds et al., 2011;Tausczik and Pennebaker, 2010).2 Related WorkOnline behavior is representative of many aspects ofa user?s demographics (Pennacchiotti and Popescu,2011; Rao et al., 2010).
Many studies have used lin-guistic cues (such as ngrams) to determine if someonebelongs to a certain age group, be it on Twitter or an-other social media platform (Al Zamal et al., 2012;Argamon et al., 2009; Nguyen et al., 2013; Rangeland Rosso, 2013).
Gender prediction has been studiedacross blogs (Burger and Henderson, 2006; Goswamiet al., 2009), Yahoo!
search queries (Jones et al., 2007),and Twitter (Burger et al., 2011; Nguyen et al., 2013;Liu and Ruths, 2013; Rao et al., 2010).
Because Twit-ter does not make gender or age available, such workinfers gender and age by leveraging profile informa-tion, such as gender-discriminating names or crawlingfor links to publicly available data (e.g.
Burger et al.,2The LIWC lexicon, derived manually based on psycho-logical theory, (Pennebaker et al., 2001) had 1136 citations in2013 alone.11462011).While many studies have examined prediction of ageor gender, none (to our knowledge) have released amodel to the public, much less in the form of a lexi-con.
Additionally, most works in age prediction clas-sify users into bins rather than predicting a continuousreal-valued age as we do (exceptions: Nguyen et al.,2013; Jones et al., 2007).
People have also used onlinemedia to infer other demographic-like attributes suchas native language (Argamon et al., 2009), origin (Raoet al., 2010), and location (Jones et al., 2007).
An ap-proach similar to the one presented here could be usedto create lexica for any of these outcomes.While lexica are not often used for demographics,data-driven lexicon creation over social media has beenwell studied for sentiment, in which univariate tech-niques (e.g.
point-wise mutual information) domi-nate3.
For example, Taboada et al.
(2011) expandedan initial lexicon by adding on co-occurring words.More recently, Mohammad?s sentiment lexicon (Mo-hammad et al., 2013) was found to be the most in-formative feature for the top system in the SemEval-2013 social media sentiment analysis task (Wilson etal., 2013).
Approaches like point-wise mutual infor-mation take a univariate view on words?i.e.
the weightgiven to one feature (word) is not affected by otherfeatures.
Since language is highly collinear, we takea multivariate lexicon development approach, whichtakes covariance into account (e.g.
someone who men-tions ?hair?
often is more likely to mention ?brushing?,?style?, and ?cut?
; weighting these words in isolationmight ?double-count?
some information).3 MethodPrimary data.
Our primary dataset consists of Face-book messages from users of the MyPersonality appli-cation (Kosinski and Stillwell, 2012).
Messages wereposted between January 2009 and October 2011.
Werestrict our analysis to those Facebook users meetingcertain criteria: they must indicate English as a primarylanguage, have written at least 1,000 words in their sta-tus updates, be younger than 65 years old (data beyondthis age becomes very sparse), and indicate their gen-der and age.
This resulted in a dataset of N = 75,394users, who wrote over 300 million words collectively.We split our sample into training and test sets.
Ourprimary test set consists of a 1,000 randomly selectedFacebook users, while the training set that we used forcreating the lexica was a subset (N = 72,874) of theremaining users.Additional data To evaluate our predictive lexica indiffering situations, we utilize three additional datasets:3Note that the point-wise information-derived sentimentlexica are often used as features in a supervised model, essen-tially dimensionally reducing a large set of words into posi-tive and negative sentiment, while our lexica represent thepredictive model itself.stratified Facebook data, blogs, and tweets.
The strat-ified Facebook data (exclusively used for testing) con-sists of equal proportions of 1,520 males and femalesacross 12 4-year age bins starting at 13 and ending at60.4This roughly matchs the size of the main test set.Seeking out-of-domain data, we downloaded ageand gender annotated blogs from 2004 (Schler et al.,2006) (also used in Goswami et al., 2009) and genderlabeled tweets (Volkova et al., 2013).
Limiting the sam-ple to users who wrote at least 1000 words, the totalnumber of bloggers is 15,006, of which 50.6% are fe-male and only 15% are over 27 (reflecting the youngerpopulation standard in social media).
From this we usea randomly selected 1,000 bloggers as a blogger test setand the remaining 14,006 bloggers for training.
Sim-ilarly for the Twitter dataset, we use 11,000 randomgender-only annotated users, in which 51.9% are fe-male.
We again randomly select 1,000 users as a testset for gender prediction and use the remaining 10,000for training.3.1 Lexicon CreationWe present a method of weighted lexicon creation byusing the coefficients from linear multivariate regres-sion and classification models.
Before delving into thecreation process, consider that a weighted lexicon is of-ten applied as the sum of all weighted word relativefrequencies over a document:usagelex=?word?lexwlex(word) ?freq(word, doc)freq(?, doc)where wlex(word) is the lexicon (lex) weight for theword, freq(word, doc) is frequency of the word in thedocument (or for a given user), and freq(?, doc) is thetotal word count for that document (or user).Further consider how one applies linear multivariatemodels in which the goal is to optimize feature coeffi-cients that best fit the continuous outcome (regression)or separate two classes (classification):y = (?f?featureswf?
xf) + w0where xfis the value for a feature (f ), wfis the fea-ture coefficient, and w0is the intercept (a constant fitto shift the data such that it passes through the origin).In the case of regression, y is the outcome value (e.g.age) while in classification y is used to separate classes(e.g.
>= 0 is female, < 0 is male).
If all features areword relative frequencies (freq(word,doc)freq(?,doc)) then manymultivariate modeling techniques can simply be seenas learning a weighted lexicon plus an intercept5.465 females and 65 males in each of the first 11 bins:[13,16], [17,20], .
.
.
, [53, 56]; the last bin ([57, 60]) contained45 males and 45 females.
The [61.64] bin was excluded as itwas much smaller.5included in the lexicon distribution1147age gendermodel\corpus randFB stratFB randBG randFB stratFB randBG randTr mae r mae r mae acc acc acc accbaseline 0 6.14 0 11.62 0 6.11 .617 .500 .508 .518FBlex.835 3.40 .801 6.94 .710 5.76 .917 .913 .774 .856BGlex.664 4.26 .656 11.39 .768 3.63 .838 .803 .824 .834FB+BGlex.831 3.42 .795 7.06 .762 3.76 .913 .909 .822 .858Tlex.816 .820 .763 .889FB+BG+Tlex.919 .910 .820 .900Table 1: Prediction accuracies for age (Pearson correlation coefficient(r); mean absolute error (mae) in years)and gender (accuracy %).
Baseline for age is mean age of training sample; for gender, it is the most frequentclass (female).
Lexica tested include those derived from Facebook (FBlex), blogs (BGlex), and Twitter (Tlex).We evaluate over a random Facebook sample (randFB), a stratified Facebook sample (stratFB), a random bloggersample (randBG), and a random twitter sample (randT).
All results were a significant (p < 0.001) improvementover the baseline.In practice, we learn our 1gram coefficients (i.e.
lex-icon weights) from ridge regression (Hoerl and Ken-nard, 1970) for age (continuous variable) and from sup-port vector classification (Fan et al., 2008) for gender(binary variable).
Ridge regression uses an L2 (?||?||2)penalization to avoid overfitting (Hoerl and Kennard,1970).
Although some words no doubt have a non-linear relationship with age (e.g., ?fiance?
peaks in the20s), we still find high accuracy from a linear model(see Table 1) and it allows for a distribution of themodel in the accessible form of a lexicon.
For genderprediction, we use an SVM with a linear kernel withL1 penalization (?||?||1) (Tibshirani, 1996).
Becausethe L1 penalization zeros-out many coefficients, it hasthe added advantage of effectively reducing the size ofthe lexica.
Using the training data, we test a variety al-gorithms including the lasso, elastic net regression, andL2 penalized SVMs in order to decide which learningalgorithms to use.To extract the words (1grams) to use as featuresand which make up lexica, we use the Happier FunTokenizer,6which handles social media content andmarkup such as emoticons or hashtags.
For our mainuser-level models, word usage is aggregated as the rel-ative frequency (freq(word,user)freq(?,user)).
Due to the sparseand large vocabulary of social media data, we limit the1grams to those used by at least 1% of users.4 EvaluationWe evaluate our predictive lexica across held-out userdata.
First, we see how well lexica derived from Face-book users predict a random set of additional users.Then, we explore generalization of the models in vari-ous other settings: on a stratified Facebook test sample,blogs, and Twitter.
Finally, we compare lexica fit to arestricted number of messages per user.Results of our evaluation over Facebook users areshown in Table 1 (randFB columns).
Accuracies forage are reported as Pearson correlation coefficients (r)6downloaded from http://www.wwbp.org/data.htmland mean absolute errors (mae), measured in years.For gender, we use an accuracy % (number-correct overtest-size).
As baselines, we use the mean for age (23.0years old) and the most frequent class (female) for gen-der.
We see that for both age and gender, accuracies aresubstantially higher than the baseline.
These accuracieswere just below with no significant difference previousstate-of-the-art results (Schwartz et al., 2013; r = 0.84for age and 91.9% accuracy for gender).7Because of the nature of our datasets (the Face-book data is private) and task (user-level predictions),comparable previous studies are nearly nonexistent.Nonetheless, the Twitter data was a random subset ofusers based on the (Burger et al., 2011) dataset exclud-ing non-English tweets, making it somewhat compa-rable.
In this case, the lexica outperformed previousresults for gender prediction of Twitter users, whichranged from 75.5% to 87% (Burger et al., 2011; Ciotet al., 2013; Liu and Ruths, 2013; Al Zamal et al.,2012).
However, the lexica were unable to match the92.0% accuracy Burger et al.
(2011) achieved whenusing profile information in addition to text.
No othersimilar studies ?
to the best of our knowledge ?
havebeen conducted.Application in other settings.
While Facebook is theideal setting to apply our lexica, we hope that they gen-eralize to other situations.
To evaluate their utility inother settings, we first tested them over a gender andage stratified Facebook sample.
Our random sample,like all of Facebook, is biased toward the young; thisstratified test sample contains equal numbers of malesand females, ages 13 to 60.
Next, we use the lexica topredict data from other domains: blogs (Schler et al.,2006) and Twitter (Volkova et al., 2013).
In this case,our goal was to account for the content and stylisticvariation that may be specific to Facebook.7Adding 2 and 3-grams increases the performance of ourmodel (r = 0.85, 92.7%), just above our previous results(Schwartz et al., 2013b).
However, with the accessibility ofsingle word lexica in mind, this current work focuses on fea-tures based entirely on 1grams.1148# Msgs: all 100 20 5 1age .831 .820 .688 .454 .156gender .919 .901 .796 .635 .554Table 2: Prediction accuracies for age (Pearson correla-tion) and gender (accuracy %) when reducing the num-ber of messages from each user.Results over these additional datasets are shown inTable 1 (stratFB, randBG, and randT columns).
Theperformance decreases as expected since these datasetshave differing distributions, but it is still substantiallyabove mean and most frequent class baselines on thestratified dataset.
Over blogs and Twitter, both ageand gender prediction accuracies drop to a greater de-gree (when only using the Facebook-trained models),suggesting stylistic or content differences between thedomains.
However, when using lexica created withdata from across multiple domains, the results in Face-book, blogs, and Twitter remain in line with resultsfrom models created specifically over their respectivedomains.
In light of this result, we release the FB+BGage & FB+BG+T gender models as lexica (available atwww.wwbp.org/data.html).Limiting messages per user.
As previously noted,some applications of demographic estimation requirepredictions over more limited messages.
We explorethe accuracy of user-level age and gender predictionsas the number of messages per user decreases in Ta-ble 2.
For these tests we used the FB+BG age &FB+BG+T gender lexica.
Confirming findings by VanDurme (2012), the fewer posts one has for each user,the less accurate the gender and age predictions.
Still,given the average user posted 205 messages, it seemsthat not all messages from a user are necessary to makea decent inference on their age and gender.
Future workmay explore models developed specifically for theselimited situations.5 ConclusionWe created publicly available lexica (words andweights) using regression and classification modelsover language usage in social media.
Evaluation of thelexica over Facebook yielded accuracies in line withstate-of-the-art age (r = 0.831) and gender (91.9% ac-curacy) prediction.
By deriving the lexica from Face-book, blogs, and Twitter, we found the predictive powergeneralized across all three domains with little sacrificeto any one domain, suggesting the lexica may be usedin additional social media domains.
We also found thelexica maintain reasonable accuracy when writing sam-ples were somewhat small (e.g.
20 messages) but otherapproaches may be best when dealing with more lim-ited data.Given that manual lexica are already extensively em-ployed in social sciences such as psychology, eco-nomics, and business, using lexical representations ofdata-driven models allows the utility of our models toextend beyond the borders of the field of NLP.AcknowledgementSupport for this work was provided by the TempletonReligion Trust and by Martin Seligman of the Univer-sity of Pennsylvania?s Positive Psychology Center.ReferencesFaiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and latent attribute inference: Inferringlatent attributes of twitter users from neighbors.
InICWSM.Shlomo Argamon, Moshe Koppel, James W Pen-nebaker, and Jonathan Schler.
2009.
Automaticallyprofiling the author of an anonymous text.
Commu-nications of the ACM, 52(2):119?123.John D Burger and John C Henderson.
2006.
An ex-ploration of observable features related to bloggerage.
In AAAI Spring Symposium: ComputationalApproaches to Analyzing Weblogs, pages 15?20.John D Burger, John C Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender ontwitter.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1301?1309.
Association for Computational Linguis-tics.2014.
Faststats: How healthy are we.
http://www.cdc.gov/nchs/fastats/healthy.htm.Accessed on March 12, 2014.Morgane Ciot, Morgan Sonderegger, and Derek Ruths.2013.
Gender inference of twitter users in non-english contexts.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, Seattle, Wash, pages 18?21.Munmun De Choudhury, Scott Counts, and EricHorvitz.
2013.
Predicting postpartum changes inemotion and behavior via social media.
In Pro-ceedings of the 2013 ACM annual conference onHuman factors in computing systems, pages 3267?3276.
ACM.Peter Sheridan Dodds, Kameron Decker Harris, Is-abel M Kloumann, Catherine A Bliss, and Christo-pher M Danforth.
2011.
Temporal patterns of hap-piness and information in a global social network:Hedonometrics and twitter.
PloS one, 6(12):e26752.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Inc.
Google.
2014.
Google flu trends.
http://www.google.org/flutrends.
Accessed onMarch 12, 2014.1149Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.2009.
Stylometric analysis of bloggers age and gen-der.
In Third International AAAI Conference on We-blogs and Social Media.Arthur E Hoerl and Robert W Kennard.
1970.
Ridgeregression: Biased estimation for nonorthogonalproblems.
Technometrics, 12(1):55?67.David A Huffaker and Sandra L Calvert.
2005.Gender, identity, and language use in teenageblogs.
Journal of Computer-Mediated Communica-tion, 10(2):00?00.Rosie Jones, Ravi Kumar, Bo Pang, and AndrewTomkins.
2007.
I know what you did last summer:query logs and user privacy.
In Proceedings of thesixteenth ACM conference on Conference on infor-mation and knowledge management, pages 909?914.ACM.Margaret L Kern, Johannes C Eichstaedt, H AndrewSchwartz, Gregory Park, Lyle H Ungar, David JStillwell, Michal Kosinski, Lukasz Dziurzynski, andMartin EP Seligman.
2014.
From sooo excited!!
!to so proud: Using language to study development.Developmental psychology, 50(1):178?188.Michal Kosinski and David J Still-well.
2012. mypersonality project.http://www.mypersonality.org/wiki/.Michal Kosinski, David J Stillwell, and Thore Graepel.2013.
Private traits and attributes are predictablefrom digital records of human behavior.
volume110, pages 5802?5805.
National Acad Sciences.David Lazer, Alex Pentland, Lada Adamic, Sinan Aral,Albert-Laszlo Barabasi, Devon Brewer, NicholasChristakis, Noshir Contractor, James Fowler, MyronGutmann, Tony Jebara, Gary King, Michael Macy,Deb Roy, and Marshall Van Alstyne.
2009.
Com-putational social science.
Science, 323(5915):721?723.Wendy Liu and Derek Ruths.
2013.
Whats in a name?using first names as features for gender inference intwitter.
In Analyzing Microtext: 2013 AAAI SpringSymposium.Alessandra Marengoni, Sara Angleman, Ren?e Melis,Francesca Mangialasche, Anita Karp, Annika Gar-men, Bettina Meinow, and Laura Fratiglioni.
2011.Aging with multimorbidity: a systematic review ofthe literature.
Ageing research reviews, 10(4):430?439.Robert R McCrae, Paul T Costa, Margarida Pedrosode Lima, Ant?onio Sim?oes, Fritz Ostendorf, AloisAngleitner, Iris Maru?si?c, Denis Bratko, Gian Vitto-rio Caprara, Claudio Barbaranelli, et al.
1999.
Agedifferences in personality across the adult life span:parallels in five cultures.
Developmental Psychol-ogy, 35(2):466?477.Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka Onnela, and J Niels Rosenquist.
2011.Understanding the demographics of twitter users.ICWSM, 11:5th.Saif M Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
arXivpreprint arXiv:1308.6242.Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, andTheo Meder.
2013. how old do you think i am?
:A study of language and age in twitter.
In Proceed-ings of the Seventh International AAAI Conferenceon Weblogs and Social Media.Michael J Paul and Mark Dredze.
2011.
You are whatyou tweet: Analyzing twitter for public health.
InICWSM.Marco Pennacchiotti and Ana-Maria Popescu.
2011.A machine learning approach to twitter user classifi-cation.
In ICWSM.James W Pennebaker and Lori D Stone.
2003.
Wordsof wisdom: language use over the life span.
Journalof Personality and Social Psychology, 85(2):291?301.James W Pennebaker, Martha E Francis, and Roger JBooth.
2001.
Linguistic inquiry and word count:Liwc 2001.
71:2001.2014.
Social networking fact sheet.
http://www.pewinternet.org/fact-sheets/social-networking-fact-sheet/.
Ac-cessed on August 26, 2014.Francisco Rangel and Paolo Rosso.
2013.
Use of lan-guage and author profiling: Identification of genderand age.
Natural Language Processing and Cogni-tive Science, page 177.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in twitter.
In Proceedings of the 2nd in-ternational workshop on Search and mining user-generated contents, pages 37?44.
ACM.Jonathan Schler, Moshe Koppel, Shlomo Argamon,and James W Pennebaker.
2006.
Effects of ageand gender on blogging.
In AAAI Spring Sympo-sium: Computational Approaches to Analyzing We-blogs, volume 6, pages 199?205.H Andrew Schwartz, Johannes C Eichstaedt, Mar-garet L Kern, Lukasz Dziurzynski, Megha Agrawal,Gregory J Park, Shrinidhi K Lakshmikanth, SnehaJha, Martin EP Seligman, Lyle Ungar, et al.
2013a.Characterizing geographic variation in well-beingusing tweets.
In ICWSM.H Andrew Schwartz, Johannes C Eichstaedt, Mar-garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-mones, Megha Agrawal, Achal Shah, Michal Kosin-ski, David J Stillwell, Martin EP Seligman, et al.2013b.
Personality, gender, and age in the languageof social media: The open-vocabulary approach.PloS one, 8(9):e73791.1150Rong Su, James Rounds, and Patrick Ian Armstrong.2009.
Men and things, women and people: a meta-analysis of sex differences in interests.
Psychologi-cal Bulletin, 135(6):859?884.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional linguistics, 37(2):267?307.Yla R Tausczik and James W Pennebaker.
2010.
Thepsychological meaning of words: Liwc and comput-erized text analysis methods.
Journal of languageand social psychology, 29(1):24?54.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 267?288.Benjamin Van Durme.
2012.
Streaming analysis ofdiscourse participants.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Nat-ural Language Learning, pages 48?58.
Associationfor Computational Linguistics.Svitlana Volkova, Theresa Wilson, and DavidYarowsky.
2013.
Exploring demographic lan-guage variations to improve multilingual sentimentanalysis in social media.
In Proceedings of the2013 Conference on Empirical Methods on NaturalLanguage Processing.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Proceedings of the International Work-shop on Semantic Evaluation, SemEval, volume 13.1151
