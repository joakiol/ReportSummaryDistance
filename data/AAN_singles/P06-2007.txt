Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 49?56,Sydney, July 2006. c?2006 Association for Computational LinguisticsN Semantic Classes are Harder than TwoBen Carterette?CIIRUniversity of MassachusettsAmherst, MA 01003carteret@cs.umass.eduRosie JonesYahoo!
Research3333 Empire Ave.Burbank, CA 91504jonesr@yahoo-inc.comWiley Greiner?Los Angeles Software Inc.1329 Pine StreetSanta Monica, CA 90405w.greiner@lasoft.comCory BarrYahoo!
Research3333 Empire Ave.Burbank, CA 91504barrc@yahoo-inc.comAbstractWe show that we can automatically clas-sify semantically related phrases into 10classes.
Classification robustness is im-proved by training with multiple sourcesof evidence, including within-documentcooccurrence, HTML markup, syntacticrelationships in sentences, substitutabilityin query logs, and string similarity.
Ourwork provides a benchmark for automaticn-way classification into WordNet?s se-mantic classes, both on a TREC news cor-pus and on a corpus of substitutable searchquery phrases.1 IntroductionIdentifying semantically related phrases has beendemonstrated to be useful in information retrieval(Anick, 2003; Terra and Clarke, 2004) and spon-sored search (Jones et al, 2006).
Work on seman-tic entailment often includes lexical entailment asa subtask (Dagan et al, 2005).We draw a distinction between the task of iden-tifying terms which are topically related and iden-tifying the specific semantic class.
For example,the terms ?dog?, ?puppy?, ?canine?, ?schnauzer?,?cat?
and ?pet?
are highly related terms, whichcan be identified using techniques that includedistributional similarity (Lee, 1999) and within-document cooccurrence measures such as point-wise mutual information (Turney et al, 2003).These techniques, however, do not allow us to dis-tinguish the more specific relationships:?
hypernym(dog,puppy)?This work was carried out while these authors were atYahoo!
Research.?
hyponym(dog,canine)?
coordinate(dog,cat)Lexical resources such as WordNet (Miller,1995) are extremely useful, but are limited by be-ing manually constructed.
They do not contain se-mantic class relationships for the many new termswe encounter in text such as web documents, forexample ?mp3 player?
or ?ipod?.
We can useWordNet as training data for such classification tothe extent that the training on pairs found in Word-Net and testing on pairs found outside WordNetprovides accurate generalization.We describe a set of features used to train n-way supervised machine-learned classification ofsemantic classes for arbitrary pairs of phrases.
Re-dundancy in the sources of our feature informa-tion means that we are able to provide coverageover an extremely large vocabulary of phrases.
Wecontrast this with techniques that require parsingof natural language sentences (Snow et al, 2005)which, while providing reasonable performance,can only be applied to a restricted vocabulary ofphrases cooccuring in sentences.Our contributions are:?
Demonstration that binary classification re-moves the difficult cases of classification intoclosely related semantic classes?
Demonstration that dependency parser pathsare inadequate for semantic classification into7 WordNet classes on TREC news corpora?
A benchmark of 10-class semantic classifica-tion over highly substitutable query phrases?
Demonstration that training a classifier us-ing WordNet for labeling does not generalizewell to query pairs?
Demonstration that much of the performancein classification can be attained using only49syntactic features?
A learning curve for classification of queryphrase pairs that suggests the primary bottle-neck is manually labeled training instances:we expect our benchmark to be surpassed.2 Relation to Previous WorkSnow et al (2005) demonstrated binary classi-fication of hypernyms and non-hypernyms usingWordNet (Miller, 1995) as a source of training la-bels.
Using dependency parse tree paths as fea-tures, they were able to generalize from WordNetlabelings to human labelings.Turney et al (2003) combined features to an-swer multiple-choice synonym questions from theTOEFL test and verbal analogy questions fromthe SAT college entrance exam.
The multiple-choice questions typically do not consist of mul-tiple closely related terms.
A typical example isgiven by Turney:?
hidden:: (a) laughable (c) ancient(b) veiled (d) revealedNote that only (b) and (d) are at all related to theterm, so the algorithm only needs to distinguishantonyms from synonyms, not synonyms from sayhypernyms.We use as input phrase pairs recorded in querylogs that web searchers substitute during searchsessions.
We find much more closely relatedphrases:?
hidden::(a) secret (e) hiden(b) hidden camera (f) voyeur(c) hidden cam (g) hide(d) spyThis set contains a context-dependent synonym,topically related verbs and nouns, and a spellingcorrection.
All of these could cooccur on webpages, so simple cooccurrence statistics may notbe sufficient to classify each according to the se-mantic type.We show that the techniques used to performbinary semantic classification do not work as wellwhen extended to a full n-way semantic classifi-cation.
We show that using a variety of featuresperforms better than any feature alone.3 Identifying Candidate Phrases forClassificationIn this section we introduce the two data sourceswe use to extract sets of candidate related phrasesfor classification: a TREC-WordNet intersectionand query logs.3.1 Noun-Phrase Pairs Cooccuring in TRECNews SentencesThe first is a data-set derived from TREC newscorpora and WordNet used in previous work forbinary semantic class classification (Snow et al,2005).
We extract two sets of candidate-relatedpairs from these corpora, one restricted and onemore complete set.Snow et al obtained training data from the inter-section of noun-phrases cooccuring in sentences ina TREC news corpus and those that can be labeledunambiguously as hypernyms or non-hypernymsusing WordNet.
We use a restricted set since in-stances selected in the previous work are a subsetof the instances one is likely to encounter in text.The pairs are generally either related in one typeof relationship, or completely unrelated.In general we may be able to identify relatedphrases (for example with distributional similarity(Lee, 1999)), but would like to be able to automat-ically classify the related phrases by the type ofthe relationship.
For this task we identify a largerset of candidate-related phrases.3.2 Query Log DataTo find phrases that are similar or substitutable forweb searchers, we turn to logs of user search ses-sions.
We look at query reformulations: a pairof successive queries issued by a single user ona single day.
We collapse repeated searches forthe same terms, as well as query pair sequencesrepeated by the same user on the same day.3.2.1 Substitutable Query SegmentsWhole queries tend to consist of several con-cepts together, for example ?new york | maps?
or?britney spears | mp3s?.
We identify segments orphrases using a measure over adjacent terms sim-ilar to mutual information.
Substitutions occur atthe level of segments.
For example, a user mayinitially search for ?britney spears | mp3s?, thensearch for ?britney spears | music?.
By aligningquery pairs with a single substituted segment, wegenerate pairs of phrases which a user has substi-tuted.
In this example, the phrase ?mp3s?
was sub-stituted by the phrase ?music?.Aggregating substitutable pairs over millions ofusers and millions of search sessions, we can cal-culate the probability of each such rewrite, then50test each pair for statistical significance to elim-inate phrase rewrites which occurred in a smallnumber of sessions, perhaps by chance.
To testfor statistical significance we use the pair inde-pendence likelihood ratio, or log-likelihood ratio,test.
This metric tests the hypothesis that the prob-ability of phrase ?
is the same whether phrase ?has been seen or not by calculating the likelihoodof the observed data under a binomial distributionusing probabilities derived using each hypothesis(Dunning, 1993).log?
= logL (P (?|?)
= P (?|??
))L (P (?|?)
6= P (?|??
))A high negative value for ?
suggests a strongdependence between query ?
and query ?.4 Labeling Phrase Pairs for SupervisedLearningWe took a random sample of query segment sub-stitutions from our query logs to be labeled.
Thesampling was limited to pairs that were frequentsubstitutions for each other to ensure a high prob-ability of the segments having some relationship.4.1 WordNet LabelingWordNet is a large lexical database of Englishwords.
In addition to defining several hun-dred thousand words, it defines synonym sets, orsynsets, of words that represent some underly-ing lexical concept, plus relationships betweensynsets.
The most frequent relationships betweennoun-phrases are synonym, hyponym, hypernym,and coordinate, defined in Table 1.
We also mayuse meronym and holonym, defined as the PART-OFrelationship.We used WordNet to automatically label thesubset of our sample for which both phrases occurin WordNet.
Any sense of the first segment havinga relationship to any sense of the second would re-sult in the pair being labeled.
Since WordNet con-tains many other relationships in addition to thoselisted above, we group the rest into the other cate-gory.
If the segments had no relationship in Word-Net, they were labeled no relationship.4.2 Segment Pair LabelsPhrase pairs passing a statistical test are com-mon reformulations, but can be of many seman-tic types.
Rieh and Xie (2001) categorized typesof query reformulations, defining 10 general cat-egories: specification, generalization, synonym,parallel movement, term variations, operator us-age, error correction, general resource, special re-source, and site URLs.
We redefine these slightlyto apply to query segments.
The summary of thedefinitions is shown in Table 1, along with the dis-tribution in the data of pairs passing the statisticaltest.4.2.1 Hand LabelingMore than 90% of phrases in query logs do notappear in WordNet due to being spelling errors,web site URLs, proper nouns of a temporal nature,etc.
Six annotators labeled 2, 463 segment pairsselected randomly from our sample.
Annotatorsagreed on the label of 78% of pairs, with a Kappastatistic of .74.5 Automatic ClassificationWe wish to perform supervised classification ofpairs of phrases into semantic classes.
To do this,we will assign features to each pair of phrases,which may be predictive of their semantic rela-tionship, then use a machine-learned classifier toassign weights to these features.
In Section 7 wewill look at the learned weights and discuss whichfeatures are most significant for identifying whichsemantic classes.5.1 FeaturesFeatures for query substitution pairs are extractedfrom query logs and web pages.5.1.1 Web Page / Document FeaturesWe submit the two segments to a web searchengine as a conjunctive query and download thetop 50 results.
Each result is converted into anHTML Document Object Model (DOM) tree andsegmented into sentences.Dependency Tree Paths The path from the firstsegment to the second in a dependency parsetree generated by MINIPAR (Lin, 1998)from sentences in which both segments ap-pear.
These were previously used by Snowet al (2005).
These features were extractedfrom web pages in all experiments, exceptwhere we identify that we used TREC newsstories (the same data as used by Snow et al).HTML Paths The paths from DOM tree nodesthe first segment appears in to nodes the sec-ond segment appears in.
The value is thenumber of times the path occurs with the pair.51Class Description Example %synonym one phrase can be used in place of the other without loss in meaning low cost; cheap 4.2hypernym X is a hypernym of Y if and only if Y is a X muscle car; mustang 2.0hyponym X is a hyponym of Y if and only if X is a Y (inverse of hypernymy) lotus; flowers 2.0coordinate there is some Z such that X and Y are both Zs aquarius; gemini 13.9generalization X is a generalization of Y if X contains less information about the topic lyrics; santana lyrics 4.8specialization X is a specification of Y if X contains more information about the topic credit card; card 4.7spelling change spelling errors, typos, punctuation changes, spacing changes peopl; people 14.9stemmed form X and Y have the same lemmas ant; ants 3.4URL change X and Y are related and X or Y is a URL alliance; alliance.com 29.8other relationship X and Y are related in some other way flagpoles; flags 9.8no relationship X and Y are not related in any obvious way crypt; tree 10.4Table 1: Semantic relationships between phrases rewritten in query reformulation sessions, along with their prevalence in ourdata.Lexico-syntactic Patterns (Hearst, 1992) A sub-string occurring between the two segmentsextracted from text in nodes in which bothsegments appear.
In the example fragment?authors such as Shakespeare?, the featureis ?such as?
and the value is the number oftimes the substring appears between ?author?and ?Shakespeare?.5.1.2 Query Pair FeaturesTable 2 summarizes features that are inducedfrom the query strings themselves or calculatedfrom query log data.5.2 Additional Training PairsWe can double our training set by adding for eachpair u1, u2 a new pair u2, u1.
The class of the newpair is the same as the old in all cases but hyper-nym, hyponym, specification, and generalization,which are inverted.
Features are reversed fromf(u1, u2) to f(u2, u1).A pair and its inverse have different sets of fea-tures, so splitting the set randomly into trainingand testing sets should not result in resubstitutionerror.
Nonetheless, we ensure that a pair and itsinverse are not separated for training and testing.5.3 ClassifierFor each class we train a binary one-vs.-all linear-kernel support vector machine (SVM) using theoptimization algorithm of Keerthi and DeCoste(2005).5.3.1 Meta-ClassifierFor n-class classification, we calibrate SVMscores to probabilities using the method describedby Platt (2000).
This gives us P (class|pair) foreach pair.
The final classification for a pair isargmaxclassP (class|pair).Source Snow (NIPS 2005) ExperimentTask binary hypernym binary hypernymData WordNet-TREC WordNet-TRECInstance Count 752,311 752,311Features minipar paths minipar pathsFeature Count 69,592 69,592Classifier logistic Regression linear SVMmaxF 0.348 0.453Table 3: Snow et als (2005) reported performance using lin-ear regression, and our reproduction of the same experiment,using a support vector machine (SVM).5.3.2 EvaluationBinary classifiers are evaluated by ranking in-stances by classification score and finding the MaxF1 (the harmonic mean of precision and recall;ranges from 0 to 1) and area under the ROC curve(AUC; ranges from 0.5 to 1 with at least 0.8 being?good?).
The meta-classifier is evaluated by pre-cision and recall of each class and classificationaccuracy of all instances.6 Experiments6.1 Baseline Comparison to Snow et al?sPrevious Hypernym Classification onWordNet-TREC dataSnow et al (2005) evaluated binary classifi-cation of noun-phrase pairs as hypernyms ornon-hypernyms.
When training and testing onWordNet-labeled pairs from TREC sentences,they report classifier Max F of 0.348, using de-pendency path features and logistic regression.
Tojustify our choice of an SVM for classification, wereplicated their work.
Snow et al provided us withtheir data.
With our SVM we achieved a Max F of0.453, 30% higher than they reported.6.2 Extending Snow et al?s WordNet-TRECBinary Classification to N ClassesSnow et al select pairs that are ?Known Hyper-nyms?
(the first sense of the first word is a hy-52Feature DescriptionLevenshtein Distance # character insertions/deletions/substitutions to change query ?
to query ?
(Levenshtein, 1966).Word Overlap Percent # words the two queries have in common, divided by num.
words in the longer query.Possible Stem 1 if the two segments stem to the same root using the Porter stemmer.Substring Containment 1 if the first segment is a substring of the second.Is URL 1 if either segment matches a handmade URL regexp.Query Pair Frequency # times the pair was seen in the entire unlabeled corpus of query pairs.Log Likelihood Ratio The Log Likelihood Ratio described in Section 3.2.1 Formula 3.2.1Dice and Jaccard Coefficients Measures of the similarity of substitutes for and by the two phrases.Table 2: Syntactic and statistical features over pairs of phrases.ponym of the first sense of the second and bothhave no more than one tagged sense in the Browncorpus) and ?Known Non-Hypernyms?
(no senseof the first word is a hyponym of any sense of thesecond).
We wished to test whether making theclasses less cleanly separable would affect the re-sults, and also whether we could use these featuresfor n-way classification.From the same TREC corpus we extractedknown synonym, known hyponym, known coordi-nate, known meronym, and known holonym pairs.Each of these classes is defined analogously to theknown hypernym class; we selected these six rela-tionships because they are the six most common.A pair is labeled known no-relationship if no senseof the first word has any relationship to any senseof the second word.
The class distribution was se-lected to match as closely as possible that observedin query logs.
We labeled 50,000 pairs total.Results are shown in Table 4(a).
Although AUCis fairly high for all classes, MaxF is low for allbut two.
MaxF has degraded quite a bit for hyper-nyms from Table 3.
Removing all instances excepthypernym and no relationship brings MaxF up to0.45, suggesting that the additional classes make itharder to separate hypernyms.Metaclassifier accuracy is very good, but this isdue to high recall of no relationship and coordi-nate pairs: more than 80% of instances with somerelationship are predicted to be coordinates, andmost of the rest are predicted no relationship.
Itseems that we are only distinguishing between novs.
some relationship.The size of the no relationship class may be bi-asing the results.
We removed those instances, butperformance of the n-class classifier did not im-prove (Table 4(b)).
MaxF of binary classifiers didimprove, even though AUC is much worse.6.3 N-Class Classification of Query PairsWe now use query pairs rather than TREC pairs.6.3.1 Classification Using Only DependencyPathsWe first limit features to dependency paths inorder to compare to the prior results.
Dependencypaths cannot be obtained for all query phrase pairs,since the two phrases must appear in the same sen-tence together.
We used only the pairs for whichwe could get path features, about 32% of the total.Table 5(a) shows results of binary classificationand metaclassification on those instances using de-pendency path features only.
We can see that de-pendency paths do not perform very well on theirown: most instances are assigned to the ?coordi-nate?
class that comprises a plurality of instances.A comparison of Tables 5(a) and 4(a) suggeststhat classifying query substitution pairs is harderthan classifying TREC phrases.Table 5(b) shows the results of binary clas-sification and metaclassification on the same in-stances using all features.
Using all features im-proves performance dramatically on each individ-ual binary classifier as well as the metaclassifier.6.3.2 Classification on All Query Pairs UsingAll FeaturesWe now expand to all of our hand-labeled pairs.Table 6(a) shows results of binary and meta classi-fication; Figure 1 shows precision-recall curves for10 binary classifiers (excluding URLs).
Our clas-sifier does quite well on every class but hypernymand hyponym.
These two make up a very smallpercentage of the data, so it is not surprising thatperformance would be so poor.The metaclassifier achieved 71% accuracy.
Thisis significantly better than random or majority-class baselines, and close to our 78% interanno-tator agreement.
Thresholding the metaclassifierto pairs with greater than .5 max class probability(68% of instances) gives 85% accuracy.Next we wish to see how much of the perfor-mance can be maintained without using the com-53binary n-way dataclass maxF AUC prec rec %no rel .980 .986 .979 .985 80.0synonym .028 .856 0 0 0.3hypernym .185 .888 .512 .019 2.1hyponym .193 .890 .462 .016 2.1coordinate .808 .971 .714 .931 14.8meronym .158 .905 .615 .050 0.3holonym .120 .883 .909 .062 0.3metaclassifier accuracy .927(a) All seven WordNet classes.
The high accuracy ismostly due to high recall of no rel and coordinate classes.binary n-way datamaxF AUC prec rec %?
?
?
?
0.086 .683 0 0 1.7.337 .708 .563 .077 10.6.341 .720 .527 .080 10.6.857 .737 .757 .986 74.1.251 .777 .500 .068 1.5.277 .767 .522 .075 1.5?
.749(b) Removing no relationship instancesimproves MaxF and recall of all classes,but performance is generally worse.Table 4: Performance of 7 binary classifier and metaclassifiers on phrase-pairs cooccuring in TREC data labeled with WordNetclasses, using minipar dependency features.
These features do not seem to be adequate for distinguishing classes other thancoordinate and no-relationship.binary n-wayclass maxf auc prec recno rel .281 .611 .067 .006synonym .269 .656 .293 .167hypernym .140 .626 0 0hyponym .121 .610 0 0coordinate .506 .760 .303 .888spelling .288 .677 .121 .022stemmed .571 .834 .769 .260URL .742 .919 .767 .691generalization .082 .547 0 0specification .085 .528 0 0other .393 .681 .384 .364metaclassifier accuracy .385(a) Dependency tree paths only.binary n-way datamaxf auc prec rec % % full.602 .883 .639 .497 10.6 3.5.477 .851 .571 .278 4.5 1.5.167 .686 .125 .017 3.7 1.2.136 .660 0 0 3.7 1.2.747 .935 .624 .862 21.0 6.9.814 .970 .703 .916 11.0 3.6.781 .972 .788 .675 4.8 1.61 1 1 1 16.2 5.3.490 .883 .489 .393 3.5 1.1.584 .854 .600 .589 3.5 1.1.641 .895 .603 .661 17.5 5.7?
.692 ?
(b) All features.Table 5: Binary and metaclassifier performance on the 32% of hand-labeled instances with dependency path features.
Addingall our features significantly improves performance over just using dependency paths.putationally expensive syntactic parsing of depen-dency paths.
To estimate the marginal gain of theother features over the dependency paths, we ex-cluded the latter features and retrained our clas-sifiers.
Results are shown in Table 6(b).
Eventhough binary and meta-classifier performance de-creases on all classes but generalizations and spec-ifications, much of the performance is maintained.Because URL changes are easily identifiable bythe IsURL feature, we removed those instancesand retrained the classifiers.
Results are shown inTable 6(c).
Although overall accuracy is worse,individual class performance is still high, allow-ing us to conclude our results are not only due tothe ease of classifying URLs.We generated a learning curve by randomlysampling instances, training the binary classifierson that subset, and training the metaclassifier onthe results of the binary classifiers.
The curve isshown in Figure 2.
With 10% of the instances, wehave a metaclassifier accuracy of 59%; with 100%of the data, accuracy is 71%.
Accuracy shows nosign of falling off with more instances.6.4 Training on WordNet-Labeled Pairs OnlyFigure 2 implies that more labeled instances willlead to greater accuracy.
However, manually la-beled instances are generally expensive to obtain.Here we look to other sources of labeled instancesfor additional training pairs.6.4.1 Training and Testing on WordNetWe trained and tested five classifiers using 10-fold cross validation on our set of WordNet-labeled query segment pairs.
Results for each classare shown in Table 7.
We seem to have regressedto predicting no vs. some relationship.Because these results are not as good as thehuman-labeled results, we believe that some of ourperformance must be due to peculiarities of ourdata.
That is not unexpected: since words that ap-pear in WordNet are very common, features aremuch noisier than features associated with queryentities that are often structured within web pages.54binary n-wayclass maxf auc prec recno rel .531 .878 .616 .643synonym .355 .820 .506 .212hypernym .173 .821 .100 .020hyponym .173 .797 .059 .010coordinate .635 .921 .590 .703spelling .778 .960 .625 .904stemmed .703 .973 .786 .589URL 1 1 1 1generalization .565 .916 .575 .483specification .661 .926 .652 .506other .539 .898 .575 .483metaclassifier accuracy .714(a) All features.binary n-way datamaxf auc prec rec %.466 .764 .549 .482 10.4.351 .745 .493 .178 4.2.133 .728 0 0 2.0.163 .733 0 0 2.0.539 .832 .565 .732 13.9.723 .917 .628 .902 14.9.656 .964 .797 .583 3.41 1 1 1 29.8.492 .852 .604 .604 4.8.578 .869 .670 .644 4.7.436 .790 .550 .444 9.8?
.714(b) Dependency path features removed.binary n-waymaxf auc prec rec.512 .808 .502 .486.350 .759 .478 .212.156 .710 .250 .020.187 .739 .125 .020.634 .885 .587 .706.774 .939 .617 .906.717 .967 .802 .601?
?
?
?.581 .885 .598 .634.665 .906 .657 .468.529 .847 .559 .469?
.587(c) URL class removed.Table 6: Binary and metaclassifier performance on all classes and all hand-labeled instances.
Table (a) provides a benchmarkfor 10-class classification over highly substitutable query phrases.
Table (b) shows that a lot of our performance can be achievedwithout computationally-expensive parsing.binary meta dataclass maxf auc prec rec %no rel .758 .719 .660 .882 57.8synonym .431 .901 .617 .199 2.4hypernym .284 .803 .367 .061 1.8hyponym .212 .804 .415 .056 1.6coordinate .588 .713 .615 .369 35.5other .206 .739 .375 .019 0.8metaclassifier accuracy .648Table 7: Binary and metaclassifier performance on WordNet-labeled instances with all features.binary meta dataclass maxf auc prec rec %no rel .525 .671 .485 .354 31.9synonym .381 .671 .684 .125 13.0hypernym .211 .605 0 0 6.2hyponym .125 .501 0 0 6.2coordinate .623 .628 .485 .844 42.6metaclassifier accuracy .490Table 8: Training on WordNet-labeled pairs and testing onhand-labeled pairs.
Classifiers trained on WordNet do notgeneralize well.6.4.2 Training on WordNet, Testing onWordNet and Hand-Labeled PairsWe took the five classes for which human andWordNet definitions agreed (synonyms, coordi-nates, hypernyms, hyponyms, and no relationship)and trained classifiers on all WordNet-labeled in-stances.
We tested the classifiers on human-labeled instances from just those five classes.
Re-sults are shown in Table 8.
Performance wasnot very good, reinforcing the idea that while ourfeatures can distinguish between query segments,they cannot distinguish between common words.0.560.580.60.620.640.660.680.70.720  500  1000  1500  2000  2500  3000  3500  4000  4500  5000MetaclassifieraccuracyNumber of query pairsFigure 2: Meta-classifier accuracy as a function of number oflabeled instances for training.7 DiscussionAlmost all high-weighted features are eitherHTML paths or query log features; these are theones that are easiest to obtain.
Many of thehighest-weight HTML tree features are symmet-ric, e.g.
both words appear in cells of the same ta-ble, or as items in the same list.
Here we note aselection of the more interesting predictors.synonym ?
?X or Y?
expressed as a dependencypath was a high-weight feature.hyper/hyponym ?
?Y and other X?
as a depen-dency path has highest weight.
An interestingfeature is X in a table cell and Y appearing intext outside but nearby the table.sibling ?many symmetric HTML features.
?X tothe Y?
as in ?80s to the 90s?.
?X and Y?, ?X,Y, and Z?
highly-weighted minipar paths.general/specialization ?the top three featuresare substring containment, word subset dif-ference count, and prefix overlap.spelling change ?many negative features, indi-5500.20.40.60.810  0.2  0.4  0.6  0.8  1PrecisionRecallF=0.531F=0.634F=0.354F=0.172F=0.173no relationshipsiblingsynonymhyponymhypernym00.20.40.60.810  0.2  0.4  0.6  0.8  1PrecisionRecallF=0.777F=0.538F=0.702F=0.565F=0.661spelling changerelated in some other waystemmed formgeneralizationspecificationFigure 1: Precision-recall curves for 10 binary classifiers on all hand-labeled instances with all features.cating that two words that cooccur in a webpage are not likely to be spelling differences.other ?many symmetric HTML features.
Twowords emphasized in the same way (e.g.
bothbolded) may indicate some relationship.none ?many asymmetric HTML features, e.g.one word in a blockquote, the other boldedin a different paragraph.
Dice coefficient is agood negative features.8 ConclusionWe have provided the first benchmark for n-class semantic classification of highly substi-tutable query phrases.
There is much room for im-provement, and we expect that this baseline willbe surpassed.AcknowledgmentsThanks to Chris Manning and Omid Madani forhelpful comments, to Omid Madani for providingthe classification code, to Rion Snow for providingthe hypernym data, and to our labelers.This work was supported in part by the CIIRand in part by the Defense Advanced ResearchProjects Agency (DARPA) under contract numberHR001-06-C-0023.
Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect those of the sponsor.ReferencesPeter G. Anick.
2003.
Using terminological feedback forweb search refinement: a log-based study.
In SIGIR 2003,pages 88?95.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.The pascal recognising textual entailment challenge.
InPASCAL Challenges Workshop on Recognising TextualEntailment.Ted E. Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguistics,19(1):61?74.Marti A. Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of Coling 1992,pages 539?545.Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substitutions.
In 15thInternational World Wide Web Conference (WWW-2006),Edinburgh.Sathiya Keerthi and Dennis DeCoste.
2005.
A modified fi-nite newton method for fast solution of large scale linearsvms.
Journal of Machine Learning Research, 6:341?361,March.Lillian Lee.
1999.
Measures of distributional similarity.
In37th Annual Meeting of the Association for ComputationalLinguistics, pages 25?32.V.
I. Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
Cyberneticsand Control Theory, 10(8):707?710.
Original in DokladyAkademii Nauk SSSR 163(4): 845?848 (1965).Dekang Lin.
1998.
Dependency-based evaluation of mini-par.
In Workshop on the Evaluation of Parsing Systems.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38(11):39?41.J.
Platt.
2000.
Probabilistic outputs for support vector ma-chines and comparison to regularized likelihood methods.pages 61?74.Soo Young Rieh and Hong Iris Xie.
2001.
Patterns and se-quences of multiple query reformulations in web search-ing: A preliminary study.
In Proceedings of the 64th An-nual Meeting of the American Society for Information Sci-ence and Technology Vol.
38, pages 246?255.Rion Snow, Dan Jurafsky, and Andrew Y. Ng.
2005.
Learn-ing syntactic patterns for automatic hypernym discovery.In Proceedings of the Nineteenth Annual Conference onNeural Information Processing Systems (NIPS 2005).Egidio Terra and Charles L. A. Clarke.
2004.
Scoring miss-ing terms in information retrieval tasks.
In CIKM 2004,pages 50?58.P.D Turney, M.L.
Littman, J. Bigham, and V. Shnayder, 2003.Recent Advances in Natural Language Processing III: Se-lected Papers from RANLP 2003, chapter Combining in-dependent modules in lexical multiple-choice problems,pages 101?110.
John Benjamins.56
