Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 56?64,October 29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsFrom Visualisation to Hypothesis Constructionfor Second Language AcquisitionShervin MalmasiCentre for Language TechnologyMacquarie UniversitySydney, NSW, Australiashervin.malmasi@mq.edu.auMark DrasCentre for Language TechnologyMacquarie UniversitySydney, NSW, Australiamark.dras@mq.edu.auAbstractOne research goal in Second Language Acqui-sition (SLA) is to formulate and test hypothe-ses about errors and the environments in whichthey are made, a process which often involvessubstantial effort; large amounts of data andcomputational visualisation techniques promisehelp here.
In this paper we have defined a newtask for finding contexts for errors that varywith the native language of the speaker that arepotentially useful for SLA research.
We pro-pose four models for approaching this task, andfind that one based only on error-feature co-occurrence and another based on determiningmaximum weight cliques in a feature associ-ation graph discover strongly distinguishingcontexts, with an apparent trade-off betweenfalse positives and very specific contexts.1 IntroductionSLA researchers are interested in a wide variety of as-pects of humans learning a new language (L2) differentfrom their native one (L1): cognitive issues and devel-opmental sequences for learners Pienemann (2005), so-ciocultural factors (Lantolf, 2001), and so on.
One long-standing question, dating back to at least Lado (1957),is expressed by Ortega (2009) in the following way:?What is the role played by first language in L2 develop-ment, vis-`a-vis the role of other universal developmentforces?
?An example of SLA research that looks at this ques-tion is the study of Di?ez-Bedmar and Papp (2008), com-paring Chinese and Spanish learners of English withrespect to the English article system (a, an, the) usingcorpora of essays by native and non-native speakersof English (Granger, 2011).
Drawing on the 175 non-native texts, they take a particular theoretical analysis(the so-called Bickerton semantic wheel), use the simpleWordsmith tools designed to extract data for lexicogra-phers to identify errors in a semi-automatic way, andevaluate whether Chinese and Spanish L1 speakers dobehave differently via hypothesis testing (ANOVA, chi-square and z-tests, in their case).
They conclude thatChinese and Spanish do have characteristic differences,with patterns of zero article and definite article use dif-fering according to semantic context.
Such studies aretypically carried out on relatively small datasets, anduse fairly elementary tools.
Sources such as Ellis (2008)and Ortega (2009) give good overviews of such studiesand of SLA research in general.A goal of this paper is to investigate a particular wayin which Natural Language Processing (NLP) can use-fully contribute to SLA.
In terms of existing work, thesubfield of Native Language Identification (NLI) hasbeen quite active recently, which looks at predictingthe L1 of writers writing in a common L2 within aclassification task framework; see for example the re-cent NLI shared task with 29 entrants (Tetreault et al.,2013).1From within linguistics, there has been muchinterest in how data-driven approaches can contribute toSLA.
Granger (2011) discusses a body of work basedon the the methodology of carrying out corpus-basedapproaches to SLA with a focus on NLP tools; Jarvisand Crossley (2012) in an edited collection present re-cent work by linguists who extend the corpus-basedsetup by using a text classification approach, looking atwhat feature selection might say for SLA.
From withinNLP, Swanson and Charniak (2013) and Swanson andCharniak (2014) take a data-driven approach to SLAinvestigations much in the spirit of this work.One particular approach to finding aspects of textscharacteristic of their L1s that has motivated the presentwork is described in Yannakoudakis et al.
(2012), thegoal of which is to develop visualisation tools for SLAresearchers.
They present graphs of the relationshipsbetween errors and their contexts, such that SLA re-searchers can navigate through the graphs to find con-texts for particular errors that can lead to hypotheseslike that of Di?ez-Bedmar and Papp (2008) above.
In thispaper, we look at approaches to finding such hypothesiscandidates automatically in the context of L1?L2 inter-action by analysing the graphs used in the visualisations1http://sites.google.com/site/nlisharedtask2013/56of Yannakoudakis et al.
(2012).
Specifically, we do thefollowing:?
We propose a new task that is more directly ori-ented to SLA research than NLI has been for themost part, with the goal of identifying error-relatedcontexts that are characteristic of L1s.?
We evaluate a number of models for finding suchcontexts, ranging from a simple baseline to treat-ing the problem as a graph-theoretic maximumweighted clique one.?
We examine the results of some of the models tosee how the task and the models might contributeto SLA research.Because we draw heavily on the work of Yan-nakoudakis et al.
(2012), we first review relevant aspectsof that work in ?2; we then present our task definitionand experimental setup in ?3; we give results along witha discussion in ?4; we follow with some more detail onrelated work in ?5; and we conclude in ?6.2 Developing Hypotheses: AVisualisation ToolThe context of the Yannakoudakis et al.
(2012) workis automated grading of English as a Second or OtherLanguage (ESOL) exam scripts, as described in Briscoeet al.
(2010).
The automated grading takes a classifi-cation approach, using a binary discriminative learner,with useful features including lexical and part-of-speech(PoS) n-grams.The publicly available dataset on which the work wascarried out consists of texts from the First Certificate inEnglish (FCE) exam, aimed at upper-intermediate stu-dents of English across various L1s, and was presentedin Yannakoudakis et al.
(2011).
This FCE corpus2con-sists of a subset of 1244 texts of the Cambridge LearnerCorpus,3and is manually annotated with errors and theircorrections, as well as a classification according to anerror typology, as in Figure 1.Yannakoudakis et al.
(2012) present their EnglishProfile (EP) visualiser as a way to ?visually analyse aswell as perform a linguistic interpretation of discrimi-native features that characterise learner English?, usingthe features of this essay classification task.
They de-fine a measure of co-occurrence of features, amongthemselves and with errors, as a core part of theiranalysis.
Given the set of all sentences in the corpusS = {s1, s2, .
.
.
, s|S|} and the set of all features F ={f1, f2, .
.
.
, f|F |}, a feature fi?
F is associated witha feature fj?
F (i 6= j, 1 ?
i, j ?
M ) according tothe score given in Equation (1), for sk?
S, 1 ?
k ?
N2http://ilexir.co.uk/applications/ep-visualiser/3http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item364603/and exists() a binary function returning true if the inputfeature occurs in sk.scoreff(fj, fi) =?|S|k=1exists(fj, fi, sk)?|S|k=1exists(fi, sk)(1)They mention an analogous measure for feature-errorco-occurrence; we assume given the set of all errorsE = {e1, e2, .
.
.
, e|E|} that this is defined as follows:scoreef(fj, ei) =?|S|k=1exists(fj, ei, sk)?|S|k=1exists(ei, sk)(2)A graph is defined with features and errors as vertices;an edge between features (resp.
features and errors) isestablished if scoreff() (resp.
scoreef) is within someuser-defined range.
This graph of feature?feature (resp.feature?error) relationships is then presented visually.The paper then presents a case study of how the EP vi-sualiser can be used to assist SLA researchers.
The casestudy starts by noting that RG_JJ_NN1 is the 18th mostdiscriminative negative feature from the essay classi-fier; then, further inspecting the graph of discriminativefeatures, that it?s linked to JJ_NN1_II and VBZ_RG.Then, looking at feature-error relations, it investigatesan association with error MD (missing determiner), andpresents some examples that match the features (e.g.Unix is very powerful system but there is one thingagainst it), along with a discussion of relationships tovarious L1s.
It is this process of finding interesting fea-tures and linking them to particular errors and L1s thatwe present an approach to automating in this paper.3 Task Definition & Experimental SetupAt a general level, our goal is to find which kinds ofconstructions (in a loose sense) centred around errorsare particularly characteristic of various L1s.The specific task we define for this paper, then, isto select a set of features (in the terminology of Yan-nakoudakis et al.
(2012))?which we refer to as theERROR CONTEXT?that, when combined with the er-ror, show a strong association with L1, in a mannerwe describe below.
So, for example, this may involvefinding that an MD error in the context of RG_JJ_NN1,JJ_NN1_II and VBZ_RG shows a strong associationwith L1.
We investigate a number of models for thisselection process: the task then is the identification ofwhich models produce poor error contexts (which willnot rank highly in hypothesis testing) and which pro-duce good ones (potentially worth considering by anSLA researcher).
Below we discuss the data we use,the measure of association for an error and its context,the set of errors chosen, and the models for selectingcontext.3.1 DataThe corpus we use for evaluating the models for our taskis derived from the FCE corpus of Yannakoudakis et al.57Verb Agreement <p>Some people <ns type="AGV"><i>says</i><c>say</c></ns> ...</p>Incorrect Verb <p>The day I <ns type="IV"><i>shaked</i><c>shook</c></ns> theirInflectionhands,...</p>Missing Determiner <p>I am <ns type="MD"><c>a</c></ns> really good singer.</p>Figure 1: FCE corpus examples.
Error types indicated by <ns type>...</ns>; errors indicated by <i>...</i>;corrections indicated by <c>...</c>.language sizeChinese CHI 66French FRE 146German GER 69Italian ITA 76Japanese JAP 81Korean KOR 86Spanish SPA 200Turkish TUR 75Table 1: FCESUB, broken down by language(2012).
The full FCE corpus consists of 1244 scriptsover 16 languages; script counts range from 2 (Dutch)to 200 (Spanish).The features used by Yannakoudakis et al.
(2012)were derived from their essay classification task.
As weare interested in associations with L1, we instead usefeatures from a system submitted to the NLI shared task(Anonymous, 2013), which was applied to a dataset ofTest of English as a Foreign Language (TOEFL) scripts:the task and its designated corpus are described in thetask overview paper (Tetreault et al., 2013).
In this workwe use a system trained on the TOEFL11 corpus con-sisting of texts written in English from speakers of 11different L1s, with 1100 essays per L1 and balancedacross topic.
We only use PoS n-grams (n = 1, 2, 3) asfeatures in this work.
Note that we use the terminologyof Yannakoudakis et al.
(2012) here: what had theirorigin as features in the essay classification task are stillreferred to as features in the visualisation tool, althoughthe task carried out there is not a classification one.
Sim-ilarly, we refer to our PoS n-grams as features, althoughwe are not classifying errors using these features andso are not carrying out feature selection for the typicalpurpose of optimising classification performance.For this, as did Yannakoudakis et al.
(2012), we usethe RASP parser (Briscoe et al., 2006) for tagging; thetags are consequently from the CLAWS2 tagset,4whichare more fine-grained in terms of linguistic analysis thanthe more frequently used Penn Treebank tags.For our task, we then used the subset of the FCE cor-pus where the languages overlapped with the TOEFL11corpus: we refer to this as FCESUB.
This gives 799scripts over 8 languages, distributed as in Table 1; apositive byproduct is that the L1s are more similar insize than the full FCE corpus.4http://ucrel.lancs.ac.uk/claws2tags.htmllanguage meanCHI 0.885790FRE 0.460894GER 0.366587ITA 0.581401JAP 1.058159KOR 1.067211SPA 0.472253TUR 1.014129F-stat 18.031sig.
<0.001Table 2: ANOVA results giving mean score (numberof sentences with MD error per 10 sentences) for eachlanguage, the ANOVA F-statistic, and significance value3.2 Association MeasureWe noted in ?1 that SLA studies such as Di?ez-Bedmarand Papp (2008) use standard hypothesis testing tech-niques.
We take this as a starting point.
We could, forexample, evaluate whether a particular raw error (thatis, without a feature context) is strongly associated withL1s by using a single factor ANOVA test.5The indepen-dent variable would be the L1.
The dependent variablecould be one of a number of alternatives; we choose thenumber of sentences with a particular error per 10 sen-tences.6To illustrate, we give the ANOVA results fromFCESUB for the MD error in Table 2.
The ANOVAcalculation is based on an F-statistic which comparesvariance between treatments against variance withintreatments; this is compared against critical values forthe F-statistic to determine statistical significance.
Theexpected value of the F-statistic under the null hypoth-esis is 1, with values above 1 increasingly inconsistentwith the null hypothesis.
The data in Table 2 showsthat the MD error does vary significantly with L1; apost-hoc Tukey HSD test lets us identify which specificlanguages exhibit this difference and shows that, forexample (and as can be observed in the means), GermanL1 speakers are significantly different from Korean L1speakers in the occurrence of MD errors.For our task we are not interested in significance perse.
Rather, we are interested in whether we can find oc-currences of errors plus contexts that are more stronglyassociated with, or that vary across, L1s, e.g.
that an5See, e.g., Jackson (2009).6We note that the texts differ significantly in length by L1,so it would not be suitable to normalise as occurrences perdocument.58type name F-stat p-val NDJ Wrong Derived 3.27 .002 332AdjectiveDN Wrong Derived 0.70 .671 294NounMD Missing Determiner 18.03 .000 1702MT Missing Preposition 2.81 .007 985UD Unnecessary Determiner 1.20 .301 807UT Unnecessary Preposition 0.26 .968 689UV Unnecessary Verb 0.78 .606 317Table 3: Error types chosen for evaluation, including F-statistic, ANOVA p-value and corpus count of sentencescontaining error.MD error in the context of RG_JJ_NN1, JJ_NN1_IIand VBZ_RG is more strongly associated with L1s; andwe are also interested in which of our proposed methodsfor identifying an error?s feature context does this best.For this purpose, then, we use just the F-statistic fromthe ANOVA test, this time with the dependent variableas the ratio of occurrences of error plus error contextper 10 sentences: a higher F-statistic shows a strongerassociation with L1s.7We also consider the ?2-statistic from Pearson?s chi-squared test, noting that it is also used in SLA hypothe-sis testing and that it was additionally found by Swansonand Charniak (2013) to be good at distinguishing inter-esting features in their related task (see ?5 for moredetail).
The F-statistic and ?2-statistic are closely re-lated: a random variate of the F-distribution is the ratioof two chi-squared variates scaled by their degrees offreedom.
A difference is that ?2compares observedversus expected counts rather than proportions: to takeaccount of the differing text lengths, our observed fre-quency is the number of sentences with error and errorcontext per L1; our expected frequency is the total num-ber of sentences with that error and error context scaledaccording to the proportion of sentences labelled withthat L1 relative to the corpus as a whole.3.3 Errors ChosenFrom the 74 error types in the FCE corpus, we select asubset to evaluate our models.
In addition to the MD er-ror used in the case study of Yannakoudakis et al.
(2012),we choose a subset which has a range of F-statistic val-ues as described above: some show very similar patternsacross L1s (i.e.
with low F-statistic), such as DN WrongDerived Noun (e.g.
hot vs heat); others do vary signif-icantly with L1, such as DJ Wrong Derived Adjective(e.g.
reasonally vs reasonable).
Having errors witha range of F-statistic values lets us evaluate whetherfinding good error contexts works only for strongly L1-associated errors, weakly L1-associated errors, or across7As we are only using the F-statistic to evaluate ranks, wedo not need a multiple comparison adjustment such as theBonferroni correction: this would only apply for comparisonsto a significance threshold, and in any case the Bonferroni ismonotonic and does not affect rankings.the spectrum.
Our subset is in Table 3, along with theirF-statistic, ANOVA p-value and counts in FCESUB.3.4 ModelsWe propose four models for choosing error contexts.These models rank error contexts; we evaluate theranked error contexts by F-statistic and ?2-statistic val-ues (?3.2).ERRORCOOCC In this model we rank features byerror-feature co-occurrence scores given by Equation(2).
The L1 is not taken into account, so this will justreturn common features which may be equally stronglyassociated with errors across all L1s.
We look at resultsfor when k = 1..3 features are chosen.
For k = 2, 3,we add the individual error-feature scores together forthe ranking.8It may be the case that interesting resultscould be obtained for k > 3, but we only look at thek = 1..3 in this preliminary work to see if there areany discernible trends suggesting that larger values of kcould help.L1ASSOC Here we use features that are strongly as-sociated with the L1s from the TOEFL11 corpus andNLI shared task.
Specifically, we rank features by theirInformation Gain with respect to L1s as in the process offeature selection from the shared task.9The relationshipbetween errors and features (in the form of error-featureco-occurrence scores) is not taken into account here.Again, we look at results for when k = 1..3 featuresare chosen, and for k = 2, 3, we add the individualerror-feature scores together for the ranking.MAXWEIGHTCLIQUE Both of the preceding mod-els look only at one factor that might be relevant: error-feature scores (finding features that are related to theerrors) and a measure of the association of features withL1s; but there is no link between them, and interactionof features is not taken into account.
In Yannakoudakiset al.
(2012), the visualiser provides to the SLA re-searcher a graph showing the relatedness of features,based on Equation (1), and the SLA researcher com-bines this with error-feature scores to find interestingcandidate error contexts; we create a similar graph andaim to imitate the process by incorporating error-featurescores as follows.We define a weighted undirected graph G = (V,A)such that V is the set of features used in the abovemodels (i.e.
PoS n-grams from ERRORCOOCC); A isdefined such that (vi, vj) ?
A for vertices vi, vj?
Vif 0.8 ?
scoreff(vi, vj) ?
1.0 where scoreff() is asdefined as in Equation (1).10Given our set of errorsE defined at Equation (2) above, the weight of a ver-tex viis defined as scoreef(vi, ej) for some ej?
E.8For k = 2 the combinations were made from the top 100features from k = 1, and for k = 3 from the top 50.9We recalculated this over the subset of eight languagesused in this paper.10We choose this threshold value as it is the one used in thegraph definition of Yannakoudakis et al.
(2012).59model rERRORCOOCC 0.95L1ASSOC 0.97MAXWEIGHTCLIQUE 0.95MAXWEIGHTCLIQUE-L1 0.92Table 4: Average correlation coefficient r between F-statistic and ?2-statistic for each modelGiven this graph, it is possible to characterise the find-ing of related features with strong aggregate associationswith errors as an instance of the MAXIMUM WEIGHTCLIQUE PROBLEM (Bomze et al., 1999).
As the namesuggests, this finds a clique of maximum weight, herethe strongest aggregate feature?error association.
Whilethis is an NP-hard problem, there are quite efficient algo-rithms for solving it; we use one proposed by?Osterg?ard(1999).11MAXWEIGHTCLIQUE-L1 We also look at a vari-ant of MAXWEIGHTCLIQUE where we construct thegraphs based only on relationships among features fora particular L1.
That is, there will be eight weightedgraphs per error of interest.4 Results and Discussion4.1 Overall ResultsWe only present the F-statistic results here; the ?2-statistic showed very similar patterns.
The averagecorrelation between the two for each model shows thestrong similarity (Table 4).For the F-statistic results, presented in Table 5, wereport the highest F-statistic in the N -best list (N =1, 5, 20, 50) for each model.
For models ERRORCOOCCand L1ASSOC we report the highest F-statistic for eachvalue of k (k = 1, 2, 3).
The number of occurrencesof the error context with the highest F-statistic is givenin parentheses after the F-statistic; the highest valuefor each N is in bold.
For MAXWEIGHTCLIQUE-L1,we also note the language of the graph from which thehighest score was derived.We note by comparing Table 5 with Table 3 that foreach error type except for MD, it is possible to findan error context that is more strongly associated withL1s than is the raw error type alone.
For MD this isnot surprising, as its frequency of occurrence is verystrongly linked to the L1, as noted in Table 2 and ?3.2.12(For the error type MT also, no model produces an errorcontext more strongly associated with the L1 for thesingle best choice where N = 1, but does for largervalues of N .
)11Code for the used wclique is available at http://tcs.legacy.ics.tkk.fi/?pat/wclique.html.12The fact that determiner errors are very widely studied interms of analysing cross-linguistic influence suggests a broadconsensus that they vary strongly with L1.
In addition to Di?ez-Bedmar and Papp (2008), a sample of other studies includesParrish (1987), Young (1996) and Ionin and Montrul (2010).With respect to the individual models, the simple ER-RORCOOCC scores highly, giving the best result abouthalf the time, and the best results can occur for anyof k = 1, 2, 3.
The number of instances returned foreach error plus error context is larger than for the othermodels as well, which is not surprising as the modelaims to find contexts strongly associated with the errorsrather than with L1s.
However, these are then likely tobe features that are fairly common across L1s; we lookat some examples in ?4.2.L1ASSOC performs fairly poorly on our evaluationmeasure, although in many cases it does find an errorcontext more strongly associated with the L1 than justthe raw error type.
Counts are also lower.
Also, for thismodel, k = 2, 3 are always worse than k = 1: bringingin a second context feature reduces the number of oc-currences to such an extent that the F-statistic can dropdramatically.
This is probably in part an artefact of thesize of the FCE corpus (and particularly our FCESUBsubcorpus): these features derived from the TOEFL11corpus just do not occur sufficiently often in our evalua-tion corpus (and in fact there are often large numbers ofzero occurrences for k = 2, 3).MAXWEIGHTCLIQUE also performs fairly poorly.However, in many cases it also finds an error contextmore strongly associated with L1 than the raw error typealone (DN, MT, UD, UT, UV), even if not always forN = 1, and it has intermediate counts of occurrences.MAXWEIGHTCLIQUE-L1 gives the best results inthe other half of the cases where ERRORCOOCC doesnot.
The error contexts that it finds, however, are veryspecific, often to a single language (as might be expectedby its definition) with very small numbers of counts.4.2 Some ExamplesWe look at some examples in Figure 2, to illustrate bothinteresting error contexts found and areas where themodels do a poor job.
In these sample sentences, onlyerrors of interest are retained and highlighted.The DJ error with context { JJ, NN1 } illustrates thetop result found under the ERRORCOOCC model forN = 20.
In the first sentence the model seems to find auseful pattern: the adjective that is at the centre of theerror occurs in the context of a singular noun.
On theother hand, the second sentence illustrates a problem:because the range of the context is the whole sentence,frequent features such as NN1 will occur a lot in otherparts of the sentence that have no apparent relation tothe actual error.
The ERRORCOOCC model is thus likelyto be picking up false positives by virtue of the relativelyhigh frequencies of its error contexts.The UV error with context { TO_VV0_II, NNL1,II, NN2, VV0_II } illustrates the top result foundunder the MAXWEIGHTCLIQUE-L1 model for N =5.
This is very specific, and its three instances onlyappear in Turkish.
But all three are similar errors fromdifferent documents, so it appears likely to be a genuinepattern, although the NN2 seems only to have a tenuous60errorNERRORCOOCCL1ASSOCMAXWEIGHTCLIQUEMAXWEIGHTCLIQUE-L1DJ12.78(274)/3.19(227)/2.95(158)1.59(31)/1.59(31)/0.81(6)0.99(15)3.08(2)[GER]53.60(268)/3.19(227)/3.02(148)2.19(12)/1.59(31)/0.81(6)1.74(41)3.24(2)[CHI]203.72(194)/3.33(163)/4.02(93)2.53(70)/1.59(31)/1.36(1)2.34(24)3.50(5)[ITA]503.72(194)/3.39(114)/4.02(93)2.58(107)/1.59(31)/1.59(31)2.48(18)3.84(3)[ITA]DN10.77(268)/1.63(185)/1.73(119)1.09(40)/1.09(40)/0.70(7)1.26(63)3.24(2)[CHI]51.80(191)/2.29(153)/2.54(142)1.25(5)/1.36(1)/1.36(1)1.26(63)3.24(2)[CHI]202.34(86)/2.69(144)/2.95(113)2.04(26)/1.36(1)/1.36(1)1.76(30)3.24(2)[CHI]502.86(61)/3.16(120)/2.95(113)3.89(4)/2.75(2)/2.75(2)3.41(18)4.27(10)[SPA]MD114.28(1319)/9.09(985)/6.38(753)5.83(198)/5.83(198)/0.54(2)3.07(297)4.05(91)[KOR]514.28(1310)/12.18(769)/6.75(582)8.20(268)/5.83(198)/1.93(3)5.83(198)5.83(198)[KOR]2014.41(850)/12.18(769)/6.82(593)8.20(268)/5.83(198)/2.60(36)5.83(198)5.83(198)[KOR]5014.41(850)/12.18(769)/7.99(483)8.36(831)/5.83(198)/5.83(198)5.83(198)6.47(110)[KOR]MT13.34(794)/3.00(666)/3.02(485)1.85(79)/1.85(79)/1.55(13)1.70(61)2.48(20)[CHI]53.34(794)/3.46(478)/3.37(378)2.54(101)/1.85(79)/1.55(13)2.14(64)4.47(3)[CHI]204.44(295)/3.64(375)/4.60(294)4.44(295)/3.11(25)/3.11(25)2.79(44)4.47(3)[CHI]504.50(277)/5.21(247)/4.72(215)4.44(295)/3.86(33)/3.11(25)4.54(74)4.61(3)[GER]UD10.69(679)/1.05(475)/2.08(334)1.45(62)/1.45(62)/0.73(10)0.64(47)1.54(20)[GER]51.70(405)/1.17(452)/2.08(334)1.59(26)/1.45(62)/1.36(1)1.45(62)3.54(9)[CHI]202.08(223)/2.11(360)/2.32(276)3.41(51)/1.45(62)/1.36(1)1.90(29)3.93(3)[ITA]503.27(112)/3.01(188)/2.33(198)3.41(51)/1.54(4)/1.54(4)2.85(66)4.06(3)[ITA]UT10.14(548)/0.45(414)/1.12(259)1.01(51)/1.01(51)/0.43(1)0.81(35)3.06(2)[GER]50.82(368)/1.16(321)/1.58(249)2.28(23)/1.36(1)/1.36(1)1.01(51)4.10(3)[TUR]201.51(351)/1.77(275)/1.89(225)2.91(51)/1.53(6)/1.36(1)2.58(45)4.10(3)[TUR]502.25(112)/2.66(201)/3.18(178)2.91(51)/1.53(6)/1.36(1)2.58(45)4.10(3)[TUR]UV10.88(260)/0.97(186)/1.18(119)1.06(15)/1.06(15)/1.29(2)1.49(28)2.53(2)[JAP]52.22(175)/2.21(162)/1.68(109)2.29(8)/1.29(2)/1.29(2)1.49(28)4.09(3)[TUR]202.25(125)/2.82(127)/3.13(96)3.22(8)/1.52(1)/1.52(1)2.38(15)4.09(3)[TUR]502.56(61)/3.01(101)/3.13(96)3.22(8)/1.52(1)/1.52(1)2.38(15)4.63(3)[CHI]Table5:Resultsforthechosenerrortypesunderthefourproposedmodels.AllerrortypesandmodelsreportthebestF-statisticfortheselectederrorcontextandfrequencywithinthetopN(N=1,5,20,50).ERRORCOOCCandL1ASSOCgivethebestscoreforthesetofkfeatures(k=1,2,3).MAXWEIGHTCLIQUE-L1alsonotesthelanguagegraphwiththebestresult.61error context example sentencesDJ JJ, NN1 Basically/RR ,/, I/PPIS1 helped/VVD them/PPHO2 liaise/VV0with/IW the/AT local/JJ police/NN and/CC get/VV0 some/DD<ns type="DJ"><i>electronical</i><c>electronic/JJ</c></ns> equipmen-t/NN1 that/CST they/PPHS2 needed/VVD.The/AT show/NN1 will/VM be/VB0 at/II the/AT Central/JJExhibition/NN1 Hall/NP1 and/CC it/PPH1 will/VM be/VB0<ns type="DJ"><i>opened</i><c>open/JJ</c></ns> until/ICS 7/MC.UV TO_VV0_II,NNL1, II, NN2,VV0_III/PPIS1 used/VMK to/TO <ns type="UV"><i>be</i></ns> play/VV0 in/II the/ATschool/NNL1 team/NN1 .
.
.
and/CC our/APP$ team/NN1 was/VBDZ one/MC1 of/IO the/ATbest/JJT basketball/NN1 teams/NN2 .
.
.DN XX, XX_VV0,VM_XX_VV0, NN1Never/RR the/AT less/DAR ,/, in/II summer/NNT1 we/PPIS2 can/VM n?t/XX resist/VV0such/DA <ns type="DN"><i>hot</i><c>heat/NN1</c></ns>!.
.
.
I/PPIS1 think/VV0 you/PPY should/VM have/VH0 a/AT1 <ns type="DN"><i>baby-parking</i><c>kindergarten/NP1</c></ns> ,/, in/II fact/NN1 a/AT1 certain/JJ num-ber/NN1 of/IO women/NN2 could/VM n?t/XX see/VV0 the/AT Festival/NN1 because/CSof/IO their/APP$ sons/NN2.MD VBZ_RG,RG_JJ_NN1The/AT first/MD and/CC most/RR important/JJ thing/NN1 is/VBZ that/RG modern/JJtechnology/NN1 has/VHZ made/VVN our/APP$ life/NN1 easier/JJR ,/, for/IF instance/NN1<ns type="MD"><c>the/AT</c></ns> rice/NN1 cooker/NN1 is/VBZ a/AT1 great/JJinvention/NN1 .
.
.Figure 2: Examples for sample error types and specific error contexts.
Error contexts are bolded.connection.The DN error with context { XX, XX_VV0,VM_XX_VV0, NN1 } illustrates the top result found un-der the MAXWEIGHTCLIQUE-L1 model for N = 50.A number of this reasonably sized set are similar to thefirst sentence, where the context appears interesting.
Inthis example, hot is used for heat; the other examplesof this type are from Spanish and Italian (similarly, e.g.,live for life), where the error seems to be connectedto words where the English derivational morphologyis not simply affixation.
However, there are some likethe second sentence, where (as for the DJ error) theerror context appears in a different clause, and likelyirrelevant.The MD error in the last row we examine because (amore complex version of) it was the focus of the casestudy in Yannakoudakis et al.
(2012), which from theexamples of that paper looked quite convincing as anerror context of relevance to SLA research.
However, itand the related examples of Yannakoudakis et al.
(2012)were not in the publicly available corpus,13and in factthere is only one example of this error and context in thewhole FCE corpus, illustrating the issue of data sparsity.Further, this example also illustrates the issue of taggingerror: that is tagged as RG (degree adverb) where itshould be CST.So as might be anticipated from the frequency num-bers in Table 5, the MAXWEIGHTCLIQUE-L1 modelproduces context that looks interesting from an SLA per-spective, but is relatively limited in scope; the ERROR-COOCC model produces a much larger set of candidates,and can successfully find error context such that theybehave differently with respect to the L1s accordingto the ANOVA F-statistic, but produces false positives.Overall, a recurring issue illustrated for all models by13We assume that the multiple examples come from thelarger CLC corpus.the examples is the proposal of error context far awayfrom any likely relevance to SLA.5 Related WorkWhile Native Language Identification (NLI) as a sub-field of NLP has seen much new work in the last fewyears ?
the papers from the shared task (Tetreault etal., 2013) provide a recent sample ?
the emphasis onoptimising classification task results, for example byusing classifier ensembles (Malmasi et al., 2013), ver-sus analysing features for relevance to other tasks hasvaried.
Below we discuss works which directly lookat how features might be related to language-learningtasks or SLA research.The seminal work of Koppel et al.
(2005) that pre-sented NLI as a classification task included, in additionto standard lexical and PoS n-gram features, errors madeby the writers; these errors were automatically identi-fied using Microsoft Word grammar checker.
Kochmar(2011) used the FCE corpus for NLI, including the man-ually annotated errors as features, and presented an anal-ysis of usefulness of features (including errors) withrespect to L1.Wong and Dras (2011) used syntactic features on thebasis of SLA theory that posits that L1 constructionsmay be reflected in some form of characteristic errors orpatterns in L2 constructions to some extent, or throughoveruse or avoidance of particular constructions in L2(Lado, 1957; Ellis, 2008); they did note distributionaldifferences of features related to L1.
Wong et al.
(2012)induced topic models over function words and PoS n-grams, where some of the topics appeared to reflect L1-specific characteristics.
These works, while interestedin the nature of the features, do not evaluate them exceptvia classification accuracy.Swanson and Charniak (2012) similarly explore us-ing syntax, where they propose a richer representation62for L1-specific constructions through Tree SubstitutionGrammar (TSG).
Swanson and Charniak (2013) sub-sequently examine both relevancy and redundancy offeatures through a number of metrics (including the?2-statistic used in this paper).
They then extend aBayesian induction model for TSG inference based ona supervised mixture of hierarchical grammars, in orderto extract a filtered set of more linguistically informedfeatures that could benefit both NLI and SLA research;an aim was to find relatively rare features that are nev-ertheless useful for L1 prediction.
Swanson and Char-niak (2014) continue on from this with a data-drivenapproach to inferring possible relationships between L1and L2 structures, again using TSGs.
Malmasi and Dras(2014c) also propose a method for identifying potentiallanguage transfer effects by using additional linguisticfeatures such as adaptor grammars and grammatical de-pendencies to analyse differences in learner language.This body of work thus shares some similarities with thepresent paper, but our focus is on errors rather than onthe distributional differences, and we look at error con-texts that may not constitute a TSG tree or grammaticaldependency.Coming from a linguistic perspective, the works inJarvis and Crossley (2012) use Linear DiscriminantAnalysis for classification of texts by L1, and identifyinteresting features by a stepwise feature selection pro-cess in the course of classification, rather than via themeasurement of their variability across L1s as here.More recently, several of these NLI techniques havebeen adapted and applied to languages other than En-glish, such as Arabic and Chinese (Malmasi and Dras,2014a; Malmasi and Dras, 2014b).6 ConclusionIn this paper, prompted by work on using computa-tional visualisation techniques to help SLA researchersform hypotheses about errors and the environments inwhich they are made, we have defined a new task forfinding interesting contexts for errors that vary withthe native language of the speaker.
We proposed fourmodels, ranging from one based on simple error-featureco-occurrence statistics to one based on the maximumweighted clique on an L1-specific feature associationgraph; these all managed to find contexts that were morestrongly associated with L1s than the raw errors alone,and produced (albeit with many false positives in thecase of the simple model) some error contexts that lookpotentially useful for SLA.This paper is largely intended to prompt more workon applying NLP techniques to SLA more broadly.
Assuch, there are many ways in which the work could befurther developed.
First, to get rid of obviously incor-rect cases, the size of the area over which the feature-feature and feature-error scores are calculated could berestricted, perhaps to the relevant clause or a certainwindow size.
Second, it may not be the case that theANOVA F-statistic or ?2are the best evaluation mea-sure: in medical work, for example, there is the notionof clinical significance, which takes effect size into ac-count and is often more relevant to the practitioner thanstatistical significance.
Similarly, the current featuresmay not be the most meaningful.
As part of this, an im-portant step would be to bring in SLA researchers, to as-sess proposed error contexts and look at what evaluationmeasures best relate to this.
The role of the present workwould then be to rule out models for producing errorcontexts (like L1ASSOC) that produce weaker results inhypothesis testing: it would thus be complementary tothe visualisation work from which it stems, guiding SLAresearchers away from unproductive areas of the spaceof possible hypotheses.
And third, the size of the corpusis (as always) an issue: as these error-annotated corporaare few and far between, a semi-supervised approachor one that in some way incorporated unannotated datawould be useful, perhaps using some of the extensiverecent work on error annotation.ReferencesImmanuel M. Bomze, Marco Budinich, Panos Parda-los, and Marcello Pelillo.
1999.
The MaximumClique Problem.
In D.-Z.
Du and P. M. Pardalos, edi-tors, Handbook of Combinatorial Optimization (supp.Vol.
A), pages 1?74.
Kluwer Academic, Dordrecht,Netherlands.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proc.
ofthe COLING/ACL Interactive Presentation Sessions,pages 77?80, Stroudsburg, PA, USA.Ted Briscoe, Ben Medlock, and ?istein Andersen.
2010.Automated Assessment of ESOL Free Text Exami-nations.
Technical Report TR-790, University ofCambridge, Computer Laboratory.Mar?
?a Bel?en Di?ez-Bedmar and Szilvia Papp.
2008.
Theuse of the English article system by Chinese and Span-ish learners.
Language and Computers, 66(1):147?176.Rod Ellis.
2008.
The Study of Second Language Acqui-sition, 2nd edition.
Oxford University Press, Oxford,UK.Sylviane Granger.
2011.
How to Use Foreign and Sec-ond Language Learner Corpora.
In Alison Mackeyand Susan M. Gass, editors, Research Methods inSecond Language Acquisition: A Practical Guide.Wiley-Blackwell.Tania Ionin and Silvina Montrul.
2010.
The role of L1transfer in the interpretation of articles with definiteplurals.
Language Learning, 60(4):877?925.Sherri L. Jackson.
2009.
Statistics: Plain and Simple.Wadsworth, Cengage Learning, Belmont, CA, US.Scott Jarvis and Scott Crossley, editors.
2012.
Ap-proaching Language Transfer Through Text Classi-fication: Explorations in the Detection-based Ap-proach.
Multilingual Matters, Bristol, UK.63Ekaterina Kochmar.
2011.
Identification of a writer?snative language by error analysis.
MPhil thesis, Uni-versity of Cambridge.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Automatically determining an anonymous author?snative language.
In Intelligence and Security In-formatics, volume 3495 of LNCS, pages 209?217.Springer-Verlag.Robert Lado.
1957.
Linguistics Across Cultures: Ap-plied Linguistics for Language Teachers.
Univ.
ofMichigan Press, Ann Arbor, MI, US.James P. Lantolf.
2001.
Sociocultural Theory andSecond Language Learning.
Oxford University Press,Oxford, UK.Shervin Malmasi and Mark Dras.
2014a.
Arabic Na-tive Language Identification.
In Proceedings of theArabic Natural Language Processing Workshop (co-located with EMNLP 2014), Doha, Qatar, October.Association for Computational Linguistics.Shervin Malmasi and Mark Dras.
2014b.
Chinese Na-tive Language Identification.
Proceedings of the 14thConference of the European Chapter of the Associa-tion for Computational Linguistics.Shervin Malmasi and Mark Dras.
2014c.
LanguageTransfer Hypotheses with Linear SVM Weights.
Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras.2013.
NLI Shared Task 2013: MQ Submission.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 124?133, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Lourdes Ortega.
2009.
Understanding Second Lan-guage Acquisition.
Hodder Education, Oxford, UK.Patric?Osterg?ard.
1999.
A New Algorithm for theMaximum-Weight Clique Problem.
Electronic Notesin Discrete Mathematics, 3:153?156, May.Betsy Parrish.
1987.
A New Look at Methodologies inthe Study of Article Acquisition for Learners of ESL.Language Learning, 37(3):361?384.Manfred Pienemann.
2005.
Cross-linguistic Aspects ofProcessability Theory.
John Benjamins, Amsterdam,Netherlands.Benjamin Swanson and Eugene Charniak.
2012.
NativeLanguage Detection with Tree Substitution Gram-mars.
In Proc.
Meeting Assoc.
Computat.
Linguistics(ACL), pages 193?197.Ben Swanson and Eugene Charniak.
2013.
Extractingthe native language signal for second language ac-quisition.
In Proc.
Conf.
North American Assoc.
forComputat.
Linguistics: Human Language Technolo-gies (NAACL-HLT), pages 85?94, Atlanta, Georgia,June.Ben Swanson and Eugene Charniak.
2014.
Data DrivenLanguage Transfer Hypotheses.
In Proc.
Conf.
Euro-pean Assoc.
for Computat.
Linguistics (EACL), pages169?173, Gothenburg, Sweden, April.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.2013.
A report on the first native language identi-fication shared task.
In Proceedings of the EighthWorkshop on Innovative Use of NLP for Building Ed-ucational Applications (BEA), pages 48?57, Atlanta,Georgia, June.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploitingparse structures for native language identification.
InProc.
Conf.
Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1600?1610.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring Adaptor Grammars for Native Lan-guage Identification.
In Proc.
Conf.
Empirical Meth-ods in Natural Language Processing (EMNLP), pages699?709.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for AutomaticallyGrading ESOL Texts.
In Proc.
Meeting Assoc.
Com-putat.
Linguistics (ACL), pages 180?189.Helen Yannakoudakis, Ted Briscoe, and Theodora Alex-opoulou.
2012.
Automating Second Language Ac-quisition Research: Integrating Information Visualisa-tion and Machine Learning.
In Proc.
EACL Workshopof LINGVIS & UNCLH, pages 35?43.Richard Young.
1996.
Form-Function Relations in Arti-cles in English Interlanguage.
In R. Bayley and D. R.Preston, editors, Second Language Acquisition andLinguistic Variation, pages 135?175.
John Benjamins,Amsterdam, The Netherlands.64
