In: R. Levy & D. Reitter (Eds.
), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 80?89,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsA Computational Model of Memory, Attention, and Word LearningAida Nematzadeh, Afsaneh Fazly, and Suzanne StevensonDepartment of Computer ScienceUniversity of Toronto{aida,afsaneh,suzanne}@cs.toronto.eduAbstractThere is considerable evidence that peoplegenerally learn items better when the presen-tation of items is distributed over a period oftime (the spacing effect).
We hypothesize thatboth forgetting and attention to novelty playa role in the spacing effect in word learning.We build an incremental probabilistic compu-tational model of word learning that incorpo-rates a forgetting and attentional mechanism.Our model accounts for experimental resultson children as well as several patterns ob-served in adults.1 Memory, Attention, and Word LearningLearning the meaning of words is an important com-ponent of language acquisition, and an extremelychallenging task faced by young children (e.g.,Carey, 1978; Bloom, 2000).
Much psycholinguis-tic research has investigated the mechanisms under-lying early word learning, and the factors that mayfacilitate or hinder this process (e.g., Quine, 1960;Markman and Wachtel, 1988; Golinkoff et al, 1992;Carpenter et al, 1998).
Computational modeling hasbeen critical in this endeavor, by giving precise ac-counts of the possible processes and influences in-volved (e.g., Siskind, 1996; Regier, 2005; Yu, 2005;Fazly et al, 2010).
However, computational modelsof word learning have generally not given sufficientattention to the broader interactions of language ac-quisition with other aspects of cognition and cogni-tive development.Memory limitations and attentional mechanismsare of particular interest, with recent computationalstudies reconfirming their important role in aspectsof word learning.
For example, Frank et al (2010)show that memory limitations are key to matchinghuman performance in a model of word segmenta-tion, while Smith et al (2010) further demonstratehow attention plays a role in word learning by form-ing the basis for abstracting over the input.
Butmuch potential remains for computational modelingto contribute to a better understanding of the role ofmemory and attention in word learning.One area where there is much experimental evi-dence relevant to these interactions is in the investi-gation of the spacing effect in learning (Ebbinghaus,1885; Glenberg, 1979; Dempster, 1996; Cepedaet al, 2006).
The observation is that people gen-erally show better learning when the presentationsof the target items to be learned are ?spaced?
?
i.e.,distributed over a period of time ?
instead of be-ing ?massed?
?
i.e., presented together one afterthe other.
Investigations of the spacing effect oftenuse a word learning task as the target learning event,and such studies have looked at the performance ofadults as well as children (Glenberg, 1976; Pavlikand Anderson, 2005; Vlach et al, 2008).
Whilethis work involves controlled laboratory conditions,the spacing effect is very robust across domains andtasks (Dempster, 1989), suggesting that the underly-ing cognitive processes likely play a role in naturalconditions of word learning as well.Hypothesized explanations for the spacing effecthave included both memory limitations and atten-tion.
For example, many researchers assume that theprocess of forgetting is responsible for the improvedperformance in the spaced presentation: Becauseparticipants forget more of what they have learnedin the longer interval, they learn more from sub-sequent presentations (Melton, 1967; Jacoby, 1978;80Cuddy and Jacoby, 1982).
However, the precise re-lation between forgetting and improved learning hasnot been made clear.
It has also been proposed thatsubjects attend more to items in the spaced presen-tation because accessing less recent (more novel)items in memory requires more effort or attention(Hintzman, 1974).
However, the precise attentionalmechanism at work in the spacing experiments is notcompletely understood.While such proposals have been discussed formany years, to our knowledge, there is as yet no de-tailed computational model of the precise manner inwhich forgetting and attention to novelty play a rolein the spacing effect.
Moreover, while mathemat-ical models of the effect help to clarify its proper-ties (Pavlik and Anderson, 2005), it is very impor-tant to situate these general cognitive mechanismswithin a model of word learning in order to under-stand clearly how these various processes might in-teract in the natural word learning setting.We address this gap by considering memory con-straints and attentional mechanisms in the contextof a computational model of word-meaning acquisi-tion.
Specifically, we change an existing probabilis-tic incremental model of word learning (Fazly et al,2010) by integrating two new factors: (i) a forgettingmechanism that causes the learned associations be-tween words and meanings to decay over time; and(ii) a mechanism that simulates the effects of atten-tion to novelty on in-the-moment learning.
The re-sult is a more cognitively plausible word learningmodel that includes a precise formulation of bothforgetting and attention to novelty.
In simulationsusing this new model, we show that a possible ex-planation for the spacing effect is the interplay ofthese two mechanisms, neither of which on its owncan account for the effect.2 The Computational ModelWe extend the model of Fazly et al (2010) ?
hence-forth referred to as FAS10 ?
by integrating newfunctionality to capture forgetting and attention tonovelty.
The model of FAS10 is an appropriate start-ing point for our study because it is an incremen-tal model of word learning that learns probabilis-tic associations between words and their semanticproperties from naturalistic data.
Nonetheless, themodel assumes equal attention to all words and ob-jects present in the input, and, although incremental,it has a perfect memory for the internal represen-tation of each processed input.
Hence, as we willshow, it is incapable of simulating the spacing ef-fects observed in humans.2.1 The FAS10 ModelThe input to the model is a sequence of utterances (aset of words), each paired with a scene representa-tion (a set of semantic features, representing what isperceived when the words are heard), as in:Utterance: { she, drinks, milk }Scene: { ANIMATE, PERSON, FEMALE, CONSUME,DRINK, SUBSTANCE, FOOD, DAIRY-PRODUCT }For each word, the model of FAS10 learns a proba-bility distribution over all possible features, p(.|w),called the meaning probability of the word.
Beforeprocessing any input, all features are equally likelyfor a word, and the word?s meaning probability isuniform over all features.
At each time step t, aninput utterance?scene pair (similar to the aboveexample) is processed.
For each word w and seman-tic feature f in the input pair, an alignment score,at(w| f ), is calculated that specifies how stronglythe w?
f pair are associated at time t. The alignmentscore in FAS10 uses the meaning probabilities ofall the words in the utterance, which reflect theknowledge of the model of word meanings up tothat point, as in:at(w|f ) =pt?1(f |w)?w?
?Utpt?1(f |w?
)(1)where pt?1( f |w) is the probability of f being part ofthe meaning of word w at time t?1.In the FAS10 model, pt(.|w) is then updated forall the words in the utterance, using the accumulatedevidence from all prior and current co-occurrencesof w?
f pairs.
Specifically, an association score isdefined between a word and a feature, assoct(w, f ),which is a summation of all the alignments for thatw and f up to time t.1 This association score is thennormalized using a smoothed version of the follow-1In FAS10, assoct(w, f ) = assoct?1(w, f )+at(w| f ).81ing to yield pt( f |w):pt( f |w) =assoct( f , w)?f ?
?Massoct( f?, w)(2)whereM is the set of all observed features.There are two observations to make about theFAS10 model in the context of our desire to exploreattention and forgetting mechanisms in word learn-ing.
First, the calculation of alignments at(w| f ) inEqn.
(1) treats all words equally, without special at-tention to any particular item(s) in the input.
Sec-ond, the assoct( f ,w) term in Eqn.
(2) encodes per-fect memory of all calculated alignments since it is asimple accumulated sum.
These properties motivatethe changes to the formulation of the model that wedescribe next.2.2 Adding Attention to Novelty to the ModelAs noted just above, the FAS10 model lacks anymechanism to focus attention on certain words, as issuggested by theories on the spacing effect (Hintz-man, 1974).
One robust observation in studies onattention is that people attend to new items in alearning scenario more than other items, leading toimproved learning of the novel items (e.g., Snyderet al, 2008; MacPherson and Moore, 2010; Horstet al, 2011).
We thus model the effect of attentionto novelty when calculating alignments in our newmodel: attention to a more novel word increases thestrength of its alignment with a feature ?
and con-sequently the learned word?feature association ?compared to the alignment of a less novel word.We modify the original alignment formulation ofFAS10 to incorporate a multiplicative novelty termas follows (cf.
Eqn.
(1)):at(w, f ) =pt(f |w)?w?
?Utpt(f |w?
)?noveltyt(w) (3)where noveltyt(w) specifies the degree of novelty ofa word as a simple inverse function of recency.
Thatis, we assume that the more recently a word has beenobserved by the model, the less novel it appears tothe model.
Given a word w at time t that was lastobserved at time tlastw , we calculate noveltyt(w) as:noveltyt(w) = 1?
recency(t, tlastw) (4)where recency(t, tlastw) is inversely proportionalto the difference between t and tlastw .
We setnovelty(w) to be 1 for the first exposure of the word.2.3 Adding a Forgetting Mechanism to theModelGiven the observation above (see end of Section 2.1)that assoct(w, f ) embeds perfect memory in theFAS10 model, we add a forgetting mechanism by re-formulating assoct(w, f ) to incorporate a decay overtime of the component alignments at(w| f ).
In or-der to take a cognitively plausible approach to calcu-lating this function, we observe that assoct(w, f ) inFAS10 serves a similar function to activation in theACT-R model of memory (Anderson and Lebiere,1998).
In ACT-R, activation of an item is the sumof individual memory strengthenings for that item,just as assoct(w, f ) is a sum of individual align-ment strengths for the pair (w, f ).
A crucial dif-ference is that memory strengthenings in ACT-Rundergo decay.
Specifically, activation of an itemm after t presentations is calculated as: act(m)t =ln(?tt ?=1 1/(t?t?
)d), where t ?
is the time of each pre-sentation, and d is a constant decay parameter.We adapt this formulation for assoct(w, f ) withthe following changes: First, in the act formula, theconstant 1 in the numerator is the basic strength ofeach presentation to memory.
In our model, thisis not a constant but rather the strength of align-ment, at(w| f ).
Second, we assume that strongeralignments should be more entrenched in memoryand thus decay more slowly than weaker alignments.Thus, each alignment undergoes a decay which isdependent on the strength of the alignment ratherthan a constant decay d. We thus define assoct(w, f )to be:assoct( f ,w) = ln(t?t ?=1at ?
(w| f )(t?
t ?)dat?)
(5)where the decay for each alignment dat?
is:dat?
=dat ?
(w| f )(6)where d is a constant parameter.
Note that the dat?decreases as at ?
(w| f ) increases.82apple: { FOOD:1, SOLID:.72, PRODUCE:.63,EDIBLE-FRUIT:.32, PLANT-PART:.22,PHYSICAL-ENTITY:.17, WHOLE:.06, ?
?
?
}Figure 1: True meaning features & scores for apple.3 Input GenerationThe input data consists of a set of utterances pairedwith their corresponding scene representations.
Theutterances are taken from the child-directed speech(CDS) portion of the Manchester corpus (Theakstonet al, 2001), from CHILDES (MacWhinney, 2000),which includes transcripts of conversations with 12British children, ages 1;8 to 3;0.
Every utteranceis considered as a bag of lemmatized words.
Half ofthe data is used as the development set, and the otherhalf in the final experiments.Because no manually-annotated semantic repre-sentation is available for any such large corpus ofCDS, we use the approach of Nematzadeh et al(2012) to generate scene representations.
For eachutterance a scene representation is generated artifi-cially, by first creating an input-generation lexiconthat contains the true meaning (t(w)) of all the words(w) in our corpus.
The true meaning is a vectorof semantic features and their assigned scores (Fig-ure 1).
The semantic features for a word, depend-ing on its part of speech, are chosen from differentsources such as WordNet.2 The score of each featureis calculated automatically to give a higher value tothe more specific features (such as FRUIT for apple),rather than more general features (like PHYSICAL-ENTITY for apple).To generate the scene representation S of an utter-ance U, we probabilistically sample a subset of fea-tures from the features in t(w) for each word w ?U.Thus, in each occurrence of w some of its featuresare missing from the scene, resulting in an imper-fect sampling.
This imperfect sampling allows us tosimulate noise and uncertainty in the input, as wellas the uncertainty of a child in determining the rele-vant meaning elements in a scene.
The scene S is theunion of all the features sampled for all the words inthe utterance.
We note that the input-generation lex-icon is only used in creating input corpora that arenaturalistic (based on child-directed speech), and notin the learning of the model.2http://wordnet.princeton.edu4 ExperimentsFirst, we examine the overall word learning be-haviour in our new model.
Then we look at spacingeffects in the learning of novel words.
In both theseexperiments, we compare the behavior of our modelwith the model of FAS10 to clearly illustrate the ef-fects of forgetting and attention to novelty in the newmodel.
Next we turn to further experiments explor-ing in more detail the interaction of forgetting andattention to novelty in producing spacing effects.4.1 Word Learning over TimeGenerally, the model of FAS10 has increasing com-prehension of words as it is exposed to more inputover time.
In our model, we expect attention to nov-elty to facilitate word learning, by focusing moreon newly observed words, whereas forgetting is ex-pected to hinder learning.
We need to see if the newmodel is able to learn words effectively when sub-ject to the combined effects of these two influences.To measure how well a word w is learned in eachmodel, we compare its learned meaning l(w) (a vec-tor holding the values of the meaning probabilityp(.|w)) to its true meaning t(w) (see Section 3):acq(w) = sim(l(w), t(w)) (7)where sim is the cosine similarity between the twomeaning vectors, t(w) and l(w).
The better themodel learns the meaning of w, the closer l(w)would get to t(w), and the higher the value of simwould become.
To evaluate the overall behaviour ofa model, at each point in time, we average the acqscore of all the words that the model has seen.We train each model on 10,000 input utterance?scene pairs and compare their patterns of word learn-ing over time (Figure 2).3 We can see that in theoriginal model, the average acq score is mostly in-creasing over time before leveling off.
Our model,starts at a higher average acq score compared toFAS10?s model, since the effect of attention to nov-elty is stronger than the effect of forgetting in earlystages of training.
There is a sharp decrease in theacq scores after the early training stage, which thenlevels off.
The early decrease in acq scores oc-curs because many of the words the model is ex-3The constant decay parameter d in Eqn.
(6) is set to 0.03 inthis experiment.83Figure 2: Average acq score of the words over time, forour model and FAS10?s model.posed to early on are not learned very well initially,and so forgetting occurs at a higher rate during thatstage.
The model subsequently stabilizes, and theacq scores level off although at a lower absolutelevel than the FAS10 model.
Note that when com-paring these two models, we are interested in thepattern of learning; in particular, we need to en-sure that our new word learning model will even-tually stabilize as expected.
Our model stabilizesat a lower average acq score since unlike FAS10?smodel, it does not implement a perfect memory.4.2 The Spacing Effect in Novel WordLearningVlach et al (2008) performed an experiment to in-vestigate the effect of presentation spacing in learn-ing novel word?object pairs in three-year-old chil-dren.
Each pair was presented 3 times in each oftwo settings, either consecutively (massed presenta-tion), or with a short play interval between each pre-sentation (spaced presentation).
Children were thenasked to identify the correct object corresponding tothe novel word.
The number of correct responseswas significantly higher when the pairs were in thespaced presentation compared to the massed presen-tation.
This result clearly demonstrates the spacingeffect in novel word learning in children.Experiments on the spacing effect in adults havetypically examined and compared different amountsof time between the spaced presentations, which werefer to as the spacing interval.
Another importantparameter in such studies is the time period betweenthe last training trial and the test trial(s), which werefer to as the retention interval (Glenberg, 1976;Bahrick and Phelps, 1987; Pavlik and Anderson,2005).
Since the experiment of Vlach et al (2008)was designed for very young children, the proce-dures were kept simple and did not vary these twoparameters.
We design an experiment similar to thatof Vlach et al (2008) to examine the effect of spac-ing in our model, but extend it to also study the roleof various spacing and retention intervals, for com-parison to earlier adult studies.4.2.1 Experimental SetupFirst, the model is trained on 100 utterance?scenepairs to simulate the operation of normal word learn-ing prior to the experiment.4 Then a randomlypicked novel word (nw) that did not appear in thetraining trials is introduced to the model in 3 teach-ing trials, similar to Vlach et al?s (2008) experiment.For each teaching trial, nw is added to a different ut-terance, and its probabilistically-generated meaningrepresentation (see Section 3) is added to the corre-sponding scene.
We add nw to an utterance?scenepair from our corpus to simulate the presentation ofthe novel word during the natural interaction withthe child in the experimental setting.The spacing interval between each of these 3teaching trials is varied from 0 to 29 utterances, re-sulting in 30 different simulations for each nw.
Forexample, when the spacing interval is 5, there are5 utterances between each presentation of nw.
Aspacing of 0 utterances yields the massed presenta-tion.
We run the experiment for 20 randomly-chosennovel words to ensure that the pattern of the resultsis not related to the meaning representation of a spe-cific word.For each spacing interval, we look at the acq scoreof the novel word at two points in time, to simu-late two retention intervals: One immediately afterthe last presentation of the novel word (imm condi-tion) and one at a later point in time (lat condition).By looking at these two conditions, we can furtherobserve the effect of forgetting in our model, sincethe decay in the model?s memory would be more se-vere in the lat condition, compared to the imm con-dition.5 The results reported here for each spacing4In the experiments of Section 4.2.2 and Section 4.3, theconstant decay parameter d is equal to 0.04.5Recall that each point of time in our model corresponds to84Figure 3: Average acq score of novel words over spacingintervals, in our model and FAS10?s model.interval average the acq scores of all the novel wordsat the corresponding points in time.4.2.2 The Basic Spacing Effect ResultsFigure 3 shows the results of the simulations inour model and the FAS10 model.
We assume thatvery small spacing intervals (but greater than 0)correspond to the spaced presentation in the Vlachet al (2008) experiments, while a spacing of 0 cor-responds to the massed presentation.
In the FAS10model, the average acq score of words does notchange with spacing, and there is no difference be-tween the imm and lat conditions, confirming thatthis model fails to mimic the observed spacing ef-fects.
By contrast, in our model the average acqscore is greater in the small spacing intervals (1-3) than in the massed presentation, mimicking theVlach et al (2008) results on children.
This happensbecause a nw appears more novel with larger spacingintervals between each of its presentations resultingin stronger alignments.We can see two other interesting patterns in ourmodel: First, the average acq score of words for allspacing intervals is greater in the imm condition thanin the lat condition.
This occurs because there ismore forgetting in the model over the longer reten-tion interval of lat.
Second, in both conditions theaverage acq score initially increases from a massedpresentation to the smaller spacing intervals.
How-ever, at spacing intervals between about 3 and 5,processing an input pair.
The acq score in the imm condition iscalculated at time t, which is immediately after the last presen-tation of nw.
The lat condition corresponds to t +20.the acq score begins to decrease as spacing intervalsgrow larger.
As explained earlier, the initial increasein acq scores for small spacing intervals results fromnovelty of the words in a spaced presentation.
How-ever, for bigger spacing intervals the effect of nov-elty is swamped by the much greater degree of for-getting after a bigger spacing interval.Although Vlach et al (2008) did not vary theirspacing and retention intervals, other spacing effectstudies on adults have done so.
For example, Glen-berg (1976) presented adults with word pairs to learnunder varying spacing intervals, and tested them af-ter several different retention intervals (his experi-ment 1).
Our pattern of results in Figure 3 is in linewith his results.
In particular, he found a nonmono-tonic pattern of spacing similar to the pattern in ourmodel: learning of pairs was improved with increas-ing spacing intervals up to a point, but there was adecrease in performance for larger spacing intervals.Also, the proportion of recalled pairs decreased forlonger retention intervals, similar to our lower per-formance in the lat condition.4.3 The Role of Forgetting and AttentionTo fully understand the role as well as the neces-sity of, both forgetting and attention to novelty inour results, we test two other models under the sameconditions as the previous spacing experiment: (a) amodel with our mechanism for attention to noveltybut not forgetting, and (b) a model with our forget-ting mechanism but no attention to novelty; see Fig-ure 4 and Figure 5, respectively.In the model that attends to novelty but does notincorporate a memory decay mechanism (Figure 4),the average acq score consistently increases as spac-ing intervals grow bigger.
This occurs because thenovel words appear more novel following biggerspacing intervals, and thus attract more alignmentstrength.
Since the model does not forget, there isno difference between the immediate (imm) and later(lat) retention intervals.
This pattern does not matchthe spacing effect patterns of people, suggesting thatforgetting is a necessary aspect of our model?s abil-ity to do so in the previous section.On the other hand, in the model with forgettingbut no attentional mechanism (Figure 5), we see twodifferent behaviors in the imm and lat conditions.
Inthe imm condition, the average acq score decreases85Figure 4: Average acq score of the novel words over spac-ing intervals, for the model with novelty but without for-getting.Figure 5: Average acq score of the novel words over spac-ing intervals, for the model with forgetting but withoutnovelty.consistently over spacing intervals.
This is as ex-pected, because the greater time between presenta-tions means a greater degree of forgetting.
Specif-ically, the alignment scores decay more betweenpresentations of the word to be learned, given thegreater passage of time in larger spacing intervals.The weaker alignments then lead to lower acq scoresin this condition.Paradoxically, although this effect on learningalso holds in the lat condition, another factor is atplay, leading to better performance than in the immcondition at all spacing intervals.
Here the greaterretention interval ?
the time between the last learn-ing presentation and the test time ?
leads to greaterforgetting in a manner that instead improves the acqscores.
Consider that the meaning representationfor a word includes some probability mass assignedto irrelevant features ?
i.e., those features that oc-curred in an utterance?scene pair with the word butare not part of its true meaning.
Because such fea-tures generally have lower probability than relevantfeatures (which are observed more consistently witha word), a longer retention interval leads to them de-caying more than the relevant features.
Thus the latcondition enables the model to better focus on thefeatures relevant to a word.In conclusion, neither attention to novelty nor for-getting alone achieves the pattern typical of the spac-ing effects in people that our model shows in thelower two plots in Figure 3.
Hence we conclude thatboth factors are necessary to our account, suggestingthat it is an interaction between the two that accountsfor people?s behaviour.4.4 The ?Spacing Crossover Interaction?In our model with attention to novelty and forgetting(see Section 4.2), the average acq score was alwaysbetter in the imm condition than the lat condition.However, researchers have observed other patternsin spacing experiments.
A particularly interestingpattern found in some studies is that the plots of theresults for earlier and later retention intervals crossas the spacing intervals are increased.
That is, withsmaller spacing intervals, a shorter retention inter-val (such as our imm condition) leads to better re-sults, but with larger spacing intervals, a longer re-tention interval (such as our lat condition) leads tobetter results (Bahrick, 1979; Pavlik and Anderson,2005).
This interaction of spacing and retention in-tervals results in a pattern referred to as the spacingcrossover interaction (Pavlik and Anderson, 2005).This pattern is different from Glenberg?s (1976) ex-periment and from the pattern of results shown ear-lier for our model (Figure 3).We looked at an experiment in which the spac-ing crossover pattern was observed: Pavlik and An-derson (2005) taught Japanese?English pairs to sub-jects, varying the spacing and retention intervals.One difference we noticed between the experimentof Pavlik and Anderson (2005) and Glenberg (1976)was that in the former, the presentation period of thestimulus was 5 seconds, whereas in the latter, it was3 seconds.
We hypothesize that the difference be-tween the amount of time for the presentation peri-86Figure 6: Average acq score of the novel words over spac-ing intervalsods might explain the different spacing patterns inthese experiments.We currently cannot model presentation time di-rectly in our model, since having access to an in-put longer does not change its computation of align-ments between words and features.
However, wecan indirectly model a difference in presentationtime by modifying the amount of memory decay:We assume that when an item is presented longer, itis learned better and therefore subject to less forget-ting.
We run the spacing experiment with a smallerforgetting parameter to model the longer presenta-tion period used in Pavlik and Anderson?s (2005)versus Glenberg (1976).6Our results using the decreased level of forgetting,given in Figure 6, show the expected crossover inter-action between the retention and spacing intervals:for smaller spacing intervals, the acq scores are bet-ter in the imm condition, whereas for larger spacingintervals, they are better in the lat condition.
Thus,our model suggests an explanation for the observedcrossover: in tasks which strengthen the learning ofthe target item ?
and thus lessen the effect of forget-ting ?
we expect to see a benefit of later retentiontrials in experiments with people.5 General Discussion and Future WorkThe spacing effect (where people learn items betterwhen multiple presentations are spread over time)has been studied extensively and is found to be ro-bust over different types of tasks and domains.
Many6Here, the decay parameter is set to 0.03.experiments have examined the spacing effect in thecontext of word learning and other similar tasks.Particularly, in a recent study of Vlach et al (2008),young children demonstrated a spacing effect in anovel word learning task.We use computational modeling to show that bychanging a probabilistic associative model of wordlearning to include both a forgetting and attentionalmechanism, the new model can account not only forthe child data, but for various patterns of spacing ef-fect data in adults.
Specifically, our model shows thenonmonotonic pattern of spacing observed in the ex-perimental data, where learning improves in shorterspacing intervals, but worsens in bigger spacing in-tervals.
Our model can also replicate the observedcross-over interaction between spacing and retentionintervals: for smaller spacing intervals, performanceis better when tested after a shorter retention inter-val, whereas for bigger spacing intervals, it is betterafter longer retention intervals.
Finally, our resultsconfirm that by modelling word learning as a stan-dalone development process, we cannot account forthe spacing effect.
Instead, it is important to con-sider word learning in the context of fundamentalcognitive processes of memory and attention.Much remains to be investigated in our model.For example, most human experiments examine theeffect of frequency of presentations of target items.Also, the range of retention intervals that has beenstudied is greater than what we have consideredhere.
In the future, we plan to study the effect ofthese two parameters.
In addition, with our currentmodel, the amount of time an item is presented tothe learner does not play a role.
We can also re-formulate our alignment mechanism to incorporatea notion of the amount of time to consider an itemto be learned.
Another interesting future direction,especially in the context of word learning, is to de-velop a more complete attentional mechanism, thatconsiders different parameters such as social cuesand linguistic cues.
Finally, we will study the roleof forgetting and attention in modelling other rele-vant experimental data (e.g., Kachergis et al, 2009;Vlach and Sandhofer, 2010).87ReferencesJohn .R.
Anderson and Christian Lebiere.
1998.
Theatomic components of thought.
Lawrence Erl-baum Associates.Harry P. Bahrick.
1979.
Maintenance of knowl-edge: Questions about memory we forgot to ask.Journal of Experimental Psychology: General,108(3):296?308.Harry P. Bahrick and Elizabeth Phelps.
1987.
Reten-tion of Spanish vocabulary over 8 years.
Journalof Experimental Psychology: Learning, Memory,and Cognition, 13(2):344?349.Paul Bloom.
2000.
How Children Learn the Mean-ings of Words.
MIT Press.Susan Carey.
1978.
The child as word learner.
InM.
Halle, J. Bresnan, and G. A. Miller, editors,Linguistic Theory and Psychological Reality.
TheMIT Press.Malinda Carpenter, Katherine Nagell, MichaelTomasello, George Butterworth, and ChrisMoore.
1998.
Social cognition, joint attention,and communicative competence from 9 to 15months of age.
Monographs of the Society forResearch in Child Development, 63(4).Nicholas J. Cepeda, Harold Pashler, Edward Vul,John T. Wixted, and Doug Rohrer.
2006.
Dis-tributed practice in verbal recall tasks: A reviewand quantitative synthesis.
Psychological Bul-letin, 132(3):354 ?
380.Lauren J. Cuddy and Larry L. Jacoby.
1982.
Whenforgetting helps memory: an analysis of repetitioneffects.
Journal of Verbal Learning and VerbalBehavior, 21(4):451 ?
467.Frank Dempster.
1989.
Spacing effects and their im-plications for theory and practice.
EducationalPsychology Review, 1:309?330.Frank N. Dempster.
1996.
Distributing and manag-ing the conditions of encoding and practice.
Mem-ory, pages 317?344.Hermann Ebbinghaus.
1885.
Memory: A contri-bution to experimental psychology.
New York,Teachers College, Columbia University.Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-son.
2010.
A probabilistic computational modelof cross-situational word learning.
Cognitive Sci-ence, 34(6):1017?1063.Michael C. Frank, Sharon Goldwater, Thomas L.Griffiths, and Joshua B. Tenenbaum.
2010.
Mod-eling human performance in statistical word seg-menation.
Cognition, 117:107?125.Arthur Glenberg.
1979.
Component-levels theory ofthe effects of spacing of repetitions on recall andrecognition.
Memory and Cognition, 7:95?112.Arthur M. Glenberg.
1976.
Monotonic and non-monotonic lag effects in paired-associate andrecognition memory paradigms.
Journal of Ver-bal Learning & Verbal Behavior, 15(1).Roberta M. Golinkoff, Kathy Hirsh-Pasek, Leslie M.Bailey, and Neil R. Wegner.
1992.
Young chil-dren and adults use lexical principles to learn newnouns.
Developmental Psychology, 28(1):99?108.Douglas L. Hintzman.
1974.
Theoretical implica-tions of the spacing effect.Jessica S. Horst, Larissa K. Samuelson, Sarah C.Kucker, and Bob McMurray.
2011.
Whats new?children prefer novelty in referent selection.
Cog-nition, 118(2):234 ?
244.Larry L. Jacoby.
1978.
On interpreting the effectsof repetition: Solving a problem versus remem-bering a solution.
Journal of Verbal Learning andVerbal Behavior, 17(6):649 ?
667.George Kachergis, Chen Yu, and Richard Shiffrin.2009.
Temporal contiguity in cross-situationalstatistical learning.Amy C. MacPherson and Chris Moore.
2010.
Un-derstanding interest in the second year of life.
In-fancy, 15(3):324?335.Brian MacWhinney.
2000.
The CHILDES Project:Tools for Analyzing Talk, volume 2: TheDatabase.
Erlbaum, 3rd edition.Ellen M. Markman and Gwyn F. Wachtel.
1988.Children?s use of mutual exclusivity to constrainthe meanings of words.
Cognitive Psychology,20:121?157.Arthur W. Melton.
1967.
Repetition and retrievalfrom memory.
Science, 158:532.88Aida Nematzadeh, Afsaneh Fazly, and SuzanneStevenson.
2012.
Interaction of word learning andsemantic category formation in late talking.
InProc.
of CogSci?12.
To appear.Philip I. Pavlik and John R. Anderson.
2005.
Prac-tice and forgetting effects on vocabulary memory:An activationbased model of the spacing effect.Cognitive Science, 29:559?586.W.V.O.
Quine.
1960.
Word and Object.
MIT Press.Terry Regier.
2005.
The emergence of words: At-tentional learning in form and meaning.
CognitiveScience, 29:819?865.Jeffery Mark Siskind.
1996.
A computational studyof cross-situational techniques for learning word-to-meaning mappings.
Cognition, 61:39?91.Linda B Smith, Eliana Colunga, and HanakoYoshida.
2010.
Knowledge as process:Contextually-cued attention and early wordlearning.
Cogn Sci, 34(7):1287?314.Kelly A. Snyder, Michael P. BlanK, and chad J. Mar-solek.
2008.
What form of memory underlies nov-elty preferences?
Psychological Bulletin and Re-view, 15(2):315 ?
321.Anna L. Theakston, Elena V. Lieven, Julian M. Pine,and Caroline F. Rowland.
2001.
The role of per-formance limitations in the acquisition of verb?argument structure: An alternative account.
J. ofChild Language, 28:127?152.Haley A Vlach and Catherine M Sandhofer.
2010.Desirable difficulties in cross-situational wordlearning.Haley A. Vlach, Catherine M. Sandhofer, and NateKornell.
2008.
The Spacing Effect in Children?sMemory and Category Induction.
Cognition,109(1):163?167, October.Chen Yu.
2005.
The emergence of links betweenlexical acquisition and object categorization: Acomputational study.
Connection Science, 17(3?4):381?397.89
