Learning a Robust Word Sense Disambiguation Model usingHypernyms in Definition SentencesKiyoaki Shirai, Tsunekazu YagiSchool of Information Science, Japan Advanced Institute of Science and Technology1-1, Asahidai, Tatsunokuchi, 923-1292, Ishikawa, Japan{kshirai,t-yagi}@jaist.ac.jpAbstractThis paper proposes a method to improvethe robustness of a word sense disambigua-tion (WSD) system for Japanese.
TwoWSD classifiers are trained from a wordsense-tagged corpus: one is a classifier ob-tained by supervised learning, the other isa classifier using hypernyms extracted fromdefinition sentences in a dictionary.
The for-mer will be suitable for the disambiguationof high frequency words, while the latter isappropriate for low frequency words.
A ro-bust WSD system will be constructed bycombining these two classifiers.
In our ex-periments, the F-measure and applicabilityof our proposed method were 3.4% and 10%greater, respectively, compared with a singleclassifier obtained by supervised learning.1 IntroductionWord sense disambiguation (WSD) is the pro-cess of selecting the appropriate meaning orsense for a given word in a document.
Obvi-ously, WSD is one of the fundamental and im-portant processes needed for many natural lan-guage processing (NLP) applications.
Over thepast decade, many studies have been made onWSD of Japanese.
Most current research usesmachine learning techniques (Li and Takeuchi,1997; Murata et al, 2001; Takamura et al,2001), has achieved good performance.
How-ever, as supervised learning methods requireword sense-tagged corpora, they often sufferfrom data sparseness, i.e., words which do notoccur frequently in a training corpus can not bedisambiguated.
Therefore, we cannot use su-pervised learning algorithms alone in practicalNLP applications, especially when it is neces-sary to disambiguate both high frequency andlow frequency words.To tackle this problem, this paper proposes amethod to combine two WSD classifiers.
Oneis a classifier obtained by supervised learning.The learning algorithm used for this classifier isthe Support Vector Machine (SVM); this clas-sifier will work well for the disambiguation ofhigh frequency words.
The second classifier isthe Naive Bayes model, which will work wellfor the disambiguation of low frequency words.In this model, hypernyms extracted from defi-nition sentences in a dictionary are consideredin order to overcome data sparseness.The details of the SVM classifier are de-scribed in Section 2, and the Naive Bayes modelin Section 3.
The combination of these two clas-sifiers is described in Section 4.
The experi-mental evaluation of the proposed method is re-ported in Section 5.
We mention some relatedworks in Section 6, and conclude the paper inSection 7.2 SVM ClassifierThe first classifier is the SVM classifier.
SinceSVM is a supervised learning algorithm, a wordsense-tagged corpus is required as training data,and the classifier can not be used to disam-biguate words which do not occur frequentlyin the data.
However, as the effectiveness ofSVM has been widely reported for a variety ofNLP tasks including WSD (Murata et al, 2001;Takamura et al, 2001), we know that it willwork well for disambiguation of high frequencywords.When training the SVM classifier, each train-ing instance should be represented by a featurevector.
We used the following features, whichare typical for WSD.?
S(0), S(?1), S(?2), S(+1), S(+2)Surface forms of a target word and wordsjust before or after a target word.
A num-ber in parentheses indicates the position ofa word from a target word.?
P (?1), P (?2), P (+1), P (+2)Parts-of-speech (POSs) of words just beforeor after a target word.?
S(?2)?S(?1), S(+1)?S(+2), S(?1)?S(+1)Pairs of surface forms of words surroundinga target word.?
P (?2)?P (?1), P (+1)?P (+2), P (?1)?P (+1)Pairs of POSs of words surrounding a tar-get word.?
BsentBase forms of content words in a sentence1.?
CsentSemantic classes of content words in asentence.
Semantic classes used hereare derived from the Japanese the-saurus ?Nihongo-Goi-Taikei?
(Ikehara etal., 1997).?
Bhead, BmodBase forms of the head (Bhead) or modifiers(Bmod) of a target word.?
(Bcase;Bnoun)A pair of the base forms of a case marker(Bcase) and a case filler noun (Bnoun) whenthe target word is a verb.?
(Bcase;Cnoun)A pair of the base form of a case marker(Bcase) and the semantic class of a casefiller noun (Cnoun) when the target wordis a verb.?
(Bcase;Bverb)A pair of the base forms of a case marker(Bcase) and a head verb (Bverb) when thetarget word is a case filler noun of a certainverb.We used the LIBSVM package 2 for train-ing the SVM classifier.
The SVM model is?
?SVM (Scho?lkopf, 2000) with a linear kernel,where the parameter ?
= 0.0001.
The pairwisemethod is used to apply SVM to multi classifi-cation.3 Naive Bayes Classifier usingHypernyms in DefinitionSentencesIn this section, we will describe the details ofthe WSD classifier using hypernyms of words1We tried using the special symbol ?NUM?
as a fea-ture for any numbers in a sentence, but the performancewas slightly worse in our experiment.
We thank theanonymous reviewer who gave us the comment aboutthis.2http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/CID Definition sentence3c5631 ?????????
??
(a comicstory-telling entertainment)1f66e3 ????????
(a rambling story)Figure 1: Sense Set of ???
?extracted from definition sentences in a dictio-nary.3.1 OverviewLet us explain the basic idea of the model byconsidering the case in which the word ????
(mandan; comic chat) in the following examplesentence (A) should be disambiguated:(A) ??????????????...(Mr.
Sakano was initiated into theworld of comic chat ...)In this paper, word senses are defined accord-ing to the EDR concept dictionary (EDR, 1995).Figure 1 illustrates two meanings for ????
(comic chat) in the EDR concept dictionary.?CID?
indicates a concept ID, an identificationnumber of a sense.One of the ways to disambiguate the sensesof ????
(comic chat) is to train the WSD clas-sifier from the sense-tagged corpus, as with theSVM classifier.
However, when ????
(comicchat) occurs infrequently or not at all in thetraining corpus, we can not train any reliableclassifiers.To train the WSD classifier for low frequencywords, we looked at hypernyms of senses in def-inition sentences.
For Japanese, in most casesthe last word in a definition sentence is a hy-pernym.
For example, the hypernym of sense3c5631 in Figure 1 is the last underlined word????
(engei ; entertainment), while the hyper-nym of 1f66e3 is ???
(hanashi ; story).In the EDR concept dictionary, there aresenses whose hypernyms are also ????
(en-tertainment) or ???
(story).
For example, asshown in Figure 2, 10d9a4, 3c3fbb and 3c5ab3are senses whose hypernyms are ????
(enter-tainment), while the hypernym of 3cf737, 0f73c1and 3c3071 is ???
(story).
If these senses oc-cur in the training corpus, we can train a clas-sifier that determines whether the hypernym of????
(comic chat) is ????
(entertainment)or ???
(story).
If we can determine the correcthypernym, we can also determine which is thecorrect sense, 3c5631 or 1f66e3.
Notice that wecan train such a model even when ????
(comic????
10d9a4 ????????
?, ???????????
??
(a monologue-style,comic story-telling entertainment always ending with a punch line)????
3c3fbb ????????????
(a type of medieval folk entertainment of Japan,called ?Sarugaku?)????
3c5ab3 ??????????????????
(entertainment of cutting shapesout of paper)????
3cf737 ??????????????
(a story passed down among people sinceancient times)????
0f73c1 ??????????
(a true story)????
101156 ??????????
(a story for children)Figure 2: Examples of Senses whose Hypernyms are ????
(entertainment) or ???
(story)0efb60 ??/?/??/?/??/?/??/?
(a word representing the number ofcompetitions or contests)Figure 3: Definition Sentence of the sense0efb60 of the word ????
?chat) itself does not occur in the training cor-pus.As described later, we train the probabilisticmodel that predicts a hypernym of a given word,instead of a word sense.
Much more trainingdata will be available to train the model pre-dicting hypernyms rather than the model pre-dicting senses, because there are fewer typesof hypernyms than of senses.
Figure 2 illus-trates this fact clearly: all words labeled with10d9a4, 3c3fbb and 3c5ab3 in the training datacan be used as the data labeled with the hy-pernym ????
(entertainment).
In this way,we can train a reliable WSD classifier for lowfrequency words.
Furthermore, hypernyms willbe automatically extracted from definition sen-tences, as described in Subsection 3.2, so thatthe model can be automatically trained withouthuman intervention.3.2 Extraction of HypernymsIn this subsection, we will describe how to ex-tract hypernyms from definition sentences in adictionary.
In principle, we assume that the hy-pernym of a sense is the last word of a defi-nition sentence.
For example, in the definitionsentence of sense 3c5631 of ????
(comic chat),the last word ????
(entertainment) is the hy-pernym, as shown in Figure 1.
However, we can-not always regard the last word as a hypernym.Let us consider the definition of sense 0eb70d ofthe word ?????
(ge?mu; game).
In the EDRconcept dictionary, the expression ?A/?/???/??
(a word representing A) often appears indefinition sentences.
In this case, the hypernymof the sense is not the last word but A. Thusthe hypernym of 0efb60 is not the last word???
(go; word) but ????
(kaisuu; number)in Figure 3.When we extract a hypernym from a defini-tion sentence, the definition sentence is first an-alyzed morphologically (word segmentation andPOS tagging) by ChaSen 3.
Then a hypernymin a definition sentence is identified by patternmatching.
An example of patterns used here isthe rule extracting A when the expression ?A/?/???/??
is found in a definition sentence.We made 64 similar patterns manually in orderto extract hypernyms appropriately.Out of the 194,303 senses of content words inthe EDR concept dictionary, hypernyms wereextracted for 191,742 senses (98.7%) by ourpattern matching algorithm.
Furthermore, wechose 100 hypernyms randomly and checkedtheir validity, and found that 96% of the hyper-nyms were appropriate.
Therefore, our methodfor extracting hypernyms worked well.
The ma-jor reasons why acquisition of hypernyms failedwere lack of patterns and faults in the morpho-logical analysis of definition sentences.3.3 Naive Bayes ModelWe will describe the details of our probabilis-tic model that considers hypernyms in defini-tion sentences.
First of all, let us consider thefollowing probability:P (s, c|F ) (1)In (1), s is a sense of a target word, c is a hy-pernym extracted from the definition sentenceof s, and F is the set of features representing aninput sentence including a target word.3ChaSen is the Japanese morphological analyzer.http://chasen.aist-nara.ac.jp/hiki/ChaSen/Next, we approximate Equation (1) as (2):P (s, c|F ) = P (s|c, F )P (c|F )  P (s|c)P (c|F )(2)The first term, P (s|c, F ), is the probabilisticmodel that predicts a sense s given a featureset F (and c).
It is similar to the ordinaryNaive Bayes model for WSD (Pedersen, 2000).However, we assume that this model can not betrained for low frequency words due to a lackof training data.
Therefore, we approximateP (s|c, F ) to P (s|c).Using Bayes?
rule, Equation (2) can be com-puted as follows:P (s|c)P (c|F ) =P (s)P (c|s)P (c)P (c)P (F |c)P (F )(3)=P (s)P (F |c)P (F )(4)Notice that P (c|s) in (3) is equal to 1, becausea hypernym c of a sense s is uniquely extractedby pattern matching (Subsection 3.2).As all we want to do is to choose an s?
whichmaximizes (4), P (F ) can be eliminated:s?
= arg maxsP (s)P (F |c)P (F )(5)= arg maxsP (s)P (F |c) (6)Finally, by the Naive Bayes assumption, that isall features in F are conditionally independent,Equation (6) can be approximated as follows:s?
= arg maxsP (s)?fi?FP (fi|c) (7)In (7), P (s) is the prior probability of a senses which reflects statistics of the appearance ofsenses, while P (fi|c) is the posterior probabilitywhich reflects collocation statistics between anindividual feature fi and a hypernym c. Theparameters of these probabilistic models can beestimated from the word sense-tagged corpus.We estimated P (s) by Expected Likelihood Es-timation and P (fi|c) by linear interpolation.Feature SetThe features used in the Naive Bayes model arealmost same as ones used in the SVM classifierexcept for the following features:[Features not used in the Naive Bayes model]?
S(?2), S(+2), P (?2), P (+2)?
S(?2)?S(?1), S(+1)?S(+2), S(?1)?S(+1)?
P (?2)?P (?1), P (+1)?P (+2), P (?1)?P (+1)?
Csent, (Bcase;Cnoun)According to the preliminary experiment, theaccuracy of the Naive Bayes model slightly de-creased when all features in the SVM classifierwere used.
This was the reason why we did notuse the above features.3.4 DiscussionThe following discussion examines our methodfor extracting hypernyms from definition sen-tences.Multiple HypernymsIn general, two or more hypernyms can be ex-tracted from a definition sentence, when the def-inition of a sense consists of several sentencesor a definition sentence contains a coordinatestructure.
However, for this work we extractedonly one hypernym for a sense, because defini-tions of all senses in the EDR concept dictionaryare described by a single sentence, and most ofthem contain no coordinate structure.In order to apply our model for multiple hy-pernyms, we must consider the probabilisticmodel P (s,C|F ) instead of Equation (1), whereC is a set of hypernyms.
Unfortunately, the es-timation of P (s,C|F ) is not obvious, so inves-tigation of this will be done in future.Ambiguity of hypernymsThe fact that hypernyms may have severalmeanings does not appear to be a major prob-lem, because most hypernyms in definition sen-tences of a certain dictionary have a singlemeaning according to our rough observation.
Sofor this work we ignored the possible ambiguityof hypernyms.Using other dictionariesAs described in Subsection 3.2, hypernyms areextracted by pattern matching.
We would haveto rebuild these patterns when we use other dic-tionaries, but we do not expect to require toomuch labor.
Generally, in Japanese the lastword in a definition sentence can be regardedas a hypernym.
Furthermore, many extractionpatterns for the EDR concept dictionary mayalso be applicable for other dictionaries.
Weare already building patterns to extract hyper-nyms from the other major Japanese dictionary,the Iwanami Kokugo Jiten, and developing theWSD system that will use them.4 Combined ModelThe details of two WSD classifiers are describedin the previous two sections: one is the SVMclassifier for high frequency words, and theother is the Naive Bayes classifier for low fre-quency words.
These two classifiers are com-bined to construct the robust WSD system.
Wedeveloped two kinds of combined models, de-scribed below in subsections 4.1 and 4.2.4.1 Simple EnsembleIn this model, the process combining the twoclassifiers is quite simple.
When only one ofclassifiers, SVM or Naive Bayes, outputs sensesfor a given word, the combined model outputssenses provided by that classifier.
When bothclassifiers output senses, the ones provided bythe SVM classifier are always chosen for the finaloutput.In the experiment in Section 5, SVM clas-sifiers were trained for words which occur morethan 20 times in the training corpus.
Therefore,the simple ensemble described here is summa-rized as follows: we use the SVM classifier forhigh frequency words those which occur morethan 20 times and the Naive Bayes classifier forthe low frequency words.4.2 Ensemble using Validation DataFirst, we prepare validation data, which is asense-tagged corpus, as common test data forthe classifiers.
The performance of the classi-fiers for a word w is evaluated by correctnessCw, defined by (8).Cw =# of words in which one of the sensesselected by a classifier is correct# of words for which a classifier selectsone or more senses(8)The main reason for combining two classifiersis to improve the recall and applicability of theWSD system.
Note that a classifier which oftenoutputs a correct sense would achieve high cor-rectness Cw, even though it also outputs wrongsenses.
Thus, the higher the Cw of a classifier,the more it improves the recall of the combinedmodel.Next, the correctness Cw of each classifier foreach word w is measured on the validation data.When two classifiers output senses for a givenword, their Cw scores are compared.
Then, theword senses provided by the better classifier areselected as the final outputs.When the number of words in the validationdata is small, comparison of the classifiers?
Cwis unreliable.
For that reason, when the numberof words in the validation data is less that a cer-tain threshold Oh, a sense output by the SVMclassifier is chosen for the final output.
This isbecause the correctness for all words in the vali-dation data is higher for the SVM classifier thanfor the Naive Bayes classifier.
In the experimentin Section 5, we set Oh to 10.5 ExperimentIn this section, we will describe the experimentto evaluate our proposed method.
We used theEDR corpus (EDR, 1995) in the experiment.
Itis made up of about 200,000 Japanese sentencesextracted from newspaper articles and maga-zines.
In the EDR corpus, each word was an-notated with a sense ID (CID).
We used 20,000sentences in the EDR corpus as the test data,20,000 sentence as the validation data, andthe remaining 161,332 sentences as the trainingdata.
The training data was used to train theSVM classifier and the Naive Bayes classifier,while the validation data was used for the com-bined model described in Subsection 4.2.
Thetarget instances used for evaluation were all am-biguous content words in the test data; the num-ber of target instances was 91,986.We evaluated three single WSD classifiers andtwo combined models:?
BLThe baseline model.
This is the WSD clas-sifier which always selects the most fre-quently used sense.
When there is morethan one sense with equally high frequency,the classifier chooses all those senses.?
NBThe Naive Bayes classifier (Section 3).?
SVMThe SVM classifier (Section 2).?
SVM+NB(simple)The combined model by simple ensemble(Subsection 4.1).?
SVM+NB(valid)The combined model using the validationdata (Subsection 4.2).Table 1 reveals the precision(P), recall(R), F-measure(F) 4, applicability(A) and number ofword types(T) of these five classifiers on the test42PRP+Rwhere P and R represent the precision and re-call, respectively.Table 1: Results of WSD ClassifiersR P F A T1) .6047 .6036 .6042 .9962 10,3102) .6274 .6543 .6406 .9568 10,5013) .6366 .7080 .6704 .8992 4,5754) .7016 .7010 .7013 .9993 10,5925) .7050 .7043 .7046 .9993 10,5921)=BL, 2)=NB, 3)=SVM,4)=SVM+NB(simple), 5)=SVM+NB(valid)data.
A(applicability) indicates the ratio of thenumber of instances disambiguated by a classi-fier to the total number of target instances; Tindicates the number of word types which couldbe disambiguated by a classifier.The two combined models outperformed theSVM classifier, for all criteria except precision.The gains in recall and applicability were espe-cially remarkable.
Notice the figures in column?T?
in Table 1: the SVM classifiers could be ap-plied only to 4,575 words, while the Naive Bayesclassifiers were applicable to 10,501 words, in-cluding low frequency words.
Thus, the ensem-ble of these two classifiers would significantlyimprove applicability and recall with little lossof precision.Comparing the performance of the two com-bined models, ?SVM+NB(validation)?
slightlyoutperformed ?SVM+NB(simple)?, but therewas no significant difference between them.
Thecorrectness, Cw, of the SVM classifier on thevalidation data was usually greater than that ofthe Naive Bayes classifier, so the SVM classifierwas preferred when both were applicable.
Thiswas the almost same strategy for the simple en-semble, and we think this was the reason whythe performance of two combined models werealmost the same.
In the rest of this section, wewill show the results for the combined modelusing the validation data only.Our goal was to improve the robustness of theWSD system.
The naive way to construct a ro-bust WSD system is to create an ensemble of asupervised learned classifier and a baseline clas-sifier.
So, we compared our proposed method(SVM+NB) with the combined model of theSVM and baseline classifier (SVM+BL).
The re-sults are shown in Table 2 and Figure 4.
Table 2shows the same criteria as in Table 1, indicatingthat ?SVM+NB?
outperformed ?SVM+BL?
forall criteria.
Figure 4 shows the relation betweenthe F-measure of the classifiers and word fre-quency in the training data.
The horizontal axisTable 2: Results of the Combined Models (1)R P F A T5) .7050 .7043 .7046 .9993 10,5926) .6977 .6976 .6977 .9962 10,3105)=SVM+NB, 6)=SVM+BL0.20.30.40.50.60.70.8-0.5 0 0.5 1 1.5 2 2.5 3 3.5 4 4.50100200300400500600700800F (SVM+NB) F (SVM+BL) NF(o) N(o)log   o10Figure 4: Results of the Combined Models (2)indicates the occurrence of words in the train-ing data (o) in log scale.
Squares and triangleswith lines indicates the F (o) of the ?SVM+NB?and ?SVM+BL?, respectively, where F (o) is themacro average of F-measures for words whichoccur o times in the training data.
The brokenline indicates N(o), the number of word typeswhich occur o times 5.
For convenience, wheno = 0, we plot F (0) and N(0) at x = ?0.5instead of ??
(= log100).
As shown in Fig-ure 4, ?SVM+NB?
significantly outperformed?SVM+BL?
for low frequency words, and thenumber of word types (N(o)) became obviouslygreater when o was small.
In other words, theNaive Bayes classifier proposed here could prob-ably handle many more low frequency wordsthan the baseline classifier.
Therefore, it wasmore effective to combine the Naive Bayes clas-sifier with the SVM classifier rather than thebaseline classifier in order to improve the ro-bustness of the overall WSD system.Finally, we constructed a combined model ofall three classifiers, the SVM, Naive Bayes andbaseline classifiers.
As shown in Table 3, thismodel slightly outperformed the two-classifiercombined models shown in Table 2.5To be more accurate, F (o) and N(o) are the figuresfor words which occurred more than or equal o times andless than o+ t times, where o+ t is the next point at thehorizontal axis.
t was chosen as the smallest integer sothat N(o) would be more than 100.Table 3: Results of SVM+NB+BLR P F A T7) .7079 .7066 .7072 1 10,6367)=SVM+NB+BL6 Related WorkAs described in Section 1, the goal of thisproject was to improve robustness of the WSDsystem.
One of the promising ways to constructa robust WSD system is unsupervised learn-ing as with the EM algorithm (Manning andSchu?tze, 1999), i.e.
training a WSD classifierfrom an unlabeled data set.
On the other hand,our approach is to use a machine readable dic-tionary in addition to a corpus as knowledgeresources for WSD.
Notice that we used hyper-nyms of definition sentences in a dictionary totrain the Naive Bayes classifier, and this pro-cess worked well for words which did not oc-cur frequently in the corpus.
However, we didnot compare our method and the unsupervisedlearning method empirically.
This will be oneof our future projects.Using hypernyms of definition sentences issimilar to using semantic classes derived from athesaurus.
One of the advantages of our methodis that a thesaurus is not obligatory when wordsenses are defined according to a machine read-able dictionary.
Furthermore, our method isto train the probabilistic model that predictsa hypernym of a word, while most previous ap-proaches use semantic classes as features (i.e.,the condition of the posterior probability inthe case of the Naive Bayes model).
In facts,we also use features associated with semanticclasses derived from the thesaurus, Csent and(Bcase;Cnoun), as described in Section 2.Several previous studies have used both acorpus and a machine readable dictionary forWSD (Litkowski, 2002; Rigau et al, 1997;Stevenson and Wilks, 2001).
The differencebetween those methods and ours is the waywe use information derived from the dictionaryfor WSD.
Training the probabilistic model thatpredicts a hypernym in a dictionary is our ownapproach.
However, these various methods arenot in competition with our method.
In fact,the robustness of the WSD system would beeven more improved by combining these meth-ods with that described in this paper.7 ConclusionThis paper has proposed a method to developa robust WSD system.
We combined a WSDclassifier obtained by supervised learning forhigh frequency words and a classifier using hy-pernyms in definition sentences in a dictionaryfor low frequency words.
Experimental resultsshowed that both recall and applicability wereremarkably improved with our method.
In fu-ture, we plan to investigate the optimum way tocombine these two classifiers or to train a singleprobabilistic model using hypernyms in defini-tion sentences, which is suitable for both highand low frequency words.ReferencesEDR.
1995.
EDR electronic dictionary technicalguide (second edition).
Technical Report TR?045,Japan Electronic Dictionary Research Institute.Satoshi Ikehara et al 1997.
Nihongo Goi Taikei (inJapanese).
Iwanami Shoten, Publishers.Hand Li and Jun-ichi Takeuchi.
1997.
Using evi-dence that is both strong and reliable in Japanesehomograph disambiguation.
In SIG-NL, Informa-tion Processing Society of Japan, pages 53?59.Kenneth C. Litkowski.
2002.
Sense informationfor disambiguation: Confluence of supervisedand unsupervised methods.
In Proceedings of theSIGLEX/SENSEVAL Workshop on Word SenseDisambiguation, pages 47?53.Christopher D. Manning and Hinrich Schu?tze, 1999.Foundations of Statistical Natural Language Pro-cessing, chapter 7.
MIT Press.Masaki Murata, Masao Utiyama, Kiyotaka Uchi-moto, Qing Ma, and Hitoshi Isahara.
2001.Japanese word sense disambiguation using thesimple bayes and support vector machine meth-ods.
In Proceedings of the SENSEVAL-2, pages135?138.Ted Pedersen.
2000.
A simple approach to build-ing ensembles of naive baysian classifiers forword sense disambiguation.
In Proceedings of theNAACL, pages 63?69.German Rigau, Jordi Atserias, and Eneko Agirre.1997.
Combining unsupervised lexical knowledgemethods for word sense disambiguation.
In Pro-ceedings of the ACL, pages 48?55.Bernhard Scho?lkopf.
2000.
New support vector al-gorithms.
Neural Computation, 12:1083?1121.Mark Stevenson and Yorick Wilks.
2001.
The inter-action of knowledge sources in word sense disam-biguation.
Computational Linguistics, 27(3):321?349.Hiroya Takamura, Hiroyasu Yamada, Taku Kudoh,Kaoru Yamamoto, and Yuji Matsumoto.
2001.Ensembling based on feature space restructuringwith application to WSD.
In Proceedings of theNLPRS, pages 41?48.
