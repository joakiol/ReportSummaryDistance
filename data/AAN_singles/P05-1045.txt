Proceedings of the 43rd Annual Meeting of the ACL, pages 363?370,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsIncorporating Non-local Information into InformationExtraction Systems by Gibbs SamplingJenny Rose Finkel, Trond Grenager, and Christopher ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{jrfinkel, grenager, mannning}@cs.stanford.eduAbstractMost current statistical natural language process-ing models use only local features so as to permitdynamic programming in inference, but this makesthem unable to fully account for the long distancestructure that is prevalent in language use.
Weshow how to solve this dilemma with Gibbs sam-pling, a simple Monte Carlo method used to per-form approximate inference in factored probabilis-tic models.
By using simulated annealing in placeof Viterbi decoding in sequence models such asHMMs, CMMs, and CRFs, it is possible to incorpo-rate non-local structure while preserving tractableinference.
We use this technique to augment anexisting CRF-based information extraction systemwith long-distance dependency models, enforcinglabel consistency and extraction template consis-tency constraints.
This technique results in an errorreduction of up to 9% over state-of-the-art systemson two established information extraction tasks.1 IntroductionMost statistical models currently used in natural lan-guage processing represent only local structure.
Al-though this constraint is critical in enabling tractablemodel inference, it is a key limitation in many tasks,since natural language contains a great deal of non-local structure.
A general method for solving thisproblem is to relax the requirement of exact infer-ence, substituting approximate inference algorithmsinstead, thereby permitting tractable inference inmodels with non-local structure.
One such algo-rithm is Gibbs sampling, a simple Monte Carlo algo-rithm that is appropriate for inference in any factoredprobabilistic model, including sequence models andprobabilistic context free grammars (Geman and Ge-man, 1984).
Although Gibbs sampling is widelyused elsewhere, there has been extremely little useof it in natural language processing.1 Here, we useit to add non-local dependencies to sequence modelsfor information extraction.Statistical hidden state sequence models, suchas Hidden Markov Models (HMMs) (Leek, 1997;Freitag and McCallum, 1999), Conditional MarkovModels (CMMs) (Borthwick, 1999), and Condi-tional Random Fields (CRFs) (Lafferty et al, 2001)are a prominent recent approach to information ex-traction tasks.
These models all encode the Markovproperty: decisions about the state at a particular po-sition in the sequence can depend only on a small lo-cal window.
It is this property which allows tractablecomputation: the Viterbi, Forward Backward, andClique Calibration algorithms all become intractablewithout it.However, information extraction tasks can benefitfrom modeling non-local structure.
As an example,several authors (see Section 8) mention the value ofenforcing label consistency in named entity recogni-tion (NER) tasks.
In the example given in Figure 1,the second occurrence of the token Tanjug is mis-labeled by our CRF-based statistical NER system,because by looking only at local evidence it is un-clear whether it is a person or organization.
The firstoccurrence of Tanjug provides ample evidence thatit is an organization, however, and by enforcing la-bel consistency the system should be able to get itright.
We show how to incorporate constraints ofthis form into a CRF model by using Gibbs sam-pling instead of the Viterbi algorithm as our infer-ence procedure, and demonstrate that this techniqueyields significant improvements on two establishedIE tasks.1Prior uses in NLP of which we are aware include: Kim etal.
(1995), Della Pietra et al (1997) and Abney (1997).363the news agency Tanjug reported .
.
.
airport , Tanjug said .Figure 1: An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset.2 Gibbs Sampling for Inference inSequence ModelsIn hidden state sequence models such as HMMs,CMMs, and CRFs, it is standard to use the Viterbialgorithm, a dynamic programming algorithm, to in-fer the most likely hidden state sequence given theinput and the model (see, e.g., Rabiner (1989)).
Al-though this is the only tractable method for exactcomputation, there are other methods for comput-ing an approximate solution.
Monte Carlo methodsare a simple and effective class of methods for ap-proximate inference based on sampling.
Imaginewe have a hidden state sequence model which de-fines a probability distribution over state sequencesconditioned on any given input.
With such a modelM we should be able to compute the conditionalprobability PM (s|o) of any state sequence s ={s0, .
.
.
, sN} given some observed input sequenceo = {o0, .
.
.
, oN}.
One can then sample se-quences from the conditional distribution defined bythe model.
These samples are likely to be in highprobability areas, increasing our chances of findingthe maximum.
The challenge is how to sample se-quences efficiently from the conditional distributiondefined by the model.Gibbs sampling provides a clever solution (Ge-man and Geman, 1984).
Gibbs sampling defines aMarkov chain in the space of possible variable as-signments (in this case, hidden state sequences) suchthat the stationary distribution of the Markov chainis the joint distribution over the variables.
Thus itis called a Markov Chain Monte Carlo (MCMC)method; see Andrieu et al (2003) for a good MCMCtutorial.
In practical terms, this means that wecan walk the Markov chain, occasionally outputtingsamples, and that these samples are guaranteed tobe drawn from the target distribution.
Furthermore,the chain is defined in very simple terms: from eachstate sequence we can only transition to a state se-quence obtained by changing the state at any oneposition i, and the distribution over these possibletransitions is justPG(s(t)|s(t?1)) = PM (s(t)i |s(t?1)?i ,o).
(1)where s?i is all states except si.
In other words, thetransition probability of the Markov chain is the con-ditional distribution of the label at the position giventhe rest of the sequence.
This quantity is easy tocompute in any Markov sequence model, includingHMMs, CMMs, and CRFs.
One easy way to walkthe Markov chain is to loop through the positions ifrom 1 to N , and for each one, to resample the hid-den state at that position from the distribution givenin Equation 1.
By outputting complete sequencesat regular intervals (such as after resampling all Npositions), we can sample sequences from the con-ditional distribution defined by the model.This is still a gravely inefficient process, how-ever.
Random sampling may be a good way to es-timate the shape of a probability distribution, but itis not an efficient way to do what we want: findthe maximum.
However, we cannot just transi-tion greedily to higher probability sequences at eachstep, because the space is extremely non-convex.
Wecan, however, borrow a technique from the studyof non-convex optimization and use simulated an-nealing (Kirkpatrick et al, 1983).
Geman and Ge-man (1984) show that it is easy to modify a GibbsMarkov chain to do annealing; at time t we replacethe distribution in (1) withPA(s(t)|s(t?1)) =PM (s(t)i |s(t?1)?i ,o)1/ct?j PM (s(t)j |s(t?1)?j ,o)1/ct(2)where c = {c0, .
.
.
, cT } defines a cooling schedule.At each step, we raise each value in the conditionaldistribution to an exponent and renormalize beforesampling from it.
Note that when c = 1 the distri-bution is unchanged, and as c ?
0 the distribution364Inference CoNLL SeminarsViterbi 85.51 91.85Gibbs 85.54 91.85Sampling 85.51 91.8585.49 91.8585.51 91.8585.51 91.8585.51 91.8585.51 91.8585.51 91.8585.51 91.86Mean 85.51 91.85Std.
Dev.
0.01 0.004Table 1: An illustration of the effectiveness of Gibbs sampling,compared to Viterbi inference, for the two tasks addressed inthis paper: the CoNLL named entity recognition task, and theCMU Seminar Announcements information extraction task.
Weshow 10 runs of Gibbs sampling in the same CRF model thatwas used for Viterbi.
For each run the sampler was initializedto a random sequence, and used a linear annealing schedule thatsampled the complete sequence 1000 times.
CoNLL perfor-mance is measured as per-entity F1, and CMU Seminar An-nouncements performance is measured as per-token F1.becomes sharper, and when c = 0 the distributionplaces all of its mass on the maximal outcome, hav-ing the effect that the Markov chain always climbsuphill.
Thus if we gradually decrease c from 1 to0, the Markov chain increasingly tends to go up-hill.
This annealing technique has been shown tobe an effective technique for stochastic optimization(Laarhoven and Arts, 1987).To verify the effectiveness of Gibbs sampling andsimulated annealing as an inference technique forhidden state sequence models, we compare Gibbsand Viterbi inference methods for a basic CRF, with-out the addition of any non-local model.
The results,given in Table 1, show that if the Gibbs sampler isrun long enough, its accuracy is the same as a Viterbidecoder.3 A Conditional Random Field ModelOur basic CRF model follows that of Lafferty et al(2001).
We choose a CRF because it represents thestate of the art in sequence modeling, allowing bothdiscriminative training and the bi-directional flow ofprobabilistic information across the sequence.
ACRF is a conditional sequence model which rep-resents the probability of a hidden state sequencegiven some observations.
In order to facilitate ob-taining the conditional probabilities we need forGibbs sampling, we generalize the CRF model in aFeature NER TFCurrent Word X XPrevious Word X XNext Word X XCurrent Word Character n-gram all length ?
6Current POS Tag XSurrounding POS Tag Sequence XCurrent Word Shape X XSurrounding Word Shape Sequence X XPresence of Word in Left Window size 4 size 9Presence of Word in Right Window size 4 size 9Table 2: Features used by the CRF for the two tasks: namedentity recognition (NER) and template filling (TF).way that is consistent with the Markov Network lit-erature (see Cowell et al (1999)): we create a linearchain of cliques, where each clique, c, represents theprobabilistic relationship between an adjacent pairof states2 using a clique potential ?c, which is justa table containing a value for each possible state as-signment.
The table is not a true probability distribu-tion, as it only accounts for local interactions withinthe clique.
The clique potentials themselves are de-fined in terms of exponential models conditioned onfeatures of the observation sequence, and must beinstantiated for each new observation sequence.
Thesequence of potentials in the clique chain then de-fines the probability of a state sequence (given theobservation sequence) asPCRF(s|o) ?N?i=1?i(si?1, si) (3)where ?i(si?1, si) is the element of the clique po-tential at position i corresponding to states si?1 andsi.3Although a full treatment of CRF training is be-yond the scope of this paper (our technique assumesthe model is already trained), we list the featuresused by our CRF for the two tasks we address inTable 2.
During training, we regularized our expo-nential models with a quadratic prior and used thequasi-Newton method for parameter optimization.As is customary, we used the Viterbi algorithm toinfer the most likely state sequence in a CRF.2CRFs with larger cliques are also possible, in which casethe potentials represent the relationship between a subsequenceof k adjacent states, and contain |S|k elements.3To handle the start condition properly, imagine also that wedefine a distinguished start state s0.365The clique potentials of the CRF, instantiated forsome observation sequence, can be used to easilycompute the conditional distribution over states ata position given in Equation 1.
Recall that at posi-tion i we want to condition on the states in the restof the sequence.
The state at this position can beinfluenced by any other state that it shares a cliquewith; in particular, when the clique size is 2, thereare 2 such cliques.
In this case the Markov blanketof the state (the minimal set of states that rendersa state conditionally independent of all other states)consists of the two neighboring states and the obser-vation sequence, all of which are observed.
The con-ditional distribution at position i can then be com-puted simply asPCRF(si|s?i,o) ?
?i(si?1, si)?i+1(si, si+1) (4)where the factor tables F in the clique chain are al-ready conditioned on the observation sequence.4 Datasets and EvaluationWe test the effectiveness of our technique on two es-tablished datasets: the CoNLL 2003 English namedentity recognition dataset, and the CMU SeminarAnnouncements information extraction dataset.4.1 The CoNLL NER TaskThis dataset was created for the shared task of theSeventh Conference on Computational Natural Lan-guage Learning (CoNLL),4 which concerned namedentity recognition.
The English data is a collectionof Reuters newswire articles annotated with four en-tity types: person (PER), location (LOC), organi-zation (ORG), and miscellaneous (MISC).
The datais separated into a training set, a development set(testa), and a test set (testb).
The training set con-tains 945 documents, and approximately 203,000 to-kens.
The development set has 216 documents andapproximately 51,000 tokens, and the test set has231 documents and approximately 46,000 tokens.We evaluate performance on this task in the man-ner dictated by the competition so that results can beproperly compared.
Precision and recall are evalu-ated on a per-entity basis (and combined into an F1score).
There is no partial credit; an incorrect entity4Available at http://cnts.uia.ac.be/conll2003/ner/.boundary is penalized as both a false positive and asa false negative.4.2 The CMU Seminar Announcements TaskThis dataset was developed as part of Dayne Fre-itag?s dissertation research Freitag (1998).5 It con-sists of 485 emails containing seminar announce-ments at Carnegie Mellon University.
It is annotatedfor four fields: speaker, location, start time, and endtime.
Sutton and McCallum (2004) used 5-fold crossvalidation when evaluating on this dataset, so we ob-tained and used their data splits, so that results canbe properly compared.
Because the entire dataset isused for testing, there is no development set.
Wealso used their evaluation metric, which is slightlydifferent from the method for CoNLL data.
Insteadof evaluating precision and recall on a per-entity ba-sis, they are evaluated on a per-token basis.
Then, tocalculate the overall F1 score, the F1 scores for eachclass are averaged.5 Models of Non-local StructureOur models of non-local structure are themselvesjust sequence models, defining a probability distri-bution over all possible state sequences.
It is pos-sible to flexibly model various forms of constraintsin a way that is sensitive to the linguistic structureof the data (e.g., one can go beyond imposing justexact identity conditions).
One could imagine manyways of defining such models; for simplicity we usethe formPM (s|o) ??????#(?,s,o)?
(5)where the product is over a set of violation types ?,and for each violation type ?
we specify a penaltyparameter ??.
The exponent #(?, s,o) is the countof the number of times that the violation ?
occursin the state sequence s with respect to the observa-tion sequence o.
This has the effect of assigningsequences with more violations a lower probabil-ity.
The particular violation types are defined specif-ically for each task, and are described in the follow-ing two sections.This model, as defined above, is not normalized,and clearly it would be expensive to do so.
This5Available at http://nlp.shef.ac.uk/dot.kom/resources.html.366PER LOC ORG MISCPER 3141 4 5 0LOC 6436 188 3ORG 2975 0MISC 2030Table 3: Counts of the number of times multiple occurrences ofa token sequence is labeled as different entity types in the samedocument.
Taken from the CoNLL training set.PER LOC ORG MISCPER 1941 5 2 3LOC 0 167 6 63ORG 22 328 819 191MISC 14 224 7 365Table 4: Counts of the number of times an entity sequence islabeled differently from an occurrence of a subsequence of itelsewhere in the document.
Rows correspond to sequences, andcolumns to subsequences.
Taken from the CoNLL training set.doesn?t matter, however, because we only use themodel for Gibbs sampling, and so only need to com-pute the conditional distribution at a single positioni (as defined in Equation 1).
One (inefficient) wayto compute this quantity is to enumerate all possi-ble sequences differing only at position i, computethe score assigned to each by the model, and renor-malize.
Although it seems expensive, this compu-tation can be made very efficient with a straightfor-ward memoization technique: at all times we main-tain data structures representing the relationship be-tween entity labels and token sequences, from whichwe can quickly compute counts of different types ofviolations.5.1 CoNLL Consistency ModelLabel consistency structure derives from the fact thatwithin a particular document, different occurrencesof a particular token sequence are unlikely to be la-beled as different entity types.
Although any oneoccurrence may be ambiguous, it is unlikely that allinstances are unclear when taken together.The CoNLL training data empirically supports thestrength of the label consistency constraint.
Table 3shows the counts of entity labels for each pair ofidentical token sequences within a document, whereboth are labeled as an entity.
Note that inconsis-tent labelings are very rare.6 In addition, we also6A notable exception is the labeling of the same text as bothorganization and location within the same document.
This is aconsequence of the large portion of sports news in the CoNLLwant to model subsequence constraints: having seenGeoff Woods earlier in a document as a person isa good indicator that a subsequent occurrence ofWoods should also be labeled as a person.
How-ever, if we examine all cases of the labelings ofother occurrences of subsequences of a labeled en-tity, we find that the consistency constraint does nothold nearly so strictly in this case.
As an exam-ple, one document contains references to both TheChina Daily, a newspaper, and China, the country.Counts of subsequence labelings within a documentare listed in Table 4.
Note that there are many off-diagonal entries: the China Daily case is the mostcommon, occurring 328 times in the dataset.The penalties used in the long distance constraintmodel for CoNLL are the Empirical Bayes estimatestaken directly from the data (Tables 3 and 4), exceptthat we change counts of 0 to be 1, so that the dis-tribution remains positive.
So the estimate of a PERalso being an ORG is 53151 ; there were 5 instance ofan entity being labeled as both, PER appeared 3150times in the data, and we add 1 to this for smoothing,because PER-MISC never occured.
However, whenwe have a phrase labeled differently in two differ-ent places, continuing with the PER-ORG example,it is unclear if we should penalize it as PER that isalso an ORG or an ORG that is also a PER.
To dealwith this, we multiply the square roots of each esti-mate together to form the penalty term.
The penaltyterm is then multiplied in a number of times equalto the length of the offending entity; this is meant to?encourage?
the entity to shrink.7 For example, saywe have a document with three entities, Rotor Vol-gograd twice, once labeled as PER and once as ORG,and Rotor, labeled as an ORG.
The likelihood of aPER also being an ORG is 53151 , and of an ORG alsobeing a PER is 53169 , so the penalty for this violationis (?53151 ?
?53151 )2.
The likelihood of a ORG be-ing a subphrase of a PER is 2842 .
So the total penaltywould be 53151 ?
53169 ?
2842 .dataset, so that city names are often also team names.7While there is no theoretical justification for this, we foundit to work well in practice.3675.2 CMU Seminar AnnouncementsConsistency ModelDue to the lack of a development set, our consis-tency model for the CMU Seminar Announcementsis much simpler than the CoNLL model, the num-bers where selected due to our intuitions, and we didnot spend much time hand optimizing the model.Specifically, we had three constraints.
The first isthat all entities labeled as start time are normal-ized, and are penalized if they are inconsistent.
Thesecond is a corresponding constraint for end times.The last constraint attempts to consistently label thespeakers.
If a phrase is labeled as a speaker, we as-sume that the last word is the speaker?s last name,and we penalize for each occurrance of that wordwhich is not also labeled speaker.
For the start andend times the penalty is multiplied in based on howmany words are in the entity.
For the speaker, thepenalty is only multiplied in once.
We used a handselected penalty of exp?4.0.6 Combining Sequence ModelsIn the previous section we defined two models ofnon-local structure.
Now we would like to incor-porate them into the local model (in our case, thetrained CRF), and use Gibbs sampling to find themost likely state sequence.
Because both the trainedCRF and the non-local models are themselves se-quence models, we simply combine the two mod-els into a factored sequence model of the followingformPF (s|o) ?
PM (s|o)PL(s|o) (6)where M is the local CRF model, L is the new non-local model, and F is the factored model.8 In thisform, the probability again looks difficult to com-pute (because of the normalizing factor, a sum overall hidden state sequences of length N ).
However,since we are only using the model for Gibbs sam-pling, we never need to compute the distribution ex-plicitly.
Instead, we need only the conditional prob-ability of each position in the sequence, which canbe computed asPF (si|s?i,o) ?
PM (si|s?i,o)PL(si|s?i,o).
(7)8This model double-generates the state sequence condi-tioned on the observations.
In practice we don?t find this tobe a problem.CoNLLApproach LOC ORG MISC PER ALLB&M LT-RMN ?
?
?
?
80.09B&M GLT-RMN ?
?
?
?
82.30Local+Viterbi 88.16 80.83 78.51 90.36 85.51NonLoc+Gibbs 88.51 81.72 80.43 92.29 86.86Table 5: F1 scores of the local CRF and non-local models on theCoNLL 2003 named entity recognition dataset.
We also providethe results from Bunescu and Mooney (2004) for comparison.CMU Seminar AnnouncementsApproach STIME ETIME SPEAK LOC ALLS&M CRF 97.5 97.5 88.3 77.3 90.2S&M Skip-CRF 96.7 97.2 88.1 80.4 90.6Local+Viterbi 96.67 97.36 83.39 89.98 91.85NonLoc+Gibbs 97.11 97.89 84.16 90.00 92.29Table 6: F1 scores of the local CRF and non-local models onthe CMU Seminar Announcements dataset.
We also providethe results from Sutton and McCallum (2004) for comparison.At inference time, we then sample from the Markovchain defined by this transition probability.7 Results and DiscussionIn our experiments we compare the impact of addingthe non-local models with Gibbs sampling to ourbaseline CRF implementation.
In the CoNLL namedentity recognition task, the non-local models in-crease the F1 accuracy by about 1.3%.
Althoughsuch gains may appear modest, note that they areachieved relative to a near state-of-the-art NER sys-tem: the winner of the CoNLL English task reportedan F1 score of 88.76.
In contrast, the increases pub-lished by Bunescu and Mooney (2004) are relativeto a baseline system which scores only 80.9% onthe same task.
Our performance is similar on theCMU Seminar Announcements dataset.
We showthe per-field F1 results that were reported by Suttonand McCallum (2004) for comparison, and note thatwe are again achieving gains against a more compet-itive baseline system.For all experiments involving Gibbs sampling, weused a linear cooling schedule.
For the CoNLLdataset we collected 200 samples per trial, and forthe CMU Seminar Announcements we collected 100samples.
We report the average of all trials, and in allcases we outperform the baseline with greater than95% confidence, using the standard t-test.
The trialshad low standard deviations - 0.083% and 0.007% -and high minimun F-scores - 86.72%, and 92.28%368- for the CoNLL and CMU Seminar Announce-ments respectively, demonstrating the stability ofour method.The biggest drawback to our model is the com-putational cost.
Taking 100 samples dramaticallyincreases test time.
Averaged over 3 runs on bothViterbi and Gibbs, CoNLL testing time increasedfrom 55 to 1738 seconds, and CMU Seminar An-nouncements testing time increases from 189 to6436 seconds.8 Related WorkSeveral authors have successfully incorporated alabel consistency constraint into probabilistic se-quence model named entity recognition systems.Mikheev et al (1999) and Finkel et al (2004) in-corporate label consistency information by using ad-hoc multi-stage labeling procedures that are effec-tive but special-purpose.
Malouf (2002) and Curranand Clark (2003) condition the label of a token ata particular position on the label of the most recentprevious instance of that same token in a prior sen-tence of the same document.
Note that this violatesthe Markov property, but is achieved by slightly re-laxing the requirement of exact inference.
Insteadof finding the maximum likelihood sequence overthe entire document, they classify one sentence at atime, allowing them to condition on the maximumlikelihood sequence of previous sentences.
This ap-proach is quite effective for enforcing label consis-tency in many NLP tasks, however, it permits a for-ward flow of information only, which is not suffi-cient for all cases of interest.
Chieu and Ng (2002)propose a solution to this problem: for each to-ken, they define additional features taken from otheroccurrences of the same token in the document.This approach has the added advantage of allowingthe training procedure to automatically learn goodweightings for these ?global?
features relative to thelocal ones.
However, this approach cannot easilybe extended to incorporate other types of non-localstructure.The most relevant prior works are Bunescu andMooney (2004), who use a Relational Markov Net-work (RMN) (Taskar et al, 2002) to explicitly mod-els long-distance dependencies, and Sutton and Mc-Callum (2004), who introduce skip-chain CRFs,which maintain the underlying CRF sequence model(which (Bunescu and Mooney, 2004) lack) whileadding skip edges between distant nodes.
Unfortu-nately, in the RMN model, the dependencies mustbe defined in the model structure before doing anyinference, and so the authors use crude heuristicpart-of-speech patterns, and then add dependenciesbetween these text spans using clique templates.This generates a extremely large number of over-lapping candidate entities, which then necessitatesadditional templates to enforce the constraint thattext subsequences cannot both be different entities,something that is more naturally modeled by a CRF.Another disadvantage of this approach is that it usesloopy belief propagation and a voted perceptron forapproximate learning and inference ?
ill-foundedand inherently unstable algorithms which are notedby the authors to have caused convergence prob-lems.
In the skip-chain CRFs model, the decisionof which nodes to connect is also made heuristi-cally, and because the authors focus on named entityrecognition, they chose to connect all pairs of identi-cal capitalized words.
They also utilize loopy beliefpropagation for approximate learning and inference.While the technique we propose is similar math-ematically and in spirit to the above approaches, itdiffers in some important ways.
Our model is im-plemented by adding additional constraints into themodel at inference time, and does not require thepreprocessing step necessary in the two previouslymentioned works.
This allows for a broader class oflong-distance dependencies, because we do not needto make any initial assumptions about which nodesshould be connected, and is helpful when you wishto model relationships between nodes which are thesame class, but may not be similar in any other way.For instance, in the CMU Seminar Announcementsdataset, we can normalize all entities labeled as astart time and penalize the model if multiple, non-consistent times are labeled.
This type of constraintcannot be modeled in an RMN or a skip-CRF, be-cause it requires the knowledge that both entities aregiven the same class label.We also allow dependencies between multi-wordphrases, and not just single words.
Additionally,our model can be applied on top of a pre-existingtrained sequence model.
As such, our method doesnot require complex training procedures, and can369instead leverage all of the established methods fortraining high accuracy sequence models.
It can in-deed be used in conjunction with any statistical hid-den state sequence model: HMMs, CMMs, CRFs, oreven heuristic models.
Third, our technique employsGibbs sampling for approximate inference, a simpleand probabilistically well-founded algorithm.
As aconsequence of these differences, our approach iseasier to understand, implement, and adapt to newapplications.9 ConclusionsWe have shown that a constraint model can be effec-tively combined with an existing sequence model ina factored architecture to successfully impose var-ious sorts of long distance constraints.
Our modelgeneralizes naturally to other statistical models andother tasks.
In particular, it could in the futurebe applied to statistical parsing.
Statistical contextfree grammars provide another example of statisticalmodels which are restricted to limiting local struc-ture, and which could benefit from modeling non-local structure.AcknowledgementsThis work was supported in part by the AdvancedResearchand Development Activity (ARDA)?sAdvanced Question Answeringfor Intelligence(AQUAINT) Program.
Additionally, we would liketo that our reviewers for their helpful comments.ReferencesS.
Abney.
1997.
Stochastic attribute-value grammars.
Compu-tational Linguistics, 23:597?618.C.
Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.
2003.An introduction to MCMC for machine learning.
MachineLearning, 50:5?43.A.
Borthwick.
1999.
A Maximum Entropy Approach to NamedEntity Recognition.
Ph.D. thesis, New York University.R.
Bunescu and R. J. Mooney.
2004.
Collective informationextraction with relational Markov networks.
In Proceedingsof the 42nd ACL, pages 439?446.H.
L. Chieu and H. T. Ng.
2002.
Named entity recognition:a maximum entropy approach using global information.
InProceedings of the 19th Coling, pages 190?196.R.
G. Cowell, A. Philip Dawid, S. L. Lauritzen, and D. J.Spiegelhalter.
1999.
Probabilistic Networks and Expert Sys-tems.
Springer-Verlag, New York.J.
R. Curran and S. Clark.
2003.
Language independent NERusing a maximum entropy tagger.
In Proceedings of the 7thCoNLL, pages 164?167.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
1997.
Induc-ing features of random fields.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 19:380?393.J.
Finkel, S. Dingare, H. Nguyen, M. Nissim, and C. D. Man-ning.
2004.
Exploiting context for biomedical entity recog-nition: from syntax to the web.
In Joint Workshop on NaturalLanguage Processing in Biomedicine and Its Applications atColing 2004.D.
Freitag and A. McCallum.
1999.
Information extractionwith HMMs and shrinkage.
In Proceedings of the AAAI-99Workshop on Machine Learning for Information Extraction.D.
Freitag.
1998.
Machine learning for information extractionin informal domains.
Ph.D. thesis, Carnegie Mellon Univer-sity.S.
Geman and D. Geman.
1984.
Stochastic relaxation, Gibbsdistributions, and the Bayesian restoration of images.
IEEETransitions on Pattern Analysis and Machine Intelligence,6:721?741.M.
Kim, Y. S. Han, and K. Choi.
1995.
Collocation mapfor overcoming data sparseness.
In Proceedings of the 7thEACL, pages 53?59.S.
Kirkpatrick, C. D. Gelatt, and M. P. Vecchi.
1983.
Optimiza-tion by simulated annealing.
Science, 220:671?680.P.
J.
Van Laarhoven and E. H. L. Arts.
1987.
Simulated Anneal-ing: Theory and Applications.
Reidel Publishers.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
ConditionalRandom Fields: Probabilistic models for segmenting andlabeling sequence data.
In Proceedings of the 18th ICML,pages 282?289.
Morgan Kaufmann, San Francisco, CA.T.
R. Leek.
1997.
Information extraction using hidden Markovmodels.
Master?s thesis, U.C.
San Diego.R.
Malouf.
2002.
Markov models for language-independentnamed entity recognition.
In Proceedings of the 6th CoNLL,pages 187?190.A.
Mikheev, M. Moens, and C. Grover.
1999.
Named entityrecognition without gazetteers.
In Proceedings of the 9thEACL, pages 1?8.L.
R. Rabiner.
1989.
A tutorial on Hidden Markov Models andselected applications in speech recognition.
Proceedings ofthe IEEE, 77(2):257?286.C.
Sutton and A. McCallum.
2004.
Collective segmentationand labeling of distant entities in information extraction.
InICML Workshop on Statistical Relational Learning and Itsconnections to Other Fields.B.
Taskar, P. Abbeel, and D. Koller.
2002.
Discriminativeprobabilistic models for relational data.
In Proceedings ofthe 18th Conference on Uncertianty in Artificial Intelligence(UAI-02), pages 485?494, Edmonton, Canada.370
