Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35?40,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsNatural Language Models for Predicting Programming CommentsDana Movshovitz-AttiasComputer Science DepartmentCarnegie Mellon Universitydma@cs.cmu.eduWilliam W. CohenComputer Science DepartmentCarnegie Mellon Universitywcohen@cs.cmu.eduAbstractStatistical language models have success-fully been used to describe and analyzenatural language documents.
Recent workapplying language models to program-ming languages is focused on the taskof predicting code, while mainly ignoringthe prediction of programmer comments.In this work, we predict comments fromJAVA source files of open source projects,using topic models and n-grams, and weanalyze the performance of the modelsgiven varying amounts of background dataon the project being predicted.
We evalu-ate models on their comment-completioncapability in a setting similar to code-completion tools built into standard codeeditors, and show that using a commentcompletion tool can save up to 47% of thecomment typing.1 Introduction and Related WorkStatistical language models have traditionallybeen used to describe and analyze natural lan-guage documents.
Recently, software engineer-ing researchers have adopted the use of languagemodels for modeling software code.
Hindle et al(2012) observe that, as code is created by humansit is likely to be repetitive and predictable, similarto natural language.
NLP models have thus beenused for a variety of software development taskssuch as code token completion (Han et al, 2009;Jacob and Tairas, 2010), analysis of names in code(Lawrie et al, 2006; Binkley et al, 2011) and min-ing software repositories (Gabel and Su, 2008).An important part of software programming andmaintenance lies in documentation, which maycome in the form of tutorials describing the code,or inline comments provided by the programmer.The documentation provides a high level descrip-tion of the task performed by the code, and mayinclude examples of use-cases for specific codesegments or identifiers such as classes, methodsand variables.
Well documented code is easier toread and maintain in the long-run but writing com-ments is a laborious task that is often overlookedor at least postponed by many programmers.Code commenting not only provides a summa-rization of the conceptual idea behind the code(Sridhara et al, 2010), but can also be viewed as aform of document expansion where the commentcontains significant terms relevant to the describedcode.
Accurately predicted comment words cantherefore be used for a variety of linguistic usesincluding improved search over code bases usingnatural language queries, code categorization, andlocating parts of the code that are relevant to a spe-cific topic or idea (Tseng and Juang, 2003; Wan etal., 2007; Kumar and Carterette, 2013; Shepherdet al, 2007; Rastkar et al, 2011).
A related andwell studied NLP task is that of predicting naturallanguage caption and commentary for images andvideos (Blei and Jordan, 2003; Feng and Lapata,2010; Feng and Lapata, 2013; Wu and Li, 2011).In this work, our goal is to apply statistical lan-guage models for predicting class comments.
Weshow that n-gram models are extremely success-ful in this task, and can lead to a saving of upto 47% in comment typing.
This is expected asn-grams have been shown as a strong model forlanguage and speech prediction that is hard to im-prove upon (Rosenfeld, 2000).
In some cases how-ever, for example in a document expansion task,we wish to extract important terms relevant to thecode regardless of local syntactic dependencies.We hence also evaluate the use of LDA (Blei et al,2003) and link-LDA (Erosheva et al, 2004) topicmodels, which are more relevant for the term ex-traction scenario.
We find that the topic model per-formance can be improved by distinguishing codeand text tokens in the code.352 Method2.1 ModelsWe train n-gram models (n = 1, 2, 3) over sourcecode documents containing sequences of com-bined code and text tokens from multiple trainingdatasets (described below).
We use the BerkeleyLanguage Model package (Pauls and Klein, 2011)with absolute discounting (Kneser-Ney smooth-ing; (1995)) which includes a backoff strategy tolower-order n-grams.
Next, we use LDA topicmodels (Blei et al, 2003) trained on the same data,with 1, 5, 10 and 20 topics.
The joint distributionof a topic mixture ?, and a set of N topics z, fora single source code document with N observedword tokens, d = {wi}Ni=1, given the Dirichlet pa-rameters ?
and ?, is thereforep(?, z, w|?, ?)
= (1)p(?|?)?wp(z|?
)p(w|z, ?
)Under the models described so far, there is no dis-tinction between text and code tokens.Finally, we consider documents as having amixed membership of two entity types, code andtext tokens, d = ({wcodei }Cni=1, {wtexti }Tni=1), wherethe text words are tokens from comment andstring literals, and the code words include the pro-gramming language syntax tokens (e.g., public,private, for, etc? )
and all identifiers.
In thiscase, we train link-LDA models (Erosheva et al,2004) with 1, 5, 10 and 20 topics.
Under the link-LDA model, the mixed-membership joint distribu-tion of a topic mixture, words and topics is thenp(?, z, w|?, ?)
= p(?|?)?
(2)?wtextp(ztext|?
)p(wtext|ztext, ?)??wcodep(zcode|?
)p(wcode|zcode, ?
)where ?
is the joint topic distribution, w is the setof observed document words, ztext is a topic asso-ciated with a text word, and zcode a topic associ-ated with a code word.The LDA and link-LDA models use Gibbs sam-pling (Griffiths and Steyvers, 2004) for topic infer-ence, based on the implementation of Balasubra-manyan and Cohen (2011) with single or multipleentities per document, respectively.2.2 Testing MethodologyOur goal is to predict the tokens of the JAVA classcomment (the one preceding the class definition)in each of the test files.
Each of the models de-scribed above assigns a probability to the nextcomment token.
In the case of n-grams, the prob-ability of a token word wi is given by consideringprevious words p(wi|wi?1, .
.
.
, w0).
This proba-bility is estimated given the previous n?
1 tokensas p(wi|wi?1, .
.
.
, wi?
(n?1)).For the topic models, we separate the docu-ment tokens into the class definition and the com-ment we wish to predict.
The set of tokens ofthe class comment wc, are all considered as texttokens.
The rest of the tokens in the documentwr, are considered to be the class definition, andthey may contain both code and text tokens (fromstring literals and other comments in the sourcefile).
We then compute the posterior probabilityof document topics by solving the following infer-ence problem conditioned on the wr tokensp(?, zr|wr, ?, ?)
= p(?, zr, wr|?, ?
)p(wr|?, ?)
(3)This gives us an estimate of the document distri-bution, ?, with which we infer the probability ofthe comment tokens asp(wc|?, ?)
=?zp(wc|z, ?)p(z|?)
(4)Following Blei et al (2003), for the caseof a single entity LDA, the inference problemfrom equation (3) can be solved by consideringp(?, z, w|?, ?
), as in equation (1), and by takingthe marginal distribution of the document tokensas a continuous mixture distribution for the setw = wr, by integrating over ?
and summing overthe set of topics zp(w|?, ?)
=?p(?|?)?
(5)(?w?zp(z|?
)p(w|z, ?
))d?For the case of link-LDA where the document iscomprised of two entities, in our case code to-kens and text tokens, we can consider the mixed-membership joint distribution ?, as in equation (2),and similarly the marginal distribution p(w|?, ?
)over both code and text tokens from wr.
Sincecomment words in wc are all considered as texttokens they are sampled using text topics, namelyztext, in equation (4).363 Experimental Settings3.1 Data and Training MethodologyWe use source code from nine open source JAVAprojects: Ant, Cassandra, Log4j, Maven, Minor-Third, Batik, Lucene, Xalan and Xerces.
For eachproject, we divide the source files into a trainingand testing dataset.
Then, for each project in turn,we consider the following three main training sce-narios, leading to using three training datasets.To emulate a scenario in which we are predict-ing comments in the middle of project develop-ment, we can use data (documented code) from thesame project.
In this case, we use the in-projecttraining dataset (IN).
Alternatively, if we train acomment prediction model at the beginning of thedevelopment, we need to use source files fromother, possibly related projects.
To analyze thisscenario, for each of the projects above we trainmodels using an out-of-project dataset (OUT) con-taining data from the other eight projects.Typically, source code files contain a greateramount of code versus comment text.
Since we areinterested in predicting comments, we consider athird training data source which contains more En-glish text as well as some code segments.
We usedata from the popular Q&A website StackOver-flow (SO) where users ask and answer technicalquestions about software development, tools, al-gorithms, etc?.
We downloaded a dataset of all ac-tions performed on the site since it was launched inAugust 2008 until August 2012.
The data includes3,453,742 questions and 6,858,133 answers postedby 1,295,620 users.
We used only posts that aretagged as JAVA related questions and answers.All the models for each project are then testedon the testing set of that project.
We report resultsaveraged over all projects in Table 1.Source files were tokenized using the EclipseJDT compiler tools, separating code tokens andidentifiers.
Identifier names (of classes, methodsand variables), were further tokenized by camelcase notation (e.g., ?minMargin?
was converted to?min margin?).
Non alpha-numeric tokens (e.g.,dot, semicolon) were discarded from the code, aswell as numeric and single character literals.
Textfrom comments or any string literals within thecode were further tokenized with the Mallet sta-tistical natural language processing package (Mc-Callum, 2002).
Posts from SO were parsed usingthe Apache Tika toolkit1 and then tokenized withthe Mallet package.
We considered as raw codetokens anything labeled using a <code> markup(as indicated by the SO users who wrote the post).3.2 EvaluationSince our models are trained using various datasources the vocabularies used by each of them aredifferent, making the comment likelihood given byeach model incomparable due to different sets ofout-of-vocabulary tokens.
We thus evaluate mod-els using a character saving metric which aims atquantifying the percentage of characters that canbe saved by using the model in a word-completionsettings, similar to standard code completion toolsbuilt into code editors.
For a comment word withn characters, w = w1, .
.
.
, wn, we predict the twomost likely words given each model filtered by thefirst 0, .
.
.
, n characters ofw.
Let k be the minimalki for which w is in the top two predicted word to-kens where tokens are filtered by the first ki char-acters.
Then, the number of saved characters for wis n?
k. In Table 1 we report the average percent-age of saved characters per comment using each ofthe above models.
The final results are also aver-aged over the nine input projects.
As an example,in the predicted comment shown in Table 2, takenfrom the project Minor-Third, the token entity isthe most likely token according to the model SOtrigram, out of tokens starting with the prefix ?en?.The saved characters in this case are ?tity?.4 ResultsTable 1 displays the average percentage of char-acters saved per class comment using each of themodels.
Models trained on in-project data (IN)perform significantly better than those trained onanother data source, regardless of the model type,with an average saving of 47.1% characters usinga trigram model.
This is expected, as files fromthe same project are likely to contain similar com-ments, and identifier names that appear in the com-ment of one class may appear in the code of an-other class in the same project.
Clearly, in-projectdata should be used when available as it improvescomment prediction leading to an average increaseof between 6% for the worst model (26.6 for OUTunigram versus 33.05 for IN) and 14% for the best(32.96 for OUT trigram versus 47.1 for IN).1http://tika.apache.org/37Model n-gram LDA Link-LDAn / topics 1 2 3 20 10 5 1 20 10 5 1IN 33.05 43.27 47.1 34.20 33.93 33.63 33.05 35.76 35.81 35.37 34.59(3.62) (5.79) (6.87) (3.63) (3.67) (3.67) (3.62) (3.95) (4.12) (3.98) (3.92)OUT 26.6 31.52 32.96 26.79 26.8 26.86 26.6 28.03 28 28 27.82(3.37) (4.17) (4.33) (3.26) (3.36) (3.44) (3.37) (3.60) (3.56) (3.67) (3.62)SO 27.8 33.29 34.56 27.25 27.22 27.34 27.8 28.08 28.12 27.94 27.9(3.51) (4.40) (4.78) (3.67) (3.44) (3.55) (3.51) (3.48) (3.58) (3.56) (3.45)Table 1: Average percentage of characters saved per comment using n-gram, LDA and link-LDA modelstrained on three training sets: IN, OUT, and SO.
The results are averaged over nine JAVA projects (withstandard deviations in parenthesis).Model Predicted CommentIN trigram ?Train a named-entity extractor?IN link-LDA ?Train a named-entity extractor?OUT trigram ?Train a named-entity extractor?SO trigram ?Train a named-entity extractor?Table 2: Sample comment from the Minor-Thirdproject predicted using IN, OUT and SO basedmodels.
Saved characters are underlined.Of the out-of-project data sources, models us-ing a greater amount of text (SO) mostly out-performed models based on more code (OUT).This increase in performance, however, comes ata cost of greater run-time due to the larger worddictionary associated with the SO data.
Note thatin the scope of this work we did not investigate thecontribution of each of the background projectsused in OUT, and how their relevance to the tar-get prediction project effects their performance.The trigram model shows the best performanceacross all training data sources (47% for IN, 32%for OUT and 34% for SO).
Amongst the testedtopic models, link-LDA models which distinguishcode and text tokens perform consistently betterthan simple LDA models in which all tokens areconsidered as text.
We did not however find acorrelation between the number of latent topicslearned by a topic model and its performance.
Infact, for each of the data sources, a different num-ber of topics gave the optimal character saving re-sults.Note that in this work, all topic models arebased on unigram tokens, therefore their resultsare most comparable with that of the unigram inDataset n-gram link-LDAIN 2778.35 574.34OUT 1865.67 670.34SO 1898.43 638.55Table 3: Average words per project for which eachtested model completes the word better than theother.
This indicates that each of the models is bet-ter at predicting a different set of comment words.Table 1, which does not benefit from the back-off strategy used by the bigram and trigram mod-els.
By this comparison, the link-LDA topic modelproves more successful in the comment predictiontask than the simpler models which do not distin-guish code and text tokens.
Using n-grams withoutbackoff leads to results significantly worse thanany of the presented models (not shown).Table 2 shows a sample comment segment forwhich words were predicted using trigram modelsfrom all training sources and an in-project link-LDA.
The comment is taken from the TrainEx-tractor class in the Minor-Third project, a ma-chine learning library for annotating and catego-rizing text.
Both IN models show a clear advan-tage in completing the project-specific word Train,compared to models based on out-of-project data(OUT and SO).
Interestingly, in this example thetrigram is better at completing the term named-entity given the prefix named.
However, the topicmodel is better at completing the word extractorwhich refers to the target class.
This example indi-cates that each model type may be more successfulin predicting different comment words, and thatcombining multiple models may be advantageous.38This can also be seen by the analysis in Table 3where we compare the average number of wordscompleted better by either the best n-gram or topicmodel given each training dataset.
Again, whilen-grams generally complete more words better, aconsiderable portion of the words is better com-pleted using a topic model, further motivating ahybrid solution.5 ConclusionsWe analyze the use of language models for pre-dicting class comments for source file documentscontaining a mixture of code and text tokens.
Ourexperiments demonstrate the effectiveness of us-ing language models for comment completion,showing a saving of up to 47% of the commentcharacters.
When available, using in-project train-ing data proves significantly more successful thanusing out-of-project data.
However, we find thatwhen using out-of-project data, a dataset based onmore words than code performs consistently bet-ter.
The results also show that different modelsare better at predicting different comment words,which motivates a hybrid solution combining theadvantages of multiple models.AcknowledgmentsThis research was supported by the NSF undergrant CCF-1247088.ReferencesRamnath Balasubramanyan and William W Cohen.2011.
Block-lda: Jointly modeling entity-annotatedtext and entity-entity links.
In Proceedings of the 7thSIAM International Conference on Data Mining.Dave Binkley, Matthew Hearn, and Dawn Lawrie.2011.
Improving identifier informativeness usingpart of speech information.
In Proc.
of the WorkingConference on Mining Software Repositories.
ACM.David M Blei and Michael I Jordan.
2003.
Modelingannotated data.
In Proceedings of the 26th annualinternational ACM SIGIR conference on Researchand development in informaion retrieval.
ACM.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research.Elena Erosheva, Stephen Fienberg, and John Lafferty.2004.
Mixed-membership models of scientific pub-lications.
Proceedings of the National Academy ofSciences of the United States of America.Yansong Feng and Mirella Lapata.
2010.
How manywords is a picture worth?
automatic caption gener-ation for news images.
In Proc.
of the 48th AnnualMeeting of the Association for Computational Lin-guistics.
Association for Computational Linguistics.Yansong Feng and Mirella Lapata.
2013.
Automaticcaption generation for news images.
IEEE transac-tions on pattern analysis and machine intelligence.Mark Gabel and Zhendong Su.
2008.
Javert: fully au-tomatic mining of general temporal properties fromdynamic traces.
In Proceedings of the 16th ACMSIGSOFT International Symposium on Foundationsof software engineering, pages 339?349.
ACM.Thomas L Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
Proc.
of the National Academy ofSciences of the United States of America.Sangmok Han, David R Wallace, and Robert C Miller.2009.
Code completion from abbreviated input.In Automated Software Engineering, 2009.
ASE?09.24th IEEE/ACM International Conference on, pages332?343.
IEEE.Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,and Premkumar Devanbu.
2012.
On the naturalnessof software.
In Software Engineering (ICSE), 201234th International Conference on.
IEEE.Ferosh Jacob and Robert Tairas.
2010.
Code templateinference using language models.
In Proceedingsof the 48th Annual Southeast Regional Conference.ACM.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.In Acoustics, Speech, and Signal Processing, 1995.ICASSP-95., volume 1, pages 181?184.
IEEE.Naveen Kumar and Benjamin Carterette.
2013.
Timebased feedback and query expansion for twittersearch.
In Advances in Information Retrieval, pages734?737.
Springer.Dawn Lawrie, Christopher Morrell, Henry Feild, andDavid Binkley.
2006.
Whats in a name?
a studyof identifiers.
In Program Comprehension, 2006.ICPC 2006.
14th IEEE International Conference on,pages 3?12.
IEEE.Andrew Kachites McCallum.
2002.
Mallet: A ma-chine learning for language toolkit.Adam Pauls and Dan Klein.
2011.
Faster and smallern-gram language models.
In Proceedings of the49th annual meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, volume 1, pages 258?267.Sarah Rastkar, Gail C Murphy, and Alexander WJBradley.
2011.
Generating natural language sum-maries for crosscutting source code concerns.
InSoftware Maintenance (ICSM), 2011 27th IEEE In-ternational Conference on, pages 103?112.
IEEE.39Ronald Rosenfeld.
2000.
Two decades of statisticallanguage modeling: Where do we go from here?Proceedings of the IEEE, 88(8):1270?1278.David Shepherd, Zachary P Fry, Emily Hill, Lori Pol-lock, and K Vijay-Shanker.
2007.
Using natu-ral language program analysis to locate and under-stand action-oriented concerns.
In Proceedings ofthe 6th international conference on Aspect-orientedsoftware development, pages 212?224.
ACM.Giriprasad Sridhara, Emily Hill, Divya Muppaneni,Lori Pollock, and K Vijay-Shanker.
2010.
To-wards automatically generating summary commentsfor java methods.
In Proceedings of the IEEE/ACMinternational conference on Automated software en-gineering, pages 43?52.
ACM.Yuen-Hsien Tseng and Da-Wei Juang.
2003.Document-self expansion for text categorization.
InProceedings of the 26th annual international ACMSIGIR conference on Research and development ininformaion retrieval, pages 399?400.
ACM.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.Single document summarization with document ex-pansion.
In Proc.
of the National Conference onArtificial Intelligence.
Menlo Park, CA; Cambridge,MA; London; AAAI Press; MIT Press; 1999.Roung-Shiunn Wu and Po-Chun Li.
2011.
Videoannotation using hierarchical dirichlet process mix-ture model.
Expert Systems with Applications,38(4):3040?3048.40
