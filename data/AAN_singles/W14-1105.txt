Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 30?37,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsTowards Cross-Domain PDTB-Style Discourse ParsingEvgeny A. Stepanov and Giuseppe RiccardiSignals and Interactive Systems LabDepartment of Information Engineering and Computer ScienceUniversity of Trento, Trento, Italy{stepanov,riccardi}@disi.unitn.itAbstractDiscourse relation parsing is an impor-tant task with the goal of understandingtext beyond the sentence boundaries.
Withthe availability of annotated corpora (PennDiscourse Treebank) statistical discourseparsers were developed.
In the litera-ture it was shown that the discourse pars-ing subtasks of discourse connective de-tection and relation sense classification donot generalize well across domains.
Thebiomedical domain is of particular interestdue to the availability of Biomedical Dis-course Relation Bank (BioDRB).
In thispaper we present cross-domain evaluationof PDTB trained discourse relation parserand evaluate feature-level domain adapta-tion techniques on the argument span ex-traction subtask.
We demonstrate that thesubtask generalizes well across domains.1 IntroductionDiscourse analysis is one of the most challeng-ing tasks in Natural Language Processing that hasapplications in many language technology areassuch as opinion mining, summarization, informa-tion extraction, etc.
(see (Webber et al., 2011)and (Taboada and Mann, 2006) for detailed re-view).
The release of the large discourse rela-tion annotated corpora, such as Penn DiscourseTreebank (PDTB) (Prasad et al., 2008), markedthe development of statistical discourse parsers(Lin et al., 2012; Ghosh et al., 2011; Xu et al.,2012; Stepanov and Riccardi, 2013).
Recently,PDTB-style discourse annotation was applied tobiomedical domain and Biomedical Discourse Re-lation Bank (BioDRB) (Prasad et al., 2011) wasreleased.
This milestone marks the beginning ofthe research on cross-domain evaluation and do-main adaptation of PDTB-style discourse parsers.In this paper we address the question of howwell PDTB-trained discourse parser (news-wiredomain) can extract argument spans of explicit dis-course relations in BioDRB (biomedical domain).The use cases of discourse parsing in biomed-ical domain are discussed in detail in (Prasad etal., 2011).
Here, on the other hand, we providevery general connection between the two.
Thegoal of Biomedical Text Mining (BioNLP) is toretrieve and organize biomedical knowledge fromscientific publications; and detecting discourse re-lations such as contrast and causality is an impor-tant step towards this goal (Prasad et al., 2011).
Toillustrate this point consider a quote from (Brunnerand Wirth, 2006), given below.The addition of an anti-Oct2 antibodydid not interfere with complex formation(Figure 3, lane 6), since HeLa cells donot express Oct2.
(Cause:Reason)In the example, the discourse connective since sig-nals a causal relation between the clauses it con-nects.
That is, the reason why ?the addition of ananti-Oct2 antibody did not interfere with complexformation?
is ?HeLa cells?
not expressing Oct2?.PDTB adopts non-hierarchical binary view ondiscourse relations: Argument 1 (Arg1) (in italicsin the example) and Argument 2 (Arg2), which issyntactically attached to a discourse connective (inbold).
Thus, a discourse relation is a triplet of aconnective and its two arguments.
In the literature(Lin et al., 2012; Stepanov and Riccardi, 2013)PDTB-style discourse parsing is partitioned intodiscourse relation detection, argument positionclassification, argument span extraction, and rela-tion sense classification.
For the explicit discourserelations (i.e.
signaled by a connective), discourserelation detection is cast as classification of con-nectives as discourse and non-discourse.
Argu-ment position classification, on the other hand, in-volves detection of the location of Arg1 with re-30DiscourseConnectiveDetectionArgumentPositionClassif.SS Arg2ExtractionSS Arg1ExtractionPS Arg1CandidateHeuristicPS Arg2ExtractionPS Arg1ExtractionSSPSFigure 1: Discourse Parser Architecture.
(CRFArgument Span Extraction models are in bold.
)spect to Arg2, that is to detect whether a relation isinter- or intra- sentential.
Argument span extrac-tion is the extraction (labeling) of text segmentsthat belong to each of the arguments.
Finally, re-lation sense classification is the annotation of re-lations with the senses from the sense hierarchy(PDTB or BioDRB).To the best of our knowledge, the only subtasksthat were addressed cross-domain are the detec-tion of explicit discourse connectives (Ramesh andYu, 2010; Ramesh et al., 2012; Faiz and Mercer,2013) and relation sense classification (Prasad etal., 2011).
While the discourse parser of Faiz andMercer (2013)1provides models for both domainsand does identification of argument head words inthe style of Wellner and Pustejovsky (2007); thereis no decision made on arguments spans.
More-over, there is no cross-domain evaluation availablefor each of the models.
In this paper we addressthe task of cross-domain argument span extractionof explicit discourse relations.
Additionally, weprovide evaluation for cross-domain argument po-sition classification as far as the data allows, sinceBioDRB lacks manual sentence segmentation.The paper is structured as follows.
In Section 2we present the comparative analysis of PDTB andBioDRB corpora and the relevant works on cross-domain discourse parsing.
In Section 3 we de-scribe the PDTB discourse parser used for cross-domain experiments.
In Section 4 we present theevaluation methodology and the experimental re-sults.
Section 5 provides concluding remarks.2 PDTB vs. BioDRB Corpora Analysisand Related Cross-Domain WorksThe two corpora used in our experiments are PennDiscourse Treebank (PDTB) (Prasad et al., 2008)1Made available on https://code.google.com/p/discourse-parser/and Biomedical Discourse Relation Bank (Bio-DRB) (Prasad et al., 2011).
Both corpora followthe same discourse relation annotation style overdifferent domain corpora: PDTB is annotated ontop of Wall Street Journal (WSJ) corpus (financialnews-wire domain); and it is aligned with PennTreebank (PTB) syntactic tree annotation; Bio-DRB, on the other hand, is a corpus annotated over24 open access full-text articles from the GENIAcorpus (Kim et al., 2003) (biomedical domain),and, unlike PDTB, there is no reference tokeniza-tion or syntactic parse trees.The detailed comparison of the corpora is outof the scope of this paper, and it is available in(Prasad et al., 2011).
Similarly, the review ofPDTB-style discourse parsing literature is not inits scope.
Here, on the other hand, we focus on thecorpus differences relevant for discourse parsingtasks and cross-domain application of discourseparsing subtasks.Discourse relations in both corpora are binary:Arg1 and Arg2, where Arg2 is an argument syn-tactically attached to a discourse connective.
Withrespect to Arg2, Arg1 can appear in the same sen-tence (SS case), one or several of the preceding(PS case) or following (FS case) sentences.
Adiscourse connective is a member of a well de-fined list of connectives and a relation expressedvia such connective is an Explicit relation.
Thereare other types of discourse and non-discourse re-lations annotated in the corpora; however, they areout of the scope of this paper.
Discourse relationsare annotated using a hierarchy of senses: eventhough the organization of senses and the numberof levels are different between corpora, the mostgeneral top level senses are mapped to the PDTBtop level senses: Comparison, Contingency, Ex-pansion, and Temporal (Prasad et al., 2011).The difference between the two corpora with re-spect to discourse connectives is that in case ofPDTB the annotated connectives belong to one ofthe three syntactic classes: subordinating conjunc-tions (e.g.
because), coordinating conjunctions(e.g.
but), and discourse adverbials (e.g.
how-ever), while BioDRB is also annotated for a forthsyntactic class ?
subordinators (e.g.
by).There are 100 unique connective types in PDTB(after connectives like 1 year after are stemmedto after) in 18,459 explicit discourse relations.Whereas in BioDRB there are 123 unique con-nective types in 2,636 relations.
According to31the discourse connective analysis in (Ramesh etal., 2012), the subordinators comprise 33% of allconnective types in BioDRB.
Additionally, 11%of connective types in common syntactic classesthat occur in BioDRB do not occur in PDTB; e.g.In summary, as a consequence.
Thus, only 56%of connective types of BioDRB are common toboth corpora.
While in-domain discourse connec-tive detection has good performance (Ramesh andYu, 2010), this difference makes the cross-domainidentification of discourse connectives a hard task,which is exemplified by experiments in (Rameshand Yu, 2010) (F1= 0.55).With respect to relation sense classification, theconnective surface provides already high baselines(Prasad et al., 2011).
However, cross-domainsense classification experiments indicate that thereare significant differences in the semantic usage ofconnectives between two domains, since the per-formance of the classifier trained on PDTB doesnot generalize well to BioDRB (F1= 0.57).To sum up, the corpora differences with respectto discourse connective usage affect the cross-domain generalization of connective detection andsense classification tasks negatively.
The exper-iments in this paper are intended to evaluate thegeneralization of argument span extraction, as-suming that the connective is already identified.In the following section, we present the PDTB-trained discourse parser optimized for in-domainperformance.3 PDTB-Style Discourse ParserThe discourse parser (see Figure 1) is a combi-nation of argument position classification modelfor classifying discourse connectives as inter- orintra-sentential, and specific Conditional RandomFields argument extraction models for each of thearguments in these configurations.
In the follow-ing subsections we provide descriptions for eachof the components.3.1 Argument Position ClassificationDiscourse connectives have a very strong prefer-ence on the location of the Arg1 with respect totheir syntactic category (Subordinating Conjunc-tion, Coordinating Conjunction, and DiscourseAdverbial) and position in the sentence (sentenceinitial or sentence medial); thus, classification ofdiscourse connectives into inter-sentential or intra-sentential is an easy task yielding high supervisedmachine learning performance (Stepanov and Ric-cardi, 2013; Lin et al., 2012).
With respect to thedecision made in this step a specific argument spanextraction model is applied.For Argument Position Classification the un-igram BoosTexter (Schapire and Singer, 2000)model with 100 iterations is trained on PDTB sec-tions 02-22 and tested on sections 23-24.
Sim-ilar to the previously published results, it has ahigh performace: F1 = 98.12.
The featuresare connective surface string, POS-tags, and IOB-chains.
The results obtained with automatic sen-tence splitting, tokenization, and syntactic parsingusing Stanford Parser (Klein and Manning, 2003)are also high F1 = 97.81.Since, unlike PTB for PDTB, for BioDRB thereis no manual sentence splitting, tokenization, andsyntactic tree annotation; the precise cross-domainevaluation of Argument Span Extraction step is notpossible.
However, in Section 4 we estimate theperformance using automatic sentence splitting.3.2 Argument Span ExtractionArgument span extraction is cast as token-level se-quence labeling using Conditional Random Fields(CRF) (Lafferty et al., 2001).
Previously, it wasobserved that in PDTB for inter-sentential dis-course relations Arg1 precedes Arg2 in most of thecases.
Thus, the CRF models are trained for theconfigurations where both of the arguments are inthe same sentence (SS), and for Arg1 in one of theprevious sentences (PS); the following sentenceArg1 case (FS) is ignored due to too few traininginstances being available (in PDTB 8 / 18,459).Consequently, there are 4 CRF models SS Arg1and Arg2, and PS Arg1 and Arg2.Same sentence case models are applied in a cas-cade, such that output of Arg2 model is used as afeature for Arg1 span extraction.
For the case ofArg1 in the previous sentences; based on the ob-servation that in PDTB Arg2 span is fully locatedin the sentence containing the connective in 98.5%of instances; and Arg1 span is fully located in thesentence immediately preceding Arg2 in 71.7% ofinstances; the sentences in these positions are se-lected and CRF models are trained to label thespans.The features used for training the models arepresented in Table 1.
The feature sets are opti-mized for each of the arguments in (Ghosh et al.,2011) (see the Table columns Arg1 and Arg2).
Be-32sides the features commonly used in NLP taskssuch that token, lemma, inflectional affixes, andpart-of-speech tag, the rest of the features are:?
IOB-Chain (IOB) is the path string of the syn-tactic tree nodes from the root node to the to-ken, prefixed with the information whether atoken is at the beginning (B-) or inside (I-)the constituent.
The chunklink tool (Buch-holz, 2000) is used to extract this feature fromsyntactic trees.?
PDTB Level 1 Connective sense (CONN) isthe most general sense of a connective inPDTB sense hierarchy.
It?s general purpose isto label the discourse connective tokens, i.e.the value of the feature is ?NULL?
for all to-kens except the discourse connective.?
Boolean Main Verb (BMV) is a boolean fea-ture that indicates whether a token is a mainverb of a sentence or not (Yamada and Mat-sumoto, 2003).?
Arg2 Label (ARG2) is an output of Arg2 spanextraction model, that is used as a feature forArg1 span extraction.
Arg2 span is easier toidentify (Ghosh et al., 2011; Stepanov andRiccardi, 2013) since it is syntactically at-tached to the discourse connective.
Thus, thisfeature serves to constrain the Arg1 searchspace for intra-sentential argument span ex-traction.
The value of the feature is eitherARG2 suffixed for whether a token is Inside(I), Begin (B), or End (E) of the span, or ?O?if it does not belong to the Arg2 span.These features are expanded during trainingwith n-grams (feature of CRF++2): tokens with2-grams in the window of ?1 tokens, and the restof the features with 2 & 3-grams in the window of?2 tokens.The in-domain performance of argument spanextraction models is provided in the followingsection, after the description of the evaluationmethodology.4 Experiments and ResultsIn this Section we first describe the evaluationmethodology and then the experiments on cross-domain evaluation of argument position classifi-cation and argument span extraction models.2https://code.google.com/p/crfpp/Feature ABBR Arg2 Arg1Token TOK Y YPOS-Tag POSLemma LEM Y YInflection INFL Y YIOB-Chain IOB Y YConnective Sense CONN Y YBoolean Main Verb BMV YArg2 Label ARG2 YTable 1: Feature sets for Arg2 and Arg1 argumentspan extraction.The experimental settings for PDTB are the fol-lowing: Sections 02-22 are used for training andSections 23-24 for testing.
For BioDRB, on theother hand, 12 fold cross-validation is used (2 doc-uments in each fold, since in BioDRB there are 24documents).4.1 Evaluation MethodologyThe performance of Argument Span Extraction isevaluated in terms of precision (p), recall (r), andF-measure (F1) using the equations 1 ?
3.
Anargument span is considered to be correct, if itexactly matches the reference string.
Following(Ghosh et al., 2011) and (Lin et al., 2012), argu-ment initial and final punctuation marks are re-moved .p =Exact MatchExact Match + No Match(1)r =Exact MatchReferences in Gold(2)F1=2 ?
p ?
rp + r(3)In the equations, Exact Match is the count of cor-rectly tagged argument spans; No Match is thecount of argument spans that do not match the ref-erence string exactly, i.e.
even a single token dif-ference is counted as an error; and References inGold is the total number of arguments in the refer-ence.Since argument span extraction is applied afterargument position classification, the classificationerror is propagated.
Thus, for the evaluation ofargument span extraction, misclassified instancesare reflected in the counts of Exact Matches andNo Matches.
For example, misclassified same sen-tence relation results in that both its arguments are33Arg2 Arg1P R F1 P R F1GoldSS 90.36 87.49 88.90 70.27 66.67 68.42PS 79.01 77.10 78.04 46.23 36.61 40.86ALL 85.93 83.45 84.67 61.94 54.98 58.25AutoSS 86.83 85.14 85.98 64.26 63.01 63.63PS 75.00 73.67 74.33 37.66 37.00 37.33ALL 82.24 80.69 81.46 53.93 52.92 53.42Table 2: In-domain performance of the PDTB-trained argument span extraction models on thetest set with ?Gold?
and ?Automatic?
sentencesplitting, tokenization, and syntactic features.
Theresults are reported together with the error prop-agation from argument position classification forSame Sentence (SS), Previous Sentence (PS) mod-els and joined results (ALL) as precision (P), recall(R) and F-measure (F1).considered as not recalled for the SS, and for thePS they are considered as No Match.However, we do not propagate error in cross-domain evaluation on BioDRB, since there is noreference information.
Additionally, while Arg1span extraction models are trained on Gold Arg2features, for testing they are always automatic.4.2 Cross-Domain Argument PositionClassificationAs it was mentioned above, there is no manualsentence splitting for BioDRB; thus, there is noreferences for whether a discourse relation has itsArg1 in the same or different sentences.
In orderto evaluate cross-domain argument position clas-sification we evaluate classifier decisions againstautomatic sentence splitting using Stanford Parser(Klein and Manning, 2003) on whole of BioDRB.The BoosTexter model described in Section 3.1has a high in-domain performance of 97.81.
OnBioDRB its performance is 95.26, which is stillhigh.
Thus, we can conclude that argument posi-tion classification generalizes well cross-domain,and that it is little affected by the presence of ?sub-ordinators?
that were not annotated in PDTB.4.3 In-Domain Argument Span Extraction:PDTBThe in-domain performance of the argument spanextraction models trained on PDTB sections 02-22and tested on sections 23-24 is given on Table 2.The results are for 2 settings: ?Gold?
and ?Auto?.In the ?Gold?
settings the sentence splitting, tok-enization and syntactic features are extracted fromPTB, and in the ?Auto?
they are extracted from au-tomatic parse trees obtained using Stanford Parser(Klein and Manning, 2003).The general trend in the literature, is that the ar-gument span extraction for Arg1 has lower perfor-mance than for Arg2, which is expected since Arg2position is signaled by a discourse connective.
Ad-ditionally, Previous Sentence Arg1 model perfor-mance is much lower than that of the other modelsdue to the fact that it only considers immediatelyprevious sentence; which, as was mentioned ear-lier, covers only 71.7% of the inter-sentential re-lations.
In the next subsections, these models areevaluated on biomedical domain.4.4 In-Domain Argument Span Extraction:BioDRBIn order to evaluate PDTB-BioDRB cross-domainperformance we first evaluate the in-domain Bio-DRB argument span extraction.
Since there is nogold sentence splitting, tokenization and syntacticparse trees, the models are trained using the fea-tures extracted from automatic parse trees.
We useexactly the same feature sets as for PDTB models,which are optimized for PDTB.
An important as-pect is that in BioDRB the connective senses aredifferent: there are 16 top level senses that aremapped to 4 top level PDTB senses.
For the in-domain BioDRB models, the 16 senses were keptas is.Since we do not have gold argument positioninformation, we do not train in-domain argumentclassification model.
Thus, the reported results arewithout error propagation.
Later, this will allow usto assess cross-domain argument span extractionperformance better.The results reported in Table 3 are averageprecision, recall and f-measure of 12-fold cross-validation.
With respect to automatic sentencesplitting, there are 717 inter-sentential and 1,919intra-sentential relations (27% to 73%).
Thus,BioDRB is less affected by PS Arg1 performancethan PDTB models, where the ratio is 619 to976 (39% to 61%).
Additionally, BioDRB PSArg1 performance is generally higher than thatof PDTB.
Overall, in-domain BioDRB argumentextraction model performance is in-line with the34Arg2 Arg1P R F1 P R F1SS 80.94 79.88 80.41 66.51 61.82 64.07PS 82.99 82.99 82.99 57.50 55.62 56.53ALL 81.45 80.67 81.06 63.87 60.00 61.87Table 3: In-domain performance of the BioDRB-trained argument span extraction models.
Bothtraining and testing are on automatic sentencesplitting, tokenization, and syntactic features.
Theresults are reported for Same Sentence (SS) andPrevious Sentence (PS) models, and the joined re-sults for each of the arguments (ALL) as averageprecision (P), recall (R), and F-measure (F1) of12-fold cross-validation.PDTB models, with the exception that previoussentence Arg2 has higher performance than thesame sentence one.4.5 Cross-Domain Argument SpanExtraction: PDTB - BioDRBSimilar to in-domain BioDRB argument span ex-traction, we perform 12 fold cross-validation forPDTB-BioDRB cross-domain argument span ex-traction.
The cross-domain performance of themodels described in Section 4.3 is given in theTable 4 under the ?Gold?.
To make the cross-domain evaluation settings closer to the BioDRBin-domain evaluation, we additionally train PDTBmodels on the automatic features, i.e.
features ex-tracted from PDTB with automatic sentence split-ting, tokenization and syntactic parsing.
Similarto the in-domain BioDRB evaluation, results arereported without error propagation from argumentposition classification step.The first observation from cross-domain eval-uation is that argument span extraction general-izes to biomedical domain much better that thediscourse parsing subtasks of discourse connectivedetection and relation sense classification.
Unlikethose subtasks, the difference between in-domainBioDRB argument span extraction models and themodels trained on PDTB is much less: e.g.
fordiscourse connective detection the in-domain andcross-domain difference for BioDRB is 14 points(f-measures 69 and 55 in (Ramesh and Yu, 2010)),and for argument span extraction 2 and 4 pointsfor Arg2 and Arg1 respectively (see Tables 3 & 4).The difference between the models trained onautomatic and gold parse trees is also not high, andgold feature trained models perform better withArg2 Arg1P R F1 P R F1GoldSS 80.37 76.58 78.42 60.82 56.40 58.52PS 80.73 80.50 80.62 57.74 52.95 55.19ALL 80.53 77.71 79.09 59.76 55.29 57.43AutoSS 77.60 75.05 76.30 60.76 55.21 57.83PS 81.39 81.23 81.31 57.71 51.72 54.47ALL 78.72 76.80 77.74 59.60 54.12 56.71Table 4: Cross-domain performance of the PDTB-trained argument span extraction models on Bio-DRB.
For the ?Gold?
setting the models from in-domain PDTB section are used.
For ?Auto?, themodels are trained on automatic sentence splitting,tokenization, and syntactic features.
The resultsare reported for Same Sentence (SS) and Previ-ous Sentence (PS) models, and the joined resultsfor each of the arguments (ALL) as average preci-sion (P), recall (R), and F-measure (F1) of 12-foldcross-validation.the exception of PS Arg2.
Since training on auto-matic parse trees does not improve cross-domainperformance, the rest of the experiments is usinggold features for training.4.6 Feature-Level Domain AdaptationThe two major differences between PDTB andBioDRB are vocabulary and connective senses.The out-of-vocabulary rate of PDTB on the wholeBioDRB is 22.7% and of BioDRB on PDTB is33.1%, which are very high.
Thus, PDTB lexi-cal features might not be very effective, and themodels generalize well due to syntactic features.To test this hypothesis we train additional PDTBmodels on only syntactic features: POS-tags andIOB-chain and ?connective labels?
?
?CONN?
suf-fixed for the Beginning (B), Inside (I) or End (E)of the connective span, simulating discourse con-nective detection output.
Moreover, we reduce thefeature set to unigrams only (recall that featureswere enriched by 2 and 3 grams), such that themodels become very general.Even though BioDRB connective senses can bemapped to PDTB, in (Prasad et al., 2011) it wasobserved that relation sense classification does notgeneralize well.
To reduce the dependency of ar-gument span extraction models on relation senseclassification, the connective sense feature in the35Arg2 Arg1P R F1 P R F1BaselineSS 80.37 76.58 78.42 60.82 56.40 58.52PS 80.73 80.50 80.62 57.74 52.95 55.19ALL 80.53 77.71 79.09 59.76 55.29 57.43SyntacticSS 82.00 75.03 78.33 61.07 51.80 56.01PS 75.56 74.47 75.01 56.64 46.66 51.11ALL 80.31 74.98 77.54 59.69 50.42 54.63No Relation SenseSS 81.35 74.00 77.47 62.46 56.11 59.10PS 80.35 80.13 80.24 57.58 52.25 54.74ALL 81.16 75.67 78.30 60.86 54.87 57.69Table 5: Cross-domain performance of the PDTB-trained argument span extraction models on Bio-DRB.
For the ?Syntactic?
setting the models aretrained on only syntactic features (POS-tag + IOB-chain) and ?connective labels?.
For ?No RelationSense?, the models are trained by replacing con-nective sense with ?connective labels?.
The ?Base-line?
is repeated from Table 4.
The results are re-ported for Same Sentence (SS) and Previous Sen-tence (PS) models, and the joined results for eachof the arguments (ALL) as average precision (P),recall (R), and F-measure (F1) of 12-fold cross-validation.?Baseline?
models (i.e.
the models from Section4.3) is also replaced by ?connective labels?.
Wetrain these models using gold features only, and,similar to previous experiments, do 12-fold cross-validation.The performance of the adapted models is givenin Table 5.
The ?Syntactic?
section gives the re-sults of the models trained on syntactic featuresand the ?No Relation Sense?
section gives the re-sults for the models with ?connective labels?
in-stead of connective senses, and the ?Baseline?repeats the performance of the PDTB-optimizedmodels.The PDTB-optimized baseline, outperforms theadapted models on Arg2; however, ?No RelationSense?
Arg1 yields the best performance, and,though insignificantly, outperforms the baseline.Thus, the effect of replacing connective senseswith ?connective labels?
is negative for all casesexcept SS Arg1.
Overall, the difference in perfor-mance between the ?Baseline?
and ?No RelationSense?
models is an acceptable price to pay for theArg2 Arg1P R F1 P R F1SS 81.72 76.14 78.82 61.53 56.36 58.82PS 80.31 79.84 80.07 58.55 52.82 55.44ALL 81.27 77.10 79.12 60.56 55.30 57.80Table 6: Cross-domain performance of the PDTB-trained argument span extraction model on uni-gram and bigrams of token, POS-tag, IOB-chainand ?connective label?.
The results are reported forSame Sentence (SS) and Previous Sentence (PS)models, and the joined results for each of the argu-ments (ALL) as average precision (P), recall (R),and F-measure (F1) of 12-fold cross-validation.independence from relation sense classification.The most general models ?
unigrams of Part-of-Speech tags and IOB-chains together with ?con-nective labels?
in the window of ?2 tokens ?all have the performance lower than the baseline,which is expected given its feature set.
However,for the easiest case of intra-sentential Arg2 it out-performs the model trained by replacing the con-nective sense in the baseline (i.e.
?No RelationSense?).
Degraded performance of Arg1 modelsindicates that lexical features are helpful.Introducing the tokens back into the ?Syntactic?model, and increasing the features to include also2-grams, boosts the performance of the models tooutperform the ?No Relation Sense?
models in allbut Previous Sentence Arg2 category.
However,the models now yield performance comparable tothe PDTB optimized baseline (insignificantly bet-ter), while being unaffected by poor cross-domaingeneralization of relation sense classification (seeTable 6).The cross-domain argument extraction exper-iments indicate that models trained on PDTB-optimized feature set already have good general-ization.
However, they are dependent on relationsense classification task, which does not gener-alize well.
By replacing connective senses with?connective labels?
we obtain models independentof this task while maintaining comparable perfor-mance.
The in-domain trained BioDRB models,however, perform better, as expected.5 ConclusionIn this paper we presented cross-domain discourseparser evaluation on subtasks of argument posi-tion classification and argument span extraction.36The observed cross-domain performances are in-dicative of good model generalization.
However,since these models are applied later in the pipeline,they are affected by the cross-domain performanceof the other tasks.
Specifically, discourse connec-tive detection, which was shown not to generalizewell in the literature.
Additionally, we have pre-sented feature-level domain adaptation techniquesto reduce the dependence of the cross-domain ar-gument span extraction on other discourse parsingsubtasks.The syntactic parser (Stanford) that providessentence splitting and tokenization is trained onPenn Treebank, i.e.
it is in-domain for PDTBand out-of-domain for BioDRB; and it is knownthat domain-optimized tokenization improves per-formance on various NLP tasks.
Thus, the fu-ture direction of this work is to evaluate argumentspan extraction using tools optimized for biomed-ical domain.AcknowledgmentsThe research leading to these results has re-ceived funding from the European Union ?
Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement No.
610916 ?
SENSEI.ReferencesCornelia Brunner and Thomas Wirth.
2006.
Btk ex-pression is controlled by oct and bob.
1/obf.
1.
Nu-cleic acids research, 34(6):1807?1815.Sabine Buchholz.
2000.
Readme for perl script chun-klink.pl.Syeed Ibn Faiz and Robert E Mercer.
2013.
Iden-tifying explicit discourse connectives in text.
InAdvances in Artificial Intelligence, pages 64?76.Springer.Sucheta Ghosh, Richard Johansson, Giuseppe Ric-cardi, and Sara Tonelli.
2011.
Shallow discourseparsing with conditional random fields.
In Proceed-ings of the 5th International Joint Conference onNatural Language Processing (IJCNLP 2011).J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-jii.
2003.
Genia corpusa semantically annotatedcorpus for bio-textmining.
Bioinformatics, 19(suppl1):i180?i182.Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems 15 (NIPS 2002).John Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of 18th InternationalConference on Machine Learning.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012.A pdtb-styled end-to-end discourse parser.
NaturalLanguage Engineering, 1:1 ?
35.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC 2008).Rashmi Prasad, Susan McRoy, Nadya Frid, AravindJoshi, and Hong Yu.
2011.
The biomedicaldiscourse relation bank.
BMC Bioinformatics,12(1):188.Balaji Polepalli Ramesh and Hong Yu.
2010.
Identify-ing discourse connectives in biomedical text.
AMIAAnnual Symposium Proceedings, 2010:657.Balaji Polepalli Ramesh, Rashmi Prasad, Tim Miller,Brian Harrington, and Hong Yu.
2012.
Automaticdiscourse connective detection in biomedical text.Journal of the American Medical Informatics Asso-ciation, 19(5):800?808.Robert E. Schapire and Yoram Singer.
2000.
Boostex-ter: A boosting-based system for text categorization.Machine Learning, 39(2/3):135?168.Evgeny A. Stepanov and Giuseppe Riccardi.
2013.Comparative evaluation of argument extraction al-gorithms in discourse relation parsing.
In 13thInternational Conference on Parsing Technologies(IWPT 2013), pages 36?44.Maite Taboada and William C. Mann.
2006.
Applica-tions of rhetorical structure theory.
Discourse Stud-ies, (8):567?88.Bonnie L. Webber, Markus Egg, and Valia Kordoni.2011.
Discourse structure and language technology.Natural Language Engineering, pages 1 ?
54.Ben Wellner and James Pustejovsky.
2007.
Automat-ically identifying the arguments of discourse con-nectives.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL 2007).Fan Xu, Qiao Ming Zhu, and Guo Dong Zhou.
2012.A unified framework for discourse argument identi-fication via shallow semantic parsing.
In Proceed-ings of 24th International Conference on Computa-tional Linguistics (COLING 2012): Posters.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of 8th International Work-shop on Parsing Technologies.37
