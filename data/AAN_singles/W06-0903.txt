Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 17?22,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic Dating of Documents and Temporal Text ClassificationAngelo DalliNLP Research GroupUniversity of SheffieldUnited Kingdomangelo@dcs.shef.ac.ukYorick WilksNLP Research GroupUniversity of SheffieldUnited Kingdomyorick@dcs.shef.ac.ukAbstractThe frequency of occurrence of words innatural languages exhibits a periodic anda non-periodic component when analysedas a time series.
This work presents anunsupervised method of extracting perio-dicity information from text, enablingtime series creation and filtering to beused in the creation of sophisticated lan-guage models that can discern betweenrepetitive trends and non-repetitive writ-ing patterns.
The algorithm performs inO(n log n) time for input of length n. Thetemporal language model is used to cre-ate rules based on temporal-word asso-ciations inferred from the time series.The rules are used to guess automaticallyat likely document creation dates, basedon the assumption that natural languageshave unique signatures of changing worddistributions over time.
Experimental re-sults on news items spanning a nine yearperiod show that the proposed methodand algorithms are accurate in discover-ing periodicity patterns and in datingdocuments automatically solely fromtheir content.1 IntroductionVarious features have been used to classifyand predict the characteristics of text and relatedtext documents, ranging from simple word countmodels to sophisticated clustering and Bayesianmodels that can handle both linear and non-linearclasses.
The general goal of most classificationresearch is to assign objects from a pre-defineddomain (such as words or entire documents) totwo or more classes/categories.
Current and pastresearch has largely focused on solving problemslike tagging, sense disambiguation, sentimentclassification, author and language identificationand topic classification.
In this paper, we intro-duce an unsupervised method that classifies textand documents according to their predicted timeof writing/creation.
The method uses a sophisti-cated temporal language model to predict likelycreation dates for a document, hence dating itautomatically.This paper presents the main assumption be-hind this work together some background infor-mation about existing techniques and the imple-mented system, followed by a brief explanationof the classification and dating method, and fi-nally concluding with results and evaluation per-formed on the LDC GigaWord English Corpus(LDC, 2003) together with its implications andrelevance to temporal-analytical frameworks andTimeML applications.2 Background and AssumptionsThe main assumption behind this work is thatnatural language exhibits a unique signature ofvarying word frequencies over time.
New wordscome into popular use continually, while otherwords fall into disuse either after a brief fad orwhen they become obsolete or archaic.
Currentevents, popular issues and topics also affect writ-ers in their choice of words and so does the timeperiod when they create documents.
This as-sumption is implicitly made when people try toguess at the creation date of a document ?
wewould expect a document written in Shake-speare?s time to contain higher frequency countsof words and phrases such as ?thou art?, ?be-twixt?, ?fain?, ?methinks?, ?vouchsafe?
and soon than would a modern 21st century document.Similarly, a document that contains a high fre-quency of occurrence of the words ?terrorism?,?Al Qaeda?, ?World Trade Center?, and so on ismore likely to be written after 11 September2001.
New words can also be used to create ab-solute constraints on the creation dates of docu-ments, for example, it is highly improbable that a17document containing the word ?blog?
was writ-ten before July 1999 (it was first used in a news-group in July 1999 as an abbreviation for ?we-blog?
), or a document containing the word?Google?
to have been written before 1997.Words that are now in common use can also beused to impose constraints on the creation date;for example, the word ?bedazzled?
has been at-tributed to Shakespeare, thus allowing docu-ments from his time onwards to be identifiableautomatically.
Traditional dictionaries often tryto record the date of appearance of new words inthe language and there are various Internet sites,such as WordSpy.com, devoted to chroniclingthe appearance of new words and their meanings.Our system is building up a knowledge base ofthe first occurrences of various words in differentlanguages, enabling more accurate constraints tobe imposed on the likely document creation dateautomatically.Commercial trademarks and company namesare also useful in dating documents, as their reg-istration date is usually available in public regis-tries.
Temporal information extracted from thedocuments itself is also useful in dating the docu-ments ?
for example, if a document containsmany references to the year 2006, it is quitelikely that the document was written in 2006 (orin the last few weeks of December 2005).These notions have been used implicitly by re-searchers and historians when validating the au-thenticity of documents, but have not been util-ised much in automated systems.
Similar appli-cations have so far been largely confined to au-thorship identification, such as (Mosteller andWallace, 1964; Fung, 2003) and the identifica-tion of association rules (Yarowsky, 1994;Silverstein et al, 1997).Temporal information is presently under-utilised for automated document classificationpurposes, especially when it comes to guessing atthe document creation date automatically.
Thiswork presents a method of using periodical tem-poral-frequency information present in docu-ments to create temporal-association rules thatcan be used for automatic document dating.Past and ongoing related research work haslargely focused on the identification and taggingof temporal expressions, with the creation of tag-ging methodologies such as TimeML/TIMEX(Gaizauskas and Setzer, 2002; Pustejovsky et al,2003; Ferro et al, 2004), TDRL (Aramburu andBerlanga, 1998) and their associated evaluationssuch as the ACE TERN competition (Sundheimet al 2004).Temporal analysis has also been applied inQuestion-Answering systems (Pustejovsky et al,2004; Schilder and Habel, 2003; Prager et al,2003), email classification (Kiritchenko et al,2004), aiding the precision of Information Re-trieval results (Berlanga et al, 2001), documentsummarisation (Mani and Wilson, 2000), timestamping of event clauses (Filatova and Hovy,2001), temporal ordering of events (Mani et al,2003) and temporal reasoning from text (Bogu-raev and Ando, 2005; Moldovan et al, 2005).A growing body of related work related to thecomputational treatment of time in language hasalso been building up largely since 2000 (COL-ING 2000; ACL 2001; LREC 2002; TERQAS2002; TANGO 2003, Dagstuhl 2005).There is also a large body of work on time se-ries analysis and temporal logic in Physics, Eco-nomics and Mathematics, providing importanttechniques and general background information.In particular, this work uses techniques adaptedfrom Seasonal ARIMA (auto-regressive inte-grated moving average) models (SARIMA).SARIMA models are a class of seasonal, non-stationary temporal models based on the ARIMAprocess.
The ARIMA process is further definedas a non-stationary extension of the stationaryARMA model.
The ARMA model is one of themost widely used models when analyzing timeseries, especially in Physics, and incorporateboth auto-regressive terms and moving averageterms (Box and Jenkins, 1976).
Non-stationaryARIMA processes are defined by the followingequation:( ) ( ) ( ) ttd ZBXBB ??
=?1            (1)where d is non-negative integer, and ( )X?
( )X?
polynomials of degrees p and q respec-tively.
The SARIMA extension adds seasonalAR and MA polynomials that can handle season-ally varying data in time series.The exact formulation of the SARIMA modelis beyond the scope of this paper and can befound in various mathematics and physics publi-cations, such as (Chatfield, 2003; Brockwell etal., 1991; Janacek, 2001).The main drawback of SARIMA modelling(and associated models built on the basic ARMAmodel) is that it requires fairly long time seriesbefore accurate results are obtained.
The major-ity of authors recommend that a time series of atleast 50 data points is used to build the SARIMAmodel.180100200300400500600237 46 24 12 17 10 19 307 22 3 16 18 13 35 33 31 14 17 5 60100200300400500600237 46 24 12 17 10 19 307 22 3 16 18 13 35 33 31 14 17 5 6Time Series for ?January?Original (Top Left), Non-Periodic Component (TopRight), Periodic Component (Bottom Right)0100200300400500600237 46 24 12 17 10 19 307 22 3 16 18 13 35 33 31 14 17 5 60100020003000400050006000700080009000100001 164 327 490 653 816 979 1142 1305 1468 1631 1794 1957050010001500200025003000350040001 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081Time Series for ?The?Original (Top Left), Non-Periodic Component (TopRight), Periodic Component (Bottom Right)01000200030004000500060007000800090001 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081Figure 1: Effects of applying the temporal periodical algorithm on time series for "January" (top threegraphs) and "the" (bottom three graphs) with the original series on the left and the remaining time seriescomponents after filtering on the right.
Y-axis shows frequency count and X-axis shows the day number(time).3 Temporal Periodicity AnalysisWe have created a high-performance system thatdecomposes time series into two parts: a periodiccomponent that repeats itself in a predictablemanner, and a non-periodic component that isleft after the periodic component has been fil-tered out from the original time series.
Figure 1shows an example of the filtering results on time-series of the words ?January?
and ?the?.
Theoriginal series is presented together with two se-ries representing the periodic and non-periodic19components of the original time series.
The timeseries are based on training documents selectedat random from the GigaWord English corpus.10% of all the documents in the corpus wereused as training documents, with the rest beingavailable for evaluation and testing.
A total of395,944 time series spanning 9 years were calcu-lated from the GigaWord corpus.
The availabilityof 9 years of data also mitigated the negative ef-fects of using short time series in combinationwith SARIMA models (as up to 3,287 datapoints were available for some words, well abovethe 50 data point minimum recommendation).Figure 2 presents pseudo-code for the time seriesdecomposition algorithm:1.
Find min/max/mean and standard devia-tion of time series2.
Start with a pre-defined maximum win-dow size (set to 366 days in our pre-sent system)3.
While window size bigger than 1 re-peat steps a. to d. below:a.
Look at current value in timeseries (starting from firstvalue)b.
Do values at positions cur-rent, current + window size,current + 2 x window size,etc.
vary by less than half astandard deviation?c.
If yes, mark currentvalue/window size pair as be-ing possible decompositionmatchd.
Look at next value in time se-ries until the end is reachede.
Decrease window size by one4.
Select the minimum number of decompo-sition matches that cover the entiretime series using a greedy algorithmFigure 2: Time Series Decomposition AlgorithmThe time series decomposition algorithm wasapplied to the 395,944 time series, taking an av-erage of 419ms per series.
The algorithm runs inO(n log n) time for a time series of length n.The periodic component of the time series isthen analysed to extract temporal associationrules between words and different ?seasons?,including Day of Week, Week Number, MonthNumber, Quarter, and Year.
The procedure ofdetermining if a word, for example, is predomi-nantly peaking on a weekly basis, is to apply asliding window of size 7 (in the case of weeklyperiods) and determining if the periodic time se-ries always spikes within this window.
Figure 3shows the frequency distribution of the periodictime series component of the days of weeknames (?Monday?, ?Tuesday?, etc.)
Note that thefrequency counts peak exactly on that particularday of the week.
Thus, for example, the word?Monday?
is automatically associated with Day1, and ?April?
associated with Month 4.The creation of temporal association rulesgeneralises the inferences obtained from the pe-riodic data.
Each association rule has the follow-ing information:?
Word ID?
Period Type (Week, Month, etc.)?
Period Number and Score MatrixThe period number and score matrix representa probability density function that shows thelikelihood of a word appearing on a particularperiod number.
Thus, for example, the score ma-trix for ?January?
will have a high score for pe-riod 1 (and period type set to Monthly).
Figure 4shows some examples of extracted associationrules.
The probability density function (PDF)scores are shown in Figure 4 as they are storedinternally (as multiples of the standard deviationof that time series) and are automatically normal-ised during the classification process at runtime.The standard deviation of values in the time se-ries is used instead of absolute values in order toreduce the variance between fluctuations in dif-ferent time series for words that occur frequently(like pronouns) and those that appear relativelyless frequently.Rule generalisation is not possible in such astraightforward manner for the non-periodic data.In this paper, the use of non-periodic data to op-timise the results of the temporal classificationand automatic dating system is not covered.
Non-periodic data may be used to generate specificrules that are associated only with particulardates or date ranges.
Non-periodic data can alsouse information obtained from hapax words andother low-frequency words to generate additionalrefinement rules.
However, there is a danger thatrelying on rules extracted from non-periodic datawill simply reflect the specific characteristics ofthe corpus used to train the system, rather thanthe language in general.
Ongoing research is be-ing performed into calculating relevance levelsfor rules extracted from non-periodic data.4 Temporal Classification and Auto-matic DatingThe periodic temporal association rules are util-ised to guess automatically the creation date of20documents.
Documents are input into the systemand the probability density functions for eachword are weighted and added up.
Each PDF isweighted according to the inverse document fre-quency (idf) of each associated word.
Periodsthat obtain high score are then ranked for eachtype of period and two guesses per period typeare obtained for each document.
Ten guesses intotal are thus obtained for Day of Week, WeekNumber, Month Number, Quarter, and Year (5period types x 2 guesses each).Su M T W Th F S0 22660 10540 7557 772 2130 3264 116721 12461 37522 10335 6599 1649 3222 34142 3394 18289 38320 9352 7300 2543 22613 2668 4119 18120 36933 10427 5762 21474 2052 2602 3910 17492 36094 9098 56675 5742 1889 2481 2568 17002 32597 78496 7994 7072 1924 1428 3050 14087 21468Av 8138 11719 11806 10734 11093 10081 7782St 7357 12711 12974 12933 12308 10746 6930Figure 3: Days of Week Temporal Frequency Dis-tribution for extracted Periodic Componentdisplayed in a Weekly Period Type formatJanuaryWeek 1 2 3 4 5Score 1.48 2.20 3.60 3.43 3.52Month 1 Score 2.95Quarter 1 Score 1.50ChristmasWeek 2 5 36 42 44Score 1.32 0.73 1.60 0.83 1.32Week 47 49 50 51 52Score 1.32 2.20 2.52 2.13 1.16Month 1 9 10 11 12Score 1.10 0.75 1.63 1.73 1.98Quarter 4 Score 1.07Figure 4: Temporal Classification Rules for Peri-odic Components of "January" and "Christmas"4.1 TimeML OutputThe system can output TimeML compliantmarkup tags using TIMEX that can be used byother TimeML compliant applications especiallyduring temporal normalization processes.
If thebase anchor reference date for a document is un-known, and a document contains relative tempo-ral references exclusively, our system output canprovide a baseline date that can be used to nor-malize all the relative dates mentioned in thedocument.
The system has been integrated with afine-grained temporal analysis system based onTimeML, with promising results, especiallywhen processing documents obtained from theInternet.5 Evaluation, Results and ConclusionThe system was trained using 67,000 news itemsselected at random from the GigaWord corpus.The evaluation took place on 678,924 news itemsextracted from items marked as being of type?story?
or ?multi?
in the GigaWord corpus.
Ta-ble 1 presents a summary of the evaluation re-sults.
Processing took around 2.33ms per item.The actual date was extracted from each newsitem in the GigaWord corpus and the day ofweek (DOW), week number and quarter calcu-lated from the actual date.This information was then used to evaluate thesystem performance automatically.
The averageerror for each type of classifier was also calcu-lated automatically.
For a result to be consideredas correct, the system had to have the predictedvalue ranked in the first position equal to the ac-tual value (of the type of period).Type Correct Incorrect Avg.ErrorDOW 218,899(32.24%)460,025(67.75%)1.89daysWeek 24,660(3.53%)654,264(96.36%)14.37wksMonth 122,777(18.08%)556,147(81.91%)2.57mthsQuarter 337,384(49.69%)341,540(50.30%)1.48qtsYear 596,009(87.78%)82,915(12.21%)1.74yrsTable 1: Evaluation Results SummaryThe system results show that reasonable accuratedates can be guessed at the quarterly and yearlylevels.
The weekly classifier had the worst per-formance of all classifiers, likely as a result ofweak association between periodical word fre-quencies and week numbers.
Logical/sanitychecks can be performed on ambiguous results.For example, consider a document written on 4January 2006 and that the periodical classifiersgive the following results for this particulardocument:?
DOW = Wednesday?
Week = 52?
Month = January21?
Quarter = 1?
Year = 2006These results are typical of the system, as par-ticular classifiers sometimes get the period incor-rect.
In this example, the weekly classifier incor-rectly classified the document as pertaining toweek 52 (at the end of the year) instead of thebeginning of the year.
The system will use thefacts that the monthly and quarterly classifiersagree together with the fact that week 1 followsweek 52 if seen as a continuous cycle of weeksto correctly classify the document as being cre-ated on a Wednesday in January 2006.The capability to automatically date texts anddocuments solely from its contents (without anyadditional external clues or hints) is undoubtedlyuseful in various contexts, such as the forensicanalysis of undated instant messages or emails(where the Day of Week classifier can be used tocreate partial orderings), and in authorship iden-tification studies (where the Year classifier canbe used to check that the text pertains to an ac-ceptable range of years).The temporal classification and analysis sys-tem presented in this paper can handle any Indo-European language in its present form.
Furtherwork is being carried out to extend the system toChinese and Arabic.
Evaluations will be carriedout on the GigaWord Chinese and GigaWordArabic corpora for consistency.
Current researchis aiming at improving the accuracy of the classi-fier by using the non-periodic components andintegrating a combined classification methodwith other systems.ReferencesAramburu, M. Berlanga, R. 1998.
A Retrieval Lan-guage for Historical Documents.
Springer VerlagLNCS, 1460, pp.
216-225.Berlanga, R. Perez, J. Aramburu, M. Llido, D. 2001.Techniques and Tools for the Temporal Analysis ofRetrieved Information.
Springer Verlag LNCS,2113, pp.
72-81.Boguraev, B. Ando, R.K. 2005.
TimeML-CompliantText Analysis for Temporal Reasoning.
IJCAI-2005, pp.
997-1003.Box, G. Jenkins, G. 1976.
Time Series Analysis:Forecasting and Control, Holden-Day.Brockwell, P.J.
Fienberg, S. Davis, R. 1991.
TimeSeries: Theory and Methods.
Springer-Verlag.Chatfield, C. 2003.
The Analysis of Time Series.
CRCPress.Ferro, L. Gerber, L. Mani, I. Sundheim, B. Wilson, G.2004.
TIDES Standard for the Annotation of Tem-poral Expressions.
The MITRE Corporation.Filatova, E. Hovy, E. 2001.
Assigning time-stamps toevent-clauses.
Proc.
EACL 2001, Toulouse.Fung, G. 2003.
The Disputed Federalist Papers: SVMFeature Selection via Concave Minimization.
NewYork City, ACM Press.Gaizauskas, R. Setzer, A.
2002.
Annotation Standardsfor Temporal Information in NL.
LREC 2002.Janacek, G. 2001.
Practical Time Series.
Oxford U.P.Kiritchenko, S. Matwin, S. Abu-Hakima, S. 2004.Email Classification with Temporal Features.Proc.
IIPWM 2004, Zakopane, Poland.
SpringerVerlag Advances in Soft Computing, pp.
523-534.Linguistic Data Consortium (LDC).
2003.
EnglishGigaword Corpus.
David Graff, ed.
LDC2003T05.Mani, I. Wilson, G. 2000.
Robust temporal processingof news.
Proc.
ACL 2000, Hong Kong.Mani, I. Schiffman, B. Zhang, J.
2003.
Inferring tem-poral ordering of events in news.
Proc.
HLT-NAACL 2003, Edmonton, Canada.Moldovan, D. Clark, C. Harabagiu, S. 2005.
Tempo-ral Context Representation and Reasoning.
IJCAI-2005, pp.
1099-1104.Mosteller, F. Wallace, D. 1964.
Inference and Dis-puted Authorship: Federalist.
Addison-Wesley.Prager, J. Chu-Carroll, J.
Brown, E. Czuba, C. 2003.Question Answering using predictive annotation.In Advances in Question Answering, Hong Kong.Pustejovsky, J. Castano, R. Ingria, R. Sauri, R. Gai-zauskas, R. Setzer, A. Katz, G. 2003.
TimeML:Robust Specification of event and temporal expres-sions in text.
IWCS-5.Pustejovsky, J. Sauri, R. Castano, J. Radev, D. Gai-zauskas, R. Setzer, A. Sundheim, B. Katz, G.
2004.?Representing Temporal and Event Knowledge forQA Systems?.
New Directions in QA, MIT Press.Schilder, F. Habel, C. 2003.
Temporal InformationExtraction for Temporal QA.
AAAI Spring Symp.,Stanford, CA.
pp.
35-44.Silverstein, C. Brin, S. Motwani, R. 1997.
BeyondMarket Baskets: Generalizing Association Rules toDependence Rules.
Data Mining and KnowledgeDiscovery.Sundheim, B. Gerber, L. Ferro, L. Mani, I. Wilson, G.2004.
Time Expression Recognition and Normali-zation (TERN).
MITRE, Northrop Grumman,SPAWAR.
http://timex2.mitre.org.Yarowsky, D. 1994.
Decision Lists For Lexical Am-biguity Resolution: Application to Accent Restora-tion in Spanish and French.
ACL 1994.22
