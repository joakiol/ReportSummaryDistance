Discourse  P lann ing  as an Opt imizat ion  ProcessIngrid Zukerman and Richard McConachyDepartment of Computer Science, Monash UniversityClayton, VICTORIA 3168, AUSTRALIAemaU: {ingrid,ricky}@bruce.cs.monash.edu.auphone: 't-61 3 905-5202 fax: --I--61 3 905-5146Abst ract .
Discourse planning systems developed to dateapply local considerations in order to generate an initial pre-sentation that achieves agiven communicative goal.
However,they lack a global criterion for selecting among alternativepresentations.
In this paper, we cast the problem of plan-ning discourse as an optimization problem, which allows thedefinition of a global optimization criterion.
In particular,we consider two such criteria: (1) generating the most con-cise discourse, and (2) generating the 'shallowest' discourse,i.e., discourse that requires the least prerequisite information.These criteria are embodied in a discourse planning mecha-nism which considers the following factors: (1) the effect of auser's inferences from planned utterances on his/her beliefs,(2) the amount of prerequisite information a user requiresto understand an utterance, and (3) the amount of infor-mation that must be included in referring expressions whichidentify the concepts mentioned in an utterance.
This mecha-nism is part of a discourse planning system called WISHFUL-II which generates explanations about concepts in technicaldomains.1 In t roduct ionSchema-based Natural Language Generation (NLG) systems,e.g., \[Weiner, 1980; McKeown, 1985; Paris, 1988\], determinethe information to be presented based on common patternsof discourse.
Goal-based planners, e.g., \[Moore and Swartout,1989; Cawsey, 1990\], select a discourse operator if its pre-scribed effect matches a given communicative goal.
If thereis more than one such operator, the operator whose prereq-uisite information is believed by the user is preferred.
How-ever, if all the candidate operators require the generationof discourse that conveys ome prerequisite information, theselection process is either random or the system designer de-termines in advance which operators hould be preferred.In this paper, we cast the problem of planning discoursethat achieves a given communicative goal as an application ofan optimization algorithm.
This approach supports the defi-nition of different optimization objectives, such as generating(1) the most concise discourse; (2) the 'shallowest' discourse,i.e., discourse that requires the least amount of prerequisiteinformation; or (3) the most concrete discourse, i.e., discoursewith the most examples.
The resulting mechanism is part ofa discourse planning system called WISHFUL-II ,  which is adescendant of the WISHFUL system described in \[gukermanand McConachy, 1993a\].Table 1 illustrates the discourse generated by our systemusing the concise and the shallow optimization objectives.Table 1.
Sample Concise and Shallow 'Wallaby' Discoursei Conc ise  Discourse Shallow DiscourseWal lab ies  have a pouch,  Wa l lab ies  are  Narsup ia l s  andwhich i s  l i ke  a pocket,  they come from Lust ra l ia .They  are  l i ke  kangaroos ,  They hop and they  are  3 f t .but they  are  3 f t .
ta l l .
ta l l .
Wal ly  i s  a ua l laby .These texts were generated in order to convey the attributestype, habitat, body parts, height and transportation modeof the concept Wallaby to a user who owns a toy wallabycalhd Wally, and knows something about kangaroos, but isnot familiar with the term pouch.The concise discourse conveys most of the intended in-formation by means of a Simile between wallabies and kan-garoos.
The Simile also yields the erroneous inference thatwallabies are the same height as kangaroos.
To contradictthis inference, the system asserts that wallabies are 3 ft. tall.Since the user does not know that kangaroos have a pouch,this is asserted, and since the user does not know what apouch is, information which evokes this concept is presented.The shallow discourse, on the other hand, uses Wally (thetoy wa.llaby) to convey the body parts of a wallaby withoutnaming them explicitly.
This information is complementedby Assertions about a wallaby's type, habitat, height andtransportation mode.In the next section, we present an overview of WISHFUL-II.
In Section 3, we describe the discourse plaaming mech-anism.
We then discuss the results we have obtained, andpresent concluding remarks.2 Overv iew of  the  SystemWISHFUL- I I  receives as input a conceptto be conveyed, e.g.,Wallaby, a list of aspects that must be conveyed about thisconcept, e.g., habitat and body parts, and a desired level ofexpertise the user should attain as a result of the presenta-tion.WISHFUL-I I  was used to generate descriptive discourse invarious technical domains, such as chemistry, high-school al-gebra, animal taxonomy and Lisp.
It produces multi-sentenceparagraphs of connected English text.
The discourse plan-ning mechanism, which is the focus of this paper, generatesa set of Rhetorical Devices (RDs), where each RD is a rhetor-ical action, such as Assert, Negate or Instantiate, applied toa proposition.
This set of RDs is optimal with respect o agiven optimization criterion, e.g., conciseness or depth.The following steps axe performed by WISHFUL-I I .1.
Content  Se lec t ion  - WISHFUL-I I  consults a model ofthe user's beliefs in order to determine which propositions377th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994must be presented to convey the given aspects of a concept.This step selects propositions about which the user hasmisconceptions, propositions that axe unknown to the user,and propositions that are not believed by the user to theextent demanded by the desired level of expertise.2.
Generat ion  o f  the  Opt i raa l  Set  o f  RDs  - WISHFUL-II searches for a set of RDs that conveys the propositionsgenerated in the previous tep while satisfying a given op-timization objective.
The process of generating candidatesets of RDs considers the following factors: (1) the effect ofinferences from an RD on a user's beliefs; (2) the amountof prerequisite information required by the user to under-stand the concepts in an RD; and (3) the amount of in-formation to be included in referring expressions whichidentify the concepts in an RD.3.
D iscourse  S t ructur ing-  A discourse structuring mech-anism extracts discourse rdations and constraints from theset of RDs generated in Step 2.
It then generates an or-dered sequence of the RDs in this set, where the strongestrelations between the RDs are represented and no con-straints are violated.
Where necessary, the RDs in thissequence are interleaved with conjunctive xpressions thatsignal the relationship between them \[Zukerman and Mc-Conachy, 1993b\].4.
Generat ion  o f  Anaphor lc  Refer r ing  Express ions -Anaphoric referring expressions are generated for RDs thatrefer to a concept in focus.
This process follows the or-ganization of the discourse, since the appropriate use ofanaphora depends on the structure of the discourse.5.
D iscourse  Rea l i za t ion  - The resulting sequence of RDsis realized in English by means of the Functional Unifica-tion Grammar described in \[Elhadad, 1992\].3 Generat ing the Optimal Set of RDsThe main stage of the optimization procedure consists ofgenerating alternative sets of RDs that can convey a set ofpropositions.
The first step in this stage consists of generatingcandidate RDs that can convey each proposition separately.To this effect, WISHFUL- I I  reasons from the propositions de-termined in the content selection step to the RDs that maybe used to convey these propositions.
This reasoning mecha-nism has been widely used in NLG systems, e.g., \[Moore andSwartout, 1989; Cawsey, 1990\].The process of generating a set of RDs that can convey aset of propositions is not a straightforward extension of theprocess of generating candidate RDs that can convey eachproposition separately.
This is due to the foUowing reasons:(1) an inference from an RD generated to convey a propo-sition pl may undermine the effect of an RD generated toconvey a proposition pj; and (2) an RD generated to con-vey a proposition pl may be made obsolete by an RD whichwas generated to convey another proposition, but from whichthe user can infer pi.
Further, it is not sufficient o proposea single set of RDs that can convey a set of propositions,because a set of RDs that initially appears to be promisingmay require a large number of RDs to convey its prerequi-site information or to identify the concepts mentioned in it.Thus, after generating the RDs that can convey each of theintended propositions separately, the optimization proceduremust consider concurrently the following inter-related factorsin order to generate candidate sets of RDs that can conveythe intended propositions: (1) the effect of the RDs in aset on a user's beliefs, (2) the prerequisite information thatthe user must know in order to understand these RDs, and(3) the referring expressions required to identify the conceptsmentioned in these RDs.Owing to the interactions between the RDs in a set, theproblems of generating the most concise set of RDs and gen-erating the shallowest set of RDs are NP-hard 1.
Since thislevel of complexity is likely to be maintained for other opti-mization objectives, we have chosen a weak search procedurefor the implementation of the optimization process.In the following sections, we describe the optimization pro-cess as an application of the Graphsearch algorithm \[Nilsson,1980\], and discuss the implementation of the main steps ofthis algorithm.3.1 The Basic Optimization ProcedureOur optimization procedure, Optimize-RDs, receives as in-put the set of propositions generated in the content selectionstep of WISHFUL-I I .
It implements a simplitied version ofthe Graphsearch algorithm \[Nilsson, 1980\] to generate a setof RDs that conveys these propositions and satisfies a givenoptimization criterion.
The discourse planning considerationsare incorporated uring the expansion stage and the evalua-tion stage of Graphsearch.The expansion stage of our procedure activates algorithmEzpand-sets-of-RDs, which generates alternative minimallysufficient sets of RDs that can convey a set of intendedpropositions (Step 5 in procedure Optimize.RDs).
A set ofRDs is minimally sufficient if the removal of any RD causesthe set to stop conveying the intended information.
Note thata minimally sufficient set of RDs is not necessarily a min-imal set of RDs.
For example, both of the alternatives inTable 1 are composed of minimally sufficient sets of RDs.In this stage, the procedure also determines which prerequi-site propositions must be known to the user to enable a set ofRDs to convey the intended propositions, and which referringexpressions are required to identify the concepts in a set ofRDs.
During the evaluation stage, the procedure ranks eachset of RDs in relation to the other candidates, and prunesredundant RDs (Step 7).
Both the ranking process and thepruning process consider the extent to which a set of RDs islikely to satisfy a given optimization criterion.Algorithm Opt imlze-RDs(  {propositions} )1.
Create a search graph G consisting solely of a start  node swhich contains {propositions}.
Put s in a list called OPEN.2.
LOOP: If OPEN is empty, exit with failure.3.
Select the first node in OPEN, remove it from OPEN.
Callthis node n.4.
If n does not require prerequisite information or referringexpressions, then exit successfuUy (n is a goal node).5.
Expans ion :  M *..- Ezpand-sets-oI-RDs(n).Install the sets of RDs in M as successors of n in G.I Finding a concise set of RDs that conveys a set of propositionsreduces to the Minimum Covering problem, and finding a shal-low set of "RDs that conveys a set of propositions reduces toSatisfiability \[Gaxey and Johnson, 1979\].387th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 19946.
Add the successors of node n to OPEN.7.
Eva luat ion :  Reorder the nodes in OPEN and prune re-dundant nodes according to the given optimization crite-rion.8.
Go LOOP.3.2 Expanding Sets of RDsProcedure Expand-sets-of-RDs receives as input a node tobe expanded, and returns all the minimally sufficient setsof RDs that convey the set of propositions in this node, ac-companied by their respective prerequisite propositions andreferring expressions.
We compute all the minimally suffi-cient sets of RDs, rather than just the minimal set of RDs,because a set of RDs that initially appears to be promisingmay require a large number of RDs in order to convey itsprerequisite information or to identify the concepts in it.Algorithm Expand-sets -o f -RDs(n)1.
Determine RDs that increase a user's belief in each propo-sition in node n. (Not all the RDs generated in this stepaxe capable of conveying an intended proposition by them-selves, but they may be able to do so in combination withother RDs.)
(Sect ion  3.2.1)2.
Use these RDs to construct minimally sufficient sets ofRDs that convey all the propositions in n jointly.
Put thesesets of I~Ds in {A47~D}.
(Sect ion  3.2.2)3.
Determine the prerequisite propositions required by eachset of RDs in {A4gD} so that the user can understand it.
(Sect ion  3.2.3)4.
Determine referring expressions which evoke the conceptsin each set of RDs in {.MT~D}.
(Sect ion  3.2.4)The output of Ezpand.sets-of.RDs takes the form of a setof RD-Graphs.
An RD-Graph is a directed graph that con-tains the following components: (1) the set of propositions tobe conveyed (pl , .
- - ,p,~ in Figure 1); (2) a minimally suffi-cient set of RDs (RD~,..., RDm); (3) the effect of the infer-ences from the RDs in this set on the intended propositions,and possibly on other (unintended) propositions (labelledwid); (4) the prerequisite propositions that enable these RDsto succeed (p~ .
.
.
.
,p~); (5) the relations between the pre-requisite propositions and the main RDs (in thick lines);(6) the sets of RDs that evoke concepts in the main RDs({RD m+l } .
.
.
.
, {RD m+t }); and (7) the relations between theevocative sets of RDs and the main RDs (labelled vm+id)-The main set of RDs and the relations between the RDs inthis set and the propositions to be conveyed are generatedin Step 2 above.
The weight wid contains information aboutthe effect of RDi on the user's belief in proposition pj.
Theprerequisite propositions are generated in Step 3, and theevocative sets of RDs and their corresponding links are pro-duced in Step 4.3.L1 Determining RDsGiven a list of propositions to be conveyed, {p}, in thisstep we propose RDs that can increase a user's belief ineach of these propositions.
To this effect, for each propo-sition pi E {p} we first consider the following RDs: Asser-tion (A), Negation (N),  Instantiation (I) and Simile (S),wl  a .
~, .~- - " 'P~ v~+1,1  , {{RD =+'}~.
.
/  ~RD,~'r "- {RD ~+'1Figure 1.
An RD-Graphwhere different Instantiations and Similes may be generatedfor each proposition.
For example, the proposition \[Bracket-Simplification step-1 +/ - \ ]  may be instantiated with respectto Numbers, e.g., 2(3 + 5) -- 2 ?
8, and to Like Terms, e.g.,2(3z + 5z) = 2 ?
8z.
Those RDs that increase a user's beliefin pl are then put in a list called RD.list(pl).
Next, for eachproposition pi, we consider the RDs in RD-list(pi).
If an in-ference from any of these RDs increases a user's belief in aproposition pj # pi, this RD is added to RD.list(pj).
In aA-dition, if any of the generated RDs yields an incorrect beliefwith respect o a proposition that is not in {p}, this propo-sition is added to {p}, and the process of determining RDsis repeated for this proposition in conjunction with the otherpropositions in {p}.
This is necessary because RDs that axeused to convey this new proposition could affect other propo-sitions previously in {p} and vice versa.
This process stopswhen no incorrect beliefs are inferred.This process is implemented in algorithm Determine-RDs,which receives three input parameters: (1) the propositionsfor which RDs were generated in the previous recursive callto Determine-RDs, (2) the propositions to be considered inthe current activation of Determine-RDs, and (3) the RD-listgenerated in the previous recursive calls.
Its initial activationis Determine-RDs(nil,{p}, nil),and its output is RD.list.Algor i thm Determine-RDs( {oldp},{newp},RD-list)1.
Backward  reason ing:For each proposition p/G {newp} do:(a) Consider the following RDs: Assert(pi), Negate(~pi),Instantiate(pi, I)  and Say-Simile(Oi,O), where the In-stantiation is performed with respect o an instance I ,and the Simile is performed between an object Oi, whichis the subject of proposition pl, and another object O ~.
(Note that several instances I and objects O may beused to generate different Instantiations and Similes, re-spectively.
)(b) Assign to RD-list(pi) the RDs that increase the user'sbelief in pl.2.
Forward  reason ing:(a) For each proposition pi E {newp} determine whetherany RD in RD.list(pi) supports other propositions in{oldp} U{newp}.
If so, add this RD to the RD-lists ofthese propositions.2 Other RDs that may be generated involve subclass or superclassconcepts of the target concept in an intended proposition.
How-ever, the generation ofthese RDs has not been incorporated intoWISHFUL-II yet.397th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994(b) For each proposition pi ~ {oldp} determine whether anyRD in RD.list(pl) supports propositions in {newp}.
Ifso, add this RD to the RD-lists of these propositions.
(c) Append the propositions in {newp} to {oldp}.
(d) If any RD used to convey a proposition pi E {newp}yields incorrect beliefs, theni.
Assign to {newp} the propositions which contradictthese beliefs.if.
Assign to RD-list the result returned by Determine-RDs({oldp},{newp},RD-list)3.
Return( RD-list)To illustrate this process, let us consider a situation wherewe want to convey to the user the following propositions:\[Wallaby hop\] and \[Kangaroo hop\].
In the first stage, our pro-cedure generates two RDs that can convey the proposition\[Wallaby hop\]: Assert\[Wallaby hop\] and Instant iate\[Wal labyhop\], where the Instantiation is performed with respect o awallaby called Wally that is known to the user.
Our proce-dure generates only the RD Assert\[Kangaroo h p\] to conveythe proposition \[Kangaroo hop\] (an Instantiation is not gen-erated since the user is not familiar with any particular kan-garoos).
In the forward reasoning stage, the inferences fromthese RDs axe considered.
If the user knows that wall?biesare similar to kangaroos, the RD generated to convey theproposition \[Kangaroo hop\] can increase the user's belief inthe proposition \[Wallaby hop\], and is therefore added to theRD-listof \[Wallaby hop\].
Similarly, the RDs generated to con-vey \[Wallaby hop\] are added to the RD.listof \[Kangaroo hop\].From the above Assertions the user may also infer incorrectlythat wombats hop.
In this case, a proposition which negatesthis incorrect conclusion, i.e., \[Wombat -~hop\], is assigned to{newp}.
The RDs that can convey this proposition in our ex-ample axe Negate\[Wombat hop\] and Instant iate\ [Wombat-~hop\], where the Instantiation is performed with respect oa wombat called Wimpy that is known to the user.
TheseRDs in turn may yield the incorrect inferences that neitherwallabies nor kangaroos hop, which contradict he intendedpropositions.
However, since the propositions that contradictthese inferences already exist in {oldp}, the process tops.3.2.2 Constructing Minimally Sufficient Sets of RDsIn this step, we generate all the minimally sufficient sets ofRDs that can convey jointly all the intended propositions.For each proposition pi, we first determine whether RDs thatwere generated to convey other propositions can decrease auser's belief in pl.
Next, for each RD in RD.list(pl), we deter-mine whether it can overcome the detrimental effect of these'negative' RDs.
This step identifies combinations of RDs thatcannot succeed in conveying the intended propositions.
It re-sults in the following labelling of the RDs in RD-list: RDsthat cart overcome all negative ffects are labelled with thesymbol Jail\] (the only RDs that may be labelled in this man-ner axe Assertions and Negations); RDs that cannot conveyan intended proposition by themselves are labelled with thesymbol \[-\]; RDs that can convey an intended proposition, butcannot overcome any negative ffects are labelled with \[none\];and the remaining RDs are labelled with the negative RDsthey can overcome.We then use a search procedure to generate all the setsof RDs which consist of one RD from each RD.list.
Thesets of RDs that convey all the intended propositions arethen stored in a list called SUCCEED; and the sets of RDsthat fall to convey one or more propositions axe stored ina list called FAILED, together with the proposition(s) thatfailed to be conveyed.
Additional minimally sufficient sets ofRDs axe then generated from FAILED as follows: we selecta proposition pi that was not conveyed, and create pairs ofRDs composed of the RD that failed to convey pi and eachof the other RDs in RD.list(pl) that is not labelled Jail\] (theRDs that are labelled Jail\] can convey pi by themselves, andtherefore there is no need to combine them with other RDs).Each pair of RDs inherits the negative RDs that can be over-come by its paxents, and may be able to overcome additionalnegative RDs which caused its parents to fail separately.
Foreach pair of RDs, a new set of RDs is created by replacingthe RD which failed to convey pi with this pair of RDs.
Thesearch is then continued for each of these new sets of RDsuntil a minimally sufficient set of RDs is generated or fail-ure occurs again.
In this case, the process of generating pairsof RDs is repeated, and the seaxch is resumed.
If a pair ofRDs fails, then it forms the basis for triplets, and so on.
Thesearch stops when the RD.list of a proposition which failedto be conveyed contains no RDs with which the failed RDs(or RD-tuples) can be combined.A lgor i thm Construct.sets-of-RDs( {p}, RD-list)1.
Initialize two lists, SUCCEED and FAILED, to empty.2.
Determine- I~Ds( nil, {p } , nil).3.
For each proposition p~ E {p}(a) Put in NegRDs(pl) all the RDs in RD-list that have anegative ffect on pi.
(b) Label each RD in RD-list(p~) according to the RDs inNegRDs(pi) whose effect it can overcome.
If there areseveral RDs in NegRDs(pi) then all the combinations ofthese RDs must be considered.4.
Exhaustively generate all the combinations of RDs con-sisting of one RD from the RD-list of each proposition.Consider the combined effect of several RDs to determinewhether a set of RDs conveys completely a set of proposi-tions.5.
Append the successful combinations of RDs to SUCCEED,and remove any sets of RDs in SUCCEED that subsumeother sets of RDs.6.
Append the failed combinations of RDs to FAILED to-gether with the reason for the failure, i.e., the RDs thatfailed and the propositions that were not conveyed.7.
If FAILED is empty, then exit.8.
Assign to CURRD the first set of RDs in FAILED, andremove it from FAILED.9.
Select from CURRD a proposition pi that was not con-veyed, and generate successors of CURRD as follows:(a) Generate children of the RD that failed to convey pi bycombining it with other RDs in RD-list(pi) that axe notlabelled Jail\].
(b) If the failed RD has no children in pl, then remove fromFAILED all the sets of RDs which failed when this RDtried to convey pi, and go to Step 7.407th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994Table 2.
Sample RDs (labelled)Proposition RDs  and labelsPl: \[Wallaby hop\] A(pI) Jail\]I(pl) \[none\]A(p2) \[-\]P2: \[Kangaroo hop\] A(p2) Jail\]A(pl) \[I(-p3)\]x(p~) \[-\]"P3: \[Wombat -~hop\] i \]~\]'(P3) \[A(pl)+I(p1), A(p2)\]I I('~p3) \[I(pl), A(pI)\](c) Attach to each combination of RDs the list of negativeRDs it can overcome.
(d) Create sets of RDs such that in each set the failed RDis replaced with one of its children.10.
Go to Step 5.To illustrate this process, let us reconsider the examplediscussed in Section 3.2.1.
Table 2 contains the RD-lists forthe propositions in this example, where each RD is labelledaccording to the RDs whose negative ffect it can overcome.For instance, Negate(p3)  can overcome the combined nega-tive effect of Assert(p~) and Instant iate(pz) ,  and also theeffect of Assert(p2).
However, it cannot overcome the com-bined effect of Assert(p1)  and Assert(p2),  or Assert(p2)and Instant iate(p1) .
Ins tant ia te(px)  can convey proposi-tion pl, but cannot overcome any negative ffects.
Assert(p2)contributes to the belief in pl but cannot convey it alone.Figure 2 contains part of the search tree generated in Step4 of algorithm Construct-sets-o/-RDs.
Each path in this treecontains one RD from each row in Table 2.
Successful pathsaxe drawn with thick lines and are marked S. Failed paths aremarked F accompanied by the propositions which were notconveyed by the RDs in these paths.
An RD that increasesa user's belief in more than one proposition may appear ina path more than once.
The repeated instances of such anRD appear in brackets, e.g., {Assert(px)}, indicating thatthe RD will be mentioned only once.
In the successful pathto the left, Assert(p1)  can overcome all negative effects toconvey p~.
In addition, it can overcome the negative ffect ofInstant iate( - ,p3)  to convey p2, and Instant iate( - ,p3)  canovercome the negative effect of Assert(p1 ) to convey ",p3.A(pl)A(p~) {A(p,))/\ /\N(p~) I(-,p~) g(p~) I(--,p~)F F F S"nP3 "aP3 P2I(pa)/\N(p3) I(-~p3)S F"~p3F igure 2.
Partial Sample Seaxdl TreeTable 3.
Sets of RDs after the Initial Search1.
Assert(p1) Instantiate(-~p3)SUCOZSSFUL 2.
Assert(p1) Instantiate(pl)Negate(p3)FA ILED3.
Assert(pl) Assert(p2) "~P3Negate(ps)4.
Assert(p1) Negate(p3) P25.
Instantiate(pl) Assert(p2) "P3Negate(p3)6.
Instant iate(pl )  Assert(p2) "~P3Instantiate(-,p3)7.
Instant iate(pl )  Negate(p3) {Pi,P2}8.
Instantlate(pl) {Pl, P2 }Instantiate(-,p3)9.
Assert(p2) Negate(p3) Pl10.
Assert(p2) Instantlate(-~p3) {Pl,'~P3}In the successful path to the right, Assert(p1) together withIns tant ia te (p l )  overcome the negative ffect of Negate(p3)to convey p2, even though neither could do so by itself; andNegate(p3)  can overcome the joint effect of Assert(p1 ) andIns tant la te (p l  ).Table 3 contains the successful minimally sufficient setsof RDs generated by this search and the failed sets of RDsaccompanied by the propositions that were not conveyed.In Step 9 of Construct-sets-o/-RDs, the RDs that failed toconvey a proposition axe combined with other RDs that canincrease the user's belief in this proposition.
For instance,Negate(p3)  is combined with Instantlate(-~p3) for all thepaths where -~p3 failed to be conveyed, and the seaxch iscontinued.
Our procedure does not generate children fromrepeated RDs that failed to convey a proposition, since thiswould yield already existing combinations of RDs.
Table 4contains the minimally sufficient sets of RDs returned by al-gorithm Gonstruet-sets-ofiRDs.
Set 5-6 is obtained by com.-plementing Set 5 in Table 3 with the RD Instantiate(-~p3),and also by complementing Set 6 with the RD Negate(p3).Additional successful sets of RDs are generated by append-ing complen~entaxy RDs to the failed sets of RDs in Table 3.However, these sets subsume Set I, 2 and 5-6, and hence axenot minimally sufficient.
For example, when Set 4 in Table3 is complemented with Instantiate(pl), it yields a set ofRDs  that is equal to Set 2.
This set is removed in Step 5 ofGonstruct-sets-of-RDs.This process ensures that only minimally sufficient setsof RDs  are generated, because it generates RD-tuples onlyfrom the unsuccessful RDs  in the RD-list of a proposition,and it prunes sets of RDs  that subsume other sets of RDs.
Inaddition, this process ensures that all the minimally sufficientsets of RDs  axe generated, because it considers all the RD-tuples resulting from the unsuccessful RDs  in the RD.list ofa proposition.Table 4.
Minimally Sufficient Sets of aDs1.
Assert(p1) Instantiate(-~p3)2.
Assert(pl)  Instant iate(pl)  Negate(p3)5-6.
Instant iate(p l )  Assert(p2) Negate(p3)Instantiate(-,p3)417th International Generation Workshop ?
Kennebunkport, Maine * June 21-24, 19943.2.3 Determining Prerequisite PropositionsThe prerequisite propositions to be conveyed epend on theuser's expertise with respect o the concepts mentioned ina set of RDs, and on the context where these concepts arementioned.
The context influences both the aspects of theseconcepts that must be understood by the user and the extentto which these aspects must be understood.The process of determining the relevant aspects of a con-cept and the required level of expertise is described in \[Zuk-erman and McConachy, 1993a\].
The relevant aspects of aconcept are determined by considering the predicates of thepropositions where a concept is mentioned, and the role of theconcept in these propositions.
For example, in order to un-derstand the RD Assert\[Marsupial has-part pouch\], the usermust know the aspects type and structure of a pouch, i.e.,what it is and what it looks like.
The extent to which a usermust know the selected aspects of a concept depends on therelevance of this concept to the original propositions to beconveyed, i.e., the system demands a high level of expertisewith respect o the more relevant concepts, and a lower levelof expertise with respect o the less relevant ones.After the relevant aspects and required level of expertise ofeach concept have been determined, WISHFUL- I I  applies thecontent selection step described in Section 2 to determine theprerequisite propositions of each concept.
WISHFUL- I I  thenmerges into a single set the prerequisite propositions gen-erated for individual concepts.
This merger is executed be-cause some prerequisite propositions of two or more conceptsmay be conveyed by a single RD.
A special case of this hap-pens when two or more concepts have common prerequisitepropositions.
For example, consider the situation depicted inFigure 3, where prerequisite information for the set of RDs{RD1,RD2} is being conveyed.
RD1 requires the prerequi-site propositions {pl,p2}, while RD2 requires the prerequi-site propositions {p2, p3, p4 }.
If we considered separately theprerequisites of these RDs, we would generate RDs to con-vey {pl,p2}, and {RD4,RDs} to convey {p2,pa,p4}.
Thiswould result in a total of three RDs.
However, by consider-ing jointly all the prerequisite propositions of {RDi, RD2},we will require two RDs only, namely {RD3, RDs}.3.2.4 Evoking the Concepts in a Set of RDsRDs that convey referring information differ from RDs thatconvey prerequisite propositions in that the former identifiesa concept by means of information known to the user, whilethe latter conveys information that the user does not knowabout a concept.
Further, the process of generating referringinformation has the flexibility of selecting the propositionsthat can identify a concept uniquely, while the propositionsFigure 3.l P1 ~ RDz RD1 P~ ~7RD2 pz RD4p4 RDsPrerequisite Propositions of a Set of RDsTable 5.
Sample Referring ExpressionsConcept Lexical Complement ingI tem InformationCompl : Algebraic Termswith one variable Like-Terms "Like Terms" CO~Ttp2 : expressions such as2(3z + 4x)Comp3: both Like TermsAlgebraic- "Algebraic and Unlike TermsTerms Terms" Comp4 : expressions such as2(3z-b 49) and 5(2z -1- 3z)that convey prerequisite information are dictated by the con-text and by the user's expertise.In order to generate referring expressions for the conceptsmentioned in a set of RDs, we propose for each concept a listof candidate lexical items that can be used to refer to it.
Ifthere is a lexical item that identifies each concept uniquelyand is known to the user, the evocation process is finished.However, if there axe concepts that are not identified uniquelyby any of their candidate lexical items, then these lexicalitems axe complemented with additional RDs that help themidentify the intended concepts.
This task is performed byiteratively selecting propositions that identify an intendedconcept until this concept is singled out, and generating RDsthat convey these propositions.
This algorithm differs fromthe procedure described in \[Dale, 1990\] in that we generateseveral alternative sets of complementing RDs in order toavoid dead-end situations where the only identifying infor-mation that is generated for a set of concepts is circular.
Theevocation process then selects the most concise non-circulaxcombination of referring expressions that identifies all theconcepts in a set of RDs.
For example, Table 5 illustratescandidate referring expressions generated for the conceptsLike-Terms and Algebraic-Terms.
Each referring expression iscomposed of a lexical item and a complement 3 .
The non-circular alternatives in this example contain the complements{Co~p~,Comp,}, {co~p~,Co~p~} and {Co~p~,Co~p,}3.3 Two Opt imizat ion  CriteriaAs indicated in Section 3.1, the optimization criterion deter-mines the manner in which the nodes in OPEN are rankedand pruned.
In our implementation we have tried two opti-mization criteria: (1) conciseness and (2) depth.3.3.1 Optimizing the Depth of the Generated RDsWhen optimizing the depth of a set of RDs, the nodes inOPEN are pruned according to the following rule:IF  Depth(hi) = Depth(n j )  AND{ Prerequisites-of(n/) D Prerequisites-of(n j )  OR{ Prerequisites-of(n/) = Prerequisites-of(hi) AND{ IReferring-exp-of(ni)l > IReferring-exp-of(nj)\[ OR{ IReferring-exp-of(n~)l = IReferring-exp-of(n~)l ANDTotal-Weight(n/) > Total-Weight(hi) } } } }THEN remove hi.3 A referring expression may contain a null lexical item, i.e., com-plementing information only.
However, at present this option isnot generated by WISHFUL-II.427th International Generation Workshop ?
Kennebunkport, Maine * June 21-24, 1994Table 6.
Prerequisite Propositions and Total Weight ofMinimaLly Sufficient Sets of RDsSet of RDs TotalWeight1.
Assert(p1) Instantiate(-,ps) 22.
Assert(p1) Instant iate(pl)  2.5Negate(ps)5-6.
Instant iate(pl)  Assert(p2) 3.5Negate(ps)Instantiate(-~ps)Prerequis i teProposit ionsPl l ,P I2,P13P l i ,P l2 ,P l3~PSlP21PsiThe tveight of a node reflects the number of RDs in this nodeand their type.
The total weight of a node is the sum of theweights of the nodes in the path from the root of the searchtree to this node.
All the RDs have a weight of 1, except anInstantiation of a proposition p that accompanies an Asser-tion of p or a Negation of ~p.
Such an Instantiation has aweight of 1 ~, because it does not contain new information,rather it is a continuation of the idea presented in the As-sertion or the Negation.
For example, the Instantiation inSet 1 in Table 6 has a weight of 1, because the instantiatedproposition is different from the asserted proposition.
In con-trust, in Set 2, the weight of the Instantiation is ?
becauseit instantiates the asserted proposition.The above rule is also applied after Step 4 of algorithmExpand-sets-afiRDs to prune the list of minimally sufficientsets of 1RDs (Section 3.2).
It removes a node if its prerequi-sites subsume those of another node.
It considers the numberof referring expressions of a node only when the same pre-requisite propositions are required by two nodes, and con-siders the total weight of a node only when two nodes havethe same prerequisite propositions and the same number ofreferring expressions.
This rule compares only nodes at thesame depth, because even if the prerequisites of a node atlevel i subsume the prerequisites of a node at level i -t- 1, thenode at level i may lead to discourse that has depth i q- 1,while the node at level i ?
1 can lead to discourse of depthi -t- 2 at best.To illustrate the pruning process let us reconsider the min-imally sufficient sets of RDs in Table 4, assuming that theprerequisite propositions required by these sets are as shownin Table 64 .
Here, the pruning rule removes Set 2, since itsprerequisite propositions subsume those of Set 1.The nodes remaining in OPEN are ordered as follows:1.
In increasing order of their depth, so that we expand themore shallow nodes first during the optimization process.2.
In increasing order of the number of prerequisite proposi-tions they require, so that the nodes that contain the setsof RDs with the fewest prerequisites are preferred amongthe nodes at the same level.3.
In increasing order of the number of referring expressionsthey have, so that the nodes with the fewest referring ex-pressions are preferred among the nodes with the samenumber of prerequisite propositions.4.
In increasing order of their total weight, so that the mostconcise set of RDs is preferred when all else is equal.4 The first coefficient of each prerequisite proposition indicatesthe RD for which it is required, e.g., pit is a prerequisite ofAssert(pl).To illustrate this process let us consider the minimally suf-ficient sets of RDs that remain after pruning, namely Set 1and Set 5-6, and assign them to nodes nl and ns-e respec-tively.
Since Set 5-6 has the fewest prerequisite propositions,us-6 will precede nl in OPEN, and will be the next nodeto be expanded by algorithm Optimize.RDs (Section 3.1).
Ifupon expansion of ns-s  we find that there is a minimallysufficient set of RDs that conveys propositions {p21 ,psi } andrequires no prerequisite information, then the node whichcontains this set of RDs is a goal node, and the search isfinished.3.3.2 Optimizing the Number of Generated RDsWhen optimizing the total number of RDs to be presented,the following rule is used to prune the nodes in OPEN:IF Total-Weight(hi) > Total-Weight(hi)  ANDPrerequisites-of(nl) D Prerequisites-of(n./)THEN remove ni.As in depth optimization, this rule is also applied afterStep 4 of algorithm Ea:pand-sets-of-RDs to prune the list ofminimally sufficient sets of RDs.The nodes remaining in OPEN are sorted in increasingorder of their total weight.To il lustrate this process let us consider once more theminimally sufficient sets of RDs in Table 6.
Since the prereq-uisite propositions of Set 2 subsume those of Set 1, and thetotal weight of Set 2 is higher than that of Set 1, Set 2 isremoved in the pruning stage.
The ordering of the remain-ing nodes in OPEN is different from the ordering obtainedfor the depth optimization, i.e., nl precedes ns-e in OPEN,since the total weight of Set 1 is less than the total weight ofSet 5-6.4 Resu l tsWISHFUL- I I  was implemented using Common Lisp on aSPARCstation 2 and on a PC-486.
The system takes lessthan 4 seconds of CPU time to produce English output, andthe optimization process alone takes less than 2 seconds fordiscourse of up to 10 RDs.
Table 1 in Section 1 and Table7 (adapted from an example in \[Moore and Swartout, 1989\])illustrate the output generated by WISHFUL-I I  for the twooptimization criteria we have implemented, viz concisenessand depth.
Appendix A contains examples of the output pro-duced by WISHFUL-I I  when the same discourse is generatedfor the concise and the shallow optimization criteria.Our mechanism can be used as a tool for evaluating differ-ent discourse optimization criteria, where the only require-ment for implementing a new criterion is the modification ofthe pruning and ranking rules described in Section 3.3.
WhenWISHFUL-I I  was tried with the two optimization criteria de-scribed in this paper, it often generated the same discoursewith both criteria, i.e., the most concise discourse was alsothe shallowest.
However, the two optimization criteria pro-duced different discourse when the most concise discoursementioned one or more concepts that were not known to theuser and therefore had to be explained, while the shallowestdiscourse avoided these explanations by presenting a larger437th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994Table 7.
Sample Concise and Shallow 'Lisp' DiscourseConcise Discourse Shallow Discoursesetq  i s  l i ke  ser f .However, the  f i r s targument of se tq  i snot  a genera l i zedvar iab le ,  which i s  anexpress ion  thatreferences a storagelocation.
The firstargument of setq isa simple variable.setq takes two arguments.The first argument ofse tq  i s  a s imp le  var iab le .The second argument ofse tq  i s  a va lue .
Theob jec t ive  of  se tq  i s  toass ign  the  va lue  to  thevar iab le .
For example,( se tq  x ' (a  b) )  resu l t sin x - -> ' (a  b).number of RDs which mentioned ifferent concepts.
In par-ticular, the concise discourse was characterized by the pres-ence of Similes that required some in-depth clarification of anon-source concept s, while the shallow discourse was chaxac-terized by the presence of a Description composed of a list ofAssertions possibly accompanied by Instantiations.5 ConclusionIn this paper, we have cast discourse planning as an opti-mization process which generates discourse that satisfies aspecific optimization criterion.
We have described a weaksearch procedure that implements this process while takinginto consideration the following factors: (1) a user's infer-ences from proposed RDs, (2) the prerequisite informationrequired by the user to understand the concepts mentionedin a set of RDs, and (3) the referring expressions requiredto enable the user to identify these concepts.
Two optimiza-tion criteria have been considered, viz conciseness and depth.The system which implements these ideas has been used togenerate descriptive discourse in various technical domains.REFERENCESA.
Cawsey (1990), Generating Explanatory Discourse.
In:R. Dale, C. Mellish and M. Zock, eds., Current Researchin Natural Language Generation (Academic Press), pp.
75-102.It.
Dale (1990), Generating Recipes: An Overview of Epi-cure.
In: R. Dale, C. Mellish and M. Zock, eds., Cur-rent Research in Natural Language Generation (AcademicPress), pp.
229-255.M.
Elhadad (1992), FUG: The Universal Unifier User Man-ual Version 5.0, Technical Report, Columbia University,New York, New York.M.R.
Garey and D.S.
Johnson (1979).
Computers and In-tractability, A Guide to the Theory o\] NP-Completeness(W.H.
Freeman and Company, San Francisco).K.
McKeown (1985), Discourse Strategies for GeneratingNatural Language Text, Artificial Intelligence 27(1), pp.1-41.J.D.
Moore and W.R. Swartout (1989), A Reactive Ap-proach to Explanation.
In: Proceedings o/ the EleventhInternational Joint Conference on Artificial Intelligence,Detroit, Michigan, pp.
1504-1510.s A source concept in a Simile is a concept from which the intendedinformation is drawn, e.g., kangaroo in Table 1 and setf in Table7 are source concepts; pouch in Table 1 and generalized-variablein Table 7 are non-source concepts.N.
Nilsson (1980), Principles o/ Artificial Intelligence (TiogaPublishing Company, Pale Alto, California).C.L.
Paris (1988), Tailoring Object Descriptions to a User's?
Level of Expertise, Computational Linguistics 14(3), pp.64-78.J.
Weiner (1980), Blab, A System Which Explains Its Rea-soning, Artificial Intelligence 15, pp.
19-48.I.
Zukerman add R.S.
McConachy (1993a), Generating Con-cise Discourse that Addresses a User's Inferences.
In: Pro-ceedings of the Thirteenth International Joint Conferenceon Artificial Intelligence, Chaxnbery, France, pp.
1202-1207.I.
Zukerman and R.S.
McConachy (1993b), An OptimizingMethod for Structuring Inferentially Linked Discourse.
In:Proceedings of the National Conference on Artificial Intel-ligence, Washington, D.C., pp.
202-207.Appendix A: Sample OutputTable 8.
'Racing car' DiscourseAn indycar  i s  an American rac ing  car .
I t  has  avery  poeer fu l  eng ine ,  wide t i res ,  huge brakesand b ig  wings to  make lo ts  o f  dowuforce.
Lo tsof  downforce he lps  i t  go around corners  qu ick ly ,however lo ts  of  dognforce  does  not  he lp  i t  gostraight quickly.
A formula I car is like anindycar, however a formula I car is a Europeanracin E car.Table 9.
'DOS and UNIX' DiscourseDOS i s  an operat ing  sys tem.
I t  has a commandl ine  in ter face ,  which i s  an in ter face  where youtype  commands a t  a text  prompt,  e .g .
,  mkd i r ,  I s .I t  i s  a s ing le  user  operat ing  sys tem and i t  doesnot  a l low mul t i task ing ,  which i s  do ing  more thanone job  a t  a t ime.
UNIX i s  l i ke  DOS, however i ti s  a mul t iuser  operat ing  system and does a l lowmul t i task ing .
Some UNIX commands are  the  sameas DOS, e .g .
,  mkd i r ,  however some are  d i f fe rent ,e .g .
,  pwd.
DOS runs  on PC compat ib les .
In  add i t ionto  PC compat ib les  UNIX runs  on works ta t ions .Table 10.
'Document layout' DiscourseTEX i s  a layout  language fo r  documents.
A layoutlanguage a l lows  you to  cont ro l  the  appearance  ofof your document,  e .g .
,  tex t  s i ze  and p lacement .
I tuses  embedded commands, eh ich  are  commands p lacedw i th in  the  document,  e .g .
,  ~ageno.
These commandsare  executed  a f te r  your  document i s  ed i ted .
T ro l li s  l i ke  TF~, however i t  has d i f fe rent  commands,e .g .
,  .BP.
A wordprocessor  a l so  a l lows  you tocont ro l  the  appearance  of  your document,  howeveri t  i s  not  a layout  language.
Mordprocessors  a l souse embedded commands, however they  are  notexecuted  a f te r  your  document i s  ed i ted ,  they  areexecuted  wh i le  your  document i s  ed i ted .WordPerfect  i s  a vordprocessor .44
