Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 386?391,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsLIG System for WMT13 QE Task: Investigating the Usefulness ofFeatures in Word Confidence Estimation for MTNgoc-Quang Luong Benjamin LecouteuxLIG, Campus de Grenoble41, Rue des Mathe?matiques,UJF - BP53, F-38041 Grenoble Cedex 9, France{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.frLaurent BesacierAbstractThis paper presents the LIG?s systemssubmitted for Task 2 of WMT13 Qual-ity Estimation campaign.
This is aword confidence estimation (WCE) taskwhere each participant was asked to la-bel each word in a translated text asa binary ( Keep/Change) or multi-class(Keep/Substitute/Delete) category.
We in-tegrate a number of features of varioustypes (system-based, lexical, syntactic andsemantic) into the conventional featureset, for our baseline classifier training.After the experiments with all features,we deploy a ?Feature Selection?
strategyto keep only the best performing ones.Then, a method that combines multiple?weak?
classifiers to build a strong ?com-posite?
classifier by taking advantage oftheir complementarity is presented and ex-perimented.
We then select the best sys-tems for submission and present the offi-cial results obtained.1 IntroductionRecently Statistical Machine Translation (SMT)systems have shown impressive gains with manyfruitful results.
While the outputs are more accept-able, the end users still face the need to post edit(or not) an automatic translation.
Then, the issueis to be able to accurately identify the correct partsas well as detecting translation errors.
If we fo-cus on errors at the word level, the issue is calledWord-level Confidence Estimation (WCE).In WMT 2013, a shared task about quality esti-mation is proposed.
This quality estimation task isproposed at two levels: word-level and sentence-level.
Our work focuses on the word-level qual-ity estimation (named Task 2).
The objective is tohighlight words needing post-edition and to detectparts of the sentence that are not reliable.
For thetask 2, participants produce for each token a labelaccording to two sub-tasks:?
a binary classification: good (keep) or bad(change) label?
a multi-class classification: the label refers tothe edit action needed for the token (i.e.
keep,delete or substitute).Various approaches have been proposed forWCE: Blatz et al(2003) combine several featuresusing neural network and naive Bayes learning al-gorithms.
One of the most effective feature combi-nations is the Word Posterior Probability (WPP) asproposed by Ueffing et al(2003) associated withIBM-model based features (Blatz et al 2004).Ueffing and Ney (2005) propose an approach forphrase-based translation models: a phrase is a se-quence of contiguous words and is extracted fromword-aligned bilingual training corpus.
The con-fidence value of each word is then computed bysumming over all phrase pairs in which the tar-get part contains this word.
Xiong et al(2010)integrate target word?s Part-Of-Speech (POS) andtrain them by Maximum Entropy Model, allow-ing significative gains compared to WPP features.Other approaches are based on external features(Soricut and Echihabi, 2010; Felice and Specia,2012) allowing to deal with various MT systems(e.g.
statistical, rule based etc.
).In this paper, we propose to use both internaland external features into a conditionnal randomfields (CRF) model to predict the label for eachword in the MT hypothesis.
We organize the arti-cle as follows: section 2 explains all the used fea-tures.
Section 3 presents our experimental settingsand the preliminary experiments.
Section 4 ex-plores a feature selection refinement and the sec-tion 5 presents work using several classifiers asso-ciated with a boosting decision.
Finally we present386our systems submissions and propose some con-clusions and perspectives.2 FeaturesIn this section, we list all 25 types of features forbuilding our classifier (see a list in Table 3).
Someof them are already used and described in detail inour previous paper (Luong, 2012), where we dealwith French - English SMT Quality Estimation.WMT13 was a good chance to re-investigate theirusefulness for another language pair: English-Spanish, as well as to compare their contributionswith those from other teams.
We categorize theminto two types: the conventional features, whichare proven to work efficiently in numerous CEworks and are inherited in our systems, and theLIG features which are more specifically sug-gested by us.2.1 The conventional featuresWe describe below the conventional features weused.
They can be found in some previous papersdealing with WCE.?
Target word features: the target word itself;the bigram (trigram) it forms with one (two)previous and one (two) following word(s); itsnumber of occurrences in the sentence.?
Source word features: all the source wordsthat align to the target one, represented inBIO1 format.?
Source alignment context features: the com-binations of the target word and one word be-fore (left source context) or after (right sourcecontext) the source word aligned to it.?
Target alnment context features: the com-binations of the source word and each wordin the window ?2 (two before, two after) ofthe target word.?
Target Word?s Posterior Probability (WPP).?
Backoff behaviour: a score assigned to theword according to how many times the targetLanguage Model has to back-off in order toassign a probability to the word sequence, asdescribed in (Raybaud et al 2011).1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/?
Part-Of-Speech (POS) features (using Tree-Tagger2 toolkit): The target word?s POS; thesource POS (POS of all source words alignedto it); bigram and trigram sequences betweenits POS and the POS of previous and follow-ing words.?
Binary lexical features that indicate whetherthe word is a: stop word (based on the stopword list for target language), punctuationsymbol, proper name or numerical.2.2 The LIG features?
Graph topology features: based on the N-bestlist graph merged into a confusion network.On this network, each word in the hypothesisis labelled with its WPP, and belongs to oneconfusion set.
Every completed path passingthrough all nodes in the network representsone sentence in the N-best, and must con-tain exactly one link from each confusion set.Looking into a confusion set, we find someuseful indicators, including: the number ofalternative paths it contains (called Nodes),and the distribution of posterior probabili-ties tracked over all its words (most interest-ing are maximum and minimum probabilities,called Max and Min).?
Language Model (LM) features: the ?longesttarget n-gram length?
and ?longest source n-gram length?
(length of the longest sequencecreated by the current target (source aligned)word and its previous ones in the target(source) LM).
For example, with the tar-get word wi: if the sequence wi?2wi?1wiappears in the target LM but the sequencewi?3wi?2wi?1wi does not, the n-gram valuefor wi will be 3.?
The word?s constituent label and its depth inthe tree (or the distance between it and thetree root) obtained from the constituent treeas an output of the Berkeley parser (Petrovand Klein, 2007) (trained over a Spanish tree-bank: AnCora3).?
Occurrence in Google Translate hypothesis:we check whether this target word appears inthe sentence generated by Google Translateengine for the same source.2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/3http://clic.ub.edu/corpus/en/ancora387?
Polysemy Count: the number of senses ofeach word given its POS can be a reliable in-dicator for judging if it is the translation ofa particular source word.
Here, we investi-gate the polysemy characteristic in both tar-get word and its aligned source word.
Forsource word (English), the number of sensescan be counted by applying a Perl exten-sion named Lingua::WordNet4, which pro-vides functions for manipulating the Word-Net database.
For target word (Spanish), weemploy BabelNet5 - a multilingual semanticnetwork that works similarly to WordNet butcovers more European languages, includingSpanish.3 Experimental Setting and PreliminaryExperimentThe WMT13 organizers provide two bilingualdata sets, from English to Spanish: the trainingand the test ones.
The training set consists of803 MT outputs, in which each token is anno-tated with one appropriate label.
In the binaryvariant, the words are classified into ?K?
(Keep)or ?C?
(Change) label, meanwhile in the multi-class variant, they can belong to ?K?
(Keep), ?S?
(Substitution) or ?D?
(Deletion).
The test set con-tains 284 sentences where all the labels accompa-nying words are hidden.
For optimizing parame-ters of the classifier, we extract 50 sentences fromthe training set to form a development set.
Sincea number of repetitive sentences are observed inthe original training set, the dev set was carefullychosen to ensure that there is no overlap with thenew training set (753 sentences), keeping the tun-ing process accurate.
Some statistics about eachset can be found in Table 1.Motivated by the idea of addressing WCE asa sequence labeling task, we employ the Con-ditional Random Fields (CRF) model (Laffertyet al 2001) and the corresponding WAPITI toolkit(Lavergne et al 2010) to train our classifier.
First,we experiment with the combination of all fea-tures.
For the multi-class system, WAPITI?s de-fault configuration is applied to determine the la-bel, i.e.
label which has the highest score is as-signed to word.
In case of the binary system,the classification task is then conducted multipletimes, corresponding to a threshold increase from4http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm5http://babelnet.org0.300 to 0.975 (step = 0.025).
When threshold =?, all words in the test set which the probability of?K?
class > ?
will be labelled as ?K?, and oth-erwise, ?C?.
The values of Precision (Pr), Recall(Rc) and F-score (F) for K and C label are trackedalong this threshold variation, allowing us to se-lect the optimal threshold that yields the highestFavg = F (K)+F (C)2 .Results for the all-feature binary system(ALL BIN) at the optimal threshold (0.500) andthe multi-class one (ALL MULT) at the defaultthreshold, obtained on our dev set, are shownin Table 2.
We can notice that with ALL BIN,?K?
label scores are very promising and ?C?
la-bel reaches acceptable performance.
In case ofALL MULT we obtain the almost similar aboveperformance for ?K?
and ?S?, respectively, ex-cept the disappointing scores for ?D?
(which canbe explained by the fact that very few instances of?D?
words (4%) are observed in the training cor-pus).Data set Train Dev Test#segments 753 50 284#distinct segments 400 50 163#words 18435 1306 7827%K : %C 70: 30 77: 23 -%K: %S: %D 70:26:4 77:19:4 -Table 1: Statistics of training, dev and test setsSystem Label Pr(%) Rc(%) F(%)ALL BIN K 85.79 84.68 85.23C 50.96 53.16 52.04ALL MULT K 85.30 84.00 84.65S 43.89 49.00 46.31D 7.90 6.30 7.01Table 2: Average Pr, Rc and F for labels of all-feature binary and multi-class systems, obtainedon dev set.4 Feature SelectionIn order to improve the preliminary scores of all-feature systems, we conduct a feature selectionwhich is based on the hypothesis that some fea-tures may convey ?noise?
rather than ?informa-tion?
and might be the obstacles weakening theother ones.
In order to prevent this drawback,we propose a method to filter the best features388based on the ?Sequential Backward Selection?
al-gorithm6.
We start from the full set of N features,and in each step sequentially remove the most use-less one.
To do that, all subsets of (N-1) fea-tures are considered and the subset that leads tothe best performance gives us the weakest feature(not involved in the considered set).
This proce-dure is also called ?leave one out?
in the litera-ture.
Obviously, the discarded feature is not con-sidered in the following steps.
We iterate the pro-cess until there is only one remaining feature inthe set, and use the following score for compar-ing systems: Favg(all) = Favg(K)+Favg(C)2 , whereFavg(K) and Favg(C) are the averaged F scoresfor K and C label, respectively, when thresholdvaries from 0.300 to 0.975.
This strategy enablesus to sort the features in descending order of im-portance, as displayed in Table 3.
Figure 1 showsthe evolution of the performance as more and morefeatures are removed.Rank Feature name Rank Feature name1 Source POS 14?
Distance to root2?
Occur in Google Trans.
15 Backoff behaviour3?
Nodes 16?
Constituent label4 Target POS 17 Proper name5 WPP 18 Number of occurrences6 Left source context 19?
Min7 Right target context 20?
Max8 Numeric 21 Left target context9?
Polysemy (target) 22?
Polysemy (source)10 Punctuation 23?
Longest target gram length11 Stop word 24?
Longest source gram length12 Right source context 25 Source Word13 Target WordTable 3: The rank of each feature (in term of use-fulness) in the set.
The symbol ?*?
indicates ourproposed features.Observations in 10-best and 10-worst perform-ing features in Table 3 suggest that numerous fea-tures extracted directly from SMT system itself(source and target POS, alignment context infor-mation, WPP, lexical properties: numeric, punc-tuation) perform very well.
Meanwhile, oppositefrom what we expected, those from word statis-tical knowledge sources (target and source lan-guage models) are likely to be much less ben-eficial.
Besides, three of our proposed featuresappear in top 10-best.
More noticeable, amongthem, the first-time-experimented feature ?Occur-rence in Google Translation hypothesis?
is themost prominent (rank 2), implying that such an on-line MT system can be a reliable reference channelfor predicting word quality.6http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdfFigure 1: Evolution of system performance(Favg(all)) during Feature Selection process, ob-tained on dev setThe above selection process also brings us thebest-performing feature set (Top 20 in Table 3).The binary classifier built using this optimal sub-set of features (FS BIN) reaches the optimal per-formance at the threshold value of 0.475, andslightly outperforms ALL BIN in terms of F scores(0.46% better for ?K?
and 0.69% better for ?C?
).We then use this set to build the multi-class one(FS MULT) and the results are shown to be abit more effective compare to ALL MULT (0.37%better for ?K?, 0.80% better for ?S?
and 0.15%better for ?D?).
Detailed results of these two sys-tems can be found in Table 4.In addition, in Figure 1, when the size of fea-ture set is small (from 1 to 7), we can observesharply the growth of system scores for both la-bels.
Nevertheless the scores seem to saturate asthe feature set increases from the 8 up to 25.
Thisphenomenon raises a hypothesis about the learn-ing capability of our classifier when coping witha large number of features, hence drives us to anidea for improving the classification scores.
Thisidea is detailed in the next section.System Label Pr(%) Rc(%) F(%)FS BIN K 85.90 85.48 85.69C 52.29 53.17 52.73FS MULT K 85.05 85.00 85.02S 45.36 49.00 47.11D 9.1 5.9 7.16Table 4: The Pr, Rc and F for labels of binary andmulti-class system built from Top 20 features, atthe optimal threshold value, obtained on dev set3895 Using Boosting technique to improvethe system?s performanceIn this section, we try to answer to the followingquestion: if we build a number of ?weak?
(or ?ba-sic?)
classifiers by using subsets of our featuresand a machine learning algorithm (such as Boost-ing), would we get a single ?strong?
classifier?When deploying this idea, our hope is that multi-ple models can complement each other as one fea-ture set might be specialized in a part of the datawhere the others do not perform very well.First, we prepare 23 feature subsets(F1, F2, ..., F23) to train 23 basic classifiers,in which: F1 contains all features, F2 is the Top20 in Table 3 and Fi (i = 3..23) contains 9randomly chosen features.
Next, a 7-fold crossvalidation is applied on our training set.
Wedivide it into 7 subsets (S1, S2, .
.
.
, S7).
EachSi (i = 1..6) contains 100 sentences, and theremaining 153 sentences constitute S7.
In theloop i (i = 1..7), Si is used as the test set andthe remaining data is trained with 23 featuresubsets.
After each loop, we obtain the resultsfrom 23 classifiers for each word in Si.
Finally,the concatenation of these results after 7 loopsgives us the training data for Boosting.
Therefore,the Boosting training file has 23 columns, eachrepresents the output of one basic classifier forour training set.
The detail of this algorithm isdescribed below:Algorithm to build Boosting training datafor i :=1 to 7 dobeginTrainSet(i) := ?Sk (k = 1..7, k 6= i)TestSet(i) := Sifor j := 1 to 23 dobeginClassifier Cj := Train TrainSet(i) with FjResult Rj := Use Cj to test SiColumn Pj := Extract the ?probability of wordto be G label?
in RjendSubset Di (23 columns) := {Pj} (j = 1..23)endBoosting training set D := ?Di (i = 1..7)Next, the Bonzaiboost toolkit7 (which imple-ments Boosting algorithm) is used for buildingBoosting model.
In the training command, we in-voked: algorithm = ?AdaBoost?, and number ofiterations = 300.
The Boosting test set is preparedas follows: we train 23 feature subsets with thetraining set to obtain 23 classifiers, then use them7http://bonzaiboost.gforge.inria.fr/x1-20001to test our dev set, finally extract the 23 probabil-ity columns (like in the above pseudo code).
In thetesting phase, similar to what we did in Section 4,the Pr, Rc and F scores against threshold variationfor ?K?
and ?C?
labels are tracked, and those cor-responding to the optimal threshold (0.575 in thiscase) are represented in Table 5.System Label Pr(%) Rc(%) F(%)BOOST BIN K 86.65 84.45 85.54C 51.99 56.48 54.15Table 5: The Pr, Rc and F for labels of Boostingbinary classifier (BOOST BIN)The scores suggest that using Boosting algo-rithm on our CRF classifiers?
output accountsfor an efficient way to make them predict better:on the one side, we maintain the already goodachievement on K class (only 0.15% lost), on theother side we gain 1.42% the performance in Cclass.
It is likely that Boosting enables differentmodels to better complement each other, in termsof the later model becomes experts for instanceshandled wrongly by the previous ones.
Anotheradvantage is that Boosting algorithm weights eachmodel by its performance (rather than treatingthem equally), so the strong models (come fromall features, top 20, etc.)
can make more dominantimpacts than the rest.6 Submissions and Official ResultsAfter deploying several techniques to improve thesystem?s prediction capability, we select two bestsof each variant (binary and multi-class) to sub-mit.
For the binary task, the submissions in-clude: the Boosting (BOOST BIN) and the Top20 (FS BIN) system.
For the multi-class task, wesubmit: the Top 20 (FS MULT) and the all-feature(ALL MULT) one.
Before the submission, thetraining and dev sets were combined to re-trainthe prediction models for FS BIN, FS MULT andALL MULT.
Table 6 reports the official resultsobtained by LIG at WMT 2013, task 2.
We ob-tained the best performance among 3 participants.These results confirm that the feature selectionstrategy is efficient (FS MULT slightly better thanALL MULT) while the contribution of Boostingis unclear (BOOST BIN better than FS BIN if F-measure is considered but worse if Accuracy isconsidered - the difference is not significant).390System Pr Rc F AccBOOST BIN 0.777882 0.884325 0.827696 0.737702FS BIN 0.788483 0.864418 0.824706 0.738213FS MULT - - - 0.720710ALL MULT - - - 0.719177Table 6: Official results of the submitted systems, obtained on test set7 Discussion and ConclusionIn this paper, we describe the systems submittedfor Task 2 of WMT13 Quality Estimation cam-paign.
We cope with the prediction of qualityat word level, determining whether each wordis ?good?
or ?bad?
(in the binary variant), or is?good?, or should be ?substitute?
or ?delete?
(inthe multi-class variant).
Starting with the ex-isting word features, we propose and add vari-ous of novel ones to build the binary and multi-class baseline classifier.
The first experiment?s re-sults show that precision and recall obtained in?K?
label (both in binary and multi-class sys-tems) are very encouraging, and ?C?
(or ?S?)
la-bel reaches acceptable performance.
A feature se-lection strategy is then deployed to enlighten thevaluable features, find out the best performing sub-set.
One more contribution we made is the proto-col of applying Boosting algorithm, training mul-tiple ?weak?
classifiers, taking advantage of theircomplementarity to get a ?stronger?
one.
Thesetechniques improve gradually the system scores(measure with F score) and help us to choose themost effective systems to classify the test set.In the future, this work can be extended in thefollowing ways.
Firstly, we take a deeper look intolinguistic features of word, such as the grammarchecker, dependency tree, semantic similarity, etc.Besides, we would like to reinforce the segment-level confidence assessment, which exploits thecontext relation between surrounding words tomake the prediction more accurate.
Moreover, amethodology to evaluate the sentence confidencerelied on the word- and segment- level confidencewill be also deeply considered.ReferencesJohn Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
Confidence estimation for machine trans-lation.
Technical report, JHU/CLSP Summer Workshop,2003.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
Confidence estimation for machine trans-lation.
In Proceedings of COLING 2004, pages 315?321,Geneva, April 2004.Mariano Felice and Lucia Specia.
Linguistic features forquality estimation.
In Proceedings of the 7th Workshop onStatistical Machine Translation, pages 96?103, Montreal,Canada, June 7-8 2012.John Lafferty, Andrew McCallum, and Fernando Pereira.Conditional random fields: Probabilistic models for seg-menting et labeling sequence data.
In Proceedings ofICML-01, pages 282?289, 2001.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
Practi-cal very large scale crfs.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguistics,pages 504?513, 2010.Ngoc-Quang Luong.
Integrating lexical, syntactic andsystem-based features to improve word confidence estima-tion in smt.
In Proceedings of JEP-TALN-RECITAL, vol-ume 3 (RECITAL), pages 43?56, Grenoble, France, June4-8 2012.Slav Petrov and Dan Klein.
Improved inference for unlexical-ized parsing.
In Proceedings of NAACL HLT 2007, pages404?411, Rochester, NY, April 2007.S.
Raybaud, D. Langlois, and K.
Sma??
li.
?this sentence iswrong.?
detecting errors in machine - translated sentences.In Machine Translation, pages 1?34, 2011.Radu Soricut and Abdessamad Echihabi.
Trustrank: Inducingtrust in automatic translations via ranking.
In Proceedingsof the 48th ACL (Association for Computational Linguis-tics), pages 612?621, Uppsala, Sweden, July 2010.Nicola Ueffing and Hermann Ney.
Word-level confidenceestimation for machine translation using phrased-basedtranslation models.
In Proceedings of Human Lan-guage Technology Conference and Conference on Empiri-cal Methods in Natural Language Processing, pages 763?770, Vancouver, 2005.Nicola Ueffing, Klaus Macherey, and Hermann Ney.
Con-fidence measures for statistical machine translation.
InProceedings of the MT Summit IX, pages 394?401, NewOrleans, LA, September 2003.Deyi Xiong, Min Zhang, and Haizhou Li.
Error detectionfor statistical machine translation using linguistic features.In Proceedings of the 48th Association for ComputationalLinguistics, pages 604?611, Uppsala, Sweden, July 2010.391
