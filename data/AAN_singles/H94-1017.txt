Session 3: Human Language EvaluationLynette HirschmanMITRE CorporationBedford, MA 01730This session focused on experimental orplanned approachesto human language technology evaluation and included anoverview and five papers: two papers on experimental eval-uation approaches\[l, 2\], and three about the ongoing workin new annotation and evaluation approaches for human lan-guage technology\[3, 4, 5\].
This was followed by fifteen min-utes of general discussion.When considering evaluation, it is important to consider thebasic issues involved in evaluation:?
.Why evaluate: what are the goals of evaluation??
What to evaluate: what function(s) of the system shouldbe evaluated, e.g., what input/outputpairs a e compared??
How to evaluate: what procedures can be used to eva-lute specific system functions (or to grade goodness ofinput/output pairs)??
Where to go from here: what additional evaluationsare needed and what can be developed to support futureresearch?1.
WHY EVALUATE?Evaluation serves a number of purposes:* Cross-system evaluation: This is a mainstay of the peri-odic ARPA evaluations on competing systems.
Multiplesites agree to run their respective systems on a single ap-plication, so that results across systems are comparable.This includes evaluations such as message understanding(MUC)\[6\], information retrieval (TREC)\[7\], spoken lan-guage systems (ATIS)\[8\], and automated speech recog-nition (CSR)\[8\]., Within-system progress: This is perhaps the most im-portant role because it supports incremental system de-velopment, debugging and even hill climbing and auto-mated learning approaches, if fast evaluation methodsare available.?
Understanding design trade-offs: It is well-known thatthere are trade-offs in system design, e.g., between speedand error rate for speech recognition systems; similarly,there may be trade-offs in error recovery and types offeedback in dialogue-based systems.
Appropriate val-uation methods make it possible to design controlledexperiments oinvestigate hese trade-offs.Directing research focus: Evaluation (especially whenassociated with research funding) brings increased atten-tion to the technology being evaluated.
It also fosters in-creased infrastructure to support evaluation, and in turn,infrastructure supports evaluation.
1 The success of theARPA human language technology program can be at-tributed in part to the judicious use of common evaluationto focus attention on particular research issues, resultingin rapid improvement in the technology, increased shar-ing of technical information, and broader participationin the research activities.2.
WHAT TO EVALUATE?Once we decide to evaluate, the first question is what to eval-uate?
Where do we put probes to inspect he input and output,in order to perform an evaluation?
This issue is discussed inthe Sparck Jones paper\[ 1 \].
In some cases, we can evaluatethe language technology in isolation from any front-end orback-end application, as shown in Figure 1, where probes areinserted on either side of the language interface itself.
Thisgives us the kind of evaluation used for word error rate inspeech (speech in, transcription out) or for machine transla-tion, as proposed in the Brew/Thompson paper (source text in,target ext out)\[2\].
This kind of evaluation computes outputas a simple function of input to the language system.Unfortunately, it is not always possible to measure a mean-ingful output- for example, researchers have struggled longand hard with measurements for unders tand ing  - how cana system demonstrate hat it has understood?
If we had ageneral semantic representation, then we could insert a probeon the output side of the semantic omponent, independentof any specific application.
The last three papers (\[3, 4, 5\])take various approaches to the issue of predicate-argument1The Penn Treebank parse annotations provide an interesting case whereannotation supported evaluation.
By creating a theory-neutral descriptionof a correct parse,  the Treebank annotation enabled researchers to take thenext step in agreeing to use  the parse annotations (bracketings) as a "goldstandard" against which to compare system-derived bracketings\[9\].
Thisevaluation, in turn, has enabled interesting automated teaming approaches toparsing.99lii OUTPUTFigure 1: Evaluat ing Language Input /Outputstructure in an attempt to define a more semantically-basedand application-independent measure.Right now, we can only measure understanding by evaluatingan interface coupled to an application - Figure 2 shows theapplication back-end included inside the evaluation.
This al-lows us to evaluate understanding in terms of getting the rightanswer for a specific task, as is done in the Air Travel In forma-tion (ATIS) system, which evaluates language input/databaseanswer output pairs.
However, this means that to evaluatespoken language understanding, it is necessary to build anentire air travel information system.E EEE!
7!.:-:::::.-..
::-:-.-..:\[ !E\[EiE~ !
!\[\[ ~ i{77\[i\[i7i7 iii~iil~,.~ i EEEEE 77\[ o UTPUT L-C~>'~i"I:~:;:~:,:~:~:~!~:~:~:,:,:~:,:,~ i ..................................................... ~:7:iiil"l -~"I,p.,,, "k , _  E,,~..~o,, ............... j lFigure 2: Evaluat ing Language Interface Plus BackendFinally, for certain kinds of applications, particularly inter-active applications, it is appropriate to enlarge the scope ofevaluation still further to include the users.
For interactivesystems, this is particularly important because the user re-sponse determines what the system does next, so that it isnot possible to use pre-recorded data.
2 Increasingly complexhuman-computer interfaces, as well as complex collaborativetools, demand that a system be evaluated in its overall contextof use (see Figure 3).?
.
~ O  :7::~:~:~..-...-~1...-~-.
: ~ ~:~:E:E:~:~:::::" " ~  ........... OUTPUTI  a:D /Figure 3: Evaluating Language Interfaces inContext of Use3.
HOW TO EVALUATEWe must not only decide what inputs and outputs to use forevaluation; we must decide how to evaluate these input/output2pre-recorded data allows the same dam to be used by all participatingsites, effectively removing human variability as a factor in the evaluation.pairs as well.
Evaluation seems relatively easy when there isan intuitive pairing between input and output, for example,between speech signal and transcription atthe word or sen-tence level.
The task is much more complex when there iseither no representation for the output (how to represent un-derstanding?)
or in situations where the result is not unique:what is the correct ranslation of a particular text?
What isthe best response to a particular query?
For such cases, it isoften expedient to rely on human judgements, provided thatthese judgements (or relative judgements) are reproducible,given a sufficient number of judges.
Evaluation of machinetranslation systems\[lO\] has used human judges to evaluatesystems with differing degrees of interactivity and across dif-ferent language pairs.
The Brew and Thompson paper\[2\] alsodescribes reliability of human judges in evaluating machinetranslation systems.
Human judges have also been used inend-to-end evaluation of spoken language interfaces\[11\].4.
WHERE TO GO FROM HERE?Because valuation plays such an important role in drivingresearch, we must weigh carefully what and how we evalu-ate.
Evaluation should be theory neutral, to avoid bias againstnovel approaches; it should also push the frontiers of whatwe know how to do; and finally, it should support a broadrange of research interests because valuation isexpensive.
Itrequires ignificant community investment in infrastructure,not to mention time devoted to running evaluations and par-ticipating in them.
For example, we estimate that the ATISevaluation required several person-years to prepare annotateddata, a staffof two to three people at NIST over several monthsto run the evaluation, time spent agreeing on standards, andmonths of staff effort at participating sites.
Altogether, the an-nual cost of an evaluation certainly exceeds five person-years,or conservatively at least $500,000 per evaluation.
Given thislevel of investment, i  is critical to co-ordinate effort and ob-tain maximum leverage.The last three papers\[3, 4 5\] all reflect a concern to developbetter evaluation methods for semantics, with a shared fo-cus on predicate-argument valuation.
The Treebank anno-tation paper\[3\] discusses the new predicate-argument annota-tion work under Treebank.
The paper by Grishman discussesa range of new evaluation efforts for MUC, which are aimedat providing finer grained component evaluations.
The lastpaper, by Moore, describes a similar, but distinct, effort to-wards developing more semantic evaluation methods for thespoken language community.5.
D ISCUSSIONThe discussion began with the question: can we afford threesomewhat similar but distinct predicate-argument valua-tions?
The resulting interchange helped to clarify the rela-tionship between these three proposals.
Both Marcus and Gr-ishman argued that the Treebank annotation should directlysupport he MUC-style predicate-argument valuation out-lined in \[4\], although the Treebank annotations may be a sub-100set of what is used for MUC predicate-argument valuation.The relation of the spoken language "predicate-argument"evaluation to the other two was less clear.
Moore explicitlystated uring the discussion (and Marcus agreed) that the Tree-bank annotation is quite different (more syntactic and more"surface") than the predicate-argument no ation planned forspoken language.
Moore believed that a deeper level (lesssyntactic and more semantic) was needed to meet the needsof (some parts of) the spoken language community.
Thus,although the spoken and written language communities havean opportunity to converge on some common annotation andevaluation metrics, this may well not happen.
These an-notation and evaluation approaches are, however, "work-in-progress" and economic and time considerations may causesome convergence, ven while theories and research agendasremain distinct.References1.
Sparck Jones, K., "Towards Better NLP System Evaluation,"this volume.2.
Brew, C., and Thompson, H. S., "Automatic Evaluation ofComputer Generated Text: A Progress Report On theTextEval?
Project", this volume.3.
Marcus, M., Kim, G., Marcinkiewicz, M. A., Maclntyre, R.,Ferguson, M., Katz, K. and Schasberger, B., "The Penn Tree-bank: Annotating Predicate Argument Structure," this volume.4.
Grishman, R., "Whither Written Language Evaluation," thisvolume.5.
Moore, R. C., "Semantic Evaluation for Spoken Language Sys-tems," this volume.6.
Sundheim, B. and Chinchor, N., "Survey of the Message Un-derstanding Conference," Proc.
of the Human Language Tech-nology Workshop, ed.
M. Bates, Princeton, March 1993.7.
Harman, D., "Overview of the Second Text Retrieval Confer-ence," this volume.8.
Pallett, D., Fiscus, J., Fisher, W., Garofolo, J., Lund, B.,Pryzbocki, M., "1993 Benchmark Tests for the ARPA Spo-ken Language Program" dais vohnne.9.
Black, E., et al, "A Procedure for Quantitatively Comparingthe Syntactic Coverage of English Grammars," Proc.
of theSpeech and Natural Language Conference, d. P. Price, 1992.10.
White, J. S., O'Connell, T., "Evaluation in the ARPA MachineTranslation Program: 1993 Methodology," this volume.11.
Hirschman, L. et al, "Multisite Data Collection and Evaluationin Spoken Language Understanding," Proc.
of the Human Lan-guage Technology Workshop, ed.
M. Bates, Princeton, March1993.101
